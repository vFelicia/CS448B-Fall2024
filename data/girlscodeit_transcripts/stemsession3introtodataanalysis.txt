00:00 - hello everyone my name is andrea and i'm
00:03 - here with john may
00:04 - and today we'll be um introducing a data
00:08 - analysis
00:09 - for scientific research but you can
00:13 - really um do this wherever you want
00:17 - and so i'll be using python
00:21 - and so what is data science i like to
00:24 - think of data science
00:25 - as telling a story so if we look at
00:29 - right here
00:31 - we can see that it's just um a bunch of
00:34 - numbers thrown at you but
00:37 - to me i don't know where i'm supposed to
00:40 - be looking at
00:41 - where i'm supposed to be focusing my
00:43 - attention
00:45 - to so data science is converting raw
00:48 - data
00:49 - into something more visually appealing
00:52 - like a graph and so from this graph
00:55 - we can see or tell a story
00:59 - we can find patterns trends
01:02 - relationships
01:04 - like for example we can see how
01:06 - transmittance
01:08 - is affected by different wavelengths
01:12 - so from graphs or from data
01:15 - visualizations
01:17 - we can find answers that we didn't
01:20 - even know existed
01:23 - if we just look at our raw data
01:28 - so there are two main types of data we
01:31 - have
01:31 - qualitative data which is talking about
01:34 - like a quality of something
01:36 - and this is based on observations or
01:39 - personal
01:40 - experience and so delving deeper
01:44 - we have nominal data which just
01:47 - stands for basically data that has
01:51 - a name and it doesn't have any
01:54 - certain order so for example we have
01:57 - gender or religion and etc
02:01 - and then we have ordinal data and
02:04 - um the first three um letters
02:08 - kinda reminds me of order so this has
02:11 - data this is data with some like
02:15 - sort of sequence or order
02:18 - so for example we have like letter
02:20 - grades right like you have
02:23 - a b c d and f
02:26 - so these have some sort of order to it
02:30 - but it's still qualitative data and then
02:33 - we have
02:34 - quantitative data which is just
02:37 - basically
02:37 - numbers and so we have discrete
02:41 - and these are whole integer
02:44 - numbers and therefore counting
02:48 - so example would be 14 toddlers
02:51 - and for discrete you would just have
02:53 - like dots
02:54 - on the graph um
02:58 - so it can't be connected and
03:01 - but for continuous that's any value
03:05 - that can be measured and so here we can
03:08 - have decimals
03:09 - an example would be like temperature
03:14 - and over here we have an example
03:17 - of qualitative data which
03:20 - is nominal because it doesn't have
03:24 - any sort of order comedy
03:28 - does not have a higher precedence
03:31 - then let's say trauma
03:35 - so now it's your turn to try for the
03:38 - following determine if
03:40 - it's qualitative or quantitative and
03:43 - then determine what type
03:45 - so go ahead and pause the video
03:50 - okay so for the color of m ms
03:53 - we ask ourselves is it a quality or is
03:56 - it
03:56 - number and the color is the quality
04:02 - and then we ask is it um nominal
04:06 - or ordinal so does it have order
04:09 - or is it just like a name and
04:12 - it's nominal because like
04:16 - let's say orange does not
04:19 - is not a greater importance than let's
04:22 - say
04:23 - brown um for the number of
04:26 - like white m m's
04:30 - that's fine uh this is a number so
04:34 - it's quantitative and is it discrete or
04:38 - continuous so hypothetically we can only
04:42 - have like whole m ms
04:46 - so this is discrete
04:50 - and for customer satisfaction from 1 to
04:54 - 10
04:55 - this is qualitative and you might be
04:58 - asking well why
04:59 - isn't it quantitative because it's 1 to
05:02 - 10
05:03 - that is true but if we just look at
05:06 - customer satisfaction
05:08 - we can't really measure that as a number
05:11 - it's more like a quality like if
05:13 - something is bad
05:14 - or satisfactory or um
05:18 - even excellent so that's qualitative
05:22 - but because we have a 1 to 10 then
05:25 - it's original because it has some sense
05:29 - of order state 1 is being worst
05:33 - and 10 is being the best
05:36 - and for monthly profit this is
05:39 - quantitative
05:41 - and it is continuous because
05:44 - we can have decimals
05:49 - okay and so to help
05:52 - us actually do something with
05:55 - the data we can take advantage
05:59 - of python libraries and
06:02 - python is an excellent language to use
06:05 - for data science because we can use
06:09 - a more wide variety of libraries
06:14 - and the first i'll only be talking
06:17 - about the first five
06:20 - but keep in mind for this session we'll
06:23 - only be
06:24 - using numpy and matplotlib
06:28 - so for pandas pandas is a
06:31 - python library that is very easy
06:35 - to use and we take we mainly take
06:38 - data from csv files
06:42 - and from this csv file we can add
06:46 - delete merge or manipulate
06:50 - data frames and a data frame is
06:54 - basically like a spreadsheet or a deity
07:00 - csv is a data frame
07:05 - and then we have numpy and numpy is
07:08 - ideal
07:08 - for arrays specifically arrays with the
07:12 - same
07:13 - or consistent data type and so we can
07:16 - add
07:17 - multiply uh reshape even flatten
07:21 - arrays and for psi pi
07:24 - this is a more advanced
07:28 - math so here we can deal with ordinary
07:31 - differential equations
07:34 - uh signal processing integration and etc
07:39 - and then for matplotlib this
07:42 - deals with
07:46 - a visual representation
07:50 - of data and so we can create
07:54 - line plots bar graphs
07:57 - etc and matplotlib gives you
08:00 - the opportunity
08:04 - to like create labels for your x and
08:07 - y-axis
08:08 - and even like legends and
08:12 - seaborn is an extension of matplotlib so
08:15 - this deals with more advanced
08:17 - statistical graphics like correlation
08:19 - linear regression
08:20 - and multi-plot grids and then we also
08:24 - have
08:24 - stats models poppy
08:27 - vocals scikit-learn and cares
08:31 - there are several things we should do to
08:34 - make sure
08:35 - that the data is clear and tidy enough
08:38 - to read by the computer be read by the
08:40 - computer
08:41 - and the process is called data
08:43 - pre-processing
08:45 - there are two steps in data
08:47 - pre-processing and the first is called
08:49 - data cleaning
08:50 - with the first small step called dealing
08:53 - with the
08:54 - missing data so the missing data usually
08:57 - appear when we are combining more than
08:59 - one data sets
09:01 - for example there are two materials that
09:04 - i selected for
09:05 - my scientific research the first is the
09:08 - silicon
09:08 - and the second is the silicon dioxide
09:11 - and we can observe that the wavelength
09:14 - of these two materials does not match
09:16 - with each other
09:18 - the first wavelength for both is 300
09:21 - yet the second wavelength for silicon
09:24 - is 302 the second for silicon dioxide is
09:28 - 310.
09:30 - so in order to make sure that the two
09:32 - materials matches with match with each
09:35 - other
09:36 - we need to make some changes for the
09:38 - first file
09:40 - and i have delete 302
09:44 - to 308 leaving only 300 and 310
09:49 - like what i've shown in the third file
09:51 - to match with the silicon dioxide
09:55 - this is the first solution that i put in
09:58 - pbt
09:58 - which is deleting the mismatching data
10:01 - before merging
10:02 - the two data sets and the second
10:05 - solution is to
10:06 - add the missing data based on the
10:08 - conclusions
10:10 - yet bear in mind that the second
10:12 - solutions have risks
10:14 - because it's kind of like making up a
10:17 - data
10:18 - on your by yourself and you cannot
10:20 - guarantee the
10:21 - accuracy of the data and it might leads
10:24 - to the bias of the final conclusion
10:26 - or maybe the inaccuracy of the final
10:29 - conclusion
10:30 - so you need to pay a lot of attention if
10:34 - you're
10:34 - choosing the second solution
10:37 - and the second small step in data
10:39 - cleaning is called dealing with the
10:40 - noisy data
10:42 - noisy data is are a set of data that are
10:46 - in
10:46 - a large number and sometimes include
10:49 - meaningless data
10:50 - so in order to solve the problems of
10:53 - having large number of data
10:55 - we need to make some several small
10:58 - changes
10:59 - for example one change is called the
11:01 - binning
11:02 - the bringing means that we separate the
11:04 - data into smaller groups
11:06 - which is called beans for example
11:10 - for the first materials that i've chosen
11:12 - the silicon
11:14 - we have the wavelength actually from 300
11:17 - to
11:19 - 1200 so it's about
11:21 - 900 data and
11:24 - it is impossible for us to plot all
11:27 - those dots
11:28 - on one single graph i mean it's possible
11:31 - but
11:31 - it may looks a bit messy and we cannot
11:34 - draw
11:36 - conclusions relatively easily so
11:39 - in order to help us draw the conclusion
11:42 - more easily
11:44 - we actually combine like the data
11:48 - beginning at 300 maybe to 400 as one
11:51 - group
11:51 - or s1 bing and 400 to 500
11:56 - as another group another being so in
11:59 - this way
12:00 - we only have about eight or nine
12:04 - a group compared with previously we have
12:08 - about
12:08 - yeah 900 dots 900 groups
12:12 - so by only having eight or nine groups
12:15 - we can make the graph
12:17 - easier and also we can draw the
12:19 - conclusion from the graph easier
12:24 - the second solution selecting the key
12:26 - data is to solve the problems of having
12:29 - a lot of minimalist data
12:31 - for example let's look at an excel file
12:36 - let's say we have seven categories of
12:38 - the data
12:39 - from delta p to negative systematic
12:43 - arrow
12:44 - yeah let's say we only need delta p and
12:47 - positive statistical arrow to plot this
12:49 - graph
12:50 - so we can just simply ignore other
12:52 - categories like
12:54 - um that in second column the third
12:57 - column the fourth column
12:59 - um the sixth column the seventh column
13:01 - the eighth column we can just
13:03 - all ignore them by merely selecting the
13:07 - first column and the fifth column
13:09 - we can plot a really clear graph with
13:11 - these two categories
13:13 - maybe the x-axis is delta p the first
13:16 - column
13:16 - and the y-axis is the positive
13:18 - statistical arrow
13:20 - the fifth column and this is what we
13:24 - called
13:27 - selecting the key data and the second
13:30 - solution for
13:31 - dealing with noisy data
13:34 - um there's a thing that needs to be paid
13:37 - special attention
13:39 - which is we need to be really really
13:41 - careful with the outliers
13:44 - outliers aren't noisy data they can
13:48 - they are either the inaccurate data
13:51 - or data that deviates so much from the
13:54 - general distribution
13:56 - and we have other steps to deal with
13:59 - them for example we can just simply
14:00 - remove them
14:02 - or cap them or we can
14:06 - replace them with a new data and so on
14:09 - but the process is are different from
14:11 - how we deal with the noisy data
14:14 - so we need to bear in mind with that
14:18 - and here comes our second step in data
14:21 - preprocessing
14:22 - the data transformation one of the basic
14:25 - small steps
14:26 - in data transformation is called
14:28 - normalization
14:30 - a normalization is the calculations we
14:32 - use to guarantee
14:33 - that the data sources used are in the
14:36 - same unit system
14:38 - so we need to employ normalization
14:41 - and while we are combining more than one
14:44 - data sources
14:46 - like the circumstance that we encounter
14:48 - in the last slide
14:50 - when we are doing
14:53 - dealing with the missing data
14:56 - so let's take an example
15:01 - let's say we have those green dots in
15:03 - the plots
15:04 - that represent this a certain data that
15:06 - relates to
15:08 - um proton proton collision we also have
15:11 - the gray lines that
15:13 - relates to the data stat
15:16 - about the proton proton coalition
15:18 - however
15:20 - by looking at this part we can figure
15:22 - out that the unit for the green dots
15:25 - does not match with the unit of the gray
15:27 - lines
15:29 - the first is the number divided by being
15:31 - width times another number
15:33 - in bracket and the second is just simply
15:36 - number in bracket
15:37 - so what i have done in order to unify
15:40 - the two units is that
15:42 - i i just make all of the data's in the
15:47 - gray lines divided by
15:48 - the bandwidth times another number
15:51 - which is done by um yeah down by
15:55 - the black figure that i put forward this
15:58 - is the python code
15:59 - so after i implemented this
16:03 - python code i achieved transforming the
16:06 - unit of the gray lines from number in
16:09 - bracket
16:09 - to number divided by being width times
16:12 - another number
16:13 - in bracket in this way we can see that
16:16 - the gray lines
16:17 - matches with the green dots pretty well
16:20 - and this is the result
16:22 - of unifying the unit systems
16:25 - of the two sets of data
16:32 - so after data pre-processing we can now
16:35 - import the data
16:36 - into the python ide yet
16:39 - there is also a small step we need to do
16:42 - before we finally import the data into
16:44 - the ide
16:46 - which is transforming the data format
16:49 - so let's say for the excel file
16:52 - presented in this ppt
16:55 - it's really hard to read by the ide
16:58 - yet for the text file on the right side
17:02 - the ide can read it really clearly so
17:06 - what i have done to make the raw data in
17:09 - the excel file to be read
17:11 - by the python ide is to copy and paste
17:15 - the data
17:16 - from excel file to the text file and
17:19 - split each data with a comma
17:22 - or a space is okay so
17:25 - in this way we have gained this text
17:27 - file and this text file can
17:30 - be easily read by the python ide
17:34 - and this process is called cleaning the
17:36 - data formats
17:39 - so after we clean the data formats we
17:42 - can finally
17:43 - read the data in the python ide
17:46 - and i have put forward two python lines
17:49 - to read the data
17:51 - and there are lots of lines other than
17:54 - these two
17:55 - since python have so many expressions on
17:58 - one function so
18:01 - the reason why i put forward these two
18:03 - python lines is just because i
18:06 - use these two yeah there are some slight
18:09 - differences between these two lines
18:12 - so for the first python lines
18:16 - the first data it gains from
18:19 - um the data file is zero
18:22 - yet for the second python line the first
18:25 - data
18:26 - is just 0.471
18:29 - and it makes us way more convenient to
18:32 - read all those datas
18:33 - um because for the first example
18:37 - since the lines can only reach the data
18:41 - one digit by one digit we need to
18:45 - make some other python codes to
18:48 - separate each real data like
18:52 - 0.471 from 0 from
18:55 - 0.942 by manually typing in those python
19:00 - codes to identify
19:03 - digit from comma or from space and so
19:07 - that's way more difficult so in most
19:10 - circumstances
19:11 - i just choose the second python line
19:14 - because
19:14 - it makes me more easy in selecting each
19:18 - data
19:19 - since the data that selected
19:21 - automatically by this
19:23 - partisan line is already a complete real
19:26 - data
19:29 - and the word data in these two python
19:32 - lines
19:32 - is actually numpy array and we don't
19:36 - have to worry about what
19:37 - what does it mean because we haven't
19:39 - learned it from our girls coded
19:42 - courses yet we the only thing that we
19:45 - need to know in this session is that
19:48 - data a br in bracket b in bracket
19:52 - just simply means that the
19:55 - data in the eighth line a
19:58 - and in column b yeah that's all we need
20:02 - to know
20:02 - in this session
20:09 - so after we finishing importing the data
20:13 - into the ide now we need to have a
20:16 - process to transform the data
20:18 - into something like some information
20:21 - and insight like i've mentioned in the
20:23 - first slide in order for us to
20:26 - reach a conclusion by looking at insight
20:30 - figuring out some patterns between
20:33 - x axis and y axis and so on
20:37 - so um one way to
20:40 - visualize the data is to plot all those
20:43 - data
20:44 - in some graphs and the
20:47 - two most typical graphs are histogram
20:50 - and scatter plot of course we have a lot
20:53 - more data
20:54 - like the color map the bar chart the pie
20:57 - chart
20:58 - um yet here i've just mentioned
21:01 - histogram and scatterplot
21:03 - as an example and you can figure out
21:07 - all those other graphs by just searching
21:10 - on
21:11 - the matplotlib in bing or google
21:16 - so below the small
21:20 - the small lines are the python lines to
21:23 - plot a histogram
21:24 - and the python lines to plot a scatter
21:27 - plot
21:28 - and if we look at what matplot lifts at
21:36 - here the ax the beams the ranged
21:40 - densities
21:41 - are all parameters of the
21:44 - histogram in the maple lip so
21:48 - let's say we have a data
21:51 - of some ages like
21:55 - age 10 h12 h13
21:59 - and we can just put those ages in x
22:02 - which is the numpy array and by
22:05 - simply putting the x in the histogram
22:09 - this this python line will help us
22:12 - automatically generate a histogram of
22:15 - the
22:16 - distribution of the age yeah
22:19 - so we can check all these parameters on
22:23 - the map
22:23 - lib website
22:26 - right here so we have the x4 array
22:30 - the bings for angel sequence or stream
22:33 - of
22:34 - this is an optional parameters so
22:36 - whenever you come across the word
22:38 - optional in
22:40 - the parameters it means you can choose
22:43 - to
22:43 - put it in your python lines or you can
22:47 - just simply ignore it
22:48 - like if you need things in your
22:50 - histogram you just
22:52 - you can just write beans in your python
22:54 - lines if you do not
22:55 - need that you can just ignore it yeah
22:58 - and we have
22:59 - range density weights
23:02 - cumulative and you can figure out
23:05 - when or when to use it or not to use it
23:09 - by reading the small
23:11 - words beneath the parameters
23:18 - so the results just shown
23:23 - on the right side the gray line
23:26 - is actually histogram and it's because
23:30 - that
23:31 - i have used the parameters of
23:34 - um some parameters that just
23:39 - just delete the shadows of the histogram
23:41 - away so
23:42 - we can only see the gray lines right
23:44 - here and
23:46 - this is the result of myself
23:48 - implementing the
23:50 - python code of the histogram and it's
23:53 - similar for the scatter plot
23:55 - like you have these python lines right
23:57 - here
23:58 - and you can choose the parameters that
24:00 - you need
24:01 - and we should notice a difference
24:03 - between the scatter plot and histogram
24:06 - the difference is that in the scatter
24:08 - plot we have
24:09 - both x and y yet in histogram we only
24:13 - have x
24:14 - it's because that for each dot in a step
24:17 - at a plot the dot includes both
24:21 - x value and y y value
24:24 - like for this green dots we have x
24:27 - value of 0.5 yet we also
24:31 - have y value about 0.28
24:35 - or so yet for the grade lines
24:38 - which is the value in the histogram the
24:41 - only thing we have
24:42 - is the x value and for the
24:46 - y-axis it represents the frequency
24:49 - that for each data value
24:53 - on the x-axis for the data sets
24:56 - of the gray lines so we need to
24:58 - distinguish the difference between the x
25:01 - and the x and y and the histogram and
25:03 - the scatter plot
25:06 - so that's a basic introduction of the
25:10 - data visualization
25:11 - and you can check more information about
25:13 - that in the matplot
25:15 - lib website and yeah
25:18 - if you're interested in that you can
25:21 - even
25:21 - try it in the python ide by yourself
25:27 - so nice work for all this session
25:30 - and if you're going to do any scientific
25:33 - research
25:34 - and in the future i hope that the
25:37 - information that
25:38 - i and andrea presented in this
25:42 - session can help you and can be really
25:44 - practical for your future uses
25:47 - and that's all thanks for your listening