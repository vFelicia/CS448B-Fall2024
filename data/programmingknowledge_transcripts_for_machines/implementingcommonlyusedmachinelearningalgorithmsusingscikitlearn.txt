first we need to understand what machine learning means with the use case or with an example we need to know the different types of machine learning and also how machine learning can be used in different industries in the world for this you can check out the intro to ml video by the programming knowledge channel and also read multiple blogs online which explain ml better the next thing is python programming skills as we all know python is the widely used programming language for machine learning so we must have the python program basics right to start learning machine learning again the programming knowledge channel has passion tutorial videos which you can check out to get your basics right and you can also follow different tutorials and blog posts the next thing is basic mathematical skills machine learning has a lot of mathematics involved behind the scenes and to understand machine learning properly one must have these basics in their bags first is linear algebra basics we should know how to solve a linear equation in one variable or two variables and for probability basics we just need to do the simple very simple basis of probability uh the basic formula and how probability works after this we can branch out to do different types based on what our end goal is so the first is academic machine learning if you want to produce novel research write research papers read research papers build machine learning tools from scratch and understand the deep underlying concepts then you can refer to these prerequisites before jumping into the academic machine learning part of it so let's start the first thing is strong mathematical concepts as you'll be producing novel research new research or try to read complicated research papers and also build tools from scratch you should have a strong mathematical foundation in algebra we must know how logarithms work how matrices work how to perform matrix multiplication and things like that in calculus we should know what does the concept of derivative what is the chain rule partial derivatives and gradients in statistics we should know the simple statistics basics which is mean median mode then how outliers work the ability to read an histogram and some algorithms like conditional probability next is intermediate python programming since we'll be writing some tools from scratch and also write the code for the new type of algorithms which we read or try to implement we should have some intermediate python programming skills and use some complicated structures which can help us write better code for this we can learn list comprehensions lambda functions and also learn some thirdparty libraries which make our jobs easier and the last thing is strong programming science computer science fundamentals as a computer science graduate or not being one we should have some simple data structures and algorithms in our back just to make sure that the concepts which we implement have been implemented in the most efficient way possible this is academic machine learning this helps you get into academia faster and also will help you have a better learning curve than before the next is industrial machine learning now if you want to work in industry or in a company or be a software engineer with a machine learning specialization if you want to create new products business value and apply existing offtheshelf tools to solve business problems then you can refer to this as the prerequisite so in a company or being a software developer with a machine learning specialization the first thing which you need to know is intermediate or advanced python programming so as you'll be writing a lot of code and we'll be busy focusing more on writing code than doing research the companies will expect you to know a lot of python to use the tools which are off the shelf again things like list comprehension lambda functions and also understanding how thirdparty libraries work with machine learning the second thing is similar to the academic machine learning part of it which is strong computer science fundamentals because these fundamentals will be useful no matter which branch are you in you are in and next we come to the most important part data analysis so before trying to implement machining algorithms in a company one must need to know what the data is uh what the data is trying to represent and also understand how the data works so people should have a proficiency with frameworks and tools such as pandas matplotlib and cbon in the upcoming videos we'll talk about how these frameworks work and also look at them with the code lastly people who would like to work in a company as a software engineer or as a data scientist they should have minimum linux skills they should have they should know how to work on the terminal how to work on the command line how to use that to the benefit and also make sure that they have enough to write most of the simple code on the terminal now these are some prerequisites which i feel can give you a bit of a better learning curve when you're trying to jump into machine learning you can also explore both the branches you can try academic machine learning first and you can jump onto industrial or vice versa now let's talk about some resources all the resources to learn these prerequisites will be in the description below and also this notion document which you see here will also be in the description below if you have any questions please use the comment section below to ask them in this video we'll start with what is linear regression the intuition behind the algorithm and understanding all the elements of it let's have an overview of linear regression it is one of the most basic machine learning algorithm and easy to implement the algorithm has already been used in statistics and is a common process using many applications of statistics in the real world so what is linear regression by definition it is used for finding a linear relationship between the target and one or more predictors the idea behind linear regression is to fit the observations of two variables into a linear relationship between them in simple terms the task is to draw the line that is best fitting or closest to the points where the x y coordinates are observations of the two variables which are expected to depend linearly on each other in more simpler terms given two variables x and y the model can predict values of y given future observations of x this idea is used to predict variables in countless situations example the outcome of political elections or the behavior of the stock market or the performance of a professional athlete there are two types of linear regression simple and multiple in this video we'll only cover simple linear regression so we had talked about drawing a line that is closest to the points which are our variables this line can be modeled based on a linear equation shown on the slide here x and y are our variables which will be present in the data set the motive of the linear regression algorithm is to find the best values for a0 and a1 which we call the parameters before moving on to the algorithm let's have a look at two important concepts you must know before understanding linear regression cost function the cost function helps us to figure out the best possible values for a0 and a1 which would provide the best fit line for the data points the difference between the predicted values and the ground truth measures the error difference we square the error difference and sum over all the data points and divide that value by the total number of data points this provides the average squared error over all the data points therefore this cost function is also known as the mean square error by cost i mean the cost of incorrectly predicting a data point or how far the line is from the point mathematically we find the mean distance between all the points and we want to minimize that distance so that the line fits the data perfectly to minimize the cost function we use a technique called gradient descent the next important concept needed to know linear regression is gradient descent gradient descent is a method of updating a0 and a1 to reduce the cost function the idea is that we start with some values of a0 and a1 and then we change these values iteratively to reduce the cost gradient descent helps us how to change the values to update a0 and a1 we take gradients from the cost function to find these gradients we take partial derivatives with respect to a 0 and a1 now to understand how partial derivatives work you would require some calculus but if you don't it is all right you can take it as it is the partial derivatives are the gradients and they are used to update the values of a0 and a1 alpha is the learning rate here which is a hyper parameter that you must specify a small learning rate could get you closer to the minima but takes more time to reach the minima a larger earning rate converges sooner but there is a chance that you could overshoot the minima this can be depicted on the slide to implement the algorithm we have two choices we can use the circuit learn library to import the linear regression model and use it directly or we can write our own regression based model based on the equations above in this video we'll implement linear regression using the scikit learn library to learn more about what is linear regression you can check out the link to the video in the description the entire code and the data set can be downloaded using the link in description which will direct you to this github page after this download the data directory and store that in your projects folder let's start with the implementation i'm using a jupyter notebook here but you can implement the same in a single python file as well first we start with importing all the libraries and the dependencies that are required we need the pandas library to manipulate the data set next we nee we import the matplotlib library to visualize our data and the results we use the pi plot here and lastly we need the linear regression model from the circuit learn library which is the main dependency so from sklearn dot linear model we import the linear regression class now we start with reading our data into the code using pandas make sure that the data directory is in the projects folder we use the read csv function here because our data is in the csv format we move inside a data directory and use the advertising data set let's just check if the spelling is right and yes that should be good now to see what the data looks like we use the head function which is data dot head as you can see here the column unnamed 0 is redundant and hence we need to remove it to remove a column we can use the drop function in pandas we have to remove the unnamed column name 0 and we specify the axis equal to 1. here axis is equal to 1 to remove the entire column and the axis is equal to 0 to remove only an index as you can see in the output the unnamed zero column is being has been removed all right now our data is clean and it is ready for linear regression for simple linear regression let's consider only the effect of tv ads on sales before jumping right into the modeling let's look at what the data looks like we use matplotlib a popular python plotting library to make a scatter plot let's set a size of the plot which can be 16 comma 8. then we generate a scatter plot using the scatter function in which we have the tv ads and the sales let's color the scatter plot with a black dot with black dots and as you can see there is a clear relationship between the amount spent on tv ads and the sales let's see how we can generate a linear approximation of this data first we convert these values into vectors and then store them into two variables so x is equal to data of the tv ads their values and we convert them into vectors using the dcf function which is minus 1 and 1 then we do do the same for the sales which is date of sales and their values which are converted into vectors now after this we use the fit function of the linear regression class to fit a line on the x and y values let's name the variable reg which is linear regression object and then we call the fit function on x and y the minimization of the cost function using gradient design works behind the scenes here behind the fit function to learn more about the cost function and how gradient descent works you can check out the introduction to linearization video in the description below now we have fit a straight line to the data set and let's visualize this using a scatter plot again now since the code for visualizing the best fit line is long i'm going to copy paste it but the entire code will be available in the github repo now here first we predict all the values on the x data set and then we use those predictions to make a line on the scatter plot here the dots will be in black and the line will be in blue the x label will be the money spent on the tv ads and the y label will be the sales from the graph it seems that a simple linear regression model can explain the general impact of amounts spent on tv ads and sales this is how we implement linear regression in scikit learn live using the scikitlearn library we'll talk about what is logistic regression classification techniques are an essential part of machine learning and determining applications approximately 70 percent of problems in data science are classification problems there are a lot of possible classification problems that are available but the logic regression is common and is a useful regression method for solving the binary classification problem logistic regression can be used for various classification problems such as spam detection diabetes prediction if a customer will purchase a product or not whether the user will click on a given ad link or not logistic regression is one of the most simple and commonly used machine learning algorithm for two class classification it is a statistical method to predict binary classes it its basic fundamental concepts are also used in deep learning it is a special case of linear regression where the target variable is categorical in nature it uses a log of odds as a dependent variable logistic regression predicts the probability of a binary event utilizing a logic function as we can see here we need to categorize the data in two different categories and our job is to define the line which does that now why is it called logistic regression if it's a classification mechanism contrary to popular belief logistic regression is a regression model the model builds a regression model to predict the probability that a given data entry belongs to the category numbered as one just like linear regression assumes that the data follows a linear function logistic regression models the data using the sigmoid function linear regression gives you continuous output but loyalty regression provides a constant output an example of continuous output would be house price prediction or stock price prediction an example of discrete output is predicting whether a patient has cancer or not or predicting whether a customer will click on an add or not now let's modify the linear regression equation we had seen in the previous video for logistic regression we apply something called as a sigmoid function on the linear linear regression equation let's see what the sigmoid function is the sigmoid function also called the logistic function gives an sshaped curve that can take any real valued number and map it into a value between 0 and 1. if the curve goes to positive infinity y predicate predicted will be 1 and if the curve goes to negative infinity y predicted will become zero if the output of the sigmoid function is more than zero point five we can classify the outcome as yes or a one and if it is less than zero point five we can classify it as zero or a no for example if the output is 0.75 we can say in terms of probability as there is 75 percent chance that patient will suffer from cancer just like we have a cost function in linear regression we need one for logistic regression as well which has to be reduced to obtain the best fit line but the cost function used in linear regression will not work here if you try to use the linear regression cost function in a logistic regression problem you would end up with a nonconvex function a weirdly shaped graph with no easy way to find minimum global point hence we have a different cost function for linear regression for logistic regression the cost function is defined as minus log h of x if y equal to 1 and minus log 1 minus h of x if y equal to 0. this is the cost the algorithm pays if it predicts a value h theta of x while the actual cost label turns out to be y by using this function we will grant the convexity to the function the gradient descent algorithm has to process there is also a mathematical proof of how we get this cost function which is outside the scope of this video the final cost function can be seen at the bottom of the slide now we have the hypothesis function and the cost function and we are almost done it is now time to find the best values for our parameters in the cost function or in other words to minimize the cost function by running the gradient decision algorithm the procedure is identical to what we did for linear regression to understand more about gradient descent please find the link in the description which will explain in regression and also gradient descent to minimize the cost function we have to run the gradient descent function on each parameter and that is how logistic regression works at the end we get the best parameters that can work with the hypothesis function to predict whether a data point belongs to one class or the other now for the implementation we can either use the circuitron library to import the logitech regression model and use it directly or we can also write our own model based on the equations above logistic regression is amongst the most commonly known core machine learning algorithm out there with its cousin linear regression it has many applications in businesses one of which is the pricing optimization in this video we will learn how to code logistic regression in python using the scikit learn library to solve a bit pricing problem let's have some recap logistic regression is a predictive linear model that aims to explain the relationship between a dependent binary variable and one or more independent variables the output of logical regression is a number between 0 and 1 which you can think of as being the probability that a given class is true or not the output is between 0 and 1 because the output is transformed by a function which is usually the sigmoid function let's start implementing logic regression in python with a very simple example note that the intent of this video is only to implement a very basic logistic regression model using circuit learn without using a trained test split on the data set and with minimum data visualization so let's start first we import all the dependencies that are required we need matplotlib for visualization so we need the pi plot as plt next is numpy to store our data and finally we need the sklearn logistic regression model which we can use to fit our data yeah so the next is that we have to define a data set let's generate a data set that will be using to learn how to apply logistic regression to a pricing problem the bid price is contained in our x variable while the result a binary lost or one category is encoded as one or zero in our y variable here i have defined my own data set but for complicated or more advanced uh examples you can also import a data set from kegel and use that let's go ahead and visualize this data using matplotlib to gain a better understanding of what we're dealing with let's have a scatter plot of x and y and let's actually give it a title of pricing bins and the x label is going to be price and the y label is the binary output 1 or loss so status 1 is a 1 and 0 is a lost so here each point above represents a build that we participated in on the xaxis you can see the price that was offered and on the yaxis you see the result if we won the bid or not our goal is to use logistic regression to come up with a model that generates the probability of winning or losing a bed at a particular place in python logistic regression is made simple thanks to the circuit learn module for the task at hand we'll be using the logic regression class by the sql linear model so log reg let's start let that be the name of the variable and logistic regression class where the regularization strength c is equal to 1.0 and the solver uh let that be lb fgs which is an optimization just like we did in descent and for multiclass we specify ovr because we're using a binary classification problem here so multiclass is equal to ovr for binary classification the next step is to fit the logical equation model by running the fit function of a class and before we do that we transform our x array into a 2d array as is required by the sql model this is because we only have one feature which is the price and if we had more than one feature our array would already be 2d so let's reshape our data as one comma minus one comma one and finally we can fit our model so log reg dot fit capital x and y now we have a model and now let's predict some data if we wanted to run the prediction on a specific price you can also do that as shown so let's print a prediction uh let's say we need to find whether we've lost or won if the price is 110 so as you can see on the graph above if the price is 110 we should be winning so let's try that as you can see when the price is around 110 which is between 100 200 we win the bed and if the price is around 275 we should lose the bet so let's try that again which 275 we should lose the bet as you can see we have lost the bed this is a very basic implementation of florida's regression using the scikitlearn library to understand how the algorithm works on a data set as we have a basic understanding now we can start working with the kegel data set and also study more about data analytics and data visualization this video is introduction to support vector machines for an indepth understanding please refer to the links in the description support vector machines are perhaps one of the most popular and talked about machine learning algorithms they were extremely popular around the time they were developed in the 1990s and continued to be the goto method for a high performing algorithm with a little tuning support vector machine is a supervised machine learning algorithm which can be used for classification challenges in addition to performing linear classification svms can efficiently perform on nonlinear classification as well so what are support vector machines it is a discriminative classifier formally defined by a separating hyperplane in other words given label training data the algorithm outputs an optimal hyperplane which categorizes new examples in simple terms an svm model is a representation of the examples as points in space mapped so that the examples of the sub examples of the separate categories are divided by a clear gap that is as wide as possible let's visualize this in this graph we can see that the two classes are separated by the largest cap possible the space between the red line and the closest point to the red line is called a margin so for one dimensional data the support vector classifier is a point for two dimensional data the support vector classifier is aligned as seen in the previous slide for three dimensional data the support vector is a plane and for four dimensional or more the support vector classifier is a hyperplane so let's talk about the hyperplane now a hyperplane in an ndimensional euclidean space is a flat n minus onedimensional subset of that space that divides the space into two disconnected parts so a line is a hyperplane or even a 2d plane for a 3d data is a hyperplane svm algorithms use a set of mathematical functions that are defined as the kernel sometimes it is not possible to find a hyperplane or a linear decision boundary for some classification problems if we project the data into higher dimension from the original space we may get a hyperplane in the projected dimension that helps to classify the data let's see what we mean here as shown in the figure it is impossible to find a line to separate the two classes green and blue in the input space but after projecting the data into higher dimension we were able to classify the data using the hyper plane hence kernel helps to find a hyper plane in the higher dimension space without increasing the computation cost much usually the computational cost will increase if the dimension of the data increases the mathematics behind how kernels work is out of scope for this video the svm model needs to be solved using optimization procedure you can use a numerical optimization procedure to search for the coefficients of the hyperplane the most popular method for fitting an svm is the sequential minimal optimization smo method that is very efficient it bakes the problem into sub problems that can be solved analytically by calculating rather than numerically by searching or optimizing in the next video we'll implement the support vector machine we have two choices here we can either use the circuit learn library to import large the svm model and use it directly or we can write our own model based on the equations above a support vector machine is a type of supervised machine learning classification algorithm svms were introduced initially in the 1960s and were later refined in 1990s however it is only now that they are becoming extremely popular owing to their ability to achieve brilliant results svms are implemented in a unique way when compared to other machine learning algorithms in this video we'll implement support vector machines with the help of the scikitlearn library for the implementation our task is to predict whether a bank currency note is authentic or not based on four attributes of the note those attributes are the skewness of the wavelet performed image the variance of the image entropy of the image and the kurtosis of the image this is a binary classification problem and we will use the svm algorithm to solve this problem the detailed information about the data and a link to download the data set can be found in the description download the data set and store it locally on a computer where you intend to write the implementation so let's start the implementation by importing all the necessary libraries first we need to import pandas as we need to store our data and the data frame and the next is numpy followed by a matplotlib and then we need the circuit learn modules here we'll be using the train test split module which we have had not in the previous videos for linear knowledge equation implementations but for svm we'll actually have a train test split and see how the algorithm works on the test data next we need to import the svm class which is svc support vector classifier and at last we need to evaluate our algorithm so we'll need some matrix for that and let's use the classification report module and yeah i think we're done so now let's import the data okay i think we have some problem yeah it's a spelling mistake dot metrics i have a lot of spelling mistakes here okay now let's import the data into a program to read the data from the csv file the simplest way is to use the read csv method of the pandas library the following code which i'm going to write is going to read the bank currency node data into a pandas data frame let's have bank data equal to pd dot read csv and the name of the csv file is bill authentication which you can find in the description of the video okay now there are virtually limitless ways to analyze data sets with a variety of python libraries for the sake of simplicity we will only check the dimensions of the data and see the first few rows to see the rows and columns of the data execute the following command and in the output you'll see 137 to comma 5 this means that the bank node data set has 1372 rows and five columns now to get a feel of how our dataset actually looks let's actually see the first five rows of the data set using the head command and here you can see the first firozor data set and you can also see the attributes of the dataset are numeric the label is also numeric that is class one or zero let's preprocess the data before training the model data preprocessing involves two steps first dividing the data into attributes and labels and second dividing the data into training and testing sets to divide the data into attributes and labels execute the following code let x equal to the attributes so bank data dot drop one and y equal to bank data class which is the label in the first line of the script in the cell all the columns of the bank data data frame are being stored in x except the class column which is the label column the drop method drops this column in the second line only the class column is being stored in the y variable at this point of time x variable contains attributes while the y variable contains corresponding labels once the data set is divided into attributes and labels the final preprocessing step is to divide the data into training and test sets luckily the model selection library of the cyclic learn library contains the trained test split method that allows us to seamlessly divide the data into training and test sets let's write the code for that x strain comma x test comma y train comma by test is equal to train test split x comma y and we want the test size to be twenty percent uh yeah we have divided the data into training and testing sites now is the time to train our scm on the training data circuit learn contains the svm library which contains builtin classes for different svm algorithms since we are going to perform a classification task we will use the support vector classifier class which is written as svc in the circuit learns svm library this class takes one parameter which is the kernel type this is very important in the case of a simple svm we simply set this parameter as linear since simple scms can only classify linearly separable data let's write the code for that so svc classifier is equal to svc and the kernel is equal to linear the fit method of the fvc fsbc class is called to train the algorithm on the training data which is passed as a parameter to fit the mod fit the men the fit method svc classy fire dot fit x train comma y train to make predictions the predict method of the svc class is used so why prediction is equal to sv classifier dot predict x test and let's actually print vibrate to see our predictions and as you can see the algorithm has been run on the xtest data and all the predictions have been saved in the vibrate variable and we can see the predictions for each of the row for x text now to evaluate the algorithm confusion matrix precision recall and f1 measures are the most commonly used metrics circuit learns matrix library contains the classification report which can be readily used to find out the values for these important metrics so let's actually print the classification report for y test and why spread so here as you can see the most important metric which we can see is the output of the accuracy of algorithm which is 99 this is a very basic implementation of svm using the cyclic learn library and now you can go ahead and implement the algorithm on different data sets from kegel etc this video is an introduction to random forest for indepth understanding please refer to the links in the description a big part of machine learning is classification that is we want to know what class an observation belongs to the ability to precisely classify observations is extremely valuable for various business applications like predicting whether a particular user will buy a product or not or whether a loan has to be given to a person or not in this video we'll talk about the random forest classifier random forest is a flexible easy to use machine learning algorithm that produces even without hyper parameter tuning a great result most of the time it is also one of the most used algorithms because of its simplicity and diversity it can be used for both classification and regression so before learning about random forest one must know how decision trees work so make sure that you know what are decision trees before watching this video you can find a very good explanation of decision trees in the link in the description so let's start what are random forests random forest is a supervised learning algorithm the forest it builds is an ensemble or a group of decision trees usually trained with the bagging method we'll talk about bagging a little late in the video the general idea of bagging method is that a combination of learning models increases the overall result now let's see what this means in simple layman terms random forests build multiple decision trees and merges them together to get a more accurate and stable protection each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model's prediction in the figure six decision trees predict one and one predicts a zero hence the final prediction of the classifier is one now the fundamental concept behind random forest is a simple but a powerful one the wisdom of crowds now what does that mean a large number of relatively uncorrelated modules or decision trees operating as a committee will outperform any of the individual constituent models for individual decision trees this is the most fundamental concept in random forest the low correlation between models is the key the reason for this wonderful effect is that the trees protect each other from the individual errors while some trees may be wrong many other trees will be right so as to group so as a group of trees are able to move in the correct direction now let's see how the random foils algorithm works first create a bootstrap bootstrapped dataset by randomly selecting a samples from the original data set we can pick sample from or we can pick the same sample more than once then create a decision tree using the bootstrap data set but only use a random subset of columns in each step now go back to step 1 and repeat make a new bootstrap data set and build a tree considering a subset of variables at each step this results in a wide variety of trees now the variety is what makes random forests more effective than individual decision trees now how do we measure or ensure that the trees divers these trees diversify each other bagging helps us here bootstrapping the data and using the aggregate to make a decision is known as bagging random forest takes advantage of this by allowing each individual tree to randomly sample from a data set with replacement resulting in different trees next is feature randomness each tree in a random forest can pick one can pick only from a random subset of features in a normal decision tree when it is time to split a node we consider every possible feature and pick the one that produces the separation between the observation in the left node versus those in the right node in contrast each tree is a random forest can pick only from a random subset of features this forces even more variation amongst the trees in the model and ultimately results in lower correlation across trees and more diversification so since we are selecting random features in multiple decision trees they ensure that this will diversify all of the decision trees when combined together now how do we implement this for the implementation we have two choices we can either use the circuit learn library to import the random forest model and use it directly or we can write our own model from scratch so in this video we will implement random forest using the cyclic learn library let's start with defining a problem now the task here is to predict whether a bank currency note is authentic or not based on four attributes which are the variance of the image wavelet transformed image skewness entropy and kurtosis of the image this is a binary classification problem and we will use a random forest classifier to solve this problem now for the data set you can download and learn more about the data set from the link in the description let's start with importing the required libraries unlike my previous implementations where i import all the libraries at once this time i'll import the library only when it is required so initially we need numpy and pandas library to handle the data so let's start with that let's import pandas as pd and import num pi as np and let that run and we're good to go now let's import the data set into our code so data set equal to pandas dot read csv is the name of the function and the name of the csv file is bill authentication dot csv and i think i'm right here let me just check yes awesome now let's get a high level view of the data set and let's do that by executing the following command so data set dot head and as we can as you know we can see the first five uh rows of the data set and you can see the variance the skewness sculptosis entropy and the class now here when you see the first five rows you can see that the values in the data set are not very well scaled so we will have to scale the data before training it so to do all of that let's divide the data set into x and y variables which are the attributes and the labels so x is going to be data set dot lock to separate the values and let's do 0 to 4 so that it's 0 to 3 and fourth is the class from zero indexing and we need the values and let y be equal to data set dot i lock and we just need the last index which is four so as these follow zero indexing we need the first four columns zero to three which is variance q and squared as an entropy in the x variable and the last way last column the fourth column class in the y variable okay after this let's divide the data set into a train and test split so for that we'll need the train test split module from sklearn so sklearn dot model selection we import train test and split now let's have our variables ready so like strain x test uh y train and y test and let's start give a function call here so x and y let the test size be twenty percent so tesla is equal to zero point two and we don't need a random state so let's put that to zero uh i think we're good to go let me just check back again and if we have an error so s size it shouldn't be test should be test size sorry about that yeah and we're good to go so next let's apply some feature scaling on our data so that the data can be uh really scaled and proper when we actually train it so for that we need something from the preparsing module of sql so sk learn dot pre processing import standard scalar scalar then let's scale both x train and x test so let's call the class first and make an object so we have sc as the object and now let's scale xtrain first sc dot fit transform x train and x test is equal to sc.fit transform x test and now we have scaled a data set we can train a random forest to solve this classification problem let's do that with the random forest classifier so we can get that from the sk learn ensemble module so on some so since random forest is an ensemble of many decision trees uh the ensemble module will contain the random forest classifier and um for rest classifier and then let's make this ready make the class ready so let's say class if fire is equal to random for this classifier and first we need uh the n estimators so the end decision trees let's say we have 20 of those then a random state of zero again after which we are going to fit our classifier so classifier dot fit x train comma y train and let's actually save our predictions so classifier dot predict x test awesome and let's see what the errors are so i think i have some uh error here so from sk learn hot random forest classifier cannot be let me just see what the error is oh i see it there has to be a capital f sorry about that and yeah we're good to go now the random forest classifier takes in n estimators as a parameter uh the parameter defines the number of trees in a random form list and we are using 20 t's here so for for classification problems the matrix used to evaluate algorithms are accuracy confusion matrix precision recall and f1 values luckily the circuit turn library provides all these metrics out of the box so let's actually use this matrix to see how good our model performed on this data set so from sk learn dot metrics module we're going to import a few things so classy vacation report is going to be the first thing confusion matrix is next and we need the accuracy to see how good our model is and let's just print all of them now so confusion matrix is first and why test why bread next is classification report so classification report again y test and y predicted and the last is the accuracy score for the same parameters and let's see how this works and again we have some errors here so let's solve them cannot import confusion so you have to stop misspelling things wrong yes so as you can see at the last uh print accuracy score our accuracy is 98 so that is good enough and we can see the other metrics that are used for classification problems so this was a very simple implementation of random forest with minimum minimum data processing now what you can do is practice more on kegel on a reallife data set which deals with more data processing which can help you understand how a data scientist works in this machine learning playlist until now we have only talked about supervised learning algorithms where we knew what the output of the target variable was in this video we'll explore an unsupervised learning algorithm called kmeans clustering kmeans clustering is one of the simplest and popular supervised algorithms and there are plethora of realworld applications of kmeans clustering which we will talk about in this video and in the next video we will see the implementation of kmes and how easy it is when compared to algorithms like svms and etc now before we jump into the algorithm itself or even supervised learning we must understand what clustering means so clustering is the process of dividing the entire data into groups or also known as clusters based on the patterns in the data let's try to understand that with a simple example a bank wants to give credit card offers to its customers currently they look at the details of each customer and based on this information decide which offer should be given to which customer now the bank can potentially have millions of customers right does it make sense to look at all the details of each customer separately and then make a decision certainly not it is a manual process and will take a huge amount of time so what can the bank do one option is to segment its customers into different groups for instance the bank can group the customers based on their incomes the groups that are shown here are known as clusters and the process of creating these groups is known as clustering awesome now let's talk about super unsupervised learning unsupervised learning is a type of machine learning algorithm used to draw inferences from data sets consisting of input data without labeled responses so to understand all of this let's see how a superfile supervised algorithm works first we have a label data set with the output or target variable in this particular example the task is to predict whether a loan will be approved or not as we have all the data labeled with appropriate targets we call it as supervised learning in clustering we do not have a target to predict we look at the data and try to club similar observations and form different groups hence it is an unsupervised learning algorithm so let's see where this helps us in the real world so starting with customer segmentation as we discussed before about the bank making clusters based on the income for the credit cards next thing is document clustering this is another common application let's say you have multiple documents and you need to cluster similar documents together clustering helps us group these documents such that similar documents are in the same clusters the next is image segmentation we can also use clustering to perform image segmentation here we try to club similar pixels in the image together we can apply clustering to create these clusters having similar pixels in the same group the next is recommendation engines let's say you want to watch or you want to recommend songs to your friends you can look at the songs like by that person and then use clustering to find similar songs and then finally recommend those songs to the person so let's we let's talk about kmeans clustering now we have finally arrived the main part of the video now with regards to generating clusters our aim here is to minimize the distance between the points within a cluster there is an algorithm that tries to minimize the distance of the points in a cluster with the centroid this is called the kmeans clustering technique the main objective of the kmeans algorithm is to minimize the sum of distances distances between the points and the respective cluster centroid let's see how the algorithm works in action so we have this eight points we want to apply we want to apply k means on to create clusters so let's see how we can do that the first step in k means is to pick the number of clusters k next we randomly select the centroid for each cluster let's say we have two clusters so the k is equal to 2 here we then randomly select the centroid step three once we have initialized the centroid we assign each point to the closest cluster centroid here you can see that the points which are closer to the red point are assigned to the right cluster whereas the points which are closer to the green point are assigned to the green cluster now once we have assigned all of the points to either clusters the next step is to compute the centroids of newly formed clusters here the red and green crosses are the new centroids now we repeat steps three and four so essentially there are three ways to stop kmeans clustering first is the centroid of newly formed clusters do not change so if the centroids don't change that means we have reached the end and that is the best way we can actually cluster our data second the points remain in the same cluster so if the points remain in the same cluster that means that there is no a further possible way or a possible way to improve our clustering algorithm and the last is the maximum number of iterations that i reach so the number of iterations is a subjective so it depends from person to person so we can actually focus more on the first two points and not in the last point as much so coming to the implementation we have two choices we can either use the cyclic learn library and import the kmeans model and use it directly or we can write our own model from scratch so writing our own model from scratch using numpy and python is very easy for kmeans but to see how the algorithm works very fastly will implement kmeans using the cyclic run library in this video we will use circuit learn to implement the kmeans clustering algorithm let's get started first we import all the required libraries so we need matlab sorry matplotlib dot pi plot as plt we need numpy to handle the data and we need the cluster from scikit learn let's wait for it to run the star here indicates that it's currently running and we have run it properly now let's prepare the data let's create a numpy array of 10 rows and two columns so it's better to actually show you how k means is implemented using our own premade data set and not a realtime data set because it gets really confusing so we start with a simple handcraft data set and then we move to complicated trailer dataset we create a numpy array of data points because the circuit learn library can work with numpy array type data inputs without requiring any preprocessing so we can directly focus on implementing the algorithm and not worry about precrossing in the initial stages of implementation so this is the numpy array now let's visualize the data the written code simply or the code which we're going to write simply plots all the values in the first column of x array against all the values in the second column so let's see what the code looks like so we make a scatter plot and we start from zero and we want y values now and let's give it a label as well while we're at it and this is how our data looks so from the naked eye we have to form two clusters of the above data points we will probably make one cluster of five points on the bottom left and one cluster of five points on the top right let's see if our kmeans clustering algorithm does the same or not okay so let's create the clusters now to create a kmeans cluster with two clusters simply type the following script so k means equal to the class k means and number of clusters which we want is equal to 2 and let's fit the algorithm now to our data set x and you've done that and yes it is just two lines of code to actually run the algorithm in the first line we create a kmeans object and pass it to the value 2 as the number of clusters next we simply have to call the fit method on key means and pass the data that we want to cluster which in this case is the x array that we created earlier now let's see what the central values the algorithm generated for the final clusters let's print them and yep those are centroids or the centers the output will be a 2d array of the shape 2 cross 2 to see the labels for data point let's execute the following so let's print k means labels so let's do it here again so and those are our two clusters so it is point by point so a cluster zero and cluster one so the output is a one dimensional array of ten elements corresponding to the cluster assigned to our ten data points here the first five points have been clustered together and the last five points have been clustered here zero and one are merely used to represent the cluster ids and have no mathematical significance towards towards each other if there were three clusters the third cluster would have been represented by the digit two let's plot the data point again on the graph and visualize how the data has been clustered this time we will plot the data along with the assigned label so that we can distinguish between the clusters so let's write the code for that we'll make a scatter plot again to see how this works with our data points in x 0 and x colon comma 1 and c is going to be k means labels underscore and let the c map be a rainbow so let's see how that works here we are plotting the first column of the x array and needs a second column however in this case we are also passing k means labels as the value for the c parameter that corresponds to the labels the c map rainbow parameter is parse for choosing the color type for different data points so that is how we get the differentiated bluish violet color or the purple color and the red car as expected the first five points in the bottom left have been clustered together displayed with blue while the remaining points in the top right have been clustered together with red so here we have two different opposite scenarios so the bottom has been done with red and the top right has been done with blue let's execute the kmeans algorithm with three clusters and see the output graph so let's implement it again k means equal to k means class and now the clusters is equal to three let's fit our data set on this algorithm and plot this again so scatter x again colon and 0 c is equal to k means dot labels again and the c map is going to be rainbow and yeah you can see that again the points are close to each other have been clustered together now let's plot the points along with the centroid coordinates of each cluster to see how the centered position affects clustering so here we're going to also point out the centroid of all the clusters which which we can see here so we have we have three clusters here so we'll be plotting the three centroids along with the clusters let's write the code for that we always use scatter plot for kms clustering because it's easier to see the scatter plot when we have to differentiate between the clusters colon and zero and again one the c is k means dot labels again the c map is equal to rainbow now we need to plot the centroids here so let's try that cluster centers if i'm right with that and we need only till the zeroth point and we have to plot that with the y axis so cluster underscore centers underscore colon one and let the color of these points be black awesome let's see how this looks so in the case of three clusters the two points in the middle which are displayed in red have distance closer to the centroid in the middle displayed in back between the two reds as compared to the centroids on the bottom left or top right however if there were two clusters there wouldn't have been a centroid in the center hence the red points would have been clustered together with the bottom left or top right clusters so that was a simple implementation of kmeans clustering with our very own handmade data set now you can go ahead and try implementing the algorithm on regular data sets the k nearest neighbors algorithm is a simple easy to implement supervised machine learning algorithm that can be used to solve both classification and regression problems now a supervised machine learning algorithm is one that relies on labeled input data to learn a function that produces an appropriate output when given a new unlabeled data the k n algorithm assumes that similar things exist in close proximity in other words similar things are near to each other we can relate this definition to something like birds of a feather flock together now notice in the image that most of the time similar data points are close to each other the k n algorithm hinges on this assumption being true enough for the algorithm to be useful knn captures the idea of similarity sometimes called distance proximity or closeness with some mathematics we might have learned in our childhood calculating the distance between points on a graph there are many ways to calculate distance and one might one way might be preferable depending on the problem that we are trying to solve however we are going to use something called as the euclidean distance which is a popular and a familiar choice let's see how the knn algorithm works in action first we load the data set next we initialize the number of neighbors which we want which is k in our case now for each example in our data set we calculate the distance between the query example and the current example of the data the distance here being the nuclear distance next we add the distance and the index of the example to an ordered collection for example a dictionary now sort the ordered collection of distances and indices from smallest to largest in ascending order by the distances now let's pick the first k entries from the solid collections get the labels of the selected k entries now if you want to find the mean then that is the regression problem and if you find the mode it's a classification knl algorithm now let's talk about choosing the right value of k to select the k that's right for your data we run the k n algorithm several times with different values of k and choose the k that reduces the number of errors we encounter while maintaining the algorithm's ability to accurately make predictions when given data it hasn't seen before k n has the following advantages the algorithm is simple and easy to implement and we'll see that in the next video there is no need to build a model tune some hyper parameters or even make additional assumptions it is a very simple and straightforward algorithm the algorithm is also versatile it can be used for classification regression and search as well one of the major disadvantages of the algorithm is that it gets significantly slower as the number of examples or variables increase coming to this let's talk about the applications of k n k n can be useful in solving problems that have solutions that depend on identifying similar objects right the nearest neighbors or the nearest similar objects an example of using this would be in recommender systems which is an example application of k n search now uh at a large scale this would look like recommending products on amazon or articles on medium movies on netflix although we can be certain that these companies they all use more efficient means of making recommendations due to enormous volume of data and when you have an enormous volume of data that is when k n starts to suffer so that was a very brief introduction of how the k nearest algorithm works in this video we will implement k nearest neighbors using the cyclic loan library so the k nearest neighbors algorithm is a type of supervised machine learning algorithm k n is extremely easy to implement in its most basic form and yet performs quite complex classification tasks it is a nonparametric learning algorithm which means that it does not assume anything about the underlying data this is an extremely useful feature since most of the realworld data doesn't really follow any theoretical assumptions for example linear scalability or uniform distribution let's start implementing the k n algorithm using cyclic learn now we are going to use the famous iris data set for rkn example the data set consists of four attributes sample width sample length petal width and petal length these are the attributes of specific types of iris plant the task is to predict the classes to which these plants belong there are three classes in the data set i recetosa iris versicolor and iris virginica let's start the implementation by importing some libraries so we need to import numpy as np import matplot lib.pyplot spld and import pandas as pd for data handling now let's import the data set into a notebook and then into a pandas data frame so here we have the url from which we can access the data the url will also be in the description so we assign some names to the columns of a data set and read the data set into the pandas data frame to see what the dataset actually looks like let's execute the following script dataset.head and we can see the first five rows of our dataset now the next step is to split a dataset into its attributes and labels to do so let's write the following script so x equal to data set dot along dot values y is equal to the same thing but minus 1 it's going to be 4 because the last column and let's fix this and we are good the x variable here contains the first four columns of the data set or the attributes while the y contain the labels to avoid overfitting we will divide a dataset into training and test splits which gives us which gives us a better idea as to how our algorithm perform during the testing phase this way our algorithm is tested on unseen data as it would be in a production application to create training and testing splits let's execute the following script so let's import the test train model from sk learn and let's define some variables so x test y train and y test is equal to test train split that's a very big function name x y and the test size is 0.2 or 20 let's wait for it to run so that we can see some so test train split cannot be imported let's see what the error is now so it's strained this split that makes much more sense i get it now and we're good so now the above script splits the data set into 80 train data and 20 test data this means that out of total 150 records the training set will contain 120 records and the test set contains 30 of those records now before making any actual predictions it is always a good practice to scale the features so that all of them can be uniformly evaluated the gradient is an algorithm which is used in neural network training and other machining algorithms also converges faster with normalized features so let's write the script for normalization now sk learn preprocessing import standard scalar and the scale r is equal to an object so standard scalar and let's split it now and get a fit x underscore train and let's x train is equal to scalar dot transform extreme x underscore test is equal to scalar transform for test and again i think i've made a mistake with the spellings preprocessing standard scalar i'm sorry yes now let's fit the canon algorithm to the desired dataset it is extremely straightforward to train the k n algorithm and especially makes prediction out of it when using the cyclical library so let's import our model from circuit learn so sk learn dot neighbors classif file and let me just make sure that the spelling is right now so that we don't have any more errors so k neighbors classifier and the classifier is equal to we have the same name class and let's say we want five neighbors let's fit the classifier now so the first step here was to import the k n n classifier class from the sql neighbors library in the second line we initialize the class with one parameter that is the n neighbors this is basically the value for the k there is no ideal value for k and is selected after testing and evaluation however to start out phi seems to the most commonly used k n algorithm the final step is to make predictions on our test data so why spread is equal to dot predict for x test now let's evaluate the algorithm and see how good it performs for evaluating an algorithm confusion matrix precision recall and f1 score are the most commonly used metrics all of these can be found in the scalar metrics module so from sklearn dot matrix import classification report confusion matrix and let's print them now for y test and y predicted and also print the confusion matrix to see the matrix and the same parameters y test and y and here we can see all the metrics which we need to evaluate our algorithm and how it performs so this was a very basic implementation of k nearest neighbors and after this we can actually go on kegel and download a reallife data set and actually perform and see how k n performs on a reallife data set thank you