hey guys welcome to the first video on opencv tutorial for beginners using python in this video I'm going to give you a brief introduction about opencv but first of all let's see what is computer vision because opencv is an open source computer vision Library so computer vision is the way of teaching intelligence to machines and making them see things just like humans so what happens when a human see an image he will be able to recognize the faces which are there inside the images so in its simplest form computer vision is what allows computers to see and process visual data just like humans computer vision involves analyzing images to produce useful information so to give you some examples a selfdriving car it can detect the lanes using computer Visions or you might have wondered how Facebook detects images when you upload the images of you with your friends it becomes possible by Facebook's face and image recognition technology so now let's see what is opencv so opencv which stands for open source computer vision is a library of programming functions mainly aimed at realtime computer vision it is originally developed by Intel and then it was later supported by a developer called Willow garage and now it is supported and maintained by itcs now opencv is available on Mac windows and various Linux operating systems so we can say that opencv is a cross platform library now you can work on opencv using C C plus plus or Python and we will be using python to learn opencv now opencv is a open source and Free Library which is licensed under BSD license I had said that it is very easy to use and install that we will see when we will install opencv on various operating systems now because opencv primarily deals with computer vision that means dealing with mainly images or videos so I wanted to show you how a digital image is seen by a computer so digital images are typically stored in the form of Matrix now if you have heard about PPI or pixel per inch which refers display resolution that means how many individual pixels are displayed in one inch of digital image so when a computer sees a picture it sees it in the form of pixel Matrix now there are two type of digital images One are called grayscale images and other are called colored images so in grayscale images each pixel represents the intensity of only one shade that means how bright or dark the pixel is in other word it is said that it has only one channel so on the right hand side you can see a grayscale image and on the left hand side you can see a colored image so in colored images we have three channels that is r g b which stands for red green blue so grayscale images have one channel and colored images have three channels your standard digital camera have three channels that means red green blue channel so we will learn more about images and how we can process as images using opencv in the later videos now there is one more thing which I want to show you is numpy so we are going to learn opencv using python so when you will install opencv library for python on your operating system numpy will be automatically installed with this Library so first of all what is numpy so numpy is a highly optimized library for numerical operations now as I told you digital images are 2D arrays of pixels and numpy library is a general purpose array processing package Library so it provides a high performance multidimensional array object and tools for working with these arrays which makes the processing of images easier now all the opencv array structures are converted to and converted from numpy arrays and in addition you can use more convenient indexing system rather than using for Loops so when you want to learn opencv using python you need to have some knowledge about numpy also so if you have some knowledge of numpy Library it's good but don't worry I will teach you step by step so you will not miss anything so that was a brief introduction about computer vision and opencv in this video I'm going to show you how you can install opencv for python on your Windows operating system so obviously you need to install python on your Windows operating system in order to install opencv for python so first of all I'm going to show you how you can install python on your Windows operating system and then we will see how to install opencv using python now if you have already installed python on your Windows operating system you can skip about five minutes of this video and go directly to the point where I am going to show you how you can install opencv for python so let's get started so first of all open your favorite browser on your Windows 10 operating system and then search for Python and the first link which will appear here will be from python.org so we are going to click on that link and once this python.org website is open you just need to scroll down a little until you see this downloads section and you can see at the time of making this video python 3.7.0 is the latest version of python available so we are going to click on this link which says Python 3.7.0 and you will be redirected to this page which says Python 3.7.0 and now I'm going to scroll down until I see the files here and you will see there are various kinds of installer available here we are going to install the python using the executable installer so we are going to choose this option which says Windows x86 hyphen 64 executable installer and now I will wait for this executable to be downloaded and once this executable is downloaded you just need to click on this exe file and I am going to minimize the browser here so you can see Python's 3.7.0 setup window has been started and on the first window you will see two options here one is install now and other is customize installation so what we are going to choose is this option which says customize installation because when you choose this install now option python will be installed at this path which I don't want to use you can see it's a long path which I don't want to remember so I will use this option which says customize installation and I will also check this option which says add python 3.7 to path so now let's click on customize installation and next you will see this optional feature window and you can see there are some optional feature which this python installer will install for example documentation pip it will install which is a python package manager idle IDE python test suit and other feature it's going to install so I'm going to leave everything as default and then I'm going to click next and now this next window will open which says Advanced option here I'm going to check this option which says install for all users and I'm going to leave other check boxes as checked and then you will see this section here which says customize install location so I want to install python on my C directory so what I'm going to do is I'm going to open the windows Explorer and I'm going to go to the C directory here and once the C directory is open I'm going to right click here and I'm going to create a new directory and I'm going to name this directory as Python and then I'm going to press enter and this path I am going to give here in the customize install location so I am going to just give this path which says C colon slash Python and then backslash Python 3 7.37 here means that we are going to install 3.7 version of python so now python will be installed at this location on my computer and then I'm going to click on the install button here and then you will see the installation will start and it will be finished in a few seconds so just wait for the installation to complete and after some time I can see this message which says setup was successful so I'm going to click on this close button which is going to close this installer so now in order to check whether python is installed on our Windows operating system or not we are going to search for python here and you will see few options here one is this python 3.7 terminal other is Idle IDE so first of all we are going to click on this option which says python 3.7 64bit which is is going to open this kind of terminal so this is a python terminal and here we can for example print something so I am going to just write print and in the parenthesis and in between the double quotes I can just write hello world and then press enter which is going to in return print hello world that means python 3.7 terminal is working so I'm going to close this terminal now and once again I'm going to search for python here and this time I'm going to select this option which says idle okay so just select this option which says idle and in the parenthesis python 3.7 64 bit so this idle is an IDE which comes with python installation at the time of installation we have chosen this option to install idle that's why we can see this option here and also this is an interactive shell so you can once again write a print and inside the parenthesis you can just write for example once again hello world and then press Center and it is going to give you this kind of output here so now python interactive shell is working and idle IDE is also working so I'm going to close this idle IDE and now I want to check whether python is working using my command prompt or not so I'm going to right click on this Windows button and then I'm going to click on command prompt and here I'm going to first of all write Python and then press enter and you can see this python option is working now even on your command prompt right so here also you can just write print and inside the parenthesis you can just print hello world and then press enter and it prints hello world in return now once python is installed on your Windows 10 operating system we are going to install opencv using pip now pip is automatically installed on your Windows operating system with the python installation so you don't need to separately install pip on your Windows operating system it comes automatically with your python installation so to verify this first of all I'm going to check the python version so you can just give this command python hyphen hyphen version and then you can check the PIP version so you can just give this command pip hyphen hyphen version so just give this command and it's going to give you the version of pip which is installed on your Windows operating system so to install opencv using pip you just need to give this command pip install open CV hyphen python and I'm going to press enter so you can see opencv related packages are downloading now so now opencv python package is installed using pip on my Windows operating system now you will observe one more thing here and that is numpy package will be automatically installed with your opencv python package so now once opencv python package is installed we can verify it by just opening our python shell so I'm going to just give a python command to open the python shell and then here I am going to just write import CV2 okay so once you give this command it should not give you any error and if this import gives you error that means opencv is not correctly installed on your operating system now after importing you can just check the version of opencv which you have installed using CV2 dot underscore underscore version underscore underscore and then press enter and it's going to give you the version of opencv which is installed on your operating system and in our case this is 4.0.0 at the time of making this video now you can check the Same by writing your code inside a python file also so here I have opened my visual studio code editor and I have already created sample.py file and here also I am going to import the CV2 package first of all and then I'm going to print the version of CV2 using this print statement so I'm going to just write CV2 dot underscore underscore version underscore underscore and then save this script and to open the terminal inside Visual Studio code you can just press Ctrl shift p and then type toggle integrated terminal so just type toggle integrated terminal and then click on this first option which says toggle integrated terminal this is going to open the terminal inside your Visual Studio code editor so here you can run your python script using the python command so Python and then name of the script which is sample.py in my case and then press enter and it's also going to give you the version of opencv which is installed on your operating system so this is how you can install opencv for python on your Windows operating system in the last video we have seen how we can install opencb for python now from this video we will actually start writing some code now moving forward I will be using pycharm IDE to demonstrate how opencv works but you are free to use any IDE or any other editor in order to use opencv now on a pycharm IDE you need to install opencv little bit differently so if you are using opencv you just need to create a project inside opencv and then you just need to click on file and then go to the settings now once the settings window opens you just need to go to the project and then it will say after colon your project name so my project name is opencv examples that's why it's written here opencv examples so project colon your project name so just click on the section and then click on Project interpreter and on the right hand side you will see all the packages which comes preinstalled when you create a project inside pycharm IDE now we want opencv python package so to install opencv python package on pycharm you just need to click on this plus button here and then you just need to type open CV hyphen python now the first result you can see here is opencv hyphen Python and the version which is available right now is 4.0.0.21 which is the latest version so to install this package for your pycharm IDE you just need to click on install package button and then after some time you will see this message which says package opencv hyphen python installed successfully in the green bar that means opencv package is installed successfully so you can close this window and now you will be able to see opencb hyphen python is added to your packages and also numpy is added to your packages which comes with your opencv python package so I'm going to just click OK and now you will be able to import this CV2 package in your python screen now in this video I'm going to show you how you can read images and write images using CV2 package now let me show you where you can find some sample images for your project so you can open the browser and then go to this URL github.com forward slash opencv so just go to this URL and then under this opencv project in GitHub you will be able to see these repositories you just need to choose this repository which says opencv and then you can scroll down and all the images you will find inside the samples folder so I'm going to go inside the sample folder and then inside the sample folder you just need to go inside the data folder so here you will find many sample images and videos and other files which you can use in your project for the learning purpose so you can use these images in order to develop your example so what I generally do is I just go to this repository which is under the URL github.com forward slash opencv forward slash opencv and then I either download the zip file of this project or clone this GitHub repository on my operating system and once you clone or down load this repository it will look like this so it will be downloaded as this folder which is opencv hyphen master and once again you can go to the samples folder here and inside the samples folder you can go to the data folder and you will find all those images which I have shown you on the GitHub repository now to start with we will be using this image which is Lena dot jpg so I'm going to just copy this image for now and then I'm going to go to my pycharm IDE and then I'm going to just paste this image inside my project so this jpg image will be directly available inside my project folder now let's see how we can read images using the CV2 module so you just need to use CV2 and there is a method called I am read which enables you to read the images so the first argument which you need to give here is the image name so I'm going to give the image name which is Lana dot jpg and the second argument here is a flag so there are three flags you can give here you can either give 0 or 1 or minus one flag here so this second argument is a flag which specifies the way images should be read so let me show you all the flags here so the first flag is CV2 dot I am read underscore color or you can give the integer value of it which is one and whenever you give this flag as the second argument of I am read function it's going to load the colored image if you give this flag which is CV2 dot I am read underscore grayscale or if you give this integer value which is 0 it's going to load your image in grayscale mode and the third flag is I am read underscore unchanged or the value minus 1 which is going to load your image as it is including the alpha channel so for now we are going to just give here 0 flag which means we want to load our image in grayscale so now let's run the code and let's see what happens until this point so you can see our code runs fine without giving any error now let me give any random name here as the file name and once again run this code and once again you will see that there is no exception which is thrown here so even if you give the wrong file path or file name here this function is not going to give you any error now in case of wrong path or wrong file name let's say I'm going to just assign this value to a new variable which is IMG and let's print the value of this IMG using the print method and then let me run the code once again and you can see whenever you will give the wrong file name here or wrong path here as a result of this method you will get none so so you can check the value of image and if it is equal to none then you know that you have done something wrong or you have given some wrong file name or wrong path here let's give the correct file name so I'm going to give the correct file name and then run the code once again and now this time you will see it's going to give you a matrix which means that it has read all the pixels from this image and then assigned it to our IMG variable and the result you can see in the form of this Matrix so until now we have just read the image now we want to display our image so in order to display our image what we can do is we can use a CV2 dot IM show method so just write I am show here which is going to show your image so the first argument here will be the name of your window in which your image will open so you can give any name here for example I am going to give image name here to my window and then the second argument is the image variable which you have read using the I am read method so I'm going to just pass IMG variable which is this variable here so now will it show the image let's check so I am going to run the code once again and you can see the image is shown for a millisecond and then it disappears so now we need to add something which will wait for the image to disappear so I'm going to add one more method here which is CB 2 dot weight key so this CV2 dot weight key is the keyboard binding function and the argument which it takes is the number of milliseconds for which you want to show your image window so let's give 5000 value here which means we want to show the image for five seconds and at last what we are going to do is after we have done seeing our image we will destroy the window which we have created so you can just give this method CV2 destroy all windows so destroy all windows simply destroys all the windows which we have created there is one more method which is destroy window and this method you can use to destroy a particular window which we will see little bit later but for now we will just use this method which says destroy all windows so now let's run our code and let's see what happens so now this time you can see our image is loaded for 5 seconds and our image is loaded in grayscale mode now if you give here 0 as the argument of weight key method then let's see what happens so I am going to run my code and now you will observe that your window will not disappear after 5 second or any number of seconds it's going to wait for the closing of this window which we can close from this close button and you can see it's loaded in that grayscale mode I'm going to close this window and here as an argument of IM read image the second argument I want to give here right now is 1 which means the colored image and let's run the code and you can see this image is loaded in the colored mode now let's also check the minus one argument which is unchanged so it is going to just load the image as it is with Alpha channels so let me just close this image once again so now we have understood how we can read an image using IM read function so let's see how we can write an image to a file using a function called I am right so we are going to just use CV 2 once again and then there is a method called I am write which you can use to write an image in the form of a file so the first argument here will be the image name whatever you want to give here so here let us say we want to give the name to our image as Lena underscore copy dot PNG so the image will be saved as the file name Lena underscore copy dot PNG file and the second argument which it takes is the image you want to save so let's save the same image which we have read using the IM read function inside this IMG variable and pass it as the second variable here and let's run the code and let's see what happens so our image is loaded using this I am sure show function and now when I close this window here you will observe one more file will be created here so let me just close this window and now you can see as soon as I close this window this method is called and after this this I am right method will be called and when this method is called this image is created with the name Lena underscore copy dot PNG we can also open this image and you can see it has the same image which we have seen in the case of lemona dot jpg so let me close these two images so now we have understood how we can read the images and write the images using IM read function and I am write function so let's make our code little bit better and what we want to do here now is let's say if somebody presses an Escape key then only we want to destroy all the windows without saving it into a new file otherwise if somebody presses the S key then we are going to save this file with the new name let's say Lena copy dot PNG file so I'm going to just capture the output of my weight key so just create a new variable let's say k so now when we press any key this key will be captured in this variable now as you know every key has its own value so we are going to just use uh if condition and we are going to just check whether the value of K is equal to 27 which means that we have pressed the Escape key and if somebody have pressed the Escape key we are going to Simply destroy all the windows otherwise let's give the second condition Edition which is l f k is equal to ord and this is a built in function and it takes one argument which is the key name which we want to press so let us say somebody presses the S key and if somebody presses the S key we just want to save the image which we have read using the IM read function to a second image which we call Lana underscore copy dot PNG and then we will simply destroy all the windows which we have created using IM show method so let's run the code and let's see what happens so I am going to run the code so this image is loaded and as soon as I press escape button you can see this image disappears that means this condition is met and this method is called and all the windows will be destroyed without saving the image let let's delete this image and let's see what will happen when we press the save key so let me just delete this image and now let's run the code once again and this time I'm going to press the S key and once again you can see the windows are destroyed but this image is created once again using this function that means this time this condition is fulfilled and this image is created and after that all the windows are destroyed so this code is working fine for me but in the documentation it's also written that if you are using a 64bit machine it's better to use this notation with your weight key method which is weight key and this mask here and then once again when we run our code it works as it is but in case if it doesn't work you can try try using this mask here so this is how you can read and write images using opencv in this video we will see how to read display and save videos using cameras so often we have to capture live stream from camera so first of all we will see how we can capture the live stream from the camera the same method you can use to display the video from a video file so let's get started and let's see how we can capture the live stream from your default camera so I'm going to just create a variable called CAP and then inside your CV2 package there is a class called video capture we are going to take this class and create an object of it and as an argument here you can provide either the input file name so for example if you want to just read the video from a particular file you can give the file name for example my file dot AVI or MP4 or you need to provide the device index of your camera from which you want to read so by default this index will be either 0 or in many devices its also minus one so first of all we are going to try with zero device index which in most cases works so if this device index 0 doesn't work try with 1 now if you have multiple cameras and if you want to use the other camera then you can also try one for the second camera or two for the third camera and so on so we are going to use the default camera which is ADD device index 0 so this is the argument we need to provide here and then we are going to create a while loop in order to capture the frame continuously so let's create a while loop here and this Loop we are going to run indefinitely so we are going to just say that while this Loop is true we want to capture the frames so we are going to just Define these variable r e t and frame and then using this cap instance we are going to call a method called read now this read method is going to return true if the frame is available and this Frame will be saved into this Frame variable so here the true or false will be saved if the frame is available this ret will be true otherwise it will be false and this Frame variable will actually capture or save the frame now in order to show this captured frame we can use I am sure so I am going to just use a CV 2 dot I am show which is going to show this Frame inside the window first of all all you can give the name to your window for example frame and then second argument will be the frame which you are reading which is this variable here now in the next step we have seen in the last video also we are going to use the CV2 dot weight key in order to wait for the user input and if this input will be Q we will quit the window and destroy all windows so we are going to just say CB2 dot weight key and the argument here will be 1 and I have told you you need to provide this mask for 64 bit machines so you can provide this mask and then we are going to adjust see if this key which is pressed is q or not so we are going to use the ORD method for this and we will just check if the Q key is pressed and if this q key is pressed we are going to break from the loop and we will come out of the loop and after we come out of the loop the first thing we need to do is to actually release the capture variable so this is important after reading your video you need to release the resources so you need to just call this method cap dot release and then we will just destroy all windows so let's run this script and let's see what is the output so I'm going to run the script and you can see in this window the input from my default webcam of my laptop right now I'm just showing some book in front of this camera that's why you will see this book and as soon as I press q key our window will be destroyed and we come out of this script now let's say you want to change the frames to gray so we want to convert our video input from the colored image to the grayscale image for that what you can do is you can define a variable called gray or anything else and then there is a method called CV2 dot CVT color which is to convert color and the first argument which it takes is the source so in our case the source is the frame which we are capturing from the cap dot read method the second argument is actually the conversion what we want to do so we will just call CV2 dot color underscore and by default the default colored image is captured as BGR image that means blue green red Channel images and we want to convert it to a grayscale image so we will just write BGR to gray this means we want to capture the BGR image to the grayscale image and now this is going to give us the gray scale image and this input we can just transfer to the IM show method as the second argument of this I am show method so let's run this script once again and let's see what is the output of the script and now you can see the video captured is in grayscale image and as soon as I press Q it's going to release all the captured resources and then destroy all windows now as I said if you want to display the image from a video file you just need to give the name of the video file for example name and then the extension which is let's say AVI or MP4 or any other format Now using this cap instance you can read few properties about the video which is captured and the first property is if the video is open or not so in case whenever you provide the file name and the file path is wrong then this is going to give you false so there is a method called is opened and this means if the file name of the video which you want to provide here is correct this is going to give us true otherwise this is opened is going to give us false in case the file path is wrong or the index which you give here for the device is wrong so let us give any random index here and then let's see what happens so I am going to run the script and you will see nothing will happen because this is opened is going to give you false let's print that also and let's verify with the print statement the same thing so I am going to just use this and then run the program once again and you can see it prints false that means you cannot capture the video using this index so my device is at index 0 by default so I need to give this index name otherwise for example I provide the wrong file name here also it's going to give us the false value there is a method called cap.open also so if this cap is open gives you false you can try opening your capture video using cap dot open also now there are other properties which you can read using this cap instance and the property you can read using a method called get so you can just write cap dot get and as an argument of this get method you can provide the prop ID so there are different prop IDs which you can read so let us say we want to read the property which is called frame width and frame height which is going to give you the height and width of your frame so for this you just need to write CV2 dot cap underscore prop underscore frame underscore width this is going to give you the width of your frame and if you want to get the height of your frame you can use cap underscore prop underscore frame underscore height and this whole list you can find on the official documentation of opencv so I will provide you this link where you can see different values of the prop ID so right now I have used this ID and this ID but there are several number of IDs available here which you can use to read the prop property of your frame so let's use print method to just print out what property we are reading and let's once again run this script and here you can see in the output you can see the value 640 and 480 which is the width and height of your frame by default now let's see how we can save the image which we have captured from our webcam or the default camera so as we already know that we read frame by frame when we capture the videos from your default camera so for creating the capture you have used video capture class and for saving the video we are going to create the video writer class so I'm going to first of all create a variable called out for output and then I'm going to call a class called video writer so let's call this class which is video writer and now this class takes few argument the first argument is the name of your output file so for example I can just give the name output dot Avi the extension of the file the second argument here is the four CC code now 4 cc is a 4 byte code which is used to specify the video Codec and if you want to know more about 4 CC code you can visit this website which is 4cc.org forward slash Kodak dot PHP and here you can find several four CC codes so for now what we are going to do is we are going to just get the four CC code using a class called video writer underscore 4cc so I'm going to declare a variable 4 CC and then I'm going to use CV2 to call a class called video writer 4 CC so as an argument of this class you just need to provide the 4cc code so for example I can give this kind of code so you can provide this argument which is X tricks and then your four CC code which is Xvid in this case or otherwise what you can do here is you can also give this code in this format so for example X comma then second argument is V and then third argument is I and the fourth argument is D so you can either give this type of notation or you can just use asterisks and then in single quotes you can just write x with or any other code here and then this four CC code we are going to pass as the second argument the third argument is the number of frames per second so let's say we just want to use 20 frames per second and the fourth argument is the size so we already know that the size in which we are capturing is 640 by 480 so we are going to provide this in the form of Tuple so 640 comma 480 so this will be the size of the video which will be saved in this file now inside our loop as we have seen we are just reading the frame here here in the frame variable and this is the Boolean variable if the frame is available its true otherwise it's false right so first of all we are going to check if its value is true or false so we can just write f r e t is equal to true then only we are going to just save this file into the output file so I'm going to just put everything inside this if condition otherwise we are going to break out of this Loop so I am going I am going to just say else break now inside this if condition we can just write this Frame into a file using a method called out dot right so out is the instance of video writer so I'm going to just use out dot right and then we are going to just pass the frame which we have captured which is inside the frame variable and now at last we are going to release all the resources using the out instance which is the instance of video writer so I'm going to just write out dot release and then let's run the script and let's see what happens so one thing to note here is our video will be saved as it is that is in the BGR mode that is in the colored mode so let's run the code and let's see what happens so I'm going to just start my script once again and now I am going to just press Q so you can see here our video is shown in the grayscale and our video will be saved in the original from form format because we are saving every frame before the conversion so it will be saved in the original format so I am going to just close this ah of script and as soon as I close the script you can see the output.avi file and in order to verify this file I'm going to go to the project and here I'm going to start this file using let's say VLC media player and you can see it shows the output of the output dot avi file so this is how you can read videos display and save videos using the default camera or the video file in this video we will learn how to draw different geometric shapes using opencv so to start with I have this code and I have already explained what this code does so this I am read is used to read an image and then we are just showing this image into a window using this IM show method and then using the weight key we will wait for the closing event and the destroy all window will destroy all the windows which we have created so this we have already seen now let us say we want to draw some geometric shapes on this image so to start with let's learn how to draw a line on our image which we have read from this read function so what we are going to do is we will overwrite this image so we have already ah created this image variable so what we are going to do is we will draw a line on the same image so I am going to just write IMG is equal to CV2 dot lines and you can see in the suggestion this line method takes few arguments so the first argument is the image itself the second argument is the starting coordinates of point one the third argument is the ending code coordinates of 0.2 and then the fourth argument is the color and fifth argument is the thickness so let's use this line method and then give these arguments one by one so we want to write to the image which we have read using this file so the first argument is the image variable and the second argument is the coordinates so the coordinates should be given in the form of Tuple so let us say we start with 0 comma 0 coordinate and the ending coordinates will be let us say 255 comma 255 okay the fourth argument will be the color and the color you need to give in the BGR format so if you want to give the blue color then you can just write 255 comma 0 comma 0 because first is the blue color second is the green color and third is the red Channel color so if you specify here 255 in the first channel that means the blue Channel then it's going to draw the blue line if you give here 255 and then you make other channels 0 then it's going to draw the green line and if this 255 comes at last and the other channels are 0 then it's going to draw the red line so let's say we want to draw the red line that's why I have given 255 here and the next argument is the thickness so the thickness you provide in the numbers so start starting from 1 1 is the lowest thickness you can increase the thickness two or three or let's say five or ten so it is going to increase the thickness based upon this number so let us say we want to give the thickness to our line five so this is going to draw a red line on our image so let's run the code and let's see what happens so you can see our image is loaded in the grayscale that's why you do not see any color on the line but our line is created here so let's load this image in the colored format by changing this argument to one and let's run the code once again and you will see the image is loaded in the colored format and the line color is red now if you want to change the thickness of this line you can just increase this number and if you want to change the color of this line you can change yet using these color channels so let's change the line color to Green let's say and I'm going to run the code and you can see the thickness of the line is increased and now the color of the line is green now if you want to draw the line with any other color you can just go to your favorite browser and search for RBG Color Picker but always remember our image will be loaded in the BGR format so in the reverse order so blue green and then the red channel so let's say we want this ah color here and its RBG channels are this so I am going to just copy all these channels and then I am going to give these channels in the reverse order so first of all 147 then 96 and then third Channel is the 44 and then I am going to run my code and you can see you get the same color which you have chosen here so this is how you can change the color of your lines now there is a function called arrowed line let's say we want to use this function which is called arrowed line and this is going to draw the arrowed line as it says so let's say we want to just draw this arrowed line in blue color so I am going to just give the color channels here and then run the code and this arrowed line is overlapping on the previous line that's why you don't see the previous line so let's change the coordinate of this line so let's draw this line in this coordinate which is going to draw the straight line in in my opinion let's see what happens when I run the code and you can see it draws the straight line from left to right which is the arrowed line and this was our original line now let's see how to draw the rectangle so to draw the rectangle we will do the same we will just overwrite on the same image so we will just say image is equal to c v 2 dot rectangle which is a method and you can see what are the argument it takes so the first argument is the image itself the second argument is the point 1 and 0.2 this point one and point two coordinate I am going to explain in a bit the third argument is the color which is same as line and the fourth argument is the thickness so let's use this rectangle function to draw the rectangle so first of all I am going to just pass the image variable here the second argument is the top left vertex coordinates so let me just draw something here so you will be able to understand in a better way so when you want to draw a rectangle using opencv this here is a top left vertex coordinates which is X1 and y1 and this is here the lower right vertex coordinates so the top left vertex coordinates you give in the second argument so let's give some coordinates here so 3 84 comma 0 and the lower right coordinates I want to give here is let us say 510 comma 128 so let us say we want to give the red color so I am going to just write 0 comma 0 comma 255 and the thickness I want to give here is 5 and I am going to just remove this because it will just create problems and now let's run the code and you can see the rectangle is drawn with the red color of thickness 5 you can change the thickness of this rectangle by changing the value of the thickness and then you can run the code and now the thickness of this rectangle line is increased now one more thing you can provide here is instead of giving the thickness value if you write here minus 1 then it's going to fill the rectangle with the color which you provide here so when we give minus 1 here let's see what happens so now we get the filled rectangle because we have provided minus one option here so if you provide minus 1 then your rectangle or whatever shape you are creating will be filled with the color which you specify here so let me just change the thickness to 10 once again and now let's see how we can draw the circle so to draw the circle we once again use CV2 dot Circle function and once again you can see what are the argument which it takes so the first argument is the image the second argument is the center of the circle the third argument is the radius of the circle and the fourth and fifth argument is the color and the thickness once again so once again we will provide the image the second argument is the center of the circle so let's give the center of the circle which is the coordinate on which you want to ah give the center so I'm going to provide let us say 447 comma 63 here and the third argument is the radius so radius we want to provide here is let us say 63 and the fourth argument is the color so let's use 0 comma 255 comma 0 which is going to draw the green in color and then let us give minus 1 here so our Circle will be filled with green color and let's run the code and let's see what happens so you can see this circle is drawn here and this circle is filled with the green color now let's see how we can put some text into the image so to put the text on our image we will once again use the image variable and overwrite on it and then we will use a method called put text so this is the method which we are going to use the first argument is the image the second argument here is the text which we want to put so let us say we want to just print opencv on our image so we can just write opencv as the second argument the third argument is the starting point of your text so you need to give the call coordinates where you want to start your text from so the coordinates I want to give here is let's say 10 comma 500 and then the next argument is the font face so the font face you need to give here using a variable so I'm going to create a variable let's say font and then there are many font faces available using CV2 so you can just write CB2 dot font in capital and you can see what are the options available here I am going to choose the first one itself which is font Hershey Simplex font and then we are going to pass this font as the fourth argument the fifth argument here will be the font size so let us say I want to give the font size 4 here the sixth argument here is the color of your font so let let us say I want to just draw 255 255 255 which is going to give us a whitish kind of color the next argument we will give here is the thickness so let's say I want to provide the thickness 10 here and the next argument you can give here is the line type so let's say I want to give the line type CV2 dot Capital line underscore a a and now let's run our script and let's see what happens so you can see here opencv is printed in the white color of thickness 10 and if you want to change this color you can change it from here so I'm going to just put the first channel as 0 and now this color is changed to yellow color now one more thing I want to show here is how you can create an image using num Pi zeros method so either you can use a image which you read from I am read method or what you can do here is I'm going to just comment this code and we can create an image using the numpy zeros method so I'm going to create this IMG variable and then I'm going to use the numpy module so just import this numpy as import numpy as NP and then we are going to use this MP to call the zeros method now in order to create a black image using this zeros method you need to give the first argument in the form of list and inside this list the first element will be the height second will be the width and third will be 3. so let us say we want to provide the height 512 we want to provide the width also 512 and and the third argument will be 3 and the next argument you give here is the D type or data type so you can just write NP dot U int 8 here so this method is going to give you a black image of the size 512 by 512 so let us run our code and let's see what happens so you can see now you can see the black image and on our black image the line is drawn the erode line is Dawn and the text and the circle and the rectangle are drawn here so this is how you can draw different geometric shapes on your image there are several other methods you can use for example CV2 dot polyline method or CV2 dot Eclipse method to draw eclipse and polygon on your image so just try those method to draw different shapes on your image so in this video we will see how to set some properties to our captured images so in the video capture lesson we we have seen that when we create a cap variable using the video capture class we can get many properties using the cap.get method so we were able to get the width of the frame and the height of the frame similarly we can use the cap dot set function to set some values so you can just write cap dot set and then you can set the values of the property generally all the properties which you can read like this you can also able to set those property using the set method now this notation you can also give in the form of number so every property here has a defined number so for example instead of using CV2 dot cap prop underscore frame width you can just write 3 here and that will work also so every property has a number associated with it so using that number either you can just let us say we want to set the width and height either you can write this as the first argument and the second argument is the actual width you want in the video right or you can just give the number of that property and then give its value so let us say we want to change the width of this video to let us say 1 2 0 8 and then let us just set the height so cap dot set and the associated number for the height parameter will be four so three for width and 4 for the height and let's say we want to just ah move it to 720 and then we will once again print the value of the width and height and this time we are going to just give their Associated numbers which is 3 and 4. so let's run this program you might already know this program what this program is doing so it's just capturing the video from your default device at index 0 and then its just showing all the frames using this IM show method in a window so now I am going to run this script and let's see what happens so when I run the script you can see the size of this Frame is changed so lets see in the terminal also you can see before the original size of the video we are capturing a 640 and 480 so width was 640 and the height is 4 a t now once we have changed the width and height you can see the width is changed to one two eight zero and the height is changed to 720 so even if I have given here one two zero eight the default camera will automatically set its value according to its resolution so let us just close this video and let us say we want to just change this value to some random number so let us say 700 by 700 will it work or not so let us run the script once again and let's see what happens so the script is running and you can see that even though we have provided the 700 and 700 the camera will automatically take the resolution which is available for your default camera so the resolution Remains the Same even though we have set the different value to it so you need to keep in mind even though you can give any value here but the camera will only set the resolution which is available for it so let us give a very big value here so I am going to provide let us say 3000 here and height also 3000 and let's run the script once again and see what happens so when we run the script you will see the resolution is changed but the resolution will change to the maximum resolution of my default camera which is one two eight zero and 720 this is the maximum resolution which is available for my webcam so let me just close this window so this is how you can set some values so there are many values you can set using this set method you just need to go to the documentation and then search for the value you want to set so in the last two videos we have seen how to capture videos from our default camera device or how to add geometric shapes on the images now in this video we are going to combine the knowledge we have gained in the last two videos so if you haven't seen the last two videos I will recommend you to watch those videos and then come to this video so in this video we will see how we can just draw something on a video and more specifically the aim of this video is how to show the current date and time on a live video so now in the last video we have seen how to draw shapes on images and we have also seen how to put text on our images right so let's say we just want to print the value of width and height on the default camera and let me just remove this line which we have used to convert the BGR image to the grayscale image so we will just see the colored BGR image so now what we want to do is we want to print the width and height which we get from these properties on our video which we are capturing so in the last video we have already seen that we can use a method which is CV2 dot put text yeah so this method we have seen in the last video and first of all we will Define the font which we will pass to our put text method so the font I'm using here is CV2 dot font Hershey underscore simplex and now the first argument here will be the frame which we are capturing because every frame is just like an image and a video is the combination of multiple images so the first argument here will be the frame the second argument here will be your text so the text which we want to show here is let's say width and height so let us Define a variable which we want to show on our video so let us say the variable name is text and first of all I am going to define the width so just say width and then we are going to provide the value of the width using the concatenation operator now because this value will be in integer and we want to convert it to the string so we will use the Str method to convert the integer to the string and then we can pass the width here inside our string variable once again we will use the concatenation operator and then let us provide some space here and then we will just write the height colon and then once again the concatenation operator and once again we will use this string method and inside the string method we will now take the height OK and now we will pass this text to our put text argument now the third argument is the coordinate so let us say I want to just put this text at the coordinate 10 comma 50 the fourth argument is the font which we have already declared the fifth argument is the thickness so let us say the thickness we want is one and then the color so let us say the color we want is 0 comma 255 comma 255 and then the thickness so I think the thickness comes after the color and the value 1 we have set for the font scale so you can change the font scale one two three four any font you can change it from here so this value 1 is for the font scale and the value we are providing right now is for the thickness so let us say the thickness is two and the last argument here will be the line type so I am going to just provide the line type as CB2 dot line underscore a a so what do you think will this ah text will be printed on our image or not so it will not print yet because we need to write on the frame this text so we need to just write frame is equal to and then put the text on the same frame which we are seeing right now so now this will work and let me just break this line so you will see all the code and now let's run the code and let's see what happens when we run the code so let me run this script and you will see here that now we are seeing the width and height on top of this video which is 1 2 8 0 and the height is 720.0 so this is how you can show text on your video which you are capturing from the back camera or from the video file now let me just comment these lines of code because they are changing the resolution of our video and its not fitting this video or screencast so I have commented this code and now let's do something more interesting so now let us say we want to show the current date and time on the video and you might have guessed how to print it but let me show you if you don't know how to print the date and time on your live video so first of all we are going to import the package which is available inside python which is date time and then we are going to create ah this date time variable let us say the date time variable name will be date ah T and then first of all we are going to use the Str method to convert the date and time to string and then there is a method inside this date time Library so we just need to write date time dot date time once again and then the method called now which is going to show you the current date and time so once we have converted our current date and time to the string variable then we can pass this very variable as the second argument and now let us run the script and let's see what happens once again so I am running the script and now you will see that it shows the current time and current date on the video itself so this is how you can put the text on your video you can even put some shapes which we have seen in the last video on this video itself so you can put the line or the rectangle or the circle on your video which you are capturing from the camera or some file so this was some kind of a mini project which we have created from the knowledge which we have gained from the last two videos in this video we will learn how to handle mouse event in opencv now miles event can be anything for example right click event or left Mouse button click event or left button double click event so there are many mouse event available in CV2 package now to list out all the events in the CV2 package you can write this kind of code so first of all I'm going to create a variable called events and then I'm going to just iterate over all the events inside CV2 Library so I'm going to just write I for I in dir inside our CV2 package so this dir method is the inbuilt method which is going to show all the classes and member functions inside your CV2 package okay so we are iterating over all the function names or member variable names and then we want to see what are the events available inside this package so we can just filter those events using a condition so we are going to just say that we want to just see the variables or the member properties which have this keyword event in them so event in I and then we are going to just print out all the events so I am going to just print out all the events and then I'm going to run this code and here you can see the list of all the events which are available inside your CV2 Library so you can see there is an event called event flag R button for the right button for the mouse or there is event for left button double click event or the event for L button down event so there are many such events available here and we are going to use those events to listen for the mouse events so this is how you can print all the events which are available inside your CB2 library and now we will create a script or a program to listen for the mouse event so first of all we will create a mouse callback function which is executed when mouse event takes place so in order to create this callback function we are going to just Define a function and then we will give the name to our function for example click event function and this callback function generally takes few arguments so the first argument will be the event which is taking place when we click our Mouse and then it's going to give us the x and y coordinate on the image where we are clicking with our Mouse so we are going to get the xaxis value and the yaxis value whenever we click the mouse at certain position in our image also we will get the flags and we will get the param so for creating the mouse click callback function it has this kind of specific format which is same everywhere so these are the parameter it takes and then inside your callback function you can Define the logic so let's say whenever I click the left button down then I want to show The X and Y coordinates on the same image so I can just say if the event variable is equal to CV2 dot event and then I will just look for the left button down click event so if this event occurs then I will first of all print the X and Y axis values so let's print X comma Y and you can also provide some space between X and Y coordinates using this kind of string and then what we are going to do is we are going to just put this x and y coordinate values on the same image which we are opening so we have already seen in the last videos how to put text on the videos we just need to create this font variable for the font and then there is a method called CV2 dot put text so we are going to just write CV2 dot put text which takes few argument first is the image now you can see this image shows error and this error says unresolved reference but don't worry when we write our code code fully this error will go so first will be the image the second is the string which we want to put so let's say we want to put the string Str for the X and Y value so I'm going to just write x y and then we are going to just print the x value then concatenation operator and then comma and then once again concatenation operator Y and don't forget to convert these coordinate values into the string using the Str function so Str function here and for the y axis also you need to use this Str to convert it to the string value and then the string we pass as the second argument the third argument will be the location where we want to put the text and this location education we already know from this X and Y value so it's easy for us we are going to just say x comma y because we already know the x and y coordinate using this callback event the fourth argument will be the font the fifth argument will be the font scale let us say its 1 and then the next argument is the color so let's say color we want to give here is 255 comma 250 5 comma 0 and the last argument I want to give here is the thickness let us say thickness we want to give here is 2 and then we will show this text on the image using CV2 dot I am show method so I am going to just write I am show and then the name of the image window for example image and the image itself which is is IMG so right now this is showing error to us but when we will call this callback function using a standard function called set Mouse callback then this error will go so I'm going to define the IMG variable first of all and let's say we want to create a black image using numpy so we will call NP dot zeros method here so NP dot zeros and the size of this image will be 512 comma 512 comma 3 and the data type will be NP dot U into 8 and once we have this image we are going to show this image using once again the IM show method and this image name Will be the same image and the variable we want to pass here is the image variable which is the black image which we have created using this numpy zeros function now the next and the important step here is calling a method called set Mouse callback a method so this method we are going to use to call our callback function which we have created which is Click event function whenever somebody clicks on the image which we are showing using this I am show window so the first parameter it takes is the name of your image make sure that this name here which you take in the IM show method you can see I am taking the same name here in the I am show method also here so the window name should be the same everywhere then then only it will work so here also you need to just give the parameter first parameter here is the window name so the window name is image and the second parameter is the Callback function which we want to call whenever this event take place so this is the Callback function which we have created now the next step are the obvious steps which we have already seen so first of all we will call the weight key method to wait for the Escape event and the second is the destroy all windows so we will destroy all the windows once we are finished so let's run this code and let's see what happens so now you can see the black image which is created by numpy zeros method and when I click on this image anywhere you can see the coordinates of the position where I have clicked is uh printed here so let's click here you can see when I uh give this left down button click event then the position of the X and Y coordinate is printed on this black image so I'm clicking again and again this left down button and the position is printed okay so let me just close this window now what I want to do is let's just reduce the size of this font 2.5 then the font size will be little bit smaller now what I want to do is I want to listen for some other events so I will go to my callback function once again and I will add one more condition here so once again if event is equal to CV2 and this time I want to listen for the right click event so I'm going to just write event write button down event okay so whenever somebody presses this right button down for the mouse then this event is going to be captured inside this condition now if you see remember I have told you that image is shown in opencv in the form of BGR format and we already have this image you can see we have declared the image variable that's why this error is also gone so using this image we want to find out the red blue and green channel so now inside this condition what I want to do is I want to print out the BGR channels of the image wherever I click okay so you can first of all declare a blue variable name and then we have IMG variable which is this one and using this image variable we can get the blue Channel using the coordinates so first of all you can provide y comma X we already have the y and x coordinates and then the channel for the blue color is channel 0 because it starts from Blue BGR so blue and then green and then red okay so I'm going to just copy it two more times the second is green and the channel for it will be the one or index will be 1 here and for the right channel this index will be 2 here so I'm going to just write Red here and once again I'm going to just copy this code and this time what I want to do is instead of printing in the coordinates I want to print the BGR channel so here I'm going to just write blue and then second will be the green Channel and then the third will be the red channel so I'm going to just write comma and then concatenation operator s t r and then red Channel okay so this will be the string we are going to name it as BGR and this string we will put here the color also we can change so the color for the coordinates will be different and the color for this event will be different 255 okay so it's going to print the BGR channels on your image now because we are creating the black image whenever I just click the right uh click mouse event you can see the BGR channels for this black image will be always 0 0 is 0 right when I click the left click then these are the coordinates when I click the right click these are the BGR channels so let's change this image from the black image to something visible so I already have the Lana image so we can use this lamina image using the CV2 dot IM read method so I'm going to just try CV2 dot I am read and the first will be the name of the file which is Lena dot jpg e so now let's run this code once again and now I have this colored image so we will be able to see these functionality in a better way so first of all the left button click event you see the coordinate and when I click the right click button event then you can see the BGR channels are printed once again here you can see the BGR is different here also these are BGR colors are different so you can see everywhere they are a little bit different because this is the colored image and the color differs at every pixel level so this is how the mouse click event works in the last video we have seen how to use Mouse click event in opencv using python so we have seen how we can create a callback function which listens to a mouse click event and then how to use this callback function using the set Mouse callback method now in this video I will show you some more examples about Mouse click event so the first example I want to show about drawing a point and then connecting points using the line so to start with I'm going to just remove this if condition for the right down button click event and every time somebody clicks the left button down click event of mouse then what I want to do is every time the mouse is clicked down I want to draw a circle very small circle and when he clicks on the next point then I want to join those two points using a line so for that I will need a CV2 Circle so I am going to remove this code which we do not need right now we just need this condition which listens for the left button down click event of mouse and then what we will do is we will just use CV2 to draw a circle so we will just write dot Circle and first of all this circle method takes the image so we are going to pass the image and then the second argument is the coordinates x and y coordinate so we already have x and y coordinate using this callback function with the second and third parameters and then the third parameter will be the radius so I will take the radius 3 which is like very small which will give you a effect like a point on an image and then we can give the color so let's give zero comma 0 comma 255 and then we will give the thickness now the thickness I'm going to give here is minus 1 and you might already know what this minus 1 do so this minus 1 whenever you give as a thickness it fills your circle or any closed shape okay so your closed Circle will be filled with this color which you provide here now next what I want to do is I want to create an array of points so I am going to ah just declare a variable called points and initialize it with an empty array now this MTR variable we can use inside our callback function and what we are going to do is we are going to just add or append every time this mouse is clicked so I am going to just call an append method here and then we are going to append the x and y coordinate to this points array so we know that where this mouse is clicked and we are saving the coordinates wherever the mouse is clicked in the form of array now in The Next Step what we will do is if the mouse is clicked more than two times so we can just test the length of this array which is a point and if the length of this array is greater than or equal to 2 because the first click will be only a point so we cannot connect this point with a line but when we have two or more points then we can connect those points with a line right so if this points array length is greater than 2 then we are going to just create a line between those points or the circles in our case so I am going to just call c v 2 dot line method and first argument here will be uh image the second argument here will be the point one so the coordinate of point one now we want to join the last two points right so we are going to just use this points array and then to get the last value of an array we use minus 1 here so here as an index we will give minus 1 which means the last element of an array and then we will join the second last element of an array so I am going to just give this will be points variable not print so let's give the points variable and then we are going to pass the minus 2 here which will be the second last element so last and second last element we want to join and then the next argument will be the color so let's say the color we want here is 255 comma 0 comma 0 and the next point will be the thickness so we will give the thickness of 5 here and then we will show this image using IM show method this code I have already shown you in the last video so I will not explain what this code is doing if you want to know more about this code you can see the last video and this time I will use the numpy zeros array which will be a black image so let's run this script and let's see what happens so I'm running the script and now I click on some position on this image and you can see this red circle is created the circle is created using CV2 dot Circle method and because the radius is 3 the circle is very small and because the thickness is minus 1 the circle is filled with the color which you provide here now we have said that if the point is only one then we do not want to create any line if there are points which are two or more then we want to connect those point with the line so let's click here and you can see 0.1 and 0.2 are connected with a line I click here and you can see the last and the second last points are connected with the line that is why we have taken this minus 1 and 2 argument which means the last element of the array and the second last element of the array so when I click at any point it will be now connected with this blue line so this kind of ah line drawing you can use in satellite images where you want to connect two points together with the line now let's see the next example which I want to show you so in the next example what I want to do is I want to first of all read an image and then I want to click on any point on the image and then I want to show the color of the point which on which I have clicked using a second window so for this instead of using the numpy array which is the black image I will use the normal image which is the Lana dot jpg image and now I will just remove this code from here so first of all I want to read the BGR channels so first of all I will just declare these variables first is blue and we have the image and in the last video we have already seen how we can get the BGR channels because we have the x coordinate and the y coordinate and we also know that blue is the first channel so we use the index 0 here to get the blue channel from this image at this coordinate which is X and Y same we will do for the green channel so green I am G and then X comma Y and then the channel index will be 1 here and then we will just uh get the red channel from this image and now what we are going to do is we are going to just draw a circle on this point where ah you will click this Mouse uh down button click event so I'm going to just write CV2 dot Circle and now I will not explain the parameters because you might already know what these parameters are in the next line what we are going to do is we are going to create a numpy zeros image and then we will pass our BGR channels which we got from the particular point on an image so let's create an image so I will just say my color image and then we are going to just use NP for numpy and then we will just call a zeros method here and it takes ah three argument in the form of this list which is the size of your image let us say this size will be 512 comma 512 and the channel will be three channels and then the next argument will be the data type so NP dot U and eight so we have a black image using this numpy zeros and now we want to fill this image with the BGR colors which we got from the particular point of the image so in the next line what we are going to do is we are going to just use this variable and then we are going to just write this kind of notation this means we want to fill every channel or every uh point of ah this list and then we will just pass our BGR Channel values which we got from the image so blue green and then the red Channel values we are going to pass so this will give us the BGR Channel which will be the color of the point where we have clicked and now we have the new image with the color so we can show this image using a new window with let's say this is the color window okay so this is how you will get the new window with the color on which you have clicked so let's run this code and let's see what happens so I'm going to run this code and you can see this is the image which is the colored image let's see I click on this point and you can see the same color on which I have clicked is opened in the next window let's click here on the hat you can see it's going to give you the same color on which I have clicked let's click on the eyes and you will get the same color on which I have clicked let's see what happens when we just load a Black image instead of this colored image so I'm going to just use this numpy zeros ah image which is the black image and let's run this code and now whenever I click on this every time I click on any point it will be the black color window which will open so this is how you can use some examples to understand how Mouse click events can work and you can use them to develop your applications in this video we will see some of the basic and arithmetic operations on images using opencb so let's get started so here I have this code some of this code you already know so you already know how to read the images using I am read method and then show it inside a window using I am show method and destroy all windows using this destroy all windows method but this code in between is little bit new so let me explain line by line what this code does so when you have this image using IM read method or any other method you can use these attributes like shape size and D type to get different values from this image so image dot shape is going to return a tuple which contains the number of rows columns and the number of channels in this image the image dot size will return the total number of pixel which are there in inside the image and image dot d type is going to return the data type of the image which you have obtained now here if you want to split your image in three channels then you can use CB2 dot split method and pass your image as an argument and it's going to give you the BGR channel of your image now if you have BGR channels and you want to merge those BGR channel into an image then you can use CV2 dot merge method and pass these BGR channels in the form of Tuple and it is going to give you the image which you can load using I am show method so let's run this code and let's see what we are getting using these attributes so you can see this messy 5.jpg image is loaded and here you can see first of all the shape of the image so the shape Returns the number of rows number of columns and the number of channels so number of rows here is 342 columns are 548 and number of channels are 3 here the number of pixel which we have calculated using the size is this number which is five six two two four eight and the data type of an image is uint 8. so sometimes you need to debug the data type of your image and this attribute will be very useful in those cases and you need to debug if something is correct or wrong and because we have splitted this method using this split and remerged these BGR channels using this merge method so we will at the end get the same image which we have at the beginning here in this code so there is no change in the code so once again let me just load this image and now let's talk about the ROI of an image so Roi stands for region of Interest so sometimes you need to work with certain region of the image so let's say you only want to work with the face here or you only want to work with this ball okay so this is called the region of Interest or in short form it's called Roi so let's say we want to just work with this ball here so this will be our region of Interest or Roi and I want to just copy this ball to other place in this picture so I want to just copy this ball and place it on the other place let's say somewhere here okay so how we can do this so I already have the coordinates of the ball but you already know how to get the coordinates of some place in the image we have already discussed this in our previous video so I am not going to show you how to obtain those coordinates but let us say I have those coordinates of the ball so I'm going to create a ball variable and we have our image so we will take our image and there are certain numpy indexing features which we can use here so I am going to just write ah 280 colon 340 which is going to give you one point on the ball which is the upper left hand side of this ball and then we will give 3 30 here colon 390 which is going to give us the bottom right hand corner of this ball okay so now we have this ball so this this indexing is going to copy this ball all the pixels of this ball and then now we have the ball so we can place this ball on any place on this messy image which we are reading so what we can do is we can once again use IMG and using those numpy index using features we can place this ball at some other place so let me just give those indexes here so let me give 273 colon 333 I have already tested this code so that's why I know exactly where I want to place this ball but if you are not sure where to place this ball then you might have to first calculate or know the coordinates where you want to place this ball and you already know how to find out the coordinates on an image and you will be able to place ah that Roi or interest of region at some other place so what I am doing here is I have just copied the ball and then I am placing the ball on this coordinate Okay so so I just need to just assign our ball on this coordinate and then this ball will be copied to this index on the image so let's see what happens when we run the code so now you can see we have copied this ball and we have placed this ball here on the image so this is how you work with the ROI or region of Interest okay so let me close this window now the next thing which I want to show here is how you can add two images so for that I need one more image so you can see in my project I have this messy 5.jpg and I have this other image which is opencv hyphen logo dot PNG file which is of the same size as the messy dot jpg image so I'm going to just write I am G2 and then once again CB2 dot I am read method and then I am going to give the name of this file which is opencv hyphen a logo dot PNG file okay so this is this file so this file we are reading and then there is a method called add okay so we are going to use this method here let's use this method CV2 dot add and this method I'm going to show you what it does in a moment but this method takes two argument first is the first numpy array so let me show you what this method do first of all so this is the add method inside your CV2 package you can also see the documentation on the the opencv.org and what it does is it calculates the pre element sum of two arrays or an array and a scalar okay so here we can just pass our two arrays which we got from the I am read method and pass here as the first and the second argument so I am G and I am G two are the 1 and 2 parameter and there are some other parameters also like output array input array mask and int which is the data type which we which are set by default so we are not going to set them so we are just using CV2 dot add method on these two images and then I just want to assign the new image which we have added to a new variable let us say this is DST for Destination image and then we are going to just show this image using this uh I am show method okay so we have two images let me show you those images one by one first of all so this is the first image I have and the second image is opencv hyphen logo which is like this one okay so those two images we have and when I run this code after adding those two images using add method you will see first of all you will see this error and why this error is coming because you will see here that the size of those two input is not matching okay so in order to add two images you need to have the images or the arrays of same size and then only you will be able to add those two images so let's resize those two images into a size which is common to both of them so you what we are going to do next is we are going to resize those images so once again I am going to just use IMG variable so what I get after the resizing I will once again assign to this IMG variable and there is a method called CV2 dot resize and this helps us to resize the image so first of all we need to give the source which we want to resize and then we are going to give the size which we want to get so the number of columns and number of rows we can give here let us say we want to just resize this image to 512 to comma 512 which is the number of rows and number of columns right same we will do with the next image so I am g 2 and then once again CV2 dot re size and then the Source here will be image 2 and the size which we want here is again Phi 1 2 comma five one two in the form of Tuple so we have resized this image and this image which are of different sizes to the same size and now let's run the code once again and now you will see that these two images are merged now okay so you will be able to see the hand here and little bit foot and here the ball of ah this image one which is messy five and then we have the second image which is opencv which is added to the first image so this is how you can add two image using opencv now there is one more method which is called add weighted okay so this add method is going to just add these two images but if you want to add the weight for example you want to give the weight 90 to the first image and 10 percent to the second image there is one more method so let's go to the documentation once again and there is this method called add weighted method okay so this add weighted method takes uh several arguments here you can see first is the source of the first array and second argument is the alpha value Alpha is the weight which you want to give to the first image Okay the third argument is the source two so in our case this will be the image two the fourth argument is the beta beta is the weight which you want to give to the second image right so this weight you can can give from 0 to 1 anything and this gamma is the scalar value which you want to provide and this the second last parameter is the destination and the last is the D type or the data type here okay so this is the formula which ah this method is going to use so Source multiplied by Alpha and Source 2 multiplied by Beta plus gamma so this is the method which will be used using these arguments or simply you will use this kind of methods Source multiplier by Alpha plus Source 2 multiplied by Beta plus gamma which is the scalar you can add to the image okay so let's use this method so I'm going to just copy this method and then comment this and go to the next line and instead of using add I'm going to use the add weighted method okay so the first argument is the source which is the first source which is IMG in our case second argument is the weight so first this is the messy image right so we want to just give the weight here 90 or you can just give 0.9 here and for the second image we want to give the weight uh 0.1 okay so the sum of this weight and this weight will be one and also we are going to give the gamma value here as 0 so we don't want to add any scalar value to uh this add weighted method so the next value here will be 0 which is the value of gamma and let's run this code and you can see now now we have our messy image which is dominant here because it has the weight 0.9 which is 90 percent of the 2 and the opencv image have the weight 0.1 which is 10 percent of the two okay so the opencv image is light and the messy image is a little bit uh you know dominant here you can just give 0.5 and 0.5 so the weight of the two images will be the same and now you will see those two images in the same domination okay so 50 50 percent now let us say we want to increase this value of open CB to 0.8 and the messy image weight will be 0.2 then the dominant image here will be open CV and in the background kind of thing you will see this messy image so this is how you can add two images with the are weight and the scalar and that's it for this video so in this video you have seen some of the basic operations on the images and some of the arithmetic operations on the images which you can do using opencv in this video we will talk about bitwise operations on images using Python and opencv so bitwise operations can be very useful when working with masks masks are binary images that indicates the pixel in which an operation is to be performed so let us see how we can perform bit wise operations on images so to start with I have here one image which is image underscore one dot PNG file and let me show you the image also so this image is half black and half wide now the second image I'm creating using numpy so first of all I have used NP dot zeros and I'm just creating this image with the same Dimension as this image underscore 1 is having so 250 comma 500 is the dimension of this image and the number of channels are three and this code is going to create a black image as you might already know from our previous videos now this code is just creating a white rectangle inside the black image which we got from numpy's zeros array okay so this is the dimension of the rectangle inside your black image and the color of the rectangle will be white because this is 255 comma 255 comma 255 and we are taking thickness as minus 1 that means your rectangle will be filled with white color now here I am just showing both the images using I am show method and this code you might already know what this is doing from our previous videos so let me run this code and let's see what happens first of all so when we run this code you will see first image is this one which we have created using the numpy is zero so this is IMG one and this is the black image and we are just creating a white rectangle inside this numpy zeros image and this is the second image which is half black and half wide now we want to perform some bit wise operations on these two images so let's see how we can perform these bitwise operations on these two images so to perform these bitwise operations we have some methods inside the opencv Library so the first method will be bit and so I am going to just create a variable called bit and with let's say like this and the method inside opencv is CV2 dot bit wise underscore and so this bitwise underscore and takes several arguments as you might see here the source of the first image the source of the second image and the destination which is none by default and The Mask which is also optional so we are going to just provide our images here so I am going to provide the img2 here first of all as the first argument and the second image will be IMG one and once we perform this bitwise and operation on these two images we are going to show the result in the form of Windows I am going to create one more window which is CB2 dot I am show and I am going to name this window as let us say bit and and the second argument will be our variable bit and which we got from the operation bitwise and on these two images so let's run this code once again and let's see what happens so now as a result we have the third image so let me just open all the images so this is our first image this is the second image and the last one is the result which is the bit and operation on these two images so now you might already know how the logical end works but those of you who might not know how the logical and works let me show you the truth table of logical and so this is the truth table of logical end and if the input a and input v b is 0 then we get the result 0 okay if input a and input B either of them is 0 then also we get zeros right the result one we will only get when both the sources are one so A and B are 1 then only we will get a 1 in case of and logic so same and logic will work here so this is the zeros array right so we have created this black region from the zeros so here in these images black is performing as zeros and white part is performing as 1. so when 0 and 0 the result will be 0 right so from this truth table we have seen when the input is 0 and 0 the result is 0 same here we are seeing so when the image is black and black we get the result black when the input is white and black this means 0 and 1 the result will be once again 0 using the logical end but when the input will be white and white that means 1 and 1 the result will be wide that means the 1 okay so the only reason region white here is the result of this white and this white and the resulting image you can see here and all the other part is black because the and operation on 0 and 0 is 0 and 0 and 1 is also 0. so this is how bit wise end Works let's see how bitwise R and other operation works so I'm going to just comment this code and now we are going to just create the bitwise or operation so for that I'm going to just instead of writing bit and I'm going to just write bit or and instead of bitwise and we are going to just write bit wise or here and then we will simply call this image using I am show method so we are just calling here bit or now let's run the code once again and let's see the result so you can see the result here so let's see the truth table so in the logical or if only one input is 1 then the result will be 1. so either A or B is 1 or both are 1 then the result will be 1 and if both inputs are 0 then the result will be zero so same you will be able to see here so when the First Source and the Second Source is zero the result is zero but when the First Source and Second Source is one or white the result is white when the First Source and the Second Source is 0 and 1 or black or white the result is once again 1 or white here okay so this is how the logical R works on the image now let us see how the xor operation work on those images so I'm going to once again comment this code and this time I'm going to just perform the xor operation on these two images and now we are going to run this code once again and you will see this kind of result so once again let's see how the axon logic works so when both the inputs are 0 or both the inputs are 1 then we will get the 0 and if either A or B is 1 then only we will get the result one so same you will be able to see here so when both first and second source is 0 the result is 0 when both this first and second source is one you can see here and here the result is once again 0 here right but when the input is 0 and 1 result will be 1 and in this case also the black and white will result in the white image which is The Logical xor operation so once again let us close this and now let me show you how the not operation work so I'm going to just comment this code and then I'm going to just use the bitwise knot so here bit not let us say we will perform the bit not on the first image and the second image so I'm going to just write a bit not on the first image because it only takes one argument bit not is just the opposite of the source so if you get the input 0 then the result will be 1 if you have the input 1 the result will be zero so the opposite of the input so let's perform this operation on image 1 and image two and let's comment this code and we are going to just show these two result Windows using the I am show method and also I need to change this name otherwise we will phase problems and here also I haven't changed the name of these I am show windows so let's change the name of these windows and let's run the code once again and now you will get these results so we will get the first result so the first bit not one is the not of the first image and bit not 2 is the not operation on the second image so you can see wherever we have white we got black and wherever we have black we got white so just the opposite of the input here also wherever we have the black region we got the white ah image here and wherever we have the white pixel we got the black pixel so so this is how bitwise not operation works on the images so these are some of the bitwise operations which you can perform on your images and as I said bitwise operations can be very useful when working with masks which we will see in the later videos in this video we will talk about track bars in opencv now track bars are really useful whenever you want to change some value in your image dynamically at runtime so let's see how we can use drag bars in opencb now to start with I have this simple code which you might know what it does so first of all I have imported CV2 as a CV and then I'm creating an image using the numpy zeros array with these dimensions and then I'm creating a named window with the name image so this might seem new to you because I haven't created a named window in the previous video so the named window you can use to create a window with a name and this time we have given the named window name as image now in this while loop we are just using this I am show method to call this window and then loading this image inside this named window now you might already know what this code does it just wait for the key and if the key is Escape key then we will break out of this Loop and in the last we are just destroying all the windows which we have created now in order to create a traged bar you just need to use CV and then call a method called create track bar now the first argument here you need to give is the track bar name because you can create multiple track bars in your image window that's why you need to provide the name which is unique to this track bar so I'm going to just give the name to my track bar as B because what I want to do is I want to change the BGR values of the image using the drag bar so the first track bar will change the B Channel values that's why this first argument is the track bar name which is B and the second argument here we will give is the name of the window so that one that is why we have created this named window so that we can provide the name of the window which is image in this case and that is how we know that in which window we need to add the track bar so in the image window which is this one we want to add the B track bar now the third argument here will be the value which is the initial value at which your track bar is set and the next value here will be the count which is the final value you want to set for your track bar now there is this last thing which we want to set here and this is the Callback function which will be called whenever your track bar value changes so here for example I am going to create a callback function called nothing and this callback function definition or signature I'm going to create here so we can just create a callback function with a name nothing and this function can take this value X and this is the value of the current position of your track bar so we will see what it does little bit later and what we are going to do is we are going to just print the value of x so we will know the current position if this track bar is changed so this is the Callback function which will be called whenever your track bar value changes same we will do with the other track bar so we will create the three track bars in the same window with the name V and the next track bar name will be the G and the last track bar name will be R okay so this will be capital r so now let's run this code and let's see what happens when we run this code so I am going to right click and run this script and you can see here inside this named window with the name image we have this black image which we have created using numpy zeros array and now we have three track bars here with BGR names so these track bar values you can change using this scroller and as you can see here let me show you in this terminal whenever you change the value of any bar the corresponding value will be shown here using this callback function and inside this callback function we have the print statement ok a so as I said whenever you change this value this callback function is called and it will print the value of the current track bar okay so for this functionality what we want to do is we want to get the current position of the track bar and because we can change the value of BGR channels from 0 to 255 that's why I have given the range between 0 to 255 to the track bars also so that you can change these BGR Channel values so now in order to get the current value of your track bar first of all we will just check the value of the B track bar so we will just use CV dot get track bar position which is this method get track bar POS and then we just need to get the name of our track bar so let's say we want to check the position of track bar B then we will just say we want to have this track bar position with the name b and the second argument here will be the name of your window so in which window this track bar is present so the our track bar is present inside the image window right so same we will do for the G and R values also now we have the values of b g r channels from the track bar so now we want to set these values to our image so what we can do here is we can just write for example IMG inside these square brackets you can just give this kind of notation and then give the BGR Channel value so I'm going to just write B comma G comma r that means we want to set the current b g r values to this image so let us run this code and let's see what happens now so I am going to run this code and now when I change the blue Channel values you can see this image becomes blue colored right let us bring it to 0 once again and now let's change the value of G so you can see this image color is changing to green and then we can try changing the red color and you can see when it goes to 255 the color of the image is red you can change the values of different track bars and the corresponding color will be displayed in this window here right so you can see the color is changing you can change any track bar here one more example I want to give here is how to add a switch using a track bar so for that I am going to use one variable called switch and then here I can add first of all the name of the switch and in the next line we will once again call CV2 dot create track bar with the name switch okay so now the name of our track bar will be switch so now we have added one more track bar to our named window and now here we will get the current position of this switch track bar so I'm going to name it as s and the name of the window is switch so we will just give the first argument of this get track bar position as switch okay and the window name is image itself so now we can add some condition here so let us say if this position of the switch which we have if this position is equal to 0 because we only have 0 and 1 in this last track bar so if this position is equal to 0 what we want to do is we want to set IM G and then in the square bracket this colon and we do not want to change any value so we will say that I am G uh this square bracket colon is equal to 0 which means that we do not want to do anything or in the other condition which is when your track bar is at position 1 then only we want to change the BGR channel of the image okay so let's run this code and let's see what happens so I am going to run this code and now you can see ah the position of this track bar switch is zero and when I change it to 1 so let us change this position to one you can see the value to 1 and when this position is at 0 you can change anything here any track bar nothing happens because this condition is met which means that we don't want to do anything as soon as we change the switch to 1 that means we want to change the BGR values you can see this color is changed inside the image so the 0 is just like off switch so we do not want to change any color and one is like on switch so when it's 1 the value of RBG channels can be changed now I want to give one more example of track bar to use so that's why I have created one more file which is python opencv track bar example two and this time I'm going to use just two track bars here so that's why I'm going to delete some of the code here so using the first track bar let's say I want to just change some values inside our image and I want to print that value on that image so let's say now our range is between 10 to 400 okay so the lower range is 10 and the upper range is 400 and using this track bar I want to to print the current value on our image and also I want to have a switch which I can toggle and I want to change the color of the image from the colored value or colored image to the grayscale image so now our switch is between color to the grayscale image now in here what we want to do is we want to just assign this IM show value to the image variable itself and then we want to get the current position of the track bar so we will use this method to get the current position of the track bar and I am going to name this current position as POS variable and the name of this track bar let's change this name to something else let us say CP for current position and also here CP for the current position and the name of the named window is image itself so we are not changing it so now we have the current position so first of all we will just create the font and then we will just use the CV Dot put text method you already know what this method does it just print the text on your image and then we will provide the parameters first argument is the image the second argument is the value which we get from the track bar so this is the position and because it is a number we need to convert it to the string using Str method and then the position at which you want to show this text so let's say it's 50 comma 150 and then next is the font so I am going to just give the font and then the next value is the font scale which is 4 and the next value is for the color of the text so let us say the color here will be 0 comma 0 comma 255 and this should be C V Dot font Hershey complex let's change this font all also let us say this is just the Simplex font okay so this code is going to just print the color current position of the track bar on your image and then inside this condition what we want to do is we want to get the switch value so let's use this s variable and then get the current position of the switch using this switch name from the image window and then if the switch is at zero position then we want to do nothing so we will just pass this situation and in case the value of this switch is 1 then what we want to do is we want to change the image value from color to the grayscale value right so you can just write CV dot CVT color and the first argument is the image which we are loading and the second argument is CV dot color BGR to gray which is to convert this colored image to grayscale image but you can see here we are just creating a black colored image and in our project we also have this image so let's read this image so I am going to just write CV dot IM read and then give the name of the image which is Lena dot jpg so this is our colored image and this way we will be able to see ah the change of color to gray scale image in a better way so let us run this code and let's see what happens and you can see image appears and disappears and there is an error so let's see what is the error so the error here is coming from this line so we need to read this image inside the while loop ok so this is why our error is coming and at the last we want to load this image after this if condition okay so now let's run this code once again and you can see this value is printed on our image which is 10 which is the value of CP and if we change this value it is changing on our image also right and once we change this 0 to 1 then our image is converted from colored image to the grayscale image you can also change the font size here for example let us say its 6 here and the thickness also if you want to change you can change it using this parameter let us say its ah n and let's run this code once again you can see the thickness and the size of the font is changed and you can see this value in a better way okay so this is how you can use track bars in opencv in this video we will see how we can perform object detection using HSV color space now we have already seen how to work with BGR or colored images or gray scale images and we have already seen how we can convert from colored images to grayscale images so there are more than 150 color space conversion methods in opencv and one of them is colored image to HSV image now what is HSV color space so HSV stands for Hue saturation value so H stands for Hue as for saturation and V for the value now generally r g b in RGB color space are all correlated to the color luminance that is what we Loosely call intensity in other words we cannot separate color information from luminance so HSV or hue saturation value is used to separate image luminance from color information so this makes it easier when we are working on or we need luminance in our images that is why generally we use HSV in the situation where color description plays a very important role now as I said HSV stands for huge saturation and value but what is the meaning of each and every single word in HSV now HSV is also known as the hex con color model so this color space can be described in this kind of cylindrical cone model where Hue is this circular angle which varies from 0 to 360 and hence just by selecting the range of hue you can select any color so you can see different colors are available at different angles so these colors are basically red yellow green cyan blue and magenta so Hue is this angle in this cylindrical cone now we have saturation so the saturation is amount of color that is the depth of pigment or the dominance of hue and this value is described from the center towards the outer layer of this cylindrical cone so you here you can see at the center this saturation start at 0 and it can go up to 1 at the end of this cylindrical cone and this saturation can be increased from zero to hundred percent similarly the value is basically the brightness of the color so this brightness can be increased from 0 to 1 from the bottom of the cone to the top of the cone so all these three value Hue saturation and value can be used to pick any color as we can do with the BGR color space so this is the brief introduction about HSV color space and now let's see how we can use this HSV color space to detect an object in an image so here I have this simple code to load an image using I am read method and show it inside a window so by now you might already know how this code works so let's run this code and let's see what does this code do so I have this image which is called smarties.png and here are some circles in different colors so we have blue circles or green or red orange and brown circles here inside this image so let us say we somehow want to detect only the blue circles or balls or green circles or balls how can we just detect only these balls let us say we just want to detect the green balls how can we achieve this using opencv we are going to see this using this HSV object detection and here we have one more window which is the cracking window which is coming from this code which is CV2 dot named window and the name of the window is tracking so this tracking window we are going to use little bit later when we will add the track bars to our image but let us say we want to use this image and detect these colored balls so first of all ah after this image is read what we want to do is we want to convert our colored image into our HSV image and by now you might already guess how to convert an image you can just write HSV is equal to CV2 dot CVT color and then your frame name which is frame in this case and then CV 2 dot whatever color space you want to convert from and whatever color space you want to convert to so you can just write color underscore BGR to HSV so this is the property we are going to use now in the next step we will threshold the HSV image for a range of blue color so we are going to just Define l underscore B for lower blue color and then we are going to use the numpy array so NP Dot dot array and inside this array we are going to define the lower range of blue color Now by experience I know that these HSV value for lower blue color will be 110 comma 50 comma 50 right but you might not have every time the idea what is the lower color range or the upper color range of some color so that is why later in this video we will use the track bar in order to perfectly Define the lower and upper values for this HSV color space right so right now I am just going with my experience so for the upper value I'm going to define the next variable which is u b is equal to NP dot array and then once again I am going to Define these three ah color channels which is 130 com 255 comma 255 so this will be the upper limit for the blue color for our HSV image now in The Next Step what we are going to do is we are going to threshold the HSV image to get only the blue color let's say so I am going to just Define a variable called mask here and then I am going to use CV2 dot in range method where I will provide first of all my HSV variable or image and then I will provide the upper and lower range for this function so my lower range is this numpy array for the blue color so I am going to just say l underscore B is my lower range and U underscore B is my upper range now we have already seen how we can use bitwise and or bitwise operations on images so what we are going to do next is we are going to Define a variable called res and then we will just call CV2 Dot bitwise and to mask the original image so here the first value will be our frame which which is the colored frame right so this is the frame which we have read from this image which is the smarties image so this is The Source One Source 2 will be the same so the frame itself will be the source two and what we want to do is we want to provide the mask of the lower blue color and the upper blue color values right so here we can just say mask is equal to whatever mask variable we have created so this is the attribute we can set in order to apply the mask for the Lower Blue value and the upper blue values so once we have this result frame what we can do is we can use this CV2 dot IM show method in order to show The Mask let us say so we are going to show the mask and we are going to show the result using res variable so this is going to open three windows and let's see what happens when we run this code so we are going to run this code and this opens three Windows here and now you can see the mask first of all so we are just detecting the blue colored balls using this mask that's why we have defined the lower boundation of the blue color and the upper boundation of the blue color right so that's why it's only detecting you can see the blue uh ball is here here and here and here also you can see the mask also detects only the blue values here right and then in the result you can see when we have applied this mass and we have masked all the other things other than the blue colored ball you can see only the blue balls here so the same method you can apply to detect any other colored ball from this image now as I said it's not easy to detect ah these lower and upper boundation for the colors so that's why you can use the track bar for adjusting these lower and upper boundation of any color so for that what we are going to do is for first of all we will create a named window and then we are going to create a new window which we will use to adjust the lower and upper boundation of HSV values so now I'm going to just use a CV2 dot we have already seen how to create a track bar so I'm not going to explain in detail how this works but let us say this track bar name is lure Hue for LH okay so this is the lower Hue value and then the name of the window which is tracking which is this one so we are going to provide the name of the window and the next argument will be the starting and the ending value so we are going to define the start value 0 and the end value let us say we are going to define the 255 here okay and the last thing we want to give here is the Callback function which I have already created which is this function which is ah just doing nothing we are going to just provide this callback function as a dummy function so it is not going to do anything so this is the track bar for the lower Hue value similarly we are going to define the track bar for lower saturation and lower value and upper saturation upper value and upper Hue okay so this will be lower saturation this will be lower value and then this will be u h which is upper Hue and then this will be u s for upper saturation value and this will be upper value right so HSV lower values and HSV upper values so here we are going to set the initial value you for the upper value so let us say everything is set to the maximum so 255 255 and 255 here okay so the lower values are set to zeros and upper values will be set to 255. now you already know how to get the values from attack track bar so you can use for example L underscore H for the lower Q values is equal to CV2 dot get track bar position so just use get track bar position method and then first of all give the name of the track bar from which you want to get the position so let us say we want to get the position from the l h track bar and then the name of the window which is tracking in our case so here is the second argument and similarly what we are going to do is we are going to define the other low lower values and upper values so and also the name of your track bars so once you have the values of lower HSV and upper HSV you can provide these values here in place of these static values so first element of this array will be LH and then the ls variable and then the L variables similarly for the upper boundation we will provide these three upper boundation variables and now when we will run our code let's see what happens so we are running our code and you can see these uh Windows these three Windows one is the mask other is the result and the third one is the frame and we also have these track bars in order to change the value of lower and upper HSV values so first of all let's set this mask for the blue color so I am going to just move it to 110 as we have done in the last step and then this will be around 50 and this also will be around 50 okay so let us move it to 50 and upper value here will be around 130 right so you can see once again using this track bar it's easier to adjust these lower and upper boundation and now you can see all the three blue colored balls so you can refine this object detection by moving these track bars little bit left or little bit right you can see here now let's adjust this value to detect some other balls so let us say we want to detect the green balls so let's see what happens when we just change the saturation values here and you can see now you almost see the green values and the blue color is almost disappearing so you can see now there are only green ah the balls which are detected and all the other balls are masked so you just need to play with this track bar for the lower HSV values and the upper HSB values and you will be able to detect the object whatever colored object you want to detect from the image now this is the object detection from the image similarly we can use the same method in order to track an object from a live video so I'm going to just escape to just close all the windows and in order to change this code for the video input what we can do here is we can just add this code so so we are going to just add the cap variable which is the capture variable is equal to CV2 dot video capture so we are going to use this one and we are going to ah capture the video from our default camera which is at the index 0 and then you already know how we can read the values from the camera input so I'm going to just comment this code and instead of reading the image image what we are going to do is we are going to write underscore comma frame is equal to cap dot read which is going to read the frames from your default camera and at the end when you are done playing with your images you can just destroy this cap using the release method so you can just write cap dot release just going to release all the cameras you are just capturing right so now this is the three line code you need to use in order to capture the camera input and then track any uh object of any color so I am going to run this code now and you can see I am just holding a blue colored object here and I'm moving this object on the left and right light and you can see only blue colored object is detected and every other frame value is masked so this is how you can do the object tracking of any color using the HSV color space so you can see the real image which is captured from the camera and then the mask and then the result of the mask and the real image in this blue colored object tracking so this is how you can do object detection and object tracking using HSV color space in this video we will see how we can perform simple thresholding on images using opencv so first of all what is thresholding so thresholding is a very popular segmentation technique used for separating an object from its background the process of thresholding involves comparing each pixel of an image with a predefined threshold value and this type of comparison of each pixel of an image to a threshold value divides all the pixels of the input image into two groups so the first group involves pixels having intensity value lower than the threshold value and the second group involves the pixels having intensity value greater than the threshold value and using the different thresholding techniques which are available in opencv we can give different values to these pixels which are higher than the threshold value and which have the intensity lower than the threshold value so let's see how we can use Simple thresholding techniques on an image so to start with I have this simple code which loads an image on a window and this image is called gradient dot PNG so let me show you how this image looks like so this image looks like this so as you can see in this image we have on the left hand side the black values and when we gradually move from left to right we move towards the wide value so on the left hand side the pixel value are closer to zero and on the right hand side the pixel values are closer to 255. so now we are going to ah just involve some thresholding techniques and we will see how these this image is affected by the thresholding techniques so first of all what we are going to do is we are going to ah Define two variables one is underscore because the result of the thresholding gives two result one is ret value and the second is the thresholded value of an image so I'm going to just say the second value which is given by the thresholding technique is th1 is equal to CV Dot threshold and this threshold method takes several values the first is the source so our source is image the second is the threshold value so as we I have seen that our image have on the left hand side 0 pixel value and when we move towards the right its pixel value increases to 255 right so let us say our threshold here is 127 and the maximum value of the threshold is 255 which is the white color on the right hand side and then the fourth value here will be the threshold type so there are several threshold type in simple thresholding we are going to see them one by one so the first thresholding type is CV2 dot thresh binary so first of all let me show you how the result looks like and then I will explain what does this trash binary type does so what we are going to do is we are going to use one more CV2 dot I am show method to show this thresholded value into a new window so we are going to just show this value into a new window and we already have the original image in the image window so let's run this code and let's see what happens so you can see in this binary thresholding we are comparing each and every pixel of this original image to 127 and if the value of the pixel is less than 127 the value is assigned to 0 and if the value of the pixel is greater than 127 the pixel value is assigned 255 that means wide so if the value of the pixel is 0 it will look black and if the value of the pixel is 255 it will look wide so this is how binary thresholding works and by the name itself you can understand that this is just a binary thresholding so it is either 0 0 or 1. now let us see the other type of thresholding technique so now I will just change the name of this variable as th2 and the next type of thresholding is called thresholding binary inverse and as the name suggests the thresholding binary inverse is going to give the inverse result of what you get from the trash binary so I am going to once again use the IM show method to show the result of this thresholding binary inverse value and let us run the code and let's see what happens so this is the original image and then we have the thresholding one image and the thresholding inverse image so this image you got from the first thresholding which is by using trash binary and the second image you got from this method which is trash binary inverse and this trash binary inverse image is just the inverse of what you get using the thresh binary so if the pixel value is lower than 127 which is our threshold the pixel is assigned 255 otherwise if the value is greater than 127 then the pixel value is assigned Find 0 which is the inverse of what we got in the previous step now let us change this threshold to let's say 50 and here also let us say we change this threshold to 200 and let's see how this result changes when we change the threshold value so I am going to run this code once again and you can see this is the result of thresh binary and now because our threshold is up to 50 that's why our result is like this so until the pixel value is 50 its black otherwise if the pixel value is greater than 50 it is going to give you the white ah pixel value and the thrash binary inverse is going to give you the inverse value of what you get in the thresh binary step so I'm going to once again just close these windows and let's see the next thresholding type so I am going to name my variable as th3 so the next thresholding type is called trash trunk so this is this type and let's first of all see what is the result of this thresholding technique and then I'm going to explain what it does so we are going to just show this thresholded image into a new window and run the code and now we have the result so let's move it like this and we have here the original image and the result of the thresh trunk is this th3 so here what happens is up to the threshold the value of the pixels will not be changed so up to 200 because our threshold is up to 200 so when the pixel value is up to 200 the pixel value will not change and after the threshold which is 200 the pixel value will remain the same which is 200 so from here to here the pixel value will remain 200 let us change this threshold to some other value let us say one twenty seven and then let us run this code and you will see that now from black to 127 pixel value the value of this image will not change so original image up to half is the same and after the pixel value 127 the value remains 127 okay so the pixel intensity value will remain 127 until the end so if the value is greater than 127 the value will remain 127 and if the pixel value is lesser than 127 then the pixel value will remain unchanged so this is how the trash trunk works and let's see the other method which is let us say th4 and this is the method which is called trash 2 0 so we are going to just use trash to 0 and then we are going to open this th4 into a new window and let's run this code and let's see what happens so now we have this result let's move this to the left and the result of the thresh to 0 is this one so in thresh to zero thresholding whenever your pixel value is lesser than threshold the value assigned to pixel will be zero okay so when the pixel value is lesser than the threshold the pixel value is assigned to 0 that's why you can see half of the image is black and when the pixel value is greater than 127 which is our threshold value the image or pixel value will remain the same so after 127 all the pixels will remain the same let's see the other technique which is called trash to 0 inverse which you ah already understood I think what it does so this is thresh to zero inverse and we can just change this variable name to th5 and here we can just open it into a new window and I'm going to run this code once again let me move this here and the result here so you can see this thresh to 0 inverse is just the opposite of the thresh to zero so if the value of the pixel is greater than the threshold value which is 127 the value will be assigned to 0 otherwise if the value of the pixel is lesser than threshold the value of the pixel will remain the same so this is how some of the simple thresholding techniques Works in opencv in the last video we have seen how we can perform simple thresholding in opencb using python using various thresholding techniques so we have used trash binary trash binary inverse thresh trunk trash to zero tries to zero inverse so these were all the simple thresholding techniques now in these thresholding techniques we were setting a global value of our threshold so in this example for example here the global value of threshold is 50 here 200 here 127 so we were setting in simple thresholding the global value and this means that it is same for all the pixels in the image now in this video we are going to learn how to use adoptive thresholding so adoptive thresholding is the method where the threshold value is calculated for the smaller region so the threshold is not Global for every pixel but it's calculated for a smaller region and therefore there will be different threshold value for different regions now why do we need this type of adoptive thresholding so using simple thresholding might not be a good idea in all the conditions so there might be conditions where the image has different lighting conditions in different regions and in those cases where the lighting conditions in the images varies from point to point in those cases we might want to use adoptive thresholding so as I said adoptive thresholding calculates the threshold for a smaller region of images so we get different thresholding values for different regions of the same image and as a result adoptive thresholding gives us better results for images with varying illumination so let me show you the problem with simple thresholding for the image which have different illumination at different regions so I have this image called Sudoku dot PNG which I am loading using I am read method and then I'm just showing this image using IM show method and then let us use the simple thresholding technique which is trash binary for this and we have set the global threshold value of 127 here and then we will see the result after this threshold is applied to the image so I'm going to run this program and let's see what happens so this is our original image and then this is the result which we got so on in the result you can see when we apply a same Global threshold value some of the region of this image is black and other region of this image is visible right so because the image have different elimination value at different regions that's why we see half of the image which have the good illumination and we do not see half of the image which does not have the better illumination so in that case it is a better idea to use adoptive thresholding so let's see how we can use adoptive thresholding so here what I am going to do is I am going to declare a variable called th2 and then we use CV e dot adoptive threshold so this is the method which we are going to use for performing adopting thresholding and this takes few arguments so first is the source so our source is the image variable now the second parameter here is the max value so the max value is the non zero value assigned to the pixels for which the condition is satisfied so in our case the maximum value which we can provide to a pixel is 255 and we cannot go more than that right now the third parameter here is the adoptive method so this adoptive method is going to decide how the thresholding value is calculated and there are two types of adoptive methods which we can use so the first method is called CB2 dot adoptive Thresh mean c so what is the meaning of this adoptive thresh mean underscore C so using this method the threshold value is the mean of the neighborhood area and here is the documentation of these two methods so adoptive thrash mean c gives us the threshold value using this function and this is going to give us the mean of the block size multiplied by block size neighborhood of X comma y minus C which is the constant and the second adoptive threshold type is this one which is adoptive trash gaussian underscore C and in this adoptive thresholding the threshold value is the weighted sum of neighborhood values where weights are the gaussian window so let us use the first adoptive method which is the adoptive thrash mean underscore C now the next parameter here is the threshold type so the threshold type which we are going to use is the thrash binary which we have also seen in the last video also and then the next value is the block size so block size decides the size of the neighborhood area so here we are going to give the block size 11 and the next parameter here is the value of C so we have seen that we need to ah give the value of C also when we use the adoptive thresh mean c and adopt a thrash gaussian C so this is the value of C which we are going to give and C is just a constant which is subtracted from the mean in the case of this adopted thresh mean method or the weighted mean in the case of gaussian adaptive threshold ok so constant we are going to give here is 2 and now what we are going to do is we are going to just load this image which we got after applying this adoptive thresholding and let us just comment the other window so we will just see the original image and the adoptive thresholding result so I'm going to run this code and you can see the original image here and the result of adoptive thresholding which looks much better than the simple thresholding technique so let us uncomment this also so I am going to uncomment this so we will see all the three result at the same time so this is the original image and you can see the simple thresholding gives us this value using the global threshold of 127 and adoptive thresholding gives us this value or this image which is much more readable than the simple thresholding technique image so this is how you can use adoptive fresh mean c method in a same way we are going to use the other adoptive thresholding technique which is called adoptive thrash gaussian C so instead of this we are going to use adoptive thresh gaussian C and then all the parameters we are going to leave as same and let's load the result of this type of thresholding which is stored in th3 so let us run this code and let's see what happens so we have already seen this image which is the simple thresholding this is the result of the adoptive thresholding mean c and this is the result of adoptive thresholding gaussian underscore C so both of the result looks good because the adoptive thresholding algorithm calculates the thresholding value for different regions so the thresholding value is not Global for each and every pixel of the image and we have seen the two adoptive methods which are available in adoptive thresholding so in this way you can use adoptive thresholding in opencv in this video we will talk about a library called matplotlib which you can use with opencv images so first of all what is matplotlib so matplotlib is a plotting library for python which gives you a wide variety of plotting methods and on the official website which is matplotlib.org you can see match plot lib is a python 2D plotting Library which produces publication quality figures so it's primarily a 2d plotting library but its widely used with opencv to display graphs and images and histograms so we will see how we can use matplotlib with opencv it's also written here that for simple plotting the pi plot module provides a Matlab like interface so first of all lets see how we can install matplotlib and then we are going to see how to use matplotlib with opencv so to install matplotlib using pip you just need to open your terminal and then just give this command which is PIP install mat plot lib and then press enter and in some seconds this matplotlib Library will be installed using pip so now you can see matplotlab is installed on my Windows operating system and to check it I'm going to just give the python command and here I'm going to import matplotlib so I'm going to just write from matte plot lib import Pi plot as PLT okay and then press enter and if this import does not give you any error that means it's imported successfully and you can start using matplotlib now as we are using pi charm IDE let me show you how you can install matplotlib on pycharm so just open your pycharm IDE and then here just click on file and then settings and then go to Project colon your project name my project name is opencv examples and then click on interpreter and you can see other packages are already there and we just need to install the matplot lib package so just type here in the search matplotlib and you will be able to find matplotlib here in the results so just click on matplotlib and then just click on the install package so I'm going to just click on the install package and in some seconds mat plot lab Library will be installed in your pycharm IDE so you can see this message which says package matplotlab installed successfully that means we can close this window and then you will be able to see matplotlib is available in your project interpreter so everything is fine and I'm going to just close this and now I will be able to import matplotlab so I am going to just write from matte plot lib import Pi plot as PLT now in order to show the image which you read using the opencv I'm read method you can use this code so just write PLT dot IM show so there is also a method inside your Pi plot Library which is available inside matplotlib and this mesh third you can use to show the image which you have read from the opencvm read method so for now just write this kind of code and to show the mat plot lab window you just need to write PLT dot show so this is going to show this image using the matplotlib library so we are opening this image using the opencv I am show window as well as matplotlib window also so let's run this code and let's see what's the result which we are getting so you can see this is the image which is loaded using the matte plot lib and this was our original image which is loaded using the opencv library and straight away you can see some difference so this is the original image which is the colored image and in the matte plot lib ah window we also want the same result but it is giving us the different result and the reason behind this is opencv reads the image in the BGR format and the matplotlib reads the image in the RBG format so in order to show this kind of colored window using matplotlib you need to convert your image from BGR to RBG and then only you will be able to see this kind of of colored image using matte plot lab so I'm going to just close these windows and now after I'm showing this image using the CV2 I am show method I'm going to convert this image so I'm going to just write I'm G is equal to CV2 dot c v t color and then I'm going to convert this image from BGR image so I'm going to just write CB 2 dot color underscore BGR to RGB okay so our matplotlab library shows the image in the RGB format and the opencv reads the image in BGR format so now we have converted our image from BGR to RGB image and now we are showing this image using the matplotlib and let's run this code and let's see what happens now so now when we run this code you see both the image looks the same right now let's see the advantages of using matplotlib so you can see this is quite static window but when you see in matplotlib when you hover over this image you can see X and Y coordinates of the mouse point and this is helpful you can also save this image in the form of a PNG file so you can just press this and save this image wherever you want you can also Zoom this image if this feature is available there is also configuration subplots options so you can you can just increase these values left bottom wherever you want to place your image you can do that these are some options which are available here you can also reset these options and you can see the coordinates here so because matplotlib is primarily a 2d plotting Library so you can see the x coordinates and Y coordinates and because this image is about 512 by 512 pixels that is why here it's showing 0 to 512 and here also on the y axis 0 to 512 so this is how you can load your image using matplotlib and now I'm going to show you one more thing and this is when you write PLT dot X takes here and then when you pass m t array here which is empty square bracket comma PLT dot y takes and also here you pass Mt array this is going to hide the tick value on X and Y axis so now when I run this code and you can see now that these X ticks and Y takes on X and Y axis are gone so let me just comment this out once again and you will be able to see this X and Y coordinates here on the image and when you use this code which is to hide the text on the X and Y axis then you will see the image without these X and Y axis ticks so if you remember in the last video we have seen how to use Simple thresholding in opencv and we were using six Windows to show these six different images using opencv now let us say you want to show all these six windows in one matplot lib window how you can do it with the use of matplotlib I am going to show you so first of all we are going to import matplotlib import Pi plot as PLT and then what we are going to do is we are going to define the titles and then we are going to Define these six different titles for six different images so first one is our original image second was the trash binary third was trash binary inverse fourth was trunk fifth was two zero and six was to zero inverse in the same way we are going to Define a variable called images and inside this square bracket we are going to pass first of all our original image and then th1 comma th2 comma th3 comma th 4 comma t H five ok so these are the six value we want to show and these are the six titles of these images and now we are going to use the for Loop so for I in X range so using the Python X range we are going to just iterate over these six values so I am going to just write X range and then the range we are going to provide here is 6 and then inside this for Loop we are going to just call PLT and we are going to call a method called sub plot okay and this subplot method takes few arguments so first argument is the number of rows which we want to show in our matplotlib plot so because we have six images so we are going to divide these images into two rows and three columns so the first argument and here is the number of rows and the second argument here is the number of columns and the third argument here will be the index of the image so the index of the image will be I plus 1 and then we are going to write comma PLT dot I am show so this is going to show this image and the index of the image so we are going to just write images and then square bracket I so this is going to give you a particular image at index I and then we want to show this image as a gray scale image so anyway when you use thresholding you use the grayscale image so you just need to write a gray here then we are going to show the titles of these images so we are going to just write PLT dot title and then this title method takes the title name which we are getting you using this titles array and then at the index I this is going to give you the title name which we have declared in this title array and at last if you don't want to show the text on the images you can give these two method which is PLT dot X takes ah and the argument here is the empty list and also PLT dot y takes and the argument is the empty list and at the end what we want to do is instead of using this kind of code we just want to show our window so we can just say PLT dot show and this is showing us this error unresolved reference yes so this is when you are using python 2 but in Python 3 this x range is changed to a method called range and that's why it was giving us the error so let's run this ah script once again and you can see six different results and six different titles so these are all the titles which are shown here and then the result are shown under these titles so using matplotlib you can include multiple images into one window and this is very useful when you want to show multiple image at the same time in the same window so this is how you can use matplotlib library with opencv images and there is a lot of things which you can do with matplotlib so if you want to learn more you can just go to the official website which is matplot lib dot org and then you will be able to see more documentation here in this video we are going to discuss about morphological transformations in opencv so we will discuss different morphological operations like erosion dilation opening and closing methods Etc but first of all what are morphological transformations so morphological Transformations are some simple operations based on the image shape now morphological transformation is normally performed on a binary image and when we perform morphological transformation there are two things which are required first is the original image and second is called a structuring element or a kernel which decides the nature of the operation now there are different type of morphological Transformations and we are going to see them one by one now to start with I have this simple code which reads the image using opencv IM read method and we are just loading or showing this image using matplotlib now if you are unfamiliar with matplotlib and how to use matplotlib to show images in the last video I have explained this topic in details so if you want to see that video about matplotlib you can see it and this is the code I have used in the last video also and and I have explained this code in details in the last video so if you are confused what this code is doing just see the last video now there is one important thing to notice here is I am reading this image in a grayscale mode okay so either you can provide here as the second argument of I am read CV2 dot I am read underscore grayscale or you can provide simply 0 here in order to read this image in grayscale so let's run this code and let's see what it does so as expected it's just opening the image in the grayscale mode using matplotlib now as I said normally we perform the morphological Transformations on the binary images so that's why we need to provide a mask to our image using the simple thresholding so let's just do that so I am going to just write underscore comma The Mask so I'm going to name my variable as mask here and then I'm going to just write CV2 dot threshold and this threshold take few argument as you might already guess first is the image itself second argument is the value of the threshold so for now I'm going to just provide the threshold of 220 here the maximum value of threshold will be 255 then the next argument here is the type of the threshold so we are going to provide CV 2 dot thresh binary inverse so this is our mask so let's load the mask in the matplot labor window so I'm going to just provide in this titles array one more title which is mask and then we are going to see how this image looks like after the mask okay and here the range I am going to increase it to 2 because now the array is of two elements and the subplot is also let us say 1 by 2 so we want to show two images and I'm going to just run this code and you can see this was ah the image which was the grayscale image and the second image is the masked image now if you see this image carefully let me just ah just increase the size of this image and if you see this image carefully after masking there are some black dots here on the balls and let's say we want to remove these dots which are there in between this white area this black dot or this black dot or you can see some black dots are there inside your ball in the white area and we want to remove these dots from the balls for this we are going to use the dilation transformation so first of all what we are going to do is we are going to just write dilation which will be our variable name and then we are going to use this method called CV2 dot dilate okay so this method uses the source which is mask in our case and then the second thing is the kernel okay so let me explain what the kernel is so a kernel is normally a square or some shape which we want to apply on the image so we are going to define a kernel of numpy ones which means we want to apply white square on our balls so you can see when we run our code once again it shows us error because this kernel is undefined so let me Define this kernel first of all so I'm going to just say kernel is equal to NP dot once and then we are going to define the shape of this kernel let us say this is of 2 comma 2 size and then we will just say NP dot U and 8. so this is our kernel and kernel in this case is nothing but a 2 by 2 square shape and this square shape kernel is going to be applied on our image wherever these black dots are are there so now we have defined this kernel so let's see after this kernel is applied on our masked image how it looks like so I'm going to just add one more Title Here which is dilation and then I'm going to add the image after the dilation is applied on our image and then we are just going to increase the range to 3 because now we have three images and let's say this plot contains images One by three so one row and three columns right so I'm going to just run this code once again and now you can see all these three images first was the original image second is the masked image and the third one is the image which we got after we applied the dilation let me just increase the size of this image somehow so now you can see that for example here there was a black dot and now it's reduced right the size of this black dot is reduced here also there was a black dot but its size also is reduced but still we can see these black dots here right so how we can ah remove these black dots completely so there is a third parameter which we can provide to this dilate method is called iteration so number of iterations so we can just provide iterations is equal to whatever the number of times we want to perform dilation on our image by default it's one and you can provide let us say two here and lets see what is the result now so I am going to just run this code again and now you can see those black dots which we can see here on the masked image are now gradually gone but still I can see some little dots on the images the small dots are already gone right so now what we can do here is we can increase the size of the rectangle so this rectangle is applied to our area which have these parts so we can increase the size of the rectangle and the bigger the rectangle is the better the result will be but there will be a problem which I am going to show you so let us run this code and you can see now all the black dots from our image is gone so there was a black dot here which you don't see anymore and there was a black dot here here here and here and we don't see these black dots here but you might also observe that the size of this white area is also increased after we applied the dilation on this masked image so now this ball and this ball in the result after the dilation is merging here right so you can see it's merging because the size of our kernel is big and when we apply dilation the pixel element is one if at least one pixel under the kernel is one that's why the shape of these balls are increasing so let's see how our next morphological transformation Works which is called erosion and after that I'm going to explain you how ah this erosion works and what is erosion so I'm going to just declare a variable called erosion and then I'm going to just call a method called CV2 dot erode so the method name is erode and the first argument here is the source the second argument here is the kernel as we have seen in dilate method and the third argument is the optional argument which is the iterations so for now we just apply one iteration which is by default ah also one and now we are going to just add this image to our matte plot live window so I'm going to add the title and the image and now I will increase the range of the array to 4 and let us say now we want two by two ah Matrix of these images right so let's run this code and let's see what happens so now you can see four results here and first was the original image second was the masked image third was the dilation so all the spots in the balls which are black are gone using the dilation but the size was increased and using the erosion you can see the sides of the ball eroded so the basic idea of erosion is just like soil erosion it erodes away the boundary of the foreground object so when this erosion is applied the kernel which we have defined slides through all the image and a pixel in the original image either one or zero will be considered as one only if all the pixels under the kernel is one otherwise it is eroded and this means this value will be set to 0 which means this will be a black area so let's increase the number of iterations here so let us say we want to apply the erosion two times on the same image and I am going to just run this code once again and you can see now these balls are eroded more let's say we want to increase this to 5 times and then run the code and you can see now these balls are really small because we have applied this erosion multiple number of times so let's say this is one once again and let's make this size of our kernel small two by two rectangle size right so you can see now our result is better because all the spots from these balls are gone and these balls are not so much eroded now there are two more morphological transformation methods which are called opening and closing so we are going to first of all see how opening works I'm going to Define a variable called opening and then I will call CV2 2 dot morphology X okay and then we will provide the source which is mask the second method is the type of morphological operation which we want to perform so in this we are going to just call CV2 Dot and then we can specify which type of morphological operation we want to perform on the image so just write morph and then the type of operation so we want to perform the morph open for the opening right and then the third argument here is the kernel which we have defined and now we are going to just add this opening to our matplotlab window let's add this and then let's do 5 here and then let's say our matplotlib is going to show these images in a two by three format okay so let's run this code and let's see what happens and let me increase the size of this image now and this is the result of the opening so what is opening in morphological transformations so opening is just another name of erosion followed by dilation so when you perform this opening morphological operation first of all erosion is performed on the image and then the dilation will be performed on the image so you can see the effect of the erosion followed by the dilation still you see some spots here which can go if you can just increase the size of this block so let us rerun the code let's see what happens so now this image somehow looks better than the older image so opening is the erosion followed by dilation now there is a closing method also which is just the opposite of opening in the closing morphological transformation dilation is performed first on the image and then it is followed by the erosion so let's see if we get the better result when we perform the closing morphological operations and the morphological operation here will be close and run this code and now you can see the result here so in closing as I said first of all the dilation is applied and then the erosion is applied in the opening first of all erosion is applied and then the dilation will be applied now there are different type of morphological operations you can apply using this morphology X so for example I am going to just use some of them so the main morphological operations other than opening and closing is let's say morphological gradient so I am going to just say m g for morphological gradient and you just need to change the second argument here so CV2 morph underscore morphological gradient so we are going to just call this morph gradient and it is going to apply the morphological gradient and then the next is the top hat and the black hat so there are different ah morphological techniques you can apply so I'm going to show you one more and then I will leave you with the other techniques so th for top hat and here also the second argument you just need to change it to Top Hat right otherwise you can see there are so many number of techniques you can apply on your image so there is gradient close open we have already seen Black Hat Cross dilate ellipse erode hit Miss rect and then top hat which is ah we are going to use right now right and then we can just add these two things to our list of titles and list of images so mg and then we have th for top hat and now we have eight images so range is increased to 8 and let's say we just want to show them in two by four Matrix here in the matplotlib window so you can see this is the result of morphological gradient so morphological gradient is the difference between the dilation and erosion of an image and this is the result of top hat that means it is the difference between the image and the opening of an image so this is how you can perform some of the morphological operations on the images now I will show you one more example I have a image called J dot PNG so I am going to just load this image also and because this J dot PNG is already a binary image I don't need to apply this mask here so instead of this mask I can just directly use our image variable so I'm going to just write this and let's load this image two times because we already have defined this mask variable inside our title list and image list and now I'm going to just run this code so the original image of this Js a DOT PNG looks like this and after we applied the dilation you can see the dilation increases the area of this J the erosion just erodes away the corners of this J right opening is going to apply the erosion first followed by the dilation and closing is going to first of all perform the dilation followed by the erosion this morphological gradient is going to give you the difference between the dilation and erosion of the image so it is going to give you this kind of result and you can see the top hat result here which is the difference between the input image and the opening of the image so this is how you can use different type of morphological Transformations on your images in this video we will discuss about smoothing or blurring images in opencv so smoothing which is also known as blurring is one of the most commonly used operation in image processing the most common use of smoothing operation is to remove noise in the images now when smoothing or blurring images we can use diverse linear filters because linear filters are easy to achieve and are also relatively fast now there are various kinds of filters available in opencv for example homogeneous gaussian median or bilateral filters which we will see one by one so first of all we will see the homogeneous filter so homogeneous filter is the most simple filter and in homogeneous filter each output pixel is the mean of its kernels neighbors now in homogeneous filter all pixels contribute with the equal weight and that is why they are called homogeneous filters now those of you who don't know what the kernel is I have explained about kernel in the last video so you can see the last video and in simple word a kernel is a shape which we can apply or convolve over an image and you can use for example numpy to create this kind of squared kernel so in homogeneous filter the kernel looks like this image which you see on your screen so in homogeneous filter kernel K is equal to 1 by the width of the kernel multiplied by the height of the kernel so let us say we want to use a kernel of five by five then using this formula we will have K is equal to 1 by 25 and then we will have our kernel Matrix of 5 by 5 once so let's create this kernel first of all and then we will see how to use this kernel for the image filtering using 2D column Evolution or homogeneous filter so what I have right now here is the simple code which loads this image using matplotlib and this code you might already know because I have explained in detail how matplotlib works and how to read the images using opencv one thing to note here is I am just converting the image from BGR to RGB because matplotlib reads the images in the RGB format and opencv reads the images in the BGR format so this conversion is necessary so let's define our kernel so I am going to just say kernel is equal to NP dot once and then we are going to take the kernel of five by five so we are going to Define this kernel 5 comma 5 of once so I am going to just say NP Dot not float 32 here and then we are going to divide this kernel by 25 because our kernel is of 5 by 5 because the formula which we have seen in that formula we have the kernel which was a matrix of ones and then we have the multiplication of 1 divided by the width and height of the kernel so that's why the multiplication of the width and height is 25 that's why I have taken 25 here so now we have our kernel so we can Define our destination image using this kernel and we are going to use CV2 dot there is a method called filter 2D which we are going to use which is used for this homogeneous filter so here the first argument is the image the second argument is the desired depth of the destination image so for now we are going to take it as 1 the third argument is the kernel so now when we have applied this kernel on our image using 2D filter let's see what the output will look like so I will name this image as 2D convolution and the destination is the final image which we got using filter 2D and let us increase this range by 2 and let us say we want to show this image on matplot live in one by two format okay so I am going to just run this image so this is the result on the left hand side is the original image and on the right hand side is the 2D filter applied image so this is the image which we got by applying the homogeneous filter using filter 2D function so you can see on the corners here there is a little bit noise and after applying this 2D convolution over this image you can see all the corners are now smoothened and overall this image is now smoothened or blurred a little bit so these noises are removed or suppressed by this blurring so this is one way of blurring an image using filter 2D right filter to the function now as in one dimensional signals images also can be filtered with various low pass filters or high pass filters Etc so low pass filter helps in removing the noise or blurring the image Etc and high pass filters helps in finding ages in the images now when you want to achieve image blurring we need to convolve over the image with the low pass filter kernel now there are some algorithm as I said there are various kind of algorithm available in opencv so we will also see them one by one so first algorithm is the blur method or it's also called the averaging so what I'm going to do is I'm going to Define a variable called blur and then I'm going to call a method called CV2 dot blur okay so this is the method which we will use to apply averaging algorithm for blurring the image and this takes two argument one is the image and second is the kernel so the kernel we are going to apply is once again five by five and now we are going to just see with the result of this blurring method so we are going to just loaded using the matplotlib so range I'm going to increase it by 1 once again and let's see this these three images in one by three format on the matplotlib window so this is the result and you can see the original image the result which we got using the filter to the method and the result we got using the blur method which is also called averaging so the result is more or less looks the same to me because we have applied the same kind of Kernel to both the functions so this is the result of filter 2D function and this is the result of the blur function now there are more functions which are available in opencv so let's see them so the next algorithm which we are going to see is the gaussian filter algorithm so the gaussian filter is nothing but using different weight kernel in both X and Y Direction so in the result pixels located in the middle of the kernel have the higher weight or bigger weight and the weights decreases with distance from the neighborhood center so pixels located on the side have smaller weight and the pixels located on the center have the higher weight so when we take a five by five kernel its result is going to look like this which is shown in the image and now let's see how we can use this gaussian blur in our opencv code so I'm going to remove this semicolon which I somehow added here and let's declare a variable called G blur for gaussian blur and then we are going to use CV2 dot gaussian blur so the method name is gaussian blur and the argument here are same as the blur method so first argument is the images cell second argument is our kernel we are going to take the same kernel of five by five and the third argument here is the sigma x value which we are going to take 0 for now let's see the result of the gaussian blur method when it's applied to an image so I'm going to just Define one more title which is G blur or gaussian blur or let us take this name which will be more clear and then our result image is G blur and let's increase the range to 4 and let us say we want to show this image in two by two format so two rows and two columns so I am going to run this code and for opencv logo the results looks the same you can see for the 2D convolution of filter 2D method or blur method using the gaussian blur you can see there is a little bit different between the blur method and gaussian blur method results the gaussian blur result is more better in my eyes than the blur method let's try this gaussian blur method with another image so I have this image called half tone underscore gaussian underscore blur and I'm going to run this code now with the new image and you can see the result now so this was the original image which have too much noise here so you can see ah the pixels here which have too much noise and after applying the gaussian blur you can see this eye image in much better way and all the noise is removed so the gaussian blur method is designed specifically for removing the high frequency noise from the image like this one now let's see the next method which is called the median filter so median filter is something that replaces each pixel value with the median of its neighboring pixel so this method is great when dealing with something which is called salt and pepper noise now if you don't know what the salt and pepper noise is you can open the Wikipedia and under this URL or just search for salt and pepper noise Wikipedia page and you can see ah more details about salt and pepper noise so you can see this is an image and there are some pixels which are distorted here so there are some pixels where the white dots are there or white noise is there and there are some places where the black noise is there so that's why it's called salt and pepper because we have white pixels which are distorted like salt and we have the black pixels which are which looks like pepper so that's why it's called salt and pepper uh noise so I have this same image which I'm going to use as a source now so it's called water dot PNG in my case and now let's see how we can use the median blur method so I'm going to just Define a new variable called median and then I'm going to use CV2 dot median blur method so this method is called median blur where the first argument is the image and the second argument here is the kernel size now one thing to note here is that the kernel size must be odd here so ah this must be a three or five or seven and so on except one okay so when you just give one it is going to show you the original image and let's say we just give three here as the kernel size or in our case we have the kernel size of five so let's take the five kernel size here so let's just show this result of the median filter in the matplotlab window so I am going to just increase the range 5 and let's say this is two by three Matrix now I'm going to run this code and now you can see the results of all the filtering method and you can see the best result you get using the median filter method so when you have this kind of salt and pepper dots on your images then you can use the median filter now let's see the last filter which is called the bilateral filter so by using all these filters for example homogeneous filter or averaging or the gaussian or the median filter we not only dissolve the noise but we also smooth the edges and sometimes we need to preserve the edges that means we need that all the ages must ah remain sharper even if the image is blurred so let me show you one example so I have this Lena dot PNG image so I'm going to Define ah variable called bilateral filter and then there is a method called CV2 dot bilateral filter and this bilateral filter takes the first argument which is the image the second argument is the diameter of each pixel neighborhood that is used during the filter so let us take it as a 9 the third argument is the sigma color and the fourth argument is the sigma space so the spig sigma color is the filter Sigma in the color space and sigma space is the filter Sigma in the coordinate space so for this we are going to take this filter Sigma color and sigma space as 75 and 75 here and let's see it in the result window so bilateral filter and then the result bilateral filter and this gives me error because this image is called Lana dot jpg dot p n g so j p g and then we need to increase this range by 1 to see all the six images and let us run this code and let's see what happens so you can see the result now so let me make it little bit bigger so you can see them and from here also so now you can see by applying the bilateral filter the edges are preserved in a much better way so here you can see the Hat border is blurred but here you can see in the result the border of the head are preserved so the images in which you need to preserve the borders then you can use the bilateral filter so bilateral filter is highly effective in noise removal while keeping the edge sharp so these are some of the methods and algorithms you can use to smoothen or blur your images using opencv in this video we will talk about image gradients in opencv so first of all what is an image gradient so an image gradient is a directional change in the intensity or the color inside the image now the image gradient of an image is one of the fundamental building blocks in image processing for example we use image gradients inside the image to find the edges inside an image now there are several image gradient methods available in opencv and we are going to see three of them first is the laplacian derivatives second is the Sobel X method and third one will be the Sobel y methods and all these methods which I mentioned are different gradient functions which uses different mathematical operations to produce the required image so the laplacian calculates the laplacian derivatives whereas Sobel method is joint gaussian and differentiation operations but don't be overwhelmed with the details you just need to keep in mind that these are just the functions which we use for finding out the gradients of an image to analyze the image so let's use the first method which is called the laplacian gradient now to start with I have this initial code and you might already know what this code is doing so first of all I'm just reading this image messy Five Dot jpg in the grayscale mode using the IM read method and then I'm just loading this image using the matplotlib window so let's first see how the result looks like so this is going to look like this this is just a normal image of messy and let's see how we can apply the laplacian method to find out the laplacian gradient of an image so for that we are going to declare a variable called lap and then there is a function available inside your CV2 Library which is called laplacian and this laplacian method takes few argument first argument is the image the second argument here will be the data type which we are going to use which is called CV2 Dot CV underscore 64 F so CV2 dot CV underscore 64 f is just a data type and we are using a 64bit float due to the negative slope induced by transforming the image from white to black so you just need to keep in mind that this is just a data type which is 64bit float and it supports the negative numbers which we will be dealing with when the laplacian method is run on our image now in the next line what we are going to do is we are going to take the absolute value of our laplacian image transformation and we are going to convert this value back to the unsigned 8bit integer which is suitable for our output so I am going to you just write lap and then using the numpy uint methods and NP dot U int 8 and as an argument we are going to pass NP dot absolute and then inside the absolute method we are going to just pass our image which is going to give us the absolute value of our LaPlace in image transformation which is going to convert this into the unsigned 8bit integer now let us see the result of this laplacian gradient so I am going to just add a new title to my title array which is called laplacian and also inside the images list I am going to add this lap variable which contains this image right after the laplacian gradient is applied here and here the range will be 2 and we are going to see it in one by two format on the mat plot lib window so here you can see the original image which is this one and after the laplacian gradient method is applied on this image you can see all the edges which are detected by this method when we applied this method on this messy Five Dot jpg image and an image gradient as I said is the directional change in the intensity or the color in an image so let us close this window and there is one more argument you can provide here which is the kernel size so you can just say k size is equal to 5 this is the kernel size and I am going to just run this program once again and you can see the kernel size is increased but our result is deteriorated right so let us reduce it to 3 and then once again run this program and the result looks fine and if you apply K size is equal to 1 let's see the result and you can see you get the better result I think so for now I'm going to just use case size is equal to 3 and now let's use the other two image gradient methods which are so Bill X and Sobel y so these methods which are called Sobel X and Sobel y are also called Sobel gradient representation so let's just use them and then we will discuss how they are useful so first of all I'm going to declare a variable called Sobel X and then I'm going to use the method inside this CB2 Library which is called so Bell so this is the method which takes again few arguments first is the image second is again this data type which is CV2 dot CV underscore 64 and the third argument here will be the DX so when you write 1 here this value can be one or zero so when you write 1 here that means we want to use the Sobel X method okay and then the fourth argument here is the d y value okay so this is DX which is for the X Direction and this is for the d y which is for the y direction and DX stands for the order of derivative X and the d y stands for order of derivative y now once again we are going to declare the sobell Y variable so let's declare the Sobel Y and then C V 2 dot so Bell and this also takes a few arguments here the difference will be only the third and fourth argument so I'm going to just use the second argument same the third argument will be 0 for so Bill Y and the fourth argument will be one right so this is the order of derivative X if it's 1 this is called the order of derivative which is in the X Direction and in the second case it is in the y direction and the fifth argument here can be the K size as we have seen in the laplacian method so if you want you can provide the kernel size also here as the fifth argument but we are going to skip it for now now again we are going to convert these values into the unsigned int as we have done in the case of laplacian so what we are going to do is we are going to once again overwrite this variable so we'll X and then we are going to use NP dot u in 8 and in the parenthesis we are going to just write NP dot absolute and then we are going to just pass the value inside the Sobel X variable same we are going to do with the Sobel y variable and now let's see the result how the result looks like so I'm going to just add these elements inside the title and the image list so let's add Sobel X and so we'll y here and here also so so Bill Y and now let's increase the range to 4 and let's see it in the form of 2 by 2 Matrix on the matplotlab window so I'm going to just run this code and you can see the result here so original image laplacian gradient and then Sobel X and so blue y so you can see when you apply the Sobel X gradient method the direction or the change in Direction in the intensity is in the X Direction and when you apply the Sobel y method the change in Direction in the intensity is in the y direction so this is like horizontal and this is in the vertical Direction I have one more image which will illustrate this so we'll X and so y gradient method in a better way I think and this is called Sudoku so I'm going to just write Sudoku dot PNG file file and hopefully I didn't do any mistake in the naming yes it works so you can see ah the laplacian result here and then Sobel X and so y result here so in the Sobel X you can see more vertical lines so because so will Y is good for the directional change in the vertical Direction so you can see more change in intensity in the vertical Direction and using the Sobel y you can see the directional change in the intensity in the horizontal direction or the yaxis you can also combine the result of Sobel X and so Bill Y images and how we can do this let's see so to combine these two result I am going to just create one more method which is called Sobel combined is equal to CV2 dot we are going to use the bitwise or operator in order to merge these two images so we are going to just write bitwise or and then we are going to provide the two sources one is Sobel X and the other is the syllable Y image so this is going to give us the bitwise or result of these two images and then we are going to just add this into the title list so let's say so we'll combined and also in the image list so like this and let's just increase the range to 5 and let's see it in the form of ah 2 by 3 on matplotlib so I'm going to just run this once again and you can see the result now so this here is the combination of Sobel X and Sobel y method and you can see now you can see the directional change in the vertical as well as in the horizontal Direction because this is the combination of Sobe Y and Sobel X images so this is how you can use the image gradients inside opencv in this video we will talk about Kenny Edge detector in opencv so first of all what is Kenny Edge detector so the Kenny Edge detector is an edge detection operator that uses multistage algorithm to detect a wide range of edges in images now this Kenny Edge detector was written and developed by John F Kenny in 1986 the that's why it's named after his name which is Kenny as detector now the process of Kenny Edge detection algorithm can be broken down in five different steps the first step is to apply gaussian filter to smooth the image in order to remove the noise the second step is to find the intensity gradients of the image the third step is to apply the non maximum suppression to get rid of spurious response to Edge detection the fourth step is to apply double threshold to determine the potential edges and the fifth step is to track edges by hysteresis that is to finalize the detection of the edges by suppressing all the other edges that are weak or not connected to strong edges so this seems little bit complicated but in opencv it's really simple to use so there is a builtin function in opencv which is called Kenny and we are going to use this function so to start with I have this sample code which loads this image which is called messy.jpg using the matplotlib library I'm going to just run this to show you the result so this is the image and we want to detect the edges of this image so what we are going to do is we are going to first of all declare a variable called Kenny and then there is a method as I already said inside your CV2 Library which is called Kenny method which takes few arguments so the first argument here is the image source itself the second argument and the third argument as you can see is the first threshold value and the second threshold value so this first threshold value and the second threshold value you need to provide for the hysteresis procedure so there is the last step as I I mentioned and in that step hysteresis take place and for that procedure we need to provide the values of the threshold one and the threshold two so for now I am going to provide 100 as the threshold 1 and 200 as the threshold 2 but later you might want to add a track bar in order to see the changes in the edges when you just move the track bar from left to right for the threshold one and the threshold two so this might be a small assignment for you you can just add the track bar and see how the edge detection changes when you change the value of threshold 1 and threshold 2 and I have already explained how you can use track bars with opencv so just watch that video and you will be good to go so now we have the result of Kenny Edge detection function so we are going to just add it to our list first to the list of titles and then second to the list of images and the range we are going to increase it to ah 2 and this we are going to just see the images in one by two format so I am going to just run this python script and see the result so you can see we have the original image here which we have loaded in the grayscale and on the right hand side you can see the result of the Kenny Edge detection methods so you can see all the edges which are available here on this messy Five Dot jpg image you can use this on the other images also so for example I have the Lana dot jpg image let's see the result of that and this is the result of the Kenny Edge detection method on this Lana dot jpg method so this Kenny Edge detection is really useful because in the last video we have seen how to find out the image gradients and let's see in comparison to those image gradient methods how Kenny Edge detection method performs so these are all the methods I have explained in the last video laplacian Sobel X and so Bill Y and I have shown you how to combine the result of Sobel X and Sobel Y and additionally I have added this line to the previous code which I have shown you in the last video which is Edges is equal to CB2 dot canny which gives us the result on the same image ah using the Kenny Edge detection method and I have added it to the title and the image right so let us run this script once again and let's see the differences in the result using all these methods so you can see all the six results this is the original image this is the result of the laplacian method this is the result of Sobel X and this is the result of so Bill Y and this is the combination of Sobel X and Y and you can see Kenny Edge detection gives us the result which contains lesser noises so you can see there is a lot of noise present in the other matters you can see here all the noise is present which is removed using Kenny Edge detection or in the laplacian method also you can see some noises around but in the Kenny detection method you can see you get the proper edges and more precise edges without any noise so this is the benefit of using Kenny Edge detection so this is how you can use Kenny Edge detection in this video we will discuss about image pyramids in opencv so till now normally when we have used images we have used the images of constant size but sometimes we need to work with the images of different resolution so for example if I have an image and I want to search the face inside an image this face can be of different sizes so using image pyramids we just create the images of different resolutions and then we search for the object for example face in all of these images so pyramid or pyramid representation is a type of multiscale signal representation in which a signal or an image is subject to repeated smoothing and sub sampling so a normal pyramid when you create a pyramid of images it will look like this so let's say this is the original image at the bottom then when you down scale an image using a pyramid function it is going to give you this image which have the half resolution than the original image and then when you further go up it's going to give you the 1 4 of the original image and then so on so 1 8 or 1 16 of an image now there are two types of ah image pyramids which are available in opencv first is called gaussian pyramid and second is called laplacian pyramid so first we will discuss about the gaussian pyramid so gaussian pyramid is nothing but repeated filtering and sub sampling of an image now there are two functions available for the gaussian pyramid which is called pair down and pair up so let us see them one by one so I have this sample code which is just reading an image and then showing it using the I am show method now in order to use this pair down function you can just Define a variable let us say l r for lower resolution and then you can use CV2 dot pair down so there are two functions you can see pair down and pair up so first of all we will see pair down and then we are going to pass our image as an argument here so I am going to just pass our image as an argument and we are already showing the original image and let's show the image after we have reduced the resolution of this image using the pair down method so pair down is going to reduce the resolution of an image so I am going to just use LR here and let us say this is the pair down one image okay so let's run this code and let's see what happens so you can see this is the original image and this is about you can see one fourth of this original image right so this pair down method is going to reduce the resolution of an image when you apply the same method on the second image so let's say this is l r 1 and then we create a second variable error LR2 and when we pass l r 1 as an argument for this method to create the LR2 to image then let's see what happens so this will be lr1 and let us just say this is going to give us LR2 the resolution of image will reduce further so let's see what happens so this was the original image this was ah the image which we got from the first pair down method and then we get this image which we which is further reduced in resolution so this is the image after applying the pair down method second time on the lr1 image okay so you can see the resolution of image is reducing and its creating a kind of pyramid and that is why it is called the image pyramid now there is a method called pair up also available in opencv so let's see what this pair up method do so as you can expect it is going to increase the resolution of the image so here I am going to just say h r for higher resolution and then I am going to just say c v 2 dot pair up okay and it's going to increase the resolution of an image now let's say we want to increase the resolution of an of this image which is the smallest image which we got using the pair down method right so we are going to apply the pair up on the last image which we got using the pair down method and let's see what happens so when I am going to use this hr2 here and this we got from pair up method and let us say this is the pair up one and I am going to just run this code and you you are going to see that we have converted this image which was the smallest image to a higher resolution which resulted in this image but when you see this image carefully so let me just move this ah to this side and this was the original image so let me just minimize this so this image we have converted to this image using the pair up method so ideally this image should look like this but you have to remember that this pair up image is not going to be equal to this image because once you decrease the resolution using the pair down method you lose the information about that image so when you use pair up to just increase the resolution of this image then you can see the result looks little bit blurred because some of the information is loosed using the pair down method so you have to keep this in mind that when you want to increase the resolution after you have reduced the resolution you are not going to get the same result as you might expect that this image should look like this but they are not equal images so this image is just a higher resolution of this image and it has nothing to do with this image so these are the two methods which are available in gaussian pyramid now if you want to create a pyramid of multiple resolution instead of just using this pair up or pair down method repeatedly what you can do here is I'm going to just remove this and remove this code also so what I'm going to do is I'm going to copy the image into a new variable so I am going to just say layer is equal to i m g dot copy there is a method available for copying the image which is a copy and then I'm going to create the gaussian pyramid array okay so I'm going to just create a variable called GP for gaussian pyramid is equal to 10 in square bracket I am going to just pass this image here as the first element of this list then what I can do is I can use a for Loop instead of just rewriting this pair down method again and again and you might already know how to use for Loop in Python so for I in range and here we can provide any ah range so let us say we want to create five ah image pyramid ok so five time we want to reduce the resolution so we are going to give 6 here because range goes ah the number minus one so whatever you give here minus 1 right so now what we are going to do is we are going to just use our layer parameter once again and then we are going to just call CV2 dot pair down method so pair down and then we want to just ah say layer okay and then we want to append to the gaussian pyramid list okay so we are going to just say GP dot append and we are going to append the result of this pair down to our list which we have created here okay so this is going to just append this image to our list of images and then let's just show this image using CB2 dot I am show method so CV 2 dot I am show and here we can just say s t r for converting the integer to the string because the first parameter you give to I am show is a string parameter that's why I am converting the integer to the string and the second parameter is the image so let's pass this layer here okay so you have the original image which will be shown using this line of code and then you will see multiple number of images of different resolution using this code so let's run this code and let's see what happens so I am going to run this code and you can see there are different images ah resulted using that code which we have written so this was the first image which is 0 and then this is the second image and then this is the third fourth fifth and sixth so sixth you can see have a very small resolution so this is how you can use pair down method multiple times using a for Loop now what are laplacian pyramids so laplacian pyramids are formed from the gaussian pyramids there is no exclusive function for creating the laplacian pyramid so as you have seen that in gaussian pyramids there are two methods available pair up and pair down but there is no exclusive function for creating the laplacian pyramid so how we can create a laplacian pyramid if there is no function available for creating them so you can create a laplacian pyramid or a level of laplacian pyramid is formed by the difference between that level in the gaussian pyramid and the extended version of its upper level in the gaussian pyramid so this definition might be confusing to you guys so let me explain you with the code what I mean by this definition so what I'm going to do is first of all I'm going to take the top level layer of the gaussian pyramid so top level layer of the gaussian pyramid is the last image which is generated using this for Loop so let us say we have six images or five images using this for Loop so what we are going to do is because we have appended each and every image to this list right so we have all the images inside this list so we can just get the last image using the indexing so again I'm going to use the layer variable and then I'm going to just say GP for gaussian Pyramid list and then there is the index 5 because last image will be available at the index 5 of this list so we get the last image of ah that gaussian pyramid and then let's show this image so I am going to just say CV2 dot I am show and this is the upper level or the last image so I'm going to say upper level gaussian pyramid and then we are going to pass this layer variable here so this is going to show just the last image of this list and let us just comment this code out because we don't want to see all the images and then I'm going to create a new list for laplacian Pyramid so I'm going to just say LP for laplacian pyramid and then I'm going to create a list using the layer variable itself as we have done for the gaussian pyramid list also so the first element here is the layer variable itself and now we are going to use the for Loop and then I in range and this time what we are going to do is you might already know how to use the range function and if you don't know you can see you can give the stop integer here or you can give a multiple parameters here so you can see there is one more implementation of this range function so you can give the start parameter and the stop range so start is the starting point stop is the stopping point and also you can give the steps so this step means ah in what number you want to reduce ok so let us say we want to start from 5 and then we want to go until 0 and we want to reduce in the step of minus 1 okay so five four three two one so let's print the value of I first of all if you uh might be interested in the result of this range function then let's just run this code and let's see what happens so this is the images which we get but we are not interested in these images we are interested in the print function output so you can see the output of this print function code is 54321 as I said the lower limit is not reached so if you give 0 here then it's going to go until 1 and not 0 if you give 6 here then it's going to go until 5 not 6. so let me repeat the definition of laplacian pyramid once again so laplacian pyramid is formed by the difference between that level in the gaussian pyramid and the extended version of its upper level in the gaussian pyramid so let us first create the extended version of that level so we are going to just create a variable called gaussian extend or extended and then we are going to extend the level of that image which are there in the gaussian pyramid list by using C V 2 dot dot pair up method and here what you need to give is the gaussian pyramid list and then we just need to get the index I from this so this line gives us the extended version of the upper level in gaussian pyramid now let's create the laplacian Prima pyramid so laplacian is equal to CV2 dot subtract because we want to find out the difference between that level in the gaussian pyramid and the extended version of its upper level so I'm going to just say GP for gaussian pyramid and then we are going to just say I minus 1 as the first parameter and the second parameter is the extended version of the gaussian upper limit and now we can use the I am show method to show all these laplacian images so I'm going to just say CV2 dot I am show and once again I'm going to use Str function to convert from a number to string and then in the next parameter I'm going to just pass the laplacian parameter here as an image source so what do you think will this code work so let's see what happens when we are going to run this code so you can see the laplacian pyramid looks just like the edge detection so all the edges are shown here on every image this is the first level this is the second level third level fourth fifth level so these images are called the laplacian pyramid now what is the use of creating those laplacian pyramids or the gaussian pyramids so the laplacian and gaussian pyramid helps us to blend the images and the Reconstruction of the images so these are the two benefits of creating those laplacian and the gaussian pyramids so in the next video we are going to see how we can blend the images or how can we reconstruct the images using the opencv and the image pyramids in the last video we have seen what are image pyramids and I have told you there are two kinds of image pyramids in opencv one is called the gaussian pyramid and the other is called the laplacian pyramid and we have seen in the last video how we can create the gaussian pyramid and the laplacian pyramid now in the last video I have also told you some applications of image pyramids and one of the application of image pyramids is the image blending so let me show you one example so here in this code I have two images one is of apple and other is of orange and I want to blend or merge these two images so let me just run this code first of all so you can see there are two images first is of apple and other is of orange and I have also printed the shape of these two images so you can see the shape is similar 512 by 512 and orange image shape is also 512 by 512. so here what I want to do is I want to blend half of the orange to half of the Apple so let's say I want to just blend right hand side of this orange to the left hand half of this Apple so how can I achieve this now you might say that I can just cut these two images into half and then I can stack these two images side by side and I will get the half and half of the two images and that's how I can just get the result so let's first of all try this technique first of all we are going to just create the half and half of the apple and orange images and we are going to just stack these images side by side so let's say I'm going to create the variable called Apple underscore orange and then here there is a method in numpy so I'm going to just say numpy dot h stack so there is this method called H stack and here what I can do is in the form of Tuple I can provide the half of my Apple image so apple is the image variable name and then what I am going to do is the half of this image because this image is 512 by 512 so I am going to just give this kind of expression colon comma and then colon 256 which is the half of the Apple image on the left hand side right and then I am going to just do the same with the orange image so I'm going to just take orange and then colon comma 256 colon so one thing to observe here is I have taken colon before 256 in the Apple image and I have taken colon after 256 in the orange image and then I am going to just show this apple orange image and let's see what result we get when we run our code so these two are the apple and orange image and this is the result of adding the two halves of the orange and the Apple image but still you can see this line which is clearly visible and from this line you can say half of this is orange and half of this is an apple so in image blending what we need to do is we need to blend this line also so the orange and the Apple image should be merged or Blended in a perfect way so for blending this half apple and half orange image what we can do is we can use the image pyramid techniques to blend these two images now in order to blend two images using image pyramids Technique we need to follow five steps the first step is to load two images in our case these images are of apple and orange which we are already doing so first step is to load these two images the second step is to find out the gaussian Pyramid of our apple and orange image the third step will be from these gaussian pyramids find out the laplacian pyramids ok so we will find out the gaussian pyramid in the second step and then in the third step we are going to find out the laplacian pyramids now in the first step we are going to join the left half of the apple and the right half of the orange in each levels of laplacian pyramid and finally in the fifth step what we are going to do is we are going to just join these image pyramids and reconstruct the original image so let us follow these steps one by one and let's see what result we get so as I said first step is already done which is just loading these two images and the Second Step would be to find out the gaussian pyramid so let me just ah just write this step generate gaussian pyramid for Apple first of all and then we are going to find out the gaussian Pyramid of the orange so first of all what I am going to do is I am going to just copy the Apple image so I'm going to just say apple underscore copy is equal to Apple dot copy so there is a method called copy which you can use to copy the this image so from this copy what we are going to do is we are going to generate the gaussian pyramid so I am going to once again name my variable as GP let's say underscore apple and then we are going to just pass our image which we have copied in the form of list so I am going to just say apple copy here so these steps we have already seen in the last video how to create the gaussian pyramid and the laplacian Pyramid of an image so I am not going to explain this in detail if you want ah the detailed explanation you can see the last video next what I am going to do is I'm going to create a for Loop and I'm going to just say for I in our range so I'm going to use the range function and we are going to use the six levels in this example so I am going to provide the range up to 6 and then what we are going to do is we are going to just say apple copy or you might have named this variable as Apple layer also because we are just creating multiple layer of the Apple image for the gaussian pyramid right and then we are going to use the CV2 dot pair down method to create the gaussian pyramid okay this we have already seen in the last video and now as an argument we are going to pass our Apple copy a variable here and in The Next Step what we are going to do is we are going to just append to our GP underscore Apple variable which is our gaussian pyramid for the Apple image and then we are going to just append this apple copy after we have applied pair down method on the same image so this is just giving us multiple layer of the Apple image right the same method we are going to apply for the orange also so I'm going to just copy this code and then I'm going to just paste this code once again and this time this will be for orange and I am going to just say this is the orange copy and we are going to copy from the orange image and then we are going to just generate the gaussian pyramid for the orange image and this will be passed here and also here and also here and also here and this gaussian pyramid orange will be passed here okay so we have generated the gaussian pyramid for the apple and the orange now we are going to generate the laplacian pyramid for apple and orange so this also we have seen in the last video so I'm going to just comment generate laplacian pyramid for Apple first of all and to find out the laplacian pyramid for the Apple what we are going to do is we are going to once again take our Apple copy and then using our gaussian pyramid so let us take gaussian pyramid for the apple and we are going to use the fifth element of this list so what we have learned in the last video how we can find out the laplacian pyramid a level in the laplacian pyramid is formed by the difference between the level in the gaussian pyramid and extended version of its upper level in the gaussian pyramid so this difference we are going to find out in this step so I am going to just say this is LP for the Apple which stands for laplacian Pyramid for the Apple is equal to in the list we are going to just pass the Apple copy and then we are going to use the for Loop so for I in the range so we are going to take the range and in the last video I have shown you how to take the range for the LaPlace in pyramid so we want to go from 5 until 0 in the steps of minus 1 and then in the next step we are going to create the gaussian extended variables gaussian extended is equal to CV 2 dot pair of this time we are going to use the pair up method and then we are going to pass our GP Apple which is gaussian Pyramid for apple and then the index here will be I in the next step we are going to create the laplacian variables is equal to CV2 dot subtract so there is a method in CV2 which is called subtract and then we are going to take our gaussian pyramid for the Apple so GP apple and the index here will be I minus 1 and the second argument for this subtract method will be our gaussian extended variable so we are going to just pass this gaussian extended variable and in the next step we are going to just append to our laplacian pyramid for the Apple so LP underscore Apple dot append and we are going to just append this laplacian variable to the laplacian pyramid for the Apple same we will do for the orange image also so we are going to generate the laplacian pyramid for the orange orange here and this will be the core p of the orange copy here and here also and then this will be the GP Orange right this also will be GP orange this also will be g b orange and here instead of LP Apple we are going to just say LP orange and then we are going to just pass this variable here also so now we have finished three steps one is to load both the images second is to find out or generate the gaussian pyramid and the third step is to generate the laplacian pyramid for both the images now the fourth step is to just join the half of these two images so what I'm going to do is now I'm going to just create one more variable which will be apple underscore orange underscore let's say pyramid is equal to and also we are going to create a variable called n and we are going to see ah later how to use this variable and then we are going to use the for Loop and then we are going to create two variables one for the first image so I'm going to just say Apple and then lap comma orange lab okay so these two variables I am creating just same as this I in this for Loop in zip so there is a method zip which we can use to zip the LaPlace in pyramid one which is for the apple and for the orange also so I am going to just say LP for Apple comma LP for the orange and inside this for Loop first of all we are going to just increment the value of n by 1 each time so n plus equals 1 and in the next step we are going to find out the shape of the Apple image so the Apple image shape gives us three values first is columns so I am going to just say CEO LS for columns then rows and then the third value is the number of channels and then we are going to just say apple lab dot shape in the next step we are going to just create a variable called laplacian and we are going to just join the two halves of these two images which we are getting inside the variable Apple lab and orange lab so we are basically doing this step after applying the gaussian pyramid and the LaPlace in pyramid on both the images so NP dot h take method we are going to apply in this step so I am going to just write NP dot h stack and then as an argument what we are going to do is we in the form of Tuple first of all we are going to take our Apple lamp variable which is this one and in the square bracket we are going to just write colon comma 0 comma int so we are going to just type cast the number of columns in the apple shape so this we got from the shape of the Apple index and then divided by 2 so we are going to just dividing the columns into half and same we will do for the orange lab so we are going to just say orange lap in the square bracket colon comma int and then once again in the parenthesis we are going to just say calls for the number of columns divided by 2 and then Olin as we have done in this step also and at last we are going to just append this laplacian variable to this list which we have created so Apple underscore orange underscore pyramid dot append and then we are going to pass the laplacian variable here now the last and the final step is to reconstruct our image so let's reconstruct our image so now what we are going to do is we are going to once again create a variable called Apple orange underscore reconstruct is equal to this will be the first index of our Apple orange pyramid so I'm going to just say apple orange underscore pyramid and this will be the zeroth index and once again we are going to use the follow so for I in the range so we are going to go from 1 until 6 and the default step is of 1 so we don't need to give the third argument and inside the for Loop we are going to just take this variable once again and then we are going to apply the pair up method on this so CV2 dot pair up and as an argument we are going to pass the same variable so we are going to just apply the pair up on this apple orange reconstruct from the zeroth index of the pyramid up to the sixth level and the last step will be to add all the layers so Apple orange reconstructed once again or reconstruct is equal to CV2 dot add so this is also one method which is called add and here we are adding Apple orange pyramid and the reconstructed Apple orange image okay so this is this variable which we got by just adding the left and right halves of each level and then we are just reconstructing this image using the pair up method and thus just adding the pyramid level so this should be I think the index I right we cannot just add the list to the image directly okay so this will be at each layer we are just reconstructing and adding it to the image which we got by just addition of this half of the images now in the end let's try to just load this reconstructed Apple orange image in the I am show window and let's hope it works I haven't checked it yet so I'm not sure it will work or not and you can see it's working in the first go so that's a good thing so you can see the difference so this result we got by just stacking this apple and orange image side by side but this line is clearly visible but when we applied the gaussian pyramid and the laplacian pyramid technique for blending the images then you can see this line is perfectly Blended and this line is not any more visible so this is the perfect blending of the orange and the Apple image so this is how you can use the laplacian and gaussian pyramids to reconstruct and blend two images together and result is in front of you so you can see how it can blend two images so perfectly so this is how you can blend images using image pyramid technique in this video we are going to understand what Contours are and we are going to see how to find Contours and how to draw Contours so first of all what are Contours so Contours can be explained as the curve joining all the continuous point along the boundary which are having the same color or intensity now Contours can be a useful tool for shape analysis or object detection or object recognition now for better accuracy we generally use binary image for finding the Contour so first of all we are going to generate the binary image and then before finding out the Contours we are going to apply the threshold or Kenny Edge detection to find the Contours on the image so let's start with an example so here I have a simple code which reads an image and then converts this image into a gray scale image and then I am just showing both the images using I am show method so let's run this code and let's see what result we get so this is the original image with these colors and after the conversion of this image to the grayscale image this is the result which we are get typing and then we are going to find out the threshold or the Kenny Edge so in this example we are going to just apply the threshold so for applying the threshold on this image we are going to Define first of all two variable r e t comma thresh is equal to c v e 2 dot threshold so there is a method called threshold which we have already seen how threshold work in detail in the previous videos so the first argument which this threshold method takes is the image so we are going to pass our grayscale image as the source the second argument is the threshold value so because it is a binary image let's set the threshold to 127 which is around half of the 255 right the third argument is the maximum value so in the maximum value here will be 255 the next argument will be the type and type here will be zero so this is going to give us the threshold value for this grayscale image and after finding out the threshold of this image we are going to find out the Contours so for this we are going to Define two variables one is Contours and the second is the hierarchy because the method which we are going to use which is CV2 dot find Contours this is the method it's going to give us these two values Contours and the hierarchy and we are going to see what are Contours and hierarchy in details after applying this method on this image so the first argument will be the thresh which we got using this threshold method the second argument will be the Contour mode so this is called the Contour retrieval mode also and there can be several possibilities here which we can apply but for Simplicity and in the most common case we use r e t r underscore tree here okay as the mode the third argument here will be the method which we want to apply and this is also called the Contour approximation method and here also several possibilities are possible but for now what we are going to use here is this will be c v two dot approx none so now as you are seeing here this fine Contour method gives us Contours and hierarchy so the Contour is a python list of all Contours in the image and each individual Contour is a numpy array of X comma Y coordinates of boundary points of the object and the hierarchy is the optional output Vector which is containing the information about image topology and this we are going to see in the later videos so for now we are only concerned about finding out the Contours so for this as I said this contains the number of Contours right so we can print out these number of Contours is equal to and then we are going to just convert this number into the string and there is a method called length and then inside this length method we are going to pass our Contour variable so this line is going to print out the number of Contours which are found inside the image which we are providing so let's run this code and let's see what result we get so we already know that this gives us a grayscale image and the original image but we are interested in this print message and the number of Contours which are found is 9 inside the source image which we are providing here now we already found out the number of Contours now we need to draw these controls on the image itself so how can we achieve this but before this let's see the individual Contour also so I am going to just print out the value of the first Contour which will be at index 0 so let us run it once again and let's see what happens so we are running this code once again and you can see after printing out the number of Contours it is going to give us the numpy array of the X and Y coordinates so if we plot or join all these X and Y coordinates we are going to get the boundary of the Contour so now we are going to just take these Contours and pass it to a method called Draw Contours which is going to draw or join all these coordinates of those Contours so to get this we are going to just say c v 2 dot draw Contours and then the first argument here will be our original image because we want to draw the Contours on our original image so this will be the IMG and its the original image and the second argument will be the Contours so we are going to just pass the Contours which we found inside the image the third argument will be the Contours indexes so if we ah just give here minus 1 then it's going to draw all the nine Contours which were found inside the image these all Contours so first of all we will give minus 1 here as an argument and then we will see how to give other arguments uh as the numbers here also the fourth argument here will be I think the color so we are going to just give the color 0 comma 255 comma 0 let us say and the next argument will be the thickness so we are going to take the thickness 3 here so using this method what we have achieved is we have drawn the Contours on the original image so let's run this code once again and let's see what result we get so you can see this was the grayscale image and this we have used for finding out the Contours but the interesting image here is this one and here you can see all the Contours are drawn on this image so all the green lines or green boundaries are all Contours so because we have given minus 1 it has drawn all the Contours on this image and we can also give the Contour index so let us say we just want to ah find out the Contour 0 which will be the first Contour which is found inside the image we are going to just run this code once again and the first Contour which was find out found out is this Contours this ah P Contour right in a similar way we can go up to eight so 0 1 and let's rerun this code again you will see that the second Contour is this Contour so this whole Contour from the boundary of this image is the second contour and in a similar way you can go let's say 2 I am going to run this code once again you will see the next Contour here and similarly you can go up to the index 8 because the total number of Contours are 9 and we are starting from the index 0 that's why we can go up to eight so this value depends on the number of Contours ok so because we found out the number of Contours are nine so that is why we can go up to eight and let's run this code and the last Contour which was find out and we have drawn this Contour here on this blue circle right now if we go Beyond this index let us say we give 9 here we are going to get the error right so you can go up to 8 here and if you want to just draw all the Contours then you can just give minus 1 here and it's going to draw all the Contour on the image which you are providing so this is how you can find out the Contours and draw Contours on the images using fine contour and draw Contour methods in opencv in this video I am going to show you how you can create a very basic and simple motion detection and tracking system using Python and opencv so let me show you what we are going to achieve at the end of this video so I have this video which is a sample video and you can see some people are walking around inside this video now what I want to do here is I want to show these rectangles around these moving people or persons so this is tracking and when some movement occurs I also want to show this kind of status that status is movement because somebody is moving inside the video so if nobody is moving this status will be blank and if somebody is moving then the status will be movement so this is what we are going to achieve at the end of this video so we are going to try to track each and every person and also we are going to track this person with this rectangle and also we will show the status as movement when somebody moves inside the video so let's get started so to start with I have this basic code which just reads a video using video capture class and then if this video is valid then I'm going to just show this frame by frame inside I am show window and I am sure you might be knowing all this code because I have shown you step by step how to capture the video or how you can read the video frames using video capture method okay so this is just to load this video and show it frame by frame using amp show methods so let me run this code first of all to start with so our original video looks like this so some people are moving but we want to track the movement of each and every person and also we want to show a rectangle around them whoever is moving so let's get started so under this video capture code line what I'm going to do is first of all I want to read two frames from the cap instance so I'm going to just copy this code and paste it here so this will be our frame 1 let's say and similarly I am going to just read the second frame so ah simply we are just declaring two frame one after another okay and we do not need this code anymore so first of all I'm going to declare a variable diff and using CV2 dot a b s diff method so absolute difference we are going to find out the difference between the first frame and the second in frame so this method ABS diff is for finding out the absolute difference between the first frame and the second frame now once we have the difference then we are going to convert this difference into a gray scale mode so we are going to just say gray is equal to CV2 Dot convert color so CVT color and the first parameter here will be our difference which we have found between the two frames so I am going to just pass diff as the first argument and the second argument will be c v 2 dot we are going to convert this BGR color to the gray scale mode and why we are finding out the grayscale mode of this diff because we are going to find out the Contour in the later stages and in the last video we have learned that it is easier to find out the Contours in this grayscale mode as compared to the colored mode or the B gr mode so once we have this grayscale mode we are going to just blur our grayscale frame so we are going to just declare a variable called blur and then we are going to apply the gaussian blur on our gray variables so CV2 dot gaussian blur the first parameter here will be gray so let's give this gray parameter which we have defined here the second parameter here is the case size or the kernel side so let's say we want to provide the kernel size 5 comma 5 and the third parameter here will be the sigma x value so we are going to just pass 0 here as the sigma x value now we are going to find out the threshold so we are going to just say underscore because we don't need this first variable and then the second variable will be thrash is equal to CV2 dot threshold and the first parameter which it takes is the source so we are going to pass our blurred image as the source and then the second parameter here will be the threshold value so we are going to just provide 20 here then the maximum threshold value will be 255 then the type will be ah CV2 dot thresh binary so in the next step what we are going to do is we are going to dilate the thresholded image to fill in all the holes this will help us to find out the better Contours so there is a method called CV2 dot dilate so we are going to just declare a variable called dilated and then we are going to apply this method so CV2 dot dilate which takes few argument the first argument will be our thresholded version of the image the second argument here will be the kernel so kernel let us say for now we are going to provide none here okay so the kernel size will be none and then third parameter will be the number of iterations so let's provide the number of iterations and the number of iterations we are going to provide here will be three so if it doesn't work we can increase or decrease the number of iterations now in The Next Step what we are going to do is we are going to find out the Contour so as you all know that Contour or fine Contour method is going to give you two Result One Is The Contours and other is the hierarchy so we are going to just say contour and the second result we are going to just say underscore because we are not going to use this second result and then we are going to just say CV2 dot find Contours and we are going to find the Contours on this dilated image so we are going to say dilated now the next argument here will be the mode so the mode which we are going to use here will be writer underscore tree so I'm going to just write ret underscore tree which is most commonly used and then the next argument here will be the method so the method here will be CV2 dot ah chain approx simple and once we have our Contours we are going to just draw the Contours because we already found out the Contours so we are going to just say draw Contours and the first argument here will be frame 1 because we want to apply all the Contours on the original frame right so we are going to apply all the Contours which we have found using all these method on the frame one and then the second argument here will be the Contour so you can just give the Contours here and the third argument here will be uh the Contour ID I can just say minus 1 which is going to apply all the Contours and the third and the next argument will be the color so let's say we want to provide the green color so I'm going to just say 0 comma 255 comma 0 and the next will be the thickness so let us say we want to provide the thickness of two here so now it's going to draw all the Contours which we have found with the difference of frame 1 and Frame 2 right and then we are going to just display this Frame one so we can just say this is our feed and the result after applying the Contour will be saved in the frame 1 which we will display now in The Next Step what we are going to do is we are going to assign the value inside Frame 2 into frame 1 so we are going to just say frame 1 is equal to frame 2 and then inside our Frame 2 we are going to read a new value so we are going to just say r e t comma Frame 2 is equal to cap dot read okay so we are reading the new frame in the variable Frame 2 and before reading the new frame we are assigning the value inside the frame 2 to the frame 1 in this way we are reading the two frames and finding out the difference between ah the two frames so let us run this code and let's see if it works or not let us test this so you can see now there are these Contours which are drawn around all the moving ah persons also there are some Contours ah which are drawn around this rope which is also moving right so we have successfully determined the Contours and we have already drawn these Contours on the frame one but this was not the result we are looking for we want to draw the rectangle around these moving persons and also we want uh some noises to be removed so we do not want to draw the Contour on the moving rope let us say ok so how to remove these noises and how to draw these rectangles let's see so now in The Next Step what we are going to do is under or before we are drawing these Contours we do not want to draw the controls now we want to draw the rectangles right so what we are going to do is we are going to iterate over all the controls so we are going to just say for Contour so from Contours we are going to find out Contour in Contours right so this is the list and we are iterating over this list so inside this for Loop the first step will be to save all the coordinates of the found Contours ok so we are going to define the x coordinate then the y coordinate and then we are going to just say width comma height and there is a method called bounding rect which we are going to apply on the Contour so we are going to just say is equal to c v 2 dot bounding rect this is the method which we are going to apply which is going to give us the x and y coordinate and the width and height right and we are going to apply this bounding rect method on the Contour which we are getting using this Contours list now in the next step we are going to find out the area of the Contour and we are going to just say if this area is less than certain value then we don't want to do anything we do not want to ah draw a rectangle or anything we just want to continue otherwise if this ah Contour area is greater then let us say ah some kind of a person's area then we want to draw a rectangle on it so inside this for Loop we are going to just ah Define ah if condition so we can say if c v 2 dot Contour area so there is a method called Contour area which is this one where we can pass our Contour so we are going to pass our contour and if the area of this kind to let us say is less than 700 then we are going to just say continue so this code essentially mean that if the area of the Contour is less than 700 then we are going to do nothing we don't want to draw any rectangle otherwise if the area is greater than 700 then we want to draw the rectangle so we are going to just say CV2 dot rectangle we have already learned how to draw a rectangle on an image using the rectangle method the first argument here will be the source which will be frame one the second argument will be the point one so we are going to just say 0.1 will be X comma y the third argument will be 0.2 so we are going to just say X plus W comma y plus h the next argument will be the the color so let us say the color will be the same 0 comma 255 comma 0 the next argument will be the thickness let us say we want to give the thickness ah 2 as we have done ah with the draw Contour we have provided the thickness of two here right now in the next step we are going to just ah print some text on the image if some moment is observed so we can just say c v 2 dot put text this also we have seen in the previous videos how to put text on an image so this time the source will be our frame 1 the second will be the text so we will just say ah status let us say and if there is some movement we are going to just say ah colon in the curly brackets we are going to just use the format method so this is just formatting the result using the string and we are going to just say movement the next argument here will be the origin so where we want to put this text let's say we want to put this text on 10 comma 20 coordinate and then the next argument will be the font face so we are going to just say font face will be c v 2 dot font font Hershey Simplex let us say so we are going to use this font and the the next argument will be the font scale so let me just ah do this on the next line so what scale will be let us say 1 the next will be the color of the font so let us say the color will be 0 comma 0 comma 255 and then the last argument will be the thickness so let us say the thickness will be 3 and this code is going to put the rectangle around your moving persons if the area of that Contour is greater than 700 ok so let us run this code and lets see if it works or not so I am going to just run this code and you can see that status is moment because all the persons here are moving and you can see these rectangles which are drawn around the moving persons and this noise which we were seeing in the previous result is also gone around the movement of this rope okay so sometimes ah this ah rectangle is drawn on the movement of the Rope also so in this case you can also increase the expected area let us say we just want to find out the Contours which are greater than 900 and we can now you can see ah these rectangles are drawn around these moving persons with the area which have the Contour area more than 900 so you can remove these kind of noises from the frame using this area so this was a very basic example how you can detect the motion and track your moving object inside your video using Python and opencv in this video we are going to see how we can detect simple geometrical shapes using opencv so to start with I have this simple codes which reads an image and then show it into a I am show window so let's run this simple code first of all and let's see what it does so you can see I have this image which I'm loading into a opencv window using IM show method third and here we have some shapes so we have a pentagon Circle rectangle square triangle and this star shape right and let us say we want to detect using opencv which shape it is based upon the geometrical shape and we want to write the name on top of this shape so how we can achieve this lets see using openc beam so as you can see if the first step is to read an image and then in the second line I'm just converting this image into a grayscale mode image so using this code I am just converting this image into a grayscale mode and in the next step we are going to find out the threshold so I am going to just say underscore comma trash is equal to CV 2 dot threshold so CV2 Dot threshold and we are going to pass our image which is a grayscale image which we have converted as a source and then the next two values are the threshold values and the maximum value of the threshold so for now I am giving the threshold value to 40 because I know this will work but if you want to be more flexible you can always use the track bar to find out which threshold will work with your image the second value is the maximum value of the threshold and the next value will be the type so the type here will be CV2 dot thresh binary so we are going to just say CV2 dot thresh binary now in the next step we are going to find out the Contours so Contours we have already ah seen in the last videos how to find out the Contours and what are Contours so for that I am going to Define two variables one is Contours variable other is the underscore variable because we do not need the second result and then I am going to just say CB2 dot find Contours the first argument here will be the thresholded image and then the second argument here will be the mode and third will be the method so let us give these two values so c v two dot r e t r tree and the method will be CV2 dot chain approx none okay so let us give this method so this is the simple procedure to find out the Contours inside an image now in the next step I am going to iterate over all the Contours so I am going to just say for Contour in Contour so we are going to iterate over all the Contours and then we are going to first of all use a method called CV2 dot approx poly DP so I'm going to just declare a variable first of all I'm going to just say approx is equal to c v 2 dot this method which I have mentioned which is called approx poly DP so this method approximates a polygonal curves with a specific precision and the first argument which it takes is the curve so our curve here will be the Contour which we have found on the shape the second argument here will be Epsilon so Epsilon is the parameter specifying the approximation accuracy so here what we are going to do is we are going to Define Epsilon is equal to 0.01 and then we are going to multiply this number by c v 2 dot Arc Length so there is this method called Arc Len and what does this Earth length method do it calculates a Contours parameter or a curve length so here in this Arc Length parameter we are going to pass once again our Contour variable and the second argument here will be if it's closed or the open Contour so in our case we know that all the shapes which you want to detect are closed so we are going to just pass through here and the next argument in the approx poly DP method will be once again if its a closed shape tip or the open shape so once again we are going to pass through here because all the shapes which we have are closed shapes now once we have this approximation we are just going to draw all the Contours first of all so we are going to just say CV2 dot draw Contours on which image on our original image so we are going to draw these Contours on the original image and then we are going to pass the second argument and this will be our approximation so we can in the square bracket this is ah one other notation of ah just giving the number of Contours as an argument to the draw Contours method so in the square brackets you can just pass the approx the next parameter here will be the Contour index so because we are iterating over all the Contours that's why the index will always be zero because there will be only one contour which we are working at a time so this index will be zero the next argument here will be ah the color so you can give any color here I am going to give 0 comma 0 comma 0 let us say and then the next will be the thickness so thickness I am going to give here is 5. now the next step is to print out the shape so which shape it is we want to print on the shape which shape it is in simple English let us say so for that we need to find out the coordinates on which we want to ah print this text on the shape so we need to find out the X and Y coordinates so we can find this X and Y coordinates using this approx variable and we can just say approx dot revwell so this is a method called Ravel and then the first index here will be the x coordinate and see in the same way we are going to just say approx dot Ravel and on this method the second argument or the second index at index 1 will be the y coordinate so on these X and Y coordinates we are going to print our text now in The Next Step what we are going to do is so because this approx poly d p is going to approximate the number of polygonal curves so based upon the number of polygonal curves we can just approximate which shape it can be so if this approx length so let us just find out the length of this approx and if the length of this approx variable is equal to 3 then we are going to say that it's a triangle because triangle can be made with three points so this length of approx variable if it is equal to 3 then we are going to say that it is a triangle because if the number of Curves here are three then most probably it is going to be a triangle so if we know that this is a triangle then we can easily ah just print or put text on that image so we are going to just say put text and the first variable here will be the image so we are going to put text on the image the second variable will be the text and we know that this will be a triangle so we are going to just say triangle here and then the next argument here will be the coordinates on which you want to print this text so we already found out the the coordinates at which we want to put this text the next argument here Will be the font so we are going to just say c v 2 dot font Hershey complex and the next argument here will be the font scale so let us say font scale will be 0.5 and the next argument here will be the color so you can give any color let's ah say we just want to print this text in the black color itself so we are going to just say 0 comma 0 comma 0 then using this logic we can also say that if the length of this approx is equal to 4 then it can either be ah square or a rectangle so here if the approx length is 4 then it can be a square or a rectangle but we don't know if it is a square or a rectangle so for now we can just write that it's a rectangle and we are going to decide if it is a rectangle or a square in The Next Step but let's define the other if else conditions also so this was L if similarly if number of approx points are 5 then we are going to say that it is a pentagon so we are going to print out the Pentagon text on the X and Y coordinates and if the number of points are 10 then we are going to just say that its a star shape so we are going to just say star because in the star the number of points are 10 and then we are going to say that in any other condition so we are going to just say else and we are going to just remove this condition from here else in any other condition it is going to be a circle okay so if approx length is 3 it's a triangle if approx length is 4 it's a rectangle or a square a 5 Pentagon if it's 10 it's a star if it's nothing out of all these options then it is a circle you can also find out ah for example octagon or hexagon here if it's six it's a hexagon if it's eight it's a ah octagon and so on right now let us once again come to this step and in this step we ah just know that if the number of points are 4 then it's a rectangle or a square but how can we find if it is a rectangle or or a square so let us decide that now so what we are going to do for that is we are going to just say x comma Y and then we are going to just say ah W comma H for width and height and there is a method called CV2 dot bounding rect which is going to give us the X and Y coordinates and the width and height of the rectangle right so we are going to apply that method so CV2 dot bounding rect on our approximate variable or approx variable which is going to give us the x and y coordinate and width and height now based upon the width and height we can find out the aspect ratio so we are going to just say s fact ratio ratio is equal to float first of all we need to type cast the width into a float so we are going to just say ah float W divided by height and this will be the aspect ratio of the rectangle now if this aspect ratio let's print out the aspect ratio also so we know what aspect ratio we are getting using the rectangle or the square and we are going to just say if this aspect ratio is between ah 0.95 and 1.05 then its going to be a square right because the width and height are almost same okay so we just give some room for some noises that's why we are providing here ideally it should be a one aspect ratio should be one ah in order to have a square but let us say we are just approximating so we can just say if its 0.95 if it's greater than 0.95 and if its less than so aspect ratio is less than or equal to 1.05 then it is a square OK in ideal situation you might want to give here one but in images ah it can be ah little bit different so we are just giving this limit so if ah the spect ratio Falls in this limit then it is going to be ah Square otherwise it is going to be a rectangle right and I am going to just say that if this is the case then it is going to be ah Square otherwise so in the else condition so let us give this else condition here else it is going to be as rectangle so let's print rectangle in the put text okay so this is the code which we we have written and now finally what we are going to do we are going to just show the shapes image including all the Contours and the text which we have put on these shapes so let us run this code and let's see if it works or not so you can see now it is going to work like this so all the Contours are drawn across these shapes and you can see the text on top of these shapes so Circle rectangle Pentagon star triangle and squares what you can also do here is you can just change ah this text position using the X and Y coordinates so let's say I just want to change this y position to just little bit top of the shape so I just added minus 5 offset here in the y axis and now you can see it goes little bit up this text right so now it's much visible this text and you can see rectangle and square text is not going up because we have declared the local X and Y here also so we can just say x 1 and y 1 here and then run this code once again and you can see this rectangle and square text is also moved little bit up so I think the offset of 5 is okay to show these text on top of these shapes so this is how you can detect simple geometric shapes using opencb in this video we will discuss about histograms in opencv so what is a histogram so you can consider histogram as a graph or a plot which gives you an overall idea about the intensity distribution of an image so let me give you some examples and then I will be able to explain you better how histogram works and why they are useful so to start with I have this example which is a very normal example here I'm creating 200 by 200 pixel image using a numpy zeros which essentially mean that we are going to get a 200 by 200 pixel image of black pixels so let me just ah just start this example and you can see this is the final result so all the pixels here in this image are black and the size is 200 by 200. now let us say we want to calculate or find out the histogram of this image so there are several ways of finding out histogram of an image so let's see them one by one so first of all we are going to find out the histogram using the mat plot lib because ah the plot using matplotlib you can draw easily so let us use that first of all so for that what I am going to do is I'm going to use PLT because I have already imported this matplot Library as PLT so PLT ah dot hist there is a function called PLT dot hist which calculates the histogram of an image and because it's just a grayscale image or it is just a black image so it's easier to find out the histogram so you what you can do here is the first argument here will be your image or your source so I'm going to just say image dot raw well okay so there is a method called Ravel the second argument here will be a maximum number of pixel values so I am going to just say 256 the third argument here will be the range so the range will vary from 0 to 256 okay so this is all you need to find out the histogram using the mat plot lib and you just need to show this plot in a matplotlab window so you can just say ah p l t dot show so that's it so let's run this code and let's see what happens so you see this plot using matplotlib and also our original image so as we have created the image of 200 by 200 pixel of black pixels so all the intensity of this graph you can see is zero so you can see here 200 multiplied by 200 is equal to ah 40 000 so these are the number of pixels so on the y axis you will see total number of pixels and here the intensity so intensity starts from 0 to 256 so this graph is showing how many number of pixels inside an image which have this uh pixel values so in our example all the pixels inside this image have the pixel value 0 that's why this graph is like this so all the 40 000 pixels inside the image have the pixel value 0 so you will get this type of histograms so once again the histogram is a graph or a plot which gives you the overall idea about the intensity distribution of an image now histogram is just another way of understanding the image by looking at the histogram of an image you can get the intuition about the contrast brightness intensity distribution Etc now let us improve this example which we have so I am going to just close this window and let us say I want to add some white pixel also inside this image so what I am going to say is I am going to just CV2 dot rectangle so I am going to just add the rectangle inside this image and the source here will be the IMG variable then where I want to introduce this rectangle so I want to introduce this rectangle at this point which will be let us say which starts from 0 comma 100 and the second Point here will be let us say 200 comma 200 okay so this will be ah 200 and the next value here will be the color so let us say we want to add the white pixels so this will be 255 which will be the maximum value value and then the next argument will be the thickness so I am going to just say minus 1 which will fill this rectangle inside this image so when I run now this code you will see this graph and this image so you can see half of this image contains black pixels and half of this image contains the white pixels and we already know that the size of this image is 200 by 200 that is why ah here in the graph you will see 20 000 pixels are black which means that 20 000 pixels have the pixel value 0 and 20 000 pixels have the pixel value 255 that's why you see this here so you can see you can easily find out the pixel intensity of an image easily using histograms now next we are going to add some more pixels into this image and this time what we are going to do is we are going to add the rectangle ah inside the same image so let us say it goes from 0 comma 50 to 100 comma 100 and the color here we are going to provide the pixel value of 127 let us say ok so which is the half of 0 and 255 approximately so I am going to run this ah example once again and now you will see this kind of image so you can see half of the pixels here are wide that means 20 000 pixels have the pixel value of 255 so you can see here now around 15 000 pixels here in the half of this image have the pixel value of 0 that's why you can see this line here and we have added the rectangle of pixel value 127 also so around you can see around 5000 pixels here have the pixel value of 1 to 27 so this is how the histogram is going to work so let's use now the original image so some kind of image instead of this black or white image so now what I'm going to do is I'm going to just once again declare a variable and then I'm going to just say CB2 dot I am read and we are going to read some files so let us say I have this Lena dot jpg image so I'm going to just read that I hope the extension is a correct JP uh G and we are going to read this image in the grayscale mode so I am going to just say 0 here and now I'm going to run this example once again and you can see this Lena image is loaded in the grayscale mode and here is the histogram of this image so these are all the pixel intensities inside this image so you can see from this ah graph that most number of pixels contained inside this image have the pixel value around 150. now you can also find out the pixel intensity of different colors so till now we have been just using the grayscale mode or black or white pixels but you can also uh use the same histogram for the BGR values also so let's see how we can undo that so what we are going to do is let me just remove this code or I'm going to just leave it commented and here I'm going to just say B comma G comma R and there is a method we have already seen which is called CV dot split which is going to split your image into BGR values so we are going to just give the source which is our image and then if you want to show these BGR values you can just show in the I'm show window so BG R and here also B G and R and when you want to uh show the histogram of BGR values then also you can use matplotlib dot hist method you just need to change this Source from image to BGR so B uh G and R okay so now what we are going to do is we are going to run our code and let's see what happens so it is giving me ah this error because I'm reading this image in the grayscale mode so I'm going to remove this extra parameter from I am read because we want to read this image in the color form and then only we will be able to get the BGR channels right in the grayscope scale mode there are no BGR channels so I am going to run this script once again and let's see what happens so you can see this histogram of blue channels and green channels and the red channels and these are the images which are loaded in these different channels so this is the image which is loaded in the blue Channel and this is the green and this is the red Channel and you can see the histogram of each Channel differently using matplotlib so let me just close all these windows now there is a method word in CV2 also which is called calc hist which is going to give you the histogram of an image so for that what you can do is I'm going to just just comment all the this code because I just want to show how you can use the CV2 calc hist method okay so what you can do is you can use ah method so let's say hist and then CV Dot calc hist and this method takes few arguments so the first argument here will be the image so it's the source which you give but the only special thing is you just give this image in the square brackets okay the second argument here is the channel so it is the index of channels for which we calculate the histogram so here in our case because we are going to read the image in grayscale mode we can just give the channel 0 here so for one channel you can give 0 here for different Channel you can give 0 1 2 value the next argument here is the image mask so to find histogram of full image it is given as a none because our because our image is loaded in the gray scale mode so we can give here none the next value is the hist size so this hiss size is the representation of Bin counts and this is also given in the square bracket so we are going to just say 256 here the next argument is the range so range will vary from 0 to 256 so minimum and the maximum range of the xaxis you can say so 256 and then we can just show this uh hist or histogram inside the PLT so PLT Dot Plot method so Dot Plot and then we can just give this histogram value here okay so let us run this code and let's see what happens so you can see you get the histogram of this image using the opencv calc hist method and what are the uses of the histogram so a histogram can tell you whether or not your image has been properly exposed so when you take a digital image ah it's very useful it it can also tell you whether the lighting conditions were flat or harsh when you took that image and using the histogram you can also make the adjustments which will work best for your digital images so this ah the usefulness of the histograms we will see in the later videos this was just the basics about the histograms in opencv in this video we will discuss about template matching in opencb so first of all what is template matching so template matching is a method of searching and finding the location of a template image inside a larger image in opencv there is a method called match template for achieving this purpose so let us get started and let's see an example about it so I have this simple code which just loads this image and let's see ah what this image looks like so this is the image and this is the messy image and what I want to do is I want to match the face template which I have which looks like this which is the smaller template which is also available inside this image so this will act like a template for us and we will try to find this template inside this larger image so let's get started and let's see how we can search this template inside this larger image so first of all what we need to do is obviously we need to load this image and also load our template so before loading our template image I am going to just convert my original image which is the larger image into the grayscale image so I have declared this variable gray underscore image and then I'm going to just say CB to dot CVT color which is going to convert my image IMG and let's convert this image into CB2 dot color underscore BGR to Gray now let's load our face image which is called messy underscore face dot jpg so I am going to just change this name as C underscore face dot jpg and this will be our face image or you can also say this is a template and I am going to also load this image as a grayscale image so I'm going to just pass the second argument in the read method as 0 which is going to load this messy image as a grayscale image now in the next line we will simply ah use this method which is called match template and we are going to save it into some variables so we can just say r e s is equal to c v 2 dot match template which is this method which takes few argument first is our image so I'm going to pass our grayscale image here the second argument here will be the template which we are trying to search inside this image so this will be our template the third is the method so the method can be a several method there are several methods available for the template matching so I want to show you these method for the template matching so you can see a type of template matching operations and there is separate formula involved in order to match that template inside that image so so for now we are going to use this method which is TM underscore C Co F underscore normed dot TM underscore C Co F normed which is this method now let us try to print this result and let's see what is the content inside this result so I am going to just print the content inside this result which we got so I'm going to run this code and this image is load loaded but for now we are interested in this array Matrix which you are seeing here so you can see when you observe these values carefully you will see all are relatively smaller values so you can see ah 0.2 0.2 almost every value is around until 0.3 so the maximum value I can see here is 0.3 so let me just show this image once again and the the template also so what this result contains is these all values and there will be one value which contains the number for example 0.8 or the brightest Point okay so if here this ah Matrix contains a value which have the value 1 it is the brightest point and it will be there inside this image after applying this mesh template method which will be around this point at this point at which ah this template matches so top left corner of this template so at the point at which this left top corner of this image will match inside this large image there will be a brightest point there and that brightest point will be reflected inside this image in the form of this decimal number and all the other values will be slightly uh darker darker values ok so that is how ah this Matrix from this Matrix we will come to know the the top left corner of the template inside this larger image so now how can we filter out that value which is the brightest Point inside this Matrix so all the points ah you can see looks like under 0 0.3 but there are some points here you can see three dots and there are thousands and thousands of values will be available here all the values are not printed ok so what we are going to do is we are going to try to find out the brightest point so this we can find out with the numpy method ah there is a method called where ah using which we can find out or filter out those values which are greater than certain number so I'm going to ah first of all declare a variable called threshold is equal to I am going to declare the value of threshold initially as 0.8 which will be ah relatively brighter Point ah inside the Matrix which we are getting using this result variable right and then there is a method called where numpy where so I am going to declare once again loc variable and then P Dot where method and here we are going to pass our result which we got and we are going to filter out using this expression so this will be a Boolean expression so I am going to just say give me all those values which are greater than or equal to the threshold inside this result Matrix okay so this where method is going to uh just evaluate this expression each and every value will be evaluated and if this value inside the Matrix is greater than 0.8 which is our threshold then it is going to ah give those values to us so let us print out those values after the filtering out of most of the values and let us just print this loc variable also so I am going to run this code once again again and you can see here this is the Matrix which we got so you can see this is the array which we got so still we can increase this threshold in order to find out only one point so there are several points available here so let us say I am going to increase this value to 0.9 and let's run this code again and you will see only two points 85 and 220 so this is what we were expecting so we wanted to find out ah this point which will be the brightest Point ah inside this result Matrix so once we got the brightest Point ah which will be around here which will be the top left corner as I said of this template and it will be located somewhere here in the original image then we can draw the rectangle ah around this original image ah same as the size of this template so this will be the easier task because we already know the width and height of this template we already know how to get the width and height of this template and same size rectangle we just want to draw on this original image so let's see how we can do this so there is already a method so I'm going to just declare two variables width and height and you already know the method so template dot shape is going to give you the shape of your image right so I'm going to just say template dot shape and then inside the square brackets we are going to just give two colons and minus one this means that we want to get the column and the rows value in the reverse order so width and height that's why I have given this minus 1 index here now in the next step what we are going to do is we are going to just draw all the rectangles where the template is is matched so by seeing this template image and the original image we know that there is only one messy face inside this image but let us say there are several number of ah matched templates inside original image ah for that we need to iterate over ah the result which we got after applying the filter on the result so for that we are going to just iterate over that result in our case as we know that there is only one point ah so we do not even a need to iterate over it but if there are multiple number of matched templates then this ah for Loop will be ah handy so for p t in your loc variable so we are going to just say zip which is going to iterate over this loc variable so Asterix l o c and then we are going to find out the methanite here also so we are just reversing the x axis and y axis right so we are going to just say colon colon minus 1 here and then once again inside this for Loop so c v 2 dot rectangle method and the first argument here will be our original image because we want to draw the rectangle on the original image the second argument will be the first point of the rectangle so the first point will be this one p t which we are getting using the loc uh uh variable so as you all know that the first point here will be the top left corner of the rectangle and the second Point here will be the bottom right corner so how can we get the bottom right corner we will get the bottom right corner using this PT variable and then on the zeroth index we are going to just add the width comma on the first index so p t ah square bracket first ah we are going to add the height okay so essentially we have just found out the width and height of our template and we are getting the second Point using this addition on the first point width and height so it is going to give us this bottom right corner of this template or this point so this is how we are getting our two points to draw the rectangle now the third and fourth variable will be a simple which are the color so you can just say 0 comma 0 comma 255 which will be the green color and the width let's say 2 here so we want to give the width 2 here so let's run this code and let's see what happens happens so I'm going to run this code and you can see this red rectangle is drawn on the face of the messy and you can s here also see this rectangle will match our template image so whatever image is inside this rectangle will be ah exactly same as our template and once again you can see the result let me explain this code once again so if this point this threshold will be ah 0.08 let us say in the case of point zero nine threshold we are only getting two values ah this 85 and 220 right that's why we are seeing the clear rectangle here when we are giving the threshold 0.8 here let's see what happens so I am going to run this code once again you can see there will be ah this rectangle but it will be much thicker why it's much thicker because we are getting several number of values 1 2 3 4 5 6 7 8 9. so we are getting the nine points on the x axis and the y axis so this for Loop will iterate nine times and this rectangle will be drawn nine times on the image and that's why this rectangle is much thicker let's uh just change this value to 0.9 once again and you will see this rectangle is ah you know the single rectangle that's why it's much thinner right now ah when you give this value let us say we give the value 0.3 so most of the point as you can see here have the value 0.3 and when we run this code you will see so many rectangles here so that's why this thresholding is essential for us to find out the brightest point or the value which have the maximum value right so that's why we were filtering out this these points and finding out the values more than 0.9 threshold and about the method so let's try different methods so let's try to give different methods here these two methods behave little bit differently so we can start with this TM c c o r r normed and we can apply it here and it's going to give us this kind of result you can see we are getting several points here after filtering so ah let us try to increase this value to 0.95 and let's rerun this code and let's see what happens now you are getting four values uh you can also filter that out let us say 0.99 now let's see what happens so now you are getting only two values OK so you need to uh try to change this value to the maximum point so try to change this value and you will get this kind of rectangle only one rectangle so every method is going to give you different uh result and that's why you need to uh try all the result not all the result will give you the perfect rectangle or template matching so you need to try different methods on your images so this is how you can do template matching in opencv in this video we will understand the concept behind the half transform so first of all what is half transform so half transform is a popular technique to detect any shape if you can represent that shape in a mathematical form half transform can detect the shape even if it is broken or distorted a little bit now this explanation might seem a little bit confusing so let me explain it by an example so let us say you have an image of this road and you want to detect these Lane lines in ah this road image so the first step in order to detect these Lane lines in this road is to find the edge pixels using Kenny Edge detection or any other Edge detection method now after you found out the edges using any Edge detection method you want a geometrical representation of that edge and in order to find out the geometrical representation for example you want to find out the slope of this Edge or its intercept you can use half transform to represent these pixels or edges in the mathematical or geometrical form so after you find out the edges using any Edge detector you just just have the sequence of pixels so you can Loop through all the pixels and somehow figure out the slope and intercepts but it is a very difficult task so we want some mechanism that gives more weightage to pixels that are already in line and this is what we can achieve using half transform so let's begin and let's start with the lines so a line in the image can be represented by two coordinate systems first is using the Cartesian coordinate system and using this equation you can represent a line which is y is equal to m x plus C and you can also represent this line using polar coordinate system using this equation which is X cos theta plus y sine Theta is equal to r or rho sometimes so lets start with ah this equation first which is a Cartesian coordinate system equation which is y is equal to m x plus c so ah when you represent a line in X and Y coordinates which is also called the x y space ah this equation looks like this so Y is equal to m x plus c where m is the slope of the line and C is The Intercept of this line so if you know the values of M and C you can represent this line in the x and y coordinate now in half transform you can represent this line in other form also and this is called the MC space or the half space so using this equation when you take M on this axis and C on this vertical axis then this is called the MC space so earlier we have represented this line in the x y space and now we are saying that we want to represent this using the m c coordinate where m is on the horizontal line and C is on the vertical line so when you represent this simple line in the m c space or the half space it can be represented as a point so this line can be represented as a point so we all know that a line is a collection of points and managing the collection of points is tougher than managing a single point so if you want to manage a collection of point and if you were to manage a single point which will you prefer and an obvious answer will be to manage the single point and this is what this MC space is doing it's representing a line in the form of a point in MC space or the half space and the opposite of this concept is also possible so if you can represent a point using this coordinate in the x y space then it can be represented as a line in the MC space okay and the formula now will turn into this equation which is C is equal to minus X A M plus y a right so you can represent a point and if you have the x and y coordinate in the MC space you can represent this as a line and this will be the equation where X will be the slope now and Y will be The Intercept earlier M was the slope and C was The Intercept but when you are just transform or just represent this point into MC space then your X becomes or minus X becomes the slope and Y becomes The Intercept so how does these concepts are going to help us so the half transform is all about doing what we have learned converting points in the x y space to the lines in the MC space or the half space so for example you can see Four Points one two three four which are joined by a line right so you can represent these four points and you can join all these four points and its our representation of a line and here slope is equal to M and intercept is equal to C in the x y space the same line you can represent in the MC space uh using these four lines okay so every point is a line in the MC space and you see the intersection Point here which is on the MC chord in it so you have taken an edge detected image and for every point that is a nonblack point you draw lines in the MC space and obviously when you draw these lines these lines will intersect with each other and these intersections Mark are the parameter of a line okay so in the MC space you can represent each and every Point as a line and they will intersect on a single point and now this intersection Point can be used to draw a line so this was the represent iteration of points in a line using MC space using a Cartesian coordinate system now let us apply the same Concepts which we have learned using the Cartesian coordinate system ah into a polar coordinate system so as we all know that in the polar coordinate system we can represent a line using this equation also which is R is equal to X multiplied by cos theta plus y multiplied by sine Theta or in other form you can also represent this equation like this where Y is equal to minus cos Theta by sine Theta multiplied by X plus r divided by sine Theta so this is your x y space where line can be represented like this and we are going to transform or represent this line using this equation into the r Theta space or the half space okay so this line using this equation can also be represented as a point in R Theta or the half space like this so let us take an example about this so as I said the equation was R is equal to X multiplied by cos theta plus y multiplied by sine Theta where this Theta is the angle of the line and R is the distance from the origin to the line so let us say we want to represent a point which is from x y space into a half space into R Theta space so we give the values of x 0 and Y 0 which will be the first point we can represent this point in the form of line in the half space or the r Theta space ah in this formation which looks like a sine curve using this equation so this is for the one point representation in x y space to a line representation in the half space so let's say you have multiple points so we take three points then ah it is going to look like this so let us say x 0 is equal to 8 and Y 0 is equal to 6 x 1 is equal to 4 y 1 is equal to 9 and X2 is equal to 12 and Y 2 is equal to 3. so we have three points in the x y space they can be represented in the half space using three lines and as we have seen in the Cartesian coordinate system these points can be represented in these lines in the half space in the polar coordinate system also using these curved line and this intersection is going to represent a line in the half space so which representation we are going to use in order to use the half transform so this equation is not able to represent the vertical lines that's why generally we use this equation or a polar coordinate system in order to use half transform so the half transform algorithm involves these four important steps in the first step Edge detection is done using canningh detector or any Edge detection method in the Second Step mapping of the edge points to the half space is done and all these Edge points are stored in an accumulator and the third step interpretation of accumulated yield lines of infinite length is done and this interpretation can be done by thresholding or any other constraint the fourth step involves the conversion of infinite line to finite lines now opencv implements two kind of half line transforms the first is the standard half transform which is done using half lines method the second type is the probabilistic half line transform which is done by half lines P method so this is the half lines method and this is the half lines P method in the last video we have seen a brief Theory introduction about half line transform so I have told you that opencv implements two kind of half line transforms one is a standard half line transform using half lines method and the second is the probabilistic half line transform using half lines capital P method so we are going to use the half line method in this video and see how we can use this half line method to detect the lines inside an image using half transform now I also told you that there are four steps associated with half transform so the first step was the edge detection step using any Edge detection method preferably Kenny Edge detection the second step is the mapping of edge points to the half space and store these H point to an accumulator the third step was the interpretation of accumulator to yield lines of infinite length and the fourth step was the conversion of these lines to the finite lines so let us say we have this image of this Sudoku dot PNG and you can see all these lines here which we want to detect so this is the line and this is the line so all these lines we want to detect using the half line transform so I have already written this code so I am going to go step by step uh to explain how this code works so in the first step you just need to import the normal CV2 and the numpy as the NP then here I'm just reading this image using I am read method in The Next Step I'm converting this image into a grayscale image and storing it into this variable which is gray because for Kenny Edge detection it's preferred to have grayscale images rather than your normal colored images now in the next step we are applying the Kenny Edge detection method on this gray scale image so here this cv2.kenny method takes these arguments first argument is the image second and third argument is the first threshold and the second threshold so I am giving the first threshold as 50 and the second threshold here as 150 and the fourth argument here I'm giving a purchase size is equal to 3. now in the next step I am using this half lines method this is the normal Hub transform method which is implemented in opencv now this half line method takes few argument the first argument is the image so we are just just passing this Edge detected image to the first argument of this half lines method the second argument here is the row value this row value is the distance resolution of the accumulator in pixels normally it's taken as one the third value is the Theta value which is the angle resolution of accumulator in radians so for that we are just using numpy so NP dot pi divided by 180 so this is also typical in this method and the next argument here is the accumulator threshold parameter so what does this mean it's a threshold so only those lines are returned that get enough vote that means that those lines will be returned which have threshold greater than this value so starting value I have taken here as 200 as threshold so now this half lines method is going to return the output Vector of lines now I have explained you how polar coordinate works for the half transform in the last video so these lines will be in the polar coordinates so each line is represented by two or three element vectors either rho and Theta or rho Theta and volts so as you can see this is the output Vector of lines so I am going to iterate over each and every line vector and what it gives is the first element of this line is going to give you these two values rho comma Theta it is going to give you rho comma Theta or rho comma Theta comma volt right so right now I'm using just two parameters here row comma Theta so rho is the distance from the coordinate 0 comma 0 row which is the top left corner of the image and the Theta is the line rotation angle in radians so all this rho and Theta I have explained you in the last video and we have seen how we can represent these row and Theta values in the half space so first of all what we are going to do is once we get the row and Theta value is we are going to uh just get the COS Theta value and the sine Theta value because we want to convert these polar coordinates into the normal Cartesian coordinates for the line method because this line method as you can imagine takes uh these coordinates right which are the Cartesian coordinates so this is the point one parameter and this is the 0.2 parameter so X1 y1 and X2 Y2 so first of all we are just getting the COS Theta value and Theta here is this Theta so cos Theta we are just assigning to a and the sine Theta value we are just assigning to B and we are just uh multiplying this a to the row so this will give us the X 0 value and the Y 0 value when you multiply B uh by rho so this row is this row value so this x 0 and Y 0 is going to give you the origin which is 0 comma 0 or top left corner of the image but we want the lines not the top left corner of the image so how we can get these X1 and Y one coordinate and X2 and Y to coordinate uh this is uh given in this equation so once you get your x 0 and Y 0 value you can get the value of X1 and Y 1 coordinate using this equation so you just need to Typecast everything into integer so this equation x one value stores the rounded off value of rho as I have shown here so this R represent rho so row multiplied by cos Theta cos Theta we have already uh taken in the a variable so we are essentially here multiplying the rho multiplied by cos Theta minus thousand multiplied by sine Theta sine Theta value is the value of the B right so x 0 plus 1000 multiplied by minus B here okay why when we get using this equation so y 1 is equal to INT in the bracket y 0 plus thousand multiplied by a which is essentially this equation which is rho multiplied by sine theta plus thousand multiplied by cos Theta so these two values are going to give you the first coordinates and similarly we are going to get the X2 and Y 2 coordinate using these two equations so here everything is same just this minus is Nu right so in this equation you just need to replace a plus by minus and you get the X2 value same you have to do in the case of Y2 so in this equation if you just replace this plus by minus you will get the Y2 value and we have already seen how to use the CB2 dot line method it takes a few argument as you can see here first is the image so image is our original image second is the X1 and Y one coordinate which is the first point comma the second point so as you already know that a line is a collection of point so you need at least two point to create a line right so this is the coordinate of the first point and this is the coordinates of the second point the next argument here is the color so color I have taken simply 0 comma zero comma 255 and the last parameter here is the thickness of the line which I have taken 2 here and the next line of code you already know I think so after this line we come out of the loop and we are just plotting all the lines using this loop on the original image and once we get all these lines on the original image we are just showing it using I am show method and at the last we are just destroying our window once we are done with the image so let's run this code and let's see what happens so I'm going to run this code and you can see all these lines are plotted here let's see the Kenny as detected image also so I'm going to just after the Kenny Edge detection I'm going to once again add this I'm sure method to show the Kenny Edge detected image also so you can see here this is the Kenny Edge detected image all the edges are detected and based upon all these lines which are detected here these lines are drawn but the problem here is these lines are of infinite length so there is no end to this line This these lines just go from the start or the corner of the image to the other corner of the image so you can see they start from here and go to the next Corner they don't just stop here so in this ah half transform you ah see that even for the line with two argument it takes a lot of computation and we don't even get the correct result so this problem can be a solved using the other method which is implemented using this half line P method which is the probabilistic half line transform which I am going to show you in the next video so how we can get the better result using half line P method we are going to see in the next video in the last video we have seen how to use standard half transform using half lines method in opencv now in this video we are going to see how to use probabilistic half line transform using a method called half lines capital P method in opencv so let's go to our editor and this was the code we have written last time and we have used half lines method for detecting lines inside this image which was the Sudoku image so let's run this example really fast to see what was the result which we got last time so this was the result which we got last time and the problem with this result is you can see these lines just go from one end to the other end and in this kind of half transform ah you will be able to see that even for the lines which have two arguments it takes a lot of computation so in opencv there is also a method called half lines capital P which stands for probabilistic half lines transform and this probabilistic half line transform is an optimization of the normal half transform which we have seen in the last video so let me close this example and let's open the example which we are going to see in this video and you can see in this example we have used this Huff lines capital P method so when we use this half lines capital P method it doesn't take all the points into consideration instead it takes only the random subset of the points which is sufficient for the line detection so let us go through this code from the top uh to the end so as you can see I have imported these two packages CV2 and numpy as NP and then I'm reading this image so doku using I'm read method and then I am converting this image to the grayscale image using CVT color method in CV2 now the next step is to find out the edges of the images this we have also seen in the last video so until here everything is same so once we got the edge detected image using Kenny Edge detection instead of using the half lines method we are now using this half lines a capital P method and it takes few arguments the first argument is your Edge detected image the second argument is the row which is the distance resolution of the accumulator in pixels the third argument is the Theta value which we have taken NP dot pi divided by 180 which is the angle resolution of the accumulator in radians the next value is the threshold so it's right now we have taken this threshold as 100 and this threshold is the accumulator threshold parameter which means that only those lines are returned that get enough vote that means greater than the threshold value the extra two argument here are a little bit different from the half lines method so you can see all these arguments are almost same these four arguments but there are two extra arguments here or parameter here which we need to provide so the first parameter here is the Min line length and this we have taken 100 so this Min line length is the minimum length of the line which means that line segments shorter than this length which is 100 it in our case will be rejected the next argument is the maximum line Gap and it is the maximum allowed gap between the line segments to treat them as a single line so these are the two extra argument we have taken and this half lines capital P method is going to return again the output Vector of the lines but the difference between this return value from half line P method and the half lines method is you can see here this line at index 0 is going to directly give you the values of X1 y1 and X2 Y2 which are the two points which we will be able to join and we will be able to draw the line using CB2 dot line method in the last video I have shown you that you have to do so much calculation in order to find this X1 y1 and X2 Y2 and this probabilistic half line transform method is going to do our job easy and it is going to directly give us these four values so you don't need to do anything you just need to pass this X1 y1 and X2 y to Value to the CB2 dot line method so CV2 dot line method is going to take the first argument which is the image and then the second argument is the point one coordinate which is X1 and Y 1 which we got from the line variable at index 0 and the third parameter here is the point 2 which are the coordinate of the point two which is here x 2 and Y two the next argument here is the color which we have taken right now 0 comma 2 55 5 comma 0 and the last parameter here is the thickness of the line so we have taken 2 here and the next three line are going to just show this image first of all all these lines which we found out are drawn on the image this image which is the original image and then we are just showing this image after drawing all the lines which we got using half lines P method on the original image and then we are just loading this image using this I'm sure method and then after we are done we are just restoring all the windows so let's see what result we get after this script is run so I'm going to run the script and this is the Kenny Edge detected image and this is the image you got when you apply this half lines P method on your Kenny Edge detected image so you can see these lines are no longer going to the end to end these are more ah you know accurately detecting all the lines which are there in this doku image you can see some lines are broken here so that's why these lines are not even you know drawn because they are not even detected by Kenny Edge detection so this one or this one are not detected by Kenny Edge detection so that's why these lines are not drawn so so let me show you these results side by side so this was the result which we got after applying the Hop line transform method which is half lines on our Kenny Edge detected image and you can see all these lines here and this is the result which we got after applying half lines P method which is the probabilistic half line transform so these two methods are available in opencv to detect these lines in an image now let us go back to our script and here instead of this image which is the sudoku.png image I have one more image which is called Road dot JPEG and this is the image which contains a road and inside this road we have some Lane lines so you can see this result now here which is the road and these are the lane lines which are detected using this half lines P method so in case of Lane line detection you can use this half line P method but you need to decide your Roi or region of interest because you can see some lines are detected here here here and here so you just need to ah you know Define your line of Interest region and you will be able to detect all the lines or Lane lines on the road so maybe in the next video we are going to see how we can detect these Lane lines on the road accurately without these noises which we are seeing here ah on the other part of the image so we just need to detect these Lane lines and nothing more and we will do the same on a video so on the video in which these Lane lines are there and we just need to continuously detect these Lane lines so in the case of let's say selfdriving car you need to ah detect these Lane lines we are going to see how to detect these Lane lines in the last videos we have learned some important Concepts in opencv now in this video and the next few coming videos I'm going to create a simple project which uses most of these Concepts which we have learned in the previous videos so what we are going to do is we are going to create a very simple Lane detection system so first of all we will start with a still image you can see there is an image which contains uh this road and this road contains Lanes so what we want to achieve is we want to detect these Lanes on which our vehicle is traveling so first of all we will do this with this image and gradually we will move towards the video frames so first of all we will see how to detect these Lanes in this image and then we will see how to detect these Lanes in the moving video so let's get started so I have created this new project in my pycharm IDE you can use any other editor of your choice and first of all obviously you just need to install open CV python package and matplotlib package once you have done that I will create a new file here so I'm going to just right click here and create a new file and I'm going to name this file as detector dot py file so here we are going to import a few packages for example matplotlip so matplotlib dot Pi plot as PLT so let's say as PLT also we are going to import the CV2 package and we are going to import numpy so on import numpy as NP in the next section what we are going to do is we are going to Simply load an image so I am going to create a image variable so image is equal to CV2 dot IM read and we are going to read our image which is the road image so Road dot jpg now in the next line we are going to convert this image into the RGB format because we are going to load this image using matte plot lib so I'm going to just try it once again image so I'm going to overwrite this image variable with the converted image so CV 2 dot CVT color and the source is our image so this is the variable and then CV2 dot color from BGR to RGB right so this is what we want to use now in the next line what we want to do is we want to load our image using uh p l t dot I am show method and at last we are going to just say PLT dot show so this is how we are going to just load our image so I am going to right click on this file and then run this script and you can see this road.jpg image is loaded now on this plot you also see uh these values and one things to observe here is horizontally these values goes from 0 to 1200 something and vertically normally in the graphs you will see that values increases from the bottom to top but in matplotlib this value goes from top to bottom right so 0 is at the top and then the maximum value will be at the bottom so this is a one thing to note because we are going to Define our region of interest and that will be based upon these values now in the next step we want to Define our region of Interest so once again let me just run this code once again and one thing to notice here is this plane in which our vehicle is traveling is parallel so there are two parallel lines and eventually they are going to merge here right so all the Lanes on which the vehicle travels have the same pattern so this Lane and this Lane are parallel to each other and they are going to merge at some point so its not merging but it seems to be merging at some point so we can Define our region of interest from this point to this point point and from this point to this point so this region of interest will be the triangle so this region of Interest we are going to Define for our vehicle will mask any other obstruction for example this is also one lane line for us its not important because this is the other side of the lane so here ah the vehicle will come in the opposite direction so this is our region of Interest so it will mask out this Lane line or any other lines or distortions which we have in this picture we are going to just mask them and we are going to just concentrate on this triangle so let's do this first so first of all we are going to find out the shape of the image so I am going to just print and then we are going to just say image Dot what shape and also we are going to ah just Define the height and width of the image so I am going to just say Okay so let's print this value and let's see what happens so what's at 0 and what's at 1 so you can see it prints seven zero four as our height and one two seven nine as the width so this is what I am just taking from this image shape method so it is going to return this kind of Tuple so at zeroth index there will be height and at the first index there will be ah the width and as I said it starts from 0 to 704 from top to bottom and horizontally it goes from 0 to 1279 from the left hand side to the right hand side right so once we have the width and height we can Define our region of Interest so we are going to Define a variable called region of Interest vertices and here we are going to provide some values so we are going to provide three points which will be the three points of our region of Interest so as I said that our region of Interest we want is this point which is the left bottom corner this point which is the right bottom corner and somewhere in the middle of this image so here so in the image because the vertical height starts from zero so I am going to just say 0 comma height and the second point will be the half of the width and half of the height which will be the center of the image so I am going to just say width divided by 2 comma height divided by ah 2 and this will be inside these ah parenthesis and the third point will be the next corner so this will be width and then the height so let us try to see these points in our mat plot live window so the first point here is 0 comma 704 which is this point the second point is somewhere here which is the half of the height and half of the width and the third point will be here which is width comma height which is 700 comma 1279 which is this one right so this will be our region of Interest now we are going to Define one function to mask every other thing other than our region of Interest so I will just Define this function def region of interest and this is going to take two parameter first will be the image and second will be the vertices so vertices and inside this function let me just ah minimize this terminal also so you can see the function so inside this function in the first step we are going to define a blank Matrix that matches the image height and the width so this will be the easiest step we are going to Define a variable called mask and we are going to use NP dot zeros like method which is going to take one parameter which will be our image Matrix now in the next step we are going to just retrieve the number of color channels from the image this will be the easy step also so Channel count and then we are going to just say image dot shape and at the second index we are going to find out the channels because we have seen that image.shape is going to give you three values height width and the channel count so this channel count is come coming from this index now in The Next Step what we are going to do is we are going to create a match color with the same color Channel counts so I'm going to just say match underscore mask underscore color this will be our variable name and then we are going to just take 255 comma and then multiply it by the channel count so let's multiply it by the channel count so this is going to create a match color with the same color Channel counts now in the next step we are going to fill inside the polygon using the fill poly method because we have our region of interest and we want to mask every other thing other than our region of Interest so we are going to just say CV2 dot fill poly which is going to takes few arguments first will be our mask second will be the vertices which we are providing using the second argument and the third argument will be our match mask color variable so we are going to pass this variable as the third argument and in the next step we are going to just return the image only where the mask pixel matches so I'm going to just say masked underscore image is equal to CV 2 dot bit wise and so we are going to just apply bitwise and using this bitwise and Method and the first argument here will be the image and the second argument is the mask which we got using this zeros like method right and in the last step we are going to just return this so I am going to just write return this masked image and that's it so we are going to just apply our region of interest on the image using this method and then we are going to just get our image which contains region of interest and any other thing will be masked so now it's time to use this method so we are going to just use this method using this variable I am going to just Define a variable called let us say cropped image or masked image whatever you want to write here so let's say cropped underscore image and then we are going to just use this function which is region of Interest function which takes this argument so because we have already read our image in the image variable we are going to pass this as the first argument and the vertices is simply our region of Interest variable so this region of Interest variable we are going to pass using numpy dot array method and let us split this line so we will be able to see what I am doing inside this NP dot array method so first of all the first argument will be our region of Interest variable which is this one region of Interest vertices so in the square bracket we are going to just pass a region of Interest vertices and the second argument here will be NP dot in 32 so NP dot int 32 and now we are going to just show this image using our mat plot lib window so let's run this code and let's see what happens when it runs and there is a problem here so let's see what the problem is so you can see uh this problem is coming from this line and most probably this region of Interest has some problem so you can see we have passed this first element as the Tuple second element as the Tuple and the third element also we need to pass as a tuple and that is why it's giving us the problem so I have just fixed it and let's see what happens when we run this code again and you can see our image is now masked with our region of Interest so we have defined our region of interest from this point to this point to this point so now we have only this region of Interest so we will be able to easily find out this Lane line and this Lane line inside our region of interest and any other distraction will be masked now right so this is the first step which we have achieved which is masking our image and just applying our region of interest on the image in the next step we are going to see how we can apply the edge detection and find out the lane lines on the image in the last video we have started our simple project of detecting Lane lines on the road using opencv and we came to the point where we were able to Define our region of interest and our result was looking like this so let me run this project so we have defined this region of interest and now the only thing which remains here is to detect these Lane lines so we will once again go to the next step and the next step will be to find out the edges and then we are going to apply half line transform to draw the lines so first thing first what I am going to do is I'm going to just move this region of Interest function which we have created in the last video on the top of this script so we can see ah this other code clearly this code which we have written so we have this region of Interest function which we have created then we have just created this region of Interest variable and then we just used our region of Interest function using this region of Interest vertices variable so the next step as I said is to find out the edges and for that we need to First convert our image into our grayscale image so I'm going to just say gray image and then we all know how to find out the gray scale image out of an image so we just need to write CVT color and the source is our cropped image so we are going to pass our cropped image and then we are going to just convert it into a grayscale image using CV2 Dot color underscore rgb2 gray so let's do this so once we got our grayscale image we can apply Kenny H detection on this image so I am going to just write Kenny image and then I'm going to just say CV2 dot ah Kenny which is the function which we want to use which takes few parameter first parameter will be our grayscale image the second parameter will be the first threshold and the second threshold so generally we are going to take here 100 as the first threshold and 200 as the second threshold now in the next step we are going to just uh display this image on our matplotlib window and let's see what happens once ah we apply the scanning Edge detection method on the image so now you can see this result which detects all the edges and here you can see the lane line edges are detected but there is one more thing here which is the edges of our region of Interest are also detected so how to solve this how to remove these edges because these edges doesn't interest us the interesting ah edges here in this image for us are these edges which are of the lanes Road planes right so to solve this problem we can apply the scanning Edge detection before we find out the region of Interest so I am going to just copy this code and paste it just before we apply this region of Interest method which we have created in the last video so now in our Kenny Edge detection ah we will pass the gray scale image but here instead of this ah cropped image which we were getting in the last step from this variable we directly are going to pass our image which we have read using the I am read method right so let me just ah remove all these line breaks so you will be able to see the code at once so here you can see I have directly passed now this image variable to the CVT color or method so we get the grayscale image of the original image and then we apply the Kenny Edge detection on the original image and then we are applying the region of Interest method which we have created in the last video now because we are applying this region of Interest method on the grayscale image or the edge detected image therefore we do not need this channel here so we can comment out this code which was kind counting out the number of channels and for the grayscale image and the Kenny has detected the image we just take this match mask color as 255 because it's only one color right we don't need any color channels here because we are just passing the grayscale image which has only one color so that's why we don't need any channel because there will be only one channel and that is why I have commented this code and the value of the match mask color will be 255 now once you do that let's try to run this code and let's see what happens ah once again we need to load the cropped image not the Kenny image so just replace this variable here in the IM show method and let's run this code once again so you can see now uh there is some mistake here because we were expecting the edge detected image and we are getting this image so let's see what's the problem is so the problem I see here is because we have applied this region of interest on the original image which we do not want now we want to apply this ah region of interest on the Kenny as detected image so we have to pass as the first variable of the region of Interest method the scanning Edge detected image not the original image right so once again for you you can see this code region of Interest method and all this code at a one glance let's run the code and let's see what happens so now we get the better result so we have these edges which are detected by the Kenny Edge detection for only the lane lines inside our region of interest and now it will be easier to draw the lines on these edges which we have detected so the next step will be to draw the lines on these edges using the half line transform so we have in the previous videos have already seen how to use the half line transform so I'm not going to go into the details so let's just directly jump into using that half line transform so what we are going to do is in the next line after we have got our cropped image we are going to just Define a variable called lines and we are going to use this half line transform probabilistic half line transform method so here CB2 dot half line transform and this will be this method which takes few argument first argument will be the image so I am going to pass this cropped image here the second argument here will be the value of rho so let's provide this row value variable value which will be 6 in our case then in the next parameter we have to pass the value of theta and Theta will be ah equal to n p dot Pi which is the method inside the numpy Library so NP dot pi divided by 60 so I am going to pass here divided by 60 then the next parameter here will be the threshold so the threshold value we are going to provide here will be 160 the next parameter here will be lines which is equal to none by default so I am going to provide this variable lines is equal to and then ah we are going to pass the empty numpy array so I am going to just say ah num pi and P dot array and then we are going to just pass the blank square bracket here the next two parameters will be the Min line length so let's Pro provide this Min line length and let us say we want 40 as the minimum line length and the max line Gap so let's provide that also MAX Line Gap and this will be let us say initial value for that will be 25. so now after applying this half line transform you know that it is going to return the line Vector of all the lines which are detected inside our image which we have provided as the source here so if you don't know what are these parameters which I am using here you can see my last videos about probabilistic half line transform and you will be able to know what they actually mean now once we got our line vectors then we can draw the lines easily and for that we are going to Define our next function which is to draw the lines so I am going to just Define this function with the name draw the lines for example and it's going to take few parameters so let's pass these parameters first parameter will be the image or the original image the second parameter will be the line vectors which we have found out and that's it so there are these two ah parameters we are going to pass here now ah inside this function what we are going to do is we are going to first of all copy our image so I am going to just say I'm G is equal to NP Dot copy and then we are going to just make a copy of the image variable which we are providing and then or you can write here copy image whatever I'm just just reassigning this copy the image to the same variable but you can define a new variable here for the copied image also now in the next line we are going to create a blank image that matches the original image size so the dimension should be equal so for this we can just say ah line image and then we are going to just say NP dot zeros inside these parentheses we are going to provide the shape of our image right so you can provide the shape of our image using the image variable so first of all it's going to take the height and then the width and then the number of channels so because we know that this is a colored image which we are working with so we are going to just say IMG dot shape and we all know that the zeroth index parameter here will be the height the second parameter I am g dot shape the value add the first index will be the width and the number of channel for the colored image are always three so we are going to provide the third parameter as three here so this is in the form of Tuple I am providing and the next parameter here will be the data type or D type so let's provide that D type is equal to num Pi dot U in it okay so U and eighth not uninted you into it so this will be ah the second parameter so once we have this image which is exactly same as the size of our original image we are going to loop around these line vectors and then we are going to draw the lines right so let's loop around these uh line vectors and draw all these lines which were found so for that we are going to use the for Loop and then we are going to say line in lines and these this lines variable is coming from this lines variable so we are going to use this draw lines function and we are going to pass this lines Vector as the second parameter here so this is how ah this line variable is coming here so now inside this for Loop we are going to ah just Define one more for Loop because this line is going to give us four parameters which is the coordinates of the first point in the line and the coordinates of the second point in the line so we are going to just once again say for X1 which is the first coordinate of the first point and the Y 1 and then similarly x 2 and the Y 2 so this will be ah the line coordinate in the line ah which we got from the line vector and then inside this for Loop we are going to draw a line and drawing line is really easy by using CV2 dot line method which takes a few parameters as you already know the image and then the second parameter is the coordinate of the first point which we already have using this iteration which is x 1 comma y1 and third parameter here will be the coordinates of the second point x 2 comma Y 2 and then you can provide the color and thickness so let's provide this color so the color here I'm going to take let's say 0 comma 255 comma 0 you can take any color here and the thickness so the thickness here I'm going to take is ah let's provide this parameter thickness is equal to 3 okay so this is the thickness of the line which we want to draw and here I think this blank image should uh be uh given because we want to draw the line on the blank image and then merge it with the original image so here we have to provide this line image or you can say this is the blank image which is more appropriate in this case so we want to draw the line on the blank image which is of the same size of the original image and now once we draw these lines on the blank image we can merge this blank image and the original image which will give us the a line which are drawn on the original image so outside this for Loop we are going to merge the image with the lines into the original image so our original image is the image ah itself so image variable is the our original image and then we are going to just say CB 2 dot add weighted this function also we have seen in the last videos and this is the function which we use to merge two images with some weights so the first parameter here will be image now the second parameter here will be the value of alpha so which we are going to give here 8 this is like a weight to an image which we want to provide and then the third parameter here will be the second image so we want to merge the blank image with the original image the fourth parameter is the value of beta so this value we are going to take as one and the last value will be of gamma so gamma we are going to take as 0.0 here okay so this add weighted also we have seen in the last video how to use it so I am not going into the details and at last once we have the lines on the image then we are going to Simply return it so let's uh return this image IMG so once we have this function we are going to call this function after applying the half line transform method which is the probabilistic half line transform so here in the next line we are going to just Define a variable called image with lines let us say is equal to our method which is draw the lines method the first argument is the original image so we are going to pass the original image the second argument is the line Vector which we got from this method right so the original image and the line Vector variable which we got here at last we are going to just see what is the result which we got after applying this draw the lines method on the original image so let us run this code and let's see what happens so now you can see let me just maximize it you can see this line is drawn on our image so this is the first line and this is the second line so we got the result which we wanted if you want to change the thickness or the color of this line on the image you can just change it using this draw lines method so this is the line and thickness parameter so for example I want to change this to 255 here some different color and the thickness let us say 4 and I'm going to run this code and now you can see this yellow color here right so you can change the thickness and the color using uh this method so let's say for now we want the the red color so we are going to go with this red color on the lane lines so this is what we wanted to achieve we wanted to draw the lane lines on these lanes and we have achieved this in the next video we are going to see how we can apply the same concept on a live video or on a video of this road for example so for example this car is running on the road and we want to continuously draw these lines on the lane lines how we can achieve this using opencv we are going to see in the next video in the last two videos we have seen how we can detect the lane lines on the road using opencv now till now we have only worked with this image and in this video we are going to try to apply what we have written not on an image but with the video frames but you have already learned in the previous videos that a video frame is like an image so a video contains many number of frames so if we apply the same technique on each frame we will be able to detect these Lane lines on the video frames also so let's apply that concept on our script what we have till now so right now I have added this test dot MP4 video inside my project so let me show you how it looks like so our video looks like this so we are going to apply all these concept which we have applied on an image on this video so let's get started so I hope you have this code which we have written in the last two videos the only thing we need to do here is we need to read the video instead of an image and then apply those Concepts on the frame instead of an image so we till now have two functions region of Interest draw the line and we have this code so this all code we are going to enclose inside the function so that it will be easier to apply all this code on the video frames now as you already know that this will not be used because we are reading the videos so we don't need to read the image obviously so we are going to comment these two lines out so we do not need to convert gr to RGB because we are going to use this native CV2 Library not the mat plot lab library for which we have converted this BGR to RGB image so now we are going to define a function so let us Define this function and I am going to name this function as process and it is going to take an argument which will be the image argument and all this code which is under this which we have written in the last video we are going to enclose this code inside this process function we don't need these two lines because we are not going to use matplotlib for processing this video so I am going to remove these two lines and I'm going to just give a space here for this code so it can be enclosed inside this function now at last or at the end of this function we are going to just return this image with lines so we are going to return this image with lines using this process function that means on every frame we are going to draw the lines and return it using this process function next we are going to read the video using the video capture functions so I am going to declare a variable cap is equal to CV2 dot video capture and then we are going to just pass one argument which will be our video file which is test dot MP4 in our case so test dot MP4 and then once we have this video we are going to check if the video frame is available using the while loop so let's use this while loop and we are going to check if cap dot is open is valid or not so is opened and this function is going to return the Boolean value so if this video frame is still available is going to return true and whatever we write inside the while loop is going to be executed now in the next line we are going to just read every frame so we all know from our previous videos that this cap dot read returns to a result or two variables one is re T and the other is the frame and we are going to just say cap dot read and then we are going to apply our process function on this Frame so we are going to once again take this Frame variable and we are going to overwrite this frame with the lines on the frames so this we are going to get from our process functions let's call the process function and pass the frame variable inside it okay so this Frame is going to go to the process function it is going to process everything and then the final result which we get is going to be saved once again into the frame variable with the actual lines on the frame in the next line we are going to just show our result using CB2 dot I am show method and we are going to just pass the frame variable here in the next line we are going to just write the code for the quitting from this Loop so we are going to just say if c v 2 dot weight key is going to be 1 and then we are going to apply the end operator and then write 0xff for the cross platform functionality and then we are going to just say is equal to ord so whenever somebody presses the queue key then we are going to exit from this Loop and then in the next line we are going to just say break so break out of the loop the last two line in the last two line outside this Loop we are going to just ah call the release function on the cap variable and we are going to destroy all the windows in the CV2 so we are going to just say destroy all windows that's it hopefully this is going to work so let's uh just run this script once again and let's see what happens and here we got the error and it's coming from this line which is CV2 dot I am sure we forgot to give the first argument here which will be the name of this window we are going to just say frame here and let's run this script once again and let's see what happens and you can see on this video on this Lane line our lines are drawn right so this is the result which we were expecting we can improve this result by adjusting few variables so we are going to first of all press Q to quit and let's change some of these values here in the half line transform so we are going to just say that the max line Gap is going to be 100 we are going to reduce the threshold value to 50 and row value to 2 okay inside this half lines P method and let's run this code so let's see what result we get this is also OK let us improve it little bit more in the Kenny Edge detection we can reduce this threshold value here to 120 the second threshold value and let's run this code once again and now we get the better result so the problem might be the edge detection so we have reduced our second threshold and now we get the better result you can see on this Middle Lane the lines are drawn clearly so this is how you can write a simple script to detect Lane lines on the road I hope you have enjoyed this video and I will see you in the next video we have already seen how to use half line transform to detect lines in opencv in this video we are going to see how we can use half circle transform to detect circles in an image now as you can see here I have this ah small example which loads an image and shows it into the I am show window so let me run this code and let me show you how this image looks like so you can see there are so many Smarties here inside this image and all the smarties are of circle form right they are not of the perfect circles but they are in the form of circles and we want to detect all these circles forms inside the image we can use half circle transform for that so let's see how we can use this half circle transform to detect the circles in the image so a circle is represented mathematically by this equation which you see on your screen so here X Center and the Y Center are the coordinates of the center and R here is the radius of the circle so if you know these three parameters then you can draw a circle so this coordinates of the circle and the radius of the circle we need to detect so now let's see how half circle method is applied using opencv so you might observe here that I have created a copy of this original image which I have read using this I am a read method in The Next Step I'm going to just convert this image into a grayscale image so I'm going to just write the gray is equal to CV dot CVT color which is going to take two parameters first is the source and second is the method so we are going to convert the color BGR to gray now in The Next Step because our half circle method works better with the Blurred images so we are going to uh create this blurred image using median blur so I'm going to just say gray so we are going to override this gray variable with CV2 dot median blur which is going to take a few arguments first is the image itself so we are going to pass gray here and the second is the case size or the kernel size so we are going to initially provide the kernel size of 5 here now we are going to apply our half circles uh method so I am going to declare this circles variable and then I'm going to just call this method which is called CV dot half circles method so this is the method and you can see it takes few parameters so we are going to give these parameters one by one first is the obvious one which is the image so we are going to provide the gray scale image here which is already blurred so the second parameter here is the method which we want to use currently the only implemented method is half gradient method so the choice is very simple here we are going to just provide this CV dot half gradient method the third parameter here will be DP DP is the inverse ratio of accumulator resolution to the image resolution so for example when DP is equal to 1 the accumulator has the same resolution as the input image and if the DP is equal to 2 then accumulator has the half as big as width and the height so we are going to take this DP value as 1 the next parameter here will be Min dist it is the minimum distance between the center of the detected circles okay so here we are going to give initially the value of 20 and later we will adjust this value if the circles are very near to each other the next parameter which we are going to give here is the value of parameter 1 and parameter 2 or param 1 or param 2. the param 1 is the first method specific parameter in case of half gradient it is higher threshold of the two passed to the Kenny Edge detector param 2 is the second method specific to the method which we have provided here which is the half gradient method it is the accumulator threshold for the circle centers at the detection stage so we are going to provide the value of the param 1 and param 2 here so let's start with the param one value and we are going to provide param 1 value is equal to 50 and param 2 value is equal to ah let us say 30. so those param 1 and param 2 parameters are specific to this method which we are using the next parameter which we are going to pass here is the Min radius and the Min radius is the minimum Circle radius and we are going to just start with the zero so we are going to say that anything which is greater than 0 we are going to just draw it and then we are going to provide the max radius if this Max radius is greater than or equal to 0 it uses the maximum image Dimension if it's only greater than 0 it returns Center without finding the radius so this also we are going to start with 0 let me just break this function so you can see all the parameters here so this half circle method is going to give us the circle Vector which we can iterate upon but first of all we need to convert those Circle parameters which we got using this circle Vehicles variable that is x and y coordinate and the radius into an integer so to do that we are going to just declare a parameter called detected circles and then we are going to use numpy to convert them into an integer so I am going to just say NP dot u in 16 and then in the parenthesis I'm going to just use NP dot around and we are going to pass our circles parameter which we got using the half circles method now in the next step we are going to iterate over those detected circles so we are going to just say ah for and because this circle Vector is going to give us x y and the radius we can directly just extract those values so we are going to just say x comma y comma R and then in our detected circles and those circles will be at this index so 0 comma colon and then inside this for Loop we are going to first of all draw the circle and also we are going to draw the center so to draw the circle we already know that we have this circle method available which takes few parameter first is the image so we are going to pass us the copy of this image here so let's pass this copy which is output the second argument here will be the center which are the coordinate of the center which we already got in the form of X comma y so we will give them uh in the form of Tuple the third argument is the radius so radius is extracted in the r parameter here so we are going to pass the radius here and then the next parameter will be the color so let's start with let's say green color and then the thickness so we are going to give the thickness of let's say 3 here similarly when we use the same Circle method and we want to draw the center then we know that this is the center so these are the coordinates of the center and if the radius is very small let us say 2 then it is going to just draw a small point right so that is why I have given very small value for example 2 here so it is going to just draw a very small circle which will look like a DOT on the circle that's why this value is very small and we are going to just say that this will be also 3 and let us change the color of this dot let us say this will be this color okay so we are just drawing those circle on the copy of the image which is called the output so let's run this code and let's see what happens when we run this code and you can see this dot is first of all drawn on each circle which is detected which is in the form of yellow and also in the form of green all the circles are drawn so you can see this circle is drawn so every ah circular shape is ah you know enclosed by the detected Circle we also strangely detected this circle somehow because opencv think that this is also a circle I have one more image which is shapes dot jpg so we are going to just ah see that also so I am going to just say shapes dot j p g let me show you this image first of all so it looks like this so it has only one Circle and some other shapes right so we are going to just run this code once again and you can see it just detect the circle and it just draws a small dot on the center and all the other shapes are undetected so this is how you can detect the circles inside an image using half circle transform in this video we are going to discuss about the basics of phase detection using har feature based Cascade classifiers so object detection using har feature based Cascade classifiers is an effective object detection method proposed by Paul Viola and Michael Jones in their paper now har feature based Cascades classifier is a machine learning based approach where a Cascade function is trained for a lot of positive and negative images now what are these positive and negative images so first a classifier is trained with few hundred sample views of particular object that is a face or a car or any other object that is called a positive example so whatever you want to detect if you train your classifier with those kind of values so for example if you want to detect face then you need to train your classifier with the number of images which contain faces so these are called the positive images which contains the object which you want to detect similarly we want the classifier to train with the negative images that means the images which doesn't contain the object which you want to detect so in our case for example we want to detect the face then the image which doesn't contain the face then it is called the negative image and if the image contains phase or number of faces then it is called the positive image and after a classifier is trained it can be applied to a region of interest in an input image and the classifier outputs a 1 if the region is likely to show the object or zero otherwise so let's see how we can use hard Cascade detection in opencv so opencv comes with a trainer as well as a detector so if you want to train your classifier for any object for example a watch or a car or train or anything then you can use this classifier also on open CVS GitHub page you can find some trained classifier XML files so let me show you these classifiers on the open series GitHub page so here is the opencv repository and inside this repository you can see this data folder and then go to Har Cascades I will just share the link with you in the description so you can directly navigate to this website and this location and you can see plenty of trained classifiers are available inside this report repository so for our example we want to detect the face so we are going to use this trained classifier which is called heart crack Cascade underscore frontal phase underscore default dot XML file so you just need to open this file and then download it you can just click on the raw icon here this button and once this raw file is open you can just right click and save it on your computer so you can just say save page as and then you can just save this inside your opencb project so I have already saved this file inside my opencv project you can see this file here which is a XML file which I have downloaded using this repository so as you can see here I have this code which is the minimal code to a load an image and show it using opencv window now in The Next Step what I am going to do is before this reading we are going to just Define our classifiers so because it's a phase classifier I'm going to name my variable as face Cascade and then in opencv there is a method called so I am going to just call this method and there is a method called Cascade classifier so this is this method called Cascade classifier where you can provide your classifier name which is the XML file so just provide your trained classifier file name in our case it's hard Cascade underscore frontal face underscore default dot XML so once we have our classifier we read the image and then because this class if I will work with the grayscale images we are going to convert our image into a grayscale image and it will be really easy to convert our image to a grayscale image now once we have our grayscale image the next step is to detect the faces inside this image so for that we are going to declare this variable let us say faces and then we are going to use this result which we got using this Cascade classifier and then we can call a method called detect multi scale so we are going to ah just call this method which takes few argument first is the image so we are going to provide our gray scale image here and the second argument we are going to use here will be the scale factor so the scale factor parameter specifies how much the image size is reduced at each image scale so to start with we are going to provide a 1.1 ah value here and then the next parameter which we are going to provide here will be the Min neighbors parameter so Min neighbor's parameter is going to specify how many neighbors each candidate rectangle should have to retain it so we are going to provide this value 4 here to start with and if it does not give us the proper result we are going to change it and the last step here will be to iterate over all the faces which we have detected and then draw a rectangle on them so this phase variable will be the vector of rectangle where each rectangle contains the detected object in our case this will be the detected phase so the rectangle may be partially outside the original image if it's on the corner so the if the object or the face is on the corner then this rectangle may be little bit outside the original image so we are going to iterate over this faces uh object and here we are going to get the parameter X comma y comma W comma H which means the values of X and Y and the width and height of the rectangle of the object in our case this is the faces right so we got all the four parameters for drawing the rectangle and then we can just call CV2 dot rectangle method to draw the rectangles the first parameter here will be the image the second parameter will be the point one which will be X comma Y which we got using this faces vector and then we need to give the second point which will be X plus W comma y plus height okay and then the next two parameters are the color and the thickness so we are going to give the color 255 comma 0 comma 0 here and the thickness to start with we are going to give 3 here that's it so its this simple to detect faces inside the images using har Cascade classifiers so now I am going to run this this code and let's see what happens so you can see this is the face so this is how you can detect the face or a multiple number of faces inside an image let's try to detect the face inside a video so I'm going to just close this window and now we are going to try to detect the face inside a video so this will be nothing different than this approach we just need to apply this approach on each and every single frame so instead of ah this code we are going to use the video capture method to capture the video so you can see I have this test dot MP4 video here so we are going to define a cap variable is equal to CB2 dot video capture and then in the parenthesis we are going to provide a the test dot mp4 file here or if you have the camera you can provide 0 here as the parameter and then all this code we are going to just enclose inside a while loop so we are going to just say that while cap dot is opened so if cap dot is opened is going to give us ah True Value then we are going to read the frame so ah underscore let us say ah the parameter name will be IMG in this case also normally we take the variable name frame here because we are reading each and every frame and then I am going to just say cap dot read okay so cap dot read this means we are reading every frame and let us enclose this code also inside this while loop so I am going to just provide a little space here so basically we are getting every frame and then applying the same procedure on each and every frame and at last outside our while loop we are going to release our cap so we are going to just say cap dot release and here instead of using this CB2 dot weight key you are going to provide a condition if c v two dot weight key and in the parenthesis we are going to provide one and zero x f f is equal to or D and we are going to listen for the key ah Q so if somebody presses the Key Queue then we are going to break out of this while loop so let's run this script and let's see what happens when we run the same script on a video so this is the wind video and this is in this video so you can see in this video the face is detected in real time in the real live video so this is how you can use har based Cascade classifiers to detect faces or any other object inside an image in the last video we have seen the basics of face detection using har feature based Cascade classifiers in this video we are going to extend our code to detect eyes using the same har Cascade classifier so for that first of all you need to download the pretrained cross classifier for the eyes from the same source which I have shown you last time also which is the GitHub repository of opencv again I'm going to give you this link in the description so you can directly come to this page and this time we are going to download this XML file with the name Har Cascade I underscore tree underscore I glass dot XML file so this is the pretrained classifier for detecting eyes so you can just click on draw and then save it as this same file name in your project okay so I have already downloaded this XML file you can see here our Cascade I underscore tree underscore eyeglass dot XML file and now we are ready to write our code so this is the code which we have written last time so if you don't know how this code works you can see the last video I'm going to just extend this code to detect eyes so first thing first we need to create the Cascade classifier for the eyes so instead of phase Cascade we are going to name it as I Cascade and this file name will be the file which we have downloaded which is I underscore tree underscore iglass.xml file so once we have our classifier then in the last video we have already seen how to detect faces so our region of interest will be the phase this time because the eyes will not be present outside the phase right so eyes will always be present inside the face so our region of interest will be the phase and phase we have already detected last time so this phase will be now our region of Interest so go inside this for Loop where we are iterating over this phase variable and then we are going to create our Roi so I am going to create this variable which is called Roi underscore gray and this will be the original grayscale ah image which we have created here but we just want the face out of this image so we can just index the space using Y colon Y plus h comma X colon X plus W which is the width so this line is going to give us the gray scale region of interest but we also want the colored image also so we are going to just say Roi color which will be the colored Roi and here instead of gray we are going to take the direct image which will be before we have converted this BGR image to the grayscale image so we have the colored Roi and the grayscale ROI once we have this we will follow the same ah concept which we have applied for detecting the faces so so we are going to use this detect multiscale method so I am going to just write eyes is equal to because we already have our I Cascade which is a classifier so we are going to use this variable and then use this method called detect multiscale and then we are going to Simply ah pass our Roi gray which we got using the faces now we are going to iterate over those eyes so inside this for Loop we are going to create one more for Loop to iterate over all the eyes which are found on the face so far and then this will be e x comma e y comma e w comma e h for x y coordinate and the width and height now we will just say in I's and then we are going to just draw this rectangle which is also very simple CB2 dot rectangle and then we are going to pass our image first of all which will be our colored Roi image which is this one so here we will pass this Roi color and then the first point in the rectangle which will be ah e x and e y so I am going to just say e x comma e y and the second point will be E X Plus e w which is X Plus width so we are going to just write this E X Plus e w comma e y plus e h which is the y coordinate and the height the next parameter will be the color so let's provide the color let us say this will be 0 comma 255 comma 0 and then the next parameter will be the width so let us say the width we want here is five so that's it so hopefully this code is going to work out of the box we don't need to do anything else we just need to Define our classifier and then we just need to uh use this detect multiscale method to detect the eyes and then we just need to draw the rectangle on all the eyes which are detected so let's run this code and let's see what happens so we are going to see you can see eyes are detected but there is some problem because something is wrong so I'm going to just quit this script and see what's going wrong here so you can see this should be x e x comma e y and then our problem will be solved hopefully so I'm going to run this code once again and you can see the eyes are properly detected so this is how you can detect eyes in the face using opencv and Har Cascade classifiers in this video we are going to try to understand how we can find out the corners inside an image using a method called Harris Corner detection now first of all what are corners so Corners are the region in the image with large variation in intensity in all the direction now this Harris Corner detector was first introduced by Chris Harris and Mike Stephens in their paper in 1988. now detecting Corners using Harris Corner detector contains three main steps so the first step is to determine which Windows produces very large variation in intensity when we move in the X Direction and the y direction now what are windows here so windows in this case means that let us say we want to just find out this corner here so windows will be your small box here and then you check for the intensity when you move in the vertical Direction and also in the horizontal Direction so you check for the change or large variation in the intensity when you move in the X Direction and when you move in the y direction in the second step with each such window which we found a score R is computed so this r value which is computed is going to give us the estimate or give us an idea about where this corner is located depending upon the value of R and in the third step after applying a threshold to this score the important Corners are selected and marked so let me explain you all these steps one by one what do I mean by detecting the windows and calculating the value of R's let's see step by step so as I said in the first step we determine which Windows produces very large variation in the intensity in the X Direction and in the y direction so let us say a window or a center is located at the position X comma Y and let's say the intensity of the pixel at this location is i x comma y so if this window is slightly shifted to a new location and let us say this displacement is U comma V then the intensity of the pixel at this location will be X Plus U and Y plus b because our displacement is U comma V so we are just adding it to the x value and the Y value and hence the difference between the shifted intensity and the original intensity will be the difference in the intensities of the windows shift so for a corner this difference will be very large and that's how we detect the corners using this Harris Corner detection method now as you can see here this value will be given in the e u comma V format so we have to maximize this function for the corner detection and this we can achieve by applying a Taylor expansion to this equation which is given here and by using some mathematical steps so I'm not going to go deep into the mathematical steps but after applying the Taylor expansion you will get this kind of approximate value where m is equal to this value and here in this equation i x and I Y are the image derivatives in the X and Y Direction respectively so this can be easily found out using the CV dot Sobel method in opencv now comes the second step and in this step we find out or calculate the score for r so this R is equal to this value and the M we have already ah seen how we can get this m value in the first step right so in this equation d e t m is equal to Lambda 1 multiplied by Lambda 2 and Trace m is equal to Lambda 1 plus Lambda 2 where Lambda 1 and Lambda 2 are the eigen values of M so again if you want to go into the details you can refer to some book or you can go to the Wikipedia page to learn more about this equation so once we got the value of R then based upon the value of R we can make some decision and this we can do in the third step so if the value of R is very small that means the value of Lambda 1 and Lambda 2 are also very small and we can conclude that the region is a flat region and not the corner if the value of R is less than 0 that means Lambda 1 is very large in comparison to Lambda 2 or vice versa and that means it's an edge and not the corner and if the value of R is large which happens when Lambda 1 and Lambda 2 are large and this means that this region is a corner so if the value of R is very large that means the region is a corner and that's how Harris Corner detector detects if it's a corner or a edge or a flat area so this was the theory about Harris Corner detector let's see how we can use this Harris Corner detection concept inside opencv using our python code so I have this script already written here so just import CV2 and numpy and then we are reading this image called cross board underscore image.png using I'm read method and after we read this image I am just showing the original image so we have the original image and the output at the end to compare now in the next step I am converting this image into a grayscale image to get the better results and because this CV2 dot Corner Harris method takes the grayscale image in the float 32 format that's why we need to convert our image into float 32 format so that's why we are using numpy dot float32 to convert this image into a floating Point values because our Corner Harris method which we are going to use in the next step is going to take this kind of value and not the value which comes directly from the conversion of this image to the grayscale image so this step is necessary for the Harris Corner method and in the next ah step we are just applying the CV2 dot Corner Harris method which takes few arguments first is our image in the floating point so this we have passed and the second parameter here is called the block size so here I have given the value 2 here so block size means the window in the first step so we have seen we have to define the window right so for example we Define this block size 2 that means neighborhood size is equal to 2 that means for each pixel value block size multiplied by block size that means two by two neighborhood is considered the next parameter here is called the K size and it's the aperture parameter for the Sobel operation and then we have the next parameter here and this next parameter is called the K which is the Harris detector free parameter in the equation so after applying this Harris card method to our image we get this destination image and to get the better result we need to dilate this result so we apply CV2 dot dialect method on our image which we get using the Harris corner so this image are marked through the dilated corners and then in the next step we are reverting back to the original image with optimal threshold value and we are just just marking all our Corners with this color so basically we want to mark all the corners with the red color here and in the next step we are just showing our result in the I am show window and at last we are destroying all the windows so let's run this code and let's see what happens when we run this code and we will see the results so you can see this is the original image which have so many corners and all the corners are detected and it's marked with this red color here so this is how you can find out and Mark all the corners using Harris Corner detection in opencv in the last video we have seen how we can use Harris Corner detector in order to find out the corners inside an image in this video I am going to show you how you can use sheet tomasi Corner detector method to detect the corners inside an image so in late 1994 JC and C tomasi made a small modification in the Harris Corner detector method in their paper which was called good features to track so this C tomasi method is similar to Harris Corner detector apart from the way the score of R is calculated which we have seen in the last video so this C tomasi method gives us better result in comparison to Harris Corner detector and also when you use this C tomasi method we can find the top and Corners which means we can provide the number of Corners we want and this might be useful in cases where we don't want to detect all the corners inside an image so let's see in the code how we can implement this C tomasi Corner detector in opencv so here I have already written all the code so let me explain you all the lines of the code one by one so as you can see here I'm just importing the libraries in the first two line and in the next line I am just reading the image using I am read method and then I'm converting this image into a grayscale image using this CBT color method so I'm converting this image from BGR to grayscale image now as I said the paper which was published by She and tomasi was named good features to track that's why in opencv this method is also called good features to track so here in this line we are just using this method CV dot good features to track which takes few arguments so first argument here is our input image which is a grayscale image which we are providing as the first parameter the second parameter is the maximum number of Corners so here we can limit the number of Corners we want to detect so for example I have given 25 here that means we just want to detect 25 corners and if there are more than 25 Corners which are present in the image they will not be shown so this value means maximum number of Corners to return and if there are more corners then the corners found then the strongest Corners will be returned right now the third parameter here is called the quality level so this is the parameter characterizing the minimal expected quality of the image Corner the next parameter here is the Min distance which is the minimum possible euclidean distance between the returned Corners so I have taken ah 10 here as the minimum distance and 0.01 as the quality level now once all the corners are detected using this good features to track method we convert those Corners into the integer values and here int 0 is a mere Alias for INT 64 and once all the corners are detected we iterate over all the corners and then we find out the value of X and Y using this I and then it's easier to just draw the circles over these values using the CV dot Circle method so this CV dot Circle method takes few arguments first is the input image so the second parameter is the center the third parameter is the radius of the circle which we want to provide the fourth parameter is the color we want to provide and the fifth parameter is the thickness and if it's minus one that means we want to fill the color inside that Circle and at last once all the circles are drawn on the corner which are detected then we are just showing this using I am show method so let's see how this works in the case of method on an image so I am going to run this code so you can see all the corners inside this image are detected and because we have just provided this number 25 here so maximum number of Corners which will be detected here will be 25 and rest of them will not be shown so if we increase the value of the maximum number of Corners let's increase it to 100 let us say so I am going to just increase it to the value 100 you will see more number of circles are drawn on an image now let us compare the result of the Harris Corner detector and C tomasi Corner detector so on the left hand side you can see the original image and this middle image shows the Harris Corner detector method and you can see all the corners are detected using Harris Corner detector and using the sheet omasi Corner detector it gives us better result and we can control the number of Corners we want to detect and you can see all the important Corners are detected using Chi tomasi Corner detector in a better way so that's how she tomasi Corner detector Works in this video we are going to see how to use background subtraction method in opencv so first of all what is background subtraction so background subtraction is common and widely used technique for generating the foreground mask which is also known as the binary image containing the pixels belonging to the moving object of a scene when these images are captured using a static camera and as the name suggests background subtraction calculates the foreground mask performing the subtraction between the current frame and the background model containing the static part of the scene so for example the background subtraction method can be used in the case of visitor counter where you have a static camera capturing the number of visitors entering or leaving the room or you have a traffic camera which wants to count the the various telematic data from the moving car or moving car data which is captured by that traffic camera now there are several algorithms which were introduced for the purpose of this background subtraction and opencv has implemented few of them which we are going to see one by one so as you can see here I have this example which is a very simple example of just taking a video and then we are extracting each and every frame of that video and showing it into a window so using I am show method I'm just showing each and every frame of that video so this you already know from the previous videos how to capture the video frames from a video file or the live camera so when I run this code you will see that there are few persons which are moving here and we want to detect all those moving persons which are moving in the image so for that we are going to use a few methods which are available in opencv so let's first write some code and I will explain you what this code is going to do so I'm going to Define a variable after this line of code and I'm going to Define a variable name f g BG for foreground background and then I'm going to just call CV dot BG s e g m so b g s e g m and then I'm going to call a method called create background subtraction MOG method so this create background subtraction Emoji method is a gaussian mixture based background and foreground segmentation algorithm so using this line what we are doing is we are just creating a background object of the function using this method create background subtraction emoji now ah this method has some optional parameters like history number of gaussian mixtures and threshold but all of them are set by default so you don't need to set anything specifically unless you want to change some of the optional parameters so I'm going to leave everything as default and I'm not going to give any argument here for this method and then after I captured each and every frame inside this while loop what I'm going to do is I'm going to create a new variable called fgmask for foreground mask so I'm going to just write FG mask is equal to and for getting the foreground mask we are going to just call a method called apply on this fgbg or the background subtractor image so we are going to just take fgbg and then we are going to call a method called apply here and it takes one argument which is the frame which we are capturing okay so we have applied this method and then we are just getting the foreground mask using the apply method on this background subtractor variable and that's it so this is your foreground mask so when I just use one more I'm show window and this is for the FG frame let's say so f g uh mask frame let's say okay so FG mask frame and we are going to just pass this argument here so let's see what result we get after we apply create background subtractor MOG method so you can see this normal image here and also you will see uh these are moving persons in the foreground mask right so you have subtracted the background for from the foreground and you can easily detect the moving persons here inside this image using this mask you will also observe that there is a very ah little noise not much ah when you use this kind of subtraction using Create background subtractor MOG method there is one more method which is called background subtractor mog2 which is also gaussian mixture based background for and foreground segmentation algorithm so let's use that method also so this method is directly available under CV2 so you just need to write CV dot create background subtracted a mog2 okay and everything will remain the same so it's going to return you the background subtractor variable which you can use with this apply method to get the foreground a mask okay so let's see how this method performs so you can see the result which is quite different from the first method which we have used so in the previous case we have to create the background subtract object and here in this method you have an option of detecting the Shadows so there is an optional parameter which you can give into this method which is this create background subtractor mog2 which is called detect Shadows so by default this detect Shadows is true that's why you see the Shadows there if you just write the text Shadows is equal to false then it is not going to detect the Shadows so I'm going to just run the code once again and you can see now Shadows are less visible right so let's run the default case once again so let us say we just write true here and you will see the shadows in the gray color right so these shadows in the gray color and when we just make it false so the text Shadows Falls you will not see that gray color okay so Shadows are displayed in the gray color so if you don't see any gray color then Shadows are not detected this is a noise which ah is detected but not the shadow okay so this is the difference between the first background subtractor method and the background subtractor Emoji 2 method there is one more method which is called the background subtractor GMG so this algorithm which we are going to use so let's use this method first of all which is called background subtractor g m g which is available under CV dot BG segment as the first method so just write b g s e g m dot create background subtractor GMG method so this create background subtractor GMG method algorithm combines statistical background image estimation and pre pixel bias in segmentation let's see how this method performs when we just use this GMG method and when you will see here there is nothing on this foreground mask frame so to get the better result you need to apply morphological opening to the result to remove the noises so we are going to do just that so I am going to just override this FG mask frame using a method called CV dot morphology X this also we have seen in the previous videos right so the first parameter here will be FG mask parameter the second parameter here will be the up so CV2 Dot morph open we are going to use the morph open method and then the third parameter will be the kernel so we need to define the kernel also so for defining the kernel let's define the kernel outside this while loop so I am going to just write the kernel which takes few argument first is the shape so we are going to say we want the more Eclipse shape so I'm going to just write morph eclipse and then the kernel size will be let's say 3 comma 3 okay so we are going to apply this kernel using this morphology X method and when we are going to run this code you can see these kind of results which are not as good as you have seen in the first method now let me show you the last background subtraction method which is called the background subtractor k n n method so this method is available under CV2 directly so we are going to just comment this kernel code because for this method we don't need need to Define any kernel so we can just write CV dot create background subtractor and then at last you just need to write K and N in capital okay and it also takes few optional parameters like history and other parameters but these are optional parameters so for now we are not going to set any ah parameter and let's see the result which we get using this k n method so I'm going to run this code and you can see this KNN method result it also shows the shadows in the form of gray pixels so whatever gray pixels you see here in this image are the shadows in this method also there is an optional parameter which is called detect Shadows which is set by default to true so when you make it false the shadows will not be detected so you can see No Gray pixels are visible now when you make it true then the gray pixels will be visible and those gray pixels indicates the Shadows right so these are the few methods which you can use for the background subtraction in opencv in this video we will talk about an object tracking method which is called mean shift so first of all what is object tracking so in simple words object tracking is the process of locating a moving object over time using a camera and what is mean shift the idea behind mean shift is really simple consider you have a set of points it can be a pixel density like histogram bag projection and you are given a window which is a very small window which can be a circle or rectangle or a square and you have to move that window to a area of Maximum pixel density or maximum number of points so in the image you can see this illustration very easily so essentially the working of mean shift algorithm can be summarized in following points so in the first step we pass the initial location of our Target of object and the histogram bag projected image to a mean shift function and then in the Second Step as the object moves the histogram back projected image also changes and in the third step the mean shift function moves the window to the new location with the maximum probability density so we will see all these steps with the help of an example so here I have the simple code where I'm loading a video which is called slow traffic small dot MP4 and I'm just iterating over each and every frame of that video so this code till now you already know how it works so I am going to just run this code and let's say I just want to track this window of the white car okay so let me just run this video once again so I want to track this window of the white car or window in general of each and every car let's say okay so how can I track this window using mean shift algorithm let's see so as I said the first step is the passing of the initial location of our Target so this can be you can say a disadvantage of mean shift that you have to provide the initial location of your Target in our case that Target is the car window so what I have done is I have just calculated the initial position of the white car window and that we are going to see in the next step so first of all we are going to take the first frame of our video so the first frame of our video can be retrieved by this code so ret comma frame is equal to cap we have this cap function and we will read the first frame using the read method and this is going to give you the first frame of the video so this is our first frame now once we have our first frame we are going to Define the initial location of the car window in our case we want to track first of all the white car window right so I'm going to Define four variables first two are X comma Y and the next two are width and height so and because I have already calculated the initial position of the window I am going to hard code this position of the window so 300 200 comma 100 comma 50 okay so this is the hard coded value which I have already calculated which is the initial position of the car window now we can say that this x y and width and height is our track window so we are going to Define a variable called track underscore window let's say and then we are going to just pass all these four variables X comma y comma width comma height okay so let's pass all these four variables and in the next step we are going to Define our region of Interest so let's define this region of Interest with the variable called Roi and we already have our first frame so we are going to take our first frame and then we are going to pass that window so why colon Y plus height comma X colon X Plus width so this is our window or the position of the window so as I said in the first step we will pass the initial location of our Target object and the histogram bag projected image of the mean shift function so histogram bag projection in simple words creates an image of the same size but of a single Channel as of our input image in our case this will be our frame where each pixel corresponds to the probability of that pixel belonging to our object that is the output image will have our object of Interest or region of it in more white color compared to the remaining part of that image so this is back projections so for calculating the histogram back projection there are some steps which are involved so we are going to follow all these steps to calculate the histogram back projection but first of all let's just see the region of interest because we already have a region of Interest so I am going to just right CV dot I am show and our region of Interest so let us say how our region of Interest looks like so I have this video and this image which is our region of Interest right so this is the initial position I'm going to pass to our mean shift function right so now in The Next Step what we are going to do is we are going to define the histogram back projection so we already have our Roi so in the next step we are going to just convert this Roi to the HSV color space so I am going to just write HSV underscore Roi HSB we have already learned in the previous video so I am going not going to go into the details of HSV color space I'm going to just try it CV dot c v t color which is going to convert this image into the HSV color space so our input image will be the ROI and the next parameter will be CV Dot color underscore BGR to HSV okay so we are converting this image to the HSP color space and then we are going to calculate the mask so let's say we Define a variable called mask and for the mask we are going to just write a CV dot in range so this also we have learned in the HSV tutorial so if you want to learn more about all these functions you can just go to that video so first uh parameter we are going to pass is our HSV image and the second parameter and the third parameter will be the lower and the upper bound so the lower limit will be 0 dot comma 60 dot comma 32 dot okay so let's pass this and the upper limit so let's define the upper limit also so the third parameter will be the upper limit in the form of the Tuple but we need to use the numpy for that right so numpy dot array and inside that we just pass this Tuple value which will be 180 dot ah 255.2255 okay so 180 dot comma 255 dot comma 255. so why we use the in range function because for the histogram only Hue is considered from HSV right so the first channel right and also to avoid the false value due to low light or low light value we use the in range function okay so these low light values are discarded using the inner range function and then in the next step we are going to calculate our histogram value so I am going to Define this variable called Roi hist also we have learned in the previous video so I am not going to go into the details so I'm going to just use the function called calc hist which takes the first parameter which will be the image so I am going to just pass our HSV Roi so just pass HSB underscore Roi the second value here will be the channels so we are just using only Hue channel the first channel in the HSP space so we are going to just write 0 here ask now the next parameter will be the mass so we have already calculated the mass so we are going to just pass this mask parameter here the next parameter will be the hist size so as we have already learned in the previous videos that this hist size ah starts from 0 to 179 so essentially 180 values and then we just need to pass the ranges so as I said it starts from 0 to 1 and now in the next step we are going to just normalize these values using the normalize function so this normalized function takes few values first is the source so the source is our Roi hist variable the next value is the destination so let us say we have the same destination we just want to overwrite this Roi hist value the next parameter here will be the value of alpha so Alpha will start from 0 and the value of beta will be 255 so we want to normalize these values between 0 to 255 okay and then the next value will be the norm type so the norm type we are going to take is C V Dot Nom min max okay so we are going to just take this one Norm min max so all these steps which we have written here is going to give us the histogram bag projected image now once we have this histogram back projected image we are going to use this histogram bag projected image ah which is also going to change with the moving object so now in the next step we are going to go inside our while loop and read each and every frame one by one and first of all what we are going to do is we are going to calculate the HSV value of the frame as we have done with the first frame also right so we are going to just take the frame and then calculate the HSB Roi value let us say this time we are going to name it as HSB and we are going to pass frame as the source instead of this Roi value now in the next step we are going to use a function called calculate back project so let's define the variable called test for Destination and then CV dot calc back project which is the function for calculating the back projection and this function takes few argument first is the number of images so we only have our HSB image so we are going to pass in the form of the list the second argument will be the channels so as I said we just want to use the Hue values here so only one channel so we are going to just write ah 0 so because Channel starts from 0 1 2 so that's why I have written 0 here the third parameter is the hist value so in this case our hist value is the ROI hist which we have calculated the next parameter is the ranges so we will start from 0 to 180 e as we are talking about the HSV color space and the next value will be the scale so let us say scale for now we take 1 as the scale so this is going to give you the back projected image and then in the next step we are going to apply the mean shift to get the new location so I'm going to just write ret comma track window so I'm going to just say track window which is this variable which we have already ah defined and then we are going to just use CV dot mean shift which is going to take few arguments first is the image which is the destination image which we got from the back project function calc back project and next argument will be our Track image which is the track window so we have to Define this term criteria so I am going to just write term crit for criteria and then we have to Define this outside this while loop so I am going to go here and we are going to set up the termination criteria either 10 iterations or move by at least one point okay so we are going to Define that criteria so here in the curly brackets we are going to just say CV dot term criteria ESP or CV dot term criteria count so because we want to either provide the termination criteria for either 10 iterations so we just give 10 or we want to terminate by moving at least one point this is the criteria for the mean shift and we are providing these two track criteria so once we have our track window for the car we can draw a rectangle with the help of this track window and this will be visible on our video so we are going to draw that window we are going to just say x comma y comma W comma H for X Y width and height and this will be our track window and then we are going to just draw a rectangle so I am going to just say we have the final image and then we are going to just write C V Dot rectangle which is going to take the frame and then the point for the first point of the rectangle and the second point of the rectangle which are the coordinates of that point so the first point coordinates will be X comma Y and the second Point coordinate will be X Plus width comma y plus height okay and then the next value will be the color let us say we want to use 255 here and the thickness so thickness we want to take three here for example now we can just show this final image using I am show method so till now we've just uh showing our original frame so we can just say let's say we want to show the final image here also if you want to see the back projected image you can just use this destination so we can print this destination image also and see how does this back projected image looks like so let's run this code and you can see this car window is dragged right so as the car moves this window also moves once this car goes out of the scope it tracks the other window so this is how the mean shift algorithm Works in opencv now as I said this mean shift has few disadvantages or limitations the first limitation is the size of the target window does not change so as we have seen once this car is coming near to us the size of this window is not changing it remains always same so this is one problem the second problem is we have to give the initial position of our region of Interest so for example if initial position of the region of interest is not known then it will be really hard to apply main shift method so there are these two main limitations of this mean shift algorithm and we are going to try to solve these limitations in the next video when we learn cam shift which stands for continuously adaptive mean shift in the last video we have learned how to use mean shift algorithm to find and track objects in the video in this video we are going to learn camshaft algorithm to track the object in the video so if you have seen the last video we have written this code so we are going to use all this code which we have written in the main shift video tutorial and first of all let me just run this mean shift code which we have written in the last video and we have discussed about this problem of this rectangle which always Remains the Same even if the object is coming closer to the camera so we need to adopt the window size with the size and rotation of the target so once again the solution came from opencv labs and this time they introduced an algorithm which is called cam shift which stands for continuously adaptive mean shift so this camshaft algorithm applies mean shift per first and then once the mean shift converges it updates the size of the window in addition it also calculates the orientation of the best fitting Eclipse to it now let's talk about the implementation part of the cam shift so as I said all the code which we have written in the last video will remain the same except one thing which is we have used this main shift algorithm in the last video and in this video we are going to use the cam shift shift so just write CV Dot cam shift and all the parameters also will remain the same which is destination track window and the termination criteria so let's run this code once again and let's see what result came out of this algorithm so you can see this rectangle is changing its size according to the Target now this result which we have seen can be better because the camshaft function returns a rotated rectangle that is our result and also the Box parameters which are used to be passed as the search window in the next iteration so here when we see the result inside the ret variable so let us print the result inside the ret variable I am going to just print it using the print function now let's run this code and let's see what this ret variable prints on the terminal so let me just press Escape so what is this result so here you will see the value of x and y and also you will see these three values which are your width height and the value of rotation so in cam shift you can also rotate your rectangle according to your object size so now we are going to use all these parameters which are there inside this ret variable and we are going to try to draw the rectangle which might be ah rotating so there will be a different approach other than this rectangle we are going to use that approach to print those points which we got using the ret variable so let us draw that rectangle so here we are going to Define a variable called pts and there is is a function called CV dot box points so we are going to use that function here which is box points and it takes a few arguments we just need to uh pass our ret variable here so we are going to just pass our ret so let us see what values this is going to give us so I am going to just print this pts value so I am going to just print the value of pts now let's run this code once again you won't see anything and you will see these values right so it is going to give these floating Point values which we need to convert it into the integers and the error was due to this because this is no longer defined right so for that we are going to just convert these points pts into the integer value so I am going to overwrite this variable pts and then there is a function in numpy which is called int 0 and here when you pass this pts variable it is going to convert those Point into the integers and now we can just draw a rectangle but remember this is a rotating rectangle so we cannot use this normal rectangle function so we need to use the other function for drawing those points so I am going to Define this final image variable once again and then then I'm going to use CV dot poly lines so there is this function called poly lines which can you can use to draw those lines which you get using this points variable so we are going to just pass the frame first of all so we need to pass the frame as a first parameter the second parameter will be our pts value and then the third parameter will be the closed or not closed so when we pass true here then this rectangle will be closed right then we need to pass the color so you can pass any color here let us say it 0 comma 255 comma 0 and then you can also pass the thickness so let's say we just need to give the thickness of 2 here okay so this is our final image and now we are going to run this example once again and let's see what happens so you can see this rectangle is drawn and it can rotate also with the object so this is how camshaft algorithm works with opencb I hope you've enjoyed this video and I will see you in the next video