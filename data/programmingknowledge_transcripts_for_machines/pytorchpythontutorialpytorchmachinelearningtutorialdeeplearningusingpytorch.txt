hey guys welcome to your first lesson on machine learning with PI torch in collaboration with the programming knowledge YouTube channel I've uploaded this tutorial as part of my complete course on PI torch for deep learning and computer vision where you'll build highly sophisticated deep learning and computer vision applications with PI torch by the time you finish this tutorial if you're interested in the full course feel free to check it out by clicking on the link in the description below no experience is required for this particular course and I'm also covering free machine learning and AI content on my channel namely Ryan slim which is also accessible in the description so feel free to check that out but without further ado let's begin this tutorial on machine learning with PI torch in the section we will look to implement a machine learning based algorithm to train a linear model to fit a set of data points as this course does not assume any prior knowledge with deep learning we'll begin our discussion with the notion of supervised learning what it is and how it relates to this course if you've taken one of my deep learning courses before the material in this video should be very familiar to you we will begin with a very broad definition what is machine learning a simplistic definition is that it's the concept of building computational algorithms that can learn over time based on experience so it's that rather than explicitly programming a hardcoded set of instructions an intelligent system is given the capacity to learn is given the capacity to detect and predict meaningful patterns while one can distinguish between supervised and unsupervised learning let us consider supervised learning in supervised learning and the learner is trained and makes use of data sets associated with labeled features which define the meaning of our training data that way once introduced to newly inputted data given a new input the learner is able to predict a corresponding output before discussing any more theory let's jump into some code we will be writing our code using Google collab accessing Google collab is very simple and intuitive there is no setup required it runs entirely on the cloud to access it on your browser search up Google Club and it should be the first link make sure you're signed in to your Gmail if you don't have a gmail you'll need to make one and once you're logged in simply create a new Python 3 notebook and we will call this notebook tensors crash course this is quite similar to Jupiter notebooks we will eventually be using it for its free GPU but notably many packages come preinstalled for us when using Google clop unfortunately torch isn't one of them so in our first so we were in the following installation pip3 install torch alright and as it's installing inside of our first cell we import torch the ultimate goal of this fraud we'll be to create a data set which generally will follow a linear pattern then we'll declare a line a linear model with some random set of parameters which chances are will incorrectly fit our data set however then we make use of a simple optimization procedure known as gradient descent which is often used in many machine learning algorithms what this optimization algorithm will do is minimize the error function of our linear model and it keeps doing so until the model is trained to fit our data really well our first step in this venture will be to the Clarion linear model and use it to make predictions so we initialize two variables W and B and the reason for this being we know the equation of a line can be modeled as y is equal to W times X plus B where W is the slope the weight and B is the bias term the UI intercept of our line these are the parameters which distinguish our line and ultimately with linear regression provided with a bunch of data points we first start with some linear model which most likely will not fit our data well we then use these data points to train a linear model to have the optimal weight and bias values that of which will provide us with the line of best fit and then after training this model to have suitable parameters given an input X we can then use this linear model to make a prediction Y all right so back to our code we set W is equal to Torche dot tensor and we will give the wait some random value say 3.0 and as we will be trading our model we will require the gradient of our linear function so requires grad must equal true alright don't worry about why we did this for now it will be a more apparent to you later on why we need the gradient but for now we do the same thing for the bias term we'll just copy and paste that call this B delete this one and instead of a value of 3.0 let's change this to 1.0 it really doesn't matter and now given a linear model y is equal to W times X plus B provided an input X we can make a prediction we can determine the output Y and the purpose of this lesson is to show you how to make predictions with PI torch provided that you do have a model to do so what we're going to do is actually copy this delete it and on a new cell we define the forward function deff forward in this forward function receives the input the independent variable the xvalue and this input is going to be passed into our linear model y is equal to W times X plus B W and B being the parameters of our line and then based on the input X we predict the corresponding output Y a prediction is computed and returned returning Y let us now test our model by going inside of a new cell and reset the input X is equal to torch dot tensor we will choose input of two and outputting the result of forward X our model makes a prediction of seven instead of passing only one output one can pass in many inputs suppose you wanted to make a prediction for the input for and for the input seven respectively our forward function expects only one argument X so we must wrap this up in an extra set of brackets since that it receives both four and seven as separate inputs and the computation will occur as follows making a prediction for the input for to be thirteen and for the input seven to be twentytwo that is all for making predictions in the next video we will look into a more standardized way of creating a linear model I will see you there welcome back to another lesson in the last one you learn to make predictions using a linear model in this video we will look at a more standardized way of initializing a linear class we will discuss the theory as we implement it into code so on a new Python notebook first you must import the irrelevant Torche library and from torch dot and n we will import the linear class from the n n' module you will first write the following line of code on a new cell torch dot manuel you seem to have a problem being that i have to rerun this cell and so i'm just going to fast forward this video until I do see you in a bit alright and we're back we will start by finishing up this line of code torch dot manuel underscore seed one what this does is it sets a seed for generating random numbers the reason we're doing this is when constructing our model with the linear class it will be given random values for the linear class which makes sense since recall we start with a random value for the weight and bias and then we train our model through a gradient descent algorithm to obtain the optimal parameters to fit our data we'll talk about training shortly but for clarity and to see progress in training our network we want to ensure that the random values assigned to our weight and bias values are consistent hence why we set a seed the seed can be anything that you want it to be so long as you don't change it if you wish to obtain the same results that I get and feel free to use the same seed all right let us now construct our linear model with model is equal to linear and it will accept n features is equal to 1 out features is equal to 1 what this means is that for every prediction that we make using our linear model for every output there is a single input by creating this object over here we've created a linear model and we can determine the parameters of this line by printing print model dot bias as well as model dot weight we get our weight and we get our bias value this would result in the equation y is equal to 0.51 5/3 X minus 0.4 for one for perfect now provided this model we can now make a prediction based on an input X which we will set equal to torch dot tensor and we will pass in an input of 2.0 noting that this is a float of value and we will call upon our model model and pass in X as an input this should return a prediction which we can print out print model X it output prediction of 0.58 911 can make several predictions by passing in several inputs we can pass in an input equaling 3.3 and noting that our linear model only takes in a single argument we must enclose this in a bracket so what's going to happen is for each input we will get one output one prediction and printing this out the prediction for 2.0 is the same thing as earlier and we also get a prediction for 3.3 to be 1.25 90 this lesson was to get you familiar with the linear model class which provides a more concise easier and more standardized way of constructing a linear model we will be using this class all throughout this course in your next video we will explore custom modules a robust and efficient way of building neural networks with PI torch and for the purposes of this section we will use it to further configure our linear model I will see you there hello everyone in this tutorial we'll explore custom modules a robust an intuitive way of building neural networks in Python or complex neural models in the next section and thereafter for now we'll simply use it to build our simple linear model just to learn how to do it now that you're more familiar with the basics the method in which we're currently going to implement a model construction granting more freedom and is much cleaner and more intuitive in its implementation so on a new collab notebook I've already one had an imported torch and inside of a new cell we start by creating a new class which I will call LR as in linear regression and I know that we haven't covered classes in the Python cross course so if you're unfamiliar with them we can go through them now very quickly as we go along but essentially we will use this class as a template as a blueprint for our models construction generally it is a template for creating new objects new instances of our LR class class will be immediately followed by the init method which can be written as death in it ensure that you use the double underscores before and after the method if you come from another programming language for example es6 javascript this would be known as a constructor and so we use this initializer we use this constructor to construct to initialize new instances of this class the first argument for our initializer is usually self where self simply represents the instance of the class the object that's yet to be initialized and then after self we can add additional arguments our choice for these arguments is dictated on the basis that we are using this class to initialize a new linear model instance if you look back at the previous code initializing a linear model requires that we have an input size as well as an output size in this case one on one for both since each inputs will produce one output but we will be more general inside of our constructor that is the arguments passed and will be input size and output size whatever input size and output size is passed in as we create our instance and perfect that is all now to create a linear model to access our linear class we first needed to import torch DNN let's do that again we will import torch dot n n but this time let us do it as d elias n n we will use inheritance such that this subclass will leverage code from our base class n n module module itself will typically act as a base class for all neural network modules in this case the linear regression model class will be a subclass of n n module thereby inherit methods and variables from this class and when dealing with class inheritance one should be mindful of calling super dot init which simply allows for more freedom in the use of multiple inheritance from parent classes alright now this is just boilerplate code that you always need to write to create your custom class and now having performed all the necessary steps to inherit from our parent class we will declare self the linear' which represents the instance of the class the object that we intend to initialize and it will be set to n n dot linear as we've done before accepting both the input and output size that's being passed in input size and output size great we finally finished our initializer which we can now use to initialize a new linear model you can do this by setting we seem to have made an error and that is because I should have run this before running an end on module okay back to it we will initialize a new linear model an instance of this class a model is equal to LR and we can pass in two arguments into the initializer as clearly shown as before we pass in an input size of one and an output size of one as well and there is our linear model you can now seamlessly create more linear regression models to your heart's content we will stick with one and print out the random weights and bias values that were assigned to it two from n n dot linear we do this by printing model dot parameters and we will print it as a list in order to better see our results in before making any conclusions as done before we will seed our random number generator to see consistent results with thought Emanuel seed and it will contain the same seed as earlier one all right and our weight term equates a 0.5 1 5 3 and the bias term negative 0.44 1 4 as before it and now recall to make predictions we make use of the forward method and conveniently we can define the forward method inside of our class such that the method can be accessed for every instance of the class so we write def forward once again in the first argument being self and arguments there after being the ones you need to actually pass in in this case we pass in the input X all right and now the forgot my colon and now the prediction pred will equal the prediction that comes out of X being passed into our linear model self dot linear let's not forget our brackets and we will return the prediction return pred running the cell and back here we can now use this method for any instance of our class in this case we will use it for our model so we write model dot forward and into it we pass in a tensor as we have always done so we write x is equal to torch dot tensor 1.0 ensure that this is a float and into our forward function we put the X and print the results print model dot forward X like so and it outputs the appropriate prediction we can try this for two values of x one for no 2.0 wrap them up in a single brackets and yet it's consistent as it now makes two predictions based on two independent variables such that for every one input there is one output very well you should now have become very comfortable in making predictions using the linear class in an objectoriented programming approach going forward this is how we will approach a model initialization in this course the next step before we conclude the linear regression section is to actually train our model to develop new weight and bias values based on previously labeled data rather than just working with a random parameters we will do that going forward welcome back to another lesson previously we looked at various ways of making predictions with a linear model using PI torch and worked our way up to a more standard way that of which will be implemented going forward before we start discussing the machine learning concepts involved in training our model before we even train our model to fit a data set we must actually create the status at first and plot it as such we will keep this video short if you wish to skip this preliminary step and simply move on to the next video where we train our model I've included the source code in a subsequent article as well as on github otherwise if you wish to follow along feel free to use this earlier notebook as a starter project as we will be simply building over it firstly for visualization purposes we begin by importing mat plot Lib dot PI plot as PLT as we have always done before and instead of pressing Shift + Enter to run the cell press Alt + Enter to create a new cell right below as it is in this cell that we will be creating our dataset ultimately this is the data set that we will try and fit a linear model into our data set is plotted on a two dimensional coordinate system such that each data point is characterized by an X and the y coordinate as such we will be specifying the X values for our data points which we will set equal to torch dot R and N and what this does is it returns a tensor filled with random numbers that are normally distributed this function accepts a sequence of integers which define the shape of our tensor we eret answer to have 100 rows and 1 column such that there will be 100 points and each point has a single value within the normal distribution if we print this value print X its outputs a tensor filled with numbers that are normally spaced out however the numbers are relatively small since they are centered around 0 with a small variance so what we can do is multiply all the numbers in the scalar by a value of 10 times 10 to set up a larger range as very dealing with a 2dimensional coordinate system each data point will have both an x value and a y value which we will set equal to the output Y for now will simply be a function of x to further modify the Y value let us simply plot our current set of data points with PLT dot plot X recall this must be converted to a numpy array and the Y value as well y dot numpy and we want each data point to show up as a circle so what we can do is just place the letter O as our third argument oh not 0 outputting this should result in a straight line of data points this was to be expected such that all data points are normally distributed within the X all right now fitting a linear model into a straight line of data points would be a relatively easy we will challenge our model by adding a bit of noise to our output to each Y value of the point we want to shift it upwards or downwards since that the noise is also normally distributed across the entire range so instead of just setting the output equal to the input X let us add this normal distribution of noise to all 100 points adding torch dot R and n 100 to 1 and recall that R and n will center around the 0 with a relatively small Center deviation we want that appoints to be relatively spaced out so we want the noise to be reasonably significant as such we'll multiply the noise ratio by 3 3 times torched on randon and this should create our noisy data set all right now for aesthetic purposes or actually for clarity let us assign the yaxis a label PLT dot y label y the xaxis is well PLT dot X label alright and we are good to go we have our noisy data and now that we've created our data set it is time to train a model to fit this data set seeing that we've already done the code for creating a model we'll reuse it this linear model that we currently have it has random weight and bias values random parameters assigned to it from the linear model one can only assume that this model does not fit our data wall all right well let's see for ourselves what we'll do is we'll create a new cell you know what we'll actually do is just delete what's inside of this cell delete these cells as well you can do so by pressing ctrl M and D curl m and D and over here inside the cell we will first obtain the model parameters by unpacking model dot parameters into a list of two elements W and B and before proceeding any further it seems that none of our cells are run and I'll just go ahead and add it print a model in here to see an output but aside from nuts let's rerun all of ourselves to ensure that everything goes by smoothly as we go along all right back to it we were unpacking model not parameters into our list of two elements don't forget your brackets and now printing W and B and we see that the weight is a two dimensional tensor with one a row and one column so we can access this weight one is equal to the weight row index zero and column index zero and same thing for the bias term except it is only a single dimension within the 0th index of our a tensor b0 alright and printing both values print of W 1 and B 1 we obtain them let's remove that we obtain them as a tensor type we can actually add a dot item to both terms and what that's going to do is give us a Python number from both tensor values outputting this we get our parameters all right and let's make this into a function for cleanliness what we can do is write def gets params and this will return the two values as a tuple it will return w0 0.8 'm as well as b 0.8 'm removing the following now on the next cell we will plot our linear model alongside the data points so we will call a function and def plot fit the argument it will accept is a title where our PLT title will equal the title that was just passed in now since Matt polyp is most compatible with numpy let us first actually on top import numpy as NP we know the equation of a line is also y is equal to w ax plus b we have W we have B as they were initialized that once we created our model so what we can do is determine numerical expressions for x1 and for y1 so first what we do is we set W 1 and B 1 equal to the return value of get params where xaxis seems to go from a negative 30 to 30 so what we can do is simply set X 1 is equal to numpy array negative 30 to 30 make sure this is an array not a tensor to ensure compatibility with PI plot and now from these 2 X 1 points going from one extremity till the end we can get two Y points we can compute y1 is equal to W 1 times x1 plus b1 ultimately this will return to y1 points one at negative 30 and another one at 30 which will be connected by a line hence plotting our linear model all right let's plot it PLT dot plot x1 y1 and given that our data set is blue let's plot it as a red line by specifying the string are all right and that's it now we also want to plot our scatter data points and we do this with PLT dot scatter we will plot x and y and finally showing our plot with PLT dot show and on a new cell we simply call plot fits with the title being initial model all right plotting our model alongside the data points we can clearly see that this is not the line that best fits our data we will need to use gradient descent to update its parameters we will start doing that going forward hey welcome back hope you're doing well previously we created a set of data points and initialized a linear model we will now move on to the next step which is provided a set of data points it is our goal to find the parameters of a line that will fit this data well as you've seen before the linear function will initially assign random weight and vice parameters to our line suppose that returns a line with the fallen parameters clearly this line does not represent our data well we need some kind of optimization algorithm that will adjust these parameters based on the total error until we end up with a line containing suitable parameters how do we determine these parameters for simplicity and to get a brief understanding let's limit our discussion to a single data point whatever line that we choose the error is determined by subtracting the prediction at that point from the actual Y value the closer the prediction is to the Y value the smaller the error the prediction as you should already know can be rewritten as W x1 plus B however since we're dealing with a single dot an infinite amount of lines can be drawn through it so we will remove the bias removing that extra degree of freedom for now and we can cancel it out by ensuring the bias term is fixed at zero all right now whatever line that we're dealing with the optimal line will have a weight that will reduce this error as close to zero as possible say we're dealing with the point negative three and three the loss function would translate to three minus W times a minus three all of that's squared now what we're going to do is make a table and try out different values for W and see which one gives us the smallest error a weight of negative two results in the following which has a total error of nine a weight of negative 1.5 gives us an error of 2.25 notice how the error is analogous to the spacing between the predicted value and the actual value be smaller the spacing the smaller the error a weight of 1.5 gives an error of fifty six point two five two gives an error of 81 however if we use a line with a slope of negative one a weight of negative one clearly this perfectly fits this point if we calculate the error between the predicted value of our linear model and the actual value the error is zero which makes sense as the prediction lines up well with the actual labeled value the different errors they vary based on which weight parameter to use for our line so that it fits our points I've plotted different error values for different weights in matplotlib for visualization purposes clearly the absolute to minimum in this case corresponds to a weight of negative one now that we know how to evaluate the error corresponding to our linear equations the question remains how do we train a model to know that this weights right here that is the weight parameter that will yield the lowest error let's talk about that in the next lesson welcome back hope you're doing well previously we looked at and evaluated the loss function of various linear models with respect to different weights we saw that given previously labeled data there exists weight parameters for a that will yield the smallest error the question remains how do we train a model to determine the weight parameters which will minimize our error function the most well this is where we introduce a gradient descent the way this works is that first year linear model will begin with a random initial parameters recall when we initialize a model with the linear function and indeed gave us a random initial parameters let's ignore the bias value for now and based on the error associated with this initial parameter W we want to move in the direction that gives us the smallest error the trick is if I take the gradient of our error function the derivative the slope of the tangent at the current value that I'm at this derivative will take us in the direction of the highest error so what we do is we move in the negative of the gradient this will take us in the direction of the lowest error so we take the current weight and we subtract the derivative of that function at that same point this will take us in the direction of the least error you we are descending with the gradient however to ensure optimal results one should descend in really small steps as such we will multiply the gradient by a very small number known as their learning rates the value of the learning rate is empirical although a good standard starting value tends to be 1 over 10 or 1 over 100 the learning rate needs to be sufficiently small since as the line is adjusting itself you never wanted to move too drastically in one direction as that can cause for unwanted divergent behavior throughout the course you will learn to adjust the learning rate based on empirical results and we will code a gradient descent algorithm in the next few videos but to just end this lesson and to follow through with our gradient descent example let us refer to my demonstration on excel in order to simply visualize the effect of gradient descent alright as you can see on top we have the coordinates of the point themselves x and y and below a set up two columns the weight and the error the initial weight is clearly negative 1.5 set up the spreadsheet such that the second weight is equal to the value of the first weight minus the derivative of its error function at that weight times the learning rate of 0.01 to give us the new weights thus at each step we are moving in the negative of the gradient and if we do it for one step notice that the error gets smaller and if we keep doing it for such when cells notice how the weight eventually converges to negative one such that the error at that value approaches zero this is consistent with the graph that we observed earlier as a weight of negative one would indeed yield the smallest error for the point negative three and three as we saw in the last video all right so clearly gradient descent thus ending in the negative of the gradient is a very effective approach in training our model don't worry too much about this excel sheet this was purely for visualization purposes if you want I can include it as a resource for this lecture if you wish to play around with it in the next few videos we will work our way up to implementing gradient descent into our code but before doing so let us discuss the mean squared error we'll do that in the next section welcome back to another lesson let us conclude the theory part of this section by discussing the mean squared error the mean squared error is calculated in much the same way as the general loss equation from earlier except now we will consider the bias value as well since that is also a parameter that needs to be updated during the training process the mean squared error is best explained with an illustration suppose we had a bunch of values and we start by drawing some regression line parameterised by a random set of weight and bias values as before the errors correspond to how far the actual value is from the predicted value the vertical distance between them the error for each point by comparing the predicted values made by our linear model with the actual values using the following formula each point is associated with an error which means we would need to take the summation of the error for each point denote that the prediction can be rewritten as WX plus B as we are calculating the mean squared error we then take the average by dividing by the number of data points and I mentioned before the gradient of the error function should take us in the direction of the greatest increase in error so naturally by moving towards the negative of the gradient of our cost function we move in the direction of greatest ascent in the direction of the smallest error we will use this gradient as a compass to always take us downhill for simplicity in the last video we ignored the presence of a bias but the error is defined by two parameters both M and B we're not going to go through the math since PI Torche does all of it for us out of the box but really it's the exact same concept as earlier where we differentiate our error function but this time since our error function is defined by two parameters m and B we compute a partial derivative for each and just like before we start with any M and B value pair we use the gradient descent algorithm to update m and B in the direction of the least error and we update them based on the two partial derivatives up above such that for every single iteration the new weight is equal to the old weight its gradient times your learning rates and the new bias value is equal to the old bias value its corresponding gradient value times the learning rate one implementing gradient descent into our code we're not going to have to worry too much about the math as pi Torche does all of it for us out of the box it's just nice to know what's going on under the hood when you write your code the main idea being we start with some random model with a random set of width and bias value parameters this random model will tend to have a large error function a large cost function and we then use gradient descent to update the weights of our model in the direction of the least error minimizing that error to return an optimized model that is all guys let's put all of this into code I will see you then welcome to another lesson let's jump right into implement in gradient descent the first step is to specify the loss funk we intend to minimize given what I've shown you you might think to specify it according to the following equation however pi torch allows us an easy way to specify the loss function we can access the builtin a loss function from n n dot M s loss very easy the mean squared loss and store this in a value called criterion or a variable I should say not too hard all right the next step is to specify the optimizer optimizer that we will use to update our parameters the optimizer will use a gradient descent algorithm notably stochastic gradient descents an optimization algorithm that we can access from torch dot up Tim dot s g d as in stochastic gradient descent you may be wondering what stochastic gradient descent is the gradient descent algorithm I showed you before is known as a batch gradient descent which computes the gradient using the entire data set by updating the weights based on the sum of the accumulated errors this can be very bad since imagine you were dealing with a million points running the same batch of gradient descent processes earlier where we'd have to evaluate the accumulated error for every single data point every iteration that would prove very costly whereas with stochastic gradient descent it minimizes the total loss one sample at a time and typically reaches convergence much faster as it will more frequently update the weights of our model within the same sample size although the expression for SG d is different the general concept remains the same such that we're still updating the weights towards the negative of the gradient it's just that now with stochastic gradient descent it's mostly computationally faster all right back to our code in the first argument we specify the model parameters that should be optimized in this case model parameters and for the second argument we must specify the learning rate a liar a reasonable initial learning rate is 0.01 and recall that learning rate simply corresponds to the tiny steps that we would take to reduce the error in every iteration these tiny steps need to be sufficiently small since as the line is adjusting itself you never wanted to move drastically in one direction as that can cause for unwanted a divergent behavior in the near future we will look to implement adaptive learning rates and look to see how one can further finetune these hyper parameters but for now 0.01 should be enough to observe a significant yet quick drop in the loss function all right so now that we've specified the configurations for our training process it is now time to train our model we will train our model for a specified number of epochs an epoch is simply whenever we perform a single pass through the entire data set as we iterate through this data set we calculate the error function and back propagated the gradient of this error function to update the weights as you saw earlier okay so how many epochs should our line go through let's recall the concept of gradient descent which updates the weights and biases of our network in the direction that decreases the error function the most it's an iterative process which is why we need to pass the full data set through our model multiple times to ensure an optimized results in other words we need more than one epoch if you simply specify one epoch that leads to the under fitting of a curve which doesn't capture the underlying trend of data as the number of epochs increase the more times it's able to update the weights of the neural network minimizing the error and thus producing optimal results at the same time you don't want to pass in too many epochs since that can lead to overfitting generally which is a modeling error that occurs when a function to closely fit to a limited set of data points for this specific scenario overfitting shouldn't pose too much of an issue we'll see examples of it later on we will just stick to 100 epochs which would provide more than enough training iterations for our model all right and now for every epoch iteration for I in the range of epochs given that for every epoch we want to minimize the error of our model such that the error is simply a comparison between the predictions made by the model and the actual values let us first grab the predictions with wipe red is equal to model dot forward X so for each x value we make a prediction using the forward method all of which is stored inside of wipe red and then we compute the loss we saw loss is equal to the criterion which we set equal to the mean squared error and we will calculate the mean squared error for both the predicted values as well as the actual values why let's make sure I called it Y and indeed I did and now for every epoch that we iterate through we will print the epoch we will print the epoch I let me make these into double quotes I ran it by mistake all right so we paakai that's currently being iterated through as well as the loss associated with that epoch loss dot item forgot my alright simple enough now in order to visualize the decrease and loss at every single epoch what you want to do is before the for loop we're going to set a list losses is equal to an empty list and for every loss that we compute we will append it into our losses list we do this with losses dot append loss all right now having computed the loss and every epoch we must minimize that loss recall in gradient descent we must take the gradient of the loss function the derivative and recall to compute the derivative we use the dot backward method this was in the intro to tensors section we call loss dot backward and having computed the gradient we update our model parameters with optimizer dot step using the optimizer that we initialized earlier all optimizers implement the step method that's used to update the parameters of our model and can be called once the gradients are computed with the dot backward function as was just done all right we're almost done implementing the training process of our model before the optimization step we must set the gradients to 0 since gradients accumulates following the lost backward call and we do this with optimizer dot 0 grad rest sure that every model that we train in this course will follow a very similar process where we make predictions using our model compare the predictions made by the model to the actual outputs and based on that determine the loss the mean squared error and then we use an optimization algorithm in our cases stochastic gradient descent to update the weights of our model in the direction of the least error thereby minimizing the error function of our model as we attempt to minimize the loss iteratively to obtain a model with optimal parameters the model that best fits our data all right perfect and by running at this cell we can now train our model all right and notice that as the epochs progress the loss gets significantly smaller eventually converging on a minimum value of it seems to be seven point four six although it's not zero for our purposes this is really good you have officially trained a model using gradient descents to fit a set of data points to better visualize the process let us now plot the loss on a graph on a new cell we run PLT plot the xaxis will be the number of epochs range epochs and the yaxis of the losses also before I forget just a fair warning if you get really weird results for this training process just simply make sure to restart the runtime or reset runtime whichever one then rerun your training process it should work after that anyway back to what we were doing simply plotting the losses on matplotlib and we will give the yaxis a label PLT dot well label i will name the yaxis loss PLT dot ox label we will name the xaxis epoch all right now writing the cell this further clarified that 100 a pox was indeed an appropriate amount of iterations to allow the model to eventually converge to a minimum loss of value as it seems to be dampening out right over here at around 7 point 4 allowing our model to effectively train to fit our training data to visualize how well the model fits our data sets we can now plot our new linear model by simply calling plot fit train model plotting this data our rights we see our training data and we see our linear model which has been trained to fit our training data and it seems like it fits it pretty well congratulations you've just successfully trained a linear model to fit training data using gradient descents if this is your first time applying a machine learning algorithm then feel free to pat yourself on the back as this accomplishment is a pretty big feat in the next section we will step it up a notch and try to build a classification algorithm I'll see you in there great job I'm making it past the linear regression suction you've managed to train a linear model to fit a set of data points which is pretty awesome we're going to be training highly complex neural networks in this course some of which will be in charge of performing highly sophisticated tasks for now learning how to train a model to simply fit a linear data set is a great step in the right direction the next step is to now use a linear model to learn how to classify between two discrete classes of data points we do that in the next section you