hello everyone welcome to programming knowledge in this video we will discuss what is machine learning and what exactly is scikitlearn all about and why is it so important from the point of view of machine learning we will also see how to install it properly and get going first things first let's see briefly what machine learning actually is without going into formal textbook definition in very simple terms one can say that if performance of doing a particular task is improving with experience then we can say that the machine or the model is learning so there would be some tasks and we'll see how could the task is being done and this is done by measuring the performance if we see that the performance is if we see that the performance is increasing and getting better with experience then we can say that our program or our machine is learning mainly there are three types of machine learning or we can say that all machine learning algorithms basically fall under these three categories first one is supervised learning second consumer was running and the third one B reinforcement or in case of supervised learning the correct label or answer is provided with data as a classical example the size of houses are given with prices after feeding this data the program will learn and build up a hypothesis or model and then we will give our new size and the model will predict how much price should be based on the training data that is the data we fed earlier in supervised learning no levels are provided instead based on the similarity in data it creates groups by itself if you have visited Google News before then you must have noticed that a particular piece of news covered by several websites are grouped under one section these are not just grouped these are rather clustered in one section this can be one example of unsupervised learning in case of reinforcement learning there is an agent and he has to fulfill an objective or a reward this objective can be winning a game or building an opponent so the agent has to decide which step it needs to take or which option it needs to be selected out of the various available options based on the choice he gets either an award or a penalty also selecting a choice opens up new possibilities so you have to choose one state out of the many states available and this choice has to be made while keeping in view that the reward is to be maximized if you google about the most popular programming languages for machine learning and data science you will find out that Python tops the list and it would be great to know that scikitlearn is one of the most popular libraries to implement the machine learning algorithms it is easy it is clean and as a lot of efficient tools and features for classification regression clustering and so on it is built on number II Syfy and so up live but it is also recommended to install matplotlib method because the my program is an excellent blotting library and makes the data visualizations very easy let's move ahead to Python and see how to install the cyclone library we are assuming that you have installed the latest version of Python and set path and all the other variables correctly now let's type import s killer as you can see an error has been shown by python that team module has not been formed that means it has not been installed yet so let's head over to the command prompt and install it using leap it come at opened command prompt and here goes our first command so we have to install some libraries which are numpy Syfy job live and bad not here to install them we will type pip install numpy as we have pressed enter you can see the process has begun and it will be installed in a very short period of thing in a similar fashion we will write pip install followed by the name of the library which we wish to install for example Syfy or matplotlib or job leper or s killer after pressing the enter key the library will get installed and after that we can import it in Python and use it accordingly there are some queries and suggestions from the previous video for which we are extremely thankful and they are to be addressed so the first one is that this diagram is just used to bring the three keywords in one thing and hasn't really want much to do with the Union intersection and complement or any of those operations of mending a machine is said to learn affects performance of doing a certain task increases with experience also when it's 100 percent correct that there are three main types of machine learning many algorithm stones should be fall under only one there are many algorithms for example say on recommender system that may see parts of supervised and unsupervised and come up with an optimum sense now that you have installed scikitlearn and the other required activities let's type forward before we begin to tune or build our model then obviously first need to load the data to begin our work in the waifus please write also on a short note we don't use all the data available to play in our model we reserve some part of it for testing order now why can't we use the same set of data for the training and testing silver because the model would already know the correct level for those instances it is like getting same summon exercise as it was given in salt problem section example before or getting the exact same question a test which your teacher taught a day before I hope you get the picture well discuss more about splitting our data in training and testing set later but first let's get here with loading the data analyst the library itself comes with a few small data sets which are known as toilet isms and they are mostly used for seeing the manner in which classification and regression algorithms are being implemented here is a list of all the standard toilet assets mentioned in the documentation of this library since they are prebuilt no downloading is required and we can load them as follows first of all we'll import circuit learn now from scikitlearn we will import data sets if you are interested in viewing the complete list you can always write the area and in the bracket you will have to write data sets okay so the complete list has been loaded but we are only interested in loading the void intercepts so let's just name a variable say iris and we'll write data sets dot load is so now iris contains the iris dataset and we want to know that how many example how many samples are there what kind of features are there how many features are there so let's start exploring this we can write friend iris thought after that we can write okay the list has been popped up we can use data of each year names file names if we use the feature names then it will actually show that what are the column headings so let us write feature names so you can see that there are four columns first one containing sepal length in centimeters second one containing several width in centimeter third one containing petal length and fourth one containing vital width so there are four columns let's see what's the data iris dot data so you can see it has been written that squeeze text 150 minutes so means that there are 150 examples let's double click on this to expand this okay so as you can see as we had seen earlier there were four columns and accordingly four columns have issued and 150 examples are there so now we know that there are 150 flowers whose sepal width and length and petal width and length have been recorded now let us see what are the target values and under what name are deceived so we will again write print Irish dot target 0 1 & 2 are the values in which they are mapped so first 50 so there are 3 kinds of flowers that's why we have three values under the target section and let us say see their names names okay something's wrong I messed up the syntax a bit drained hi this dot names okay yes so the three values which had been mapped to zero one and two actually be flowers to sub for sequel o and virginica for more details about this dataset you can always write print iris dot followed by de SC are all in caps des this will give you a description about this data sector okay let's double click to expand this quiz text so as you can see everything has been specified over here for example number of instances are 150 15 each three classes number of attributes for which we had already seen earlier and these attribute informations are simple and simple width and exit row the classes are also specified over you so in the deac our attribute you can also find who's the author at what time was this dataset released and the specific details like we use to load iris a few moments ago you can similarly load any data by typing or for example load and the school digits or whichever you wish in the same way other than the toilet assets the other set of real world data sets can be fetched and downloaded if necessary these are the list of data sets that can be loaded using the fetch command again this complete list has been taken from the documentation itself open ml dot to RC is also a very popular public repository for machine learning in town here's how we can download a dataset from there suppose you want to download this data set of mice protein levels which are helpful in studying associative learning so just right we'll just search mice routine press the Enter key this result which pops up here all the description about the students that can be found and we have to download this data set on using psychic lock so we input SQL on and from SQL and we import our data sets and after that from SK learn dot ETA sets we import fetch mmm after this we need to name or give a name to a variable and we will write fetch underscore open and then and here we need to specify two things about the file name is equal to mice protein and separated by version mice has been loaded and we can see the details so that is how you can download any data set from the open ml dot orgy repository we can also generate our own leaders exit psychically using the commands which speaking with me that is make underscore for example and make underscore of classifications or make underscore regression but there we are skipping this for now as we believe that it is relatively less important from beginners point of view as compared to other discussed methods previously depending on the data and its format we can use different libraries for it for example it for your data is off the SQL JSON Excel or CSV file then it is best to use understand ready for active pandas comes with read CSV read excel reaches solid read SQL commands which make it a very easy and convenient if you have files in these formats also of the data frames can be created of dictionary type object using lists and tuples but if your file is of binary format for example if it has extension of dot mat or dot a r FF or WAV then it is recommended to use Sai politely and if there is column columnar data then vampire a should always be preferred and as well as the and video our concern then we can load them to an apparent using the SK image memory all these libraries interoperate very well with the psychic law and I leave the links related to these libraries in the description below so by this point of time we think that we have covered all the major Lee use data forms and how to use how to load the data in scikitlearn using them still if you think that there is a data form or any method which is missed and has not been covered yet then do mention us then do mention it in the comments below and we will try to cover it up we are now in a position to take a step forward and to build a model and that is exactly what we were going to do in this video we'll be downloading a data set from Canon and build a classification model and finally we will see how good a model is doing at predicting or classifying now let's proceed to have a look at the data set on which we are going to work so open Kaggle and search for seed from UCI and you will land up on this page let me zoom it so that it is more clearly visible so the file is of just nine KB and it gets downloaded in a city I have already downloaded it it contains three varieties of wheat yes it contains three varieties of wheat kymaro cyan canadian seventy elements each so that gives us a total of four 210 elements it has seven data columns describing each instance or example and finally there is a target variable which is the kind of weed and that is what is to be predicted and these are the column headings or the data which describe each instance of a particular reader after downloading the file if you open it through Excel this is how it will look as you can see there are a total of eight columns seven data columns and finally your target volume and there are a total of two hundred eleven rows and the first one was for heading so this gives us a total of 210 rooms and it comes now let's head over to Python and get the real work started first of all we'll import ponders as PD following a convention well now you know variable say total data and read the contents of that file in this way this will be done using Li VD dot read underscore seriously command and if the file is present in the path then you can directly write the name of the file but in case the file is present at some other location you can write R followed by the path of the variable we can recall form about the radius of the data by using describe command that is total underscore theta dot this crime so as we had seen in that data set page itself there are exactly two hundred ten rows indexed from 0 to 209 and there are eight columns as expected so after this we'll take only data variables in no variable say X leaving aside the target variables so we'll let X equal to total underscore data and then we can use method and even the column names or just to drop the last volume but I prefer the I love method because of its clearcut indexing so we will write tutor Anita 200 code it or dot I lock and we'll put the square brackets then I want all the rules and the column number zero to seventh and if you are familiar with is mixing then you know that by doing this we are not including the last column and again we can get reconfirmed about this selection by typing X dot info you can see the last column that is the column of target variables has been dropped and we are left with two only ten rows and seven columns as we had expected in a similar fashion we may use a variable semi to store all the target variables and will underscore data and give us the I lock method put the square brackets I want all the rows but only the last column again we can check out all the details but I think Y dot describe so yeah two hundred ten rows we will now import scikitlearn and other required memories for this project from scalar import SVM that is support vector machine and from SQL on dot SVM let's do put yes we see that is a support vector classifier okay looks like I messed up the syntax a bit to the capital SVC yeah that's good before we move ahead and work on the support vector classifier let's discuss a few terms about it and have a look about what it actually does so SVC is a class offense and it's of course super west because we have to supply label training later what is does is that it comes with depressed or optimal separating boundary or hyperplane which classifies new unseen examples the different kinds or categories of data are separated by a clear cap as well as possible there are a number of parameters that can be tuned if required and we will discuss them too once we train our model later in this video itself however for simple data sets like the one we are going to use the default settings work out to be just fine as we have already explained about the trained and tested on in previous videos we'll just partition the current dataset into the ratio specified by us so so from a scale or not model selection import rail test split test split if no analyse applied the default ratio is 3 is to 1 that is 75% of data for training and 25% for testing also it's not like the force 25% instances will be used for testing or last 25 the examples are selected randomly so we'll name some variables such as X 3 X test Y tween and Y test then we'll write 3 test split then we'll specify the data which is X then our target variable that is fine and after that we can write test size equals two point two that means 20% of the total data will be used as testing center if nothing is specified then as we had said previously 0.25 is selected by default and we can specify one more variable that is one more parameter actually that is random statement and we can give any number because this is actually the stir atom number generator and the number provided over here will be used to see the values so let's say of 30 now we will import standard scale of so we read from SQL or dot preprocessing input standard scale it is always a good practice to scale the data if you would have paid attention there were some data which were zero point something and somewhere near around 15 16 or so in other data sets this difference may be even bigger so what happens is that this use chance that the bigger number will outwit the parameters these parameters and the smaller number won't be cut in you pigments so to bring them on a relatively seems clear where the difference isn't much pronounced we scale it and provide a level playing field the gradient descent also takes a lot of time to converge at a minimum because the contours are skewed if not scaled but ok if we start discussing about that this video will become too let me do let us know if you want to cover if you want gradient descent to be covered in the next video but for now let's focus on the implementation so will now write SC equals 2 standard scalar followed by a couple of brackets after that will write extreme is equal to SC dot fit transform extreme and X test as just SC dot transform of X test now you must be wondering that why we are using fit transform method in case of train and transfer just the transform method in case of test so the thing is that we can use Budi fit and transform method one after the other or we can Club it together as we have done in the training set so when we scaled in it out we actually apply the mean normalization so the two parameters that are learned while scaling the training data the same parameters are used to scheme the test data and hence no fit method is used in case of test data so let's proceed now let's name a variable and call our classifier after pass after calling classifier we will now pass the training data in it that is will write CL f dot v that will write x let's go for my Y under school train there are lot of parameters but they will discuss about the most important ones so C is the penalty parameter if C is less decision boundaries will be smooth which is desirable but accuracy only that if C is worth then overfitting may occur but accuracy in business so there is a tradeoff again we have different algorithms regarding this tool but for now let's stick to default Williams kernel is RBF RB a stands for radial basis function and it has a finite number of dimensions it it can have infinite number of dimensions and its value decreases as we move away from the center finally we have gamma which which is a parameter for nonlinear equity and the iron the higher it tries to fit the training data more exactly we are now left with the job of predicting using our classifier so we'll name a variable say red underscore CLS and we'll call the predict function see a left dot read it and we'll pass X underscore test to it but wait our job is not finished here oh we need to see how accurate our model is so we'll write the skill on dot matrix dot accuracy underscore we'll pass y underscore test that is the what it should have in and parity underscore CLF that is what has been predicted so its accuracy level is more than 95% and hence we can say that our SVM classifier is doing a pretty good job in predicting if you want to dig further deeper you can also generate a classification build report for that you will have to write a still on dot matrix don't classification report and again what it should have been y underscore test and what is fed is predicted third understood see LF it was not very readable so I use the Print command and now you can see that it is all being beautifully formatted in our table format here precision is the ability of classifier not to nibble an instance positive that is actually negative it is actually of positive predictions and the recall is ability of classifier to find all positive instances it is a fraction of positives that were correctly identified an f1 score is a harmonic mean of precision and recall so finally we made a model whose accuracy was more than 95% and there were quite a few new dumps methods and concepts in this video if you want to know about any particular topic in detail or you are unclear about any of the process so over here and make sure to leave it as a comment below and we will try to take it up in the next video thank you for your time this is the V shake sync from programming knowledge and check out other videos and subscribe to this channel for more such coordinate thank you in this video we will learn about the concept of pipeline now a typical pipeline consists of a preprocessing step that transforms or imputes the data and the final predictor that predicts target values so transformers and estimators that is the predictors can be combined together into a single unifying object it is a pipeline so pipeline is combining different Transformers array estimators to automate machine learning workflows as it sequentially applies a list of Trance a list of transforms and a final estimate now that we have now that we have a some theoretical idea about pipelining let's head over to Python and code it's simple implementation and see how it is actually done so first of all we'll import the required packages and datasets so let's first import the iris dataset scale on your data set import load is no we had used a standard scaler the last time so this time let's try the min/max kilo we will discuss about it in a while but first let's import the come in Mac scaler first scale on dot preprocessing input n max scale now we'll import a simple linear model logistic regression so from a scale on a stick and for splitting the data into training and testing set the model selection s accordingly imported s Kalon dot model selection import train test split and finally we'll import the pipeline so from this Kalon lured pipeline will import pipeline my new disc it's case sensitive so be alert and use a capital P later and a small P before most of the things that we have imported we have discussed about it in the previous videos so I'm not going to detail into all of these and in case you missed any of those concepts then do see our previous videos in this series and still if you are having any confusion then to comment it as a comment below here X is an element of a particular instance so what min/max killer does is that it first subtract the minimum of XS from X and divides the result by the difference of XS Max and min this intermediate result is now multiplied to the rage that is a Max and Max minus min and then the main is added min and Max are nothing but feature ages now let's write iris parecer variable which will load our iris dataset is equal to load iris after that we'll split our descent into training and test sent so X underscore three comma X underscore test comma y underscore train and y underscore test it's called trail test split method this will write iris dot theta for extreme then iris dot target these are for x and y respectively and then we'll specify that how much percentage of the whole dataset we want to P as our testing center so let's say our zero point two that is the twenty percent means twenty percent of our data will be used for testing and let's see the random variable so random state let's say 42 now we will create the pipeline again a variable type underscore lr4 logistic regression pipeline my new year peace capital again a variables name min max 4 min max killer after this now that this killer has been added let's add is the logistic regression estimator or predictor a stick crash okay now let's fit our data in this pipeline so pipe underscore L r dot fit extreme and white rain extreme comma y three okay now let's see the score name now let's name we will score and store this coordinate so pipe underscore L our third score let's see how good our model is doing X underscore test comma Y underscore test so as you can see our model is 90% accurate in guessing the unknown examples or the examples that it has not seen before and hence we have created a simple pipeline in which we have first scaled the data using the vascular and then applied a simple linear Rod Steiger regression for classifying the iris dataset for any machine learning model there are some hyper parameters which can be adjusted and in turn they change the efficiency or the accuracy of the model so even if we fail finalize the machine learning model that we are going to work with a lot of job is still left as we have to adjust the hyper parameters one route force approach would be to start with default values and keep on adjusting manually according to our intuition but that would be very timeconsuming and does not have any clearcut education about whether we have reached an optimum value or not that's where grid search comes out to make our life a little bit easier we have already worked with the SV and the logistic regression so instead of repeating them again let's learn something new in this video first we'll learn about a new efficient classifier that is a random forest classifier and then we will use grid search to units hyper parameters for understanding random forest classifier let us take this interesting something suppose you want to visit any one of the following cities which are Delhi Mumbai Kolkata and Chennai but you are not sure which one so you make a list of these four cities and distribute to your 100 friends and ask them to choose their favorite one out of these some choose Delhi some choose Mumbai but after the voting you found out that Kolkata has got the maximum number of foods so now you can be sure that you want to go to Kolkata that is how a random forest classifier works a random forest is a meta is a meta estimator that fits a number of four decision trees on various sub samples of data set and uses average to improve predictive accuracy drawing similarities we can say that these hundreds friends were like hundred decision trees and each one of them predicted a value and the final outcome was decided by a majority of voting that is the forest let's see how it is implemented in Python so as usual post a for will import circuit law from socket one dot n symbol we will import random forest classifier then from SQL Oh oops from Keylong dot datasets will report god iris dataset that is bring Woking load iris let's limit variable and load our iris data into it now we have to split on data set into training and testing set so for that will import s from a scale own dot model selection import train test split now let's split our data right extreme right X test then we'll read Y train then we'll write Y test now let's call this function train test split and then it will read iris dot a dot then is no target let's specify the tests nice say 20% and see the random value state equals to say 30 now let's call our classifier CLS equals to random forest classifier it can be calling this way also if we do not specify everything that only the default values will be supplied but for the purpose of this video we are going to work with three very basic hyperparameters which are number of estimators minimum samples net and minimum samples leaf so default value for number of estimators is 10 that is 10 this is a decision trees are used let's suppose we are going to work which is to number of estimators to and after that we can use min sample split then samples split equals to 3 the default value for this is 2 and then sample split is the minimum number of samples required to split an internal so of course it's difficult value is 2 and the third parameter that we are going to work with is minimum samples leave now what is minimum sample sleeve it is the minimum number of samples required at the pace that is a DB flow so its default value is 1 at least one sample is required over there so let's try to now let's fit our data into this classifier CLF not fit X train comma Y train ok the upper parameters are as we had specified them as you can see that minimum samples is minimum sample sample leaf is minimum sample split is 3 the number of estimators are so yeah let us go ahead after fitting this we are left with the job of predicting third use red underscore CLF goes to CL f dot predicted and what we are going to predict X test now for checking the accuracy wellliked scale on dot metrics dot accuracy score and in the brackets will write y underscore test and red underscore CLF let's see how much accurate our model is with these hyper parameters okay it's already 93 percent accurate so now we'll see that can we improve this accuracy or not so in order to improve the accuracy will try to adjust or doing the other parameters using grid search so first of all let's import grid search from SQL on the dot model selection input grid search CV after that we have to specify a parameter grid parameter could what this parameter grid contains is it contains a various number of features and we have to specify them according to our wish so what will take each one of the combination and dry it out so in our case since we have chosen to work with these three features so first of all we'll specify any estimators now what are the values that we can walk that it can take so we have taken 3 initially so the sorry so let's say 5 10 20 suppose that my number of estimators can take any one of these 4 values after that I can write my next feature that is men samples split and this will be it was given three so let's say two and three after that which one features their principles many samples leave and main samples we can take up any of these any of the values like its default value was 1 so let's try 1 2 & 3 so basically what we have done is we have given it four options for number of estimators two options for minimum sample split and three options for minimum sample leave so a total of 4 into 2 into 3 it is 24 option so it is going to dry on our model and the one with the best accuracy score will get selected and that is how grid search works so that is number of estimators can take up any of these 4 values 2 5 10 or 20 minimum sample split can take any of these two value that is 2 or 3 and minimum circles leaf can take similarly any of these two values which are 1 2 & 3 now let's write grid search and we will first call me grid search CV then specify our estimator or estimator was CLF which is the random forest classifier of course and after that we have to specify the parameter grid both have been saved by the same name I think so yeah param grid it will be after that after this we have to fit the data in our grid so grid underscore search dot fit lady under school serves not wait extry and white rain extreme my dream some kind of warning okay we'll ignore the warning and now that target search has been research has tenets working so we'll see what are the best parameters that it has come up with grid search dot best underscore para where I am status parameters so the grid search has told us that minimum sample sleeve should be one minimum sample spirit should be due and the number of estimator should be five for the optimum solution so let's do now random forest classifier that is our model accordingly according to these parameters and then check our accuracy till this point the code is exactly the same as we had seen a few moments ago so from this point or will specify the hyper parameters that we have learned using grid search so number of estimators were five samples leave one and then samples split use no you just have to filter data CLF not fit X train and then by train underscore train okay Rita has been fat now let's do the prediction thread underscore CL f equals to CL f dot predict and after that we'll write X underscore test the prediction has been done now the last step that is we're gonna check its accuracy Escalon dot matrix dot accuracy underscore school and accuracy we have to check Y underscore test as compared to red underscore CLF okay so as you can see the accuracy has gone to 100% this might be a case of overfitting but one thing is for sure that we do our hyper parameters using grid search and then using those hyper parameters and the accuracy for model definitely improved