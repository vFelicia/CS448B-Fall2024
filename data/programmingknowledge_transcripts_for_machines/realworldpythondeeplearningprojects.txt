hi and welcome to realworld Python declining projects by Packt publishing my name is kuba konschak and i have been programming since 1995 since then programming is my passion and I really enjoy doing it and I've been programming in a lot of different programming language and I you know built a lot of different kind of systems and since around 2006 I've started to build a pretty complex Python systems and what got me really into machine learning and deep learning specifically is that I started to work with a start of that you know they wanted to predict future promises of a real estate based on corn prices that they have in the database and they got me this task of doing all of the research and trying to figure out how we can implement and unfortunately I didn't have enough time and to really deep into this problem and I simply couldn't do that so then I decided that I want to really get into machine learning and understand how it works make it really simple for myself and also for hunters so this is the goal of this course to make machine learning and deep learning specifically really accessible for you so let's go through the course overview so in this course you'll work with a variety of machine learning problems and in each product you will learn a distinctive set of tools and how to solve a specific problem of different kind of data and with different kind of deepening methods and we'll start with section 1 when I want to give you the essential informations about deep learning so you can get started really quickly so this section is all about getting you up and running with deep learning tools really quickly and also I will show you how can set up all of the necessary tools really quickly so you can start working with our project straightaway then in section 2 this is our first project and we'll start with forecasting how much people will travel by L next year based on past data that we have and in this project you will learn the basics of building your models and training and testing your models this will be just a very gentle introduction to how to work with deep learning methods and by the way you will learn how to work with time series data and with supervised learning problem then in section 3 we'll switch to different kind of data and in this section we'll focus on working with text and with conversion networks and here we will learn how to do what we call a sentiment analysis which basically means that you want to recognize the emotional feel of a text and here we'll be using a short text or you know the tweets to do that you will learn how to prepare your data to work with convulsion neural networks and text now how to build those models that can tell if a piece of text is negative or not then after that we will switch to different kind of data as well because we'll start with working with images we'll still work with commercial neural networks but the hair will learn how to detect smiles in images and this is a very common and interesting problem and you will learn pretty much how to design the model of convolutional neural network for this type of problems and how to purge that and also how to prepare your data in a way that your training will be quick which is essential in this section at any in section 5 will end with a really changing problem which is predicting stock prices using long short term memory networks and this is changing for various reasons and you know in this section we'll explore all that and how we can solve those problems to actually get some results so I hope that you are excited and one more thing let's go through some requirements and that we have to meet before you can get into this course so the main requirement is that you have to have some point and experience you don't have to be an expert we basically in this course we're using a whole bunch of a penis we don't use any advanced point and programming tricks so you know the only thing that you need to do here or to have here that you have to have some basic part and experience and of course if you have machine learning knowledge or experience that's a big plus but I don't assume that Trump discourse and I do my best introduce and fin from the ground up as much as I can in a tone that we have in this course so I hope that you're excited about this course I am and you know let's get started so let's start with opening our preparation script or to be exact let's run our preparation script is located under prep dot apply in the source directory for this section and it will help us understand how our source data looks like and how do we actually need to prepare it so let's first look at our roll values so here we have on the left we have years and then for interview we have a number of passengers karatbar airlines and this is pretty straightforward data set and main question you might have right now is okay why we can actually use this data directly and the answer is because our new network don't know what it means and need to tell it what it means so how we can do that convert this data and phrase our problem into the way our new network looks at problems our new network look at problems as a series of observations when you have x and y and X depends on Y in some way it looks at a list of those mappings between those two values or more but in our case we need to correlate the number of passengers carried in some way I'll tell you how in a minute and then new network can learn those dependencies between those two values or a list of two values so we need to give our network some kind of observation or experience that it can learn on so how we can do that to generate to erase or lists in one array going to provide a list of actual predictions and then in the second list we need to provide what those first predictions are based on so we have x and y and here on the right we have the values for each here and we started moving 1973 here and this is just for illustration let's look at those values so on the right we have four 1970 three four and so on we have number of passengers carat and we can create an X which is values that we base our prediction on for each year we can assume that each year's value depends on previous year's value so here for 1974 we can assume that the vibe for 1974 somehow correlates to the value from 1973 the only problem is that with 1973 which is our first year we don't have any data for previews here that we can use here so here the previews data will be empty and we'll have to exclude this particular data point from our X&Y because you know we don't have any way to base our prediction on so this is the main idea behind the data preparation we need to construct this x and y arrays and the values from X will correlate for advice from Y and the resulting values will be just X list values which are the values that we base our predictions on and Y are the actual predictions and just try to grasp it it's a very simple idea and in both cases we have this number of passengers carrot they are just phrased in different and let's look now at the code that we used to generate those values let's open our prep dot PI scripts our preparation script it's located in the source directory for this section so we are mainly interested in get underscored data function where all the magic happens and we are generating those values in four stages the first stage is to read our source data I'll talk about the details of our source data a moment but here we using pandas just read the values from our CSV file and CSV file is a text file and it's a common separated value file after we've done that then we need to convert our values from this file to a numerical values so here we have values and years those two arrays we are producing using get row XY the next step is to create our X array because values in this case are our wiring those are our predictions and then we need to have files that match those predictions or advice that we base our predictions on so those will be the X we need to generate those X values and here we call them past values and when you look at the data once more when you look at it side by side you can see that those two arrays X is to slightly shifted Y array so when you look at it this value did the value number number two let's say or one when we talk in terms of Python the race match exactly the value from Y the first value or the zero value and this goes on and on to generate X we just shift y el a little bit so we can look at get on the score of ppl it's welldocumented and we generate those values in pretty standard the last step is to return this value so we have fears we need those to produce our graphs our charts then when you return our X values and then our Y bodies so those are two arrays that we are most interested in and not is that we are filtering the first and the first row in those arrays and this is one again because we don't have data for 1972 or past data any data that we can base a prediction for 1973 that's why we need to filter that out because we don't have data for this particular year okay so let's now talk hold it about our source data so our source data is a CSV file in and it looks like this and it will be located in the source directory for this section in data directory under airline data don't CSV can always download this data set just by googling World Bank airline travel this is usually how I do it and then the first result our transport bus engine current is the right result and when you go to World Bank's website for this data set you can download it by clicking on the right under download under CSV and to use this newly downloaded data I need to put it into the data folder in the source directory for this section I need to name it to airline indict a dot CSV okay that's it for now I hope that was useful so let's look at our projects plan so the first step and this is very important step is to prepare our data and the problem that we're trying to solve here is called sentiment analysis in machine learning lingo and this step is very important because you will see how when you prepare data in a different way you will get a different accuracy of our model so this step is really pretty important then we will learn how to build convolutional neural network for text classification there are a couple of things you need to be aware about that we'll use so called Wharton Manning's here recent invention that allow us to get a really really nice results then we will train and test our model with data that we've prepared to find the best model that we can get and here you'll learn which metrics are important what you should look for when training and testing your model to get the best model that you can get and the last step is to actually use this model to detect those tweets and then we'll explore what we can do even more to improve the performance of our model so let's look first at our data set that we'll be using we'll be using a data set from smile smile D project and it's open source mile detector and it uses a traditional methods to detect smiles and what we are interested most is inside this repository teh github repository we are most interested in smiles directory so when you go to smile smile D repository you can click on the smiles directory and we have two directories negatives and positives and here we have those images that we'll be using to train our network so how we can install them so to install them you have to download the whole repository so just go to the main page as you can see here and you can click on clone or download on the right when you click on this button can choose download zip when you download this repository can unzip it and you have to copy those two directories from Smiles directory negatives and positive inside the data directory inside source directory for this section and I've already done that here you can see the source directory for this section and we have negatives and positives and that's it so let's look at how our source data looks like so in negatives and positives we have faces of people that are basically smiling and having different facial expression so in positives we have a variety of people basically most of the time smiling so this is our positives data set and then we also have negatives data set and here we have different kind of facial expression it's not smiling so even now we have negative and positive it will fit our project perfectly and one thing to notice here in those two data set or in this one that is data set and in negatives and positives is that here we have imbalance because we have around 10000 negative facial expressions and 4000 positive ones but we can work with that but we have to make sure that we blend together those two data set so our network will Train effectively so let's now look at our output and as usually at this step we need to generate the right x and y's so what will be x in our case X will be the images that we have and we have to encode those images in the right way so here I've opened the terminal and run our preparation script which is in prep dot pi and here we have just the first image from X this is already encoded so each value that you can see here is the value of the pixel from each image which is converted and I will talk about it in a moment notice how this each value has its separate array and this is most often confusing for a lot of beginners that each value of this pixel have to be wrapped inside a separate list or array so this is our X and we are using our input for our network will be Xin convolutional neural network our first layer is the twodimensional convolution layer and the input for this layer is 32 bar 32 images in our case we have 32 rows and 32 combs that we need to provide data in this kind of shape and we also have to provide the number of channels and here we're using the black and white images so we can encode the black and white just with one value so we'll be using s channels will use one and we generate those valleys and we put them inside our preparation script and then our training script will pick them up and configure our network in the right way so this is our X what will be our Y so here we have our Y this is again the first encoded class and here we have just two classes we have not smiling as zero ends minus one and here will be when things will be differently than in previous section because we'll be using a soft max activation function in the last layer and that means that we can use more than two categories and we will still just use two categories here but I want to show you how you can use multiple categories here soft mint function and here we have to encode our data in a different way we have to encode them each of those classes as an array or as a vector and here we have just two classes so we'll you know the encoding will look like 4 0 0 1 0 and for smiling we have 0 1 and this is the only difference the predictions will be pretty much the same like previous section and we have even more options with when we using soft map when we doing predictions and we'll talk about in a moment so let's go through the code here here I want to highlight bits and pieces that I required and also bits and pieces that allow us to process data quicker because this is very important when you're working with convolution your networks and images and with images in the U networks in general because images contains all the data and you want to make sure that we have enough data for training but at the same time our data is small enough so we can work with it in an effective way so we'll start with our preparation script it's located in our source directory for this section it's in prep dot PI and our main function as usual is get underscore data and here where all the magic happens we starting with loading the data the negative data and the positive data from our directories and then for each of those data generating the appropriate label and we doing this first for negative samples and then for positive ones after that we need to merge them together join them together both X's and Y's this is how we do it here and you were using a lot of numpy operations because they are really quick and they work well with images or arrays that we converted from images then after that we hit optimization step so by default I got 64bit values for those images that get converted into a race this can work with our new networks but it turns out that our new networks in our case we don't need that much data 64bit values they can have a broader range that 32bit values but they are larger it will take more time time to train our network so here what we are actually doing is that we're just converting both of those values from X&Y to 32 bits and this will make our training much more quicker then the next step is to define the shape of our data and here we have our input data or 64 by 64 images black and white images or grey images and here we're just providing this information for our training script when it will define the layers it needs those kind of information and then we also need to provide those kind of information about the number of classes for our training script when it define the last layer with the softmax activation function and then we need to encode our classes in the right way and this is the you know the last output that I've showed you from the preparation script and here we're just turning our classes into those factors those arrays this is just the requirement of the softmax function then after that to do something with inequality of negative and positive samples and if you don't do that if you have imbalance between two data set your training will be much more slower here we can just use a pretty standard numpy the function is to just mix up the indexes indexes will be just distributed randomly when we do that we can just rewrite those indexes for both X and Y's in the same way so they can both match and not is that in this gait data script we don't split into training and testing sets because we'll later on in the training script will let care us take care of it the curse will automatically do that for us so this is the main idea behind preparation of the data and let's look at our load data function because this is where we have another set of optimizations so let's first look at our load data function so load data just goes through each image and turn it into array and it mainly does that using image to array function so let's look at this image to a rake function so here it is it's right at the beginning of preparation Square and there are two things that you need to know about this particular function so first it will quite easily turn the images into arrays using I am read from signcut image package this is the required step this is how you can get those values from the images and turn them just to an ordinary erase but after that there is this very important down sample step and this is when the next step comes in this is the optimization step this is where we turn our 64 by 64 images into 32 by 32 but without losing any important informations and this will allow us to really much quicker working with the network much quicker and still get the high quality that we're looking for last step is that once load data creates all those arrays for each image it will at the end run Prem underscore array all of those images turn into a race this function does two things first thing is that by default the pixel values are in the range from 0 to 255 and this range values doesn't really work with you network especially when you're working with images because we have thought of values here so the good idea is to actually convert each value to a different range to a range of 0 0 to 1.0 and this works much much more better so here we can using a pretty standard dump on the operation and just turn this data into a numpy array and divide it white 254 5.0 which is the maximum value that we can get in those values and this will turn those ranges around to 0 0 to 1.0 and you can still work without this conversion but it will be much much more small and this is not a required step in an absolute sense but it will make training much more faster and the last step is to turn those single values for each value that we have it's separated list or separate array this is the stuff that I've showed you when I run the preparation script and here we have a quick way of going about it using numpy we can just do that on the whole data set really easily and that's it when it comes to data preparation so we'll start with downloaded data from Yahoo Finance and this is freely available data and we'll be downloading it and using it with this project so the first step to do that is that you need to go to finance dot yahoo.com and then you need to type in the name of the company that you are interested in in this project we'll be using the historical stock prices data for Tesla so Candace turnip in Tesla here and once we've done it we've got all sorts of informations here but what we are most interested in is the historical data tab let's click on that and when you do that then on the left hand side you have time periods drop down menu time range of the data that you are interested in and we'll be using about a one year period data for this particle project when you choose the time range you can click on on done then click on apply and then you can download your data in CSV form and once you download it you have to put it into the data directory in the source directory for this section under stock underscore prices dot CSV so the data that i've downloaded this is the historical stock prices for Tesla and we have a couple of variables here and we'll be using the same ideas that we've used in section two to predict the amount of people that will travel by air in the next year but the difference between section two in this section is that we'll be using multiple variables to do that so we'll be using a couple of variables that are related to prices so our data comprised of rows in each row we have this information about the prices of a stock for Tesla the price is in US Dollars and we have the price for one share our variables that we'll be using is open this is the stock market opens this particular one stockexchange open this is the Dupre's that we starting with then we have hi hi is the maximum price that this particular got in this particle day then we have lo this is the lowest price and then we have closed price when the stock market closes and then we also have adjusted close and we are not really interested in this one because in our case it will be the exact same as the closing price and then you also have volume this is the number of shares that were traded in a given day so we'll be using all those variables apart from adjacent clothes and data will be used in all of those to predict the closing price for a given day so how we can do that we'll use the exact same thinking that we've used in section 2 so let's say that we want to predict closing price for this particle day so we can say that we can use all the values from previous day to predict this price okay and as you can see the thinking behind it is exactly like in section 1 we just you know phrasing problem as supervised learning problem that's pretty much it so we need to convert our data so it can match this thinking we need to create the x and y so let's look at the output of our preparation script to understand also how we can do that and also how we need to format our data so we can use it with our network so the first step is to do the shifting and we can do that with different ways in section 2 I gave you some idea that how we can do that the other idea is that we can when you've got the data for x and y so x is the variables the matrix that we're using for prediction that we will base our prediction on and the y is the actual prediction so when you have those two lists in Perl you can easily achieve this the right formatting way that you can phrase a problem as supervised learning problem just by shifting the Y upwards it means simply not here and getting value from the first element from Y yeah I knew just discard it right because we are interested in basing our prediction on previews days of metrics so our first step is to just get the data from our CSV file and crate those x and y's and without any shifted so here's how it looks like this is the our X this is not shifted right and you have our Y this is the closing price then after we've shifted that we live in the X alone and we're just moving the Y from in the next day to the present day so this is how it looks shifted so three four four goes up and we can see that those values are just going up those are closing prices are going up right and it always takes a little bit of time to get your head around those kind of ideas but you know the idea is would be simple and I've got a lot of comments for you in the actual code so you can read those I've got some examples there so and I'm pretty sure that it will help you understand what's going on so once we've shifted the Y and we phrased our XY z in the right way as a supervised learning problem we need to also shape it so that our network will accept that and those long short term memory networks they work with sequences our data looks like that after this shifting this is the just one element from the data so here we have just a list of those arrays or lists of lists of those metrics right and then in the second in the Y array or list we have just those values of those closing prices values to convert that into the format that all network will accept so our network needs another dimension another list because it assumed that it doesn't necessarily have to work with just one sequence and you can imagine that we can add more sequences or more data from more days in the past and here we're just using one day from the past the data from the past so here we just need to add one dimension which means that we need to just embrace or you know put those variables into one more list and that's it when we've done it our long shortterm memory network will accept our data so let's have a look at our data actually when you run this script it will show us what we will be working with so here's just a plot of the closing promise for our data so as you can see in blue we have our training set and in orange we have our test set we've split at 8020 so 80% of data is training set and 20% is our test set as you can see this is not an easy to model data because we have lot of peaks and valleys okay so we'll be working with this data so let's just quickly have a look at our preparation script to just give you some idea how you can get around it and it's van twenty percent is from section two but let's just go through it quickly so we're starting with our main function which is get underscore data and this is our main starting point and then we also have our prep underscore data this is d tap when we add in this one more dimension to our data set so let's just get started with get underscore data so what we're basically doing here is that we're using pandas to read our CSV file then we just get in the right columns from the data and discarding those that we don't need so we don't think the data column and we don't need the adjusted column so this is what we're doing in get underscore row XY and then after that we shift in this why right we're shifting it upwards or backwards it depending how you look at it so this is what we do we've get underscore VPL so notice here that we live in X alone and we just shift in y and then after that we just discard in the last data at the bottom because we don't have any data there because we've shifted why so we don't have data at the bottom as well so this is what we basically do here and all of those functions are pretty welldocumented so feel free to go through the minutes and what's going on and you can also go through the section to again if you're in doubt and explain it a little bit differently and deeply so feel free to check out section two for preparation data if you still feel that something is unclear