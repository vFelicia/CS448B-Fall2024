With timestamps:

00:00 - hey there how's it going everybody in
00:01 - this video we're going to be learning
00:02 - how to read and write data to different
00:04 - sources so we'll learn how to read and
00:06 - write data using CSV files Excel files
00:09 - JSON and also sequel databases now in
00:12 - this series so far we've been reading
00:14 - data from CSV files but in data science
00:17 - there are so many different ways for
00:18 - data to be stored so by the end of this
00:21 - video you should be able to get your
00:22 - data to and from pandas no matter what
00:25 - data format you're using now if you're
00:27 - watching this video because you're
00:29 - looking for how to read and write a
00:30 - specific file format then I'll be sure
00:33 - to add timestamps in the description
00:34 - section below to where we read and write
00:37 - from each different format now I would
00:39 - like to mention that we do have a
00:40 - sponsor for this series of videos and
00:42 - that is brilliant so I really want to
00:44 - thank brilliant for sponsoring this
00:45 - series and it would be great if you all
00:47 - can check them out using the link in the
00:48 - description section below and support
00:50 - the sponsors and I'll talk more about
00:51 - their services in just a bit so with
00:54 - that said let's go ahead and get started
00:55 - ok so first let's look at CSV files
00:58 - since we've already been using these
01:00 - throughout the series we should already
01:02 - be familiar with reading data in from
01:04 - CSV since that's what we've been doing
01:06 - so far but in case this is the first
01:08 - video of the series that you're watching
01:10 - let's go over this one more time and
01:12 - then we'll also learn how to write to a
01:13 - CSV file as well so up here towards the
01:16 - top of my notebook here we can see that
01:19 - I'm reading in this CSV file and this
01:22 - CSV file is within a data folder that is
01:26 - in the same location as this jupiter'
01:28 - notebook on the file system now if you
01:30 - have a CSV file loaded elsewhere on the
01:32 - system then you'll need to pass in the
01:34 - full path to that file instead of just
01:36 - this relative location that we have here
01:38 - and we can see that we have different
01:40 - arguments that we can pass in when
01:43 - reading our CSV files so in this example
01:45 - I'm at eclis setting the index to this
01:48 - respondent column here which is the
01:51 - respondent ID for each person who
01:53 - answered this survey and when I read in
01:55 - the CSV we can see that it sets this
01:58 - data frame equal to the data and we can
02:02 - print this data out down here at the
02:04 - bottom so that is the read CSV method
02:07 - and it allows us to pull data in to
02:09 - pandas
02:09 - now let's learn how to write this data
02:12 - back to a CSV
02:13 - maybe you're gonna make some changes and
02:15 - some different analysis here to your
02:17 - data frame and then we want to export
02:20 - this back to our file system for later
02:22 - use or so that we can share it with
02:24 - someone else or something like that so
02:27 - for example let's filter down for a
02:29 - specific location in this survey you
02:31 - know maybe you're doing some analysis
02:33 - for your specific country and you just
02:35 - want to see the survey results from that
02:37 - location we've seen this in previous
02:39 - videos but if we want to filter then we
02:42 - can simply say I'll create a filter here
02:44 - and just say that I want the country
02:48 - here and I'll grab if the country is
02:52 - equal to India so let's say you're doing
02:54 - some analysis and you only want the
02:56 - survey results from India so now I'm
02:58 - going to create a new data frame here
02:59 - I'll call this India DF and du ADF dot
03:03 - lok and pass in that filter so now if I
03:07 - do an India DF dot head to take a look
03:11 - at the beginning of this new data frame
03:14 - if we look over here in the country
03:16 - column then we can see that all of these
03:18 - countries here are now set to India so
03:21 - now let's say that we want to export
03:22 - this new filter data frame to a CSV file
03:26 - so to do this we can use the to CSV
03:28 - method so we can say I'll just say India
03:32 - underscore DF which is our data frame
03:34 - dot to underscore CSV and now I'm just
03:38 - going to pass it into that same location
03:40 - in that data directory and then I'll
03:43 - just call this modified dot CSV so if I
03:47 - run this we can see that we don't get
03:49 - any errors and now if I go back and look
03:52 - at my file system here then I have this
03:55 - modified D CSV so if I click on this
03:59 - then we can see that this is you know a
04:02 - little bunch together since it's a CSV
04:04 - file a raw CSV file that we're looking
04:06 - at but we can see that we have all of
04:08 - our column names here and then the
04:11 - second row should be the first result
04:12 - and I can see here that we have India
04:15 - for that country if I look at the second
04:18 - result we can see we have India again
04:20 - and India again down here most likely I
04:25 - can't see it but you
04:27 - we can just assume that it's there it's
04:29 - looking good oh and actually there it is
04:31 - right there so we can see that we did
04:34 - actually export that data frame where we
04:36 - filtered that down to a new CSV file
04:39 - okay so that was easy enough so now
04:42 - let's look at how to read and write to
04:44 - some other formats so one thing that you
04:47 - might run into is a tab delimited file
04:49 - these are almost exactly the same thing
04:51 - as CSV files but instead of your data
04:54 - being separated by a comma the data is
04:57 - instead separated by tabs so to do this
04:59 - in pandas we're still going to use the
05:01 - same CSV methods that we've already seen
05:03 - but we're going to pass in a custom
05:05 - separator so we can write to a tab
05:07 - delimited file just by changing the file
05:11 - extension here to dot TSV and I'm also
05:16 - going to specify a separate or argument
05:18 - so I'm going to say set s P and then you
05:22 - want to pass in your separator now you
05:24 - can pass in anything here if you want
05:26 - you know a file that is separated by
05:29 - hashes or anything but commas and tabs
05:32 - are probably the most common so I'm
05:35 - going to put a backslash T there because
05:37 - that's how we specify tabs in Python and
05:40 - now if I run this cell I'm going to go
05:44 - back to our data directory here we can
05:47 - see that now we have this modified dot T
05:49 - SV if I click on that then we can see
05:52 - that now this looks almost exactly the
05:54 - same as the comma separated file but now
05:56 - we have tabs here instead of commas now
06:00 - if you're reading in tab CSV files then
06:03 - all you need to do is take this se P
06:05 - equal to backslash T and you can just
06:09 - add that as an argument up here to read
06:11 - CSV so it's basically the same thing
06:14 - okay so now let's move on to some other
06:17 - file formats now a very popular file
06:19 - format when working with this kind of
06:21 - data is Excel now if we want to write to
06:24 - excel then we're going to need to pip
06:27 - install a couple of packages so I have
06:29 - my terminal open with the current
06:32 - environment that I am using this is my
06:34 - Jupiter notebook running here let me
06:36 - grab my other terminal so I have the
06:40 - same environment that I'm you
06:41 - within Jupiter you want to be sure that
06:43 - you're using that same environment so
06:45 - that your pip installing and the right
06:47 - location and now we're going to install
06:49 - a couple of packages so first I'm going
06:52 - to say pip install and this is XL WT so
06:58 - XL WT will write to an older XLS Excel
07:02 - format but if you want to write to a
07:05 - newer xlsx Excel format then we'll also
07:09 - need to install open pi excel and you
07:14 - can pip install multiple packages but
07:16 - just by listening them all right here
07:18 - and finally we want if we want to read
07:21 - excel files then we can install the
07:24 - excel our D package so I think that is
07:28 - the three packages that we're going to
07:30 - need in order to work with excel files
07:32 - here so I'll go ahead and install all of
07:34 - those and let those finish and once
07:38 - those are installed let's go back to our
07:40 - notebook and now let's try to write to
07:42 - an excel file so to write to an excel
07:45 - file I'm just going to write the same
07:47 - modified data frame that we have here
07:50 - and we are going to use the to
07:53 - underscore excel method and this is just
07:57 - as easy as passing in let's say I'll
08:00 - save it in that data folder again I'll
08:02 - call this modified dot xlsx so I'm going
08:07 - to write to the newer excel format so if
08:11 - I run this then it might take a second
08:14 - here for this to work because it's
08:16 - actually creating this excel file on the
08:18 - back end so let's let this finish and we
08:21 - can tell it's finished when this turns
08:22 - from an asterisk to a number here okay
08:25 - so once that's finished let's flip over
08:27 - to our data folder here and we can see
08:30 - that we do have that dot xlsx file now
08:34 - this likely won't open up in Jupiter
08:37 - because this is an excel file we can see
08:38 - here that we can't open this up in the
08:41 - browser we actually need Excel so let me
08:44 - open up my Finder window here I have
08:49 - this open down here and I am within this
08:52 - data folder and we can see that
08:54 - we have our modified dot xlsx file here
08:58 - now I don't actually have Excel on this
09:01 - machine I have numbers so I'm going to
09:03 - open this up in numbers it should
09:05 - basically be the same on Windows but you
09:07 - can just open it up with Excel now again
09:10 - this might take a second to open up
09:12 - because we do still have a lot of rows
09:14 - here in this data okay so we've got this
09:19 - opened up in Excel again I'm on numbers
09:23 - because I'm on a Mac and I don't have
09:24 - Excel installed but it should open up
09:26 - fine in Excel as well let me zoom in a
09:28 - little bit here so we can see and we can
09:31 - format these if if we need to so for
09:35 - example we can change the column sizes
09:37 - here so that all these fit in but we can
09:39 - see here that we have our respondent IDs
09:41 - if I look over at country we can see
09:43 - that it did export the filtered data
09:46 - frame that we were hoping to export so
09:48 - everything looks good here now there are
09:50 - also some more advanced things that we
09:52 - can do with Excel as well if you're
09:55 - familiar with Excel then you might know
09:57 - that we have the concept of different
09:59 - sheets where we can have multiple
10:00 - spreadsheets and one excel file and if
10:03 - you want to read or write to a specific
10:05 - sheet then you can pass in a sheet
10:08 - argument to these methods actually
10:11 - trying to scroll over to my notebook
10:13 - here let me scroll down here to the
10:16 - bottom so like I was saying if you want
10:18 - to read or write to a specific sheet
10:21 - then you can pass in a sheet argument to
10:23 - these methods and there's also a way to
10:25 - start from different columns and rows as
10:27 - well but I'm not gonna go into all these
10:29 - little details here if you google this
10:32 - method name to excel then you can find
10:36 - the arguments that you can pass in and
10:38 - all the additional details and the
10:40 - documentation so for now let's go ahead
10:43 - and move on and see how that we can read
10:45 - in that same excel file that we just
10:48 - created and make sure that this works
10:50 - now by default it's going to load in
10:52 - with a default index just like when we
10:55 - read a CSV file so we'll have to specify
10:58 - that we want our index column to be that
11:00 - respondent column so to do that I'm just
11:03 - going to call this test since we're
11:05 - going to be creating a new data frame
11:07 - here from that
11:08 - excel file that we just created and
11:09 - we're going to use the read underscore
11:12 - excel method here and now we just want
11:15 - to pass in the location and I'll just go
11:17 - ahead and copy this here so that is
11:21 - modified dot xlsx on my machine and now
11:26 - I want to set that index column equal to
11:29 - and that was respondent on your data
11:33 - that might be different but I want my
11:36 - index column to be equal to that
11:37 - respondent so I'm going to run that cell
11:39 - and load that in and then I'm going to
11:42 - look at that test data frame
11:43 - now before I run this I'm going to make
11:45 - sure that this finishes processing here
11:48 - and that this asterisk goes away again
11:50 - it can take some time because it's
11:52 - actually you know loading in that data
11:54 - from Excel now which is a little more
11:57 - tricky than loading it in from a CSV so
11:59 - now if we look at that test data frame
12:03 - let me just look at the head here
12:04 - instead of looking at the whole thing so
12:07 - if I look at the head then we can see
12:08 - that we have the same data frame here
12:11 - that we had up here so that was exported
12:15 - to excel and imported correctly okay so
12:18 - now let's cover some other popular file
12:20 - formats
12:21 - now JSON is also really popular for this
12:24 - kind of data so let's take a look at
12:26 - that first let's write our modified data
12:29 - frame to a JSON file now for writing to
12:32 - a JSON file then we can use the to JSON
12:35 - method so you're probably starting to
12:37 - see a pattern here these method names
12:38 - are very straightforward now this one is
12:41 - a bit different since there are some
12:43 - different orientations that we can use
12:45 - for JSON so just by using the default
12:49 - arguments I can just say so that was
12:52 - India D F dot to underscore JSON and now
12:56 - I'll pass in a file location here but
12:59 - instead of an excel file we want a JSON
13:02 - file now I'm just going to use the
13:04 - default arguments for now and then I'll
13:05 - show you how we can change this up a bit
13:08 - so if I run this we can see that ran
13:10 - very quickly now if I go back to my data
13:13 - folder here then now we have this JSON
13:15 - file if I look within here okay that
13:19 - took just a second to open up on my
13:21 - machine again
13:22 - we do have a lot of data in here but if
13:24 - we look in here then we can see that
13:26 - this is very dictionary like so we have
13:29 - a main branch key here and then the
13:32 - value for that key are all of the
13:34 - responses just for that column and if I
13:38 - was to scroll down here then I would be
13:40 - able to find the other keys and the
13:41 - other responses as well so by default
13:44 - this is a dictionary like JSON now there
13:48 - are also different ways that we can
13:50 - write JSON files again I'm not going to
13:52 - go into every single little detail here
13:55 - but let's say that we wanted this JSON
13:57 - to be list like instead of dictionary
14:00 - like which is how it is by default so to
14:03 - do this we can change the Orient
14:05 - argument so instead let's add on here to
14:11 - our arguments and I'm gonna say Orient
14:14 - is equal to and if we pass in records
14:17 - and lines equal to true then this will
14:22 - now make this records like which is list
14:25 - like and this lines equal to true let me
14:28 - spell that right we'll just make each of
14:31 - these on a new line so it might be a
14:33 - little bit easier to read now if you
14:36 - want to see the exact arguments that you
14:37 - can pass in to Orient then again just
14:40 - look pandas to JSON method and it'll
14:44 - take you to the documentation with all
14:45 - the different things that you can pass
14:47 - in here so let me run this and now let's
14:51 - go back and reload our JSON file to see
14:54 - how this looks and now what we have here
14:57 - is more lists like so before we had a
15:00 - single dictionary where the values were
15:04 - a list of all of the responses but now
15:07 - we have one response at a time so we
15:11 - have the main branch and then so this is
15:14 - actually this first one here if I scroll
15:16 - down we can see that this is the second
15:18 - response this is actually the entire
15:20 - first response so we have the main
15:22 - branch and then that answer and then
15:23 - open source and then that answer and
15:25 - then so on and we can see here that for
15:27 - the country we have India and each
15:30 - response within this survey is actually
15:32 - on a different line so that's a little
15:35 - bit different than
15:36 - and how it was before but there's just
15:37 - different ways that we can export these
15:39 - JSON files depending on your needs okay
15:41 - so now that we've written our data to
15:43 - JSON files now let's also read this JSON
15:46 - file so that we can make sure that we
15:48 - know how that's done as well now since
15:51 - we wrote the JSON file with these
15:53 - different arguments here then we need to
15:55 - use those same arguments when we read
15:57 - the data in as well so if you're reading
16:00 - in JSON files and have any issues then
16:02 - you might need to play around with the
16:04 - different arguments to fit the data that
16:06 - you're trying to read in so in this case
16:08 - I'm just going to copy this whole line
16:10 - here and I'm gonna say test is equal to
16:13 - and actually let me just grab this part
16:16 - and I'll say PD dot read underscore JSON
16:21 - and now I'll pass in all those arguments
16:24 - here so we are reading the JSON file
16:27 - from this location we know that the
16:30 - Orient is list like instead of
16:32 - dictionary like and that all of these
16:34 - are on new lines and again depending on
16:37 - your JSON data you might need to go in
16:39 - and change these around depending on how
16:42 - your data looks so if I run this then
16:45 - let's see if we have the same data that
16:51 - we exported before and it seems like we
16:53 - do this looks exactly like it did
16:58 - whenever we exported this data okay so
17:00 - now the last file format that we're
17:02 - gonna look at let's learn how we can
17:05 - read and write data from sequel
17:08 - databases now this is probably the most
17:10 - complicated simply because you have to
17:12 - have the database setup and all of that
17:15 - good stuff but for the sake of this
17:17 - video I'm going to assume that you
17:19 - already have a database with the correct
17:21 - credentials to log into that database so
17:23 - I have a Postgres database set up on my
17:25 - machine that will be reading and writing
17:27 - to so first let's see how we would
17:30 - connect to this database now just like
17:32 - with excel we're going to need to
17:34 - install a package to do this so let me
17:36 - bring up my terminal here and I'll close
17:39 - this numbers file here let's see let me
17:42 - try to quit out of this actually I'll
17:46 - just minimize it it's having trouble
17:47 - shutting down
17:49 - okay so let me go back to the terminal
17:52 - that I hope opens the two where I can
17:54 - install some different packages and
17:55 - that's my Jupiter notebook where is my
17:57 - other terminal here we go
17:59 - okay so to connect to our database we're
18:01 - going to want to install sequel alchemy
18:03 - and this is a very popular ORM for a
18:06 - Python that allows us to more easily
18:08 - work with databases if you don't know
18:10 - what an ORM is it stands for object
18:13 - relational mapper and it's just a way
18:15 - for us to use Python objects in order to
18:17 - connect to a database I plan on doing a
18:19 - complete video or a complete series on
18:21 - sequel alchemy in the future but for now
18:25 - let's go ahead and just install this so
18:27 - this is pip install SQL alchemy and I'll
18:31 - install that and depending on the
18:34 - database that you're using you might not
18:36 - need to do anything else here so for
18:38 - example if you're using sequel light or
18:40 - something like that but since I'm using
18:42 - a Postgres database for this tutorial I
18:44 - also need to install the psycho PG to
18:47 - package that allows us to work with
18:49 - Postgres I'm not sure if that's actually
18:51 - how you say that package name but that's
18:53 - what I've always called it so pip
18:56 - install and to install this package to
18:59 - work with Postgres it's psycho PG - -
19:05 - binary so I'll install that and with
19:09 - those packages installed let's go back
19:11 - to our notebook and see if we can
19:12 - connect to this database using sequel
19:15 - alchemy so first we're going to want to
19:18 - import everything that we need
19:20 - so from SQL alchemy I'm going to want to
19:25 - import there create engine and this will
19:30 - allow us to connect to the database now
19:32 - I'm also going to want to import psycho
19:36 - PG - so let me run this sell and now
19:39 - that those are imported we should be
19:41 - able to create the engine which is
19:42 - basically our database connection and
19:44 - again I'm going to assume that you've
19:46 - already created this database and have a
19:48 - username and password so to create this
19:51 - I can say engine is equal to and use
19:54 - that create engine function that we just
19:57 - imported from seek welcoming and now we
20:00 - need our Postgres connection string
20:03 - now if you don't know how to make
20:05 - Postgres connection or connection
20:07 - strings then you know they have this
20:09 - available on the sequel alchemy site as
20:13 - well let me make sure I spelled this
20:15 - correctly that is PostgreSQL and then we
20:19 - want to pass in the username and
20:21 - password for our database now for my
20:25 - case I just made a user of DB user and a
20:28 - password of DB Pass now another thing
20:31 - here that I'd like to mention is that
20:34 - you probably shouldn't put credentials
20:37 - within code like this I'll leave a link
20:40 - in the description section below where I
20:42 - show how in Python you should use you
20:44 - know something like environment
20:46 - variables or a config file to hide this
20:48 - information but for the sake of this
20:50 - tutorial I'm just going to put it
20:51 - directly in here but if you're doing
20:53 - this in a in production code I would
20:56 - highly recommend using environment
20:57 - variables so that you know you don't
21:00 - expose your username and passwords
21:02 - within your codebase okay so there we
21:04 - have our username and password and now
21:05 - the database that we want to connect to
21:07 - so this is on localhost this is on my
21:10 - local machine it's running on port 5 4 3
21:14 - 2 and now the name of the database now I
21:18 - have PG admin open here where I can see
21:21 - my databases and we can see that I've
21:23 - just created an empty database here
21:25 - called sample underscore DB so that is
21:29 - the database that I'm going to connect
21:31 - to ok so if I typed everything correctly
21:34 - here then I should be able to get a
21:37 - connection to that database so now let's
21:40 - try to write our modified data frame to
21:42 - a table in this database and this table
21:45 - doesn't need to currently exist by
21:47 - default it will create this table for us
21:49 - if it does already exists then we'll
21:52 - need to add another argument to handle
21:53 - that but we'll see that in just a second
21:55 - so to do this I can just say India
21:58 - underscore DF which is the data frame we
22:01 - want to export then this is to
22:03 - underscore SQL and now the table that we
22:08 - want to write this data to I'm just
22:10 - going to call this sample underscore
22:11 - table now again this doesn't currently
22:14 - exist but it should create it
22:17 - now we need to pass in our database
22:19 - connection here I called mine engine so
22:22 - let's pass that in and if I run this
22:25 - let's see if this works ok so we didn't
22:28 - get any errors whenever I read that or
22:31 - whenever I wrote that but now let's go
22:34 - back to my PG admin here and let's see
22:37 - if I can see this table so first I'm
22:40 - just going to right click and refresh I
22:41 - like to do that anytime I've made any
22:44 - changes and we can see there that here
22:46 - that we have a sample table down here
22:47 - I'm going to right click on that and go
22:50 - to view and edit data and look at all
22:52 - the rows here and we can see it does
22:56 - look like this worked I know that this
22:59 - is probably a little difficult to see on
23:01 - my screen here but we have all of our
23:05 - data written here into the database ok
23:07 - so that's good that we were able to get
23:10 - this from pandas into SQL but now what
23:15 - if we updated our data and wanted to
23:17 - rewrite that data to this database so
23:20 - let's go back to our notebook and see
23:22 - what this would look like now if I try
23:25 - to run this same line again where we
23:27 - export this to SQL then we're actually
23:30 - going to get it in error because this
23:32 - table already exists if you want to
23:34 - write over a table then we can add in an
23:37 - additional argument and the argument
23:39 - that we want to add in is called if
23:41 - underscore exists equals and now what we
23:46 - want to do if this table already exists
23:49 - now in my case I'm just going to replace
23:51 - that table with our new data but there
23:54 - are also other options as well we could
23:56 - have it throw an error we could which is
23:59 - what it does by default we could also
24:01 - append data to a table so if you're
24:04 - doing like a daily script where you're
24:08 - analyzing information then you can just
24:11 - append that daily data to your existing
24:14 - table but for this example I'm just
24:17 - going to have this replace the table so
24:20 - let's run this and once this is finished
24:25 - processing then I will go back to PG
24:28 - admin now again let me
24:30 - come up here and refresh this and dig
24:33 - back down into the database and let me
24:36 - close this view here and let's see if we
24:41 - still have this data okay so we can see
24:43 - that this worked we were able to rerun
24:46 - that command and it just replaced that
24:49 - data that was in that existing table
24:50 - with our new data in this case it was
24:53 - the same data but that's how you would
24:55 - do that okay so lastly now that we've
24:59 - seen how to add our data to a database
25:01 - now let's see how we can read in this
25:03 - same data using SQL now if you skip to
25:06 - this part of the video using the
25:08 - timestamps and the description sections
25:10 - below then please go back to when we
25:12 - wrote data to our database and see how I
25:15 - set up this database connection here
25:17 - because we're going to reuse that same
25:19 - connection to read in our data okay so
25:22 - this is pretty simple now that we
25:23 - actually have this database connection
25:25 - set up to do this we can just say I'll
25:29 - call this SQL underscore DF and we will
25:33 - just say PD dot read underscore SQL and
25:36 - now we want to pass in the table that
25:40 - we're going to read from and that was
25:42 - sample underscore table and now pass in
25:45 - our database connection my connection
25:47 - here I called engine and also I'm also
25:51 - going to pass in an index column just
25:54 - like we did when we read in our CSV so
25:58 - I'll say index column is equal to and
26:00 - that is going to be this respondent row
26:04 - right here for your data that might be
26:07 - different so whatever you want to be
26:08 - your index just pass it in there if you
26:10 - want pandas to just do a default index
26:12 - then you can just leave this off
26:14 - entirely okay so if I run this then
26:17 - let's look at SQL D F dot head to make
26:23 - sure this worked and we can see that
26:25 - that worked well we still have the same
26:27 - data frame here that we started off with
26:29 - where we filtered down these countries
26:32 - to just be the results from India now
26:34 - there might be instances where you don't
26:36 - want to load in an entire table but you
26:38 - want to run a specific SQL query in
26:41 - order to load in our data to
26:43 - do this we can use the method read
26:45 - underscore SQL underscore query to run a
26:49 - specific SQL query so let me just copy
26:54 - what I did here and paste this down here
26:57 - and now instead of reading in this
26:59 - entire table I'm going to actually run a
27:02 - query here so I'll do read underscore
27:05 - sequel underscore query and now instead
27:08 - of the table name here I'm actually
27:10 - going to pass in a sequel query now I'm
27:13 - just going to load in everything here so
27:17 - I'll say select star from sample
27:20 - underscore table and everything else
27:22 - here is going to be the same we're still
27:24 - have we still have our database
27:26 - connection and we still want our index
27:28 - column to be equal to respondent so this
27:30 - is still going to grab all the rows but
27:33 - if you wanted to customize this then you
27:35 - could add in a where clause here to
27:38 - filter this down so let me run this and
27:41 - now let's look at our sequel data frame
27:45 - here and we can see that that worked as
27:48 - well so we loaded in this data using a
27:50 - sequel query instead of just reading in
27:52 - the entire table so that can be
27:54 - especially useful when you're working
27:56 - with large databases where you only want
27:59 - to load in specific data using a query
28:01 - okay so we're just about finished up
28:03 - here but let me show you one more tip
28:05 - before we wrap this up so you may have
28:08 - seen people load in data using a URL
28:11 - instead of a specific file for some of
28:14 - the methods that we've looked at before
28:15 - and we can do that all you need to do is
28:19 - you need to be sure that you're using
28:20 - the correct method for whatever form of
28:23 - data is on the URL so for example in my
28:26 - flask and Django series I created a JSON
28:29 - file of some sample posts for the
28:31 - website that we were creating in that
28:33 - series and I have that JSON file on my
28:35 - github page now if I wanted to bring
28:38 - that into pandas then I could simply use
28:40 - the re JSON method and then pass in that
28:43 - URL I wouldn't actually have to download
28:46 - that JSON first and then pass it in that
28:48 - way so I have this open here if you
28:51 - didn't know on github you can look at
28:53 - the raw files so we can see that this is
28:55 - a long URL here
28:57 - but I will have this code posted in the
29:00 - description section below if you'd like
29:01 - to follow along so I'm just going to
29:03 - copy this URL and this isn't on my file
29:06 - system and now let's see if we can just
29:09 - load this in so I'm gonna call this post
29:11 - underscore D F and I'll set this equal
29:14 - to PD dot read underscore JSON since
29:18 - this is JSON on the URL if it was CSV
29:21 - then you'd want to use read CSV and so
29:23 - on so now I can just paste in that URL
29:27 - there and now let's just run that cell
29:30 - and we can see that we didn't get any
29:32 - errors so let me now I look at the head
29:38 - of our data frame here and we can see
29:42 - that I do have my sample post here these
29:44 - are the sample posts that I used on that
29:48 - website series so depending on the data
29:51 - in that URL you should be able to use
29:53 - the methods that we've seen to load in
29:55 - data from a URL just like we did here
29:57 - now before we end here I would like to
30:00 - thank the sponsor of this video and that
30:02 - is brilliant I really enjoy the
30:04 - tutorials that brilliant provides and
30:06 - would definitely recommend checking them
30:07 - out brilliant is a problem-solving
30:09 - website that helps you understand
30:11 - underlying concepts by actively working
30:13 - through guided lessons and brilliant
30:14 - would be an excellent way to supplement
30:16 - what you learn here with their hands-on
30:18 - courses they have some excellent courses
30:20 - and lessons on data science that do a
30:22 - deep dive on how to think about and
30:24 - analyze data correctly so if you're
30:26 - watching my panda series because you're
30:27 - getting into the data science field then
30:29 - I would highly recommend also checking
30:31 - out brilliant and seeing what other data
30:33 - science skills you can learn they even
30:35 - use Python in their statistics course
30:36 - and will quiz you on how to correctly
30:38 - analyze the data within the language
30:40 - they're guided lessons will challenge
30:42 - you but you'll also have the ability to
30:43 - get hints or even solutions if you need
30:45 - them it's really tailored towards
30:47 - understanding the material so to support
30:49 - my channel and learn more about
30:50 - brilliant you can go to brilliant org /c
30:54 - ms2 sign up for free and also the first
30:57 - 200 people to go to that link will get
30:59 - 20% off the annual premium subscription
31:01 - and you can find that link in the
31:03 - description section below
31:04 - again that's brilliant dot org forge /c
31:08 - m/s ok so I think that's going to
31:11 - do it for this pain this video I hope
31:13 - you feel like you got a good idea for
31:14 - how to read and write data from multiple
31:17 - different sources what we covered here
31:19 - should cover the vast majority of file
31:20 - formats that you're going to be seeing
31:22 - and using in the data science field now
31:25 - I'm probably going to take a break from
31:26 - this Panda series after this video and
31:29 - do a few one-off videos that I've been
31:31 - wanting to cover but I know that there
31:33 - are a lot of topics and pandas left to
31:35 - cover and I will get around to those
31:37 - more advanced topics in future videos
31:39 - but in the meantime if you'd like a good
31:41 - source for learning pandas then I would
31:43 - highly recommend checking out the
31:44 - channel data school that's run by Kevin
31:47 - Marcum and he's done the pandas
31:49 - tutorials at PyCon for several years now
31:51 - now he didn't you know asked me to
31:53 - suggest his channel or anything like
31:55 - that I just think that he does a good
31:57 - job and his channel was actually
31:59 - completely devoted to penas and data
32:00 - science so he's already covered some of
32:03 - the more advanced topics that I do plan
32:04 - to cover in future videos but if anyone
32:07 - has any questions about what we covered
32:08 - in this video then feel free to ask in
32:10 - the comment section below and I'll do my
32:12 - best to answer those and if you enjoy
32:13 - these tutorials and would like to
32:14 - support them then there are several ways
32:16 - you can do that the easiest ways is
32:18 - simply like the video and give it a
32:19 - thumbs up and also it's a huge help to
32:21 - share these videos with anyone who you
32:23 - think would find them useful and if you
32:24 - have the means you can contribute
32:25 - through patreon and there's a link to
32:27 - that page in the description section
32:28 - below be sure to subscribe for future
32:30 - videos and thank you all for watching
32:43 - you

Cleaned transcript:

hey there how's it going everybody in this video we're going to be learning how to read and write data to different sources so we'll learn how to read and write data using CSV files Excel files JSON and also sequel databases now in this series so far we've been reading data from CSV files but in data science there are so many different ways for data to be stored so by the end of this video you should be able to get your data to and from pandas no matter what data format you're using now if you're watching this video because you're looking for how to read and write a specific file format then I'll be sure to add timestamps in the description section below to where we read and write from each different format now I would like to mention that we do have a sponsor for this series of videos and that is brilliant so I really want to thank brilliant for sponsoring this series and it would be great if you all can check them out using the link in the description section below and support the sponsors and I'll talk more about their services in just a bit so with that said let's go ahead and get started ok so first let's look at CSV files since we've already been using these throughout the series we should already be familiar with reading data in from CSV since that's what we've been doing so far but in case this is the first video of the series that you're watching let's go over this one more time and then we'll also learn how to write to a CSV file as well so up here towards the top of my notebook here we can see that I'm reading in this CSV file and this CSV file is within a data folder that is in the same location as this jupiter' notebook on the file system now if you have a CSV file loaded elsewhere on the system then you'll need to pass in the full path to that file instead of just this relative location that we have here and we can see that we have different arguments that we can pass in when reading our CSV files so in this example I'm at eclis setting the index to this respondent column here which is the respondent ID for each person who answered this survey and when I read in the CSV we can see that it sets this data frame equal to the data and we can print this data out down here at the bottom so that is the read CSV method and it allows us to pull data in to pandas now let's learn how to write this data back to a CSV maybe you're gonna make some changes and some different analysis here to your data frame and then we want to export this back to our file system for later use or so that we can share it with someone else or something like that so for example let's filter down for a specific location in this survey you know maybe you're doing some analysis for your specific country and you just want to see the survey results from that location we've seen this in previous videos but if we want to filter then we can simply say I'll create a filter here and just say that I want the country here and I'll grab if the country is equal to India so let's say you're doing some analysis and you only want the survey results from India so now I'm going to create a new data frame here I'll call this India DF and du ADF dot lok and pass in that filter so now if I do an India DF dot head to take a look at the beginning of this new data frame if we look over here in the country column then we can see that all of these countries here are now set to India so now let's say that we want to export this new filter data frame to a CSV file so to do this we can use the to CSV method so we can say I'll just say India underscore DF which is our data frame dot to underscore CSV and now I'm just going to pass it into that same location in that data directory and then I'll just call this modified dot CSV so if I run this we can see that we don't get any errors and now if I go back and look at my file system here then I have this modified D CSV so if I click on this then we can see that this is you know a little bunch together since it's a CSV file a raw CSV file that we're looking at but we can see that we have all of our column names here and then the second row should be the first result and I can see here that we have India for that country if I look at the second result we can see we have India again and India again down here most likely I can't see it but you we can just assume that it's there it's looking good oh and actually there it is right there so we can see that we did actually export that data frame where we filtered that down to a new CSV file okay so that was easy enough so now let's look at how to read and write to some other formats so one thing that you might run into is a tab delimited file these are almost exactly the same thing as CSV files but instead of your data being separated by a comma the data is instead separated by tabs so to do this in pandas we're still going to use the same CSV methods that we've already seen but we're going to pass in a custom separator so we can write to a tab delimited file just by changing the file extension here to dot TSV and I'm also going to specify a separate or argument so I'm going to say set s P and then you want to pass in your separator now you can pass in anything here if you want you know a file that is separated by hashes or anything but commas and tabs are probably the most common so I'm going to put a backslash T there because that's how we specify tabs in Python and now if I run this cell I'm going to go back to our data directory here we can see that now we have this modified dot T SV if I click on that then we can see that now this looks almost exactly the same as the comma separated file but now we have tabs here instead of commas now if you're reading in tab CSV files then all you need to do is take this se P equal to backslash T and you can just add that as an argument up here to read CSV so it's basically the same thing okay so now let's move on to some other file formats now a very popular file format when working with this kind of data is Excel now if we want to write to excel then we're going to need to pip install a couple of packages so I have my terminal open with the current environment that I am using this is my Jupiter notebook running here let me grab my other terminal so I have the same environment that I'm you within Jupiter you want to be sure that you're using that same environment so that your pip installing and the right location and now we're going to install a couple of packages so first I'm going to say pip install and this is XL WT so XL WT will write to an older XLS Excel format but if you want to write to a newer xlsx Excel format then we'll also need to install open pi excel and you can pip install multiple packages but just by listening them all right here and finally we want if we want to read excel files then we can install the excel our D package so I think that is the three packages that we're going to need in order to work with excel files here so I'll go ahead and install all of those and let those finish and once those are installed let's go back to our notebook and now let's try to write to an excel file so to write to an excel file I'm just going to write the same modified data frame that we have here and we are going to use the to underscore excel method and this is just as easy as passing in let's say I'll save it in that data folder again I'll call this modified dot xlsx so I'm going to write to the newer excel format so if I run this then it might take a second here for this to work because it's actually creating this excel file on the back end so let's let this finish and we can tell it's finished when this turns from an asterisk to a number here okay so once that's finished let's flip over to our data folder here and we can see that we do have that dot xlsx file now this likely won't open up in Jupiter because this is an excel file we can see here that we can't open this up in the browser we actually need Excel so let me open up my Finder window here I have this open down here and I am within this data folder and we can see that we have our modified dot xlsx file here now I don't actually have Excel on this machine I have numbers so I'm going to open this up in numbers it should basically be the same on Windows but you can just open it up with Excel now again this might take a second to open up because we do still have a lot of rows here in this data okay so we've got this opened up in Excel again I'm on numbers because I'm on a Mac and I don't have Excel installed but it should open up fine in Excel as well let me zoom in a little bit here so we can see and we can format these if if we need to so for example we can change the column sizes here so that all these fit in but we can see here that we have our respondent IDs if I look over at country we can see that it did export the filtered data frame that we were hoping to export so everything looks good here now there are also some more advanced things that we can do with Excel as well if you're familiar with Excel then you might know that we have the concept of different sheets where we can have multiple spreadsheets and one excel file and if you want to read or write to a specific sheet then you can pass in a sheet argument to these methods actually trying to scroll over to my notebook here let me scroll down here to the bottom so like I was saying if you want to read or write to a specific sheet then you can pass in a sheet argument to these methods and there's also a way to start from different columns and rows as well but I'm not gonna go into all these little details here if you google this method name to excel then you can find the arguments that you can pass in and all the additional details and the documentation so for now let's go ahead and move on and see how that we can read in that same excel file that we just created and make sure that this works now by default it's going to load in with a default index just like when we read a CSV file so we'll have to specify that we want our index column to be that respondent column so to do that I'm just going to call this test since we're going to be creating a new data frame here from that excel file that we just created and we're going to use the read underscore excel method here and now we just want to pass in the location and I'll just go ahead and copy this here so that is modified dot xlsx on my machine and now I want to set that index column equal to and that was respondent on your data that might be different but I want my index column to be equal to that respondent so I'm going to run that cell and load that in and then I'm going to look at that test data frame now before I run this I'm going to make sure that this finishes processing here and that this asterisk goes away again it can take some time because it's actually you know loading in that data from Excel now which is a little more tricky than loading it in from a CSV so now if we look at that test data frame let me just look at the head here instead of looking at the whole thing so if I look at the head then we can see that we have the same data frame here that we had up here so that was exported to excel and imported correctly okay so now let's cover some other popular file formats now JSON is also really popular for this kind of data so let's take a look at that first let's write our modified data frame to a JSON file now for writing to a JSON file then we can use the to JSON method so you're probably starting to see a pattern here these method names are very straightforward now this one is a bit different since there are some different orientations that we can use for JSON so just by using the default arguments I can just say so that was India D F dot to underscore JSON and now I'll pass in a file location here but instead of an excel file we want a JSON file now I'm just going to use the default arguments for now and then I'll show you how we can change this up a bit so if I run this we can see that ran very quickly now if I go back to my data folder here then now we have this JSON file if I look within here okay that took just a second to open up on my machine again we do have a lot of data in here but if we look in here then we can see that this is very dictionary like so we have a main branch key here and then the value for that key are all of the responses just for that column and if I was to scroll down here then I would be able to find the other keys and the other responses as well so by default this is a dictionary like JSON now there are also different ways that we can write JSON files again I'm not going to go into every single little detail here but let's say that we wanted this JSON to be list like instead of dictionary like which is how it is by default so to do this we can change the Orient argument so instead let's add on here to our arguments and I'm gonna say Orient is equal to and if we pass in records and lines equal to true then this will now make this records like which is list like and this lines equal to true let me spell that right we'll just make each of these on a new line so it might be a little bit easier to read now if you want to see the exact arguments that you can pass in to Orient then again just look pandas to JSON method and it'll take you to the documentation with all the different things that you can pass in here so let me run this and now let's go back and reload our JSON file to see how this looks and now what we have here is more lists like so before we had a single dictionary where the values were a list of all of the responses but now we have one response at a time so we have the main branch and then so this is actually this first one here if I scroll down we can see that this is the second response this is actually the entire first response so we have the main branch and then that answer and then open source and then that answer and then so on and we can see here that for the country we have India and each response within this survey is actually on a different line so that's a little bit different than and how it was before but there's just different ways that we can export these JSON files depending on your needs okay so now that we've written our data to JSON files now let's also read this JSON file so that we can make sure that we know how that's done as well now since we wrote the JSON file with these different arguments here then we need to use those same arguments when we read the data in as well so if you're reading in JSON files and have any issues then you might need to play around with the different arguments to fit the data that you're trying to read in so in this case I'm just going to copy this whole line here and I'm gonna say test is equal to and actually let me just grab this part and I'll say PD dot read underscore JSON and now I'll pass in all those arguments here so we are reading the JSON file from this location we know that the Orient is list like instead of dictionary like and that all of these are on new lines and again depending on your JSON data you might need to go in and change these around depending on how your data looks so if I run this then let's see if we have the same data that we exported before and it seems like we do this looks exactly like it did whenever we exported this data okay so now the last file format that we're gonna look at let's learn how we can read and write data from sequel databases now this is probably the most complicated simply because you have to have the database setup and all of that good stuff but for the sake of this video I'm going to assume that you already have a database with the correct credentials to log into that database so I have a Postgres database set up on my machine that will be reading and writing to so first let's see how we would connect to this database now just like with excel we're going to need to install a package to do this so let me bring up my terminal here and I'll close this numbers file here let's see let me try to quit out of this actually I'll just minimize it it's having trouble shutting down okay so let me go back to the terminal that I hope opens the two where I can install some different packages and that's my Jupiter notebook where is my other terminal here we go okay so to connect to our database we're going to want to install sequel alchemy and this is a very popular ORM for a Python that allows us to more easily work with databases if you don't know what an ORM is it stands for object relational mapper and it's just a way for us to use Python objects in order to connect to a database I plan on doing a complete video or a complete series on sequel alchemy in the future but for now let's go ahead and just install this so this is pip install SQL alchemy and I'll install that and depending on the database that you're using you might not need to do anything else here so for example if you're using sequel light or something like that but since I'm using a Postgres database for this tutorial I also need to install the psycho PG to package that allows us to work with Postgres I'm not sure if that's actually how you say that package name but that's what I've always called it so pip install and to install this package to work with Postgres it's psycho PG binary so I'll install that and with those packages installed let's go back to our notebook and see if we can connect to this database using sequel alchemy so first we're going to want to import everything that we need so from SQL alchemy I'm going to want to import there create engine and this will allow us to connect to the database now I'm also going to want to import psycho PG so let me run this sell and now that those are imported we should be able to create the engine which is basically our database connection and again I'm going to assume that you've already created this database and have a username and password so to create this I can say engine is equal to and use that create engine function that we just imported from seek welcoming and now we need our Postgres connection string now if you don't know how to make Postgres connection or connection strings then you know they have this available on the sequel alchemy site as well let me make sure I spelled this correctly that is PostgreSQL and then we want to pass in the username and password for our database now for my case I just made a user of DB user and a password of DB Pass now another thing here that I'd like to mention is that you probably shouldn't put credentials within code like this I'll leave a link in the description section below where I show how in Python you should use you know something like environment variables or a config file to hide this information but for the sake of this tutorial I'm just going to put it directly in here but if you're doing this in a in production code I would highly recommend using environment variables so that you know you don't expose your username and passwords within your codebase okay so there we have our username and password and now the database that we want to connect to so this is on localhost this is on my local machine it's running on port 5 4 3 2 and now the name of the database now I have PG admin open here where I can see my databases and we can see that I've just created an empty database here called sample underscore DB so that is the database that I'm going to connect to ok so if I typed everything correctly here then I should be able to get a connection to that database so now let's try to write our modified data frame to a table in this database and this table doesn't need to currently exist by default it will create this table for us if it does already exists then we'll need to add another argument to handle that but we'll see that in just a second so to do this I can just say India underscore DF which is the data frame we want to export then this is to underscore SQL and now the table that we want to write this data to I'm just going to call this sample underscore table now again this doesn't currently exist but it should create it now we need to pass in our database connection here I called mine engine so let's pass that in and if I run this let's see if this works ok so we didn't get any errors whenever I read that or whenever I wrote that but now let's go back to my PG admin here and let's see if I can see this table so first I'm just going to right click and refresh I like to do that anytime I've made any changes and we can see there that here that we have a sample table down here I'm going to right click on that and go to view and edit data and look at all the rows here and we can see it does look like this worked I know that this is probably a little difficult to see on my screen here but we have all of our data written here into the database ok so that's good that we were able to get this from pandas into SQL but now what if we updated our data and wanted to rewrite that data to this database so let's go back to our notebook and see what this would look like now if I try to run this same line again where we export this to SQL then we're actually going to get it in error because this table already exists if you want to write over a table then we can add in an additional argument and the argument that we want to add in is called if underscore exists equals and now what we want to do if this table already exists now in my case I'm just going to replace that table with our new data but there are also other options as well we could have it throw an error we could which is what it does by default we could also append data to a table so if you're doing like a daily script where you're analyzing information then you can just append that daily data to your existing table but for this example I'm just going to have this replace the table so let's run this and once this is finished processing then I will go back to PG admin now again let me come up here and refresh this and dig back down into the database and let me close this view here and let's see if we still have this data okay so we can see that this worked we were able to rerun that command and it just replaced that data that was in that existing table with our new data in this case it was the same data but that's how you would do that okay so lastly now that we've seen how to add our data to a database now let's see how we can read in this same data using SQL now if you skip to this part of the video using the timestamps and the description sections below then please go back to when we wrote data to our database and see how I set up this database connection here because we're going to reuse that same connection to read in our data okay so this is pretty simple now that we actually have this database connection set up to do this we can just say I'll call this SQL underscore DF and we will just say PD dot read underscore SQL and now we want to pass in the table that we're going to read from and that was sample underscore table and now pass in our database connection my connection here I called engine and also I'm also going to pass in an index column just like we did when we read in our CSV so I'll say index column is equal to and that is going to be this respondent row right here for your data that might be different so whatever you want to be your index just pass it in there if you want pandas to just do a default index then you can just leave this off entirely okay so if I run this then let's look at SQL D F dot head to make sure this worked and we can see that that worked well we still have the same data frame here that we started off with where we filtered down these countries to just be the results from India now there might be instances where you don't want to load in an entire table but you want to run a specific SQL query in order to load in our data to do this we can use the method read underscore SQL underscore query to run a specific SQL query so let me just copy what I did here and paste this down here and now instead of reading in this entire table I'm going to actually run a query here so I'll do read underscore sequel underscore query and now instead of the table name here I'm actually going to pass in a sequel query now I'm just going to load in everything here so I'll say select star from sample underscore table and everything else here is going to be the same we're still have we still have our database connection and we still want our index column to be equal to respondent so this is still going to grab all the rows but if you wanted to customize this then you could add in a where clause here to filter this down so let me run this and now let's look at our sequel data frame here and we can see that that worked as well so we loaded in this data using a sequel query instead of just reading in the entire table so that can be especially useful when you're working with large databases where you only want to load in specific data using a query okay so we're just about finished up here but let me show you one more tip before we wrap this up so you may have seen people load in data using a URL instead of a specific file for some of the methods that we've looked at before and we can do that all you need to do is you need to be sure that you're using the correct method for whatever form of data is on the URL so for example in my flask and Django series I created a JSON file of some sample posts for the website that we were creating in that series and I have that JSON file on my github page now if I wanted to bring that into pandas then I could simply use the re JSON method and then pass in that URL I wouldn't actually have to download that JSON first and then pass it in that way so I have this open here if you didn't know on github you can look at the raw files so we can see that this is a long URL here but I will have this code posted in the description section below if you'd like to follow along so I'm just going to copy this URL and this isn't on my file system and now let's see if we can just load this in so I'm gonna call this post underscore D F and I'll set this equal to PD dot read underscore JSON since this is JSON on the URL if it was CSV then you'd want to use read CSV and so on so now I can just paste in that URL there and now let's just run that cell and we can see that we didn't get any errors so let me now I look at the head of our data frame here and we can see that I do have my sample post here these are the sample posts that I used on that website series so depending on the data in that URL you should be able to use the methods that we've seen to load in data from a URL just like we did here now before we end here I would like to thank the sponsor of this video and that is brilliant I really enjoy the tutorials that brilliant provides and would definitely recommend checking them out brilliant is a problemsolving website that helps you understand underlying concepts by actively working through guided lessons and brilliant would be an excellent way to supplement what you learn here with their handson courses they have some excellent courses and lessons on data science that do a deep dive on how to think about and analyze data correctly so if you're watching my panda series because you're getting into the data science field then I would highly recommend also checking out brilliant and seeing what other data science skills you can learn they even use Python in their statistics course and will quiz you on how to correctly analyze the data within the language they're guided lessons will challenge you but you'll also have the ability to get hints or even solutions if you need them it's really tailored towards understanding the material so to support my channel and learn more about brilliant you can go to brilliant org /c ms2 sign up for free and also the first 200 people to go to that link will get 20% off the annual premium subscription and you can find that link in the description section below again that's brilliant dot org forge /c m/s ok so I think that's going to do it for this pain this video I hope you feel like you got a good idea for how to read and write data from multiple different sources what we covered here should cover the vast majority of file formats that you're going to be seeing and using in the data science field now I'm probably going to take a break from this Panda series after this video and do a few oneoff videos that I've been wanting to cover but I know that there are a lot of topics and pandas left to cover and I will get around to those more advanced topics in future videos but in the meantime if you'd like a good source for learning pandas then I would highly recommend checking out the channel data school that's run by Kevin Marcum and he's done the pandas tutorials at PyCon for several years now now he didn't you know asked me to suggest his channel or anything like that I just think that he does a good job and his channel was actually completely devoted to penas and data science so he's already covered some of the more advanced topics that I do plan to cover in future videos but if anyone has any questions about what we covered in this video then feel free to ask in the comment section below and I'll do my best to answer those and if you enjoy these tutorials and would like to support them then there are several ways you can do that the easiest ways is simply like the video and give it a thumbs up and also it's a huge help to share these videos with anyone who you think would find them useful and if you have the means you can contribute through patreon and there's a link to that page in the description section below be sure to subscribe for future videos and thank you all for watching you
