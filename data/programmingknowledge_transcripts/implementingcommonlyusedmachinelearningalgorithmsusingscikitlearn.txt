00:00 - first we need to understand what machine
00:02 - learning means with the use case or with
00:04 - an example
00:05 - we need to know the different types of
00:06 - machine learning and also how machine
00:08 - learning can be used in different
00:09 - industries in the world
00:11 - for this you can check out the intro to
00:13 - ml video by the programming knowledge
00:15 - channel and also read multiple blogs
00:16 - online which explain ml better
00:19 - the next thing is python programming
00:21 - skills as we all know
00:22 - python is the widely used programming
00:25 - language
00:26 - for machine learning so we must have the
00:28 - python program basics
00:29 - right to start learning machine learning
00:33 - again the programming knowledge channel
00:35 - has passion tutorial videos which you
00:37 - can check out
00:38 - to get your basics right and you can
00:40 - also follow different tutorials and blog
00:41 - posts
00:43 - the next thing is basic mathematical
00:45 - skills
00:47 - machine learning has a lot of
00:49 - mathematics involved behind the scenes
00:50 - and
00:51 - to understand machine learning properly
00:52 - one must
00:54 - have these basics in their bags first is
00:57 - linear algebra basics
00:59 - we should know how to solve a linear
01:00 - equation in one variable or two
01:02 - variables
01:03 - and for probability basics we just need
01:05 - to do the simple very simple basis of
01:07 - probability
01:08 - uh the basic formula and how probability
01:10 - works after this
01:13 - we can branch out to do different types
01:15 - based on what our end goal is so
01:17 - the first is academic machine learning
01:20 - if you want to produce novel research
01:22 - write research papers read research
01:24 - papers
01:25 - build machine learning tools from
01:26 - scratch and understand the deep
01:28 - underlying concepts then
01:30 - you can refer to these prerequisites
01:32 - before jumping into the academic machine
01:34 - learning part of it
01:36 - so let's start the first thing is strong
01:38 - mathematical concepts
01:40 - as you'll be producing novel research
01:41 - new research or try to read
01:43 - complicated research papers and also
01:45 - build tools from scratch
01:46 - you should have a strong mathematical
01:49 - foundation
01:51 - in algebra we must know how logarithms
01:54 - work how matrices work
01:56 - how to perform matrix multiplication and
01:58 - things like that
02:00 - in calculus we should know what does the
02:02 - concept of derivative
02:03 - what is the chain rule partial
02:05 - derivatives and gradients
02:07 - in statistics we should know the simple
02:09 - statistics basics which is mean median
02:11 - mode
02:12 - then how outliers work the ability to
02:15 - read an histogram
02:16 - and some algorithms like conditional
02:18 - probability
02:20 - next is intermediate python programming
02:23 - since we'll be writing some tools from
02:25 - scratch and also write the code for the
02:27 - new
02:28 - type of algorithms which we read or try
02:31 - to implement
02:31 - we should have some intermediate python
02:33 - programming skills and use some
02:35 - complicated
02:36 - structures which can help us write
02:38 - better code for this
02:39 - we can learn list comprehensions lambda
02:42 - functions and also learn some
02:43 - third-party libraries
02:44 - which make our jobs easier and the last
02:47 - thing is strong programming science
02:49 - computer science fundamentals
02:51 - as a computer science graduate or not
02:54 - being one
02:55 - we should have some simple data
02:56 - structures and algorithms in our back
02:58 - just to make sure that the concepts
03:00 - which we implement
03:01 - have been implemented in the most
03:03 - efficient way possible
03:06 - this is academic machine learning this
03:08 - helps you
03:09 - get into academia faster and also will
03:11 - help you have a better learning curve
03:14 - than before the next is industrial
03:16 - machine learning now if you want to
03:19 - work in industry or in a company or be a
03:21 - software engineer with a machine
03:23 - learning
03:23 - specialization if you want to create new
03:26 - products
03:27 - business value and apply existing
03:29 - off-the-shelf tools to solve business
03:30 - problems then
03:31 - you can refer to this as the
03:33 - prerequisite
03:35 - so in a company or being a software
03:37 - developer with a machine learning
03:38 - specialization
03:39 - the first thing which you need to know
03:41 - is intermediate or advanced python
03:42 - programming so
03:44 - as you'll be writing a lot of code and
03:45 - we'll be busy
03:47 - focusing more on writing code than doing
03:49 - research
03:50 - the companies will expect you to know a
03:52 - lot of python
03:54 - to use the tools which are off the shelf
03:57 - again things like list comprehension
03:59 - lambda functions and also
04:00 - understanding how third-party libraries
04:01 - work with machine learning
04:04 - the second thing is similar to the
04:06 - academic machine learning part of it
04:07 - which is strong computer science
04:08 - fundamentals because
04:09 - these fundamentals will be useful no
04:12 - matter which
04:14 - branch are you in you are in and
04:17 - next we come to the most important part
04:19 - data analysis
04:21 - so before trying to implement machining
04:23 - algorithms in a company
04:25 - one must need to know what the data is
04:27 - uh
04:28 - what the data is trying to represent and
04:30 - also understand
04:32 - how the data works so people should have
04:34 - a proficiency with frameworks and tools
04:36 - such as pandas matplotlib and cbon
04:40 - in the upcoming videos we'll talk about
04:41 - how these frameworks work and also
04:44 - look at them with the code lastly
04:47 - people who would like to work in a
04:49 - company as a software engineer or as a
04:51 - data scientist they should have minimum
04:53 - linux skills they should have
04:55 - they should know how to work on the
04:56 - terminal how to work on the command line
04:59 - how to use that to the benefit and
05:02 - also make sure that they have enough
05:06 - to write most of the simple code on the
05:08 - terminal
05:10 - now these are some prerequisites which i
05:13 - feel can give you a bit of a better
05:15 - learning curve when you're trying to
05:16 - jump into machine learning you can also
05:18 - explore both the
05:20 - branches you can try academic machine
05:22 - learning first and you can jump onto
05:24 - industrial
05:24 - or vice versa now
05:27 - let's talk about some resources all the
05:30 - resources to learn these prerequisites
05:31 - will be in the description below and
05:33 - also this notion
05:35 - document which you see here will also be
05:36 - in the description below
05:38 - if you have any questions please use the
05:40 - comment section below to ask them
05:42 - in this video we'll start with what is
05:44 - linear regression
05:45 - the intuition behind the algorithm and
05:47 - understanding all the elements of it
05:49 - let's have an overview of linear
05:50 - regression it is one of the most basic
05:53 - machine learning algorithm
05:55 - and easy to implement the algorithm has
05:57 - already been used in statistics
05:59 - and is a common process using many
06:01 - applications of statistics in the real
06:04 - world
06:06 - so what is linear regression
06:09 - by definition it is used for finding a
06:12 - linear relationship between the target
06:14 - and one or more predictors the idea
06:16 - behind
06:17 - linear regression is to fit the
06:19 - observations
06:20 - of two variables into a linear
06:22 - relationship between them
06:25 - in simple terms the task is to draw the
06:28 - line that is best fitting
06:29 - or closest to the points where the x y
06:32 - coordinates are observations of the two
06:34 - variables
06:34 - which are expected to depend linearly on
06:37 - each other
06:38 - in more simpler terms given two
06:40 - variables x and y
06:42 - the model can predict values of y given
06:44 - future observations of x
06:47 - this idea is used to predict variables
06:50 - in countless situations
06:51 - example the outcome of political
06:54 - elections
06:55 - or the behavior of the stock market or
06:57 - the performance of a professional
06:58 - athlete
07:00 - there are two types of linear regression
07:02 - simple and multiple
07:04 - in this video we'll only cover simple
07:06 - linear regression
07:10 - so we had talked about drawing a line
07:12 - that is closest to the points which are
07:14 - our variables
07:15 - this line can be modeled based on a
07:18 - linear equation shown
07:19 - on the slide here x and y
07:23 - are our variables which will be present
07:24 - in the data set
07:26 - the motive of the linear regression
07:28 - algorithm is to find the best values for
07:30 - a0 and a1
07:32 - which we call the parameters before
07:35 - moving on to the algorithm
07:36 - let's have a look at two important
07:38 - concepts you must know before
07:40 - understanding linear regression
07:44 - cost function the cost function helps us
07:46 - to figure out
07:47 - the best possible values for a0 and a1
07:50 - which would provide the best fit line
07:53 - for the data points
07:55 - the difference between the predicted
07:56 - values and the ground truth measures
07:59 - the error difference we square the error
08:01 - difference and sum over all the data
08:03 - points and divide that value
08:05 - by the total number of data points
08:08 - this provides the average squared error
08:11 - over all the data points
08:12 - therefore this cost function is also
08:14 - known as the mean square
08:15 - error by cost
08:18 - i mean the cost of incorrectly
08:20 - predicting a data point or how far
08:22 - the line is from the point
08:26 - mathematically we find the mean distance
08:28 - between all the points
08:29 - and we want to minimize that distance so
08:31 - that the line fits the data perfectly
08:34 - to minimize the cost function we use a
08:36 - technique called gradient descent
08:39 - the next important concept needed to
08:41 - know linear regression
08:43 - is gradient descent gradient descent is
08:45 - a method of updating
08:46 - a0 and a1 to reduce the cost function
08:50 - the idea is that we start with some
08:53 - values of a0
08:54 - and a1 and then we change these values
08:56 - iteratively to reduce the cost
08:58 - gradient descent helps us how to change
09:00 - the values
09:03 - to update a0 and a1 we take gradients
09:06 - from the cost function
09:07 - to find these gradients we take partial
09:09 - derivatives
09:10 - with respect to a 0 and a1 now to
09:13 - understand how partial derivatives work
09:15 - you would require some calculus but if
09:17 - you don't it is all right
09:18 - you can take it as it is the partial
09:21 - derivatives are the gradients and they
09:23 - are used to update the values of a0 and
09:25 - a1
09:26 - alpha is the learning rate here which is
09:28 - a hyper parameter that you must
09:30 - specify a small learning rate could get
09:34 - you closer to the minima
09:35 - but takes more time to reach the minima
09:38 - a larger earning rate converges sooner
09:40 - but there is a chance that you could
09:41 - overshoot the minima
09:43 - this can be depicted on the slide
09:47 - to implement the algorithm we have two
09:49 - choices we can use
09:50 - the circuit learn library to import the
09:52 - linear regression model and use it
09:54 - directly
09:55 - or we can write our own regression based
09:57 - model based on the equations above
09:59 - in this video we'll implement linear
10:01 - regression using the scikit learn
10:02 - library
10:03 - to learn more about what is linear
10:05 - regression you can check out the link to
10:07 - the video in the description
10:09 - the entire code and the data set can be
10:11 - downloaded using the link in description
10:14 - which will direct you to this github
10:16 - page after this download the data
10:18 - directory
10:19 - and store that in your projects folder
10:24 - let's start with the implementation i'm
10:27 - using a jupyter notebook here
10:28 - but you can implement the same in a
10:30 - single python file as well
10:33 - first we start with importing all the
10:35 - libraries and the dependencies that are
10:37 - required
10:39 - we need the pandas library to manipulate
10:41 - the data set
10:48 - next we nee we import the matplotlib
10:50 - library to visualize our data and the
10:52 - results
10:57 - we use the pi plot here
11:00 - and lastly we need the linear regression
11:03 - model from the circuit learn library
11:05 - which is the main dependency so from
11:09 - sklearn dot linear
11:12 - model we import the linear regression
11:16 - class
11:22 - now we start with reading our data into
11:25 - the code using pandas
11:27 - make sure that the data directory is in
11:29 - the projects folder
11:33 - we use the read csv function here
11:35 - because our data is in the csv format
11:39 - we move inside a data directory and
11:42 - use the advertising data set
11:50 - let's just check if the spelling is
11:51 - right
11:54 - and yes that should be good
11:57 - now to see what the data looks like we
12:00 - use the head function which is data dot
12:02 - head
12:06 - as you can see here the column unnamed 0
12:09 - is redundant and hence
12:11 - we need to remove it to remove a column
12:14 - we can use the drop function
12:15 - in pandas
12:19 - we have to remove the unnamed column
12:23 - name 0 and we specify the axis equal to
12:26 - 1.
12:28 - here axis is equal to 1 to remove the
12:30 - entire column and the axis is equal to 0
12:33 - to remove only an index as you can see
12:36 - in the output
12:37 - the unnamed zero column is being has
12:39 - been removed
12:42 - all right now our data is clean and it
12:44 - is ready for linear regression
12:47 - for simple linear regression let's
12:49 - consider only the effect of
12:51 - tv ads on sales
12:55 - before jumping right into the modeling
12:57 - let's look at what the data looks like
13:01 - we use matplotlib a popular python
13:03 - plotting library to make a scatter plot
13:10 - let's set
13:13 - a size of the plot which can be 16
13:17 - comma 8. then we generate
13:21 - a scatter plot using the scatter
13:24 - function
13:25 - in which we have the tv ads
13:29 - and the sales
13:33 - let's color the
13:37 - scatter plot with a black dot with black
13:40 - dots
13:44 - and as you can see there is a clear
13:47 - relationship between the amount
13:49 - spent on tv ads and the sales
13:54 - let's see how we can generate a linear
13:56 - approximation of this data
13:59 - first we convert these values into
14:01 - vectors and then store them into two
14:03 - variables
14:04 - so x is equal to data of the tv ads
14:10 - their values and we convert them into
14:13 - vectors using the dcf function
14:15 - which is minus 1 and 1 then we do
14:18 - do the same for the sales which is
14:21 - date of sales and their values
14:24 - which are converted into vectors
14:30 - now after this we use the fit function
14:33 - of the linear regression class
14:35 - to fit a line on the x and y values
14:39 - let's name the variable reg which is
14:42 - linear
14:43 - regression object
14:46 - and then we call the fit function on x
14:48 - and y
14:54 - the minimization of the cost function
14:56 - using gradient design works behind the
14:58 - scenes here
14:59 - behind the fit function to learn more
15:01 - about the cost function and how gradient
15:03 - descent works
15:04 - you can check out the introduction to
15:05 - linearization video in the description
15:06 - below
15:08 - now we have fit a straight line to the
15:11 - data set and let's visualize this using
15:12 - a scatter plot again
15:16 - now since the code for visualizing the
15:19 - best fit line is long i'm going to copy
15:20 - paste it but the entire code will be
15:22 - available
15:23 - in the github repo now here
15:27 - first we predict all the values on the x
15:31 - data set and then we use those
15:33 - predictions to make a line
15:34 - on the scatter plot here the dots will
15:38 - be in black
15:39 - and the line will be in blue the x
15:42 - label will be the money spent on the tv
15:44 - ads and the y label will be
15:46 - the sales
15:50 - from the graph it seems that a simple
15:52 - linear regression
15:53 - model can explain the general impact of
15:55 - amounts spent on tv ads and sales
15:59 - this is how we implement linear
16:01 - regression in scikit learn live using
16:03 - the scikit-learn library
16:05 - we'll talk about what is logistic
16:06 - regression classification techniques are
16:09 - an essential part of machine learning
16:10 - and determining applications
16:12 - approximately 70 percent of problems in
16:14 - data science are classification problems
16:17 - there are a lot of possible
16:18 - classification problems that are
16:20 - available
16:20 - but the logic regression is common and
16:22 - is a useful regression method for
16:24 - solving the binary classification
16:26 - problem
16:27 - logistic regression can be used for
16:29 - various classification problems such as
16:31 - spam detection
16:32 - diabetes prediction if a customer will
16:34 - purchase a product or not
16:36 - whether the user will click on a given
16:38 - ad link or not
16:41 - logistic regression is one of the most
16:43 - simple and commonly used machine
16:44 - learning algorithm
16:45 - for two class classification it is a
16:47 - statistical method to predict binary
16:49 - classes
16:50 - it its basic fundamental concepts are
16:53 - also used in deep learning
16:56 - it is a special case of linear
16:57 - regression where the target variable is
16:59 - categorical in nature
17:01 - it uses a log of odds as a dependent
17:05 - variable
17:06 - logistic regression predicts the
17:07 - probability of a binary event
17:10 - utilizing a logic function
17:14 - as we can see here we need to categorize
17:16 - the data in two different categories
17:18 - and our job is to define the line which
17:20 - does that
17:22 - now why is it called logistic regression
17:24 - if it's a classification
17:25 - mechanism contrary to popular belief
17:29 - logistic regression is a regression
17:31 - model the model builds
17:33 - a regression model to predict the
17:34 - probability that a given data entry
17:36 - belongs to the category numbered as one
17:38 - just like linear regression assumes that
17:41 - the data follows a linear function
17:43 - logistic regression models the data
17:46 - using the sigmoid function
17:47 - linear regression gives you continuous
17:49 - output
17:51 - but loyalty regression provides a
17:52 - constant output
17:54 - an example of continuous output would be
17:56 - house price prediction or stock price
17:57 - prediction
17:58 - an example of discrete output is
18:00 - predicting whether a patient has cancer
18:02 - or not
18:02 - or predicting whether a customer will
18:04 - click on an add or not
18:07 - now let's modify the linear regression
18:09 - equation we had seen in the previous
18:11 - video
18:11 - for logistic regression we apply
18:14 - something called as a sigmoid function
18:15 - on the linear linear regression equation
18:18 - let's see what the sigmoid function is
18:22 - the sigmoid function also called the
18:24 - logistic function
18:26 - gives an s-shaped curve that can take
18:28 - any real valued number
18:30 - and map it into a value between 0 and 1.
18:35 - if the curve goes to positive infinity y
18:38 - predicate predicted
18:40 - will be 1 and if the curve goes to
18:41 - negative infinity
18:43 - y predicted will become zero if the
18:46 - output of the sigmoid function
18:47 - is more than zero point five we can
18:49 - classify the outcome as
18:51 - yes or a one and if it is less than zero
18:53 - point five
18:54 - we can classify it as zero or a no for
18:57 - example
18:57 - if the output is 0.75 we can say in
19:00 - terms of probability as
19:02 - there is 75 percent chance that patient
19:04 - will suffer from cancer
19:08 - just like we have a cost function in
19:09 - linear regression we need one
19:11 - for logistic regression as well which
19:13 - has to be reduced to obtain the best fit
19:15 - line
19:16 - but the cost function used in linear
19:18 - regression will not work here
19:20 - if you try to use the linear regression
19:22 - cost function
19:23 - in a logistic regression problem you
19:25 - would end up with a non-convex function
19:27 - a weirdly shaped graph with no easy way
19:29 - to find minimum global point
19:31 - hence we have a different cost function
19:33 - for linear regression
19:37 - for logistic regression the cost
19:38 - function is defined as
19:40 - minus log h of x if y equal to 1 and
19:43 - minus log 1 minus h of x
19:45 - if y equal to 0. this is the cost the
19:48 - algorithm pays
19:50 - if it predicts a value h theta of x
19:52 - while the actual cost label turns out to
19:54 - be y
19:56 - by using this function we will grant the
19:58 - convexity to the function
19:59 - the gradient descent algorithm has to
20:01 - process
20:03 - there is also a mathematical proof of
20:05 - how we get this cost function which is
20:07 - outside the scope of this video
20:09 - the final cost function can be seen at
20:11 - the bottom of the slide
20:15 - now we have the hypothesis function and
20:17 - the cost function and we are almost done
20:19 - it is now time to find the best values
20:21 - for our parameters in the cost function
20:24 - or in other words to minimize the cost
20:26 - function by running the gradient
20:27 - decision algorithm
20:29 - the procedure is identical to what we
20:32 - did for linear regression to
20:34 - understand more about gradient descent
20:36 - please find the link in the description
20:38 - which will explain in regression and
20:39 - also gradient descent
20:42 - to minimize the cost function we have to
20:44 - run the gradient descent function on
20:46 - each parameter
20:47 - and that is how logistic regression
20:49 - works
20:50 - at the end we get the best parameters
20:52 - that can work with the hypothesis
20:54 - function to predict whether
20:56 - a data point belongs to one class or the
20:58 - other
21:00 - now for the implementation we can either
21:03 - use the circuitron library to import the
21:05 - logitech regression model
21:06 - and use it directly or we can also write
21:08 - our own model based on the equations
21:10 - above
21:11 - logistic regression is amongst the most
21:13 - commonly known core machine learning
21:15 - algorithm out there
21:16 - with its cousin linear regression it has
21:19 - many applications in businesses
21:21 - one of which is the pricing optimization
21:23 - in this video
21:24 - we will learn how to code logistic
21:26 - regression in python
21:28 - using the scikit learn library to solve
21:30 - a bit pricing problem
21:33 - let's have some recap logistic
21:35 - regression is a predictive linear model
21:37 - that aims to explain the relationship
21:39 - between
21:40 - a dependent binary variable and one or
21:42 - more independent variables
21:44 - the output of logical regression is a
21:47 - number between 0 and 1
21:48 - which you can think of as being the
21:50 - probability that a given class is true
21:53 - or not the output is between 0 and 1
21:56 - because
21:57 - the output is transformed by a function
21:59 - which is usually
22:00 - the sigmoid function let's start
22:04 - implementing logic regression in python
22:06 - with a very simple example
22:08 - note that the intent of this video is
22:11 - only to implement
22:12 - a very basic logistic regression model
22:15 - using
22:15 - circuit learn without using a trained
22:18 - test split on the data set
22:20 - and with minimum data visualization
22:23 - so let's start first we import all the
22:26 - dependencies that are required
22:30 - we need matplotlib
22:34 - for visualization so we need the pi plot
22:38 - as plt next is
22:42 - numpy to store our data
22:45 - and finally we need the sklearn
22:50 - logistic regression model which we can
22:53 - use
22:56 - to fit our data
23:01 - yeah so the next
23:04 - is that we have to define a data set
23:07 - let's generate a data set
23:09 - that will be using to learn how to apply
23:11 - logistic regression to a pricing problem
23:15 - the bid price is contained in our x
23:17 - variable while the result
23:18 - a binary lost or one category is encoded
23:21 - as
23:22 - one or zero in our y variable
23:26 - here i have defined my own data set but
23:28 - for complicated or more advanced
23:30 - uh examples you can also import a data
23:32 - set from kegel and use that
23:39 - let's go ahead and visualize this data
23:41 - using matplotlib to gain a better
23:42 - understanding of what we're dealing with
23:45 - let's have a scatter plot of x and
23:49 - y and
23:52 - let's actually give it a title of
23:56 - pricing bins
24:00 - and the x label is going to be
24:04 - price and the
24:07 - y label is the binary output 1
24:12 - or loss so status
24:17 - 1 is a 1
24:20 - and 0 is a lost
24:26 - so here each point above represents a
24:30 - build that we participated in
24:32 - on the x-axis you can see the price that
24:34 - was offered and on the y-axis
24:36 - you see the result if we won the bid or
24:40 - not
24:42 - our goal is to use logistic regression
24:45 - to come up with a model that generates
24:46 - the probability of winning
24:48 - or losing a bed at a particular place
24:51 - in python logistic regression is made
24:54 - simple
24:55 - thanks to the circuit learn module for
24:57 - the task at hand we'll be using the
24:59 - logic regression class
25:00 - by the sql linear model
25:04 - so log reg let's start let that be the
25:08 - name of the variable
25:09 - and logistic
25:12 - regression class
25:15 - where the regularization strength
25:18 - c is equal to 1.0
25:22 - and the solver uh let
25:26 - that be lb
25:30 - fgs which is an optimization just like
25:32 - we did in descent
25:34 - and for multi-class we specify ovr
25:37 - because we're using a binary
25:38 - classification problem here so
25:40 - multi-class is equal to
25:44 - ovr for binary classification
25:49 - the next step is to fit the logical
25:51 - equation model by running the fit
25:53 - function of a class
25:54 - and before we do that we transform our x
25:57 - array into a 2d array as is required by
25:59 - the sql model
26:01 - this is because we only have one feature
26:02 - which is the price and if we had more
26:04 - than one feature our array would already
26:06 - be 2d
26:08 - so let's reshape our data
26:12 - as one comma minus one comma one
26:17 - and finally we can fit our model so log
26:20 - reg
26:20 - dot fit capital x and y
26:26 - now we have a model and now let's
26:30 - predict some data if we wanted to run
26:32 - the prediction on a specific price you
26:34 - can also do that
26:35 - as shown so let's print
26:39 - a prediction
26:42 - uh let's say
26:46 - we need to find whether we've lost or
26:48 - won if the price
26:50 - is 110 so as you can see on the graph
26:53 - above
26:54 - if the price is 110 we should be winning
26:58 - so let's try that
27:01 - as you can see when the price is around
27:03 - 110 which is between 100
27:05 - 200 we win the bed and
27:08 - if the price is
27:12 - around 275 we should lose the bet
27:16 - so let's try that again
27:20 - which 275 we should lose the bet
27:24 - as you can see we have lost the bed this
27:27 - is a very basic implementation of
27:29 - florida's regression using the
27:30 - scikit-learn
27:31 - library to understand how the algorithm
27:33 - works on a data set
27:34 - as we have a basic understanding now we
27:36 - can start working with the kegel data
27:38 - set
27:38 - and also study more about data analytics
27:40 - and data visualization
27:42 - this video is introduction to support
27:44 - vector machines
27:45 - for an in-depth understanding please
27:47 - refer to the links in the description
27:52 - support vector machines are perhaps one
27:54 - of the most popular and talked about
27:55 - machine learning algorithms
27:57 - they were extremely popular around the
27:59 - time they were developed in the 1990s
28:01 - and continued to be the go-to method for
28:04 - a high performing algorithm
28:05 - with a little tuning support vector
28:08 - machine
28:09 - is a supervised machine learning
28:11 - algorithm which can be used for
28:12 - classification challenges
28:14 - in addition to performing linear
28:16 - classification svms can efficiently
28:18 - perform on non-linear classification
28:21 - as well so what are support vector
28:25 - machines
28:25 - it is a discriminative classifier
28:28 - formally defined by a separating
28:30 - hyperplane
28:31 - in other words given label training data
28:34 - the algorithm outputs an optimal
28:36 - hyperplane which categorizes new
28:38 - examples
28:41 - in simple terms an svm model is a
28:44 - representation
28:45 - of the examples as points in space
28:47 - mapped
28:48 - so that the examples of the sub examples
28:50 - of the separate categories are divided
28:52 - by a clear gap
28:53 - that is as wide as possible let's
28:56 - visualize this
28:58 - in this graph we can see that the two
29:00 - classes are separated by the largest cap
29:02 - possible
29:03 - the space between the red line and the
29:05 - closest point to the red line is called
29:07 - a margin
29:09 - so for one dimensional data the support
29:12 - vector classifier
29:13 - is a point for two dimensional data the
29:15 - support vector classifier is aligned as
29:17 - seen in the previous slide
29:19 - for three dimensional data the support
29:21 - vector is a plane
29:23 - and for four dimensional or more the
29:25 - support vector classifier
29:26 - is a hyperplane so let's talk about the
29:30 - hyperplane now
29:31 - a hyperplane in an n-dimensional
29:33 - euclidean space
29:35 - is a flat n minus one-dimensional subset
29:38 - of that space
29:39 - that divides the space into two
29:40 - disconnected parts
29:42 - so a line is a hyperplane or even a 2d
29:45 - plane for a 3d data
29:47 - is a hyperplane
29:50 - svm algorithms use a set of mathematical
29:53 - functions that are defined as the kernel
29:56 - sometimes it is not possible to find a
29:58 - hyperplane
29:59 - or a linear decision boundary for some
30:01 - classification problems
30:02 - if we project the data into higher
30:04 - dimension from the original space
30:06 - we may get a hyperplane in the projected
30:08 - dimension that helps to classify the
30:10 - data
30:12 - let's see what we mean here as shown in
30:15 - the figure
30:16 - it is impossible to find a line to
30:18 - separate the two classes
30:19 - green and blue in the input space but
30:23 - after projecting the data into higher
30:24 - dimension
30:26 - we were able to classify the data using
30:29 - the
30:30 - hyper plane hence kernel helps to find
30:34 - a hyper plane in the higher dimension
30:36 - space without increasing
30:37 - the computation cost much usually the
30:40 - computational cost will increase if the
30:42 - dimension of the data increases
30:45 - the mathematics behind how kernels work
30:47 - is out of scope for this video
30:51 - the svm model needs to be solved using
30:54 - optimization procedure
30:55 - you can use a numerical optimization
30:58 - procedure to search for the coefficients
31:00 - of the hyperplane
31:01 - the most popular method for fitting an
31:03 - svm is the sequential minimal
31:06 - optimization smo method that is very
31:09 - efficient
31:10 - it bakes the problem into sub problems
31:12 - that can be solved analytically by
31:14 - calculating
31:15 - rather than numerically by searching or
31:17 - optimizing
31:19 - in the next video we'll implement the
31:21 - support vector machine we have two
31:23 - choices here
31:24 - we can either use the circuit learn
31:25 - library to import large
31:27 - the svm model and use it directly or we
31:30 - can
31:31 - write our own model based on the
31:32 - equations above a support vector machine
31:35 - is a type of supervised machine learning
31:36 - classification algorithm
31:38 - svms were introduced initially in the
31:41 - 1960s
31:42 - and were later refined in 1990s however
31:45 - it is only now that they are becoming
31:47 - extremely popular
31:48 - owing to their ability to achieve
31:50 - brilliant results
31:52 - svms are implemented in a unique way
31:54 - when compared to other machine learning
31:56 - algorithms
31:57 - in this video we'll implement support
31:59 - vector machines with the help of the
32:01 - scikit-learn library
32:04 - for the implementation our task is to
32:06 - predict whether a bank currency note is
32:08 - authentic or not
32:10 - based on four attributes of the note
32:13 - those attributes are
32:14 - the skewness of the wavelet performed
32:16 - image
32:17 - the variance of the image entropy of the
32:19 - image and the kurtosis of the image
32:22 - this is a binary classification problem
32:24 - and we will use the svm algorithm to
32:26 - solve this problem
32:29 - the detailed information about the data
32:32 - and a link to download the data set can
32:33 - be found in the description
32:35 - download the data set and store it
32:37 - locally on a computer where you intend
32:39 - to write the implementation
32:42 - so let's start the implementation by
32:45 - importing all the necessary libraries
32:49 - first we need to import pandas as we
32:52 - need to store
32:53 - our data and the data frame
32:57 - and the next is numpy
33:03 - followed by a matplotlib
33:14 - and then we need the circuit learn
33:22 - modules
33:32 - here we'll be using the train test split
33:35 - module
33:36 - which we have had not in the previous
33:38 - videos for linear knowledge equation
33:39 - implementations
33:41 - but for svm we'll actually have a train
33:43 - test split and see how
33:45 - the algorithm works on the test data
33:49 - next we need to import the
33:53 - svm class which is svc
33:57 - support vector classifier and at last we
34:00 - need to evaluate
34:01 - our algorithm so we'll need some matrix
34:04 - for that
34:10 - and let's use the classification report
34:13 - module
34:15 - and yeah i think we're done
34:19 - so now let's import the data
34:23 - okay i think we have some problem yeah
34:25 - it's a spelling mistake
34:30 - dot metrics
34:36 - i have a lot of spelling mistakes here
34:38 - okay now
34:40 - let's import the data into a program to
34:42 - read the data from the csv file
34:44 - the simplest way is to use the read csv
34:47 - method of the pandas library
34:50 - the following code which i'm going to
34:51 - write is going to read the bank currency
34:53 - node data into a pandas data frame
34:56 - let's have bank data equal to pd dot
35:00 - read csv
35:05 - and the name of the csv file is bill
35:08 - authentication which you can find in the
35:10 - description
35:11 - of the video
35:14 - okay now there are virtually limitless
35:17 - ways to analyze data sets with a variety
35:19 - of python libraries
35:21 - for the sake of simplicity we will only
35:23 - check the dimensions of the data
35:25 - and see the first few rows to see the
35:28 - rows
35:28 - and columns of the data execute the
35:30 - following command
35:34 - and in the output you'll see 137 to
35:38 - comma 5
35:39 - this means that the bank node data set
35:41 - has 1372 rows and five columns
35:48 - now to get a feel of how our dataset
35:50 - actually looks
35:51 - let's actually see the first five rows
35:53 - of the data set using the head command
36:02 - and here you can see the first firozor
36:05 - data set
36:06 - and you can also see the attributes of
36:08 - the dataset are numeric
36:09 - the label is also numeric that is class
36:12 - one or zero
36:18 - let's pre-process the data before
36:20 - training the model
36:21 - data pre-processing involves two steps
36:23 - first dividing the data into attributes
36:26 - and labels
36:27 - and second dividing the data into
36:29 - training and testing sets
36:31 - to divide the data into attributes and
36:33 - labels execute the following
36:34 - code let x equal to the attributes so
36:38 - bank
36:39 - data dot drop
36:47 - one and y equal to
36:50 - bank data class which is the
36:55 - label in the first line of the script in
36:58 - the cell
36:59 - all the columns of the bank data data
37:01 - frame are being stored in
37:03 - x except the class column which is the
37:05 - label column
37:06 - the drop method drops this column in the
37:09 - second line
37:10 - only the class column is being stored in
37:11 - the y variable
37:13 - at this point of time x variable
37:15 - contains attributes while the y variable
37:17 - contains
37:18 - corresponding labels
37:21 - once the data set is divided into
37:23 - attributes and labels
37:25 - the final pre-processing step is to
37:27 - divide the data into training and test
37:29 - sets
37:30 - luckily the model selection library
37:33 - of the cyclic learn library contains the
37:35 - trained test split method
37:36 - that allows us to seamlessly divide the
37:38 - data into training and test sets
37:41 - let's write the code for that x strain
37:44 - comma
37:49 - x test comma y
37:52 - train comma by test
37:55 - is equal to train test
38:00 - split
38:03 - x comma y and we want the test size to
38:06 - be twenty percent
38:14 - uh yeah
38:17 - we have divided the data into training
38:19 - and testing sites
38:20 - now is the time to train our scm on the
38:23 - training data
38:25 - circuit learn contains the svm library
38:27 - which contains built-in classes for
38:29 - different svm algorithms
38:31 - since we are going to perform a
38:32 - classification task we will use the
38:34 - support vector classifier class
38:36 - which is written as svc in the circuit
38:38 - learns svm library
38:40 - this class takes one parameter which is
38:42 - the kernel type
38:43 - this is very important in the case of a
38:45 - simple svm we simply set this parameter
38:48 - as linear
38:49 - since simple scms can only classify
38:51 - linearly separable data
38:53 - let's write the code for that so svc
38:57 - classifier is equal to svc
39:02 - and the kernel is equal to
39:05 - linear
39:09 - the fit method of the fvc fsbc
39:13 - class is called to train the algorithm
39:15 - on the training data
39:16 - which is passed as a parameter to fit
39:18 - the mod fit the men the fit method
39:21 - svc classy fire dot
39:26 - fit
39:28 - x train comma y train
39:35 - to make predictions the predict method
39:37 - of the svc class is used
39:39 - so why prediction is equal to sv
39:44 - classifier dot predict
39:49 - x test
39:52 - and let's actually print vibrate to see
39:54 - our predictions
39:56 - and as you can see the algorithm has
39:59 - been run on the xtest
40:01 - data and all the predictions have been
40:03 - saved in the vibrate variable and we can
40:05 - see the predictions for each of the
40:07 - row for x text now
40:10 - to evaluate the algorithm confusion
40:12 - matrix
40:13 - precision recall and f1 measures are the
40:15 - most commonly used metrics
40:17 - circuit learns matrix library contains
40:19 - the classification report which can be
40:21 - readily used to find out the values for
40:23 - these important metrics
40:25 - so let's actually print the
40:26 - classification report
40:33 - for y test
40:36 - and why spread
40:42 - so here as you can see the most
40:44 - important metric which we can see
40:46 - is the output of the accuracy of
40:48 - algorithm which is 99
40:50 - this is a very basic implementation of
40:53 - svm using the cyclic learn library
40:55 - and now you can go ahead and implement
40:57 - the algorithm on different data sets
40:59 - from kegel etc this video is an
41:01 - introduction to random forest
41:03 - for in-depth understanding
41:06 - please refer to the links in the
41:08 - description a big part of machine
41:10 - learning is classification
41:12 - that is we want to know what class an
41:15 - observation belongs to
41:17 - the ability to precisely classify
41:19 - observations is extremely valuable
41:22 - for various business applications like
41:25 - predicting whether a particular user
41:26 - will buy a product or not
41:28 - or whether a loan has to be given to a
41:31 - person or not
41:32 - in this video we'll talk about the
41:33 - random forest classifier
41:37 - random forest is a flexible easy to use
41:39 - machine learning algorithm that produces
41:41 - even without hyper parameter tuning a
41:43 - great result most of the time
41:46 - it is also one of the most used
41:48 - algorithms because
41:49 - of its simplicity and diversity it can
41:52 - be used for both
41:53 - classification and regression
41:57 - so before learning about random forest
42:00 - one must know how decision trees work
42:03 - so make sure that you know what are
42:05 - decision trees
42:06 - before watching this video you can find
42:08 - a very good explanation
42:10 - of decision trees in the link in the
42:11 - description
42:13 - so let's start what are random forests
42:16 - random forest is a supervised learning
42:18 - algorithm
42:20 - the forest it builds is an ensemble or a
42:23 - group of decision trees
42:24 - usually trained with the bagging method
42:27 - we'll talk about
42:28 - bagging a little late in the video the
42:30 - general idea of bagging method is that a
42:32 - combination of learning models
42:34 - increases the overall result now let's
42:37 - see what this means
42:38 - in simple layman terms random forests
42:42 - build multiple decision trees and merges
42:44 - them together to get a more accurate and
42:46 - stable protection
42:49 - each individual tree in the random
42:52 - forest spits out a class prediction
42:54 - and the class with the most votes
42:57 - becomes our model's prediction
42:59 - in the figure six decision trees predict
43:02 - one and one predicts a zero
43:04 - hence the final prediction of the
43:06 - classifier is one
43:09 - now the fundamental concept behind
43:11 - random forest
43:12 - is a simple but a powerful one the
43:15 - wisdom of crowds
43:16 - now what does that mean a large number
43:19 - of relatively uncorrelated modules or
43:21 - decision trees
43:22 - operating as a committee will outperform
43:25 - any of the individual constituent models
43:27 - for individual decision trees
43:29 - this is the most fundamental concept in
43:31 - random forest
43:32 - the low correlation between models is
43:34 - the key the reason for this wonderful
43:36 - effect is that the trees protect each
43:38 - other
43:39 - from the individual errors while some
43:41 - trees may be wrong
43:42 - many other trees will be right so as to
43:45 - group so as
43:46 - a group of trees are able to move in the
43:48 - correct direction
43:51 - now let's see how the random foils
43:52 - algorithm works
43:54 - first create a bootstrap bootstrapped
43:57 - dataset
43:57 - by randomly selecting a samples from the
44:00 - original data set
44:01 - we can pick sample from or we can pick
44:03 - the same sample more than once
44:06 - then create a decision tree using the
44:08 - bootstrap data set
44:10 - but only use a random subset of columns
44:13 - in each step
44:15 - now go back to step 1 and repeat make a
44:18 - new bootstrap data set
44:20 - and build a tree considering a subset of
44:22 - variables at each step
44:24 - this results in a wide variety of trees
44:27 - now the variety is what makes random
44:30 - forests
44:31 - more effective than individual decision
44:32 - trees
44:35 - now how do we measure or ensure
44:39 - that the trees divers these trees
44:41 - diversify each other
44:43 - bagging helps us here bootstrapping the
44:45 - data and using the aggregate to make a
44:47 - decision is known as bagging
44:49 - random forest takes advantage of this by
44:52 - allowing each individual tree
44:53 - to randomly sample from a data set with
44:56 - replacement resulting
44:57 - in different trees next
45:00 - is feature randomness each tree in a
45:03 - random forest
45:04 - can pick one can pick only from a random
45:07 - subset of features
45:08 - in a normal decision tree when it is
45:10 - time to split a node
45:12 - we consider every possible feature and
45:14 - pick the one that produces the
45:15 - separation between the observation in
45:17 - the left node versus those in the right
45:19 - node
45:20 - in contrast each tree is a random
45:24 - forest can pick only from a random
45:26 - subset of features
45:27 - this forces even more variation amongst
45:30 - the trees in the model and ultimately
45:32 - results in lower correlation across
45:34 - trees and more diversification
45:36 - so since we are selecting random
45:39 - features
45:40 - in multiple decision trees they ensure
45:42 - that
45:43 - this will diversify all of the decision
45:45 - trees when combined together
45:48 - now how do we implement this for the
45:50 - implementation we have two choices
45:52 - we can either use the circuit learn
45:54 - library to import the random forest
45:56 - model and use it directly
45:57 - or we can write our own model from
45:59 - scratch so in this video we will
46:01 - implement random forest using the cyclic
46:03 - learn library
46:05 - let's start with defining a problem now
46:08 - the task here is to predict whether a
46:10 - bank currency note is authentic or not
46:13 - based on four attributes which are the
46:16 - variance of the image
46:17 - wavelet transformed image skewness
46:20 - entropy and kurtosis of the image this
46:24 - is a binary classification problem
46:26 - and we will use a random forest
46:27 - classifier to solve this problem
46:31 - now for the data set you can download
46:33 - and learn more about the data set from
46:35 - the link in the description
46:38 - let's start with importing the required
46:40 - libraries
46:41 - unlike my previous implementations where
46:43 - i import all the libraries at once
46:45 - this time i'll import the library only
46:47 - when it is required
46:50 - so initially we need numpy and pandas
46:52 - library to handle the data so let's
46:54 - start with that let's import
46:56 - pandas as pd and import num
47:01 - pi as np
47:04 - and let that run and we're good to go
47:07 - now let's import the data set into our
47:10 - code
47:10 - so data set equal to
47:15 - pandas dot read csv is the name of the
47:19 - function and the name of the csv file is
47:22 - bill authentication
47:24 - dot csv and i think i'm right here let
47:28 - me just check
47:31 - yes awesome now let's get a high level
47:34 - view of the data set
47:35 - and let's do that by executing the
47:37 - following command so data set
47:39 - dot head and as we can as you know we
47:43 - can see the first five
47:44 - uh rows of the data set and you can see
47:46 - the variance the skewness sculptosis
47:49 - entropy and the class
47:52 - now here when you see the first five
47:55 - rows
47:56 - you can see that the values in the data
47:58 - set are not very well scaled
48:00 - so we will have to scale the data before
48:02 - training it
48:04 - so to do all of that let's divide the
48:06 - data set into x and y variables
48:08 - which are the attributes and the labels
48:10 - so x
48:11 - is going to be data set dot
48:14 - lock to separate
48:17 - the values and let's do 0 to 4
48:22 - so that it's 0 to 3 and fourth is the
48:25 - class
48:25 - from zero indexing and we need the
48:29 - values
48:30 - and let y be equal to data set dot
48:34 - i lock and we just need
48:37 - the last index which is four
48:40 - so as these follow zero indexing
48:44 - we need the first four columns zero to
48:47 - three which is variance q and squared as
48:49 - an entropy in the x
48:51 - variable and the last way last column
48:54 - the fourth column
48:55 - class in the y variable
49:00 - okay after this let's divide the data
49:02 - set into a train and test split
49:05 - so for that we'll need the train test
49:07 - split module from sklearn
49:10 - so sklearn dot model
49:14 - selection we import
49:18 - train test and split
49:22 - now let's have our variables ready
49:25 - so like strain x
49:30 - test uh y train
49:33 - and y test
49:37 - and let's start give a function call
49:40 - here
49:42 - so x and y
49:46 - let the test size be twenty percent so
49:48 - tesla is equal to zero point two
49:50 - and we don't need a random state so
49:52 - let's put that to
49:54 - zero uh i think we're good to go let me
49:57 - just
49:58 - check back again and
50:02 - if we have an error so
50:06 - s size it shouldn't be test should be
50:08 - test size
50:09 - sorry about that yeah and we're good to
50:12 - go
50:13 - so next let's apply some feature scaling
50:15 - on our data so that the data can be
50:17 - uh really scaled and proper when we
50:20 - actually train it
50:21 - so for that we need something from the
50:23 - pre-parsing
50:24 - module of sql so sk learn
50:28 - dot pre processing
50:32 - import standard
50:36 - scalar scalar
50:39 - then let's scale both x train and x test
50:43 - so
50:47 - let's call the class first and make an
50:50 - object so we have sc as the object
50:52 - and now let's scale xtrain first
50:56 - sc dot fit transform
51:06 - x train and
51:09 - x test is equal to sc.fit
51:13 - transform x test
51:19 - and now we have scaled a data set we can
51:22 - train a random forest to
51:24 - solve this classification problem let's
51:27 - do that with the random forest
51:28 - classifier
51:29 - so we can get that from the sk learn
51:31 - ensemble module
51:34 - so on some so
51:37 - since random forest is an ensemble of
51:40 - many decision trees
51:41 - uh the ensemble module will contain the
51:43 - random forest classifier
51:46 - and um for rest
51:50 - classifier and then
51:54 - let's
51:58 - make this ready make the class ready so
52:00 - let's say
52:02 - class if fire is equal to
52:07 - random for this classifier and
52:10 - first we need uh the n estimators so the
52:13 - end
52:13 - decision trees let's say we have 20 of
52:18 - those
52:20 - then a random state of zero again
52:26 - after which we are going to fit our
52:28 - classifier so
52:30 - classifier dot fit
52:33 - x train comma y train
52:39 - and let's actually
52:42 - save our predictions so
52:46 - classifier dot
52:49 - predict x test
52:53 - awesome and let's see what the errors
52:56 - are
52:58 - so i think i have some
53:01 - uh error here so from sk
53:04 - learn hot
53:12 - random forest classifier
53:15 - cannot be
53:22 - let me just see what the error is
53:33 - oh i see it there has to be a capital f
53:36 - sorry about that
53:40 - and yeah we're good to go now the random
53:42 - forest classifier takes in n estimators
53:44 - as a parameter
53:46 - uh the parameter defines the number of
53:48 - trees in a random form list and we are
53:49 - using 20 t's here
53:52 - so for for classification problems the
53:54 - matrix used to evaluate
53:55 - algorithms are accuracy confusion matrix
53:58 - precision recall and f1 values luckily
54:02 - the circuit turn library provides all
54:04 - these metrics out of the box
54:06 - so let's actually use this matrix to see
54:08 - how
54:09 - good our model performed on this data
54:11 - set
54:12 - so from sk learn dot metrics module
54:17 - we're going to import a few things so
54:19 - classy vacation report is going to be
54:21 - the first thing confusion
54:24 - matrix is next and we need the accuracy
54:28 - to see how good
54:29 - our model is and let's just print all of
54:32 - them now
54:34 - so confusion matrix is first
54:40 - and why test why bread
54:44 - next is classification report so
54:48 - classification report
54:52 - again y test and y predicted
54:57 - and the last is the accuracy score
55:01 - for the same parameters
55:06 - and let's see how this works and again
55:10 - we have some errors here
55:11 - so let's solve them cannot import
55:15 - confusion
55:17 - so you have to stop misspelling things
55:20 - wrong
55:22 - yes so as you can see at the last
55:26 - uh print accuracy score our accuracy is
55:29 - 98
55:30 - so that is good enough and we can see
55:33 - the other metrics
55:34 - that are used for classification
55:35 - problems so this was a very simple
55:38 - implementation of random forest with
55:39 - minimum minimum data processing
55:42 - now what you can do is practice more on
55:44 - kegel
55:45 - on a real-life data set which deals with
55:47 - more data processing which can help you
55:49 - understand
55:50 - how a data scientist works in this
55:53 - machine learning playlist
55:55 - until now we have only talked about
55:57 - supervised learning algorithms
55:58 - where we knew what the output of the
56:00 - target variable was
56:02 - in this video we'll explore an
56:03 - unsupervised learning algorithm
56:05 - called k-means clustering
56:08 - k-means clustering is one of the
56:10 - simplest and popular supervised
56:11 - algorithms
56:15 - and there are plethora of real-world
56:17 - applications of k-means clustering which
56:18 - we will talk about in this video
56:20 - and in the next video we will see the
56:22 - implementation of kmes
56:24 - and how easy it is when compared to
56:26 - algorithms like svms
56:27 - and etc
56:30 - now before we jump into the algorithm
56:33 - itself
56:34 - or even supervised learning we must
56:36 - understand what clustering means
56:38 - so clustering is the process of dividing
56:41 - the entire data
56:42 - into groups or also known as clusters
56:45 - based on the patterns in the data
56:47 - let's try to understand that with a
56:49 - simple example
56:52 - a bank wants to give credit card offers
56:54 - to its customers
56:56 - currently they look at the details of
56:58 - each customer and based on this
56:59 - information
57:00 - decide which offer should be given to
57:02 - which customer
57:04 - now the bank can potentially have
57:06 - millions of customers right
57:08 - does it make sense to look at all the
57:10 - details of each customer separately and
57:12 - then make a decision
57:13 - certainly not it is a manual process and
57:16 - will take a huge amount of time
57:18 - so what can the bank do one option is to
57:21 - segment its customers into different
57:23 - groups
57:24 - for instance the bank can group the
57:25 - customers based on their incomes
57:28 - the groups that are shown here are known
57:30 - as clusters
57:31 - and the process of creating these groups
57:33 - is known as clustering
57:35 - awesome now let's talk about super
57:38 - unsupervised learning
57:40 - unsupervised learning is a type of
57:41 - machine learning algorithm
57:43 - used to draw inferences from data sets
57:45 - consisting
57:46 - of input data without labeled responses
57:50 - so to understand all of this let's see
57:52 - how a superfile supervised algorithm
57:54 - works first
57:57 - we have a label data set with the output
57:59 - or target variable
58:00 - in this particular example the task is
58:02 - to predict whether a loan will be
58:03 - approved or not
58:05 - as we have all the data labeled with
58:07 - appropriate targets
58:08 - we call it as supervised learning
58:12 - in clustering we do not have a target to
58:14 - predict
58:15 - we look at the data and try to club
58:17 - similar observations and form different
58:19 - groups
58:20 - hence it is an unsupervised learning
58:22 - algorithm
58:24 - so let's see where this helps us in the
58:27 - real world
58:28 - so starting with customer segmentation
58:30 - as we discussed before about the bank
58:32 - making clusters based on the income for
58:34 - the credit cards
58:36 - next thing is document clustering this
58:38 - is another common application
58:40 - let's say you have multiple documents
58:42 - and you need to cluster similar
58:43 - documents together
58:44 - clustering helps us group these
58:46 - documents such that similar documents
58:48 - are in the same clusters
58:50 - the next is image segmentation we can
58:53 - also use
58:54 - clustering to perform image segmentation
58:56 - here we try to club similar pixels in
58:59 - the image together
59:00 - we can apply clustering to create these
59:03 - clusters having similar pixels in the
59:05 - same group
59:07 - the next is recommendation engines let's
59:09 - say you
59:10 - want to watch or you want to recommend
59:12 - songs to your friends
59:14 - you can look at the songs like by that
59:16 - person and then use clustering to find
59:18 - similar songs and then finally recommend
59:20 - those songs to the person
59:23 - so let's we let's talk about k-means
59:27 - clustering now
59:28 - we have finally arrived the main part of
59:30 - the video now with regards to generating
59:33 - clusters
59:34 - our aim here is to minimize the distance
59:36 - between the points within a cluster
59:39 - there is an algorithm that tries to
59:41 - minimize the distance of the points in a
59:42 - cluster
59:43 - with the centroid this is called the
59:45 - k-means clustering technique
59:49 - the main objective of the k-means
59:50 - algorithm is to minimize the sum of
59:52 - distances
59:53 - distances between the points and the
59:56 - respective cluster centroid
59:58 - let's see how the algorithm works in
60:00 - action
60:02 - so we have this eight points we want to
60:06 - apply
60:07 - we want to apply k means on to create
60:09 - clusters
60:10 - so let's see how we can do that
60:13 - the first step in k means is to pick the
60:15 - number of clusters k
60:18 - next we randomly select the centroid for
60:21 - each cluster
60:22 - let's say we have two clusters so the k
60:24 - is equal to 2 here
60:26 - we then randomly select the centroid
60:31 - step three once we have initialized the
60:34 - centroid we
60:35 - assign each point to the closest cluster
60:37 - centroid
60:38 - here you can see that the points which
60:40 - are closer to the red point
60:42 - are assigned to the right cluster
60:43 - whereas the points which are closer to
60:45 - the green point are assigned to the
60:46 - green cluster
60:49 - now once we have assigned all of the
60:51 - points to
60:52 - either clusters the next step is to
60:53 - compute the centroids of newly formed
60:56 - clusters here the red and green crosses
60:59 - are the new centroids
61:01 - now we repeat steps three and four
61:04 - so essentially there are three ways to
61:07 - stop k-means clustering
61:09 - first is the centroid of newly formed
61:11 - clusters do not change
61:12 - so if the centroids don't change that
61:14 - means we have reached the end
61:16 - and that is the best way we can actually
61:17 - cluster our data
61:19 - second the points remain in the same
61:21 - cluster so if the points remain in the
61:23 - same cluster that means that there is no
61:25 - a further possible way or a possible way
61:28 - to improve our clustering
61:29 - algorithm and the last is the maximum
61:32 - number of iterations that i reach
61:34 - so the number of iterations is
61:37 - a subjective so it depends from person
61:40 - to person
61:41 - so we can actually focus more on the
61:43 - first two points and not in the last
61:44 - point as much
61:47 - so coming to the implementation we have
61:49 - two choices we can either use the cyclic
61:51 - learn library
61:52 - and import the k-means model and use it
61:54 - directly or we can write our own model
61:56 - from scratch
61:57 - so writing our own model from scratch
61:59 - using numpy and python is very easy for
62:01 - k-means
62:02 - but to see how the algorithm works
62:06 - very fastly will implement k-means using
62:09 - the cyclic run library
62:10 - in this video we will use circuit learn
62:12 - to implement the k-means clustering
62:14 - algorithm
62:15 - let's get started first we import all
62:19 - the required libraries
62:20 - so we need matlab
62:24 - sorry matplotlib
62:27 - dot pi plot as plt
62:31 - we need numpy to handle the data
62:35 - and we need the cluster
62:41 - from scikit learn
62:51 - let's wait for it to run the star here
62:54 - indicates that it's currently running
62:56 - and we have
62:57 - run it properly now let's prepare the
62:59 - data
63:00 - let's create a numpy array of 10 rows
63:03 - and two columns
63:04 - so it's better to actually show you how
63:06 - k means is implemented using our own
63:08 - pre-made data set and not a real-time
63:10 - data set because it gets really
63:11 - confusing
63:12 - so we start with a simple handcraft data
63:16 - set and then we move to complicated
63:17 - trailer dataset
63:19 - we create a numpy array of data points
63:20 - because the circuit learn library can
63:22 - work with numpy array type data inputs
63:24 - without requiring any pre-processing so
63:26 - we can directly focus
63:27 - on implementing the algorithm and not
63:29 - worry about pre-crossing in the initial
63:30 - stages of implementation
63:32 - so this is the numpy array
63:38 - now let's visualize the data the written
63:40 - code simply
63:41 - or the code which we're going to write
63:43 - simply plots all the values in the first
63:45 - column of x array against all the values
63:47 - in the second column
63:48 - so let's see what the code looks like so
63:51 - we make a scatter plot
63:56 - and we start from zero
64:02 - and we want y values now
64:08 - and let's give it a label as well while
64:10 - we're at it
64:17 - and this is how our data looks so from
64:19 - the naked eye
64:20 - we have to form two clusters of the
64:22 - above data points
64:24 - we will probably make one cluster of
64:25 - five points on the
64:27 - bottom left and one cluster of five
64:29 - points on the top right
64:30 - let's see if our k-means clustering
64:32 - algorithm does the same or not
64:36 - okay so let's create the clusters now to
64:38 - create a k-means cluster
64:40 - with two clusters simply type the
64:42 - following script
64:43 - so k means equal to the class k
64:46 - means and
64:50 - number of clusters which we want is
64:52 - equal to 2
64:54 - and let's fit the algorithm now
65:00 - to our data set x and you've done that
65:05 - and yes it is just two lines of code to
65:07 - actually run the algorithm
65:09 - in the first line we create a k-means
65:12 - object
65:12 - and pass it to the value 2 as the number
65:15 - of clusters
65:16 - next we simply have to call the fit
65:19 - method on key means
65:20 - and pass the data that we want to
65:22 - cluster
65:23 - which in this case is the x array that
65:25 - we created earlier
65:27 - now let's see what the central values
65:29 - the algorithm generated
65:31 - for the final clusters let's print them
65:43 - and yep those are centroids or the
65:46 - centers
65:47 - the output will be a 2d array of the
65:49 - shape 2 cross 2
65:51 - to see the labels for data point let's
65:53 - execute the following
65:54 - so let's print
65:58 - k means labels so let's do it here again
66:02 - so
66:10 - and those are our two clusters so it is
66:13 - point by point so
66:14 - a cluster zero and cluster one so the
66:17 - output is a one dimensional array of ten
66:19 - elements corresponding to the cluster
66:20 - assigned to our ten data points
66:22 - here the first five points have been
66:24 - clustered together and the last five
66:26 - points have been clustered
66:27 - here zero and one are merely used to
66:29 - represent the cluster ids and have no
66:31 - mathematical significance towards
66:33 - towards each other if there were three
66:35 - clusters
66:36 - the third cluster would have been
66:37 - represented by the digit two
66:41 - let's plot the data point again on the
66:43 - graph and visualize how the data has
66:45 - been clustered
66:46 - this time we will plot the data along
66:48 - with the assigned label so that we can
66:50 - distinguish between the clusters
66:53 - so let's write the code for that we'll
66:55 - make a scatter plot again to see how
66:57 - this works
66:58 - with our data points in x
67:03 - 0 and x colon comma 1
67:09 - and c is going to be k means
67:13 - labels underscore and let
67:16 - the c map be a rainbow so let's see how
67:19 - that works
67:25 - here we are plotting the first column of
67:27 - the x array and needs a second column
67:30 - however in this case we are also passing
67:32 - k means labels as the value for the c
67:34 - parameter
67:35 - that corresponds to the labels
67:38 - the c map rainbow parameter is parse for
67:41 - choosing the color type
67:42 - for different data points so that is how
67:44 - we get the differentiated
67:46 - bluish violet color or the purple color
67:48 - and the red car
67:51 - as expected the first five points in the
67:53 - bottom left have been clustered together
67:55 - displayed with blue while the remaining
67:57 - points in the top right have been
67:59 - clustered together with red so here we
68:00 - have two different opposite
68:02 - scenarios so the bottom has been done
68:04 - with red and the top right has been done
68:06 - with blue
68:08 - let's execute the k-means algorithm with
68:10 - three clusters and see
68:12 - the output graph so let's implement it
68:14 - again
68:15 - k means equal to k means
68:18 - class and now the clusters
68:22 - is equal to three let's
68:25 - fit our data set on this algorithm
68:30 - and
68:33 - plot this again so
68:37 - scatter
68:41 - x again colon and 0
68:49 - c is equal to k means dot
68:53 - labels again and the c
68:56 - map is going to be rainbow
69:01 - and yeah you can see that again the
69:04 - points are close to each other have been
69:05 - clustered together
69:07 - now let's plot the points along with the
69:10 - centroid coordinates of each cluster to
69:12 - see how the centered position
69:14 - affects clustering so here we're going
69:17 - to also point out the
69:18 - centroid of all the clusters which which
69:20 - we can see here so
69:22 - we have we have three clusters here so
69:23 - we'll be plotting the three centroids
69:25 - along with the clusters
69:28 - let's write the code for that we always
69:31 - use scatter plot for kms clustering
69:33 - because it's easier to
69:34 - see the scatter plot when we have to
69:36 - differentiate between the clusters
69:38 - colon and zero
69:42 - and again one
69:47 - the c is k means dot labels again
69:51 - the c map is equal to rainbow
69:57 - now we need to plot the centroids here
70:00 - so let's try that
70:03 - cluster centers
70:07 - if i'm right with that and
70:11 - we need only till the zeroth point and
70:16 - we have to plot that with the y
70:19 - axis so cluster underscore centers
70:22 - underscore
70:24 - colon one and let the color of these
70:27 - points be
70:28 - black
70:31 - awesome let's see how this looks so
70:34 - in the case of three clusters the two
70:36 - points in the middle which are displayed
70:38 - in red
70:38 - have distance closer to the centroid in
70:40 - the middle displayed in back
70:42 - between the two reds as compared to the
70:45 - centroids on the bottom left or top
70:46 - right
70:48 - however if there were two clusters there
70:50 - wouldn't have been
70:51 - a centroid in the center hence the red
70:54 - points
70:55 - would have been clustered together with
70:57 - the bottom left or top right clusters
71:00 - so that was a simple implementation of
71:02 - k-means clustering with our very own
71:04 - handmade data set now you can go ahead
71:07 - and try implementing the algorithm on
71:08 - regular data sets
71:09 - the k nearest neighbors algorithm is a
71:12 - simple
71:13 - easy to implement supervised machine
71:14 - learning algorithm that can be used to
71:16 - solve both classification and regression
71:18 - problems
71:19 - now a supervised machine learning
71:21 - algorithm
71:24 - is one that relies on labeled input data
71:26 - to learn a function that produces an
71:28 - appropriate output when given a new
71:30 - unlabeled data
71:33 - the k n algorithm assumes that similar
71:35 - things exist
71:36 - in close proximity in other words
71:39 - similar things are near to each other
71:42 - we can relate this definition to
71:43 - something like birds of a feather flock
71:46 - together
71:48 - now notice in the image that most of the
71:51 - time
71:51 - similar data points are close to each
71:53 - other the k n algorithm
71:55 - hinges on this assumption being true
71:57 - enough for the algorithm to be useful
72:02 - knn captures the idea of similarity
72:05 - sometimes called distance
72:06 - proximity or closeness with some
72:08 - mathematics we might have learned in our
72:10 - childhood
72:11 - calculating the distance between points
72:13 - on a graph
72:14 - there are many ways to calculate
72:16 - distance and one might
72:17 - one way might be preferable depending on
72:19 - the problem that we are trying to solve
72:22 - however we are going to use something
72:24 - called as the euclidean distance which
72:26 - is a popular and a familiar choice
72:29 - let's see how the knn algorithm works in
72:32 - action
72:33 - first we load the data set next we
72:35 - initialize the number of
72:36 - neighbors which we want which is k in
72:38 - our case
72:39 - now for each example in our data set we
72:42 - calculate the distance
72:43 - between the query example and the
72:45 - current example of the data
72:46 - the distance here being the nuclear
72:48 - distance
72:50 - next we add the distance and the index
72:52 - of the example
72:53 - to an ordered collection for example a
72:55 - dictionary
72:57 - now sort the ordered collection of
72:58 - distances and indices from smallest to
73:00 - largest
73:01 - in ascending order by the distances now
73:04 - let's pick the first k entries from the
73:06 - solid collections
73:08 - get the labels of the selected k entries
73:12 - now if you want to find the mean then
73:14 - that is the regression problem and if
73:16 - you find the mode it's a classification
73:18 - knl algorithm
73:22 - now let's talk about choosing the right
73:24 - value of k
73:26 - to select the k that's right for your
73:28 - data
73:29 - we run the k n algorithm several times
73:31 - with different values of k
73:33 - and choose the k that reduces the number
73:35 - of errors we encounter
73:37 - while maintaining the algorithm's
73:38 - ability to accurately make predictions
73:41 - when given
73:42 - data it hasn't seen before
73:47 - k n has the following advantages
73:50 - the algorithm is simple and easy to
73:52 - implement and we'll see that in the next
73:54 - video
73:55 - there is no need to build a model tune
73:57 - some hyper parameters
73:59 - or even make additional assumptions it
74:01 - is a very simple and straightforward
74:02 - algorithm
74:03 - the algorithm is also versatile it can
74:05 - be used for classification regression
74:07 - and search as well
74:10 - one of the major disadvantages of the
74:12 - algorithm is that it gets significantly
74:14 - slower as the number of examples
74:16 - or variables increase
74:20 - coming to this let's talk about the
74:21 - applications of k n
74:24 - k n can be useful in solving problems
74:27 - that have solutions that depend on
74:28 - identifying similar objects
74:30 - right the nearest neighbors or the
74:32 - nearest similar objects
74:33 - an example of using this would be in
74:36 - recommender systems which is an example
74:38 - application of k n search
74:41 - now uh at a large scale this would look
74:44 - like recommending products on amazon or
74:46 - articles on medium movies on netflix
74:48 - although we can be certain that these
74:50 - companies
74:52 - they all use more efficient means of
74:54 - making recommendations
74:55 - due to enormous volume of data and when
74:57 - you have an enormous volume of data that
74:59 - is when k n
75:01 - starts to suffer so that was
75:04 - a very brief introduction of how the k
75:06 - nearest algorithm works
75:08 - in this video we will implement k
75:10 - nearest neighbors using the cyclic loan
75:11 - library
75:13 - so the k nearest neighbors algorithm is
75:15 - a type of supervised machine learning
75:16 - algorithm
75:18 - k n is extremely easy to implement in
75:20 - its most basic form and yet performs
75:22 - quite
75:23 - complex classification tasks it is a
75:26 - non-parametric learning algorithm which
75:28 - means that it does not assume anything
75:29 - about the underlying data
75:32 - this is an extremely useful feature
75:33 - since most of the real-world data
75:35 - doesn't really follow any theoretical
75:37 - assumptions for example linear
75:39 - scalability or uniform distribution
75:41 - let's start implementing the k n
75:43 - algorithm using cyclic learn now
75:46 - we are going to use the famous iris data
75:48 - set for rkn example
75:50 - the data set consists of four attributes
75:52 - sample width sample length
75:54 - petal width and petal length these are
75:56 - the attributes of specific types of iris
75:58 - plant
75:59 - the task is to predict the classes to
76:02 - which these plants belong
76:04 - there are three classes in the data set
76:06 - i recetosa
76:07 - iris versicolor and iris virginica
76:12 - let's start the implementation by
76:14 - importing some libraries
76:17 - so we need to import numpy
76:20 - as np import matplot
76:27 - lib.pyplot spld
76:32 - and import pandas as pd for data
76:36 - handling
76:39 - now let's import the data set into a
76:41 - notebook and
76:43 - then into a pandas data frame so here we
76:46 - have the url from which we can
76:48 - access the data the url will also be in
76:50 - the description
76:51 - so we assign some names to the columns
76:54 - of a data set
76:55 - and read the data set into the pandas
76:57 - data frame
77:02 - to see what the dataset actually looks
77:03 - like let's execute the following script
77:05 - dataset.head and we can see the first
77:08 - five rows of our dataset now
77:11 - the next step is to split a dataset into
77:13 - its attributes and labels
77:15 - to do so let's write the following
77:17 - script so x equal to data set dot
77:19 - along dot
77:22 - values y is equal to the same thing
77:28 - but
77:32 - minus 1 it's going to be 4 because the
77:35 - last
77:36 - column and
77:39 - let's fix this and we are good
77:45 - the x variable here contains the first
77:47 - four columns of the data set
77:49 - or the attributes while the y contain
77:51 - the labels
77:53 - to avoid overfitting we will divide a
77:55 - dataset into training and test splits
77:57 - which gives us which gives us a better
77:59 - idea as to how
78:01 - our algorithm perform during the testing
78:03 - phase this way our algorithm is tested
78:06 - on unseen data
78:07 - as it would be in a production
78:08 - application
78:10 - to create training and testing splits
78:12 - let's execute the following script
78:14 - so let's import the test train model
78:18 - from sk learn
78:30 - and let's define some variables
78:34 - so x test y
78:38 - train and y
78:41 - test is equal to test
78:44 - train split that's a very big function
78:47 - name
78:49 - x y and the test size
78:52 - is 0.2 or 20
79:00 - let's wait for it to run so that we can
79:01 - see some
79:04 - so test train split cannot be imported
79:06 - let's see what the
79:09 - error is
79:26 - now so it's strained this split
79:33 - that makes much more sense i get it now
79:37 - and we're good so now the above script
79:40 - splits the data set into 80
79:42 - train data and 20 test data this means
79:45 - that
79:45 - out of total 150 records the training
79:47 - set will contain 120 records and the
79:49 - test set contains 30 of those records
79:52 - now before making any actual predictions
79:54 - it is always a good practice
79:56 - to scale the features so that all of
79:58 - them can be uniformly evaluated
80:01 - the gradient is an algorithm which is
80:03 - used in neural network training and
80:05 - other machining algorithms
80:06 - also converges faster with normalized
80:08 - features
80:09 - so let's write the script for
80:10 - normalization now sk learn
80:18 - pre-processing
80:21 - import standard
80:25 - scalar
80:29 - and the scale r is equal to
80:32 - an object so standard scalar
80:38 - and let's
80:42 - split it now and get a fit
80:47 - x underscore train
80:51 - and let's x train is equal to
80:55 - scalar dot transform
81:00 - extreme x underscore test
81:03 - is equal to
81:07 - scalar transform for test
81:12 - and again i think i've made a mistake
81:14 - with the spellings
81:17 - pre-processing
81:21 - standard scalar i'm sorry
81:25 - yes now let's fit the canon algorithm to
81:29 - the desired
81:30 - dataset it is extremely straightforward
81:33 - to train the k n algorithm and
81:34 - especially makes prediction out of it
81:36 - when using the cyclical library so let's
81:40 - import
81:40 - our model from circuit learn so sk learn
81:43 - dot
81:45 - neighbors
81:50 - classif file
81:53 - and let me just make sure that the
81:55 - spelling is right now
81:57 - so that we don't have any more errors so
81:59 - k neighbors classifier
82:02 - and the classifier is equal to
82:05 - we have the same name class
82:09 - and let's say we want
82:13 - five neighbors
82:19 - let's fit the classifier now
82:34 - so the first step here was to import the
82:37 - k n n classifier class from the sql
82:39 - neighbors library in the second line
82:42 - we initialize the class with one
82:44 - parameter that is the n neighbors
82:46 - this is basically the value for the k
82:48 - there is no ideal value for k
82:50 - and is selected after testing and
82:52 - evaluation however to start out phi
82:54 - seems to the most commonly used k n
82:56 - algorithm
82:58 - the final step is to make predictions on
83:00 - our test data
83:02 - so why spread is equal to
83:05 - dot predict
83:09 - for x test
83:12 - now let's evaluate the algorithm and see
83:15 - how good it performs
83:16 - for evaluating an algorithm confusion
83:19 - matrix precision recall and f1 score are
83:21 - the most commonly used metrics
83:23 - all of these can be found in the scalar
83:27 - metrics
83:28 - module so from sklearn
83:31 - dot matrix
83:34 - import classification
83:39 - report
83:44 - confusion matrix
83:52 - and let's print them now
84:00 - for y test
84:05 - and y predicted
84:08 - and also print the confusion matrix to
84:11 - see the matrix
84:17 - and the same parameters y test and y
84:21 - and here we can see all the metrics
84:24 - which we need
84:24 - to evaluate our algorithm and how it
84:27 - performs
84:28 - so this was a very basic implementation
84:30 - of k
84:31 - nearest neighbors and after this we can
84:34 - actually go
84:34 - on kegel and download a real-life data
84:37 - set and actually
84:38 - perform and see how k n performs on a
84:41 - real-life data set
84:43 - thank you