00:00 - hey guys welcome to your first lesson on
00:01 - machine learning with PI torch in
00:03 - collaboration with the programming
00:05 - knowledge YouTube channel I've uploaded
00:06 - this tutorial as part of my complete
00:08 - course on PI torch for deep learning and
00:10 - computer vision where you'll build
00:12 - highly sophisticated deep learning and
00:14 - computer vision applications with PI
00:16 - torch by the time you finish this
00:18 - tutorial if you're interested in the
00:20 - full course feel free to check it out by
00:22 - clicking on the link in the description
00:24 - below no experience is required for this
00:26 - particular course and I'm also covering
00:29 - free machine learning and AI content on
00:31 - my channel namely Ryan slim which is
00:34 - also accessible in the description so
00:37 - feel free to check that out but without
00:39 - further ado let's begin this tutorial on
00:41 - machine learning with PI torch in the
00:44 - section we will look to implement a
00:46 - machine learning based algorithm to
00:48 - train a linear model to fit a set of
00:50 - data points as this course does not
00:53 - assume any prior knowledge with deep
00:54 - learning
00:55 - we'll begin our discussion with the
00:57 - notion of supervised learning what it is
00:59 - and how it relates to this course if
01:01 - you've taken one of my deep learning
01:03 - courses before the material in this
01:05 - video should be very familiar to you we
01:08 - will begin with a very broad definition
01:09 - what is machine learning a simplistic
01:13 - definition is that it's the concept of
01:15 - building computational algorithms that
01:18 - can learn over time based on experience
01:20 - so it's that rather than explicitly
01:22 - programming a hard-coded set of
01:24 - instructions an intelligent system is
01:27 - given the capacity to learn is given the
01:31 - capacity to detect and predict
01:33 - meaningful patterns while one can
01:36 - distinguish between supervised and
01:38 - unsupervised learning let us consider
01:41 - supervised learning in supervised
01:44 - learning and the learner is trained and
01:46 - makes use of data sets associated with
01:49 - labeled features which define the
01:51 - meaning of our training data that way
01:54 - once introduced to newly inputted data
01:56 - given a new input the learner is able to
02:00 - predict a corresponding output before
02:03 - discussing any more theory let's jump
02:05 - into some code we will be writing our
02:07 - code using Google collab accessing
02:10 - Google collab is very simple and
02:12 - intuitive there
02:13 - is no setup required it runs entirely on
02:16 - the cloud to access it on your browser
02:18 - search up Google Club and it should be
02:22 - the first link make sure you're signed
02:24 - in to your Gmail if you don't have a
02:26 - gmail you'll need to make one and once
02:29 - you're logged in simply create a new
02:30 - Python 3 notebook and we will call this
02:35 - notebook tensors crash course this is
02:39 - quite similar to Jupiter notebooks we
02:42 - will eventually be using it for its free
02:43 - GPU but notably many packages come
02:47 - pre-installed for us when using Google
02:48 - clop unfortunately torch isn't one of
02:51 - them so in our first so we were in the
02:54 - following installation pip3
02:56 - install torch alright and as it's
03:01 - installing inside of our first cell we
03:03 - import torch
03:10 - the ultimate goal of this fraud
03:12 - we'll be to create a data set which
03:14 - generally will follow a linear pattern
03:16 - then we'll declare a line a linear model
03:19 - with some random set of parameters which
03:22 - chances are will incorrectly fit our
03:25 - data set however then we make use of a
03:28 - simple optimization procedure known as
03:30 - gradient descent which is often used in
03:32 - many machine learning algorithms what
03:35 - this optimization algorithm will do is
03:37 - minimize the error function of our
03:39 - linear model and it keeps doing so until
03:41 - the model is trained to fit our data
03:43 - really well our first step in this
03:46 - venture will be to the Clarion linear
03:47 - model and use it to make predictions so
03:50 - we initialize two variables W and B and
03:54 - the reason for this being we know the
03:57 - equation of a line can be modeled as y
03:59 - is equal to W times X plus B where W is
04:05 - the slope the weight and B is the bias
04:08 - term the UI intercept of our line these
04:11 - are the parameters which distinguish our
04:14 - line and ultimately with linear
04:16 - regression provided with a bunch of data
04:19 - points we first start with some linear
04:22 - model which most likely will not fit our
04:25 - data well we then use these data points
04:28 - to train a linear model to have the
04:31 - optimal weight and bias values that of
04:34 - which will provide us with the line of
04:36 - best fit and then after training this
04:39 - model to have suitable parameters given
04:41 - an input X we can then use this linear
04:45 - model to make a prediction Y all right
04:48 - so back to our code we set W is equal to
04:51 - Torche dot tensor
05:02 - and we will give the wait some random
05:06 - value say 3.0 and as we will be trading
05:10 - our model we will require the gradient
05:12 - of our linear function so requires grad
05:16 - must equal true alright don't worry
05:21 - about why we did this for now it will be
05:24 - a more apparent to you later on why we
05:26 - need the gradient but for now we do the
05:29 - same thing for the bias term we'll just
05:31 - copy and paste that call this B delete
05:34 - this one and instead of a value of 3.0
05:37 - let's change this to 1.0 it really
05:40 - doesn't matter and now given a linear
05:43 - model y is equal to W times X plus B
05:46 - provided an input X we can make a
05:50 - prediction we can determine the output Y
05:52 - and the purpose of this lesson is to
05:55 - show you how to make predictions with PI
05:56 - torch provided that you do have a model
05:59 - to do so what we're going to do is
06:01 - actually copy this delete it and on a
06:04 - new cell we define the forward function
06:08 - deff forward in this forward function
06:12 - receives the input the independent
06:15 - variable the x-value and this input is
06:18 - going to be passed into our linear model
06:20 - y is equal to W times X plus B W and B
06:26 - being the parameters of our line and
06:28 - then based on the input X we predict the
06:31 - corresponding output Y a prediction is
06:34 - computed and returned returning Y let us
06:38 - now test our model by going inside of a
06:41 - new cell and reset the input X is equal
06:45 - to torch dot tensor we will choose
06:52 - input of two and outputting the result
06:56 - of forward X our model makes a
07:01 - prediction of seven instead of passing
07:04 - only one output one can pass in many
07:07 - inputs suppose you wanted to make a
07:10 - prediction for the input for and for the
07:16 - input seven respectively our forward
07:20 - function expects only one argument X so
07:23 - we must wrap this up in an extra set of
07:26 - brackets since that it receives both
07:28 - four and seven as separate inputs and
07:32 - the computation will occur as follows
07:36 - making a prediction for the input for to
07:39 - be thirteen and for the input seven to
07:41 - be twenty-two that is all for making
07:43 - predictions in the next video we will
07:45 - look into a more standardized way of
07:48 - creating a linear model I will see you
07:51 - there welcome back to another lesson in
07:54 - the last one you learn to make
07:55 - predictions using a linear model in this
07:58 - video we will look at a more
07:59 - standardized way of initializing a
08:02 - linear class we will discuss the theory
08:05 - as we implement it into code so on a new
08:08 - Python notebook
08:09 - first you must import the irrelevant
08:11 - Torche library and from torch dot and n
08:17 - we will import the linear class from the
08:21 - n n' module you will first write the
08:24 - following line of code on a new cell
08:26 - torch dot manuel you seem to have a
08:31 - problem being that i have to rerun this
08:34 - cell and so i'm just going to fast
08:36 - forward this video until I do see you in
08:38 - a bit
08:39 - alright and we're back we will start by
08:42 - finishing up this line of code torch dot
08:44 - manuel underscore seed one what this
08:49 - does is it sets a seed for generating
08:51 - random numbers the reason we're doing
08:53 - this is when constructing our model with
08:55 - the linear class it will be given random
08:59 - values for the linear class which makes
09:02 - sense since recall we start with a
09:04 - random value for the weight and
09:06 - bias and then we train our model through
09:09 - a gradient descent algorithm to obtain
09:11 - the optimal parameters to fit our data
09:14 - we'll talk about training shortly but
09:16 - for clarity and to see progress in
09:19 - training our network we want to ensure
09:21 - that the random values assigned to our
09:23 - weight and bias values are consistent
09:25 - hence why we set a seed the seed can be
09:28 - anything that you want it to be so long
09:30 - as you don't change it if you wish to
09:32 - obtain the same results that I get and
09:34 - feel free to use the same seed all right
09:37 - let us now construct our linear model
09:38 - with model is equal to linear and it
09:43 - will accept n features is equal to 1 out
09:48 - features is equal to 1 what this means
09:52 - is that for every prediction that we
09:55 - make using our linear model for every
09:57 - output there is a single input by
10:02 - creating this object over here we've
10:04 - created a linear model and we can
10:07 - determine the parameters of this line by
10:09 - printing print model dot bias as well as
10:17 - model dot weight we get our weight and
10:24 - we get our bias value this would result
10:26 - in the equation y is equal to 0.51 5/3 X
10:30 - minus 0.4 for one for perfect now
10:38 - provided this model we can now make a
10:39 - prediction based on an input X which we
10:44 - will set equal to torch dot tensor and
10:48 - we will pass in an input of 2.0 noting
10:54 - that this is a float of value and we
10:57 - will call upon our model model and pass
11:00 - in X as an input this should return a
11:04 - prediction which we can print out print
11:07 - model X it output
11:13 - prediction of 0.58 9-1-1 can make
11:18 - several predictions by passing in
11:20 - several inputs we can pass in an input
11:24 - equaling 3.3 and noting that our linear
11:29 - model only takes in a single argument
11:31 - we must enclose this in a bracket so
11:36 - what's going to happen is for each input
11:38 - we will get one output one prediction
11:42 - and printing this out the prediction for
11:46 - 2.0 is the same thing as earlier and we
11:49 - also get a prediction for 3.3 to be 1.25
11:52 - 9-0 this lesson was to get you familiar
11:55 - with the linear model class which
11:58 - provides a more concise easier and more
12:00 - standardized way of constructing a
12:03 - linear model we will be using this class
12:06 - all throughout this course in your next
12:08 - video we will explore custom modules a
12:10 - robust and efficient way of building
12:13 - neural networks with PI torch and for
12:15 - the purposes of this section we will use
12:17 - it to further configure our linear model
12:19 - I will see you there hello everyone in
12:23 - this tutorial we'll explore custom
12:25 - modules a robust an intuitive way of
12:27 - building neural networks in Python or
12:30 - complex neural models in the next
12:32 - section and thereafter for now we'll
12:35 - simply use it to build our simple linear
12:37 - model just to learn how to do it now
12:40 - that you're more familiar with the
12:41 - basics the method in which we're
12:43 - currently going to implement a model
12:45 - construction granting more freedom and
12:47 - is much cleaner and more intuitive in
12:50 - its implementation so on a new collab
12:52 - notebook I've already one had an
12:54 - imported torch and inside of a new cell
12:57 - we start by creating a new class which I
13:00 - will call LR as in linear regression and
13:03 - I know that we haven't covered classes
13:06 - in the Python cross course so if you're
13:08 - unfamiliar with them we can go through
13:10 - them now very quickly as we go along but
13:12 - essentially we will use this class as a
13:14 - template as a blueprint for our models
13:17 - construction generally it is a template
13:20 - for creating new objects new instances
13:23 - of our LR class
13:25 - class will be immediately followed by
13:27 - the init method which can be written as
13:31 - death in it ensure that you use the
13:38 - double underscores before and after the
13:40 - method if you come from another
13:42 - programming language for example es6
13:44 - javascript this would be known as a
13:46 - constructor and so we use this
13:48 - initializer we use this constructor to
13:52 - construct to initialize new instances of
13:55 - this class the first argument for our
13:58 - initializer is usually self where self
14:02 - simply represents the instance of the
14:04 - class the object that's yet to be
14:07 - initialized and then after self we can
14:11 - add additional arguments our choice for
14:14 - these arguments is dictated on the basis
14:16 - that we are using this class to
14:18 - initialize a new linear model instance
14:20 - if you look back at the previous code
14:24 - initializing a linear model requires
14:26 - that we have an input size as well as an
14:28 - output size in this case one on one for
14:31 - both since each inputs will produce one
14:34 - output but we will be more general
14:37 - inside of our constructor that is the
14:40 - arguments passed and will be input size
14:42 - and output size whatever input size and
14:47 - output size is passed in as we create
14:50 - our instance and perfect that is all now
14:55 - to create a linear model to access our
14:57 - linear class we first needed to import
14:59 - torch DNN let's do that again we will
15:02 - import torch dot n n but this time let
15:08 - us do it as d elias n n we will use
15:12 - inheritance such that this subclass will
15:16 - leverage code from our base class n n
15:20 - module module itself will typically act
15:25 - as a base class for all neural network
15:27 - modules in this case the linear
15:30 - regression model class will be a
15:32 - subclass of n n module thereby inherit
15:37 - methods and variables from this
15:39 - class and when dealing with class
15:41 - inheritance one should be mindful of
15:44 - calling super dot init which simply
15:52 - allows for more freedom in the use of
15:54 - multiple inheritance from parent classes
15:56 - alright now this is just boilerplate
15:58 - code that you always need to write to
16:00 - create your custom class and now having
16:03 - performed all the necessary steps to
16:05 - inherit from our parent class we will
16:08 - declare self the linear' which
16:14 - represents the instance of the class the
16:16 - object that we intend to initialize and
16:19 - it will be set to n n dot linear as
16:23 - we've done before accepting both the
16:27 - input and output size that's being
16:29 - passed in input size and output size
16:38 - great we finally finished our
16:40 - initializer which we can now use to
16:42 - initialize a new linear model you can do
16:45 - this by setting we seem to have made an
16:48 - error and that is because I should have
16:51 - run this before running an end on module
16:53 - okay back to it we will initialize a new
16:57 - linear model an instance of this class a
16:59 - model is equal to LR and we can pass in
17:03 - two arguments into the initializer as
17:06 - clearly shown as before we pass in an
17:09 - input size of one and an output size of
17:12 - one as well and there is our linear
17:15 - model you can now seamlessly create more
17:19 - linear regression models to your heart's
17:21 - content we will stick with one and print
17:25 - out the random weights and bias values
17:28 - that were assigned to it two from n n
17:29 - dot linear we do this by printing model
17:33 - dot parameters and we will print it as a
17:39 - list in order to better see our results
17:43 - in before making any conclusions as done
17:47 - before we will seed our random number
17:49 - generator to see consistent results with
17:52 - thought Emanuel seed and it will contain
17:56 - the same seed as earlier one all right
18:00 - and our weight term equates a 0.5 1 5 3
18:05 - and the bias term negative 0.44 1 4 as
18:09 - before it and now recall to make
18:12 - predictions we make use of the forward
18:15 - method and conveniently we can define
18:18 - the forward method inside of our class
18:19 - such that the method can be accessed for
18:22 - every instance of the class so we write
18:25 - def forward once again in the first
18:29 - argument being self and arguments there
18:32 - after being the ones you need to
18:33 - actually pass in in this case we pass in
18:36 - the input X all right
18:39 - and now the forgot my colon and now the
18:43 - prediction pred will equal the
18:46 - prediction that comes out of X being
18:49 - passed into our linear model self dot
18:52 - linear let's not forget our brackets and
18:56 - we will return the prediction return
18:59 - pred running the cell and back here we
19:04 - can now use this method for any instance
19:06 - of our class in this case we will use it
19:08 - for our model so we write model dot
19:11 - forward and into it we pass in a tensor
19:14 - as we have always done so we write x is
19:18 - equal to torch dot tensor 1.0 ensure
19:23 - that this is a float and into our
19:25 - forward function we put the X and print
19:29 - the results print model dot forward X
19:34 - like so and it outputs the appropriate
19:39 - prediction we can try this for two
19:42 - values of x one for no 2.0 wrap them up
19:45 - in a single brackets and yet it's
19:51 - consistent as it now makes two
19:53 - predictions based on two independent
19:56 - variables such that for every one input
19:58 - there is one output very well you should
20:01 - now have become very comfortable in
20:03 - making predictions using the linear
20:04 - class in an
20:06 - object-oriented programming approach
20:08 - going forward this is how we will
20:10 - approach a model initialization in this
20:12 - course the next step before we conclude
20:15 - the linear regression section is to
20:17 - actually train our model to develop new
20:20 - weight and bias values based on
20:23 - previously labeled data rather than just
20:26 - working with a random parameters we will
20:28 - do that going forward welcome back to
20:32 - another lesson previously we looked at
20:34 - various ways of making predictions with
20:36 - a linear model using PI torch and worked
20:39 - our way up to a more standard way that
20:41 - of which will be implemented going
20:43 - forward before we start discussing the
20:46 - machine learning concepts involved in
20:48 - training our model before we even train
20:50 - our model to fit a data set we must
20:53 - actually create the status at first and
20:55 - plot it as such we will keep this video
20:58 - short if you wish to skip this
21:01 - preliminary step and simply move on to
21:03 - the next video where we train our model
21:05 - I've included the source code in a
21:07 - subsequent article as well as on github
21:09 - otherwise if you wish to follow along
21:12 - feel free to use this earlier notebook
21:13 - as a starter project as we will be
21:16 - simply building over it firstly for
21:19 - visualization purposes we begin by
21:21 - importing mat plot Lib dot PI plot as
21:26 - PLT as we have always done before and
21:30 - instead of pressing Shift + Enter to run
21:33 - the cell press Alt + Enter to create a
21:36 - new cell right below as it is in this
21:39 - cell that we will be creating our
21:40 - dataset ultimately this is the data set
21:44 - that we will try and fit a linear model
21:46 - into our data set is plotted on a two
21:49 - dimensional coordinate system such that
21:51 - each data point is characterized by an X
21:54 - and the y coordinate as such we will be
21:58 - specifying the X values for our data
22:01 - points which we will set equal to torch
22:03 - dot R and N and what this does is it
22:08 - returns a tensor filled with random
22:11 - numbers that are normally distributed
22:13 - this function accepts a sequence of
22:16 - integers which define the shape of our
22:18 - tensor we
22:20 - eret answer to have 100 rows and 1
22:23 - column such that there will be 100
22:26 - points and each point has a single value
22:30 - within the normal distribution if we
22:33 - print this value print X its outputs a
22:39 - tensor filled with numbers that are
22:42 - normally spaced out however the numbers
22:46 - are relatively small since they are
22:48 - centered around 0 with a small variance
22:51 - so what we can do is multiply all the
22:54 - numbers in the scalar by a value of 10
22:57 - times 10 to set up a larger range as
23:01 - very dealing with a 2-dimensional
23:04 - coordinate system each data point will
23:06 - have both an x value and a y value which
23:11 - we will set equal to the output Y for
23:16 - now will simply be a function of x to
23:19 - further modify the Y value let us simply
23:21 - plot our current set of data points with
23:24 - PLT dot plot X recall this must be
23:29 - converted to a numpy array and the Y
23:33 - value as well y dot numpy and we want
23:38 - each data point to show up as a circle
23:41 - so what we can do is just place the
23:43 - letter O as our third argument oh not 0
23:48 - outputting this should result in a
23:51 - straight line of data points this was to
23:54 - be expected such that all data points
23:57 - are normally distributed within the X
24:00 - all right now fitting a linear model
24:03 - into a straight line of data points
24:05 - would be a relatively easy we will
24:08 - challenge our model by adding a bit of
24:10 - noise to our output to each Y value of
24:14 - the point we want to shift it upwards or
24:16 - downwards since that the noise is also
24:19 - normally distributed across the entire
24:21 - range so instead of just setting the
24:25 - output equal to the input X let us add
24:28 - this normal distribution of noise to all
24:31 - 100 points
24:33 - adding torch dot R and n 100 to 1 and
24:42 - recall that R and n will center around
24:45 - the 0 with a relatively small Center
24:48 - deviation we want that appoints to be
24:51 - relatively spaced out so we want the
24:53 - noise to be reasonably significant as
24:56 - such we'll multiply the noise ratio by 3
24:59 - 3 times torched on randon and this
25:02 - should create our noisy data set all
25:07 - right now for aesthetic purposes or
25:10 - actually for clarity let us assign the
25:13 - y-axis a label PLT dot y label y the
25:21 - x-axis is well PLT dot X label alright
25:26 - and we are good to go we have our noisy
25:31 - data and now that we've created our data
25:32 - set it is time to train a model to fit
25:36 - this data set seeing that we've already
25:39 - done the code for creating a model we'll
25:41 - reuse it this linear model that we
25:44 - currently have it has random weight and
25:48 - bias values random parameters assigned
25:51 - to it from the linear model one can only
25:53 - assume that this model does not fit our
25:56 - data wall all right well let's see for
25:59 - ourselves what we'll do is we'll create
26:01 - a new cell you know what we'll actually
26:05 - do is just delete what's inside of this
26:07 - cell delete these cells as well you can
26:10 - do so by pressing ctrl M and D curl m
26:13 - and D and over here inside the cell we
26:16 - will first obtain the model parameters
26:18 - by unpacking model dot parameters into a
26:23 - list of two elements W and B and before
26:29 - proceeding any further
26:30 - it seems that none of our cells are run
26:33 - and I'll just go ahead and add it print
26:36 - a model in here to see an output but
26:38 - aside from nuts let's rerun all of
26:42 - ourselves to ensure that everything goes
26:44 - by smoothly as we go along
26:47 - all right back to it we were unpacking
26:50 - model not parameters into our list of
26:52 - two elements don't forget your brackets
26:54 - and now printing W and B and we see that
27:00 - the weight is a two dimensional tensor
27:03 - with one a row and one column so we can
27:06 - access this weight one is equal to the
27:09 - weight row index zero and column index
27:13 - zero and same thing for the bias term
27:16 - except it is only a single dimension
27:18 - within the 0th index of our a tensor b0
27:24 - alright and printing both values print
27:26 - of W 1 and B 1 we obtain them let's
27:32 - remove that we obtain them as a tensor
27:35 - type we can actually add a dot item to
27:39 - both terms and what that's going to do
27:41 - is give us a Python number from both
27:45 - tensor values outputting this we get our
27:49 - parameters all right and let's make this
27:52 - into a function for cleanliness what we
27:55 - can do is write def gets params and this
28:00 - will return the two values as a tuple it
28:05 - will return w0 0.8 'm as well as b 0.8
28:12 - 'm removing the following now on the
28:17 - next cell we will plot our linear model
28:19 - alongside the data points so we will
28:22 - call a function and def plot fit the
28:26 - argument it will accept is a title where
28:30 - our PLT title will equal the title that
28:34 - was just passed in now since Matt polyp
28:36 - is most compatible with numpy let us
28:39 - first actually on top import numpy as NP
28:46 - we know the equation of a line is also y
28:49 - is equal to w ax plus b we have W we
28:53 - have B as they were initialized that
28:55 - once we created our model so what we can
28:58 - do is determine numerical
29:00 - expressions for x1 and for y1 so first
29:05 - what we do is we set W 1 and B 1 equal
29:10 - to the return value of get params where
29:17 - x-axis seems to go from a negative 30 to
29:20 - 30 so what we can do is simply set X 1
29:24 - is equal to numpy array negative 30 to
29:29 - 30 make sure this is an array not a
29:32 - tensor to ensure compatibility with PI
29:34 - plot and now from these 2 X 1 points
29:38 - going from one extremity till the end we
29:42 - can get two Y points we can compute y1
29:46 - is equal to W 1 times x1 plus b1
29:53 - ultimately this will return to y1 points
29:56 - one at negative 30 and another one at 30
30:00 - which will be connected by a line hence
30:03 - plotting our linear model all right
30:06 - let's plot it PLT dot plot x1 y1 and
30:12 - given that our data set is blue let's
30:15 - plot it as a red line by specifying the
30:17 - string are all right and that's it now
30:21 - we also want to plot our scatter data
30:23 - points and we do this with PLT dot
30:26 - scatter we will plot x and y and finally
30:35 - showing our plot with PLT dot show and
30:38 - on a new cell we simply call plot fits
30:43 - with the title being initial model all
30:49 - right plotting our model alongside the
30:51 - data points we can clearly see that this
30:55 - is not the line that best fits our data
30:58 - we will need to use gradient descent to
31:01 - update its parameters we will start
31:03 - doing that going forward
31:06 - hey welcome back hope you're doing well
31:08 - previously we created a set of data
31:09 - points and initialized a linear model we
31:12 - will now move on to the next step which
31:14 - is provided a set of data points it is
31:16 - our goal to find the parameters of a
31:18 - line that will fit this data well as
31:21 - you've seen before the linear function
31:23 - will initially assign random weight and
31:26 - vice parameters to our line suppose that
31:29 - returns a line with the fallen
31:30 - parameters clearly this line does not
31:32 - represent our data well we need some
31:35 - kind of optimization algorithm that will
31:37 - adjust these parameters based on the
31:39 - total error until we end up with a line
31:42 - containing suitable parameters how do we
31:45 - determine these parameters for
31:47 - simplicity and to get a brief
31:48 - understanding let's limit our discussion
31:50 - to a single data point whatever line
31:53 - that we choose the error is determined
31:55 - by subtracting the prediction at that
31:58 - point from the actual Y value the closer
32:01 - the prediction is to the Y value the
32:03 - smaller the error the prediction as you
32:06 - should already know can be re-written as
32:08 - W x1 plus B
32:11 - however since we're dealing with a
32:14 - single dot an infinite amount of lines
32:17 - can be drawn through it
32:21 - so we will remove the bias removing that
32:25 - extra degree of freedom for now and we
32:27 - can cancel it out by ensuring the bias
32:29 - term is fixed at zero
32:38 - all right now whatever line that we're
32:41 - dealing with the optimal line will have
32:43 - a weight that will reduce this error as
32:45 - close to zero as possible say we're
32:49 - dealing with the point negative three
32:51 - and three the loss function would
32:53 - translate to three minus W times a minus
32:57 - three all of that's squared now what
33:00 - we're going to do is make a table and
33:02 - try out different values for W and see
33:05 - which one gives us the smallest error a
33:07 - weight of negative two results in the
33:10 - following which has a total error of
33:13 - nine a weight of negative 1.5 gives us
33:17 - an error of 2.25 notice how the error is
33:20 - analogous to the spacing between the
33:23 - predicted value and the actual value be
33:27 - smaller the spacing the smaller the
33:29 - error a weight of 1.5 gives an error of
33:33 - fifty six point two five two gives an
33:36 - error of 81 however if we use a line
33:39 - with a slope of negative one a weight of
33:41 - negative one clearly this perfectly fits
33:44 - this point if we calculate the error
33:47 - between the predicted value of our
33:49 - linear model and the actual value the
33:52 - error is zero which makes sense as the
33:55 - prediction lines up well with the actual
33:58 - labeled value the different errors they
34:01 - vary based on which weight parameter to
34:04 - use for our line so that it fits our
34:07 - points I've plotted different error
34:10 - values for different weights in
34:12 - matplotlib for visualization purposes
34:15 - clearly the absolute to minimum in this
34:17 - case corresponds to a weight of negative
34:20 - one now that we know how to evaluate the
34:23 - error corresponding to our linear
34:24 - equations the question remains how do we
34:28 - train a model to know that this weights
34:30 - right here that is the weight parameter
34:33 - that will yield the lowest error let's
34:36 - talk about that in the next lesson
34:38 - welcome back hope you're doing well
34:40 - previously we looked at and evaluated
34:42 - the loss function of various linear
34:44 - models with respect to different weights
34:46 - we saw that given previously labeled
34:48 - data there exists weight parameters for
34:51 - a
34:51 - that will yield the smallest error the
34:54 - question remains how do we train a model
34:57 - to determine the weight parameters which
34:59 - will minimize our error function the
35:01 - most well this is where we introduce a
35:03 - gradient descent the way this works is
35:06 - that first year linear model will begin
35:09 - with a random initial parameters recall
35:12 - when we initialize a model with the
35:14 - linear function and indeed gave us a
35:17 - random initial parameters let's ignore
35:19 - the bias value for now and based on the
35:22 - error associated with this initial
35:24 - parameter W we want to move in the
35:27 - direction that gives us the smallest
35:29 - error the trick is if I take the
35:32 - gradient of our error function the
35:34 - derivative the slope of the tangent at
35:37 - the current value that I'm at this
35:40 - derivative will take us in the direction
35:41 - of the highest error so what we do is we
35:45 - move in the negative of the gradient
35:47 - this will take us in the direction of
35:49 - the lowest error so we take the current
35:52 - weight and we subtract the derivative of
35:55 - that function at that same point this
35:58 - will take us in the direction of the
35:59 - least error
36:08 - you
36:33 - we are descending with the gradient
36:35 - however to ensure optimal results one
36:39 - should descend in really small steps as
36:42 - such we will multiply the gradient by a
36:45 - very small number known as their
36:47 - learning rates the value of the learning
36:50 - rate is empirical although a good
36:52 - standard starting value tends to be 1
36:55 - over 10 or 1 over 100 the learning rate
36:58 - needs to be sufficiently small since as
37:00 - the line is adjusting itself you never
37:02 - wanted to move too drastically in one
37:04 - direction as that can cause for unwanted
37:07 - divergent behavior throughout the course
37:10 - you will learn to adjust the learning
37:11 - rate based on empirical results and we
37:14 - will code a gradient descent algorithm
37:15 - in the next few videos but to just end
37:18 - this lesson and to follow through with
37:20 - our gradient descent example let us
37:21 - refer to my demonstration on excel in
37:23 - order to simply visualize the effect of
37:26 - gradient descent alright as you can see
37:29 - on top we have the coordinates of the
37:31 - point themselves x and y and below a set
37:34 - up two columns the weight and the error
37:37 - the initial weight is clearly negative
37:39 - 1.5
37:42 - set up the spreadsheet such that the
37:44 - second weight is equal to the value of
37:47 - the first weight minus the derivative of
37:51 - its error function at that weight times
37:54 - the learning rate of 0.01 to give us the
37:58 - new weights
38:05 - thus at each step we are moving in the
38:09 - negative of the gradient and if we do it
38:12 - for one step notice that the error gets
38:15 - smaller
38:23 - and if we keep doing it for such
38:26 - when cells notice how the weight
38:29 - eventually converges to negative one
38:31 - such that the error at that value
38:36 - approaches zero this is consistent with
38:39 - the graph that we observed earlier as a
38:41 - weight of negative one would indeed
38:44 - yield the smallest error for the point
38:46 - negative three and three as we saw in
38:48 - the last video
38:49 - all right so clearly gradient descent
38:52 - thus ending in the negative of the
38:54 - gradient is a very effective approach in
38:57 - training our model don't worry too much
38:59 - about this excel sheet this was purely
39:01 - for visualization purposes if you want I
39:04 - can include it as a resource for this
39:06 - lecture if you wish to play around with
39:08 - it in the next few videos we will work
39:10 - our way up to implementing gradient
39:12 - descent into our code but before doing
39:14 - so let us discuss the mean squared error
39:17 - we'll do that in the next section
39:20 - welcome back to another lesson let us
39:22 - conclude the theory part of this section
39:24 - by discussing the mean squared error the
39:27 - mean squared error is calculated in much
39:29 - the same way as the general loss
39:31 - equation from earlier except now we will
39:34 - consider the bias value as well since
39:37 - that is also a parameter that needs to
39:39 - be updated during the training process
39:41 - the mean squared error is best explained
39:44 - with an illustration
39:45 - suppose we had a bunch of values and we
39:49 - start by drawing some regression line
39:51 - parameterised by a random set of weight
39:53 - and bias values as before the errors
39:57 - correspond to how far the actual value
39:59 - is from the predicted value the vertical
40:02 - distance between them the error for each
40:05 - point by comparing the predicted values
40:07 - made by our linear model with the actual
40:10 - values using the following formula each
40:14 - point is associated with an error which
40:18 - means we would need to take the
40:19 - summation of the error for each point
40:22 - denote that the prediction can be
40:25 - rewritten as WX plus B
40:29 - as we are calculating the mean squared
40:32 - error we then take the average by
40:34 - dividing by the number of data points
40:35 - and I mentioned before the gradient of
40:38 - the error function should take us in the
40:40 - direction of the greatest increase in
40:42 - error
40:43 - so naturally by moving towards the
40:45 - negative of the gradient of our cost
40:47 - function we move in the direction of
40:49 - greatest ascent in the direction of the
40:52 - smallest error we will use this gradient
40:55 - as a compass to always take us downhill
40:58 - for simplicity in the last video we
41:01 - ignored the presence of a bias but the
41:04 - error is defined by two parameters both
41:07 - M and B we're not going to go through
41:10 - the math since PI Torche does all of it
41:12 - for us out of the box but really it's
41:14 - the exact same concept as earlier where
41:17 - we differentiate our error function but
41:20 - this time since our error function is
41:22 - defined by two parameters m and B we
41:25 - compute a partial derivative for each
41:27 - and just like before we start with any M
41:30 - and B value pair we use the gradient
41:32 - descent algorithm to update m and B in
41:34 - the direction of the least error and we
41:37 - update them based on the two partial
41:38 - derivatives up above such that for every
41:41 - single iteration the new weight is equal
41:44 - to the old weight - its gradient times
41:46 - your learning rates and the new bias
41:48 - value is equal to the old bias value -
41:52 - its corresponding gradient value times
41:54 - the learning rate one implementing
41:57 - gradient descent into our code we're not
41:59 - going to have to worry too much about
42:00 - the math as pi Torche does all of it for
42:03 - us out of the box it's just nice to know
42:05 - what's going on under the hood when you
42:07 - write your code the main idea being we
42:10 - start with some random model with a
42:12 - random set of width and bias value
42:15 - parameters this random model will tend
42:18 - to have a large error function a large
42:20 - cost function and we then use gradient
42:23 - descent to update the weights of our
42:25 - model in the direction of the least
42:27 - error minimizing that error to return an
42:30 - optimized model that is all guys let's
42:33 - put all of this into code I will see you
42:35 - then welcome to another lesson let's
42:37 - jump right into implement in gradient
42:39 - descent the first step is to specify the
42:43 - loss funk
42:43 - we intend to minimize given what I've
42:46 - shown you you might think to specify it
42:48 - according to the following equation
42:51 - however pi torch allows us an easy way
42:53 - to specify the loss function we can
42:56 - access the built-in a loss function from
42:58 - n n dot M s loss very easy the mean
43:06 - squared loss and store this in a value
43:10 - called criterion or a variable I should
43:13 - say not too hard all right the next step
43:16 - is to specify the optimizer optimizer
43:20 - that we will use to update our
43:23 - parameters the optimizer will use a
43:26 - gradient descent algorithm notably
43:29 - stochastic gradient descents an
43:32 - optimization algorithm that we can
43:33 - access from torch dot up Tim dot s g d
43:38 - as in stochastic gradient descent you
43:42 - may be wondering what stochastic
43:43 - gradient descent is the gradient descent
43:46 - algorithm I showed you before is known
43:48 - as a batch gradient descent which
43:50 - computes the gradient using the entire
43:52 - data set by updating the weights based
43:55 - on the sum of the accumulated errors
43:57 - this can be very bad since imagine you
44:00 - were dealing with a million points
44:02 - running the same batch of gradient
44:04 - descent processes earlier where we'd
44:07 - have to evaluate the accumulated error
44:09 - for every single data point every
44:12 - iteration that would prove very costly
44:15 - whereas with stochastic gradient descent
44:17 - it minimizes the total loss one sample
44:21 - at a time and typically reaches
44:23 - convergence much faster as it will more
44:25 - frequently update the weights of our
44:26 - model within the same sample size
44:29 - although the expression for SG d is
44:31 - different the general concept remains
44:34 - the same such that we're still updating
44:36 - the weights towards the negative of the
44:38 - gradient it's just that now with
44:40 - stochastic gradient descent it's mostly
44:42 - computationally faster all right back to
44:46 - our code in the first argument we
44:48 - specify the model parameters that should
44:50 - be optimized in this case model
44:53 - parameters and for the
44:57 - second argument we must specify the
44:58 - learning rate a liar
45:00 - a reasonable initial learning rate is
45:03 - 0.01 and recall that learning rate
45:07 - simply corresponds to the tiny steps
45:10 - that we would take to reduce the error
45:12 - in every iteration these tiny steps need
45:15 - to be sufficiently small since as the
45:18 - line is adjusting itself
45:19 - you never wanted to move drastically in
45:22 - one direction as that can cause for
45:24 - unwanted a divergent behavior in the
45:27 - near future we will look to implement
45:29 - adaptive learning rates and look to see
45:31 - how one can further fine-tune these
45:32 - hyper parameters but for now 0.01 should
45:37 - be enough to observe a significant yet
45:39 - quick drop in the loss function all
45:43 - right so now that we've specified the
45:45 - configurations for our training process
45:47 - it is now time to train our model we
45:51 - will train our model for a specified
45:53 - number of epochs an epoch is simply
45:59 - whenever we perform a single pass
46:01 - through the entire data set as we
46:05 - iterate through this data set we
46:07 - calculate the error function and back
46:09 - propagated the gradient of this error
46:11 - function to update the weights as you
46:13 - saw earlier okay so how many epochs
46:17 - should our line go through let's recall
46:19 - the concept of gradient descent which
46:22 - updates the weights and biases of our
46:24 - network in the direction that decreases
46:26 - the error function the most it's an
46:28 - iterative process which is why we need
46:31 - to pass the full data set through our
46:34 - model multiple times to ensure an
46:36 - optimized results in other words we need
46:39 - more than one epoch if you simply
46:42 - specify one epoch that leads to the
46:45 - under fitting of a curve which doesn't
46:47 - capture the underlying trend of data as
46:49 - the number of epochs increase the more
46:53 - times it's able to update the weights of
46:54 - the neural network minimizing the error
46:57 - and thus producing optimal results at
47:00 - the same time you don't want to pass in
47:02 - too many epochs
47:04 - since that can lead to overfitting
47:06 - generally which is a modeling error that
47:09 - occurs when a function
47:11 - to closely fit to a limited set of data
47:14 - points for this specific scenario
47:17 - overfitting shouldn't pose too much of
47:19 - an issue we'll see examples of it later
47:21 - on we will just stick to 100 epochs
47:24 - which would provide more than enough
47:26 - training iterations for our model all
47:30 - right and now for every epoch iteration
47:32 - for I in the range of epochs given that
47:39 - for every epoch we want to minimize the
47:41 - error of our model such that the error
47:43 - is simply a comparison between the
47:45 - predictions made by the model and the
47:47 - actual values let us first grab the
47:50 - predictions with wipe red is equal to
47:54 - model dot forward X so for each x value
47:59 - we make a prediction using the forward
48:01 - method all of which is stored inside of
48:04 - wipe red and then we compute the loss we
48:07 - saw loss is equal to the criterion which
48:12 - we set equal to the mean squared error
48:14 - and we will calculate the mean squared
48:17 - error for both the predicted values as
48:20 - well as the actual values why let's make
48:23 - sure I called it Y and indeed I did
48:28 - and now for every epoch that we iterate
48:32 - through we will print the epoch we will
48:37 - print the epoch I let me make these into
48:43 - double quotes I ran it by mistake
48:48 - all right so we
48:50 - paakai that's currently being iterated
48:52 - through as well as the loss associated
48:55 - with that epoch loss dot item forgot my
49:04 - alright simple enough now in order to
49:08 - visualize the decrease and loss at every
49:11 - single epoch what you want to do is
49:13 - before the for loop we're going to set a
49:16 - list losses is equal to an empty list
49:20 - and for every loss that we compute we
49:24 - will append it into our losses list we
49:27 - do this with losses dot append loss all
49:34 - right now having computed the loss and
49:37 - every epoch we must minimize that loss
49:41 - recall in gradient descent we must take
49:44 - the gradient of the loss function the
49:47 - derivative and recall to compute the
49:49 - derivative we use the dot backward
49:52 - method this was in the intro to tensors
49:55 - section we call loss dot backward and
49:59 - having computed the gradient we update
50:01 - our model parameters with optimizer dot
50:05 - step using the optimizer that we
50:09 - initialized earlier all optimizers
50:12 - implement the step method that's used to
50:15 - update the parameters of our model and
50:17 - can be called once the gradients are
50:20 - computed with the dot backward function
50:23 - as was just done all right we're almost
50:26 - done implementing the training process
50:28 - of our model before the optimization
50:31 - step we must set the gradients to 0
50:33 - since gradients accumulates following
50:37 - the lost backward call and we do this
50:39 - with optimizer dot 0 grad rest
50:46 - sure that every model that we train in
50:48 - this course will follow a very similar
50:50 - process where we make predictions using
50:53 - our model compare the predictions made
50:56 - by the model to the actual outputs and
50:59 - based on that determine the loss the
51:02 - mean squared error and then we use an
51:05 - optimization algorithm in our cases
51:07 - stochastic gradient descent to update
51:10 - the weights of our model in the
51:12 - direction of the least error thereby
51:14 - minimizing the error function of our
51:16 - model as we attempt to minimize the loss
51:19 - iteratively to obtain a model with
51:22 - optimal parameters the model that best
51:24 - fits our data all right perfect and by
51:27 - running at this cell we can now train
51:30 - our model all right and notice that as
51:36 - the epochs
51:38 - progress the loss gets significantly
51:41 - smaller eventually converging on a
51:44 - minimum value of it seems to be seven
51:49 - point four six although it's not zero
51:53 - for our purposes this is really good you
51:55 - have officially trained a model using
51:57 - gradient descents to fit a set of data
52:00 - points to better visualize the process
52:03 - let us now plot the loss on a graph on a
52:06 - new cell we run PLT plot the x-axis will
52:11 - be the number of epochs range epochs and
52:15 - the y-axis of the losses also before I
52:20 - forget just a fair warning if you get
52:22 - really weird results for this training
52:24 - process just simply make sure to restart
52:29 - the runtime or reset runtime whichever
52:32 - one then rerun your training process it
52:35 - should work after that
52:37 - anyway back to what we were doing simply
52:41 - plotting the losses on matplotlib and we
52:44 - will give the y-axis a label PLT dot
52:47 - well label i will name the y-axis loss
52:50 - PLT dot ox label we will name the x-axis
52:54 - epoch all right now writing the cell
52:58 - this further clarified that 100 a pox
53:02 - was indeed an appropriate amount of
53:04 - iterations to allow the model to
53:07 - eventually converge to a minimum loss of
53:10 - value as it seems to be dampening out
53:12 - right over here at around 7 point 4
53:15 - allowing our model to effectively train
53:18 - to fit our training data to visualize
53:21 - how well the model fits our data sets we
53:25 - can now plot our new linear model by
53:27 - simply calling plot fit train model
53:34 - plotting this data our rights we see our
53:39 - training data and we see our linear
53:42 - model which has been trained to fit our
53:45 - training data and it seems like it fits
53:47 - it pretty well congratulations you've
53:50 - just successfully trained a linear model
53:52 - to fit training data using gradient
53:55 - descents if this is your first time
53:58 - applying a machine learning algorithm
53:59 - then feel free to pat yourself on the
54:01 - back as this accomplishment is a pretty
54:04 - big feat in the next section we will
54:07 - step it up a notch and try to build a
54:09 - classification algorithm I'll see you in
54:11 - there great job I'm making it past the
54:13 - linear regression suction
54:15 - you've managed to train a linear model
54:17 - to fit a set of data points which is
54:19 - pretty awesome we're going to be
54:21 - training highly complex neural networks
54:22 - in this course some of which will be in
54:25 - charge of performing highly
54:26 - sophisticated tasks for now learning how
54:29 - to train a model to simply fit a linear
54:31 - data set is a great step in the right
54:33 - direction the next step is to now use a
54:36 - linear model to learn how to classify
54:38 - between two discrete classes of data
54:40 - points we do that in the next section
54:50 - you