00:00 - in this video we'll talk about making a
00:02 - simple python optical character
00:04 - recognition
00:05 - script where we use uh pi test rack
00:08 - uh api over the google interest rate ocr
00:12 - engine
00:12 - to actually read text from images
00:15 - programmatically
00:16 - so in this video we will talk about what
00:19 - is
00:20 - ocr what are the applications of optical
00:22 - character recognition
00:24 - how can we actually build an ocr script
00:27 - where we can actually read image read
00:30 - text
00:31 - from images so let's get started
00:34 - so first let's have a very brief
00:36 - introduction about what ocr is
00:38 - so humans can understand the contents of
00:42 - an image simply by looking at it we
00:44 - perceive the text on the image
00:46 - as text and can read it so computers
00:49 - don't work the same way they need
00:50 - something more complete in an organized
00:52 - way to understand that
00:54 - so this is where optical character
00:55 - recognition comes into the picture
00:57 - whether it's recognition of car plates
00:59 - from a camera or handwritten documents
01:02 - that should be converted into
01:03 - additional copy this technique is very
01:05 - very very useful
01:07 - so in this video we will delve into the
01:10 - depth of
01:11 - optical character ignition and its
01:12 - application areas we'll also
01:14 - build a simple script and python that
01:16 - will help us detect characters
01:18 - from images and you can also build on
01:21 - top of this
01:21 - simple python script and build a web
01:24 - application out of it
01:26 - so what is ocr so optical character
01:29 - recognition involves detection of text
01:31 - content on images
01:32 - and translation of those images to
01:34 - encode a text
01:35 - that the computer can understand easily
01:38 - so how is this achieved uh
01:39 - so to us as i said uh text an image is
01:42 - easily
01:43 - discernable and we are always able to
01:45 - detect characters
01:46 - and read the text but for computers it's
01:49 - only a series of dots or pixels
01:52 - so the image is first scan and the text
01:54 - and the graphics
01:55 - uh elements are converted into something
01:57 - called as a bitmap
01:59 - which is basically a matrix of black and
02:01 - white dots
02:02 - then the image is pre-processed where
02:04 - the brightness and the contrast of them
02:07 - are enhanced
02:08 - to improve accuracy of process
02:12 - now the image split results identifying
02:14 - the areas of interest
02:16 - such as where the images or text are and
02:19 - this helps
02:19 - curve the extraction process the areas
02:22 - containing text can now be broken down
02:24 - into further lines
02:25 - worlds and characters and now the ocr
02:28 - engine
02:28 - or the software is able to match the
02:30 - characters through comparison and
02:32 - various ai algorithms
02:34 - the final result is the text in the
02:36 - image that we're given
02:38 - now uh be aware that the process may not
02:41 - be 100 accurate
02:42 - and it might need some human
02:44 - intervention to correct
02:46 - some elements that were not scanned
02:47 - correctly the error correction
02:50 - part can also be achieved using a
02:51 - dictionary or even an np
02:53 - natural language processing so
02:56 - let's see where we use ocr so ocr is
02:58 - very commonly used in airports to
03:00 - automate the process of pass protocol
03:01 - mission
03:02 - extraction from information out of it
03:05 - the other
03:06 - applications include automation of data
03:08 - entry processes
03:09 - detection and recognition of carbon
03:11 - outputs
03:13 - so in this video we'll use python
03:15 - tessraft or simply by tesseract
03:17 - library which is a wrapper for google's
03:20 - tesseract or cr engine
03:22 - so i chose to make this video with
03:24 - pydescript because it's completely open
03:26 - source
03:26 - and it's being developed and maintained
03:28 - by a giant that is google
03:31 - so now first we need to download by
03:34 - tesseract
03:35 - or install it so that we can start
03:37 - working with it so to install pi
03:39 - tesseract you can go to this website
03:41 - uh and just do a simple pip install
03:43 - python
03:44 - so let's do
03:54 - the next thing which we need to do is
03:56 - have opencv
03:57 - in our computer to work with images
03:59 - using python so for that we do a pip
04:01 - install opencv
04:10 - this might take some time since opencv
04:12 - is a very very big software
04:14 - but since i already have it it was done
04:17 - in like a few seconds
04:19 - so next thing is for let's start
04:21 - creating our
04:24 - script a python script and start writing
04:26 - the code
04:32 - this should take about a few seconds to
04:35 - build
04:36 - almost there
04:40 - now first before we jump on to writing
04:44 - our script first actually let's see
04:48 - which image will be actually be reading
04:50 - from
04:51 - or how do we which image do we actually
04:53 - choose to extract the characters from so
04:55 - i have this image.png here
04:58 - so basically we'll be actually reading
05:00 - the characters from this image so this
05:01 - image has a few words
05:03 - so we try to write a python script which
05:06 - can actually read
05:07 - all of this from the image directly or
05:09 - using pi tessraft or the tesseract of
05:11 - crg
05:13 - so let's start first we make an ocr.py
05:17 - file
05:19 - now we need to import a few things for
05:21 - suppose we need to import
05:22 - cv2
05:32 - and the next thing actually we don't
05:34 - need numpy for now
05:36 - the next thing which we need is import
05:38 - by
05:40 - yeah so first uh
05:43 - let's actually read the image that we
05:45 - have so using
05:46 - again uh opencv so we have image equal
05:49 - to
06:00 - and now let's actually write our code in
06:03 - a much readable format and actually
06:05 - fragment
06:06 - the parts of our function that we need
06:08 - to write
06:09 - so let's define a function called as ocr
06:13 - and this is going to be dealing with the
06:16 - image
06:17 - and actually run our entire
06:20 - api and give us back the text from the
06:22 - image so basically text
06:24 - equal to by tesseract dot
06:29 - image to spring
06:32 - and
06:38 - so as you can see it's pretty easy and
06:40 - straightforward to actually use pi
06:42 - tesseract where we just have to make a
06:44 - function called
06:45 - where the method is image to strength
06:47 - passing the image parameter
06:48 - which again returns a text which we
06:50 - return back to our standard output
06:53 - so now it is not so straightforward to
06:56 - actually
06:57 - write nocr using python this might
06:59 - actually give us the results but
07:01 - as i said the results are not 100
07:03 - accurate we need to do some
07:04 - pre-processing of our own
07:06 - to get the result that we want so let's
07:09 - do that processing now the first thing
07:11 - which we need to do
07:13 - is convert the image to grayscale so
07:15 - let's write the function for that
07:21 - image parameter and we return
07:28 - series
07:33 - [Music]
07:41 - so color
07:56 - okay
08:00 - next thing which we need to do is uh
08:02 - remove the noise from the image so some
08:04 - images might be distorted
08:06 - and might have if you have some pixels
08:08 - which are not
08:10 - the way they're supposed to be so for
08:11 - that we actually remove some noise in
08:13 - the image to make it a bit more
08:14 - sharp let's see how we do that
08:18 - nice again we're leaving functions all
08:21 - of these the code is much more readable
08:30 - [Music]
08:34 - again let's give it a comment so that we
08:36 - know what the function is
08:52 - something called as thresholding so we
08:55 - need to have
08:56 - a threshold basically which states that
08:59 - if
09:00 - the pixel value is about uh it's above
09:02 - the threshold then the pixel
09:04 - is black or white and if it's below the
09:06 - threshold then again it's either black
09:08 - or white
09:09 - it's the opposite of what it is above
09:11 - the threshold so in this way we have
09:13 - a very concrete black and white image
09:16 - and because that
09:17 - it is very easy for pythagoras to
09:19 - predict or to give us out
09:21 - the characters from the image so again
09:24 - let's comment it first
09:27 - folding
09:30 - function the shoulder
09:34 - image and this is a bit
09:37 - long function called let me just
09:40 - copy it and then get it here
09:44 - so again we have a threshold we
09:47 - threshold this again depending on
09:48 - structured binary from the threshold
09:50 - also
09:52 - you can read more about these two
09:53 - functions uh in description below there
09:56 - is information in the description about
09:57 - all of this what i'm doing
09:59 - and now that we have a few functions
10:01 - which can help allow us to actually
10:04 - you know preprocess the image and
10:06 - enhance our image properly
10:07 - let's write the code to actually call
10:09 - these functions image equal to
10:12 - get grayscale and g first we
10:15 - convert this to grayscale and the next
10:17 - thing which we need to do is
10:19 - threshold image equal to
10:22 - thresholding image and finally now that
10:25 - we have thresholded
10:26 - it i don't know so the next thing which
10:30 - we need to do is remove the noise or
10:32 - excess noise
10:33 - that the image might have nice
10:36 - ing and yeah we're good to go so now
10:39 - that we have
10:41 - uh done our preprocessed or image using
10:45 - a very simple opencv techniques which is
10:47 - just you know
10:48 - converting it to greyscale removing the
10:50 - noise and just thresholding it
10:52 - the last part here is to actually call
10:54 - our ocr
10:55 - code so let's call our ocr code
11:00 - and pass the enhanced image here
11:03 - and once when we print it hopefully we
11:06 - should get this
11:07 - rtr this is the output where the output
11:10 - says a pipeline approach to category
11:11 - condition
11:13 - so let's just check the code once again
11:16 - and make sure that everything here is
11:17 - right
11:21 - looks about right and now we can
11:25 - run our code and see whether we have the
11:29 - output that we actually mean
11:32 - ocr
11:35 - and yeah as you can see we have the
11:38 - exact output which the image had
11:40 - which was basically a python approach to
11:42 - character recognition
11:43 - and we were able to extract that
11:45 - information from the image
11:47 - using the python test rank api
11:51 - so let's just go through uh what we have
11:53 - done here
11:54 - uh you know
12:02 - which just converts or uses the api to
12:06 - convert image to string
12:07 - but before that we had done a bit of pin
12:10 - processing on our
12:10 - image so we covered it to grayscale
12:13 - thresholded it
12:14 - and then remove the noise and after the
12:17 - image has been pre-processed
12:18 - we then pass that image to the ocr code
12:20 - and then we get our desired output which
12:22 - is
12:24 - as you can see a python approach to that
12:26 - reaction so
12:28 - uh that is all that is it for the
12:29 - tutorial for pythag
12:31 - there will be much more interesting and
12:33 - complicated tutorials
12:35 - about pipeline in the future but for now
12:38 - that is it thank you for watching i'll
12:40 - see you in the next video bye