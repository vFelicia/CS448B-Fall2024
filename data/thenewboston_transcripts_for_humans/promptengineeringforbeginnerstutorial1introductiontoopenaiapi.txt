With timestamps:

00:00 - hey everyone how's it going my name is
00:02 - Ian and in today's tutorial I'm going to
00:04 - be teaching you all about the open AI
00:06 - API and how we can interact with it
00:08 - using the Python programming language so
00:12 - in this video we're going to
00:13 - specifically focus on interacting with
00:15 - the chat completions API from open aai
00:18 - which essentially allows us to engage
00:21 - with a GPT model like GPT 3.5 turbo or
00:25 - GPT 4 in a conversational format so this
00:28 - is something that you're probably
00:29 - already familiar with with regards to
00:31 - using tools like chat gbt this is
00:33 - essentially what's happening behind the
00:34 - scenes and now we're going to use a
00:36 - programming language like python to have
00:39 - more granular control and programmatic
00:41 - control of how we interact with that API
00:44 - so instead of just typing information
00:45 - into a graphical user interface we're
00:47 - actually going to write the code to send
00:49 - these questions to the model and get
00:51 - back responses so learning the skill can
00:54 - be super useful especially if you're
00:56 - interested in like building chat Bots
00:58 - image generators re recommendation
01:00 - engines uh code review tools there's
01:03 - just so many options in fact if you ever
01:05 - wanted to see kind of what kind of
01:07 - projects are out there you might go
01:08 - check out theirs and AI for that.com
01:12 - they have a whole host of different
01:14 - projects that people have built using
01:16 - these kinds of tools so you can kind of
01:18 - see what some of the possibilities are
01:20 - and and Spark your creativity in that
01:22 - way so this video is actually the first
01:25 - in a larger Series where we'll continue
01:26 - to introduce to you the ins and outs of
01:28 - using opena
01:30 - uh it's API as well as the anthropic API
01:33 - and the prompt layer apis so this series
01:37 - is really great for anyone who's
01:39 - interested in prompt engineering as a
01:41 - whole and so if you think that's you
01:42 - then definitely stick around because I
01:43 - think we're going to have some content
01:45 - here that you'll really enjoy and be
01:46 - able to learn from so before we get
01:48 - started if you want access to the GitHub
01:50 - repo the code that I'll be using in this
01:53 - video and the videos that follow be sure
01:55 - and get that link from the description
01:56 - below you can clone or download that
01:59 - repository on onto your local
02:00 - environment and follow along with the
02:02 - videos that way so yeah without further
02:04 - Ado let's go ahead and get into the code
02:06 - okay so I'm working inside of Visual
02:08 - Studio code which editor you use does
02:11 - not matter you just need an environment
02:14 - where you can write some python code and
02:16 - a terminal to be able to run that code
02:18 - so the first thing I'm going to do up
02:20 - here at the top of my program is
02:21 - introduce some boilerplate code and I've
02:24 - got a couple modules that I need access
02:25 - to in order to make this program work
02:28 - before we talk about that let's talk
02:30 - about what this program is going to do
02:33 - essentially we are going to send a
02:35 - request to open ai's chat completion API
02:38 - and we're going to create a new chat
02:40 - completion object so that is essentially
02:42 - what happens when you're using a tool
02:44 - like chat GPT and you type in a query or
02:47 - a prompt and then you get back a
02:49 - response so this is what's happening
02:51 - behind the scenes we're going to
02:52 - actually do this programmatically using
02:54 - this API using python so we're going to
02:56 - tell it which model we want to use we're
02:58 - going to create kind of a history of the
03:00 - conversation uh we're going to set the
03:02 - tone of the conversation we're going to
03:04 - determine how uh variable it is how
03:08 - creative it is or how concise it is with
03:10 - its responses and we're also going to
03:12 - dictate uh the max number of tokens
03:14 - we'll talk more about what some of those
03:16 - terms mean here in a second but
03:17 - essentially once we run this we're going
03:19 - to get an answer back from the API to
03:22 - our question which is which NHL team
03:24 - plays in
03:26 - Pittsburgh so down here at the bottom we
03:28 - can see an example response where it
03:30 - comes back to us and it says the NHL
03:32 - team that plays in Pittsburgh is the
03:34 - Pittsburgh Penguins but that's not all
03:35 - it gives us it gives us a whole bunch of
03:37 - other information that can be really
03:38 - useful when we're creating larger
03:40 - applications around this technology so
03:43 - let's back up to the very beginning now
03:44 - that we've done kind of an overview of
03:46 - what it is we're building and let's
03:47 - start getting into the actual syntax so
03:50 - at the top here on the first line I'm
03:52 - importing the OS module we need that
03:55 - because we are going to export a
03:58 - environment variable for the open AI API
04:00 - key which you'll need to get from the
04:03 - open AI API so you might want to pause
04:05 - the video or after you're done watching
04:07 - the video go to openai's website go find
04:10 - their API uh link we can put that in the
04:12 - description of course and sign up for an
04:15 - account if you don't already have one
04:17 - set up your billing buy some credits you
04:19 - can spend as little or as much as you
04:21 - want uh based on how many tokens you'll
04:23 - think you'll need and we'll talk about
04:24 - what tokens are and how they work here
04:26 - in a second but once you do that in your
04:28 - settings you can generate a API key
04:31 - which you can then export as an
04:33 - environment variable and you're going to
04:35 - want to name it open aore aior key if
04:39 - you're going to be using this code from
04:41 - the repo you can name it whatever you
04:42 - want if you're just following along and
04:44 - you want to use a different value for
04:46 - that environment variable all right so
04:48 - then the next line the kind of meat and
04:49 - potatoes of this entire program is
04:51 - importing the open AI module without
04:54 - that we cannot interact with open AI
04:57 - apis in this case the chatet completion
04:59 - API so we got to have that one now the
05:03 - first thing we do on line 4 after
05:05 - importing those modules is we set the
05:07 - API key property or attribute equal to
05:10 - the result of doing a os. get EMV on
05:14 - that environment variable that we set
05:16 - okay so that's going to set up your API
05:18 - key on open ai's API and now you are
05:21 - able to send and receive request
05:23 - response from that API so let's look at
05:26 - this doc string here it's going to
05:28 - outline exactly what's happening inside
05:30 - of the code that follows so in the code
05:33 - we have a model argument a messages
05:35 - argument a temperature argument and a
05:37 - Max tokens argument these are not all of
05:39 - the parameters that can be used with the
05:41 - chat completion apis create method these
05:44 - are just the bare minimum that we need
05:46 - to get a response back an answer to a
05:49 - question that we're going to ask so
05:50 - let's talk about what each of those do
05:52 - starting with the model so the model is
05:54 - an ID of the model to use and we've got
05:56 - some links here you can go to the
05:58 - documentation you can see which models
06:00 - are available to you essentially there's
06:02 - GPT 3.5 turbo and there's GPT 4 GPT 4 is
06:08 - is more creative and more powerful in
06:10 - many ways but it also comes with
06:12 - additional cost in terms of the tokens
06:14 - and then the credits that uh represent
06:16 - those tokens you can see what those
06:18 - costs are by visiting open ai's pricing
06:21 - page and you can determine which one you
06:23 - should use based on that the next
06:25 - argument is messages so messages is
06:28 - going to be a list of messes is
06:29 - comprising the conversation so far so
06:31 - you can think of this as the context we
06:34 - can start with our initial question and
06:37 - that's fine the GPT uh API can take that
06:41 - and it can answer it for us but then if
06:44 - it doesn't have a history of the
06:46 - conversation that we've had and we have
06:47 - a follow-up question that doesn't have
06:50 - any context to support it then it's not
06:52 - going to know the best way to answer
06:54 - that follow-up question so by creating
06:56 - this list of messages in a ordered
06:59 - format the API can go back and look at
07:03 - the history of our conversation thus far
07:05 - to determine the context of whatever the
07:07 - current question is that way it can give
07:09 - us the best possible answer and that
07:12 - allows us to have this continual back
07:13 - and forth chat type
07:16 - conversation so after that we've got
07:18 - temperature so temperature is going to
07:21 - be the argument for the sampling
07:22 - temperature that we want to use the
07:24 - values are between 0 and two higher
07:27 - values like 0.8 or even 1 or 1.2 will
07:30 - make the output more random it'll make
07:33 - it more creative uh lower values closer
07:35 - to zero are going to make it more
07:38 - focused and deterministic so essentially
07:41 - if you need something to be very concise
07:43 - and Technical then you might want to
07:45 - keep that value lower but if you want to
07:47 - give the API some more creative freedom
07:50 - then you might want to make it a little
07:51 - bit higher it just depends you can keep
07:54 - changing it and tweaking it until you
07:56 - get it right where you need it to be
07:57 - based on a number of respons es that you
07:59 - can look at so then Max tokens here at
08:03 - the bottom is going to be the maximum
08:05 - number of tokens to generate in the chat
08:08 - completion after we're done looking at
08:10 - the code I'm going to pull up a couple
08:12 - pages and show you exactly how tokens
08:15 - are uh created from the text that we use
08:18 - in our prompts and from the text that is
08:21 - given back to us as a response to our
08:24 - prompts but essentially you can think of
08:26 - tokens as the measurement of the inputs
08:29 - and outputs to this API so if we're
08:32 - sending a request with some text
08:34 - something like which NHL team plays in
08:37 - Pittsburgh then most fourl roughly
08:41 - four-letter words are going to comprise
08:44 - one token when you get two larger words
08:46 - like Pittsburgh then it's going to start
08:48 - breaking it up into multiple tokens okay
08:51 - so which this word right here it might
08:54 - just be one token it might break it up
08:56 - into two tokens and then the space
08:59 - leading to the next word including the
09:01 - next word in this case a three-letter
09:03 - word NHL will be our next token so this
09:06 - is roughly two tokens and then the space
09:08 - before the next word with the next four
09:10 - characters is going to be the next token
09:12 - it's going to repeat that pattern until
09:13 - it gets to a word that has multiple um
09:17 - more than four essentially maybe like
09:19 - seven or more characters and then it's
09:22 - going to break it up into multiple
09:24 - tokens and so the output to our request
09:27 - will actually tell us exactly how many
09:29 - tokens were used for that request so
09:32 - that's going to include everything that
09:33 - we piped in plus whatever we're getting
09:35 - back so we'll see that more in a minute
09:37 - in any event because all of this isn't
09:39 - free and it costs money uh and because
09:42 - these GPT models actually have token
09:44 - limits you want to set a Max tokens
09:48 - value so that you can dictate exactly
09:51 - how many of your tokens are being used
09:52 - or like what the maximum amount of
09:54 - tokens can be used before you max out
09:57 - your limits that way you don't spend too
09:58 - much money money and you don't surpass
10:00 - whatever the predefined limits are for
10:02 - the models again the link up top that we
10:04 - looked at a moment ago for the models is
10:06 - going to have more information about how
10:08 - many tokens are required or what the
10:10 - maximum tokens are for any one of these
10:13 - models that are listed there all right
10:16 - so down here in this other doc string
10:18 - and when I say dock string if you're not
10:20 - familiar with those they're just
10:21 - multi-line comments with information
10:23 - about the program and so this is an
10:26 - example of the response data it has our
10:28 - chat completion object remember we are
10:30 - accessing the chat completion API and so
10:34 - if you want to learn more about all of
10:36 - the different fields that are included
10:38 - in this object you can visit this link
10:40 - right here but let's just do a brief
10:43 - overview of everything that we're
10:44 - looking at here so this is an example of
10:46 - what we would get back from executing
10:48 - our code right here so this would be the
10:51 - response variable pointing to the
10:53 - response from this API call so down here
10:57 - we have an ID that that's just a unique
10:59 - ID representing this particular uh chat
11:02 - completion object that we're getting
11:03 - back and then what type of object it is
11:06 - this is always going to be chat.
11:08 - completion when dealing with the chat
11:09 - completion API created is just a Unix
11:12 - timestamp in seconds the model is which
11:14 - model we used up above we used 3.5 for
11:18 - this particular example it was using gp4
11:21 - so it's just telling you hey gp4 but it
11:23 - has some additional numbering with
11:25 - regards to the current version that was
11:27 - used these models are changed over time
11:29 - and so they have varying versions and
11:31 - they track those with these additional
11:32 - numbers we have choices here choices is
11:35 - a list of multiple objects or
11:38 - dictionaries when we made the request we
11:40 - did not add an additional parameter
11:42 - indicating that we wanted multiple
11:43 - choices we could using a parameter
11:46 - called in we can talk more about that in
11:48 - future videos but by default you only
11:50 - get back one choice if you don't
11:51 - indicate something other then n is equal
11:54 - to one or if you just omit in all
11:56 - together then you get one choice back so
11:58 - that's what we have here we have one
11:59 - choice inside this list it has an index
12:02 - indicating its order and all the choices
12:04 - if there were more than one and then it
12:07 - has a message object or a key that
12:10 - points to an object or what we can also
12:12 - call a dictionary that indicates the
12:14 - role of the message so who is it that is
12:18 - is giving this message in this case it's
12:20 - the assistant and in the context of the
12:23 - open aai API uh especially with regards
12:27 - to chat completions we have three
12:29 - different roles we have the system we
12:32 - have the assistant and we have the user
12:33 - so we'll talk more about those as we
12:35 - execute our code here in a moment uh but
12:38 - this one is from the assistant which is
12:40 - essentially the part of the API that has
12:43 - the back and forth conversation with
12:45 - us so the content is the actual textual
12:48 - content that is a response to our
12:51 - initial question or our follow-up
12:52 - question if we're having a continued
12:54 - conversation so in this case the content
12:56 - says the NHL team that plays in
12:58 - Pittsburgh is the Pittsburgh
13:01 - Penguins the last thing that we're going
13:03 - to see with regards to the choice object
13:05 - is the Finish reason in this case it's
13:08 - stop there are multiple values that can
13:11 - go inside of uh the value here for the
13:13 - Finish reason key stop is a good one to
13:16 - see because it lets us know that it did
13:18 - not stop because of like our Max token
13:20 - limit or anything like that it stopped
13:22 - because it reached a natural stopping
13:24 - point um it's essentially if you're
13:27 - thinking about like stat St codes it's
13:29 - kind of like the 200 of status codes
13:31 - it's just saying hey everything worked
13:32 - properly and we stopped naturally there
13:35 - was no errors or there was no limits
13:37 - that were reached or anything like
13:38 - that the last thing down here outside of
13:42 - the choices list is the usage key that
13:44 - points to another dictionary
13:47 - object where we have prompt tokens and
13:51 - completion tokens so prompt tokens are
13:53 - our prompts right that we inputed and
13:55 - then completion tokens are for the
13:57 - completion object that was generated uh
14:00 - back to us and so 24 + 12 is 36 so total
14:04 - tokens is going to be the combination of
14:05 - the prompt and completion tokens this is
14:08 - useful because as you're creating these
14:11 - prompts you can start seeing how much
14:14 - the usage is and then you can determine
14:16 - okay if I did this x number of times it
14:18 - would cost me this many tokens that
14:19 - would uh you know essentially be this
14:21 - much money and that's how much this type
14:23 - of program would cost to run over you
14:25 - know some time span so it gives you an
14:27 - idea during development
14:29 - uh what your usage looks like you can
14:31 - also go to the open AI dashboard the
14:34 - same place where you'll have generated
14:36 - your API key and set up your billing
14:38 - there's going to be a usage button in
14:40 - there you can take a look at a graph
14:41 - that shows you your usage day by day so
14:44 - that's helpful as well so at the bottom
14:46 - here we have a print statement where we
14:48 - pass in our response variable we have an
14:50 - empty print just to give us some nice
14:52 - formatting with that output and then we
14:54 - have an additional print where we
14:55 - actually Traverse down through that
14:57 - dictionary looking at the choices list
14:59 - the first element inside of it with the
15:01 - zeroth index and then the message
15:03 - attribute with the content attribute
15:06 - from there which ultimately traverses
15:08 - all the way down to the string value and
15:11 - that's what will get printed to the
15:13 - console here so we can actually run this
15:16 - let me move my little video out of the
15:18 - way and run this python file you can
15:21 - actually see where my previous run is in
15:23 - the history of my
15:25 - terminal but it does take a second or
15:28 - two to run right this is an
15:30 - API this is an API call and that means
15:35 - that there's the time the latency
15:36 - between our requests what happens out
15:39 - there on the open AI server and it
15:41 - coming back to us but in addition to
15:43 - that you know we're dealing with a GPT
15:45 - model that it has to take a little time
15:47 - to process all this information so uh it
15:50 - does take a second or two and there are
15:52 - ways to increase the uh user experience
15:55 - or make the user experience a little bit
15:57 - better whenever you're building
15:58 - applications around this sort of thing
16:00 - uh you can think like when you're using
16:02 - chat gbt and you type in a question it
16:04 - might give you like a little loading
16:05 - wheel or a dot dot dot sort of thing and
16:07 - then when it actually starts giving you
16:09 - the output back it does it uh kind of
16:11 - like typing out on the screen as opposed
16:13 - to just dumping it all at once so those
16:15 - are types of things you can think about
16:17 - once you get to the uiux part of
16:19 - developing your applications for now of
16:21 - course we're just kind of waiting we
16:22 - know it's going to give us a response of
16:24 - some sort and then after a second or two
16:26 - we get back our response so let's take a
16:29 - look at what we got back
16:31 - here we can see that this is very
16:34 - similar to what we were just looking at
16:37 - with that doc string where we had some
16:39 - example output so it's going to have the
16:41 - ID the object the created timestamp the
16:44 - model in this case because we Ed GPT 3.5
16:47 - turbo it's going to say that that's the
16:49 - one that we used and then it has the
16:51 - versioning at the end there the choice
16:53 - is again because we didn't pass in any
16:55 - in argument to determine that there
16:57 - would be more more than one choice then
16:59 - it defaults to one so we get one object
17:01 - inside of here its index of course is
17:04 - zero it is the first element inside of
17:06 - this list it has a message which points
17:08 - to another dictionary or object where
17:11 - the role is assistant and the assistant
17:13 - is responding to our question with the
17:15 - content the NHL team that plays in
17:17 - Pittsburgh is the Pittsburgh Penguins
17:19 - this is actually the same exact response
17:21 - to the initial uh the example output I
17:25 - should say and that's because we have a
17:27 - pretty low um temperature right so again
17:31 - if the temperature value is low closer
17:33 - to zero uh then you're going to get more
17:35 - consistent responses not a lot of
17:37 - variation and as you go higher closer to
17:40 - one or even two then you're going to get
17:43 - more variation more creativity all right
17:46 - so the Finish reason is stop that's good
17:48 - that's what we're looking for and then
17:49 - the usage here is actually the same as
17:51 - the example that we saw in our doc
17:53 - string where we have a total of 36
17:55 - tokens awesome so let's let's pull this
17:59 - down a little bit and go back and look
18:01 - at our code one thing that I want to
18:04 - address a little more here is the role
18:09 - of the messages list okay so or the the
18:12 - role of the roles of the messages left
18:14 - we're going to talk about the roles for
18:16 - each of these messages how about that so
18:18 - the first message that we have here the
18:20 - first object representing a message in
18:22 - our messages list has a roll key and
18:25 - you'll notice both of these objects have
18:27 - that and even in our response we have
18:29 - that so the role for the initial one is
18:32 - system and like I said before there are
18:34 - three right there is the uh what do they
18:38 - call it the assistant and then we have
18:39 - the system and we have the user so the
18:41 - system is only ever going to happen once
18:43 - at the very beginning This Is Us kind of
18:46 - providing the tone or um maybe the
18:49 - context for the assistant future
18:52 - responses we're saying hey you the
18:54 - system you are a and then some sort of
18:57 - something that they are so in this case
18:59 - we're saying you're a helpful assistant
19:00 - just so you know if you were to Omit
19:02 - this by default it's essentially going
19:04 - to be a helpful assistant that's kind of
19:06 - GPT 3.5 GPT 4 is like default mode but
19:10 - if you want to modify it to where like
19:13 - you are a expert in the field of some
19:16 - sort of science or mathematics then you
19:18 - can do that and it'll give you answers
19:21 - it'll do its best to form its answers in
19:24 - such a way uh that would respond to you
19:26 - as if it was that role so here we're
19:29 - just saying you are a helpful assistant
19:31 - that's great you can mess around with
19:32 - this and and set the mode or set the
19:34 - mood rather set the tone for the
19:37 - assistant by modifying the value to the
19:40 - content key inside of this
19:43 - object so the next one in line is the
19:45 - role for the user this is our initial
19:47 - question this is like what you would
19:49 - type into the input on chat gbt the
19:52 - actual website so we're saying hey
19:54 - here's our initial question which NHL
19:56 - team plays in Pittsburgh and and what we
19:58 - get back of course is going to be from
20:01 - the assistant so if we go down here to
20:04 - the response you can see that the
20:06 - message that comes back has a role of
20:08 - assistant and then the content is what
20:11 - the answer is to our question and so
20:13 - this is useful because then we can take
20:15 - that object and push it into or append
20:17 - it to the end of our messages list and
20:21 - include that in our next request back to
20:23 - the API if we have a follow-up question
20:26 - as the user role so what will happen is
20:28 - we're creating this history of the
20:30 - conversation between the user and the
20:32 - assistant of course with the initial
20:33 - setup as the system and then every
20:37 - additional request we make to the API
20:39 - will have that context and be able to
20:40 - answer us more efficiently and
20:43 - effectively so that is more or less it
20:46 - I'm going to pull up uh a couple more
20:48 - pages here to introduce you to a few
20:50 - more things I think are really useful
20:51 - before we wrap up this tutorial so let
20:53 - me jump over here to uh this browser
20:56 - window where I've got a couple different
20:57 - pages PES for the the API itself that
21:00 - are open Okay so this one is for the
21:04 - chat completions API if you want to dive
21:06 - a little bit deeper or you want a a
21:09 - different perspective on everything
21:10 - you've learned in today's video this is
21:12 - a great place to start you're going to
21:14 - read through what it tells you it's
21:16 - going to have examples you can change it
21:18 - to a different language if you prefer
21:20 - something like nodejs to Python and you
21:21 - can see the code for that and you can
21:24 - see some example results and how to
21:26 - Traverse through those objects to get
21:28 - whatever it is you're looking for in
21:29 - this case the content it's going to give
21:32 - you information about all the various
21:34 - parts of the response object and so on
21:38 - so this is helpful as review and if you
21:40 - want to dive a little bit deeper of
21:41 - course you have access to way more than
21:43 - what we talked about in this tutorial so
21:46 - this is a good place to bookmark and
21:48 - come back to as needed meanwhile over
21:51 - here we have access to the playground by
21:53 - the way all the links to these will be
21:55 - included in the description of this
21:56 - video but this is the the playground
21:58 - where let's just say you're in an
22:00 - environment where you don't have
22:01 - immediate access to node.js or python to
22:04 - be able to run this code in a editor or
22:07 - some type of IDE no problem you can
22:09 - still experiment with this if you have
22:11 - an API key set up you can go to the
22:13 - playground here you can tell the system
22:16 - what the system is that's that zeroi
22:18 - message and that messages list and then
22:20 - you can start with your user user rooll
22:24 - content just by adding a message here
22:26 - and then you'll start getting responses
22:27 - back back from the assistant role after
22:30 - you submit the user message so then you
22:33 - can modify the mode whether it's a chat
22:35 - and then there's a couple other Legacy
22:36 - options here that you can look into the
22:38 - model that you want we have things like
22:40 - 3.5 turbo which is what we used in our
22:42 - code example or we have GPT 4 GPT 3.5
22:46 - turbo 16k you can learn more about what
22:48 - that is from the documentation or that
22:50 - you have even more models than that that
22:52 - you can access just a reminder there gb4
22:55 - will charge you more tokens per request
22:59 - so you want to make sure that you're
23:00 - familiar with the pricing structure and
23:01 - everything and again we've linked to the
23:03 - pricing earlier in the tutorial you'll
23:05 - have that in the description of this
23:07 - video as well but you can see here you
23:09 - can modify things like the maximum
23:11 - length which is your tokens and the
23:13 - temperature and there's some other
23:15 - things in here that we didn't talk about
23:16 - that you're feel free to mess around
23:18 - with uh these are some of the parameters
23:20 - that are not essentially required but
23:22 - can give you a little more control as
23:24 - needed so then the last thing I have
23:26 - here is this tokenizer page P which I
23:28 - want to show you all because we talked
23:30 - about tokens and I kind of briefly
23:32 - explained that one token is roughly four
23:34 - characters of text and the common
23:36 - English language uh or common English
23:38 - text and so they translate to roughly
23:41 - 3/4 of a word so like let's say you have
23:43 - 100 tokens that's around 75 words but if
23:46 - you wanted a nice visualization of how
23:48 - this actually works then I want to show
23:50 - that to you here so if I click on show
23:52 - example here on this tokenizer
23:54 - page it's going to give me some text it
23:57 - even gives me me an emoji and it gives
23:58 - me some numbers things like that and you
24:01 - can see here how many tokens it pulled
24:05 - from this text or how many tokens uh
24:08 - this text is equal to and then how many
24:11 - characters were in the text overall and
24:13 - you can see with this color highlighting
24:15 - how exactly it breaks it up so for
24:17 - example a word like emojis is M and then
24:20 - o and then e or Unicode is Yun and then
24:23 - I and then OD but there's some other
24:25 - words that are long that still equal one
24:28 - token like underlying the exact science
24:30 - isn't necessarily four characters but
24:32 - that's just kind of the average that you
24:34 - can round it uh down to but if you want
24:37 - to get more of an understanding of kind
24:39 - of how this process works you can look
24:41 - you also notice that some of these these
24:44 - tokens are broken up by like common
24:46 - patterns so for example one two 3 is a
24:49 - common pattern so out of 1 2 3 4 5 6 7 8
24:51 - 90 it broke up 1 123 into its own token
24:55 - and then 4 five and then 678 and then 90
24:59 - so you can look at the token IDs and you
25:01 - can see that essentially if there's two
25:03 - matching tokens they have the same token
25:05 - ID uh but each one of these tokens is
25:08 - given its own ID we're going to talk a
25:10 - lot more about this more advanced stuff
25:12 - in future videos but for now this is
25:14 - just a good introduction to kind of
25:16 - what's Happening behind the scenes what
25:18 - a token is and kind of how it's used as
25:21 - a measurement of your requests and the
25:24 - response that come back to you from the
25:26 - API so with that said thanks for
25:28 - watching this video if you have any
25:30 - questions please feel free to ask them
25:31 - in the comments below check the
25:32 - description out for all the resources
25:34 - the links to the repo for this series
25:37 - and be sure and be on the lookout for
25:38 - the next video in the series coming soon
25:40 - thanks a lot and we'll catch you all in
25:42 - the next video

Cleaned transcript:

hey everyone how's it going my name is Ian and in today's tutorial I'm going to be teaching you all about the open AI API and how we can interact with it using the Python programming language so in this video we're going to specifically focus on interacting with the chat completions API from open aai which essentially allows us to engage with a GPT model like GPT 3.5 turbo or GPT 4 in a conversational format so this is something that you're probably already familiar with with regards to using tools like chat gbt this is essentially what's happening behind the scenes and now we're going to use a programming language like python to have more granular control and programmatic control of how we interact with that API so instead of just typing information into a graphical user interface we're actually going to write the code to send these questions to the model and get back responses so learning the skill can be super useful especially if you're interested in like building chat Bots image generators re recommendation engines uh code review tools there's just so many options in fact if you ever wanted to see kind of what kind of projects are out there you might go check out theirs and AI for that.com they have a whole host of different projects that people have built using these kinds of tools so you can kind of see what some of the possibilities are and and Spark your creativity in that way so this video is actually the first in a larger Series where we'll continue to introduce to you the ins and outs of using opena uh it's API as well as the anthropic API and the prompt layer apis so this series is really great for anyone who's interested in prompt engineering as a whole and so if you think that's you then definitely stick around because I think we're going to have some content here that you'll really enjoy and be able to learn from so before we get started if you want access to the GitHub repo the code that I'll be using in this video and the videos that follow be sure and get that link from the description below you can clone or download that repository on onto your local environment and follow along with the videos that way so yeah without further Ado let's go ahead and get into the code okay so I'm working inside of Visual Studio code which editor you use does not matter you just need an environment where you can write some python code and a terminal to be able to run that code so the first thing I'm going to do up here at the top of my program is introduce some boilerplate code and I've got a couple modules that I need access to in order to make this program work before we talk about that let's talk about what this program is going to do essentially we are going to send a request to open ai's chat completion API and we're going to create a new chat completion object so that is essentially what happens when you're using a tool like chat GPT and you type in a query or a prompt and then you get back a response so this is what's happening behind the scenes we're going to actually do this programmatically using this API using python so we're going to tell it which model we want to use we're going to create kind of a history of the conversation uh we're going to set the tone of the conversation we're going to determine how uh variable it is how creative it is or how concise it is with its responses and we're also going to dictate uh the max number of tokens we'll talk more about what some of those terms mean here in a second but essentially once we run this we're going to get an answer back from the API to our question which is which NHL team plays in Pittsburgh so down here at the bottom we can see an example response where it comes back to us and it says the NHL team that plays in Pittsburgh is the Pittsburgh Penguins but that's not all it gives us it gives us a whole bunch of other information that can be really useful when we're creating larger applications around this technology so let's back up to the very beginning now that we've done kind of an overview of what it is we're building and let's start getting into the actual syntax so at the top here on the first line I'm importing the OS module we need that because we are going to export a environment variable for the open AI API key which you'll need to get from the open AI API so you might want to pause the video or after you're done watching the video go to openai's website go find their API uh link we can put that in the description of course and sign up for an account if you don't already have one set up your billing buy some credits you can spend as little or as much as you want uh based on how many tokens you'll think you'll need and we'll talk about what tokens are and how they work here in a second but once you do that in your settings you can generate a API key which you can then export as an environment variable and you're going to want to name it open aore aior key if you're going to be using this code from the repo you can name it whatever you want if you're just following along and you want to use a different value for that environment variable all right so then the next line the kind of meat and potatoes of this entire program is importing the open AI module without that we cannot interact with open AI apis in this case the chatet completion API so we got to have that one now the first thing we do on line 4 after importing those modules is we set the API key property or attribute equal to the result of doing a os. get EMV on that environment variable that we set okay so that's going to set up your API key on open ai's API and now you are able to send and receive request response from that API so let's look at this doc string here it's going to outline exactly what's happening inside of the code that follows so in the code we have a model argument a messages argument a temperature argument and a Max tokens argument these are not all of the parameters that can be used with the chat completion apis create method these are just the bare minimum that we need to get a response back an answer to a question that we're going to ask so let's talk about what each of those do starting with the model so the model is an ID of the model to use and we've got some links here you can go to the documentation you can see which models are available to you essentially there's GPT 3.5 turbo and there's GPT 4 GPT 4 is is more creative and more powerful in many ways but it also comes with additional cost in terms of the tokens and then the credits that uh represent those tokens you can see what those costs are by visiting open ai's pricing page and you can determine which one you should use based on that the next argument is messages so messages is going to be a list of messes is comprising the conversation so far so you can think of this as the context we can start with our initial question and that's fine the GPT uh API can take that and it can answer it for us but then if it doesn't have a history of the conversation that we've had and we have a followup question that doesn't have any context to support it then it's not going to know the best way to answer that followup question so by creating this list of messages in a ordered format the API can go back and look at the history of our conversation thus far to determine the context of whatever the current question is that way it can give us the best possible answer and that allows us to have this continual back and forth chat type conversation so after that we've got temperature so temperature is going to be the argument for the sampling temperature that we want to use the values are between 0 and two higher values like 0.8 or even 1 or 1.2 will make the output more random it'll make it more creative uh lower values closer to zero are going to make it more focused and deterministic so essentially if you need something to be very concise and Technical then you might want to keep that value lower but if you want to give the API some more creative freedom then you might want to make it a little bit higher it just depends you can keep changing it and tweaking it until you get it right where you need it to be based on a number of respons es that you can look at so then Max tokens here at the bottom is going to be the maximum number of tokens to generate in the chat completion after we're done looking at the code I'm going to pull up a couple pages and show you exactly how tokens are uh created from the text that we use in our prompts and from the text that is given back to us as a response to our prompts but essentially you can think of tokens as the measurement of the inputs and outputs to this API so if we're sending a request with some text something like which NHL team plays in Pittsburgh then most fourl roughly fourletter words are going to comprise one token when you get two larger words like Pittsburgh then it's going to start breaking it up into multiple tokens okay so which this word right here it might just be one token it might break it up into two tokens and then the space leading to the next word including the next word in this case a threeletter word NHL will be our next token so this is roughly two tokens and then the space before the next word with the next four characters is going to be the next token it's going to repeat that pattern until it gets to a word that has multiple um more than four essentially maybe like seven or more characters and then it's going to break it up into multiple tokens and so the output to our request will actually tell us exactly how many tokens were used for that request so that's going to include everything that we piped in plus whatever we're getting back so we'll see that more in a minute in any event because all of this isn't free and it costs money uh and because these GPT models actually have token limits you want to set a Max tokens value so that you can dictate exactly how many of your tokens are being used or like what the maximum amount of tokens can be used before you max out your limits that way you don't spend too much money money and you don't surpass whatever the predefined limits are for the models again the link up top that we looked at a moment ago for the models is going to have more information about how many tokens are required or what the maximum tokens are for any one of these models that are listed there all right so down here in this other doc string and when I say dock string if you're not familiar with those they're just multiline comments with information about the program and so this is an example of the response data it has our chat completion object remember we are accessing the chat completion API and so if you want to learn more about all of the different fields that are included in this object you can visit this link right here but let's just do a brief overview of everything that we're looking at here so this is an example of what we would get back from executing our code right here so this would be the response variable pointing to the response from this API call so down here we have an ID that that's just a unique ID representing this particular uh chat completion object that we're getting back and then what type of object it is this is always going to be chat. completion when dealing with the chat completion API created is just a Unix timestamp in seconds the model is which model we used up above we used 3.5 for this particular example it was using gp4 so it's just telling you hey gp4 but it has some additional numbering with regards to the current version that was used these models are changed over time and so they have varying versions and they track those with these additional numbers we have choices here choices is a list of multiple objects or dictionaries when we made the request we did not add an additional parameter indicating that we wanted multiple choices we could using a parameter called in we can talk more about that in future videos but by default you only get back one choice if you don't indicate something other then n is equal to one or if you just omit in all together then you get one choice back so that's what we have here we have one choice inside this list it has an index indicating its order and all the choices if there were more than one and then it has a message object or a key that points to an object or what we can also call a dictionary that indicates the role of the message so who is it that is is giving this message in this case it's the assistant and in the context of the open aai API uh especially with regards to chat completions we have three different roles we have the system we have the assistant and we have the user so we'll talk more about those as we execute our code here in a moment uh but this one is from the assistant which is essentially the part of the API that has the back and forth conversation with us so the content is the actual textual content that is a response to our initial question or our followup question if we're having a continued conversation so in this case the content says the NHL team that plays in Pittsburgh is the Pittsburgh Penguins the last thing that we're going to see with regards to the choice object is the Finish reason in this case it's stop there are multiple values that can go inside of uh the value here for the Finish reason key stop is a good one to see because it lets us know that it did not stop because of like our Max token limit or anything like that it stopped because it reached a natural stopping point um it's essentially if you're thinking about like stat St codes it's kind of like the 200 of status codes it's just saying hey everything worked properly and we stopped naturally there was no errors or there was no limits that were reached or anything like that the last thing down here outside of the choices list is the usage key that points to another dictionary object where we have prompt tokens and completion tokens so prompt tokens are our prompts right that we inputed and then completion tokens are for the completion object that was generated uh back to us and so 24 + 12 is 36 so total tokens is going to be the combination of the prompt and completion tokens this is useful because as you're creating these prompts you can start seeing how much the usage is and then you can determine okay if I did this x number of times it would cost me this many tokens that would uh you know essentially be this much money and that's how much this type of program would cost to run over you know some time span so it gives you an idea during development uh what your usage looks like you can also go to the open AI dashboard the same place where you'll have generated your API key and set up your billing there's going to be a usage button in there you can take a look at a graph that shows you your usage day by day so that's helpful as well so at the bottom here we have a print statement where we pass in our response variable we have an empty print just to give us some nice formatting with that output and then we have an additional print where we actually Traverse down through that dictionary looking at the choices list the first element inside of it with the zeroth index and then the message attribute with the content attribute from there which ultimately traverses all the way down to the string value and that's what will get printed to the console here so we can actually run this let me move my little video out of the way and run this python file you can actually see where my previous run is in the history of my terminal but it does take a second or two to run right this is an API this is an API call and that means that there's the time the latency between our requests what happens out there on the open AI server and it coming back to us but in addition to that you know we're dealing with a GPT model that it has to take a little time to process all this information so uh it does take a second or two and there are ways to increase the uh user experience or make the user experience a little bit better whenever you're building applications around this sort of thing uh you can think like when you're using chat gbt and you type in a question it might give you like a little loading wheel or a dot dot dot sort of thing and then when it actually starts giving you the output back it does it uh kind of like typing out on the screen as opposed to just dumping it all at once so those are types of things you can think about once you get to the uiux part of developing your applications for now of course we're just kind of waiting we know it's going to give us a response of some sort and then after a second or two we get back our response so let's take a look at what we got back here we can see that this is very similar to what we were just looking at with that doc string where we had some example output so it's going to have the ID the object the created timestamp the model in this case because we Ed GPT 3.5 turbo it's going to say that that's the one that we used and then it has the versioning at the end there the choice is again because we didn't pass in any in argument to determine that there would be more more than one choice then it defaults to one so we get one object inside of here its index of course is zero it is the first element inside of this list it has a message which points to another dictionary or object where the role is assistant and the assistant is responding to our question with the content the NHL team that plays in Pittsburgh is the Pittsburgh Penguins this is actually the same exact response to the initial uh the example output I should say and that's because we have a pretty low um temperature right so again if the temperature value is low closer to zero uh then you're going to get more consistent responses not a lot of variation and as you go higher closer to one or even two then you're going to get more variation more creativity all right so the Finish reason is stop that's good that's what we're looking for and then the usage here is actually the same as the example that we saw in our doc string where we have a total of 36 tokens awesome so let's let's pull this down a little bit and go back and look at our code one thing that I want to address a little more here is the role of the messages list okay so or the the role of the roles of the messages left we're going to talk about the roles for each of these messages how about that so the first message that we have here the first object representing a message in our messages list has a roll key and you'll notice both of these objects have that and even in our response we have that so the role for the initial one is system and like I said before there are three right there is the uh what do they call it the assistant and then we have the system and we have the user so the system is only ever going to happen once at the very beginning This Is Us kind of providing the tone or um maybe the context for the assistant future responses we're saying hey you the system you are a and then some sort of something that they are so in this case we're saying you're a helpful assistant just so you know if you were to Omit this by default it's essentially going to be a helpful assistant that's kind of GPT 3.5 GPT 4 is like default mode but if you want to modify it to where like you are a expert in the field of some sort of science or mathematics then you can do that and it'll give you answers it'll do its best to form its answers in such a way uh that would respond to you as if it was that role so here we're just saying you are a helpful assistant that's great you can mess around with this and and set the mode or set the mood rather set the tone for the assistant by modifying the value to the content key inside of this object so the next one in line is the role for the user this is our initial question this is like what you would type into the input on chat gbt the actual website so we're saying hey here's our initial question which NHL team plays in Pittsburgh and and what we get back of course is going to be from the assistant so if we go down here to the response you can see that the message that comes back has a role of assistant and then the content is what the answer is to our question and so this is useful because then we can take that object and push it into or append it to the end of our messages list and include that in our next request back to the API if we have a followup question as the user role so what will happen is we're creating this history of the conversation between the user and the assistant of course with the initial setup as the system and then every additional request we make to the API will have that context and be able to answer us more efficiently and effectively so that is more or less it I'm going to pull up uh a couple more pages here to introduce you to a few more things I think are really useful before we wrap up this tutorial so let me jump over here to uh this browser window where I've got a couple different pages PES for the the API itself that are open Okay so this one is for the chat completions API if you want to dive a little bit deeper or you want a a different perspective on everything you've learned in today's video this is a great place to start you're going to read through what it tells you it's going to have examples you can change it to a different language if you prefer something like nodejs to Python and you can see the code for that and you can see some example results and how to Traverse through those objects to get whatever it is you're looking for in this case the content it's going to give you information about all the various parts of the response object and so on so this is helpful as review and if you want to dive a little bit deeper of course you have access to way more than what we talked about in this tutorial so this is a good place to bookmark and come back to as needed meanwhile over here we have access to the playground by the way all the links to these will be included in the description of this video but this is the the playground where let's just say you're in an environment where you don't have immediate access to node.js or python to be able to run this code in a editor or some type of IDE no problem you can still experiment with this if you have an API key set up you can go to the playground here you can tell the system what the system is that's that zeroi message and that messages list and then you can start with your user user rooll content just by adding a message here and then you'll start getting responses back back from the assistant role after you submit the user message so then you can modify the mode whether it's a chat and then there's a couple other Legacy options here that you can look into the model that you want we have things like 3.5 turbo which is what we used in our code example or we have GPT 4 GPT 3.5 turbo 16k you can learn more about what that is from the documentation or that you have even more models than that that you can access just a reminder there gb4 will charge you more tokens per request so you want to make sure that you're familiar with the pricing structure and everything and again we've linked to the pricing earlier in the tutorial you'll have that in the description of this video as well but you can see here you can modify things like the maximum length which is your tokens and the temperature and there's some other things in here that we didn't talk about that you're feel free to mess around with uh these are some of the parameters that are not essentially required but can give you a little more control as needed so then the last thing I have here is this tokenizer page P which I want to show you all because we talked about tokens and I kind of briefly explained that one token is roughly four characters of text and the common English language uh or common English text and so they translate to roughly 3/4 of a word so like let's say you have 100 tokens that's around 75 words but if you wanted a nice visualization of how this actually works then I want to show that to you here so if I click on show example here on this tokenizer page it's going to give me some text it even gives me me an emoji and it gives me some numbers things like that and you can see here how many tokens it pulled from this text or how many tokens uh this text is equal to and then how many characters were in the text overall and you can see with this color highlighting how exactly it breaks it up so for example a word like emojis is M and then o and then e or Unicode is Yun and then I and then OD but there's some other words that are long that still equal one token like underlying the exact science isn't necessarily four characters but that's just kind of the average that you can round it uh down to but if you want to get more of an understanding of kind of how this process works you can look you also notice that some of these these tokens are broken up by like common patterns so for example one two 3 is a common pattern so out of 1 2 3 4 5 6 7 8 90 it broke up 1 123 into its own token and then 4 five and then 678 and then 90 so you can look at the token IDs and you can see that essentially if there's two matching tokens they have the same token ID uh but each one of these tokens is given its own ID we're going to talk a lot more about this more advanced stuff in future videos but for now this is just a good introduction to kind of what's Happening behind the scenes what a token is and kind of how it's used as a measurement of your requests and the response that come back to you from the API so with that said thanks for watching this video if you have any questions please feel free to ask them in the comments below check the description out for all the resources the links to the repo for this series and be sure and be on the lookout for the next video in the series coming soon thanks a lot and we'll catch you all in the next video
