With timestamps:

00:00 - all righty y'all welcome back to another
00:02 - video and this one is going to be a fun
00:04 - one because what we're going to be doing
00:06 - is we are going to be improving ya we're
00:09 - going to be making sure that she is
00:11 - getting better smarter and to do that
00:14 - what I first want to do is kind of show
00:17 - you guys what a lot of people do
00:18 - whenever they try to improve their llms
00:21 - and why it may not be the best technique
00:24 - so I'm on the prompt template here and
00:26 - I'm just going to take this and open it
00:27 - in the playground because in instead of
00:31 - that question variable I want to give a
00:33 - some other questions now we already
00:36 - tested it with a very simple HTML
00:37 - question now let's go ahead and test a
00:41 - with a python question so I'll say
00:43 - what's the
00:44 - problem with this and then I'll just go
00:48 - ahead and make a simple print statement
00:50 - I'll print out hey however I will remove
00:53 - that second quotation mark and we'll see
00:56 - if I can pick up on this bug right
00:58 - here okay looks like you're missing the
01:01 - second quotation mark perfect um now let
01:04 - me give it a JavaScript related issue so
01:09 - I'll say VAR equals 3 and actually let
01:11 - me remove that semicolon too so got a
01:15 - couple issues here well first of all it
01:18 - should be VAR something a variable name
01:21 - and then of course it doesn't need to
01:23 - have that semi coolon but it's nice and
01:26 - okay so we see that a has some issues
01:29 - right here first of all um I was kind of
01:33 - giving this as a JavaScript example and
01:35 - she didn't know it was JavaScript I mean
01:38 - thinking back how could she she thought
01:40 - it was python so she definitely didn't
01:43 - get that missing
01:44 - semicolon huh now how would I tweak this
01:48 - so she answered the question correctly
01:50 - so let me remove that response and oh I
01:52 - know I can say you are a
01:56 - JavaScript AI coding assistant uh your
01:59 - job is to help people with programming
02:01 - questions let's see if we can get some
02:03 - better results this way oh there we go
02:07 - so I added the variable name and also
02:10 - added the missing semicolon okay good
02:12 - job a but um oh you know what I just
02:15 - added this JavaScript keyword so it
02:18 - probably isn't going to do that great
02:20 - for Python and HTML anymore so let me
02:22 - remove that and uh now it's not going to
02:26 - be great at JavaScript okay so already
02:29 - I'm noticing a couple things one
02:33 - whenever I say is I getting better or
02:36 - worse it's a little bit tricky because
02:38 - the question itself is vague now
02:40 - whenever your llm is getting better or
02:42 - worse it typically is getting better in
02:44 - some areas and worse in other areas now
02:48 - aside from that
02:49 - ambiguity I am also noticing that this
02:52 - entire method of testing it isn't very
02:55 - scientific I mean I'm pretty much just
02:57 - asking random questions and I'm trying
02:59 - to keep track in my head of is this
03:02 - better or is this worse nothing's
03:04 - quantifiable everything's subjective and
03:07 - it's just uh it feels messy off the bat
03:10 - so let me go ahead and show you a much
03:13 - better way to approach these kind of
03:15 - problems now let me go ahead and click
03:16 - on evaluate just so we have a nice clean
03:19 - page to look at and now let's go ahead
03:22 - and talk through some of the flaws we
03:24 - had in our logic so whenever the video
03:26 - first started I said that the goal was
03:29 - for us to improve ya or to make ya
03:32 - better but already that thought is a
03:35 - little bit subjective because what do I
03:37 - mean by better do I mean that ya is
03:40 - going to be giving shorter responses or
03:43 - do I mean that by better ya feels more
03:46 - like I'm talking to a human or do I mean
03:49 - that it's just giving more accurate
03:50 - responses just answering uh the
03:52 - questions correctly so I need to Define
03:56 - better now whenever we are developing l
04:00 - applications what we do is we need to
04:03 - assess a certain Behavior at a time and
04:07 - that means instead of using uh vague
04:09 - terms like better we need to give it a
04:11 - more clear um indicator of what we're
04:14 - assessing so instead of me saying I want
04:17 - to make a better instead what we can do
04:19 - in this example is we can say I want to
04:22 - make a better at answering common
04:25 - programming questions fixing easy
04:27 - programming bugs that's a lot more more
04:30 - quantifiable something that we can
04:31 - actually test now once we have that
04:34 - clarified the next thing we have to
04:36 - figure out is how can we test this and
04:38 - believe it or not the way that we test
04:41 - this for a llm is very similar to how we
04:43 - would test if we were a teacher and we
04:46 - were testing if a student is getting
04:49 - smarter or Dumber uh it sounds kind of
04:51 - mean but how we would do that is we
04:53 - would basically give them a test and
04:56 - then over time we can see if their test
04:57 - scores were going up or down
05:00 - so we're going to be doing the same
05:01 - thing with ya we're going to be creating
05:03 - a test and seeing if she is doing better
05:06 - or worse on it so now let me go ahead
05:09 - and pop open a spreadsheet and I'll show
05:11 - you guys the structure of how we're
05:13 - going to set up this
05:14 - test so what we're going to be doing is
05:18 - we are first going to be creating some
05:21 - type of question and we're going to ask
05:23 - it something like we did before let me
05:25 - just pop in a quick example here I'll
05:28 - say uh what's wrong with
05:31 - this and then we're going to be giving
05:33 - it some code let me just throw a little
05:37 - issue here and then in the next column
05:41 - we'll just have the answer and I'll say
05:43 - uh bad closing
05:46 - tag so I'm sure you guys can see where
05:48 - this is going um in this test we are
05:52 - actually going to have a lot of question
05:54 - and answers probably like 10 of them and
05:57 - the question what we're going to be
05:59 - doing with that is we are going to be
06:01 - throwing that into this prompt template
06:04 - and we're going to be sending it to the
06:05 - llm now when we get our response back
06:09 - actually let me say llm
06:11 - response is going to give us some type
06:14 - of answer right here and that when we
06:17 - get that answer back as the
06:20 - teacher we can just go ahead and see if
06:23 - it's right or if it's the wrong answer
06:26 - if it's the right answer oops if it's
06:29 - the right answer we'll just go ahead and
06:30 - give it a score of one if it's the wrong
06:33 - answer we'll give it a score of zero and
06:36 - that's basically our test the more
06:38 - questions that a gets right the more
06:40 - points she gets the more points she gets
06:43 - the smarter she's becoming and it's
06:45 - pretty much it the same way that you
06:47 - would test the kid like I said you were
06:48 - going to test a the same way and really
06:51 - gamify the whole experience now before
06:54 - this video what I actually did is I went
06:56 - and saved some time and I created a
06:59 - bunch of these questions and answers
07:02 - again these are just very simple um
07:05 - programming questions they usually just
07:06 - have a typo or missing semicolon or
07:09 - something and again that's the behavior
07:12 - that we want to assess the ability to
07:14 - answer common or easy programming
07:17 - questions and I'll go through each of
07:20 - these one by one real quick there's just
07:22 - 10 of them it seems kind of uh tedious
07:25 - but we're going to be working with the
07:26 - same data set a lot so it's important
07:29 - you understand the questions because
07:30 - then the responses are going to make a
07:32 - lot more sense so this first thing is
07:34 - just the HTML question you got a bad
07:36 - closing tag this second one is just this
07:40 - JavaScript question where the semicolon
07:42 - is missing the third one is just
07:45 - printing out hello world but it's
07:46 - missing those ending quotation marks now
07:49 - this fourth one is react code however
07:53 - instead of the um rendering the hello
07:56 - component I just had a typo and I called
07:58 - it jello
08:00 - so this fifth one right here this is
08:02 - just some D Jango code just instead of
08:05 - model it says schodel so that's the
08:07 - issue with that this SQL code right here
08:11 - is using two equal signs when it should
08:14 - use one for this one it's just python
08:16 - code however it's missing the colon
08:18 - right here after the for Loop there
08:20 - should be a colon now this JavaScript
08:23 - function this console log is not
08:26 - supposed to be included in this function
08:28 - body and there's also the function body
08:31 - never closes so that's another issue um
08:34 - this is SQL code 2 but instead of order
08:36 - by there's a little typo it says order
08:38 - re bu and the last question that we're
08:41 - going to be testing is some CSS code we
08:44 - can see that in this color property
08:45 - there's supposed to be a semicolon here
08:47 - and it's just missing so either way just
08:51 - a very simple 10 question test and from
08:55 - here what I want to do is I just want to
08:57 - download it so I'm going to download it
08:59 - as a CSV and then I just want to upload
09:02 - it to prompt layer so back in prompt
09:05 - layer what I'm going to do is over in
09:07 - the evaluate tab if I click new batch
09:10 - run then what I like to do is I like to
09:13 - name my batch runs or my tests basically
09:16 - the same as my data set just because I
09:19 - think it's um I don't know easy to keep
09:21 - things organized that way so my data set
09:23 - is named programming bugs easy so here
09:26 - I'm just going to call it programming
09:31 - bugs easy and now to upload this data
09:34 - set I'm going to click this button right
09:36 - here and you are going to see that I
09:38 - already have a data set in here and
09:40 - that's because I just recorded this
09:42 - video however for some reason my
09:44 - software the video and the audio got
09:47 - like mismatched and uh anyways more of
09:49 - the story I had to re-record it which
09:51 - I'm doing right now so I'll show you
09:53 - guys how to upload the data set that we
09:54 - just created if you click create data
09:57 - set then you could upload that CSV file
09:59 - just go ahead and select it here and
10:02 - then choose your CSV file hit open and
10:06 - for the data set name I'm just going to
10:08 - name
10:09 - it programming bugs easy data set and
10:13 - I'll say data set 2 because I already
10:15 - have a data set name that name create
10:17 - one and now I can select it and just say
10:20 - select data set and now last but not
10:23 - least I'm going to hit create Pipeline
10:25 - and you can see that we now in prompt
10:27 - layer have a view that's very similar to
10:29 - the spreadsheet however since it's in
10:32 - prompt layer now we get a lot of extra
10:34 - functionality so from here what I want
10:36 - to do next is I want to add that other
10:39 - column that's going to be responsible
10:40 - for making that llm request and it's
10:44 - going to be doing it using our prompt
10:46 - template so let me go ahead and add
10:48 - another column by clicking this add step
10:51 - button and I'm going to be making it a
10:53 - prompt template
10:55 - column and then I just need to give it a
10:58 - name I'm going to name the column llm
11:00 - response and then I need to choose what
11:02 - prompt template I'm going to be using
11:04 - and of course we only have one so very
11:07 - easy decision and I just always want to
11:09 - use the latest version so by version I'm
11:12 - going to select default and that's going
11:14 - to always use the latest version of The
11:16 - Prompt template and now the last thing
11:19 - that this is saying is hey in your
11:21 - prompt template you have a variable
11:24 - called question now whenever I'm going
11:27 - through which of these columns do you
11:30 - want me to plug in for that question and
11:32 - of course it's the question column so
11:34 - then we're just going to map that
11:35 - question column to the question variable
11:39 - and now I'm going to hit run step and
11:41 - what's going to happen is I believe
11:42 - we're going to get some errors and I'll
11:45 - show you uh why we we are getting them
11:48 - and how to fix
11:50 - them and all right so it says that
11:52 - there's error please select llm provider
11:55 - and engine none found so what does this
11:58 - mean well let's go ahead and open our
12:01 - template again and it is in here
12:04 - University I assistant and check it out
12:07 - so right now we already know that we
12:09 - want to make an llm request using this
12:12 - prompt which we're going to be passing
12:14 - in a question variable and we already
12:16 - hooked it up to say that this is the
12:18 - question that you're going to be using
12:20 - first and then use this one so on so
12:22 - forth looks like everything is good to
12:24 - go but what we never specified to promp
12:26 - layer is which llm provider are we using
12:29 - are we using open AI are we using
12:31 - anthropic are we using another one so on
12:33 - and so forth now there are two different
12:36 - ways that we can configure this the
12:39 - first way is we can configure on a per
12:42 - column basis so anytime we run this
12:44 - column if we click this edit button and
12:47 - we can set the model engine parameters
12:49 - here so if I just want to say yes I want
12:52 - to use open AI with this model I can go
12:54 - ahead and save that however I don't like
12:57 - doing it this way and instead what I
13:00 - like doing is I like saving the type of
13:04 - provider and model I'm going to be using
13:06 - directly on my prompt template so how do
13:08 - I do that well if I go ahead and edit
13:11 - this prompt template then right here
13:13 - where it says parameters I can go ahead
13:16 - and choose set parameters and then I'll
13:19 - just stick with this right now open AI
13:21 - uh GPT 3.5 turbo those look good now I'm
13:25 - going to go ahead and just update my
13:26 - template to make sure those parameters
13:29 - are set and now back in my report all I
13:33 - need to do is really just um rerun this
13:37 - and now hopefully if everything went
13:39 - good since I have my parameters tied to
13:42 - my prompt template now it knows to use
13:45 - open AI so now let's see whenever it
13:48 - makes this request so now you see we
13:51 - start to get some answers back and let's
13:54 - go ahead and take a quick peek all right
13:56 - error in your HTML code that looks good
13:59 - nothing technically wrong still think
14:00 - it's python code but that's all right
14:03 - all right so this preview is looking
14:05 - good so far I got everything configured
14:07 - how I want it the only thing that I want
14:09 - to do before I kick off the full run
14:12 - again right now it's only doing the
14:13 - first four while I get everything
14:15 - configure but when I click this it's
14:17 - going to do the full 10 before I do that
14:19 - I want to add one more column so we can
14:21 - actually score these answers so I'm
14:23 - going to add another column and since
14:26 - it's just going to be me looking at the
14:28 - answers in manually giv a score I'm
14:30 - going to choose human input and then I'm
14:33 - going to click next and then for this
14:36 - column I'm just going to say score and
14:38 - for the data type it's just going to be
14:40 - a number again zero for wrong one for
14:43 - right and I'll just say text box it's
14:46 - the easiest thing so I'm just going to
14:48 - save this and now I can go ahead and
14:51 - enter whatever number I want and there
14:54 - you go so this is all good right now and
14:58 - uh yeah from here just go ahead and hit
15:00 - run full batch and then that's going to
15:02 - go ahead and run your full test
15:06 - Suite so after a couple seconds you see
15:08 - that our full test is now populated and
15:12 - then ya is going to be figuring out
15:15 - these answers and I'll give her some
15:17 - time and it will be just a few seconds
15:20 - but I'll give her a few more all right
15:24 - now after a couple seconds you see that
15:26 - ya has now responded with all our
15:29 - answers and what I'm going to do now is
15:31 - pretty much just go through like I'm a
15:34 - teacher grading a test just look at her
15:36 - answers and if it's right give her a one
15:39 - if it's wrong give her a zero and I'll
15:41 - do about like two or three of these and
15:44 - then I'll pause the video so you don't
15:46 - have to watch me do each one so the
15:48 - first one closing
15:50 - tag
15:52 - okay and it also says the title tag is
15:56 - missing but I don't think it was
15:59 - missing the closing title tag no it
16:01 - wasn't was it okay so she got like uh I
16:05 - don't know that's just like uh uh kind
16:07 - of got it so I'll give her half a point
16:09 - for
16:10 - that um let's see what else uh in this
16:14 - one nothing wrong but if you want to
16:17 - follow convention you should add
16:18 - semicolons okay that one's
16:21 - right and let's see this last one
16:23 - quotation mark she got that one right so
16:25 - that's another one and yeah so I'm basic
16:28 - basically just going to go through the
16:30 - rest of these I'll pause the video now
16:32 - and at the end of it I'll let you know
16:35 - the final score all right so I just
16:37 - finished grading the test and once I
16:40 - added the last value here prom player
16:42 - updated the average score so we can see
16:45 - that a got an 85% out of 100 she missed
16:50 - this first one where she said that there
16:52 - was no closing title or head tag and in
16:55 - fact there were and also another rea one
16:59 - where she said to import something from
17:02 - somewhere but uh it actually was the
17:04 - correct import so either way we got a
17:07 - score of 85 not too bad but I have a
17:12 - feeling that we could do some prompt
17:14 - engineering and make this a little bit
17:16 - better so what I'm going to do is I'm
17:18 - going to go back to my template right
17:20 - here and I'm going to edit it and how
17:25 - about we do something like this um you
17:28 - are a e coding assistant you are the
17:32 - best programmer in the
17:36 - world and another thing that I want to
17:38 - change is before we were using 3.5 turbo
17:42 - let's go ahead and see if GPT 4 is any
17:45 - better so now I made some changes to
17:47 - this I'm going to go ahead and update my
17:49 - template uh I don't need a commit
17:52 - message now here what I want to do is
17:54 - since I updated my template and I want
17:56 - to taste it test it out on that same
17:58 - test that we built I'm go ahead and
18:00 - select this do you want to run an
18:02 - evaluation basically do you want to run
18:04 - a test and for this I'm going to select
18:06 - the one that we just made programming
18:09 - bugs easy so now I'm going to go ahead
18:11 - and confirm this and when I do you can
18:14 - see if I hover over my latest version
18:16 - the updates that we just made I have
18:19 - this little link to eval report and if I
18:21 - click that you can see that the updated
18:23 - test is now running well in fact it's
18:25 - the same test just with the updated
18:28 - prompt templ
18:29 - and that brings me to another point
18:31 - which is whenever you are evaluating a
18:34 - prompt template what you want to make
18:37 - sure of is you're using a consistent
18:39 - test and the reason for that is because
18:42 - in this experiment you already have one
18:44 - of the variables changing which is the
18:47 - prompt template itself now if you are
18:49 - also changing it by giving it new test
18:52 - questions every time your experiment
18:55 - isn't going to be very scientific you're
18:57 - basically testing too much data or in
18:59 - other words what's the reason why the
19:01 - system is either improving or not
19:03 - improving so now it looks like I V2
19:08 - let's call her uh finish answering this
19:11 - test so now let me go ahead and score it
19:15 - and see if she did any better all right
19:17 - and check it out this time after our
19:20 - adjustments it does indeed look like a
19:23 - improved she now had a perfect score all
19:27 - right so off the bat it feels like like
19:28 - this process is indeed better than just
19:32 - using the playground and trying to keep
19:33 - track of everything in your head and not
19:35 - being too scientific however there's
19:38 - still some things in this process that
19:40 - feel a little bit tedious first of all
19:43 - me going through in scoring each of
19:46 - these one by one I mean honestly it's
19:48 - taken a little bit of time it's kind of
19:50 - annoying and imagine if I have a bigger
19:54 - data set with a 100 or a thousand cases
19:57 - and I have a bunch of different prompt
19:58 - templates that I want to test I mean I'm
20:00 - going to get carpal tunnel in no time
20:02 - having to do this so I need to figure
20:04 - out a better way where I can actually
20:06 - score these
20:07 - responses and in addition to that
20:10 - another thing that I was thinking is
20:13 - check this out so before I just go ahead
20:17 - and release this to the new boston.com
20:19 - and let people start typing in any
20:21 - answer or excuse me any question that
20:23 - they want I want to think about this
20:26 - from a user's perspective so they are
20:29 - going to be on my website and they're
20:31 - going to be watching a video and what
20:33 - are they going to be typing in a lot of
20:35 - the time they're going to be typing in
20:36 - questions like we just saw programming
20:38 - related questions so on and so forth but
20:41 - let's say that they're watching the
20:42 - video of me teaching how to make beer or
20:45 - how to build a go-kart they could also
20:48 - ask questions
20:49 - like uh this who is the dude in this
20:54 - video and well it may seem obvious just
20:58 - you know detect who's in the video and
21:00 - see but from A's perspective whenever
21:03 - she is embedded here she has no idea
21:05 - that she's on a web page she has no idea
21:07 - that there's a video right here or
21:09 - anything like that so she's not going to
21:11 - be able to respond to that question just
21:13 - because she doesn't have the context
21:16 - necessary now another thing is if we
21:19 - just put ourselves in the user shoes
21:22 - again they are going to be talking to a
21:25 - chat bot so they may ask some questions
21:27 - which they think is a simple question
21:30 - like what is your
21:33 - name and of course we know her name is a
21:36 - I mean I already said that in the other
21:38 - video and you know here you can see it
21:41 - says a on the left but for her huh her
21:45 - response would be as an i AI I don't
21:48 - have a personal name and we know that's
21:50 - not really true either so it seems like
21:54 - yes we are getting better but there's
21:56 - still some Kinks that we have to work
21:58 - out and in the next videos we're going
22:00 - to be taking a look at how to solve
22:01 - those problems but for now thank you
22:03 - guys for watching don't forget to
22:05 - subscribe and I'll see you next time

Cleaned transcript:

all righty y'all welcome back to another video and this one is going to be a fun one because what we're going to be doing is we are going to be improving ya we're going to be making sure that she is getting better smarter and to do that what I first want to do is kind of show you guys what a lot of people do whenever they try to improve their llms and why it may not be the best technique so I'm on the prompt template here and I'm just going to take this and open it in the playground because in instead of that question variable I want to give a some other questions now we already tested it with a very simple HTML question now let's go ahead and test a with a python question so I'll say what's the problem with this and then I'll just go ahead and make a simple print statement I'll print out hey however I will remove that second quotation mark and we'll see if I can pick up on this bug right here okay looks like you're missing the second quotation mark perfect um now let me give it a JavaScript related issue so I'll say VAR equals 3 and actually let me remove that semicolon too so got a couple issues here well first of all it should be VAR something a variable name and then of course it doesn't need to have that semi coolon but it's nice and okay so we see that a has some issues right here first of all um I was kind of giving this as a JavaScript example and she didn't know it was JavaScript I mean thinking back how could she she thought it was python so she definitely didn't get that missing semicolon huh now how would I tweak this so she answered the question correctly so let me remove that response and oh I know I can say you are a JavaScript AI coding assistant uh your job is to help people with programming questions let's see if we can get some better results this way oh there we go so I added the variable name and also added the missing semicolon okay good job a but um oh you know what I just added this JavaScript keyword so it probably isn't going to do that great for Python and HTML anymore so let me remove that and uh now it's not going to be great at JavaScript okay so already I'm noticing a couple things one whenever I say is I getting better or worse it's a little bit tricky because the question itself is vague now whenever your llm is getting better or worse it typically is getting better in some areas and worse in other areas now aside from that ambiguity I am also noticing that this entire method of testing it isn't very scientific I mean I'm pretty much just asking random questions and I'm trying to keep track in my head of is this better or is this worse nothing's quantifiable everything's subjective and it's just uh it feels messy off the bat so let me go ahead and show you a much better way to approach these kind of problems now let me go ahead and click on evaluate just so we have a nice clean page to look at and now let's go ahead and talk through some of the flaws we had in our logic so whenever the video first started I said that the goal was for us to improve ya or to make ya better but already that thought is a little bit subjective because what do I mean by better do I mean that ya is going to be giving shorter responses or do I mean that by better ya feels more like I'm talking to a human or do I mean that it's just giving more accurate responses just answering uh the questions correctly so I need to Define better now whenever we are developing l applications what we do is we need to assess a certain Behavior at a time and that means instead of using uh vague terms like better we need to give it a more clear um indicator of what we're assessing so instead of me saying I want to make a better instead what we can do in this example is we can say I want to make a better at answering common programming questions fixing easy programming bugs that's a lot more more quantifiable something that we can actually test now once we have that clarified the next thing we have to figure out is how can we test this and believe it or not the way that we test this for a llm is very similar to how we would test if we were a teacher and we were testing if a student is getting smarter or Dumber uh it sounds kind of mean but how we would do that is we would basically give them a test and then over time we can see if their test scores were going up or down so we're going to be doing the same thing with ya we're going to be creating a test and seeing if she is doing better or worse on it so now let me go ahead and pop open a spreadsheet and I'll show you guys the structure of how we're going to set up this test so what we're going to be doing is we are first going to be creating some type of question and we're going to ask it something like we did before let me just pop in a quick example here I'll say uh what's wrong with this and then we're going to be giving it some code let me just throw a little issue here and then in the next column we'll just have the answer and I'll say uh bad closing tag so I'm sure you guys can see where this is going um in this test we are actually going to have a lot of question and answers probably like 10 of them and the question what we're going to be doing with that is we are going to be throwing that into this prompt template and we're going to be sending it to the llm now when we get our response back actually let me say llm response is going to give us some type of answer right here and that when we get that answer back as the teacher we can just go ahead and see if it's right or if it's the wrong answer if it's the right answer oops if it's the right answer we'll just go ahead and give it a score of one if it's the wrong answer we'll give it a score of zero and that's basically our test the more questions that a gets right the more points she gets the more points she gets the smarter she's becoming and it's pretty much it the same way that you would test the kid like I said you were going to test a the same way and really gamify the whole experience now before this video what I actually did is I went and saved some time and I created a bunch of these questions and answers again these are just very simple um programming questions they usually just have a typo or missing semicolon or something and again that's the behavior that we want to assess the ability to answer common or easy programming questions and I'll go through each of these one by one real quick there's just 10 of them it seems kind of uh tedious but we're going to be working with the same data set a lot so it's important you understand the questions because then the responses are going to make a lot more sense so this first thing is just the HTML question you got a bad closing tag this second one is just this JavaScript question where the semicolon is missing the third one is just printing out hello world but it's missing those ending quotation marks now this fourth one is react code however instead of the um rendering the hello component I just had a typo and I called it jello so this fifth one right here this is just some D Jango code just instead of model it says schodel so that's the issue with that this SQL code right here is using two equal signs when it should use one for this one it's just python code however it's missing the colon right here after the for Loop there should be a colon now this JavaScript function this console log is not supposed to be included in this function body and there's also the function body never closes so that's another issue um this is SQL code 2 but instead of order by there's a little typo it says order re bu and the last question that we're going to be testing is some CSS code we can see that in this color property there's supposed to be a semicolon here and it's just missing so either way just a very simple 10 question test and from here what I want to do is I just want to download it so I'm going to download it as a CSV and then I just want to upload it to prompt layer so back in prompt layer what I'm going to do is over in the evaluate tab if I click new batch run then what I like to do is I like to name my batch runs or my tests basically the same as my data set just because I think it's um I don't know easy to keep things organized that way so my data set is named programming bugs easy so here I'm just going to call it programming bugs easy and now to upload this data set I'm going to click this button right here and you are going to see that I already have a data set in here and that's because I just recorded this video however for some reason my software the video and the audio got like mismatched and uh anyways more of the story I had to rerecord it which I'm doing right now so I'll show you guys how to upload the data set that we just created if you click create data set then you could upload that CSV file just go ahead and select it here and then choose your CSV file hit open and for the data set name I'm just going to name it programming bugs easy data set and I'll say data set 2 because I already have a data set name that name create one and now I can select it and just say select data set and now last but not least I'm going to hit create Pipeline and you can see that we now in prompt layer have a view that's very similar to the spreadsheet however since it's in prompt layer now we get a lot of extra functionality so from here what I want to do next is I want to add that other column that's going to be responsible for making that llm request and it's going to be doing it using our prompt template so let me go ahead and add another column by clicking this add step button and I'm going to be making it a prompt template column and then I just need to give it a name I'm going to name the column llm response and then I need to choose what prompt template I'm going to be using and of course we only have one so very easy decision and I just always want to use the latest version so by version I'm going to select default and that's going to always use the latest version of The Prompt template and now the last thing that this is saying is hey in your prompt template you have a variable called question now whenever I'm going through which of these columns do you want me to plug in for that question and of course it's the question column so then we're just going to map that question column to the question variable and now I'm going to hit run step and what's going to happen is I believe we're going to get some errors and I'll show you uh why we we are getting them and how to fix them and all right so it says that there's error please select llm provider and engine none found so what does this mean well let's go ahead and open our template again and it is in here University I assistant and check it out so right now we already know that we want to make an llm request using this prompt which we're going to be passing in a question variable and we already hooked it up to say that this is the question that you're going to be using first and then use this one so on so forth looks like everything is good to go but what we never specified to promp layer is which llm provider are we using are we using open AI are we using anthropic are we using another one so on and so forth now there are two different ways that we can configure this the first way is we can configure on a per column basis so anytime we run this column if we click this edit button and we can set the model engine parameters here so if I just want to say yes I want to use open AI with this model I can go ahead and save that however I don't like doing it this way and instead what I like doing is I like saving the type of provider and model I'm going to be using directly on my prompt template so how do I do that well if I go ahead and edit this prompt template then right here where it says parameters I can go ahead and choose set parameters and then I'll just stick with this right now open AI uh GPT 3.5 turbo those look good now I'm going to go ahead and just update my template to make sure those parameters are set and now back in my report all I need to do is really just um rerun this and now hopefully if everything went good since I have my parameters tied to my prompt template now it knows to use open AI so now let's see whenever it makes this request so now you see we start to get some answers back and let's go ahead and take a quick peek all right error in your HTML code that looks good nothing technically wrong still think it's python code but that's all right all right so this preview is looking good so far I got everything configured how I want it the only thing that I want to do before I kick off the full run again right now it's only doing the first four while I get everything configure but when I click this it's going to do the full 10 before I do that I want to add one more column so we can actually score these answers so I'm going to add another column and since it's just going to be me looking at the answers in manually giv a score I'm going to choose human input and then I'm going to click next and then for this column I'm just going to say score and for the data type it's just going to be a number again zero for wrong one for right and I'll just say text box it's the easiest thing so I'm just going to save this and now I can go ahead and enter whatever number I want and there you go so this is all good right now and uh yeah from here just go ahead and hit run full batch and then that's going to go ahead and run your full test Suite so after a couple seconds you see that our full test is now populated and then ya is going to be figuring out these answers and I'll give her some time and it will be just a few seconds but I'll give her a few more all right now after a couple seconds you see that ya has now responded with all our answers and what I'm going to do now is pretty much just go through like I'm a teacher grading a test just look at her answers and if it's right give her a one if it's wrong give her a zero and I'll do about like two or three of these and then I'll pause the video so you don't have to watch me do each one so the first one closing tag okay and it also says the title tag is missing but I don't think it was missing the closing title tag no it wasn't was it okay so she got like uh I don't know that's just like uh uh kind of got it so I'll give her half a point for that um let's see what else uh in this one nothing wrong but if you want to follow convention you should add semicolons okay that one's right and let's see this last one quotation mark she got that one right so that's another one and yeah so I'm basic basically just going to go through the rest of these I'll pause the video now and at the end of it I'll let you know the final score all right so I just finished grading the test and once I added the last value here prom player updated the average score so we can see that a got an 85% out of 100 she missed this first one where she said that there was no closing title or head tag and in fact there were and also another rea one where she said to import something from somewhere but uh it actually was the correct import so either way we got a score of 85 not too bad but I have a feeling that we could do some prompt engineering and make this a little bit better so what I'm going to do is I'm going to go back to my template right here and I'm going to edit it and how about we do something like this um you are a e coding assistant you are the best programmer in the world and another thing that I want to change is before we were using 3.5 turbo let's go ahead and see if GPT 4 is any better so now I made some changes to this I'm going to go ahead and update my template uh I don't need a commit message now here what I want to do is since I updated my template and I want to taste it test it out on that same test that we built I'm go ahead and select this do you want to run an evaluation basically do you want to run a test and for this I'm going to select the one that we just made programming bugs easy so now I'm going to go ahead and confirm this and when I do you can see if I hover over my latest version the updates that we just made I have this little link to eval report and if I click that you can see that the updated test is now running well in fact it's the same test just with the updated prompt templ and that brings me to another point which is whenever you are evaluating a prompt template what you want to make sure of is you're using a consistent test and the reason for that is because in this experiment you already have one of the variables changing which is the prompt template itself now if you are also changing it by giving it new test questions every time your experiment isn't going to be very scientific you're basically testing too much data or in other words what's the reason why the system is either improving or not improving so now it looks like I V2 let's call her uh finish answering this test so now let me go ahead and score it and see if she did any better all right and check it out this time after our adjustments it does indeed look like a improved she now had a perfect score all right so off the bat it feels like like this process is indeed better than just using the playground and trying to keep track of everything in your head and not being too scientific however there's still some things in this process that feel a little bit tedious first of all me going through in scoring each of these one by one I mean honestly it's taken a little bit of time it's kind of annoying and imagine if I have a bigger data set with a 100 or a thousand cases and I have a bunch of different prompt templates that I want to test I mean I'm going to get carpal tunnel in no time having to do this so I need to figure out a better way where I can actually score these responses and in addition to that another thing that I was thinking is check this out so before I just go ahead and release this to the new boston.com and let people start typing in any answer or excuse me any question that they want I want to think about this from a user's perspective so they are going to be on my website and they're going to be watching a video and what are they going to be typing in a lot of the time they're going to be typing in questions like we just saw programming related questions so on and so forth but let's say that they're watching the video of me teaching how to make beer or how to build a gokart they could also ask questions like uh this who is the dude in this video and well it may seem obvious just you know detect who's in the video and see but from A's perspective whenever she is embedded here she has no idea that she's on a web page she has no idea that there's a video right here or anything like that so she's not going to be able to respond to that question just because she doesn't have the context necessary now another thing is if we just put ourselves in the user shoes again they are going to be talking to a chat bot so they may ask some questions which they think is a simple question like what is your name and of course we know her name is a I mean I already said that in the other video and you know here you can see it says a on the left but for her huh her response would be as an i AI I don't have a personal name and we know that's not really true either so it seems like yes we are getting better but there's still some Kinks that we have to work out and in the next videos we're going to be taking a look at how to solve those problems but for now thank you guys for watching don't forget to subscribe and I'll see you next time
