With timestamps:

00:00 - hey welcome back everybody this is Ian
00:02 - bringing you another video in this AI
00:04 - series with the New Boston so in the
00:06 - past videos we've talked a lot about
00:08 - open AI chat gbt 3 and a half turbo gbt
00:11 - 4 we've also discussed how to use prompt
00:14 - player in this video we're going to
00:15 - introduce a different llm from a company
00:17 - called anthropic or maybe it's anthropic
00:20 - I'm not sure in any event their llm or
00:23 - large language model is called Claude 2
00:26 - in fact their latest version is 2.1 in
00:28 - this video we're going to introduce you
00:30 - to CLA 2 with the Legacy completion
00:32 - syntax just so you're familiar with it
00:34 - in case you come across it in a code
00:35 - base or if you're already using it in
00:37 - your own codebase and then in the next
00:39 - video we'll show you how to migrate to
00:40 - the newer version of the llm clae 2.1
00:43 - using the newer API called the messages
00:46 - API so the implementation and usage of
00:49 - this API is really simple so simple in
00:52 - fact that we're going to wire it up into
00:54 - prompt ler right away so that we can of
00:56 - course keep track of all of our prompts
00:57 - and their responses so let's take a look
00:59 - at this this code let's see what's going
01:01 - on let's run it and then let's check
01:03 - prompt layer to make sure that the
01:04 - tracking is in order so the first thing
01:06 - to know is that you do have to have an
01:09 - API key for anthropic so you're going to
01:12 - go to their website sign up get API
01:14 - access generate that key and then you're
01:16 - going to export it and so the value that
01:18 - you're going to export it as is just
01:21 - going to be enthropy API key with
01:25 - underscores so anthropic uncore API
01:27 - uncore key so set that as an environment
01:29 - VAR able however you set up environment
01:31 - variables on your computer or in your
01:33 - project and point that to your secret
01:35 - key that you get from anthropic the next
01:38 - thing we're going to do is inside of our
01:39 - main.py file we're going to import
01:41 - prompt layer so then we're going to pull
01:43 - anthropic from prompt layer so we say
01:46 - anthropic is equal to prompt layer.
01:48 - anthropic now we're going to generate a
01:50 - client from anthropic do anthropic
01:52 - you'll notice that the second anthropic
01:54 - here has a capital A so make sure that
01:56 - you put that there and then we're going
01:58 - to invoke this so open close parentheses
02:00 - and the result of that the object that
02:02 - gets returned is going to be our client
02:04 - assigned to our client variable now we
02:06 - can do things like access the
02:08 - completions API through this SDK so
02:10 - we're going to say completion is equal
02:12 - to and then we're going to use that
02:14 - client and we're going to call its
02:15 - completions method or object which has a
02:18 - create method on it so client.
02:20 - completion. create and then it's just
02:22 - going to take a handful of arguments
02:24 - here the first argument is our prompt so
02:26 - you'll notice we use an F string here
02:28 - and we plug in something called enthropy
02:30 - human prompt and then at the end we have
02:32 - another string injected from enthropy a
02:35 - prompt I'll talk about what those are
02:37 - and why we need them here in a moment
02:39 - the next thing is just indicating which
02:40 - model we're going to use so in this
02:41 - video we're using
02:43 - cl-2 but in future videos we'll migrate
02:46 - to Claud
02:48 - -2.1 then of course Max tokens in this
02:51 - case they call it Max tokens to sample
02:53 - so you should be familiar with that
02:54 - concept from our open AI videos but
02:56 - essentially we just want to control how
02:58 - many tokens are used with this this
02:59 - request response
03:01 - cycle at the end here we're going to
03:03 - print the whole completion object that
03:05 - comes back and then we're going to drill
03:06 - down into it a little bit and print the
03:08 - completion. completion object that comes
03:10 - back from that so let's go ahead and
03:13 - save this file and run our code so I
03:17 - have my virtual environment activated
03:19 - for this AI playground project and I
03:22 - have of course all my dependencies
03:23 - installed we're ready to go to run
03:25 - python
03:26 - main.py uh first I want to CD into where
03:30 - are we over here anthropic examples and
03:33 - then the first folder here 01 basic so
03:35 - inside of here I've got main.py I'm
03:38 - going to run main.py with Python
03:42 - 3 so the prompt was the same one that we
03:46 - used way back in the beginning weeks and
03:47 - weeks ago whenever we introduced uh chat
03:50 - gbt or open AI gbt API and that was we
03:54 - just asked which NHL team plays in
03:56 - Pittsburgh so National Hockey League and
03:58 - the response that we get back here is
04:00 - you can see here's the completion object
04:03 - the entire thing it's got a bit of
04:05 - information here like the ID and then
04:07 - this completion string and then some
04:09 - other things in here like type stop log
04:12 - ID and so on after that we're actually
04:15 - logging the completion completion which
04:18 - is just the string the content uh that
04:21 - comes back so the answer here is the
04:23 - Pittsburg Penguins are the NHL team that
04:25 - plays in Pittsburgh and it actually goes
04:27 - on and says quite a bit more they play
04:29 - their home games at the PBG paints Arena
04:31 - they have won five Stanley Cup
04:33 - championships and so on and so forth we
04:35 - didn't actually ask it to give us that
04:37 - information so you can already see that
04:39 - it's kind of overcompensating and it's
04:42 - just going to be up to us to prompt it
04:43 - in such a way that makes it to where it
04:46 - doesn't give us too much information if
04:48 - that's not what we want so we'll show
04:50 - you how to do that in future videos with
04:51 - the system prompt but for now we're just
04:54 - going to focus on getting this
04:55 - functionality working and making sure
04:57 - that it's being tracked over in prompt
04:58 - layer so let's head over to prompt layer
05:01 - and you can see at the top here I have
05:03 - this brand new history for Claude 2 and
05:06 - if I click on it you can see that the
05:08 - human prompt was which NHL team which
05:11 - NHL team plays in Pittsburgh and then
05:14 - the assistant response was the one that
05:15 - we just read you can see our Max tokens
05:18 - we can see how many tokens were used so
05:19 - it only used 79 and then you can see our
05:21 - cost and our latency here you can even
05:24 - see that it's using anthropic anthropic
05:27 - do completion. create so in the future
05:29 - when we migrate to the messaging API
05:31 - you'll see that it uses messages instead
05:33 - of completion also we can see that we're
05:35 - using CLA 2 model again that will change
05:38 - once we migrate to
05:39 - 2.1 so back over in our code the last
05:42 - thing I want to talk about before we get
05:43 - out of here is the prompting formatting
05:47 - so there is a validation that occurs
05:49 - when you send your completions request
05:51 - over to the anthropic API it's going to
05:53 - look at your prompt string and it's
05:54 - going to ensure that it's written a
05:56 - certain way so what it's looking for at
05:58 - the very beginning is either back slash
06:01 - in back slash in so the return statement
06:03 - double return statement and then human
06:06 - with a colon to indicate that what comes
06:08 - after that is the human prompt or
06:11 - likewise it's going to look for back
06:12 - slashin back slashin assistant all caps
06:15 - for human all caps for assistant and
06:17 - then a colon so what that'll look like
06:19 - is this it'll be back sln back
06:22 - sln and then human and then some prompt
06:26 - and then back slash in back slash in
06:31 - assistant and then some prompt and
06:33 - you're just going to alternate back and
06:35 - forth between human and assistant human
06:36 - and assistant well rather than having to
06:38 - type that out yourself and assuming that
06:41 - they might make changes to it in the
06:42 - future we actually pull that from the
06:45 - SDK itself so you see we have anthropic
06:47 - up here inside of this F string which
06:50 - just allows us to inject a variable or
06:52 - an expression into our string we have
06:55 - enthropy human prompt as a constant
06:58 - variable so that's actually going to
07:00 - give us that double back slash in and
07:02 - then human for the human prompt and
07:04 - likewise for the AI prompt there's back
07:06 - slashin backin assistant colon stored
07:09 - inside of enthropy doore prompt so if
07:12 - you don't do this correctly then you'll
07:14 - get an error back because it is passing
07:16 - through a layer of validation it's
07:18 - looking to ensure that you do it
07:19 - correctly now if you have you know a
07:22 - missing back slash in or maybe you have
07:24 - a space in there it will try to correct
07:27 - it for you but you definitely should not
07:28 - rely on that because it could change in
07:30 - the future and then you could find that
07:31 - your code is breaking the easiest
07:33 - solution is just to remember to pull
07:35 - those values from these constant
07:37 - variables that they have available
07:39 - within the client and to make sure that
07:42 - you put them at the beginning and at the
07:45 - end of your prompt so that's the
07:47 - formatting for sending your prompts and
07:49 - then of course you're just going to send
07:51 - this through get your response back
07:54 - parse through the response print out the
07:56 - actual completion which is the text that
07:58 - you're looking for and do whatever you
08:00 - want with it you can create an ongoing
08:02 - conversation if you want to it's a
08:03 - little bit easier to do when you're
08:04 - using the messages API versus the
08:06 - completions API so we'll talk about that
08:08 - in the next video but for now what we've
08:10 - talked about is how to get up and
08:12 - running really quickly with anthropics
08:13 - CLA 2 API and to link it into prompt
08:16 - layer so that we can track all our
08:18 - prompts and see whatever data we need
08:20 - about those prompts thanks a lot for
08:22 - joining us in this video can't wait to
08:23 - see you all in the next one until then
08:25 - peace

Cleaned transcript:

hey welcome back everybody this is Ian bringing you another video in this AI series with the New Boston so in the past videos we've talked a lot about open AI chat gbt 3 and a half turbo gbt 4 we've also discussed how to use prompt player in this video we're going to introduce a different llm from a company called anthropic or maybe it's anthropic I'm not sure in any event their llm or large language model is called Claude 2 in fact their latest version is 2.1 in this video we're going to introduce you to CLA 2 with the Legacy completion syntax just so you're familiar with it in case you come across it in a code base or if you're already using it in your own codebase and then in the next video we'll show you how to migrate to the newer version of the llm clae 2.1 using the newer API called the messages API so the implementation and usage of this API is really simple so simple in fact that we're going to wire it up into prompt ler right away so that we can of course keep track of all of our prompts and their responses so let's take a look at this this code let's see what's going on let's run it and then let's check prompt layer to make sure that the tracking is in order so the first thing to know is that you do have to have an API key for anthropic so you're going to go to their website sign up get API access generate that key and then you're going to export it and so the value that you're going to export it as is just going to be enthropy API key with underscores so anthropic uncore API uncore key so set that as an environment VAR able however you set up environment variables on your computer or in your project and point that to your secret key that you get from anthropic the next thing we're going to do is inside of our main.py file we're going to import prompt layer so then we're going to pull anthropic from prompt layer so we say anthropic is equal to prompt layer. anthropic now we're going to generate a client from anthropic do anthropic you'll notice that the second anthropic here has a capital A so make sure that you put that there and then we're going to invoke this so open close parentheses and the result of that the object that gets returned is going to be our client assigned to our client variable now we can do things like access the completions API through this SDK so we're going to say completion is equal to and then we're going to use that client and we're going to call its completions method or object which has a create method on it so client. completion. create and then it's just going to take a handful of arguments here the first argument is our prompt so you'll notice we use an F string here and we plug in something called enthropy human prompt and then at the end we have another string injected from enthropy a prompt I'll talk about what those are and why we need them here in a moment the next thing is just indicating which model we're going to use so in this video we're using cl2 but in future videos we'll migrate to Claud 2.1 then of course Max tokens in this case they call it Max tokens to sample so you should be familiar with that concept from our open AI videos but essentially we just want to control how many tokens are used with this this request response cycle at the end here we're going to print the whole completion object that comes back and then we're going to drill down into it a little bit and print the completion. completion object that comes back from that so let's go ahead and save this file and run our code so I have my virtual environment activated for this AI playground project and I have of course all my dependencies installed we're ready to go to run python main.py uh first I want to CD into where are we over here anthropic examples and then the first folder here 01 basic so inside of here I've got main.py I'm going to run main.py with Python 3 so the prompt was the same one that we used way back in the beginning weeks and weeks ago whenever we introduced uh chat gbt or open AI gbt API and that was we just asked which NHL team plays in Pittsburgh so National Hockey League and the response that we get back here is you can see here's the completion object the entire thing it's got a bit of information here like the ID and then this completion string and then some other things in here like type stop log ID and so on after that we're actually logging the completion completion which is just the string the content uh that comes back so the answer here is the Pittsburg Penguins are the NHL team that plays in Pittsburgh and it actually goes on and says quite a bit more they play their home games at the PBG paints Arena they have won five Stanley Cup championships and so on and so forth we didn't actually ask it to give us that information so you can already see that it's kind of overcompensating and it's just going to be up to us to prompt it in such a way that makes it to where it doesn't give us too much information if that's not what we want so we'll show you how to do that in future videos with the system prompt but for now we're just going to focus on getting this functionality working and making sure that it's being tracked over in prompt layer so let's head over to prompt layer and you can see at the top here I have this brand new history for Claude 2 and if I click on it you can see that the human prompt was which NHL team which NHL team plays in Pittsburgh and then the assistant response was the one that we just read you can see our Max tokens we can see how many tokens were used so it only used 79 and then you can see our cost and our latency here you can even see that it's using anthropic anthropic do completion. create so in the future when we migrate to the messaging API you'll see that it uses messages instead of completion also we can see that we're using CLA 2 model again that will change once we migrate to 2.1 so back over in our code the last thing I want to talk about before we get out of here is the prompting formatting so there is a validation that occurs when you send your completions request over to the anthropic API it's going to look at your prompt string and it's going to ensure that it's written a certain way so what it's looking for at the very beginning is either back slash in back slash in so the return statement double return statement and then human with a colon to indicate that what comes after that is the human prompt or likewise it's going to look for back slashin back slashin assistant all caps for human all caps for assistant and then a colon so what that'll look like is this it'll be back sln back sln and then human and then some prompt and then back slash in back slash in assistant and then some prompt and you're just going to alternate back and forth between human and assistant human and assistant well rather than having to type that out yourself and assuming that they might make changes to it in the future we actually pull that from the SDK itself so you see we have anthropic up here inside of this F string which just allows us to inject a variable or an expression into our string we have enthropy human prompt as a constant variable so that's actually going to give us that double back slash in and then human for the human prompt and likewise for the AI prompt there's back slashin backin assistant colon stored inside of enthropy doore prompt so if you don't do this correctly then you'll get an error back because it is passing through a layer of validation it's looking to ensure that you do it correctly now if you have you know a missing back slash in or maybe you have a space in there it will try to correct it for you but you definitely should not rely on that because it could change in the future and then you could find that your code is breaking the easiest solution is just to remember to pull those values from these constant variables that they have available within the client and to make sure that you put them at the beginning and at the end of your prompt so that's the formatting for sending your prompts and then of course you're just going to send this through get your response back parse through the response print out the actual completion which is the text that you're looking for and do whatever you want with it you can create an ongoing conversation if you want to it's a little bit easier to do when you're using the messages API versus the completions API so we'll talk about that in the next video but for now what we've talked about is how to get up and running really quickly with anthropics CLA 2 API and to link it into prompt layer so that we can track all our prompts and see whatever data we need about those prompts thanks a lot for joining us in this video can't wait to see you all in the next one until then peace
