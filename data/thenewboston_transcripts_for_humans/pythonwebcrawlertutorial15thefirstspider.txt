With timestamps:

00:00 - already houses welcome back in in this
00:02 - video we only have one more file to
00:05 - write for this project and this is
00:07 - probably going to be two more videos
00:08 - just because we have like three or four
00:11 - functions but basically we already have
00:13 - everything created that we need the
00:15 - spider the link finder all of that good
00:17 - stuff and all we need to do now is we
00:20 - need to call the spider over and over
00:23 - and over again so we can just make this
00:25 - real quick and just make a wild loop and
00:27 - say hey so long as you have links just
00:29 - keep calling it but like I said the
00:32 - reason this is going to take a little
00:34 - bit longer is because we're gonna make
00:35 - this program multi-threaded so instead
00:39 - of just having one function or one
00:41 - program that goes and does the same
00:44 - thing over and over again we're gonna
00:45 - have multiple threads so it runs like
00:47 - eight or sixteen or if you have an
00:50 - awesome computer like 64 times as fast
00:52 - and all of these programs all these
00:54 - spiders are going to work simultaneously
00:56 - at the same time so if you never use the
01:00 - threads before in Python I'll keep it
01:01 - real simple for you the first thing you
01:03 - need to do is import threading and from
01:05 - cue import queue alright so before we
01:11 - continue I'll explain how they work and
01:13 - if you guys are like wow I looked at
01:15 - some stuff about threading before it
01:16 - looked really complicated trust me guys
01:19 - it's not it's really simple a thread
01:21 - well let's break it down like this you
01:24 - know that whenever you own a factory you
01:27 - have a bunch of workers and they each
01:29 - are doing their own job it's the most
01:31 - simple thing to understand whenever you
01:33 - have a thread each thread is like a
01:36 - worker and the queue is just a job so
01:39 - it's pretty much the exact same thing
01:41 - just different terminology so we're
01:43 - basically gonna have a bunch of workers
01:44 - or spiders or threads whatever you want
01:47 - to call them and we give them each a job
01:49 - and then they do it now again unlike
01:52 - regular program they can all work at the
01:54 - same time which is awesome so that's all
01:56 - we're doing right there so now let's
01:58 - just go ahead and pour all the stuff
01:59 - that we made so from spider import
02:03 - spider from domain dominate hole from
02:10 - domain import all and that's what we did
02:15 - in last video and of course from general
02:18 - and pour all of those all right so now
02:23 - I'm just gonna make a couple constants
02:25 - and if you were running this is a
02:27 - command-line tool then you can just have
02:29 - the user fill this in and parse the
02:31 - arguments but for right now I'm just
02:34 - gonna make simple variables so project
02:37 - name it's my mom texted me and I'll say
02:40 - uh
02:40 - the new Boston so again if you decide to
02:43 - make this a GUI or a command-line tool
02:45 - you're gonna want the user to fill this
02:47 - in but just for this little demo
02:49 - whenever I'm teaching it I'm just gonna
02:51 - tuck myself and by the way you can't
02:54 - really have um there is no such thing as
02:57 - constants in Python but you can just
03:00 - kind of make them yourself so since
03:03 - there is no like constant keyword the
03:05 - convention is whenever your program in
03:07 - Python if you ever have a variable
03:08 - that's not going to change throughout
03:10 - the entire running of your program you
03:13 - just write it all in caps and that way
03:15 - later on whenever another developer is
03:18 - working on a program they don't set this
03:20 - equal to a new value so again there are
03:22 - no built-in constants but that's the
03:24 - Playtone convention so for the home page
03:27 - this is also constant and that is gonna
03:32 - be this so that's my project that's my
03:39 - home page now for the domain name
03:43 - remember what we did is we already made
03:46 - a function called get domain name and we
03:49 - just need to pass it in the URL of the
03:51 - home page so that's gonna return this
03:54 - right here and the reason I just don't
03:56 - want to pop this right in here like this
03:59 - is because like I said later on if we
04:03 - ever decide to you know use this in a
04:06 - GUI kind of way or maybe we're making a
04:09 - consumer application we want the user to
04:11 - type in the least amount of information
04:13 - possible send hey just give me your
04:15 - homepage URL and we're gonna take care
04:17 - of all the hard stuff for you so it just
04:18 - makes it easier for the end user so the
04:21 - last thing I'm going to need
04:23 - are well not the last thing we got a
04:25 - couple more things we need to do is I'm
04:26 - gonna have a constant to that waiting
04:28 - list and also that crowd file because
04:30 - those file paths they never change and
04:33 - I'm also going to use those variables a
04:35 - few times later on so I'm gonna write
04:39 - cue file and again all this is equal to
04:42 - is the project name plus Q dot txt and I
04:51 - need to add that forward slash and I
04:52 - don't need to copy it so kraut file
04:57 - equals project name plus Garage dot text
05:05 - actually my mom is texting me because my
05:08 - sister's in school right now high school
05:09 - and she applied it a bunch of colleges
05:11 - and the one college that she really
05:15 - wants to go to it was the last letter
05:18 - that she got and it actually came in the
05:20 - mail but my sister doesn't know that
05:22 - it's in the mail yet she apply it like
05:24 - eight colleges she got into all of them
05:26 - but this one which is the eighth one um
05:30 - she doesn't know if she got in or not in
05:32 - so my sister doesn't even know the
05:35 - letter came here so later on I'll let
05:37 - you know if she got in or not it's
05:39 - pretty cool I'm pretty excited for it
05:40 - alright now the last thing we're going
05:42 - to do is make a variable called number
05:44 - of threads and the number you set this
05:49 - equal to is actually dependent on your
05:50 - operating system how many threads is
05:52 - capable of handling for this I'm just
05:54 - going to write eight but again I don't
05:56 - just want to say use eight or use this
05:59 - number this number it's operating
06:01 - systems specific and there's bunch of
06:03 - other factors as well but I know my
06:06 - operating system can handle eight so
06:08 - that's what I'm writing that alright so
06:10 - the last two things we're gonna do is
06:13 - I'm gonna make a variable called
06:15 - Q now this Q I'm talking about right
06:19 - here again the job of each spider is
06:22 - just to get a link crawl it and whatever
06:26 - page we gave it gather all the links on
06:28 - page we know that already so this Q is
06:30 - actually the thread Q so it's not the
06:33 - waiting list of links essentially
06:35 - they're the same thing
06:36 - again the thread queue or the job is
06:40 - what I'm going to refer to right here so
06:44 - what I could do is I can rename this
06:45 - thread queue and it may make it a little
06:48 - bit easier to understand but eventually
06:50 - in like two seconds you guys are going
06:52 - to see what's going on and the reason
06:53 - I'm not naming it thread queue is
06:54 - because this name is Q so that's my
06:58 - reasoning for that now the last line I
07:01 - write before you start writing any
07:03 - functions is this remember when our
07:06 - program first starts out we can't just
07:09 - boot right up into multi-threaded
07:11 - because the first thing it needs to do
07:13 - is it needs to curl this home page and
07:16 - gather all of the links on this home
07:18 - page right here so what we're going to
07:21 - do is that first spider we make we don't
07:24 - need to stick inside any thread we just
07:26 - call it from a normal function now
07:29 - whenever you call a spider we just pass
07:31 - in three parameters the project name the
07:33 - home page and the domain name so that
07:38 - actually I can go ahead and run this
07:40 - right now if I want let me go ahead and
07:42 - run this and check it out so what we did
07:45 - with this very first spider is whenever
07:49 - this first booted up it created I'll
07:51 - show you guys it first created that
07:54 - project directory so that's why we see
07:56 - the new Boston right here and then
07:58 - another thing it did is it created those
08:00 - kraut files in queue so what's in the
08:03 - crowd file it already crawled the home
08:06 - page what's in the waiting list all of
08:08 - the links on my home page and this was
08:11 - all the links that it discovered right
08:13 - here and now that we had that spider
08:16 - taken care of
08:17 - this was the only special one so now
08:19 - what we can do is we can go ahead and
08:21 - make a bunch of spiders and they're each
08:23 - going to run simultaneously and what job
08:27 - do they have well each of these links is
08:29 - essentially what we want the spiders to
08:33 - curl so now that we got the core
08:36 - functionality in that main base spider
08:38 - taken care of we can go ahead and start
08:41 - making this program a lot faster and
08:44 - optimize it it's going to be sweet I'll
08:46 - see you guys next time

Cleaned transcript:

already houses welcome back in in this video we only have one more file to write for this project and this is probably going to be two more videos just because we have like three or four functions but basically we already have everything created that we need the spider the link finder all of that good stuff and all we need to do now is we need to call the spider over and over and over again so we can just make this real quick and just make a wild loop and say hey so long as you have links just keep calling it but like I said the reason this is going to take a little bit longer is because we're gonna make this program multithreaded so instead of just having one function or one program that goes and does the same thing over and over again we're gonna have multiple threads so it runs like eight or sixteen or if you have an awesome computer like 64 times as fast and all of these programs all these spiders are going to work simultaneously at the same time so if you never use the threads before in Python I'll keep it real simple for you the first thing you need to do is import threading and from cue import queue alright so before we continue I'll explain how they work and if you guys are like wow I looked at some stuff about threading before it looked really complicated trust me guys it's not it's really simple a thread well let's break it down like this you know that whenever you own a factory you have a bunch of workers and they each are doing their own job it's the most simple thing to understand whenever you have a thread each thread is like a worker and the queue is just a job so it's pretty much the exact same thing just different terminology so we're basically gonna have a bunch of workers or spiders or threads whatever you want to call them and we give them each a job and then they do it now again unlike regular program they can all work at the same time which is awesome so that's all we're doing right there so now let's just go ahead and pour all the stuff that we made so from spider import spider from domain dominate hole from domain import all and that's what we did in last video and of course from general and pour all of those all right so now I'm just gonna make a couple constants and if you were running this is a commandline tool then you can just have the user fill this in and parse the arguments but for right now I'm just gonna make simple variables so project name it's my mom texted me and I'll say uh the new Boston so again if you decide to make this a GUI or a commandline tool you're gonna want the user to fill this in but just for this little demo whenever I'm teaching it I'm just gonna tuck myself and by the way you can't really have um there is no such thing as constants in Python but you can just kind of make them yourself so since there is no like constant keyword the convention is whenever your program in Python if you ever have a variable that's not going to change throughout the entire running of your program you just write it all in caps and that way later on whenever another developer is working on a program they don't set this equal to a new value so again there are no builtin constants but that's the Playtone convention so for the home page this is also constant and that is gonna be this so that's my project that's my home page now for the domain name remember what we did is we already made a function called get domain name and we just need to pass it in the URL of the home page so that's gonna return this right here and the reason I just don't want to pop this right in here like this is because like I said later on if we ever decide to you know use this in a GUI kind of way or maybe we're making a consumer application we want the user to type in the least amount of information possible send hey just give me your homepage URL and we're gonna take care of all the hard stuff for you so it just makes it easier for the end user so the last thing I'm going to need are well not the last thing we got a couple more things we need to do is I'm gonna have a constant to that waiting list and also that crowd file because those file paths they never change and I'm also going to use those variables a few times later on so I'm gonna write cue file and again all this is equal to is the project name plus Q dot txt and I need to add that forward slash and I don't need to copy it so kraut file equals project name plus Garage dot text actually my mom is texting me because my sister's in school right now high school and she applied it a bunch of colleges and the one college that she really wants to go to it was the last letter that she got and it actually came in the mail but my sister doesn't know that it's in the mail yet she apply it like eight colleges she got into all of them but this one which is the eighth one um she doesn't know if she got in or not in so my sister doesn't even know the letter came here so later on I'll let you know if she got in or not it's pretty cool I'm pretty excited for it alright now the last thing we're going to do is make a variable called number of threads and the number you set this equal to is actually dependent on your operating system how many threads is capable of handling for this I'm just going to write eight but again I don't just want to say use eight or use this number this number it's operating systems specific and there's bunch of other factors as well but I know my operating system can handle eight so that's what I'm writing that alright so the last two things we're gonna do is I'm gonna make a variable called Q now this Q I'm talking about right here again the job of each spider is just to get a link crawl it and whatever page we gave it gather all the links on page we know that already so this Q is actually the thread Q so it's not the waiting list of links essentially they're the same thing again the thread queue or the job is what I'm going to refer to right here so what I could do is I can rename this thread queue and it may make it a little bit easier to understand but eventually in like two seconds you guys are going to see what's going on and the reason I'm not naming it thread queue is because this name is Q so that's my reasoning for that now the last line I write before you start writing any functions is this remember when our program first starts out we can't just boot right up into multithreaded because the first thing it needs to do is it needs to curl this home page and gather all of the links on this home page right here so what we're going to do is that first spider we make we don't need to stick inside any thread we just call it from a normal function now whenever you call a spider we just pass in three parameters the project name the home page and the domain name so that actually I can go ahead and run this right now if I want let me go ahead and run this and check it out so what we did with this very first spider is whenever this first booted up it created I'll show you guys it first created that project directory so that's why we see the new Boston right here and then another thing it did is it created those kraut files in queue so what's in the crowd file it already crawled the home page what's in the waiting list all of the links on my home page and this was all the links that it discovered right here and now that we had that spider taken care of this was the only special one so now what we can do is we can go ahead and make a bunch of spiders and they're each going to run simultaneously and what job do they have well each of these links is essentially what we want the spiders to curl so now that we got the core functionality in that main base spider taken care of we can go ahead and start making this program a lot faster and optimize it it's going to be sweet I'll see you guys next time
