With timestamps:

00:00 - are already made so now that we
00:01 - understand the concept of how these
00:03 - spiders are going to be created let's go
00:05 - ahead and start creating them so right
00:08 - now we don't have a whole lot going on
00:10 - so whenever we make an instance of this
00:12 - class whenever we make a spider boom so
00:16 - let's take a look at our code all right
00:19 - so right now our spider is going to be
00:20 - created and he's going to be sitting
00:22 - there like um am I supposed to be doing
00:25 - something here I guess that's how the
00:27 - spider is going to sound so let's go
00:30 - ahead and give him some information and
00:32 - of course some things to do so of course
00:34 - for every spider the user needs to give
00:36 - us some information so it knows exactly
00:38 - what pages are supposed to be curling
00:40 - what project it's working on so on and
00:42 - so forth so we're going to allow the
00:43 - user to pass in the name of the project
00:46 - such as the new Boston also the base URL
00:49 - and this is just going to be the home
00:51 - page URL so the first spider knows where
00:53 - to start crawling and also the domain
00:56 - name so we can use some cool domain name
01:01 - functions to ensure that we're
01:02 - connecting to a valid web page without
01:04 - generating a bunch of errors so now what
01:07 - I want to do is now that we have some
01:09 - information that the user passed in we
01:11 - want to go ahead and set these to the
01:13 - class variables and that's going to
01:15 - ensure that all spiders are looking at
01:17 - the same information so that way all
01:20 - spiders are crawling the same site which
01:22 - is a good thing all right so for the
01:25 - class variable just set equal to
01:27 - whatever the user passed in for the
01:29 - project name and we're just going to do
01:32 - this for base URL base URL and spider
01:38 - domain name all right so again all
01:42 - spiders are going to have this shared
01:45 - information it's never going to change
01:46 - no matter if you have a hundred spiders
01:48 - or a thousand whatever now another thing
01:51 - I want to do is I want to set the file
01:52 - path for the cue file in the crowd file
01:55 - and again that's just so I don't have to
01:57 - type in the full path every time just
01:59 - going to save me a little bit of time so
02:02 - the cue file is equal to the project
02:06 - name
02:08 - so something like the New Boston plus Q
02:14 - file Q text and I'm Way too lazy to type
02:19 - all that again
02:20 - so I'm just going to say the file path
02:23 - for the crowd file is equal to crowd by
02:28 - the way I'm like I suddenly got a stuffy
02:30 - nose like oh no just happened today I
02:33 - was sleeping earlier and once I can I
02:36 - don't know every half an hour so one of
02:38 - my nostrils just starts running I'm like
02:40 - what the heck body come on so stupid
02:44 - nose alright so that had nothing to do
02:46 - with Python Python so let's get back on
02:48 - track here so once we pretty much set
02:52 - the information for you know whatever
02:53 - the user typed in all these spiders have
02:56 - a general idea of what they're supposed
02:57 - to be doing we now need to essentially
03:00 - create that project directory and create
03:03 - those two files those two text files now
03:06 - we can go ahead and just type them right
03:08 - in there but I actually want to stick
03:10 - this in a new method and it's just going
03:13 - to help it um you know my code look a
03:15 - little bit cleaner so I'm going to call
03:17 - the method boo so that's what we're
03:19 - going to be doing in there and also
03:21 - we're going to make another method later
03:24 - on called crawl page and again all this
03:28 - is going to do is it's going to connect
03:29 - a some web page I like this one and it's
03:34 - essentially going to you know do exactly
03:35 - we've been talking about this whole time
03:37 - crawl it gather of links whatever now
03:39 - check out one thing that I need to
03:42 - mention so the very first time we create
03:47 - a spider it's going to have only one
03:50 - page in the waiting list and that is the
03:52 - home page of the website so we can't
03:55 - really make a bunch of spiders right off
03:57 - the bat because it'll be pointless it
04:00 - only has one single page now once that
04:03 - spider let me where that is my there it
04:06 - is alright so once the spider gathers a
04:08 - bunch of links let's say gathers like 40
04:13 - links from this home page that's when we
04:15 - can go ahead and make our program
04:17 - multi-threaded and we're like hey
04:19 - spiders go do your thing so
04:21 - that's how it's going to work now
04:23 - whenever I crawl a page what I'm going
04:26 - to do is in the command prompt just so
04:29 - the user has something look at while
04:31 - this program is running is I'm going to
04:33 - say the name of the spider and this is
04:35 - going to be the name of the thread like
04:36 - um thread one through a to three so
04:40 - we'll save the name and what page it's
04:43 - currently crawling and that way the user
04:44 - just knows something's going on so again
04:46 - the first spider it doesn't run on a
04:50 - thread so we'll just go ahead and since
04:53 - this function is going to need a name we
04:55 - can either just say first spider um but
04:58 - I'm just going to say like a let me go
05:01 - ahead and say first later all right
05:04 - that looks pretty good alright now
05:06 - another thing that we're going to throw
05:08 - in here is of course whenever we call
05:10 - this function we need to give a page to
05:12 - crawl so what page is the first spider
05:17 - going to curl holy raviolis this base
05:22 - URL URL which is the home page and again
05:26 - this is only going to happen once
05:29 - because later on after it's done
05:32 - crawling it it's going to be in that
05:33 - crowd file so all the rest of the
05:35 - spiders are just going to ignore it
05:37 - simple enough sweet

Cleaned transcript:

are already made so now that we understand the concept of how these spiders are going to be created let's go ahead and start creating them so right now we don't have a whole lot going on so whenever we make an instance of this class whenever we make a spider boom so let's take a look at our code all right so right now our spider is going to be created and he's going to be sitting there like um am I supposed to be doing something here I guess that's how the spider is going to sound so let's go ahead and give him some information and of course some things to do so of course for every spider the user needs to give us some information so it knows exactly what pages are supposed to be curling what project it's working on so on and so forth so we're going to allow the user to pass in the name of the project such as the new Boston also the base URL and this is just going to be the home page URL so the first spider knows where to start crawling and also the domain name so we can use some cool domain name functions to ensure that we're connecting to a valid web page without generating a bunch of errors so now what I want to do is now that we have some information that the user passed in we want to go ahead and set these to the class variables and that's going to ensure that all spiders are looking at the same information so that way all spiders are crawling the same site which is a good thing all right so for the class variable just set equal to whatever the user passed in for the project name and we're just going to do this for base URL base URL and spider domain name all right so again all spiders are going to have this shared information it's never going to change no matter if you have a hundred spiders or a thousand whatever now another thing I want to do is I want to set the file path for the cue file in the crowd file and again that's just so I don't have to type in the full path every time just going to save me a little bit of time so the cue file is equal to the project name so something like the New Boston plus Q file Q text and I'm Way too lazy to type all that again so I'm just going to say the file path for the crowd file is equal to crowd by the way I'm like I suddenly got a stuffy nose like oh no just happened today I was sleeping earlier and once I can I don't know every half an hour so one of my nostrils just starts running I'm like what the heck body come on so stupid nose alright so that had nothing to do with Python Python so let's get back on track here so once we pretty much set the information for you know whatever the user typed in all these spiders have a general idea of what they're supposed to be doing we now need to essentially create that project directory and create those two files those two text files now we can go ahead and just type them right in there but I actually want to stick this in a new method and it's just going to help it um you know my code look a little bit cleaner so I'm going to call the method boo so that's what we're going to be doing in there and also we're going to make another method later on called crawl page and again all this is going to do is it's going to connect a some web page I like this one and it's essentially going to you know do exactly we've been talking about this whole time crawl it gather of links whatever now check out one thing that I need to mention so the very first time we create a spider it's going to have only one page in the waiting list and that is the home page of the website so we can't really make a bunch of spiders right off the bat because it'll be pointless it only has one single page now once that spider let me where that is my there it is alright so once the spider gathers a bunch of links let's say gathers like 40 links from this home page that's when we can go ahead and make our program multithreaded and we're like hey spiders go do your thing so that's how it's going to work now whenever I crawl a page what I'm going to do is in the command prompt just so the user has something look at while this program is running is I'm going to say the name of the spider and this is going to be the name of the thread like um thread one through a to three so we'll save the name and what page it's currently crawling and that way the user just knows something's going on so again the first spider it doesn't run on a thread so we'll just go ahead and since this function is going to need a name we can either just say first spider um but I'm just going to say like a let me go ahead and say first later all right that looks pretty good alright now another thing that we're going to throw in here is of course whenever we call this function we need to give a page to crawl so what page is the first spider going to curl holy raviolis this base URL URL which is the home page and again this is only going to happen once because later on after it's done crawling it it's going to be in that crowd file so all the rest of the spiders are just going to ignore it simple enough sweet
