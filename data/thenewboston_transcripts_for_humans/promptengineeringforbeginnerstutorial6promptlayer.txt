With timestamps:

00:00 - hey welcome back everyone this is Ian
00:02 - bringing you the sixth video in this AI
00:04 - series with the New Boston in this video
00:07 - I'm going to show you this platform
00:09 - called prompt layer promp layer is this
00:11 - really neat tool that has an API that we
00:13 - can use on top of open AI to do a lot of
00:16 - really cool things in particular we can
00:19 - track the history of all of our prompts
00:21 - that we're using we can rate those
00:22 - prompts we can add metadata to those
00:24 - prompts there's all kinds of cool things
00:26 - so we're going to go over that in this
00:27 - video and getting set up with it is
00:30 - super simple so let's go ahead and dive
00:31 - right into it this is the website right
00:33 - here prompt layer.com it'll be linked in
00:35 - the description below you'll want to log
00:37 - in or sign up and then log in and create
00:40 - a API key so generating an API key is
00:43 - really easy once you're logged in the
00:46 - first thing you're going to see at the
00:47 - homepage on the guide tab is this create
00:50 - new API key button also the instructions
00:54 - for installing pip install prompt layer
00:56 - really straightforward so just two steps
00:58 - here and then a couple lines of code
00:59 - code whenever you get into your actual
01:01 - codee base so let's do a quick kind of a
01:04 - tour of what we have here in the
01:06 - dashboard and then we'll jump over to
01:08 - our code and I'll show you how it works
01:10 - once you're logged in you have your
01:12 - guide now you're not going to have
01:13 - anything on the left hand side here
01:15 - until you actually integrate this into
01:17 - your codebase but once you start using
01:20 - it with open AI every time you make a
01:22 - request over to the chat completion API
01:25 - it's going to log that request over here
01:27 - so this is going to create a history you
01:29 - can the history from the history tab
01:31 - it's essentially this right here but
01:33 - then as you click on each of these items
01:35 - in the history it'll show you everything
01:38 - about that particular prompt and so in
01:40 - this case we have our system prompt and
01:43 - then we have our initial user prompt and
01:45 - then we have our initial response from
01:47 - the assistant one really nice thing
01:49 - about prompt player is that it tells you
01:52 - exactly how much a certain request cost
01:56 - so in this case this request cost
01:58 - 0025 58 super cheap which is great but
02:02 - if you're over in the open AI dashboard
02:04 - and you're looking at your usage you can
02:06 - see how much overall you've spent you
02:08 - can see if it was GPT 3.5 or if it was
02:11 - GPT 4 but for each you know granular
02:14 - request you don't get to see how much it
02:16 - costs you get to see the tokens which
02:18 - you can see the total tokens here as
02:20 - well 143 tokens so you would have to
02:23 - calculate that on your own in this case
02:25 - it's calculated for you now there is a
02:27 - playground over in open AI of course
02:29 - we've seen that in previous video but
02:31 - over here with prompt layer when we open
02:33 - this in the playground it's actually
02:35 - going to pick up where we left off with
02:37 - this request to the API so we can toggle
02:40 - back and forth between the completions
02:42 - Legacy API and the new chat completions
02:44 - API we're going to leave it on the chat
02:46 - completions API because that's how we
02:48 - initially sent this request but you can
02:50 - see here's the system message you know
02:52 - you're a helpful coding assistant you
02:54 - only answer coding related questions
02:55 - nothing else if you asked a question
02:57 - that is not related to coding you reply
02:59 - politely but declined to answer and then
03:02 - the user's first question is is HTML
03:04 - hard to learn and then over there we saw
03:07 - the response you could run this again
03:09 - and when you run it you'll get the
03:11 - assistant response here in a second so
03:13 - we wait a second for it to come back to
03:14 - us and here's the assistant's answer
03:17 - HTML or hypertext markup language is
03:20 - blah blah blah so it's answering the
03:22 - user R question now you can add a new
03:24 - message here so if you want to respond
03:26 - to the assistant you can do that so
03:28 - essentially you're getting granular
03:30 - control of what you would be doing in
03:31 - your code but now you're doing it in the
03:33 - playground that way you don't have to
03:35 - keep going back and changing your code
03:36 - over and over again you can play with it
03:38 - here and you can kind of get a feel for
03:40 - which prompts are going to work best for
03:41 - whatever it is you're trying to write
03:43 - and then you can go Implement that into
03:45 - your code base so you'll notice they
03:47 - have functions down here this is brand
03:49 - new this is something that open AI does
03:51 - not currently have in their playground
03:54 - this is similar to what we showed you in
03:55 - the function call video where you can
03:57 - actually have a list of functions that
04:00 - the chat completions API can see and it
04:03 - can see what the parameters are and if
04:04 - it decides it wants to call one of those
04:06 - or if you explicitly tell it to call one
04:08 - of those functions it'll send you back
04:10 - the arguments for that function that it
04:12 - wants you to call including the name of
04:13 - the function so that's really neat you
04:15 - can integrate that here as well there's
04:17 - also some additional stuff that does not
04:19 - exist inside of open aai things like
04:21 - tags so you can see here it says no tags
04:24 - so you can actually add some tags and
04:26 - some meta information to each of your
04:28 - requests and it'll allow you to keep
04:30 - better tracking of the requests so for
04:33 - example let's say you're building a chat
04:35 - bot for like Discord and your members of
04:38 - your Discord server are able to ask
04:40 - questions of this bot and then those
04:42 - questions will be piped through the GPT
04:44 - 3.5 turbo or gbt 4 apis and the
04:48 - responses will come back to the users in
04:50 - the Discord interface that's really cool
04:53 - except if you're offering this as a free
04:54 - service then people might take advantage
04:56 - of it so you'd want to track that and
04:58 - you'd want to see you know who's asking
05:00 - what how often are they asking what are
05:02 - they asking uh you know what is the the
05:04 - Cadence and everything so you could add
05:07 - some information to your Bot to where
05:09 - every request through to the chat
05:11 - completions API has some information
05:14 - some meta information about the user
05:16 - from your Discord server that asked it
05:18 - like their their username or their user
05:20 - ID that way if somebody is abusing the
05:22 - system you can see okay this user with
05:24 - this user ID is the one that's doing it
05:26 - you know they're overusing it or they're
05:27 - asking questions that aren't meant to be
05:29 - asked or whatever the case may be you
05:31 - can go in there and you can identify
05:33 - those things you can of course bake that
05:35 - into the application that you're writing
05:37 - but having it right here in a dashboard
05:40 - is really nice because it's automatic
05:42 - and you can share it with other team
05:44 - members so if you're working on larger
05:45 - projects and you're doing some prompt
05:47 - engineering then you can have anybody on
05:49 - your team in your startup or company or
05:52 - just friends that you're working with be
05:53 - able to log in you can share this with
05:55 - them and they can look at the different
05:57 - prompts and the different logs and
05:58 - histories and the different tags
06:01 - everything that's available from this
06:02 - tool prompt layer it can be available to
06:05 - everyone that you're working with so
06:07 - there's a lot of really cool stuff this
06:08 - is just the playground and the history
06:10 - over here we have the registry so the
06:13 - registry is going to allow you to create
06:14 - templates where you can inject variables
06:17 - into like pre-made prompts and use those
06:19 - in your code so that's really neat and
06:22 - then we have analytics so this is going
06:24 - to tell you you know the total cost of
06:25 - all the different requests the average
06:27 - latency how many requests there were all
06:30 - kinds of things and it's going to break
06:31 - it down into like pie charts and graphs
06:33 - and things like that so this is really
06:35 - helpful to have in addition to whatever
06:37 - you have going on over in your default
06:39 - open aai dashboard so let's head over to
06:41 - our code and see how we can integrate
06:43 - this because it's super easy it's so
06:44 - easy you're going to be like I can't
06:46 - believe it it's only two or three lines
06:47 - of code and then you're up and running
06:49 - in no time if you remember the first
06:51 - thing we want to do was a pip install
06:53 - prompt layer that way we can import it
06:55 - up here so if you're inside of your
06:58 - terminal make make sure that you have
07:00 - your virtual environment activated and
07:02 - then just do a pip install prompt layer
07:05 - once you run that it'll get installed
07:07 - and of course you can freeze that to
07:09 - your requirements.txt file if you're
07:11 - keeping track of all the different
07:12 - dependencies for your project everything
07:15 - after that is the same you know we're
07:16 - importing OS so that we can get access
07:18 - to our environment variables in this
07:20 - case you're going to want to take the
07:22 - API key and Export it so in your
07:25 - dashboard you know when you generate the
07:27 - key from your dashboard you're going to
07:28 - want to copy that and Export it as an
07:30 - environment variable in this case we
07:32 - named it prompt layer aior key you can
07:35 - name it whatever you like just make sure
07:37 - that it matches what you have here after
07:39 - that you'll notice that we no longer
07:41 - import open AI right that's how we were
07:43 - doing it previously we had import open
07:45 - AI now we're saying openai the variable
07:48 - is equal to prompt layer. openai and
07:50 - that's all you have to do to layer
07:52 - prompt layer on top of open AI so that
07:56 - now it's going to track all the requests
07:58 - to open a we of course have to add the
08:01 - API key for open AI same thing we just
08:04 - take the openai variable say API key is
08:07 - equal to and then pull that API key from
08:09 - the environment variable that we have
08:10 - set so that's the same as it was in all
08:12 - the previous videos you're going to make
08:14 - your request to open AI the same way
08:16 - that you would regularly and then you
08:17 - just go over to prompt layer your
08:19 - dashboard you log in and you look at the
08:21 - log for all the requests that you made
08:23 - so this doesn't actually change response
08:26 - is equal to open AI chat completion.
08:28 - create tell it which model you want tell
08:30 - it your messages so you start with your
08:32 - system role give it some content start
08:34 - with your user role after that give it
08:36 - some content in this case you know who
08:38 - was the first president of United States
08:40 - set your temperature and your max tokens
08:42 - but here we have an additional argument
08:43 - to the chat completion call that is not
08:46 - normally included in our previous API
08:49 - call so now that we have prompt player
08:51 - on top of this we can add some
08:53 - additional things that prompt ler knows
08:54 - about so in this case we have plore tags
08:57 - is equal to a list of strings and these
09:00 - are tags comma separated inside this
09:02 - list that we can attach to the request
09:05 - that way if we want to be able to
09:07 - categorize and search through all of our
09:09 - previous requests we can do that very
09:11 - easily by using these tag names so in
09:13 - this case I'm asking a question about
09:15 - who the first president of the United
09:17 - States was I'm going to tag this with US
09:20 - presidents so we'll see how that works
09:21 - more in a second after we send this
09:24 - request we take our response variable
09:26 - and we print it to the console so
09:27 - nothing new there so we can run real
09:29 - quick and wait for it to come
09:32 - back sure enough there's our response so
09:35 - we have all the typical things that we
09:37 - normally have in our response the answer
09:39 - to our question from the assistant role
09:41 - is the first president of the United
09:42 - States was George Washington we see how
09:44 - many tokens Etc so now if we jump back
09:47 - over to our dashboard for prompt layer
09:50 - we can see the most recent request that
09:52 - was logged is this one right here so it
09:55 - cost us
09:57 - 00625 we can see that it was 38 tokens
10:00 - if we go back we can see sure enough
10:02 - total tokens was 38 so that's good and
10:05 - you'll notice somewhere in here where is
10:08 - it there it is US presidents is the tag
10:12 - so this is helpful because if we go up
10:13 - here and we search for US presidents
10:17 - then it's going to pull up the two
10:19 - different requests this one we just did
10:20 - and one that I did earlier while I was
10:22 - testing this out that have those tags so
10:24 - you can see how useful that can be for
10:26 - you know categorizing and being able to
10:29 - you know organize and look at the
10:30 - different uh logs for your various
10:32 - requests that you made in the past you
10:34 - can even export this stuff you can
10:36 - fine-tune your model which is really
10:38 - crazy we'll talk more about that in a
10:39 - future video in this video we just want
10:41 - to show you how simple it was to get up
10:44 - and running with prompt player on top of
10:45 - the openai API and it's as simple as a
10:48 - couple lines of code a pip install with
10:50 - your virtual environment activated you
10:52 - know generating a key exporting that key
10:54 - as an environment variable and logging
10:56 - into your dashboard and viewing the logs
10:58 - of all your requests to the open API so
11:01 - that's it for this video thanks a lot
11:03 - and we'll catch you in the next one

Cleaned transcript:

hey welcome back everyone this is Ian bringing you the sixth video in this AI series with the New Boston in this video I'm going to show you this platform called prompt layer promp layer is this really neat tool that has an API that we can use on top of open AI to do a lot of really cool things in particular we can track the history of all of our prompts that we're using we can rate those prompts we can add metadata to those prompts there's all kinds of cool things so we're going to go over that in this video and getting set up with it is super simple so let's go ahead and dive right into it this is the website right here prompt layer.com it'll be linked in the description below you'll want to log in or sign up and then log in and create a API key so generating an API key is really easy once you're logged in the first thing you're going to see at the homepage on the guide tab is this create new API key button also the instructions for installing pip install prompt layer really straightforward so just two steps here and then a couple lines of code code whenever you get into your actual codee base so let's do a quick kind of a tour of what we have here in the dashboard and then we'll jump over to our code and I'll show you how it works once you're logged in you have your guide now you're not going to have anything on the left hand side here until you actually integrate this into your codebase but once you start using it with open AI every time you make a request over to the chat completion API it's going to log that request over here so this is going to create a history you can the history from the history tab it's essentially this right here but then as you click on each of these items in the history it'll show you everything about that particular prompt and so in this case we have our system prompt and then we have our initial user prompt and then we have our initial response from the assistant one really nice thing about prompt player is that it tells you exactly how much a certain request cost so in this case this request cost 0025 58 super cheap which is great but if you're over in the open AI dashboard and you're looking at your usage you can see how much overall you've spent you can see if it was GPT 3.5 or if it was GPT 4 but for each you know granular request you don't get to see how much it costs you get to see the tokens which you can see the total tokens here as well 143 tokens so you would have to calculate that on your own in this case it's calculated for you now there is a playground over in open AI of course we've seen that in previous video but over here with prompt layer when we open this in the playground it's actually going to pick up where we left off with this request to the API so we can toggle back and forth between the completions Legacy API and the new chat completions API we're going to leave it on the chat completions API because that's how we initially sent this request but you can see here's the system message you know you're a helpful coding assistant you only answer coding related questions nothing else if you asked a question that is not related to coding you reply politely but declined to answer and then the user's first question is is HTML hard to learn and then over there we saw the response you could run this again and when you run it you'll get the assistant response here in a second so we wait a second for it to come back to us and here's the assistant's answer HTML or hypertext markup language is blah blah blah so it's answering the user R question now you can add a new message here so if you want to respond to the assistant you can do that so essentially you're getting granular control of what you would be doing in your code but now you're doing it in the playground that way you don't have to keep going back and changing your code over and over again you can play with it here and you can kind of get a feel for which prompts are going to work best for whatever it is you're trying to write and then you can go Implement that into your code base so you'll notice they have functions down here this is brand new this is something that open AI does not currently have in their playground this is similar to what we showed you in the function call video where you can actually have a list of functions that the chat completions API can see and it can see what the parameters are and if it decides it wants to call one of those or if you explicitly tell it to call one of those functions it'll send you back the arguments for that function that it wants you to call including the name of the function so that's really neat you can integrate that here as well there's also some additional stuff that does not exist inside of open aai things like tags so you can see here it says no tags so you can actually add some tags and some meta information to each of your requests and it'll allow you to keep better tracking of the requests so for example let's say you're building a chat bot for like Discord and your members of your Discord server are able to ask questions of this bot and then those questions will be piped through the GPT 3.5 turbo or gbt 4 apis and the responses will come back to the users in the Discord interface that's really cool except if you're offering this as a free service then people might take advantage of it so you'd want to track that and you'd want to see you know who's asking what how often are they asking what are they asking uh you know what is the the Cadence and everything so you could add some information to your Bot to where every request through to the chat completions API has some information some meta information about the user from your Discord server that asked it like their their username or their user ID that way if somebody is abusing the system you can see okay this user with this user ID is the one that's doing it you know they're overusing it or they're asking questions that aren't meant to be asked or whatever the case may be you can go in there and you can identify those things you can of course bake that into the application that you're writing but having it right here in a dashboard is really nice because it's automatic and you can share it with other team members so if you're working on larger projects and you're doing some prompt engineering then you can have anybody on your team in your startup or company or just friends that you're working with be able to log in you can share this with them and they can look at the different prompts and the different logs and histories and the different tags everything that's available from this tool prompt layer it can be available to everyone that you're working with so there's a lot of really cool stuff this is just the playground and the history over here we have the registry so the registry is going to allow you to create templates where you can inject variables into like premade prompts and use those in your code so that's really neat and then we have analytics so this is going to tell you you know the total cost of all the different requests the average latency how many requests there were all kinds of things and it's going to break it down into like pie charts and graphs and things like that so this is really helpful to have in addition to whatever you have going on over in your default open aai dashboard so let's head over to our code and see how we can integrate this because it's super easy it's so easy you're going to be like I can't believe it it's only two or three lines of code and then you're up and running in no time if you remember the first thing we want to do was a pip install prompt layer that way we can import it up here so if you're inside of your terminal make make sure that you have your virtual environment activated and then just do a pip install prompt layer once you run that it'll get installed and of course you can freeze that to your requirements.txt file if you're keeping track of all the different dependencies for your project everything after that is the same you know we're importing OS so that we can get access to our environment variables in this case you're going to want to take the API key and Export it so in your dashboard you know when you generate the key from your dashboard you're going to want to copy that and Export it as an environment variable in this case we named it prompt layer aior key you can name it whatever you like just make sure that it matches what you have here after that you'll notice that we no longer import open AI right that's how we were doing it previously we had import open AI now we're saying openai the variable is equal to prompt layer. openai and that's all you have to do to layer prompt layer on top of open AI so that now it's going to track all the requests to open a we of course have to add the API key for open AI same thing we just take the openai variable say API key is equal to and then pull that API key from the environment variable that we have set so that's the same as it was in all the previous videos you're going to make your request to open AI the same way that you would regularly and then you just go over to prompt layer your dashboard you log in and you look at the log for all the requests that you made so this doesn't actually change response is equal to open AI chat completion. create tell it which model you want tell it your messages so you start with your system role give it some content start with your user role after that give it some content in this case you know who was the first president of United States set your temperature and your max tokens but here we have an additional argument to the chat completion call that is not normally included in our previous API call so now that we have prompt player on top of this we can add some additional things that prompt ler knows about so in this case we have plore tags is equal to a list of strings and these are tags comma separated inside this list that we can attach to the request that way if we want to be able to categorize and search through all of our previous requests we can do that very easily by using these tag names so in this case I'm asking a question about who the first president of the United States was I'm going to tag this with US presidents so we'll see how that works more in a second after we send this request we take our response variable and we print it to the console so nothing new there so we can run real quick and wait for it to come back sure enough there's our response so we have all the typical things that we normally have in our response the answer to our question from the assistant role is the first president of the United States was George Washington we see how many tokens Etc so now if we jump back over to our dashboard for prompt layer we can see the most recent request that was logged is this one right here so it cost us 00625 we can see that it was 38 tokens if we go back we can see sure enough total tokens was 38 so that's good and you'll notice somewhere in here where is it there it is US presidents is the tag so this is helpful because if we go up here and we search for US presidents then it's going to pull up the two different requests this one we just did and one that I did earlier while I was testing this out that have those tags so you can see how useful that can be for you know categorizing and being able to you know organize and look at the different uh logs for your various requests that you made in the past you can even export this stuff you can finetune your model which is really crazy we'll talk more about that in a future video in this video we just want to show you how simple it was to get up and running with prompt player on top of the openai API and it's as simple as a couple lines of code a pip install with your virtual environment activated you know generating a key exporting that key as an environment variable and logging into your dashboard and viewing the logs of all your requests to the open API so that's it for this video thanks a lot and we'll catch you in the next one
