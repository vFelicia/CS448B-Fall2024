With timestamps:

00:00 - alright guys so in this video what we're
00:02 - going to do is we're going to take a
00:03 - look at crawling an actual page so this
00:06 - is actually a pretty fun function and
00:08 - again all this function is going to do
00:10 - is probably exactly like you think we're
00:14 - going to pass it in a URL or a web page
00:17 - and it can be any web page on your
00:19 - website and then it's going to go ahead
00:21 - and connect to that page get all of the
00:24 - links and once it has those links it's
00:26 - going to add them to the waiting list if
00:28 - you didn't curl the page already so once
00:32 - all of those links are in the waiting
00:34 - list and the page has been curled what
00:36 - it also needs to do is it needs to take
00:38 - that URL and add it to the crowd file
00:41 - and that ensures that you aren't
00:42 - crawling the same page over and over and
00:44 - over again so again that's what it's
00:47 - going to be doing and also after all
00:49 - that's done we need to update those data
00:52 - files because whenever we create another
00:54 - spider we need to make sure that it has
00:57 - an up-to-date copy of you know the
01:00 - newest links the most up-to-date links
01:02 - so let's go ahead and get started and
01:04 - this is also going to be a static method
01:07 - like before and what I named it crawl
01:09 - page all right
01:14 - so hold on my freaking knows not even
01:18 - going to pause my video you know I have
01:21 - this box of tissues by my computer that
01:23 - sounded kind of weird well I do just
01:26 - because I have a runny nose all the time
01:27 - and I actually have to like split them
01:29 - in half lengthwise because if I just try
01:33 - to stick an entire thing up my nostril
01:35 - then I can't fit all the tissue in there
01:38 - so yeah I'm sure you guys wanted to know
01:40 - the details of how I you know get snot
01:43 - out of my nose but anyways whenever we
01:46 - curl the page we need to display well we
01:49 - don't need to but it's good to display
01:51 - what page you're curling to the user so
01:53 - since this is a multi-threaded program
01:55 - I'm going to say like thread two
01:56 - currently curling the forum home page
01:59 - thread five crawling the videos page
02:02 - thread one crawling Bucky's profile
02:04 - whatever and it's always good to display
02:06 - something because if they just boot up
02:08 - your program and it's just a black box
02:10 - with a cursor blinking
02:12 - meg um is this program running then my
02:16 - computer just freeze whatever so make
02:18 - sure you display something might as well
02:19 - display what URL we're on so the first
02:22 - one like I said is going to be the
02:26 - thread name the first time it's actually
02:27 - just called first spider but it's
02:29 - usually going to be the thread name and
02:30 - the second one is just a page URL pretty
02:35 - sweet so the first thing we're going to
02:38 - want to do inside this method is we just
02:40 - want to make sure that we didn't already
02:41 - crawl this page so we're going to say
02:44 - get that URL and make sure that it's not
02:47 - in spider crawled now make sure that you
02:51 - aren't using crowd file use the crawled
02:54 - set for faster operations and then
02:58 - before we start you know writing all the
03:00 - good stuff let's just go ahead and print
03:02 - our indicators to the user so they you
03:05 - know know that something's going on so
03:07 - the first thing I won't pronounce
03:09 - what name of the thread it is and what
03:11 - pages are currently curling so the
03:13 - thread name crawling page URL so it's
03:21 - going to say like thread two is I'll say
03:24 - now crawling arm you know the New Boston
03:28 - slash index dot PHP or whatever now I'm
03:31 - also going to print out some other cool
03:34 - stats so I thought it would be a good
03:37 - idea to print out how many links were in
03:39 - the waiting list and also how many links
03:42 - have already been crawled so that way
03:44 - the user can boot up this program go you
03:46 - know to McDonald's or whatever come back
03:48 - and back oh so it already crawled like
03:51 - ten thousand pages that's pretty cool so
03:54 - qu e Yui
03:57 - uh-huh all right so the amount of items
04:01 - in the waiting list is we first need to
04:03 - convert to a string since it's going to
04:05 - be a number we just need to get the
04:07 - length of spider Q so this is our set
04:15 - get however many items are in it and
04:17 - since that's going to be integer value
04:19 - just go ahead and convert it to a string
04:21 - simple enough
04:23 - and now let me just do the same thing
04:25 - for crawled and let me go ahead and
04:26 - separate it with a pipe symbol
04:28 - maybe I deal with a tab now pipes good
04:33 - why are my fingers like sausage fingers
04:36 - say so for crawl do we basically want to
04:39 - do the same thing and spider crawl all
04:47 - right so it's going to say like Q arm 20
04:51 - crawled 6 something like that
04:55 - come on scrollbar all right so now we
04:58 - actually have to make the spider do
04:59 - something and a lot of these functions
05:01 - we didn't create yet so if we get a
05:04 - little issue don't worry we're going to
05:06 - be creating them in like the next two
05:07 - tutorials so the first one is spider add
05:13 - links to Q so I don't even need to
05:21 - explain what this is going to do once we
05:24 - retrieve a set of Link's we're just
05:26 - going to want to add them to the waiting
05:28 - list now where do we get those links
05:31 - from well spider gather links is
05:35 - essentially what we're going to do there
05:39 - and let me just throw this in and then
05:41 - I'll explain alright so this is a
05:42 - function we're going to make to actually
05:44 - connect to a web page and it's going to
05:47 - return a set of all of the links that it
05:51 - found on that web page now once we have
05:53 - that set of links we're just going to
05:55 - add it to the waiting list and this is
05:57 - the waiting list that all spiders can
05:59 - see and they can all be synchronized
06:02 - that way pretty easy stuff so after that
06:05 - what we need to do is we need to go back
06:11 - to the waiting list and we need to
06:13 - remove the page that we just curled so
06:16 - we're actually taking it from the
06:18 - waiting list and we need to put it on
06:20 - the crowd file so spider dots crawled
06:24 - and since this is set you can just use
06:27 - the built in add function and paid euro
06:31 - so again after you're done crawling a
06:33 - page all you're doing in these two lines
06:35 - is moving it from the waiting list
06:37 - to the crowd list simple enough and the
06:40 - last thing you need to do is sense this
06:44 - right here takes care of the sets we now
06:48 - need to update the actual files all
06:52 - right so these are for fast operations
06:55 - and then once it's done with all that
06:56 - that's when you actually update the file
06:59 - and I'll just say update files and all
07:02 - this function is going to do is it's
07:04 - going to call both of those sets and
07:08 - it's going to convert them to file
07:09 - easiest thing ever so there you go that
07:12 - is your curl page make sure I'm not
07:14 - missing anything
07:15 - all right so in the next couple toriel's
07:18 - we got three more functions to make we
07:20 - first need to connect a page gather all
07:22 - the links and again if that sounds tough
07:26 - 90% of the work is done for us right
07:28 - here so it's going to be real quick once
07:31 - we have the links we need to add them to
07:32 - the queue not that hard we already have
07:35 - that set and then we need to update
07:37 - files we already got that function
07:39 - created as well so it's going to be a
07:40 - piece of cake so thank you guys I will
07:42 - see you then

Cleaned transcript:

alright guys so in this video what we're going to do is we're going to take a look at crawling an actual page so this is actually a pretty fun function and again all this function is going to do is probably exactly like you think we're going to pass it in a URL or a web page and it can be any web page on your website and then it's going to go ahead and connect to that page get all of the links and once it has those links it's going to add them to the waiting list if you didn't curl the page already so once all of those links are in the waiting list and the page has been curled what it also needs to do is it needs to take that URL and add it to the crowd file and that ensures that you aren't crawling the same page over and over and over again so again that's what it's going to be doing and also after all that's done we need to update those data files because whenever we create another spider we need to make sure that it has an uptodate copy of you know the newest links the most uptodate links so let's go ahead and get started and this is also going to be a static method like before and what I named it crawl page all right so hold on my freaking knows not even going to pause my video you know I have this box of tissues by my computer that sounded kind of weird well I do just because I have a runny nose all the time and I actually have to like split them in half lengthwise because if I just try to stick an entire thing up my nostril then I can't fit all the tissue in there so yeah I'm sure you guys wanted to know the details of how I you know get snot out of my nose but anyways whenever we curl the page we need to display well we don't need to but it's good to display what page you're curling to the user so since this is a multithreaded program I'm going to say like thread two currently curling the forum home page thread five crawling the videos page thread one crawling Bucky's profile whatever and it's always good to display something because if they just boot up your program and it's just a black box with a cursor blinking meg um is this program running then my computer just freeze whatever so make sure you display something might as well display what URL we're on so the first one like I said is going to be the thread name the first time it's actually just called first spider but it's usually going to be the thread name and the second one is just a page URL pretty sweet so the first thing we're going to want to do inside this method is we just want to make sure that we didn't already crawl this page so we're going to say get that URL and make sure that it's not in spider crawled now make sure that you aren't using crowd file use the crawled set for faster operations and then before we start you know writing all the good stuff let's just go ahead and print our indicators to the user so they you know know that something's going on so the first thing I won't pronounce what name of the thread it is and what pages are currently curling so the thread name crawling page URL so it's going to say like thread two is I'll say now crawling arm you know the New Boston slash index dot PHP or whatever now I'm also going to print out some other cool stats so I thought it would be a good idea to print out how many links were in the waiting list and also how many links have already been crawled so that way the user can boot up this program go you know to McDonald's or whatever come back and back oh so it already crawled like ten thousand pages that's pretty cool so qu e Yui uhhuh all right so the amount of items in the waiting list is we first need to convert to a string since it's going to be a number we just need to get the length of spider Q so this is our set get however many items are in it and since that's going to be integer value just go ahead and convert it to a string simple enough and now let me just do the same thing for crawled and let me go ahead and separate it with a pipe symbol maybe I deal with a tab now pipes good why are my fingers like sausage fingers say so for crawl do we basically want to do the same thing and spider crawl all right so it's going to say like Q arm 20 crawled 6 something like that come on scrollbar all right so now we actually have to make the spider do something and a lot of these functions we didn't create yet so if we get a little issue don't worry we're going to be creating them in like the next two tutorials so the first one is spider add links to Q so I don't even need to explain what this is going to do once we retrieve a set of Link's we're just going to want to add them to the waiting list now where do we get those links from well spider gather links is essentially what we're going to do there and let me just throw this in and then I'll explain alright so this is a function we're going to make to actually connect to a web page and it's going to return a set of all of the links that it found on that web page now once we have that set of links we're just going to add it to the waiting list and this is the waiting list that all spiders can see and they can all be synchronized that way pretty easy stuff so after that what we need to do is we need to go back to the waiting list and we need to remove the page that we just curled so we're actually taking it from the waiting list and we need to put it on the crowd file so spider dots crawled and since this is set you can just use the built in add function and paid euro so again after you're done crawling a page all you're doing in these two lines is moving it from the waiting list to the crowd list simple enough and the last thing you need to do is sense this right here takes care of the sets we now need to update the actual files all right so these are for fast operations and then once it's done with all that that's when you actually update the file and I'll just say update files and all this function is going to do is it's going to call both of those sets and it's going to convert them to file easiest thing ever so there you go that is your curl page make sure I'm not missing anything all right so in the next couple toriel's we got three more functions to make we first need to connect a page gather all the links and again if that sounds tough 90% of the work is done for us right here so it's going to be real quick once we have the links we need to add them to the queue not that hard we already have that set and then we need to update files we already got that function created as well so it's going to be a piece of cake so thank you guys I will see you then
