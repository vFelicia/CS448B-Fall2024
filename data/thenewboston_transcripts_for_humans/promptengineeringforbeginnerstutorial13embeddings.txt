With timestamps:

00:00 - hey welcome back everyone this is Ian
00:02 - bringing you another video in this AI
00:03 - series with the New Boston in today's
00:06 - video we're going to talk about
00:07 - something called embeddings so what are
00:09 - embeddings let's go straight to the
00:10 - source this is the open aai
00:12 - documentation and the answer to that
00:14 - question is open AI text embeddings
00:17 - measure the relatedness of text strings
00:20 - so it has some things that they're
00:21 - commonly used for listed here search
00:23 - clustering recommendations anomaly
00:25 - detection diversity measurement and
00:27 - classification specifically in this
00:29 - video we're going to use it for text
00:31 - similarity so we'll pass a word or
00:34 - multiple words into the embeddings
00:37 - function and then when it comes back
00:39 - we'll use some math to figure out the
00:41 - relatedness or the closeness of those
00:43 - two words it's going to give us a score
00:46 - and then based on the value of that
00:47 - score we'll be able to determine how
00:50 - similar two words are to each other so
00:52 - there's a lot of cool things you can do
00:54 - with embeddings I highly encourage you
00:55 - to go to the documentation go through
00:58 - here and see what all you can do we're
01:00 - specifically going to be using this
01:02 - example here to just create an embedding
01:05 - and then we'll use some math something
01:06 - called cosine similarity to calculate a
01:09 - score to determine how close two
01:11 - different words are to one another so
01:14 - when we send a request to the open AI
01:17 - embedding API what we get back is this
01:20 - embedding object here and so inside of
01:23 - it it has a list called data and that
01:26 - has one or more objects that include the
01:29 - embedding data each embedding is going
01:31 - to look like a list of floating Point
01:34 - numbers negative or positive and there
01:36 - will be a lot of them in there in this
01:39 - case specifically
01:40 - 1,536 but it could be less or more based
01:43 - on the model that you use there's some
01:45 - other information here like the index of
01:47 - that specific object inside of the data
01:50 - list and then what type of object it is
01:52 - in this case it's an embedding object
01:54 - which model is used with the request
01:56 - what type of object it is in this case
01:58 - it's a list and then how many tokens you
02:00 - used for your prompt and your total
02:02 - tokens with the overall usage of the
02:05 - request before we dive into the actual
02:08 - code let's talk a little bit about
02:10 - embeddings so that you can get a visual
02:12 - overview of exactly what they are and
02:14 - how they work so if we scroll back up
02:16 - here we can see that this embedding is a
02:19 - list of numbers and essentially numbers
02:23 - that are closer together in separate
02:25 - lists will help us determine how close
02:28 - the words that that those numbers are
02:30 - derived from are to one another how
02:33 - similar they are if we look at this
02:37 - right here you can see I have this kind
02:39 - of world mapped out and I've got these
02:41 - islands and on each island there's a
02:44 - word notice that words like cat and
02:46 - Feline and purr and lion are all very
02:49 - similar all very close to each other
02:51 - because they're similar and then you
02:53 - have something like dog or animal which
02:55 - isn't too far away because you know both
02:58 - a cat and a dog are animals
03:00 - and then we have things like eagle and
03:01 - elephant they're kind of a similar
03:03 - distance from animal as are cat and dog
03:06 - but then you have something like
03:07 - microscope which is way over here on the
03:09 - left far away from everything because
03:11 - it's not really similar to these other
03:13 - words we use something in this case we
03:16 - use cosine similarity which is a
03:18 - mathematical equation that helps us
03:20 - determine how close each of these
03:22 - islands are to one another that's what
03:24 - we're going to do we're going to take
03:25 - these words from these islands and we're
03:27 - going to pass them through this
03:28 - embedding function or we're going to
03:30 - send them up to the embedding API I
03:32 - should say and what we get back are
03:34 - these embeddings and so then we'll use
03:36 - the embeddings with that cosine
03:37 - similarity function which is just a
03:39 - mathematical equation it's going to give
03:41 - us a number and that number is going to
03:43 - help us determine how close these
03:45 - islands are to one another that'll help
03:47 - us know how similar the words are so cat
03:49 - should be closer to feline than cat is
03:51 - to microscope or cat should be closer to
03:54 - feline than elephant is to microscope
03:56 - things like that there's one last thing
03:58 - I want to show you before we jump into
03:59 - to the code and that is this YouTube
04:01 - video by stat quest with Josh starmer
04:04 - it's called cosine similarity clearly
04:06 - explained this is a great video it
04:08 - helped me better understand this concept
04:10 - it's really short only 10 minutes so if
04:12 - you want to learn more about the math
04:14 - behind figuring out the similarity
04:16 - between these different words with their
04:18 - embeddings then this is definitely a
04:20 - video for you let's go ahead and jump
04:22 - over to the code now and we'll see how
04:25 - this actually works with sending the
04:27 - requests up to open AI we are going to
04:29 - import something called num Pi that'll
04:31 - let us do the mathematical equation for
04:33 - the cosine similarity and then of course
04:35 - we need to bring in open AI library and
04:37 - create a new client from it now that we
04:39 - have that client we can actually send
04:41 - requests up to the embedding API okay so
04:43 - then we have a function here called
04:45 - cosine similarity so again if you want
04:47 - to understand the math happening behind
04:48 - the scenes for cosine similarity check
04:50 - out that video that I referenced but
04:52 - just know that what we do as we pass in
04:54 - two values the values here are going to
04:56 - be the actual embeddings so the vectors
04:58 - for each of the words and then we're
05:01 - going to get a score calculated for how
05:04 - close they are together a value between
05:07 - 0 and one if it's zero they're not
05:09 - similar at all if it's one they're very
05:11 - similar and any value in between will
05:13 - allow us to determine just how close or
05:15 - far away they are from one another the
05:17 - next function that we have here is get
05:19 - embedding so this is going to be our
05:21 - call to the client the open AI API
05:25 - embeddings endpoint so the reason I put
05:27 - this inside of a function here is
05:29 - because we're going to use it multiple
05:30 - times inside of this program I didn't
05:32 - want to repeat ourselves so I went ahead
05:34 - and wrapped it in a function called get
05:36 - embedding so get embedding is going to
05:38 - take an input if we pass in one single
05:40 - string value it's just going to give us
05:42 - one embedding if we pass in a list of
05:44 - multiple string values it'll give us an
05:45 - embedding for each of those strings and
05:48 - then we just want to pass in the model
05:49 - that we want to use so you'll see the
05:50 - model here in a second but essentially
05:52 - this function is going to return the
05:54 - response from calling client. eddings
05:56 - doc create it passes in our input and
05:59 - our model model and then it sends us
06:00 - back an object like the one that we
06:02 - looked at in the documentation a moment
06:04 - ago from that we can pull out the actual
06:06 - embedding so down here we have a
06:09 - constant variable called Model we're
06:11 - going to use the same model across all
06:12 - these requests and we only want to
06:14 - change it in one place if we decide to
06:16 - use a different one in the future so for
06:18 - now we're using text embedding Ada 002
06:21 - this is the most recent model and it's
06:22 - said to perform better and be cheaper to
06:24 - use than all of the earlier models which
06:26 - are now deprecated so down here we have
06:29 - our first response that we get back from
06:31 - calling get embedding and passing in our
06:33 - input of corn and then the model text
06:36 - embedding a to
06:37 - 002 from that we're going to go ahead
06:39 - and print the response and you'll see it
06:42 - is a expanded version of the example
06:45 - output that we saw in the documentation
06:47 - a moment ago you'll notice whenever we
06:49 - actually get to that point that the
06:50 - embedding itself has a ton of values
06:53 - inside of it and that's because like I
06:55 - mentioned it's
06:57 - 1,536 total values in inside of that
06:59 - Vector so then we're going to print the
07:02 - length that'll confirm that number that
07:04 - I just said 1536 and that'll determine
07:08 - how many values are actually inside of
07:09 - that embedding and then the next one is
07:12 - going to be similar here but instead of
07:14 - passing in a single string for the input
07:16 - we're going to pass in a list with
07:17 - multiple strings what will happen here
07:19 - is we'll actually get back and embedding
07:21 - for each one of these strings so instead
07:23 - of just having data zero. embedding
07:25 - we'll have data zero and data one and
07:28 - then each of those will have an
07:29 - embedding inside of it then we can
07:31 - assign those embeddings to variables
07:33 - here A and B and we can pass them into
07:35 - our cosine similarity function which
07:37 - will then return a score ultimately we
07:39 - will print out the score helping us see
07:42 - the similarity between the words that
07:44 - were inputed in this case cat and Feline
07:46 - we'll do that again with words that
07:48 - aren't so similar elephant and
07:49 - microscope if you remember our Islands
07:52 - from earlier elephant was all the way
07:53 - over on the right microscope was way
07:55 - over on the left so the value that comes
07:58 - back from this get embedded setting the
08:00 - score that's calculated should be lower
08:03 - than the one that we get back for cat
08:04 - and Feline which were closer together in
08:06 - our Island example so again we're going
08:09 - to extract the embeddings from that
08:11 - response we're going to pass them to
08:13 - cosine similarity that's going to
08:14 - calculate a score and then we're going
08:16 - to print out that score here let's go
08:18 - ahead and open up our terminal we'll run
08:19 - this code and we'll see what it looks
08:21 - like in real time so here we are in our
08:23 - terminal we're inside of the AI
08:25 - playground repository the open AI
08:27 - examples folder and then inside of that
08:30 - the 13 embeddings folder there's a file
08:32 - inside of there called main.py so we'll
08:34 - go ahead and run that with python main.
08:37 - pi and that file is the one that holds
08:39 - all the code that we were just looking
08:40 - at so we'll let that run it'll send the
08:42 - request out and the first thing we
08:44 - should see is a giant list of these
08:47 - values so if we scroll up to the top
08:50 - here you can see there's just a ton of
08:52 - these and so these are all the values
08:54 - inside of our embedding and so here you
08:56 - can see data is a list and then it has
08:59 - an embedding inside of it which is
09:01 - another list that has a ton of values so
09:03 - if we scroll all the way down past all
09:05 - of these values we should be able to see
09:09 - 1,536 total values inside of that
09:11 - embedding now what we're really
09:13 - concerned with here is the output of the
09:16 - scoring for the last two function calls
09:19 - the one for cat and Feline and the one
09:21 - after that for elephant and microscope
09:23 - the similarity score between cat and
09:25 - Feline is
09:27 - 0.854 blah blah blah and then the
09:29 - similarity score between elephant and
09:31 - microscope is 0.79 37 yada yada and so
09:35 - you can see right out of the box
09:37 - elephant and microscope because they're
09:39 - further away they score lower than cat
09:42 - and Feline and that's basically it we
09:45 - got an embedding from our initial call
09:49 - so if we scroll up and we look here we
09:51 - got an embedding for corn and that's the
09:53 - one that we saw here with all these
09:55 - values and then we also did the same
09:57 - thing for cat and feline we didn't
09:59 - actually output the response but what we
10:01 - did do is take those two embeddings that
10:03 - we got back for each of those words pass
10:06 - them to our cosine similarity got our
10:08 - score and then printed it we repeated
10:10 - the same process for elephant and
10:12 - microscope and then we were able to
10:13 - determine how similar or how close those
10:16 - words were together with regards to
10:18 - their embeddings so that's how you get
10:20 - embeddings with open AI embedding API
10:23 - thanks a lot for watching this video
10:24 - can't wait to see you in the next one
10:26 - until next time peace

Cleaned transcript:

hey welcome back everyone this is Ian bringing you another video in this AI series with the New Boston in today's video we're going to talk about something called embeddings so what are embeddings let's go straight to the source this is the open aai documentation and the answer to that question is open AI text embeddings measure the relatedness of text strings so it has some things that they're commonly used for listed here search clustering recommendations anomaly detection diversity measurement and classification specifically in this video we're going to use it for text similarity so we'll pass a word or multiple words into the embeddings function and then when it comes back we'll use some math to figure out the relatedness or the closeness of those two words it's going to give us a score and then based on the value of that score we'll be able to determine how similar two words are to each other so there's a lot of cool things you can do with embeddings I highly encourage you to go to the documentation go through here and see what all you can do we're specifically going to be using this example here to just create an embedding and then we'll use some math something called cosine similarity to calculate a score to determine how close two different words are to one another so when we send a request to the open AI embedding API what we get back is this embedding object here and so inside of it it has a list called data and that has one or more objects that include the embedding data each embedding is going to look like a list of floating Point numbers negative or positive and there will be a lot of them in there in this case specifically 1,536 but it could be less or more based on the model that you use there's some other information here like the index of that specific object inside of the data list and then what type of object it is in this case it's an embedding object which model is used with the request what type of object it is in this case it's a list and then how many tokens you used for your prompt and your total tokens with the overall usage of the request before we dive into the actual code let's talk a little bit about embeddings so that you can get a visual overview of exactly what they are and how they work so if we scroll back up here we can see that this embedding is a list of numbers and essentially numbers that are closer together in separate lists will help us determine how close the words that that those numbers are derived from are to one another how similar they are if we look at this right here you can see I have this kind of world mapped out and I've got these islands and on each island there's a word notice that words like cat and Feline and purr and lion are all very similar all very close to each other because they're similar and then you have something like dog or animal which isn't too far away because you know both a cat and a dog are animals and then we have things like eagle and elephant they're kind of a similar distance from animal as are cat and dog but then you have something like microscope which is way over here on the left far away from everything because it's not really similar to these other words we use something in this case we use cosine similarity which is a mathematical equation that helps us determine how close each of these islands are to one another that's what we're going to do we're going to take these words from these islands and we're going to pass them through this embedding function or we're going to send them up to the embedding API I should say and what we get back are these embeddings and so then we'll use the embeddings with that cosine similarity function which is just a mathematical equation it's going to give us a number and that number is going to help us determine how close these islands are to one another that'll help us know how similar the words are so cat should be closer to feline than cat is to microscope or cat should be closer to feline than elephant is to microscope things like that there's one last thing I want to show you before we jump into to the code and that is this YouTube video by stat quest with Josh starmer it's called cosine similarity clearly explained this is a great video it helped me better understand this concept it's really short only 10 minutes so if you want to learn more about the math behind figuring out the similarity between these different words with their embeddings then this is definitely a video for you let's go ahead and jump over to the code now and we'll see how this actually works with sending the requests up to open AI we are going to import something called num Pi that'll let us do the mathematical equation for the cosine similarity and then of course we need to bring in open AI library and create a new client from it now that we have that client we can actually send requests up to the embedding API okay so then we have a function here called cosine similarity so again if you want to understand the math happening behind the scenes for cosine similarity check out that video that I referenced but just know that what we do as we pass in two values the values here are going to be the actual embeddings so the vectors for each of the words and then we're going to get a score calculated for how close they are together a value between 0 and one if it's zero they're not similar at all if it's one they're very similar and any value in between will allow us to determine just how close or far away they are from one another the next function that we have here is get embedding so this is going to be our call to the client the open AI API embeddings endpoint so the reason I put this inside of a function here is because we're going to use it multiple times inside of this program I didn't want to repeat ourselves so I went ahead and wrapped it in a function called get embedding so get embedding is going to take an input if we pass in one single string value it's just going to give us one embedding if we pass in a list of multiple string values it'll give us an embedding for each of those strings and then we just want to pass in the model that we want to use so you'll see the model here in a second but essentially this function is going to return the response from calling client. eddings doc create it passes in our input and our model model and then it sends us back an object like the one that we looked at in the documentation a moment ago from that we can pull out the actual embedding so down here we have a constant variable called Model we're going to use the same model across all these requests and we only want to change it in one place if we decide to use a different one in the future so for now we're using text embedding Ada 002 this is the most recent model and it's said to perform better and be cheaper to use than all of the earlier models which are now deprecated so down here we have our first response that we get back from calling get embedding and passing in our input of corn and then the model text embedding a to 002 from that we're going to go ahead and print the response and you'll see it is a expanded version of the example output that we saw in the documentation a moment ago you'll notice whenever we actually get to that point that the embedding itself has a ton of values inside of it and that's because like I mentioned it's 1,536 total values in inside of that Vector so then we're going to print the length that'll confirm that number that I just said 1536 and that'll determine how many values are actually inside of that embedding and then the next one is going to be similar here but instead of passing in a single string for the input we're going to pass in a list with multiple strings what will happen here is we'll actually get back and embedding for each one of these strings so instead of just having data zero. embedding we'll have data zero and data one and then each of those will have an embedding inside of it then we can assign those embeddings to variables here A and B and we can pass them into our cosine similarity function which will then return a score ultimately we will print out the score helping us see the similarity between the words that were inputed in this case cat and Feline we'll do that again with words that aren't so similar elephant and microscope if you remember our Islands from earlier elephant was all the way over on the right microscope was way over on the left so the value that comes back from this get embedded setting the score that's calculated should be lower than the one that we get back for cat and Feline which were closer together in our Island example so again we're going to extract the embeddings from that response we're going to pass them to cosine similarity that's going to calculate a score and then we're going to print out that score here let's go ahead and open up our terminal we'll run this code and we'll see what it looks like in real time so here we are in our terminal we're inside of the AI playground repository the open AI examples folder and then inside of that the 13 embeddings folder there's a file inside of there called main.py so we'll go ahead and run that with python main. pi and that file is the one that holds all the code that we were just looking at so we'll let that run it'll send the request out and the first thing we should see is a giant list of these values so if we scroll up to the top here you can see there's just a ton of these and so these are all the values inside of our embedding and so here you can see data is a list and then it has an embedding inside of it which is another list that has a ton of values so if we scroll all the way down past all of these values we should be able to see 1,536 total values inside of that embedding now what we're really concerned with here is the output of the scoring for the last two function calls the one for cat and Feline and the one after that for elephant and microscope the similarity score between cat and Feline is 0.854 blah blah blah and then the similarity score between elephant and microscope is 0.79 37 yada yada and so you can see right out of the box elephant and microscope because they're further away they score lower than cat and Feline and that's basically it we got an embedding from our initial call so if we scroll up and we look here we got an embedding for corn and that's the one that we saw here with all these values and then we also did the same thing for cat and feline we didn't actually output the response but what we did do is take those two embeddings that we got back for each of those words pass them to our cosine similarity got our score and then printed it we repeated the same process for elephant and microscope and then we were able to determine how similar or how close those words were together with regards to their embeddings so that's how you get embeddings with open AI embedding API thanks a lot for watching this video can't wait to see you in the next one until next time peace
