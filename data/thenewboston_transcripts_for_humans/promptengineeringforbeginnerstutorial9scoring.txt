With timestamps:

00:00 - hey welcome back everyone this is Ian
00:02 - bringing you another video in this AI
00:04 - series with the New Boston in today's
00:06 - video we're going to be talking about
00:08 - another feature from prompt layer and
00:10 - that is scoring our prompts it's really
00:12 - simple so this video is not going to be
00:14 - very long but this is a super impactful
00:16 - feature and I definitely recommend that
00:18 - you play around with it so you can see
00:19 - at the top here we are bringing in OS
00:22 - and prompt layer setting up prompt layer
00:24 - with our API key and then pulling in
00:27 - open AI from prom player then we create
00:29 - our client from open Ai and we're ready
00:31 - to use the openai API while also
00:34 - tracking it with promp player so that's
00:36 - the same as it's been in the last few
00:38 - videos nothing new there but if you need
00:39 - to get caught up to speed go back a
00:41 - couple videos we have an explanation of
00:43 - every line of code that you're seeing
00:44 - here so all promp layer requests have
00:46 - unique IDs and these can be optionally
00:49 - returned when logging the request using
00:52 - prompt layer and the request ID we can
00:54 - score a request with an integer between
00:56 - 0 and 100 this is most often used to
00:59 - understand how effective certain prompts
01:01 - are in production so essentially what
01:03 - it's saying here is that we can return
01:06 - an ID that allows us to connect a prompt
01:11 - with a score that we give to it and
01:12 - we'll see that here in a second in our
01:14 - actual code but if we score our prompts
01:17 - let's say that we have a prompt maybe
01:19 - multiple prompts and we're doing like
01:20 - some AB testing then we can run our
01:24 - prompts get our responses and based on
01:26 - the response we can say okay this was
01:28 - really close to what I wanted or this
01:29 - was exactly what I wanted or this wasn't
01:31 - what I wanted at all and then we can
01:33 - score those accordingly so we can say
01:35 - okay it wasn't what I wanted at all
01:36 - that's a zero this is exactly what I
01:38 - wanted that's 100 and then when we go
01:40 - back to our prompt player dashboard
01:42 - later we can see all the scoring
01:44 - attached to all of the prompts and their
01:46 - responses this is especially helpful for
01:49 - working on teams so everyone can see how
01:51 - well these various prompts are
01:53 - performing okay so down here is the
01:55 - actual request to the openai chat
01:58 - completions API and you'll notice this
02:00 - is similar to our previous video where
02:03 - we have two variables here response
02:06 - comma plore requestor ID so the first
02:10 - thing that we're getting back from this
02:11 - call right here is going to be the
02:13 - actual response object and then the
02:16 - second thing is going to be the prompt
02:18 - layer request ID and the reason we're
02:20 - getting that second thing is because
02:22 - inside of this call to. create at the
02:25 - very bottom we have an argument here
02:27 - return plore and we have it set to the
02:31 - Boolean value of true so what that does
02:33 - of course is it gives us back the prompt
02:35 - layer request ID which will show you how
02:37 - to use here in a second at the bottom
02:39 - when we actually connect the response to
02:42 - the score that we're going to give to it
02:43 - everything else is the same we set our
02:45 - model we have our messages list which
02:47 - has an object inside of it we tell it
02:49 - what the role is and then the content in
02:51 - this case what is the capital of New
02:53 - York and then how many tokens we're
02:55 - going to use and then we have some tags
02:58 - if we want to add some tags to this
02:59 - request so in this case it's a list of
03:02 - strings and I just created one specific
03:04 - tag you can put as many as you want
03:05 - comma delimited this one is just going
03:07 - to be scoring uncore example so we want
03:10 - to make sure that we have return Poore
03:13 - ID set to true that's super important
03:15 - because we do need to get back that
03:17 - plore requestor ID so that we can
03:20 - actually use it to track the request
03:22 - with the score that we're going to
03:23 - assign to it so down here we're going to
03:25 - print the response we'll print a space
03:27 - after that we'll print the prompt layer
03:29 - request ID another space and then we'll
03:32 - Traverse through the response to get the
03:34 - content of the message inside the
03:36 - choices list and we'll assign the result
03:38 - of that to the answer variable which
03:40 - will then print the answer should
03:42 - contain the word Albany in it so we're
03:44 - going to say correct uncore answer is
03:47 - equal to and then in all lower case we
03:49 - say Albany in answer. lower and what
03:53 - this does is it returns a true or false
03:55 - Boolean based on whether or not the word
03:57 - Albany is inside of the the answer so
04:00 - we're going to use that to determine
04:02 - whether or not we give a 100 or a zero
04:04 - in our score so as we go down to the
04:07 - very bottom here this is where the magic
04:09 - happens so with prompt layer we can call
04:11 - Prompt layer. track. score this takes
04:13 - three arguments so the first one is that
04:15 - request ID and that's why it's so
04:17 - important for us to have the plore
04:19 - requestor ID because that's what we put
04:22 - in here so now it's matched up to the
04:24 - request that was made and prompt layer
04:27 - knows okay this is the one that we want
04:28 - to add the score to
04:30 - so then we give it a name in this casee
04:31 - I'm going to call it capital of NY
04:33 - capital of New York you can name it
04:35 - whatever you like you just pass that in
04:37 - as a string and then the last thing here
04:39 - is super important is the score so in
04:41 - this case I say score is equal to the
04:43 - number value of 100 if correct answer so
04:47 - if correct answer is true then it's 100
04:50 - else it's a zero so here is just Boolean
04:53 - back and forth between 100 and zero you
04:55 - can modify this logic if you want it to
04:57 - be a different number based on some
04:59 - other outcome but for us it's a matter
05:01 - of did it work or did it not work if it
05:03 - worked and it had Albany in the answer
05:05 - then we give it 100 if it didn't work
05:07 - false no albony in the answer then we
05:09 - say zero and it's as simple as that so
05:12 - we can run our
05:14 - request and when it comes back to us
05:16 - we'll have printed out the response
05:18 - we'll see that prompt layer request ID
05:21 - and then we'll see the actual answer so
05:24 - here is the response object here's the
05:26 - specific request ID and then here's the
05:28 - answer the cap of New York is Albany
05:30 - you'll see that the A and Albany is
05:32 - capitalized that's why we have to use a
05:34 - lowercase Albany and then take this
05:36 - string and make it to lower so that when
05:39 - we do the comparison we get a one toone
05:40 - comparison to find out if the word
05:42 - albony is actually in the answer in this
05:45 - case it is and so our score should be
05:47 - 100 so let's head over to the promp ler
05:49 - dashboard and see what it looks like
05:50 - there all right here we are in our
05:52 - dashboard you can see that it's updated
05:54 - automatically with this most recent
05:56 - request you can see that the tag scoring
05:58 - uncore example is there and if we click
06:01 - on this it takes us over to this part of
06:03 - the dashboard I'm going to go ahead and
06:04 - move my camera out of the way and you
06:06 - can see that we have this thing called
06:08 - scores in the top right corner so if I
06:10 - click this little carrot to drop it down
06:12 - you'll see that there's the default
06:14 - score is set to no score however down
06:17 - here you can see capital of NY which is
06:19 - what I tagged this specific score as is
06:22 - set to 100 so we can actually add
06:24 - multiple scores to a single request so
06:27 - if you were to do the same code that we
06:28 - did multiple times maybe with different
06:30 - tags because you're trying to figure out
06:32 - different aspects of the response then
06:34 - these would show up layered on top of
06:36 - each other there is of course the
06:38 - default score and so in this case you
06:40 - can go in here and score it manually if
06:42 - you like anything from Z to 100 and
06:44 - apply it and when you apply it you'll
06:46 - see it when you drop down it's still set
06:50 - also I should mention that if you do not
06:54 - include a score name here if you were to
06:57 - Omit this then it would give give you
06:59 - the score value but it would not have a
07:02 - name and therefore it would override the
07:03 - default value so the default value that
07:06 - we saw over here where it's either no
07:08 - score or score that's the one when you
07:10 - don't include the name in our case we
07:12 - did include a name and so it set the
07:14 - name to the value that we assigned to it
07:16 - so that's it for scoring with prompt
07:18 - layer super easy super quick but a
07:20 - really powerful feature that'll allow
07:21 - you to get some really good insights
07:23 - into your prompt requests and the
07:26 - outputs that you're getting back from
07:27 - them thanks a lot and we'll catch you
07:29 - all in the next video

Cleaned transcript:

hey welcome back everyone this is Ian bringing you another video in this AI series with the New Boston in today's video we're going to be talking about another feature from prompt layer and that is scoring our prompts it's really simple so this video is not going to be very long but this is a super impactful feature and I definitely recommend that you play around with it so you can see at the top here we are bringing in OS and prompt layer setting up prompt layer with our API key and then pulling in open AI from prom player then we create our client from open Ai and we're ready to use the openai API while also tracking it with promp player so that's the same as it's been in the last few videos nothing new there but if you need to get caught up to speed go back a couple videos we have an explanation of every line of code that you're seeing here so all promp layer requests have unique IDs and these can be optionally returned when logging the request using prompt layer and the request ID we can score a request with an integer between 0 and 100 this is most often used to understand how effective certain prompts are in production so essentially what it's saying here is that we can return an ID that allows us to connect a prompt with a score that we give to it and we'll see that here in a second in our actual code but if we score our prompts let's say that we have a prompt maybe multiple prompts and we're doing like some AB testing then we can run our prompts get our responses and based on the response we can say okay this was really close to what I wanted or this was exactly what I wanted or this wasn't what I wanted at all and then we can score those accordingly so we can say okay it wasn't what I wanted at all that's a zero this is exactly what I wanted that's 100 and then when we go back to our prompt player dashboard later we can see all the scoring attached to all of the prompts and their responses this is especially helpful for working on teams so everyone can see how well these various prompts are performing okay so down here is the actual request to the openai chat completions API and you'll notice this is similar to our previous video where we have two variables here response comma plore requestor ID so the first thing that we're getting back from this call right here is going to be the actual response object and then the second thing is going to be the prompt layer request ID and the reason we're getting that second thing is because inside of this call to. create at the very bottom we have an argument here return plore and we have it set to the Boolean value of true so what that does of course is it gives us back the prompt layer request ID which will show you how to use here in a second at the bottom when we actually connect the response to the score that we're going to give to it everything else is the same we set our model we have our messages list which has an object inside of it we tell it what the role is and then the content in this case what is the capital of New York and then how many tokens we're going to use and then we have some tags if we want to add some tags to this request so in this case it's a list of strings and I just created one specific tag you can put as many as you want comma delimited this one is just going to be scoring uncore example so we want to make sure that we have return Poore ID set to true that's super important because we do need to get back that plore requestor ID so that we can actually use it to track the request with the score that we're going to assign to it so down here we're going to print the response we'll print a space after that we'll print the prompt layer request ID another space and then we'll Traverse through the response to get the content of the message inside the choices list and we'll assign the result of that to the answer variable which will then print the answer should contain the word Albany in it so we're going to say correct uncore answer is equal to and then in all lower case we say Albany in answer. lower and what this does is it returns a true or false Boolean based on whether or not the word Albany is inside of the the answer so we're going to use that to determine whether or not we give a 100 or a zero in our score so as we go down to the very bottom here this is where the magic happens so with prompt layer we can call Prompt layer. track. score this takes three arguments so the first one is that request ID and that's why it's so important for us to have the plore requestor ID because that's what we put in here so now it's matched up to the request that was made and prompt layer knows okay this is the one that we want to add the score to so then we give it a name in this casee I'm going to call it capital of NY capital of New York you can name it whatever you like you just pass that in as a string and then the last thing here is super important is the score so in this case I say score is equal to the number value of 100 if correct answer so if correct answer is true then it's 100 else it's a zero so here is just Boolean back and forth between 100 and zero you can modify this logic if you want it to be a different number based on some other outcome but for us it's a matter of did it work or did it not work if it worked and it had Albany in the answer then we give it 100 if it didn't work false no albony in the answer then we say zero and it's as simple as that so we can run our request and when it comes back to us we'll have printed out the response we'll see that prompt layer request ID and then we'll see the actual answer so here is the response object here's the specific request ID and then here's the answer the cap of New York is Albany you'll see that the A and Albany is capitalized that's why we have to use a lowercase Albany and then take this string and make it to lower so that when we do the comparison we get a one toone comparison to find out if the word albony is actually in the answer in this case it is and so our score should be 100 so let's head over to the promp ler dashboard and see what it looks like there all right here we are in our dashboard you can see that it's updated automatically with this most recent request you can see that the tag scoring uncore example is there and if we click on this it takes us over to this part of the dashboard I'm going to go ahead and move my camera out of the way and you can see that we have this thing called scores in the top right corner so if I click this little carrot to drop it down you'll see that there's the default score is set to no score however down here you can see capital of NY which is what I tagged this specific score as is set to 100 so we can actually add multiple scores to a single request so if you were to do the same code that we did multiple times maybe with different tags because you're trying to figure out different aspects of the response then these would show up layered on top of each other there is of course the default score and so in this case you can go in here and score it manually if you like anything from Z to 100 and apply it and when you apply it you'll see it when you drop down it's still set also I should mention that if you do not include a score name here if you were to Omit this then it would give give you the score value but it would not have a name and therefore it would override the default value so the default value that we saw over here where it's either no score or score that's the one when you don't include the name in our case we did include a name and so it set the name to the value that we assigned to it so that's it for scoring with prompt layer super easy super quick but a really powerful feature that'll allow you to get some really good insights into your prompt requests and the outputs that you're getting back from them thanks a lot and we'll catch you all in the next video
