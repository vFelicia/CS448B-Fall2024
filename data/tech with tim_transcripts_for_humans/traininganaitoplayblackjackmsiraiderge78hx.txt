With timestamps:

00:00 - some of my favorite projects involve
00:01 - training in AI how to play a game I've
00:04 - done this for Flappy Bird chess pong
00:07 - Checkers but today I'm taking on
00:09 - Blackjack now this is an interesting
00:11 - game because we already know how to play
00:13 - it optimally so I'm interested to see if
00:15 - the AI can learn the rules on its own
00:18 - I'm not going to hardcode it I'm going
00:19 - to see if it can play enough games to
00:21 - figure out the optimal strategy and to
00:23 - even beat the casino now by no means am
00:26 - I encouraging gambling here this is just
00:28 - a fun game and the math behind all these
00:30 - casino games has always been interesting
00:32 - to me in this video I'm going to work on
00:34 - this project completely from scratch and
00:36 - I'm going to bring you along for the
00:37 - ride with that said let's dive
00:42 - in so first things first I got to figure
00:44 - out what to do I know there's a bunch of
00:46 - different approaches and right now I'm
00:48 - thinking that I could use something like
00:49 - neat which is neuro evolution of augment
00:52 - topologies I could try to build a neural
00:54 - network or I could go for something like
00:56 - reinforcement learning I'm going to do a
00:58 - bit of research consult the good old
01:00 - chat GPT see what it tells me and report
01:03 - [Applause]
01:05 - back so I've been doing a bit of
01:07 - research here let me share with you what
01:09 - I found so I asked chat gbt I want to
01:11 - train in AI how to play Blackjack what's
01:13 - the best way to do that these are kind
01:15 - of the methods I know it walked through
01:17 - a bunch of them I'm not going to bore
01:18 - you with all the results but pretty much
01:21 - what I was thinking is somewhat correct
01:23 - we can use a neural network but that's a
01:25 - little bit complex and we're going to
01:27 - kind of mix that in with something known
01:29 - as Q learning now if we go down here Q
01:31 - learning is probably the most basic
01:33 - approach that we can use and what this
01:35 - essentially means is we're going to
01:37 - generate a massive State table that has
01:39 - every single possible result or every
01:42 - single state that we could have in
01:43 - blackj just like those really popular
01:45 - cards you see where it shows your card
01:48 - value or your hand total versus the
01:49 - dealer card and what kind of action you
01:52 - should take so I asked chat gbt explain
01:55 - a bit more about Q learning just so I
01:57 - could kind of refresh my memory on how
01:58 - it worked and and then I asked it to
02:00 - give me a bit of a visualization just so
02:02 - that I could show you guys and I can see
02:03 - for myself exactly what it was talking
02:06 - about here you can see that we have a
02:08 - pretty simple table where we have the
02:10 - sum of our hand we have the Dealer's up
02:12 - card we have if we have a usable Ace or
02:14 - not and then we have the action that
02:16 - we'll take which is hit or stand I think
02:18 - I'm going to start with just hit or
02:20 - stand and then as we get a bit more
02:22 - complex I'm going to start adding in
02:23 - doubling splitting surrendering and a
02:26 - lot of the other more advanced rules in
02:27 - blackjack now what's going to happen is
02:29 - we're we're going to initialize this
02:30 - table with all of these different states
02:33 - and we're going to assign them something
02:34 - known as a q value now the Q value is
02:37 - the predicted result or the predicted
02:39 - outcome based on a specific action from
02:42 - the state before so what we'll do is
02:45 - we'll say hit a 12 versus a two when we
02:48 - hit that we'll either win or lose
02:50 - depending on the result we'll update the
02:52 - Q value and we'll continue to do this
02:54 - Millions upon millions of times
02:57 - eventually once we played enough hands
02:59 - of blackjack the computer will start to
03:01 - figure out which combinations or which
03:03 - states should take what specific action
03:06 - it'll do that by looking at the Q value
03:09 - so later we'll just have this massive
03:11 - table that will have all of the
03:12 - different combinations all of the
03:14 - actions we can take and the expected
03:16 - result for those actions and we'll just
03:18 - pick the action the results in the
03:20 - highest estimated Q value now this does
03:22 - involve a lot of iterations and that's
03:25 - actually why I need a pretty powerful
03:27 - computer for this project now
03:29 - fortunately I teamed up with MSI for
03:31 - this video and they sent me over the
03:32 - Raider g78 hx1 13v which is an absolute
03:36 - Beast for my deep learning workflow this
03:39 - laptop has a 13th gen I9 24 core
03:43 - processor an RTX 490 32 GB of RAM and a
03:47 - 240 HZ 17-in QHD display having this
03:51 - laptop feels like carrying around a
03:53 - mobile power horse where I can literally
03:55 - do anything I want with it even if I'm
03:57 - sitting in a coffee shop I can load and
03:59 - train massive models using nvidia's Cuda
04:01 - and this RTX 490 now being a power user
04:04 - this is exactly what I need something
04:06 - that gives me the confidence to know
04:08 - that no matter what my workflow is it
04:10 - can handle it in fact pretty soon I'm
04:12 - actually going to be moving overseas and
04:13 - I can't bring my big desktop PC with me
04:16 - now at first I was a bit worried but now
04:18 - that I have this laptop and it can fit
04:20 - right inside my backpack I know that no
04:22 - matter where I am I'll be able to handle
04:24 - any task and complete any project and
04:27 - that means that literally when I'm on a
04:28 - plane I could be trained a machine
04:30 - learning model and getting the same kind
04:31 - of productivity and work done that I
04:33 - would with my big PC now not to mention
04:36 - when I do eventually set this up in some
04:37 - sort of office I can connect multiple
04:39 - monitors and any peripherals I need
04:42 - because of it's HDMI 2.1 Thunderbolt 4
04:45 - ethernet SD card reader Etc I don't even
04:48 - have to bring dongles with me now as I
04:50 - transition to a bit more of a nomatic
04:52 - life this is exactly the type of machine
04:53 - I need something where I have complete
04:55 - confidence in its ability it has all of
04:58 - the Power I could ever want and it means
05:00 - I can get any task done editing gaming
05:03 - deep learning doesn't matter I have the
05:05 - machine that can do that now if you guys
05:06 - want to check out more I'll leave some
05:08 - links in the description this thing is
05:10 - insane and it's got a ton of other
05:11 - features I didn't even touch
05:15 - on so it's been about 30 minutes and I
05:18 - realized that before I can actually
05:19 - start doing any training I need to have
05:21 - a functioning game of blackjack so I
05:23 - just built out a custom blackjack game
05:26 - here that allows you to play against the
05:27 - computer we're going to use this with
05:29 - our model to determine whether or not
05:31 - we're winning losing Etc and sorry not
05:34 - model I mean kind of our Q learning Q
05:36 - table whatever you want to call it
05:38 - anyways if you have a look here on the
05:40 - computer I'll just quickly run through
05:41 - the code you can see I just created a
05:43 - simple class so it be nice and reusable
05:45 - I have all of the different symbols here
05:47 - these are kind of the Unicode symbols
05:49 - for heart spade diamond Etc just to make
05:51 - it look a bit better we generate the
05:53 - deck randomly deal the cards start the
05:57 - game and then we have some methods for
05:58 - figuring out the hand value it's
06:00 - actually not that simple in blackjack
06:02 - because of the aces we have a method for
06:04 - the player action so hitting staying Etc
06:07 - dealer action getting the status of the
06:09 - game so player bust player Blackjack Etc
06:12 - we have the game result and then we have
06:15 - a method for formatting the cards then
06:16 - just a bit of driver code here to
06:18 - actually run it so I'll show you quickly
06:20 - what I have is that if I run the code
06:22 - here it shows what the dealer has and
06:24 - what you have for right now it's just
06:26 - hidden stay no doubling or splitting so
06:29 - so let's hit I get a 15 they have a
06:31 - three so I'm going to stay and the
06:34 - dealer hits and has three 10 and a queen
06:37 - and I win let's play that one more time
06:40 - uh I have a three against a dealer three
06:42 - I'm going to stay and they get 17 and we
06:44 - lose that's usually how it goes anyways
06:47 - I'll be back once I start implementing
06:48 - the Q
06:51 - learning I am back it's been about an
06:53 - hour and a half I've messed around with
06:55 - a few different things here and I've got
06:57 - a somewhat working version here of of
06:59 - the Q table now you can see that I've
07:01 - just tested this on 10,000 games and I
07:05 - trained the Q table on 50,000 games now
07:08 - of those 10,000 games from that 50,000
07:11 - game kind of training sample I have a
07:14 - win rate or number of wins which is
07:16 - about 4,000 about 5,000 losses and
07:20 - 870 draws and that gives me about a 43%
07:24 - win rate the reason why it's 43% is I'm
07:26 - just dividing it out the total wins and
07:28 - losses is kind of avoiding the draws not
07:31 - sure if that's really how you should do
07:32 - the calculation but that's what I've got
07:34 - if we have a look here we can see that
07:36 - this is already making pretty reasonable
07:38 - choices based on blackjack basic
07:40 - strategy now if it was perfect this win
07:43 - rate should be closer to 48.5 49%
07:47 - depending on the rule set but it should
07:49 - be higher than it is right now so if we
07:51 - have a look at a few games here we can
07:52 - see that the dealer showing a six we
07:55 - have a 10 we hit to 17 the dealer has 13
07:59 - the dealer hits and then we actually end
08:01 - up losing now I don't show all of the
08:03 - cards the dealer has when they're
08:04 - hitting I'm just trying to show the
08:06 - player decision here the AI decision in
08:09 - this case dealer shows an ace we have a
08:11 - 13 we hit to 20 we stay unfortunately
08:14 - the dealer has a black check and we lose
08:16 - now let's see dealer has a five we're
08:18 - showing a five we decide to stay and we
08:21 - end up winning I assume the dealer
08:22 - busted here or they had less well I
08:24 - guess they had to bust right cuz we had
08:26 - a 15 continuing dealer shows a two we
08:28 - have a 12 we actually decide to hit
08:30 - which I thought was interesting and we
08:32 - bust and we lose now this one is where
08:34 - we're kind of making some mistakes right
08:36 - dealer shows a two we have a 14 we're
08:38 - hitting to 15 we have a 15 we're hitting
08:40 - again we're on 17 we're hitting again
08:42 - and we're getting 25 so obviously it's
08:44 - not perfect and you can see that it does
08:46 - make some mistakes especially on some of
08:48 - those lower cards like two and three so
08:50 - I'm going to try to tweak this and see
08:52 - if it can get closer to basic strategy
08:54 - so overall pretty decent progress and I
08:56 - almost argue this plays better Blackjack
08:59 - than the most average
09:02 - people all right so it's been about an
09:05 - hour and a half here I've tried a bunch
09:06 - of different things made some
09:08 - modifications to the code and most
09:10 - notably what I've done with the model is
09:12 - really increased the sample size and
09:14 - made it count for soft and hard hands so
09:17 - previously it didn't know the difference
09:19 - between a soft 16 or a hard 16 or
09:21 - whatever soft and hard hand so it was
09:23 - making a lot of weird choices when we
09:25 - had aces now I believe I fixed that
09:28 - problem but I think by doing that I may
09:30 - have introduced some others now really
09:32 - all that to say after doing all of this
09:34 - work I'm getting pretty much the exact
09:36 - same result kind of the story of my life
09:38 - when it comes to machine learning change
09:39 - a bunch of things mess with a bunch of
09:41 - parameters and not being an absolute
09:42 - expert I just get the same or Worse
09:44 - result I'm sure many of you can probably
09:46 - relate to that anyways I'm getting again
09:48 - about a 43% win rate here but this time
09:51 - it's on 5,000 games that was the
09:53 - training uh kind of data set and then 1
09:56 - million games is the testing data set
09:58 - for my wins losses and draws now keep in
10:01 - mind I'm not accounting for the fact
10:03 - that blackjacks pay 3 to2 I don't have
10:05 - rules like double down surrender and
10:07 - split which would increase the player
10:09 - odds if I had those rules I'd probably
10:12 - be doing a lot better but also it would
10:14 - be many more hours I'd be sitting here
10:16 - because the code is well much more
10:17 - complicated to write anyways let's just
10:19 - have a look at a few kind of instances
10:21 - here of where the AI is making some
10:23 - mistakes and where it's doing good so
10:25 - here you can see dealers showing a two I
10:27 - have a 16 soft 16 hitting soft 16 to
10:30 - hard 16 and then hitting to 26 so in
10:34 - that instance obviously that's making a
10:35 - mistake it's correct to hit the soft 16
10:38 - not correct to hit the hard 16 so I
10:40 - think again some improvements could be
10:42 - made there now here dealer has a three
10:44 - we have a 10 hit to 13 and then hit
10:46 - again to 19 now it works out in that
10:48 - instance but that's not the correct play
10:50 - so really what I've noticed is this has
10:52 - a bias to hit more than it should now I
10:54 - could easily fix that by hardcoding a
10:56 - few basic strategy rules in here but
10:59 - that's not what I want to do right I
11:00 - want this to learn purely from
11:01 - reinforcement learning and from trial
11:03 - and error so at this point I'm not
11:05 - exactly sure how to proceed what I will
11:08 - do is quickly show you the code and give
11:10 - you an idea of what I've been doing and
11:12 - then maybe you guys can give me some
11:13 - suggestions in the comments down below
11:15 - and we can do a part two unfortunately I
11:17 - don't have 10 more hours to sit here and
11:19 - mess around otherwise I would try to add
11:21 - those extra rules in and see how much of
11:23 - a difference that makes so if we have a
11:24 - quick look at the code here and by the
11:26 - way I'll leave this available from the
11:27 - link in the description you can see I
11:29 - have my main Blackjack module which is
11:31 - all of the code that we looked at before
11:33 - which essentially simulates one hand of
11:35 - blackjack now we don't play with shoes
11:38 - we use the same cards over and over so
11:40 - it's like we're using a continuous
11:42 - shuffler machine and just every round we
11:44 - reshuffle the cards which is different
11:46 - obviously than real blackjack where you
11:47 - can kind of count the cards and see what
11:49 - you're getting now inside of blackjack Q
11:51 - learning here we start by initializing
11:53 - with three parameters now these are
11:55 - related to the Q learning we have Alpha
11:57 - which is the learning rate gamma which
11:59 - is the discount factor and Epsilon which
12:01 - is essentially the chance of choosing a
12:03 - random action it's how much exploration
12:06 - we're going to do now these three
12:07 - factors are really what make up the core
12:10 - equation for updating the values in the
12:12 - Q table I'll see if I can put that on
12:14 - the screen for you guys right now so you
12:15 - can see what the equation is it's a
12:17 - little bit complex but pretty much Alpha
12:20 - is telling us okay how quickly should we
12:22 - update values based on the result we
12:24 - just got now gamma is telling us how
12:26 - much we should consider future rewards
12:28 - and Epsilon is our chance of kind of
12:30 - exploring or choosing a random choice so
12:33 - rather than always choosing the choice
12:35 - that has the highest Q value sometimes
12:37 - we pick a random choice so the model or
12:39 - the agent has some ability to explore
12:42 - and choose something that maybe hasn't
12:43 - been chosen before but could be the
12:45 - optimal solution these three parameters
12:47 - uh can actually have a huge impact on
12:49 - the model or on the agent I have messed
12:51 - with them a bit but obviously if you
12:53 - were able to find the right relationship
12:54 - here and mess with these a lot then
12:56 - you'd be able to probably get a better
12:57 - model now what we do after that is
12:59 - actually initialize the Q table which is
13:01 - going to have four dimensions the First
13:03 - Dimension is going to represent the
13:04 - player sum then we're going to have
13:06 - whatever the Dealer's up card is and
13:08 - then we're going to have uh the usable
13:11 - Ace so if we have an ace that can be
13:13 - used for soft hand and then we're going
13:15 - to have the action which is either hit
13:16 - or stay if we wanted to add the other
13:18 - decisions like double and split again
13:20 - that's a bit more complicated but we
13:21 - would have to change this last column to
13:23 - four next we have a function that
13:25 - chooses the action using that Epsilon
13:27 - variable so you can pretty much see that
13:28 - we have a 10% chance here of randomly
13:31 - selecting hitter stay whereas otherwise
13:33 - we're just going to see which one in the
13:35 - Q table currently has the most favorable
13:37 - possible reward or output we then have
13:40 - update this is what's actually updating
13:42 - the Q table and you can see we have kind
13:43 - of that Q function here where we're
13:45 - taking the old Value Plus the alpha
13:48 - multiplied by the reward plus the gamma
13:51 - times the future Max minus the old value
13:53 - again a bit complex you don't need to
13:54 - understand it then we have a function
13:56 - here if I can scroll up that tells us us
13:58 - if we have a usable Ace we have a
14:01 - training function and this is the main
14:03 - function where what we're doing is
14:04 - actually simulating the game we're
14:06 - getting the result so we're seeing if we
14:08 - won or if we lost and then we're calling
14:10 - those update functions to actually
14:11 - update the Q table lastly we have this
14:13 - play function here and what this will do
14:15 - is actually use the Q table that we just
14:17 - trained up and has all those values
14:18 - inside of it to determine what choice we
14:21 - should make based on the card that were
14:22 - given so you can see that I was using
14:24 - play after I did all of the training and
14:26 - we had that Q table in memory I could
14:28 - store the Q table as well that way I
14:31 - don't have to train every time I
14:32 - actually want to play the game so that's
14:34 - pretty much it I wish I could spend some
14:35 - more time and improve this but
14:37 - unfortunately I don't have more time to
14:38 - film right now let me know what you guys
14:40 - think in the comments down below and how
14:42 - I could potentially improve this model I
14:44 - think for a first shot pretty decent
14:46 - it's working fairly well and it gave me
14:48 - a chance to kind of refresh myself with
14:50 - Q learning and hopefully taught you a
14:51 - little bit about how that worked and
14:53 - gave you a realistic view of what it's
14:55 - like to work on a project after all
14:57 - that's kind of the goal this video here
14:59 - just to show you a bit of my process and
15:01 - the fact that I don't always get it
15:02 - right the first time many people see
15:04 - those finalized tutorials online and
15:06 - they think that's always what happens I
15:08 - can't tell you how many times I fail how
15:10 - many times I try to work on a project
15:12 - that just doesn't work and how many
15:14 - pieces of code essentially go in the
15:15 - garbage because I had no idea what I was
15:17 - doing kind of like this anyways if you
15:20 - guys enjoyed make sure to leave a like
15:21 - subscribe to the channel and I will see
15:23 - you in the next
15:24 - [Music]
15:27 - one
15:31 - oh

Cleaned transcript:

some of my favorite projects involve training in AI how to play a game I've done this for Flappy Bird chess pong Checkers but today I'm taking on Blackjack now this is an interesting game because we already know how to play it optimally so I'm interested to see if the AI can learn the rules on its own I'm not going to hardcode it I'm going to see if it can play enough games to figure out the optimal strategy and to even beat the casino now by no means am I encouraging gambling here this is just a fun game and the math behind all these casino games has always been interesting to me in this video I'm going to work on this project completely from scratch and I'm going to bring you along for the ride with that said let's dive in so first things first I got to figure out what to do I know there's a bunch of different approaches and right now I'm thinking that I could use something like neat which is neuro evolution of augment topologies I could try to build a neural network or I could go for something like reinforcement learning I'm going to do a bit of research consult the good old chat GPT see what it tells me and report back so I've been doing a bit of research here let me share with you what I found so I asked chat gbt I want to train in AI how to play Blackjack what's the best way to do that these are kind of the methods I know it walked through a bunch of them I'm not going to bore you with all the results but pretty much what I was thinking is somewhat correct we can use a neural network but that's a little bit complex and we're going to kind of mix that in with something known as Q learning now if we go down here Q learning is probably the most basic approach that we can use and what this essentially means is we're going to generate a massive State table that has every single possible result or every single state that we could have in blackj just like those really popular cards you see where it shows your card value or your hand total versus the dealer card and what kind of action you should take so I asked chat gbt explain a bit more about Q learning just so I could kind of refresh my memory on how it worked and and then I asked it to give me a bit of a visualization just so that I could show you guys and I can see for myself exactly what it was talking about here you can see that we have a pretty simple table where we have the sum of our hand we have the Dealer's up card we have if we have a usable Ace or not and then we have the action that we'll take which is hit or stand I think I'm going to start with just hit or stand and then as we get a bit more complex I'm going to start adding in doubling splitting surrendering and a lot of the other more advanced rules in blackjack now what's going to happen is we're we're going to initialize this table with all of these different states and we're going to assign them something known as a q value now the Q value is the predicted result or the predicted outcome based on a specific action from the state before so what we'll do is we'll say hit a 12 versus a two when we hit that we'll either win or lose depending on the result we'll update the Q value and we'll continue to do this Millions upon millions of times eventually once we played enough hands of blackjack the computer will start to figure out which combinations or which states should take what specific action it'll do that by looking at the Q value so later we'll just have this massive table that will have all of the different combinations all of the actions we can take and the expected result for those actions and we'll just pick the action the results in the highest estimated Q value now this does involve a lot of iterations and that's actually why I need a pretty powerful computer for this project now fortunately I teamed up with MSI for this video and they sent me over the Raider g78 hx1 13v which is an absolute Beast for my deep learning workflow this laptop has a 13th gen I9 24 core processor an RTX 490 32 GB of RAM and a 240 HZ 17in QHD display having this laptop feels like carrying around a mobile power horse where I can literally do anything I want with it even if I'm sitting in a coffee shop I can load and train massive models using nvidia's Cuda and this RTX 490 now being a power user this is exactly what I need something that gives me the confidence to know that no matter what my workflow is it can handle it in fact pretty soon I'm actually going to be moving overseas and I can't bring my big desktop PC with me now at first I was a bit worried but now that I have this laptop and it can fit right inside my backpack I know that no matter where I am I'll be able to handle any task and complete any project and that means that literally when I'm on a plane I could be trained a machine learning model and getting the same kind of productivity and work done that I would with my big PC now not to mention when I do eventually set this up in some sort of office I can connect multiple monitors and any peripherals I need because of it's HDMI 2.1 Thunderbolt 4 ethernet SD card reader Etc I don't even have to bring dongles with me now as I transition to a bit more of a nomatic life this is exactly the type of machine I need something where I have complete confidence in its ability it has all of the Power I could ever want and it means I can get any task done editing gaming deep learning doesn't matter I have the machine that can do that now if you guys want to check out more I'll leave some links in the description this thing is insane and it's got a ton of other features I didn't even touch on so it's been about 30 minutes and I realized that before I can actually start doing any training I need to have a functioning game of blackjack so I just built out a custom blackjack game here that allows you to play against the computer we're going to use this with our model to determine whether or not we're winning losing Etc and sorry not model I mean kind of our Q learning Q table whatever you want to call it anyways if you have a look here on the computer I'll just quickly run through the code you can see I just created a simple class so it be nice and reusable I have all of the different symbols here these are kind of the Unicode symbols for heart spade diamond Etc just to make it look a bit better we generate the deck randomly deal the cards start the game and then we have some methods for figuring out the hand value it's actually not that simple in blackjack because of the aces we have a method for the player action so hitting staying Etc dealer action getting the status of the game so player bust player Blackjack Etc we have the game result and then we have a method for formatting the cards then just a bit of driver code here to actually run it so I'll show you quickly what I have is that if I run the code here it shows what the dealer has and what you have for right now it's just hidden stay no doubling or splitting so so let's hit I get a 15 they have a three so I'm going to stay and the dealer hits and has three 10 and a queen and I win let's play that one more time uh I have a three against a dealer three I'm going to stay and they get 17 and we lose that's usually how it goes anyways I'll be back once I start implementing the Q learning I am back it's been about an hour and a half I've messed around with a few different things here and I've got a somewhat working version here of of the Q table now you can see that I've just tested this on 10,000 games and I trained the Q table on 50,000 games now of those 10,000 games from that 50,000 game kind of training sample I have a win rate or number of wins which is about 4,000 about 5,000 losses and 870 draws and that gives me about a 43% win rate the reason why it's 43% is I'm just dividing it out the total wins and losses is kind of avoiding the draws not sure if that's really how you should do the calculation but that's what I've got if we have a look here we can see that this is already making pretty reasonable choices based on blackjack basic strategy now if it was perfect this win rate should be closer to 48.5 49% depending on the rule set but it should be higher than it is right now so if we have a look at a few games here we can see that the dealer showing a six we have a 10 we hit to 17 the dealer has 13 the dealer hits and then we actually end up losing now I don't show all of the cards the dealer has when they're hitting I'm just trying to show the player decision here the AI decision in this case dealer shows an ace we have a 13 we hit to 20 we stay unfortunately the dealer has a black check and we lose now let's see dealer has a five we're showing a five we decide to stay and we end up winning I assume the dealer busted here or they had less well I guess they had to bust right cuz we had a 15 continuing dealer shows a two we have a 12 we actually decide to hit which I thought was interesting and we bust and we lose now this one is where we're kind of making some mistakes right dealer shows a two we have a 14 we're hitting to 15 we have a 15 we're hitting again we're on 17 we're hitting again and we're getting 25 so obviously it's not perfect and you can see that it does make some mistakes especially on some of those lower cards like two and three so I'm going to try to tweak this and see if it can get closer to basic strategy so overall pretty decent progress and I almost argue this plays better Blackjack than the most average people all right so it's been about an hour and a half here I've tried a bunch of different things made some modifications to the code and most notably what I've done with the model is really increased the sample size and made it count for soft and hard hands so previously it didn't know the difference between a soft 16 or a hard 16 or whatever soft and hard hand so it was making a lot of weird choices when we had aces now I believe I fixed that problem but I think by doing that I may have introduced some others now really all that to say after doing all of this work I'm getting pretty much the exact same result kind of the story of my life when it comes to machine learning change a bunch of things mess with a bunch of parameters and not being an absolute expert I just get the same or Worse result I'm sure many of you can probably relate to that anyways I'm getting again about a 43% win rate here but this time it's on 5,000 games that was the training uh kind of data set and then 1 million games is the testing data set for my wins losses and draws now keep in mind I'm not accounting for the fact that blackjacks pay 3 to2 I don't have rules like double down surrender and split which would increase the player odds if I had those rules I'd probably be doing a lot better but also it would be many more hours I'd be sitting here because the code is well much more complicated to write anyways let's just have a look at a few kind of instances here of where the AI is making some mistakes and where it's doing good so here you can see dealers showing a two I have a 16 soft 16 hitting soft 16 to hard 16 and then hitting to 26 so in that instance obviously that's making a mistake it's correct to hit the soft 16 not correct to hit the hard 16 so I think again some improvements could be made there now here dealer has a three we have a 10 hit to 13 and then hit again to 19 now it works out in that instance but that's not the correct play so really what I've noticed is this has a bias to hit more than it should now I could easily fix that by hardcoding a few basic strategy rules in here but that's not what I want to do right I want this to learn purely from reinforcement learning and from trial and error so at this point I'm not exactly sure how to proceed what I will do is quickly show you the code and give you an idea of what I've been doing and then maybe you guys can give me some suggestions in the comments down below and we can do a part two unfortunately I don't have 10 more hours to sit here and mess around otherwise I would try to add those extra rules in and see how much of a difference that makes so if we have a quick look at the code here and by the way I'll leave this available from the link in the description you can see I have my main Blackjack module which is all of the code that we looked at before which essentially simulates one hand of blackjack now we don't play with shoes we use the same cards over and over so it's like we're using a continuous shuffler machine and just every round we reshuffle the cards which is different obviously than real blackjack where you can kind of count the cards and see what you're getting now inside of blackjack Q learning here we start by initializing with three parameters now these are related to the Q learning we have Alpha which is the learning rate gamma which is the discount factor and Epsilon which is essentially the chance of choosing a random action it's how much exploration we're going to do now these three factors are really what make up the core equation for updating the values in the Q table I'll see if I can put that on the screen for you guys right now so you can see what the equation is it's a little bit complex but pretty much Alpha is telling us okay how quickly should we update values based on the result we just got now gamma is telling us how much we should consider future rewards and Epsilon is our chance of kind of exploring or choosing a random choice so rather than always choosing the choice that has the highest Q value sometimes we pick a random choice so the model or the agent has some ability to explore and choose something that maybe hasn't been chosen before but could be the optimal solution these three parameters uh can actually have a huge impact on the model or on the agent I have messed with them a bit but obviously if you were able to find the right relationship here and mess with these a lot then you'd be able to probably get a better model now what we do after that is actually initialize the Q table which is going to have four dimensions the First Dimension is going to represent the player sum then we're going to have whatever the Dealer's up card is and then we're going to have uh the usable Ace so if we have an ace that can be used for soft hand and then we're going to have the action which is either hit or stay if we wanted to add the other decisions like double and split again that's a bit more complicated but we would have to change this last column to four next we have a function that chooses the action using that Epsilon variable so you can pretty much see that we have a 10% chance here of randomly selecting hitter stay whereas otherwise we're just going to see which one in the Q table currently has the most favorable possible reward or output we then have update this is what's actually updating the Q table and you can see we have kind of that Q function here where we're taking the old Value Plus the alpha multiplied by the reward plus the gamma times the future Max minus the old value again a bit complex you don't need to understand it then we have a function here if I can scroll up that tells us us if we have a usable Ace we have a training function and this is the main function where what we're doing is actually simulating the game we're getting the result so we're seeing if we won or if we lost and then we're calling those update functions to actually update the Q table lastly we have this play function here and what this will do is actually use the Q table that we just trained up and has all those values inside of it to determine what choice we should make based on the card that were given so you can see that I was using play after I did all of the training and we had that Q table in memory I could store the Q table as well that way I don't have to train every time I actually want to play the game so that's pretty much it I wish I could spend some more time and improve this but unfortunately I don't have more time to film right now let me know what you guys think in the comments down below and how I could potentially improve this model I think for a first shot pretty decent it's working fairly well and it gave me a chance to kind of refresh myself with Q learning and hopefully taught you a little bit about how that worked and gave you a realistic view of what it's like to work on a project after all that's kind of the goal this video here just to show you a bit of my process and the fact that I don't always get it right the first time many people see those finalized tutorials online and they think that's always what happens I can't tell you how many times I fail how many times I try to work on a project that just doesn't work and how many pieces of code essentially go in the garbage because I had no idea what I was doing kind of like this anyways if you guys enjoyed make sure to leave a like subscribe to the channel and I will see you in the next one oh
