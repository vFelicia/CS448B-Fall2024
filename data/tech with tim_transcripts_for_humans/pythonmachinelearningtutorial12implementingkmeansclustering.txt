With timestamps:

00:00 - hey guys and welcome back to the machine
00:02 - learning tutorial with Python so in
00:04 - today's video we're going to be
00:05 - finishing up with k-means this will be a
00:07 - quick video we're just going to do a
00:08 - really simple implementation of k-means
00:10 - I'm kinda gonna leave you on a bit of a
00:12 - cliffhanger here in terms of I'm not
00:14 - going to explain everything I do in here
00:15 - I'm gonna give you kind of some further
00:17 - reading for those of you that are
00:18 - interested in looking at that and then
00:20 - as we move into more unsupervised
00:22 - learning algorithms so as I get into
00:25 - neural networks which will be another
00:26 - series coming soon excuse me that I'll
00:29 - be talking more about specifically what
00:31 - some of the accuracy measures mean for
00:34 - this data set because unlike other data
00:37 - sets actually more difficult to kind of
00:39 - test this for accuracy and validity so
00:42 - you'll see that in just a second so
00:44 - essentially these are my imports here
00:46 - again all this stuff will be up on Tech
00:47 - with Tim net if you guys want to copy
00:49 - that code the code that I showed in the
00:50 - last video that ran all of the woody
00:54 - code like that image stuff it'll be up
00:55 - on tech with Tim done that as well that
00:56 - you guys can just go ahead and copy with
00:58 - that so go ahead
00:59 - link with me the description and you can
01:01 - copy the code from there if you don't
01:02 - want to type it out with me but
01:03 - essentially we're gonna load in our data
01:05 - so we've got low digits here from SK
01:07 - learn this is from datasets same thing
01:10 - as like loaned what did we load before I
01:12 - forget which one we load it before we
01:14 - use some kind of a scalar and data set I
01:16 - forget what it was but essentially we
01:17 - use that so we'll do is let's say digits
01:19 - equals load underscore data and now load
01:25 - digits is what it's called isn't it load
01:28 - digits and now what we're gonna do is
01:30 - we're gonna say data equals scale talk
01:34 - about this in a second and we'll say
01:36 - digits dot data so essentially this dot
01:40 - data part right is all of our features
01:43 - so we're gonna scale all of our features
01:45 - down so that they're within the value
01:48 - negative one and one and the reason we
01:50 - do that is because our digits by default
01:52 - are gonna have large values I believe
01:53 - it's an RGB value or might be like a
01:56 - grayscale value I honestly don't know
01:57 - but they're gonna be large so by scaling
02:00 - them down we're gonna save time on the
02:02 - computations especially cuz we're doing
02:04 - including distance between points so
02:06 - having smaller values will just be
02:07 - better and it'll lead kind of to less
02:09 - outliers and all that all right so it'll
02:11 - make things faster essentially as we're
02:12 - doing the scan
02:14 - now we're gonna get our our labels so
02:16 - what we'll do is say y equals data dot
02:19 - targets like that target or targets I
02:23 - want to say it's target's but we'll see
02:25 - anyways so we've got our targets now and
02:27 - what I'm gonna do now is just set the
02:29 - amount of clusters that we're gonna look
02:30 - for the amount of centroids to make so
02:33 - we could do this fancy thing or we do
02:35 - like NP dot unique and we get of data
02:39 - targets of Y and we do the length of
02:43 - that and it would be like the dynamic
02:44 - way to do it if we were going to change
02:47 - this data set or we could just type 10
02:49 - because we're gonna have 10 digits so I
02:51 - mean feel free I just want to show you
02:52 - that way in case you want to see how you
02:54 - get it the amount of different classes
02:56 - or like classifications for data set
02:59 - dynamically you can use what I just did
03:00 - so now what we're gonna do is we're
03:03 - gonna get the amount of instances like
03:04 - the amount of numbers that we have that
03:06 - we're gonna classify and what do you
03:09 - call the amount of features that go
03:10 - along with that data so to do this I'm
03:12 - just gonna say samples features think
03:17 - that's how you spell features is equal
03:19 - to we'll say data dot shape the reason
03:22 - this works is because we have shape it
03:24 - kind of looks something like this like
03:25 - it'll look like let's say we have like a
03:26 - thousand instances and like it's by 728
03:29 - then it'll just decompose this for us
03:31 - and the side we can do that in Python
03:32 - pretty straightforward you guys probably
03:34 - already know what and now what we're
03:36 - gonna do is we're actually I'm gonna
03:37 - bring in a function that we used in the
03:41 - last video you saw when I did all that
03:43 - kind of the images were popping up and
03:46 - all that and we saw like the centroids
03:47 - and it was like a nice graph on that
03:48 - plot lib well they have a really nice
03:50 - way of scoring this and you know I could
03:53 - come up with my own way to score this
03:54 - and do the accuracy but why don't we
03:56 - just take it straight from SK learn as
03:58 - that's the module reusing so I'm just
04:00 - gonna copy this in and we'll talk about
04:02 - kind of what this is doing it's it's a
04:04 - big function just be aware okay so
04:07 - essentially this actually reminds me now
04:09 - I have to import metrics from SK learn
04:11 - so say from SK learn import metrics and
04:16 - SK learn has a bunch of functions in
04:19 - there that will automatically score are
04:22 - like supervised learning or unsupervised
04:24 - learning algorithms now we can see that
04:26 - we
04:27 - ton of different ones here it's a
04:28 - completeness score V measure score
04:30 - adjusted ran score mutual info
04:32 - silhouette score and honesty I don't
04:35 - know what all of them do there's a lot
04:37 - of them here all I know is kind of the
04:39 - range to what you're looking for for
04:41 - some of these different scores because
04:43 - they have like a crazy math background
04:45 - behind how they score the model and get
04:48 - like the best accuracy and all of them
04:49 - represent kind of a different thing now
04:52 - essentially what we do um here right is
04:56 - we have this bench k-means and we're
04:58 - gonna create a classifier down here I'm
04:59 - just gonna call this function on our
05:01 - classifier it's gonna print out all this
05:04 - information well this allows us to do
05:05 - essentially is train like a ton of
05:07 - different classifiers and just score
05:08 - them by calling this function so
05:10 - essentially what we do is we give it the
05:12 - classifier it's gonna fit our data which
05:14 - is another argument to that classifier
05:17 - and then it's just gonna use a bunch of
05:19 - different things to score it
05:21 - so essentially forward like homogeneity
05:23 - score I think that's how you say it
05:25 - we're gonna take our Y values which are
05:27 - up here right so all our targets we're
05:29 - gonna compare them to the labels that
05:31 - are estimated gave for each of our data
05:34 - now remember because this is an
05:36 - unsupervised learning algorithm and we
05:38 - don't give it the Y values when we train
05:40 - it automatically generates a Y value for
05:43 - every single test data point that we
05:44 - give it so we don't actually have to
05:46 - split into test and training data
05:48 - because well it never it doesn't know to
05:50 - start what our test data is so we can
05:53 - actually just compare the test data
05:55 - labels to what our estimator or our
05:58 - classifier estimated right like what it
06:00 - predicted each label was and that allows
06:03 - us to kind of train on maybe less data
06:05 - per se because we don't need to have
06:08 - like that training data testing data I
06:09 - do like that split whatever split test
06:12 - train thing that we used in all the
06:14 - other videos so that's good to know for
06:16 - this metric here all this is doing
06:18 - essentially is we're just saying that
06:20 - when we do like the silhouette score
06:22 - we're gonna use Euclidean distance and
06:24 - that's just like the absolute distance
06:25 - between two points or two vectors in a
06:28 - space there's some other distances that
06:30 - we could mess around with but we're just
06:31 - gonna use Euclidian for now so to make
06:33 - our classifier we're gonna say
06:34 - classifier equals k-means now this takes
06:38 - a few different per
06:40 - is this incorrect is it a capital M yes
06:43 - okay so for this classifier we need to
06:46 - define first of all them as centroids we
06:49 - need to give it the amount of times
06:50 - we're gonna actually haven't run the
06:51 - classifier we can give it a max one
06:53 - iterations there's there's a ton of
06:54 - different parameters and I'll actually
06:55 - I'll show you here so if I go to this
06:58 - one you can read through like all of the
07:00 - different parameters and they kind of go
07:02 - like this okay so the first one we're
07:04 - gonna do is n underscore clusters and
07:06 - this is just gonna be set equal to
07:08 - however many things we're trying to
07:09 - classify right so for the clusters we'll
07:11 - say under square clusters and this is
07:13 - gonna be the same as like the amount of
07:14 - centroids essentially we'll say is equal
07:16 - to K and that's what we've defined up
07:18 - here as ten okay what else do we need
07:20 - let's go back here and I'm just gonna
07:22 - read these off because obviously don't
07:23 - remember all of them an it okay so what
07:25 - this will do is remember how I was
07:27 - saying we can have our centroids like
07:29 - those little X's in random positions
07:32 - when we generate them well that is
07:34 - correct but we can also have them in
07:36 - somewhat of them more somewhat of in a
07:40 - way that makes a bit more sense and I
07:42 - don't know exactly how it does it
07:43 - mathematically but I'm pretty sure it
07:45 - just lays them out so they're like equal
07:47 - distance from each other on the grid or
07:49 - on in the space and we can do that if we
07:51 - just set k-means plus plus so you can
07:54 - play around with either choosing random
07:56 - or k-means plus plus and see if you're
07:58 - getting a better classifier it shouldn't
08:00 - make a massive difference but k-means
08:02 - plus plus essentially is just gonna
08:03 - change the location of your initial
08:06 - centroids so that maybe it runs a bit
08:08 - faster you don't have to iterate as many
08:09 - times so for a net I'm actually just
08:11 - gonna use random for right now it
08:13 - doesn't really matter that much and an
08:15 - internet I think it's a net equals a
08:18 - random okay so let's go back here so
08:21 - what else do we need n in it okay so
08:23 - this is the amount of times we're gonna
08:25 - run the algorithm so what actually is
08:26 - gonna happen is cuz we're randomly or
08:29 - because sorry we are randomly kind of
08:31 - placing these centroids we might
08:34 - sometimes get a better result by running
08:36 - the algorithm with a different random
08:39 - location for the centroid so it's
08:40 - essentially what this is saying
08:41 - all right saying the number of times
08:42 - k-means algorithm will run with
08:43 - different centroid seeds that's like how
08:46 - many times are gonna randomly generate
08:47 - the centroids for the first iteration so
08:50 - we can run this ten times and then
08:53 - essentially it's going to
08:53 - take the best one and that's gonna be
08:55 - our classifier I hope that makes sense
08:57 - to you so for n in it we can set it
09:00 - equal to ten just so we kind of have
09:02 - this here that's the default value you
09:04 - can increase it obviously if you
09:05 - increase it it's gonna take longer if
09:06 - you decrease it it's gonna be shorter
09:08 - max iterations so I recommend you just
09:11 - leave this as 300 but essentially
09:13 - remember I was saying we're gonna
09:14 - continually keep repeating the process
09:15 - until eventually nothing changes well to
09:19 - do that could take a very long time
09:21 - especially if we have a ton of data so
09:23 - this is automatically gonna cap us at
09:25 - 300 iterations now if you have if you
09:28 - want the best possible classifier and
09:31 - you want to make sure that it doesn't
09:33 - doesn't matter our time how much time it
09:34 - takes you can set this to like actually
09:36 - I don't know if there's like an infinite
09:38 - number but I think you just said it's
09:40 - like a very high value and hopefully it
09:42 - never even gets to that because it'll
09:43 - just have like a perfect classifier by
09:45 - that point does that make sense okay so
09:48 - these ones now we're kind of going into
09:49 - some more super like hyper parameters
09:52 - that I'm not really gonna talk about
09:53 - verbose like if you guys want to read
09:55 - through this I'll I have all the links
09:56 - on my website and in the description so
09:58 - you can see but essentially that's all
09:59 - we kind of need for our classifier so
10:02 - now we will pass this actually to bench
10:04 - k-means so we'll say bench k-means we'll
10:07 - give it our classifier which can be CLF
10:09 - we'll give it the name which I'm just
10:11 - gonna say is one in this case and we'll
10:12 - give it our data which is just called
10:14 - data and now if I don't know if I have
10:17 - this as the right configuration right
10:18 - now let's edit this k-means tutorial I
10:21 - believe it's working file yes it is so
10:23 - it'll apply that and let's just run this
10:25 - and see what we're getting MP dot an
10:28 - array object has no attribute targets
10:31 - pretty sure its target let's try this y
10:35 - equals target data this I think it might
10:39 - be digits data did just saw targets
10:43 - let's try this one no targets let's try
10:46 - target sorry guys no actually target
10:50 - okay give me one second yeah so I
10:53 - probably help if I spell target
10:55 - correctly Wow all right so target and in
10:59 - it go an unexpected keyword and
11:00 - underscore cluster so I believe that
11:02 - should be clusters and there we go okay
11:07 - so
11:07 - awesome so now it's printing out all of
11:09 - our accuracy scores for us okay so we
11:12 - have six nine four one seven which
11:14 - actually I believe is just giving us ya
11:15 - the inertia okay and then we have all of
11:19 - these different scores which are will
11:20 - represent like homogeneity completeness
11:22 - V measure adjusted R and all that now
11:25 - I'm not gonna talk about what these mean
11:26 - essentially the higher the better for
11:28 - most of them not all of them but if you
11:30 - want to read and I recommend you do I'll
11:32 - leave this link in the description and
11:34 - it essentially goes through what all of
11:36 - the different scores mean it'll give you
11:38 - like a mathematical like derive the
11:41 - mathematic mathematical equations for
11:43 - you and you can kind of look at that and
11:45 - it's pretty interesting so I'm not gonna
11:46 - talk about that we will do that in the
11:48 - neural networks we'll talk about what
11:49 - all these mean but for right now I'll
11:51 - leave the link if you guys want to read
11:52 - that and that's gonna be it for this
11:54 - machine running tutorial if you guys
11:55 - enjoyed these tutorials please make sure
11:57 - you let me know in the comments join my
11:59 - Twitter or join my discord and neural
12:02 - networks will be coming soon in the
12:03 - meantime I'm probably thinking about
12:04 - doing some kind of discord bought
12:06 - tutorial maybe we'll do some Kibby app
12:08 - development let me know what you guys
12:10 - want to see down below and with that
12:11 - being said I'll see you again in another
12:12 - video
12:13 - [Music]

Cleaned transcript:

hey guys and welcome back to the machine learning tutorial with Python so in today's video we're going to be finishing up with kmeans this will be a quick video we're just going to do a really simple implementation of kmeans I'm kinda gonna leave you on a bit of a cliffhanger here in terms of I'm not going to explain everything I do in here I'm gonna give you kind of some further reading for those of you that are interested in looking at that and then as we move into more unsupervised learning algorithms so as I get into neural networks which will be another series coming soon excuse me that I'll be talking more about specifically what some of the accuracy measures mean for this data set because unlike other data sets actually more difficult to kind of test this for accuracy and validity so you'll see that in just a second so essentially these are my imports here again all this stuff will be up on Tech with Tim net if you guys want to copy that code the code that I showed in the last video that ran all of the woody code like that image stuff it'll be up on tech with Tim done that as well that you guys can just go ahead and copy with that so go ahead link with me the description and you can copy the code from there if you don't want to type it out with me but essentially we're gonna load in our data so we've got low digits here from SK learn this is from datasets same thing as like loaned what did we load before I forget which one we load it before we use some kind of a scalar and data set I forget what it was but essentially we use that so we'll do is let's say digits equals load underscore data and now load digits is what it's called isn't it load digits and now what we're gonna do is we're gonna say data equals scale talk about this in a second and we'll say digits dot data so essentially this dot data part right is all of our features so we're gonna scale all of our features down so that they're within the value negative one and one and the reason we do that is because our digits by default are gonna have large values I believe it's an RGB value or might be like a grayscale value I honestly don't know but they're gonna be large so by scaling them down we're gonna save time on the computations especially cuz we're doing including distance between points so having smaller values will just be better and it'll lead kind of to less outliers and all that all right so it'll make things faster essentially as we're doing the scan now we're gonna get our our labels so what we'll do is say y equals data dot targets like that target or targets I want to say it's target's but we'll see anyways so we've got our targets now and what I'm gonna do now is just set the amount of clusters that we're gonna look for the amount of centroids to make so we could do this fancy thing or we do like NP dot unique and we get of data targets of Y and we do the length of that and it would be like the dynamic way to do it if we were going to change this data set or we could just type 10 because we're gonna have 10 digits so I mean feel free I just want to show you that way in case you want to see how you get it the amount of different classes or like classifications for data set dynamically you can use what I just did so now what we're gonna do is we're gonna get the amount of instances like the amount of numbers that we have that we're gonna classify and what do you call the amount of features that go along with that data so to do this I'm just gonna say samples features think that's how you spell features is equal to we'll say data dot shape the reason this works is because we have shape it kind of looks something like this like it'll look like let's say we have like a thousand instances and like it's by 728 then it'll just decompose this for us and the side we can do that in Python pretty straightforward you guys probably already know what and now what we're gonna do is we're actually I'm gonna bring in a function that we used in the last video you saw when I did all that kind of the images were popping up and all that and we saw like the centroids and it was like a nice graph on that plot lib well they have a really nice way of scoring this and you know I could come up with my own way to score this and do the accuracy but why don't we just take it straight from SK learn as that's the module reusing so I'm just gonna copy this in and we'll talk about kind of what this is doing it's it's a big function just be aware okay so essentially this actually reminds me now I have to import metrics from SK learn so say from SK learn import metrics and SK learn has a bunch of functions in there that will automatically score are like supervised learning or unsupervised learning algorithms now we can see that we ton of different ones here it's a completeness score V measure score adjusted ran score mutual info silhouette score and honesty I don't know what all of them do there's a lot of them here all I know is kind of the range to what you're looking for for some of these different scores because they have like a crazy math background behind how they score the model and get like the best accuracy and all of them represent kind of a different thing now essentially what we do um here right is we have this bench kmeans and we're gonna create a classifier down here I'm just gonna call this function on our classifier it's gonna print out all this information well this allows us to do essentially is train like a ton of different classifiers and just score them by calling this function so essentially what we do is we give it the classifier it's gonna fit our data which is another argument to that classifier and then it's just gonna use a bunch of different things to score it so essentially forward like homogeneity score I think that's how you say it we're gonna take our Y values which are up here right so all our targets we're gonna compare them to the labels that are estimated gave for each of our data now remember because this is an unsupervised learning algorithm and we don't give it the Y values when we train it automatically generates a Y value for every single test data point that we give it so we don't actually have to split into test and training data because well it never it doesn't know to start what our test data is so we can actually just compare the test data labels to what our estimator or our classifier estimated right like what it predicted each label was and that allows us to kind of train on maybe less data per se because we don't need to have like that training data testing data I do like that split whatever split test train thing that we used in all the other videos so that's good to know for this metric here all this is doing essentially is we're just saying that when we do like the silhouette score we're gonna use Euclidean distance and that's just like the absolute distance between two points or two vectors in a space there's some other distances that we could mess around with but we're just gonna use Euclidian for now so to make our classifier we're gonna say classifier equals kmeans now this takes a few different per is this incorrect is it a capital M yes okay so for this classifier we need to define first of all them as centroids we need to give it the amount of times we're gonna actually haven't run the classifier we can give it a max one iterations there's there's a ton of different parameters and I'll actually I'll show you here so if I go to this one you can read through like all of the different parameters and they kind of go like this okay so the first one we're gonna do is n underscore clusters and this is just gonna be set equal to however many things we're trying to classify right so for the clusters we'll say under square clusters and this is gonna be the same as like the amount of centroids essentially we'll say is equal to K and that's what we've defined up here as ten okay what else do we need let's go back here and I'm just gonna read these off because obviously don't remember all of them an it okay so what this will do is remember how I was saying we can have our centroids like those little X's in random positions when we generate them well that is correct but we can also have them in somewhat of them more somewhat of in a way that makes a bit more sense and I don't know exactly how it does it mathematically but I'm pretty sure it just lays them out so they're like equal distance from each other on the grid or on in the space and we can do that if we just set kmeans plus plus so you can play around with either choosing random or kmeans plus plus and see if you're getting a better classifier it shouldn't make a massive difference but kmeans plus plus essentially is just gonna change the location of your initial centroids so that maybe it runs a bit faster you don't have to iterate as many times so for a net I'm actually just gonna use random for right now it doesn't really matter that much and an internet I think it's a net equals a random okay so let's go back here so what else do we need n in it okay so this is the amount of times we're gonna run the algorithm so what actually is gonna happen is cuz we're randomly or because sorry we are randomly kind of placing these centroids we might sometimes get a better result by running the algorithm with a different random location for the centroid so it's essentially what this is saying all right saying the number of times kmeans algorithm will run with different centroid seeds that's like how many times are gonna randomly generate the centroids for the first iteration so we can run this ten times and then essentially it's going to take the best one and that's gonna be our classifier I hope that makes sense to you so for n in it we can set it equal to ten just so we kind of have this here that's the default value you can increase it obviously if you increase it it's gonna take longer if you decrease it it's gonna be shorter max iterations so I recommend you just leave this as 300 but essentially remember I was saying we're gonna continually keep repeating the process until eventually nothing changes well to do that could take a very long time especially if we have a ton of data so this is automatically gonna cap us at 300 iterations now if you have if you want the best possible classifier and you want to make sure that it doesn't doesn't matter our time how much time it takes you can set this to like actually I don't know if there's like an infinite number but I think you just said it's like a very high value and hopefully it never even gets to that because it'll just have like a perfect classifier by that point does that make sense okay so these ones now we're kind of going into some more super like hyper parameters that I'm not really gonna talk about verbose like if you guys want to read through this I'll I have all the links on my website and in the description so you can see but essentially that's all we kind of need for our classifier so now we will pass this actually to bench kmeans so we'll say bench kmeans we'll give it our classifier which can be CLF we'll give it the name which I'm just gonna say is one in this case and we'll give it our data which is just called data and now if I don't know if I have this as the right configuration right now let's edit this kmeans tutorial I believe it's working file yes it is so it'll apply that and let's just run this and see what we're getting MP dot an array object has no attribute targets pretty sure its target let's try this y equals target data this I think it might be digits data did just saw targets let's try this one no targets let's try target sorry guys no actually target okay give me one second yeah so I probably help if I spell target correctly Wow all right so target and in it go an unexpected keyword and underscore cluster so I believe that should be clusters and there we go okay so awesome so now it's printing out all of our accuracy scores for us okay so we have six nine four one seven which actually I believe is just giving us ya the inertia okay and then we have all of these different scores which are will represent like homogeneity completeness V measure adjusted R and all that now I'm not gonna talk about what these mean essentially the higher the better for most of them not all of them but if you want to read and I recommend you do I'll leave this link in the description and it essentially goes through what all of the different scores mean it'll give you like a mathematical like derive the mathematic mathematical equations for you and you can kind of look at that and it's pretty interesting so I'm not gonna talk about that we will do that in the neural networks we'll talk about what all these mean but for right now I'll leave the link if you guys want to read that and that's gonna be it for this machine running tutorial if you guys enjoyed these tutorials please make sure you let me know in the comments join my Twitter or join my discord and neural networks will be coming soon in the meantime I'm probably thinking about doing some kind of discord bought tutorial maybe we'll do some Kibby app development let me know what you guys want to see down below and with that being said I'll see you again in another video
