With timestamps:

00:00 - hey guys and welcome back to another
00:02 - neural network tutorial now in today's
00:04 - video what we're gonna be doing is
00:05 - talking about text classification with
00:08 - tensorflow 2.0 now what I'm gonna be
00:11 - doing just to be full fully transparent
00:12 - with you guys here is following along
00:14 - with the actual official tutorials on
00:16 - the tensorflow 2.0 tutorial now I find
00:19 - that these are actually the best in
00:21 - terms of like kind of a structure to
00:23 - start with to understand very basic
00:25 - neural networks for some pretty simple
00:27 - tasks I would say and then we're gonna
00:29 - stray away from those we're gonna start
00:30 - using our own data our own networks our
00:32 - own architecture and we'll start talking
00:34 - about kind of some of the issues you
00:35 - have when you actually start applying
00:37 - these to real data so so far you guys
00:39 - have noticed and I've seen some comments
00:41 - on already that the data is really easy
00:43 - to load in and even pre-processing it
00:46 - like in the last one we just divided
00:47 - everything by 255 I gots really simple
00:50 - in the real world your data is
00:51 - definitely not that nice and there's a
00:53 - lot of stuff that you need to play with
00:55 - and modify to make it actually usable so
00:57 - anyways we'll follow along this one for
00:59 - today and essentially the way that it
01:01 - works is we're gonna have movie reviews
01:03 - and we're just gonna classify them use
01:05 - their as either positive or negative now
01:07 - what we'll do is we'll just look at some
01:10 - of the movie reviews and then we'll talk
01:12 - about the data we'll talk about the
01:13 - architecture using stuff to predict some
01:15 - issues might run into and all of that
01:17 - now I don't know how many video parts
01:19 - this is gonna be I'm gonna try to record
01:20 - it all at once and just split it up
01:21 - based on how long it takes but with that
01:23 - being said enough talking let's get
01:25 - started so what we're gonna do obviously
01:27 - is start in our file here and again this
01:31 - is gonna be really nice because we can
01:32 - just steal kind of the data from cara's
01:34 - so what we'll start by doing is just
01:35 - importing tensorflow as TF we're gonna
01:40 - say from tensorflow
01:42 - import Chara's and then we're going to
01:45 - say import numpy as NP now before I
01:49 - start I ran into a quick issue when I
01:52 - was actually trying to do this just
01:53 - following along with the official
01:54 - tutorial and that was that the data that
01:57 - I want to grab here actually doesn't
01:59 - work with the current version a numpy
02:00 - that comes with tensorflow
02:02 - it's on their github as an issue but
02:04 - anyways to fix this what we need to do
02:06 - is install the previous version of numpy
02:08 - so to do this what I'm actually gonna do
02:11 - is just say pip
02:13 - I think I do like pip numpy version or
02:15 - something because I want to see what
02:17 - version it is incorrect
02:20 - it's say pip version number I want to
02:22 - find what version it is and then just go
02:23 - down to that version okay so I found the
02:26 - version of numpy what we're going to do
02:28 - now is actually just install the correct
02:30 - version a numpy to make it work for this
02:32 - tutorial now this should be fine for
02:34 - everything going forward and if you want
02:35 - to install the most recent version of
02:37 - numpy after doing this go feel free but
02:39 - to do this all I'm gonna do is just say
02:40 - pip install and then numpy equals in
02:43 - this case one point one six point one I
02:46 - believe the version we're using right
02:47 - now is 0.3 at least at the time of
02:49 - recording this but just change it to
02:51 - this version and hopefully in the future
02:52 - they'll fix that issues so that we don't
02:54 - have to do this but anyways I'm going to
02:56 - install that yeah you're gonna have to
02:58 - add two equal signs and I already have
03:00 - this installed so that should just not
03:02 - do anything but you guys just make sure
03:04 - you do that I'll leave the command in
03:05 - the description now after we do that
03:07 - what I'm gonna do is load in the data go
03:10 - say data equals in this case Kara's data
03:13 - sets dot I am what is it I am DB now I
03:18 - believe this stands for like some
03:20 - something movie database I don't really
03:22 - know but anyways that's what the
03:23 - database is and we're gonna do the same
03:25 - thing we did in the previous tutorial
03:26 - which is just split this into training
03:28 - and testing data so to do that I'm gonna
03:29 - say train underscore data train
03:31 - underscore labels comma and then in this
03:34 - case we'll say test underscore at data
03:36 - and then test underscore labels equals
03:39 - in this case data load underscore data
03:41 - and we're just gonna add one thing in
03:43 - here which is num underscore words
03:46 - equals in this case ten thousand now the
03:49 - reason I'm doing this is because this
03:51 - data set contains like a ton of
03:52 - different words and what we're gonna
03:54 - actually do by saying num words equals
03:56 - 10,000 is only take the words that are
03:58 - the 10,000 most frequent which means
04:01 - we're gonna leave out words that usually
04:03 - are only occurring like once or twice do
04:04 - it the entire data set because we don't
04:06 - want to throw those into our model and
04:08 - have things like be more difficult than
04:10 - they have to be and just have data
04:12 - that's kind of irrelevant because we're
04:13 - gonna be comparing obviously movie
04:16 - reviews and there's some words that are
04:18 - only in like one review we should
04:20 - probably just omit them because there's
04:22 - nothing to really compare them to in
04:23 - other data sets any
04:25 - I hope that kind of makes sense but
04:27 - that's not super important we're gonna
04:28 - be numb words equals 10,000 it's also
04:31 - shrinks our dad a little bit which makes
04:32 - it a bit nicer now what we're gonna do
04:34 - next is we're actually gonna show how we
04:37 - can display this data now if I start by
04:40 - actually just showing you like the Train
04:42 - underscore data and let's pick like the
04:44 - zero with one so I guess the first one
04:45 - and I print this out to the screen so
04:47 - let's go if I could get 2pi 'the python
04:51 - and i guess in this case we'll have to
04:52 - do I probably should just type this to
04:54 - start tutorial to when this actually
04:59 - prints out probably going to take a
05:01 - second here just to download the data
05:02 - set you can see that what we have is
05:05 - actually just a bunch of numbers now
05:08 - this doesn't really look like a movie
05:09 - review to me does it well what this
05:11 - actually is is integer encoded words so
05:15 - essentially each of these integers point
05:17 - to a certain word and what we've done
05:19 - just to make it way easier for our model
05:22 - to actually classify these and work with
05:24 - these is we've given each word one
05:26 - integer so in this case maybe like the
05:27 - word the integer one stands or something
05:29 - the integer fourteen stands for
05:31 - something and all we've done is just
05:32 - added those integers into a list that
05:35 - represents where these words are located
05:37 - in the movie review now this is nice for
05:40 - the computer but it's not very nice for
05:41 - us if we actually want to read these
05:42 - words so we have to do is find the
05:44 - mappings for these words and then find
05:46 - some way to actually display this so
05:48 - that you know we can have a look at it
05:50 - now I'll be honest here I'm just gonna
05:52 - take this from what they have on the
05:53 - tensorflow website on how to do this
05:54 - typically you would create your own
05:56 - mappings for words with your own
05:58 - dictionary and you just already have
05:59 - that information but fortunately for us
06:01 - tensorflow already does that so to do
06:03 - that I'm gonna say word underscore index
06:04 - equals in this case IMDB dot
06:07 - underscore word underscore index like
06:10 - this now what this does is actually
06:12 - going to give us a dictionary that has
06:14 - those keys and those mappings so that
06:16 - what we can do is well figure out what
06:19 - you know what these integers actually
06:21 - means so when we want to print it out
06:22 - later we can have a look at them so I'm
06:25 - gonna say now is word underscore index
06:27 - equals in this case K : and then we're
06:32 - gonna say what it could be plus three
06:35 - for K the
06:38 - in word underscore index thought items
06:41 - so I might have been incorrect here this
06:44 - doesn't actually give us a dictionary
06:45 - this just gives us like tuples that have
06:47 - the string and the word in them I
06:51 - believe and then what we're doing here
06:52 - is we're gonna say instead of C sorry
06:54 - this should be B my apologies is we're
06:56 - gonna get we're just break that tupple
06:58 - up into K and V which stands for key and
07:00 - value and the key will be the word the
07:02 - value will be obviously the integer yes
07:05 - that's what it will be and we're gonna
07:07 - say for word items and index will break
07:09 - that up and then we're just gonna add a
07:10 - bunch of different keys into our data
07:11 - set now the reason we're gonna start at
07:13 - +3 is because we're gonna have actually
07:16 - one key or three keys that are gonna be
07:18 - like special characters for our word
07:20 - mapping and you guys will see how those
07:22 - work in a second so I'm gonna start by
07:23 - just saying word index and in this case
07:25 - I'm gonna put in here pad we're gonna
07:29 - talk about this in a second so don't
07:30 - worry if you guys are kind of like what
07:31 - are you doing right now I'm gonna say
07:33 - word index and in this case starts
07:38 - equals 1
07:39 - I say word underscore index in this case
07:42 - I believe it's like UNK yeah that's
07:45 - correct when I say UNK equals 2
07:47 - now UNK just stands for unknown and I'm
07:50 - gonna explain all this in a second but
07:51 - it's easier just to type it out first
07:52 - and we're gonna say word index in this
07:55 - case inside this tag I'm used we're
07:58 - going to say equals 3 so what I'm doing
08:01 - essentially is all of the words in our
08:03 - training and testing data set have like
08:07 - keys and values associated with them
08:09 - starting at 1 so what I'm doing is I'm
08:11 - just gonna add 3 to all of those values
08:13 - so that what I can actually do is assign
08:16 - my own kind of values that are gonna
08:18 - stand for padding start unknown and
08:21 - unused so that if we get values that are
08:24 - not valid we can just assign them to
08:26 - this essentially in the dictionary now
08:28 - what I'm gonna use for padding you guys
08:29 - will see in just a second essentially
08:31 - it's just so we can make our all our
08:32 - movie sets the same length so we'll add
08:34 - this what's known as pad tag and we'll
08:37 - do that by adding 0 into our actual
08:39 - movie review list so that we're gonna
08:41 - make each movie review the same length
08:43 - and the way we do that essentially is if
08:45 - they're not the same length so maybe
08:46 - one's a hundred maybe ones 200 we want
08:48 - all of them to be 200
08:50 - hundred length movie lists will for what
08:53 - we'll do is we'll just add a bunch of
08:55 - padding to the end of it to make it
08:56 - length 200 and then obviously our model
09:00 - will hopefully be able to differentiate
09:01 - the fact that that is padding and then
09:04 - we don't care about the padding and that
09:05 - we shouldn't even bother really like
09:06 - looking at that right alright so now
09:09 - what I'm gonna do is add this kind of
09:11 - complicated line here just to I don't
09:14 - even know why they have this to be quite
09:15 - honest this is the way the tensor flows
09:17 - has decided to do they're like word
09:18 - mappings but apparently you need to add
09:20 - this reverse underscore underscore word
09:24 - underscore index which is equal to
09:27 - dictionary and then in here we're gonna
09:29 - say value comma key for key comma value
09:38 - in word underscore index I believe
09:43 - that's correct and what this is gonna do
09:44 - actually sorry not word index word index
09:47 - dot items what this is gonna do is okay
09:50 - I understand now now that I've typed it
09:52 - out you just swap all the values in the
09:53 - keys so that right now we actually have
09:55 - a dictionary that has all of the like
09:59 - the keys first which is gonna be the
10:01 - word and then the values where we
10:03 - actually want it the other way around so
10:05 - we have like the integer pointing to the
10:06 - word because we're gonna have our data
10:08 - set that is gonna contain just integers
10:10 - like we've seen here and we want these
10:12 - integers to be able to point to a word
10:14 - as opposed to the other way around so
10:16 - what we're doing is just reversing this
10:18 - with a reverse word index list just our
10:22 - dictionary sorry essentially that's what
10:23 - this is doing here all right now that
10:26 - we've done that the last step is just to
10:27 - add a function and what this function
10:29 - will do is actually decode essentially
10:33 - all of this training and testing data
10:35 - into human readable words so there's
10:38 - different ways to do this again I'm just
10:40 - gonna take this right from the
10:41 - tensorflow website because this part is
10:42 - not super important and I'd rather just
10:44 - you know do it quickly than spend too
10:46 - much time on it so we're just gonna say
10:48 - return blank string dot join and in this
10:52 - case we're gonna say reverse word index
10:54 - dog gets in this case we're gonna say I
10:57 - comma question mark now what this does
11:00 - essentially if you don't know how the
11:02 - cat works is we're gonna try
11:03 - to get index I which we're gonna define
11:05 - in a second if we can't find a value for
11:08 - that then what we'll do is just put
11:09 - question mark and that's which is a
11:11 - default value which means we won't crash
11:13 - we're having like a key error in our
11:16 - dictionary I'm gonna say for in this
11:19 - case I in text I don't know why where I
11:24 - have text typed I think I might have
11:27 - messed something up here so one second
11:29 - here ootek sorry is the parameter my
11:31 - apologies
11:31 - so anyways that's what this is gonna do
11:33 - is just going to return to us
11:34 - essentially all of the the keys that we
11:36 - want or the human readable words my
11:39 - apologies so now what we'll do is we'll
11:41 - simply just print out decode review and
11:44 - I'm just gonna give it some test status
11:45 - let's say test for example zero and I
11:48 - guess we're going to do test underscore
11:49 - data it doesn't really matter if you
11:50 - train or test data but let's just have a
11:52 - look at test out of zero and see what
11:54 - that actually looks like so let's run
11:56 - that assuming I make any mistakes we
11:58 - should actually get some valid output in
12:00 - just a second this usually takes a
12:01 - minute to run up I am DB is not defined
12:05 - what did I type here I typed that as
12:08 - data my apologies
12:09 - so where we say IMDB which is right here
12:11 - we just need to replace that with data
12:14 - in my other file I called it IMDB so
12:17 - that's why I made a mistake there but
12:18 - let's run that again and hopefully now
12:20 - we will get some better looking output
12:22 - so let's wait for this and see dict
12:25 - object has no attribute items this needs
12:27 - to be items classic typos by Jim one
12:32 - more time third time is a charm
12:33 - hopefully let's see and there we go so
12:35 - now we can see that we're actually
12:37 - getting all of this decoded into well
12:40 - this text now I'll allow you guys to
12:42 - read through it but you can see that we
12:43 - have these kind of keys that we've added
12:45 - so start which is one which will
12:48 - automatically be added at the beginning
12:49 - of all of our text and then we have
12:51 - these un Ches which stand for unknown
12:53 - character essentially and then we don't
12:56 - have any other keys in here but say for
12:57 - example we had like some padding we had
12:59 - added to this we would see those Pat
13:01 - tags as well in here it's not
13:03 - essentially how that works if you'd like
13:05 - to look at some other reviews just mess
13:07 - around with kind of the values and the
13:09 - index here throw them into decode review
13:11 - and then we can actually see what they
13:12 - look like now something to note quickly
13:14 - is that our review
13:16 - our different lengths now I've talked
13:18 - about this already but let's just
13:19 - compare two reviews to really test that
13:21 - I'm not just making this up so I'm going
13:23 - to say test underscore data why I have a
13:26 - capital here tests underscore add to
13:28 - zero so the length of testament or data
13:30 - is zero and the length of let's try test
13:33 - underscore data one just to prove to you
13:36 - guys that these are actually different
13:37 - lengths which means there's something
13:38 - kind of fancy we're gonna have to do
13:40 - with that padding tag which I was
13:42 - talking about there so let's go into
13:43 - text class classification let's go see
13:45 - MD and then Python in this case tutorial
13:49 - to PI now I guess we're gonna get that
13:52 - output again which is probably what's
13:53 - causing this to just take a second to
13:54 - run you can see that we have like 68 and
13:57 - we have length to 60 now this is not
14:00 - gonna work for our model and the reason
14:02 - this doesn't work is because we need to
14:05 - know what our inputs shut shape sorry
14:08 - and size is gonna be just like I talked
14:10 - about before we define the input nodes
14:12 - or the input neurons and the output
14:14 - neurons so we have to determine how many
14:16 - impo neurons there's gonna be and how
14:18 - many output neurons there's gonna be now
14:20 - if we're like we don't know how large
14:22 - our data is gonna be and it's different
14:23 - for each
14:24 - what do you call its entry then that's
14:27 - an issue so we need to do something to
14:29 - fix that so what we're gonna do is we're
14:32 - gonna use this padding tag to
14:34 - essentially set a definite length for
14:36 - all of our data now we could go ahead
14:39 - and pick the longest review and say that
14:41 - will make all of the reviews that length
14:43 - but what I'm gonna do is just pick an
14:44 - arbitrary number in this case we'll just
14:46 - do like 250 and say that that's the
14:48 - maximum amount of words we're gonna
14:49 - allow in one review which means that if
14:51 - you have more than 250 words in your
14:53 - review we're just gonna get rid of all
14:55 - those and if you don't have 256 words or
14:58 - 250 words or whatever it is we're just
15:00 - gonna add these padding tags to the end
15:03 - of it until eventually we reach that
15:05 - value so the way to do this is again
15:09 - using those fancy tensorflow functions
15:11 - now if you don't like these functions
15:14 - and like what these do for you and how
15:16 - they just kind of save you some time go
15:18 - ahead and try to write them yourself and
15:19 - if you want help on how to do that
15:22 - feel free to reach out to me on discord
15:23 - or in the comments or whatever but I
15:25 - personally just use them because it
15:26 - saves me a quite quite a bit of time in
15:28 - terms of like typing out the function
15:29 - and I already know how to do a lot of
15:32 - what these functions do so for me it
15:33 - doesn't really make sense to just read
15:35 - type them out when I can just use these
15:37 - kind of fancy tools so what we're gonna
15:39 - say is we're gonna redefine our training
15:41 - and testing data and what we're gonna do
15:43 - is just trim that data so that it's only
15:45 - at or kind of normalized that data so
15:48 - it's at 250 words so to do that I'm
15:51 - gonna say train underscore data equals
15:53 - in this case Kara's got pre-processing
15:58 - no idea if that's how you spell it we'll
16:01 - have to check that in a second dot
16:02 - sequence dot pad underscore sequence so
16:08 - pre-processing I think that's correct
16:10 - I guess we'll see and then in here we
16:12 - have to define a few different
16:13 - parameters so what we'll first do is
16:15 - we'll give that train underscore data
16:16 - we're gonna say value equals which will
16:19 - be the pad value so what we add to the
16:21 - end of in this case our numpy array to
16:24 - pad it per say and in this case we'll
16:27 - just use this pad tag so we'll say
16:29 - literally word index pad so let's copy
16:32 - that and put that there
16:33 - we're gonna say our padding equals in
16:37 - this case post we just means we're gonna
16:39 - Pat after as opposed to before we also
16:42 - could pad before but that doesn't really
16:44 - make too much sense for this and then
16:46 - what we'll say is max in this case Len
16:49 - equals and then you pick your number
16:50 - that you want to make all of the values
16:53 - equal to now tensorflow did like 256 I'm
16:56 - just gonna do 250 and see if this makes
16:58 - a difference in terms of our accuracy
16:59 - for the model and I'm literally just
17:01 - gonna copy this and change these values
17:02 - now to test underscore data instead of
17:05 - Train underscore data and this will do
17:07 - the same thing on our other data set
17:10 - oops didn't mean to do that so test
17:13 - underscore date sounds like that so
17:16 - quick recap here because we are at 17
17:18 - minutes now essentially what we've done
17:20 - is we've loaded in our data we've looked
17:21 - at our data we've created the word
17:24 - mappings essentially for our data so
17:26 - that we can actually figure out what at
17:27 - least integers mean we've created a
17:29 - little function here that will decode
17:31 - the mappings for us so we just pass it a
17:34 - word review that's integer encoded it
17:36 - decodes it and then it we can print that
17:38 - information out to the screen to have a
17:40 - look at it what we've just done now is
17:42 - we've done what's called pre
17:43 - our data which means just making it into
17:45 - a forum that our model can actually
17:47 - accept and that's consistent and that's
17:49 - what you're always gonna want to do with
17:51 - any data that you have typically it's
17:53 - gonna take you a bit more work than what
17:54 - we have because it's only two lines to
17:56 - pre-process our data because Cara's kind
17:58 - of does it for us but for the purpose of
18:00 - this example that's fine all right so
18:03 - now that we've done that it's actually
18:05 - time to define our model now I'll show
18:08 - you quickly just to make sure you know
18:10 - you guys believe me here that this is
18:12 - working in terms of pre-processing
18:13 - pre-processing our data so it's actually
18:16 - gonna make things the same length so
18:17 - we'll say train underscore data test
18:19 - underscore data
18:19 - let me just print this out to the screen
18:21 - so python tutorial 2 again we're gonna
18:24 - get these integer mappings but we'll get
18:25 - the length at the end as well and
18:27 - another error of course we need to add
18:30 - an S to these sequences again my
18:32 - apologies guys on that classic typos
18:36 - here so anyways I had pre process
18:38 - processing sequence we need sequences
18:40 - and now if I run this you can see that
18:43 - we have a length of 250 and 250 so we've
18:46 - kept that consistent now for some oh I'm
18:48 - printing I don't know why this is
18:50 - printing toot oh it's because I'm
18:52 - printing it here and then I'm printing
18:53 - it here but you guys get the idea in
18:56 - that we've now made them actually the
18:57 - same size so let me remove these print
18:59 - statements all of them so we can stop
19:02 - printing trained out of 0 up here as
19:04 - well and now let's start defining our
19:06 - model
19:06 - so I'll just say model down here is a
19:09 - little comment just to help us out so
19:11 - what I'm gonna do now is similar to what
19:12 - I've done before except in the last one
19:14 - you might have noticed that the way I
19:15 - define my model was I'll show you in a
19:19 - second once I finished typing this so we
19:21 - did Kara's dot sequential and then what
19:23 - we actually did was just had a listing
19:24 - here that had all the layers that's fine
19:26 - you can do that but in this case we're
19:28 - gonna have a few more layers so what
19:29 - we're gonna do actually is add these
19:32 - layers just by doing the model dot add
19:34 - it's precisely the same thing as before
19:36 - except instead of adding them in this
19:38 - list we're just gonna do it using this
19:39 - method so now we're gonna say Kara's dog
19:42 - layers dot in this case embedding and
19:45 - I'll talk about what these layers do in
19:46 - a second it's really 10016 and then
19:50 - we're just gonna actually copy this four
19:53 - times and just change these layers and
19:55 - be kind of
19:57 - parameters as well so now we're gonna
19:58 - say global average pooling 1d and then
20:07 - do that and then we're gonna add a dense
20:09 - layer here and another dense layer and
20:13 - change these parameters so we'll say
20:17 - dense and we'll say in this case 16 will
20:20 - say activation equals Lulu or rectify
20:26 - linear unit whatever you guys want to
20:28 - call it and then we'll do down here one
20:29 - and activation equals direct fire linear
20:32 - unit as well actually sorry not really
20:35 - really we're gonna do sigmoid my
20:37 - apologies
20:37 - so now we'll actually talk about the
20:39 - architecture of this model and how I
20:41 - came up with picking these layers and
20:43 - well what these layers are well what we
20:45 - want essentially is we want the final
20:48 - output to be whether the review is good
20:49 - or whether the review is bad I think I
20:51 - mentioned that at the beginning of the
20:53 - video so what we're actually gonna do is
20:55 - just have either that like we'll have
20:57 - one output neuron and that neuron should
20:59 - be either 0 or 1 we're somewhere in
21:02 - between there to give us kind of a
21:03 - probability of like we think it's like
21:05 - 20% 1 80% 0 something along those lines
21:09 - now we can accomplish that by using
21:11 - sigmoid because what it will do again
21:13 - we've talked about the sigmoid function
21:15 - is it'll squish everything so whatever
21:17 - our value is in between 0 and 1 which
21:19 - will give us a nice way to test if our
21:21 - models actually working properly and to
21:24 - get it the value that we want
21:26 - [Music]

Cleaned transcript:

hey guys and welcome back to another neural network tutorial now in today's video what we're gonna be doing is talking about text classification with tensorflow 2.0 now what I'm gonna be doing just to be full fully transparent with you guys here is following along with the actual official tutorials on the tensorflow 2.0 tutorial now I find that these are actually the best in terms of like kind of a structure to start with to understand very basic neural networks for some pretty simple tasks I would say and then we're gonna stray away from those we're gonna start using our own data our own networks our own architecture and we'll start talking about kind of some of the issues you have when you actually start applying these to real data so so far you guys have noticed and I've seen some comments on already that the data is really easy to load in and even preprocessing it like in the last one we just divided everything by 255 I gots really simple in the real world your data is definitely not that nice and there's a lot of stuff that you need to play with and modify to make it actually usable so anyways we'll follow along this one for today and essentially the way that it works is we're gonna have movie reviews and we're just gonna classify them use their as either positive or negative now what we'll do is we'll just look at some of the movie reviews and then we'll talk about the data we'll talk about the architecture using stuff to predict some issues might run into and all of that now I don't know how many video parts this is gonna be I'm gonna try to record it all at once and just split it up based on how long it takes but with that being said enough talking let's get started so what we're gonna do obviously is start in our file here and again this is gonna be really nice because we can just steal kind of the data from cara's so what we'll start by doing is just importing tensorflow as TF we're gonna say from tensorflow import Chara's and then we're going to say import numpy as NP now before I start I ran into a quick issue when I was actually trying to do this just following along with the official tutorial and that was that the data that I want to grab here actually doesn't work with the current version a numpy that comes with tensorflow it's on their github as an issue but anyways to fix this what we need to do is install the previous version of numpy so to do this what I'm actually gonna do is just say pip I think I do like pip numpy version or something because I want to see what version it is incorrect it's say pip version number I want to find what version it is and then just go down to that version okay so I found the version of numpy what we're going to do now is actually just install the correct version a numpy to make it work for this tutorial now this should be fine for everything going forward and if you want to install the most recent version of numpy after doing this go feel free but to do this all I'm gonna do is just say pip install and then numpy equals in this case one point one six point one I believe the version we're using right now is 0.3 at least at the time of recording this but just change it to this version and hopefully in the future they'll fix that issues so that we don't have to do this but anyways I'm going to install that yeah you're gonna have to add two equal signs and I already have this installed so that should just not do anything but you guys just make sure you do that I'll leave the command in the description now after we do that what I'm gonna do is load in the data go say data equals in this case Kara's data sets dot I am what is it I am DB now I believe this stands for like some something movie database I don't really know but anyways that's what the database is and we're gonna do the same thing we did in the previous tutorial which is just split this into training and testing data so to do that I'm gonna say train underscore data train underscore labels comma and then in this case we'll say test underscore at data and then test underscore labels equals in this case data load underscore data and we're just gonna add one thing in here which is num underscore words equals in this case ten thousand now the reason I'm doing this is because this data set contains like a ton of different words and what we're gonna actually do by saying num words equals 10,000 is only take the words that are the 10,000 most frequent which means we're gonna leave out words that usually are only occurring like once or twice do it the entire data set because we don't want to throw those into our model and have things like be more difficult than they have to be and just have data that's kind of irrelevant because we're gonna be comparing obviously movie reviews and there's some words that are only in like one review we should probably just omit them because there's nothing to really compare them to in other data sets any I hope that kind of makes sense but that's not super important we're gonna be numb words equals 10,000 it's also shrinks our dad a little bit which makes it a bit nicer now what we're gonna do next is we're actually gonna show how we can display this data now if I start by actually just showing you like the Train underscore data and let's pick like the zero with one so I guess the first one and I print this out to the screen so let's go if I could get 2pi 'the python and i guess in this case we'll have to do I probably should just type this to start tutorial to when this actually prints out probably going to take a second here just to download the data set you can see that what we have is actually just a bunch of numbers now this doesn't really look like a movie review to me does it well what this actually is is integer encoded words so essentially each of these integers point to a certain word and what we've done just to make it way easier for our model to actually classify these and work with these is we've given each word one integer so in this case maybe like the word the integer one stands or something the integer fourteen stands for something and all we've done is just added those integers into a list that represents where these words are located in the movie review now this is nice for the computer but it's not very nice for us if we actually want to read these words so we have to do is find the mappings for these words and then find some way to actually display this so that you know we can have a look at it now I'll be honest here I'm just gonna take this from what they have on the tensorflow website on how to do this typically you would create your own mappings for words with your own dictionary and you just already have that information but fortunately for us tensorflow already does that so to do that I'm gonna say word underscore index equals in this case IMDB dot underscore word underscore index like this now what this does is actually going to give us a dictionary that has those keys and those mappings so that what we can do is well figure out what you know what these integers actually means so when we want to print it out later we can have a look at them so I'm gonna say now is word underscore index equals in this case K and then we're gonna say what it could be plus three for K the in word underscore index thought items so I might have been incorrect here this doesn't actually give us a dictionary this just gives us like tuples that have the string and the word in them I believe and then what we're doing here is we're gonna say instead of C sorry this should be B my apologies is we're gonna get we're just break that tupple up into K and V which stands for key and value and the key will be the word the value will be obviously the integer yes that's what it will be and we're gonna say for word items and index will break that up and then we're just gonna add a bunch of different keys into our data set now the reason we're gonna start at +3 is because we're gonna have actually one key or three keys that are gonna be like special characters for our word mapping and you guys will see how those work in a second so I'm gonna start by just saying word index and in this case I'm gonna put in here pad we're gonna talk about this in a second so don't worry if you guys are kind of like what are you doing right now I'm gonna say word index and in this case starts equals 1 I say word underscore index in this case I believe it's like UNK yeah that's correct when I say UNK equals 2 now UNK just stands for unknown and I'm gonna explain all this in a second but it's easier just to type it out first and we're gonna say word index in this case inside this tag I'm used we're going to say equals 3 so what I'm doing essentially is all of the words in our training and testing data set have like keys and values associated with them starting at 1 so what I'm doing is I'm just gonna add 3 to all of those values so that what I can actually do is assign my own kind of values that are gonna stand for padding start unknown and unused so that if we get values that are not valid we can just assign them to this essentially in the dictionary now what I'm gonna use for padding you guys will see in just a second essentially it's just so we can make our all our movie sets the same length so we'll add this what's known as pad tag and we'll do that by adding 0 into our actual movie review list so that we're gonna make each movie review the same length and the way we do that essentially is if they're not the same length so maybe one's a hundred maybe ones 200 we want all of them to be 200 hundred length movie lists will for what we'll do is we'll just add a bunch of padding to the end of it to make it length 200 and then obviously our model will hopefully be able to differentiate the fact that that is padding and then we don't care about the padding and that we shouldn't even bother really like looking at that right alright so now what I'm gonna do is add this kind of complicated line here just to I don't even know why they have this to be quite honest this is the way the tensor flows has decided to do they're like word mappings but apparently you need to add this reverse underscore underscore word underscore index which is equal to dictionary and then in here we're gonna say value comma key for key comma value in word underscore index I believe that's correct and what this is gonna do actually sorry not word index word index dot items what this is gonna do is okay I understand now now that I've typed it out you just swap all the values in the keys so that right now we actually have a dictionary that has all of the like the keys first which is gonna be the word and then the values where we actually want it the other way around so we have like the integer pointing to the word because we're gonna have our data set that is gonna contain just integers like we've seen here and we want these integers to be able to point to a word as opposed to the other way around so what we're doing is just reversing this with a reverse word index list just our dictionary sorry essentially that's what this is doing here all right now that we've done that the last step is just to add a function and what this function will do is actually decode essentially all of this training and testing data into human readable words so there's different ways to do this again I'm just gonna take this right from the tensorflow website because this part is not super important and I'd rather just you know do it quickly than spend too much time on it so we're just gonna say return blank string dot join and in this case we're gonna say reverse word index dog gets in this case we're gonna say I comma question mark now what this does essentially if you don't know how the cat works is we're gonna try to get index I which we're gonna define in a second if we can't find a value for that then what we'll do is just put question mark and that's which is a default value which means we won't crash we're having like a key error in our dictionary I'm gonna say for in this case I in text I don't know why where I have text typed I think I might have messed something up here so one second here ootek sorry is the parameter my apologies so anyways that's what this is gonna do is just going to return to us essentially all of the the keys that we want or the human readable words my apologies so now what we'll do is we'll simply just print out decode review and I'm just gonna give it some test status let's say test for example zero and I guess we're going to do test underscore data it doesn't really matter if you train or test data but let's just have a look at test out of zero and see what that actually looks like so let's run that assuming I make any mistakes we should actually get some valid output in just a second this usually takes a minute to run up I am DB is not defined what did I type here I typed that as data my apologies so where we say IMDB which is right here we just need to replace that with data in my other file I called it IMDB so that's why I made a mistake there but let's run that again and hopefully now we will get some better looking output so let's wait for this and see dict object has no attribute items this needs to be items classic typos by Jim one more time third time is a charm hopefully let's see and there we go so now we can see that we're actually getting all of this decoded into well this text now I'll allow you guys to read through it but you can see that we have these kind of keys that we've added so start which is one which will automatically be added at the beginning of all of our text and then we have these un Ches which stand for unknown character essentially and then we don't have any other keys in here but say for example we had like some padding we had added to this we would see those Pat tags as well in here it's not essentially how that works if you'd like to look at some other reviews just mess around with kind of the values and the index here throw them into decode review and then we can actually see what they look like now something to note quickly is that our review our different lengths now I've talked about this already but let's just compare two reviews to really test that I'm not just making this up so I'm going to say test underscore data why I have a capital here tests underscore add to zero so the length of testament or data is zero and the length of let's try test underscore data one just to prove to you guys that these are actually different lengths which means there's something kind of fancy we're gonna have to do with that padding tag which I was talking about there so let's go into text class classification let's go see MD and then Python in this case tutorial to PI now I guess we're gonna get that output again which is probably what's causing this to just take a second to run you can see that we have like 68 and we have length to 60 now this is not gonna work for our model and the reason this doesn't work is because we need to know what our inputs shut shape sorry and size is gonna be just like I talked about before we define the input nodes or the input neurons and the output neurons so we have to determine how many impo neurons there's gonna be and how many output neurons there's gonna be now if we're like we don't know how large our data is gonna be and it's different for each what do you call its entry then that's an issue so we need to do something to fix that so what we're gonna do is we're gonna use this padding tag to essentially set a definite length for all of our data now we could go ahead and pick the longest review and say that will make all of the reviews that length but what I'm gonna do is just pick an arbitrary number in this case we'll just do like 250 and say that that's the maximum amount of words we're gonna allow in one review which means that if you have more than 250 words in your review we're just gonna get rid of all those and if you don't have 256 words or 250 words or whatever it is we're just gonna add these padding tags to the end of it until eventually we reach that value so the way to do this is again using those fancy tensorflow functions now if you don't like these functions and like what these do for you and how they just kind of save you some time go ahead and try to write them yourself and if you want help on how to do that feel free to reach out to me on discord or in the comments or whatever but I personally just use them because it saves me a quite quite a bit of time in terms of like typing out the function and I already know how to do a lot of what these functions do so for me it doesn't really make sense to just read type them out when I can just use these kind of fancy tools so what we're gonna say is we're gonna redefine our training and testing data and what we're gonna do is just trim that data so that it's only at or kind of normalized that data so it's at 250 words so to do that I'm gonna say train underscore data equals in this case Kara's got preprocessing no idea if that's how you spell it we'll have to check that in a second dot sequence dot pad underscore sequence so preprocessing I think that's correct I guess we'll see and then in here we have to define a few different parameters so what we'll first do is we'll give that train underscore data we're gonna say value equals which will be the pad value so what we add to the end of in this case our numpy array to pad it per say and in this case we'll just use this pad tag so we'll say literally word index pad so let's copy that and put that there we're gonna say our padding equals in this case post we just means we're gonna Pat after as opposed to before we also could pad before but that doesn't really make too much sense for this and then what we'll say is max in this case Len equals and then you pick your number that you want to make all of the values equal to now tensorflow did like 256 I'm just gonna do 250 and see if this makes a difference in terms of our accuracy for the model and I'm literally just gonna copy this and change these values now to test underscore data instead of Train underscore data and this will do the same thing on our other data set oops didn't mean to do that so test underscore date sounds like that so quick recap here because we are at 17 minutes now essentially what we've done is we've loaded in our data we've looked at our data we've created the word mappings essentially for our data so that we can actually figure out what at least integers mean we've created a little function here that will decode the mappings for us so we just pass it a word review that's integer encoded it decodes it and then it we can print that information out to the screen to have a look at it what we've just done now is we've done what's called pre our data which means just making it into a forum that our model can actually accept and that's consistent and that's what you're always gonna want to do with any data that you have typically it's gonna take you a bit more work than what we have because it's only two lines to preprocess our data because Cara's kind of does it for us but for the purpose of this example that's fine all right so now that we've done that it's actually time to define our model now I'll show you quickly just to make sure you know you guys believe me here that this is working in terms of preprocessing preprocessing our data so it's actually gonna make things the same length so we'll say train underscore data test underscore data let me just print this out to the screen so python tutorial 2 again we're gonna get these integer mappings but we'll get the length at the end as well and another error of course we need to add an S to these sequences again my apologies guys on that classic typos here so anyways I had pre process processing sequence we need sequences and now if I run this you can see that we have a length of 250 and 250 so we've kept that consistent now for some oh I'm printing I don't know why this is printing toot oh it's because I'm printing it here and then I'm printing it here but you guys get the idea in that we've now made them actually the same size so let me remove these print statements all of them so we can stop printing trained out of 0 up here as well and now let's start defining our model so I'll just say model down here is a little comment just to help us out so what I'm gonna do now is similar to what I've done before except in the last one you might have noticed that the way I define my model was I'll show you in a second once I finished typing this so we did Kara's dot sequential and then what we actually did was just had a listing here that had all the layers that's fine you can do that but in this case we're gonna have a few more layers so what we're gonna do actually is add these layers just by doing the model dot add it's precisely the same thing as before except instead of adding them in this list we're just gonna do it using this method so now we're gonna say Kara's dog layers dot in this case embedding and I'll talk about what these layers do in a second it's really 10016 and then we're just gonna actually copy this four times and just change these layers and be kind of parameters as well so now we're gonna say global average pooling 1d and then do that and then we're gonna add a dense layer here and another dense layer and change these parameters so we'll say dense and we'll say in this case 16 will say activation equals Lulu or rectify linear unit whatever you guys want to call it and then we'll do down here one and activation equals direct fire linear unit as well actually sorry not really really we're gonna do sigmoid my apologies so now we'll actually talk about the architecture of this model and how I came up with picking these layers and well what these layers are well what we want essentially is we want the final output to be whether the review is good or whether the review is bad I think I mentioned that at the beginning of the video so what we're actually gonna do is just have either that like we'll have one output neuron and that neuron should be either 0 or 1 we're somewhere in between there to give us kind of a probability of like we think it's like 20% 1 80% 0 something along those lines now we can accomplish that by using sigmoid because what it will do again we've talked about the sigmoid function is it'll squish everything so whatever our value is in between 0 and 1 which will give us a nice way to test if our models actually working properly and to get it the value that we want
