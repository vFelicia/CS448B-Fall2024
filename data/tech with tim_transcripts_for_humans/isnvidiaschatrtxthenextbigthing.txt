With timestamps:

00:00 - Nvidia just released their chat RTX
00:02 - application which is pretty impressive
00:04 - and solves the two biggest issues with
00:06 - llms today issue number one data access
00:10 - now sure I can copy in the contents of a
00:12 - file or an email I can upload a few
00:14 - documents to a model but it starts to
00:16 - get tricky when I want it to have access
00:18 - to hundreds of different documents or my
00:20 - entire operating system or entire
00:22 - directories on my computer and even if I
00:24 - could give a model access to all this
00:26 - information would I want to do that
00:28 - would I trust a company with all my
00:30 - private and sensitive data that's where
00:32 - issue number two comes in privacy we
00:35 - really cannot trust these Central
00:36 - companies we don't know what they're
00:37 - doing with our data if they're training
00:39 - on it if they're storing it and I don't
00:41 - know about you but I don't want random
00:43 - companies having access to my personal
00:44 - information but at the same time it
00:47 - would be super cool if I had a model
00:48 - that could answer questions about my
00:50 - personal data read through my files go
00:53 - through sensitive documents and do that
00:54 - in a secure way where I can still get
00:56 - information really quickly but I don't
00:58 - have to be worried about this being sent
01:00 - off or sold to some company now
01:02 - fortunately for us nvidia's chat RTX
01:04 - application pretty much solves all of
01:06 - these problems by running a free
01:08 - open-source model locally on your
01:10 - computer which has access to all of your
01:12 - different files that means I can ask it
01:14 - questions about my information get
01:16 - responses back extremely fast and know
01:18 - that all of my data is secure because
01:20 - it's not connected to the cloud a big
01:22 - thank you to Nvidia for sponsoring this
01:24 - video and let me share with you a quick
01:26 - demo of how this tool works so this is
01:28 - what the application looks like it's
01:29 - it's pretty straightforward and this is
01:31 - just running locally in my browser and
01:33 - I'll just quickly walk you through how
01:35 - it works so you can see that we can
01:36 - select a model that we want to use so
01:38 - mistol llama 2 we also could install
01:41 - other open source models if we want and
01:43 - then we have a data set section now the
01:45 - data set section is the cool part here
01:47 - where I can actually select a directory
01:49 - on my computer that I want to give the
01:51 - model access to now what this will use
01:53 - is something known as rag which is
01:54 - retrieval augmented generation going to
01:57 - talk more about that later in the video
01:59 - but the idea is is you can give the
02:00 - model some context about different files
02:02 - on your computer and this is all secure
02:05 - running locally and it's super fast and
02:07 - it can look up information find
02:09 - different things inside of for example
02:11 - PDF documents text documents or
02:13 - Microsoft Word documents and then
02:15 - immediately give you the result so let's
02:17 - test something out here I'll say can you
02:20 - give me a summary of my
02:23 - script related to micro services so this
02:28 - directory has a bunch of different files
02:30 - one of them is a microservices script
02:32 - for my YouTube channel and you can see
02:33 - that what it does here is it finds this
02:35 - document gives me a reference to it that
02:37 - I can click on and then it actually
02:39 - gives me a quick uh summary of that
02:41 - script so this is super cool and you can
02:43 - use this to really quickly look up
02:45 - different information and to have access
02:47 - to all of these different files within
02:49 - the model so let's imagine you had a
02:51 - directory with your flight itinerary and
02:52 - a bunch of different plans and maybe
02:54 - someone asks you hey what are you doing
02:55 - on Thursday rather than having to go
02:57 - through there and look for the
02:58 - information just say hey what am I doing
03:00 - on Thursday based on my flight plans or
03:02 - based on my itinerary point it to that
03:04 - directory and you instantly get that
03:06 - information this is super super cool
03:08 - I'll type in a few other examples here
03:10 - so you can get a sense of how it works
03:12 - so I just selected another folder here
03:13 - and this folder actually contains a
03:15 - bunch of accounting information for my
03:17 - business so invoices receipts tax
03:19 - information Etc now this is highly
03:21 - sensitive data that I wouldn't want to
03:23 - share with other people but that I might
03:24 - want to quickly get some information
03:26 - about without having to go search
03:28 - through all of the different files so
03:29 - just show you an example of doing that
03:31 - here and obviously blur anything out
03:32 - that's going to be too sensitive so I
03:34 - can say can you give me some information
03:38 - about my invoices to course careers I've
03:42 - done some development work for course
03:43 - careers it's the company that I have my
03:44 - software development course with and
03:46 - you'll see that it starts going through
03:48 - here and listing all of the different
03:49 - invoices that I've sent them giving me
03:51 - the information about it I kind of need
03:53 - to blur some of this out cuz I can't
03:55 - show you guys everything that I've done
03:56 - for them uh but yeah it kind of walks
03:58 - through how that works right invoice
04:00 - number 128 invoice number 115 and this
04:02 - is information that would usually take
04:04 - me at least a few minutes to find and
04:06 - it's quite cool that it can just really
04:08 - quickly look it up and share it with me
04:09 - in a quick summary view here so I just
04:11 - asked another style of question here can
04:13 - you find all of the receipts related to
04:15 - video editing obviously it was able to
04:16 - do that it did that super fast by the
04:18 - way as well that's the number one thing
04:20 - I've noticed with this it happens almost
04:22 - instantly I guess that's because it's
04:24 - running locally with a lot of other
04:26 - models I've used they're quite slow
04:27 - especially the ones that are hosted
04:29 - online I'm being R limited I'm waiting
04:31 - on I don't know a bunch of other people
04:33 - to get their replies and sometimes I sit
04:35 - there for 2 3 4 minutes waiting for a
04:37 - long reply to be generated whereas this
04:39 - it seems to happen almost
04:40 - instantaneously I love this for looking
04:42 - up information and I can already think
04:44 - of a bunch of different ways I could use
04:45 - this to really speed up my workflow
04:48 - especially when I'm trying to find
04:49 - different documents or files which for
04:51 - me is a bit difficult to do especially
04:53 - if I don't know like the file name or a
04:55 - specific way to search it here I can
04:57 - just use some natural language which is
05:00 - not something I'd be able to do in like
05:01 - the windows search bar right so now that
05:03 - we've looked at a quick demo let's talk
05:04 - about how you can start using this on
05:06 - your own computer and trust me it's
05:08 - quite a bit cooler when this works on
05:09 - your own machine it answers questions
05:11 - about your own data and files rather
05:13 - than seeing it work on mine anyways it
05:15 - is a free download you can get it from
05:16 - the link in the description just keep in
05:18 - mind it is demo and there are some
05:20 - pretty steep system requirements so
05:22 - let's get into those so first of all the
05:24 - platform is Windows it's not going to
05:26 - work on Mac or Linux and you do need to
05:28 - have an RTX 30 or 40 series GPU with a
05:31 - minimum of 8 GB of vram now that's
05:34 - because this is using RTX features to
05:36 - actually work properly so you do need a
05:38 - newer GPU in order for it to work now
05:40 - you're also going to need 16 GB of
05:41 - normal RAM and you're going to have to
05:43 - have 35 GB of dis space this download is
05:46 - 35 GB it took me an hour to download it
05:49 - and then going through the installer
05:51 - took almost 30 minutes where it
05:52 - downloaded and installed a bunch of
05:54 - other stuff as well so realistically
05:55 - you're probably going to need even more
05:56 - than that maybe 60 70 80 GB of space on
06:00 - your computer that said it is pretty
06:02 - cool and obviously it's all running
06:04 - locally so that's kind of the price you
06:05 - have to pay right if you want ultimate
06:07 - privacy and security you're going to
06:09 - have to run on your own computer which
06:10 - means you need a pretty insane computer
06:12 - with pretty high-end specs in order for
06:14 - this to actually work in my case I'm
06:16 - using a 4090 with 64 GB of RAM and it's
06:19 - working extremely fast so with all of
06:22 - that said let's quickly talk about how
06:24 - an application like this works now the
06:26 - first thing to understand is that behind
06:27 - the scenes we're really just using an
06:29 - open source llm now the llms that come
06:32 - with this by default are mistol and
06:33 - llama 2 llama 2 is the one provided by
06:36 - meta open source just means it's free we
06:38 - can use it however we want we can modify
06:40 - it we can fine-tune it different than a
06:42 - closed Source llm where we don't
06:44 - actually have access to the code now
06:46 - that's one of the reasons why the file
06:47 - size is so large here because we're
06:49 - actually downloading these llms and
06:51 - we're running all of their code on our
06:53 - local computer now what we do with these
06:56 - llms is we add additional data to them
06:58 - using something known as rag now before
07:00 - we dive into rag let's understand how a
07:03 - normal llm would work so if I just go to
07:05 - a normal llm and I ask it a question it
07:07 - can generate a pretty good response for
07:09 - me but it can only give me data based on
07:11 - what it was trained on so if I try to
07:13 - ask it you know what's the most recent
07:15 - YouTube video that Tech with Tim posted
07:17 - it's not going to be able to tell me
07:18 - that because it wasn't trained on that
07:20 - information it doesn't have access to
07:21 - that data it does have access to a bunch
07:23 - of historical facts it knows how to
07:25 - generate replies it can write code for
07:27 - me it could do a bunch of awesome stuff
07:29 - but when it comes to data availability
07:31 - and knowing kind of upto-date or recent
07:33 - information that's where it really
07:34 - struggles so that's why we have
07:36 - something called rag rag is retrieval
07:38 - augmented generation and all this does
07:41 - is ADD information into the llm that it
07:44 - can then reason based on it's just like
07:46 - taking the relevant information giving
07:47 - it to the llm and saying Hey Now use all
07:50 - of this new data and generate replies so
07:53 - obviously that's going to give us better
07:55 - more contextually relevant replies but
07:57 - there are some issues when it comes to
07:58 - giving so much data to an llm so we
08:01 - can't actually just give the llm all of
08:03 - the information we want it to work on we
08:05 - have to be a little bit more
08:06 - sophisticated and inject the right
08:08 - pieces of information with the prompt or
08:11 - in whatever way we're going to end up
08:12 - doing this so that it gives us the right
08:13 - reply so this is where the next stage
08:15 - comes in which is actually retrieving
08:17 - the information so what we'll first do
08:20 - kind of the first step when we ask a
08:21 - question to a rag application is we're
08:24 - going to go to something known as a
08:25 - vector store database and we're going to
08:27 - look up information relevant to the
08:29 - prompt that we've provided so we'll type
08:31 - in some prompt we'll ask some question
08:33 - we then will go to a vector store
08:35 - database which I'll talk about in a
08:36 - second and we'll pretty much look up
08:38 - information related to that prompt we'll
08:41 - then get Snippets of information from
08:43 - our data set we'll take that we'll
08:45 - provide that to the llm and we'll pretty
08:47 - much tell the llm hey now use this data
08:50 - and answer this prompt then it will do
08:52 - that and based on the data we've given
08:54 - it it will give us a better more
08:55 - contextually relevant reply as you saw
08:58 - when I was in that demo now how does a
09:00 - vector store database work well I don't
09:01 - want to get into this too much because
09:03 - it's a bit complicated but the first
09:04 - step is that we're going to take our
09:06 - entire data set so in this case all of
09:08 - our files and we're going to create
09:10 - something known as a vector embedding
09:11 - for them now a vector embedding is a
09:14 - representation in multi-dimensional
09:16 - space of the meaning and the context of
09:18 - a piece of data now what we do is we
09:21 - take all of our data we vectorize it we
09:24 - put it in this database and this gives
09:25 - us a really really fast efficient way to
09:28 - find different pieces of data so rather
09:30 - than searching through the entire
09:32 - database looking for every single piece
09:34 - of information that may be relevant we
09:36 - search using vectors again we don't need
09:38 - to get into the complexities and we can
09:40 - really quickly find the relevant
09:41 - information that we want and give that
09:43 - to the model if you're curious you can
09:45 - read more about this and you can learn
09:46 - about Vector databases but the idea is
09:48 - the first step is take all of our data
09:50 - create these Vector embeddings which can
09:52 - take a little bit of time to do we're
09:54 - going to store those vector embeddings
09:56 - and then we're going to use them when we
09:58 - actually want to ask questions about our
10:00 - data so you'll see if you start running
10:02 - this application on your own computer
10:03 - when you load in a new folder it's first
10:06 - going to vectorize that data so that we
10:08 - can really quickly look up the data and
10:10 - that's why we get such rapid replies so
10:12 - the process prompt find the data you
10:15 - want from the database inject it into
10:17 - the model and then get an accurate reply
10:19 - based on that data so that is my really
10:22 - high level overview of how this
10:23 - application works I hope it was helpful
10:25 - and gave you a little bit of insight
10:27 - into what rag is and how it's use with
10:29 - these llms that's going to wrap up this
10:32 - video I encourage you to download chat
10:33 - RTX from the link in the description and
10:35 - play around with it it's really really
10:37 - cool and I look forward to seeing you in
10:39 - another YouTube
10:41 - [Music]
10:47 - video

Cleaned transcript:

Nvidia just released their chat RTX application which is pretty impressive and solves the two biggest issues with llms today issue number one data access now sure I can copy in the contents of a file or an email I can upload a few documents to a model but it starts to get tricky when I want it to have access to hundreds of different documents or my entire operating system or entire directories on my computer and even if I could give a model access to all this information would I want to do that would I trust a company with all my private and sensitive data that's where issue number two comes in privacy we really cannot trust these Central companies we don't know what they're doing with our data if they're training on it if they're storing it and I don't know about you but I don't want random companies having access to my personal information but at the same time it would be super cool if I had a model that could answer questions about my personal data read through my files go through sensitive documents and do that in a secure way where I can still get information really quickly but I don't have to be worried about this being sent off or sold to some company now fortunately for us nvidia's chat RTX application pretty much solves all of these problems by running a free opensource model locally on your computer which has access to all of your different files that means I can ask it questions about my information get responses back extremely fast and know that all of my data is secure because it's not connected to the cloud a big thank you to Nvidia for sponsoring this video and let me share with you a quick demo of how this tool works so this is what the application looks like it's it's pretty straightforward and this is just running locally in my browser and I'll just quickly walk you through how it works so you can see that we can select a model that we want to use so mistol llama 2 we also could install other open source models if we want and then we have a data set section now the data set section is the cool part here where I can actually select a directory on my computer that I want to give the model access to now what this will use is something known as rag which is retrieval augmented generation going to talk more about that later in the video but the idea is is you can give the model some context about different files on your computer and this is all secure running locally and it's super fast and it can look up information find different things inside of for example PDF documents text documents or Microsoft Word documents and then immediately give you the result so let's test something out here I'll say can you give me a summary of my script related to micro services so this directory has a bunch of different files one of them is a microservices script for my YouTube channel and you can see that what it does here is it finds this document gives me a reference to it that I can click on and then it actually gives me a quick uh summary of that script so this is super cool and you can use this to really quickly look up different information and to have access to all of these different files within the model so let's imagine you had a directory with your flight itinerary and a bunch of different plans and maybe someone asks you hey what are you doing on Thursday rather than having to go through there and look for the information just say hey what am I doing on Thursday based on my flight plans or based on my itinerary point it to that directory and you instantly get that information this is super super cool I'll type in a few other examples here so you can get a sense of how it works so I just selected another folder here and this folder actually contains a bunch of accounting information for my business so invoices receipts tax information Etc now this is highly sensitive data that I wouldn't want to share with other people but that I might want to quickly get some information about without having to go search through all of the different files so just show you an example of doing that here and obviously blur anything out that's going to be too sensitive so I can say can you give me some information about my invoices to course careers I've done some development work for course careers it's the company that I have my software development course with and you'll see that it starts going through here and listing all of the different invoices that I've sent them giving me the information about it I kind of need to blur some of this out cuz I can't show you guys everything that I've done for them uh but yeah it kind of walks through how that works right invoice number 128 invoice number 115 and this is information that would usually take me at least a few minutes to find and it's quite cool that it can just really quickly look it up and share it with me in a quick summary view here so I just asked another style of question here can you find all of the receipts related to video editing obviously it was able to do that it did that super fast by the way as well that's the number one thing I've noticed with this it happens almost instantly I guess that's because it's running locally with a lot of other models I've used they're quite slow especially the ones that are hosted online I'm being R limited I'm waiting on I don't know a bunch of other people to get their replies and sometimes I sit there for 2 3 4 minutes waiting for a long reply to be generated whereas this it seems to happen almost instantaneously I love this for looking up information and I can already think of a bunch of different ways I could use this to really speed up my workflow especially when I'm trying to find different documents or files which for me is a bit difficult to do especially if I don't know like the file name or a specific way to search it here I can just use some natural language which is not something I'd be able to do in like the windows search bar right so now that we've looked at a quick demo let's talk about how you can start using this on your own computer and trust me it's quite a bit cooler when this works on your own machine it answers questions about your own data and files rather than seeing it work on mine anyways it is a free download you can get it from the link in the description just keep in mind it is demo and there are some pretty steep system requirements so let's get into those so first of all the platform is Windows it's not going to work on Mac or Linux and you do need to have an RTX 30 or 40 series GPU with a minimum of 8 GB of vram now that's because this is using RTX features to actually work properly so you do need a newer GPU in order for it to work now you're also going to need 16 GB of normal RAM and you're going to have to have 35 GB of dis space this download is 35 GB it took me an hour to download it and then going through the installer took almost 30 minutes where it downloaded and installed a bunch of other stuff as well so realistically you're probably going to need even more than that maybe 60 70 80 GB of space on your computer that said it is pretty cool and obviously it's all running locally so that's kind of the price you have to pay right if you want ultimate privacy and security you're going to have to run on your own computer which means you need a pretty insane computer with pretty highend specs in order for this to actually work in my case I'm using a 4090 with 64 GB of RAM and it's working extremely fast so with all of that said let's quickly talk about how an application like this works now the first thing to understand is that behind the scenes we're really just using an open source llm now the llms that come with this by default are mistol and llama 2 llama 2 is the one provided by meta open source just means it's free we can use it however we want we can modify it we can finetune it different than a closed Source llm where we don't actually have access to the code now that's one of the reasons why the file size is so large here because we're actually downloading these llms and we're running all of their code on our local computer now what we do with these llms is we add additional data to them using something known as rag now before we dive into rag let's understand how a normal llm would work so if I just go to a normal llm and I ask it a question it can generate a pretty good response for me but it can only give me data based on what it was trained on so if I try to ask it you know what's the most recent YouTube video that Tech with Tim posted it's not going to be able to tell me that because it wasn't trained on that information it doesn't have access to that data it does have access to a bunch of historical facts it knows how to generate replies it can write code for me it could do a bunch of awesome stuff but when it comes to data availability and knowing kind of uptodate or recent information that's where it really struggles so that's why we have something called rag rag is retrieval augmented generation and all this does is ADD information into the llm that it can then reason based on it's just like taking the relevant information giving it to the llm and saying Hey Now use all of this new data and generate replies so obviously that's going to give us better more contextually relevant replies but there are some issues when it comes to giving so much data to an llm so we can't actually just give the llm all of the information we want it to work on we have to be a little bit more sophisticated and inject the right pieces of information with the prompt or in whatever way we're going to end up doing this so that it gives us the right reply so this is where the next stage comes in which is actually retrieving the information so what we'll first do kind of the first step when we ask a question to a rag application is we're going to go to something known as a vector store database and we're going to look up information relevant to the prompt that we've provided so we'll type in some prompt we'll ask some question we then will go to a vector store database which I'll talk about in a second and we'll pretty much look up information related to that prompt we'll then get Snippets of information from our data set we'll take that we'll provide that to the llm and we'll pretty much tell the llm hey now use this data and answer this prompt then it will do that and based on the data we've given it it will give us a better more contextually relevant reply as you saw when I was in that demo now how does a vector store database work well I don't want to get into this too much because it's a bit complicated but the first step is that we're going to take our entire data set so in this case all of our files and we're going to create something known as a vector embedding for them now a vector embedding is a representation in multidimensional space of the meaning and the context of a piece of data now what we do is we take all of our data we vectorize it we put it in this database and this gives us a really really fast efficient way to find different pieces of data so rather than searching through the entire database looking for every single piece of information that may be relevant we search using vectors again we don't need to get into the complexities and we can really quickly find the relevant information that we want and give that to the model if you're curious you can read more about this and you can learn about Vector databases but the idea is the first step is take all of our data create these Vector embeddings which can take a little bit of time to do we're going to store those vector embeddings and then we're going to use them when we actually want to ask questions about our data so you'll see if you start running this application on your own computer when you load in a new folder it's first going to vectorize that data so that we can really quickly look up the data and that's why we get such rapid replies so the process prompt find the data you want from the database inject it into the model and then get an accurate reply based on that data so that is my really high level overview of how this application works I hope it was helpful and gave you a little bit of insight into what rag is and how it's use with these llms that's going to wrap up this video I encourage you to download chat RTX from the link in the description and play around with it it's really really cool and I look forward to seeing you in another YouTube video
