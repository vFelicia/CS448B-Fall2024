With timestamps:

00:02 - [Music]
00:08 - in this video i'm going to explain to
00:10 - you big o notation as well as how to
00:12 - perform time complex the analysis on
00:14 - various algorithms now i have a ton of
00:16 - examples and i want to make this video
00:18 - really focused on going through
00:20 - different examples and practicing
00:22 - performing time complexity analysis so
00:24 - if you're familiar with bigo notation
00:26 - skip forward in the video to those
00:27 - examples they'll be in the video player
00:29 - or in the description if you're
00:31 - unfamiliar with big o notation then
00:32 - stick around for the first few minutes
00:34 - where i give you a conceptual overview
00:36 - on the whiteboard now i will mention
00:38 - that i'm not going to be covering other
00:39 - time complexity analysis so we're not
00:42 - going to be looking at big theta big
00:44 - omega space complexity we are simply
00:46 - looking at big o notation and in a
00:48 - second i'm going to explain what that is
00:50 - now before we get into it i will mention
00:52 - that i do have a course it's called
00:53 - programming expert teaches fundamental
00:55 - programming object-oriented programming
00:57 - advanced programming tons of practice
01:00 - questions tons of projects you guys can
01:01 - check it out from the link in the
01:02 - description and use discount code tim
01:04 - with that said let's go ahead and get
01:06 - into the video all right so i want to
01:08 - begin by just discussing what big-o
01:10 - notation is very generally and why it's
01:12 - important for programmers of all types
01:14 - to actually understand this concept so
01:16 - big o notation is essentially a tool
01:18 - that we use to analyze different
01:20 - algorithms data structures and just
01:22 - sections of code and figure out
01:24 - relatively how much time they're going
01:26 - to take to execute based on the
01:28 - approximate number of operations that
01:30 - need to be completed in the worst case i
01:33 - understand that sounds a little bit
01:34 - confusing but the idea is we want to be
01:36 - able to look at two different algorithms
01:38 - and very quickly determine which one is
01:40 - going to be more efficient based on its
01:42 - big o notation so big o notation again
01:45 - is a tool we're using it to compare
01:47 - different solutions different algorithms
01:48 - different data structures the algorithm
01:50 - is data structures that have the best
01:52 - big-o notations are typically the ones
01:54 - that we prefer because they're going to
01:56 - be the most efficient they're going to
01:57 - take the least amount of system
01:59 - resources or they're at least going to
02:00 - execute the fastest now big-o notation
02:03 - is really concerned with what happens
02:05 - when we scale our problem to a very
02:07 - large size so we're not really wondering
02:09 - what's going to happen when we sort 5
02:11 - numbers or 10 numbers we're concerned
02:13 - with what's going to happen when we sort
02:14 - a million numbers if we're comparing say
02:16 - different sorting algorithms or what
02:18 - happens when we pass the worst case
02:20 - scenario of numbers to be sort so big
02:22 - annotation worst case scenario for very
02:25 - large inputs that's what we're focused
02:27 - on okay that is a very general
02:29 - introduction to big o notation the
02:31 - reason it's important for us to
02:32 - understand this is because there's a lot
02:34 - of scenarios where we can write code
02:36 - that solves a problem but doesn't do it
02:38 - in an efficient manner and the only way
02:40 - we know it's not happening efficiently
02:42 - is if we understand relatively how many
02:45 - operations this code takes compared to
02:47 - another more optimal solution so this is
02:50 - our comparison method without this
02:52 - method you really have no way of
02:54 - understanding you know how long your
02:56 - code is going to take to execute and
02:57 - that means that it might work for a
02:58 - small input but if you pass it say a
03:00 - thousand numbers or ten thousand numbers
03:02 - or a large list of strings or whatever
03:04 - the input type is it may take a really
03:06 - really long time to execute and may even
03:08 - be infeasible there's a lot of
03:10 - algorithms that you and me have probably
03:11 - both written in the past where if you
03:13 - fed them say a million numbers they may
03:16 - actually take years decades hundreds of
03:18 - years to execute based on how
03:20 - inefficient they are and just the math
03:22 - that goes on behind the scenes here
03:24 - anyways we're going to start looking at
03:25 - a ton of examples i'm going to hop over
03:27 - to the whiteboard and start explaining
03:29 - to you kind of the theory behind big o
03:31 - and how we find the big-o notation of
03:33 - specific algorithms all right so i'm
03:35 - here on the whiteboard i'm going to ask
03:37 - you to please not be too intimidated by
03:39 - the math that you're already seeing it's
03:41 - going to be really simplified once i
03:42 - start going through a few examples so as
03:44 - i mentioned before when we're talking
03:46 - about big o notation to actually find
03:48 - the big o notation or the big o function
03:50 - that represents an algorithm or a data
03:52 - structure or whatever it is we need to
03:54 - first know how many operations a
03:57 - algorithm is going to take okay so the
03:59 - first step that we always do when we're
04:01 - analyzing something for its big o
04:03 - notation to try to compare it against
04:04 - something else is we figure out the
04:06 - approximate number of operations that
04:08 - this algorithm takes we're then going to
04:10 - use those to simplify and find the big o
04:12 - function or the big o notation okay so
04:15 - let's write a super simple example here
04:17 - let me just get clicked into this screen
04:20 - and we'll start going through
04:21 - so i'm going to say total
04:23 - is equal to 0 i'm going to say 4 i in n
04:27 - i'm going to explain what n is in a
04:28 - second and hello catch just decided to
04:31 - join me here i'm going to say
04:33 - total plus equals i and then i'm just
04:35 - going to print i
04:37 - okay so this is my out now the first
04:39 - thing to discuss here is n you're going
04:41 - to see this a lot n refers to the size
04:44 - of our input so as i mentioned before
04:46 - when we're talking about notation we're
04:48 - really concerned with what happens when
04:49 - we have really large inputs
04:52 - so n is just a placeholder to represent
04:54 - the size of our input and we write our
04:56 - big o function and the number of
04:58 - operations relative to n so in this case
05:01 - here if we're counting the number of
05:03 - operations that are going to occur we
05:05 - have one operation to define the
05:06 - variable we're going to have n loops
05:09 - that's going to happen n times because n
05:10 - is the size of our input right so i'm
05:13 - going to say n and then we have two
05:15 - operations happening inside of the for
05:17 - loop so if this for loop happens n times
05:20 - and we have two operations we have a
05:21 - total of two n operations happening in
05:24 - the for loop so we're going to say 2n
05:26 - plus 1 is the number of total operations
05:29 - that this algorithm is going to take
05:31 - because we have 1 here and then n times
05:33 - 2 2n hopefully that is clear so what
05:36 - we're going to do is we're going to
05:37 - write this as what's known as our f of n
05:40 - okay i'm going to start going through
05:41 - this definition in a second let me just
05:43 - erase this part here and move over the
05:46 - 2n plus 1.
05:47 - okay so actually we'll just rewrite it
05:48 - over here
05:50 - okay so f of n
05:52 - is going to be equal to 2n plus 1 and f
05:55 - of n is going to represent the number of
05:56 - operations that our algorithm takes
05:59 - so now let's look at this definition so
06:01 - this says that f n
06:03 - is equal to the big o of g of n
06:06 - so where f is big o of g you can read it
06:09 - that way if there exists constants
06:11 - capital n and c so that for all n
06:14 - greater than or equal to uppercase n
06:16 - this inequality is true now it's
06:18 - important to note here that c and
06:20 - capital n have to be at least one so one
06:23 - or greater uh and we'll go through this
06:26 - example so this makes a bit more sense
06:27 - so let me just write out this inequality
06:29 - we have f of n
06:31 - needs to be less than or equal to c
06:34 - times g of n so just to stop and explain
06:36 - something here g of n is the function
06:38 - that we're looking for which is going to
06:40 - be the big o function or the big o
06:42 - notation of f of n or of our algorithm
06:45 - okay g of n is really what tells us
06:48 - approximately how long this code is
06:51 - going to take to run
06:52 - based on the input in this case n okay
06:55 - so everything refers to n again we're
06:56 - really concerned with what happens as n
06:58 - gets to a very very large number so we
07:01 - need to find something that satisfies
07:02 - this inequality and we want this right
07:04 - hand side here to be greater than or
07:06 - equal to n but be as close to n as
07:08 - possible the reason i'm saying that is
07:10 - because any of you who are kind of
07:12 - working this out in your head already
07:13 - can probably realize if i substitute in
07:16 - 2n plus 1 for f of n here and i say less
07:18 - than or equal to c times you can pick
07:20 - some very very massive function maybe
07:22 - something like n to the exponent 4 and
07:24 - well this is going to be true doesn't
07:26 - actually matter what value i pick for c
07:28 - it doesn't matter what value i pick for
07:30 - uppercase n this is always going to be
07:32 - true so i don't want to just pick an
07:33 - arbitrarily large function i want to
07:35 - find a function that's as close as
07:37 - possible to this one right here that
07:40 - still makes this true
07:42 - okay
07:43 - again i know this is confusing we're
07:44 - going to keep going through it there's
07:45 - just a lot to get through before you can
07:46 - really even look at an example so number
07:48 - of operations is 2n plus 1. we have f of
07:50 - 1 f of n is less than or equal to c
07:52 - times g of n and that's going to be for
07:55 - all values n that are greater than
07:57 - uppercase n so there's kind of two
07:58 - things that we need to pick here now let
07:59 - me just try to erase this okay so let's
08:02 - now write the inequality again 2n plus 1
08:05 - less than or equal to c times g of n
08:08 - and this is 4
08:10 - n greater than or equal to uppercase n
08:12 - so there's three things that we need to
08:13 - find to prove this true now what i'm
08:15 - going to do is just randomly pick a
08:16 - function for g of n and show you how
08:18 - this works so i'm going to say 2n
08:21 - plus 1 is less than or equal to c times
08:24 - and then the function i'm going to pick
08:25 - for g of n
08:26 - is simply n okay just the regular linear
08:29 - function i'm picking n
08:31 - so now that we have this and again this
08:32 - is going to be for here i'm not going to
08:33 - rewrite that we need to find some c that
08:36 - makes this true and if we find some c
08:38 - that does make it true as well as some
08:40 - uppercase n then we know that this is
08:42 - kind of our big o function that's what
08:44 - i'm going to be calling it
08:45 - all right so for c we can find this
08:48 - pretty easily we can just pick a very
08:49 - large number right i can pick a number
08:51 - like 10 i can pick a number like even 3
08:54 - here that'll make it true essentially
08:56 - anything other than 2 or 1 is going to
08:58 - make this true and remember c has a
09:00 - minimum value of 1. so if i pick c
09:02 - equals 3 then that gives me the
09:04 - inequality 2n plus 1 is less than or
09:07 - equal to 3n
09:08 - okay great and then i need to just find
09:11 - some value uppercase n where all n
09:13 - values past that or equal to it make
09:15 - this true and really i can actually pick
09:17 - the value 1 for this and it will make
09:19 - this true right so if i pick
09:21 - this so n is equal to 1 then we can just
09:24 - plug in 1 and we'll see that this is
09:26 - always going to be true so when i plug
09:27 - this in i get 3 less than or equal to 3
09:29 - so that's if 1 is 4n and if i were to
09:32 - plug in 2 3 4 5 so on and so forth
09:34 - you're going to see that this side here
09:36 - is always going to be less than or equal
09:38 - to or sorry greater than or equal to
09:39 - this
09:40 - now what does that mean
09:42 - that means that g of n is the big o
09:44 - notation of f n
09:46 - so i can say f n is equal to the big o
09:50 - of n and that's how you write this say f
09:52 - of n is equal to big o of n or the
09:54 - algorithm has a time complexity of big o
09:57 - of n
09:58 - okay
09:59 - again i know this is confusing but i've
10:00 - just showed you very very quickly how we
10:02 - kind of use this mathematical definition
10:04 - to find a g of n now what does g of n
10:07 - really mean well g of n means that as
10:09 - the input increases the amount of time
10:12 - that our function takes to execute
10:14 - increases linearly okay increases on a
10:16 - straight line as n increases in a linear
10:19 - function way the amount of operations
10:21 - increase so if n is one we have one
10:23 - operation if n is two we have two
10:25 - operations it's not telling us the exact
10:27 - number of operations because we know
10:28 - that's two n plus one it's telling us
10:31 - relatively what happens to the amount of
10:33 - time it takes for us to execute when the
10:36 - input value for the function gets
10:38 - increasingly large that's the point
10:40 - hopefully this is coming across that's
10:42 - what we're doing with big o of n here so
10:44 - yes big o of n is actually less than two
10:46 - n plus one that's fine we don't care
10:48 - about the exact number of operations we
10:50 - care what happens to the amount of time
10:52 - it takes when the input increases and
10:54 - now that we have this function n we know
10:57 - that it increases linearly the amount of
10:59 - time so let's clear all of this
11:01 - and let's just go through one more
11:02 - example here and look at some of the
11:04 - big-o functions which you can see on
11:05 - kind of the right-hand side of my screen
11:08 - okay so let's say we have a function
11:10 - here that is something like
11:13 - two n squared
11:14 - plus n plus one okay this is the amount
11:17 - of operations that it takes we'll say
11:19 - this is f of n should have wrote it in
11:20 - the other way but that's fine now i'm
11:22 - going to write the rest of this in black
11:23 - here uh just so you're aware okay so now
11:25 - we want to find g of n for this so what
11:28 - do we do well we're going to write this
11:29 - out 2n squared plus n plus 1 is less
11:33 - than or equal to c times g of n
11:36 - for n greater than or equal to n okay so
11:39 - now let's pick something for n
11:42 - or pick something for g of answer so 2 n
11:44 - squared
11:45 - plus n plus 1 less than or equal to c
11:48 - times what if we pick n now if we pick n
11:51 - immediately we're going to see here that
11:52 - it doesn't actually matter what constant
11:54 - we give even if we give a constant like
11:56 - a thousand when i start passing really
11:59 - really large values for n n squared is
12:01 - going to exceed c times n very quickly
12:04 - right so if i pass even a huge value for
12:06 - n uh like you know a thousand ten
12:08 - thousand you're gonna see that this side
12:10 - here is gonna be a lot greater than this
12:12 - side here even though i'm multiplying it
12:13 - by a thousand now that's not something
12:15 - you're necessarily gonna prove but you
12:16 - just need to be able to look at that and
12:18 - kind of figure that out when i have an n
12:20 - squared and i have an n over here
12:22 - doesn't matter what the constant is this
12:24 - is always going to outgrow this side
12:25 - eventually when we get to a very large
12:27 - number of n so that tells me that i
12:29 - cannot find some c and i cannot find
12:31 - some n and so n is not going to be a
12:33 - valid function for g of n so now instead
12:35 - let's try n squared so now if i have n
12:38 - squared i'm actually going to be able to
12:40 - prove that i can find some constant c so
12:42 - to prove this again i just need to find
12:44 - some c and i need to find some n so
12:46 - let's look for a number for c let's just
12:48 - pick a thousand okay so if i say
12:50 - c is equal to a thousand now we can plug
12:53 - this in so we're gonna say a thousand n
12:56 - squared and i don't know why i'm writing
12:59 - these in different colors but i just am
13:00 - i'm gonna say two n squared plus n
13:03 - plus one
13:04 - now it actually doesn't really matter
13:06 - what value i pick for uppercase n here
13:09 - in fact i can pick any value and you're
13:11 - gonna see that we get
13:13 - uh the correct inequality here okay
13:15 - because i've picked a large enough
13:16 - number for c now you can make a smaller
13:18 - number of receipt it doesn't matter what
13:19 - number you're picking you're just
13:20 - picking that to prove that this
13:21 - inequality is true so for example if we
13:24 - plug in say one right we have two
13:27 - one
13:28 - plus one plus one
13:30 - that's going to give us four and then
13:31 - this right is going to give us a
13:33 - thousand so this is larger if we pick 2
13:35 - this is going to give us 4 000 this is
13:37 - going to be a much smaller number so
13:38 - you're going to see that this side is
13:39 - going to be greater so there we go we
13:41 - now know that the function here is going
13:43 - to be n squared so what we can do is we
13:45 - can write
13:47 - f of n is equal to big o of n squared
13:51 - now you might start be starting to
13:52 - realize a pattern here and the pattern
13:55 - is you can essentially look at the
13:56 - largest term when you have things being
13:58 - multiplied together and understand that
14:01 - that is going to be what the big o
14:03 - function is now there's a few tricks and
14:05 - i have to go through a bunch of them but
14:06 - the point is here when we have something
14:08 - like 2n squared plus n plus 1 if my
14:11 - function g of n is simply n squared i'm
14:13 - going to be able to find a large enough
14:15 - constant that it doesn't matter what the
14:16 - other terms are here it's always going
14:18 - to be greater than this so let me just
14:21 - stop here for one second go through this
14:22 - example again and make sure it's clear
14:23 - because i get this is confusing and this
14:25 - is something that you have to look at a
14:26 - bunch of times before really starts to
14:28 - actually make any sense so i have 2n
14:31 - squared plus n plus 1 that's the number
14:33 - of operations that my algorithm takes
14:36 - now i'm trying to find a simplified
14:37 - function that is going to give me a
14:39 - general concept of what happens to the
14:42 - amount of time or the amount of
14:44 - operations required from this algorithm
14:46 - as the input grows very very very large
14:49 - so again we're thinking of really large
14:50 - values of n so when the values of n grow
14:53 - really large what's going to happen is
14:55 - this term right here is going to be
14:57 - dictating the number of operations that
14:59 - are occurring if you pick a value even
15:01 - something simple like 10 right the 2
15:04 - times n squared gives you 200 the n
15:06 - gives you 10 and the 1 gives you 1. so
15:09 - these become increasingly less important
15:11 - the n plus 1 as n squared gets larger
15:14 - and larger and the same thing happens
15:16 - with this 2 here this becomes
15:18 - increasingly less important as n squared
15:20 - gets larger and larger and larger right
15:23 - that's really the point here as n gets
15:25 - larger all of the constants that we have
15:27 - here n 1 and 2 are going to become less
15:30 - important now the n is not a constant i
15:32 - know but it still becomes less important
15:34 - because n squared is much much larger
15:36 - than n that's what we're looking at so
15:39 - what we do is we get the bigger notation
15:40 - of n squared because this is the term
15:43 - that most accurately represents what
15:45 - happens to the amount of output or i
15:47 - guess the time to execute our function
15:49 - as the input increases
15:51 - and again the way we did that is we're
15:53 - using the math here to prove this i'm
15:54 - going to show you a bunch of shortcuts
15:56 - so you don't have to do this math every
15:57 - time but it's important just to see kind
15:59 - of how this works so you understand the
16:01 - math that goes behind this and how you
16:03 - prove
16:04 - you know that your bigger notation is
16:05 - actually correct okay so now let's just
16:08 - look at a few more functions and we're
16:10 - going to do them in a really fast way
16:11 - and i'm going to show you a few
16:12 - shortcuts that are going to make this
16:13 - seem really easy so let's say we have f
16:16 - of n
16:16 - is equal to 3 and let's go with n cubed
16:21 - plus n squared plus and then we're going
16:24 - to go uh oops let's go like this
16:27 - log base 2 of n
16:30 - okay so notice these are all pluses
16:31 - right so i have 3n squared plus n
16:33 - squared plus log base 2 of n
16:35 - now a few shortcuts that you can keep in
16:37 - mind here is whenever you have terms
16:39 - that are being added together so
16:40 - additive terms you can essentially cross
16:42 - out and remove every single term and
16:45 - constant that is not the largest term
16:48 - that you have in the expression so again
16:50 - the reason that you can do this is
16:52 - because as n increases and reaches
16:54 - infinity
16:55 - n cubed is going to dictate the amount
16:57 - of operations that this is going to take
17:00 - let's just plug in a value and have a
17:01 - look at it right all right so let's just
17:02 - plug in 10 here and see what we get so
17:04 - we're gonna have i guess three times a
17:08 - thousand
17:09 - plus this is going to be a hundred plus
17:11 - approximately six point six so if we
17:14 - look at this we're gonna see
17:16 - these terms right even the three become
17:18 - increasingly less important as n gets
17:20 - large and n isn't even that large n is
17:22 - ten imagine what happens now when n is
17:24 - ten thousand ten million the n cubed
17:27 - really carries the number of operations
17:28 - and so we don't care about these ones
17:30 - right here and we can cross them off
17:32 - when it comes to our big-o notation now
17:34 - to be able to actually cross these off
17:36 - you can do a bit of math to kind of
17:37 - prove that you can cross them off so
17:39 - what i'm going to say here is let's
17:41 - assume that n squared and log base 2 of
17:44 - n is equal to n cubed so the reason i
17:46 - can assume that is because n cubed is
17:48 - greater than n squared let's assume that
17:51 - n squared is equal to n cubed now i know
17:55 - this is not sound math but just assume
17:57 - that n squared is equal to n cubed if it
17:59 - is then what i can do is change all my n
18:01 - squared terms to n cubed so if i change
18:04 - n squared here 2n cubed which is larger
18:06 - than it so it's vowel for me to do that
18:08 - because i'm adding something in that's
18:09 - larger than n squared and i change my
18:11 - log base 2 of n to the same thing n
18:14 - cubed what you see here is we end up now
18:16 - if we factor this we get 5 n cubed now
18:20 - again the 5 is a constant we don't care
18:22 - about the constant because we don't need
18:23 - to know the exact number of operations
18:25 - we just want to have the function that
18:27 - tells us what happens as the input gets
18:29 - larger so n cubed is that function so we
18:31 - simply strip off the 5 and we say that
18:33 - the bigger notation here is n cubed
18:36 - that's the way that we do this so if you
18:38 - take every single other term that you're
18:40 - adding it's very important that you
18:41 - realize we're adding these terms here
18:43 - and you say okay well if it's smaller
18:44 - than n cubed let's assume it's n cubed
18:46 - now if we add all of these together and
18:48 - not factor so i said factor four if you
18:50 - add all of them together we get 5n cubed
18:52 - we know we don't care about the
18:53 - constants because we're just looking for
18:55 - a function to give us
18:57 - the relative amount of operations then
18:59 - what we can do is cross off the constant
19:00 - which would have been 5 and we're left
19:02 - with n cubed
19:03 - okay so that's one example let's now
19:05 - look at another example that has to do
19:08 - with
19:08 - multiplying terms so let's now say we
19:10 - have something like 3n and we're going
19:12 - to multiply this by the log base 2 of
19:16 - and then this will be n squared
19:18 - okay so this is actually what we have
19:20 - now what we want to do is simplify this
19:22 - and find g of n so we can find the big o
19:24 - notation so for this function here the
19:25 - first thing that we know we can do is
19:27 - remove the constants again we can remove
19:29 - them because we don't care about
19:30 - constant values we only care about a
19:32 - function based on the input that tells
19:34 - us what happens to the number of
19:35 - operations or the expected kind of
19:37 - running time of the algorithm based on
19:39 - the size of the input so we can remove
19:41 - the constant so we remove the constant 3
19:43 - and that leaves us with n multiplied by
19:46 - log base 2
19:47 - of n squared okay now we don't really
19:50 - know how to handle this n squared inside
19:52 - of here and i'm going to show you how we
19:54 - can simplify this even more you could
19:55 - technically say that this is the big o
19:57 - notation right now we can get even
19:58 - simpler we can take this 2 out based on
20:01 - using exponent rules i'm not going to
20:03 - derive these rules for you i assume most
20:04 - of you probably know this already at
20:06 - least if you're in high school let me
20:08 - write log properly
20:10 - we're going to be able to take the 2 out
20:11 - right before the log
20:13 - and then we can put this here now again
20:15 - the reason we take the 2 out is because
20:17 - whenever we have something to the
20:18 - exponent inside of a logarithm that
20:21 - exponent we can simply put as a
20:22 - multiplication to whatever the logarithm
20:24 - is of the base of that number
20:27 - and so that's what i'm doing so i'm
20:28 - saying 2 log base 2 of n i can take the
20:30 - 2 out now the 2 is what a constant we
20:33 - don't care about the constant so i can
20:34 - remove the constant okay so now if we
20:36 - continue i get n and then i have log
20:40 - base 2 of n now can i simplify this
20:43 - anymore no i cannot i cannot simplify
20:45 - because we are multiplying these terms
20:47 - not adding them if we were adding the
20:49 - terms together i could remove the log
20:51 - base 2 of n because it's smaller than n
20:53 - but i cannot in any way here find a way
20:56 - to actually write this as a constant
20:58 - multiply by n like previously if i had n
21:01 - plus and then i had log base 2 of n i
21:04 - can say that since log base 2 of n is
21:05 - smaller than n i can write this as n
21:07 - plus n that gives me 2n and then i can
21:10 - cross out 2 but here since we're
21:11 - multiplying the terms together i don't
21:13 - have a way to do that and so i can't
21:15 - simply remove the log base 2. okay if
21:17 - you wanted the math version of that
21:18 - that's why it also makes sense here
21:20 - since we have two factors being
21:21 - multiplied they both really affect how
21:23 - much time this is taking so we need to
21:25 - keep n log base 2 of n around
21:28 - okay
21:29 - now that we've looked at that let's just
21:30 - have a look at what's on the right-hand
21:31 - side of my screen and then let's get
21:32 - into some some examples because i'm sure
21:34 - a lot of you are tired of this math
21:36 - so on the right-hand side of my screen
21:38 - here we have the big-o complexity chart
21:40 - now let's just see if we can make this
21:41 - bigger
21:42 - so that we can take up the full screen
21:44 - with it nice okay so this is the chart
21:46 - now it actually is pretty good i don't
21:48 - really need to explain it too much but
21:49 - it simply shows you the big o functions
21:51 - and which ones are better than each
21:53 - other because once you find the big o
21:54 - notation that's great but you need to
21:56 - know which functions are better than
21:58 - other functions in terms of their
22:00 - complexity now we've already looked at
22:01 - log of n
22:03 - log of n is great that means that
22:04 - essentially as the input increases the
22:06 - number of operations we need to do is on
22:09 - a logarithmic basic basis which means
22:11 - it's getting smaller and smaller when
22:13 - log is equal to a hundred we have six
22:15 - when log is equal to uh 128 we have
22:17 - seven when log is equal to 256 uh we
22:21 - have eight right
22:22 - so on and so forth now i might have been
22:24 - messing those up a little bit you get
22:25 - the point as we get to a really really
22:26 - large number it stays really small when
22:28 - we have a logarithmic time complexity
22:32 - then we have a constant time complexity
22:33 - o of one this means that there's
22:35 - actually nothing based on the input this
22:37 - would be like printing a number or you
22:40 - know checking if a number is odd or even
22:42 - performing a modulus this is a constant
22:44 - time operation this isn't something that
22:47 - requires the input so if it doesn't
22:49 - matter to the input at all it only ever
22:51 - takes 100 steps or 50 steps or a
22:52 - thousand steps a constant number of
22:54 - steps then we have o1 a constant time
22:57 - o n this is linear this is a fair or
23:00 - good time complex if you have a linear
23:01 - time complexity usually that's pretty
23:03 - good it means your algorithm's time
23:05 - scales with the input so as the input
23:07 - increases the amount of operations uh
23:09 - increase relatively at the same amount
23:11 - if we have 10 inputs we have you know
23:13 - about 10 operations if we have 20
23:15 - operations or 20 inputs we have 20
23:18 - operations now it might actually be 100
23:20 - operations or 200 operations but that's
23:22 - fine because it's happening linearly not
23:24 - exponentially
23:26 - or any of the other complexities that
23:27 - we're going to look at
23:28 - okay continuing with n log n now n log n
23:31 - is kind of a middle ground here as you
23:33 - can see we're multiplying a linear time
23:35 - complexity by a logarithmic so it's not
23:37 - horrible it's not as bad as something
23:38 - like n squared but it is you know fair
23:41 - it's going to take a long period of time
23:43 - if we have a lot of inputs again all of
23:46 - these are time estimates used to be
23:48 - compared against each other we're not
23:49 - looking at an exact amount of operations
23:51 - or exact amount of time so n log n fair
23:53 - time complexity it is better or sorry
23:55 - worse than a big o of n
23:57 - okay now we have n squared n squared is
23:59 - a horrible time complexity that means
24:01 - that we increase the number of
24:03 - operations we need to do exponentially
24:05 - as n increases and i'm sure many of you
24:07 - have looked at exponents before
24:09 - not very good when n gets to a large
24:11 - number say we add 10 we have n squared
24:13 - that gives us 100 already keeps getting
24:15 - larger and larger and larger then we
24:17 - have something like 2 to the exponent n
24:18 - now this is even worse this is a very
24:21 - very poor time conflict as you can see
24:23 - then we have something like n factorial
24:25 - and we have even more complex time
24:26 - complexities where we have something
24:28 - like n factorial multiplied by 2n
24:31 - plus m and would be another input i'll
24:33 - talk about that later but this is the
24:35 - general graph so you kind of need to
24:36 - memorize this graph it's pretty easy you
24:38 - could just plug in some values and
24:39 - actually see which ones are getting
24:41 - larger faster if you are unsure but
24:43 - these are the most important big o
24:44 - functions and we're going to be using
24:46 - combinations of these functions when
24:48 - we're analyzing the time complexity of
24:50 - the examples i'm about to go through so
24:52 - i apologize i know this was long but i
24:54 - just wanted to try to explain this as
24:56 - best as i could hopefully that made
24:57 - sense now we're going to go through 12
24:59 - examples and i'm going to walk you
25:00 - through exactly how we solve them all
25:02 - right so we're moving on to the first
25:04 - example here example zero i'm going to
25:06 - ask you right now what is the time
25:08 - complexity or big o notation of this
25:10 - function think about it pause the video
25:12 - if you need to i'm about to go in and
25:14 - explain
25:15 - so the answer here is that this is
25:17 - constant time okay now i'll explain why
25:20 - it's constant time but this is big o of
25:22 - one
25:23 - so inside of here what we're doing is
25:25 - we're performing a constant number of
25:27 - operations
25:28 - it does not matter what num is and num
25:31 - is going to be n right num is n because
25:33 - that's our input so whatever the size of
25:35 - num is we represent that with the
25:37 - variable n and it doesn't matter what
25:39 - this is it's always going to take us
25:41 - approximately the same amount of time to
25:42 - perform this calculation right the
25:44 - modulus of 2 or the num modulus 2 and
25:47 - we're just checking if this number is
25:48 - odd or even that's what we're doing here
25:49 - so this is a constant time operation
25:52 - fairly straightforward now i want to
25:53 - mention that if i did something like x
25:55 - equals 1 y equals 3 whatever this is
25:57 - still a constant time operation i don't
26:00 - write something like o of 3 i just keep
26:02 - it at 0 1 which means we're going to do
26:04 - the same amount of steps no matter what
26:06 - n is no matter what the size of our
26:08 - input is and as we go through these next
26:10 - examples you're going to see that i've
26:12 - labeled inputs different values
26:14 - you know numbers strings whatever
26:17 - you still want to represent them using n
26:20 - n is the length of your input or the
26:22 - size of your input in every single
26:23 - situation okay so let's now go to
26:25 - example one
26:27 - example one we've seen this example kind
26:29 - of already i've total equal zero for
26:31 - number in nums total plus equals number
26:34 - return total pause the video what is the
26:36 - big o notation of this example or of
26:38 - this algorithm right here
26:40 - okay so the answer as i'm sure many of
26:42 - you have gotten and probably fairly
26:44 - quickly is big o of n now n is
26:47 - representing the number of numbers that
26:49 - we have in nums so the size of our input
26:52 - now what we do here to calculate this is
26:54 - exactly what we did in that first
26:56 - example that i showed you we have a
26:58 - variable this takes one one operation
27:00 - right
27:01 - we have a for loop this is going to take
27:03 - n it takes n because the number of
27:05 - numbers
27:06 - is n right so this for loop runs n times
27:08 - we're doing one operation so what we end
27:10 - up getting here at the end you could
27:12 - count this as an operation if you want
27:13 - as well is something like
27:15 - n plus one or n plus two whatever you
27:18 - want to write okay so we'll say it's n
27:19 - plus 1. if we have n plus 1 we know that
27:22 - we can simply remove the constants which
27:24 - are 1 because as n gets larger they're
27:26 - not going to matter and write the big o
27:28 - notation as big o of n
27:30 - okay so that's that example
27:32 - let's move forward okay now this one is
27:33 - getting a bit more complicated
27:35 - now we have
27:36 - an example where we have a nested for
27:38 - loop so again pause the video go ahead
27:40 - and read this and it's going to get even
27:42 - more complicated as we go on all right
27:43 - so let's go line by line we have results
27:46 - here and results we're creating a list
27:48 - now this list we have a for loop here
27:50 - we've got to look at all of our loops
27:51 - okay and any operation that could be
27:53 - looping and what happens is we need to
27:56 - 4n right so for underscore in range len
27:59 - of nums lenum nums really is just n
28:01 - we're adding 1 into the list so this is
28:03 - going to take us n to do because we're
28:05 - adding 1 n times
28:07 - pretty straightforward
28:08 - move on to the next loop okay inside of
28:10 - this next loop well how many times does
28:12 - it take for this loop to run this loop
28:14 - is going to take n times to run because
28:16 - we're doing it for the size of our input
28:18 - which is n
28:19 - okay now we have another for loop inside
28:22 - of this for loop now that means that
28:23 - this for loop needs to run how many
28:25 - times
28:26 - n times and how long does it take for
28:29 - the for loop to run well this for loop
28:31 - is looping through nums as well and so
28:33 - that gives us
28:35 - n okay so we have n stuff happening n
28:38 - times because this for loop is happening
28:40 - now inside of here we can count the
28:42 - operations we could say this is an
28:43 - operation this is an operation and this
28:45 - is an operation now remember we're
28:47 - considering the worst case so in one
28:50 - case when num1 is equal to num2 we're
28:52 - not going to perform this operation
28:54 - however we don't really care about that
28:56 - we're not going to factor that into our
28:58 - calculations here because this
29:00 - could happen right it's happening every
29:02 - time except once so we're just going to
29:04 - assume that this operation happens the
29:07 - end times for this for
29:08 - hopefully you get what i'm saying but
29:10 - i'm just trying to get through like
29:11 - don't over complicate analyzing the
29:13 - number of operations here we're just
29:15 - trying to get a rough estimate and then
29:16 - we can actually figure out what the big
29:17 - o notation is
29:19 - okay so we have about three operations
29:21 - at most we have to perform a condition
29:22 - we're going to continue we could count
29:24 - as an operation i'm going to do a
29:25 - multiplication we'll just sum all of
29:27 - those up even though they're not all
29:28 - going to happen together okay so we say
29:30 - approximately we're going to get three
29:31 - operations inside of here then we're
29:33 - returning results okay that's another
29:35 - operation so we have n
29:37 - now we're adding that because this is it
29:39 - not inside the for loop where it's the
29:40 - next line down to n
29:43 - okay and then how many times or
29:45 - how many operations are going on inside
29:47 - of this n
29:49 - well we have three n so we're going to
29:51 - say n and then this is just going to be
29:53 - multiplied by 3n now i get this a bit
29:56 - confusing but what's happening is this
29:57 - for loop is happening
29:59 - every single time this for loop runs so
30:01 - we're doing this for loop n times which
30:03 - means we're going to have n
30:05 - times n right two n's there so that's n
30:09 - squared and then we're doing three
30:11 - operations inside of this internal for
30:12 - loop and so what we end up getting is
30:14 - three n squared if we multiply these all
30:16 - together we're gonna get n plus
30:18 - three n squared and then we have one
30:19 - more operation here so we add one
30:22 - so this is going to be the you know
30:24 - approximate number of operations that
30:26 - we're doing inside of this algorithm now
30:28 - what do we do well we look for the
30:30 - largest term
30:31 - which is n squared we cross off all of
30:33 - our constants and then whenever we have
30:35 - a variable we say okay was this term
30:37 - smaller or larger than this one well
30:39 - it's smaller so if we assume it's n
30:41 - squared then we could write this as 2n
30:43 - squared which means i can now get rid of
30:44 - that and that means my big o notation is
30:46 - going to be n squared i know i made a
30:47 - complete mess here but this is going to
30:49 - be n squared okay that's the big o
30:51 - notation let me get out of this mass
30:53 - let's look at it here
30:54 - in the code edit so we perform an n
30:57 - operation
30:58 - that happens okay then we go inside of
31:00 - here we have an n times n operation
31:03 - because we're doing this for n times for
31:05 - this for loop which happens n times we
31:07 - need to multiply them together
31:08 - inside this for loop we're doing three
31:10 - things so we have three n squared that's
31:12 - how long all of this takes to run so we
31:15 - add that to n
31:16 - add that to one and we get 3n squared
31:19 - plus n plus 1. we know the other terms
31:22 - are smaller so we cross them off and
31:24 - that leaves us with the big o notation
31:25 - of n squared
31:27 - okay so big o
31:28 - of n to the exponent 2.
31:31 - let's write that there
31:32 - okay this is going to start to make
31:33 - sense as we go through more examples
31:35 - though that was an n-squared algorithm
31:36 - now we're moving on to this one so let's
31:38 - look at this now we have two inputs here
31:39 - so it gets a bit more complicated num1s
31:41 - and nums 2. now just like we used n to
31:44 - represent the size of our first input
31:45 - we're going to use another letter in
31:47 - this case m you can use whatever you
31:49 - want but the tradition is n and m for
31:51 - the second input so we have two inputs
31:53 - you got to consider with the length of
31:55 - the size of both of the inputs how long
31:56 - does this take n and m
31:59 - okay so pause the video if you need to
32:00 - and take a guess and i'm going to run
32:02 - through so again let's take a full
32:03 - screen snip here
32:05 - let's go
32:06 - full screen snip
32:08 - and let's try this okay so we have nums
32:10 - one nums two so for nums one can i
32:13 - change the pen color here actually i
32:14 - think red is fine we'll say this is n
32:16 - and we'll say that's m okay results one
32:19 - operation for num in nums one well nums
32:22 - one that's n so this is going to take
32:24 - how many operations are we doing just
32:26 - one so we can say this whole thing just
32:28 - takes n time okay
32:30 - nums two this is m right size of that
32:33 - input inside of here how many operations
32:35 - are we doing just quickly add them up
32:37 - doesn't need to be super precise one two
32:38 - three operations okay so this is going
32:40 - to be taking 3m time
32:43 - results that takes 1. let's add these
32:45 - all together because they're not inside
32:47 - of each other so we can just add them
32:48 - together so we're going to have 1 plus
32:51 - we'll add the other one down here so 1
32:53 - plus n plus three m
32:56 - okay now we need to cross off all the
32:58 - constants so we get rid of one get rid
33:00 - of one get rid of three what does that
33:02 - leave us n plus m
33:04 - okay we cannot simplify this any further
33:07 - because n and m are going to be two
33:09 - distinct variables representing the two
33:11 - inputs that we have to our function so
33:13 - you're going to see this a lot you have
33:14 - multiple inputs you need to use a
33:16 - different variable to represent each
33:18 - input and then that tells you the
33:20 - approximate time complexity of the
33:21 - function so again we're going to write
33:23 - this as big o of n plus m now if you
33:25 - wanted to do this really quickly all you
33:27 - would do is look for the loops that
33:29 - involve n and m so we see we have a loop
33:31 - here with n
33:32 - see we have a loop here with m boom all
33:34 - we do n plus m that's all we need to
33:35 - look at now we can look inside of these
33:37 - loops and try to see if there's more
33:38 - looping if there's no looping and it's
33:40 - constant time operations which all of
33:42 - these are here then we don't have to
33:44 - consider them we just cross them off
33:45 - because they're constant every loop is
33:47 - going to take the same amount of time so
33:48 - we don't need to consider it it's not
33:50 - based on the size of the input
33:52 - okay so o n plus m
33:55 - okay let's continue to example four i
33:57 - got a ton of examples to keep practicing
33:58 - this
33:59 - okay so now we're doing a bunch of stuff
34:01 - here so let's again go to the full
34:02 - screen snip
34:04 - you guys can pause the video if you want
34:05 - and take a guess at what the time
34:07 - complexity is
34:08 - okay so we have example four we take in
34:11 - a nested list now the nested list is
34:13 - going to be something like this
34:14 - right there's going to be a list
34:16 - inside of another list
34:18 - and this can make it a bit complicated
34:20 - so when we're dealing with a nested list
34:22 - what we're going to do is assume that
34:23 - the list has a height and a width now we
34:26 - could draw this in kind of more of a
34:27 - square fashion but it might be something
34:29 - like this
34:31 - okay
34:32 - and what we would get here is this would
34:34 - be the height so the number of lists
34:37 - inside of the list and then the width
34:39 - would be the number of elements inside
34:41 - of each list and we would assume that
34:42 - would just be the same okay so we have
34:44 - height and width and those are the two
34:46 - variables that we're going to use for
34:47 - the time complexity here because we need
34:49 - to represent the number of elements in
34:50 - the interior list as well as how many
34:52 - lists we have okay so let's look at this
34:55 - we have one operation i'm going to stop
34:56 - counting these now because we don't need
34:58 - to even consider them in our operations
35:00 - we're going to cross them out anyways
35:02 - it's just one single operation it's
35:03 - constant doesn't matter we can just stop
35:06 - looking at it because it's not going to
35:07 - be based on the size of the input
35:09 - however we have a for loop we say 4
35:10 - inner list in nested list well nested
35:13 - list
35:14 - we're getting all of the inner lists so
35:15 - how many of those lists do we have we
35:17 - have h of those right h nested lists
35:20 - okay we could write that lowercase h
35:21 - doesn't really matter
35:22 - then what are we doing we're looping
35:24 - through the inner list now the inner
35:26 - list is what well it's going to be w
35:28 - whatever the width is or the number of
35:30 - elements in the inner list so we're
35:32 - doing w here we're doing w here because
35:35 - it's the same thing and we're doing w
35:36 - here
35:37 - so we have h
35:38 - times three w right so inside of here
35:42 - three w operations times h
35:45 - so we get h
35:46 - and then this is multiplied
35:48 - by three w
35:50 - so that gives us three h w
35:52 - and what do we do
35:53 - remove the constants and we get h w okay
35:56 - so we can write this as run out of space
35:58 - here big o of h
36:01 - w and you know we'll just make it
36:03 - uppercase to stick with what we have
36:04 - before so the big o of the height times
36:06 - the width that's the number of
36:08 - operations that we're doing that's our
36:10 - big o uh or time complexity for this
36:12 - algorithm
36:13 - hopefully that makes sense
36:15 - hopefully i'm staying with you guys here
36:17 - you're staying with me actually please
36:18 - leave a comment if it's starting to get
36:20 - super confusing obviously doing this
36:22 - with no one giving me any feedback it's
36:23 - a little bit difficult to know if people
36:24 - are following or not but i think this is
36:26 - a decent explanation we have h in our
36:29 - list so we have h times we're going to
36:31 - do this for loop right here and then
36:33 - inside of the inner list we have w
36:34 - elements right so we're doing this for
36:37 - loop w times this for loop w times as
36:39 - well so we're doing 3w for every h so
36:43 - that's going to be h times 3w for the
36:45 - number of operations we have a constant
36:47 - in there which is 3. we don't care about
36:48 - the constant we remove it and we get w
36:51 - times h
36:52 - great okay next example
36:55 - example five now this is a recursive
36:58 - example uh going to be a bit more
37:00 - difficult here what i would recommend
37:02 - you do is try this with a very small
37:04 - value so try this with number three or
37:06 - try this with a value 2 and see what
37:08 - actually happens when you give it kind
37:10 - of these small values and how many
37:12 - operations end up happening in that case
37:15 - okay so i'm going to go to the screen
37:16 - snip now and let's look at this
37:19 - okay
37:19 - so this is a recursive example is
37:21 - actually a very famous problem it's the
37:22 - fibonacci sequence now i will just tell
37:24 - you that the big-o notation is 2 to the
37:26 - exponent n but we want to walk through
37:29 - why we're getting 2 to the exponent n so
37:31 - we need to understand how this code
37:32 - actually works
37:33 - so we're going to deal with a lot of
37:34 - recursive stuff when we're looking at
37:36 - time complexity analysis and the
37:38 - important thing to look at here with
37:39 - recursive functions is how many
37:42 - recursive function calls are happening
37:45 - in every step
37:47 - so we want to figure out essentially how
37:50 - many times is this function going to run
37:52 - and then we want to multiply that by the
37:54 - number of operations occurring in the
37:56 - function that's how you deal with
37:58 - recursive function calls how many times
38:00 - does the function run
38:01 - multiply that by the number of
38:03 - operations inside of the function now
38:05 - the first thing we're going to look at
38:06 - here is the operations in the function
38:08 - because that's the easiest to figure out
38:10 - so we have
38:11 - these are constant time operations right
38:13 - so we don't need to consider them these
38:14 - all take constant time okay all that so
38:16 - we can kind of cross all those out we
38:18 - don't even care about those now this
38:20 - operation right here this is a recursive
38:22 - function call so we're not going to
38:23 - consider that because that's going to
38:24 - have to do with how many times the
38:25 - function runs again recursive function
38:27 - call and then we have another constant
38:29 - time operation that we don't really care
38:31 - about so if we're looking inside of the
38:33 - the actual function itself we'll just
38:34 - say that the function takes constant
38:37 - time to execute however how many times
38:40 - is the function going to run well that's
38:42 - based on these recursive calls so once
38:44 - we figure that out we'll know the time
38:45 - complexity so let's just do a simple
38:47 - example here with an input of size five
38:50 - so if we have five then what we need to
38:51 - do is we need to call the function at
38:53 - four and at three because we have four
38:55 - here and three here when n is five okay
38:57 - those are the two function calls we need
38:58 - to make from four we're gonna have to
39:00 - make the function call three and two by
39:01 - the way i'm not making these in the
39:03 - order they're going to be done i'm just
39:04 - trying to write out the whole tree from
39:06 - three we're going to have to call two
39:07 - and one okay because our base cases are
39:09 - one and two so whenever we hit one and
39:11 - two we're done so done done done from 3
39:14 - we're going to have to call 2 and 1.
39:17 - okay so this is what our tree looks like
39:18 - so what you should be noticing here is
39:21 - that we're going to end up getting a
39:22 - time complexity
39:24 - of 2n so the reason why we're getting 2n
39:26 - here is because for each input to the
39:28 - function we're doing two recursive calls
39:30 - so if we start at 5 and we do a call to
39:33 - 4 and we do a call to 3 we've done two
39:36 - calls and then from these numbers we
39:37 - need to do two calls as well now we are
39:40 - going to end up doing less than two n
39:41 - operations however this is the best
39:43 - function we can use to approximate how
39:45 - many operations are going to occur in
39:48 - this function or in this algorithm so we
39:50 - have two recursive calls the calls are
39:52 - going down by any minus one and n minus
39:54 - two so a constant value they're
39:56 - decreasing by not something like divided
39:59 - by 2 or divided by 4 which would be much
40:00 - different we'll look at that in a second
40:02 - we're reducing these by a constant value
40:04 - so the most amount of steps we're going
40:06 - to have to do for every input is 2 and
40:08 - that means we're going to have 2 to the
40:10 - exponent and potential steps now if you
40:11 - want to prove that to yourself keep
40:13 - expanding this tree look at 6 look at 7
40:15 - look at 8 look at 9 and you're going to
40:16 - see how many operations you have to do
40:18 - and how we expand in this way in an
40:21 - exponential way okay so 2 to the
40:22 - exponent n is the time complexity here
40:25 - this is kind of just a complex do you
40:26 - have to be familiar with again the way i
40:28 - determine this is how many function
40:29 - calls i can have
40:31 - at most now again i will have less
40:33 - function calls than this but this is the
40:34 - most approximate function i can use for
40:37 - the big-o notation so we have big o of
40:38 - 2n all kinds of other videos on the
40:41 - fibonacci sequence that can probably
40:42 - explain that as well in more depth so
40:44 - let's write the comment
40:45 - of big o
40:47 - of n or sorry 2 to the exponent
40:51 - okay now let's go to example six
40:53 - all right example six so this one we
40:56 - have lst and search list actually let me
40:58 - go back i don't know why i keep keep
41:00 - going full screen here let's go
41:02 - and take a snip
41:04 - okay so we're gonna call lst here
41:06 - n and this will be m so two different
41:08 - inputs we have two lists go ahead pause
41:10 - the video determine the time complexity
41:13 - okay so let's get started here so the
41:14 - first thing we need to do is we've got
41:15 - to look at this operation here now a lot
41:18 - of you are going to say that this is a
41:19 - constant time operation it just happens
41:21 - in one right we just count this as one
41:23 - operation now we cannot because what max
41:26 - actually does in python is it loops
41:28 - through the entire list and finds the
41:30 - maximum value so you've got to be
41:31 - careful here whenever you're using
41:32 - internal language features like built-in
41:35 - functions max min stuff like that even
41:37 - you know slicing an array for example
41:39 - slicing a list you need to actually
41:41 - consider what's happening on the lower
41:43 - level and how long that's actually going
41:45 - to take so if i want to find the maximum
41:47 - number in the list the only way for me
41:48 - to do that is to search through every
41:50 - element in the list so if i have a list
41:52 - i have one two three four
41:55 - and i'm looking for element two okay
41:57 - yeah it only takes me two steps to go
41:58 - through this but i still need to say
42:00 - that this is going to take me o n time
42:02 - because if the element 2 was pushed to
42:04 - the very end of the list i would have to
42:06 - look through every element in the list
42:08 - to find it so this is where the worst
42:10 - case comes in right when i'm searching
42:11 - through a list i might find the element
42:13 - immediately or i might find it at the
42:14 - very end
42:15 - so in the worst case i have to assume
42:17 - it's going to take me n time to find the
42:20 - element so i say that finding the
42:21 - maximum element in a list takes me o n
42:24 - time
42:25 - hopefully that is clear so o n time for
42:28 - this first operation
42:30 - so let me just write an n here okay so
42:32 - that takes n to find the max
42:35 - then we're gonna look for a value in the
42:37 - search list so we're gonna say four
42:39 - value in search list and we're gonna
42:40 - check if the maximum value is equal to
42:42 - the value and return true so this here
42:44 - this is going to take us m time
42:47 - now again this might not take us m time
42:50 - if the maximum value is equal to the
42:52 - value in the very first value that we
42:54 - look at we immediately return true and
42:56 - this actually took us constant time it
42:57 - took us one operation to do this
42:59 - however if the value that we're looking
43:01 - for isn't in this list or it's one of
43:03 - the later elements in this list it takes
43:05 - us m time to look through it so in the
43:07 - worst case the element doesn't exist we
43:10 - have to loop through every single
43:11 - element in the search list just to find
43:13 - that it's not there that takes m time
43:16 - now that makes the total time complexity
43:17 - here n plus m so big o of n plus m we do
43:21 - the first operation which is n looking
43:23 - for the maximum element in the first
43:25 - list and then we search through all of
43:26 - the elements in the second list in the
43:28 - worst case giving us big o of n plus m
43:32 - all right let's continue
43:33 - uh example seven okay so this one is a
43:35 - bit more complicated again has to do
43:37 - with recursion uh pause the video take
43:39 - your time and try to guess what the time
43:41 - complexity is
43:42 - okay let's open up our snip here and
43:44 - let's go through this so for this one it
43:46 - is a little bit tricky but we really
43:48 - want to pay attention to this n over 2
43:50 - here and what's happening in this
43:51 - recursive call so previously when we saw
43:54 - recursive calls or when that one example
43:56 - we saw it we had a time complexity of
43:58 - two of the exponent n
43:59 - now the reason we had that is because we
44:02 - were calling two calls for every single
44:05 - input right so each input five i'm
44:07 - calling i'm calling the function twice
44:09 - and i was only reducing n by a constant
44:12 - amount i was reducing it by two three
44:14 - four one any constant amount that i'm
44:16 - reducing it by is going to cause me to
44:18 - get something like 2n now if i had three
44:20 - recursive calls i would have got 3n if i
44:23 - had four recursive calls for every input
44:25 - i would have got 4n assuming that i was
44:27 - reducing everything by a constant value
44:30 - now here i'm only doing one recursive
44:32 - call and i'm reducing this not by a
44:35 - constant i'm saying n over 2. so i'm
44:37 - actually reducing it by
44:39 - n like n is a part of the reduction
44:41 - factor if that makes sense so when i do
44:43 - something like n over 2 you can see that
44:45 - i'm very quickly going to get smaller
44:46 - numbers so 40 goes to 20 20 goes to 10
44:49 - 10 goes to 5. 5 goes to 2.5 2.5 let's
44:53 - round it goes to 1 right so
44:55 - this is how quickly we reduce from a
44:57 - very large number now this is actually
44:59 - logarithmic okay this gives us the log
45:02 - base 2 of n is the time complex again
45:04 - i'm going to continue to explain this
45:06 - but you need to keep this in mind so we
45:07 - have log base 2 of n
45:09 - so n is constantly being divided by 2.
45:12 - every time it gets divided by 2 that's
45:14 - reducing in a much
45:16 - smaller end right and so that means we
45:18 - only are actually going to do log base 2
45:20 - of n recursive calls or at least that's
45:22 - what our time complexity is going to be
45:24 - because that's how we're reducing n so
45:26 - again if we start with n equal to say
45:28 - 100 then we're only doing one recursive
45:30 - call and the next recursive call goes to
45:32 - 50. then it goes to 25 right then it's
45:35 - going to go to approximately 12.5 we're
45:37 - rounding this so it would go to 13 okay
45:39 - that's fine goes to 13 we divide 13 by 2
45:43 - that's going to give us approximately 7
45:45 - okay divide 7 by 2 that's going to give
45:47 - us approximately 4. divide that by 2
45:49 - we're going to get 2 divide that by 2
45:51 - we're going to get 1. so log base 2 of n
45:52 - is telling us how many operations
45:54 - because we're reducing by a factor of 2.
45:56 - i don't really know if i can explain
45:57 - this much better than that if we had a 3
46:00 - then this would be the log base 3 of n
46:03 - okay not log base 2. the reason it's 2
46:05 - is because that's what we're dividing by
46:07 - so really you can just kind of plug in
46:09 - some numbers and see how many operations
46:11 - it's actually going to take when we're
46:12 - doing recursive calls or we're reducing
46:14 - the number of operations by dividing it
46:16 - by two we're dividing it by four
46:18 - dividing it by five and n is what we're
46:20 - dividing it by well that means we're
46:22 - going to get some type of logarithmic
46:23 - time complexity and the larger n gets
46:26 - the fewer amount of operations we're
46:27 - actually going to have to be doing now
46:29 - that wasn't the most accurate way to say
46:30 - that but you get the point as n gets
46:32 - really really large the amount of
46:33 - operations we have to do is not growing
46:35 - with the size of n it's really small
46:37 - relative to n when n gets very large
46:39 - that's kind of what i was trying to say
46:41 - okay so that's that one so this is
46:43 - simply
46:44 - the
46:45 - log
46:47 - of n okay and this is log base two of n
46:49 - i don't really have a better way to
46:51 - write that okay now let's go to example
46:53 - eight
46:55 - so let me get the snip here again pause
46:56 - the video if you would like to try this
46:58 - out on your own
46:59 - so we're saying we're taking a list of
47:01 - strings okay so these are going to be
47:02 - multiple strings inside of a list we're
47:04 - saying 4i string and enumerate strings
47:07 - so immediately we're going to say this
47:09 - is going to take end time where n is the
47:11 - number of strings that we have okay the
47:12 - size of our input n
47:14 - now with strings though right if we look
47:16 - at a string like hello
47:18 - the number of characters in a string is
47:20 - going to dictate how long it takes to
47:22 - process that string which you're about
47:23 - to see here so i have 4 char in string
47:26 - now string is an individual string so
47:29 - there's not really a good way for me to
47:31 - figure out how many operations it's
47:33 - going to take for me to process a string
47:35 - because i don't know the length of the
47:36 - string so i need to come up with another
47:38 - variable here i'm going to use the
47:40 - variable k and k is going to be the max
47:44 - length of any string inside of this list
47:48 - so i'm going to say this is k
47:50 - now the reason i'm doing this is because
47:51 - whatever the maximum length of a string
47:54 - is that's the most number of times that
47:56 - this for loop is going to have to
47:57 - execute and i'm looking for the worst
47:59 - case scenario here so i can approximate
48:02 - all of the other strings length 2k
48:04 - because k is the longest length of any
48:06 - string that i have so that's the most
48:08 - amount of times it's going to have to
48:09 - happen so i say it's k
48:10 - now inside of here what am i doing i'm
48:12 - saying if char is in string i for i in
48:15 - range 0 10 now some of you may be
48:16 - tempted to include this as part of your
48:18 - time complexity this is constant this
48:21 - takes 10 steps every time so since it
48:24 - always takes 10 steps i don't have to
48:26 - count it i don't care about that and i'm
48:27 - not even going to look at this if
48:28 - statement okay
48:30 - so now we have n okay
48:31 - then i have this i have if digits
48:33 - greater than or equal to length of
48:34 - string over two strings i equals sorted
48:37 - strings i now the only part that i'm
48:38 - looking at here is sorted now we should
48:41 - know that to sort a list to sort a
48:43 - string to sort anything the most
48:44 - efficient way we can do this is in n
48:47 - log n time where this is base 2. now i
48:51 - completely butcher that so let me
48:52 - rewrite it and i'm also going to write
48:54 - it in k because that's what we're going
48:55 - to have to use here so we're going to
48:56 - say k and then this is going to be log
48:59 - base 2
49:00 - of k now k is the length of the longest
49:04 - string so the most amount of time it
49:05 - will take to sort something here is k
49:07 - log base 2 of k
49:08 - now if you don't know why that's the
49:10 - case you're just going to have to trust
49:12 - me here the most efficient way to sort
49:13 - something
49:14 - at least a general thing that you're
49:16 - sorting not something specific is k log
49:18 - base 2 of k
49:19 - or n log base 2 of n now k is the
49:22 - longest length of the string that we
49:23 - have we have to use k because we don't
49:25 - know how long the string is going to be
49:27 - so we say whatever the maximum length is
49:29 - that's what we'll put here because that
49:30 - will give us an upper bound and give us
49:32 - the worst case scenario so we have k log
49:34 - base 2 of k that's how long this takes i
49:36 - know i kind of butchered writing it
49:37 - there but
49:38 - that's the operation okay so now we've
49:40 - figured out our operations so we have n
49:42 - we have k and then we have k log base 2
49:45 - of k
49:46 - so inside of this for loop right here
49:48 - this takes
49:50 - k
49:51 - plus
49:52 - and then this is going to be
49:54 - okay you know what let me clear this
49:56 - because it's going to be hard to to do
49:58 - this okay so this is going to be
50:00 - n
50:01 - multiplied by and then this is going to
50:03 - be k
50:05 - plus
50:06 - and then k and this will be the log
50:09 - base 2.
50:11 - okay now again this is not always going
50:13 - to happen right we're not always going
50:15 - to have to sort but we could always have
50:17 - to sort that's the thing it's the worst
50:19 - case so since we could always have to
50:20 - sort this we're going to have to factor
50:22 - that into our time complexity and assume
50:24 - we're always sorting it so we have n
50:26 - which again was this we have k which was
50:28 - this and then we have the k log base 2
50:30 - of k which is here so inside the for
50:32 - loop
50:33 - we have the k plus k log base 2 of k
50:35 - then we have to multiply that by n
50:37 - because we're going to do that at most n
50:38 - times now though we're able to simplify
50:40 - things because we have k plus k log base
50:43 - 2 of k you can do the factoring here and
50:45 - you'll see that we'll get k multiplied
50:47 - by 1 plus log base 2 of k which means we
50:49 - could get rid of the 1 and so i can
50:51 - instead now say it's going to be n
50:53 - and then k
50:55 - and then log of k so this is my time
50:58 - complexity i can parenthesize it if i'd
51:00 - like but it's going to be n times k log
51:02 - base 2 of k
51:05 - all right there we go
51:06 - so that is that time complexity
51:08 - i'm going to start going a little bit
51:09 - faster just because we've looked at
51:11 - these a lot already so it's going to be
51:12 - n k
51:14 - log 2 k
51:16 - and we can write it like this if we'd
51:18 - like
51:20 - uh and maybe even do another parenthesis
51:21 - although it doesn't really matter all
51:23 - right so now let's move on to example
51:24 - nine okay this one's a complicated one
51:26 - so please pause the video and try to
51:28 - figure it out but let me explain it now
51:30 - we have two keys or we have two
51:31 - dictionaries sorry so i'm going to say
51:32 - these are n and m in terms of their size
51:35 - and these dictionaries are going to have
51:36 - key uh or sorry string keys okay or keys
51:40 - that are strings whatever so i say keys
51:42 - one and keys two and i'm sorting a bunch
51:43 - of strings now to sort the strings we're
51:45 - just going to say that this takes n and
51:47 - then this is going to be log base 2 of n
51:49 - time and then for this one since we're
51:52 - sorting dictionary 2 which is m we're
51:53 - going to say this is m
51:55 - log base 2 of m
51:57 - okay now we have our process now we have
51:59 - keys 1 plus keys 2. now there's going to
52:01 - be n keys in keys 1 and m keys in keys
52:04 - two so this is going to take us n plus m
52:06 - time to create that make a set constant
52:09 - time operation now we have this while
52:11 - loop now what we need to do here for the
52:12 - while loop is figure out how many times
52:13 - it's gonna run and then how much or how
52:16 - much time it takes sorry
52:18 - inside of the while loop uh to perform
52:20 - operations
52:21 - so how many operations are we doing in
52:22 - the for loop how many times is the uh
52:24 - sorry how many operations are we doing
52:26 - in the while loop how many times does
52:27 - the wall loop run multiply them together
52:30 - there we go we have the time for the
52:31 - while
52:32 - so let's start by figuring out the
52:34 - operations in the well
52:36 - so we have element equals process dot
52:37 - pop zero now popping from zero
52:40 - immediately tells me that this is going
52:42 - to be a big o of n operation or in this
52:44 - case n plus m so let me show you what i
52:45 - mean if we have a list and let's just
52:47 - say we have four elements
52:49 - if i pop the first element and remove
52:51 - this element what that means i have to
52:53 - do is shift every element over one
52:55 - position
52:56 - okay to restore its position in the list
52:59 - there is data structures that allow you
53:00 - to do this in constant time however the
53:02 - data structure list in python does not
53:04 - so this is an o n time operation where n
53:07 - is the number of elements in the list
53:09 - now since the number of elements in our
53:11 - process list is going to be at most n
53:14 - plus m that means we're gonna have to
53:16 - take if i could freaking erase all this
53:18 - stuff here n plus m time to perform
53:20 - these pops so i'm gonna say
53:23 - okay let's get this properly n plus m
53:25 - now again we're not always to have n
53:27 - plus m elements in the process list
53:29 - because we're removing elements that's
53:30 - what we're doing in here but the most we
53:32 - will have when we start is n plus m
53:34 - so we need to write that here as how
53:36 - much time this operation will take in
53:38 - the worst case
53:40 - continuing we add something to the
53:41 - results list don't care about that we're
53:43 - checking the length don't care about
53:45 - that now we have process dot append now
53:47 - what we're doing is appending a string
53:49 - but we're removing the last character of
53:52 - the string so what you need to realize
53:54 - is happening here is we are processing
53:56 - every single string key in both of the
53:58 - lists and we're going to keep adding the
54:00 - string itself back into the process list
54:03 - until it has no elements left in it so
54:06 - for every string that we process we're
54:08 - going to be adding that string minus its
54:10 - last character back to the process list
54:13 - whatever the length of the string times
54:15 - is so if i have a basic string like high
54:18 - then what i do is i strip off this last
54:20 - character but i add h back to the
54:22 - process list which means it needs to be
54:24 - processed again causing the while loop
54:26 - to happen another time
54:28 - so again more complicated algorithm here
54:29 - but i wanted to show you how you come up
54:31 - with these time complexities so since
54:33 - that's the case now what i need to do is
54:35 - say okay well the while loop is going to
54:37 - take the existing process time right so
54:40 - n plus m but this is all going to be
54:42 - multiplied by k where k is the length of
54:45 - the longest string so the most amount of
54:48 - times that one single element is going
54:49 - to be reinserted into process is k
54:52 - because that is well the length of the
54:54 - longest string right so if i have a
54:56 - string the longest string is five well
54:57 - it's going to take me five reinsertions
55:00 - it's really going to be four but still
55:02 - we're going to have to process that
55:03 - element five times by adding it back
55:05 - into the process list so i need to
55:07 - multiply the maximum length of the
55:09 - string by n plus m which is how many
55:11 - elements are already in there because
55:13 - for each of those elements they could be
55:15 - inserted a maximum of k times
55:18 - again i know we're getting into the
55:19 - weeds here it's getting complicated but
55:21 - i wanted to give you some advanced
55:22 - examples because this is stuff that
55:24 - you're going to have to learn at some
55:25 - point in time so n plus m multiplied by
55:28 - k is how long this loop is taken so now
55:31 - what we need to factor in here is n plus
55:33 - m multiplied by k going to be more or
55:36 - less than n times log base 2 of n and m
55:38 - times log base 2 of m because now we
55:41 - have to add these operations together so
55:42 - really what we're going to have is n
55:44 - then we're going to have log base 2 of n
55:47 - then we're going to add that to m
55:50 - of log base 2 of m then we have this
55:52 - operation which is n plus m now n plus m
55:55 - we know is going to be less than n plus
55:57 - m times k so we can simply remove that
56:00 - from our operation here and then we're
56:02 - going to have n
56:03 - plus m
56:04 - multiplied by k
56:06 - now we actually do not know if n plus m
56:09 - times k is going to be more or less than
56:11 - n log base 2 n and m log base 2 m so we
56:14 - need to keep them in so this would be
56:16 - our total time complexity here okay n
56:18 - log base 2 n plus m log base 2 m plus n
56:22 - times n times k and then you would write
56:24 - after this what each of these variables
56:26 - mean m is the number of keys in
56:28 - dictionary 1 m number of keys in
56:29 - dictionary two and k the length of the
56:32 - longest string out of dictionary one or
56:34 - dictionary two okay again i know really
56:36 - complicated one probably not one that
56:38 - you're going to get on like a computer
56:39 - science exam but it's possible so i
56:41 - figured we'd go through this example
56:43 - here okay let's look at example 10.
56:45 - now example 10 again we're getting into
56:47 - more complicated ones only got three
56:48 - left here let's take a snip
56:51 - pause the video and give it a go okay so
56:53 - actually we're going to skip example 10.
56:55 - i was just going through it and it's way
56:56 - too complicated to even try to explain
56:58 - in this video so if you guys want to
57:00 - guess the answer you can in the comments
57:02 - down below um anyways we're just going
57:04 - to move on to 11 because it's a bit
57:05 - simpler i don't want to overwhelm you
57:07 - guys too much so let's just understand
57:08 - this code before we proceed any further
57:10 - we have sum to the end empty list count
57:13 - equals zero four i in range the len of
57:15 - nums okay we're looping through getting
57:17 - one number then we're saying sum to the
57:19 - end out of pen zero so this is going to
57:21 - approach n
57:23 - then we're looping through all of the
57:24 - numbers past this number in our list
57:27 - okay because i plus 1 is the start of
57:29 - our for loop range up to the len of nums
57:31 - so that's n right
57:33 - okay then what we're doing is saying sum
57:35 - to the ni plus equals num2
57:37 - don't have to worry too much about the
57:38 - addition we have another for loop in
57:40 - here that is going to loop through all
57:42 - of the elements in sum to the end so
57:44 - whatever the current length of this list
57:46 - is that's how many times this is
57:47 - happening
57:48 - then we're printing out our count which
57:49 - is just how many times this for loop has
57:51 - happened in total
57:53 - so you can read this and understand a
57:54 - bit more i know i didn't really
57:55 - necessarily explain it the point is this
57:57 - sum to the end list is going to contain
57:59 - the sum of all of the numbers in the
58:01 - list past the current number
58:03 - not including the current number
58:06 - until we get to the end of list that's
58:07 - kind of what it's doing so if we have
58:09 - like one
58:10 - two
58:11 - three four then the result here is going
58:13 - to be whatever two plus three plus four
58:15 - is so nine and then whatever three plus
58:18 - four is which is seven then whatever
58:20 - four is which is four and then zero
58:22 - okay that's what the result is going to
58:23 - be because we sum these three then we
58:25 - sum these two then we sum this one and
58:27 - place those in the appropriate positions
58:29 - in the list and then we would sum the
58:31 - last element which which would be zero
58:32 - so we place zero at the end okay let's
58:35 - just get rid of those though
58:37 - that's what this code is doing okay so
58:38 - now that we understand that
58:40 - let us continue and do the time
58:42 - complexity so we're going to say this is
58:43 - n
58:44 - all right so that means this happens n
58:46 - times and then here what's going to
58:48 - happen is this is going to happen n
58:50 - times then n minus 1 times then n minus
58:53 - 2 then n minus 3 as i increases now if
58:56 - you actually look at the value of this
58:58 - sequence you'll see that it approaches n
59:00 - squared so we can simply say that all of
59:02 - these added together is n squared
59:04 - okay
59:05 - which means that we can just kind of
59:07 - write this
59:08 - as it takes n times okay we don't need
59:11 - to factor in all the minus ones or minus
59:13 - two minus three so on and so forth now
59:15 - continuing we have another for loop now
59:17 - this for loop is happening whatever the
59:18 - length of sum to the end is so it's
59:20 - going to happen first one time because
59:22 - that'll be the length when i equals one
59:25 - then it will happen two times then three
59:27 - then four all the way until it gets to n
59:30 - so similar to how we had n minus 1 n
59:32 - minus 2
59:33 - and minus 3
59:35 - here that's exactly what we have in the
59:36 - reverse order so we know that we can
59:39 - just say this is going to take n as well
59:41 - because 1 plus 2 plus 3 plus 4 up to n
59:43 - approaches n squared
59:46 - again i don't want to derive all the
59:47 - math here but i'm going to write that
59:49 - this takes n
59:50 - so we have n multiplied by n multiplied
59:53 - by n and that's all that we're doing
59:55 - inside of here which is going to give us
59:56 - an n cubed time algorithm so this for
59:59 - loop happens n times
60:01 - since inside of here this for loop
60:03 - happens n times we get n cubed so that's
60:06 - the time complexity big o of n cubed
60:09 - great let's move on to the last example
60:12 - here okay another complex example pause
60:15 - the video take a guess let's go through
60:17 - the time complexity
60:18 - right now
60:20 - okay so i'm just going to tell you the
60:22 - time complexity here is n factorial now
60:24 - if you're unfamiliar with the factorial
60:25 - obviously you would not have guessed
60:26 - this but a factorial is the following so
60:29 - if i have something like four factorial
60:31 - it is four times three times two times
60:34 - one if i had three factorial that's
60:36 - three times two times one
60:39 - okay that's factor so we see these in
60:41 - time complexities and a factorial is one
60:43 - of the worst time complexities that we
60:45 - can have refer to that graph i showed
60:47 - earlier if you want to see exactly where
60:48 - it ranks i believe it's actually the
60:49 - worst that was on that graph
60:51 - so n factorial is this time complexity
60:53 - but how do we determine well this is
60:55 - recursive so we're going to have to find
60:57 - how many times this recursive function
60:59 - runs and then multiply that by how much
61:01 - work is being done
61:02 - okay
61:04 - so
61:04 - we have n work being done inside the
61:07 - forward you know before we do that let's
61:08 - look at the recursive calls so we say if
61:10 - n is equal to one return one total
61:12 - equals zero
61:13 - four underscore in range n total plus
61:16 - equals example 12 n minus one so let's
61:19 - try this with an input of three if we
61:21 - try this with an input of three let's
61:23 - say n is currently equal to three what
61:25 - happens is total is equal to zero we're
61:26 - saying four i in range
61:28 - n and then we're going to call example
61:31 - 12 with n minus one how many times we
61:33 - call this three times okay we call it n
61:35 - times or whatever the input to the
61:37 - function was that's how many recursive
61:38 - calls we do so we have one two three
61:41 - recursive calls all to the same value
61:44 - which is n minus one which is two
61:46 - okay so there we go now what happens is
61:49 - we come to the function and now our
61:50 - input is two it's not one so we continue
61:53 - so we're looping through two and then
61:54 - we're doing this how many times whatever
61:57 - n is we're gonna now have two recursive
61:59 - calls uh with n being
62:01 - uh less than one okay so we're gonna say
62:03 - one one
62:04 - one one and one one now this is going to
62:07 - look very similar to the to the exponent
62:09 - n except what you're going to realize
62:11 - is that the levels in each tree or the i
62:14 - guess width of each section of the tree
62:17 - increases as the input increases so if i
62:19 - have four i have one two three four
62:22 - right so let's say i have three levels
62:24 - here well now what i'm doing is i'm
62:25 - saying okay i'm gonna have three levels
62:27 - and i'm gonna multiply that by however
62:28 - many levels this one's gonna have and
62:30 - this is gonna have how many levels is
62:31 - gonna have two so that gives me 3
62:34 - times 2 levels then how many levels this
62:36 - one have it has 1. this is a factorial
62:39 - that's what the tree looks like now
62:41 - again i know this is confusing i know a
62:42 - lot of you are going to be lost at this
62:44 - point but this is the factorial
62:46 - algorithm and if you look at this and i
62:48 - punch in a number say like uh i guess
62:50 - what's the factorial of 5 i believe it's
62:51 - 120 we'll get 120 meaning we had to do
62:54 - 120 operations because that's the way
62:57 - that we've calculated them so that's
62:59 - what i was trying to illustrate here
63:00 - with the factorial let's just look at
63:02 - one more higher tree level of four so
63:04 - you can really see kind of how this
63:05 - works so if we have an input of four
63:08 - here what happens is we get
63:10 - four calls okay four recursive calls
63:13 - that all go now to recursive calls of
63:15 - three and then we get one two three one
63:18 - two three one two three one two three
63:21 - then we have two two two two two two
63:24 - okay i'm gonna write a bunch of these
63:26 - here
63:27 - okay and then we have one two one two
63:29 - one two one two and if you count how
63:32 - many of these we actually have how many
63:34 - base cases we hit we're going to be
63:36 - hitting whatever n factorial base cases
63:39 - is okay that's how many base cases in
63:41 - the recursion that we hit this is n
63:43 - factorial
63:44 - now what we can say
63:46 - is that theoretically we're going to
63:47 - have an upper bound of 2 times n
63:50 - factorial recursive calls in total so we
63:53 - hit the base case n factorial times but
63:55 - we're going to have in total two times n
63:57 - factorial because to be able to hit the
63:59 - base case we had to do all of the other
64:01 - calls above now since we have a constant
64:04 - we remove the two and that gives us n
64:06 - factorial for the time complexity of our
64:08 - algorithm again
64:10 - hard to explain this in much more depth
64:11 - than this for some of these things you
64:13 - kind of just get it or you don't and you
64:15 - have to look at a ton of different
64:16 - examples to really see how this works
64:18 - understand factorials understand math
64:20 - maybe i'm just not the best at teaching
64:21 - it definitely open to your comments
64:23 - below but that's really the best that i
64:25 - can kind of explain how the factorial
64:27 - works here
64:28 - we start with four
64:29 - and then four
64:30 - four we're doing four fact uh four
64:32 - recursive calls for every single one of
64:34 - those recursive calls we're doing three
64:35 - recursive calls then we're doing two
64:37 - recursive calls then we're doing one
64:39 - call simply which is just like returning
64:41 - one so we're not really doing a call but
64:42 - we're returning a value the number of
64:44 - times we hit this base case is equal to
64:46 - n factorial which means we actually have
64:49 - more than n factorial recursive calls
64:51 - but the most we're going to have is 2
64:53 - times n factorial because any other
64:56 - calls above this the sum of them are
64:57 - going to be less than the amount of base
64:59 - cases that we have so we simply can
65:01 - cross off the 2 here and we get n
65:03 - factorial all right
65:05 - that is going to wrap up this video my
65:07 - voice is officially dying i've been
65:09 - filming this for about an hour and a
65:10 - half we'll see how long the video
65:11 - actually ends up being but hopefully
65:13 - this was helpful i understand that if
65:15 - you're a beginner in this the last
65:17 - example is probably to confuse you a
65:18 - little bit you don't have to stress out
65:20 - too much about those if you understood
65:22 - the first let's say what do we go up to
65:23 - here uh let's say eight examples then i
65:26 - think you're doing really well with big
65:28 - o notation i wanted to throw in some
65:30 - more advanced ones because i know some
65:31 - people are looking for that and i think
65:33 - that will be helpful to a few people
65:34 - anyways if you guys appreciate this
65:36 - video make sure to leave a like
65:37 - subscribe to the channel check out
65:38 - programming expert and i will see you in
65:40 - another youtube video
65:42 - [Music]
65:50 - you

Cleaned transcript:

in this video i'm going to explain to you big o notation as well as how to perform time complex the analysis on various algorithms now i have a ton of examples and i want to make this video really focused on going through different examples and practicing performing time complexity analysis so if you're familiar with bigo notation skip forward in the video to those examples they'll be in the video player or in the description if you're unfamiliar with big o notation then stick around for the first few minutes where i give you a conceptual overview on the whiteboard now i will mention that i'm not going to be covering other time complexity analysis so we're not going to be looking at big theta big omega space complexity we are simply looking at big o notation and in a second i'm going to explain what that is now before we get into it i will mention that i do have a course it's called programming expert teaches fundamental programming objectoriented programming advanced programming tons of practice questions tons of projects you guys can check it out from the link in the description and use discount code tim with that said let's go ahead and get into the video all right so i want to begin by just discussing what bigo notation is very generally and why it's important for programmers of all types to actually understand this concept so big o notation is essentially a tool that we use to analyze different algorithms data structures and just sections of code and figure out relatively how much time they're going to take to execute based on the approximate number of operations that need to be completed in the worst case i understand that sounds a little bit confusing but the idea is we want to be able to look at two different algorithms and very quickly determine which one is going to be more efficient based on its big o notation so big o notation again is a tool we're using it to compare different solutions different algorithms different data structures the algorithm is data structures that have the best bigo notations are typically the ones that we prefer because they're going to be the most efficient they're going to take the least amount of system resources or they're at least going to execute the fastest now bigo notation is really concerned with what happens when we scale our problem to a very large size so we're not really wondering what's going to happen when we sort 5 numbers or 10 numbers we're concerned with what's going to happen when we sort a million numbers if we're comparing say different sorting algorithms or what happens when we pass the worst case scenario of numbers to be sort so big annotation worst case scenario for very large inputs that's what we're focused on okay that is a very general introduction to big o notation the reason it's important for us to understand this is because there's a lot of scenarios where we can write code that solves a problem but doesn't do it in an efficient manner and the only way we know it's not happening efficiently is if we understand relatively how many operations this code takes compared to another more optimal solution so this is our comparison method without this method you really have no way of understanding you know how long your code is going to take to execute and that means that it might work for a small input but if you pass it say a thousand numbers or ten thousand numbers or a large list of strings or whatever the input type is it may take a really really long time to execute and may even be infeasible there's a lot of algorithms that you and me have probably both written in the past where if you fed them say a million numbers they may actually take years decades hundreds of years to execute based on how inefficient they are and just the math that goes on behind the scenes here anyways we're going to start looking at a ton of examples i'm going to hop over to the whiteboard and start explaining to you kind of the theory behind big o and how we find the bigo notation of specific algorithms all right so i'm here on the whiteboard i'm going to ask you to please not be too intimidated by the math that you're already seeing it's going to be really simplified once i start going through a few examples so as i mentioned before when we're talking about big o notation to actually find the big o notation or the big o function that represents an algorithm or a data structure or whatever it is we need to first know how many operations a algorithm is going to take okay so the first step that we always do when we're analyzing something for its big o notation to try to compare it against something else is we figure out the approximate number of operations that this algorithm takes we're then going to use those to simplify and find the big o function or the big o notation okay so let's write a super simple example here let me just get clicked into this screen and we'll start going through so i'm going to say total is equal to 0 i'm going to say 4 i in n i'm going to explain what n is in a second and hello catch just decided to join me here i'm going to say total plus equals i and then i'm just going to print i okay so this is my out now the first thing to discuss here is n you're going to see this a lot n refers to the size of our input so as i mentioned before when we're talking about notation we're really concerned with what happens when we have really large inputs so n is just a placeholder to represent the size of our input and we write our big o function and the number of operations relative to n so in this case here if we're counting the number of operations that are going to occur we have one operation to define the variable we're going to have n loops that's going to happen n times because n is the size of our input right so i'm going to say n and then we have two operations happening inside of the for loop so if this for loop happens n times and we have two operations we have a total of two n operations happening in the for loop so we're going to say 2n plus 1 is the number of total operations that this algorithm is going to take because we have 1 here and then n times 2 2n hopefully that is clear so what we're going to do is we're going to write this as what's known as our f of n okay i'm going to start going through this definition in a second let me just erase this part here and move over the 2n plus 1. okay so actually we'll just rewrite it over here okay so f of n is going to be equal to 2n plus 1 and f of n is going to represent the number of operations that our algorithm takes so now let's look at this definition so this says that f n is equal to the big o of g of n so where f is big o of g you can read it that way if there exists constants capital n and c so that for all n greater than or equal to uppercase n this inequality is true now it's important to note here that c and capital n have to be at least one so one or greater uh and we'll go through this example so this makes a bit more sense so let me just write out this inequality we have f of n needs to be less than or equal to c times g of n so just to stop and explain something here g of n is the function that we're looking for which is going to be the big o function or the big o notation of f of n or of our algorithm okay g of n is really what tells us approximately how long this code is going to take to run based on the input in this case n okay so everything refers to n again we're really concerned with what happens as n gets to a very very large number so we need to find something that satisfies this inequality and we want this right hand side here to be greater than or equal to n but be as close to n as possible the reason i'm saying that is because any of you who are kind of working this out in your head already can probably realize if i substitute in 2n plus 1 for f of n here and i say less than or equal to c times you can pick some very very massive function maybe something like n to the exponent 4 and well this is going to be true doesn't actually matter what value i pick for c it doesn't matter what value i pick for uppercase n this is always going to be true so i don't want to just pick an arbitrarily large function i want to find a function that's as close as possible to this one right here that still makes this true okay again i know this is confusing we're going to keep going through it there's just a lot to get through before you can really even look at an example so number of operations is 2n plus 1. we have f of 1 f of n is less than or equal to c times g of n and that's going to be for all values n that are greater than uppercase n so there's kind of two things that we need to pick here now let me just try to erase this okay so let's now write the inequality again 2n plus 1 less than or equal to c times g of n and this is 4 n greater than or equal to uppercase n so there's three things that we need to find to prove this true now what i'm going to do is just randomly pick a function for g of n and show you how this works so i'm going to say 2n plus 1 is less than or equal to c times and then the function i'm going to pick for g of n is simply n okay just the regular linear function i'm picking n so now that we have this and again this is going to be for here i'm not going to rewrite that we need to find some c that makes this true and if we find some c that does make it true as well as some uppercase n then we know that this is kind of our big o function that's what i'm going to be calling it all right so for c we can find this pretty easily we can just pick a very large number right i can pick a number like 10 i can pick a number like even 3 here that'll make it true essentially anything other than 2 or 1 is going to make this true and remember c has a minimum value of 1. so if i pick c equals 3 then that gives me the inequality 2n plus 1 is less than or equal to 3n okay great and then i need to just find some value uppercase n where all n values past that or equal to it make this true and really i can actually pick the value 1 for this and it will make this true right so if i pick this so n is equal to 1 then we can just plug in 1 and we'll see that this is always going to be true so when i plug this in i get 3 less than or equal to 3 so that's if 1 is 4n and if i were to plug in 2 3 4 5 so on and so forth you're going to see that this side here is always going to be less than or equal to or sorry greater than or equal to this now what does that mean that means that g of n is the big o notation of f n so i can say f n is equal to the big o of n and that's how you write this say f of n is equal to big o of n or the algorithm has a time complexity of big o of n okay again i know this is confusing but i've just showed you very very quickly how we kind of use this mathematical definition to find a g of n now what does g of n really mean well g of n means that as the input increases the amount of time that our function takes to execute increases linearly okay increases on a straight line as n increases in a linear function way the amount of operations increase so if n is one we have one operation if n is two we have two operations it's not telling us the exact number of operations because we know that's two n plus one it's telling us relatively what happens to the amount of time it takes for us to execute when the input value for the function gets increasingly large that's the point hopefully this is coming across that's what we're doing with big o of n here so yes big o of n is actually less than two n plus one that's fine we don't care about the exact number of operations we care what happens to the amount of time it takes when the input increases and now that we have this function n we know that it increases linearly the amount of time so let's clear all of this and let's just go through one more example here and look at some of the bigo functions which you can see on kind of the righthand side of my screen okay so let's say we have a function here that is something like two n squared plus n plus one okay this is the amount of operations that it takes we'll say this is f of n should have wrote it in the other way but that's fine now i'm going to write the rest of this in black here uh just so you're aware okay so now we want to find g of n for this so what do we do well we're going to write this out 2n squared plus n plus 1 is less than or equal to c times g of n for n greater than or equal to n okay so now let's pick something for n or pick something for g of answer so 2 n squared plus n plus 1 less than or equal to c times what if we pick n now if we pick n immediately we're going to see here that it doesn't actually matter what constant we give even if we give a constant like a thousand when i start passing really really large values for n n squared is going to exceed c times n very quickly right so if i pass even a huge value for n uh like you know a thousand ten thousand you're gonna see that this side here is gonna be a lot greater than this side here even though i'm multiplying it by a thousand now that's not something you're necessarily gonna prove but you just need to be able to look at that and kind of figure that out when i have an n squared and i have an n over here doesn't matter what the constant is this is always going to outgrow this side eventually when we get to a very large number of n so that tells me that i cannot find some c and i cannot find some n and so n is not going to be a valid function for g of n so now instead let's try n squared so now if i have n squared i'm actually going to be able to prove that i can find some constant c so to prove this again i just need to find some c and i need to find some n so let's look for a number for c let's just pick a thousand okay so if i say c is equal to a thousand now we can plug this in so we're gonna say a thousand n squared and i don't know why i'm writing these in different colors but i just am i'm gonna say two n squared plus n plus one now it actually doesn't really matter what value i pick for uppercase n here in fact i can pick any value and you're gonna see that we get uh the correct inequality here okay because i've picked a large enough number for c now you can make a smaller number of receipt it doesn't matter what number you're picking you're just picking that to prove that this inequality is true so for example if we plug in say one right we have two one plus one plus one that's going to give us four and then this right is going to give us a thousand so this is larger if we pick 2 this is going to give us 4 000 this is going to be a much smaller number so you're going to see that this side is going to be greater so there we go we now know that the function here is going to be n squared so what we can do is we can write f of n is equal to big o of n squared now you might start be starting to realize a pattern here and the pattern is you can essentially look at the largest term when you have things being multiplied together and understand that that is going to be what the big o function is now there's a few tricks and i have to go through a bunch of them but the point is here when we have something like 2n squared plus n plus 1 if my function g of n is simply n squared i'm going to be able to find a large enough constant that it doesn't matter what the other terms are here it's always going to be greater than this so let me just stop here for one second go through this example again and make sure it's clear because i get this is confusing and this is something that you have to look at a bunch of times before really starts to actually make any sense so i have 2n squared plus n plus 1 that's the number of operations that my algorithm takes now i'm trying to find a simplified function that is going to give me a general concept of what happens to the amount of time or the amount of operations required from this algorithm as the input grows very very very large so again we're thinking of really large values of n so when the values of n grow really large what's going to happen is this term right here is going to be dictating the number of operations that are occurring if you pick a value even something simple like 10 right the 2 times n squared gives you 200 the n gives you 10 and the 1 gives you 1. so these become increasingly less important the n plus 1 as n squared gets larger and larger and the same thing happens with this 2 here this becomes increasingly less important as n squared gets larger and larger and larger right that's really the point here as n gets larger all of the constants that we have here n 1 and 2 are going to become less important now the n is not a constant i know but it still becomes less important because n squared is much much larger than n that's what we're looking at so what we do is we get the bigger notation of n squared because this is the term that most accurately represents what happens to the amount of output or i guess the time to execute our function as the input increases and again the way we did that is we're using the math here to prove this i'm going to show you a bunch of shortcuts so you don't have to do this math every time but it's important just to see kind of how this works so you understand the math that goes behind this and how you prove you know that your bigger notation is actually correct okay so now let's just look at a few more functions and we're going to do them in a really fast way and i'm going to show you a few shortcuts that are going to make this seem really easy so let's say we have f of n is equal to 3 and let's go with n cubed plus n squared plus and then we're going to go uh oops let's go like this log base 2 of n okay so notice these are all pluses right so i have 3n squared plus n squared plus log base 2 of n now a few shortcuts that you can keep in mind here is whenever you have terms that are being added together so additive terms you can essentially cross out and remove every single term and constant that is not the largest term that you have in the expression so again the reason that you can do this is because as n increases and reaches infinity n cubed is going to dictate the amount of operations that this is going to take let's just plug in a value and have a look at it right all right so let's just plug in 10 here and see what we get so we're gonna have i guess three times a thousand plus this is going to be a hundred plus approximately six point six so if we look at this we're gonna see these terms right even the three become increasingly less important as n gets large and n isn't even that large n is ten imagine what happens now when n is ten thousand ten million the n cubed really carries the number of operations and so we don't care about these ones right here and we can cross them off when it comes to our bigo notation now to be able to actually cross these off you can do a bit of math to kind of prove that you can cross them off so what i'm going to say here is let's assume that n squared and log base 2 of n is equal to n cubed so the reason i can assume that is because n cubed is greater than n squared let's assume that n squared is equal to n cubed now i know this is not sound math but just assume that n squared is equal to n cubed if it is then what i can do is change all my n squared terms to n cubed so if i change n squared here 2n cubed which is larger than it so it's vowel for me to do that because i'm adding something in that's larger than n squared and i change my log base 2 of n to the same thing n cubed what you see here is we end up now if we factor this we get 5 n cubed now again the 5 is a constant we don't care about the constant because we don't need to know the exact number of operations we just want to have the function that tells us what happens as the input gets larger so n cubed is that function so we simply strip off the 5 and we say that the bigger notation here is n cubed that's the way that we do this so if you take every single other term that you're adding it's very important that you realize we're adding these terms here and you say okay well if it's smaller than n cubed let's assume it's n cubed now if we add all of these together and not factor so i said factor four if you add all of them together we get 5n cubed we know we don't care about the constants because we're just looking for a function to give us the relative amount of operations then what we can do is cross off the constant which would have been 5 and we're left with n cubed okay so that's one example let's now look at another example that has to do with multiplying terms so let's now say we have something like 3n and we're going to multiply this by the log base 2 of and then this will be n squared okay so this is actually what we have now what we want to do is simplify this and find g of n so we can find the big o notation so for this function here the first thing that we know we can do is remove the constants again we can remove them because we don't care about constant values we only care about a function based on the input that tells us what happens to the number of operations or the expected kind of running time of the algorithm based on the size of the input so we can remove the constant so we remove the constant 3 and that leaves us with n multiplied by log base 2 of n squared okay now we don't really know how to handle this n squared inside of here and i'm going to show you how we can simplify this even more you could technically say that this is the big o notation right now we can get even simpler we can take this 2 out based on using exponent rules i'm not going to derive these rules for you i assume most of you probably know this already at least if you're in high school let me write log properly we're going to be able to take the 2 out right before the log and then we can put this here now again the reason we take the 2 out is because whenever we have something to the exponent inside of a logarithm that exponent we can simply put as a multiplication to whatever the logarithm is of the base of that number and so that's what i'm doing so i'm saying 2 log base 2 of n i can take the 2 out now the 2 is what a constant we don't care about the constant so i can remove the constant okay so now if we continue i get n and then i have log base 2 of n now can i simplify this anymore no i cannot i cannot simplify because we are multiplying these terms not adding them if we were adding the terms together i could remove the log base 2 of n because it's smaller than n but i cannot in any way here find a way to actually write this as a constant multiply by n like previously if i had n plus and then i had log base 2 of n i can say that since log base 2 of n is smaller than n i can write this as n plus n that gives me 2n and then i can cross out 2 but here since we're multiplying the terms together i don't have a way to do that and so i can't simply remove the log base 2. okay if you wanted the math version of that that's why it also makes sense here since we have two factors being multiplied they both really affect how much time this is taking so we need to keep n log base 2 of n around okay now that we've looked at that let's just have a look at what's on the righthand side of my screen and then let's get into some some examples because i'm sure a lot of you are tired of this math so on the righthand side of my screen here we have the bigo complexity chart now let's just see if we can make this bigger so that we can take up the full screen with it nice okay so this is the chart now it actually is pretty good i don't really need to explain it too much but it simply shows you the big o functions and which ones are better than each other because once you find the big o notation that's great but you need to know which functions are better than other functions in terms of their complexity now we've already looked at log of n log of n is great that means that essentially as the input increases the number of operations we need to do is on a logarithmic basic basis which means it's getting smaller and smaller when log is equal to a hundred we have six when log is equal to uh 128 we have seven when log is equal to 256 uh we have eight right so on and so forth now i might have been messing those up a little bit you get the point as we get to a really really large number it stays really small when we have a logarithmic time complexity then we have a constant time complexity o of one this means that there's actually nothing based on the input this would be like printing a number or you know checking if a number is odd or even performing a modulus this is a constant time operation this isn't something that requires the input so if it doesn't matter to the input at all it only ever takes 100 steps or 50 steps or a thousand steps a constant number of steps then we have o1 a constant time o n this is linear this is a fair or good time complex if you have a linear time complexity usually that's pretty good it means your algorithm's time scales with the input so as the input increases the amount of operations uh increase relatively at the same amount if we have 10 inputs we have you know about 10 operations if we have 20 operations or 20 inputs we have 20 operations now it might actually be 100 operations or 200 operations but that's fine because it's happening linearly not exponentially or any of the other complexities that we're going to look at okay continuing with n log n now n log n is kind of a middle ground here as you can see we're multiplying a linear time complexity by a logarithmic so it's not horrible it's not as bad as something like n squared but it is you know fair it's going to take a long period of time if we have a lot of inputs again all of these are time estimates used to be compared against each other we're not looking at an exact amount of operations or exact amount of time so n log n fair time complexity it is better or sorry worse than a big o of n okay now we have n squared n squared is a horrible time complexity that means that we increase the number of operations we need to do exponentially as n increases and i'm sure many of you have looked at exponents before not very good when n gets to a large number say we add 10 we have n squared that gives us 100 already keeps getting larger and larger and larger then we have something like 2 to the exponent n now this is even worse this is a very very poor time conflict as you can see then we have something like n factorial and we have even more complex time complexities where we have something like n factorial multiplied by 2n plus m and would be another input i'll talk about that later but this is the general graph so you kind of need to memorize this graph it's pretty easy you could just plug in some values and actually see which ones are getting larger faster if you are unsure but these are the most important big o functions and we're going to be using combinations of these functions when we're analyzing the time complexity of the examples i'm about to go through so i apologize i know this was long but i just wanted to try to explain this as best as i could hopefully that made sense now we're going to go through 12 examples and i'm going to walk you through exactly how we solve them all right so we're moving on to the first example here example zero i'm going to ask you right now what is the time complexity or big o notation of this function think about it pause the video if you need to i'm about to go in and explain so the answer here is that this is constant time okay now i'll explain why it's constant time but this is big o of one so inside of here what we're doing is we're performing a constant number of operations it does not matter what num is and num is going to be n right num is n because that's our input so whatever the size of num is we represent that with the variable n and it doesn't matter what this is it's always going to take us approximately the same amount of time to perform this calculation right the modulus of 2 or the num modulus 2 and we're just checking if this number is odd or even that's what we're doing here so this is a constant time operation fairly straightforward now i want to mention that if i did something like x equals 1 y equals 3 whatever this is still a constant time operation i don't write something like o of 3 i just keep it at 0 1 which means we're going to do the same amount of steps no matter what n is no matter what the size of our input is and as we go through these next examples you're going to see that i've labeled inputs different values you know numbers strings whatever you still want to represent them using n n is the length of your input or the size of your input in every single situation okay so let's now go to example one example one we've seen this example kind of already i've total equal zero for number in nums total plus equals number return total pause the video what is the big o notation of this example or of this algorithm right here okay so the answer as i'm sure many of you have gotten and probably fairly quickly is big o of n now n is representing the number of numbers that we have in nums so the size of our input now what we do here to calculate this is exactly what we did in that first example that i showed you we have a variable this takes one one operation right we have a for loop this is going to take n it takes n because the number of numbers is n right so this for loop runs n times we're doing one operation so what we end up getting here at the end you could count this as an operation if you want as well is something like n plus one or n plus two whatever you want to write okay so we'll say it's n plus 1. if we have n plus 1 we know that we can simply remove the constants which are 1 because as n gets larger they're not going to matter and write the big o notation as big o of n okay so that's that example let's move forward okay now this one is getting a bit more complicated now we have an example where we have a nested for loop so again pause the video go ahead and read this and it's going to get even more complicated as we go on all right so let's go line by line we have results here and results we're creating a list now this list we have a for loop here we've got to look at all of our loops okay and any operation that could be looping and what happens is we need to 4n right so for underscore in range len of nums lenum nums really is just n we're adding 1 into the list so this is going to take us n to do because we're adding 1 n times pretty straightforward move on to the next loop okay inside of this next loop well how many times does it take for this loop to run this loop is going to take n times to run because we're doing it for the size of our input which is n okay now we have another for loop inside of this for loop now that means that this for loop needs to run how many times n times and how long does it take for the for loop to run well this for loop is looping through nums as well and so that gives us n okay so we have n stuff happening n times because this for loop is happening now inside of here we can count the operations we could say this is an operation this is an operation and this is an operation now remember we're considering the worst case so in one case when num1 is equal to num2 we're not going to perform this operation however we don't really care about that we're not going to factor that into our calculations here because this could happen right it's happening every time except once so we're just going to assume that this operation happens the end times for this for hopefully you get what i'm saying but i'm just trying to get through like don't over complicate analyzing the number of operations here we're just trying to get a rough estimate and then we can actually figure out what the big o notation is okay so we have about three operations at most we have to perform a condition we're going to continue we could count as an operation i'm going to do a multiplication we'll just sum all of those up even though they're not all going to happen together okay so we say approximately we're going to get three operations inside of here then we're returning results okay that's another operation so we have n now we're adding that because this is it not inside the for loop where it's the next line down to n okay and then how many times or how many operations are going on inside of this n well we have three n so we're going to say n and then this is just going to be multiplied by 3n now i get this a bit confusing but what's happening is this for loop is happening every single time this for loop runs so we're doing this for loop n times which means we're going to have n times n right two n's there so that's n squared and then we're doing three operations inside of this internal for loop and so what we end up getting is three n squared if we multiply these all together we're gonna get n plus three n squared and then we have one more operation here so we add one so this is going to be the you know approximate number of operations that we're doing inside of this algorithm now what do we do well we look for the largest term which is n squared we cross off all of our constants and then whenever we have a variable we say okay was this term smaller or larger than this one well it's smaller so if we assume it's n squared then we could write this as 2n squared which means i can now get rid of that and that means my big o notation is going to be n squared i know i made a complete mess here but this is going to be n squared okay that's the big o notation let me get out of this mass let's look at it here in the code edit so we perform an n operation that happens okay then we go inside of here we have an n times n operation because we're doing this for n times for this for loop which happens n times we need to multiply them together inside this for loop we're doing three things so we have three n squared that's how long all of this takes to run so we add that to n add that to one and we get 3n squared plus n plus 1. we know the other terms are smaller so we cross them off and that leaves us with the big o notation of n squared okay so big o of n to the exponent 2. let's write that there okay this is going to start to make sense as we go through more examples though that was an nsquared algorithm now we're moving on to this one so let's look at this now we have two inputs here so it gets a bit more complicated num1s and nums 2. now just like we used n to represent the size of our first input we're going to use another letter in this case m you can use whatever you want but the tradition is n and m for the second input so we have two inputs you got to consider with the length of the size of both of the inputs how long does this take n and m okay so pause the video if you need to and take a guess and i'm going to run through so again let's take a full screen snip here let's go full screen snip and let's try this okay so we have nums one nums two so for nums one can i change the pen color here actually i think red is fine we'll say this is n and we'll say that's m okay results one operation for num in nums one well nums one that's n so this is going to take how many operations are we doing just one so we can say this whole thing just takes n time okay nums two this is m right size of that input inside of here how many operations are we doing just quickly add them up doesn't need to be super precise one two three operations okay so this is going to be taking 3m time results that takes 1. let's add these all together because they're not inside of each other so we can just add them together so we're going to have 1 plus we'll add the other one down here so 1 plus n plus three m okay now we need to cross off all the constants so we get rid of one get rid of one get rid of three what does that leave us n plus m okay we cannot simplify this any further because n and m are going to be two distinct variables representing the two inputs that we have to our function so you're going to see this a lot you have multiple inputs you need to use a different variable to represent each input and then that tells you the approximate time complexity of the function so again we're going to write this as big o of n plus m now if you wanted to do this really quickly all you would do is look for the loops that involve n and m so we see we have a loop here with n see we have a loop here with m boom all we do n plus m that's all we need to look at now we can look inside of these loops and try to see if there's more looping if there's no looping and it's constant time operations which all of these are here then we don't have to consider them we just cross them off because they're constant every loop is going to take the same amount of time so we don't need to consider it it's not based on the size of the input okay so o n plus m okay let's continue to example four i got a ton of examples to keep practicing this okay so now we're doing a bunch of stuff here so let's again go to the full screen snip you guys can pause the video if you want and take a guess at what the time complexity is okay so we have example four we take in a nested list now the nested list is going to be something like this right there's going to be a list inside of another list and this can make it a bit complicated so when we're dealing with a nested list what we're going to do is assume that the list has a height and a width now we could draw this in kind of more of a square fashion but it might be something like this okay and what we would get here is this would be the height so the number of lists inside of the list and then the width would be the number of elements inside of each list and we would assume that would just be the same okay so we have height and width and those are the two variables that we're going to use for the time complexity here because we need to represent the number of elements in the interior list as well as how many lists we have okay so let's look at this we have one operation i'm going to stop counting these now because we don't need to even consider them in our operations we're going to cross them out anyways it's just one single operation it's constant doesn't matter we can just stop looking at it because it's not going to be based on the size of the input however we have a for loop we say 4 inner list in nested list well nested list we're getting all of the inner lists so how many of those lists do we have we have h of those right h nested lists okay we could write that lowercase h doesn't really matter then what are we doing we're looping through the inner list now the inner list is what well it's going to be w whatever the width is or the number of elements in the inner list so we're doing w here we're doing w here because it's the same thing and we're doing w here so we have h times three w right so inside of here three w operations times h so we get h and then this is multiplied by three w so that gives us three h w and what do we do remove the constants and we get h w okay so we can write this as run out of space here big o of h w and you know we'll just make it uppercase to stick with what we have before so the big o of the height times the width that's the number of operations that we're doing that's our big o uh or time complexity for this algorithm hopefully that makes sense hopefully i'm staying with you guys here you're staying with me actually please leave a comment if it's starting to get super confusing obviously doing this with no one giving me any feedback it's a little bit difficult to know if people are following or not but i think this is a decent explanation we have h in our list so we have h times we're going to do this for loop right here and then inside of the inner list we have w elements right so we're doing this for loop w times this for loop w times as well so we're doing 3w for every h so that's going to be h times 3w for the number of operations we have a constant in there which is 3. we don't care about the constant we remove it and we get w times h great okay next example example five now this is a recursive example uh going to be a bit more difficult here what i would recommend you do is try this with a very small value so try this with number three or try this with a value 2 and see what actually happens when you give it kind of these small values and how many operations end up happening in that case okay so i'm going to go to the screen snip now and let's look at this okay so this is a recursive example is actually a very famous problem it's the fibonacci sequence now i will just tell you that the bigo notation is 2 to the exponent n but we want to walk through why we're getting 2 to the exponent n so we need to understand how this code actually works so we're going to deal with a lot of recursive stuff when we're looking at time complexity analysis and the important thing to look at here with recursive functions is how many recursive function calls are happening in every step so we want to figure out essentially how many times is this function going to run and then we want to multiply that by the number of operations occurring in the function that's how you deal with recursive function calls how many times does the function run multiply that by the number of operations inside of the function now the first thing we're going to look at here is the operations in the function because that's the easiest to figure out so we have these are constant time operations right so we don't need to consider them these all take constant time okay all that so we can kind of cross all those out we don't even care about those now this operation right here this is a recursive function call so we're not going to consider that because that's going to have to do with how many times the function runs again recursive function call and then we have another constant time operation that we don't really care about so if we're looking inside of the the actual function itself we'll just say that the function takes constant time to execute however how many times is the function going to run well that's based on these recursive calls so once we figure that out we'll know the time complexity so let's just do a simple example here with an input of size five so if we have five then what we need to do is we need to call the function at four and at three because we have four here and three here when n is five okay those are the two function calls we need to make from four we're gonna have to make the function call three and two by the way i'm not making these in the order they're going to be done i'm just trying to write out the whole tree from three we're going to have to call two and one okay because our base cases are one and two so whenever we hit one and two we're done so done done done from 3 we're going to have to call 2 and 1. okay so this is what our tree looks like so what you should be noticing here is that we're going to end up getting a time complexity of 2n so the reason why we're getting 2n here is because for each input to the function we're doing two recursive calls so if we start at 5 and we do a call to 4 and we do a call to 3 we've done two calls and then from these numbers we need to do two calls as well now we are going to end up doing less than two n operations however this is the best function we can use to approximate how many operations are going to occur in this function or in this algorithm so we have two recursive calls the calls are going down by any minus one and n minus two so a constant value they're decreasing by not something like divided by 2 or divided by 4 which would be much different we'll look at that in a second we're reducing these by a constant value so the most amount of steps we're going to have to do for every input is 2 and that means we're going to have 2 to the exponent and potential steps now if you want to prove that to yourself keep expanding this tree look at 6 look at 7 look at 8 look at 9 and you're going to see how many operations you have to do and how we expand in this way in an exponential way okay so 2 to the exponent n is the time complexity here this is kind of just a complex do you have to be familiar with again the way i determine this is how many function calls i can have at most now again i will have less function calls than this but this is the most approximate function i can use for the bigo notation so we have big o of 2n all kinds of other videos on the fibonacci sequence that can probably explain that as well in more depth so let's write the comment of big o of n or sorry 2 to the exponent okay now let's go to example six all right example six so this one we have lst and search list actually let me go back i don't know why i keep keep going full screen here let's go and take a snip okay so we're gonna call lst here n and this will be m so two different inputs we have two lists go ahead pause the video determine the time complexity okay so let's get started here so the first thing we need to do is we've got to look at this operation here now a lot of you are going to say that this is a constant time operation it just happens in one right we just count this as one operation now we cannot because what max actually does in python is it loops through the entire list and finds the maximum value so you've got to be careful here whenever you're using internal language features like builtin functions max min stuff like that even you know slicing an array for example slicing a list you need to actually consider what's happening on the lower level and how long that's actually going to take so if i want to find the maximum number in the list the only way for me to do that is to search through every element in the list so if i have a list i have one two three four and i'm looking for element two okay yeah it only takes me two steps to go through this but i still need to say that this is going to take me o n time because if the element 2 was pushed to the very end of the list i would have to look through every element in the list to find it so this is where the worst case comes in right when i'm searching through a list i might find the element immediately or i might find it at the very end so in the worst case i have to assume it's going to take me n time to find the element so i say that finding the maximum element in a list takes me o n time hopefully that is clear so o n time for this first operation so let me just write an n here okay so that takes n to find the max then we're gonna look for a value in the search list so we're gonna say four value in search list and we're gonna check if the maximum value is equal to the value and return true so this here this is going to take us m time now again this might not take us m time if the maximum value is equal to the value in the very first value that we look at we immediately return true and this actually took us constant time it took us one operation to do this however if the value that we're looking for isn't in this list or it's one of the later elements in this list it takes us m time to look through it so in the worst case the element doesn't exist we have to loop through every single element in the search list just to find that it's not there that takes m time now that makes the total time complexity here n plus m so big o of n plus m we do the first operation which is n looking for the maximum element in the first list and then we search through all of the elements in the second list in the worst case giving us big o of n plus m all right let's continue uh example seven okay so this one is a bit more complicated again has to do with recursion uh pause the video take your time and try to guess what the time complexity is okay let's open up our snip here and let's go through this so for this one it is a little bit tricky but we really want to pay attention to this n over 2 here and what's happening in this recursive call so previously when we saw recursive calls or when that one example we saw it we had a time complexity of two of the exponent n now the reason we had that is because we were calling two calls for every single input right so each input five i'm calling i'm calling the function twice and i was only reducing n by a constant amount i was reducing it by two three four one any constant amount that i'm reducing it by is going to cause me to get something like 2n now if i had three recursive calls i would have got 3n if i had four recursive calls for every input i would have got 4n assuming that i was reducing everything by a constant value now here i'm only doing one recursive call and i'm reducing this not by a constant i'm saying n over 2. so i'm actually reducing it by n like n is a part of the reduction factor if that makes sense so when i do something like n over 2 you can see that i'm very quickly going to get smaller numbers so 40 goes to 20 20 goes to 10 10 goes to 5. 5 goes to 2.5 2.5 let's round it goes to 1 right so this is how quickly we reduce from a very large number now this is actually logarithmic okay this gives us the log base 2 of n is the time complex again i'm going to continue to explain this but you need to keep this in mind so we have log base 2 of n so n is constantly being divided by 2. every time it gets divided by 2 that's reducing in a much smaller end right and so that means we only are actually going to do log base 2 of n recursive calls or at least that's what our time complexity is going to be because that's how we're reducing n so again if we start with n equal to say 100 then we're only doing one recursive call and the next recursive call goes to 50. then it goes to 25 right then it's going to go to approximately 12.5 we're rounding this so it would go to 13 okay that's fine goes to 13 we divide 13 by 2 that's going to give us approximately 7 okay divide 7 by 2 that's going to give us approximately 4. divide that by 2 we're going to get 2 divide that by 2 we're going to get 1. so log base 2 of n is telling us how many operations because we're reducing by a factor of 2. i don't really know if i can explain this much better than that if we had a 3 then this would be the log base 3 of n okay not log base 2. the reason it's 2 is because that's what we're dividing by so really you can just kind of plug in some numbers and see how many operations it's actually going to take when we're doing recursive calls or we're reducing the number of operations by dividing it by two we're dividing it by four dividing it by five and n is what we're dividing it by well that means we're going to get some type of logarithmic time complexity and the larger n gets the fewer amount of operations we're actually going to have to be doing now that wasn't the most accurate way to say that but you get the point as n gets really really large the amount of operations we have to do is not growing with the size of n it's really small relative to n when n gets very large that's kind of what i was trying to say okay so that's that one so this is simply the log of n okay and this is log base two of n i don't really have a better way to write that okay now let's go to example eight so let me get the snip here again pause the video if you would like to try this out on your own so we're saying we're taking a list of strings okay so these are going to be multiple strings inside of a list we're saying 4i string and enumerate strings so immediately we're going to say this is going to take end time where n is the number of strings that we have okay the size of our input n now with strings though right if we look at a string like hello the number of characters in a string is going to dictate how long it takes to process that string which you're about to see here so i have 4 char in string now string is an individual string so there's not really a good way for me to figure out how many operations it's going to take for me to process a string because i don't know the length of the string so i need to come up with another variable here i'm going to use the variable k and k is going to be the max length of any string inside of this list so i'm going to say this is k now the reason i'm doing this is because whatever the maximum length of a string is that's the most number of times that this for loop is going to have to execute and i'm looking for the worst case scenario here so i can approximate all of the other strings length 2k because k is the longest length of any string that i have so that's the most amount of times it's going to have to happen so i say it's k now inside of here what am i doing i'm saying if char is in string i for i in range 0 10 now some of you may be tempted to include this as part of your time complexity this is constant this takes 10 steps every time so since it always takes 10 steps i don't have to count it i don't care about that and i'm not even going to look at this if statement okay so now we have n okay then i have this i have if digits greater than or equal to length of string over two strings i equals sorted strings i now the only part that i'm looking at here is sorted now we should know that to sort a list to sort a string to sort anything the most efficient way we can do this is in n log n time where this is base 2. now i completely butcher that so let me rewrite it and i'm also going to write it in k because that's what we're going to have to use here so we're going to say k and then this is going to be log base 2 of k now k is the length of the longest string so the most amount of time it will take to sort something here is k log base 2 of k now if you don't know why that's the case you're just going to have to trust me here the most efficient way to sort something at least a general thing that you're sorting not something specific is k log base 2 of k or n log base 2 of n now k is the longest length of the string that we have we have to use k because we don't know how long the string is going to be so we say whatever the maximum length is that's what we'll put here because that will give us an upper bound and give us the worst case scenario so we have k log base 2 of k that's how long this takes i know i kind of butchered writing it there but that's the operation okay so now we've figured out our operations so we have n we have k and then we have k log base 2 of k so inside of this for loop right here this takes k plus and then this is going to be okay you know what let me clear this because it's going to be hard to to do this okay so this is going to be n multiplied by and then this is going to be k plus and then k and this will be the log base 2. okay now again this is not always going to happen right we're not always going to have to sort but we could always have to sort that's the thing it's the worst case so since we could always have to sort this we're going to have to factor that into our time complexity and assume we're always sorting it so we have n which again was this we have k which was this and then we have the k log base 2 of k which is here so inside the for loop we have the k plus k log base 2 of k then we have to multiply that by n because we're going to do that at most n times now though we're able to simplify things because we have k plus k log base 2 of k you can do the factoring here and you'll see that we'll get k multiplied by 1 plus log base 2 of k which means we could get rid of the 1 and so i can instead now say it's going to be n and then k and then log of k so this is my time complexity i can parenthesize it if i'd like but it's going to be n times k log base 2 of k all right there we go so that is that time complexity i'm going to start going a little bit faster just because we've looked at these a lot already so it's going to be n k log 2 k and we can write it like this if we'd like uh and maybe even do another parenthesis although it doesn't really matter all right so now let's move on to example nine okay this one's a complicated one so please pause the video and try to figure it out but let me explain it now we have two keys or we have two dictionaries sorry so i'm going to say these are n and m in terms of their size and these dictionaries are going to have key uh or sorry string keys okay or keys that are strings whatever so i say keys one and keys two and i'm sorting a bunch of strings now to sort the strings we're just going to say that this takes n and then this is going to be log base 2 of n time and then for this one since we're sorting dictionary 2 which is m we're going to say this is m log base 2 of m okay now we have our process now we have keys 1 plus keys 2. now there's going to be n keys in keys 1 and m keys in keys two so this is going to take us n plus m time to create that make a set constant time operation now we have this while loop now what we need to do here for the while loop is figure out how many times it's gonna run and then how much or how much time it takes sorry inside of the while loop uh to perform operations so how many operations are we doing in the for loop how many times is the uh sorry how many operations are we doing in the while loop how many times does the wall loop run multiply them together there we go we have the time for the while so let's start by figuring out the operations in the well so we have element equals process dot pop zero now popping from zero immediately tells me that this is going to be a big o of n operation or in this case n plus m so let me show you what i mean if we have a list and let's just say we have four elements if i pop the first element and remove this element what that means i have to do is shift every element over one position okay to restore its position in the list there is data structures that allow you to do this in constant time however the data structure list in python does not so this is an o n time operation where n is the number of elements in the list now since the number of elements in our process list is going to be at most n plus m that means we're gonna have to take if i could freaking erase all this stuff here n plus m time to perform these pops so i'm gonna say okay let's get this properly n plus m now again we're not always to have n plus m elements in the process list because we're removing elements that's what we're doing in here but the most we will have when we start is n plus m so we need to write that here as how much time this operation will take in the worst case continuing we add something to the results list don't care about that we're checking the length don't care about that now we have process dot append now what we're doing is appending a string but we're removing the last character of the string so what you need to realize is happening here is we are processing every single string key in both of the lists and we're going to keep adding the string itself back into the process list until it has no elements left in it so for every string that we process we're going to be adding that string minus its last character back to the process list whatever the length of the string times is so if i have a basic string like high then what i do is i strip off this last character but i add h back to the process list which means it needs to be processed again causing the while loop to happen another time so again more complicated algorithm here but i wanted to show you how you come up with these time complexities so since that's the case now what i need to do is say okay well the while loop is going to take the existing process time right so n plus m but this is all going to be multiplied by k where k is the length of the longest string so the most amount of times that one single element is going to be reinserted into process is k because that is well the length of the longest string right so if i have a string the longest string is five well it's going to take me five reinsertions it's really going to be four but still we're going to have to process that element five times by adding it back into the process list so i need to multiply the maximum length of the string by n plus m which is how many elements are already in there because for each of those elements they could be inserted a maximum of k times again i know we're getting into the weeds here it's getting complicated but i wanted to give you some advanced examples because this is stuff that you're going to have to learn at some point in time so n plus m multiplied by k is how long this loop is taken so now what we need to factor in here is n plus m multiplied by k going to be more or less than n times log base 2 of n and m times log base 2 of m because now we have to add these operations together so really what we're going to have is n then we're going to have log base 2 of n then we're going to add that to m of log base 2 of m then we have this operation which is n plus m now n plus m we know is going to be less than n plus m times k so we can simply remove that from our operation here and then we're going to have n plus m multiplied by k now we actually do not know if n plus m times k is going to be more or less than n log base 2 n and m log base 2 m so we need to keep them in so this would be our total time complexity here okay n log base 2 n plus m log base 2 m plus n times n times k and then you would write after this what each of these variables mean m is the number of keys in dictionary 1 m number of keys in dictionary two and k the length of the longest string out of dictionary one or dictionary two okay again i know really complicated one probably not one that you're going to get on like a computer science exam but it's possible so i figured we'd go through this example here okay let's look at example 10. now example 10 again we're getting into more complicated ones only got three left here let's take a snip pause the video and give it a go okay so actually we're going to skip example 10. i was just going through it and it's way too complicated to even try to explain in this video so if you guys want to guess the answer you can in the comments down below um anyways we're just going to move on to 11 because it's a bit simpler i don't want to overwhelm you guys too much so let's just understand this code before we proceed any further we have sum to the end empty list count equals zero four i in range the len of nums okay we're looping through getting one number then we're saying sum to the end out of pen zero so this is going to approach n then we're looping through all of the numbers past this number in our list okay because i plus 1 is the start of our for loop range up to the len of nums so that's n right okay then what we're doing is saying sum to the ni plus equals num2 don't have to worry too much about the addition we have another for loop in here that is going to loop through all of the elements in sum to the end so whatever the current length of this list is that's how many times this is happening then we're printing out our count which is just how many times this for loop has happened in total so you can read this and understand a bit more i know i didn't really necessarily explain it the point is this sum to the end list is going to contain the sum of all of the numbers in the list past the current number not including the current number until we get to the end of list that's kind of what it's doing so if we have like one two three four then the result here is going to be whatever two plus three plus four is so nine and then whatever three plus four is which is seven then whatever four is which is four and then zero okay that's what the result is going to be because we sum these three then we sum these two then we sum this one and place those in the appropriate positions in the list and then we would sum the last element which which would be zero so we place zero at the end okay let's just get rid of those though that's what this code is doing okay so now that we understand that let us continue and do the time complexity so we're going to say this is n all right so that means this happens n times and then here what's going to happen is this is going to happen n times then n minus 1 times then n minus 2 then n minus 3 as i increases now if you actually look at the value of this sequence you'll see that it approaches n squared so we can simply say that all of these added together is n squared okay which means that we can just kind of write this as it takes n times okay we don't need to factor in all the minus ones or minus two minus three so on and so forth now continuing we have another for loop now this for loop is happening whatever the length of sum to the end is so it's going to happen first one time because that'll be the length when i equals one then it will happen two times then three then four all the way until it gets to n so similar to how we had n minus 1 n minus 2 and minus 3 here that's exactly what we have in the reverse order so we know that we can just say this is going to take n as well because 1 plus 2 plus 3 plus 4 up to n approaches n squared again i don't want to derive all the math here but i'm going to write that this takes n so we have n multiplied by n multiplied by n and that's all that we're doing inside of here which is going to give us an n cubed time algorithm so this for loop happens n times since inside of here this for loop happens n times we get n cubed so that's the time complexity big o of n cubed great let's move on to the last example here okay another complex example pause the video take a guess let's go through the time complexity right now okay so i'm just going to tell you the time complexity here is n factorial now if you're unfamiliar with the factorial obviously you would not have guessed this but a factorial is the following so if i have something like four factorial it is four times three times two times one if i had three factorial that's three times two times one okay that's factor so we see these in time complexities and a factorial is one of the worst time complexities that we can have refer to that graph i showed earlier if you want to see exactly where it ranks i believe it's actually the worst that was on that graph so n factorial is this time complexity but how do we determine well this is recursive so we're going to have to find how many times this recursive function runs and then multiply that by how much work is being done okay so we have n work being done inside the forward you know before we do that let's look at the recursive calls so we say if n is equal to one return one total equals zero four underscore in range n total plus equals example 12 n minus one so let's try this with an input of three if we try this with an input of three let's say n is currently equal to three what happens is total is equal to zero we're saying four i in range n and then we're going to call example 12 with n minus one how many times we call this three times okay we call it n times or whatever the input to the function was that's how many recursive calls we do so we have one two three recursive calls all to the same value which is n minus one which is two okay so there we go now what happens is we come to the function and now our input is two it's not one so we continue so we're looping through two and then we're doing this how many times whatever n is we're gonna now have two recursive calls uh with n being uh less than one okay so we're gonna say one one one one and one one now this is going to look very similar to the to the exponent n except what you're going to realize is that the levels in each tree or the i guess width of each section of the tree increases as the input increases so if i have four i have one two three four right so let's say i have three levels here well now what i'm doing is i'm saying okay i'm gonna have three levels and i'm gonna multiply that by however many levels this one's gonna have and this is gonna have how many levels is gonna have two so that gives me 3 times 2 levels then how many levels this one have it has 1. this is a factorial that's what the tree looks like now again i know this is confusing i know a lot of you are going to be lost at this point but this is the factorial algorithm and if you look at this and i punch in a number say like uh i guess what's the factorial of 5 i believe it's 120 we'll get 120 meaning we had to do 120 operations because that's the way that we've calculated them so that's what i was trying to illustrate here with the factorial let's just look at one more higher tree level of four so you can really see kind of how this works so if we have an input of four here what happens is we get four calls okay four recursive calls that all go now to recursive calls of three and then we get one two three one two three one two three one two three then we have two two two two two two okay i'm gonna write a bunch of these here okay and then we have one two one two one two one two and if you count how many of these we actually have how many base cases we hit we're going to be hitting whatever n factorial base cases is okay that's how many base cases in the recursion that we hit this is n factorial now what we can say is that theoretically we're going to have an upper bound of 2 times n factorial recursive calls in total so we hit the base case n factorial times but we're going to have in total two times n factorial because to be able to hit the base case we had to do all of the other calls above now since we have a constant we remove the two and that gives us n factorial for the time complexity of our algorithm again hard to explain this in much more depth than this for some of these things you kind of just get it or you don't and you have to look at a ton of different examples to really see how this works understand factorials understand math maybe i'm just not the best at teaching it definitely open to your comments below but that's really the best that i can kind of explain how the factorial works here we start with four and then four four we're doing four fact uh four recursive calls for every single one of those recursive calls we're doing three recursive calls then we're doing two recursive calls then we're doing one call simply which is just like returning one so we're not really doing a call but we're returning a value the number of times we hit this base case is equal to n factorial which means we actually have more than n factorial recursive calls but the most we're going to have is 2 times n factorial because any other calls above this the sum of them are going to be less than the amount of base cases that we have so we simply can cross off the 2 here and we get n factorial all right that is going to wrap up this video my voice is officially dying i've been filming this for about an hour and a half we'll see how long the video actually ends up being but hopefully this was helpful i understand that if you're a beginner in this the last example is probably to confuse you a little bit you don't have to stress out too much about those if you understood the first let's say what do we go up to here uh let's say eight examples then i think you're doing really well with big o notation i wanted to throw in some more advanced ones because i know some people are looking for that and i think that will be helpful to a few people anyways if you guys appreciate this video make sure to leave a like subscribe to the channel check out programming expert and i will see you in another youtube video you
