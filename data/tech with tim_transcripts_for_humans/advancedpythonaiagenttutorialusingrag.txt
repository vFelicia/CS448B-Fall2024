With timestamps:

00:00 - get ready because this video is going to
00:01 - get really cool really fast I'll be
00:04 - showing you how to build an artificial
00:06 - intelligence agent which can utilize a
00:08 - bunch of different tools that we provide
00:10 - it with that means we make the tools we
00:13 - give it to the AI and it will
00:14 - automatically decide the best tool to
00:16 - use this is super cool actually pretty
00:19 - easy to build and even if you're a
00:21 - beginner or intermediate programmer you
00:23 - should be able to follow along with that
00:25 - said let's get into a quick demo so in
00:27 - this demo you're going to see an agent
00:28 - who can answer questions a population
00:30 - and demographic data using something
00:33 - known as rag now rag is retrieval
00:35 - augmented generation and all this really
00:37 - means is that we're providing some extra
00:40 - data to the model so that it can go in
00:42 - reason based off that rather than it's
00:44 - old training data or something that
00:46 - might be out of date so in this instance
00:48 - I've got two data sources which we
00:50 - feeding to the model and then there's
00:51 - another kind of thing it can do which
00:53 - we'll talk about in a second so we have
00:55 - this population. CSV file now this is
00:58 - structured data and it's typically
01:00 - pretty easy for our model to ingest this
01:02 - and read it so this has pretty much all
01:04 - of the information about population
01:06 - density the change Etc and it will be
01:08 - able to kind of answer questions based
01:10 - on this CSV file we then have a PDF
01:13 - specifically about Canada now if we were
01:14 - building this project out we' put PDFs
01:17 - in for all of the different countries so
01:19 - we could get some more advanced
01:20 - information but just for demo purposes
01:22 - I've included one about Canada which is
01:24 - my home country now what this will allow
01:26 - the model to do is answer really
01:27 - specific questions based on information
01:30 - provided in this PDF now this is just
01:32 - the Wikipedia page for Canada what that
01:34 - means that the model can actually switch
01:36 - between using these different data
01:38 - sources or use both of them at the same
01:40 - time to give us the answer to the
01:42 - question that we need now we also have
01:44 - another functionality which is notes so
01:46 - at any point in time I can actually ask
01:48 - the agent to just take a note and it
01:50 - will go and save a note for me in this
01:52 - notes.txt file now this is pretty simple
01:55 - functionality but the idea is you can
01:57 - give this agent as many tools as you
01:59 - want want and it can automatically
02:01 - select the correct tool to use go ahead
02:03 - and utilize that which means you could
02:05 - tell the agent to call an API you can
02:08 - get it to do all kinds of advanced
02:09 - behavior and this video is really just
02:11 - going to scratch the surface but show
02:13 - you what's possible with this type of
02:14 - technology which is incredible so let's
02:16 - run this and have a look at how it works
02:19 - all right so I've ran the code here and
02:20 - I'm going to ask you the first question
02:22 - what countries have the highest
02:24 - population give me the top three now
02:26 - what's really interesting is we'll get
02:28 - to see the thoughts of the model here as
02:30 - it actually looks for the top three most
02:32 - populous countries so you can see that
02:34 - after a bit of thinking here it's gone
02:36 - through it's ran some operations it's
02:37 - actually use the different data sources
02:39 - that it has access to it's given us the
02:41 - top three most populous countries based
02:43 - on this data set which is China India
02:46 - and the United States now we can ask it
02:48 - a bunch of other questions as well and
02:49 - if we ask it to do something like take a
02:52 - note what it will actually do is save
02:55 - that information for us in our notes so
02:57 - if I go here to notes.txt you can see
02:59 - see now that it says the countries with
03:01 - the highest population China India and
03:03 - the United States pretty cool so that's
03:05 - kind of the agent capability right you
03:07 - can go outside and interact with other
03:09 - systems and tools that we give it now we
03:11 - can also ask get some specific
03:13 - information about Canada and you'll see
03:15 - when we do that that it will now go to
03:17 - the Canada data source instead so I'm
03:18 - going to ask it here what percent of
03:20 - Canadians speak English or French as
03:22 - their first language as a few typos
03:24 - there but not a big deal and now it's
03:26 - going to say okay we're going to use the
03:27 - Canada data set this time and it's going
03:30 - to go and find that information in there
03:32 - and then give us the result so it says
03:33 - approximately 98% of Canadians can speak
03:35 - either English or French as their first
03:37 - language which is correct based on that
03:39 - PDF data source this is super cool guys
03:42 - I have a lot of stuff to share with you
03:43 - in this video I'm going to show you how
03:45 - we build this exact application out and
03:48 - then by learning this you're going to
03:49 - see how you can extend this to use
03:50 - really any type of data source and have
03:52 - any kind of Aging capability now we are
03:55 - going to get into a step-by-step
03:56 - tutorial here but I want to give you
03:57 - some more information about what's going
03:59 - on on so you understand what we're about
04:01 - to build and the type of tooling we're
04:03 - going to be using now really what we
04:05 - need to do here for our agent to work
04:07 - well is we need to provide the correct
04:09 - information in a way that the llm can
04:11 - ingest it now in order to do that I've
04:13 - actually partnered up with llama index
04:15 - for this video now don't worry they're
04:17 - completely free they provide an open-
04:19 - Source package that allows you to
04:21 - actually ingest pretty much any
04:23 - different type of data whether it's
04:25 - structured or unstructured now just give
04:27 - you some information on why this is
04:28 - important really what we want to do with
04:30 - the agent is we want to give it our own
04:33 - data and we want to provide a set of
04:35 - tools to the agent that it can act upon
04:37 - right so it can go and query something
04:39 - from this tool it can go and save a note
04:41 - using this tool maybe it calls an API
04:43 - now we can write that on our own but
04:45 - it's a little bit more difficult and
04:47 - what llama index will do is provide us
04:49 - with a set of tools to allow us to
04:50 - ingest all different types of data so
04:53 - I'm on their landing page right now just
04:54 - because it's very quick to kind of see
04:56 - why we're actually going to use this
04:57 - free open- source tool as it says
05:00 - Unleash the Power of llms over your data
05:02 - you guys can obviously check it out from
05:04 - the link in the description and what it
05:05 - allows us to do is not just ingest the
05:07 - data but to index it as well and then
05:10 - gives us an interface where we can very
05:12 - easily query over that data which is
05:14 - exactly what our agent will be able to
05:17 - do we'll use these data sources to
05:19 - provide context for the answer it's
05:20 - going to give us you can see there's all
05:22 - kinds of apps you can build you know
05:24 - question and answer which is kind of
05:25 - similar to what we just did data
05:27 - augmented chat Bots knowledge agents
05:30 - structured analysis and what's great
05:32 - about this is that it works on
05:34 - unstructured structured and
05:35 - semi-structured data now typically it's
05:38 - going to be a lot easier to read
05:39 - structured data that's something that's
05:41 - in like a CSV file maybe it's in an
05:43 - Excel spreadsheet it has some known
05:45 - structure maybe rows and columns or it's
05:47 - in some format where we kind of know how
05:49 - the data is going to be structured
05:51 - whereas something like a PDF which is
05:53 - what we're going to be using for this
05:54 - example we have no idea how the data is
05:56 - going to be structured what it actually
05:58 - looks like and it makes makes it a lot
05:59 - more difficult for us to ingest it so
06:01 - with llama index as I'll show you in
06:03 - this video we can actually read
06:05 - unstructured data as well which really
06:07 - extends the capabilities of what we can
06:09 - do with our agent so what I want to do
06:11 - now is hop over to VSS code let's start
06:14 - going through some of the setup steps
06:15 - here and we're going to build out kind
06:17 - of the individual components of the
06:19 - agent you'll see how they work in
06:20 - isolation and then we'll combine them
06:22 - together to create that entire agent and
06:24 - you'll learn quickly how you can make
06:26 - your own agents that can do pretty much
06:27 - anything you want so let's get into into
06:29 - some of the setup steps here what we
06:31 - need to do is create a virtual
06:32 - environment we're going to install a few
06:34 - different python packages in there we're
06:36 - going to activate the environment and
06:38 - then we're going to get access to the
06:39 - data we need for this specific agent if
06:41 - you want to build a different agent
06:43 - totally fine you'll see quite easily how
06:45 - you can ingest different types of data
06:47 - and I'll talk about that when we get to
06:48 - this stage for now though we're going to
06:50 - open up a new folder which I have open
06:52 - in Visual Studio code feel free to use
06:54 - any editor you want and we're going to
06:56 - type the following command from our
06:58 - terminal or command prompt instance now
07:00 - this is going to be Python 3 hyphen M
07:03 - then V so this is our virtual
07:05 - environment and then we're going to say
07:06 - AI now you can call this anything that
07:08 - you want this should create a new
07:09 - virtual environment for us that's within
07:11 - this directory now what we want to do is
07:14 - activate this environment to activate it
07:16 - is a little bit different depending on
07:17 - your operating system for example if
07:19 - you're on Windows you would have just
07:20 - typed python hym venv aai python 3 might
07:23 - not work for you and then to activate if
07:25 - you're on Mac or Linux you can type
07:28 - source and then I believe this is AI SL
07:32 - bin SL activate and you should then see
07:35 - you get the prefix AI here whereas if
07:37 - you're on Windows you should be able to
07:40 - do SL AI SL bin SL activate and that
07:46 - should actually run this as an
07:47 - executable file if that doesn't work try
07:50 - doing this in a Powershell and that
07:52 - should actually activate it for you you
07:53 - should get access to this okay if you
07:55 - want to deactivate you can simply type
07:58 - the deactivate uh command I think I
08:00 - spelled that correctly and that'll
08:02 - deactivate the environment but from now
08:04 - we're just going to install the python
08:05 - packages we need so I'm going to type
08:08 - pip 3 install and we're going to install
08:10 - llama index again allowing us to ingest
08:13 - these different data sources and then we
08:15 - also install something we need for
08:16 - reading the PDFs so what we're going to
08:18 - install here is the following we're
08:20 - going to have llama index Pi PDF we're
08:24 - going to have Python
08:27 - d.v and we're going to have pan P these
08:29 - are the dependencies that we need
08:31 - obviously pandas is for reading in our
08:33 - CSV file Pi PDF for the PDF file llama
08:36 - index for setting up this whole agent
08:38 - and then python. EnV is for loading in
08:40 - some environment variable files which
08:42 - we'll look at in a second so go ahead
08:44 - and hit enter once this is finished I'll
08:46 - be back and then we'll continue with the
08:47 - rest of the video all right so all of
08:49 - this has been installed what we're going
08:50 - to do now is get the data sources that
08:52 - we'll need we're also going to get our
08:54 - open AI API key which we're going to use
08:56 - to be able to utilize the open AI models
08:59 - as a part of this project all right so
09:01 - I'm in my browser now what we're going
09:02 - to do is just download the different
09:04 - data that we need for this tutorial now
09:06 - the first one is just something I found
09:07 - on kagle you can download anything you
09:09 - want but this is the world population by
09:11 - country 2023 I'll leave this exact Link
09:14 - in the description it's free to download
09:16 - again from kagle next we are going to
09:18 - download the pdf version of the
09:20 - Wikipedia page of Canada now you can do
09:23 - this for any country you want in fact
09:24 - it's very easy to switch the country you
09:26 - could even do it for hundreds of
09:28 - countries but again for our purposes
09:30 - we'll just do a single one in order to
09:32 - do that you can simply go to tools here
09:34 - and you can go to download as PDF you
09:36 - can do this for any Wikipedia page you
09:38 - want again in our case we'll just do
09:40 - Canada now I've already got these
09:42 - downloaded but what we're going to want
09:43 - to do is place them inside of a folder
09:46 - called Data so create a new folder in
09:49 - the same environment uh where you have
09:51 - like your project open and then you're
09:52 - going to place those files you
09:54 - downloaded inside of here let me do that
09:56 - and I'll be right back all right so I've
09:58 - gone ahead and done done that and notice
09:59 - that I've renamed these files so I have
10:02 - canada.pdf and then population. CSV just
10:05 - make sure you name them something that's
10:07 - going to be easy to access from the code
10:09 - while we're here we'll also make a new
10:11 - file called notes.txt and this will
10:13 - store the different notes assuming that
10:15 - we spell the name correctly that we're
10:17 - going to store for this project and then
10:20 - we're going to make a new file outside
10:22 - of the directory so in the root kind of
10:24 - base directory called EnV and this is
10:27 - where we're going to store our open AI a
10:29 - API key now in order to actually
10:31 - interact with an AI model here we're
10:33 - going to be using open AI now I'll
10:35 - explain how this works in a second but
10:36 - for now we're going to say opencore aore
10:40 - API undor key uh and actually it's just
10:43 - going to be open AI as one word here is
10:46 - equal to and then this is where we're
10:48 - going to place the open AI API key so
10:50 - let's go now back to our browser so let
10:52 - me open up this page here and you see
10:57 - that we can go to platform. open
10:59 - a.com API keys I will leave this link in
11:02 - the description now I believe that in
11:04 - order to generate this API key you will
11:06 - have a credit card on file however there
11:08 - should be some free usage or you
11:10 - shouldn't actually be spending like any
11:12 - money by using this just a few times
11:14 - from your code what we do need is this
11:16 - openai API key so again go to openai
11:20 - sign into your account and then you're
11:21 - going to go to this URL we're going to
11:23 - click on create new secret key now from
11:26 - here we'll just give this a name I'm
11:28 - just going to call this uh something
11:31 - like llama cuz we're going to be using
11:32 - llama index and I will go ahead and
11:34 - create the key I'm going to copy this
11:37 - and paste this inside of my uh what do
11:40 - you call it environment variable file
11:41 - and then we should be good to go and we
11:43 - don't need to access the browser anymore
11:45 - all right so I've just pasted my open
11:46 - API key here inside of my environment
11:49 - variable file don't worry I will delete
11:50 - it after the tutorial so that you guys
11:52 - canot copy it uh but there we go we have
11:55 - it inside of here now that we've done
11:57 - that we're just going to create some
11:58 - files here and the first thing we're
12:00 - going to do is start looking at how we
12:02 - can query over pandas data so pandas is
12:05 - obviously a popular data science library
12:07 - and python allows us to read in
12:09 - structured data like CSV files so that's
12:12 - exactly what we're going to do we're
12:13 - going to read in our CSV file then I'm
12:15 - going to show you how we can actually
12:16 - query over top of it and ask questions
12:19 - based on that data source using kind of
12:21 - a streamlined or simple agent then we'll
12:24 - start adding extra data sources and
12:26 - you'll see how we work for PDF how we
12:28 - make not notes Etc all right so to get
12:30 - started here we're going to make a new
12:31 - file this is going to be
12:33 - main.py this is where we're going to
12:35 - write our code for now again we're going
12:37 - to work with that CSV file to start so
12:40 - first thing that we need to do is just
12:41 - activate this environment variable file
12:43 - what I mean by that is we just need to
12:45 - essentially load that in and to do that
12:47 - we can use the EnV Library so I'm going
12:49 - to say from EnV import load. EnV and
12:55 - then we're simply going to call the
12:56 - load. EnV file which will look for the
12:58 - presence of the EnV file and load in the
13:01 - environment variables we're also going
13:03 - to import OS and we're going to import
13:06 - pandas as PD we're going to use this to
13:09 - actually read in our CSV file there's a
13:11 - few other Imports we'll need but for now
13:13 - we can stick it at that and we can load
13:15 - in this data uh population. CSV so first
13:18 - thing we'll do is specify the path to
13:21 - our data so we'll say the population
13:23 - path is equal to os. path. jooy and
13:28 - we're going to join in the data
13:29 - directory with the population. CSV file
13:33 - which is the name of our file so now
13:35 - that we have that we're going to load
13:36 - this in with pandas so to do that we can
13:39 - say the population dataframe is equal to
13:44 - and this will be pd. read CSV and we're
13:47 - just going to read in the population
13:49 - path so now just to quickly test that
13:51 - this is working and by the way let's put
13:53 - this all beneath load
13:55 - CSV we can simply say print the
13:58 - population data frame. head Actually I
14:02 - don't even know if we need to print this
14:03 - but either way let's do this and we
14:05 - should see that we get some entries from
14:06 - our CSV file so let's go Python 3
14:11 - main.py make sure you're activated by
14:13 - the way in the
14:14 - environment and there you go we can see
14:17 - that we get the first five entries or
14:19 - the head of our data frame meaning we're
14:21 - loading in the pandas data frame
14:23 - correctly so now that we've done this
14:25 - what we want to do is create something
14:26 - known as a query engine which is going
14:28 - to allow must ask specific questions
14:30 - about this data source so in order to do
14:32 - that we're going to say from llama
14:34 - index. query engine import and then
14:38 - there's one specifically for pandas
14:40 - there's all kinds of different ones by
14:41 - the way but pandas is obviously one we
14:43 - can do and now we're going to say the
14:45 - population unor query engine is equal to
14:50 - the pandas query engine let me just
14:52 - check my notes here we're going to have
14:54 - to pass in the data frame which is our
14:57 - population data frame and then we can
14:59 - specify this option which is verbose
15:01 - equal to true now when you do that it's
15:03 - just going to give you all of the
15:05 - thoughts and uh kind of some more
15:07 - verbose or in this case detailed output
15:10 - when we use this query engine now all
15:13 - this is doing is essentially wrapping
15:14 - over top of this data frame and giving
15:16 - us an interface to ask questions about
15:18 - this data using this kind of retrieval
15:21 - augmented generation system now there's
15:23 - all kinds of different ways or query
15:25 - engines you can create Sor with llama
15:27 - index obviously panda is just the one
15:28 - we're using right now all right so now
15:30 - that we've got the query engine defined
15:31 - there's a few things we can pass to this
15:33 - to make it work a little bit better and
15:35 - to optimize its performance now what we
15:37 - want to do is actually pass this kind of
15:39 - an instruction string that specifies
15:42 - what it should be doing and then we want
15:44 - to kind of give it a template on how the
15:47 - prompt should be handled when we
15:48 - actually start querying some information
15:50 - from it so what I'm going to do is I'm
15:52 - just going to copy in the prompt
15:54 - templates and the strings we're going to
15:55 - use because they're a little bit long
15:57 - and then you can find all of the this
15:58 - from the link in the description I'll
16:00 - have all of the codes you can simply
16:01 - copy it for yourself but I'm going to
16:03 - make a new file here called prompts dopy
16:06 - and I'm just going to paste in a little
16:08 - bit of code here now let's make it so we
16:10 - can actually read most of this so what
16:12 - I've done is said from llama index
16:14 - import The Prompt template and we've
16:17 - specified two things an instruction
16:18 - string and a new prompt now in the
16:21 - instruction string you can see that it's
16:22 - kind of telling this engine what it
16:25 - should be actually doing with our pandis
16:27 - data so it says convert the query to
16:29 - executable python code using pandas the
16:31 - final line of the code should be a
16:32 - python expression that can be called
16:33 - with the evaluation function the code
16:36 - should represent a solution to the query
16:37 - print only the expression do not quote
16:40 - the expression okay so it's telling it
16:42 - what it needs to do we then have a new
16:44 - prompt now a prompt template is
16:47 - something that we can specify where we
16:49 - can embed whatever it is that we type
16:52 - inside of the template to provide some
16:54 - more context to the model when it's
16:57 - actually performing this query
16:59 - so in our case it says you're working
17:00 - with a panda's data frame in Python the
17:02 - name of the data frame is DF this is the
17:04 - result of DF do head then we specify the
17:08 - data frame string and it says follow
17:10 - this instructions the instruction string
17:12 - and then the query string which is what
17:14 - we actually give it okay so this is just
17:17 - kind of templating what we want the
17:19 - actual prompt to look like making it
17:21 - easier for us as the user to just give a
17:23 - hum human readable kind of query that's
17:26 - quite a bit shorter so we put this
17:27 - inside a prompts Pi now we're going to
17:29 - import these strings and use them here
17:31 - with our population query engine so
17:33 - we're going to say from prompts import
17:37 - the new prompt and the instruction
17:39 - string and now inside of our population
17:42 - query engine we're going to say the
17:44 - instruction string is equal to the
17:46 - instruction string and to update the
17:48 - prompt it's slightly different we're
17:50 - going to say population query engine do
17:52 - update prompts and then we're going to
17:54 - pass a python dictionary and here we're
17:56 - going to say the pandas underscore
17:59 - prompt and this is going to be equal to
18:01 - the new prompt okay so we can zoom that
18:04 - in a little bit so now that we have all
18:06 - of this and we've kind of given it the
18:07 - context we've given it the prompt again
18:09 - if we go back here you can see we have
18:11 - the prompt template and the instruction
18:12 - string what we can do is actually give
18:14 - this a query and we can see if we get a
18:16 - result so I can say population query
18:19 - engine. query so for the prompt here
18:21 - I'll just paste in what is the
18:23 - population of Canada we'll save our file
18:25 - we'll rerun this ignore all of the
18:27 - output you're getting here here it's
18:28 - just because we're in verbose mode we
18:30 - can disable that if we want and you can
18:32 - see that we get the population here of
18:34 - 38 m781
18:37 - 291 okay so that's working we can query
18:40 - directly using this Source but what's
18:42 - really going to happen is our agent is
18:44 - going to be able to use this as a tool
18:46 - where it will get that output and then
18:48 - it will parse that output with anything
18:50 - else that it needs and then give us a
18:52 - more human readable response so this is
18:54 - how you load pandas data this is
18:56 - structured data it's a bit easier to
18:58 - read and you can see that it's pretty
18:59 - simple right we just use the pandis
19:01 - query engine now what I want to do is
19:03 - show you how we can actually interface
19:04 - with a tool so what we can do is have
19:07 - multiple tools right in this case the
19:09 - population engine as well as say our
19:11 - note engine so now we can get some
19:13 - information and then ask the agent to
19:15 - take a note of it so let's look at how
19:16 - we do that so what we're going to do for
19:18 - the note engine is we're going to make a
19:19 - new file just to keep this nice and
19:21 - organized and we're going to call this
19:23 - the note engine. Pi now inside of here
19:26 - we're just going to write a python on
19:28 - function that can be executed by our
19:30 - model we can make this as complex as we
19:32 - want in our case it'll just be a simple
19:34 - function it will take in a note and it
19:36 - will just save it to a file so what
19:37 - we're going to do is say from the Llama
19:40 - index dot I believe this is something
19:42 - like tools import the function tool now
19:47 - we're going to use this to kind of wrap
19:48 - our function and tell llama index hey
19:51 - this is a tool that the model can use so
19:53 - now we'll write the location to the file
19:55 - that we'll actually save a note to so
19:56 - we'll say the Note file F will be equal
19:59 - to os. path. jooy which reminds me I
20:03 - also need to import the OS module and
20:05 - we're going to join on the data
20:07 - directory and then we call this
20:09 - notes.txt now what we're going to do is
20:12 - make a function and the function will
20:13 - simply save a note so we're going to say
20:15 - Define savecore note this is going to
20:19 - take in the note that we want to save
20:21 - first we're going to say if not os.
20:24 - path. exists the Note file
20:28 - then what we want to do is create this
20:30 - file so we're going to say open the Note
20:32 - file in W mode which just means we're
20:34 - going to create a new empty file pretty
20:36 - much otherwise we're going to say with
20:39 - open and this is going to be the Note
20:41 - file and we're going to open this in a
20:43 - mode which stands for append mode we're
20:45 - going to open this as the file named F
20:48 - then what we're going to do is say f.
20:50 - right lines and we're going to pass to
20:52 - this an array or a list inside the list
20:54 - we're simply going to have the note and
20:56 - we're going to append to this the back
20:58 - sln character so that we go down to the
21:00 - next line so it's just kind of a simpler
21:02 - way to do this here we're saying f.
21:04 - right lines so we're writing in a pen
21:05 - mode this just means we start at the end
21:07 - of the file and then this is the single
21:09 - note that we want to write and we want
21:11 - to have the back SL end so it goes down
21:13 - to the next character then we're just
21:15 - going to return from this saying
21:17 - something like note saved the reason why
21:19 - we want to do this is because the llm
21:21 - will actually be able to look at the
21:23 - return value of the function so we could
21:25 - see if it was like successful or if it
21:27 - failed or something went wrong so we
21:28 - just want to give it something to
21:30 - indicate that hey this did indeed work
21:32 - so that it knows that the note was
21:34 - actually saved now you can return any
21:35 - type of data you want here and again
21:37 - like I'm just doing a really simple
21:39 - python function that saves a note but we
21:41 - could have functions that do some
21:42 - complex calculus we could have a
21:44 - function that goes on my computer and
21:45 - cleans up some files like any type of
21:48 - code you can write the llm can call that
21:51 - code for you right so I'm just showing
21:53 - you a simple example with the note but
21:55 - we can really Implement anything we want
21:56 - which is where I think this gets quite
21:58 - cool so now we've got the kind of tool
22:01 - that we want the llm to have access to
22:03 - what we need to do now is wrap this and
22:05 - kind of create an engine that the um
22:07 - agent will be able to use so we're going
22:09 - to say note engine is equal to function
22:13 - tool do from defaults and we're going to
22:16 - specify the function so we're going to
22:18 - say FN is equal to save note we're going
22:21 - to give this a name now the information
22:23 - we pass here can help the model
22:26 - understand what this tool does so so for
22:28 - the name I'm going to say this is my
22:29 - note
22:31 - saer okay and then we can pass a
22:33 - description and for the description
22:35 - we'll say this tool can save a
22:39 - text based note to a file for the user
22:44 - now you can write this more detailed if
22:46 - you want but you should just give it
22:47 - like a decent description and a decent
22:49 - name so that it actually kind of
22:51 - specifies what the tool does so that the
22:53 - model knows how to pick between the
22:54 - different tools okay so let me zoom out
22:56 - so we can read all of this pretty
22:58 - straightforward we're just defining a
22:59 - function again you could do any type of
23:01 - functions you want here what we're going
23:02 - to do now is bring this function in and
23:05 - we're going to start kind of specifying
23:06 - a pipeline here of different tools that
23:09 - the uh agent has access to and then it
23:12 - will choose which one it needs to use so
23:14 - we're going to say from the note engine
23:17 - import the note engine I think that will
23:19 - work fine and now we're going to start
23:21 - kind of creating this collection of
23:22 - tools all right so first of all there's
23:24 - a few things we need to import so I'm
23:25 - going to say from llama index . tools
23:29 - import the query engine tool and then
23:32 - there's another tool we need I got to
23:33 - look at it over here this is the tool
23:36 - metadata we're then going to say from
23:38 - llama index Dot and I believe this is
23:41 - going to be agent import the react agent
23:46 - we're then going to say from llama index
23:49 - Dot and in this case it is going to be
23:52 - llms we're going to import the openai
23:55 - llm okay so that should be most of the
23:57 - Imports that we need what we're going to
23:59 - do now is we're going to specify the
24:00 - different tools that we have access to
24:02 - so we're going to say tools is equal to
24:04 - and we're going to create a list now the
24:06 - first tool that we can use is simply the
24:08 - note engine so we just put this inside
24:10 - of here the next tool is going to be the
24:12 - population query engine so by the way
24:14 - let's remove this line here because we
24:16 - don't want to manually be querying this
24:18 - and what we need to do is wrap this in
24:20 - the query engine tool so it kind of
24:23 - specifies hey this query engine here
24:25 - we're going to add some instructions
24:27 - we're going to add some description to
24:28 - this you'll see how it works so we're
24:29 - just going to say query engine tool now
24:32 - for the query engine tool we're going to
24:33 - pass the query engine which is the
24:36 - population query engine we're then going
24:38 - to say the metadata is equal to the tool
24:41 - metadata again the other thing that we
24:43 - imported here and for the tool metadata
24:45 - similarly to the tool we just created we
24:48 - will give this a name and we will give
24:50 - this a description uh now for the name
24:53 - we will call this the population under _
24:58 - data and we'll say this gives
25:02 - information about the world population
25:06 - and
25:07 - demo Graphics okay so now we have two
25:10 - tools that we can use the note engine
25:12 - and the query engine tool again just
25:14 - kind of a wrapper to allow us to provide
25:16 - some metadata here to this tool what
25:18 - we'll do now is we'll set up an agent
25:20 - which will have access to these tools
25:21 - and you'll see kind of how it works and
25:23 - how it can query this different data so
25:25 - we're going to say llm is equal to open
25:28 - Ai and we're going to pass the model
25:30 - that we want to use now the model is
25:32 - going to be GPT uh-
25:35 - 3.5 Das turbo and then I got to look at
25:39 - the one I was using
25:41 - 0613 I'm sure there's probably some
25:43 - newer versions but this is the one that
25:44 - I was using that was working well and
25:46 - now that we have the llm we're going to
25:48 - create an agent and the agent will have
25:50 - access to these tools so we're going to
25:51 - say the agent is the react agent and
25:55 - this is going to be Dot from tools and
25:59 - we're going to pass to this the tools so
26:01 - the tools are going to be the tools the
26:04 - llm is going to be the llm and the
26:08 - verbose is going to be equal to true
26:10 - just so we get some detailed information
26:11 - on the thoughts of the llm so we know
26:14 - what type of tool it's going to be using
26:16 - now the react agent from tools is just
26:18 - setting everything up for us so that we
26:20 - can pass in these individual tools and
26:22 - it will kind of tell the agent like you
26:24 - need to pick the best tool for the job
26:26 - it will then do that and we will be able
26:28 - to utilize the agent now another thing
26:30 - we can specify here is some context so
26:32 - we can say context is equal to and then
26:35 - we can pass any string we want and this
26:37 - can tell the agent beforehand what it is
26:39 - that it's supposed to be doing so it has
26:41 - some more information and well context
26:43 - about what it should do so we can
26:45 - specify a context string by going inside
26:47 - of prompts and we can say context is
26:50 - equal to and I'm just going to paste one
26:52 - in here again you can copy it from the
26:53 - link in the description all right so I
26:55 - pasted this in it says purpose the
26:57 - primary rule role of the agent is to
26:58 - assist users by providing accurate
27:00 - information about world population
27:01 - statistics and details about a country
27:04 - okay pretty straightforward but that is
27:05 - our context now if I go back to main.py
27:08 - I can import that from my prompts so
27:11 - we'll just go from prompts import that
27:13 - and then for the context we will specify
27:16 - the context now what we can do is set up
27:19 - a simple W Loop and we can just have the
27:20 - W Loop continually use the agent and ask
27:24 - it different prompts right and then the
27:25 - agent can utilize the tools and give us
27:27 - a response resp once so we're going to
27:28 - say wow and we're actually going to use
27:30 - the wallor operator here which is new in
27:32 - Python 3.9 so make sure you have that
27:35 - this is prompt callon equals to input
27:38 - and we're going to say enter a prompt
27:41 - and then Q to quit like that and I'm
27:45 - going to say while this is not equal to
27:47 - Q then we're going to go in here and
27:49 - we're going to use the agents we're
27:51 - going to say result is equal to agent.
27:54 - query and we're going to query the
27:56 - prompt
27:58 - and then we're going to print the result
28:02 - okay so quickly what this is doing is
28:03 - we're defining a variable prompt it's
28:05 - equal to whatever the user inputs if
28:07 - they type in Q we're simply going to
28:08 - quit otherwise we're going to specify
28:10 - the result is equal to querying the
28:12 - agent utilizing the prompt here and then
28:15 - we're going to print the result all
28:16 - right so we can run the code here and we
28:18 - can start by maybe asking it to save a
28:20 - note can you save a note for me
28:25 - saying I love Tim okay let's see if we
28:29 - can do that it says note saver I love
28:31 - Tim I can answer without using any more
28:33 - tools note saved successfully if we go
28:35 - to notes you can see it says I love Tim
28:37 - and then we can ask it what is
28:40 - the population of Vatican
28:44 - City let's see if it can find that for
28:47 - us just going to go look at the
28:48 - population data and I guess Vatican City
28:51 - is probably not in there hence why it's
28:52 - not able to find that for us let's say
28:55 - What is the
28:58 - population of India let's see if we can
29:01 - do that one and there you go it finds
29:03 - the population for us and it tells us in
29:05 - human readable form this is the
29:06 - population if you don't like all of this
29:08 - stuff being spit out you can simply go
29:10 - and remove ver Bose mode and then you
29:12 - won't see kind of all of the thoughts
29:13 - but I think this actually pretty
29:14 - interesting and it kind of proves how
29:16 - this is working all right so let's quit
29:17 - out of that by hitting q and now I want
29:20 - to actually start reading in that PDF
29:22 - data and kind of adding that to our tool
29:24 - set so that we can get some specific
29:26 - information about Canada and view how to
29:28 - read that unstructured data so let's
29:31 - make a new file here and let's call this
29:33 - PDF dopy now inside of here we're going
29:37 - to start reading in that PDF file now
29:39 - for the PDF file since this is
29:41 - unstructured we're going to read it in a
29:43 - little bit differently and we're going
29:44 - to use one of the readers that comes
29:46 - from llama index now you might be asking
29:49 - yourself okay well what's different
29:51 - about unstructured and structured well
29:53 - again with unstructured data we can't
29:55 - just kind of put this in a nice table
29:57 - right which is easy to access and easy
29:59 - to reference with something like Panda's
30:00 - code in our case what we're actually
30:02 - going to use is something known as a
30:04 - vector store index now the way the
30:06 - vector store index works is we're pretty
30:08 - much taking all of our data and we're
30:10 - creating something known as embeddings
30:12 - for it now embeddings are these kind of
30:14 - multi-dimensional objects or embeddings
30:16 - vectors whatever you want to call them
30:18 - and we can very quickly index them and
30:20 - query them in this database so we do
30:23 - that by kind of checking for similarity
30:25 - of intent and words it's a lot more
30:27 - complicated than I can really explain
30:29 - here in a few minutes I'd encourage you
30:30 - to look it up if you're more curious
30:32 - about how it works but in our case since
30:33 - we're working with this unstructured
30:35 - data we turn it into this Vector store
30:38 - index that contains all of our info and
30:40 - then we can go to that index with our
30:42 - query and we can very quickly retrieve
30:44 - the specific parts of this unstructured
30:46 - data that we're looking for to be able
30:48 - to answer the question hopefully that's
30:50 - kind of a simple enough explanation it's
30:52 - probably not 100% accurate but it gives
30:54 - you enough of an idea of what's going to
30:56 - be going on here here so what we're
30:58 - going to do inside of here is import a
30:59 - few things again we're going to say
31:00 - import OS and we're going to say from
31:04 - llama index import the storage context
31:09 - the vector store index and the load
31:12 - index from storage now the great thing
31:14 - here is that we don't need to keep
31:15 - recreating this index we're just going
31:17 - to make it one time and then once it's
31:19 - created we can just read from it hence
31:21 - why I'm importing a function like this
31:23 - load index from storage because it might
31:24 - already be created now we're also going
31:26 - to read or get sorry the PDF reader so
31:29 - I'm going to say from llama index.
31:33 - readers import the PDF reader now there
31:37 - is this kind of General reader in llama
31:40 - index that can read in like most types
31:42 - of data so it can read in text files it
31:44 - can read in um you know PDFs it can read
31:47 - in all different types of things but I
31:49 - want to show you how we can use a
31:51 - specific reader and how you can find a
31:53 - reader that you may want for the
31:55 - different type of data that you have so
31:57 - so what I'll do is just quickly open up
31:58 - this document here this web page and
32:00 - show you that there is something called
32:01 - llama Hub which contains kind of a
32:04 - directory of all of the different
32:05 - readers you can have access to so I'll
32:07 - link this in the description but when I
32:09 - found the PDF reader the way I found
32:11 - that was by going to llah HUB and then
32:12 - just typing in here PDF and then it
32:15 - showed me file PDF so I clicked on this
32:18 - and realized okay there's this PDF
32:19 - reader I can use and then I found that
32:21 - it was actually included by default in
32:23 - llama index so I just went ahead and
32:24 - used it but if we go back here right
32:26 - you'll see that there's all kinds of
32:27 - ones unstructured markdown Google Docs
32:29 - PowerPoint mongodb pretty much anything
32:31 - you want to read in it's already here so
32:33 - you don't need to manually write that
32:35 - integration now I will show you that
32:36 - there is something more General called
32:38 - the simple directory reader now this is
32:40 - something that just reads an entire
32:42 - directory and it can handle all
32:43 - different types of data including a PDF
32:46 - I didn't want to use that one for this
32:47 - tutorial but I just want to show you
32:49 - that it does exist and it can read in
32:51 - again like pretty much any different
32:52 - type of data and then for some reason it
32:54 - can't handle the data you have then you
32:56 - can go to llah HUB and you can probably
32:57 - find an integration that's already been
32:59 - built anyways back inside of our code
33:01 - here what we're going to do now again is
33:03 - get access to this Canada PDF file we're
33:06 - going to read it in using the PDF reader
33:08 - and then we're going to create that
33:09 - Vector store index so we're going to say
33:12 - the PDF path is equal to os. path. jooy
33:17 - and this is going to be
33:18 - data and canada.pdf we're now going to
33:22 - use the PDF reader so I'm going to say
33:24 - the Canada PDF is equal to the PDF
33:28 - reader we're going to create an instance
33:30 - of that and then this is going to be dot
33:33 - load data and we're going to load in the
33:36 - file equal to the PDF path now we have
33:40 - access uh to the loaded in PDF file and
33:44 - now what we're going to do is create a
33:45 - function that will actually get this
33:46 - Vector store index for us so we're going
33:49 - to say
33:50 - Define get index we're going to take in
33:53 - some data and name of our index and what
33:57 - we're going to do is we're going to
33:58 - start by checking if the index exists if
34:01 - it does uh then we can simply grab it
34:03 - otherwise we need to create it so we're
34:05 - going to say here index equals none
34:09 - we're then going to say if not os. path.
34:12 - exist the index name which is what we're
34:14 - going to store it under we need to
34:16 - create it otherwise we can load it and
34:19 - then eventually we can just return the
34:22 - index now fortunately we don't need to
34:24 - be math geniuses to do this we could
34:26 - just use llama index to actually create
34:27 - the index so we're just going to print
34:31 - building index and then we'll specify
34:34 - the index name then we're going to say
34:37 - the index is equal to the vector store
34:40 - index and this is going to be Dot from
34:43 - documents we're going to say data and
34:46 - then we're going to say show progress is
34:48 - equal to true so what this is doing is
34:51 - when we load in the PDF radar we're
34:52 - getting something known as a document
34:54 - we're actually going to add a bunch of
34:55 - different documents we're going to pass
34:57 - this as our data we're then going to
34:58 - create an index from that data so that's
35:01 - what we're doing here and we're just
35:02 - showing that progress as we do it after
35:04 - we create that we want to store it so
35:07 - we're going to say index. storage
35:11 - context. persist and we're going to say
35:13 - the persistent directory is equal to the
35:16 - index name and this is just going to go
35:18 - ahead and save the index for us in a
35:20 - folder then what we'll do here so if we
35:24 - uh already have the index we'll load it
35:26 - we'll say index is equal to and this is
35:29 - going to be load index from storage and
35:32 - we're going to pass the storage
35:35 - context. from defaults and this is going
35:38 - to be the persist directory at the index
35:41 - name now if you're wondering Tim how the
35:44 - heck do you know how to type all this
35:45 - stuff well I just found it from the
35:47 - Llama index documentation you guys can
35:49 - reference that as well but obviously
35:50 - this video is to help you get through
35:51 - this a little bit faster because this
35:53 - did take me you know a bit longer when I
35:54 - was trying to go through all the
35:55 - documentation and anyways we say index
35:57 - equals To None okay now what we're doing
35:59 - does the index exist if it doesn't let's
36:01 - go ahead and build it out and then save
36:03 - it if it did exist we can simply load
36:05 - this from Storage so we say okay well
36:07 - where is it it's in this location so
36:09 - let's load it from storage and then
36:11 - return the index so now we need to
36:12 - actually call this function so we'll say
36:15 - the
36:16 - Canada index is equal to and this will
36:19 - be get index and then we're going to
36:22 - pass the Canada PDF so this is going to
36:25 - have all of our documents and our data
36:27 - and then we can give this a name and I'm
36:29 - just going to call this Canada okay last
36:33 - thing we need to do is create an engine
36:34 - so we're going to say Canada engine is
36:37 - equal to the Canada index. asore query
36:41 - engine now what this does is use this
36:43 - Vector store index as a query engine
36:45 - just like we add the query engine for
36:47 - our population data now that we have
36:49 - this inside of here it's an additional
36:51 - tool that we can use with our agent so
36:54 - again we could say you know Canada
36:57 - engine Dot and then we can query this
37:00 - because we have the query interface
37:01 - available and it would give us some
37:03 - result but now we already know how that
37:04 - works so what we can do is we can go
37:06 - back to mupy and we can import it so we
37:09 - can say from what do we call this the
37:12 - PDF import and this is the Canada engine
37:17 - and now what we did before right with
37:18 - our query engine tool we can copy this
37:21 - paste it and we can now specify that we
37:24 - want to have the uh what is this the
37:27 - Canada engine and for the tool metadata
37:30 - we're going to say that this is Canada
37:32 - data and then we can say this gives
37:37 - detailed information
37:40 - about
37:42 - Canada the country okay so now we've
37:45 - added another tool to our tool set that
37:47 - the agent can use and that's it that's
37:49 - all we need to do that actually finishes
37:51 - the entire project we can now run the
37:53 - code and make sure all of this is
37:54 - working so let's run this here and we
37:56 - want to give it a query that um will
37:59 - kind of force it to use this Canada data
38:01 - set right so first of all it says okay
38:03 - building index for Canada so it's going
38:05 - to parse the nodes create the embeddings
38:07 - notice that it creates a new directory
38:08 - for us with Canada so notice here we've
38:10 - got all these different files right
38:12 - containing information about the vector
38:14 - store index they don't mean anything to
38:16 - me they probably don't mean anything to
38:17 - you but they're here and now what we can
38:19 - do is ask some questions so I can say
38:21 - something like tell me
38:24 - about the language is spoken in Canada
38:30 - and let's see what it gives us here says
38:31 - okay I need to use some data source I'm
38:33 - going to use the Canada data input
38:35 - languages and then it gives us this kind
38:37 - of big blurp here and it says in Canada
38:38 - the two main languages are English and
38:40 - French approximately 98% of Canadians
38:42 - can speak either or both English and
38:43 - French English spoken by 50% 7% of
38:46 - Canadians blah blah blah and then we can
38:47 - ask it more and more questions now we
38:49 - can say save a note of that I'm going to
38:52 - hope that this one works and for some
38:54 - reason it's not using the previous
38:56 - answer in the note um but what I can do
38:59 - is say you know let's ask it the same
39:01 - thing tell me about the
39:06 - languages in Canada and save a note of
39:12 - that okay so let's see if it works this
39:14 - time let's give it a sec so there you go
39:16 - you can see it actually ended up using
39:17 - both of the tools here it says I can use
39:19 - the Canada data tool and then use the
39:21 - node saer tool to save a notab kind of
39:23 - cool that it deduces that and we get
39:25 - languages and that says okay here's a
39:27 - note boom spits out the information here
39:30 - as a note and we can obviously check
39:31 - that in notes.txt and you see that we
39:35 - get that information stored as a note so
39:38 - I think guys with that said that's going
39:39 - to wrap up the coding component of this
39:41 - video all of this will be linked in the
39:43 - description in case you got lost or you
39:44 - want to copy any of this and it's really
39:46 - interesting to me what's actually
39:48 - possible here with agent-based AI being
39:51 - able to pass these different tools data
39:53 - sets functions to an llm and letting it
39:56 - read and using that is super interesting
39:58 - to me and that allows you as the human
40:00 - to have a little bit more control but to
40:02 - let the agent kind of you know make the
40:04 - decision on what tool it should be using
40:06 - and what it actually needs to do I
40:08 - already can imagine a ton of different
40:10 - great applications that I could build
40:12 - using this type of technology that
40:14 - wouldn't be that overly complicated
40:16 - because of how easy it was here to use
40:17 - llama index for the query engines and
40:19 - even for just wrapping python functions
40:21 - right like there's probably a 100
40:23 - different python functions I could write
40:25 - that would be super cool that this agent
40:26 - can then go and execute and use and I
40:28 - could probably create an entire
40:30 - application quite a bit faster just by
40:32 - making all these individual tools
40:33 - wrapping it with this llm and saying
40:35 - okay agent you know go use the tool
40:37 - based on whatever the response is
40:39 - anyways I'm going to wrap it up here if
40:41 - you guys enjoyed make sure you leave a
40:43 - like you can check out all the
40:44 - documentation and code from the link in
40:46 - the description and I look forward to
40:48 - seeing you in another YouTube
40:51 - [Music]
40:55 - video
40:57 - m

Cleaned transcript:

get ready because this video is going to get really cool really fast I'll be showing you how to build an artificial intelligence agent which can utilize a bunch of different tools that we provide it with that means we make the tools we give it to the AI and it will automatically decide the best tool to use this is super cool actually pretty easy to build and even if you're a beginner or intermediate programmer you should be able to follow along with that said let's get into a quick demo so in this demo you're going to see an agent who can answer questions a population and demographic data using something known as rag now rag is retrieval augmented generation and all this really means is that we're providing some extra data to the model so that it can go in reason based off that rather than it's old training data or something that might be out of date so in this instance I've got two data sources which we feeding to the model and then there's another kind of thing it can do which we'll talk about in a second so we have this population. CSV file now this is structured data and it's typically pretty easy for our model to ingest this and read it so this has pretty much all of the information about population density the change Etc and it will be able to kind of answer questions based on this CSV file we then have a PDF specifically about Canada now if we were building this project out we' put PDFs in for all of the different countries so we could get some more advanced information but just for demo purposes I've included one about Canada which is my home country now what this will allow the model to do is answer really specific questions based on information provided in this PDF now this is just the Wikipedia page for Canada what that means that the model can actually switch between using these different data sources or use both of them at the same time to give us the answer to the question that we need now we also have another functionality which is notes so at any point in time I can actually ask the agent to just take a note and it will go and save a note for me in this notes.txt file now this is pretty simple functionality but the idea is you can give this agent as many tools as you want want and it can automatically select the correct tool to use go ahead and utilize that which means you could tell the agent to call an API you can get it to do all kinds of advanced behavior and this video is really just going to scratch the surface but show you what's possible with this type of technology which is incredible so let's run this and have a look at how it works all right so I've ran the code here and I'm going to ask you the first question what countries have the highest population give me the top three now what's really interesting is we'll get to see the thoughts of the model here as it actually looks for the top three most populous countries so you can see that after a bit of thinking here it's gone through it's ran some operations it's actually use the different data sources that it has access to it's given us the top three most populous countries based on this data set which is China India and the United States now we can ask it a bunch of other questions as well and if we ask it to do something like take a note what it will actually do is save that information for us in our notes so if I go here to notes.txt you can see see now that it says the countries with the highest population China India and the United States pretty cool so that's kind of the agent capability right you can go outside and interact with other systems and tools that we give it now we can also ask get some specific information about Canada and you'll see when we do that that it will now go to the Canada data source instead so I'm going to ask it here what percent of Canadians speak English or French as their first language as a few typos there but not a big deal and now it's going to say okay we're going to use the Canada data set this time and it's going to go and find that information in there and then give us the result so it says approximately 98% of Canadians can speak either English or French as their first language which is correct based on that PDF data source this is super cool guys I have a lot of stuff to share with you in this video I'm going to show you how we build this exact application out and then by learning this you're going to see how you can extend this to use really any type of data source and have any kind of Aging capability now we are going to get into a stepbystep tutorial here but I want to give you some more information about what's going on on so you understand what we're about to build and the type of tooling we're going to be using now really what we need to do here for our agent to work well is we need to provide the correct information in a way that the llm can ingest it now in order to do that I've actually partnered up with llama index for this video now don't worry they're completely free they provide an open Source package that allows you to actually ingest pretty much any different type of data whether it's structured or unstructured now just give you some information on why this is important really what we want to do with the agent is we want to give it our own data and we want to provide a set of tools to the agent that it can act upon right so it can go and query something from this tool it can go and save a note using this tool maybe it calls an API now we can write that on our own but it's a little bit more difficult and what llama index will do is provide us with a set of tools to allow us to ingest all different types of data so I'm on their landing page right now just because it's very quick to kind of see why we're actually going to use this free open source tool as it says Unleash the Power of llms over your data you guys can obviously check it out from the link in the description and what it allows us to do is not just ingest the data but to index it as well and then gives us an interface where we can very easily query over that data which is exactly what our agent will be able to do we'll use these data sources to provide context for the answer it's going to give us you can see there's all kinds of apps you can build you know question and answer which is kind of similar to what we just did data augmented chat Bots knowledge agents structured analysis and what's great about this is that it works on unstructured structured and semistructured data now typically it's going to be a lot easier to read structured data that's something that's in like a CSV file maybe it's in an Excel spreadsheet it has some known structure maybe rows and columns or it's in some format where we kind of know how the data is going to be structured whereas something like a PDF which is what we're going to be using for this example we have no idea how the data is going to be structured what it actually looks like and it makes makes it a lot more difficult for us to ingest it so with llama index as I'll show you in this video we can actually read unstructured data as well which really extends the capabilities of what we can do with our agent so what I want to do now is hop over to VSS code let's start going through some of the setup steps here and we're going to build out kind of the individual components of the agent you'll see how they work in isolation and then we'll combine them together to create that entire agent and you'll learn quickly how you can make your own agents that can do pretty much anything you want so let's get into into some of the setup steps here what we need to do is create a virtual environment we're going to install a few different python packages in there we're going to activate the environment and then we're going to get access to the data we need for this specific agent if you want to build a different agent totally fine you'll see quite easily how you can ingest different types of data and I'll talk about that when we get to this stage for now though we're going to open up a new folder which I have open in Visual Studio code feel free to use any editor you want and we're going to type the following command from our terminal or command prompt instance now this is going to be Python 3 hyphen M then V so this is our virtual environment and then we're going to say AI now you can call this anything that you want this should create a new virtual environment for us that's within this directory now what we want to do is activate this environment to activate it is a little bit different depending on your operating system for example if you're on Windows you would have just typed python hym venv aai python 3 might not work for you and then to activate if you're on Mac or Linux you can type source and then I believe this is AI SL bin SL activate and you should then see you get the prefix AI here whereas if you're on Windows you should be able to do SL AI SL bin SL activate and that should actually run this as an executable file if that doesn't work try doing this in a Powershell and that should actually activate it for you you should get access to this okay if you want to deactivate you can simply type the deactivate uh command I think I spelled that correctly and that'll deactivate the environment but from now we're just going to install the python packages we need so I'm going to type pip 3 install and we're going to install llama index again allowing us to ingest these different data sources and then we also install something we need for reading the PDFs so what we're going to install here is the following we're going to have llama index Pi PDF we're going to have Python d.v and we're going to have pan P these are the dependencies that we need obviously pandas is for reading in our CSV file Pi PDF for the PDF file llama index for setting up this whole agent and then python. EnV is for loading in some environment variable files which we'll look at in a second so go ahead and hit enter once this is finished I'll be back and then we'll continue with the rest of the video all right so all of this has been installed what we're going to do now is get the data sources that we'll need we're also going to get our open AI API key which we're going to use to be able to utilize the open AI models as a part of this project all right so I'm in my browser now what we're going to do is just download the different data that we need for this tutorial now the first one is just something I found on kagle you can download anything you want but this is the world population by country 2023 I'll leave this exact Link in the description it's free to download again from kagle next we are going to download the pdf version of the Wikipedia page of Canada now you can do this for any country you want in fact it's very easy to switch the country you could even do it for hundreds of countries but again for our purposes we'll just do a single one in order to do that you can simply go to tools here and you can go to download as PDF you can do this for any Wikipedia page you want again in our case we'll just do Canada now I've already got these downloaded but what we're going to want to do is place them inside of a folder called Data so create a new folder in the same environment uh where you have like your project open and then you're going to place those files you downloaded inside of here let me do that and I'll be right back all right so I've gone ahead and done done that and notice that I've renamed these files so I have canada.pdf and then population. CSV just make sure you name them something that's going to be easy to access from the code while we're here we'll also make a new file called notes.txt and this will store the different notes assuming that we spell the name correctly that we're going to store for this project and then we're going to make a new file outside of the directory so in the root kind of base directory called EnV and this is where we're going to store our open AI a API key now in order to actually interact with an AI model here we're going to be using open AI now I'll explain how this works in a second but for now we're going to say opencore aore API undor key uh and actually it's just going to be open AI as one word here is equal to and then this is where we're going to place the open AI API key so let's go now back to our browser so let me open up this page here and you see that we can go to platform. open a.com API keys I will leave this link in the description now I believe that in order to generate this API key you will have a credit card on file however there should be some free usage or you shouldn't actually be spending like any money by using this just a few times from your code what we do need is this openai API key so again go to openai sign into your account and then you're going to go to this URL we're going to click on create new secret key now from here we'll just give this a name I'm just going to call this uh something like llama cuz we're going to be using llama index and I will go ahead and create the key I'm going to copy this and paste this inside of my uh what do you call it environment variable file and then we should be good to go and we don't need to access the browser anymore all right so I've just pasted my open API key here inside of my environment variable file don't worry I will delete it after the tutorial so that you guys canot copy it uh but there we go we have it inside of here now that we've done that we're just going to create some files here and the first thing we're going to do is start looking at how we can query over pandas data so pandas is obviously a popular data science library and python allows us to read in structured data like CSV files so that's exactly what we're going to do we're going to read in our CSV file then I'm going to show you how we can actually query over top of it and ask questions based on that data source using kind of a streamlined or simple agent then we'll start adding extra data sources and you'll see how we work for PDF how we make not notes Etc all right so to get started here we're going to make a new file this is going to be main.py this is where we're going to write our code for now again we're going to work with that CSV file to start so first thing that we need to do is just activate this environment variable file what I mean by that is we just need to essentially load that in and to do that we can use the EnV Library so I'm going to say from EnV import load. EnV and then we're simply going to call the load. EnV file which will look for the presence of the EnV file and load in the environment variables we're also going to import OS and we're going to import pandas as PD we're going to use this to actually read in our CSV file there's a few other Imports we'll need but for now we can stick it at that and we can load in this data uh population. CSV so first thing we'll do is specify the path to our data so we'll say the population path is equal to os. path. jooy and we're going to join in the data directory with the population. CSV file which is the name of our file so now that we have that we're going to load this in with pandas so to do that we can say the population dataframe is equal to and this will be pd. read CSV and we're just going to read in the population path so now just to quickly test that this is working and by the way let's put this all beneath load CSV we can simply say print the population data frame. head Actually I don't even know if we need to print this but either way let's do this and we should see that we get some entries from our CSV file so let's go Python 3 main.py make sure you're activated by the way in the environment and there you go we can see that we get the first five entries or the head of our data frame meaning we're loading in the pandas data frame correctly so now that we've done this what we want to do is create something known as a query engine which is going to allow must ask specific questions about this data source so in order to do that we're going to say from llama index. query engine import and then there's one specifically for pandas there's all kinds of different ones by the way but pandas is obviously one we can do and now we're going to say the population unor query engine is equal to the pandas query engine let me just check my notes here we're going to have to pass in the data frame which is our population data frame and then we can specify this option which is verbose equal to true now when you do that it's just going to give you all of the thoughts and uh kind of some more verbose or in this case detailed output when we use this query engine now all this is doing is essentially wrapping over top of this data frame and giving us an interface to ask questions about this data using this kind of retrieval augmented generation system now there's all kinds of different ways or query engines you can create Sor with llama index obviously panda is just the one we're using right now all right so now that we've got the query engine defined there's a few things we can pass to this to make it work a little bit better and to optimize its performance now what we want to do is actually pass this kind of an instruction string that specifies what it should be doing and then we want to kind of give it a template on how the prompt should be handled when we actually start querying some information from it so what I'm going to do is I'm just going to copy in the prompt templates and the strings we're going to use because they're a little bit long and then you can find all of the this from the link in the description I'll have all of the codes you can simply copy it for yourself but I'm going to make a new file here called prompts dopy and I'm just going to paste in a little bit of code here now let's make it so we can actually read most of this so what I've done is said from llama index import The Prompt template and we've specified two things an instruction string and a new prompt now in the instruction string you can see that it's kind of telling this engine what it should be actually doing with our pandis data so it says convert the query to executable python code using pandas the final line of the code should be a python expression that can be called with the evaluation function the code should represent a solution to the query print only the expression do not quote the expression okay so it's telling it what it needs to do we then have a new prompt now a prompt template is something that we can specify where we can embed whatever it is that we type inside of the template to provide some more context to the model when it's actually performing this query so in our case it says you're working with a panda's data frame in Python the name of the data frame is DF this is the result of DF do head then we specify the data frame string and it says follow this instructions the instruction string and then the query string which is what we actually give it okay so this is just kind of templating what we want the actual prompt to look like making it easier for us as the user to just give a hum human readable kind of query that's quite a bit shorter so we put this inside a prompts Pi now we're going to import these strings and use them here with our population query engine so we're going to say from prompts import the new prompt and the instruction string and now inside of our population query engine we're going to say the instruction string is equal to the instruction string and to update the prompt it's slightly different we're going to say population query engine do update prompts and then we're going to pass a python dictionary and here we're going to say the pandas underscore prompt and this is going to be equal to the new prompt okay so we can zoom that in a little bit so now that we have all of this and we've kind of given it the context we've given it the prompt again if we go back here you can see we have the prompt template and the instruction string what we can do is actually give this a query and we can see if we get a result so I can say population query engine. query so for the prompt here I'll just paste in what is the population of Canada we'll save our file we'll rerun this ignore all of the output you're getting here here it's just because we're in verbose mode we can disable that if we want and you can see that we get the population here of 38 m781 291 okay so that's working we can query directly using this Source but what's really going to happen is our agent is going to be able to use this as a tool where it will get that output and then it will parse that output with anything else that it needs and then give us a more human readable response so this is how you load pandas data this is structured data it's a bit easier to read and you can see that it's pretty simple right we just use the pandis query engine now what I want to do is show you how we can actually interface with a tool so what we can do is have multiple tools right in this case the population engine as well as say our note engine so now we can get some information and then ask the agent to take a note of it so let's look at how we do that so what we're going to do for the note engine is we're going to make a new file just to keep this nice and organized and we're going to call this the note engine. Pi now inside of here we're just going to write a python on function that can be executed by our model we can make this as complex as we want in our case it'll just be a simple function it will take in a note and it will just save it to a file so what we're going to do is say from the Llama index dot I believe this is something like tools import the function tool now we're going to use this to kind of wrap our function and tell llama index hey this is a tool that the model can use so now we'll write the location to the file that we'll actually save a note to so we'll say the Note file F will be equal to os. path. jooy which reminds me I also need to import the OS module and we're going to join on the data directory and then we call this notes.txt now what we're going to do is make a function and the function will simply save a note so we're going to say Define savecore note this is going to take in the note that we want to save first we're going to say if not os. path. exists the Note file then what we want to do is create this file so we're going to say open the Note file in W mode which just means we're going to create a new empty file pretty much otherwise we're going to say with open and this is going to be the Note file and we're going to open this in a mode which stands for append mode we're going to open this as the file named F then what we're going to do is say f. right lines and we're going to pass to this an array or a list inside the list we're simply going to have the note and we're going to append to this the back sln character so that we go down to the next line so it's just kind of a simpler way to do this here we're saying f. right lines so we're writing in a pen mode this just means we start at the end of the file and then this is the single note that we want to write and we want to have the back SL end so it goes down to the next character then we're just going to return from this saying something like note saved the reason why we want to do this is because the llm will actually be able to look at the return value of the function so we could see if it was like successful or if it failed or something went wrong so we just want to give it something to indicate that hey this did indeed work so that it knows that the note was actually saved now you can return any type of data you want here and again like I'm just doing a really simple python function that saves a note but we could have functions that do some complex calculus we could have a function that goes on my computer and cleans up some files like any type of code you can write the llm can call that code for you right so I'm just showing you a simple example with the note but we can really Implement anything we want which is where I think this gets quite cool so now we've got the kind of tool that we want the llm to have access to what we need to do now is wrap this and kind of create an engine that the um agent will be able to use so we're going to say note engine is equal to function tool do from defaults and we're going to specify the function so we're going to say FN is equal to save note we're going to give this a name now the information we pass here can help the model understand what this tool does so so for the name I'm going to say this is my note saer okay and then we can pass a description and for the description we'll say this tool can save a text based note to a file for the user now you can write this more detailed if you want but you should just give it like a decent description and a decent name so that it actually kind of specifies what the tool does so that the model knows how to pick between the different tools okay so let me zoom out so we can read all of this pretty straightforward we're just defining a function again you could do any type of functions you want here what we're going to do now is bring this function in and we're going to start kind of specifying a pipeline here of different tools that the uh agent has access to and then it will choose which one it needs to use so we're going to say from the note engine import the note engine I think that will work fine and now we're going to start kind of creating this collection of tools all right so first of all there's a few things we need to import so I'm going to say from llama index . tools import the query engine tool and then there's another tool we need I got to look at it over here this is the tool metadata we're then going to say from llama index Dot and I believe this is going to be agent import the react agent we're then going to say from llama index Dot and in this case it is going to be llms we're going to import the openai llm okay so that should be most of the Imports that we need what we're going to do now is we're going to specify the different tools that we have access to so we're going to say tools is equal to and we're going to create a list now the first tool that we can use is simply the note engine so we just put this inside of here the next tool is going to be the population query engine so by the way let's remove this line here because we don't want to manually be querying this and what we need to do is wrap this in the query engine tool so it kind of specifies hey this query engine here we're going to add some instructions we're going to add some description to this you'll see how it works so we're just going to say query engine tool now for the query engine tool we're going to pass the query engine which is the population query engine we're then going to say the metadata is equal to the tool metadata again the other thing that we imported here and for the tool metadata similarly to the tool we just created we will give this a name and we will give this a description uh now for the name we will call this the population under _ data and we'll say this gives information about the world population and demo Graphics okay so now we have two tools that we can use the note engine and the query engine tool again just kind of a wrapper to allow us to provide some metadata here to this tool what we'll do now is we'll set up an agent which will have access to these tools and you'll see kind of how it works and how it can query this different data so we're going to say llm is equal to open Ai and we're going to pass the model that we want to use now the model is going to be GPT uh 3.5 Das turbo and then I got to look at the one I was using 0613 I'm sure there's probably some newer versions but this is the one that I was using that was working well and now that we have the llm we're going to create an agent and the agent will have access to these tools so we're going to say the agent is the react agent and this is going to be Dot from tools and we're going to pass to this the tools so the tools are going to be the tools the llm is going to be the llm and the verbose is going to be equal to true just so we get some detailed information on the thoughts of the llm so we know what type of tool it's going to be using now the react agent from tools is just setting everything up for us so that we can pass in these individual tools and it will kind of tell the agent like you need to pick the best tool for the job it will then do that and we will be able to utilize the agent now another thing we can specify here is some context so we can say context is equal to and then we can pass any string we want and this can tell the agent beforehand what it is that it's supposed to be doing so it has some more information and well context about what it should do so we can specify a context string by going inside of prompts and we can say context is equal to and I'm just going to paste one in here again you can copy it from the link in the description all right so I pasted this in it says purpose the primary rule role of the agent is to assist users by providing accurate information about world population statistics and details about a country okay pretty straightforward but that is our context now if I go back to main.py I can import that from my prompts so we'll just go from prompts import that and then for the context we will specify the context now what we can do is set up a simple W Loop and we can just have the W Loop continually use the agent and ask it different prompts right and then the agent can utilize the tools and give us a response resp once so we're going to say wow and we're actually going to use the wallor operator here which is new in Python 3.9 so make sure you have that this is prompt callon equals to input and we're going to say enter a prompt and then Q to quit like that and I'm going to say while this is not equal to Q then we're going to go in here and we're going to use the agents we're going to say result is equal to agent. query and we're going to query the prompt and then we're going to print the result okay so quickly what this is doing is we're defining a variable prompt it's equal to whatever the user inputs if they type in Q we're simply going to quit otherwise we're going to specify the result is equal to querying the agent utilizing the prompt here and then we're going to print the result all right so we can run the code here and we can start by maybe asking it to save a note can you save a note for me saying I love Tim okay let's see if we can do that it says note saver I love Tim I can answer without using any more tools note saved successfully if we go to notes you can see it says I love Tim and then we can ask it what is the population of Vatican City let's see if it can find that for us just going to go look at the population data and I guess Vatican City is probably not in there hence why it's not able to find that for us let's say What is the population of India let's see if we can do that one and there you go it finds the population for us and it tells us in human readable form this is the population if you don't like all of this stuff being spit out you can simply go and remove ver Bose mode and then you won't see kind of all of the thoughts but I think this actually pretty interesting and it kind of proves how this is working all right so let's quit out of that by hitting q and now I want to actually start reading in that PDF data and kind of adding that to our tool set so that we can get some specific information about Canada and view how to read that unstructured data so let's make a new file here and let's call this PDF dopy now inside of here we're going to start reading in that PDF file now for the PDF file since this is unstructured we're going to read it in a little bit differently and we're going to use one of the readers that comes from llama index now you might be asking yourself okay well what's different about unstructured and structured well again with unstructured data we can't just kind of put this in a nice table right which is easy to access and easy to reference with something like Panda's code in our case what we're actually going to use is something known as a vector store index now the way the vector store index works is we're pretty much taking all of our data and we're creating something known as embeddings for it now embeddings are these kind of multidimensional objects or embeddings vectors whatever you want to call them and we can very quickly index them and query them in this database so we do that by kind of checking for similarity of intent and words it's a lot more complicated than I can really explain here in a few minutes I'd encourage you to look it up if you're more curious about how it works but in our case since we're working with this unstructured data we turn it into this Vector store index that contains all of our info and then we can go to that index with our query and we can very quickly retrieve the specific parts of this unstructured data that we're looking for to be able to answer the question hopefully that's kind of a simple enough explanation it's probably not 100% accurate but it gives you enough of an idea of what's going to be going on here here so what we're going to do inside of here is import a few things again we're going to say import OS and we're going to say from llama index import the storage context the vector store index and the load index from storage now the great thing here is that we don't need to keep recreating this index we're just going to make it one time and then once it's created we can just read from it hence why I'm importing a function like this load index from storage because it might already be created now we're also going to read or get sorry the PDF reader so I'm going to say from llama index. readers import the PDF reader now there is this kind of General reader in llama index that can read in like most types of data so it can read in text files it can read in um you know PDFs it can read in all different types of things but I want to show you how we can use a specific reader and how you can find a reader that you may want for the different type of data that you have so so what I'll do is just quickly open up this document here this web page and show you that there is something called llama Hub which contains kind of a directory of all of the different readers you can have access to so I'll link this in the description but when I found the PDF reader the way I found that was by going to llah HUB and then just typing in here PDF and then it showed me file PDF so I clicked on this and realized okay there's this PDF reader I can use and then I found that it was actually included by default in llama index so I just went ahead and used it but if we go back here right you'll see that there's all kinds of ones unstructured markdown Google Docs PowerPoint mongodb pretty much anything you want to read in it's already here so you don't need to manually write that integration now I will show you that there is something more General called the simple directory reader now this is something that just reads an entire directory and it can handle all different types of data including a PDF I didn't want to use that one for this tutorial but I just want to show you that it does exist and it can read in again like pretty much any different type of data and then for some reason it can't handle the data you have then you can go to llah HUB and you can probably find an integration that's already been built anyways back inside of our code here what we're going to do now again is get access to this Canada PDF file we're going to read it in using the PDF reader and then we're going to create that Vector store index so we're going to say the PDF path is equal to os. path. jooy and this is going to be data and canada.pdf we're now going to use the PDF reader so I'm going to say the Canada PDF is equal to the PDF reader we're going to create an instance of that and then this is going to be dot load data and we're going to load in the file equal to the PDF path now we have access uh to the loaded in PDF file and now what we're going to do is create a function that will actually get this Vector store index for us so we're going to say Define get index we're going to take in some data and name of our index and what we're going to do is we're going to start by checking if the index exists if it does uh then we can simply grab it otherwise we need to create it so we're going to say here index equals none we're then going to say if not os. path. exist the index name which is what we're going to store it under we need to create it otherwise we can load it and then eventually we can just return the index now fortunately we don't need to be math geniuses to do this we could just use llama index to actually create the index so we're just going to print building index and then we'll specify the index name then we're going to say the index is equal to the vector store index and this is going to be Dot from documents we're going to say data and then we're going to say show progress is equal to true so what this is doing is when we load in the PDF radar we're getting something known as a document we're actually going to add a bunch of different documents we're going to pass this as our data we're then going to create an index from that data so that's what we're doing here and we're just showing that progress as we do it after we create that we want to store it so we're going to say index. storage context. persist and we're going to say the persistent directory is equal to the index name and this is just going to go ahead and save the index for us in a folder then what we'll do here so if we uh already have the index we'll load it we'll say index is equal to and this is going to be load index from storage and we're going to pass the storage context. from defaults and this is going to be the persist directory at the index name now if you're wondering Tim how the heck do you know how to type all this stuff well I just found it from the Llama index documentation you guys can reference that as well but obviously this video is to help you get through this a little bit faster because this did take me you know a bit longer when I was trying to go through all the documentation and anyways we say index equals To None okay now what we're doing does the index exist if it doesn't let's go ahead and build it out and then save it if it did exist we can simply load this from Storage so we say okay well where is it it's in this location so let's load it from storage and then return the index so now we need to actually call this function so we'll say the Canada index is equal to and this will be get index and then we're going to pass the Canada PDF so this is going to have all of our documents and our data and then we can give this a name and I'm just going to call this Canada okay last thing we need to do is create an engine so we're going to say Canada engine is equal to the Canada index. asore query engine now what this does is use this Vector store index as a query engine just like we add the query engine for our population data now that we have this inside of here it's an additional tool that we can use with our agent so again we could say you know Canada engine Dot and then we can query this because we have the query interface available and it would give us some result but now we already know how that works so what we can do is we can go back to mupy and we can import it so we can say from what do we call this the PDF import and this is the Canada engine and now what we did before right with our query engine tool we can copy this paste it and we can now specify that we want to have the uh what is this the Canada engine and for the tool metadata we're going to say that this is Canada data and then we can say this gives detailed information about Canada the country okay so now we've added another tool to our tool set that the agent can use and that's it that's all we need to do that actually finishes the entire project we can now run the code and make sure all of this is working so let's run this here and we want to give it a query that um will kind of force it to use this Canada data set right so first of all it says okay building index for Canada so it's going to parse the nodes create the embeddings notice that it creates a new directory for us with Canada so notice here we've got all these different files right containing information about the vector store index they don't mean anything to me they probably don't mean anything to you but they're here and now what we can do is ask some questions so I can say something like tell me about the language is spoken in Canada and let's see what it gives us here says okay I need to use some data source I'm going to use the Canada data input languages and then it gives us this kind of big blurp here and it says in Canada the two main languages are English and French approximately 98% of Canadians can speak either or both English and French English spoken by 50% 7% of Canadians blah blah blah and then we can ask it more and more questions now we can say save a note of that I'm going to hope that this one works and for some reason it's not using the previous answer in the note um but what I can do is say you know let's ask it the same thing tell me about the languages in Canada and save a note of that okay so let's see if it works this time let's give it a sec so there you go you can see it actually ended up using both of the tools here it says I can use the Canada data tool and then use the node saer tool to save a notab kind of cool that it deduces that and we get languages and that says okay here's a note boom spits out the information here as a note and we can obviously check that in notes.txt and you see that we get that information stored as a note so I think guys with that said that's going to wrap up the coding component of this video all of this will be linked in the description in case you got lost or you want to copy any of this and it's really interesting to me what's actually possible here with agentbased AI being able to pass these different tools data sets functions to an llm and letting it read and using that is super interesting to me and that allows you as the human to have a little bit more control but to let the agent kind of you know make the decision on what tool it should be using and what it actually needs to do I already can imagine a ton of different great applications that I could build using this type of technology that wouldn't be that overly complicated because of how easy it was here to use llama index for the query engines and even for just wrapping python functions right like there's probably a 100 different python functions I could write that would be super cool that this agent can then go and execute and use and I could probably create an entire application quite a bit faster just by making all these individual tools wrapping it with this llm and saying okay agent you know go use the tool based on whatever the response is anyways I'm going to wrap it up here if you guys enjoyed make sure you leave a like you can check out all the documentation and code from the link in the description and I look forward to seeing you in another YouTube video m
