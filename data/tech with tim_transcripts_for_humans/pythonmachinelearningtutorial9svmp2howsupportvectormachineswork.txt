With timestamps:

00:00 - hey guys and welcome back John the
00:01 - machine learning tutorial with Python so
00:04 - in today's video we're gonna be talking
00:05 - about SVM and how support vector
00:07 - machines work so SVM obviously stands
00:09 - for support vector machines and these
00:12 - are gonna be used at least in our
00:13 - purposes for classification although we
00:15 - can use them for something known as
00:17 - regression which I talked about in the
00:18 - first few videos but I'm not gonna be
00:20 - explaining that here so essentially
00:22 - really basically how SVM's work is they
00:25 - attempt to create something known as a
00:28 - hyperplane and a hyperplane is something
00:31 - that can divide your data using
00:33 - hopefully something that's straight so I
00:36 - cap Lane is straight a what he called a
00:38 - line is straight so you can have like
00:40 - four-dimensional stuff so anything
00:41 - that's straight right so like a linear
00:42 - way to divide your data so essentially a
00:46 - hyperplane for the data set that I have
00:47 - up here could look something like this
00:50 - right and you can see that it's dividing
00:52 - our data points so Green is on this side
00:54 - and red is on this side right now how do
00:57 - we create this hyperplane what are like
00:59 - the parameters for it like what do we
01:00 - need to do to you to do this well the
01:03 - requirements for a hyperplane
01:04 - essentially are that you find or the
01:08 - hyperplane story is the same distance
01:10 - from the two closest points of each
01:13 - opposite class that's confusing but I'll
01:15 - draw it out and essentially any closest
01:19 - point to the line now is this red point
01:20 - and the closest point from the green
01:23 - side is actually have to draw a new one
01:24 - to make sure that this works well let's
01:26 - do one like here is this green point
01:30 - that I just drew okay so these are the
01:32 - two closest points to the line from
01:34 - either class this red point is the
01:36 - closest and this green point is the
01:38 - closest okay and the distance between
01:40 - this red point and the line and this
01:42 - green point in the line are the exact
01:44 - same so imagine if we start tilting the
01:47 - line this way then obviously this red
01:49 - points give you closer in this green
01:51 - points gonna be further away right so
01:52 - just just you can picture that okay so
01:55 - these two points are the closest points
01:56 - to the line and the distance between
01:58 - them and the line is the same that's how
02:00 - we generate a hyperplane so with that
02:03 - information we actually now know that we
02:05 - can generate an infinite amount of
02:06 - hyperplanes for this data set another
02:09 - hyperplane could look something like
02:11 - this
02:12 - we're the two closest points are this
02:13 - green point and this red point and their
02:16 - distance to the line is the exact same
02:18 - okay we can draw all kinds of different
02:20 - hyper planes but which ones are the best
02:22 - right so for example I could probably
02:24 - I'm trying to try to do one like this we
02:26 - could say that like this point and this
02:28 - point are the two closest points aligned
02:30 - and their distance is the same from the
02:32 - line and that again is another
02:33 - hyperplane now how do we pick which
02:37 - hyperplane to generate what is the best
02:39 - one what is going to give us the best
02:40 - result so essentially since I didn't
02:42 - really talk about it what anything on
02:45 - this side of the hyperplane is gonna be
02:46 - green right for when we're predicting
02:48 - and anything on this side is gonna be
02:49 - red that's essentially how we're gonna
02:50 - use this hyperplane but how do we pick
02:52 - the best one
02:53 - well this one would probably be the best
02:57 - hyperplane to for our data set and the
03:00 - reason is because the two closest points
03:03 - to the line are actually the farthest
03:06 - possible points away like the distance
03:09 - between them and the lines so let's say
03:10 - like this one and maybe let's say this
03:13 - one I know this one's a bit closer but
03:14 - just imagine the same distance is the
03:16 - largest distance that we can generate no
03:19 - matter what other points we pick or
03:21 - where we draw this line there's no way
03:22 - that we can find a distance d2 noted by
03:25 - D that is greater than this now why
03:28 - would we want this distance to be so so
03:30 - big like why would we want this distance
03:32 - to be as large as possible well I'm
03:35 - gonna just draw two lines here quickly
03:37 - let's say like this okay and the
03:41 - distance denoted by D and between these
03:44 - two points are like in between all this
03:46 - right so where there's no other points
03:48 - is known as our margin okay and we
03:51 - obviously we want to maximize this
03:53 - margin now why why is that the case why
03:55 - do we want to do that well let's think
03:57 - about it for you for a second alright so
03:59 - if I remove all this okay
04:00 - so let's erase I don't click that let's
04:02 - erase this line and let's draw another
04:04 - line and let's compare that line that I
04:07 - just use like that one to the one I'm
04:09 - about to to do okay so let's say we have
04:12 - a point that we're trying to predict its
04:14 - class and the point is right here well
04:17 - what point do you think is like a human
04:19 - this should be English we green in
04:20 - English be red I would hope most of you
04:22 - would say it's green but if we use a
04:24 - what he called a hyper
04:26 - like this it's going to classify this as
04:28 - red because it's on the left side of the
04:30 - hyperplane so why would we want it to be
04:33 - as large as possible because one's like
04:35 - this if we had that at that same point
04:37 - then somewhere around here it's going to
04:39 - be classified more correctly as green
04:40 - because the larger the distance and the
04:43 - larger the margin the more we can
04:45 - separate the two classes and do more
04:48 - accurate predictions so essentially that
04:50 - is the basis behind support vector
04:52 - machines we're trying to find something
04:54 - like a line or a plane that we can
04:56 - separate our data points by we want to
04:58 - find the largest distance between our
04:59 - what's known as support vectors which is
05:01 - going to be this red one and this green
05:03 - one and then we'll generate that
05:05 - hyperplane we'll use it to predict out
05:07 - it based on what side of the hyperplane
05:08 - song okay that's the most basic
05:11 - understanding now let's talk about the
05:13 - problems that we run into when we do
05:15 - this so right now we have data that is
05:18 - in my opinion quite nice right we look
05:21 - at this and we can tell really quickly
05:23 - that we're gonna have a line like this
05:24 - that's what it's gonna look like right
05:26 - but what if we change this up a bit and
05:28 - we go to make look at data that is not
05:31 - like this it doesn't linearly kind of
05:32 - correlate like this is really nice data
05:34 - that is not common to find especially in
05:37 - the real world when you're dealing with
05:38 - well real data and stuff that is not
05:40 - gonna look exactly like that so let's do
05:43 - some red points I'm not gonna do as many
05:45 - cuz I'm gonna do something with these in
05:46 - a second and maybe they look like this
05:48 - okay that's our red points and maybe we
05:51 - have some green points and our green
05:52 - points look something like this well if
05:56 - I asked you can you draw me a hyperplane
05:59 - for this you would say what a hyperplane
06:02 - where where does a hyperplane go does it
06:04 - go like that does it go like that does
06:06 - it go like that we honestly have no idea
06:08 - where to draw the hyperplane and even if
06:11 - we can come up with one is it gonna be
06:13 - accurate because on here we have no idea
06:15 - if it's actually gonna be red or green
06:17 - or that's gonna be a correct prediction
06:18 - we're pretty much just guessing at that
06:19 - point so what do we do to fix this
06:22 - problem well we use something called
06:24 - kernels now this is in two dimensions
06:27 - right so we have X 2 and X 1 as our
06:30 - features and then red it's green is
06:31 - denoting what the class is going to be
06:33 - ok so what we want to do is we actually
06:36 - want to turn this data into a form where
06:38 - we can draw a
06:40 - for plane or make an accurate hyperplane
06:42 - through it that divides our training
06:44 - data so what we're gonna do is we're
06:47 - actually gonna bring this whole thing up
06:48 - so this whole two-dimensional data we're
06:50 - gonna bring this into three dimensions
06:53 - and the way we do that is using
06:54 - something called a kernel now a kernel
06:57 - sounds confusing is essentially just a
07:00 - function so you can have F okay denoting
07:03 - a function that takes inputs in this
07:06 - case it's gonna have x1 and x2 and spits
07:08 - out an output which is gonna be x3 which
07:11 - is a higher dimension okay I'm gonna
07:13 - show this in and drawing in just one
07:14 - second so it's gonna take our two
07:16 - features x1 and x2 for every one of our
07:19 - data points and based on that input it's
07:22 - gonna give us an output of x3 which is
07:24 - going to represent like the third
07:26 - coordinate for our points because we're
07:27 - going to go up a dimension we're gonna
07:28 - go to three dimensions from two
07:30 - dimensions so if right now we have x1
07:32 - and x2 for every one of our points we're
07:35 - gonna be adding an x3 and then plotting
07:37 - it on a three dimensional graph so let's
07:40 - erase this and let's do just that so
07:43 - three dimensional graphs we're gonna
07:44 - have our three axes so we'll keep this
07:47 - as X 2 this is X 1 and we'll draw one
07:50 - out like here which is gonna be our X 3
07:51 - okay now I'm just gonna draw some red
07:54 - points and some green points so do some
07:58 - red ones and we'll do some green ones
08:01 - now imagine that these are the same
08:03 - points that we had in two dimensions
08:06 - right so I'll draw a small graph here
08:08 - just so you remember what it looks like
08:10 - in two dimensions right
08:11 - we had like we're at red points and
08:13 - green points kind of all just scattered
08:16 - around in the middle so I know this is
08:17 - not the same but essentially when we go
08:19 - from this to this now we look here and
08:22 - well we can draw a hyperplane look at
08:26 - this by generating that Third Point
08:28 - we've successfully been able to divide
08:31 - our green points and our red points
08:33 - because of this third dimension right so
08:35 - if you imagine now you squish this back
08:37 - to two dimensions then we're gonna come
08:39 - back to looking something like this okay
08:41 - so they're right if we remove that x3
08:44 - coordinate but now that we have this and
08:47 - we've used this kernel we can actually
08:49 - generate a hyperplane looking something
08:51 - like this that divides
08:54 - data and this works the exact same as we
08:56 - did for hyperplanes in two dimensional
08:57 - space where let's say like the distance
08:59 - here to this red point is the same as
09:01 - the distance here to this green point
09:02 - and they're the two closest points to
09:04 - our plane so I know this might be a bit
09:06 - confusing because I'm trying to explain
09:08 - a pretty complex topic like really
09:10 - simply and sometimes that even makes it
09:11 - more confusing but essentially if we
09:13 - have data that looks like this right in
09:15 - two dimensions and we can't classify it
09:17 - in two dimensions we can't draw a
09:18 - hyperplane we can add a third dimension
09:21 - to it in hopes of getting data that
09:23 - looks like this right and hopes of
09:25 - getting a graph that looks like this now
09:27 - obviously this doesn't always work right
09:29 - so when we use this function we use that
09:31 - kernel function we get X 1 and we get X
09:34 - 2 and that brings us to X 3 maybe our
09:38 - data still is is impossible to classify
09:40 - right maybe we can't do a hyperplane
09:43 - through it there's just there's no way
09:44 - and we still get points over here and
09:46 - here and they're all mixed up in that
09:48 - case we would repeat the process and add
09:51 - a fourth dimension to our data now I
09:54 - want to just talk about kernels a little
09:56 - bit more because I feel like some people
09:57 - might have still have no idea what they
09:59 - are essentially it's just a function
10:01 - that takes our features so in two
10:04 - dimensions we so here we had X 2 and we
10:07 - had X 1 and we fed that into the
10:09 - function it did something with it and it
10:11 - returned to us a third dimension and
10:13 - then with that third dimension right we
10:16 - can now plot our data and by using that
10:18 - third dimension we've spread our data
10:20 - out we went from like only being on this
10:22 - plane right in two dimensions and we've
10:24 - just brought it up and down so that we
10:27 - can divide it by a hyperplane so I hope
10:30 - that makes sense all kernel is is a
10:31 - function there's all different kinds of
10:33 - kernels um you typically don't create
10:35 - your own you just use an existing kernel
10:37 - an example of a kernel could be
10:39 - something like x1 squared plus x2
10:43 - squared this is a kernel this is
10:45 - perfectly fine and that would result in
10:47 - the value x3 right so say this is our
10:51 - point and in 2 space it's coordinates is
10:54 - like 1 & 2 okay if we want to turn this
10:57 - into a 3 coordinate or a 3d object or
11:00 - point we take x1 squared plus x2 squared
11:03 - so we get 1 plus 4 because 2 squared
11:06 - right and that
11:07 - the value 5 and now we have a czar let's
11:11 - actually let's draw it here I guess we
11:13 - have as our corner point one two and
11:15 - five and that's what we plot here and we
11:18 - hope that by applying this kernel it's
11:21 - gonna make all of our red points kind of
11:22 - maybe either shoot up and all of our
11:24 - green points go down or go to the side
11:26 - or something that we can draw an
11:27 - accurate hyperplane for so that is
11:30 - essentially how a support vector machine
11:32 - works I'm not gonna go any further than
11:35 - this because if I keep going it's just
11:36 - gonna get way more confusing and way
11:38 - more math there's a ton of stuff behind
11:40 - how we generate this and how we do this
11:42 - this is a super high-level understanding
11:45 - now you kind of know enough to be able
11:47 - to implement this and so we can kind of
11:49 - tune some parameters and whatnot but
11:51 - yeah that is essentially all there is to
11:54 - SVM there's one more thing which is like
11:57 - a soft um what do you call it a soft
12:00 - margin a hard margin which I'll cover
12:01 - really quickly but this reason I left it
12:03 - to the end is not that important
12:06 - essentially like if we have our
12:07 - two-dimensional data like this I'll do
12:09 - really quick because I don't want to
12:10 - waste any more time but let's say we
12:13 - have some pink points or purple points
12:14 - all right and we have some red points
12:18 - like this okay well we can again we can
12:22 - draw our hyperplane and right now you
12:24 - might say ok so the hyperplane is gonna
12:25 - have to look something like that but
12:27 - that might not be a great hyperplane to
12:29 - draw what we hopefully might want to
12:31 - draw instead is something that looks
12:33 - like this now you're saying well we
12:35 - can't do this hyperplane and you're
12:37 - correct right now based on what I've
12:39 - told you because well this point is what
12:43 - do you call it it's the closest point to
12:44 - the line and like we're not using that
12:47 - because we're gonna use this and this is
12:49 - our support vectors for this this
12:50 - hyperplane and this red point you're
12:52 - like well that can't be there that
12:53 - that's incorrect well you're correct
12:55 - right now but this is where we
12:56 - introduced something called a soft
12:58 - margin so if we're saying that this is
13:00 - our what we call it support vector and
13:02 - this is our support vector we'll draw
13:03 - these and we'll say that we're gonna use
13:07 - something called a soft margin and we're
13:08 - gonna allow for a few points like this
13:10 - to exist in between the margin we're
13:13 - gonna say you know what by allowing
13:14 - these kind of outlier points to exist
13:16 - we're actually going to get a better
13:17 - classifier with
13:19 - what he called hyperplane here and
13:21 - that's fine so that's another parameter
13:24 - that we can we can tweak we can give
13:26 - something called a soft margin allowing
13:27 - a few points or however many we say to
13:30 - exist inside this margin and not affect
13:33 - the hyperplane so that's another
13:35 - parameter if you're using a hard margin
13:37 - essentially all that means is what I
13:39 - taught before you can't have any points
13:41 - like this like this red this is not a
13:43 - valid hyperplane but with the soft
13:46 - margin
13:46 - this is fine so there it is that's it
13:49 - for support vector machines please feel
13:51 - free to contact me on discord ask me any
13:53 - questions leave some questions in the
13:54 - comments I try to respond to everyone if
13:56 - you guys enjoyed the video please make
13:57 - sure you leave a like and in the next
13:59 - video we're gonna be getting into
13:59 - implementing this algorithm
14:01 - [Music]

Cleaned transcript:

hey guys and welcome back John the machine learning tutorial with Python so in today's video we're gonna be talking about SVM and how support vector machines work so SVM obviously stands for support vector machines and these are gonna be used at least in our purposes for classification although we can use them for something known as regression which I talked about in the first few videos but I'm not gonna be explaining that here so essentially really basically how SVM's work is they attempt to create something known as a hyperplane and a hyperplane is something that can divide your data using hopefully something that's straight so I cap Lane is straight a what he called a line is straight so you can have like fourdimensional stuff so anything that's straight right so like a linear way to divide your data so essentially a hyperplane for the data set that I have up here could look something like this right and you can see that it's dividing our data points so Green is on this side and red is on this side right now how do we create this hyperplane what are like the parameters for it like what do we need to do to you to do this well the requirements for a hyperplane essentially are that you find or the hyperplane story is the same distance from the two closest points of each opposite class that's confusing but I'll draw it out and essentially any closest point to the line now is this red point and the closest point from the green side is actually have to draw a new one to make sure that this works well let's do one like here is this green point that I just drew okay so these are the two closest points to the line from either class this red point is the closest and this green point is the closest okay and the distance between this red point and the line and this green point in the line are the exact same so imagine if we start tilting the line this way then obviously this red points give you closer in this green points gonna be further away right so just just you can picture that okay so these two points are the closest points to the line and the distance between them and the line is the same that's how we generate a hyperplane so with that information we actually now know that we can generate an infinite amount of hyperplanes for this data set another hyperplane could look something like this we're the two closest points are this green point and this red point and their distance to the line is the exact same okay we can draw all kinds of different hyper planes but which ones are the best right so for example I could probably I'm trying to try to do one like this we could say that like this point and this point are the two closest points aligned and their distance is the same from the line and that again is another hyperplane now how do we pick which hyperplane to generate what is the best one what is going to give us the best result so essentially since I didn't really talk about it what anything on this side of the hyperplane is gonna be green right for when we're predicting and anything on this side is gonna be red that's essentially how we're gonna use this hyperplane but how do we pick the best one well this one would probably be the best hyperplane to for our data set and the reason is because the two closest points to the line are actually the farthest possible points away like the distance between them and the lines so let's say like this one and maybe let's say this one I know this one's a bit closer but just imagine the same distance is the largest distance that we can generate no matter what other points we pick or where we draw this line there's no way that we can find a distance d2 noted by D that is greater than this now why would we want this distance to be so so big like why would we want this distance to be as large as possible well I'm gonna just draw two lines here quickly let's say like this okay and the distance denoted by D and between these two points are like in between all this right so where there's no other points is known as our margin okay and we obviously we want to maximize this margin now why why is that the case why do we want to do that well let's think about it for you for a second alright so if I remove all this okay so let's erase I don't click that let's erase this line and let's draw another line and let's compare that line that I just use like that one to the one I'm about to to do okay so let's say we have a point that we're trying to predict its class and the point is right here well what point do you think is like a human this should be English we green in English be red I would hope most of you would say it's green but if we use a what he called a hyper like this it's going to classify this as red because it's on the left side of the hyperplane so why would we want it to be as large as possible because one's like this if we had that at that same point then somewhere around here it's going to be classified more correctly as green because the larger the distance and the larger the margin the more we can separate the two classes and do more accurate predictions so essentially that is the basis behind support vector machines we're trying to find something like a line or a plane that we can separate our data points by we want to find the largest distance between our what's known as support vectors which is going to be this red one and this green one and then we'll generate that hyperplane we'll use it to predict out it based on what side of the hyperplane song okay that's the most basic understanding now let's talk about the problems that we run into when we do this so right now we have data that is in my opinion quite nice right we look at this and we can tell really quickly that we're gonna have a line like this that's what it's gonna look like right but what if we change this up a bit and we go to make look at data that is not like this it doesn't linearly kind of correlate like this is really nice data that is not common to find especially in the real world when you're dealing with well real data and stuff that is not gonna look exactly like that so let's do some red points I'm not gonna do as many cuz I'm gonna do something with these in a second and maybe they look like this okay that's our red points and maybe we have some green points and our green points look something like this well if I asked you can you draw me a hyperplane for this you would say what a hyperplane where where does a hyperplane go does it go like that does it go like that does it go like that we honestly have no idea where to draw the hyperplane and even if we can come up with one is it gonna be accurate because on here we have no idea if it's actually gonna be red or green or that's gonna be a correct prediction we're pretty much just guessing at that point so what do we do to fix this problem well we use something called kernels now this is in two dimensions right so we have X 2 and X 1 as our features and then red it's green is denoting what the class is going to be ok so what we want to do is we actually want to turn this data into a form where we can draw a for plane or make an accurate hyperplane through it that divides our training data so what we're gonna do is we're actually gonna bring this whole thing up so this whole twodimensional data we're gonna bring this into three dimensions and the way we do that is using something called a kernel now a kernel sounds confusing is essentially just a function so you can have F okay denoting a function that takes inputs in this case it's gonna have x1 and x2 and spits out an output which is gonna be x3 which is a higher dimension okay I'm gonna show this in and drawing in just one second so it's gonna take our two features x1 and x2 for every one of our data points and based on that input it's gonna give us an output of x3 which is going to represent like the third coordinate for our points because we're going to go up a dimension we're gonna go to three dimensions from two dimensions so if right now we have x1 and x2 for every one of our points we're gonna be adding an x3 and then plotting it on a three dimensional graph so let's erase this and let's do just that so three dimensional graphs we're gonna have our three axes so we'll keep this as X 2 this is X 1 and we'll draw one out like here which is gonna be our X 3 okay now I'm just gonna draw some red points and some green points so do some red ones and we'll do some green ones now imagine that these are the same points that we had in two dimensions right so I'll draw a small graph here just so you remember what it looks like in two dimensions right we had like we're at red points and green points kind of all just scattered around in the middle so I know this is not the same but essentially when we go from this to this now we look here and well we can draw a hyperplane look at this by generating that Third Point we've successfully been able to divide our green points and our red points because of this third dimension right so if you imagine now you squish this back to two dimensions then we're gonna come back to looking something like this okay so they're right if we remove that x3 coordinate but now that we have this and we've used this kernel we can actually generate a hyperplane looking something like this that divides data and this works the exact same as we did for hyperplanes in two dimensional space where let's say like the distance here to this red point is the same as the distance here to this green point and they're the two closest points to our plane so I know this might be a bit confusing because I'm trying to explain a pretty complex topic like really simply and sometimes that even makes it more confusing but essentially if we have data that looks like this right in two dimensions and we can't classify it in two dimensions we can't draw a hyperplane we can add a third dimension to it in hopes of getting data that looks like this right and hopes of getting a graph that looks like this now obviously this doesn't always work right so when we use this function we use that kernel function we get X 1 and we get X 2 and that brings us to X 3 maybe our data still is is impossible to classify right maybe we can't do a hyperplane through it there's just there's no way and we still get points over here and here and they're all mixed up in that case we would repeat the process and add a fourth dimension to our data now I want to just talk about kernels a little bit more because I feel like some people might have still have no idea what they are essentially it's just a function that takes our features so in two dimensions we so here we had X 2 and we had X 1 and we fed that into the function it did something with it and it returned to us a third dimension and then with that third dimension right we can now plot our data and by using that third dimension we've spread our data out we went from like only being on this plane right in two dimensions and we've just brought it up and down so that we can divide it by a hyperplane so I hope that makes sense all kernel is is a function there's all different kinds of kernels um you typically don't create your own you just use an existing kernel an example of a kernel could be something like x1 squared plus x2 squared this is a kernel this is perfectly fine and that would result in the value x3 right so say this is our point and in 2 space it's coordinates is like 1 & 2 okay if we want to turn this into a 3 coordinate or a 3d object or point we take x1 squared plus x2 squared so we get 1 plus 4 because 2 squared right and that the value 5 and now we have a czar let's actually let's draw it here I guess we have as our corner point one two and five and that's what we plot here and we hope that by applying this kernel it's gonna make all of our red points kind of maybe either shoot up and all of our green points go down or go to the side or something that we can draw an accurate hyperplane for so that is essentially how a support vector machine works I'm not gonna go any further than this because if I keep going it's just gonna get way more confusing and way more math there's a ton of stuff behind how we generate this and how we do this this is a super highlevel understanding now you kind of know enough to be able to implement this and so we can kind of tune some parameters and whatnot but yeah that is essentially all there is to SVM there's one more thing which is like a soft um what do you call it a soft margin a hard margin which I'll cover really quickly but this reason I left it to the end is not that important essentially like if we have our twodimensional data like this I'll do really quick because I don't want to waste any more time but let's say we have some pink points or purple points all right and we have some red points like this okay well we can again we can draw our hyperplane and right now you might say ok so the hyperplane is gonna have to look something like that but that might not be a great hyperplane to draw what we hopefully might want to draw instead is something that looks like this now you're saying well we can't do this hyperplane and you're correct right now based on what I've told you because well this point is what do you call it it's the closest point to the line and like we're not using that because we're gonna use this and this is our support vectors for this this hyperplane and this red point you're like well that can't be there that that's incorrect well you're correct right now but this is where we introduced something called a soft margin so if we're saying that this is our what we call it support vector and this is our support vector we'll draw these and we'll say that we're gonna use something called a soft margin and we're gonna allow for a few points like this to exist in between the margin we're gonna say you know what by allowing these kind of outlier points to exist we're actually going to get a better classifier with what he called hyperplane here and that's fine so that's another parameter that we can we can tweak we can give something called a soft margin allowing a few points or however many we say to exist inside this margin and not affect the hyperplane so that's another parameter if you're using a hard margin essentially all that means is what I taught before you can't have any points like this like this red this is not a valid hyperplane but with the soft margin this is fine so there it is that's it for support vector machines please feel free to contact me on discord ask me any questions leave some questions in the comments I try to respond to everyone if you guys enjoyed the video please make sure you leave a like and in the next video we're gonna be getting into implementing this algorithm
