With timestamps:

00:00 - hey guys and welcome back to another
00:02 - machine learning tutorial so in today's
00:04 - video we're gonna be continuing with KN
00:06 - n so k nearest neighbors and we're gonna
00:08 - be implementing that algorithm
00:09 - throughout our code I'll be showing you
00:11 - guys some cool things we can do with it
00:13 - how we can actually check the neighbors
00:14 - between different points obviously how
00:16 - we can score it see how well we're doing
00:18 - and test and train our data now I just
00:21 - want to remind you guys in case you want
00:23 - to see any of this stuff text-based
00:25 - you can go to my website tech with Tim
00:27 - done that the link is always in the
00:28 - description down below and currently
00:30 - this page says protected just because at
00:32 - the moment that I'm recording this this
00:33 - video is not out but once these videos
00:35 - come out these pages will be unpassed
00:37 - word protected so you'll be able to
00:38 - access them and essentially obviously
00:41 - this is not the tutorial we're doing
00:42 - right now cuz I haven't yet written this
00:43 - tutorial but you guys will see that on
00:45 - the website okay uh and yeah also if you
00:49 - guys have any questions please feel free
00:50 - to go to the forum and post some stuff
00:52 - in here some people have already posted
00:53 - and I've answered them like right away
00:55 - because I get email notified so if you
00:57 - do this I'll actually probably respond
00:58 - to you excuse me faster then if you
01:01 - leave a comment on the video but again
01:03 - you can feel free to do that as well
01:04 - okay so let's go ahead and get started
01:08 - so you guys might have noticed a trend
01:10 - by now that the first video is typically
01:12 - like collecting our data talking about
01:14 - the data set the next one is then kind
01:16 - of either talking about how the model
01:19 - works and the final one is implementing
01:21 - it if you guys like this kind of process
01:24 - let me know I think this works the best
01:25 - and as we continue to go further and
01:27 - more complex we're probably gonna have
01:29 - to dedicate more videos to talking about
01:31 - how these algorithms really work but I
01:34 - hope you guys noticed by now that
01:35 - collecting the data is usually the
01:37 - hardest process right because we need to
01:39 - get our data in the correct form so
01:41 - that's just something to think about as
01:42 - we continue on with machine learning
01:44 - okay so we're gonna create our
01:45 - classifier and we've already done
01:48 - similar things too so we're just gonna
01:49 - do model equals K nearest classifier
01:51 - like this will give us some brackets
01:53 - here and these brackets actually take
01:56 - one parameter and this is the amount of
01:58 - neighbors so there's a few other ones
02:00 - that we can do in here but essentially
02:02 - remember I was talking about how many
02:03 - neighbors we want now again this is a
02:05 - hyper parameter meaning that you kind of
02:07 - tweak it as you continue to train the
02:09 - model for me I'm just gonna start with
02:11 - five I think actually
02:12 - to do like yeah after un-under squirty
02:15 - neighbors equals five but play around
02:18 - with this do 7 do 9 do 11 do one and see
02:22 - what a Curie score is you're getting
02:23 - based on this and if you guys find like
02:25 - a really good accuracy let me know what
02:27 - neighbors you you use okay so I'm not
02:29 - going to play around with it too much
02:30 - now that we've done this we're gonna do
02:32 - the exact same thing we've done before
02:33 - we're just going to do model dot fit
02:35 - with your X underscore train Y
02:38 - underscore train and again that's
02:41 - literally all we have to do to train the
02:43 - model and now we're just gonna test it
02:44 - for accuracy so we can just do model dot
02:47 - what do you call it
02:48 - score and then in here we're gonna do X
02:52 - underscore test y underscore test and
02:54 - then we can simply print our accuracy to
02:57 - the screen like we've been doing through
02:58 - the other tutorials so let's just go
03:00 - ahead and run this quickly and you see
03:03 - we get a zero point nine percent
03:04 - accuracy so that's okay but let's
03:07 - actually just see what we can get if
03:08 - we're tweaking the amount of neighbors
03:09 - and we do some stuff like that so let's
03:10 - do neighbors equals seven and ninety-one
03:13 - okay maybe let's try nine and see if we
03:17 - can increase this accuracy at all so
03:19 - we're getting 94 then off of nine
03:21 - neighbors so with this data set maybe
03:22 - more neighbors is what's gonna work well
03:24 - again you guys got to play with that I'm
03:26 - not gonna go through all of it okay so I
03:28 - want to do again a similar thing that we
03:30 - did with linear regression where I want
03:31 - to see what the data points are and what
03:34 - our prediction is and what the actual
03:36 - value is so I'm just gonna do a for loop
03:39 - and loop through the test data and print
03:41 - out that test data and then the
03:43 - prediction and what the actual value is
03:44 - so we can see how well we're really
03:46 - doing just by looking at data points so
03:49 - I'm just gonna create a list called
03:50 - names first and this is just gonna be
03:52 - the names that like our classifier
03:55 - classifies our things as right because
03:59 - what our classifier is actually doing is
04:01 - it's classifying from zero to three
04:04 - right where zero is gonna represent on
04:06 - AC C and then three is gonna represent
04:09 - AC see I have the names are very good
04:12 - sorry so I'm just gonna put these names
04:14 - here so we can actually get not just a
04:16 - number we can get the actual value okay
04:18 - so good and we have very good now this
04:22 - is just what the data set uses this name
04:23 - to feel free to change ease if you want
04:26 - but this is what I'm going to do for
04:27 - here okay so now I'm gonna create a full
04:28 - loop and I'm just gonna say four and I
04:31 - guess we're actually got have to do
04:32 - somebody call its will just do X in
04:36 - range and then the Len of X underscore
04:40 - test because we're gonna need the index
04:42 - here so what we'll do now is we'll
04:44 - simply print out pretty cool predicted
04:48 - oh yeah we'll do predicted data first I
04:52 - guess if I can smoke prick predicted
04:55 - correctly and all we'll do there is
04:58 - we're gonna do hmm I am forgetting
05:01 - something our time I need to predict the
05:03 - data first okay so let's do that
05:06 - predicted equals model dot predict and
05:11 - then in here we can just do X underscore
05:13 - test there we go so now we'll get all
05:15 - that predicted data and then instead of
05:17 - X test let's just do predicted here just
05:19 - go that's what I did before and then in
05:21 - here we'll just gonna do predicted and
05:24 - then whatever that x value is and then I
05:27 - don't know if I want to do this on the
05:28 - same line or not you know what maybe
05:29 - it'll look better on the same line
05:30 - really predicted we'll do data and the
05:33 - data is just gonna be this X test data
05:35 - right so X test X and I guess I could
05:39 - actually just do a comma that'll make
05:41 - things a bit easier okay and then we'll
05:43 - just do actual and this is just gonna be
05:46 - be y train data or the Y test data right
05:49 - and that x value so assuming I didn't
05:53 - make any mistakes which I probably did
05:55 - this should just print out all of our
05:56 - test data with the predicted value first
05:58 - the actual data and then what the actual
06:01 - value of that data is okay so let's see
06:04 - here and there we go okay so essentially
06:07 - let's go up to the top I guess we did
06:08 - actually have a lot of testing data here
06:11 - so it predicted to we had this is the
06:14 - actual data and the actual value is two
06:15 - and see if you can find a mistake okay
06:18 - so this one's a mistake too and it the
06:20 - actual value is zero right so you can go
06:22 - through and kind of look for that now I
06:24 - just realized I didn't even end up using
06:25 - this names so what I'm actually gonna do
06:27 - is I'm gonna do names surrounding this
06:31 - predicted here and essentially all this
06:32 - is gonna do a little do the same thing
06:34 - here is it's just gonna use because
06:36 - these numbers are gonna be indexes right
06:38 - they're going to be a 0 through
06:40 - so if the predicted value zero is just
06:42 - gonna print on a cc if it's one it's
06:44 - gonna print a CC and then exact same
06:45 - thing for names that's pretty
06:47 - straightforward how that works so let's
06:49 - run this and there we go so we see it
06:51 - says good and then be good and like you
06:53 - can go through and look at all of that
06:55 - okay so that's essentially it
06:58 - for like predicting and doing that no I
07:00 - just want to go through a few other
07:01 - things we can do with kN
07:02 - and some more values that we can kind of
07:04 - look at in case that's something that
07:05 - we're interested in or we want to like
07:07 - graph some data or whatnot so I'm gonna
07:09 - open up Google here sorry that's not
07:11 - what I want to have open I want to have
07:12 - this open and this is actually I want to
07:15 - show you guys this because the SK
07:16 - learned kind of documentation and
07:18 - essentially this is the documentation
07:20 - for kN
07:21 - so you can see we have Fitz get params
07:22 - neighbors predicts score if we've
07:26 - already used three of these ray we just
07:27 - fit predict and score but if we actually
07:30 - wanted to get the neighbors for each
07:32 - data point that we're predicting we can
07:34 - do that with with neighbors okay so
07:37 - essentially what this is going to return
07:39 - to us and you can kind of just look at
07:40 - it here is well it takes the x value the
07:43 - amount of neighbors and that's gonna
07:44 - return the distance to each of those
07:46 - neighbors so let's just go down and have
07:48 - a look at this documentation and you can
07:50 - see it's gonna give us two arrays if we
07:52 - have this last value true which it's
07:55 - default to be true and it's gonna give
07:56 - us the distance to each point that is
07:59 - the amount of neighbors and it's going
08:00 - to give us the index of that point
08:02 - within our data set so if we want to
08:06 - have a look at exactly what those points
08:08 - are we can index them and look at them
08:10 - so rather than just talking about this
08:12 - let's actually just copy this in and
08:14 - lets you use this so what we can do is
08:16 - underneath here I'll do it in the same
08:18 - loop actually we'll just do a model dot
08:21 - neighbors okay and then we're gonna give
08:23 - it that x value now this is gonna be
08:26 - weird how you give it that data but
08:28 - essentially you just have to put
08:29 - brackets like this and then you do what
08:32 - do you call it X underscore test and X
08:35 - now the reason we have to do this is
08:36 - because you can technically give this
08:38 - where it's actually supposed to take a
08:40 - two-dimensional array but when we give
08:42 - it this and we only want one value we
08:45 - just have to put it inside of another
08:46 - like little list thing so then it comes
08:48 - in as two dimensionals because it
08:49 - doesn't know how to look at data that's
08:51 - not two-dimensional essentially
08:53 - and I think we can actually do up the
08:55 - amount of neighbors in this case nine
08:56 - and then we'll just put true here even
08:58 - though that's not really necessary and
09:00 - if we wanted to decrease the amount of
09:01 - neighbors we're looking at we could put
09:02 - like five we put three you put one and
09:05 - it'll just give you the closer ones in
09:06 - that case right so let's actually store
09:09 - this under let's just say n and if we
09:11 - want print out this data for each point
09:14 - what we just have to print n right so
09:16 - it's gonna give us two arrays for each
09:18 - of these sets so let's just do like n
09:22 - just so we know what we're kind of
09:23 - looking at here and we'll put comm like
09:26 - that okay so let's try this now can and
09:30 - Wow okay so this outputs not very pretty
09:32 - but we are getting the output that we
09:34 - want so let's try to have a look at this
09:35 - so predicted was good the data was this
09:37 - the actual value is this and then here's
09:40 - our array okay so essentially it's
09:42 - saying that the distance between all
09:43 - these points is one so between the nine
09:46 - neighbors and the O and then we have
09:48 - some distances of 1.41 and yeah you can
09:51 - see that and then it's going to give us
09:53 - the index of all of our different
09:55 - neighbors here so you can see that the
09:57 - first neighbor was this and I had one of
09:59 - the closest values of one and they
10:00 - correspond to obviously the lengths that
10:02 - are here so if you wanted to technically
10:05 - plot this data like on matplotlib it
10:07 - wouldn't be particularly easy to do so
10:09 - but working with this data you could
10:11 - definitely get some kind of a plot going
10:14 - if you want to look at that so with that
10:17 - being said I think I'm gonna wrap it up
10:19 - here essentially I just want to show you
10:21 - guys how we can do this you guys can
10:23 - probably guess how to use the other
10:25 - classifiers by now but I really
10:27 - recommend you keep going through and
10:28 - following along with me because I'm
10:30 - gonna use more and more complex data and
10:32 - you guys already know that the data is
10:33 - kind of the hardest part of this getting
10:34 - it in the right form so understanding
10:36 - how you can do that will help you be
10:38 - able to use your own data in the future
10:39 - which is obviously the goal so if you
10:41 - guys enjoyed the video again make sure
10:42 - you leave a like and subscribe go follow
10:44 - to my Twitter for exclusive updates and
10:47 - video release dates and yeah
10:49 - [Music]

Cleaned transcript:

hey guys and welcome back to another machine learning tutorial so in today's video we're gonna be continuing with KN n so k nearest neighbors and we're gonna be implementing that algorithm throughout our code I'll be showing you guys some cool things we can do with it how we can actually check the neighbors between different points obviously how we can score it see how well we're doing and test and train our data now I just want to remind you guys in case you want to see any of this stuff textbased you can go to my website tech with Tim done that the link is always in the description down below and currently this page says protected just because at the moment that I'm recording this this video is not out but once these videos come out these pages will be unpassed word protected so you'll be able to access them and essentially obviously this is not the tutorial we're doing right now cuz I haven't yet written this tutorial but you guys will see that on the website okay uh and yeah also if you guys have any questions please feel free to go to the forum and post some stuff in here some people have already posted and I've answered them like right away because I get email notified so if you do this I'll actually probably respond to you excuse me faster then if you leave a comment on the video but again you can feel free to do that as well okay so let's go ahead and get started so you guys might have noticed a trend by now that the first video is typically like collecting our data talking about the data set the next one is then kind of either talking about how the model works and the final one is implementing it if you guys like this kind of process let me know I think this works the best and as we continue to go further and more complex we're probably gonna have to dedicate more videos to talking about how these algorithms really work but I hope you guys noticed by now that collecting the data is usually the hardest process right because we need to get our data in the correct form so that's just something to think about as we continue on with machine learning okay so we're gonna create our classifier and we've already done similar things too so we're just gonna do model equals K nearest classifier like this will give us some brackets here and these brackets actually take one parameter and this is the amount of neighbors so there's a few other ones that we can do in here but essentially remember I was talking about how many neighbors we want now again this is a hyper parameter meaning that you kind of tweak it as you continue to train the model for me I'm just gonna start with five I think actually to do like yeah after ununder squirty neighbors equals five but play around with this do 7 do 9 do 11 do one and see what a Curie score is you're getting based on this and if you guys find like a really good accuracy let me know what neighbors you you use okay so I'm not going to play around with it too much now that we've done this we're gonna do the exact same thing we've done before we're just going to do model dot fit with your X underscore train Y underscore train and again that's literally all we have to do to train the model and now we're just gonna test it for accuracy so we can just do model dot what do you call it score and then in here we're gonna do X underscore test y underscore test and then we can simply print our accuracy to the screen like we've been doing through the other tutorials so let's just go ahead and run this quickly and you see we get a zero point nine percent accuracy so that's okay but let's actually just see what we can get if we're tweaking the amount of neighbors and we do some stuff like that so let's do neighbors equals seven and ninetyone okay maybe let's try nine and see if we can increase this accuracy at all so we're getting 94 then off of nine neighbors so with this data set maybe more neighbors is what's gonna work well again you guys got to play with that I'm not gonna go through all of it okay so I want to do again a similar thing that we did with linear regression where I want to see what the data points are and what our prediction is and what the actual value is so I'm just gonna do a for loop and loop through the test data and print out that test data and then the prediction and what the actual value is so we can see how well we're really doing just by looking at data points so I'm just gonna create a list called names first and this is just gonna be the names that like our classifier classifies our things as right because what our classifier is actually doing is it's classifying from zero to three right where zero is gonna represent on AC C and then three is gonna represent AC see I have the names are very good sorry so I'm just gonna put these names here so we can actually get not just a number we can get the actual value okay so good and we have very good now this is just what the data set uses this name to feel free to change ease if you want but this is what I'm going to do for here okay so now I'm gonna create a full loop and I'm just gonna say four and I guess we're actually got have to do somebody call its will just do X in range and then the Len of X underscore test because we're gonna need the index here so what we'll do now is we'll simply print out pretty cool predicted oh yeah we'll do predicted data first I guess if I can smoke prick predicted correctly and all we'll do there is we're gonna do hmm I am forgetting something our time I need to predict the data first okay so let's do that predicted equals model dot predict and then in here we can just do X underscore test there we go so now we'll get all that predicted data and then instead of X test let's just do predicted here just go that's what I did before and then in here we'll just gonna do predicted and then whatever that x value is and then I don't know if I want to do this on the same line or not you know what maybe it'll look better on the same line really predicted we'll do data and the data is just gonna be this X test data right so X test X and I guess I could actually just do a comma that'll make things a bit easier okay and then we'll just do actual and this is just gonna be be y train data or the Y test data right and that x value so assuming I didn't make any mistakes which I probably did this should just print out all of our test data with the predicted value first the actual data and then what the actual value of that data is okay so let's see here and there we go okay so essentially let's go up to the top I guess we did actually have a lot of testing data here so it predicted to we had this is the actual data and the actual value is two and see if you can find a mistake okay so this one's a mistake too and it the actual value is zero right so you can go through and kind of look for that now I just realized I didn't even end up using this names so what I'm actually gonna do is I'm gonna do names surrounding this predicted here and essentially all this is gonna do a little do the same thing here is it's just gonna use because these numbers are gonna be indexes right they're going to be a 0 through so if the predicted value zero is just gonna print on a cc if it's one it's gonna print a CC and then exact same thing for names that's pretty straightforward how that works so let's run this and there we go so we see it says good and then be good and like you can go through and look at all of that okay so that's essentially it for like predicting and doing that no I just want to go through a few other things we can do with kN and some more values that we can kind of look at in case that's something that we're interested in or we want to like graph some data or whatnot so I'm gonna open up Google here sorry that's not what I want to have open I want to have this open and this is actually I want to show you guys this because the SK learned kind of documentation and essentially this is the documentation for kN so you can see we have Fitz get params neighbors predicts score if we've already used three of these ray we just fit predict and score but if we actually wanted to get the neighbors for each data point that we're predicting we can do that with with neighbors okay so essentially what this is going to return to us and you can kind of just look at it here is well it takes the x value the amount of neighbors and that's gonna return the distance to each of those neighbors so let's just go down and have a look at this documentation and you can see it's gonna give us two arrays if we have this last value true which it's default to be true and it's gonna give us the distance to each point that is the amount of neighbors and it's going to give us the index of that point within our data set so if we want to have a look at exactly what those points are we can index them and look at them so rather than just talking about this let's actually just copy this in and lets you use this so what we can do is underneath here I'll do it in the same loop actually we'll just do a model dot neighbors okay and then we're gonna give it that x value now this is gonna be weird how you give it that data but essentially you just have to put brackets like this and then you do what do you call it X underscore test and X now the reason we have to do this is because you can technically give this where it's actually supposed to take a twodimensional array but when we give it this and we only want one value we just have to put it inside of another like little list thing so then it comes in as two dimensionals because it doesn't know how to look at data that's not twodimensional essentially and I think we can actually do up the amount of neighbors in this case nine and then we'll just put true here even though that's not really necessary and if we wanted to decrease the amount of neighbors we're looking at we could put like five we put three you put one and it'll just give you the closer ones in that case right so let's actually store this under let's just say n and if we want print out this data for each point what we just have to print n right so it's gonna give us two arrays for each of these sets so let's just do like n just so we know what we're kind of looking at here and we'll put comm like that okay so let's try this now can and Wow okay so this outputs not very pretty but we are getting the output that we want so let's try to have a look at this so predicted was good the data was this the actual value is this and then here's our array okay so essentially it's saying that the distance between all these points is one so between the nine neighbors and the O and then we have some distances of 1.41 and yeah you can see that and then it's going to give us the index of all of our different neighbors here so you can see that the first neighbor was this and I had one of the closest values of one and they correspond to obviously the lengths that are here so if you wanted to technically plot this data like on matplotlib it wouldn't be particularly easy to do so but working with this data you could definitely get some kind of a plot going if you want to look at that so with that being said I think I'm gonna wrap it up here essentially I just want to show you guys how we can do this you guys can probably guess how to use the other classifiers by now but I really recommend you keep going through and following along with me because I'm gonna use more and more complex data and you guys already know that the data is kind of the hardest part of this getting it in the right form so understanding how you can do that will help you be able to use your own data in the future which is obviously the goal so if you guys enjoyed the video again make sure you leave a like and subscribe go follow to my Twitter for exclusive updates and video release dates and yeah
