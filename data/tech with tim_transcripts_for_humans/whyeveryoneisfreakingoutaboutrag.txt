With timestamps:

00:00 - what is Rag and what you need to know
00:01 - about it as a developer well rag stands
00:04 - for retrieval augmented generation and
00:06 - this is something that promises to
00:08 - drastically enhance the usability of
00:10 - llms we've probably seen that llms have
00:13 - quite a few issues they are very usable
00:15 - and a massive tool but if you wanted to
00:16 - implement them into your own application
00:18 - you can't be confident that the results
00:20 - that they're giving you are accurate and
00:22 - you don't know how they came up with the
00:24 - answer rag is something that helps with
00:26 - that problem and I'm going to break down
00:27 - everything you need to know about it in
00:29 - this video
00:30 - [Music]
00:33 - so let's begin with the problems with
00:34 - current llms these llms are giving you
00:37 - answers that often times are out of date
00:40 - the reason for that is they're only
00:41 - pulling data from the data that they
00:43 - were trained on whenever that date
00:45 - occurred so in the case of Chad gbt
00:47 - you've probably seen that famous reply
00:49 - where it says I don't know that answer
00:51 - because I've only been trained up to
00:52 - September 2021 or in a worst case
00:55 - scenario it gives you an answer but that
00:57 - answer is actually incorrect because
00:59 - since then then we've come to some new
01:01 - discovery or found some new information
01:03 - that makes that answer obsolete for
01:05 - example if you're asking things about
01:06 - scientific research it'll actually give
01:08 - you an answer most times but it might
01:10 - not be the most upto-date or current
01:12 - answer yet it's very convincing and if
01:14 - you don't know the correct answer you
01:16 - could be easily misled and end up using
01:19 - inaccurate information so that's the
01:21 - first major problem out ofd data next
01:24 - problem is that a lot of times you don't
01:26 - have any idea how the llm actually came
01:29 - up with the answer it doesn't give you a
01:31 - source you're kind of just blindly
01:32 - trusting that what it gave you is
01:34 - accurate and even if it is accurate a
01:36 - lot of times you'd probably like to know
01:38 - where it C that information from so you
01:40 - could go and double check it yourself so
01:41 - these are the two major issues outof
01:44 - date and no source so how do we fix that
01:47 - using retrieval augmented generation or
01:52 - rack well rag is actually quite simple
01:55 - what this does is connect an llm to a
01:58 - data store this means that that when the
02:00 - llm actually wants to come up with an
02:02 - answer rather than just using its
02:04 - training data it will actually go and
02:06 - ask a retriever to get a specific piece
02:08 - of data or some content from the data
02:10 - store it will then inject that kind of
02:12 - into the prompt or into the llm model
02:15 - and then it will use that data that it
02:17 - retrieved that likely is up to date to
02:19 - generate a reply so to give you a super
02:21 - simple example of this let's imagine I
02:23 - want to use an llm to retrieve upto-date
02:26 - scores for a football game now obviously
02:28 - there's better ways to do this but let's
02:30 - say for some reason I want to use an llm
02:32 - well what I would do is connect that to
02:34 - a realtime database that contains all of
02:36 - the NFL scores that database we're going
02:39 - to trust is always up toate and contains
02:41 - the accurate information now what will
02:43 - happen is when I have a prompt or I give
02:45 - that to chat GPT or the llm whatever it
02:47 - is it will now go to the data store it
02:50 - will retrieve the relevant information
02:52 - based on the data in my prompt it will
02:54 - kind of inject that into the llm and
02:56 - then it will use the data it just
02:58 - retrieved to answer my question so now
03:00 - we've kind of solved both problems I
03:02 - have updated information and I have a
03:05 - source for where that information is
03:06 - coming from it's that data store so if I
03:08 - want to know where I actually got my
03:10 - data or I want to go there and double
03:11 - check it I can simply go to the original
03:14 - source and now what we're doing is we're
03:16 - using the llm for its reasoning ability
03:18 - and ability to understand natural
03:20 - language not its huge memory from a
03:23 - massive data or training set this is
03:25 - really where this type of technique
03:27 - comes in rather than using an llm as a
03:29 - knowledge base you're using it as
03:31 - something that can reason something that
03:33 - can give you a natural reply and can
03:35 - understand what you're asking better
03:37 - than probably any model you could train
03:38 - on your own so it's really the interface
03:41 - for some type of application and then
03:43 - you have the actual data that gets
03:44 - injected and brought into the llm and
03:47 - you can control that data you could have
03:49 - data for I don't know your call center
03:51 - you could have data for an application
03:53 - or a stat tracking website whatever you
03:55 - want any data you want you can augment
03:57 - the llm with that and allow it to reason
04:00 - solely based on that data and that way
04:02 - you know you're getting accurate
04:04 - upto-date information and if you want to
04:05 - check that information you can go
04:07 - directly to the source now just as a
04:09 - quick note here before we dive in too
04:10 - far to really understand the benefit of
04:12 - something like rag you first need to
04:15 - understand how llms actually work and
04:17 - for most of you you're probably going to
04:18 - be interacting with something like chat
04:20 - gbt now fortunately our video sponsor
04:22 - HubSpot actually has a completely free
04:25 - resource called how to use chat gbt at
04:27 - work that breaks down exactly how chat
04:29 - gbd Works gives you expert insights and
04:32 - tells you how you can use it to its full
04:33 - ability now I put the link in the
04:35 - description so you guys can check it out
04:37 - but this resource is packed with
04:38 - knowledge and it even has 100 actionable
04:41 - prps that you can use today to really
04:43 - leverage the full power of chat GPT
04:45 - knowing how to use a tool like this
04:47 - effectively is absolutely a game Cher
04:49 - especially in the programming industry
04:51 - and again you guys can check that out
04:52 - from the link in the description a
04:54 - massive thank you to HubSpot for making
04:55 - this resource and tons of others
04:57 - completely free
05:00 - [Music]
05:01 - so to give you the highlevel
05:03 - architecture here we have a prompt this
05:05 - is the first step once you generate the
05:08 - prompt you can actually vectorize that
05:10 - and you can then go to a retriever which
05:12 - will go and find all of the relevant
05:14 - data required for the specific prompt it
05:17 - will then augment the llm with that so
05:19 - it will essentially pass that into the
05:21 - prompt the llm will give you some type
05:23 - of reply and then it can actually
05:25 - provide evidence directly from your data
05:27 - source as to why it came up with that
05:29 - answer now this also gives another
05:31 - advantage that if your data source
05:33 - doesn't have the answer the llm can tell
05:35 - you that rather than giving you a
05:37 - convincing answer that's actually wrong
05:39 - or misleading it can simply say hey
05:41 - based on what you gave me here for data
05:43 - I don't actually have the answer now you
05:45 - can decide whether that's better or
05:46 - worse but in my opinion I'd rather the
05:48 - llm tell me I don't know than tell me an
05:50 - incorrect and false statement or kind of
05:53 - false information that might mislead me
05:54 - or have some pretty bad
05:58 - repercussions
06:00 - so now let's quickly dive into some of
06:01 - the major benefits of using a technique
06:03 - like rack first of all this allows you
06:06 - to avoid retraining language models and
06:09 - instead augmenting them with up-to-date
06:11 - information we've already touched on
06:12 - this but typically if you'd want the llm
06:14 - to have upto-date data you'd have to
06:16 - retrain it on that data or if you want
06:18 - it to work specifically for your company
06:20 - information you train it on that data
06:23 - now you no longer need to do that you
06:24 - simply keep your data source up to dat
06:27 - and all of a sudden the model works
06:28 - exactly as you would expect now the next
06:30 - major benefit of course is the source
06:32 - knowing where the data actually came
06:34 - from and being able to validate that is
06:36 - massive and then lastly kind of touching
06:38 - on that being able to actually answer
06:40 - and say I don't know is a huge benefit
06:43 - so if the model doesn't know you can
06:44 - then go to the data source add the
06:46 - correct information or at least you know
06:48 - you're not getting a misleading
06:52 - answer now as well as all of these
06:54 - benefits there are a few concerns that
06:56 - you want to keep in mind first of all
06:58 - all of this only works works if the data
07:00 - that's augmented into the model is good
07:02 - and relevant now that means you need to
07:04 - come up with a quick efficient and
07:06 - accurate way for retrieving relevant
07:08 - information based on a prompt now
07:10 - there's a lot of complex stuff you can
07:12 - do here but the simplest way to do this
07:14 - would be to use something like a vector
07:15 - database that means you would vectorize
07:18 - the prompt you would then go in the
07:19 - vector database and you would find all
07:21 - of the documents that have similarity
07:23 - based on their Vector representation and
07:25 - then return those but you can't simply
07:27 - return every piece of information you
07:29 - have to be selective at what you're
07:30 - returning and this needs to happen very
07:33 - very quickly it can't introduce a ton of
07:35 - latency otherwise that's going to be a
07:36 - poor user experience now at the same
07:38 - time you want to make sure that the
07:40 - model is accurately using this augmented
07:42 - data so there's some special prompts and
07:44 - ways that you can kind of instruct the
07:45 - model to make sure it only gives you
07:48 - information based on the data that was
07:50 - augmented into it now just as a point of
07:51 - clarity here by no means am I an expert
07:54 - in rag I'm sure there might be some
07:55 - slight inaccuracies in this video but I
07:58 - wanted to share with you the the general
07:59 - idea of rag so that you guys can go look
08:02 - it up and see how you can use it in your
08:04 - applications now I actually did make a
08:06 - video that uses this type of technique
08:08 - and framework to generate a Choose Your
08:10 - Own Adventure game really interesting
08:12 - and probably the simplest example to see
08:14 - exactly how this works so if you want
08:15 - that I'll put it in the description and
08:17 - pop the video on the screen here anyways
08:19 - if you guys enjoyed make sure you leave
08:21 - a like subscribe to the channel and I
08:22 - will see you in another
08:24 - [Music]
08:28 - one

Cleaned transcript:

what is Rag and what you need to know about it as a developer well rag stands for retrieval augmented generation and this is something that promises to drastically enhance the usability of llms we've probably seen that llms have quite a few issues they are very usable and a massive tool but if you wanted to implement them into your own application you can't be confident that the results that they're giving you are accurate and you don't know how they came up with the answer rag is something that helps with that problem and I'm going to break down everything you need to know about it in this video so let's begin with the problems with current llms these llms are giving you answers that often times are out of date the reason for that is they're only pulling data from the data that they were trained on whenever that date occurred so in the case of Chad gbt you've probably seen that famous reply where it says I don't know that answer because I've only been trained up to September 2021 or in a worst case scenario it gives you an answer but that answer is actually incorrect because since then then we've come to some new discovery or found some new information that makes that answer obsolete for example if you're asking things about scientific research it'll actually give you an answer most times but it might not be the most uptodate or current answer yet it's very convincing and if you don't know the correct answer you could be easily misled and end up using inaccurate information so that's the first major problem out ofd data next problem is that a lot of times you don't have any idea how the llm actually came up with the answer it doesn't give you a source you're kind of just blindly trusting that what it gave you is accurate and even if it is accurate a lot of times you'd probably like to know where it C that information from so you could go and double check it yourself so these are the two major issues outof date and no source so how do we fix that using retrieval augmented generation or rack well rag is actually quite simple what this does is connect an llm to a data store this means that that when the llm actually wants to come up with an answer rather than just using its training data it will actually go and ask a retriever to get a specific piece of data or some content from the data store it will then inject that kind of into the prompt or into the llm model and then it will use that data that it retrieved that likely is up to date to generate a reply so to give you a super simple example of this let's imagine I want to use an llm to retrieve uptodate scores for a football game now obviously there's better ways to do this but let's say for some reason I want to use an llm well what I would do is connect that to a realtime database that contains all of the NFL scores that database we're going to trust is always up toate and contains the accurate information now what will happen is when I have a prompt or I give that to chat GPT or the llm whatever it is it will now go to the data store it will retrieve the relevant information based on the data in my prompt it will kind of inject that into the llm and then it will use the data it just retrieved to answer my question so now we've kind of solved both problems I have updated information and I have a source for where that information is coming from it's that data store so if I want to know where I actually got my data or I want to go there and double check it I can simply go to the original source and now what we're doing is we're using the llm for its reasoning ability and ability to understand natural language not its huge memory from a massive data or training set this is really where this type of technique comes in rather than using an llm as a knowledge base you're using it as something that can reason something that can give you a natural reply and can understand what you're asking better than probably any model you could train on your own so it's really the interface for some type of application and then you have the actual data that gets injected and brought into the llm and you can control that data you could have data for I don't know your call center you could have data for an application or a stat tracking website whatever you want any data you want you can augment the llm with that and allow it to reason solely based on that data and that way you know you're getting accurate uptodate information and if you want to check that information you can go directly to the source now just as a quick note here before we dive in too far to really understand the benefit of something like rag you first need to understand how llms actually work and for most of you you're probably going to be interacting with something like chat gbt now fortunately our video sponsor HubSpot actually has a completely free resource called how to use chat gbt at work that breaks down exactly how chat gbd Works gives you expert insights and tells you how you can use it to its full ability now I put the link in the description so you guys can check it out but this resource is packed with knowledge and it even has 100 actionable prps that you can use today to really leverage the full power of chat GPT knowing how to use a tool like this effectively is absolutely a game Cher especially in the programming industry and again you guys can check that out from the link in the description a massive thank you to HubSpot for making this resource and tons of others completely free so to give you the highlevel architecture here we have a prompt this is the first step once you generate the prompt you can actually vectorize that and you can then go to a retriever which will go and find all of the relevant data required for the specific prompt it will then augment the llm with that so it will essentially pass that into the prompt the llm will give you some type of reply and then it can actually provide evidence directly from your data source as to why it came up with that answer now this also gives another advantage that if your data source doesn't have the answer the llm can tell you that rather than giving you a convincing answer that's actually wrong or misleading it can simply say hey based on what you gave me here for data I don't actually have the answer now you can decide whether that's better or worse but in my opinion I'd rather the llm tell me I don't know than tell me an incorrect and false statement or kind of false information that might mislead me or have some pretty bad repercussions so now let's quickly dive into some of the major benefits of using a technique like rack first of all this allows you to avoid retraining language models and instead augmenting them with uptodate information we've already touched on this but typically if you'd want the llm to have uptodate data you'd have to retrain it on that data or if you want it to work specifically for your company information you train it on that data now you no longer need to do that you simply keep your data source up to dat and all of a sudden the model works exactly as you would expect now the next major benefit of course is the source knowing where the data actually came from and being able to validate that is massive and then lastly kind of touching on that being able to actually answer and say I don't know is a huge benefit so if the model doesn't know you can then go to the data source add the correct information or at least you know you're not getting a misleading answer now as well as all of these benefits there are a few concerns that you want to keep in mind first of all all of this only works works if the data that's augmented into the model is good and relevant now that means you need to come up with a quick efficient and accurate way for retrieving relevant information based on a prompt now there's a lot of complex stuff you can do here but the simplest way to do this would be to use something like a vector database that means you would vectorize the prompt you would then go in the vector database and you would find all of the documents that have similarity based on their Vector representation and then return those but you can't simply return every piece of information you have to be selective at what you're returning and this needs to happen very very quickly it can't introduce a ton of latency otherwise that's going to be a poor user experience now at the same time you want to make sure that the model is accurately using this augmented data so there's some special prompts and ways that you can kind of instruct the model to make sure it only gives you information based on the data that was augmented into it now just as a point of clarity here by no means am I an expert in rag I'm sure there might be some slight inaccuracies in this video but I wanted to share with you the the general idea of rag so that you guys can go look it up and see how you can use it in your applications now I actually did make a video that uses this type of technique and framework to generate a Choose Your Own Adventure game really interesting and probably the simplest example to see exactly how this works so if you want that I'll put it in the description and pop the video on the screen here anyways if you guys enjoyed make sure you leave a like subscribe to the channel and I will see you in another one
