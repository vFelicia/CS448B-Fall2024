With timestamps:

00:00 - hey guys and welcome to a brand new
00:02 - tutorial series on neural networks with
00:05 - Python and tensorflow 2.0 now tensorflow
00:08 - 2.0 is the brand-new version of
00:10 - tensorflow still actually in the alpha
00:12 - stages right now but it should be
00:14 - released within the next few weeks but
00:16 - because it's an alpha tensorflow has
00:18 - been kind enough to release us that
00:20 - alpha version so that's what we're going
00:21 - to be working with in this tutorial
00:23 - series and this will work for all future
00:26 - versions of tensorflow
00:27 - 2.0 so don't be worried about that now
00:30 - before I get too far into this first
00:32 - video I just want to quickly give you an
00:33 - overview of exactly what I'm gonna be
00:35 - doing throughout this series so you guys
00:36 - have an idea of what to expect and what
00:39 - you're going to learn now the beginning
00:41 - videos and especially this one are going
00:42 - to be dedicated to understanding how a
00:44 - neural network works and I think this is
00:47 - absolutely fundamental and you have to
00:49 - have some kind of basis on the math
00:51 - behind a neural network before you're
00:53 - really able to actually properly
00:55 - implement one now tensorflow does a
00:58 - really nice job of making it super easy
01:00 - to implement neural networks and use
01:02 - them but to actually have a successful
01:04 - and complex nail network you have to
01:06 - understand how they work on the lower
01:08 - level so that's we're gonna be doing for
01:09 - the first few videos after that what
01:12 - we'll do is we'll start designing our
01:13 - own neural networks that can solve the
01:15 - very basic M&S data sets that tensorflow
01:18 - provides to us now these are pretty
01:20 - straightforward and pretty simple but
01:22 - they give us a really good building
01:23 - block on understanding how the
01:25 - architecture of a neural network works
01:26 - what are some of the different
01:28 - activation functions how you can connect
01:30 - layers and all of that which will
01:32 - transition us nicely into creating our
01:33 - own neural networks using our own data
01:36 - for something like playing a game now
01:38 - personally I'm really interested with
01:40 - neural networks playing games and I'm
01:42 - sure a lot of you are as well and that's
01:44 - what I'm gonna be aiming to do near the
01:45 - end of the series on kind of our larger
01:47 - project I'll be designing a neural
01:49 - network and tweaking it so they can play
01:51 - a very basic game that I've personally
01:53 - designed in Python with PI game now with
01:57 - that being said that's kind of it for
01:59 - what we're gonna be doing in this series
02:00 - I may continue this on future in later
02:03 - videos and do like very specific neural
02:05 - network series maybe a chatbot or
02:06 - something like that but I need you guys
02:08 - to let me know what you'd like to see in
02:10 - the comments down below with that being
02:12 - said if you're excited about the series
02:13 - make sure you drop a like on this video
02:15 - and subscribe to the channel to be
02:16 - notified when I post the new videos and
02:19 - with that being said let's get into this
02:20 - first video on how a neural network
02:22 - works and what a neural network is so
02:25 - let's start talking about what a neural
02:27 - network is and how they work now when
02:29 - you hear neural network you usually
02:31 - think of neurons
02:32 - now neurons are what compose our brain
02:35 - and I believe don't quote me on this we
02:37 - have billions of them in our body or in
02:39 - our brain now the way that neurons work
02:41 - on a very simple and high level is you
02:44 - have a bunch of them that are connected
02:46 - in some kind of way so let's say these
02:48 - are four neurons and they're connected
02:50 - in some kind of pattern now in this case
02:53 - our pattern is completely like like
02:55 - random we're just arbitrary we're just
02:57 - picking a connection but this is the way
02:58 - that they're connected okay now neurons
03:01 - can either fire or not fire so you need
03:04 - to be on or off just like a 1 or 0 ok so
03:07 - let's say that for some reason this
03:09 - neuron decides to fire maybe you touch
03:11 - something maybe you smelt something
03:14 - something fires in your brain and this
03:16 - neuron decides to fire now it's
03:19 - connected to in this case all of the
03:20 - other neurons so what it will do is it
03:23 - will look at its other neurons and a
03:24 - connection and it will possibly cause
03:27 - it's connected neurons to fire or to not
03:29 - fire so in this case let's say maybe did
03:32 - what this one firing causes this
03:33 - connected neuron to fire this one to
03:36 - fire and maybe this one was already
03:37 - firing and now it's decided it turned it
03:40 - off or something like that ok so that's
03:42 - what happened now when this neuron fires
03:45 - well it's connected to this neuron and
03:46 - it's connected to this nerve well it's
03:48 - already got that connection but let's
03:50 - say that maybe when this one fires it
03:52 - causes this one to on fire because it
03:54 - was just fired something like that right
03:56 - and then this one now that it's off it
03:59 - causes this one to fire back up and then
04:00 - it goes it's just a chain of firing and
04:02 - unfired
04:03 - and that's just kind of how it works
04:06 - right firing and unfired now that's as
04:09 - far as I'm going to go into explaining
04:10 - neurons but this kind of gives us a
04:12 - little bit of a basis for a neural
04:13 - network now a neural network essentially
04:16 - is a connected layer of neurons or
04:19 - connected layers so multiple of neurons
04:22 - so in this case let's say that we have a
04:24 - first layer we're going to call this our
04:26 - input
04:26 - that has four gnomes and we have one
04:30 - more layer that only contains one now
04:34 - these neurons are connected now in our
04:37 - neural network we can have our
04:38 - connections happening in different ways
04:40 - we can have each what decodes neuron
04:43 - connected to each other neurons so from
04:45 - layer to layer or we can have like some
04:48 - connected to others some not connected
04:50 - so I'm connected multiple times it
04:51 - really depends on the type of neural
04:53 - network we're doing now in most cases
04:55 - what we do is we have what's called a
04:57 - fully connected neural network which
04:59 - means that each neuron in one layer is
05:03 - connected to each neuron in the next
05:05 - layer exactly one time so if I were to
05:08 - add another neuron here then what would
05:11 - happen is each of these neurons would
05:13 - also connect to this neuron one time so
05:16 - we would have a total of eight
05:18 - connections because four times two is
05:19 - eight right and that's how that would
05:21 - work now for simplicity sake we're just
05:24 - gonna use one neuron in the next layer
05:27 - just to make things a little bit easier
05:28 - to understand now all of these
05:31 - connections have what is known as a wait
05:34 - now this is in a neural network
05:36 - specifically okay so we're gonna say
05:37 - this is known as weight one this is
05:39 - known as weight to this is weight three
05:42 - and this is weight 4 and again just to
05:45 - rehab size this is known as our input
05:47 - layer because it is the first layer in
05:50 - our connected layers of neurons okay and
05:53 - going with that the last layer in our
05:56 - connected layer of neurons is known as
05:59 - our output layer now these are the only
06:02 - two layers that we really concern
06:04 - ourselves with when we look and use a
06:07 - neural network now obviously when we
06:09 - create them we have to determine what
06:10 - layers we're gonna have in the
06:12 - connection type but when we're actually
06:13 - using the neural network to make
06:15 - predictions or to Train it we are only
06:18 - concerning ourselves with the input
06:19 - layer and the output layer now what does
06:22 - this do and how do these neural networks
06:24 - work well essentially given some kind of
06:26 - input we want to do something with it
06:30 - and get some kind of output right in
06:31 - most instances that's what you want
06:33 - input results in the output in this case
06:36 - we have four inputs and we have
06:38 - output but we could have a case where we
06:41 - have four inputs and we have 25 outputs
06:43 - right it really depends on the kind of
06:45 - problem we're trying to solve so this is
06:48 - a very simple example but what I'm going
06:49 - to do is show you how we would or how a
06:52 - neural network would work to train a
06:54 - very basic snaking so let's look at a
06:59 - very basic snake game so let's say this
07:00 - is our snake okay and this is his head
07:05 - actually yeah let's say this is his head
07:08 - but like this is what the position the
07:09 - snake looks like where this is the tail
07:11 - okay we'll circle the tail now what I
07:13 - want to do is I want to train a neural
07:15 - network that will allow this snake to
07:18 - stay alive so essentially its output
07:20 - will be what direction to go in or like
07:23 - to follow a certain direction or not
07:24 - okay essentially just keep this snake a
07:26 - lot that's what I want it to do now how
07:29 - am I gonna do this well the first step
07:30 - is to decide what our input is gonna be
07:32 - and then to decide what our output is
07:34 - going to be so in this case I think a
07:36 - clever input is gonna be do we have
07:38 - something in front of the snake do we
07:40 - have something to the left of the snake
07:41 - and do we have something to the right to
07:43 - the snake because in this case all
07:45 - that's here is just the snake and he
07:46 - just needs to be able to survive so what
07:49 - we'll do is we'll say okay is there
07:50 - something to the left yes no something
07:52 - in front yes no so 0 1 something to the
07:55 - right yes no and then our last input
07:57 - will be a recommended direction for the
07:59 - snake to go in so the recommended
08:02 - direction could be anything so in this
08:04 - case maybe we'll say the recommended
08:05 - direction is left and what our output
08:07 - will be is whether or not to follow that
08:09 - recommended direction or not or to try
08:12 - to do a different recommendation
08:14 - essentially or go to a different
08:16 - direction so let's do one case on how we
08:19 - would expect this neural network to
08:21 - perform without Trant like once it's
08:23 - trained right based on some given input
08:25 - so let's say there's not something to
08:28 - the left so we're gonna put a 0 here
08:29 - because this one will represent if
08:30 - there's anything to the left the next
08:33 - one will be front so we'll say well
08:36 - there's nothing in front the next one
08:38 - will be to the right so we'll say right
08:40 - and we'll say yes there is something to
08:42 - the right of the snake and our
08:43 - recommended direction what can be
08:45 - anything we'd like so in this case we
08:47 - say the recommended direction is left
08:48 - and we'll way will do the recommend
08:50 - direction is negative
08:52 - 1:01 where negative one is left a zero
08:56 - is in front and one is to the right okay
09:00 - so we'll say in this case our
09:02 - recommended Direction is negative one
09:03 - and we'll just denote this by direction
09:07 - now our output in this instance should
09:10 - either be a zero or one representing do
09:14 - we follow the recommended direction or
09:16 - do we not so let's see in this case
09:19 - following the recommended direction
09:21 - would keep our snake alive
09:22 - so we'll say 1 yes we will follow the
09:24 - recommended direction that is acceptable
09:26 - that is fine we're gonna stay alive when
09:28 - we do that now let's see what happens
09:30 - when we change the recommended direction
09:32 - to be right so let's say that we say 1
09:35 - as a recommended direction again this is
09:37 - durn here then what should our output be
09:40 - well if we decide to go right we're
09:42 - gonna crash into our tail which means
09:45 - that we should not follow that direction
09:47 - so our output should be 0 so I hope
09:49 - you're understanding how we would expect
09:51 - this neural network to perform all right
09:54 - so now how do we actually design this
09:57 - neural network how do we get this work
09:59 - how do we train this right well that is
10:01 - a very good question and that is what
10:02 - I'm gonna talk about now so let me
10:04 - actually just erase some of this stuff
10:06 - so we have a little bit more room to
10:07 - work with some math stuff right here but
10:10 - right now what we start by doing is we
10:12 - start by designing what's known as the
10:13 - architecture of our neural network so
10:15 - we've already done this we have the
10:16 - input and we have the output now each of
10:19 - our inputs is connected to our outputs
10:21 - and each of these connections has what's
10:23 - known as a weight now another thing that
10:26 - we have is each of our input neurons has
10:28 - a value right we had in this case we
10:30 - either had 0 or we had 1 now these
10:34 - values can be different right these
10:36 - values can either be decimal values or
10:37 - they can be like between 0 and 100 they
10:40 - don't have to be just between 0 and 1
10:41 - but the point is that we have some kind
10:43 - of value right so what we're gonna do in
10:46 - this output layer to determine what way
10:48 - we should go is essentially we are going
10:50 - to take the weighted sum of the values
10:53 - multiplied by the weights I'm gonna talk
10:55 - about how this works more in depth in a
10:57 - second but just just follow me now so
10:59 - what this symbol means is take the sum
11:01 - and what we do is I'm gonna say in this
11:03 - case I which is gonna be our variable
11:05 - and
11:06 - talk about how this kind of thing works
11:07 - in a second we'll say I equals one and
11:09 - I'm going to say we'll take the weighted
11:11 - sum of in this case value I multiplied
11:15 - by weight I so what this means
11:18 - essentially is we're going to start at I
11:20 - equals one we're gonna use I as our
11:22 - variable for looping and we're gonna say
11:24 - in this case we're gonna do b1 times VI
11:27 - or sorry VI times WI and then we're
11:30 - gonna add all this so what this will
11:32 - return to us actually will be V 1 W 1
11:36 - plus V 2 W 2 plus V 3 w 3 plus V 4 w 4
11:44 - and this will be our output that's
11:49 - that's what our output layer is going to
11:50 - have as a value now this doesn't really
11:53 - make much sense right now right like why
11:56 - why we doing this weights what is this
11:58 - multiplication we'll just follow with me
12:00 - for one second so this is what our
12:02 - output layer is going to do now there's
12:04 - one thing that we have to add to this as
12:06 - well and this is what is known as our
12:09 - biases okay so what we're gonna do is
12:12 - we're going to take this weighted sum
12:13 - but we're also going to have some kind
12:16 - of bias on each of these weights okay
12:18 - and what this bias is known as it's
12:20 - denoted by C typically but essentially
12:23 - it is some value that we just
12:25 - automatically add or subtract it's a
12:27 - constant value for each of these weights
12:29 - so we're gonna say all of these these
12:31 - connections have a weight but they also
12:32 - have a bias so we're gonna have B 1 B 2
12:35 - B 3 and B 4 twice what we'll call it B
12:40 - instead of C so what I'll do here is
12:42 - what I'm also going to do is I'm also
12:44 - gonna add these biases in when I do
12:46 - these weights so we're gonna say B I as
12:48 - well so now what we'll have is we'll
12:50 - have at the end here plus bi or plus B 1
12:54 - plus B 2 plus B 3 + B 4 now again I know
12:59 - you guys like what the heck am I doing
13:00 - with this this makes no sense it's about
13:02 - to make sense in one second so now what
13:05 - we need to do is we need to train the
13:07 - network so we've understood now this is
13:09 - essentially what this output layers
13:10 - doing we're taking all of these weights
13:13 - and these values we're multiplying them
13:15 - together and we're adding them and we're
13:16 - taking what's known as the weighted sum
13:18 - okay
13:19 - but how do we like what are these values
13:21 - how do we get these values and how is
13:22 - this gonna give us a valid output well
13:25 - what we're going to do is we're gonna
13:26 - train the network on a ton of different
13:28 - information so let's say we play 1,000
13:32 - games of snake and we get all of the
13:34 - different inputs and all the different
13:36 - outputs so what we'll do is we'll
13:37 - randomly decide like a recommended
13:39 - direction and we'll just take the state
13:41 - of the snake which will be either
13:43 - there's something left to the right or
13:45 - in front of it and then we'll take the
13:46 - output which will be like did the snake
13:49 - survive or did the snake not survive so
13:52 - well what we'll do is we'll train the
13:55 - network using that information so we'll
13:57 - generate all of this different
13:58 - information and then train the network
14:00 - and what the network will do is it will
14:03 - look at all of this information and it
14:05 - will start adjusting these biases and
14:07 - these weights to properly get a correct
14:10 - output because what we'll do is we'll
14:12 - give it all this input right so let's
14:13 - say we give it the input again of zero
14:16 - one zero and maybe one like this random
14:19 - input and let's say the output for this
14:21 - case is what do you call it so one is go
14:25 - to the right the output is one which is
14:26 - correct well what the network could do
14:29 - is say okay I got that correct so what
14:30 - I'm gonna do is I'm not gonna bother
14:32 - adjusting the network because this fine
14:34 - so I don't have to change any of these
14:36 - biases I don't have to change any of
14:37 - these weights everything is working fine
14:40 - but let's say that we get the answer
14:42 - wrong so maybe the output was zero but
14:43 - the answer should have been one because
14:45 - we know the answer obviously because
14:47 - we've generated all the input and the
14:48 - output so now what the network will do
14:50 - is it will start adjusting these weights
14:52 - and adjusting these biases they'll say
14:54 - all right so I got this one wrong and
14:57 - I've gotten like five or six wrong
14:59 - before and this is what was familiar
15:01 - when I got something wrong so let's add
15:03 - one to this bias or let's multiply this
15:05 - weight by two and what it will do is
15:07 - it'll start adjusting these weights and
15:09 - these biases so that it gets more things
15:12 - correct so obviously that's why neural
15:15 - networks typically take a massive amount
15:17 - of information to Train because what you
15:19 - do is you pass it all of this
15:20 - information and then it keeps going
15:22 - through the network and at the beginning
15:24 - it sucks right because it doesn't this
15:26 - network just starts with random weights
15:28 - and random biases but as it goes through
15:31 - and it learns it says okay
15:33 - well I got this one correct so let's
15:36 - leave the weights and the bias is the
15:37 - same but let's remember that this is
15:39 - what the way in the bias was when this
15:40 - was correct and then maybe he gets
15:42 - something wrong and it says okay so
15:44 - let's adjust bias one a little bit
15:46 - let's adjust weight one let's mess with
15:48 - these and then let's try another example
15:50 - and then it says okay I got this example
15:52 - right maybe we're moving in the right
15:53 - direction maybe you will adjust another
15:55 - way maybe we'll adjust another bias and
15:56 - that's eventually your goal is that you
15:59 - get to a point where your network is
16:01 - very accurate because you've given it a
16:03 - ton of data and it's adjusted the
16:04 - weights and the biases correctly so that
16:06 - this kind of formula here of this
16:08 - weighted average will just always give
16:10 - you the correct answer or has a very
16:13 - high accuracy or high chance of giving
16:15 - you the correct answer so I hope that
16:17 - kind of makes sense I'm definitely over
16:19 - simplifying things in how the adjustment
16:22 - of these weights and these biases work
16:23 - but it's not crazy important and we're
16:25 - not going to be doing any of the
16:27 - adjustment ourselves we're we are just
16:29 - gonna be kind of tweaking a few things
16:31 - with the network so as long as you
16:32 - understand that when you feed
16:34 - information what happens is it checks
16:36 - whether the network got it correct or it
16:37 - got it incorrect and then it adjusts the
16:40 - network accordingly and that is how the
16:41 - learning process works for a neural
16:43 - network alright so now it's time to
16:45 - discuss a little bit about activation
16:48 - functions so right now what I've
16:50 - actually just described to you is a very
16:52 - advanced technique of linear regression
16:54 - so essentially I was saying we are
16:57 - adjusting weights we're adjusting biases
16:59 - and essentially we are creating a
17:00 - function that given the inputs of like X
17:03 - Y Z W or like left front right we are
17:06 - giving some kind of output but all we've
17:08 - been doing to do that essentially is
17:09 - just adjusting a linear function because
17:12 - our degree is only 1 right we have
17:15 - weights of degree one multiplying by
17:17 - values of degree one and we're adding
17:19 - some kind of bias and that kind of
17:21 - reminds you of the form MX plus B we're
17:24 - literally just adding a bunch of MX plus
17:26 - B is together which gives us like a
17:28 - fairly complex linear function but this
17:32 - is really not a great way to do things
17:35 - because it limits the degree of
17:37 - complexity that our network can actually
17:40 - have to be linear and that's not what we
17:43 - want so now we have to talk about
17:44 - activation functions
17:46 - so if you understand everything that
17:48 - I've talked about so far you're doing
17:49 - amazing this is great you understand
17:51 - that essentially the way that the
17:53 - network works is you feed information in
17:54 - and it adjusts these weights and biases
17:57 - there's a specific way it does that
17:58 - which we'll talk about later and then
18:00 - you get some kind of output and based on
18:03 - that output you're trying to adjust the
18:05 - weights and biases and and all that
18:06 - right so now what we need to do is talk
18:09 - about activation functions and what an
18:10 - activation function does is it's
18:12 - essentially a nonlinear function that
18:15 - will allow you to add a degree of
18:17 - complexity to your network so that you
18:19 - can have more of a function that's like
18:21 - this as opposed to a function that is a
18:24 - straight line so an example of an
18:26 - activation function is something like a
18:28 - sigmoid function now a sigmoid function
18:31 - what it does is it'll map any value you
18:35 - give it in between the value of negative
18:37 - 1 and 1
18:38 - so for example when we create this
18:41 - Network our output might be like the
18:44 - number 7 now this number 7 well it is
18:48 - closer to 1 that is to 0 so we might
18:50 - deem that a correct answer or we might
18:53 - say that this is actually way off
18:55 - because it's way above 1 right but what
18:57 - we want to do essentially is in our
18:59 - output layer we only want our values to
19:01 - be within a certain range we want them
19:04 - to be in this case between 0 and 1 or
19:06 - maybe we want them to be between
19:07 - negative 1 and 1 I'm saying like how
19:11 - close we are to 0 making that decision
19:13 - how close we are to 1 something like
19:14 - that right so what the sigmoid
19:16 - activation function does it's a
19:17 - nonlinear function and it takes any
19:20 - value and essentially the closer that
19:22 - value is to infinity the closer the
19:25 - output is to 1 and the closer that value
19:28 - is to negative infinity the closer that
19:29 - output is to negative 1 so what it does
19:33 - is it adds a degree of complexity to our
19:35 - network now if you don't if you're not a
19:37 - high level like math student or you only
19:39 - know like very basic high school math
19:41 - this might not really make sense to you
19:42 - but essentially the degree of something
19:44 - right is honestly how complex you can
19:46 - get if you have like a degree 9 function
19:49 - then what you could do is you can have
19:51 - some crazy kind of curve and stuff going
19:55 - on especially in multiple dimensions
19:56 - that will just make things like much
19:59 - more complex
20:00 - so for example if you have like a degree
20:02 - nine function you can have curves that
20:04 - are going like like this like all around
20:06 - here that are mapping your different
20:08 - values and if you only have a linear
20:11 - function well you can only have a
20:12 - straight line which limits your degree
20:14 - of complexity by a significant amount
20:17 - now what these activation functions also
20:20 - do is they shrink down your data so that
20:22 - it is not as large so for example right
20:24 - like say we're looking with data that is
20:26 - like hundreds of thousands of like
20:27 - characters long or digits we'd want to
20:30 - shrink that into like normalize that
20:32 - data so that it's easier to actually
20:34 - work with so let me give you a more
20:36 - practical example of how to use the
20:37 - activation function I talked about what
20:39 - sigmoid does what we would do is we
20:41 - would take this weighted sum so we did
20:43 - the sum of wi-vi plus bi right and we
20:51 - would apply an activation function to
20:53 - this so we'd say maybe our activation
20:55 - function is f of X and we would say F of
20:58 - this and this gives us some value which
21:00 - is now going to be our output neuron and
21:02 - the reason we do that again is so that
21:05 - when we are adjusting our weights and
21:07 - biases and we add that activation
21:09 - function in now we can have a way more
21:11 - complex function as opposed to just
21:14 - having the kind of linear regression
21:15 - straight line which is what we've talked
21:18 - about in my other machine learning
21:19 - courses so if this is kind of going a
21:21 - little bit over your head it may be my
21:23 - lack of explaining it I'd love to hear
21:25 - in the comments below how you think this
21:27 - explanation but essentially that's what
21:28 - the activation function does
21:30 - now another activation function that is
21:32 - very popular and is actually used way
21:34 - more than sigmoid nowadays is known as
21:36 - rectified linear unit and what this does
21:38 - is it having draw it in red actually so
21:41 - it we can see it better is it takes all
21:43 - of the values that are negative and
21:45 - automatically puts them to zero and
21:47 - takes all of the values that are
21:49 - positive and just makes them more
21:51 - positive essentially or like to some
21:54 - level positive right and what this again
21:57 - is gonna do is it's a nonlinear function
21:58 - so it's going to enhance the complexity
22:01 - of our model and just make our data
22:04 - points in between the range zero and
22:06 - positive infinity which is better than
22:07 - having between negative infinity and
22:09 - positive infinity for when were
22:10 - calculating air all right
22:14 - last thing to talk about for neural
22:15 - networks in this video I'm trying to
22:16 - kind of get everything like briefly into
22:18 - one long video is a loss function so
22:23 - this is again gonna help us understand
22:25 - how these weights and these biases are
22:27 - actually adjusted so we know that
22:29 - they're adjusted and we know that what
22:30 - we do is we look at the output and we
22:32 - compare it to what the output should be
22:35 - from our test data and then we say okay
22:38 - well it's adjust the weights and the
22:39 - biases accordingly but how do we adjust
22:41 - that and how do we know how far off we
22:44 - are how much to tune by if an adjustment
22:46 - even needs to be made well we use what's
22:49 - known as a loss function so a loss
22:52 - function essentially is a way of
22:54 - calculating error now there's a ton of
22:56 - different loss loss functions some of
22:58 - them are like mean squared error that's
23:00 - the name of one of them I think one is
23:01 - like I can't even remember the name of
23:05 - this one but there's there's a bunch of
23:06 - very popular ones if you know some leave
23:08 - them in the comments love to hear all
23:09 - the different ones but anyways what the
23:12 - loss function will do is tell you how
23:14 - wrong your answer is because like let's
23:18 - think about this right if you get an
23:19 - answer of let's say maybe our output is
23:22 - like zero point seven nine and the
23:24 - actual answer was one well that's pretty
23:27 - close
23:27 - like that's pretty close to one but
23:29 - right now all we're gonna get is the
23:30 - fact that we were zero point two one off
23:33 - okay so zero point two went off so
23:35 - adjust the weights a certain degree
23:36 - based on zero point two one but the
23:38 - thing is what if we get like zero point
23:42 - eight five
23:44 - well is this like this is significantly
23:46 - better than zero point seven nine but
23:47 - this is only gonna say that we were
23:49 - better by what is this zero point one
23:52 - five so we're still gonna do significant
23:54 - amount of justing to the weights and the
23:55 - biases so what we need to do is need to
23:57 - apply a loss function to this that will
23:59 - give us a better kind of degree of like
24:03 - how wrong or how right we were now these
24:05 - loss functions are again not linear loss
24:08 - functions which means that we're gonna
24:10 - add a higher degree of complexity to our
24:12 - model which will allow us to create way
24:15 - more complex models and neural networks
24:17 - that can solve better problems I don't
24:19 - really want to talk about loss functions
24:20 - too much because I'm definitely no
24:22 - expert on how they work but essentially
24:25 - what you do is you're comparing the
24:26 - output to the
24:28 - what the output should be so like
24:29 - whatever the model generated based what
24:31 - it should be and then you're gonna get
24:32 - some value and based on that value you
24:34 - are going to adjust the biases and the
24:36 - weights accordingly the reason we use
24:38 - the loss function again is because we
24:39 - want a higher degree of complexity
24:41 - they're nonlinear and you know if you
24:43 - get 0 if you're 99 percent like say your
24:45 - point one away from the correct answer
24:47 - we probably want to adjust the weights
24:49 - very very little but if you're like way
24:52 - off the answer you're two whole points
24:54 - maybe our answer is negative one we want
24:56 - it to be one well we want to adjust the
24:58 - model like crazy right because that
25:00 - model was horribly wrong it wasn't even
25:02 - close so we would adjust it way more
25:04 - than just like two points of adjustment
25:06 - right
25:07 - we'd adjust it based on whatever that
25:09 - loss function gave to us so anyways this
25:12 - has kind of been my explanation of a
25:14 - neural network I want a bear I want to
25:16 - stay right here for every one that I am
25:18 - no pro on neural networks this is my
25:20 - understanding there might be some stuff
25:21 - that's a little bit flawed or some areas
25:23 - that I skipped over and quickly actually
25:26 - because I know some people probably
25:28 - gonna say this when you're creating
25:29 - neural networks as well you have another
25:32 - thing that is called hidden layers so
25:34 - right now we've only been using two
25:36 - layers but in most neural networks what
25:37 - you have is a ton of different input
25:39 - neurons that connect to what's known as
25:41 - a hidden layer or multiple hidden layers
25:44 - of neurons so let's say we have like an
25:46 - architecture maybe that looks something
25:47 - like this so all these connections and
25:50 - then these ones connect to this and what
25:51 - this allows you to do is have way more
25:54 - complex models that can solve way more
25:56 - difficult problems because you can
25:58 - generate different combinations of
25:59 - inputs and he didn't what what is known
26:02 - as hidden layered neurons to solve your
26:05 - problem and have more weights and more
26:06 - biases to adjust which means you can on
26:09 - average be more accurate to produce
26:12 - certain models so you can have crazy
26:14 - neural networks that look something like
26:16 - this but with way more neurons and way
26:18 - more layers and all this kind of stuff I
26:20 - just wanted to show a very basic network
26:23 - today because I didn't want to go in and
26:25 - talk about like a ton of stuff
26:27 - especially because I know a lot of
26:28 - people that watch my videos are not pro
26:30 - math guys are just trying to get a basic
26:32 - understanding and be able to implement
26:34 - some of this stuff so anyways again that
26:36 - has been my understanding if you guys
26:37 - have any questions please leave them
26:39 - below or join my discord server and
26:41 - feel free to ask I'm always helping
26:43 - people out but that being said if you
26:44 - guys enjoyed the video please make sure
26:45 - you leave a like and subscribe and I
26:48 - will see you again in the next one
26:54 - [Music]

Cleaned transcript:

hey guys and welcome to a brand new tutorial series on neural networks with Python and tensorflow 2.0 now tensorflow 2.0 is the brandnew version of tensorflow still actually in the alpha stages right now but it should be released within the next few weeks but because it's an alpha tensorflow has been kind enough to release us that alpha version so that's what we're going to be working with in this tutorial series and this will work for all future versions of tensorflow 2.0 so don't be worried about that now before I get too far into this first video I just want to quickly give you an overview of exactly what I'm gonna be doing throughout this series so you guys have an idea of what to expect and what you're going to learn now the beginning videos and especially this one are going to be dedicated to understanding how a neural network works and I think this is absolutely fundamental and you have to have some kind of basis on the math behind a neural network before you're really able to actually properly implement one now tensorflow does a really nice job of making it super easy to implement neural networks and use them but to actually have a successful and complex nail network you have to understand how they work on the lower level so that's we're gonna be doing for the first few videos after that what we'll do is we'll start designing our own neural networks that can solve the very basic M&S data sets that tensorflow provides to us now these are pretty straightforward and pretty simple but they give us a really good building block on understanding how the architecture of a neural network works what are some of the different activation functions how you can connect layers and all of that which will transition us nicely into creating our own neural networks using our own data for something like playing a game now personally I'm really interested with neural networks playing games and I'm sure a lot of you are as well and that's what I'm gonna be aiming to do near the end of the series on kind of our larger project I'll be designing a neural network and tweaking it so they can play a very basic game that I've personally designed in Python with PI game now with that being said that's kind of it for what we're gonna be doing in this series I may continue this on future in later videos and do like very specific neural network series maybe a chatbot or something like that but I need you guys to let me know what you'd like to see in the comments down below with that being said if you're excited about the series make sure you drop a like on this video and subscribe to the channel to be notified when I post the new videos and with that being said let's get into this first video on how a neural network works and what a neural network is so let's start talking about what a neural network is and how they work now when you hear neural network you usually think of neurons now neurons are what compose our brain and I believe don't quote me on this we have billions of them in our body or in our brain now the way that neurons work on a very simple and high level is you have a bunch of them that are connected in some kind of way so let's say these are four neurons and they're connected in some kind of pattern now in this case our pattern is completely like like random we're just arbitrary we're just picking a connection but this is the way that they're connected okay now neurons can either fire or not fire so you need to be on or off just like a 1 or 0 ok so let's say that for some reason this neuron decides to fire maybe you touch something maybe you smelt something something fires in your brain and this neuron decides to fire now it's connected to in this case all of the other neurons so what it will do is it will look at its other neurons and a connection and it will possibly cause it's connected neurons to fire or to not fire so in this case let's say maybe did what this one firing causes this connected neuron to fire this one to fire and maybe this one was already firing and now it's decided it turned it off or something like that ok so that's what happened now when this neuron fires well it's connected to this neuron and it's connected to this nerve well it's already got that connection but let's say that maybe when this one fires it causes this one to on fire because it was just fired something like that right and then this one now that it's off it causes this one to fire back up and then it goes it's just a chain of firing and unfired and that's just kind of how it works right firing and unfired now that's as far as I'm going to go into explaining neurons but this kind of gives us a little bit of a basis for a neural network now a neural network essentially is a connected layer of neurons or connected layers so multiple of neurons so in this case let's say that we have a first layer we're going to call this our input that has four gnomes and we have one more layer that only contains one now these neurons are connected now in our neural network we can have our connections happening in different ways we can have each what decodes neuron connected to each other neurons so from layer to layer or we can have like some connected to others some not connected so I'm connected multiple times it really depends on the type of neural network we're doing now in most cases what we do is we have what's called a fully connected neural network which means that each neuron in one layer is connected to each neuron in the next layer exactly one time so if I were to add another neuron here then what would happen is each of these neurons would also connect to this neuron one time so we would have a total of eight connections because four times two is eight right and that's how that would work now for simplicity sake we're just gonna use one neuron in the next layer just to make things a little bit easier to understand now all of these connections have what is known as a wait now this is in a neural network specifically okay so we're gonna say this is known as weight one this is known as weight to this is weight three and this is weight 4 and again just to rehab size this is known as our input layer because it is the first layer in our connected layers of neurons okay and going with that the last layer in our connected layer of neurons is known as our output layer now these are the only two layers that we really concern ourselves with when we look and use a neural network now obviously when we create them we have to determine what layers we're gonna have in the connection type but when we're actually using the neural network to make predictions or to Train it we are only concerning ourselves with the input layer and the output layer now what does this do and how do these neural networks work well essentially given some kind of input we want to do something with it and get some kind of output right in most instances that's what you want input results in the output in this case we have four inputs and we have output but we could have a case where we have four inputs and we have 25 outputs right it really depends on the kind of problem we're trying to solve so this is a very simple example but what I'm going to do is show you how we would or how a neural network would work to train a very basic snaking so let's look at a very basic snake game so let's say this is our snake okay and this is his head actually yeah let's say this is his head but like this is what the position the snake looks like where this is the tail okay we'll circle the tail now what I want to do is I want to train a neural network that will allow this snake to stay alive so essentially its output will be what direction to go in or like to follow a certain direction or not okay essentially just keep this snake a lot that's what I want it to do now how am I gonna do this well the first step is to decide what our input is gonna be and then to decide what our output is going to be so in this case I think a clever input is gonna be do we have something in front of the snake do we have something to the left of the snake and do we have something to the right to the snake because in this case all that's here is just the snake and he just needs to be able to survive so what we'll do is we'll say okay is there something to the left yes no something in front yes no so 0 1 something to the right yes no and then our last input will be a recommended direction for the snake to go in so the recommended direction could be anything so in this case maybe we'll say the recommended direction is left and what our output will be is whether or not to follow that recommended direction or not or to try to do a different recommendation essentially or go to a different direction so let's do one case on how we would expect this neural network to perform without Trant like once it's trained right based on some given input so let's say there's not something to the left so we're gonna put a 0 here because this one will represent if there's anything to the left the next one will be front so we'll say well there's nothing in front the next one will be to the right so we'll say right and we'll say yes there is something to the right of the snake and our recommended direction what can be anything we'd like so in this case we say the recommended direction is left and we'll way will do the recommend direction is negative 101 where negative one is left a zero is in front and one is to the right okay so we'll say in this case our recommended Direction is negative one and we'll just denote this by direction now our output in this instance should either be a zero or one representing do we follow the recommended direction or do we not so let's see in this case following the recommended direction would keep our snake alive so we'll say 1 yes we will follow the recommended direction that is acceptable that is fine we're gonna stay alive when we do that now let's see what happens when we change the recommended direction to be right so let's say that we say 1 as a recommended direction again this is durn here then what should our output be well if we decide to go right we're gonna crash into our tail which means that we should not follow that direction so our output should be 0 so I hope you're understanding how we would expect this neural network to perform all right so now how do we actually design this neural network how do we get this work how do we train this right well that is a very good question and that is what I'm gonna talk about now so let me actually just erase some of this stuff so we have a little bit more room to work with some math stuff right here but right now what we start by doing is we start by designing what's known as the architecture of our neural network so we've already done this we have the input and we have the output now each of our inputs is connected to our outputs and each of these connections has what's known as a weight now another thing that we have is each of our input neurons has a value right we had in this case we either had 0 or we had 1 now these values can be different right these values can either be decimal values or they can be like between 0 and 100 they don't have to be just between 0 and 1 but the point is that we have some kind of value right so what we're gonna do in this output layer to determine what way we should go is essentially we are going to take the weighted sum of the values multiplied by the weights I'm gonna talk about how this works more in depth in a second but just just follow me now so what this symbol means is take the sum and what we do is I'm gonna say in this case I which is gonna be our variable and talk about how this kind of thing works in a second we'll say I equals one and I'm going to say we'll take the weighted sum of in this case value I multiplied by weight I so what this means essentially is we're going to start at I equals one we're gonna use I as our variable for looping and we're gonna say in this case we're gonna do b1 times VI or sorry VI times WI and then we're gonna add all this so what this will return to us actually will be V 1 W 1 plus V 2 W 2 plus V 3 w 3 plus V 4 w 4 and this will be our output that's that's what our output layer is going to have as a value now this doesn't really make much sense right now right like why why we doing this weights what is this multiplication we'll just follow with me for one second so this is what our output layer is going to do now there's one thing that we have to add to this as well and this is what is known as our biases okay so what we're gonna do is we're going to take this weighted sum but we're also going to have some kind of bias on each of these weights okay and what this bias is known as it's denoted by C typically but essentially it is some value that we just automatically add or subtract it's a constant value for each of these weights so we're gonna say all of these these connections have a weight but they also have a bias so we're gonna have B 1 B 2 B 3 and B 4 twice what we'll call it B instead of C so what I'll do here is what I'm also going to do is I'm also gonna add these biases in when I do these weights so we're gonna say B I as well so now what we'll have is we'll have at the end here plus bi or plus B 1 plus B 2 plus B 3 + B 4 now again I know you guys like what the heck am I doing with this this makes no sense it's about to make sense in one second so now what we need to do is we need to train the network so we've understood now this is essentially what this output layers doing we're taking all of these weights and these values we're multiplying them together and we're adding them and we're taking what's known as the weighted sum okay but how do we like what are these values how do we get these values and how is this gonna give us a valid output well what we're going to do is we're gonna train the network on a ton of different information so let's say we play 1,000 games of snake and we get all of the different inputs and all the different outputs so what we'll do is we'll randomly decide like a recommended direction and we'll just take the state of the snake which will be either there's something left to the right or in front of it and then we'll take the output which will be like did the snake survive or did the snake not survive so well what we'll do is we'll train the network using that information so we'll generate all of this different information and then train the network and what the network will do is it will look at all of this information and it will start adjusting these biases and these weights to properly get a correct output because what we'll do is we'll give it all this input right so let's say we give it the input again of zero one zero and maybe one like this random input and let's say the output for this case is what do you call it so one is go to the right the output is one which is correct well what the network could do is say okay I got that correct so what I'm gonna do is I'm not gonna bother adjusting the network because this fine so I don't have to change any of these biases I don't have to change any of these weights everything is working fine but let's say that we get the answer wrong so maybe the output was zero but the answer should have been one because we know the answer obviously because we've generated all the input and the output so now what the network will do is it will start adjusting these weights and adjusting these biases they'll say all right so I got this one wrong and I've gotten like five or six wrong before and this is what was familiar when I got something wrong so let's add one to this bias or let's multiply this weight by two and what it will do is it'll start adjusting these weights and these biases so that it gets more things correct so obviously that's why neural networks typically take a massive amount of information to Train because what you do is you pass it all of this information and then it keeps going through the network and at the beginning it sucks right because it doesn't this network just starts with random weights and random biases but as it goes through and it learns it says okay well I got this one correct so let's leave the weights and the bias is the same but let's remember that this is what the way in the bias was when this was correct and then maybe he gets something wrong and it says okay so let's adjust bias one a little bit let's adjust weight one let's mess with these and then let's try another example and then it says okay I got this example right maybe we're moving in the right direction maybe you will adjust another way maybe we'll adjust another bias and that's eventually your goal is that you get to a point where your network is very accurate because you've given it a ton of data and it's adjusted the weights and the biases correctly so that this kind of formula here of this weighted average will just always give you the correct answer or has a very high accuracy or high chance of giving you the correct answer so I hope that kind of makes sense I'm definitely over simplifying things in how the adjustment of these weights and these biases work but it's not crazy important and we're not going to be doing any of the adjustment ourselves we're we are just gonna be kind of tweaking a few things with the network so as long as you understand that when you feed information what happens is it checks whether the network got it correct or it got it incorrect and then it adjusts the network accordingly and that is how the learning process works for a neural network alright so now it's time to discuss a little bit about activation functions so right now what I've actually just described to you is a very advanced technique of linear regression so essentially I was saying we are adjusting weights we're adjusting biases and essentially we are creating a function that given the inputs of like X Y Z W or like left front right we are giving some kind of output but all we've been doing to do that essentially is just adjusting a linear function because our degree is only 1 right we have weights of degree one multiplying by values of degree one and we're adding some kind of bias and that kind of reminds you of the form MX plus B we're literally just adding a bunch of MX plus B is together which gives us like a fairly complex linear function but this is really not a great way to do things because it limits the degree of complexity that our network can actually have to be linear and that's not what we want so now we have to talk about activation functions so if you understand everything that I've talked about so far you're doing amazing this is great you understand that essentially the way that the network works is you feed information in and it adjusts these weights and biases there's a specific way it does that which we'll talk about later and then you get some kind of output and based on that output you're trying to adjust the weights and biases and and all that right so now what we need to do is talk about activation functions and what an activation function does is it's essentially a nonlinear function that will allow you to add a degree of complexity to your network so that you can have more of a function that's like this as opposed to a function that is a straight line so an example of an activation function is something like a sigmoid function now a sigmoid function what it does is it'll map any value you give it in between the value of negative 1 and 1 so for example when we create this Network our output might be like the number 7 now this number 7 well it is closer to 1 that is to 0 so we might deem that a correct answer or we might say that this is actually way off because it's way above 1 right but what we want to do essentially is in our output layer we only want our values to be within a certain range we want them to be in this case between 0 and 1 or maybe we want them to be between negative 1 and 1 I'm saying like how close we are to 0 making that decision how close we are to 1 something like that right so what the sigmoid activation function does it's a nonlinear function and it takes any value and essentially the closer that value is to infinity the closer the output is to 1 and the closer that value is to negative infinity the closer that output is to negative 1 so what it does is it adds a degree of complexity to our network now if you don't if you're not a high level like math student or you only know like very basic high school math this might not really make sense to you but essentially the degree of something right is honestly how complex you can get if you have like a degree 9 function then what you could do is you can have some crazy kind of curve and stuff going on especially in multiple dimensions that will just make things like much more complex so for example if you have like a degree nine function you can have curves that are going like like this like all around here that are mapping your different values and if you only have a linear function well you can only have a straight line which limits your degree of complexity by a significant amount now what these activation functions also do is they shrink down your data so that it is not as large so for example right like say we're looking with data that is like hundreds of thousands of like characters long or digits we'd want to shrink that into like normalize that data so that it's easier to actually work with so let me give you a more practical example of how to use the activation function I talked about what sigmoid does what we would do is we would take this weighted sum so we did the sum of wivi plus bi right and we would apply an activation function to this so we'd say maybe our activation function is f of X and we would say F of this and this gives us some value which is now going to be our output neuron and the reason we do that again is so that when we are adjusting our weights and biases and we add that activation function in now we can have a way more complex function as opposed to just having the kind of linear regression straight line which is what we've talked about in my other machine learning courses so if this is kind of going a little bit over your head it may be my lack of explaining it I'd love to hear in the comments below how you think this explanation but essentially that's what the activation function does now another activation function that is very popular and is actually used way more than sigmoid nowadays is known as rectified linear unit and what this does is it having draw it in red actually so it we can see it better is it takes all of the values that are negative and automatically puts them to zero and takes all of the values that are positive and just makes them more positive essentially or like to some level positive right and what this again is gonna do is it's a nonlinear function so it's going to enhance the complexity of our model and just make our data points in between the range zero and positive infinity which is better than having between negative infinity and positive infinity for when were calculating air all right last thing to talk about for neural networks in this video I'm trying to kind of get everything like briefly into one long video is a loss function so this is again gonna help us understand how these weights and these biases are actually adjusted so we know that they're adjusted and we know that what we do is we look at the output and we compare it to what the output should be from our test data and then we say okay well it's adjust the weights and the biases accordingly but how do we adjust that and how do we know how far off we are how much to tune by if an adjustment even needs to be made well we use what's known as a loss function so a loss function essentially is a way of calculating error now there's a ton of different loss loss functions some of them are like mean squared error that's the name of one of them I think one is like I can't even remember the name of this one but there's there's a bunch of very popular ones if you know some leave them in the comments love to hear all the different ones but anyways what the loss function will do is tell you how wrong your answer is because like let's think about this right if you get an answer of let's say maybe our output is like zero point seven nine and the actual answer was one well that's pretty close like that's pretty close to one but right now all we're gonna get is the fact that we were zero point two one off okay so zero point two went off so adjust the weights a certain degree based on zero point two one but the thing is what if we get like zero point eight five well is this like this is significantly better than zero point seven nine but this is only gonna say that we were better by what is this zero point one five so we're still gonna do significant amount of justing to the weights and the biases so what we need to do is need to apply a loss function to this that will give us a better kind of degree of like how wrong or how right we were now these loss functions are again not linear loss functions which means that we're gonna add a higher degree of complexity to our model which will allow us to create way more complex models and neural networks that can solve better problems I don't really want to talk about loss functions too much because I'm definitely no expert on how they work but essentially what you do is you're comparing the output to the what the output should be so like whatever the model generated based what it should be and then you're gonna get some value and based on that value you are going to adjust the biases and the weights accordingly the reason we use the loss function again is because we want a higher degree of complexity they're nonlinear and you know if you get 0 if you're 99 percent like say your point one away from the correct answer we probably want to adjust the weights very very little but if you're like way off the answer you're two whole points maybe our answer is negative one we want it to be one well we want to adjust the model like crazy right because that model was horribly wrong it wasn't even close so we would adjust it way more than just like two points of adjustment right we'd adjust it based on whatever that loss function gave to us so anyways this has kind of been my explanation of a neural network I want a bear I want to stay right here for every one that I am no pro on neural networks this is my understanding there might be some stuff that's a little bit flawed or some areas that I skipped over and quickly actually because I know some people probably gonna say this when you're creating neural networks as well you have another thing that is called hidden layers so right now we've only been using two layers but in most neural networks what you have is a ton of different input neurons that connect to what's known as a hidden layer or multiple hidden layers of neurons so let's say we have like an architecture maybe that looks something like this so all these connections and then these ones connect to this and what this allows you to do is have way more complex models that can solve way more difficult problems because you can generate different combinations of inputs and he didn't what what is known as hidden layered neurons to solve your problem and have more weights and more biases to adjust which means you can on average be more accurate to produce certain models so you can have crazy neural networks that look something like this but with way more neurons and way more layers and all this kind of stuff I just wanted to show a very basic network today because I didn't want to go in and talk about like a ton of stuff especially because I know a lot of people that watch my videos are not pro math guys are just trying to get a basic understanding and be able to implement some of this stuff so anyways again that has been my understanding if you guys have any questions please leave them below or join my discord server and feel free to ask I'm always helping people out but that being said if you guys enjoyed the video please make sure you leave a like and subscribe and I will see you again in the next one
