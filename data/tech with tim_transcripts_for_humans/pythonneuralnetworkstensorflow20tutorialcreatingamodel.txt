With timestamps:

00:00 - hey guys and welcome back to the third
00:02 - neural network tutorial now in today's
00:04 - video we're actually gonna be working
00:05 - with the neural network so we're gonna
00:07 - be setting up a model we're gonna be
00:08 - training that model we're gonna be
00:09 - testing that model to see how well it
00:11 - performed we will also use it to predict
00:13 - on individual images and all of that fun
00:16 - stuff so without further ado let's get
00:19 - started
00:19 - now the first thing that I want to do
00:21 - before we really get into actually
00:22 - writing any code is talk about the
00:24 - architecture of the neural network we're
00:26 - going to create now I always found in
00:28 - tutorials that I watch they never really
00:29 - explained exactly what the layers were
00:32 - doing what they looked like and why we
00:34 - chose such layers and that's what I'm
00:36 - hoping to give to you guys right now so
00:39 - if you remember before we know now that
00:41 - our images they come in essentially as
00:43 - like 28 by 28 pixels and the way that we
00:46 - have them is we have an array and we
00:48 - have another array inside it's like a
00:50 - two dimensional array and house pixel
00:51 - values so maybe it's like 0.1 0.3 which
00:55 - is the grayscale value and this goes and
00:57 - there's times 28 and each row of these
01:00 - these pixels now there's 28 rows
01:03 - obviously because while 28 by 28 pixels
01:06 - so in here again we have the same thing
01:08 - more pixel values and we go down 28
01:12 - times right and that's what we have and
01:14 - that's what our array looks like now
01:16 - that's what our input data is that's
01:17 - fine but this isn't really gonna work
01:19 - well for our neural network what are we
01:22 - gonna do we're gonna have one neuron and
01:23 - we're just gonna pass this whole thing
01:24 - to it I don't think so that's not gonna
01:26 - work very well so what we need to
01:28 - actually do before we can even like
01:30 - start talking about the neural network
01:31 - is figure out a way that we can change
01:34 - this information into a way that we can
01:36 - give it to the neural network so what
01:38 - I'm actually gonna do and what I mean
01:41 - most people do is they they do what's
01:43 - called flatten the data so actually
01:45 - maybe we'll go I can't even go back once
01:46 - I clear it but flattening the data
01:48 - essentially is taking any like interior
01:50 - list so let's say we have a list like
01:52 - this and just like squishing them all
01:54 - together so rather than so let's say
01:57 - this is like 1 2 3 if we were to flatten
02:00 - this what we would do is while we remove
02:03 - all of these interior arrays or lists or
02:06 - whatever it is so we would just end up
02:07 - getting data it looks like 1 2 3 and
02:10 - this
02:11 - turns out to work just fine for us so in
02:14 - this instance we only had like one
02:16 - element in each array but when we're
02:18 - dealing with 28 elements in each sorry
02:20 - list
02:21 - listen array they're interchangeable
02:22 - just in case I keep saying those what
02:25 - will essentially have is what flat in
02:27 - the data so we get a list of length 784
02:32 - and I believe that is because well I
02:34 - mean I know this house because 28 times
02:36 - 28 equals 784 so when we flatten that
02:40 - data so 28 rows of 28 pixels then we end
02:43 - up getting 784 pixels just one after
02:46 - each other and that's what we're gonna
02:47 - feed in as the input to our neural
02:49 - network so that means that our initial
02:51 - input layer is gonna look something like
02:53 - this we're gonna have a bunch of neurons
02:55 - and they're gonna go all the way down so
02:57 - we're gonna have 784 neurons so let's
03:00 - say this is 7 8 4 I know you could
03:04 - probably hardly read that but you get
03:05 - the point and this is our input layer
03:07 - now before we even talk about any kind
03:10 - of hidden layers let's talk about our
03:12 - output layer so what is our output well
03:14 - our output is gonna be a number between
03:16 - 0 & 9
03:18 - ideally that's what we want so what
03:20 - we're actually gonna do for our output
03:21 - layer is rather than just having one
03:22 - neuron that we use kind of in the last
03:24 - the two videos ago as an example is
03:26 - we're actually gonna have 10 neurons
03:28 - each one representing one of these
03:31 - different classes right so we have 0 to
03:33 - 9
03:34 - so obviously 10 neurons or 10 classes so
03:37 - let's have 10 neurons so 1 2 3 4 5 6 7 8
03:41 - 9 10 now what's gonna happen with these
03:45 - neurons is each one of them is going to
03:48 - have a value and that value is gonna
03:50 - represent how much the network thinks
03:52 - that it is each neuron so for example
03:55 - say we're classifying the image that
03:58 - looks like a t-shirt or maybe like a
04:00 - pair of pants so those are pretty easy
04:01 - to draw so let's say this is the image
04:03 - we're given a little pair of pants
04:04 - what's gonna happen is let's say pants
04:07 - is like this one like this is the one it
04:09 - actually should be all of these will be
04:11 - lit up a certain amount so essentially
04:13 - maybe we'll say like we think it's 0.05
04:16 - percent this we have like a degree of
04:19 - certainty that it's 10 percent this one
04:21 - and then it is like we think it's 75
04:24 - percent pants
04:25 - so what we'll do when we are looking at
04:27 - this output layer is essentially will
04:28 - define whatever one is the greatest so
04:31 - whatever probability is the greatest and
04:32 - then say that's the one that the network
04:34 - predicts is the class of the given
04:37 - object right so when we're training the
04:39 - network what we'll do essentially is
04:41 - we'll say okay well we're giving the
04:44 - pants so we know that this one should be
04:46 - one right this should be a hundred
04:48 - percent it should be one that's what it
04:50 - should be and all these other ones
04:51 - should be zero right because it should
04:53 - be a zero percent chance to say anything
04:55 - else because we know that it is pants
04:56 - and then the network will look at all
04:58 - this and adjust all the weights and
05:00 - biases accordingly so that we get it so
05:02 - that it lights this one up directly as
05:04 - one at least that's our goal right so
05:06 - once we do that so now we've talked with
05:09 - the input layer and the output layer now
05:11 - it's time to talk about our hidden
05:12 - layers so we could technically train a
05:15 - network that would just be two layers
05:17 - right and we just have all these inputs
05:18 - that go to some kind of outputs but that
05:20 - wouldn't really do much for us because
05:23 - essentially that would just mean we're
05:24 - just gonna look at all the pixels and
05:25 - based on that configuration of pixels
05:28 - will point to you know these output
05:30 - layers and that means we're only gonna
05:31 - have which I know it sounds only 784
05:35 - times 10 weights and biases so 784 times
05:38 - 10 which means that we're only gonna
05:39 - have 7840 weights right weights and
05:43 - biases things to address so what we're
05:46 - actually gonna do is we're gonna add a
05:47 - hidden layer inside of here now you can
05:50 - kind of arbitrarily arbitrarily pick how
05:53 - many neurons you're gonna have in your
05:55 - hidden layer it's a good idea to kind of
05:56 - go off based on percentages from your
05:58 - input layer but what we're gonna have is
06:00 - we're gonna have a hidden layer and in
06:03 - this case this hidden layer is gonna
06:04 - have a hundred and twenty eight neurons
06:06 - so we'll say this is 128 and this is
06:09 - known as our hidden layer so what will
06:12 - happen now is we're gonna have our
06:13 - inputs connecting to the hidden layer so
06:16 - fully connected and then the hidden
06:18 - layer will be connected to all of our
06:19 - output neurons which will allow for much
06:21 - more complexity of our network because
06:24 - we're gonna have a ton more biases and a
06:27 - ton more weights connecting to this
06:28 - middle layer which maybe we'll be able
06:30 - to figure out some patterns like maybe
06:32 - to look for like a straight line that
06:34 - looks like a pant sleeve it looks like
06:36 - an arm sleeve maybe they'll look for
06:38 - concentration of a certain area in the
06:40 - picture right and that's what we're
06:42 - hoping that our hidden layer will maybe
06:44 - be able to do for us maybe pick on pick
06:46 - up on some kind of patterns and then
06:48 - maybe with these combination of patterns
06:50 - we can pick out what specific image it
06:54 - actually is now we don't really know
06:56 - what the hidden network or hidden layer
06:58 - is gonna do we just kind of have some
07:00 - hopes for it and by picking 128 neurons
07:03 - we're saying okay we're going to allow
07:04 - this Hinn layer to kind of figure its
07:06 - own way out and figure out some way of
07:08 - analyzing this image and then that's
07:11 - essentially what we're gonna do so if
07:13 - you have any questions about that please
07:14 - do not hesitate to ask but the hidden
07:17 - layers are pretty arbitrary sorry I just
07:19 - dropped my pen which means that you know
07:21 - you can kind of experiment with them
07:23 - kind of tweak with them there's some
07:25 - that are known to be to do well but
07:27 - typically when you're picking a hidden
07:28 - layer you pick one and you typically go
07:31 - at like maybe 15-20 percent of the input
07:33 - size but again it really depends on the
07:35 - application that you're you're using so
07:37 - let's now actually just start working
07:39 - with our data and creating a model so if
07:43 - we want to create a model the first
07:44 - thing that we need to do is define the
07:46 - architecture or the layers for our model
07:48 - and that's what we've just done so I'm
07:50 - gonna type it out fairly quickly here
07:51 - and again you guys will see how this
07:53 - works so I'm gonna say model equals in
07:56 - this case Cara's dot sequential believe
07:59 - that's how you spell it
08:01 - and then what we're gonna do is inside
08:03 - here put a list and we're gonna start
08:04 - defining our different layers
08:06 - Syrena side care apps dot layers and our
08:10 - first layer is gonna be an input layer
08:11 - but it's gonna be a flattened input
08:13 - layer and the input underscore shape is
08:17 - gonna be equal to 28 by 28 so remember I
08:20 - talked about that initially what we need
08:21 - to do is well we need to flatten our
08:23 - data so that it is passable to all those
08:27 - different neurons right so essentially I
08:29 - got misspelled shaped correct shape
08:31 - correctly so essentially whenever you're
08:33 - passing in information that's in like a
08:34 - 2d or 3d array you need to flatten that
08:37 - information so that you're gonna be able
08:38 - to pass it to an individual neuron as
08:40 - opposed to like sending a whole list
08:42 - into one neuron right now the next layer
08:45 - that we're gonna have is going to be
08:46 - what's known as a dense layer now a
08:48 - dense layer essentially just means a
08:50 - fully connected
08:51 - which means that what we've showed so
08:53 - far which is only fully connected neural
08:55 - networks that's what we're gonna have so
08:57 - each node or each neuron is connected to
08:59 - every other neuron in the next network
09:01 - so I'm going to say layers dense and in
09:03 - this case we're gonna give it a hundred
09:04 - twenty-eight neurons that's what we've
09:06 - talked about and we're gonna set the
09:07 - activation function which we talked
09:10 - about before as well to be rectified
09:12 - linear unit now again this activation
09:15 - function is somewhat arbitrary in the
09:17 - fact that you can kick different ones
09:19 - but rectifier linear unit is a very fast
09:21 - activation function and it works well
09:23 - for a variety of applications and that
09:25 - is why we are picking that now the next
09:27 - layer is gonna be another dense layer
09:29 - which means essentially another fully
09:30 - connected layer sorry and we're gonna
09:34 - have ten neurons and this is gonna be
09:36 - our output layer and we're gonna have an
09:38 - activation of softmax
09:40 - now what softmax does is exactly what i
09:43 - explained when showing you that kind of
09:46 - architecture picture it will pick values
09:49 - for each neuron so that all of those
09:51 - values add up to one so essentially it
09:54 - is like the probability of the network
09:56 - thinking it's a certain value so it's
09:59 - like I believe that it's 80% this 2%
10:02 - this 5% this but all of the neurons
10:04 - there those values will add up to one
10:07 - and that's what the softmax phone
10:08 - softmax function does so that actually
10:11 - means that we can look at the last layer
10:13 - and we can see the probability or what
10:16 - the network thinks for each given class
10:18 - and say maybe those are two classes that
10:20 - are like 45% each we can maybe tweak the
10:23 - output of the network to say like I am
10:25 - not sure rather than predicting a
10:27 - specific value right all right so now
10:30 - what we're gonna do is we're gonna just
10:32 - set up some parameters for our model so
10:34 - I'm gonna say model dot compile and in
10:36 - this case we're gonna use an optimizer
10:38 - of atom now I'm not really gonna talk
10:40 - about the optimizer Adam is typically
10:43 - like pretty standard especially for
10:44 - something like this we're gonna use the
10:45 - loss function of sparse and in this case
10:49 - underscore katz AGG oracle believe i
10:52 - spoke that correctly and then
10:53 - cross-entropy now if you're interested
10:55 - in what these do and how they work in
10:57 - terms like the math kind of side of them
10:59 - just look them up there's their very
11:00 - famous and popular and there again are
11:04 - somewhat arbitrary
11:05 - terms are how you pick them now when I
11:07 - do metrics I'm gonna say metrics equals
11:09 - accuracy and again this is just gonna
11:10 - define what we're looking at when we're
11:13 - testing the model in this case we care
11:14 - about the accuracy or how low we can get
11:17 - this loss function to be so yeah you
11:20 - guys can look these up there's tons of
11:21 - different loss functions some of them
11:22 - have different applications and
11:23 - typically when you're making a neural
11:25 - network your mess around with different
11:27 - loss functions different optimizers and
11:29 - in some cases different metrics so now
11:32 - it is actually time to train our model
11:34 - so to train our model what we're gonna
11:36 - do is model dot fit and when we fit it
11:38 - all we're gonna do is give it our train
11:40 - images and our train labels now we're
11:44 - gonna set the amount of epochs so now
11:48 - it's time to talk about epochs now
11:50 - epochs are actually fairly
11:51 - straightforward you've probably heard of
11:52 - the word epoch before but essentially it
11:54 - means how many times the model is gonna
11:56 - see this information so what an epoch is
12:00 - gonna do is it's gonna kind of randomly
12:02 - pick images and labels obviously
12:05 - correspond to each other and it's gonna
12:07 - feed that through the neural network so
12:09 - how many epochs you decide is how many
12:11 - times you're gonna see the same image so
12:14 - the reason we do this is because the
12:17 - order in which images come in will
12:19 - influence how parameters and things are
12:21 - tweaked with the network maybe seeing
12:23 - like 10 images that are pants is gonna
12:26 - tweak it differently than if it sees
12:28 - like a few better pants and a few that
12:29 - are a shirt and some that are sandals so
12:32 - this is a very simple explanation of how
12:34 - the epochs work but essentially it just
12:36 - is giving the same images in a different
12:39 - order and then maybe if it got one image
12:41 - wrong it's gonna see it again and be
12:42 - able to tweak and it's just a way to
12:44 - increase hopefully the accuracy of our
12:47 - model that being said giving more epochs
12:49 - does not always necessarily increase the
12:51 - accuracy of your model it's something
12:53 - that you kind of have to play with and
12:54 - anyone that does any machine learning or
12:56 - neural networks will tell you that they
12:58 - can't really like they don't know the
12:59 - exact number epoch they have to play
13:01 - with it and tweak it and see what gives
13:03 - them the best accuracy so anyways now it
13:06 - is time to actually well we can run this
13:08 - but let's first get some kind of output
13:10 - here so I'm gonna actually evaluate this
13:12 - model directly after we run it so that
13:15 - we can see how it works on our test data
13:17 - so right now what this is doing
13:18 - actually just training the model on our
13:20 - training data which means we're tweaking
13:22 - all the weights and biases we're
13:24 - applying those activation functions and
13:26 - we're defining like a mean function for
13:28 - the model but if we actually want to see
13:30 - how this works we can't really just test
13:33 - it on the training images and labels for
13:35 - the same reason I talked about before so
13:37 - we have to test it on the test images
13:39 - and the test labels and essentially see
13:41 - how many it gets correct so the way we
13:43 - do this is we're gonna say test
13:46 - underscore loss test underscore AC which
13:50 - stands for accuracy equals mall dot
13:53 - evaluate is that how you spell it maybe
13:56 - and then we're gonna do test images test
13:59 - underscore labels and I believe that is
14:02 - the last parameter yes it is so now if
14:05 - we want to see the accuracy of our model
14:07 - we can simply print out test underscore
14:10 - ACC and we'll just say like tested ACC
14:16 - just so we know because there is gonna
14:17 - be some other metrics that are going to
14:18 - be printing our test when we run this
14:20 - all right so now that we've done that
14:22 - let's actually run our file and see how
14:25 - this works so this is it this whole part
14:28 - here is all we actually need to do to
14:29 - create a neural network and do a model
14:32 - now actually let me just quickly say
14:34 - that this Karis not sequential what this
14:36 - does is it means a like a sequence of
14:38 - layers so you're justifying them in
14:40 - order where you say the first layer
14:42 - obviously is gonna be your input layer
14:43 - we're flattening the data then we're
14:46 - adding to dense layers which are fully
14:47 - connected to the input layer as well and
14:50 - that's what our model looks like and
14:52 - this is typically how you go about
14:54 - creating a neural network all right so
14:56 - let's run this now and see what we get
15:00 - so this will take a second or two to run
15:03 - just because obviously there is what we
15:06 - have 60,000 images in this data set so
15:08 - you know it's got a run through them
15:09 - it's doing all the epochs and you can
15:11 - see that we're getting metrics here on
15:13 - our accuracy and our loss now our test
15:17 - accuracy was 87% so you can see that
15:19 - it's actually slightly lower than what
15:22 - do you call it like the accuracy here oh
15:23 - it's the exact same oh it actually Auto
15:25 - tested on some data sets but anyways so
15:29 - essentially that is
15:31 - how this works you can see that the
15:33 - first five epochs which are these ones
15:34 - here ran and they increase typically
15:38 - with each epoch now again we could try
15:41 - like 10 epochs 20 bucks and see what it
15:43 - does but there is a point where the more
15:45 - epochs you do the actual like the less
15:47 - reliable your model becomes and you can
15:50 - see that our accuracy was started at
15:52 - 88.9 essentially and that was on like
15:55 - that's what it said our model accuracy
15:57 - was when we were training the model but
15:59 - then once we actually tested it which of
16:01 - these two lines here it was lower than
16:03 - the be tested or like the trained
16:07 - accuracy which shows you that you
16:08 - obviously have to be testing on
16:10 - different images because when we tested
16:11 - it here it said well it was 89% but then
16:14 - here we only got 87% right so let's do a
16:16 - quick tweak here and just see what we
16:18 - get maybe if we add like 10 epochs I
16:20 - don't think this will take a crazy long
16:22 - amount of time so we'll run this and see
16:24 - maybe if it makes a massive difference
16:25 - or if it starts leveling out or it
16:27 - starts going lower or whatnot
16:29 - let me let this run here for a second
16:32 - and obviously you can see the tweaked
16:34 - accuracy as we continue to go I'm
16:36 - interested to see here if we're gonna
16:37 - increase by much or if it's just kind of
16:38 - gonna stay at the same level all right
16:42 - so we're hitting about 90% and let's see
16:45 - here
16:46 - 91 okay so we got up to 91% but you can
16:50 - see that it was kind of diminishing
16:51 - returns as soon as we ended up getting
16:53 - to about 70 parks even yeah even like
16:56 - eight epochs after this we only
16:58 - increased by a marginal amount and our
17:00 - accuracy on the testing data was
17:02 - slightly better but again for the amount
17:05 - of epochs five extra epochs it did not
17:07 - give us a five times better result right
17:09 - so it's something you got to play with
17:10 - and see so anyways that has been it for
17:13 - this video in the next video I'm gonna
17:15 - continue using this model a little bit
17:16 - to actually predict on individual images
17:18 - I know I said I was gonna do that in
17:20 - this video but it's gotten a bit longer
17:21 - so let's move that into the next video
17:23 - if you guys enjoyed please make sure you
17:24 - leave a like and subscribe and I will
17:26 - see you again there
17:28 - [Music]

Cleaned transcript:

hey guys and welcome back to the third neural network tutorial now in today's video we're actually gonna be working with the neural network so we're gonna be setting up a model we're gonna be training that model we're gonna be testing that model to see how well it performed we will also use it to predict on individual images and all of that fun stuff so without further ado let's get started now the first thing that I want to do before we really get into actually writing any code is talk about the architecture of the neural network we're going to create now I always found in tutorials that I watch they never really explained exactly what the layers were doing what they looked like and why we chose such layers and that's what I'm hoping to give to you guys right now so if you remember before we know now that our images they come in essentially as like 28 by 28 pixels and the way that we have them is we have an array and we have another array inside it's like a two dimensional array and house pixel values so maybe it's like 0.1 0.3 which is the grayscale value and this goes and there's times 28 and each row of these these pixels now there's 28 rows obviously because while 28 by 28 pixels so in here again we have the same thing more pixel values and we go down 28 times right and that's what we have and that's what our array looks like now that's what our input data is that's fine but this isn't really gonna work well for our neural network what are we gonna do we're gonna have one neuron and we're just gonna pass this whole thing to it I don't think so that's not gonna work very well so what we need to actually do before we can even like start talking about the neural network is figure out a way that we can change this information into a way that we can give it to the neural network so what I'm actually gonna do and what I mean most people do is they they do what's called flatten the data so actually maybe we'll go I can't even go back once I clear it but flattening the data essentially is taking any like interior list so let's say we have a list like this and just like squishing them all together so rather than so let's say this is like 1 2 3 if we were to flatten this what we would do is while we remove all of these interior arrays or lists or whatever it is so we would just end up getting data it looks like 1 2 3 and this turns out to work just fine for us so in this instance we only had like one element in each array but when we're dealing with 28 elements in each sorry list listen array they're interchangeable just in case I keep saying those what will essentially have is what flat in the data so we get a list of length 784 and I believe that is because well I mean I know this house because 28 times 28 equals 784 so when we flatten that data so 28 rows of 28 pixels then we end up getting 784 pixels just one after each other and that's what we're gonna feed in as the input to our neural network so that means that our initial input layer is gonna look something like this we're gonna have a bunch of neurons and they're gonna go all the way down so we're gonna have 784 neurons so let's say this is 7 8 4 I know you could probably hardly read that but you get the point and this is our input layer now before we even talk about any kind of hidden layers let's talk about our output layer so what is our output well our output is gonna be a number between 0 & 9 ideally that's what we want so what we're actually gonna do for our output layer is rather than just having one neuron that we use kind of in the last the two videos ago as an example is we're actually gonna have 10 neurons each one representing one of these different classes right so we have 0 to 9 so obviously 10 neurons or 10 classes so let's have 10 neurons so 1 2 3 4 5 6 7 8 9 10 now what's gonna happen with these neurons is each one of them is going to have a value and that value is gonna represent how much the network thinks that it is each neuron so for example say we're classifying the image that looks like a tshirt or maybe like a pair of pants so those are pretty easy to draw so let's say this is the image we're given a little pair of pants what's gonna happen is let's say pants is like this one like this is the one it actually should be all of these will be lit up a certain amount so essentially maybe we'll say like we think it's 0.05 percent this we have like a degree of certainty that it's 10 percent this one and then it is like we think it's 75 percent pants so what we'll do when we are looking at this output layer is essentially will define whatever one is the greatest so whatever probability is the greatest and then say that's the one that the network predicts is the class of the given object right so when we're training the network what we'll do essentially is we'll say okay well we're giving the pants so we know that this one should be one right this should be a hundred percent it should be one that's what it should be and all these other ones should be zero right because it should be a zero percent chance to say anything else because we know that it is pants and then the network will look at all this and adjust all the weights and biases accordingly so that we get it so that it lights this one up directly as one at least that's our goal right so once we do that so now we've talked with the input layer and the output layer now it's time to talk about our hidden layers so we could technically train a network that would just be two layers right and we just have all these inputs that go to some kind of outputs but that wouldn't really do much for us because essentially that would just mean we're just gonna look at all the pixels and based on that configuration of pixels will point to you know these output layers and that means we're only gonna have which I know it sounds only 784 times 10 weights and biases so 784 times 10 which means that we're only gonna have 7840 weights right weights and biases things to address so what we're actually gonna do is we're gonna add a hidden layer inside of here now you can kind of arbitrarily arbitrarily pick how many neurons you're gonna have in your hidden layer it's a good idea to kind of go off based on percentages from your input layer but what we're gonna have is we're gonna have a hidden layer and in this case this hidden layer is gonna have a hundred and twenty eight neurons so we'll say this is 128 and this is known as our hidden layer so what will happen now is we're gonna have our inputs connecting to the hidden layer so fully connected and then the hidden layer will be connected to all of our output neurons which will allow for much more complexity of our network because we're gonna have a ton more biases and a ton more weights connecting to this middle layer which maybe we'll be able to figure out some patterns like maybe to look for like a straight line that looks like a pant sleeve it looks like an arm sleeve maybe they'll look for concentration of a certain area in the picture right and that's what we're hoping that our hidden layer will maybe be able to do for us maybe pick on pick up on some kind of patterns and then maybe with these combination of patterns we can pick out what specific image it actually is now we don't really know what the hidden network or hidden layer is gonna do we just kind of have some hopes for it and by picking 128 neurons we're saying okay we're going to allow this Hinn layer to kind of figure its own way out and figure out some way of analyzing this image and then that's essentially what we're gonna do so if you have any questions about that please do not hesitate to ask but the hidden layers are pretty arbitrary sorry I just dropped my pen which means that you know you can kind of experiment with them kind of tweak with them there's some that are known to be to do well but typically when you're picking a hidden layer you pick one and you typically go at like maybe 1520 percent of the input size but again it really depends on the application that you're you're using so let's now actually just start working with our data and creating a model so if we want to create a model the first thing that we need to do is define the architecture or the layers for our model and that's what we've just done so I'm gonna type it out fairly quickly here and again you guys will see how this works so I'm gonna say model equals in this case Cara's dot sequential believe that's how you spell it and then what we're gonna do is inside here put a list and we're gonna start defining our different layers Syrena side care apps dot layers and our first layer is gonna be an input layer but it's gonna be a flattened input layer and the input underscore shape is gonna be equal to 28 by 28 so remember I talked about that initially what we need to do is well we need to flatten our data so that it is passable to all those different neurons right so essentially I got misspelled shaped correct shape correctly so essentially whenever you're passing in information that's in like a 2d or 3d array you need to flatten that information so that you're gonna be able to pass it to an individual neuron as opposed to like sending a whole list into one neuron right now the next layer that we're gonna have is going to be what's known as a dense layer now a dense layer essentially just means a fully connected which means that what we've showed so far which is only fully connected neural networks that's what we're gonna have so each node or each neuron is connected to every other neuron in the next network so I'm going to say layers dense and in this case we're gonna give it a hundred twentyeight neurons that's what we've talked about and we're gonna set the activation function which we talked about before as well to be rectified linear unit now again this activation function is somewhat arbitrary in the fact that you can kick different ones but rectifier linear unit is a very fast activation function and it works well for a variety of applications and that is why we are picking that now the next layer is gonna be another dense layer which means essentially another fully connected layer sorry and we're gonna have ten neurons and this is gonna be our output layer and we're gonna have an activation of softmax now what softmax does is exactly what i explained when showing you that kind of architecture picture it will pick values for each neuron so that all of those values add up to one so essentially it is like the probability of the network thinking it's a certain value so it's like I believe that it's 80% this 2% this 5% this but all of the neurons there those values will add up to one and that's what the softmax phone softmax function does so that actually means that we can look at the last layer and we can see the probability or what the network thinks for each given class and say maybe those are two classes that are like 45% each we can maybe tweak the output of the network to say like I am not sure rather than predicting a specific value right all right so now what we're gonna do is we're gonna just set up some parameters for our model so I'm gonna say model dot compile and in this case we're gonna use an optimizer of atom now I'm not really gonna talk about the optimizer Adam is typically like pretty standard especially for something like this we're gonna use the loss function of sparse and in this case underscore katz AGG oracle believe i spoke that correctly and then crossentropy now if you're interested in what these do and how they work in terms like the math kind of side of them just look them up there's their very famous and popular and there again are somewhat arbitrary terms are how you pick them now when I do metrics I'm gonna say metrics equals accuracy and again this is just gonna define what we're looking at when we're testing the model in this case we care about the accuracy or how low we can get this loss function to be so yeah you guys can look these up there's tons of different loss functions some of them have different applications and typically when you're making a neural network your mess around with different loss functions different optimizers and in some cases different metrics so now it is actually time to train our model so to train our model what we're gonna do is model dot fit and when we fit it all we're gonna do is give it our train images and our train labels now we're gonna set the amount of epochs so now it's time to talk about epochs now epochs are actually fairly straightforward you've probably heard of the word epoch before but essentially it means how many times the model is gonna see this information so what an epoch is gonna do is it's gonna kind of randomly pick images and labels obviously correspond to each other and it's gonna feed that through the neural network so how many epochs you decide is how many times you're gonna see the same image so the reason we do this is because the order in which images come in will influence how parameters and things are tweaked with the network maybe seeing like 10 images that are pants is gonna tweak it differently than if it sees like a few better pants and a few that are a shirt and some that are sandals so this is a very simple explanation of how the epochs work but essentially it just is giving the same images in a different order and then maybe if it got one image wrong it's gonna see it again and be able to tweak and it's just a way to increase hopefully the accuracy of our model that being said giving more epochs does not always necessarily increase the accuracy of your model it's something that you kind of have to play with and anyone that does any machine learning or neural networks will tell you that they can't really like they don't know the exact number epoch they have to play with it and tweak it and see what gives them the best accuracy so anyways now it is time to actually well we can run this but let's first get some kind of output here so I'm gonna actually evaluate this model directly after we run it so that we can see how it works on our test data so right now what this is doing actually just training the model on our training data which means we're tweaking all the weights and biases we're applying those activation functions and we're defining like a mean function for the model but if we actually want to see how this works we can't really just test it on the training images and labels for the same reason I talked about before so we have to test it on the test images and the test labels and essentially see how many it gets correct so the way we do this is we're gonna say test underscore loss test underscore AC which stands for accuracy equals mall dot evaluate is that how you spell it maybe and then we're gonna do test images test underscore labels and I believe that is the last parameter yes it is so now if we want to see the accuracy of our model we can simply print out test underscore ACC and we'll just say like tested ACC just so we know because there is gonna be some other metrics that are going to be printing our test when we run this all right so now that we've done that let's actually run our file and see how this works so this is it this whole part here is all we actually need to do to create a neural network and do a model now actually let me just quickly say that this Karis not sequential what this does is it means a like a sequence of layers so you're justifying them in order where you say the first layer obviously is gonna be your input layer we're flattening the data then we're adding to dense layers which are fully connected to the input layer as well and that's what our model looks like and this is typically how you go about creating a neural network all right so let's run this now and see what we get so this will take a second or two to run just because obviously there is what we have 60,000 images in this data set so you know it's got a run through them it's doing all the epochs and you can see that we're getting metrics here on our accuracy and our loss now our test accuracy was 87% so you can see that it's actually slightly lower than what do you call it like the accuracy here oh it's the exact same oh it actually Auto tested on some data sets but anyways so essentially that is how this works you can see that the first five epochs which are these ones here ran and they increase typically with each epoch now again we could try like 10 epochs 20 bucks and see what it does but there is a point where the more epochs you do the actual like the less reliable your model becomes and you can see that our accuracy was started at 88.9 essentially and that was on like that's what it said our model accuracy was when we were training the model but then once we actually tested it which of these two lines here it was lower than the be tested or like the trained accuracy which shows you that you obviously have to be testing on different images because when we tested it here it said well it was 89% but then here we only got 87% right so let's do a quick tweak here and just see what we get maybe if we add like 10 epochs I don't think this will take a crazy long amount of time so we'll run this and see maybe if it makes a massive difference or if it starts leveling out or it starts going lower or whatnot let me let this run here for a second and obviously you can see the tweaked accuracy as we continue to go I'm interested to see here if we're gonna increase by much or if it's just kind of gonna stay at the same level all right so we're hitting about 90% and let's see here 91 okay so we got up to 91% but you can see that it was kind of diminishing returns as soon as we ended up getting to about 70 parks even yeah even like eight epochs after this we only increased by a marginal amount and our accuracy on the testing data was slightly better but again for the amount of epochs five extra epochs it did not give us a five times better result right so it's something you got to play with and see so anyways that has been it for this video in the next video I'm gonna continue using this model a little bit to actually predict on individual images I know I said I was gonna do that in this video but it's gotten a bit longer so let's move that into the next video if you guys enjoyed please make sure you leave a like and subscribe and I will see you again there
