With timestamps:

00:00 - hey guys and welcome back so in today's
00:02 - video we're gonna be doing is talking
00:04 - about saving and loading our models and
00:06 - then we're gonna be doing a prediction
00:08 - on some data that doesn't come from this
00:10 - actual data set now I know this might
00:12 - seem kind of trivial we already know how
00:14 - to do predictions but trust me when I
00:15 - tell you this is a lot harder than it
00:17 - looks because if we're just taking in
00:19 - string data that means we have to
00:20 - actually do the encoding all of the
00:23 - pre-processing removing certain
00:24 - characters making sure that that data
00:27 - looks the same as the data that our
00:29 - neural network is expecting which in
00:31 - this case is a list of encoded numbers
00:33 - right or have encoded words that is
00:35 - essentially just numbers so what were
00:37 - you do to start is just save our model
00:39 - so let's talk about that now so up until
00:41 - this point every time we've wanted to
00:43 - make a prediction we've had to retrain
00:45 - the model now on small models like this
00:47 - that's fine you have to wait a minute
00:49 - two minutes but it's not very convenient
00:50 - when you have models that maybe take you
00:52 - days weeks months years to Train right
00:54 - so what you want to do is when you're
00:56 - done training the model you want to save
00:58 - it or sometimes you even want to save it
01:00 - like halfway through the training
01:01 - process this is known as checkpointing
01:04 - the model so that you can go back and
01:05 - continue to train it later now in this
01:07 - video we're just gonna talk about saving
01:09 - the model once it's completely finished
01:10 - but in future videos when we have larger
01:12 - networks we will talk about
01:14 - checkpointing and how you know how to
01:15 - load your or train your model in like
01:18 - batches of a different size data and all
01:20 - that so what I'm gonna start by doing is
01:22 - just actually bumping the vocabulary
01:24 - size of this model up to 88000 now the
01:28 - reason I'm doing that is just because
01:29 - for our next exercise which is gonna be
01:31 - making predictions on outside data we
01:34 - want to have as many words in our model
01:36 - as possible so that when it gets kind of
01:38 - some weirder words that aren't that
01:40 - common it knows what to do with them so
01:42 - I've done a few tests and I noticed that
01:44 - with the what he called with the
01:45 - vocabulary size bumped up it performs a
01:47 - little bit better so we're gonna do that
01:48 - so what I mean is we bump the vocabulary
01:51 - size and now after we train the model we
01:53 - need to save it now to save the model
01:55 - all we have to do is literally type the
01:57 - name of our model in this case model dot
01:59 - Save and then we give it a name so in
02:01 - this case let's call it model dot H 5
02:03 - now H 5 is just like an extension that
02:07 - means I don't know it's like I honestly
02:10 - don't know why they use H 5 but
02:13 - it's the extension for a saved model and
02:15 - Cara's and tensorflow so we're just
02:16 - gonna work with that and that's as easy
02:18 - as this is it was just gonna save our
02:20 - model in binary data which means we'll
02:22 - be able to read it in really quickly and
02:24 - use the model when we want to actually
02:26 - make predictions let's go ahead and run
02:28 - this now and then we're gonna have the
02:30 - model saved and then from now on we
02:32 - won't have to continually train the
02:33 - model when we want to make predictions
02:34 - but I'm gonna say Python tutorial - and
02:37 - I'll be right back once this finishes
02:39 - finishes running alright so the model is
02:42 - finished training notice that our
02:43 - accuracy is slightly lower than it was
02:47 - in the previous video really kind of a
02:49 - negligible difference here but anyways
02:51 - just notice that because we did bump the
02:53 - vocabulary size so anyways now that
02:55 - we've saved the model we actually don't
02:56 - have to go through this tedious process
02:58 - every time we run the code of creating
03:01 - and training and fitting the model and
03:03 - in fact we don't actually need to save
03:04 - it as well either here to load our model
03:06 - in now that's save and you can see the
03:08 - file right here with all this this big
03:10 - massive binary blob here all we have to
03:13 - do to load this in is just type one line
03:15 - now the line is whatever the name of
03:17 - your model is it doesn't matter I'm just
03:18 - gonna call it model is equal to in this
03:21 - case Kara's dot models dot load
03:24 - underscore model and then here you just
03:26 - put the name of that file so in this
03:27 - case model dot h5 now what's really nice
03:30 - about this as well as you can actually
03:32 - train a bunch of different models and
03:33 - tweak like hyper parameters of them and
03:36 - only save the best one what I mean by
03:38 - that is like maybe you mess with for
03:40 - example the amount of neurons in the
03:41 - second activation layer or something
03:43 - like that or in the second hidden layer
03:45 - and then you train a bunch of models you
03:47 - figure out which one has the highest
03:48 - accuracy and then you only save that one
03:50 - that's nice as well
03:52 - and that's something we could do like
03:53 - overnight you could run like your script
03:55 - for a few hours train a bunch of models
03:57 - figure out which one is the best only
03:59 - save and then use that one so anyways
04:01 - we're gonna load in this model
04:03 - notice that I've actually just commented
04:05 - out this aspect down here is we are not
04:07 - gonna use this anymore and now what
04:09 - we're gonna start doing is actually
04:11 - training or testing model on some
04:13 - outside data so I've gone ahead and
04:15 - picked a movie review for one of my
04:17 - favorite movies some of you guys can
04:19 - read this if you want but it's the Lion
04:21 - King absolutely loved that movie so I've
04:22 - decided to go with this this review was
04:24 - a 10 out of 10 review so a positive
04:26 - review
04:26 - we're gonna test our model on this one
04:28 - now actually did take this off like the
04:30 - IMDB website or whatever that's called
04:33 - but the data set that they use is
04:35 - different so this is you guys will see
04:38 - it why this works a little bit
04:39 - differently and what we have to do with
04:40 - this so this is in a text file so what
04:43 - I'm gonna do is load in the text file
04:45 - here in code and then get that big blob
04:47 - that's string and convert it into a form
04:49 - that our model can actually use so the
04:52 - first step to do this obviously is to
04:53 - get that string so we're gonna say with
04:55 - open and in this case I've called my
04:56 - file test dot txt and then I'm just
05:00 - gonna set the encoding because I was
05:01 - running into some issues here you guys
05:03 - probably don't have to do this I was
05:04 - gonna say UTF - 8 which is just kind of
05:06 - a standard text encoding and we're gonna
05:08 - say as f now again the reason I use
05:11 - width is just because that means I don't
05:13 - have to close the file afterwards better
05:15 - practice if you want to use that and now
05:17 - I'm gonna say poor line in F dots read
05:21 - lines which essentially just means we're
05:24 - gonna each line in this case we only
05:25 - have one line but if we wanted to throw
05:27 - in a few more reviews in here and do
05:30 - some predictions on those that would be
05:31 - very easy to do by just keeping this
05:33 - code structure you just throw another
05:34 - line in there and now I'm just gonna say
05:36 - we're gonna grab this line and we're
05:38 - gonna start pre-processing it so that we
05:41 - can actually feed it to our model now
05:43 - notice that this when we read this in
05:45 - all we're gonna get is a large string
05:47 - but that's no good to us we actually
05:49 - need to convert this into an encoded
05:51 - list of numbers right and essentially we
05:54 - need to say okay so of that's a word
05:56 - what number represents that put that in
05:58 - a list same with all same with the same
06:01 - with animation right and we keep going
06:03 - and keep going pretty well for all the
06:06 - words in here and we also have to make
06:08 - sure that the size of our text is only
06:11 - at max 250 words because that's what we
06:14 - were using when we were training the
06:15 - data so it's expecting a size of that
06:17 - and if you give it something larger
06:19 - that's not gonna work or it might but
06:21 - you're gonna get a few errors with that
06:22 - so anyways the first step here is I'm
06:24 - going to say n line is equal to line
06:27 - dots and I'm gonna remove a bunch of
06:30 - characters that I don't want so I'm just
06:31 - gonna say dot replace I think this is
06:34 - the best way to do it but maybe not um
06:36 - and I'm gonna replace all the commas all
06:39 - of the period
06:39 - all of the brackets and all of the
06:41 - colons and I'll talk about more why we
06:44 - want to do that in just one second so
06:46 - we'll do daughter place
06:47 - I guess this daughter place should
06:49 - probably be outside the bracket and then
06:51 - workplace with a bracket with nothing
06:55 - and I know this is there probably is a
06:58 - better way to do this but for our
06:59 - purposes it's not really that important
07:01 - and finally we will replace all our
07:03 - colons with nothing as well now again
07:06 - the reason I'm doing this is because
07:08 - let's go here if you have a look for
07:11 - example when we split this because we're
07:13 - just gonna split this data by spaces and
07:16 - to get all the words what will end up
07:19 - happening is we're gonna get words like
07:20 - company comma we're gonna get words like
07:22 - I'm trying to find something it has a
07:24 - period like art dot and then a quotation
07:27 - mark right and we don't want those to be
07:30 - words in our list because there's no
07:32 - mapping for art period there's only a
07:34 - mapping for art which means that I need
07:36 - to remove all of these kind of symbols
07:38 - so that when we split our data we get
07:41 - the correct words now there'll be a few
07:43 - times where the split doesn't work
07:45 - correctly but that's okay as long as the
07:47 - majority of them are working well same
07:49 - thing with brackets right I can't have
07:50 - irons and then a closing bracket as one
07:52 - of my words so I need to get rid of that
07:54 - now this reminds me I need to remove
07:55 - quotation marks as well because they use
07:57 - quite a few of those in there I don't
07:58 - know why I closed that document so let's
08:00 - do that as well with one last replace
08:02 - this a daughter place in this case we'll
08:05 - actually just do backslash quotation
08:07 - mark and then again with nothing now I'm
08:10 - adding a dot strip here to get rid of
08:12 - that backslash N and now we're gonna say
08:14 - dot split and in this case we'll split
08:17 - out of space now I know this is a long
08:19 - line but that's all we need to do to
08:21 - remove everything and now we actually
08:23 - need to encode and trim our data down to
08:25 - 250 words so to encode our data I'm
08:28 - gonna say encode equals in this case and
08:31 - we're just literally will make a
08:32 - function called like review underscore
08:35 - encode and we'll pass in our end line
08:39 - now what review and code will do is look
08:42 - up the mappings for all of the words and
08:44 - return to us an encoded list and then
08:47 - finally what we're gonna do and we'll
08:48 - create this function in just a second
08:50 - don't worry it doesn't already exist is
08:52 - we're actually gonna use what we've done
08:53 - up here with this test data train data
08:55 - care as pre processing stuff we're just
08:57 - going to apply this to in this case our
08:59 - encoded data so we add those pad tags or
09:02 - we trim it down to what it needs to be
09:03 - so this case will say encode equals
09:07 - Kara's dot pre-processing instead of
09:09 - train data we'll just pass in this case
09:11 - actually a list and then encode inside
09:13 - it because that's what it's expecting to
09:15 - get a list of list all right so now that
09:17 - we've done that our final step would be
09:20 - to use the model to actually make a
09:22 - prediction so we're gonna say model dot
09:23 - predict and then in this case we'll pass
09:26 - it simply this encode right here which
09:28 - will be in the correct form now we'll
09:30 - save that under predict and then what
09:33 - we'll do is just simply print out the
09:34 - model so we'll say print or not the
09:36 - model sorry we'll print the original
09:37 - text which will be the review so in this
09:40 - case we'll print line and then we will
09:42 - print out the encoded review just so we
09:44 - can have a look at what that is and then
09:46 - finally we will print the prediction so
09:49 - what whether the model thinks it's
09:50 - positive or negative so we'll just say
09:52 - predict and in this case we'll just put
09:53 - 0 because we're only gonna be doing like
09:56 - one at a time right ok sweet so now the
09:59 - last thing that we need to do is just
10:01 - simply write this reviewing code
10:03 - function and it'll be good to go and
10:05 - start actually using our model so I'm
10:06 - just gonna say define review underscore
10:09 - encode this is gonna take a string we'll
10:11 - just call that s lowercase s and what
10:14 - we're gonna do in here is set up a new
10:16 - list that we're going to append some
10:17 - stuff into so I'm just gonna say like
10:19 - return let's just say like it encoded
10:22 - equals and then I'm gonna start this
10:24 - with 1 now the reason I start one in
10:26 - here is because all of our data here
10:28 - where it starts has a 1 so we're gonna
10:31 - start with 1 because we won't have added
10:34 - that in from the other way I hope you
10:36 - guys understand that just we're setting
10:38 - like a starting tag to be consistent
10:39 - with the rest of them and now what we're
10:41 - gonna do is we're gonna loop through
10:43 - every single word that's in our S here
10:45 - which will be passed in as elisa words
10:47 - we'll look up the numbers associated
10:49 - with those words and add them into this
10:51 - encoded list they're gonna say for word
10:53 - and in this case we're gonna say word a
10:56 - and s now you'll say if words in this
11:01 - case word underscore index and again
11:04 - we're going to use word
11:04 - underscore index as opposed to reverse
11:06 - word index because word index stores all
11:09 - of the words corresponding to the
11:11 - letters or not the letters the numbers
11:13 - which means that we can literally just
11:15 - throw our data into word index and it'll
11:17 - give us the number associated with each
11:19 - of those words so we're gonna say if
11:21 - word in word index then we'll say
11:23 - encoded God append and in this case
11:25 - we'll simply append in this case word
11:28 - index word now otherwise what we'll do
11:32 - is we'll say encoded dot append - now
11:36 - what will happen is we're gonna check
11:38 - here if word if the word is actually in
11:41 - our vocabulary which is represented by
11:44 - word index which is just a dictionary of
11:46 - all the words corresponding to all the
11:48 - numbers that represent those words now
11:50 - if it's not what we'll do is we'll add
11:52 - in that unknown tag so that the program
11:54 - knows that this is an unknown word
11:56 - otherwise we'll simply add the number
11:58 - associated with that word now one last
12:00 - thing to do is actually just do word dot
12:02 - lower here just to make sure that if we
12:03 - get any words that have some weird
12:05 - capitalization they are still found in
12:08 - our vocabulary so like words at the
12:10 - beginning of a sentence and stuff like
12:11 - that uh and now with that being done I
12:14 - believe we're actually finished and
12:16 - ready to run this code so what's nice
12:18 - about this is now that we've saved the
12:19 - model we don't have to train it again so
12:21 - I can literally just run this and it
12:22 - should happen fairly quickly
12:24 - fingers crossed let's see all right must
12:30 - be a list of integrals found non
12:32 - iterable object so what air is that here
12:34 - um in code encoding coded code
12:39 - alright so print reviewing code ah well
12:42 - it would be helpful if I returned the
12:45 - encoded list and that would have been
12:46 - our issue there so let's run that one
12:48 - more time and see what we're getting
12:49 - there
12:49 - and there we go sweet so this is
12:52 - actually the review I know it's very
12:54 - really hard to read here but if you guys
12:56 - want to go ahead and read it
12:57 - feel free since it's on the Lion King
12:59 - it's obviously a positive review and
13:00 - then you could see this is what we've
13:02 - ended up with so our review has been
13:03 - translated into this which means we'd
13:05 - actually trimmed quite a bit of the
13:07 - review and you can see that wherever it
13:08 - says - that is actually a word that we
13:11 - didn't know or that wasn't in our
13:12 - vocabulary for represents the that's why
13:15 - there's a lot of fours and then all the
13:16 - other words have there
13:17 - correspondence right now fortunately for
13:20 - us we picked a 88000 vocabulary which
13:23 - means that we can get indexes like
13:25 - 20,000 whereas before would all been
13:27 - under 10,000 and you can see that our
13:29 - prediction here is now 96% positive
13:32 - which means that obviously like we were
13:34 - going between 0 where 0 is a negative
13:36 - review and once a positive review so
13:38 - this classified correctly as very
13:40 - positive review and we could try this on
13:42 - all other cons reviews and see what we
13:44 - get but that is how you go about kind of
13:47 - transforming your data into the form
13:49 - that the network expects and that's what
13:51 - I'm trying to get you guys at right now
13:53 - is to understand that yes it's really
13:55 - easy when we're doing it with this kind
13:58 - of data that just comes in like IMDB
14:00 - like Cara's load data but as soon as you
14:03 - actually have to start using your own
14:04 - data there's a quite a bit of
14:05 - manipulation that you have to do and
14:07 - things that you might not think about
14:09 - when you're actually feeding it to the
14:11 - network and in most cases you can
14:13 - probably be sure that your network is
14:15 - not actually the thing that's happening
14:17 - incorrectly but it's the data that
14:19 - you're feeding it is not in the correct
14:20 - form and it can be tricky to figure out
14:23 - what's wrong with that data so with that
14:24 - being said that has been it for this
14:26 - video I hope you guys enjoyed that's
14:28 - gonna wrap up the text classification
14:29 - aspect here of neural networks I'm
14:31 - thinking about either going into
14:33 - convolutional neural networks which is
14:35 - image processing which is really
14:37 - interesting
14:37 - or the kind of game playing neural
14:39 - networks in the next few videos please
14:41 - let me know what you guys want in the
14:42 - comments down below and I'll make sure
14:44 - to accommodate that
14:46 - [Music]

Cleaned transcript:

hey guys and welcome back so in today's video we're gonna be doing is talking about saving and loading our models and then we're gonna be doing a prediction on some data that doesn't come from this actual data set now I know this might seem kind of trivial we already know how to do predictions but trust me when I tell you this is a lot harder than it looks because if we're just taking in string data that means we have to actually do the encoding all of the preprocessing removing certain characters making sure that that data looks the same as the data that our neural network is expecting which in this case is a list of encoded numbers right or have encoded words that is essentially just numbers so what were you do to start is just save our model so let's talk about that now so up until this point every time we've wanted to make a prediction we've had to retrain the model now on small models like this that's fine you have to wait a minute two minutes but it's not very convenient when you have models that maybe take you days weeks months years to Train right so what you want to do is when you're done training the model you want to save it or sometimes you even want to save it like halfway through the training process this is known as checkpointing the model so that you can go back and continue to train it later now in this video we're just gonna talk about saving the model once it's completely finished but in future videos when we have larger networks we will talk about checkpointing and how you know how to load your or train your model in like batches of a different size data and all that so what I'm gonna start by doing is just actually bumping the vocabulary size of this model up to 88000 now the reason I'm doing that is just because for our next exercise which is gonna be making predictions on outside data we want to have as many words in our model as possible so that when it gets kind of some weirder words that aren't that common it knows what to do with them so I've done a few tests and I noticed that with the what he called with the vocabulary size bumped up it performs a little bit better so we're gonna do that so what I mean is we bump the vocabulary size and now after we train the model we need to save it now to save the model all we have to do is literally type the name of our model in this case model dot Save and then we give it a name so in this case let's call it model dot H 5 now H 5 is just like an extension that means I don't know it's like I honestly don't know why they use H 5 but it's the extension for a saved model and Cara's and tensorflow so we're just gonna work with that and that's as easy as this is it was just gonna save our model in binary data which means we'll be able to read it in really quickly and use the model when we want to actually make predictions let's go ahead and run this now and then we're gonna have the model saved and then from now on we won't have to continually train the model when we want to make predictions but I'm gonna say Python tutorial and I'll be right back once this finishes finishes running alright so the model is finished training notice that our accuracy is slightly lower than it was in the previous video really kind of a negligible difference here but anyways just notice that because we did bump the vocabulary size so anyways now that we've saved the model we actually don't have to go through this tedious process every time we run the code of creating and training and fitting the model and in fact we don't actually need to save it as well either here to load our model in now that's save and you can see the file right here with all this this big massive binary blob here all we have to do to load this in is just type one line now the line is whatever the name of your model is it doesn't matter I'm just gonna call it model is equal to in this case Kara's dot models dot load underscore model and then here you just put the name of that file so in this case model dot h5 now what's really nice about this as well as you can actually train a bunch of different models and tweak like hyper parameters of them and only save the best one what I mean by that is like maybe you mess with for example the amount of neurons in the second activation layer or something like that or in the second hidden layer and then you train a bunch of models you figure out which one has the highest accuracy and then you only save that one that's nice as well and that's something we could do like overnight you could run like your script for a few hours train a bunch of models figure out which one is the best only save and then use that one so anyways we're gonna load in this model notice that I've actually just commented out this aspect down here is we are not gonna use this anymore and now what we're gonna start doing is actually training or testing model on some outside data so I've gone ahead and picked a movie review for one of my favorite movies some of you guys can read this if you want but it's the Lion King absolutely loved that movie so I've decided to go with this this review was a 10 out of 10 review so a positive review we're gonna test our model on this one now actually did take this off like the IMDB website or whatever that's called but the data set that they use is different so this is you guys will see it why this works a little bit differently and what we have to do with this so this is in a text file so what I'm gonna do is load in the text file here in code and then get that big blob that's string and convert it into a form that our model can actually use so the first step to do this obviously is to get that string so we're gonna say with open and in this case I've called my file test dot txt and then I'm just gonna set the encoding because I was running into some issues here you guys probably don't have to do this I was gonna say UTF 8 which is just kind of a standard text encoding and we're gonna say as f now again the reason I use width is just because that means I don't have to close the file afterwards better practice if you want to use that and now I'm gonna say poor line in F dots read lines which essentially just means we're gonna each line in this case we only have one line but if we wanted to throw in a few more reviews in here and do some predictions on those that would be very easy to do by just keeping this code structure you just throw another line in there and now I'm just gonna say we're gonna grab this line and we're gonna start preprocessing it so that we can actually feed it to our model now notice that this when we read this in all we're gonna get is a large string but that's no good to us we actually need to convert this into an encoded list of numbers right and essentially we need to say okay so of that's a word what number represents that put that in a list same with all same with the same with animation right and we keep going and keep going pretty well for all the words in here and we also have to make sure that the size of our text is only at max 250 words because that's what we were using when we were training the data so it's expecting a size of that and if you give it something larger that's not gonna work or it might but you're gonna get a few errors with that so anyways the first step here is I'm going to say n line is equal to line dots and I'm gonna remove a bunch of characters that I don't want so I'm just gonna say dot replace I think this is the best way to do it but maybe not um and I'm gonna replace all the commas all of the period all of the brackets and all of the colons and I'll talk about more why we want to do that in just one second so we'll do daughter place I guess this daughter place should probably be outside the bracket and then workplace with a bracket with nothing and I know this is there probably is a better way to do this but for our purposes it's not really that important and finally we will replace all our colons with nothing as well now again the reason I'm doing this is because let's go here if you have a look for example when we split this because we're just gonna split this data by spaces and to get all the words what will end up happening is we're gonna get words like company comma we're gonna get words like I'm trying to find something it has a period like art dot and then a quotation mark right and we don't want those to be words in our list because there's no mapping for art period there's only a mapping for art which means that I need to remove all of these kind of symbols so that when we split our data we get the correct words now there'll be a few times where the split doesn't work correctly but that's okay as long as the majority of them are working well same thing with brackets right I can't have irons and then a closing bracket as one of my words so I need to get rid of that now this reminds me I need to remove quotation marks as well because they use quite a few of those in there I don't know why I closed that document so let's do that as well with one last replace this a daughter place in this case we'll actually just do backslash quotation mark and then again with nothing now I'm adding a dot strip here to get rid of that backslash N and now we're gonna say dot split and in this case we'll split out of space now I know this is a long line but that's all we need to do to remove everything and now we actually need to encode and trim our data down to 250 words so to encode our data I'm gonna say encode equals in this case and we're just literally will make a function called like review underscore encode and we'll pass in our end line now what review and code will do is look up the mappings for all of the words and return to us an encoded list and then finally what we're gonna do and we'll create this function in just a second don't worry it doesn't already exist is we're actually gonna use what we've done up here with this test data train data care as pre processing stuff we're just going to apply this to in this case our encoded data so we add those pad tags or we trim it down to what it needs to be so this case will say encode equals Kara's dot preprocessing instead of train data we'll just pass in this case actually a list and then encode inside it because that's what it's expecting to get a list of list all right so now that we've done that our final step would be to use the model to actually make a prediction so we're gonna say model dot predict and then in this case we'll pass it simply this encode right here which will be in the correct form now we'll save that under predict and then what we'll do is just simply print out the model so we'll say print or not the model sorry we'll print the original text which will be the review so in this case we'll print line and then we will print out the encoded review just so we can have a look at what that is and then finally we will print the prediction so what whether the model thinks it's positive or negative so we'll just say predict and in this case we'll just put 0 because we're only gonna be doing like one at a time right ok sweet so now the last thing that we need to do is just simply write this reviewing code function and it'll be good to go and start actually using our model so I'm just gonna say define review underscore encode this is gonna take a string we'll just call that s lowercase s and what we're gonna do in here is set up a new list that we're going to append some stuff into so I'm just gonna say like return let's just say like it encoded equals and then I'm gonna start this with 1 now the reason I start one in here is because all of our data here where it starts has a 1 so we're gonna start with 1 because we won't have added that in from the other way I hope you guys understand that just we're setting like a starting tag to be consistent with the rest of them and now what we're gonna do is we're gonna loop through every single word that's in our S here which will be passed in as elisa words we'll look up the numbers associated with those words and add them into this encoded list they're gonna say for word and in this case we're gonna say word a and s now you'll say if words in this case word underscore index and again we're going to use word underscore index as opposed to reverse word index because word index stores all of the words corresponding to the letters or not the letters the numbers which means that we can literally just throw our data into word index and it'll give us the number associated with each of those words so we're gonna say if word in word index then we'll say encoded God append and in this case we'll simply append in this case word index word now otherwise what we'll do is we'll say encoded dot append now what will happen is we're gonna check here if word if the word is actually in our vocabulary which is represented by word index which is just a dictionary of all the words corresponding to all the numbers that represent those words now if it's not what we'll do is we'll add in that unknown tag so that the program knows that this is an unknown word otherwise we'll simply add the number associated with that word now one last thing to do is actually just do word dot lower here just to make sure that if we get any words that have some weird capitalization they are still found in our vocabulary so like words at the beginning of a sentence and stuff like that uh and now with that being done I believe we're actually finished and ready to run this code so what's nice about this is now that we've saved the model we don't have to train it again so I can literally just run this and it should happen fairly quickly fingers crossed let's see all right must be a list of integrals found non iterable object so what air is that here um in code encoding coded code alright so print reviewing code ah well it would be helpful if I returned the encoded list and that would have been our issue there so let's run that one more time and see what we're getting there and there we go sweet so this is actually the review I know it's very really hard to read here but if you guys want to go ahead and read it feel free since it's on the Lion King it's obviously a positive review and then you could see this is what we've ended up with so our review has been translated into this which means we'd actually trimmed quite a bit of the review and you can see that wherever it says that is actually a word that we didn't know or that wasn't in our vocabulary for represents the that's why there's a lot of fours and then all the other words have there correspondence right now fortunately for us we picked a 88000 vocabulary which means that we can get indexes like 20,000 whereas before would all been under 10,000 and you can see that our prediction here is now 96% positive which means that obviously like we were going between 0 where 0 is a negative review and once a positive review so this classified correctly as very positive review and we could try this on all other cons reviews and see what we get but that is how you go about kind of transforming your data into the form that the network expects and that's what I'm trying to get you guys at right now is to understand that yes it's really easy when we're doing it with this kind of data that just comes in like IMDB like Cara's load data but as soon as you actually have to start using your own data there's a quite a bit of manipulation that you have to do and things that you might not think about when you're actually feeding it to the network and in most cases you can probably be sure that your network is not actually the thing that's happening incorrectly but it's the data that you're feeding it is not in the correct form and it can be tricky to figure out what's wrong with that data so with that being said that has been it for this video I hope you guys enjoyed that's gonna wrap up the text classification aspect here of neural networks I'm thinking about either going into convolutional neural networks which is image processing which is really interesting or the kind of game playing neural networks in the next few videos please let me know what you guys want in the comments down below and I'll make sure to accommodate that
