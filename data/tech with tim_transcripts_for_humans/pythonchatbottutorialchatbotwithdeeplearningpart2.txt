With timestamps:

00:00 - hey guys and welcome back to part 2 of
00:02 - AI chat bots in Python now in the last
00:05 - video we were kind of just working on
00:06 - figuring out how the chat BOTS gonna
00:08 - work talking about the data how we're
00:10 - gonna feed it information and now we're
00:12 - just gonna work a little bit more on the
00:13 - pre-processing of the data so we just
00:15 - kind of were loading in some the JSON
00:17 - file I just want to add a little bit to
00:19 - what we've already done because I kind
00:20 - of forgot it in the last video so what
00:22 - I'm gonna do is create a new list call
00:25 - it Docs underscore @y I'm gonna change
00:27 - this to Doc's underscore X now the
00:29 - reason I'm gonna do this is because for
00:32 - each pattern I also want to put another
00:35 - element in Doc's Y that stands for what
00:39 - intent it's a part of so like what tag
00:42 - it is a part of so to do that we're just
00:44 - gonna say Doc's underscore Y dot append
00:47 - and in this case we'll just say intent
00:50 - and tag so this way each entry in Doc's
00:54 - X corresponds to an entry in Doc's Y and
00:58 - the entry in Doc's X is going to be that
00:59 - pattern and then the intent will be in
01:02 - Doc's Y so we know kind of how to
01:04 - classify each of our patterns which will
01:06 - be important for training the model so
01:08 - now that we've done that we're actually
01:09 - gonna stem all of the words that we have
01:12 - in this words list and remove any
01:14 - duplicate elements because we just want
01:15 - to figure out what our kind of
01:17 - vocabulary size of the model is so how
01:20 - many words it has seen already so to do
01:23 - that we're gonna say words equals and in
01:25 - this case we're gonna say stammer dot
01:27 - stem and W dot lower it's important you
01:31 - add this lower because we want to
01:32 - convert all of our words into lowercase
01:34 - so they don't get mixed up where we
01:36 - don't think they're different than
01:37 - uppercase words the same we're gonna say
01:40 - for W in words just like that
01:44 - now with that what we can do now is take
01:47 - our words list we can make it a set and
01:51 - just you guys will see how this to this
01:54 - is gonna remove all the duplicates
01:55 - essentially we're gonna say sorted list
01:59 - set words now what set does is it takes
02:03 - all the words make sure there's no
02:05 - duplicates or just removes any duplicate
02:07 - elements list is just gonna convert this
02:09 - back into a list because the set is its
02:11 - own data type and sorted is obviously
02:13 - just gonna sort these words just so we
02:15 - can use those a little bit nicer now for
02:18 - our labels I don't think we need to do
02:20 - anything with that but we can actually
02:22 - we could sort it if we want a so let's
02:24 - sort our labels as well to say labels
02:26 - sorted labels and now what we're gonna
02:31 - do is start creating our training and
02:33 - testing output now right now all we've
02:37 - done essentially set up these few lists
02:39 - so we have all of our labels in one list
02:40 - all of the different words in our file
02:42 - in one or all of the words sorry in our
02:45 - patterns in one we have docks X which
02:48 - has list of all of the different
02:50 - patterns and then docks Y and the
02:52 - corresponding entries in docks X and
02:54 - docks Y are like the words and then the
02:58 - tag for that for those words which is
03:01 - the pattern right hopefully I didn't
03:03 - confuse you too much with that but this
03:05 - output or this input is not actually
03:07 - going to work for our neural network
03:08 - because right now we have strings and
03:10 - neural networks only understand numbers
03:12 - so what we're gonna do is create what's
03:14 - known as a bag of words that represents
03:17 - all of the words in any given pattern
03:20 - and we're gonna use that to train our
03:22 - model now a bag of words is what's known
03:25 - as one hot encoded which means that
03:28 - essentially we're gonna have a list
03:30 - maybe something like this that's gonna
03:32 - be the length of the amount of words
03:34 - that we have so if we had 100 words then
03:37 - each encoding is going to have 100
03:39 - entries and it's just going to be zeros
03:41 - and ones
03:42 - now each position in this list will
03:46 - represent either if a word exists or if
03:50 - a word doesn't exist now this could also
03:51 - be - it could be three could be four it
03:54 - just tells you how many times each word
03:56 - occurs so if you're confused right now
03:58 - let me try to break this down a little
04:00 - bit more we're gonna say that when we
04:02 - encode this the first word in our list
04:04 - maybe is a the second word in our list
04:07 - is like maybe bytes maybe the third one
04:09 - is like goodbye okay so what we do
04:13 - essentially is Ranse we're gonna look at
04:15 - a sentence and we're gonna encode it in
04:17 - this form and we're gonna say is there
04:19 - an A in our sentence if there's an A
04:21 - we're gonna put a 1 if there's two A's
04:23 - we're gonna put two and so on so the
04:25 - frequency goes as
04:26 - the entry because it's matching up the
04:28 - first entry with the first number right
04:30 - guessing now we're on the second one
04:31 - now despite exist in our sentence if it
04:35 - doesn't we're gonna put a zero if it
04:36 - does we're gonna put how many times it
04:38 - exists and we're just gonna keep going
04:40 - throughout this list and throughout all
04:42 - of our words and figure out the
04:43 - frequency of our words and put them in a
04:45 - list that's like this now the reason
04:48 - it's called one hot encoded is because
04:50 - it just represents like if the word is
04:51 - there or not usually one hot is because
04:54 - you only do if the word exists in this
04:56 - case we're doing it with the frequency
04:57 - so like um two or or three you're gonna
05:00 - have that as well and this is just a
05:02 - really good input to our neural network
05:04 - so we can essentially just determine
05:06 - what words are there and what words
05:07 - aren't there as opposed to giving it
05:10 - some string which it has nothing to do
05:11 - or it doesn't have any idea what to do
05:13 - with so to create this bag of words
05:16 - we're gonna have to do the following so
05:19 - we're gonna say training equals a blank
05:22 - list we're going to say output equals a
05:24 - blank list and we're going to say out
05:28 - underscore empty equals in this case 0
05:32 - for X or actually for underscore in
05:37 - range and then the blend of classes now
05:41 - I first to talk about our output data so
05:43 - our input data is going to be that big
05:45 - list with however many word entries we
05:48 - have an interest gonna say whether a
05:49 - word exists or whether it doesn't so
05:51 - we're gonna have a bunch of zeros and a
05:52 - few ones for each word that exists so a
05:55 - bag of words that's what that is now our
05:58 - output actually has to be in a different
06:02 - form as well so right now our output is
06:03 - simply something like what's in it
06:06 - what's an intent we have we have
06:07 - greeting so like it's all of our tags so
06:09 - it's a bunch of strings we have greeting
06:11 - we have like goodbye or something we
06:14 - have shoppe we have all of these
06:16 - different tags now we need to turn these
06:19 - into one hot encoded as well which means
06:21 - we're gonna have a list that has a 0 in
06:25 - it for all of the different classes so
06:27 - for however many classes we have and if
06:29 - that class or if that tag exists or is
06:33 - the one that we want we'll put a 1 there
06:34 - so let's say we have a list again that
06:37 - looks like 0 0 0 1 maybe
06:40 - for tags and the first tag is like hi
06:43 - the second one is by third one is like
06:47 - cell and the last one is help or
06:50 - something like that then this would mean
06:51 - that help is the tag associated with
06:54 - whatever output or input we have here
06:57 - because it has the one beside it and
06:59 - that's the way that this is going to
07:00 - work for our output so we're gonna have
07:03 - a bunch of output list and they are all
07:04 - going to be the length of the amount of
07:07 - classes we have all right so now it's
07:09 - time to actually create these bag of
07:10 - words we're gonna say for Doc in Doc's
07:14 - underscore X and we're actually gonna do
07:17 - X comma enumerate because we're gonna
07:21 - need to use this what we're gonna do is
07:23 - we're gonna say bag equals a blank list
07:25 - and this is gonna be our bag of words so
07:27 - that one hot encoded words that you guys
07:30 - will see we're gonna save pattern
07:32 - underscore words equals in this case
07:35 - actually we don't need to do that sorry
07:37 - am i bad we're gonna say W RDS equals
07:42 - and now we're gonna stem all of the
07:45 - words that are in our patterns because
07:47 - when we stem them we only stem each word
07:50 - in our like words list we didn't stem
07:53 - them when we added them into Docs X so
07:55 - we're just gonna stem them here we could
07:57 - have stemmed them when we added them but
07:58 - we're just gonna go ahead and stem them
07:59 - here so we're gonna say stammer don't
08:01 - stem and in this case we'll do W for W
08:04 - in and I guess in this case it's gonna
08:07 - be docs are just talk like that because
08:10 - doc will get all of the different docs
08:12 - in dot X now let's make sure we close
08:15 - that list off and I think that should be
08:18 - good okay so now what we're gonna do is
08:21 - we're gonna go through all of the
08:22 - different words that are in our document
08:26 - or in this word list now that our
08:27 - stemmed and we're gonna put either a 1
08:29 - or a 0 into our bag of words depending
08:32 - on if it's in the main word list which
08:35 - we have here or not whereas say for and
08:37 - in this case we're gonna say W in words
08:41 - and again we're looping through this
08:42 - which means all of the words that we
08:44 - have I'm gonna say if
08:47 - what'd he call its if W in this case
08:51 - we're gonna say Doc's underscore X or
08:53 - just doc my bad um is it doc centers for
08:57 - X no it's W RDS my apologies guys so if
09:00 - W in W RDS I probably should have called
09:03 - this something else but it's fine
09:04 - meaning that the word exists in the
09:06 - current pattern that we're looping
09:07 - through then what we're gonna do is
09:09 - we're gonna say bag dot append 1 which
09:13 - means that yep this word is here so we
09:15 - put a 1 representing that this word
09:17 - exists otherwise what we're gonna do is
09:20 - say bag dot append 0 which means that
09:25 - you know this word isn't here so we're
09:28 - putting a 0 it's not here there we go so
09:30 - actually I lied when I was telling you
09:32 - we were using frequency we're just doing
09:33 - one hot encoded which means if the word
09:35 - exists we put a 1 we don't care how many
09:37 - times it exists we just put a 1 2 exists
09:40 - 5 times we still only put a 1 so my
09:42 - apologies about that now what we're
09:43 - gonna do is now generate the output and
09:46 - append these into training and output so
09:49 - the output has to be generated like this
09:51 - right where we have a bunch of different
09:54 - zeros or ones representing the tag that
09:56 - is well that thing so what we're gonna
09:59 - do is say output underscore row equals
10:04 - in this case list and then out empty and
10:08 - actually we're not gonna do list what I
10:10 - want to do is just make a copy of this
10:12 - list would have done that as well but
10:13 - we'll just do that it's a little bit
10:15 - nicer and we're gonna say output row in
10:17 - this case classes dot index and then
10:22 - this we're gonna say Docs underscore Y X
10:25 - and we'll talk about all this in a
10:27 - second equals one so what we're gonna do
10:29 - is we're gonna look through this classes
10:30 - list or actually what am i calling it
10:32 - classes for its labels my apologies
10:35 - we're gonna look through this labels
10:37 - list here we're gonna see where the tag
10:40 - is in that list and then we're gonna set
10:43 - that value to one in our output row and
10:46 - that should work if you guys to
10:48 - understand that leave a comment but it's
10:50 - fairly straightforward and now what
10:51 - we're gonna do is real estate training
10:53 - dot append and in this case we are
10:56 - simply going to append
10:57 - list that has deep bag and I'm going to
11:02 - go to output and we're gonna go into a
11:06 - pend the output row so now what we're
11:09 - gonna have is two lists so we're gonna
11:10 - have a training list that is gonna have
11:12 - a bunch of bags of words which are like
11:15 - just a list of zeros and ones and we're
11:17 - gonna have a bunch of outputs which
11:18 - again our list of zeros and ones now
11:21 - they are both one hot encoded and we
11:23 - talked about what that means okay so now
11:26 - that we've done that what we're gonna do
11:27 - is we're going to shuffle up our data
11:29 - actually we won't shuffle it because if
11:32 - we shuffle it then it's not gonna work
11:33 - from output and training but we're gonna
11:37 - do is we're gonna turn these into
11:38 - actually NP arrays just cuz if we need
11:42 - to work with numpy arrays for TF learns
11:44 - we say numpy dot array training and I'm
11:49 - gonna say output equals NP array and in
11:55 - this case codebook
11:56 - alright so that is actually all we're
11:59 - gonna do for this tutorial we wrote this
12:01 - bit of code here which essentially is
12:03 - gonna give us a bat bunch of bags of
12:05 - words and we're just getting our data
12:06 - ready to feed into our model so in the
12:08 - next video what we're gonna do is start
12:10 - writing the model hopefully we can get
12:13 - done with writing the model maybe do a
12:15 - little bit of testing on it and then in
12:17 - the final one or two videos of the
12:19 - series depending on how long this takes
12:20 - we're going to work on actually using
12:23 - the model and making a nice framework
12:24 - for it so that we can type much of stuff
12:26 - and have it look really awesome and nice
12:28 - so anyways that has been it for this
12:30 - video quick recap again we just did the
12:32 - bags words here if you guys to
12:34 - understand this
12:34 - leave a comment join discord I will try
12:36 - to help you out and I will see you in
12:38 - the next video

Cleaned transcript:

hey guys and welcome back to part 2 of AI chat bots in Python now in the last video we were kind of just working on figuring out how the chat BOTS gonna work talking about the data how we're gonna feed it information and now we're just gonna work a little bit more on the preprocessing of the data so we just kind of were loading in some the JSON file I just want to add a little bit to what we've already done because I kind of forgot it in the last video so what I'm gonna do is create a new list call it Docs underscore @y I'm gonna change this to Doc's underscore X now the reason I'm gonna do this is because for each pattern I also want to put another element in Doc's Y that stands for what intent it's a part of so like what tag it is a part of so to do that we're just gonna say Doc's underscore Y dot append and in this case we'll just say intent and tag so this way each entry in Doc's X corresponds to an entry in Doc's Y and the entry in Doc's X is going to be that pattern and then the intent will be in Doc's Y so we know kind of how to classify each of our patterns which will be important for training the model so now that we've done that we're actually gonna stem all of the words that we have in this words list and remove any duplicate elements because we just want to figure out what our kind of vocabulary size of the model is so how many words it has seen already so to do that we're gonna say words equals and in this case we're gonna say stammer dot stem and W dot lower it's important you add this lower because we want to convert all of our words into lowercase so they don't get mixed up where we don't think they're different than uppercase words the same we're gonna say for W in words just like that now with that what we can do now is take our words list we can make it a set and just you guys will see how this to this is gonna remove all the duplicates essentially we're gonna say sorted list set words now what set does is it takes all the words make sure there's no duplicates or just removes any duplicate elements list is just gonna convert this back into a list because the set is its own data type and sorted is obviously just gonna sort these words just so we can use those a little bit nicer now for our labels I don't think we need to do anything with that but we can actually we could sort it if we want a so let's sort our labels as well to say labels sorted labels and now what we're gonna do is start creating our training and testing output now right now all we've done essentially set up these few lists so we have all of our labels in one list all of the different words in our file in one or all of the words sorry in our patterns in one we have docks X which has list of all of the different patterns and then docks Y and the corresponding entries in docks X and docks Y are like the words and then the tag for that for those words which is the pattern right hopefully I didn't confuse you too much with that but this output or this input is not actually going to work for our neural network because right now we have strings and neural networks only understand numbers so what we're gonna do is create what's known as a bag of words that represents all of the words in any given pattern and we're gonna use that to train our model now a bag of words is what's known as one hot encoded which means that essentially we're gonna have a list maybe something like this that's gonna be the length of the amount of words that we have so if we had 100 words then each encoding is going to have 100 entries and it's just going to be zeros and ones now each position in this list will represent either if a word exists or if a word doesn't exist now this could also be it could be three could be four it just tells you how many times each word occurs so if you're confused right now let me try to break this down a little bit more we're gonna say that when we encode this the first word in our list maybe is a the second word in our list is like maybe bytes maybe the third one is like goodbye okay so what we do essentially is Ranse we're gonna look at a sentence and we're gonna encode it in this form and we're gonna say is there an A in our sentence if there's an A we're gonna put a 1 if there's two A's we're gonna put two and so on so the frequency goes as the entry because it's matching up the first entry with the first number right guessing now we're on the second one now despite exist in our sentence if it doesn't we're gonna put a zero if it does we're gonna put how many times it exists and we're just gonna keep going throughout this list and throughout all of our words and figure out the frequency of our words and put them in a list that's like this now the reason it's called one hot encoded is because it just represents like if the word is there or not usually one hot is because you only do if the word exists in this case we're doing it with the frequency so like um two or or three you're gonna have that as well and this is just a really good input to our neural network so we can essentially just determine what words are there and what words aren't there as opposed to giving it some string which it has nothing to do or it doesn't have any idea what to do with so to create this bag of words we're gonna have to do the following so we're gonna say training equals a blank list we're going to say output equals a blank list and we're going to say out underscore empty equals in this case 0 for X or actually for underscore in range and then the blend of classes now I first to talk about our output data so our input data is going to be that big list with however many word entries we have an interest gonna say whether a word exists or whether it doesn't so we're gonna have a bunch of zeros and a few ones for each word that exists so a bag of words that's what that is now our output actually has to be in a different form as well so right now our output is simply something like what's in it what's an intent we have we have greeting so like it's all of our tags so it's a bunch of strings we have greeting we have like goodbye or something we have shoppe we have all of these different tags now we need to turn these into one hot encoded as well which means we're gonna have a list that has a 0 in it for all of the different classes so for however many classes we have and if that class or if that tag exists or is the one that we want we'll put a 1 there so let's say we have a list again that looks like 0 0 0 1 maybe for tags and the first tag is like hi the second one is by third one is like cell and the last one is help or something like that then this would mean that help is the tag associated with whatever output or input we have here because it has the one beside it and that's the way that this is going to work for our output so we're gonna have a bunch of output list and they are all going to be the length of the amount of classes we have all right so now it's time to actually create these bag of words we're gonna say for Doc in Doc's underscore X and we're actually gonna do X comma enumerate because we're gonna need to use this what we're gonna do is we're gonna say bag equals a blank list and this is gonna be our bag of words so that one hot encoded words that you guys will see we're gonna save pattern underscore words equals in this case actually we don't need to do that sorry am i bad we're gonna say W RDS equals and now we're gonna stem all of the words that are in our patterns because when we stem them we only stem each word in our like words list we didn't stem them when we added them into Docs X so we're just gonna stem them here we could have stemmed them when we added them but we're just gonna go ahead and stem them here so we're gonna say stammer don't stem and in this case we'll do W for W in and I guess in this case it's gonna be docs are just talk like that because doc will get all of the different docs in dot X now let's make sure we close that list off and I think that should be good okay so now what we're gonna do is we're gonna go through all of the different words that are in our document or in this word list now that our stemmed and we're gonna put either a 1 or a 0 into our bag of words depending on if it's in the main word list which we have here or not whereas say for and in this case we're gonna say W in words and again we're looping through this which means all of the words that we have I'm gonna say if what'd he call its if W in this case we're gonna say Doc's underscore X or just doc my bad um is it doc centers for X no it's W RDS my apologies guys so if W in W RDS I probably should have called this something else but it's fine meaning that the word exists in the current pattern that we're looping through then what we're gonna do is we're gonna say bag dot append 1 which means that yep this word is here so we put a 1 representing that this word exists otherwise what we're gonna do is say bag dot append 0 which means that you know this word isn't here so we're putting a 0 it's not here there we go so actually I lied when I was telling you we were using frequency we're just doing one hot encoded which means if the word exists we put a 1 we don't care how many times it exists we just put a 1 2 exists 5 times we still only put a 1 so my apologies about that now what we're gonna do is now generate the output and append these into training and output so the output has to be generated like this right where we have a bunch of different zeros or ones representing the tag that is well that thing so what we're gonna do is say output underscore row equals in this case list and then out empty and actually we're not gonna do list what I want to do is just make a copy of this list would have done that as well but we'll just do that it's a little bit nicer and we're gonna say output row in this case classes dot index and then this we're gonna say Docs underscore Y X and we'll talk about all this in a second equals one so what we're gonna do is we're gonna look through this classes list or actually what am i calling it classes for its labels my apologies we're gonna look through this labels list here we're gonna see where the tag is in that list and then we're gonna set that value to one in our output row and that should work if you guys to understand that leave a comment but it's fairly straightforward and now what we're gonna do is real estate training dot append and in this case we are simply going to append list that has deep bag and I'm going to go to output and we're gonna go into a pend the output row so now what we're gonna have is two lists so we're gonna have a training list that is gonna have a bunch of bags of words which are like just a list of zeros and ones and we're gonna have a bunch of outputs which again our list of zeros and ones now they are both one hot encoded and we talked about what that means okay so now that we've done that what we're gonna do is we're going to shuffle up our data actually we won't shuffle it because if we shuffle it then it's not gonna work from output and training but we're gonna do is we're gonna turn these into actually NP arrays just cuz if we need to work with numpy arrays for TF learns we say numpy dot array training and I'm gonna say output equals NP array and in this case codebook alright so that is actually all we're gonna do for this tutorial we wrote this bit of code here which essentially is gonna give us a bat bunch of bags of words and we're just getting our data ready to feed into our model so in the next video what we're gonna do is start writing the model hopefully we can get done with writing the model maybe do a little bit of testing on it and then in the final one or two videos of the series depending on how long this takes we're going to work on actually using the model and making a nice framework for it so that we can type much of stuff and have it look really awesome and nice so anyways that has been it for this video quick recap again we just did the bags words here if you guys to understand this leave a comment join discord I will try to help you out and I will see you in the next video
