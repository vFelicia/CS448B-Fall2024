With timestamps:

00:00 - in this video I'll show you how to build
00:02 - your own artificial intelligence
00:03 - application that utilizes rag retrieval
00:06 - augmented generation without writing a
00:09 - single line of code and in just a few
00:11 - minutes now the way we'll do that is
00:13 - using something called langlow now I
00:15 - have it open on my screen you can see
00:17 - what the finished application will look
00:18 - like and notice that it's all visual we
00:21 - can pull in here some pre-built
00:22 - components we can connect them in a
00:24 - really intuitive way and then we can run
00:26 - the entire application right from this
00:28 - platform so let me run for you give you
00:30 - a quick demo and then I'll show you
00:32 - exactly how we build this so the demo is
00:34 - running and just to give you a bit of
00:35 - information this app is going to
00:37 - represent a chatbot for something like a
00:39 - restaurant now really it could work in
00:41 - any context but the idea is we'll have
00:43 - something like a PDF here that has some
00:45 - commonly answered questions so if you're
00:47 - a restaurant or some store you probably
00:48 - get asked the same thing all the time
00:50 - when are you open where you located do
00:52 - you accept credit cards cash whatever so
00:54 - that's inside of some PDF document we're
00:57 - going to pass that to the llm and then
00:59 - the llm will be able to answer questions
01:01 - based on that document and it's also
01:03 - going to store our conversation history
01:05 - so it will remember what we asked last
01:07 - so let's ask it a question and notice
01:09 - that I can even put in my name here so
01:10 - that we can see who's answering or sorry
01:12 - who's asking the question so I'll say
01:14 - what time are you open and do I need a
01:19 - reservation question mark let's see what
01:21 - it says even though I spelled that
01:22 - incorrectly so there you go we got a
01:24 - response we're open from 11:00 a.m. to
01:26 - 10: p.m. blah blah blah and while
01:27 - reservations are recommended we always
01:29 - try to accommodate Watkins let's ask it
01:32 - something else let's say can you tell me
01:35 - about the specials and the menu question
01:39 - mark and there we are we have a reply
01:41 - here that's giving us some information
01:42 - from the PDF so variety of vegetarian
01:45 - and vegan dishes they have a kids menu
01:47 - and there specials based on the chef's
01:50 - selection now if I go back to the PDF
01:52 - you can see that if we look here at our
01:55 - specials for example it says yes we
01:56 - feature specials that include dishes and
01:58 - Chef's selection do you have a
02:00 - children's menu yes we have a children's
02:02 - menu do you accommodate food options Etc
02:04 - it kind of gave us all of those
02:05 - different responses and combined them
02:07 - together so I know it seems simple but
02:09 - this is something you can really extend
02:10 - and make quite cool so let's have a look
02:12 - now at how we build this okay so to get
02:15 - started here we are going to need to
02:16 - install langlow don't worry it's free it
02:19 - runs locally on our computer and we can
02:21 - actually install it using pip so long as
02:23 - we have python version 3.10 or above now
02:26 - once we do that we're actually going to
02:28 - create a vector store database using
02:30 - something called Astro DB from the
02:32 - company data Stacks now data Stacks is
02:34 - actually the sponsor of this video don't
02:36 - worry they're free you don't need to pay
02:38 - for them and they've actually teamed up
02:39 - with Lang flow just to make this that
02:41 - much better once we do that we're then
02:43 - going to use open AI so we're going to
02:45 - create an open aai API key and we're
02:47 - going to use that to connect to it so
02:49 - that we can have a really powerful llm
02:51 - but if you don't want to use open AI you
02:53 - can use really any model that you want
02:54 - and you'll see in Lang flow that you can
02:56 - connect to a bunch of different ones so
02:58 - let's go through the first step here
02:59 - which is installing langlow to do that
03:01 - we're going to copy this command from
03:03 - the documentation which I'll link in the
03:05 - description to install the pre-release
03:07 - of langlow so we'll go back to vs code
03:10 - we'll open up some kind of terminal in
03:11 - this case I've just created a new folder
03:13 - this is where I'm going to have my PDF
03:15 - file now we'll talk about the PDF in one
03:17 - second but for now let's paste this
03:19 - command in and if you are on Mac or
03:22 - Linux it's going to be pip 3 install
03:24 - Lang flow-- pre-- Force D install and if
03:29 - you're on Windows it's just going to be
03:30 - pip okay so we'll run that command I've
03:33 - already got it installed so I'm not
03:34 - going to do that it will take a few
03:36 - minutes to run because there are quite a
03:37 - lot of packages that need to be
03:39 - installed and then what we can do is run
03:42 - the command in our terminal which is
03:43 - langlow run when we do that it should
03:46 - start Lang flow on Local Host and then
03:48 - give us a browser window where we can
03:50 - start creating our flows all right so
03:53 - after you wait a second you should get
03:54 - some kind of output starting Lang flow
03:56 - whatever the version is and then it will
03:57 - actually open it at this address so I've
03:59 - got open here in my browser you're not
04:01 - going to see any flows because you
04:03 - haven't created any yet but in my case
04:05 - I've created a few so they're here so
04:07 - now what we can do is just press on new
04:08 - project here and we can make a new flow
04:11 - now you can work off of a template or
04:12 - you can just click blank flow which is
04:14 - what we're going to do for now now the
04:16 - way these flows are actually stored is
04:18 - with Json so what you're able to do is
04:20 - Import and Export the flows with a
04:22 - simple Json document and I'll actually
04:24 - leave a link to a GitHub repository in
04:26 - the description that will have the Json
04:28 - for the flow we're about to build as
04:30 - well as the PDF document and any other
04:32 - information that you need in case you
04:34 - just want to go right into the finished
04:36 - product and not actually build it out
04:37 - yourself anyways we're now inside of a
04:39 - flow and we can start building out our
04:41 - application but I do want to mention
04:43 - that we do want to get access to some
04:45 - kind of PDF that we're going to be
04:47 - providing to the app so in my case I
04:49 - have this restaurant Q&A I'll leave this
04:51 - in the GitHub repo in case you just want
04:53 - to download this one but you can also
04:54 - make your own or you could just get a
04:56 - PDF you already have and you can feed
04:58 - that into the app but just make sure you
04:59 - have some kind of PDF ready and
05:01 - accessible because we are going to need
05:03 - that as we build out this flow okay so
05:05 - let's start building the flow here and
05:07 - kind of go through all the different
05:08 - components see how to connect them and
05:10 - build this application so I'm just going
05:12 - to grab a few components from the
05:13 - sidebar here and bring them in and we'll
05:15 - start building out this app and kind of
05:16 - talk through it as we go so I'm going to
05:18 - go into inputs and notice that you can
05:20 - also just search for stuff up here as
05:22 - well so for inputs I'm going to grab a
05:24 - text input I'm going to grab a prompt
05:27 - and I'm also going to grab a chat input
05:30 - now let's zoom out a little bit so that
05:32 - we can see all of these and if you kind
05:33 - of hover over them or you look at them
05:35 - it tells you exactly what the component
05:36 - is meant for so what I want to do is I
05:39 - want to get some text input which is the
05:40 - user's name now this is how we'll
05:43 - understand which user is asking which
05:44 - questions and store that in some kind of
05:46 - chat memory so for the text input here I
05:49 - can actually change the name of it so
05:51 - I'm just going to call that name by
05:53 - clicking on edit and then for the value
05:55 - this will be something the users
05:56 - expected to input so we don't need to
05:58 - actually put anything here now what
05:59 - we'll do is we'll take that and we'll
06:02 - pass that as the sender name so notice
06:04 - I'm taking the output here which is
06:06 - whatever they type in and then that's
06:08 - going to be the sender name into my chat
06:10 - input now the chat input is going to be
06:12 - the question that the user types so they
06:14 - can just type whatever they want here
06:16 - I'll show you how we run that in one
06:17 - second and then we're going to take that
06:19 - and we're going to pass that into our
06:21 - prompt now the prompt here is kind of
06:22 - like a prompt template where we can
06:24 - actually write out a few different
06:26 - variables that we want it to accept and
06:28 - then we can kind of structure how we
06:29 - want to ask the llm question so what
06:32 - I'll actually do is I'll just click on
06:33 - this link here and I'll delete it
06:35 - because I actually don't want to do that
06:37 - I'm going to click on template and I'm
06:38 - going to write a simple template and I
06:40 - can embed some prompt variables using my
06:42 - curly braces so I can say something like
06:45 - hey
06:46 - answer the users's question based on the
06:52 - following context and then I can
06:55 - say the context is this and I can pass
07:00 - inside of here context and now that's
07:02 - going to be a prompt variable that this
07:04 - can accept next we're going to say the
07:07 - users question is this and then this
07:11 - will be the question and then we'll have
07:13 - the message history as well we'll say
07:15 - and this is the message history and
07:20 - we'll say that that is history and we
07:22 - can adjust this if we want to make it
07:24 - better but let's say hey answer the
07:25 - question based on the following context
07:28 - we'll say context message history
07:30 - question okay looks good to me let's
07:32 - check that and then notice these
07:33 - variables now appear here in our prompt
07:36 - and I can pass them in so for the chat
07:38 - input that's going to be this so that'll
07:40 - go as the question and then for the
07:42 - context we need to provide that that's
07:44 - going to be from our rag retrieval
07:45 - augmented generation we're going to use
07:47 - a vector store database for that then
07:49 - the history is going to come from our
07:50 - chat history so how do we do that we
07:53 - let's bring in our history here I
07:55 - believe it's message history that we're
07:57 - using or actually it's going to be
07:59 - memory sorry we're going to have chat
08:01 - memory so this retrieves store chat
08:03 - messages with a specific session ID and
08:06 - the session ID is going to be the name
08:08 - of the user that's interacting with the
08:10 - model so we're going to take the name
08:12 - and we're going to pass that as the
08:14 - session ID so we'll link that up right
08:16 - here and the idea is whenever we change
08:18 - the name then it will change the memory
08:19 - that we're using so if we were talking
08:21 - as Tim and then we change it to Sarah
08:23 - well then we get the appropriate memory
08:25 - uh that the user was typing in before so
08:27 - it kind of remembers who's saying what
08:29 - that's the idea here so we're going to
08:31 - take that and that's going to be our
08:32 - history and now that we have that we can
08:35 - actually pass that to an llm and then we
08:37 - can pass that to some chat output and we
08:39 - can test this before we actually
08:41 - implement the rag component which we'll
08:42 - do in a second okay so how do we do that
08:45 - well we need to pass this to an llm so
08:47 - actually if we go to models here you can
08:50 - see there's a bunch of different models
08:51 - that we can use now I'm going to use
08:53 - open AI just because that's the simplest
08:55 - for this tutorial but you can use olama
08:57 - you can use Vector AI hug face API
09:00 - there's a bunch of free versions here
09:01 - and one version you could use is AMA you
09:04 - just need to set that up locally and
09:05 - then you can run your own local llm
09:08 - again in our case we use open AI but you
09:09 - can literally just Swap this llm with
09:12 - any llm on the left side here that you
09:13 - want to use okay so what we'll do is
09:16 - we'll take the text and we'll pass that
09:18 - as the input here to the llm now what we
09:22 - need to do is we need to get an open AI
09:23 - API key so let's do that and then we'll
09:25 - move on to the next step so in order to
09:27 - get this API key we do need account with
09:29 - chat GPT or with openai the URL is
09:32 - platform.
09:34 - open.com ai- Keys now they've changed
09:37 - this recently where you do actually need
09:39 - to fund the account so you will need to
09:41 - put like a dollar or $2 into the account
09:43 - to be able to use this again if you
09:45 - don't want to use this you can use any
09:46 - other llm or API that you want but open
09:49 - AI right now is really just the simplest
09:50 - way to do this so you need to go to API
09:52 - keys and then create a new secret key
09:55 - and then copy that key so we'll make a
09:56 - new key here I'm just going to call this
09:59 - tutorial like that and then you can give
10:01 - it uh Scopes but in my case I'm just
10:03 - going to give it access to everything
10:05 - it's going to give me some key that I
10:07 - will delete afterwards let me copy that
10:09 - and then we can utilize that key now
10:11 - just to show you here if you want to see
10:13 - if you're able to actually use this or
10:15 - not you can go into usage and it's going
10:17 - to show you how much usage you have so
10:19 - I've used this a little bit today and
10:21 - then you can click on increase limit now
10:23 - when you click on increase limit it's
10:24 - going to bring you to this page right
10:26 - here you'll have the ability to set a
10:27 - monthly budget uh and you can you can
10:29 - kind of go through all of this stuff
10:30 - here then you also have the ability to
10:32 - buy credits and to fund the account so
10:34 - what I can do is go to buy credits here
10:37 - and when I go to buy credits what I can
10:38 - do is I can fund the account with a
10:40 - certain amount of money in my case I
10:41 - just put in $5 and you can do that from
10:43 - your credit card balance you can have it
10:44 - auto recharge Etc they've changed a
10:46 - little bit of how this works but if
10:48 - you're getting an error saying that
10:49 - you've reached your quota it's because
10:51 - you do need to fund the account with a
10:52 - little bit of money for it to actually
10:54 - work properly if you guys know another
10:55 - way to get around that that's great uh
10:57 - but in my case that's what I had to do
10:59 - okay so now that we have the API key I'm
11:01 - going to go and I'm going to paste my
11:03 - API key here into the open API key or
11:06 - open AI API key and what I can actually
11:08 - do if I want is I can make a new
11:09 - variable I've had some variables here
11:11 - before but I'm going to make a new one
11:13 - I'm going to call this open AI key new
11:17 - and then I'm going to paste the value
11:19 - here and I'm going to make this of type
11:21 - credential this is automatically going
11:23 - to be encrypted and then I can just
11:25 - utilize that variable so I'm going to go
11:27 - with that here in case I need to use
11:28 - this later on my program okay so now
11:31 - that we have that we're passing the
11:32 - prompt to the llm now we need to take
11:35 - the output from the llm and actually
11:36 - display that to the user so to do that
11:39 - we're going to go to outputs and you see
11:40 - that we have a chat output now this will
11:42 - display a chat message in the
11:43 - interactive panel so we'll just take the
11:45 - text and we'll put that to the message
11:47 - and the sender name will simply be AI
11:50 - okay so I know it's a little bit small
11:52 - but you guys get the idea we have our
11:53 - chat output llm prompt chat input name
11:57 - and chat memory and now now what I can
11:59 - do is click on run here and when I click
12:02 - on run you can see that I have a name so
12:03 - I can enter something like Tim and I can
12:05 - ask this a question I can say hey how is
12:08 - your day going question mark and when I
12:11 - hit on enter you can see it shows my
12:14 - name and then it responds my day is
12:16 - going well thank you for asking how
12:17 - about yours perfect so we have a basic
12:19 - llm pipeline set up now what we need to
12:22 - do is Implement rag retrieval augmented
12:24 - generation so we actually load in the
12:26 - PDF and then we're able to answer
12:28 - questions based on it now in order to do
12:30 - that we need to set up a vector store
12:31 - database so let me talk about how that
12:33 - works all right so I'm now on the data
12:35 - Stacks website where they're talking
12:36 - about their Astra database which is a
12:38 - nosql and Vector database specifically
12:40 - for generative AI apps now this is a
12:43 - really fast way to retrieve relevant
12:45 - information while we're building things
12:47 - like retrieval augmented generation
12:49 - pipelines which is what we're doing now
12:51 - I just want to quickly explain how this
12:53 - works and what a vector database
12:55 - actually is so when we build this rag
12:57 - app what we're trying to do is take
12:59 - relevant piece of information from all
13:01 - of our context in this case it's a PDF
13:04 - document and inject that into the prompt
13:07 - alongside whatever the user asked that
13:10 - way the llm has relevant data to
13:12 - actually answer the question however in
13:14 - order to do that we need to really
13:15 - quickly be able to retrieve the relevant
13:17 - data and we need to get the best match
13:19 - we possibly can so the process goes like
13:21 - this user types something in we then go
13:24 - to the vector database and based on what
13:26 - they typed in we search for things that
13:28 - are relevant to that we get that
13:30 - information we inject that into the
13:31 - prompt we also pass the user's question
13:34 - then the llm gives us some response but
13:36 - first we need to of course take all of
13:38 - our data we need to turn it into vectors
13:40 - and we need to put it into the vector
13:41 - database so that's what we'll be using
13:43 - data stacks and their Astro database 4
13:46 - this runs on Apachi Cassandra and again
13:48 - it just provides a really really quick
13:50 - way to get relevant information and
13:52 - inject that into the prompt when I start
13:54 - building this out I'll talk about it
13:55 - more but for now go to the link in the
13:57 - description create a free account and
14:00 - I'm going to show you how we can spin up
14:01 - a new database that we can then connect
14:03 - to from langlow so once you've made your
14:06 - account and signed in you should be
14:07 - brought to a page that looks like this
14:09 - now what we're going to do is look for
14:11 - this button that says create a database
14:13 - we're going to click on it and we're
14:14 - going to go with the serverless vector
14:16 - database which is exactly what we want
14:18 - for this app now you can name this
14:20 - anything that you want just keep in mind
14:21 - that you can't change it so I'm just
14:22 - going to call this
14:24 - langlow tutorial then we can choose our
14:27 - provider I'm going to go with Google
14:29 - cloud and in the US East now obviously
14:32 - if you want to use this in production
14:33 - you can upgrade your account but in our
14:35 - case we just want something simple that
14:36 - we can test with for the video which is
14:38 - free okay so it's going to make the
14:40 - database that'll take a few minutes once
14:42 - it's done I'll be right back and then
14:43 - I'll show you how we get the information
14:45 - that we need to connect to this from LF
14:47 - flow all right so now that this is
14:48 - loaded we're good to go and we're just
14:50 - going to be looking for the token and
14:51 - then the end point here as well as a
14:53 - collection name but we can deal with
14:55 - that later so let's leave this open and
14:57 - let's go back to langlow and let's start
14:59 - creating this connection so what we're
15:00 - going to do is we're going to go to the
15:02 - chat input and then once we have that
15:04 - input we're going to pass that to a
15:06 - vector search and that's going to look
15:08 - through our database and find any of
15:10 - those relevant pieces of information and
15:12 - then inject that in the prompt but
15:14 - before we can do that we need to build
15:15 - the database with that information so
15:17 - let's do that down here going to zoom in
15:19 - a bit and on the left hand side here
15:21 - we're going to go to our Vector stores
15:23 - and we're going to take in astrab which
15:25 - is what we're using what we're going to
15:27 - do here is we're going to put in the AP
15:29 - endpoint the token Etc okay so we're
15:31 - going to go to data Stacks Astra I'm
15:33 - going to copy in the endpoint and I'm
15:36 - going to make a new variable for this so
15:38 - I'm just going to go here and click on
15:40 - variable let me actually just delete the
15:42 - ones that I had before just to make sure
15:44 - we don't get messed up here okay get rid
15:47 - of all of those and let's make a new one
15:50 - here and call this endpoint okay we'll
15:54 - just make this generic because we don't
15:56 - need to hide that and then we can save
15:58 - the variable okay so let's put our
16:00 - endpoint here for the API endpoint sorry
16:03 - for some reason that went for our token
16:05 - then we need our token so let's generate
16:07 - the token okay let's copy that close go
16:11 - back here and let's make a new variable
16:13 - for the token and we're going to call
16:14 - this our Astra uncore token we're then
16:18 - going to paste the value here and we're
16:20 - going to make this a credential okay so
16:23 - let's set that as Astra token and then
16:25 - for the collection name we're going to
16:27 - make a new variable okay so let's do
16:30 - that here and we're just going to call
16:32 - this collection and the value of this is
16:34 - just going to be PDF because we're
16:36 - storing some PDFs and this will be
16:38 - generic okay so now we'll set that to
16:41 - our collection so now we have our token
16:43 - our endpoint and our collection what we
16:45 - now need to do is we need to pass some
16:47 - inputs to this and we need to pass the
16:49 - embeddings now the input is going to be
16:51 - the file that we want to load or the
16:52 - files that we want to load and the
16:54 - embeddings are going to be how we
16:55 - vectorize this okay so let's first start
16:59 - with the embeddings the embeddings will
17:01 - be here so on the side click on
17:03 - embeddings and we're going to go to open
17:05 - AI embeddings now leave this as the
17:07 - default text embeddings and for the open
17:10 - AI key we can just set this to our open
17:12 - AI key new which is the variable we
17:14 - created we're going to drag this and
17:16 - connect the embeddings and now what
17:18 - we're going to do is pass our inputs so
17:21 - for the inputs we're going to get a file
17:23 - so we can just search for one here this
17:25 - is a generic file loader so let's load
17:28 - in our file file and we're going to find
17:29 - the file in my case it is on my desktop
17:32 - it's in this folder called restaurant
17:34 - Q&A obviously you put yours wherever
17:36 - youve put it but locate it and then load
17:38 - it in and then we're going to pass this
17:40 - into our text splitter so let's find
17:44 - that this is the split text okay let me
17:47 - give us some more room here all right
17:50 - sorry so what this split text is going
17:51 - to do is it's going to take the file
17:53 - it's going to split it into a bunch of
17:54 - different chunks it's then going to pass
17:55 - it to the Astro database where will then
17:57 - be converted into vectors so rather than
18:00 - just passing the entire file at once we
18:02 - want to split it into small different
18:03 - chunks then we pass it to Astro database
18:06 - along with our open Ai embeddings and
18:08 - the embeddings is a special model that
18:10 - actually does this kind of search for us
18:13 - right so it will convert things into
18:15 - vectors and then allow us to compare the
18:16 - vectors for similarity and get the
18:18 - results back that we want now we're
18:20 - going to be using the same embedding
18:21 - model for actually creating the database
18:23 - as well as for searching in the database
18:25 - which you'll see in a second so I'm
18:27 - going to take my file and I'm going to
18:28 - pass that as the input here to my split
18:31 - text we don't need to change any of
18:33 - these settings but we can if we want to
18:35 - and then we're going to take this record
18:36 - and we're going to pass that to the
18:38 - inputs to Astra database so that's it
18:40 - this will actually now create a new
18:42 - collection in Astra database called PDF
18:45 - or whatever we named it it will do that
18:47 - by taking in this file splitting into
18:49 - chunks passing it here and then
18:51 - embedding all of them as vectors okay
18:53 - perfect now what we're going to do is
18:55 - we're just going to take this embedding
18:57 - and we're just going to make this a
18:58 - little bit easier to view because we're
19:00 - going to use this embeddings up here as
19:03 - well I know this is difficult to see um
19:05 - I'm going to try to zoom in as much as I
19:07 - can okay so now what we want to do is
19:09 - after we have the chat input we want to
19:11 - pass that into a vector search where we
19:14 - then go and look for all of the relevant
19:16 - information so let's move our prompt up
19:18 - here and let's get our Vector search so
19:21 - we're going to have our Aster database
19:23 - search which is right here let's zoom in
19:25 - so we can see this for the token this is
19:28 - going to be our variable so this is
19:30 - going to be the AST token for the
19:32 - endpoint this is going to be our
19:33 - variable endpoint and for the collection
19:35 - name it's going to be the collection
19:37 - that's why we made variables now for the
19:39 - input value that's actually going to be
19:41 - the chat input so whatever the user
19:43 - typed that's the input here to our Astra
19:46 - database search and then the embedding
19:48 - is going to be the one that we had here
19:51 - so we'll just connect those embeddings
19:53 - and give me a second I'm going to clean
19:54 - this up just so it's a little bit easier
19:56 - to see okay so I just clean this up a
19:57 - little bit so it's a bit more organized
19:59 - and now what we're going to do is we're
20:00 - going to take the output from our astd
20:02 - database search and we're going to
20:03 - connect that all the way up here to the
20:06 - content for our prompt so remember we
20:09 - have the content the history and the
20:11 - question we already have the chat
20:12 - history and we already have the question
20:14 - the last thing that we need was the
20:15 - context and that comes from our database
20:17 - search Okay so let's quickly run through
20:19 - this at a high level first thing we do
20:22 - is we get the name from the user we then
20:24 - pass that into the chat input and that's
20:27 - kind of like the name of the person who
20:28 - said send the message then we ask them
20:30 - hey type something in this is their
20:32 - question then we connect this to chat
20:34 - memory the chat memory is just so that
20:36 - we can store what the last in this case
20:38 - five messages were that the user was
20:40 - typing and then we pass the chat input
20:42 - into the prompt now as well as that we
20:44 - pass the history which is our chat
20:45 - memory and then we pass the context from
20:48 - our Vector search so our chat input goes
20:51 - to the vector search we then embed that
20:53 - using the open Ai embeddings and we look
20:56 - for any relevant information from our
20:58 - PDF F or whatever documents we put in
21:00 - here and then we get that and we pass
21:02 - that to the prompt so that's what
21:04 - database search does it searches inside
21:06 - of this Vector database for any relevant
21:08 - information gives it to us and then
21:10 - injects it in the prompt or that's how
21:12 - the prompt works here then we take the
21:14 - prompt we pass that to open AI then open
21:16 - AI gives us some output and we print
21:18 - that out meanwhile over here we have our
21:21 - file we load in the file we split it
21:23 - into a bunch of different chunks we then
21:25 - take that and we pass that to our Astro
21:28 - database so we can create this database
21:30 - and we do that using the same embeddings
21:32 - that we will use for our database search
21:34 - so now if we want to run this we can
21:36 - click on run let's just erase all of
21:39 - this and let's ask it some kind of
21:40 - question hey can you tell me the hours
21:45 - the store is open and let's see if this
21:48 - works so I was just playing with this
21:50 - and it's working properly one thing to
21:52 - note you do need to ask it one PRT first
21:54 - that it's not going to give you a good
21:56 - response for because it just needs to
21:58 - build that Vector store so before the
22:00 - vector store is built it's not able to
22:02 - actually give you any valid response
22:04 - because it doesn't have any context so
22:06 - you're not seeing it on the screen cuz I
22:07 - cleared it and then did it again but the
22:09 - first question I asked it didn't give me
22:10 - a valid answer then the second question
22:12 - it worked because it actually ran this
22:14 - secondary component chain that we had
22:16 - down here which actually built out that
22:18 - Vector store so last thing I want to
22:20 - show you here is that we can change our
22:22 - name and when we change our name what
22:25 - that does is it resets the message
22:27 - history so I'm now talking talking as
22:29 - Joey and I'm going to say hey have I
22:31 - asked you anything yet and you're going
22:34 - to notice here that it's going to say no
22:36 - and the reason for that is that the last
22:37 - person that was talking to this was Tim
22:39 - not Joey so when we change the name it
22:42 - changes the message history which allows
22:44 - us to have kind of our own log based on
22:46 - who's asking questions so it says no you
22:48 - haven't asked anything yet and then we
22:49 - can say okay cool thanks whatever right
22:53 - and then we'll track this in Joey's
22:55 - message history whereas Tim's message
22:57 - history will be different kind of just
22:59 - an added feature that I wanted to add to
23:01 - this project so the last thing I really
23:03 - quickly want to mention is that you can
23:04 - Import and Export these just as Json so
23:07 - what I can do is go here I can change
23:09 - the name obviously and then I can export
23:11 - this I can give it a name name something
23:13 - like Rag and then I can download it as
23:15 - Json and then if I already have one that
23:17 - I want to import I can just go back here
23:20 - I can load it up so let me just go to my
23:23 - downloads and I can just take this and
23:26 - just drag it in and sorry need to
23:28 - actually be in the correct thing so if I
23:30 - drag it here you can see that when I do
23:32 - that it just loads it in so that's as
23:34 - easy as it is to actually load in the
23:36 - Json and to share with other people so
23:38 - what I'm going to do is I'm going to
23:39 - have this Json and I'm just going to
23:41 - leave it in the GitHub repository Linked
23:43 - In the description so you can just drag
23:45 - it into Lang flow and you can play
23:46 - around with it there you are guys I hope
23:48 - you enjoyed this video If You did leave
23:50 - a like subscribe to the channel and I
23:52 - will see you in the next one
23:56 - [Music]
23:58 - oh

Cleaned transcript:

in this video I'll show you how to build your own artificial intelligence application that utilizes rag retrieval augmented generation without writing a single line of code and in just a few minutes now the way we'll do that is using something called langlow now I have it open on my screen you can see what the finished application will look like and notice that it's all visual we can pull in here some prebuilt components we can connect them in a really intuitive way and then we can run the entire application right from this platform so let me run for you give you a quick demo and then I'll show you exactly how we build this so the demo is running and just to give you a bit of information this app is going to represent a chatbot for something like a restaurant now really it could work in any context but the idea is we'll have something like a PDF here that has some commonly answered questions so if you're a restaurant or some store you probably get asked the same thing all the time when are you open where you located do you accept credit cards cash whatever so that's inside of some PDF document we're going to pass that to the llm and then the llm will be able to answer questions based on that document and it's also going to store our conversation history so it will remember what we asked last so let's ask it a question and notice that I can even put in my name here so that we can see who's answering or sorry who's asking the question so I'll say what time are you open and do I need a reservation question mark let's see what it says even though I spelled that incorrectly so there you go we got a response we're open from 1100 a.m. to 10 p.m. blah blah blah and while reservations are recommended we always try to accommodate Watkins let's ask it something else let's say can you tell me about the specials and the menu question mark and there we are we have a reply here that's giving us some information from the PDF so variety of vegetarian and vegan dishes they have a kids menu and there specials based on the chef's selection now if I go back to the PDF you can see that if we look here at our specials for example it says yes we feature specials that include dishes and Chef's selection do you have a children's menu yes we have a children's menu do you accommodate food options Etc it kind of gave us all of those different responses and combined them together so I know it seems simple but this is something you can really extend and make quite cool so let's have a look now at how we build this okay so to get started here we are going to need to install langlow don't worry it's free it runs locally on our computer and we can actually install it using pip so long as we have python version 3.10 or above now once we do that we're actually going to create a vector store database using something called Astro DB from the company data Stacks now data Stacks is actually the sponsor of this video don't worry they're free you don't need to pay for them and they've actually teamed up with Lang flow just to make this that much better once we do that we're then going to use open AI so we're going to create an open aai API key and we're going to use that to connect to it so that we can have a really powerful llm but if you don't want to use open AI you can use really any model that you want and you'll see in Lang flow that you can connect to a bunch of different ones so let's go through the first step here which is installing langlow to do that we're going to copy this command from the documentation which I'll link in the description to install the prerelease of langlow so we'll go back to vs code we'll open up some kind of terminal in this case I've just created a new folder this is where I'm going to have my PDF file now we'll talk about the PDF in one second but for now let's paste this command in and if you are on Mac or Linux it's going to be pip 3 install Lang flow pre Force D install and if you're on Windows it's just going to be pip okay so we'll run that command I've already got it installed so I'm not going to do that it will take a few minutes to run because there are quite a lot of packages that need to be installed and then what we can do is run the command in our terminal which is langlow run when we do that it should start Lang flow on Local Host and then give us a browser window where we can start creating our flows all right so after you wait a second you should get some kind of output starting Lang flow whatever the version is and then it will actually open it at this address so I've got open here in my browser you're not going to see any flows because you haven't created any yet but in my case I've created a few so they're here so now what we can do is just press on new project here and we can make a new flow now you can work off of a template or you can just click blank flow which is what we're going to do for now now the way these flows are actually stored is with Json so what you're able to do is Import and Export the flows with a simple Json document and I'll actually leave a link to a GitHub repository in the description that will have the Json for the flow we're about to build as well as the PDF document and any other information that you need in case you just want to go right into the finished product and not actually build it out yourself anyways we're now inside of a flow and we can start building out our application but I do want to mention that we do want to get access to some kind of PDF that we're going to be providing to the app so in my case I have this restaurant Q&A I'll leave this in the GitHub repo in case you just want to download this one but you can also make your own or you could just get a PDF you already have and you can feed that into the app but just make sure you have some kind of PDF ready and accessible because we are going to need that as we build out this flow okay so let's start building the flow here and kind of go through all the different components see how to connect them and build this application so I'm just going to grab a few components from the sidebar here and bring them in and we'll start building out this app and kind of talk through it as we go so I'm going to go into inputs and notice that you can also just search for stuff up here as well so for inputs I'm going to grab a text input I'm going to grab a prompt and I'm also going to grab a chat input now let's zoom out a little bit so that we can see all of these and if you kind of hover over them or you look at them it tells you exactly what the component is meant for so what I want to do is I want to get some text input which is the user's name now this is how we'll understand which user is asking which questions and store that in some kind of chat memory so for the text input here I can actually change the name of it so I'm just going to call that name by clicking on edit and then for the value this will be something the users expected to input so we don't need to actually put anything here now what we'll do is we'll take that and we'll pass that as the sender name so notice I'm taking the output here which is whatever they type in and then that's going to be the sender name into my chat input now the chat input is going to be the question that the user types so they can just type whatever they want here I'll show you how we run that in one second and then we're going to take that and we're going to pass that into our prompt now the prompt here is kind of like a prompt template where we can actually write out a few different variables that we want it to accept and then we can kind of structure how we want to ask the llm question so what I'll actually do is I'll just click on this link here and I'll delete it because I actually don't want to do that I'm going to click on template and I'm going to write a simple template and I can embed some prompt variables using my curly braces so I can say something like hey answer the users's question based on the following context and then I can say the context is this and I can pass inside of here context and now that's going to be a prompt variable that this can accept next we're going to say the users question is this and then this will be the question and then we'll have the message history as well we'll say and this is the message history and we'll say that that is history and we can adjust this if we want to make it better but let's say hey answer the question based on the following context we'll say context message history question okay looks good to me let's check that and then notice these variables now appear here in our prompt and I can pass them in so for the chat input that's going to be this so that'll go as the question and then for the context we need to provide that that's going to be from our rag retrieval augmented generation we're going to use a vector store database for that then the history is going to come from our chat history so how do we do that we let's bring in our history here I believe it's message history that we're using or actually it's going to be memory sorry we're going to have chat memory so this retrieves store chat messages with a specific session ID and the session ID is going to be the name of the user that's interacting with the model so we're going to take the name and we're going to pass that as the session ID so we'll link that up right here and the idea is whenever we change the name then it will change the memory that we're using so if we were talking as Tim and then we change it to Sarah well then we get the appropriate memory uh that the user was typing in before so it kind of remembers who's saying what that's the idea here so we're going to take that and that's going to be our history and now that we have that we can actually pass that to an llm and then we can pass that to some chat output and we can test this before we actually implement the rag component which we'll do in a second okay so how do we do that well we need to pass this to an llm so actually if we go to models here you can see there's a bunch of different models that we can use now I'm going to use open AI just because that's the simplest for this tutorial but you can use olama you can use Vector AI hug face API there's a bunch of free versions here and one version you could use is AMA you just need to set that up locally and then you can run your own local llm again in our case we use open AI but you can literally just Swap this llm with any llm on the left side here that you want to use okay so what we'll do is we'll take the text and we'll pass that as the input here to the llm now what we need to do is we need to get an open AI API key so let's do that and then we'll move on to the next step so in order to get this API key we do need account with chat GPT or with openai the URL is platform. open.com ai Keys now they've changed this recently where you do actually need to fund the account so you will need to put like a dollar or $2 into the account to be able to use this again if you don't want to use this you can use any other llm or API that you want but open AI right now is really just the simplest way to do this so you need to go to API keys and then create a new secret key and then copy that key so we'll make a new key here I'm just going to call this tutorial like that and then you can give it uh Scopes but in my case I'm just going to give it access to everything it's going to give me some key that I will delete afterwards let me copy that and then we can utilize that key now just to show you here if you want to see if you're able to actually use this or not you can go into usage and it's going to show you how much usage you have so I've used this a little bit today and then you can click on increase limit now when you click on increase limit it's going to bring you to this page right here you'll have the ability to set a monthly budget uh and you can you can kind of go through all of this stuff here then you also have the ability to buy credits and to fund the account so what I can do is go to buy credits here and when I go to buy credits what I can do is I can fund the account with a certain amount of money in my case I just put in $5 and you can do that from your credit card balance you can have it auto recharge Etc they've changed a little bit of how this works but if you're getting an error saying that you've reached your quota it's because you do need to fund the account with a little bit of money for it to actually work properly if you guys know another way to get around that that's great uh but in my case that's what I had to do okay so now that we have the API key I'm going to go and I'm going to paste my API key here into the open API key or open AI API key and what I can actually do if I want is I can make a new variable I've had some variables here before but I'm going to make a new one I'm going to call this open AI key new and then I'm going to paste the value here and I'm going to make this of type credential this is automatically going to be encrypted and then I can just utilize that variable so I'm going to go with that here in case I need to use this later on my program okay so now that we have that we're passing the prompt to the llm now we need to take the output from the llm and actually display that to the user so to do that we're going to go to outputs and you see that we have a chat output now this will display a chat message in the interactive panel so we'll just take the text and we'll put that to the message and the sender name will simply be AI okay so I know it's a little bit small but you guys get the idea we have our chat output llm prompt chat input name and chat memory and now now what I can do is click on run here and when I click on run you can see that I have a name so I can enter something like Tim and I can ask this a question I can say hey how is your day going question mark and when I hit on enter you can see it shows my name and then it responds my day is going well thank you for asking how about yours perfect so we have a basic llm pipeline set up now what we need to do is Implement rag retrieval augmented generation so we actually load in the PDF and then we're able to answer questions based on it now in order to do that we need to set up a vector store database so let me talk about how that works all right so I'm now on the data Stacks website where they're talking about their Astra database which is a nosql and Vector database specifically for generative AI apps now this is a really fast way to retrieve relevant information while we're building things like retrieval augmented generation pipelines which is what we're doing now I just want to quickly explain how this works and what a vector database actually is so when we build this rag app what we're trying to do is take relevant piece of information from all of our context in this case it's a PDF document and inject that into the prompt alongside whatever the user asked that way the llm has relevant data to actually answer the question however in order to do that we need to really quickly be able to retrieve the relevant data and we need to get the best match we possibly can so the process goes like this user types something in we then go to the vector database and based on what they typed in we search for things that are relevant to that we get that information we inject that into the prompt we also pass the user's question then the llm gives us some response but first we need to of course take all of our data we need to turn it into vectors and we need to put it into the vector database so that's what we'll be using data stacks and their Astro database 4 this runs on Apachi Cassandra and again it just provides a really really quick way to get relevant information and inject that into the prompt when I start building this out I'll talk about it more but for now go to the link in the description create a free account and I'm going to show you how we can spin up a new database that we can then connect to from langlow so once you've made your account and signed in you should be brought to a page that looks like this now what we're going to do is look for this button that says create a database we're going to click on it and we're going to go with the serverless vector database which is exactly what we want for this app now you can name this anything that you want just keep in mind that you can't change it so I'm just going to call this langlow tutorial then we can choose our provider I'm going to go with Google cloud and in the US East now obviously if you want to use this in production you can upgrade your account but in our case we just want something simple that we can test with for the video which is free okay so it's going to make the database that'll take a few minutes once it's done I'll be right back and then I'll show you how we get the information that we need to connect to this from LF flow all right so now that this is loaded we're good to go and we're just going to be looking for the token and then the end point here as well as a collection name but we can deal with that later so let's leave this open and let's go back to langlow and let's start creating this connection so what we're going to do is we're going to go to the chat input and then once we have that input we're going to pass that to a vector search and that's going to look through our database and find any of those relevant pieces of information and then inject that in the prompt but before we can do that we need to build the database with that information so let's do that down here going to zoom in a bit and on the left hand side here we're going to go to our Vector stores and we're going to take in astrab which is what we're using what we're going to do here is we're going to put in the AP endpoint the token Etc okay so we're going to go to data Stacks Astra I'm going to copy in the endpoint and I'm going to make a new variable for this so I'm just going to go here and click on variable let me actually just delete the ones that I had before just to make sure we don't get messed up here okay get rid of all of those and let's make a new one here and call this endpoint okay we'll just make this generic because we don't need to hide that and then we can save the variable okay so let's put our endpoint here for the API endpoint sorry for some reason that went for our token then we need our token so let's generate the token okay let's copy that close go back here and let's make a new variable for the token and we're going to call this our Astra uncore token we're then going to paste the value here and we're going to make this a credential okay so let's set that as Astra token and then for the collection name we're going to make a new variable okay so let's do that here and we're just going to call this collection and the value of this is just going to be PDF because we're storing some PDFs and this will be generic okay so now we'll set that to our collection so now we have our token our endpoint and our collection what we now need to do is we need to pass some inputs to this and we need to pass the embeddings now the input is going to be the file that we want to load or the files that we want to load and the embeddings are going to be how we vectorize this okay so let's first start with the embeddings the embeddings will be here so on the side click on embeddings and we're going to go to open AI embeddings now leave this as the default text embeddings and for the open AI key we can just set this to our open AI key new which is the variable we created we're going to drag this and connect the embeddings and now what we're going to do is pass our inputs so for the inputs we're going to get a file so we can just search for one here this is a generic file loader so let's load in our file file and we're going to find the file in my case it is on my desktop it's in this folder called restaurant Q&A obviously you put yours wherever youve put it but locate it and then load it in and then we're going to pass this into our text splitter so let's find that this is the split text okay let me give us some more room here all right sorry so what this split text is going to do is it's going to take the file it's going to split it into a bunch of different chunks it's then going to pass it to the Astro database where will then be converted into vectors so rather than just passing the entire file at once we want to split it into small different chunks then we pass it to Astro database along with our open Ai embeddings and the embeddings is a special model that actually does this kind of search for us right so it will convert things into vectors and then allow us to compare the vectors for similarity and get the results back that we want now we're going to be using the same embedding model for actually creating the database as well as for searching in the database which you'll see in a second so I'm going to take my file and I'm going to pass that as the input here to my split text we don't need to change any of these settings but we can if we want to and then we're going to take this record and we're going to pass that to the inputs to Astra database so that's it this will actually now create a new collection in Astra database called PDF or whatever we named it it will do that by taking in this file splitting into chunks passing it here and then embedding all of them as vectors okay perfect now what we're going to do is we're just going to take this embedding and we're just going to make this a little bit easier to view because we're going to use this embeddings up here as well I know this is difficult to see um I'm going to try to zoom in as much as I can okay so now what we want to do is after we have the chat input we want to pass that into a vector search where we then go and look for all of the relevant information so let's move our prompt up here and let's get our Vector search so we're going to have our Aster database search which is right here let's zoom in so we can see this for the token this is going to be our variable so this is going to be the AST token for the endpoint this is going to be our variable endpoint and for the collection name it's going to be the collection that's why we made variables now for the input value that's actually going to be the chat input so whatever the user typed that's the input here to our Astra database search and then the embedding is going to be the one that we had here so we'll just connect those embeddings and give me a second I'm going to clean this up just so it's a little bit easier to see okay so I just clean this up a little bit so it's a bit more organized and now what we're going to do is we're going to take the output from our astd database search and we're going to connect that all the way up here to the content for our prompt so remember we have the content the history and the question we already have the chat history and we already have the question the last thing that we need was the context and that comes from our database search Okay so let's quickly run through this at a high level first thing we do is we get the name from the user we then pass that into the chat input and that's kind of like the name of the person who said send the message then we ask them hey type something in this is their question then we connect this to chat memory the chat memory is just so that we can store what the last in this case five messages were that the user was typing and then we pass the chat input into the prompt now as well as that we pass the history which is our chat memory and then we pass the context from our Vector search so our chat input goes to the vector search we then embed that using the open Ai embeddings and we look for any relevant information from our PDF F or whatever documents we put in here and then we get that and we pass that to the prompt so that's what database search does it searches inside of this Vector database for any relevant information gives it to us and then injects it in the prompt or that's how the prompt works here then we take the prompt we pass that to open AI then open AI gives us some output and we print that out meanwhile over here we have our file we load in the file we split it into a bunch of different chunks we then take that and we pass that to our Astro database so we can create this database and we do that using the same embeddings that we will use for our database search so now if we want to run this we can click on run let's just erase all of this and let's ask it some kind of question hey can you tell me the hours the store is open and let's see if this works so I was just playing with this and it's working properly one thing to note you do need to ask it one PRT first that it's not going to give you a good response for because it just needs to build that Vector store so before the vector store is built it's not able to actually give you any valid response because it doesn't have any context so you're not seeing it on the screen cuz I cleared it and then did it again but the first question I asked it didn't give me a valid answer then the second question it worked because it actually ran this secondary component chain that we had down here which actually built out that Vector store so last thing I want to show you here is that we can change our name and when we change our name what that does is it resets the message history so I'm now talking talking as Joey and I'm going to say hey have I asked you anything yet and you're going to notice here that it's going to say no and the reason for that is that the last person that was talking to this was Tim not Joey so when we change the name it changes the message history which allows us to have kind of our own log based on who's asking questions so it says no you haven't asked anything yet and then we can say okay cool thanks whatever right and then we'll track this in Joey's message history whereas Tim's message history will be different kind of just an added feature that I wanted to add to this project so the last thing I really quickly want to mention is that you can Import and Export these just as Json so what I can do is go here I can change the name obviously and then I can export this I can give it a name name something like Rag and then I can download it as Json and then if I already have one that I want to import I can just go back here I can load it up so let me just go to my downloads and I can just take this and just drag it in and sorry need to actually be in the correct thing so if I drag it here you can see that when I do that it just loads it in so that's as easy as it is to actually load in the Json and to share with other people so what I'm going to do is I'm going to have this Json and I'm just going to leave it in the GitHub repository Linked In the description so you can just drag it into Lang flow and you can play around with it there you are guys I hope you enjoyed this video If You did leave a like subscribe to the channel and I will see you in the next one oh
