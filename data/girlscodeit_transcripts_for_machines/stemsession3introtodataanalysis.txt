hello everyone my name is andrea and i'm here with john may and today we'll be um introducing a data analysis for scientific research but you can really um do this wherever you want and so i'll be using python and so what is data science i like to think of data science as telling a story so if we look at right here we can see that it's just um a bunch of numbers thrown at you but to me i don't know where i'm supposed to be looking at where i'm supposed to be focusing my attention to so data science is converting raw data into something more visually appealing like a graph and so from this graph we can see or tell a story we can find patterns trends relationships like for example we can see how transmittance is affected by different wavelengths so from graphs or from data visualizations we can find answers that we didn't even know existed if we just look at our raw data so there are two main types of data we have qualitative data which is talking about like a quality of something and this is based on observations or personal experience and so delving deeper we have nominal data which just stands for basically data that has a name and it doesn't have any certain order so for example we have gender or religion and etc and then we have ordinal data and um the first three um letters kinda reminds me of order so this has data this is data with some like sort of sequence or order so for example we have like letter grades right like you have a b c d and f so these have some sort of order to it but it's still qualitative data and then we have quantitative data which is just basically numbers and so we have discrete and these are whole integer numbers and therefore counting so example would be 14 toddlers and for discrete you would just have like dots on the graph um so it can't be connected and but for continuous that's any value that can be measured and so here we can have decimals an example would be like temperature and over here we have an example of qualitative data which is nominal because it doesn't have any sort of order comedy does not have a higher precedence then let's say trauma so now it's your turn to try for the following determine if it's qualitative or quantitative and then determine what type so go ahead and pause the video okay so for the color of m ms we ask ourselves is it a quality or is it number and the color is the quality and then we ask is it um nominal or ordinal so does it have order or is it just like a name and it's nominal because like let's say orange does not is not a greater importance than let's say brown um for the number of like white m m's that's fine uh this is a number so it's quantitative and is it discrete or continuous so hypothetically we can only have like whole m ms so this is discrete and for customer satisfaction from 1 to 10 this is qualitative and you might be asking well why isn't it quantitative because it's 1 to 10 that is true but if we just look at customer satisfaction we can't really measure that as a number it's more like a quality like if something is bad or satisfactory or um even excellent so that's qualitative but because we have a 1 to 10 then it's original because it has some sense of order state 1 is being worst and 10 is being the best and for monthly profit this is quantitative and it is continuous because we can have decimals okay and so to help us actually do something with the data we can take advantage of python libraries and python is an excellent language to use for data science because we can use a more wide variety of libraries and the first i'll only be talking about the first five but keep in mind for this session we'll only be using numpy and matplotlib so for pandas pandas is a python library that is very easy to use and we take we mainly take data from csv files and from this csv file we can add delete merge or manipulate data frames and a data frame is basically like a spreadsheet or a deity csv is a data frame and then we have numpy and numpy is ideal for arrays specifically arrays with the same or consistent data type and so we can add multiply uh reshape even flatten arrays and for psi pi this is a more advanced math so here we can deal with ordinary differential equations uh signal processing integration and etc and then for matplotlib this deals with a visual representation of data and so we can create line plots bar graphs etc and matplotlib gives you the opportunity to like create labels for your x and yaxis and even like legends and seaborn is an extension of matplotlib so this deals with more advanced statistical graphics like correlation linear regression and multiplot grids and then we also have stats models poppy vocals scikitlearn and cares there are several things we should do to make sure that the data is clear and tidy enough to read by the computer be read by the computer and the process is called data preprocessing there are two steps in data preprocessing and the first is called data cleaning with the first small step called dealing with the missing data so the missing data usually appear when we are combining more than one data sets for example there are two materials that i selected for my scientific research the first is the silicon and the second is the silicon dioxide and we can observe that the wavelength of these two materials does not match with each other the first wavelength for both is 300 yet the second wavelength for silicon is 302 the second for silicon dioxide is 310. so in order to make sure that the two materials matches with match with each other we need to make some changes for the first file and i have delete 302 to 308 leaving only 300 and 310 like what i've shown in the third file to match with the silicon dioxide this is the first solution that i put in pbt which is deleting the mismatching data before merging the two data sets and the second solution is to add the missing data based on the conclusions yet bear in mind that the second solutions have risks because it's kind of like making up a data on your by yourself and you cannot guarantee the accuracy of the data and it might leads to the bias of the final conclusion or maybe the inaccuracy of the final conclusion so you need to pay a lot of attention if you're choosing the second solution and the second small step in data cleaning is called dealing with the noisy data noisy data is are a set of data that are in a large number and sometimes include meaningless data so in order to solve the problems of having large number of data we need to make some several small changes for example one change is called the binning the bringing means that we separate the data into smaller groups which is called beans for example for the first materials that i've chosen the silicon we have the wavelength actually from 300 to 1200 so it's about 900 data and it is impossible for us to plot all those dots on one single graph i mean it's possible but it may looks a bit messy and we cannot draw conclusions relatively easily so in order to help us draw the conclusion more easily we actually combine like the data beginning at 300 maybe to 400 as one group or s1 bing and 400 to 500 as another group another being so in this way we only have about eight or nine a group compared with previously we have about yeah 900 dots 900 groups so by only having eight or nine groups we can make the graph easier and also we can draw the conclusion from the graph easier the second solution selecting the key data is to solve the problems of having a lot of minimalist data for example let's look at an excel file let's say we have seven categories of the data from delta p to negative systematic arrow yeah let's say we only need delta p and positive statistical arrow to plot this graph so we can just simply ignore other categories like um that in second column the third column the fourth column um the sixth column the seventh column the eighth column we can just all ignore them by merely selecting the first column and the fifth column we can plot a really clear graph with these two categories maybe the xaxis is delta p the first column and the yaxis is the positive statistical arrow the fifth column and this is what we called selecting the key data and the second solution for dealing with noisy data um there's a thing that needs to be paid special attention which is we need to be really really careful with the outliers outliers aren't noisy data they can they are either the inaccurate data or data that deviates so much from the general distribution and we have other steps to deal with them for example we can just simply remove them or cap them or we can replace them with a new data and so on but the process is are different from how we deal with the noisy data so we need to bear in mind with that and here comes our second step in data preprocessing the data transformation one of the basic small steps in data transformation is called normalization a normalization is the calculations we use to guarantee that the data sources used are in the same unit system so we need to employ normalization and while we are combining more than one data sources like the circumstance that we encounter in the last slide when we are doing dealing with the missing data so let's take an example let's say we have those green dots in the plots that represent this a certain data that relates to um proton proton collision we also have the gray lines that relates to the data stat about the proton proton coalition however by looking at this part we can figure out that the unit for the green dots does not match with the unit of the gray lines the first is the number divided by being width times another number in bracket and the second is just simply number in bracket so what i have done in order to unify the two units is that i i just make all of the data's in the gray lines divided by the bandwidth times another number which is done by um yeah down by the black figure that i put forward this is the python code so after i implemented this python code i achieved transforming the unit of the gray lines from number in bracket to number divided by being width times another number in bracket in this way we can see that the gray lines matches with the green dots pretty well and this is the result of unifying the unit systems of the two sets of data so after data preprocessing we can now import the data into the python ide yet there is also a small step we need to do before we finally import the data into the ide which is transforming the data format so let's say for the excel file presented in this ppt it's really hard to read by the ide yet for the text file on the right side the ide can read it really clearly so what i have done to make the raw data in the excel file to be read by the python ide is to copy and paste the data from excel file to the text file and split each data with a comma or a space is okay so in this way we have gained this text file and this text file can be easily read by the python ide and this process is called cleaning the data formats so after we clean the data formats we can finally read the data in the python ide and i have put forward two python lines to read the data and there are lots of lines other than these two since python have so many expressions on one function so the reason why i put forward these two python lines is just because i use these two yeah there are some slight differences between these two lines so for the first python lines the first data it gains from um the data file is zero yet for the second python line the first data is just 0.471 and it makes us way more convenient to read all those datas um because for the first example since the lines can only reach the data one digit by one digit we need to make some other python codes to separate each real data like 0.471 from 0 from 0.942 by manually typing in those python codes to identify digit from comma or from space and so that's way more difficult so in most circumstances i just choose the second python line because it makes me more easy in selecting each data since the data that selected automatically by this partisan line is already a complete real data and the word data in these two python lines is actually numpy array and we don't have to worry about what what does it mean because we haven't learned it from our girls coded courses yet we the only thing that we need to know in this session is that data a br in bracket b in bracket just simply means that the data in the eighth line a and in column b yeah that's all we need to know in this session so after we finishing importing the data into the ide now we need to have a process to transform the data into something like some information and insight like i've mentioned in the first slide in order for us to reach a conclusion by looking at insight figuring out some patterns between x axis and y axis and so on so um one way to visualize the data is to plot all those data in some graphs and the two most typical graphs are histogram and scatter plot of course we have a lot more data like the color map the bar chart the pie chart um yet here i've just mentioned histogram and scatterplot as an example and you can figure out all those other graphs by just searching on the matplotlib in bing or google so below the small the small lines are the python lines to plot a histogram and the python lines to plot a scatter plot and if we look at what matplot lifts at here the ax the beams the ranged densities are all parameters of the histogram in the maple lip so let's say we have a data of some ages like age 10 h12 h13 and we can just put those ages in x which is the numpy array and by simply putting the x in the histogram this this python line will help us automatically generate a histogram of the distribution of the age yeah so we can check all these parameters on the map lib website right here so we have the x4 array the bings for angel sequence or stream of this is an optional parameters so whenever you come across the word optional in the parameters it means you can choose to put it in your python lines or you can just simply ignore it like if you need things in your histogram you just you can just write beans in your python lines if you do not need that you can just ignore it yeah and we have range density weights cumulative and you can figure out when or when to use it or not to use it by reading the small words beneath the parameters so the results just shown on the right side the gray line is actually histogram and it's because that i have used the parameters of um some parameters that just just delete the shadows of the histogram away so we can only see the gray lines right here and this is the result of myself implementing the python code of the histogram and it's similar for the scatter plot like you have these python lines right here and you can choose the parameters that you need and we should notice a difference between the scatter plot and histogram the difference is that in the scatter plot we have both x and y yet in histogram we only have x it's because that for each dot in a step at a plot the dot includes both x value and y y value like for this green dots we have x value of 0.5 yet we also have y value about 0.28 or so yet for the grade lines which is the value in the histogram the only thing we have is the x value and for the yaxis it represents the frequency that for each data value on the xaxis for the data sets of the gray lines so we need to distinguish the difference between the x and the x and y and the histogram and the scatter plot so that's a basic introduction of the data visualization and you can check more information about that in the matplot lib website and yeah if you're interested in that you can even try it in the python ide by yourself so nice work for all this session and if you're going to do any scientific research and in the future i hope that the information that i and andrea presented in this session can help you and can be really practical for your future uses and that's all thanks for your listening