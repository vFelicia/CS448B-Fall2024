With timestamps:

00:00 - hello and welcome i'm your code monkey
00:02 - let's learn how to use machine learning
00:03 - ai in unity using ml agents to create an
00:06 - ai with vision that can actually see
00:08 - game engines are actually perfect for
00:10 - doing ai vision since you can actually
00:12 - create a virtual world
00:14 - in the real world in order to get a
00:15 - visual input you need an actual camera
00:17 - pointing at the world and capturing some
00:19 - photons
00:20 - however in a game engine you literally
00:21 - just press a button and create a camera
00:23 - that can render the image and what it
00:25 - sees
00:25 - now i've got two interesting demos here
00:27 - first a very simple one where birds jump
00:29 - around and the ai sees them and shoots
00:32 - and another interesting demo where the
00:33 - ai is hurting animals by moving around
00:35 - the map and identifying the sheep and
00:37 - the pigs
00:38 - the second demo also combines both
00:40 - classic ai and machine learning ai in an
00:42 - interesting way so stay tuned
00:44 - to the end of the video i also covered
00:45 - the complete getting started guide to
00:46 - machine learning with unity ml agents in
00:49 - another video so go watch that if you're
00:50 - not familiar with the tonkit
00:52 - machine learning in unity is actually
00:54 - very simple and easy to use once you
00:55 - understand the basics
00:57 - there's a complete playlist in the
00:58 - description and adding vision to your
01:00 - ais is also quite simple as we will soon
01:02 - see
01:03 - it's really just as easy as adding a
01:05 - component to your agent
01:06 - learn all about vr and ar with the
01:08 - patreon sponsor
01:10 - xr bootcamp it's a six to eight week
01:12 - bootcamp taught by industry
01:13 - professionals
01:14 - learn how to interact in vr optimize
01:16 - your rendering and learn about dots
01:18 - check them out at xrbootcamp.com and use
01:20 - the coupon cm10 to get 10 off any of the
01:23 - master classes
01:24 - so vision is awesome and really
01:26 - intuitive to understand for a human
01:28 - being
01:28 - but the one big downside that is
01:30 - inherent to using vision for machine
01:32 - learning is simply due to the scale of
01:33 - the pro
01:34 - for example in the very first machine
01:36 - learning video where i made an agent go
01:38 - to a go i simply needed 6 observations
01:41 - then on the flippy bird agent i used 9
01:43 - raycasts to detect two types of objects
01:46 - for a total of about
01:47 - 40 observations and on the card driver
01:50 - which also used raycast it had a total
01:52 - of about 50 observations
01:54 - and here i'm using a camera sensor with
01:56 - a size of 50 by 50
01:58 - meaning for a total of 2 500
02:00 - observations
02:01 - so that's a lot higher than all of the
02:03 - other examples which means a lot more
02:04 - time is needed for training
02:06 - that's why those big vision-based
02:07 - algorithms have to be trained on massive
02:09 - gpu farms
02:10 - so with that in mind let's first see how
02:11 - we can use this awesome tool
02:13 - so here is my first demo we can see
02:15 - birds jumping around from both sides
02:17 - and right now i'm playing so i can click
02:19 - anywhere in order to shoot
02:20 - now naturally the goal is to actually
02:22 - shoot the bird so as the bird comes i
02:24 - click on it and if there you go i've got
02:25 - a hit and i can shoot all the birds
02:27 - so as you can see very simple demo so
02:29 - here i am in the editor and here's my
02:31 - agent
02:32 - everything as you can see is pretty
02:33 - basic so if you've seen the getting
02:35 - started video then all of this should be
02:37 - familiar
02:38 - so it's got the behavior parameters and
02:39 - the script which runs the agent
02:41 - a decision requester and then here is
02:43 - the new one
02:44 - we have the camera sensor so what it
02:46 - does is exactly what the name implies
02:48 - so it takes a camera input and uses that
02:50 - as the visual observation
02:52 - so i can see the camera that i'm using
02:54 - it's this one and here this camera is
02:56 - set up to view a simplified version of
02:58 - the scene
02:59 - so the main thing is over here on the
03:00 - culling mask this one is set to only
03:02 - render objects on the vision camera
03:03 - layer
03:04 - so over here we can see the camera
03:05 - preview and as you can see it's all in
03:07 - black except for white for where the
03:09 - bird is
03:10 - then on the burned object here we have
03:12 - just a basic sprite renderer with a
03:13 - white pixel
03:14 - and it's on that layer so if we play the
03:17 - game and look at the camera
03:18 - here we can see what the ai is actually
03:20 - seeing so as you can see it's constantly
03:22 - seeing as the bird jumps in and jumps
03:23 - out
03:24 - so this goes back to what i said about
03:26 - the total size and total amount of
03:28 - observations
03:29 - when dealing with visuals it's very
03:30 - important to simplify your vision as
03:31 - much as possible
03:32 - although of course as with anything
03:34 - related to machine learning brute force
03:36 - is always a possibility
03:37 - so if i had a massive amount of compute
03:39 - processing power i could just use the
03:41 - standard camera view in 1080p and full
03:43 - color
03:44 - and eventually after enough time the
03:46 - machine would actually learn
03:47 - but in here i just have my humble
03:49 - machine so simplifying the visual made
03:51 - it so that the training time didn't take
03:52 - hundreds of hours so back into the
03:54 - sensor let's just look at the other
03:56 - parameters
03:57 - so we can see the width and height for
03:59 - the visual size
04:00 - again remember the bigger the image the
04:02 - longer it won't take to train
04:04 - then we also have a toggle for grayscale
04:06 - which again if you sound like grayscale
04:08 - then what ends up is you have one
04:10 - channel so essentially just one float
04:11 - per pixel
04:12 - whereas if you go with color you end up
04:15 - with three floats per pixel
04:16 - so with a 50x50 image if it's in
04:19 - grayscale you have 2500 observations
04:22 - and if you change it to color then all
04:24 - of a sudden you have 7500 observations
04:27 - so it's a ton more with a ton more
04:29 - variation once again assuming you're
04:31 - trying to train your agents in a
04:32 - reasonable amount of time with a
04:33 - standard machine
04:34 - you should really simplify things as
04:36 - much as possible so here i'm giving the
04:38 - ai a simple 50x50 image with a
04:40 - simplified grayscale view of the
04:42 - scene then for the actions up here you
04:45 - can see that they are set up as discrete
04:47 - with just two possible actions and since
04:49 - the camera has a width and height of 50
04:51 - and over here for the branch size i'm
04:52 - giving them 50 of each
04:54 - so essentially the ai is pretty much
04:56 - just working on a grin
04:58 - here on the agent code we can see what
05:00 - happens on the on action received
05:02 - and we have our two discrete actions so
05:03 - one of them is for the position x
05:05 - another one for the y
05:06 - so he simply gets the x and y and tries
05:08 - to shoot it and if it does hit the
05:10 - target then it gets a positive reward
05:12 - and ends the episode
05:13 - and if it does not hit the target then
05:14 - it gets a negative reward just to
05:16 - encourage you to hit the target as soon
05:17 - as possible
05:18 - so this is really all it takes in order
05:20 - to train this agent so i've got this
05:22 - very simple very small script
05:24 - and then here just the normal ml agents
05:26 - components
05:27 - as well as the extra camera sensor and
05:29 - again for training i use a manual
05:31 - curriculum
05:31 - essentially teaching it bit by bit just
05:33 - like i did for the flipping bird agent
05:35 - so i started off with a static very
05:37 - large target then when it learned that i
05:40 - increased the difficulty by making the
05:41 - target smaller
05:42 - here is the training graph as you can
05:44 - see it took quite a bit of time to learn
05:45 - even though i started with a very easy
05:47 - scenario with a large target
05:48 - so once again this is the difficulty
05:50 - when dealing with vision
05:52 - ai but over time it didn't learn and
05:54 - then when i added the movement it didn't
05:56 - even need to train anymore
05:57 - i don't really learn to hit the white
05:59 - target so movement doesn't really matter
06:01 - and over here is the result so as soon
06:03 - as the bird spawns it gets shot pretty
06:05 - much almost immediately
06:06 - so no matter where it comes from the ai
06:08 - always hits it
06:09 - now when training the ai i actually made
06:11 - a small mistake so
06:12 - here in the actions as you can see i
06:14 - just set up to receive two actions so
06:16 - just an x and y
06:17 - meaning that the ai has no chance to
06:19 - simply not
06:20 - shoot so whenever it takes a decision it
06:23 - always has to shoot somewhere
06:25 - so right now the way that it's set up
06:26 - with a decision requester on a period of
06:28 - 20
06:30 - this works just fine that's about enough
06:31 - time for the bird to spawn and get
06:33 - inside of the view however if i pull it
06:36 - back and tell it to shoot on every
06:37 - single frame then yep there you go
06:39 - that's the issue since once again i
06:40 - didn't train the ai to be able to not
06:42 - shoot
06:43 - so it always shoots somewhere and when
06:44 - it doesn't see any white pixel then just
06:46 - start shooting randomly
06:47 - so what i should have done is instead
06:49 - add another action for either shoot or
06:51 - don't shoot
06:52 - okay so this is one way to add vision to
06:54 - your ai you add a camera sensor and
06:56 - assign a camera
06:57 - but there's one more way if you go into
07:00 - add a component
07:01 - and in here you browse inside the ml
07:03 - agents
07:04 - you can see we've got our behavior
07:06 - camera sensor decision and so on and all
07:08 - of here
07:08 - we have the render texture sensor so
07:11 - functionally this works pretty much
07:12 - exactly the same
07:13 - you would create a render texture then
07:15 - for example make this camera render onto
07:17 - that render texture
07:19 - and then use that render texture in here
07:20 - and as you can see we have the other
07:22 - parameters as well then just for these
07:24 - last two which i didn't mention
07:26 - previously
07:27 - for the observation stacks this one is
07:29 - how many frames won't be stacked before
07:31 - being fed into the network
07:33 - meaning if you set it to just one then
07:34 - network will take a decision based on
07:36 - only a single frame whereas if you put
07:39 - it to more for example to 10
07:40 - the network will be asked to make a
07:42 - decision based on the information of
07:44 - these 10 frames
07:45 - so for example if you send more than one
07:47 - the ai won't be able to infer some sort
07:49 - of direction from the white pixels
07:52 - but once again that has a training cost
07:53 - so in my case i don't really care for
07:55 - the velocity so if i set it to one
07:57 - everything works
07:58 - just the same and then finally for the
08:00 - compression you can set it to no
08:01 - compression or png compression
08:03 - this is just to make it more efficient
08:04 - to send the data from unity onto the
08:06 - python trainer
08:07 - if you set it to png then essentially
08:09 - the final observation won't be smaller
08:11 - so that communication will go a bit
08:12 - faster than if you send it as completely
08:14 - uncompressed
08:15 - alright so that's one demo showcasing
08:17 - one way you can use machine learning
08:19 - vision in unity
08:20 - and over here i've got another
08:22 - interesting demo and by the way if you
08:24 - find the video helpful consider
08:26 - subscribing and hitting the like button
08:27 - it really helps out the channel
08:29 - so i've got this ai over here and it's
08:31 - moving around it goes towards a random
08:33 - animal
08:34 - looks at it identifies if it's a pig or
08:36 - a sheep
08:37 - and then sends it to the correct pen so
08:39 - sheep go in here
08:40 - and pigs go in here the models for these
08:43 - animals are from a great asset pack
08:44 - there's a link in the description in
08:45 - case you want to see it
08:46 - and this is also an interesting example
08:48 - because i'm combining both machine
08:50 - learning and classic ai at the same time
08:52 - and for another different thing it's
08:54 - also using manual decisions as to
08:56 - opposed to
08:57 - automatic decisions on every single
08:58 - frame so the movement is handled through
09:00 - a classic ai
09:02 - and the identification of the animal is
09:03 - using machine learning ai
09:05 - so let's look at how it's set up over
09:07 - here i've got my environment
09:09 - and it's got this main script and in
09:11 - here i just got references to all of the
09:12 - various prefabs
09:14 - and on awake we simply start spawning
09:16 - and spawn the initial animals on random
09:18 - positions
09:19 - then on the agent it has a rigid body in
09:23 - order to move
09:23 - and then it also has a basic mover
09:25 - script so here it is
09:27 - it is literally just the most basic
09:28 - classic ai you can think of
09:31 - it just kind of lights a move direction
09:32 - and moves the rigid body towards it
09:34 - so it's a very small very simple script
09:36 - we can set a target position and look at
09:38 - position
09:39 - and the agent goes there and looks
09:40 - towards a certain position so once again
09:42 - this is classic ai there's no machine
09:44 - learning ai used in here at all
09:46 - so over here on the environment script i
09:48 - can essentially tell the
09:50 - agent mover and i tell it to go to
09:52 - target position and look at a certain
09:53 - position
09:54 - and when it gets there that one fires
09:56 - off this event and runs this function
09:58 - which then caused it to take an
10:00 - action so again these actions are not
10:02 - happening automatically like we saw
10:03 - previously on the decision requester
10:05 - instead they are manually so as soon as
10:07 - the agent reaches a position it manually
10:08 - takes an action
10:09 - and then on this take action it simply
10:11 - manually requests a decision
10:13 - so the ai does its thing it processes
10:15 - the visual input
10:16 - and for the visual input it is pretty
10:18 - much the same thing that we saw
10:19 - previously
10:20 - so using a camera sensor and in here has
10:23 - a camera
10:24 - now this camera instead of being an
10:26 - overhead camera like we saw in the
10:27 - previous demo
10:28 - this one is attached to the agent and
10:29 - once again it's set up with a calling
10:31 - mask to only view the vision camera
10:33 - and if we run the game in c so in there
10:36 - we can see what the camera sees so as
10:37 - you can see the sheep
10:38 - and the pigs they have essentially cubes
10:41 - on top of them
10:42 - and they have a flat collar so as you
10:44 - can see the agent goes to a certain
10:45 - position
10:46 - rotates to face the animal and then it
10:48 - simply has that
10:49 - so then it takes a decision and the ai
10:51 - is simply looking at the color of the
10:53 - pixels pretty much
10:54 - so i'm looking at the pig here it's just
10:56 - the normal pig mesh
10:57 - and then it's got just a basic cube set
11:00 - up to an unlit color
11:01 - then for the action it's just a simple
11:03 - discrete action either zero or one
11:05 - and then it tries to select the animal
11:07 - and if it gets the correct one then gets
11:08 - a positive reward and if it's a negative
11:10 - one then a negative reward
11:12 - now for training i just place the agent
11:14 - in a fixed position with no movement
11:15 - then i spawn the random animal in front
11:17 - and sdai to take the decision
11:19 - so it was a very simple training process
11:21 - and now here for this example use case i
11:23 - don't need
11:24 - let's say the shape of the image all i
11:26 - want is pretty much just the color of
11:27 - the pixel in front
11:28 - so in the camera sensor i set it up with
11:30 - the smallest width and height
11:32 - which happens to be 20 by 20. so
11:33 - essentially you cannot have this lower
11:35 - than this
11:36 - now in theory you could obviously
11:37 - accomplish the scenario with just a
11:39 - raycast or by just using the center
11:40 - pixel of the screen as an observation
11:42 - but using vision is a bit more fun so
11:45 - here is the
11:45 - demo in action the agent goes towards a
11:48 - random animal
11:49 - looks towards it and then takes a
11:50 - decision it says that this one is a pig
11:52 - and the pig starts going towards a pig
11:53 - pen that one is a ship
11:54 - and goes in there so it just goes
11:56 - randomly from animal to animal
11:58 - constantly identifying them
11:59 - and it works out pretty much all the
12:01 - time so this is a great example of how
12:03 - you can mix and match both classic ai
12:05 - and machine learning ai
12:07 - obviously you could handle the movement
12:08 - through ml agents as well by using the
12:10 - camera sensor or just some recas
12:12 - but using classic ai works just fine and
12:14 - doesn't require tens of training
12:16 - so before you just decide to solve every
12:18 - problem using machine learning remember
12:20 - that you can always combine both
12:22 - alright so as you can see adding vision
12:23 - to your ai is awesome and very easy to
12:25 - do when using unity ml agents
12:28 - but it does have a pretty significant
12:29 - training cost think of it as a problem
12:31 - with regards to a noisy signal ratio
12:34 - visual observations have a ton of noise
12:36 - and very little signal
12:37 - so in the first demo the aic is this
12:39 - image but the only useful parts of this
12:42 - observation the only signal is just
12:43 - these handful of white pixels
12:45 - everything else is just noise that the
12:47 - ai needs to learn to ignore
12:48 - so vision is awesome but in most cases
12:50 - you really want to simplify your
12:52 - observations to exactly what your ai
12:54 - needs to know
12:54 - one of the benefits of working in a game
12:56 - engine as opposed to the real world is
12:58 - the fact that you can have the complete
12:59 - world state at your fingertips
13:01 - for example when training a robot in the
13:03 - real world in order to identify an
13:05 - object
13:06 - you don't have access to something like
13:07 - the economical state of the world so you
13:09 - need to process the visual input in
13:10 - order to identify the object
13:12 - but when working in a game engine you
13:14 - have complete access to that object and
13:16 - you can identify it easily
13:17 - just fire raycast into a component call
13:19 - that is much simpler to do and will
13:21 - result in significantly faster training
13:23 - time
13:24 - so adding vision is awesome but before
13:26 - you use it make sure there's really no
13:27 - other way to simplify your observations
13:30 - alright so that's machine learning
13:31 - vision ai in unity check the phone
13:33 - playlist link in the description where
13:34 - i'm adding all machine learning videos
13:36 - as long as you can download your project
13:38 - files and utilities from
13:38 - indiegogomonk.com
13:40 - this video is made possible thanks to
13:41 - these awesome supporters
13:43 - go to patreon.com unitycodemonkey to get
13:45 - some perks and help keep the videos free
13:47 - for everyone
13:48 - if you found this video home funk
13:49 - instead liking and subscribing wasn't
13:50 - fair to having comments and i'll see you
13:52 - next time

Cleaned transcript:

hello and welcome i'm your code monkey let's learn how to use machine learning ai in unity using ml agents to create an ai with vision that can actually see game engines are actually perfect for doing ai vision since you can actually create a virtual world in the real world in order to get a visual input you need an actual camera pointing at the world and capturing some photons however in a game engine you literally just press a button and create a camera that can render the image and what it sees now i've got two interesting demos here first a very simple one where birds jump around and the ai sees them and shoots and another interesting demo where the ai is hurting animals by moving around the map and identifying the sheep and the pigs the second demo also combines both classic ai and machine learning ai in an interesting way so stay tuned to the end of the video i also covered the complete getting started guide to machine learning with unity ml agents in another video so go watch that if you're not familiar with the tonkit machine learning in unity is actually very simple and easy to use once you understand the basics there's a complete playlist in the description and adding vision to your ais is also quite simple as we will soon see it's really just as easy as adding a component to your agent learn all about vr and ar with the patreon sponsor xr bootcamp it's a six to eight week bootcamp taught by industry professionals learn how to interact in vr optimize your rendering and learn about dots check them out at xrbootcamp.com and use the coupon cm10 to get 10 off any of the master classes so vision is awesome and really intuitive to understand for a human being but the one big downside that is inherent to using vision for machine learning is simply due to the scale of the pro for example in the very first machine learning video where i made an agent go to a go i simply needed 6 observations then on the flippy bird agent i used 9 raycasts to detect two types of objects for a total of about 40 observations and on the card driver which also used raycast it had a total of about 50 observations and here i'm using a camera sensor with a size of 50 by 50 meaning for a total of 2 500 observations so that's a lot higher than all of the other examples which means a lot more time is needed for training that's why those big visionbased algorithms have to be trained on massive gpu farms so with that in mind let's first see how we can use this awesome tool so here is my first demo we can see birds jumping around from both sides and right now i'm playing so i can click anywhere in order to shoot now naturally the goal is to actually shoot the bird so as the bird comes i click on it and if there you go i've got a hit and i can shoot all the birds so as you can see very simple demo so here i am in the editor and here's my agent everything as you can see is pretty basic so if you've seen the getting started video then all of this should be familiar so it's got the behavior parameters and the script which runs the agent a decision requester and then here is the new one we have the camera sensor so what it does is exactly what the name implies so it takes a camera input and uses that as the visual observation so i can see the camera that i'm using it's this one and here this camera is set up to view a simplified version of the scene so the main thing is over here on the culling mask this one is set to only render objects on the vision camera layer so over here we can see the camera preview and as you can see it's all in black except for white for where the bird is then on the burned object here we have just a basic sprite renderer with a white pixel and it's on that layer so if we play the game and look at the camera here we can see what the ai is actually seeing so as you can see it's constantly seeing as the bird jumps in and jumps out so this goes back to what i said about the total size and total amount of observations when dealing with visuals it's very important to simplify your vision as much as possible although of course as with anything related to machine learning brute force is always a possibility so if i had a massive amount of compute processing power i could just use the standard camera view in 1080p and full color and eventually after enough time the machine would actually learn but in here i just have my humble machine so simplifying the visual made it so that the training time didn't take hundreds of hours so back into the sensor let's just look at the other parameters so we can see the width and height for the visual size again remember the bigger the image the longer it won't take to train then we also have a toggle for grayscale which again if you sound like grayscale then what ends up is you have one channel so essentially just one float per pixel whereas if you go with color you end up with three floats per pixel so with a 50x50 image if it's in grayscale you have 2500 observations and if you change it to color then all of a sudden you have 7500 observations so it's a ton more with a ton more variation once again assuming you're trying to train your agents in a reasonable amount of time with a standard machine you should really simplify things as much as possible so here i'm giving the ai a simple 50x50 image with a simplified grayscale view of the scene then for the actions up here you can see that they are set up as discrete with just two possible actions and since the camera has a width and height of 50 and over here for the branch size i'm giving them 50 of each so essentially the ai is pretty much just working on a grin here on the agent code we can see what happens on the on action received and we have our two discrete actions so one of them is for the position x another one for the y so he simply gets the x and y and tries to shoot it and if it does hit the target then it gets a positive reward and ends the episode and if it does not hit the target then it gets a negative reward just to encourage you to hit the target as soon as possible so this is really all it takes in order to train this agent so i've got this very simple very small script and then here just the normal ml agents components as well as the extra camera sensor and again for training i use a manual curriculum essentially teaching it bit by bit just like i did for the flipping bird agent so i started off with a static very large target then when it learned that i increased the difficulty by making the target smaller here is the training graph as you can see it took quite a bit of time to learn even though i started with a very easy scenario with a large target so once again this is the difficulty when dealing with vision ai but over time it didn't learn and then when i added the movement it didn't even need to train anymore i don't really learn to hit the white target so movement doesn't really matter and over here is the result so as soon as the bird spawns it gets shot pretty much almost immediately so no matter where it comes from the ai always hits it now when training the ai i actually made a small mistake so here in the actions as you can see i just set up to receive two actions so just an x and y meaning that the ai has no chance to simply not shoot so whenever it takes a decision it always has to shoot somewhere so right now the way that it's set up with a decision requester on a period of 20 this works just fine that's about enough time for the bird to spawn and get inside of the view however if i pull it back and tell it to shoot on every single frame then yep there you go that's the issue since once again i didn't train the ai to be able to not shoot so it always shoots somewhere and when it doesn't see any white pixel then just start shooting randomly so what i should have done is instead add another action for either shoot or don't shoot okay so this is one way to add vision to your ai you add a camera sensor and assign a camera but there's one more way if you go into add a component and in here you browse inside the ml agents you can see we've got our behavior camera sensor decision and so on and all of here we have the render texture sensor so functionally this works pretty much exactly the same you would create a render texture then for example make this camera render onto that render texture and then use that render texture in here and as you can see we have the other parameters as well then just for these last two which i didn't mention previously for the observation stacks this one is how many frames won't be stacked before being fed into the network meaning if you set it to just one then network will take a decision based on only a single frame whereas if you put it to more for example to 10 the network will be asked to make a decision based on the information of these 10 frames so for example if you send more than one the ai won't be able to infer some sort of direction from the white pixels but once again that has a training cost so in my case i don't really care for the velocity so if i set it to one everything works just the same and then finally for the compression you can set it to no compression or png compression this is just to make it more efficient to send the data from unity onto the python trainer if you set it to png then essentially the final observation won't be smaller so that communication will go a bit faster than if you send it as completely uncompressed alright so that's one demo showcasing one way you can use machine learning vision in unity and over here i've got another interesting demo and by the way if you find the video helpful consider subscribing and hitting the like button it really helps out the channel so i've got this ai over here and it's moving around it goes towards a random animal looks at it identifies if it's a pig or a sheep and then sends it to the correct pen so sheep go in here and pigs go in here the models for these animals are from a great asset pack there's a link in the description in case you want to see it and this is also an interesting example because i'm combining both machine learning and classic ai at the same time and for another different thing it's also using manual decisions as to opposed to automatic decisions on every single frame so the movement is handled through a classic ai and the identification of the animal is using machine learning ai so let's look at how it's set up over here i've got my environment and it's got this main script and in here i just got references to all of the various prefabs and on awake we simply start spawning and spawn the initial animals on random positions then on the agent it has a rigid body in order to move and then it also has a basic mover script so here it is it is literally just the most basic classic ai you can think of it just kind of lights a move direction and moves the rigid body towards it so it's a very small very simple script we can set a target position and look at position and the agent goes there and looks towards a certain position so once again this is classic ai there's no machine learning ai used in here at all so over here on the environment script i can essentially tell the agent mover and i tell it to go to target position and look at a certain position and when it gets there that one fires off this event and runs this function which then caused it to take an action so again these actions are not happening automatically like we saw previously on the decision requester instead they are manually so as soon as the agent reaches a position it manually takes an action and then on this take action it simply manually requests a decision so the ai does its thing it processes the visual input and for the visual input it is pretty much the same thing that we saw previously so using a camera sensor and in here has a camera now this camera instead of being an overhead camera like we saw in the previous demo this one is attached to the agent and once again it's set up with a calling mask to only view the vision camera and if we run the game in c so in there we can see what the camera sees so as you can see the sheep and the pigs they have essentially cubes on top of them and they have a flat collar so as you can see the agent goes to a certain position rotates to face the animal and then it simply has that so then it takes a decision and the ai is simply looking at the color of the pixels pretty much so i'm looking at the pig here it's just the normal pig mesh and then it's got just a basic cube set up to an unlit color then for the action it's just a simple discrete action either zero or one and then it tries to select the animal and if it gets the correct one then gets a positive reward and if it's a negative one then a negative reward now for training i just place the agent in a fixed position with no movement then i spawn the random animal in front and sdai to take the decision so it was a very simple training process and now here for this example use case i don't need let's say the shape of the image all i want is pretty much just the color of the pixel in front so in the camera sensor i set it up with the smallest width and height which happens to be 20 by 20. so essentially you cannot have this lower than this now in theory you could obviously accomplish the scenario with just a raycast or by just using the center pixel of the screen as an observation but using vision is a bit more fun so here is the demo in action the agent goes towards a random animal looks towards it and then takes a decision it says that this one is a pig and the pig starts going towards a pig pen that one is a ship and goes in there so it just goes randomly from animal to animal constantly identifying them and it works out pretty much all the time so this is a great example of how you can mix and match both classic ai and machine learning ai obviously you could handle the movement through ml agents as well by using the camera sensor or just some recas but using classic ai works just fine and doesn't require tens of training so before you just decide to solve every problem using machine learning remember that you can always combine both alright so as you can see adding vision to your ai is awesome and very easy to do when using unity ml agents but it does have a pretty significant training cost think of it as a problem with regards to a noisy signal ratio visual observations have a ton of noise and very little signal so in the first demo the aic is this image but the only useful parts of this observation the only signal is just these handful of white pixels everything else is just noise that the ai needs to learn to ignore so vision is awesome but in most cases you really want to simplify your observations to exactly what your ai needs to know one of the benefits of working in a game engine as opposed to the real world is the fact that you can have the complete world state at your fingertips for example when training a robot in the real world in order to identify an object you don't have access to something like the economical state of the world so you need to process the visual input in order to identify the object but when working in a game engine you have complete access to that object and you can identify it easily just fire raycast into a component call that is much simpler to do and will result in significantly faster training time so adding vision is awesome but before you use it make sure there's really no other way to simplify your observations alright so that's machine learning vision ai in unity check the phone playlist link in the description where i'm adding all machine learning videos as long as you can download your project files and utilities from indiegogomonk.com this video is made possible thanks to these awesome supporters go to patreon.com unitycodemonkey to get some perks and help keep the videos free for everyone if you found this video home funk instead liking and subscribing wasn't fair to having comments and i'll see you next time
