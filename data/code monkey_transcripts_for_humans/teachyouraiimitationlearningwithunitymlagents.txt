With timestamps:

00:00 - hello and welcome i'm your code monkey
00:02 - and here let's check out the awesome
00:03 - power of imitation learning using unity
00:06 - ml agents
00:07 - first if you're not familiar with ml
00:08 - agents that is unity's machine learning
00:10 - ai toolkit
00:11 - i cover the detailed getting start guide
00:13 - that goes through the whole installation
00:15 - process and how to use it
00:16 - so go watch that video if you haven't
00:18 - already and check the full machine
00:19 - learning playlist in the description
00:21 - ml agent supports two types of learning
00:24 - reinforcement learning and imitation
00:26 - learning
00:26 - now imitation learning is how you can
00:28 - teach your ai directly how to behave in
00:31 - order to achieve a certain goal
00:33 - meaning that instead of trying to work
00:34 - by just randomly testing actions
00:36 - it will instead try to imitate what the
00:38 - player did and build upon it
00:40 - so think of it as if you were trying to
00:42 - play a game in one method you just
00:43 - randomly push every button on the
00:45 - controller until something happens
00:47 - and in another method you have someone
00:49 - who knows how to play teach you what the
00:50 - buttons do
00:51 - and how to push them in the correct
00:53 - order to get the reward now let's first
00:55 - think about the
00:55 - limitations when using reinforcement
00:57 - learning and then we can see how
00:59 - imitation learning is an awesome very
01:00 - powerful tool
01:02 - in the getting started video i cover the
01:04 - most basic type of learning just
01:05 - reinforcement learning where the agent
01:07 - learns how to do something based on the
01:09 - rewards that it gets
01:10 - now that method works great when you
01:12 - have a relatively simple scenario
01:14 - for example in that video we made a
01:16 - simple agent and it wants to touch the
01:18 - goal
01:19 - it's pretty simple so it can easily
01:20 - complete the task it spawns in a random
01:22 - position and moves towards the goal
01:24 - so that one works out great but over
01:26 - here i have another more complex demo
01:29 - i'm currently controlling the character
01:31 - using heuristics so the goal is to get
01:33 - into this button
01:34 - so i can move the character go in there
01:36 - then i press a button on my keyboard to
01:38 - push the actual button
01:39 - and as soon as i do a food pallet get
01:41 - spawned so now i need to go there and
01:43 - touch it and there you go i went
01:45 - now for you assuming that you are a
01:46 - human being this is a pretty simple task
01:49 - even if i don't give you the
01:50 - instructions you can probably figure out
01:51 - after a few seconds
01:53 - but for an ai that randomly pushes
01:55 - buttons until it gets a reward then this
01:57 - scenario is extremely complex
01:59 - just think in order for the ai to
02:00 - complete the whole task first it needs
02:02 - to trigger the right random actions to
02:04 - get it to the button
02:05 - then it needs to again randomly push the
02:07 - button and then again needs to trigger
02:09 - the right random combination of actions
02:11 - that gets it to the food pellet
02:12 - so as you can see that's a ton of very
02:14 - specific random actions that need to
02:16 - happen in a very specific sequence
02:18 - so the odds of that happening based on
02:20 - pure luck alone are extremely
02:22 - tiny so essentially you'd have better
02:23 - luck just getting the ai to play the
02:25 - lottery
02:26 - so that's where imitation learning comes
02:27 - in instead of the ai
02:29 - trying random actions until something
02:30 - happens we teach it how it should
02:32 - complete the task and the i learned from
02:34 - that
02:35 - and just like with reinforcement
02:36 - learning using ml agents to achieve this
02:38 - is quite simple
02:39 - again if you haven't seen the getting
02:41 - started video go and watch that
02:43 - in there i go through all the basics on
02:45 - getting started and logic for how it all
02:46 - works
02:47 - in order to do imitation learning it is
02:49 - pretty much the same thing that we saw
02:51 - for reinforcement learning
02:52 - so over here i have my scene and i can
02:54 - control the character using heuristics
02:56 - so the agent class is already nicely
02:58 - implemented let's look at it so i've got
03:00 - my environment and inside i've got the
03:02 - agent
03:03 - and here it is the various behavior
03:04 - parameters so very simple stuff
03:06 - and let's inspect the food agent script
03:08 - so here is the script
03:10 - and again remember how reinforcement
03:12 - learning works which is first the ai
03:14 - takes some observations of its
03:15 - environment
03:16 - then it takes a decision it makes an
03:19 - action and sees if it gets a reward
03:21 - that's the reinforcement learning cycle
03:23 - over here i have my food agent class
03:26 - and as you can see the code is pretty
03:27 - simple so if you watch the other video
03:29 - then all of this should be quite
03:31 - familiar
03:32 - first up here we have the on episode
03:33 - begin function so this is where we set
03:35 - up the scene with some randomness so
03:37 - that the ai doesn't learn how to solve
03:39 - just one specific set of positions
03:41 - so we randomize the player position and
03:43 - then on the food button script
03:45 - when we reset the button we're also
03:47 - randomizing the position so both the
03:48 - player and the button get random
03:50 - positions
03:50 - that's all we do on the on episode begin
03:52 - then we have our collect observations
03:54 - function
03:55 - so here in terms of observations first
03:57 - i'm giving it the state of the button
03:59 - so if the button can be used then the
04:01 - machine learning algorithm receives a 1
04:02 - and if not it receives a 0.
04:04 - then i'm also giving it the direction
04:06 - towards the food button
04:08 - then another observation for the current
04:09 - state of the food so if it has been
04:11 - spawned or not
04:12 - and if the food has been spawned then i
04:13 - give it the direction towards the food
04:15 - and if not just two zeros
04:16 - so those are all the observations we
04:18 - just have over here one
04:20 - two three four five and six floats
04:23 - then down here for the actions i am
04:25 - using discrete actions to move the
04:27 - character and here they have some very
04:29 - simple very straightforward actions
04:31 - so just don't move move left move right
04:33 - move back or move forward
04:35 - so it's a very basic movement controller
04:37 - over here modifying the rigid body
04:39 - velocity
04:40 - and then another action for the is use
04:42 - button down
04:43 - so when this one is set to one then that
04:45 - is the agent using the use button
04:47 - and when that happens then it simply
04:48 - looks for objects around the agent and
04:50 - finds a button to push so if it can't
04:52 - find a button
04:53 - it tries to push that button so i
04:55 - covered this simple way of pushing
04:56 - buttons in the waste open doors video
04:59 - so those are all the actions and then
05:01 - for the rewards
05:02 - over here we have a reward when it
05:04 - successfully uses the button
05:05 - and then down here when we consume the
05:08 - food we also get another reward
05:10 - and lastly over here on the on action
05:12 - received on every action i'm also giving
05:14 - it a negative reward in order to
05:16 - essentially encourage the agent to
05:17 - finish as quickly as possible
05:19 - now one important thing is that i only
05:21 - added this negative reward to training
05:23 - after it was already succeeding in
05:25 - achieving the task so i didn't start
05:26 - with this one by default
05:28 - essentially in the beginning you want to
05:29 - help your ai to learn and then later on
05:31 - you can focus on actually optimizing
05:33 - okay so here is the script as you can
05:35 - see it's pretty small pretty simple
05:37 - first let's actually look at what
05:38 - happens if we try training our ai
05:41 - using just reinforcement learning and by
05:43 - the way here's a quick tip that someone
05:45 - mentioned in the comments on the
05:46 - previous video if you go into the folder
05:48 - where you have your project
05:49 - you can just go up here into the actual
05:52 - path and in here you type cmd
05:54 - and when you do it opens up the command
05:56 - prompt directly onto that folder so very
05:58 - useful tip
05:59 - now here let's try doing the normal
06:01 - training
06:05 - okay so there it is training is
06:07 - currently running and you can see
06:09 - it's happening exactly as you would
06:11 - expected so the agent is just trying
06:13 - random actions which caused it to
06:14 - randomly move around
06:16 - so again remember in reinforcement
06:18 - learning it's very important to get a
06:19 - reward and in this case the agent is
06:21 - only getting a reward when it pushes the
06:22 - button
06:23 - so when trying these random actions the
06:25 - agent needs to be unlucky enough in
06:27 - order to push the right actions to move
06:28 - it towards the button and then actually
06:30 - push the button
06:31 - so achieving that first part is actually
06:33 - somewhat doable so it might get lucky
06:34 - and achieve that
06:35 - but even if it does achieve doing that
06:37 - then it won't learn to go towards the
06:38 - button and push it
06:39 - but then it's going to have a lot of
06:41 - trouble to go towards the pellet after
06:42 - pushing the button
06:44 - now again you could technically leave it
06:46 - like this and through sheer chance alone
06:48 - then maybe in a million steps you would
06:49 - get lucky and complete the whole task
06:51 - but the odds of that happening are
06:53 - really slim so this is just basically
06:55 - reinforcement learning it tries random
06:57 - actions and hopes that it achieves
06:58 - something
06:59 - so here it already got lucky and hit the
07:01 - button so now it learned to go towards
07:03 - the button and push it
07:04 - but again achieving the whole task is
07:06 - going to be very difficult so now let's
07:08 - give this ai some help and show it how
07:10 - this task should be completed
07:12 - and by the way if you find the video
07:13 - helpful consider subscribing and hitting
07:15 - the like button
07:16 - it really helps out the channel now as i
07:18 - said that one is
07:19 - very simple all we need to do is go into
07:22 - our agent
07:23 - and let me actually go inside the prefab
07:25 - so i modify all the agents
07:27 - so here i'm in the prefab and select the
07:28 - agent and then i just have to go down
07:31 - here and going to add another component
07:33 - go inside ml agents and in here we're
07:36 - going to add a demonstration recorder
07:38 - then here we see some fields so first of
07:40 - all we've got a toggle to enable or
07:42 - disable recording
07:43 - so just go ahead toggle that so we start
07:45 - recording then for the number of steps
07:47 - this is in case you want to unlimit how
07:49 - many steps you record
07:50 - but if you just want to continue
07:52 - recording until you stop the game then
07:53 - just leave it at zero
07:54 - then we've got a demonstration name so
07:57 - let's name this our food
07:58 - agent demo and then directory where you
08:01 - want to save it so let's just name it
08:02 - demos
08:03 - alright so that's on the setup for the
08:05 - demonstration and now hey record is very
08:07 - simple
08:08 - just make sure that the agent is over
08:10 - here set the heuristic okay here is the
08:12 - calling
08:12 - and all i have to do is just run the
08:14 - scene as normal
08:16 - so here i am and now my actions are
08:18 - being recorded so essentially the ai is
08:20 - now looking at everything that i do all
08:22 - the actions i take and all of the
08:23 - observation values as i take them
08:25 - now all i have to do is really just
08:27 - teach it correctly so i guess that means
08:29 - that if i wanted to be mean to the poor
08:30 - ai i would just ram into a wall non-stop
08:33 - and now the poor ai would learn just to
08:35 - slam into a wall now that's probably not
08:37 - very useful so let's actually teach it
08:39 - how to achieve the task
08:40 - so here i just move towards the button i
08:42 - press the keyboard button to
08:44 - trigger the action and now go towards
08:46 - the palette and consume it
08:48 - and now again go into that one press the
08:49 - button and so on and so on
08:51 - and essentially this way the agent the
08:53 - ai is actually
08:55 - looking at me looking at how i achieve
08:56 - this analyzing all the actions that i'm
08:58 - taking and all the
08:59 - various observations so that's pretty
09:02 - much all it takes to record the demo you
09:03 - just complete this as many times as you
09:05 - can
09:06 - essentially the more that i give it the
09:07 - more the agent will learn
09:09 - and then in order to stop recording you
09:11 - just simply exit play mode
09:12 - okay recording is done and if we go into
09:15 - our project files we can see over here
09:16 - we have a folder called demos
09:18 - and then inside we have our food agent
09:19 - demo so just go ahead copy this one
09:22 - inside our assets
09:23 - so there it is and here in our project
09:25 - files we can see our demo and if we
09:27 - select it we can look in the inspector
09:29 - and see the various stats
09:30 - so you can see how many steps it
09:32 - recorded how many episodes what was the
09:34 - main reward
09:35 - what is the size of the observations and
09:37 - the various actions
09:38 - so there's this demo that i just made
09:40 - where i trained for a tiny bit now
09:42 - really very optimally
09:43 - and then over here i have another demo
09:45 - and in this one i train more properly so
09:47 - you can see the main reward almost close
09:49 - to two which is essentially one for that
09:50 - one and one for hitting the pellet
09:52 - so again the more data you give it and
09:54 - the better the data the better the ad
09:56 - will actually
09:57 - learn so now the next step is just to
09:59 - tell the ai to use this demo to learn
10:00 - from
10:01 - and for that we need to go into the
10:02 - config file so here i set it up
10:05 - exactly like i covered on the getting
10:06 - started video so i've got a folder for
10:08 - my configs
10:09 - and inside i've got my foodagen.yaml and
10:12 - here is the config with all of the hyper
10:14 - parameters
10:14 - and now in order to use imitation
10:16 - learning we go down here
10:18 - inside the reward signals and we're
10:20 - going to add another section
10:21 - and this one we're going to name it gale
10:24 - gale stands for
10:25 - generative adversarial imitation
10:27 - learning meaning that the ai won't try
10:29 - to beat the demo that you give it
10:31 - so at a high level how it works is by
10:33 - creating a second learning algorithm
10:35 - called the discriminator
10:37 - and the goal of that one is to figure
10:38 - out if a certain action came from the
10:40 - agent or from the demo
10:42 - so essentially over time our agent will
10:44 - learn how to behave more like the demo
10:45 - in order to trick the discriminator
10:48 - and again you can combine this with
10:49 - extrinsic rewards which essentially
10:51 - means that it works on top of
10:52 - reinforcement learning
10:54 - so after it learns how to behave like
10:56 - the demo it will continue improving
10:57 - until it becomes essentially superhuman
10:59 - so we add the scale section and then
11:01 - here we have a whole bunch of parameters
11:03 - you can check the docs to see all the
11:05 - various parameters and what they do
11:07 - the main ones that we need are the
11:08 - strength so this is how much the demo
11:11 - will impact how the agent behaves so if
11:14 - you want it to act
11:15 - exactly like the demo you give it 1.0
11:17 - but that's probably a bit too much we
11:19 - still want the agent to learn to get
11:21 - better than the demo so let's start off
11:23 - at 0.5
11:24 - and then the other parameter that we
11:26 - absolutely need is the demo underscore
11:28 - path
11:29 - and this is just the path towards the
11:30 - demo so here in the project folder we've
11:33 - got the demos and inside we've got the
11:35 - food agent demo so this is the one that
11:37 - i prepared previously so let's use this
11:39 - one since it has more values
11:40 - so just paste this pattern here that's
11:42 - it so essentially all it takes is adding
11:45 - these parameters and then all of a
11:46 - sudden you just added imitation learning
11:49 - now there's actually two types of
11:50 - imitation learning that you can use
11:52 - so there's gale here and there's another
11:54 - one called bc or
11:56 - behavioral cloning so that one goes
11:58 - outside of the reward signal so nothing
12:00 - here but rather down here this one is
12:02 - named behavioral cloning
12:03 - and inside we've also got these two
12:06 - so we have a strength let's also put it
12:08 - at 0.5 and then again
12:10 - a demo path now gail like i said works
12:12 - by trying to trick
12:14 - a discriminator into pretending that the
12:16 - actions came from the demo
12:17 - whereas bc simply tries to copy exactly
12:20 - what you did
12:21 - now the limitations of bc is that it can
12:23 - never get better than the demos
12:25 - so in order to get the best results you
12:26 - really just combine all three
12:28 - so first using bc it learns to act
12:31 - exactly like you
12:32 - then when combined with gale it learns
12:34 - to act similarly to you while achieving
12:37 - the same goal
12:38 - and when combined with extrinsic rewards
12:40 - it continues improving upon those two
12:42 - and that's how you get superhuman
12:43 - learning
12:44 - then essentially the trick is just to
12:45 - play around with all these parameters
12:47 - so at the start we want the agent to
12:49 - learn from the demos so we can put both
12:51 - of these with a pretty high strength
12:53 - but then we want the agent to go beyond
12:54 - the demo so after it learns the basics
12:56 - then we can reduce this to something
12:57 - like point one so it gets more impacted
13:00 - by the actual extrinsic rewards
13:02 - okay so now it's time to check out our
13:04 - ai nc learning using imitation warning
13:06 - now before we do some mass learning
13:09 - let's go inside our prefab
13:10 - and in here let's make sure that we
13:12 - disable recording on demonstrations
13:14 - so just untick this and let's set the
13:16 - behavior type back into default so that
13:18 - it trains
13:19 - and now let's run the same logic using
13:20 - our configuration.yemo
13:22 - and food agent let's name it imitation
13:24 - okay so let's run
13:26 - and here let's also enable all of them
13:28 - so we've got mass training and hit on
13:29 - play
13:30 - and if there it is the agent is now
13:32 - learning to play using our demos as
13:34 - a demonstration so you can see some of
13:36 - them already managed to hit and grab the
13:38 - food palette
13:39 - here we go one two three four five five
13:41 - of them managed to get the palette and
13:43 - two more and this one actually completed
13:45 - the whole task
13:46 - so as you can see this one is already
13:47 - doing quite a lot better than our
13:48 - previous training which was just trying
13:50 - random actions
13:51 - so quite a handful of them have already
13:53 - learned to press the button and go
13:54 - towards the palette look at that one
13:55 - almost went perfectly
13:57 - so all of the logic is working and our
13:59 - model is currently being built
14:00 - now we can go and look at tensorboard to
14:02 - visualize in learning
14:03 - so here it is and we can view the graphs
14:05 - and we can see that it's already working
14:07 - so in the beginning as it was trying
14:09 - just pretty much random things
14:10 - like this it wasn't getting anything so
14:12 - reward of minus one
14:13 - and then as it started to behave more
14:15 - and more like the demos you can see it's
14:17 - skyrocketed
14:17 - the reward right away and over here the
14:20 - episode length so essentially it's
14:21 - learning how to achieve the task quicker
14:23 - and quicker
14:24 - so after just a little bit of time we
14:26 - can clearly see the results
14:27 - so most of the agents have already
14:29 - learned how to do that complex task
14:31 - so they go towards the button they press
14:33 - the button then they go towards the food
14:34 - palette
14:35 - so let's stop training and let's go
14:37 - inside the results to grab
14:39 - our brain model here it is and if
14:41 - there's the agent using our model in
14:42 - order to achieve the tasks so he goes
14:44 - towards the button
14:45 - pushes the button and then goes towards
14:47 - the food pellet
14:48 - so look at that isn't that awesome at
14:50 - first the agent couldn't do anything it
14:51 - would just randomly move around
14:53 - and now thanks to our demonstration we
14:55 - have taught the agent how to achieve
14:56 - this relatively complex
14:58 - task so it goes towards the button
15:00 - pushes the button then goes towards the
15:01 - food palette
15:02 - here i have another brain that i trained
15:04 - for quite a bit longer and as you can
15:06 - see this one is quite a lot faster at
15:08 - achieving the goal so it goes directly
15:09 - towards the button doesn't even hesitate
15:11 - as soon as it gets inside the button
15:13 - it triggers the button and then goes
15:15 - straight towards the food pellet
15:17 - and again we can still see the magic of
15:19 - machine learning which is
15:20 - remember that we didn't write any code
15:22 - telling the agent how to move or how to
15:23 - press a button
15:24 - the agent learn all those actions by
15:26 - itself with the help of the human player
15:29 - teaching it how to play
15:30 - so here we have imitation learning it's
15:32 - a massively powerful tool and something
15:34 - that will greatly help you when training
15:36 - your agents in more and more complex
15:38 - scenarios as long as you give it enough
15:39 - data in your demo you can teach your ai
15:41 - to solve pretty much any problem
15:43 - so stay tuned for more machine learning
15:45 - videos where i will be applying this to
15:47 - even more complex scenarios
15:48 - you can check the full machine learning
15:50 - playlist linked in the description as
15:52 - always you can download the project
15:53 - files and utilities from
15:54 - unitycodemonkey.com
15:55 - this video is made possible thanks to
15:57 - these awesome supporters
15:58 - go to patreon.com unitycodemonkey to get
16:01 - some perks and help keep the videos free
16:03 - for everyone
16:04 - if you found the video helpful consider
16:05 - liking and subscribing post any
16:07 - questions you have in comments and i'll
16:08 - see you next time

Cleaned transcript:

hello and welcome i'm your code monkey and here let's check out the awesome power of imitation learning using unity ml agents first if you're not familiar with ml agents that is unity's machine learning ai toolkit i cover the detailed getting start guide that goes through the whole installation process and how to use it so go watch that video if you haven't already and check the full machine learning playlist in the description ml agent supports two types of learning reinforcement learning and imitation learning now imitation learning is how you can teach your ai directly how to behave in order to achieve a certain goal meaning that instead of trying to work by just randomly testing actions it will instead try to imitate what the player did and build upon it so think of it as if you were trying to play a game in one method you just randomly push every button on the controller until something happens and in another method you have someone who knows how to play teach you what the buttons do and how to push them in the correct order to get the reward now let's first think about the limitations when using reinforcement learning and then we can see how imitation learning is an awesome very powerful tool in the getting started video i cover the most basic type of learning just reinforcement learning where the agent learns how to do something based on the rewards that it gets now that method works great when you have a relatively simple scenario for example in that video we made a simple agent and it wants to touch the goal it's pretty simple so it can easily complete the task it spawns in a random position and moves towards the goal so that one works out great but over here i have another more complex demo i'm currently controlling the character using heuristics so the goal is to get into this button so i can move the character go in there then i press a button on my keyboard to push the actual button and as soon as i do a food pallet get spawned so now i need to go there and touch it and there you go i went now for you assuming that you are a human being this is a pretty simple task even if i don't give you the instructions you can probably figure out after a few seconds but for an ai that randomly pushes buttons until it gets a reward then this scenario is extremely complex just think in order for the ai to complete the whole task first it needs to trigger the right random actions to get it to the button then it needs to again randomly push the button and then again needs to trigger the right random combination of actions that gets it to the food pellet so as you can see that's a ton of very specific random actions that need to happen in a very specific sequence so the odds of that happening based on pure luck alone are extremely tiny so essentially you'd have better luck just getting the ai to play the lottery so that's where imitation learning comes in instead of the ai trying random actions until something happens we teach it how it should complete the task and the i learned from that and just like with reinforcement learning using ml agents to achieve this is quite simple again if you haven't seen the getting started video go and watch that in there i go through all the basics on getting started and logic for how it all works in order to do imitation learning it is pretty much the same thing that we saw for reinforcement learning so over here i have my scene and i can control the character using heuristics so the agent class is already nicely implemented let's look at it so i've got my environment and inside i've got the agent and here it is the various behavior parameters so very simple stuff and let's inspect the food agent script so here is the script and again remember how reinforcement learning works which is first the ai takes some observations of its environment then it takes a decision it makes an action and sees if it gets a reward that's the reinforcement learning cycle over here i have my food agent class and as you can see the code is pretty simple so if you watch the other video then all of this should be quite familiar first up here we have the on episode begin function so this is where we set up the scene with some randomness so that the ai doesn't learn how to solve just one specific set of positions so we randomize the player position and then on the food button script when we reset the button we're also randomizing the position so both the player and the button get random positions that's all we do on the on episode begin then we have our collect observations function so here in terms of observations first i'm giving it the state of the button so if the button can be used then the machine learning algorithm receives a 1 and if not it receives a 0. then i'm also giving it the direction towards the food button then another observation for the current state of the food so if it has been spawned or not and if the food has been spawned then i give it the direction towards the food and if not just two zeros so those are all the observations we just have over here one two three four five and six floats then down here for the actions i am using discrete actions to move the character and here they have some very simple very straightforward actions so just don't move move left move right move back or move forward so it's a very basic movement controller over here modifying the rigid body velocity and then another action for the is use button down so when this one is set to one then that is the agent using the use button and when that happens then it simply looks for objects around the agent and finds a button to push so if it can't find a button it tries to push that button so i covered this simple way of pushing buttons in the waste open doors video so those are all the actions and then for the rewards over here we have a reward when it successfully uses the button and then down here when we consume the food we also get another reward and lastly over here on the on action received on every action i'm also giving it a negative reward in order to essentially encourage the agent to finish as quickly as possible now one important thing is that i only added this negative reward to training after it was already succeeding in achieving the task so i didn't start with this one by default essentially in the beginning you want to help your ai to learn and then later on you can focus on actually optimizing okay so here is the script as you can see it's pretty small pretty simple first let's actually look at what happens if we try training our ai using just reinforcement learning and by the way here's a quick tip that someone mentioned in the comments on the previous video if you go into the folder where you have your project you can just go up here into the actual path and in here you type cmd and when you do it opens up the command prompt directly onto that folder so very useful tip now here let's try doing the normal training okay so there it is training is currently running and you can see it's happening exactly as you would expected so the agent is just trying random actions which caused it to randomly move around so again remember in reinforcement learning it's very important to get a reward and in this case the agent is only getting a reward when it pushes the button so when trying these random actions the agent needs to be unlucky enough in order to push the right actions to move it towards the button and then actually push the button so achieving that first part is actually somewhat doable so it might get lucky and achieve that but even if it does achieve doing that then it won't learn to go towards the button and push it but then it's going to have a lot of trouble to go towards the pellet after pushing the button now again you could technically leave it like this and through sheer chance alone then maybe in a million steps you would get lucky and complete the whole task but the odds of that happening are really slim so this is just basically reinforcement learning it tries random actions and hopes that it achieves something so here it already got lucky and hit the button so now it learned to go towards the button and push it but again achieving the whole task is going to be very difficult so now let's give this ai some help and show it how this task should be completed and by the way if you find the video helpful consider subscribing and hitting the like button it really helps out the channel now as i said that one is very simple all we need to do is go into our agent and let me actually go inside the prefab so i modify all the agents so here i'm in the prefab and select the agent and then i just have to go down here and going to add another component go inside ml agents and in here we're going to add a demonstration recorder then here we see some fields so first of all we've got a toggle to enable or disable recording so just go ahead toggle that so we start recording then for the number of steps this is in case you want to unlimit how many steps you record but if you just want to continue recording until you stop the game then just leave it at zero then we've got a demonstration name so let's name this our food agent demo and then directory where you want to save it so let's just name it demos alright so that's on the setup for the demonstration and now hey record is very simple just make sure that the agent is over here set the heuristic okay here is the calling and all i have to do is just run the scene as normal so here i am and now my actions are being recorded so essentially the ai is now looking at everything that i do all the actions i take and all of the observation values as i take them now all i have to do is really just teach it correctly so i guess that means that if i wanted to be mean to the poor ai i would just ram into a wall nonstop and now the poor ai would learn just to slam into a wall now that's probably not very useful so let's actually teach it how to achieve the task so here i just move towards the button i press the keyboard button to trigger the action and now go towards the palette and consume it and now again go into that one press the button and so on and so on and essentially this way the agent the ai is actually looking at me looking at how i achieve this analyzing all the actions that i'm taking and all the various observations so that's pretty much all it takes to record the demo you just complete this as many times as you can essentially the more that i give it the more the agent will learn and then in order to stop recording you just simply exit play mode okay recording is done and if we go into our project files we can see over here we have a folder called demos and then inside we have our food agent demo so just go ahead copy this one inside our assets so there it is and here in our project files we can see our demo and if we select it we can look in the inspector and see the various stats so you can see how many steps it recorded how many episodes what was the main reward what is the size of the observations and the various actions so there's this demo that i just made where i trained for a tiny bit now really very optimally and then over here i have another demo and in this one i train more properly so you can see the main reward almost close to two which is essentially one for that one and one for hitting the pellet so again the more data you give it and the better the data the better the ad will actually learn so now the next step is just to tell the ai to use this demo to learn from and for that we need to go into the config file so here i set it up exactly like i covered on the getting started video so i've got a folder for my configs and inside i've got my foodagen.yaml and here is the config with all of the hyper parameters and now in order to use imitation learning we go down here inside the reward signals and we're going to add another section and this one we're going to name it gale gale stands for generative adversarial imitation learning meaning that the ai won't try to beat the demo that you give it so at a high level how it works is by creating a second learning algorithm called the discriminator and the goal of that one is to figure out if a certain action came from the agent or from the demo so essentially over time our agent will learn how to behave more like the demo in order to trick the discriminator and again you can combine this with extrinsic rewards which essentially means that it works on top of reinforcement learning so after it learns how to behave like the demo it will continue improving until it becomes essentially superhuman so we add the scale section and then here we have a whole bunch of parameters you can check the docs to see all the various parameters and what they do the main ones that we need are the strength so this is how much the demo will impact how the agent behaves so if you want it to act exactly like the demo you give it 1.0 but that's probably a bit too much we still want the agent to learn to get better than the demo so let's start off at 0.5 and then the other parameter that we absolutely need is the demo underscore path and this is just the path towards the demo so here in the project folder we've got the demos and inside we've got the food agent demo so this is the one that i prepared previously so let's use this one since it has more values so just paste this pattern here that's it so essentially all it takes is adding these parameters and then all of a sudden you just added imitation learning now there's actually two types of imitation learning that you can use so there's gale here and there's another one called bc or behavioral cloning so that one goes outside of the reward signal so nothing here but rather down here this one is named behavioral cloning and inside we've also got these two so we have a strength let's also put it at 0.5 and then again a demo path now gail like i said works by trying to trick a discriminator into pretending that the actions came from the demo whereas bc simply tries to copy exactly what you did now the limitations of bc is that it can never get better than the demos so in order to get the best results you really just combine all three so first using bc it learns to act exactly like you then when combined with gale it learns to act similarly to you while achieving the same goal and when combined with extrinsic rewards it continues improving upon those two and that's how you get superhuman learning then essentially the trick is just to play around with all these parameters so at the start we want the agent to learn from the demos so we can put both of these with a pretty high strength but then we want the agent to go beyond the demo so after it learns the basics then we can reduce this to something like point one so it gets more impacted by the actual extrinsic rewards okay so now it's time to check out our ai nc learning using imitation warning now before we do some mass learning let's go inside our prefab and in here let's make sure that we disable recording on demonstrations so just untick this and let's set the behavior type back into default so that it trains and now let's run the same logic using our configuration.yemo and food agent let's name it imitation okay so let's run and here let's also enable all of them so we've got mass training and hit on play and if there it is the agent is now learning to play using our demos as a demonstration so you can see some of them already managed to hit and grab the food palette here we go one two three four five five of them managed to get the palette and two more and this one actually completed the whole task so as you can see this one is already doing quite a lot better than our previous training which was just trying random actions so quite a handful of them have already learned to press the button and go towards the palette look at that one almost went perfectly so all of the logic is working and our model is currently being built now we can go and look at tensorboard to visualize in learning so here it is and we can view the graphs and we can see that it's already working so in the beginning as it was trying just pretty much random things like this it wasn't getting anything so reward of minus one and then as it started to behave more and more like the demos you can see it's skyrocketed the reward right away and over here the episode length so essentially it's learning how to achieve the task quicker and quicker so after just a little bit of time we can clearly see the results so most of the agents have already learned how to do that complex task so they go towards the button they press the button then they go towards the food palette so let's stop training and let's go inside the results to grab our brain model here it is and if there's the agent using our model in order to achieve the tasks so he goes towards the button pushes the button and then goes towards the food pellet so look at that isn't that awesome at first the agent couldn't do anything it would just randomly move around and now thanks to our demonstration we have taught the agent how to achieve this relatively complex task so it goes towards the button pushes the button then goes towards the food palette here i have another brain that i trained for quite a bit longer and as you can see this one is quite a lot faster at achieving the goal so it goes directly towards the button doesn't even hesitate as soon as it gets inside the button it triggers the button and then goes straight towards the food pellet and again we can still see the magic of machine learning which is remember that we didn't write any code telling the agent how to move or how to press a button the agent learn all those actions by itself with the help of the human player teaching it how to play so here we have imitation learning it's a massively powerful tool and something that will greatly help you when training your agents in more and more complex scenarios as long as you give it enough data in your demo you can teach your ai to solve pretty much any problem so stay tuned for more machine learning videos where i will be applying this to even more complex scenarios you can check the full machine learning playlist linked in the description as always you can download the project files and utilities from unitycodemonkey.com this video is made possible thanks to these awesome supporters go to patreon.com unitycodemonkey to get some perks and help keep the videos free for everyone if you found the video helpful consider liking and subscribing post any questions you have in comments and i'll see you next time
