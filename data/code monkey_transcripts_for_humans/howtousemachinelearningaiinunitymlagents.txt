With timestamps:

00:00 - hello there i'm your code monkey and
00:01 - let's learn how to use machine learning
00:03 - and ml agents in unity
00:05 - this is a very powerful toolkit that
00:06 - lets you create some extremely
00:08 - intelligent ai
00:09 - it helps you solve tons of problems that
00:10 - would simply be impossible to solve
00:12 - while using classic app there's immense
00:14 - massive potential in this toolkit so you
00:16 - should know how to use it so you know
00:17 - how it can help you and when to apply it
00:19 - this is a long video but it's the only
00:21 - video you're going to need in order to
00:22 - learn how to get started
00:23 - working with machine learning in unity
00:25 - we're going to start completely from
00:27 - scratch and go through the entire
00:28 - installation process
00:30 - then learn how to use it by setting up a
00:32 - scene to train an ai using reinforcement
00:34 - learning
00:34 - and finally look at the results to see
00:36 - the ai in action using our trained brain
00:38 - model
00:39 - so make sure you watch the video until
00:40 - the end to understand the whole process
00:42 - this video is meant to help you get
00:43 - started and after watching it go check
00:45 - out the playlist link in the description
00:47 - where i will be adding videos covering
00:49 - interesting use cases made with machine
00:51 - learning in unity
00:52 - for example i'm currently working on a
00:53 - specific match 3 use case and many other
00:55 - different ones so stay tuned for that
00:57 - now the way machine learning works in
00:59 - unity is through the ml agent's toolkit
01:01 - which combines several tools
01:03 - first you have the ml agents python
01:05 - package which runs the machine learning
01:07 - algorithm
01:08 - then you have your learning environment
01:10 - which is your unity scene with the game
01:12 - running
01:12 - and then you have the ml agent c sharp
01:15 - package which lets you define the data
01:16 - that you feed into the algorithm
01:18 - as well as using the resulting brain so
01:20 - let's go through that whole process
01:21 - starting from scratch
01:22 - first over here is the github page for
01:25 - the ml agents package
01:26 - there's a link in the description you
01:28 - can find tons of documentation here so
01:30 - definitely give it a look
01:31 - you have a quick readme talking about
01:33 - how the whole thing works all the
01:34 - features the release the documentations
01:36 - and so on
01:37 - you've got the docs folder where you
01:38 - have all the documentation so tons of
01:40 - topics on installation getting started
01:43 - making some environments and so on
01:44 - and it also has lots of awesome examples
01:46 - which you can browse around to see how
01:48 - they work
01:48 - now the first thing we need to do is
01:50 - actually install python
01:52 - and as of the time of this recording the
01:53 - recommend python version is either 3.6
01:56 - or 3.7
01:57 - so over here on the python website i'm
01:59 - going to go ahead and download 3.7.9
02:02 - again if you're watching this in the
02:03 - future make sure you check the official
02:04 - docs to see which version you should
02:06 - install
02:07 - so go ahead just download and install it
02:09 - after installing python open up the
02:11 - command prompt so just click on the
02:13 - start button and type cmd
02:14 - so here it is and now there is actually
02:17 - one quirky thing about windows 10
02:19 - which is in theory you should be able to
02:21 - run python by just typing in python
02:23 - however if you do here on windows 10 it
02:26 - opens up the microsoft store instead of
02:28 - actually running python
02:29 - so if you see this behavior the solution
02:31 - is to instead of python just type py
02:33 - so over here instead of python just py
02:35 - and i hit enter and there you go now i'm
02:37 - inside python
02:38 - and over here you can verify that first
02:40 - of all python is running and you can
02:41 - verify you have the correct version
02:42 - which in this case 3.7.9
02:44 - okay so far so good now let's just exit
02:47 - out of python okay back in the command
02:49 - line
02:49 - now the next step is we need to change
02:51 - the directory to go to our unity project
02:54 - so over here is the unity project i'm
02:55 - going to use so just go ahead copy the
02:57 - entire path
02:58 - and on the command prompt just change
02:59 - directory onto that directory
03:01 - okay now in here what we're going to do
03:03 - is create a python virtual environment
03:06 - this will help us by keeping all of our
03:08 - projects separate so each virtual
03:10 - environment is completely separate from
03:11 - the others
03:12 - meaning that we can have multiple
03:14 - projects in the same machine each of
03:15 - them using their own python packages and
03:17 - they will not cause conflicts with each
03:19 - other so again first go into your unity
03:22 - project directory
03:23 - and then in here we're going to type the
03:24 - command python-m
03:27 - means we're going to run a module and
03:29 - one we want to run
03:30 - is called vnv to create a virtual
03:32 - environment
03:33 - and then afterwards this requires a
03:35 - folder name where the environment won't
03:37 - be created
03:38 - so just keep things nice and organized
03:39 - and give it the exact same name so just
03:41 - vnv so this will create the virtual
03:43 - environment inside a folder named vnv
03:46 - now if you're on linux or mac the
03:47 - commands are slightly different so check
03:49 - the official docs
03:50 - and again like i said previously if you
03:52 - have the issue with python not running
03:53 - when you type python
03:55 - then here instead of python just type py
03:57 - mvnv
03:58 - vnv so go ahead hit on enter and yep now
04:01 - it's creating the virtual environment
04:03 - all right it's done and you can verify
04:05 - that it worked by opening up your file
04:06 - explorer and yep over here there's a
04:08 - folder called vnv
04:09 - and over there we have our virtual
04:11 - environment over here you see some
04:12 - folders and you see this one with a
04:14 - bunch of scripts
04:15 - and here we see a activate script this
04:17 - is how we're actually going to activate
04:19 - the virtual environment
04:21 - so back in the command line we go inside
04:22 - the vnv folder we access the scripts
04:25 - folder and then run the activate so when
04:28 - you do
04:28 - yep the command prompt changes over
04:30 - there it says vnv so we are now inside
04:32 - the python virtual environment
04:33 - so any changes that you make here will
04:35 - not impact any other python projects you
04:37 - have in your machine
04:38 - like for example any other unt projects
04:40 - with other python libraries
04:42 - now before we install our python
04:44 - packages let's make sure our installer
04:46 - is updated
04:47 - the python packaging seller is named pip
04:50 - so in order to make sure that it's using
04:51 - the latest version let's run the command
04:53 - python-m
04:55 - we're going to do a pip install dash
04:58 - dash
04:58 - upgrade and we're going to want to
05:00 - upgrade our pip package
05:01 - so just go ahead hit on enter
05:06 - and you have now it's successfully
05:07 - installed the latest pip package
05:09 - okay so far so good now we can begin
05:11 - installing our packages
05:13 - and the first one we're going to need is
05:14 - a package called pytorch
05:16 - this is an open source library for
05:18 - performing computations using data flow
05:20 - graphs
05:20 - so it's the underlying representation of
05:22 - the deep learning models
05:24 - for that let's run the command pip
05:25 - install and here we need a specific
05:27 - version
05:31 - so here it is in song torch version 1.7
05:33 - and download it from this website
05:36 - now if you have issues or you're
05:37 - watching this many months in the future
05:39 - check the github installation docs to
05:40 - see which version to use
05:42 - so you go ahead run this and now we wait
05:44 - for it to complete
05:49 - okay pytorch is now installed next up we
05:51 - install the ml agents package
05:53 - so we just do pip install ml agents
05:56 - however just running like this may give
05:58 - you some compatibility errors
06:00 - so let's try it and see
06:05 - and up here we see an error where we
06:06 - have the incorrect numpy version
06:08 - so it's incompatible so if you didn't
06:10 - get that error then it's fine just keep
06:12 - going but if you did get the error just
06:14 - like i did then the solution is to use a
06:16 - different package resolver
06:17 - so do pip install ml agents and then
06:20 - dash dash
06:21 - use the feature and the feature is the
06:24 - 2020 resolver
06:26 - so if you use the new resolver and run
06:28 - if there you go now you can see it's
06:29 - uninstalling the incorrect version and
06:31 - selling the correct one
06:32 - alright so now we have all of our
06:33 - correct version and we can verify that
06:35 - the ml agents package was correctly
06:37 - installed by running the command
06:39 - go into mlagents-learn
06:42 - and then use the command-help and hit
06:45 - enter
06:46 - and yep if everything went correctly
06:47 - then you should be able to see the helm
06:49 - files for the ml agents learned package
06:51 - so here it is everything went correctly
06:53 - so everything is installed correctly
06:55 - and as of the time of this recording i'm
06:57 - using ml legends release 10
06:59 - with the python package 0.22 so again
07:02 - if you have any issues or you're
07:03 - watching this many months in the future
07:04 - check the official docs for any version
07:06 - changes
07:07 - okay so far so good and with that the
07:09 - python side is all installed correctly
07:12 - now there's actually one more optional
07:14 - step here if you look in the console
07:16 - you might be seeing some warning
07:17 - messages now these are not directly
07:19 - related to the mlegens package but
07:21 - whether it's due to one of the
07:22 - dependencies
07:23 - so if you're watching this in the future
07:25 - with another ml engine's release it
07:26 - might not show these warnings
07:28 - something saying it could not load a
07:29 - dynamic library with the name
07:31 - cuda rt64 underscore 101 so if you see
07:35 - something like that it's telling you
07:36 - that it cannot find the cooldown
07:37 - libraries
07:38 - now this is optional everything will run
07:40 - just fine without them
07:42 - so if you have no gpu you can skip this
07:44 - step and it will use your cpu
07:46 - instead of your gpu but if you do have
07:49 - an nvidia gpu you can optionally install
07:51 - cuda
07:52 - so if you see that message pay attention
07:53 - to the name of the missing library
07:56 - so in my case i was seeing a missing
07:58 - library that ends with underscore
08:00 - 101.dll
08:01 - so that means that it requires cuda
08:03 - version 10.1
08:04 - so just go into nvidia's website and
08:06 - download cuda
08:08 - however again pay attention to the
08:09 - version as of the time of this recording
08:12 - the latest cuda version is actually
08:13 - version 11
08:14 - however the missing library is version
08:16 - 10. so when you go into the download
08:18 - page
08:19 - don't download version 11. instead go
08:21 - into the archive
08:22 - in this case we're looking for version
08:24 - 10.1 so go ahead and download that one
08:27 - after installing it if you once again
08:29 - run ml agent stash learn
08:31 - dash dash help if you run that again you
08:33 - should be able to see that the warnings
08:35 - are gone
08:36 - so it now finds the cooldown libraries
08:38 - however you might be seeing another
08:40 - warning
08:41 - which again check the library name so
08:43 - you might be seeing another missing
08:44 - library named could
08:46 - nn64 underscore seven so this is the
08:48 - cuda deep neural network library
08:50 - so once again just go into nvidia's
08:53 - website and search for
08:54 - coo dnn so over here just go ahead and
08:56 - download it
08:57 - but again pay attention to the library
08:59 - name again in my case it's missing the
09:01 - code dnn64 underscore 7
09:04 - meaning that it uses version 7 and again
09:06 - the latest one is actually
09:08 - version eight so when you download it
09:10 - make sure you download the correct
09:11 - version seven
09:12 - when you download it you get a zip file
09:14 - and inside you see a cuda folder
09:16 - and a bunch of files so in order to
09:18 - install it you just go into your cuda
09:20 - installation folder
09:21 - so in my case i put it on default so on
09:23 - program files and nvidia gpu computing
09:25 - toolkit
09:26 - then inside you see the cuda folder and
09:28 - inside we see we go into version 10.1
09:31 - and then over here we see our various
09:32 - folders so just go ahead and copy all
09:34 - these
09:35 - so the include folder the lib and the
09:37 - bin so just take these and drag them all
09:39 - in there
09:40 - and after doing that you can verify by
09:41 - going inside the bin library
09:43 - and over here you should be able to find
09:45 - all the dlls so in my case the cuda
09:47 - rt64101
09:49 - and the other one is the could nn 64
09:51 - underscore seven
09:52 - and now if we run ml agent learn help
09:56 - now you should be able to see everything
09:57 - run without any warnings
09:59 - so we're here the command ran and nope
10:01 - no warnings okay so far so good
10:03 - so with this we have all the setup for
10:05 - the python side including the optional
10:07 - codon libraries
10:08 - now let's go into our unity project over
10:11 - here i have a project that's pretty much
10:13 - just brand new
10:13 - so just a simple demo i have prepared
10:16 - the ml engines package
10:18 - works with any unity version starting
10:19 - from 2018.4
10:21 - now i want to make sure that this video
10:22 - stays relevant for as long as possible
10:24 - so in this project i'm currently using
10:26 - 2020.2
10:27 - but everything works exactly the same if
10:29 - you're using 2019.4 or if you're in the
10:32 - future using the 2020 lts version
10:35 - so to install the ml agents package just
10:37 - go ahead open up the package manager
10:39 - then here select the packages and make
10:40 - sure you go into the unt registry
10:42 - and in here just scroll down and find
10:44 - the ml agents package
10:46 - here you can see the latest stable
10:48 - package which is the time this recording
10:50 - is version 1.0.6
10:52 - and again i want this video to be
10:53 - relevant for a long time so i'm going to
10:55 - instead install the latest preview
10:57 - package so for that i'm going to click
10:59 - on the gear icon go into the advanced
11:00 - project settings
11:01 - and in here i'm going to enable the
11:03 - preview packages and yep i understand
11:05 - so over here on ml legends i can expand
11:07 - it see the other versions
11:09 - and here i see the latest preview
11:10 - package which at the time this recording
11:12 - is 1.6
11:13 - but again if you're in the production
11:15 - stage of your development and you want
11:16 - maximum stability
11:18 - then go with the stable unity lts
11:20 - version as well as a stable ml agents
11:22 - package
11:22 - so select your choice and go ahead and
11:24 - click on install
11:30 - okay it's done and you can verify that
11:33 - everything installed correctly by just
11:35 - creating an empty game object
11:36 - and over here if you go into add
11:38 - component you should be able to now see
11:40 - a group for the ml agents and over here
11:42 - the various
11:42 - scripts alright so we have everything
11:45 - correctly installed
11:46 - now over here for testing i have this
11:48 - demo which is pretty much taken from the
11:50 - official examples
11:51 - it's just a nice character and over here
11:53 - is the goal so the objective
11:55 - is to teach this character to move
11:57 - towards the goal and not fall off the
11:58 - map
11:59 - so let's see how we actually use our ml
12:01 - agents
12:02 - now for that first we need to create an
12:04 - agent an agent
12:06 - is what's going to run our ai both for
12:08 - training and then for playing
12:10 - and in order to make an agent we just
12:11 - make a normal c sharp script so just
12:13 - right click over here
12:14 - going to create a new c sharp script and
12:17 - let's name this the move
12:18 - to goal agent and go ahead open up the
12:21 - script
12:22 - and now in here we need to go up here to
12:24 - add using
12:26 - go inside unity and let's use the unity
12:29 - ml agent
12:30 - then over here let's get rid of the
12:32 - default methods we don't need them right
12:34 - now and instead of inheriting from
12:36 - modern behavior
12:37 - we're going to inherit from the agent
12:39 - class
12:41 - so over here you can right click on the
12:43 - agent and go into the definition
12:46 - and here we see the definition of that
12:47 - class so as you can see we have a whole
12:49 - bunch of methods all of them related to
12:51 - machine learning
12:52 - now the way the agent learns is through
12:54 - reinforcement learning
12:56 - so it's based on a relatively simple
12:58 - loop of observation where the agent
13:00 - gathers data from its environment
13:02 - then it makes a decision based on the
13:03 - data that has and then it takes an
13:06 - action
13:07 - and if it does the right action then it
13:09 - gets a reward so this is a continuous
13:11 - cycle where the agent grows to learn
13:13 - based on its observations and what
13:14 - actions lead to the highest rewards
13:16 - okay so let's see how to implement this
13:18 - cycle again here
13:20 - is the agent class and we're going to
13:21 - need to override two functions
13:24 - so we're going to need to override this
13:25 - one collect observations in order to
13:27 - give the agent some observations
13:29 - and then we're also going to need to
13:31 - override this one which receives a
13:32 - buffer with all of our
13:34 - actions okay so let's go back into our
13:36 - script so our move to goal agent
13:38 - and first let's look at how the ai takes
13:40 - actions
13:41 - so we're going to do a public override
13:44 - and we're going to override the on
13:46 - action received which takes an
13:48 - action buffer so this buffer then
13:50 - contains our actions as either floats or
13:52 - ends
13:53 - now one thing to keep in mind is that
13:55 - the machine learning algorithm only
13:57 - works with numbers
13:58 - meaning that it doesn't have an
13:59 - understanding of what exactly is a
14:01 - player object
14:02 - or what it means to move to the right
14:04 - all it knows is numbers
14:06 - it's easier to understand this if we see
14:08 - it in action
14:09 - so for now let's go back into the editor
14:12 - and over here let's select our agent so
14:13 - i have my nice agent in here and i'm
14:16 - just going to drag the move to goal
14:17 - agent and attach it in there
14:19 - and yep here's our move to go agent
14:20 - script and when we added this it also
14:23 - added the behavior parameter script
14:25 - these are the various parameters that
14:27 - our ai uses first we have the behavior
14:29 - name so let's rename this to move to
14:31 - go so give it a proper name to this
14:34 - agent
14:35 - and then over here let's look at the
14:36 - vector action
14:38 - so let's learn what all of these mean
14:40 - first of all you've got the space type
14:42 - so in here you can choose between the
14:44 - script and continuous
14:45 - now essentially discrete are whole
14:47 - numbers so you can have 0
14:49 - 1 2 3 and so on and continuous are
14:51 - floats
14:52 - so going between minus 1 to plus 1 and
14:54 - all the numbers in between
14:56 - so 0.2.3 minus 0.4 and so on we're going
14:59 - to do a quick test to see these
15:00 - differences in a bit
15:02 - let's just learn about the other
15:03 - parameters so first if you select
15:06 - continuous
15:06 - over here you see the space size and
15:09 - this is how many actions you won't get
15:10 - on the vector
15:11 - so for example if you put a 2 here then
15:14 - in the code if we inspect our action
15:16 - buffers
15:17 - we see that this contains two action
15:19 - segments one for the continuous actions
15:20 - and one for discrete actions
15:22 - so the action segment here is
15:24 - essentially an array so when you set the
15:25 - space size you are defining the size for
15:27 - the array of the type that you select
15:29 - so if you set a space size of two then
15:31 - this will have two positions with two
15:32 - values
15:33 - a value on index zero and another one on
15:35 - index one
15:36 - and then if you choose discrete the size
15:38 - is the same so it's how many
15:40 - values you get on that array but then
15:42 - you also have the second parameter which
15:44 - is the maximum value for this branch
15:46 - so like i said discrete means integers
15:48 - or whole numbers
15:50 - so for example if you put a 1 in here
15:52 - then you won't get an action value
15:54 - of just zero however if you put a two
15:56 - then you're going to get an action value
15:58 - of either zero or one
15:59 - and if you put a five then you can get a
16:02 - zero one two three or four
16:04 - and each branch can have its own size so
16:06 - for example if you're making a car ai
16:08 - you would make the first branch refer to
16:11 - accelerating and breaking
16:12 - so you would put it with two values and
16:15 - then for the second branch
16:16 - let's say it would represent turning so
16:18 - you would put three values one for
16:20 - turning left turning right and don't
16:21 - turn
16:22 - okay now before we go further and look
16:24 - into how to define training let's just
16:25 - test these actions to get a better
16:27 - understanding for how all of this works
16:29 - so first let's put a discrete with just
16:31 - one branch keep it simple and let's put
16:33 - it with a size of five
16:34 - and now over here in the code we simply
16:36 - do a debug.log
16:38 - we go inside the action buffers then we
16:41 - access our discrete actions
16:42 - and let's just print out what's on index
16:44 - 0. since we have just one branch that
16:46 - means we have
16:47 - one value on this array so that value is
16:50 - on index zero
16:51 - okay now before we can test this ai we
16:53 - need to add one more thing
16:55 - over here in our agent let's add the
16:57 - component go into ml agents
16:59 - and we're going to add a decision
17:00 - requester like i said previously the way
17:03 - reinforcement learning works
17:04 - is through a cycle of observation
17:06 - decision action and reward
17:08 - so in order to take an action we need to
17:10 - first request the decision
17:12 - and what this script does is simply
17:13 - request a decision every certain amount
17:15 - of time and then takes actions
17:16 - now there are other ways of requesting
17:18 - decisions but for now let's just use
17:20 - this simple script
17:21 - okay so over here we are ready to begin
17:23 - training and run our test to see what
17:24 - the ai will output over here
17:26 - so for that let's go back into our
17:28 - command prompt and here make sure you
17:30 - are
17:30 - inside the virtual environment and in
17:32 - order to train it it's very easy we just
17:34 - run the command
17:35 - ml agents dash alert so just hit enter
17:39 - and yep we see a nice ascii unity logo
17:41 - and a message telling us that we can
17:42 - start training by pressing the play
17:44 - button
17:45 - so just do that just in here press the
17:47 - play button
17:48 - and yep we now have our training running
17:49 - so we can check in the command prompt
17:51 - yep we have everything running
17:52 - so it listens that and it's running our
17:54 - training and over here we can check on
17:56 - the console
17:57 - and now we can verify and see what the
17:59 - actual action vector contains
18:02 - so let's hit on collapse and over here
18:03 - we can see that we put just one branch
18:05 - with a branch size of five so over here
18:07 - we do have values going from zero one
18:09 - two three and four
18:11 - so we have five values zero to four so
18:13 - this is what it means to have a discrete
18:15 - vector with a branch size of five
18:17 - now let's test with the continuous type
18:19 - to see what actions we get
18:21 - so here let's just swap it from discrete
18:22 - into continuous and with space size of
18:25 - just one
18:25 - then here in the code it's pretty much
18:27 - the same the only difference is we
18:28 - access the action segment for the
18:31 - continuous actions instead of discrete
18:32 - and the position is the same on index
18:34 - zero
18:34 - and now we want to run this test so here
18:37 - in the command line first of all we can
18:38 - see that the previous test worked
18:39 - correctly so we have our training we
18:41 - have our model and so on
18:42 - and here if we run the exact same
18:44 - command ml agent learn
18:46 - if we run it like this there it is we
18:48 - get an error
18:49 - and the error is because we're trying to
18:50 - run training again on using the same
18:53 - default id
18:54 - so over here we have two options we can
18:56 - call our ml agents learn with the force
18:59 - tag
18:59 - so this will override the previous data
19:02 - or we can specify a different id name
19:05 - so let's try doing that so dash dash run
19:08 - dash id
19:09 - equals and then something name so say
19:11 - test2
19:12 - so now if we go ahead hit on enter and
19:15 - yep we have it we are listening on the
19:16 - port so just start training
19:18 - so just hit it and let's see and yep we
19:20 - have training running and now we can see
19:22 - what this one does
19:23 - so we can see what a continuous action
19:25 - looks like so over here we are getting
19:27 - values pretty much
19:28 - between minus one and plus one and
19:30 - everything in between
19:31 - all right so now you should have a
19:33 - better understanding of exactly how the
19:35 - actions work
19:36 - so discrete is integers and in
19:38 - continuous we've got floats from -1 to
19:40 - plus one
19:41 - so as you can see these are really just
19:43 - numbers so it's up to you to decide what
19:45 - they represent
19:46 - now let's look at another part of the
19:47 - reinforcement learning cycle
19:49 - let's look at observations so back in
19:51 - the code here
19:52 - the way we collect observations is by
19:55 - overriding a function
19:56 - so we just do public override and we're
19:59 - going to override this one collect
20:00 - observations which
20:02 - takes a vector sensor and as soon as you
20:04 - do up here it will add the using unity
20:06 - ml agent sensors
20:08 - so this is where this sensor exists so
20:10 - we have our collect observations
20:12 - function
20:13 - and now observations are how the agent
20:15 - observes its environment
20:16 - so think of it kind of like the inputs
20:18 - for the ai
20:19 - and obviously this will differ based on
20:21 - what problem you're trying to solve
20:22 - so essentially you need to think about
20:24 - what data does the ai need
20:26 - in order to solve the problem you're
20:27 - giving it now our goal in this example
20:30 - is we have a character and we have a
20:32 - goal and we want to move the character
20:34 - towards the goal
20:35 - so if you think about it if you are
20:37 - controlling the player so what
20:38 - information do you need
20:40 - well first of all obviously you need to
20:42 - know where you are so we should pass in
20:44 - the player position
20:45 - and then you also need to know where the
20:46 - target is so we also need to pass in
20:48 - that position
20:49 - so over here in the script how we pass
20:51 - that into the ai is very simple
20:53 - we just go into the sensor and call the
20:55 - function add observation
20:57 - and over here first let's pass in the
20:59 - transform.position so the player
21:01 - position so with this the ai will have
21:03 - the data for the player position
21:05 - and then let's also pass in the target
21:07 - position so up here let's just add a
21:09 - serialized film for a reference
21:11 - so a transform for the target transform
21:17 - so back in the editor we have our film
21:18 - let's just drag the gold transform on
21:20 - there
21:21 - and in here we do the same thing sensor
21:22 - add an observation and pass in the
21:24 - target
21:25 - transform position all right so with
21:27 - these two positions the ai should have
21:29 - enough data
21:30 - taken from observations of its
21:31 - environment in order to be able to
21:32 - complete its task
21:34 - so we're passing in these two
21:35 - observations and back in here let's now
21:37 - look at the vector observation
21:39 - parameters
21:40 - so we see a space size so this is how
21:42 - many inputs we're going to give it
21:44 - and back in our code you might think
21:46 - that we're sending in two inputs
21:48 - however we're only sending in two
21:49 - positions and you have to remember that
21:52 - a position is really a vector three
21:54 - which is composed of three
21:55 - loads for the x y and z so for each of
21:58 - these two positions
21:59 - each of them is passing in three values
22:01 - so with two positions we're actually
22:02 - passing in six values or six floats
22:04 - so in here for the space size of the
22:06 - observation we're going to set it to six
22:09 - then the other parameter is the sect
22:11 - vectors so
22:12 - this is for more advanced use cases
22:14 - where you need the ai to have
22:16 - some sort of memory so if you set it to
22:18 - one then it just takes one observation
22:20 - grabs all of its six values and makes
22:22 - its decision
22:23 - and if you set this to two then it takes
22:26 - one observation
22:27 - and also the last one and uses both of
22:30 - those to make its decision
22:32 - so for example if you pass a stacked
22:33 - vector of more than one
22:35 - and you use the position as the
22:36 - observation then the ai could then infer
22:39 - the direction of the object
22:40 - but like i said that's for more advanced
22:42 - use cases so here let's keep it simple
22:44 - and just put it at one
22:45 - alright so with this we have our
22:46 - observations taken care of and we
22:48 - already saw how the actions work
22:50 - so now let's actually use those actions
22:53 - again the goal in this test is to move
22:55 - the character towards the goal
22:56 - so for that let's set the action space
22:58 - into continuous so we have floats
23:01 - and let's set it to receive two so we're
23:03 - going to receive one for the x movement
23:05 - of our character and another one for the
23:07 - z movement
23:08 - so back here in our code let's grab our
23:11 - actions we're going to define the first
23:13 - position as the x
23:14 - so we float move x we go into our
23:16 - actions
23:17 - in this case we're using continuous
23:18 - actions so you grab that one on index
23:21 - zero this will be our move x
23:23 - and then the other one on index one is
23:25 - for the move z
23:27 - so again like i said the ai only works
23:29 - with numbers so these aren't just floats
23:31 - and it's up to you to define what they
23:33 - represent
23:33 - so here i am saying that the first float
23:36 - on index zero refers to the move
23:38 - x and the one on index one refers to the
23:40 - moveset so we have this and then let's
23:42 - just do a very basic transform so
23:44 - just transform move the position just
23:46 - increase it
23:48 - let's make a new three with the move x
23:51 - with a zero on the y we don't want to
23:52 - move on the y and it moves z
23:54 - then we multiply this by time dot delta
23:56 - time and then by a certain
23:58 - move speed so here we fold for move
24:02 - speed and for now let's leave it just at
24:04 - one
24:04 - okay so with this very basic logic the
24:06 - ai should be able to move the character
24:09 - now once again let's go back to the
24:11 - reinforcement learning cycle
24:12 - we've taken care of the observation
24:14 - decision and action
24:16 - now all that's left is to add a reward
24:19 - our goal here is to have the character
24:20 - hit the target
24:22 - and our character here has a rigid body
24:24 - as well as a box highlighter
24:26 - and then on the target itself it also
24:28 - has a collider with set to trigger so we
24:30 - can easily test for this collision
24:32 - so on the agent here we just add a very
24:34 - basic private void
24:38 - we add a on trigger enter and when we
24:40 - enter the trigger then we have our goal
24:42 - now there's two ways that we can give a
24:44 - reward we can call the function
24:46 - set reward so this one sets the reward
24:49 - to a specific amount
24:51 - and then you also have the other one
24:52 - which is add reward which increments the
24:54 - current reward
24:55 - so for example when making an ai card
24:58 - driver you would increment on every
25:00 - checkpoint you hit but over here we just
25:02 - have a single goal so using set reward
25:04 - is perfect
25:05 - so just call set reward and set it to
25:07 - let's say 1f
25:08 - now the specific value that you choose
25:10 - here doesn't really matter so it can be
25:12 - 1 or 10 or 0.3 or pretty much anything
25:14 - it only matters relative to your other
25:16 - rewards like for example when we hit a
25:18 - wall
25:19 - we should give a large penalty okay so
25:21 - with this we are setting the reward when
25:23 - we hit the collider
25:24 - now another thing about how ml agents
25:27 - works is the concept of episodes
25:29 - so one episode is essentially one run
25:32 - and the episode should end when the
25:34 - character either achieves the final goal
25:36 - or
25:36 - loses so in here after setting the
25:38 - reward let's end our episode
25:40 - so we just call the function and episode
25:44 - so this will end the episode and then
25:45 - when the episode ends
25:47 - the game doesn't actually quit but we
25:49 - need some way of resetting the state so
25:51 - we can train again
25:52 - so for that we can override another one
25:54 - so a public override void
25:56 - and we're going to override the function
25:58 - on episode begin
26:03 - so this one is called as soon as the
26:04 - episode begins and here we can reset
26:06 - everything back to normal
26:08 - now in this very simple example we just
26:09 - need to reset the character position
26:11 - back into its starting state
26:12 - which for now for the simple demo i have
26:14 - here the starting state just on zero
26:16 - zero zero
26:16 - now later on we're going to add some
26:18 - randomness but for now let's just keep
26:20 - it simple and reset it back into the
26:21 - exact same point
26:22 - so here just transform that position and
26:24 - put it on vector 3.0
26:26 - so this will correctly reset the state
26:28 - so that it can train again
26:30 - okay so over here we have almost
26:32 - everything ready to train
26:33 - the last thing we need is just a penalty
26:36 - so here in order to make our training
26:37 - more effective
26:38 - let's add some collectors on the edges
26:40 - so we can give it a negative reward and
26:42 - then end the episode
26:44 - so let's just make a new 3d cue
26:47 - let's name this the wall and let's just
26:49 - put it on the edges
27:01 - okay so here i added some walls just
27:03 - some basic colliders
27:05 - and let's also make it as triggers and
27:07 - now we just need to identify
27:09 - if the player collides with either the
27:10 - goal or the wall
27:12 - so for that let's just make some basic
27:14 - tag components so
27:15 - one for the goal and another one for the
27:18 - wall
27:18 - and just add the empty component just to
27:20 - serve as tags so
27:21 - the wall and the goal so now here when
27:24 - we have the untrigger enter
27:26 - we can go into the other and try get
27:29 - component
27:30 - first of all try get the goal
27:35 - so if it does have a goal then we're
27:36 - going to give a positive reward and end
27:38 - the episode
27:39 - and then we check if it has a wall
27:41 - instead
27:43 - if so then we're going to give a
27:45 - negative reward and also in the episode
27:48 - alright so that's it everything should
27:50 - be almost done
27:51 - now before we actually start training
27:53 - the first thing we should do is validate
27:55 - to make sure that everything is indeed
27:57 - working
27:57 - so for testing there's another thing we
27:59 - can do which is we can drive the actions
28:01 - ourselves
28:02 - so let's override another function so
28:05 - we're going to override this one
28:06 - it's called heuristic and takes and
28:08 - actions out for the action buffers
28:10 - and now here we can essentially modify
28:12 - the actions that will then be received
28:14 - by this function
28:15 - so in this case we are using continuous
28:16 - actions so we go into the actions out
28:19 - and we access the continuous actions
28:21 - this is of type action segment float
28:28 - so we get those and then we can easily
28:29 - modify them so in this case let's use
28:32 - the input to move the character with the
28:33 - arrow keys
28:34 - so we just modify these values so first
28:36 - one on z we've got the move x
28:38 - so let's go into the input to get the
28:40 - axis raw for the horizontal
28:42 - and then the vertical
28:45 - okay so that's it so this is just for
28:47 - testing and now back in the editor we
28:50 - select our agent
28:51 - and over here we have a film for the
28:53 - behavior type so we have default
28:55 - heuristic
28:56 - and inference now in this case we can
28:58 - manually set it to heuristic only which
29:00 - will force it to use heuristics
29:02 - or you can leave it as default and as
29:04 - long as you don't have python with ml
29:06 - agents running and you have no model
29:08 - selected it will automatically use
29:09 - heuristics
29:10 - so if we do like this and we run here is
29:12 - the game running and if i use the mouse
29:14 - keys
29:14 - yep now i can move the character let's
29:17 - just increase the speed by a tiny bit
29:19 - okay i have up the speed now let's make
29:21 - sure that everything is working so first
29:22 - of all movement is working so we are
29:24 - correctly passing in the actions and
29:25 - mapping those actions into movement
29:28 - next let's try hitting the wall so go up
29:30 - there hit a wall
29:31 - and if there you go it does happen so it
29:33 - ends the episode as you can see it reset
29:35 - back into zero
29:36 - and now if we hit the goal yep it also
29:38 - happens alright so here we can verify
29:40 - that everything is perfectly working and
29:42 - we have everything ready for training
29:44 - now in order to train is the exact same
29:45 - thing that we saw previously just in
29:47 - here make sure that the behavior type is
29:49 - set back into default
29:50 - and then open up the command prompt
29:54 - and here let's run the same thing that
29:55 - we saw previously so let's give it a
29:57 - different id and let's say this is test
29:58 - three
29:59 - so hit on enter and yep now it's
30:01 - listening so start training so just
30:02 - press on the playing
30:03 - and if there it is there we have our
30:05 - agent and it's correctly working
30:07 - so you can see now it is indeed going
30:08 - through the training process
30:10 - so it's trying all kinds of values until
30:12 - it finds something that might give it a
30:13 - positive reward
30:15 - now all we have to do is wait however
30:17 - there's one thing that we can do to
30:19 - massively speed up training
30:20 - and let's also solve one potential issue
30:23 - that might happen
30:24 - so if the issue is that if the ai never
30:26 - touches the goal then it might simply
30:28 - learn to avoid the walls and just stay
30:29 - in place forever
30:30 - so we can fix that to make sure that
30:32 - doesn't happen by setting a max step
30:34 - so here on the agent we can see a field
30:36 - for the max step
30:38 - now a step is kind of like an update on
30:40 - the training
30:41 - by default it runs 50 times per second
30:43 - exactly the same as the physics update
30:45 - so here let's give it a max step of
30:47 - something like a thousand
30:48 - just to make sure that the episode ends
30:50 - and doesn't run forever
30:51 - okay so that's one problem solved and
30:54 - here let's just
30:55 - visually hide the walls just so it looks
30:57 - a bit better
30:58 - now in order to speed up training it's
31:00 - very simple we can just use more than
31:02 - one agent
31:04 - so let's take all of our training
31:06 - environment here and put in an
31:07 - actual object so just a container let's
31:10 - name it our environment
31:15 - and let's just drag our entire
31:17 - environment inside of there
31:19 - and then we take this and let's just
31:20 - drag it onto our project files in order
31:22 - to make it into a prefab
31:23 - so we have our prefab and now we simply
31:25 - copy paste this several times
31:27 - so just duplicate put one there another
31:29 - one there
31:30 - and now again you can put as many as you
31:32 - want in order to train quite a bit
31:34 - faster than just one at once
31:39 - all right so there it is here we have 20
31:42 - environments
31:42 - all of them correctly for training now
31:45 - there's one very
31:46 - important thing when using this method
31:48 - here we are duplicating and moving our
31:50 - environments
31:51 - so if you take this approach in order to
31:52 - speed up training you need to make sure
31:54 - that all of your logic works based on
31:56 - low composition and not on global
31:58 - position
31:59 - so for example this character here is
32:01 - indeed on local position of 0
32:03 - but it's on a global position of 13 so
32:06 - if you reset it back into a level
32:08 - position of 0 then it's going to go back
32:09 - in there
32:10 - and not where it should actually go to
32:12 - so here on our logic we're using
32:13 - position and let's just replace all of
32:15 - instances of position within said
32:17 - low composition
32:21 - okay everything shouldn't be working and
32:23 - now here just to make this easier to
32:25 - visualize i'm going to add something
32:27 - so on the script i'm going to add two
32:29 - more fields
32:33 - so just some references to a wind
32:35 - material in loose material and the floor
32:37 - mesh renderer
32:38 - this just so we can visualize the
32:39 - training obviously this is not necessary
32:41 - so just go down here when we have our
32:44 - wind
32:44 - let's set the floor mesh rendering
32:46 - material into the wind material
32:48 - and when we lose let's set it to the
32:49 - unloose material
32:52 - so back in the editor let's open up the
32:54 - prefab select the
32:55 - agent and here we have our fields let's
32:58 - pass in the field for the platform
33:00 - and just the win and the illness
33:01 - material again this is just for visual
33:03 - just to make it easier to see the
33:05 - training happening on the video
33:06 - it's obviously not necessary to actually
33:08 - train the engine
33:14 - okay now before we start mass testing
33:17 - let's make sure everything is working
33:19 - so once again validated just with
33:20 - heuristic only and let's see
33:22 - here's all our agents and yep they all
33:24 - move and it works good
33:26 - and if we go towards the wall yep it
33:28 - turns into red so we can easily
33:29 - visualize that the training failed
33:31 - and in there any open turns too green
33:33 - okay so the logic is working and we can
33:35 - visualize the training
33:36 - now we're ready to do some mass training
33:39 - just go into your agent and make sure
33:41 - that the behavior type is at the default
33:44 - and now with our command prompt let's
33:46 - just run our ml agents learn
33:48 - and for the run id and let's give it a
33:50 - proper id so let's name it
33:52 - move to go okay so just run it and it's
33:55 - ready so just hit the play button
33:57 - and here we can see all the agents
33:58 - happening we see some reds
34:00 - some greens and yep it's actually
34:02 - learning quite quickly
34:04 - so you see some reds happening and now
34:06 - it's really just mostly green
34:08 - so over time the agent is learning and
34:10 - it's constantly getting better and
34:11 - better
34:12 - and with this very simple example after
34:14 - just a tiny bit
34:15 - yep everything is working and we can see
34:16 - pretty much all of them all in green
34:18 - so here we have an ai that correctly
34:20 - learned how to move towards the target
34:22 - goal
34:22 - okay so that's awesome now let's just
34:24 - stop training so just stop the editor
34:26 - and over here in the command prompt you
34:28 - can see it saved the model and the brain
34:30 - is this dot on x file
34:32 - and you can see that copy the results to
34:33 - results move to go move to goal and we
34:35 - have the brain
34:36 - so open up the file explorer and go into
34:38 - your project folder
34:40 - and in here go inside the results in
34:42 - this case we have the move to goal
34:44 - and here we have the move to go on dot
34:45 - onyx this is our brain
34:47 - so just go ahead copy this paste it onto
34:50 - our normal assets
34:51 - and up here we can see the move to goal
34:53 - so we have our nice brain so this is our
34:55 - neural network model
34:56 - and now in order to use this brain let's
34:58 - just select our environment
35:00 - so for now let's disable all the others
35:01 - just to see this one in action
35:04 - so select the agent and just click and
35:07 - drag and assign our neural network model
35:09 - and then on the behavior type we can
35:11 - leave it as default or you can directly
35:13 - set it to inference only
35:14 - inference means it uses the brain model
35:16 - rather than training okay so let's test
35:18 - like this and we should be able to see
35:19 - our character
35:20 - using this brain to achieve the goal and
35:22 - if there it is we have our character
35:24 - correctly using our brain to achieve our
35:26 - goal
35:26 - alright so congratulations you've just
35:28 - trained your very first machine learning
35:30 - ai
35:30 - awesome now the real challenge in
35:32 - machine learning is how to do training
35:34 - effectively
35:35 - so there's the design of your training
35:37 - scenario which matters a lot
35:39 - so for example over here we test an
35:41 - extremely simple possible setting
35:43 - so we're just getting a character to
35:45 - move from here all the way to here
35:47 - so that's what the eon learned however
35:50 - if i now take this goal
35:51 - and i just move it down here and yep
35:54 - there you go all of a sudden the
35:55 - character does not know what to do
35:57 - with the way that we set up our training
35:58 - our ai only learned that it moves to the
36:00 - right and gets a reward
36:01 - so by moving the transform it didn't
36:03 - actually learn how to go into the actual
36:05 - goal position
36:06 - so it's a very simple example of the eye
36:08 - doesn't know what to do since it wasn't
36:10 - trained for a moving goal
36:12 - so this is why when training usually you
36:14 - want to add some randomness to prevent
36:15 - the ai from being trained on just
36:17 - one very specific scenario so there's a
36:19 - lot that you can do in order to define a
36:21 - proper training scenario
36:22 - and then there's also tons of parameters
36:24 - that you can play around with the
36:25 - parameters for the algorithm are stored
36:27 - in a configuration file
36:29 - so if you go into the github page onto
36:31 - the docs for the learning environment
36:33 - create the new
36:34 - and you scroll all the way down here we
36:36 - can see the format for the training yaml
36:38 - file
36:39 - so here i'm just going to go ahead and
36:41 - copy all this
36:42 - then onto the project folder let's make
36:44 - a new folder keep things nice and
36:46 - organized name it config
36:48 - and now in here let's create just a
36:50 - brand new text object
36:52 - name it move to goal.yml
36:56 - then just open this with notepad or any
36:58 - text editor
36:59 - and here just pass in those parameters
37:01 - now here i will not go into too much
37:03 - detail onto every single one of these
37:05 - parameters
37:06 - if you want you can go into the github
37:08 - docs to see what each one does
37:09 - so here there's pretty much just one
37:11 - thing that we need to change
37:13 - which is over here this name here is the
37:14 - name of the brain that we want to train
37:16 - so here in our agent we give it the
37:18 - behavior name move to goal so that's
37:19 - what we need to add
37:20 - so here instead of rollerball let's use
37:22 - that name okay so go ahead save that
37:25 - file
37:25 - so here it is on the config folder move
37:27 - to gold.yml
37:28 - and now once you have this file you can
37:30 - run training using these parameters
37:32 - so just open up the command editor and
37:35 - we're going to run the ml
37:36 - agents dash learn and then we pass in
37:38 - the config so it's on config
37:41 - and then we have in this case the move
37:42 - to goal dot yaml
37:45 - and then let's give it a run id let's
37:48 - name it test
37:48 - parameters and now it's the same as
37:52 - previously so just click on enter
37:54 - and now it's ready to run so here on the
37:56 - engine let's set it back into the
37:58 - default so that it runs training and run
38:00 - it
38:00 - and yep now the agent is training and
38:02 - it's training using those custom
38:04 - parameters
38:04 - again like i said go check out that page
38:06 - to see what they all do
38:08 - now with this one more thing we need to
38:09 - learn is how do we improve upon a model
38:12 - so previously we made this model which
38:13 - works pretty well the character goes
38:15 - there and it goes towards the target but
38:18 - as we saw if we suddenly move the goal
38:20 - and all of a sudden the character
38:21 - completely fails
38:22 - so we can take this model and improve
38:24 - upon it so first of all let's improve
38:26 - the actual training scenario so let's
38:28 - add some randomness to both the start
38:29 - position of the goal as well as the
38:31 - character
38:32 - so over here when we have the on episode
38:34 - begin let's take the transform local
38:36 - position and add some randomness
38:38 - so new factor 3 random.range and let's
38:41 - see the random values
38:42 - so here is the agent on local position
38:44 - of 0 so let's go from that one
38:47 - so on minus three and let's go up to
38:49 - maybe plus one
38:51 - so for the x from minus three f to plus
38:53 - one f
38:54 - then for the y let's leave it at zero
38:56 - and then for the z let's see
38:58 - so let's go from -2 all the way to plus
39:01 - two
39:06 - okay so we have the character on a
39:08 - random position
39:10 - and then let's also move the transform
39:12 - target
39:14 - so here let's take the goal and let's
39:17 - see the randomness
39:18 - first of all on the x let's start on
39:20 - this one so on 2.4 and we're going to
39:22 - random
39:22 - up to that so between 2.4 and 5.
39:27 - so here between two point four f and
39:29 - five f
39:31 - and then for these end let's start from
39:34 - all the way down there so from minus two
39:36 - to plus two
39:37 - okay so with this every time we start a
39:39 - new episode we're going to select
39:40 - different random positions
39:42 - so this will enable the model to
39:44 - actually learn how to go towards the
39:46 - target rather than just a specific
39:47 - position
39:48 - now once again before we do anything
39:50 - let's validate to make sure that
39:51 - everything is working so in here let's
39:53 - choose heuristic only
39:55 - and yep it's spawned on a random
39:56 - position and now if i end
39:58 - and if it's on a different position
39:59 - different different and so on okay so
40:01 - both the character
40:02 - and the goal they're both on random
40:04 - positions okay so now let's run
40:06 - training and improve upon the previous
40:08 - model so for that let's run the same
40:10 - thing so the ml agents
40:11 - learn we pass in the config and then the
40:14 - way that we learn from
40:15 - a previous brain is dash dash initialize
40:19 - from and then we pass in the run id that
40:23 - we previously used
40:25 - which was moved to goal so it's going to
40:27 - load up that brain
40:29 - and then let's give it another id so
40:30 - dash dash run dash id
40:32 - equals move to goal 2 okay so let's
40:35 - press
40:35 - enter now it's ready to learn and in
40:38 - here let's just enable
40:39 - all the other environments and make sure
40:41 - that this one is set into custom so it
40:43 - learns and lets it on play
40:45 - and up here we have the training at work
40:47 - and you can see that they are indeed
40:49 - going into random positions
40:51 - and we've got some reds and some greens
40:53 - and yep it seems to be working
40:55 - all right now there's one last thing
40:56 - related to machine learning which is a
40:58 - nice visualization
40:59 - so let's look at that while our train is
41:01 - running so for that open up a brand new
41:03 - command prompt
41:04 - and here let's go into the project
41:05 - folder
41:11 - then once again go inside the virtual
41:13 - environment so
41:14 - vnv script activate
41:19 - so we are inside the virtual environment
41:21 - and now in here let's run the command
41:23 - tensorboard so tensorboard is the name
41:26 - of the utility that visualize our
41:27 - results
41:28 - and then we pass in the folder with our
41:30 - results which by default is named
41:31 - results so
41:32 - dash dash then log here and log there is
41:35 - results
41:36 - so click on enter and yep now we see
41:38 - this message so tensorboard is running
41:40 - on this url so localhost on port
41:43 - 6006. so then just open up a browser
41:46 - and go into localhost 6006.
41:49 - and yep here it is and we can now
41:51 - visualize everything so most importantly
41:53 - over here we see our cumulative reward
41:56 - so we gave it a goal of one when it hits
41:58 - the target so we should be able to see
42:00 - this constantly increasing as the brain
42:02 - becomes a lot better
42:03 - then the episode length is also going
42:05 - down meaning that the ai
42:07 - is learning how to get to the goal
42:08 - faster and on the command prompt you can
42:10 - see every time it updates so right now
42:12 - it's updating the graph on every 10 000
42:14 - steps
42:15 - so just click on refresh and over there
42:18 - we see this is the one that we're
42:19 - currently running and as you can see it
42:20 - started off in there and it
42:21 - raised all the way up there if we look
42:23 - into our unity build we can see that it
42:25 - is indeed working
42:26 - so we've got pretty much a sea of green
42:28 - so even with the random positions it
42:30 - seems that our ai has already learned
42:32 - how to go towards the target
42:33 - then down here you can also visualize
42:35 - the policy so these are all the inner
42:37 - workings of it
42:39 - so you've got tons of things like the
42:40 - beta the entropy
42:42 - you've got the reward estimate and so on
42:44 - so here you have tons of graphs for you
42:46 - to analyze and
42:47 - improve your training and your ai now
42:50 - back in here we can see that the
42:51 - training went very well
42:52 - so we can just stop training and once
42:54 - again we see that we save the model onto
42:56 - that position
42:57 - so here let's go into results this is
42:59 - the move to go to
43:00 - let's copy the brain paste it onto our
43:03 - assets
43:04 - and now here we do the same thing to use
43:05 - the spring so let's hide the other
43:07 - environments
43:08 - and just leave on this one and select it
43:10 - set it to use that brain and set it to
43:12 - inference
43:13 - and if we run and if there it is we can
43:15 - verify that our training went indeed
43:17 - very well
43:18 - so even with random positions the ai is
43:20 - smart enough to actually know
43:21 - that the goal is not just to move to the
43:23 - right rather to move towards the goal
43:25 - so here we have fully trained our ai
43:27 - from scratch without giving it any
43:29 - specific commands
43:30 - again remember how all we did was we
43:33 - gave it the current position and the
43:34 - target position
43:35 - we did not tell it how to move we did
43:37 - not tell it what move means we did not
43:39 - tell it any of that
43:40 - so the aion learned to take those values
43:42 - and learn what it needed to do in order
43:44 - to gain a reward
43:45 - so that almost feels like magic that is
43:47 - the awesome power of machine learning
43:49 - alright so now you know everything in
43:50 - order to get started working with
43:52 - machine learning and ml agents in unity
43:54 - machine learning is some really exciting
43:56 - tech with tons of potential applications
43:57 - so definitely stay tuned for some more
43:59 - awesome videos
44:00 - you can explore the official examples
44:02 - which have tons of awesome use cases
44:04 - if you have a specific scenario you'd
44:06 - like to see let me know in the comments
44:08 - also i'm currently working on a match 3
44:10 - use case so definitely stay tuned for
44:11 - that
44:12 - and like i said there's a playlist in
44:14 - the description that i won't keep
44:15 - updated as i explore ml agents more and
44:17 - more
44:17 - so if you're watching this in the future
44:19 - check that link to see all the videos
44:21 - alright so this video was a ton of work
44:22 - to make but i really hope you learned a
44:24 - lot
44:25 - if you did please hit the like button
44:26 - and consider subscribing this video is
44:28 - made possible thanks to these awesome
44:30 - supporters
44:30 - go to patreon.com unitycodemonkey to get
44:33 - some perks and help keep the videos free
44:35 - for everyone
44:36 - as always post any questions you have in
44:38 - the comments and i'll see you next time
44:50 - you

Cleaned transcript:

hello there i'm your code monkey and let's learn how to use machine learning and ml agents in unity this is a very powerful toolkit that lets you create some extremely intelligent ai it helps you solve tons of problems that would simply be impossible to solve while using classic app there's immense massive potential in this toolkit so you should know how to use it so you know how it can help you and when to apply it this is a long video but it's the only video you're going to need in order to learn how to get started working with machine learning in unity we're going to start completely from scratch and go through the entire installation process then learn how to use it by setting up a scene to train an ai using reinforcement learning and finally look at the results to see the ai in action using our trained brain model so make sure you watch the video until the end to understand the whole process this video is meant to help you get started and after watching it go check out the playlist link in the description where i will be adding videos covering interesting use cases made with machine learning in unity for example i'm currently working on a specific match 3 use case and many other different ones so stay tuned for that now the way machine learning works in unity is through the ml agent's toolkit which combines several tools first you have the ml agents python package which runs the machine learning algorithm then you have your learning environment which is your unity scene with the game running and then you have the ml agent c sharp package which lets you define the data that you feed into the algorithm as well as using the resulting brain so let's go through that whole process starting from scratch first over here is the github page for the ml agents package there's a link in the description you can find tons of documentation here so definitely give it a look you have a quick readme talking about how the whole thing works all the features the release the documentations and so on you've got the docs folder where you have all the documentation so tons of topics on installation getting started making some environments and so on and it also has lots of awesome examples which you can browse around to see how they work now the first thing we need to do is actually install python and as of the time of this recording the recommend python version is either 3.6 or 3.7 so over here on the python website i'm going to go ahead and download 3.7.9 again if you're watching this in the future make sure you check the official docs to see which version you should install so go ahead just download and install it after installing python open up the command prompt so just click on the start button and type cmd so here it is and now there is actually one quirky thing about windows 10 which is in theory you should be able to run python by just typing in python however if you do here on windows 10 it opens up the microsoft store instead of actually running python so if you see this behavior the solution is to instead of python just type py so over here instead of python just py and i hit enter and there you go now i'm inside python and over here you can verify that first of all python is running and you can verify you have the correct version which in this case 3.7.9 okay so far so good now let's just exit out of python okay back in the command line now the next step is we need to change the directory to go to our unity project so over here is the unity project i'm going to use so just go ahead copy the entire path and on the command prompt just change directory onto that directory okay now in here what we're going to do is create a python virtual environment this will help us by keeping all of our projects separate so each virtual environment is completely separate from the others meaning that we can have multiple projects in the same machine each of them using their own python packages and they will not cause conflicts with each other so again first go into your unity project directory and then in here we're going to type the command pythonm means we're going to run a module and one we want to run is called vnv to create a virtual environment and then afterwards this requires a folder name where the environment won't be created so just keep things nice and organized and give it the exact same name so just vnv so this will create the virtual environment inside a folder named vnv now if you're on linux or mac the commands are slightly different so check the official docs and again like i said previously if you have the issue with python not running when you type python then here instead of python just type py mvnv vnv so go ahead hit on enter and yep now it's creating the virtual environment all right it's done and you can verify that it worked by opening up your file explorer and yep over here there's a folder called vnv and over there we have our virtual environment over here you see some folders and you see this one with a bunch of scripts and here we see a activate script this is how we're actually going to activate the virtual environment so back in the command line we go inside the vnv folder we access the scripts folder and then run the activate so when you do yep the command prompt changes over there it says vnv so we are now inside the python virtual environment so any changes that you make here will not impact any other python projects you have in your machine like for example any other unt projects with other python libraries now before we install our python packages let's make sure our installer is updated the python packaging seller is named pip so in order to make sure that it's using the latest version let's run the command pythonm we're going to do a pip install dash dash upgrade and we're going to want to upgrade our pip package so just go ahead hit on enter and you have now it's successfully installed the latest pip package okay so far so good now we can begin installing our packages and the first one we're going to need is a package called pytorch this is an open source library for performing computations using data flow graphs so it's the underlying representation of the deep learning models for that let's run the command pip install and here we need a specific version so here it is in song torch version 1.7 and download it from this website now if you have issues or you're watching this many months in the future check the github installation docs to see which version to use so you go ahead run this and now we wait for it to complete okay pytorch is now installed next up we install the ml agents package so we just do pip install ml agents however just running like this may give you some compatibility errors so let's try it and see and up here we see an error where we have the incorrect numpy version so it's incompatible so if you didn't get that error then it's fine just keep going but if you did get the error just like i did then the solution is to use a different package resolver so do pip install ml agents and then dash dash use the feature and the feature is the 2020 resolver so if you use the new resolver and run if there you go now you can see it's uninstalling the incorrect version and selling the correct one alright so now we have all of our correct version and we can verify that the ml agents package was correctly installed by running the command go into mlagentslearn and then use the commandhelp and hit enter and yep if everything went correctly then you should be able to see the helm files for the ml agents learned package so here it is everything went correctly so everything is installed correctly and as of the time of this recording i'm using ml legends release 10 with the python package 0.22 so again if you have any issues or you're watching this many months in the future check the official docs for any version changes okay so far so good and with that the python side is all installed correctly now there's actually one more optional step here if you look in the console you might be seeing some warning messages now these are not directly related to the mlegens package but whether it's due to one of the dependencies so if you're watching this in the future with another ml engine's release it might not show these warnings something saying it could not load a dynamic library with the name cuda rt64 underscore 101 so if you see something like that it's telling you that it cannot find the cooldown libraries now this is optional everything will run just fine without them so if you have no gpu you can skip this step and it will use your cpu instead of your gpu but if you do have an nvidia gpu you can optionally install cuda so if you see that message pay attention to the name of the missing library so in my case i was seeing a missing library that ends with underscore 101.dll so that means that it requires cuda version 10.1 so just go into nvidia's website and download cuda however again pay attention to the version as of the time of this recording the latest cuda version is actually version 11 however the missing library is version 10. so when you go into the download page don't download version 11. instead go into the archive in this case we're looking for version 10.1 so go ahead and download that one after installing it if you once again run ml agent stash learn dash dash help if you run that again you should be able to see that the warnings are gone so it now finds the cooldown libraries however you might be seeing another warning which again check the library name so you might be seeing another missing library named could nn64 underscore seven so this is the cuda deep neural network library so once again just go into nvidia's website and search for coo dnn so over here just go ahead and download it but again pay attention to the library name again in my case it's missing the code dnn64 underscore 7 meaning that it uses version 7 and again the latest one is actually version eight so when you download it make sure you download the correct version seven when you download it you get a zip file and inside you see a cuda folder and a bunch of files so in order to install it you just go into your cuda installation folder so in my case i put it on default so on program files and nvidia gpu computing toolkit then inside you see the cuda folder and inside we see we go into version 10.1 and then over here we see our various folders so just go ahead and copy all these so the include folder the lib and the bin so just take these and drag them all in there and after doing that you can verify by going inside the bin library and over here you should be able to find all the dlls so in my case the cuda rt64101 and the other one is the could nn 64 underscore seven and now if we run ml agent learn help now you should be able to see everything run without any warnings so we're here the command ran and nope no warnings okay so far so good so with this we have all the setup for the python side including the optional codon libraries now let's go into our unity project over here i have a project that's pretty much just brand new so just a simple demo i have prepared the ml engines package works with any unity version starting from 2018.4 now i want to make sure that this video stays relevant for as long as possible so in this project i'm currently using 2020.2 but everything works exactly the same if you're using 2019.4 or if you're in the future using the 2020 lts version so to install the ml agents package just go ahead open up the package manager then here select the packages and make sure you go into the unt registry and in here just scroll down and find the ml agents package here you can see the latest stable package which is the time this recording is version 1.0.6 and again i want this video to be relevant for a long time so i'm going to instead install the latest preview package so for that i'm going to click on the gear icon go into the advanced project settings and in here i'm going to enable the preview packages and yep i understand so over here on ml legends i can expand it see the other versions and here i see the latest preview package which at the time this recording is 1.6 but again if you're in the production stage of your development and you want maximum stability then go with the stable unity lts version as well as a stable ml agents package so select your choice and go ahead and click on install okay it's done and you can verify that everything installed correctly by just creating an empty game object and over here if you go into add component you should be able to now see a group for the ml agents and over here the various scripts alright so we have everything correctly installed now over here for testing i have this demo which is pretty much taken from the official examples it's just a nice character and over here is the goal so the objective is to teach this character to move towards the goal and not fall off the map so let's see how we actually use our ml agents now for that first we need to create an agent an agent is what's going to run our ai both for training and then for playing and in order to make an agent we just make a normal c sharp script so just right click over here going to create a new c sharp script and let's name this the move to goal agent and go ahead open up the script and now in here we need to go up here to add using go inside unity and let's use the unity ml agent then over here let's get rid of the default methods we don't need them right now and instead of inheriting from modern behavior we're going to inherit from the agent class so over here you can right click on the agent and go into the definition and here we see the definition of that class so as you can see we have a whole bunch of methods all of them related to machine learning now the way the agent learns is through reinforcement learning so it's based on a relatively simple loop of observation where the agent gathers data from its environment then it makes a decision based on the data that has and then it takes an action and if it does the right action then it gets a reward so this is a continuous cycle where the agent grows to learn based on its observations and what actions lead to the highest rewards okay so let's see how to implement this cycle again here is the agent class and we're going to need to override two functions so we're going to need to override this one collect observations in order to give the agent some observations and then we're also going to need to override this one which receives a buffer with all of our actions okay so let's go back into our script so our move to goal agent and first let's look at how the ai takes actions so we're going to do a public override and we're going to override the on action received which takes an action buffer so this buffer then contains our actions as either floats or ends now one thing to keep in mind is that the machine learning algorithm only works with numbers meaning that it doesn't have an understanding of what exactly is a player object or what it means to move to the right all it knows is numbers it's easier to understand this if we see it in action so for now let's go back into the editor and over here let's select our agent so i have my nice agent in here and i'm just going to drag the move to goal agent and attach it in there and yep here's our move to go agent script and when we added this it also added the behavior parameter script these are the various parameters that our ai uses first we have the behavior name so let's rename this to move to go so give it a proper name to this agent and then over here let's look at the vector action so let's learn what all of these mean first of all you've got the space type so in here you can choose between the script and continuous now essentially discrete are whole numbers so you can have 0 1 2 3 and so on and continuous are floats so going between minus 1 to plus 1 and all the numbers in between so 0.2.3 minus 0.4 and so on we're going to do a quick test to see these differences in a bit let's just learn about the other parameters so first if you select continuous over here you see the space size and this is how many actions you won't get on the vector so for example if you put a 2 here then in the code if we inspect our action buffers we see that this contains two action segments one for the continuous actions and one for discrete actions so the action segment here is essentially an array so when you set the space size you are defining the size for the array of the type that you select so if you set a space size of two then this will have two positions with two values a value on index zero and another one on index one and then if you choose discrete the size is the same so it's how many values you get on that array but then you also have the second parameter which is the maximum value for this branch so like i said discrete means integers or whole numbers so for example if you put a 1 in here then you won't get an action value of just zero however if you put a two then you're going to get an action value of either zero or one and if you put a five then you can get a zero one two three or four and each branch can have its own size so for example if you're making a car ai you would make the first branch refer to accelerating and breaking so you would put it with two values and then for the second branch let's say it would represent turning so you would put three values one for turning left turning right and don't turn okay now before we go further and look into how to define training let's just test these actions to get a better understanding for how all of this works so first let's put a discrete with just one branch keep it simple and let's put it with a size of five and now over here in the code we simply do a debug.log we go inside the action buffers then we access our discrete actions and let's just print out what's on index 0. since we have just one branch that means we have one value on this array so that value is on index zero okay now before we can test this ai we need to add one more thing over here in our agent let's add the component go into ml agents and we're going to add a decision requester like i said previously the way reinforcement learning works is through a cycle of observation decision action and reward so in order to take an action we need to first request the decision and what this script does is simply request a decision every certain amount of time and then takes actions now there are other ways of requesting decisions but for now let's just use this simple script okay so over here we are ready to begin training and run our test to see what the ai will output over here so for that let's go back into our command prompt and here make sure you are inside the virtual environment and in order to train it it's very easy we just run the command ml agents dash alert so just hit enter and yep we see a nice ascii unity logo and a message telling us that we can start training by pressing the play button so just do that just in here press the play button and yep we now have our training running so we can check in the command prompt yep we have everything running so it listens that and it's running our training and over here we can check on the console and now we can verify and see what the actual action vector contains so let's hit on collapse and over here we can see that we put just one branch with a branch size of five so over here we do have values going from zero one two three and four so we have five values zero to four so this is what it means to have a discrete vector with a branch size of five now let's test with the continuous type to see what actions we get so here let's just swap it from discrete into continuous and with space size of just one then here in the code it's pretty much the same the only difference is we access the action segment for the continuous actions instead of discrete and the position is the same on index zero and now we want to run this test so here in the command line first of all we can see that the previous test worked correctly so we have our training we have our model and so on and here if we run the exact same command ml agent learn if we run it like this there it is we get an error and the error is because we're trying to run training again on using the same default id so over here we have two options we can call our ml agents learn with the force tag so this will override the previous data or we can specify a different id name so let's try doing that so dash dash run dash id equals and then something name so say test2 so now if we go ahead hit on enter and yep we have it we are listening on the port so just start training so just hit it and let's see and yep we have training running and now we can see what this one does so we can see what a continuous action looks like so over here we are getting values pretty much between minus one and plus one and everything in between all right so now you should have a better understanding of exactly how the actions work so discrete is integers and in continuous we've got floats from 1 to plus one so as you can see these are really just numbers so it's up to you to decide what they represent now let's look at another part of the reinforcement learning cycle let's look at observations so back in the code here the way we collect observations is by overriding a function so we just do public override and we're going to override this one collect observations which takes a vector sensor and as soon as you do up here it will add the using unity ml agent sensors so this is where this sensor exists so we have our collect observations function and now observations are how the agent observes its environment so think of it kind of like the inputs for the ai and obviously this will differ based on what problem you're trying to solve so essentially you need to think about what data does the ai need in order to solve the problem you're giving it now our goal in this example is we have a character and we have a goal and we want to move the character towards the goal so if you think about it if you are controlling the player so what information do you need well first of all obviously you need to know where you are so we should pass in the player position and then you also need to know where the target is so we also need to pass in that position so over here in the script how we pass that into the ai is very simple we just go into the sensor and call the function add observation and over here first let's pass in the transform.position so the player position so with this the ai will have the data for the player position and then let's also pass in the target position so up here let's just add a serialized film for a reference so a transform for the target transform so back in the editor we have our film let's just drag the gold transform on there and in here we do the same thing sensor add an observation and pass in the target transform position all right so with these two positions the ai should have enough data taken from observations of its environment in order to be able to complete its task so we're passing in these two observations and back in here let's now look at the vector observation parameters so we see a space size so this is how many inputs we're going to give it and back in our code you might think that we're sending in two inputs however we're only sending in two positions and you have to remember that a position is really a vector three which is composed of three loads for the x y and z so for each of these two positions each of them is passing in three values so with two positions we're actually passing in six values or six floats so in here for the space size of the observation we're going to set it to six then the other parameter is the sect vectors so this is for more advanced use cases where you need the ai to have some sort of memory so if you set it to one then it just takes one observation grabs all of its six values and makes its decision and if you set this to two then it takes one observation and also the last one and uses both of those to make its decision so for example if you pass a stacked vector of more than one and you use the position as the observation then the ai could then infer the direction of the object but like i said that's for more advanced use cases so here let's keep it simple and just put it at one alright so with this we have our observations taken care of and we already saw how the actions work so now let's actually use those actions again the goal in this test is to move the character towards the goal so for that let's set the action space into continuous so we have floats and let's set it to receive two so we're going to receive one for the x movement of our character and another one for the z movement so back here in our code let's grab our actions we're going to define the first position as the x so we float move x we go into our actions in this case we're using continuous actions so you grab that one on index zero this will be our move x and then the other one on index one is for the move z so again like i said the ai only works with numbers so these aren't just floats and it's up to you to define what they represent so here i am saying that the first float on index zero refers to the move x and the one on index one refers to the moveset so we have this and then let's just do a very basic transform so just transform move the position just increase it let's make a new three with the move x with a zero on the y we don't want to move on the y and it moves z then we multiply this by time dot delta time and then by a certain move speed so here we fold for move speed and for now let's leave it just at one okay so with this very basic logic the ai should be able to move the character now once again let's go back to the reinforcement learning cycle we've taken care of the observation decision and action now all that's left is to add a reward our goal here is to have the character hit the target and our character here has a rigid body as well as a box highlighter and then on the target itself it also has a collider with set to trigger so we can easily test for this collision so on the agent here we just add a very basic private void we add a on trigger enter and when we enter the trigger then we have our goal now there's two ways that we can give a reward we can call the function set reward so this one sets the reward to a specific amount and then you also have the other one which is add reward which increments the current reward so for example when making an ai card driver you would increment on every checkpoint you hit but over here we just have a single goal so using set reward is perfect so just call set reward and set it to let's say 1f now the specific value that you choose here doesn't really matter so it can be 1 or 10 or 0.3 or pretty much anything it only matters relative to your other rewards like for example when we hit a wall we should give a large penalty okay so with this we are setting the reward when we hit the collider now another thing about how ml agents works is the concept of episodes so one episode is essentially one run and the episode should end when the character either achieves the final goal or loses so in here after setting the reward let's end our episode so we just call the function and episode so this will end the episode and then when the episode ends the game doesn't actually quit but we need some way of resetting the state so we can train again so for that we can override another one so a public override void and we're going to override the function on episode begin so this one is called as soon as the episode begins and here we can reset everything back to normal now in this very simple example we just need to reset the character position back into its starting state which for now for the simple demo i have here the starting state just on zero zero zero now later on we're going to add some randomness but for now let's just keep it simple and reset it back into the exact same point so here just transform that position and put it on vector 3.0 so this will correctly reset the state so that it can train again okay so over here we have almost everything ready to train the last thing we need is just a penalty so here in order to make our training more effective let's add some collectors on the edges so we can give it a negative reward and then end the episode so let's just make a new 3d cue let's name this the wall and let's just put it on the edges okay so here i added some walls just some basic colliders and let's also make it as triggers and now we just need to identify if the player collides with either the goal or the wall so for that let's just make some basic tag components so one for the goal and another one for the wall and just add the empty component just to serve as tags so the wall and the goal so now here when we have the untrigger enter we can go into the other and try get component first of all try get the goal so if it does have a goal then we're going to give a positive reward and end the episode and then we check if it has a wall instead if so then we're going to give a negative reward and also in the episode alright so that's it everything should be almost done now before we actually start training the first thing we should do is validate to make sure that everything is indeed working so for testing there's another thing we can do which is we can drive the actions ourselves so let's override another function so we're going to override this one it's called heuristic and takes and actions out for the action buffers and now here we can essentially modify the actions that will then be received by this function so in this case we are using continuous actions so we go into the actions out and we access the continuous actions this is of type action segment float so we get those and then we can easily modify them so in this case let's use the input to move the character with the arrow keys so we just modify these values so first one on z we've got the move x so let's go into the input to get the axis raw for the horizontal and then the vertical okay so that's it so this is just for testing and now back in the editor we select our agent and over here we have a film for the behavior type so we have default heuristic and inference now in this case we can manually set it to heuristic only which will force it to use heuristics or you can leave it as default and as long as you don't have python with ml agents running and you have no model selected it will automatically use heuristics so if we do like this and we run here is the game running and if i use the mouse keys yep now i can move the character let's just increase the speed by a tiny bit okay i have up the speed now let's make sure that everything is working so first of all movement is working so we are correctly passing in the actions and mapping those actions into movement next let's try hitting the wall so go up there hit a wall and if there you go it does happen so it ends the episode as you can see it reset back into zero and now if we hit the goal yep it also happens alright so here we can verify that everything is perfectly working and we have everything ready for training now in order to train is the exact same thing that we saw previously just in here make sure that the behavior type is set back into default and then open up the command prompt and here let's run the same thing that we saw previously so let's give it a different id and let's say this is test three so hit on enter and yep now it's listening so start training so just press on the playing and if there it is there we have our agent and it's correctly working so you can see now it is indeed going through the training process so it's trying all kinds of values until it finds something that might give it a positive reward now all we have to do is wait however there's one thing that we can do to massively speed up training and let's also solve one potential issue that might happen so if the issue is that if the ai never touches the goal then it might simply learn to avoid the walls and just stay in place forever so we can fix that to make sure that doesn't happen by setting a max step so here on the agent we can see a field for the max step now a step is kind of like an update on the training by default it runs 50 times per second exactly the same as the physics update so here let's give it a max step of something like a thousand just to make sure that the episode ends and doesn't run forever okay so that's one problem solved and here let's just visually hide the walls just so it looks a bit better now in order to speed up training it's very simple we can just use more than one agent so let's take all of our training environment here and put in an actual object so just a container let's name it our environment and let's just drag our entire environment inside of there and then we take this and let's just drag it onto our project files in order to make it into a prefab so we have our prefab and now we simply copy paste this several times so just duplicate put one there another one there and now again you can put as many as you want in order to train quite a bit faster than just one at once all right so there it is here we have 20 environments all of them correctly for training now there's one very important thing when using this method here we are duplicating and moving our environments so if you take this approach in order to speed up training you need to make sure that all of your logic works based on low composition and not on global position so for example this character here is indeed on local position of 0 but it's on a global position of 13 so if you reset it back into a level position of 0 then it's going to go back in there and not where it should actually go to so here on our logic we're using position and let's just replace all of instances of position within said low composition okay everything shouldn't be working and now here just to make this easier to visualize i'm going to add something so on the script i'm going to add two more fields so just some references to a wind material in loose material and the floor mesh renderer this just so we can visualize the training obviously this is not necessary so just go down here when we have our wind let's set the floor mesh rendering material into the wind material and when we lose let's set it to the unloose material so back in the editor let's open up the prefab select the agent and here we have our fields let's pass in the field for the platform and just the win and the illness material again this is just for visual just to make it easier to see the training happening on the video it's obviously not necessary to actually train the engine okay now before we start mass testing let's make sure everything is working so once again validated just with heuristic only and let's see here's all our agents and yep they all move and it works good and if we go towards the wall yep it turns into red so we can easily visualize that the training failed and in there any open turns too green okay so the logic is working and we can visualize the training now we're ready to do some mass training just go into your agent and make sure that the behavior type is at the default and now with our command prompt let's just run our ml agents learn and for the run id and let's give it a proper id so let's name it move to go okay so just run it and it's ready so just hit the play button and here we can see all the agents happening we see some reds some greens and yep it's actually learning quite quickly so you see some reds happening and now it's really just mostly green so over time the agent is learning and it's constantly getting better and better and with this very simple example after just a tiny bit yep everything is working and we can see pretty much all of them all in green so here we have an ai that correctly learned how to move towards the target goal okay so that's awesome now let's just stop training so just stop the editor and over here in the command prompt you can see it saved the model and the brain is this dot on x file and you can see that copy the results to results move to go move to goal and we have the brain so open up the file explorer and go into your project folder and in here go inside the results in this case we have the move to goal and here we have the move to go on dot onyx this is our brain so just go ahead copy this paste it onto our normal assets and up here we can see the move to goal so we have our nice brain so this is our neural network model and now in order to use this brain let's just select our environment so for now let's disable all the others just to see this one in action so select the agent and just click and drag and assign our neural network model and then on the behavior type we can leave it as default or you can directly set it to inference only inference means it uses the brain model rather than training okay so let's test like this and we should be able to see our character using this brain to achieve the goal and if there it is we have our character correctly using our brain to achieve our goal alright so congratulations you've just trained your very first machine learning ai awesome now the real challenge in machine learning is how to do training effectively so there's the design of your training scenario which matters a lot so for example over here we test an extremely simple possible setting so we're just getting a character to move from here all the way to here so that's what the eon learned however if i now take this goal and i just move it down here and yep there you go all of a sudden the character does not know what to do with the way that we set up our training our ai only learned that it moves to the right and gets a reward so by moving the transform it didn't actually learn how to go into the actual goal position so it's a very simple example of the eye doesn't know what to do since it wasn't trained for a moving goal so this is why when training usually you want to add some randomness to prevent the ai from being trained on just one very specific scenario so there's a lot that you can do in order to define a proper training scenario and then there's also tons of parameters that you can play around with the parameters for the algorithm are stored in a configuration file so if you go into the github page onto the docs for the learning environment create the new and you scroll all the way down here we can see the format for the training yaml file so here i'm just going to go ahead and copy all this then onto the project folder let's make a new folder keep things nice and organized name it config and now in here let's create just a brand new text object name it move to goal.yml then just open this with notepad or any text editor and here just pass in those parameters now here i will not go into too much detail onto every single one of these parameters if you want you can go into the github docs to see what each one does so here there's pretty much just one thing that we need to change which is over here this name here is the name of the brain that we want to train so here in our agent we give it the behavior name move to goal so that's what we need to add so here instead of rollerball let's use that name okay so go ahead save that file so here it is on the config folder move to gold.yml and now once you have this file you can run training using these parameters so just open up the command editor and we're going to run the ml agents dash learn and then we pass in the config so it's on config and then we have in this case the move to goal dot yaml and then let's give it a run id let's name it test parameters and now it's the same as previously so just click on enter and now it's ready to run so here on the engine let's set it back into the default so that it runs training and run it and yep now the agent is training and it's training using those custom parameters again like i said go check out that page to see what they all do now with this one more thing we need to learn is how do we improve upon a model so previously we made this model which works pretty well the character goes there and it goes towards the target but as we saw if we suddenly move the goal and all of a sudden the character completely fails so we can take this model and improve upon it so first of all let's improve the actual training scenario so let's add some randomness to both the start position of the goal as well as the character so over here when we have the on episode begin let's take the transform local position and add some randomness so new factor 3 random.range and let's see the random values so here is the agent on local position of 0 so let's go from that one so on minus three and let's go up to maybe plus one so for the x from minus three f to plus one f then for the y let's leave it at zero and then for the z let's see so let's go from 2 all the way to plus two okay so we have the character on a random position and then let's also move the transform target so here let's take the goal and let's see the randomness first of all on the x let's start on this one so on 2.4 and we're going to random up to that so between 2.4 and 5. so here between two point four f and five f and then for these end let's start from all the way down there so from minus two to plus two okay so with this every time we start a new episode we're going to select different random positions so this will enable the model to actually learn how to go towards the target rather than just a specific position now once again before we do anything let's validate to make sure that everything is working so in here let's choose heuristic only and yep it's spawned on a random position and now if i end and if it's on a different position different different and so on okay so both the character and the goal they're both on random positions okay so now let's run training and improve upon the previous model so for that let's run the same thing so the ml agents learn we pass in the config and then the way that we learn from a previous brain is dash dash initialize from and then we pass in the run id that we previously used which was moved to goal so it's going to load up that brain and then let's give it another id so dash dash run dash id equals move to goal 2 okay so let's press enter now it's ready to learn and in here let's just enable all the other environments and make sure that this one is set into custom so it learns and lets it on play and up here we have the training at work and you can see that they are indeed going into random positions and we've got some reds and some greens and yep it seems to be working all right now there's one last thing related to machine learning which is a nice visualization so let's look at that while our train is running so for that open up a brand new command prompt and here let's go into the project folder then once again go inside the virtual environment so vnv script activate so we are inside the virtual environment and now in here let's run the command tensorboard so tensorboard is the name of the utility that visualize our results and then we pass in the folder with our results which by default is named results so dash dash then log here and log there is results so click on enter and yep now we see this message so tensorboard is running on this url so localhost on port 6006. so then just open up a browser and go into localhost 6006. and yep here it is and we can now visualize everything so most importantly over here we see our cumulative reward so we gave it a goal of one when it hits the target so we should be able to see this constantly increasing as the brain becomes a lot better then the episode length is also going down meaning that the ai is learning how to get to the goal faster and on the command prompt you can see every time it updates so right now it's updating the graph on every 10 000 steps so just click on refresh and over there we see this is the one that we're currently running and as you can see it started off in there and it raised all the way up there if we look into our unity build we can see that it is indeed working so we've got pretty much a sea of green so even with the random positions it seems that our ai has already learned how to go towards the target then down here you can also visualize the policy so these are all the inner workings of it so you've got tons of things like the beta the entropy you've got the reward estimate and so on so here you have tons of graphs for you to analyze and improve your training and your ai now back in here we can see that the training went very well so we can just stop training and once again we see that we save the model onto that position so here let's go into results this is the move to go to let's copy the brain paste it onto our assets and now here we do the same thing to use the spring so let's hide the other environments and just leave on this one and select it set it to use that brain and set it to inference and if we run and if there it is we can verify that our training went indeed very well so even with random positions the ai is smart enough to actually know that the goal is not just to move to the right rather to move towards the goal so here we have fully trained our ai from scratch without giving it any specific commands again remember how all we did was we gave it the current position and the target position we did not tell it how to move we did not tell it what move means we did not tell it any of that so the aion learned to take those values and learn what it needed to do in order to gain a reward so that almost feels like magic that is the awesome power of machine learning alright so now you know everything in order to get started working with machine learning and ml agents in unity machine learning is some really exciting tech with tons of potential applications so definitely stay tuned for some more awesome videos you can explore the official examples which have tons of awesome use cases if you have a specific scenario you'd like to see let me know in the comments also i'm currently working on a match 3 use case so definitely stay tuned for that and like i said there's a playlist in the description that i won't keep updated as i explore ml agents more and more so if you're watching this in the future check that link to see all the videos alright so this video was a ton of work to make but i really hope you learned a lot if you did please hit the like button and consider subscribing this video is made possible thanks to these awesome supporters go to patreon.com unitycodemonkey to get some perks and help keep the videos free for everyone as always post any questions you have in the comments and i'll see you next time you
