00:00 - hey there how's it going everybody in
00:01 - this video we're going to be learning
00:02 - how we can group and aggregate our data
00:04 - now if you don't know what grouping and
00:06 - aggregating really entails then I'd
00:08 - really recommend sticking around for
00:10 - this video because basically this is
00:12 - what most people think of when they
00:13 - think of actually analyzing data in a
00:15 - meaningful sense so this will be the
00:17 - first video where we actually get some
00:19 - statistics back on our data sets and
00:21 - aren't just modifying our data frames in
00:23 - different ways so for example maybe you
00:25 - want to know what the average salary for
00:27 - a developer is or maybe you want to know
00:29 - how many people from each country knows
00:32 - Python or another programming language
00:33 - so what we're going to learn here is
00:35 - going to allow us to answer those types
00:37 - of questions now I would like to mention
00:39 - that we do have a sponsor for this
00:40 - series of videos and that is brilliant
00:42 - so I really want to thank brilliant if
00:44 - we're sponsoring this series and it
00:46 - would be great if you all could check
00:47 - them out using the link in the
00:48 - description section below and support
00:49 - the sponsors and I'll talk more about
00:51 - their services in just a bit so with
00:53 - that said let's go ahead and get started
00:55 - ok so before we start doing some more
00:58 - advanced data analysis let's start off
01:00 - slow and build up to the more advanced
01:02 - stuff so that all of this makes sense
01:03 - along the way so I have my developer
01:06 - survey data open here that we've been
01:08 - using throughout this series and as
01:10 - usual if you'd like to follow along then
01:12 - I have links to this code and the data
01:14 - in the description section below so
01:16 - let's look at some basic aggregations so
01:19 - if you don't know what aggregation means
01:20 - basically it means that we're going to
01:22 - be combining multiple pieces of data
01:24 - into a single result so for example if
01:27 - you've ever used a mean median or mode
01:29 - in mathematics these are aggregate
01:32 - functions because they take multiple
01:34 - values and give you either the mean
01:36 - median or mode of those results so if we
01:40 - wanted to run some analysis on our
01:42 - developer survey here one question we
01:44 - might ask is okay what is a typical
01:47 - salary for developers who answered this
01:49 - survey so that might be some good
01:51 - information to have if you're looking
01:53 - for a job and want to get an idea of
01:55 - what these salaries look like at the
01:57 - moment so to do this we can grab the
01:59 - median salaries of our data frame so
02:02 - first let's look at these salaries so
02:05 - our salary column within this data frame
02:07 - here of all these survey results is
02:09 - called converted comp and that is
02:12 - converted to
02:13 - u.s. dollars it's actually further over
02:17 - here in the survey it is about right
02:21 - here so I'm gonna copy that now first
02:25 - let's just look at this column so as
02:28 - we've seen before we can just access the
02:31 - column just like we're accessing a key
02:33 - of a dictionary and I'm gonna grab the
02:35 - first let's get the first 15 salaries or
02:38 - so so I'm gonna look at the head of the
02:40 - of the results here and these are
02:43 - salaries here that developers put down
02:46 - for this survey and these na n values
02:49 - here just mean not a number in this
02:51 - context it means that they just skipped
02:53 - that question in the survey okay
02:55 - so we can see the median salary for this
02:59 - survey just by running the median method
03:01 - on this series so to do this I'm going
03:04 - to go ahead and copy what I have here
03:06 - and now instead of looking at the head I
03:09 - can just run median on that series so if
03:13 - I run this then we can see that the
03:15 - median salary for this survey was around
03:18 - fifty seven thousand dollars so that
03:20 - takes all of the salaries responses from
03:22 - our survey from this series here and it
03:25 - gives us the median value of all of
03:27 - those and ignores the na n values now
03:30 - this probably doesn't give us as much
03:32 - information as we'd really like to have
03:34 - so for example different countries pay
03:37 - different amounts since there are
03:39 - different cost-of-living and things like
03:41 - that so it'd be nice if we could look at
03:43 - the median salary broken down by country
03:45 - and we'll look at that here in a second
03:47 - when we learn about grouping data but
03:49 - first I want to cover a few more basic
03:51 - concepts before we move on to grouping
03:54 - so one thing that I'd like to look at is
03:56 - running these aggregate functions on our
03:58 - entire data frame so let's see what we
04:01 - get if we just run this median function
04:04 - that we just ran on our entire data
04:06 - frame instead of just this single series
04:08 - so here I'm just going to say D F dot
04:12 - median so we're no longer accessing just
04:14 - a single column so if I run this then it
04:17 - might take a second to spin up here so
04:19 - when I do this it's going to look
04:21 - through our data frame and find the
04:23 - columns that contain numerical values
04:25 - where it can grab a median
04:27 - you and some of these might not make
04:30 - sense to use with the median but others
04:33 - might be pretty useful to us so for
04:35 - example we can see that the median age
04:37 - down here at the bottom for this survey
04:40 - was 29 years old and the median number
04:43 - of work hours per week that was 40 which
04:47 - is pretty standard so that makes sense
04:49 - now if you want to get a broad overview
04:51 - of your data and a statistical overview
04:53 - we can use the describe method on our
04:56 - data frame instead so if I instead run
05:01 - describe instead of median and I run
05:04 - this then this is going to give us a
05:06 - broad overview of some different stats
05:08 - so if we look at the converted comp
05:11 - column here then we can see a few
05:13 - different stats about this column so it
05:16 - gives us the count it gives us the mean
05:19 - it gives us the standard deviation the
05:22 - minimum and then it also gives us the 25
05:25 - 50 and 75 percent quantiles here now
05:29 - this 50 percent marker is just the
05:31 - median value by the way and just like we
05:34 - saw before when we look this median
05:37 - value up specifically this is around
05:39 - 57,000 now this is in scientific
05:42 - notation here so it looks a little bit
05:43 - different basically this means that we
05:45 - just need to move four spots over from
05:47 - the decimal point so one two three four
05:50 - so that would be 57,000 there so this
05:53 - described method gives us a bunch of
05:55 - these aggregates in one place if we just
05:58 - want to get a quick overview of our data
06:00 - now if you're wondering why I wanted to
06:02 - look at the median of our salaries
06:04 - instead of the mean which is the average
06:06 - basically it's because the mean is
06:08 - affected too heavily by outliers it's
06:11 - not really a good metric to use because
06:13 - a few outliers can affect the average
06:15 - very heavily we can see that the mean
06:19 - salary up here if I highlight this right
06:22 - here if we were to count this up then
06:24 - that's actually about a hundred and
06:25 - twenty-seven thousand dollars on average
06:28 - but that gives us an unrealistic
06:30 - expectation of what a typical developer
06:33 - salary is because the largest salaries
06:36 - in our data set are just pulling up that
06:38 - average so heavily so in cases like that
06:41 - definitely want to use the mean instead
06:43 - I think that's a better representation
06:44 - or I'm sorry you're going to want to use
06:46 - the median instead because that's a
06:48 - better representation now if we only
06:50 - wanted to get this overview for a single
06:53 - column then we could just run this
06:55 - describe method on a single column as
06:57 - well and get those results for that
06:59 - specific series now you might be
07:01 - wondering what that count value is
07:04 - listed at the top of these described
07:06 - results now the count value is the
07:09 - number of non na rows which basically
07:12 - means that it counts the non missing
07:14 - rows so in the context of this survey
07:17 - and missing row just means that the
07:19 - respondent didn't answer that a specific
07:21 - question so if I look at the count for
07:24 - the converted comp column so I'm gonna
07:27 - go up here and grab this and instead of
07:31 - grabbing the median I'm just going to
07:33 - grab the count we can see here that only
07:36 - about 55 the 65 or 55 to 56 thousand
07:40 - people answered that question now I
07:42 - think there are about 89 thousand rows
07:44 - for this data so that means that there
07:46 - are about thirty thousand people or so
07:48 - who didn't answer the salary question on
07:50 - this survey now I sometimes see the
07:53 - mistake that some people think that the
07:54 - count function will count up the
07:56 - individual values in a specific row and
07:59 - report how many of those values were in
08:01 - the column but if that's what you're
08:03 - trying to do then that's what we would
08:05 - use the value counts function for now in
08:08 - case that doesn't quite make sense let's
08:10 - look at an example to see what this
08:12 - looks like so for example we had the
08:15 - question on the survey that asked each
08:17 - person whether they coded in their free
08:19 - time as a hobby so to see all of these
08:21 - responses for that question we can look
08:24 - at the hobbyist
08:25 - column so I'll just access that hobbyist
08:29 - column here and run that and we can see
08:31 - that we get a series returned here and
08:33 - these are just a bunch of yes-or-no
08:36 - questions so it was just a yes-or-no
08:38 - question that each person answered so
08:40 - you might get the survey results back
08:42 - and you might think to yourself okay
08:44 - well I can see the responses here in the
08:46 - survey but I just want to know how many
08:48 - people answered yes and how many people
08:50 - answered no so how would we do that
08:53 - well we can get that information
08:54 - with the value counts function so if I
08:58 - just look at the value counts and that
09:01 - is value underscore counts if we run
09:05 - that method on that series then that is
09:08 - going to give us a breakdown of how many
09:10 - people answered yes and how many people
09:13 - answered no as to whether or not they
09:15 - code as a hobby so I use the value
09:17 - counts all the time when exploring data
09:19 - and we can find out some interesting
09:21 - things from our survey by using this on
09:23 - some different fields so for example
09:25 - there is a question on this survey that
09:28 - asks each person what social media
09:30 - platform they use the most so if you're
09:33 - building an app or a website and want to
09:35 - keep track of the most popular social
09:37 - media sites then you might be interested
09:39 - in what the most popular answers to that
09:41 - question are so to view these results we
09:43 - can access the social media column of
09:46 - the survey so let me do that and before
09:51 - I run value counts on this let me just
09:54 - show you what this column looks like so
09:57 - this column was called social media so
10:00 - I'm gonna run this and we can see that
10:02 - respondent number one said that they use
10:05 - Twitter more than any other social media
10:06 - this person used Instagram reddit reddit
10:09 - Facebook YouTube and so on now I've
10:12 - pointed this out in previous videos so
10:14 - far but if you've forgotten or if this
10:16 - is your first video that you've watched
10:18 - in this series then at the top of my
10:20 - notebook here I've also loaded in a
10:22 - schema data frame right here and this
10:27 - data frame tells us the exact question
10:30 - that was asked on the survey for each of
10:33 - these column names so for example if we
10:35 - want to see the exact question that was
10:37 - asked for this social media column then
10:39 - I can just access that schema data frame
10:42 - and do a dot Lok because the indexes are
10:47 - going to be the column names and then we
10:50 - can just search for social media and if
10:52 - I run that then we can see that the
10:54 - question that they asked on the survey
10:55 - specifically was what social media site
10:58 - do you use the most okay so we can see
11:00 - that we get a few different responses
11:03 - here but which of these are the most
11:05 - popular so to find that out let's look
11:07 - at the value
11:08 - of this series to see what the most
11:10 - popular social media sites are overall
11:12 - for these developers so I'm going to run
11:16 - this and then I'm going to run that
11:18 - value counts function here and now we
11:21 - can see here at the top that reddit was
11:24 - the most popular with about 14,000
11:26 - people and then we have YouTube whatsapp
11:28 - Facebook Twitter Instagram I don't use
11:31 - social media was one of the answers now
11:34 - we also have some foreign social
11:37 - networks here so I've never heard of
11:39 - these but I believe these are Chinese
11:41 - characters so this is probably a Chinese
11:45 - social media site I don't know really
11:49 - Russian writing but I would assume that
11:51 - this is Russian writing here so this is
11:53 - probably a Russian social media site so
11:55 - it's kind of interesting seeing all of
11:57 - these different answers from around the
11:59 - world
11:59 - now one more quick tip if we want to see
12:02 - these broken down by percentage instead
12:04 - of raw numbers then we can pass in the
12:07 - normalized argument to the value counts
12:09 - function and set that equal to true so
12:12 - let me show you what this looks like so
12:14 - I can say normalize equals true and now
12:19 - we're going to get these broken down by
12:21 - percentage so 17% of the people said
12:24 - that they use reddit 16 said YouTube
12:27 - about 16 said whatsapp and so on okay so
12:30 - we can see that we have some social
12:32 - media sites here from some other
12:34 - countries so obviously this is most
12:36 - likely a regional thing my guess would
12:39 - be that the popularity of these social
12:40 - media platforms varies a lot based on
12:43 - what country you're in so how would we
12:46 - break up these results so that we can
12:48 - see the most popular social media sites
12:50 - for each country now in order to do this
12:53 - we're going to have to learn about
12:54 - grouping our data so again this is a
12:57 - topic that can be a little confusing
12:58 - when you first see it so let me start
13:01 - off slow so that we can see exactly
13:02 - what's going on here so first of all if
13:05 - we want to see specific results based on
13:07 - the country or based on some other
13:10 - column then we're going to have to group
13:12 - on that specific column and we have the
13:15 - group by function for this so what
13:17 - actually does it mean to say that we're
13:19 - going to use the group by function so
13:21 - the pandas documentation it says that a
13:24 - group by operation involves some
13:27 - combination of splitting the object
13:29 - applying a function and combining the
13:32 - results so I'm gonna try to walk through
13:34 - each of those processes one at a time so
13:36 - that we can see exactly how this works
13:38 - so again in the pandas documentation it
13:41 - says that a group by operation involves
13:44 - some combination of splitting up our
13:47 - object applying a function and then
13:50 - combining those results so let's do each
13:52 - of those now first just for a reference
13:55 - let's display the value counts for each
13:58 - country so that we can see the countries
14:00 - that have the most results for this
14:02 - particular survey so to do this we can
14:05 - just access the country column and if I
14:09 - run this we can see that this gives us
14:12 - the country that each respondent said
14:14 - that they were from and if we look at
14:16 - the value counts for this then this is
14:18 - going to tally up all of the unique
14:21 - responses so we can see that the
14:23 - majority of this survey was answered by
14:25 - developers in the United States and in
14:28 - second was India then Germany United
14:31 - Kingdom Canada and so on okay so now
14:34 - let's look at how to use the group by
14:36 - function on our country column so first
14:38 - we're going to split the object and then
14:41 - we're going to apply a function and then
14:43 - it will combine those results so first
14:46 - let's look at splitting the object now
14:48 - in this case we want to group all over
14:50 - the results by country so to do this we
14:53 - can simply say D F dot group by and then
14:59 - we will pass in this is going to be a
15:02 - list of columns that we want to group
15:04 - one and I'm just going to pass in a
15:06 - single column here for country so if I
15:10 - run this then what we get back here is
15:12 - this data frame group by object so what
15:16 - is this object and what exactly can we
15:18 - do with this
15:19 - so first let's explain a bit what this
15:21 - is so this object contains a bunch of
15:24 - groups and to better understand what
15:26 - this is let's take a look at an
15:29 - individual group that this data frame
15:31 - has now before we do that I'm going to
15:34 - set this
15:35 - as a variable so that we can reuse this
15:37 - and not have to retype our code over and
15:40 - over and also it'll be easier to read so
15:43 - I'm going to call this country group and
15:46 - I'm just going to set this equal to this
15:48 - DF group by and now instead of typing
15:51 - this every time we can just reference
15:53 - this country group variable here so now
15:57 - let's take a look at one of these groups
16:00 - so since we grouped our rows by country
16:03 - then we can grab a specific group by
16:06 - country name so I'll grab the group for
16:09 - the United States so to do this we can
16:12 - say country group dot get underscore
16:16 - group and then pass in the name of the
16:18 - group in this case I want to get the
16:20 - group for United States so if I run this
16:23 - cell whoops and this is telling me that
16:25 - country group is not defined and it's
16:28 - because I didn't rerun this cell up here
16:30 - after I set that variable so if I run
16:33 - this and grab the group for the United
16:35 - States then we can see that we get a
16:38 - data frame returned here with some
16:40 - survey results so this doesn't look like
16:43 - anything special yet but if I look at
16:45 - the country name for each of these
16:47 - survey results the country is listed
16:49 - right here then we can see that all of
16:52 - these responses are from people who said
16:54 - that they were from the United States
16:56 - and if I look at the group for India so
17:00 - if I instead change United States to
17:03 - India here and grab that group if we
17:05 - look at the country here then these are
17:07 - all the survey results for people who
17:09 - said that they were from India so that's
17:11 - what our data frame group by object that
17:14 - we saw before consists of it has broken
17:16 - up all of the different responses into
17:19 - groups by country name so this would be
17:22 - similar to running a filter on our
17:24 - original data frame so I should be able
17:26 - to get these same results for a single
17:28 - country just by doing what we've seen in
17:31 - previous videos and creating a filter so
17:34 - I could say okay I want to grab I want
17:38 - our filter to be equal to any time the
17:41 - country is equal to the United States
17:46 - and then I can apply this
17:49 - to our data frame by saying okay D F dot
17:52 - Lok and give me all the results that
17:55 - match that filter and if I run this cell
17:58 - then we can see over here in the country
18:01 - column that all of these results are
18:03 - respondents from the United States so if
18:06 - we're just looking to get information on
18:08 - a single country then it's very similar
18:10 - to just creating a filter like we did
18:12 - here but instead of just grabbing the
18:16 - results for one country group by instead
18:19 - splits all of these responses up by
18:21 - country name so now that we have all of
18:23 - those split up and grouped by country
18:25 - name now we can apply a function and
18:28 - bring those results back together so
18:30 - what kind of function would we like to
18:32 - apply well like I mentioned before maybe
18:35 - we want to see the most popular social
18:37 - media sites broken down by country now
18:40 - if you just wanted to get the most
18:42 - popular social media sites by the United
18:45 - States or by India then we've already
18:47 - seen how we can do this
18:49 - so right here I have some filtered
18:51 - results down to where we have the
18:53 - responses for the United States so we
18:57 - can just do what we did before where we
18:58 - ran the value counts method on the
19:01 - social media column so I could just say
19:04 - here at the end I could access that
19:07 - social media column of that filtered
19:09 - data frame and then I could just run
19:11 - value counts here so if I run this then
19:16 - we can see that for the United States we
19:19 - have Reddit and Twitter and Facebook and
19:21 - YouTube as the top four social media
19:24 - sites and if we wanted to look at these
19:26 - specifically for India then I could
19:29 - instead change that filter for India and
19:31 - run this and we can see that whatsapp
19:34 - came first and then YouTube then
19:36 - LinkedIn and then Facebook so these are
19:39 - the results for one specific country but
19:42 - if we were to run this on our data frame
19:44 - group by object then it will give us the
19:47 - results for all of those country groups
19:49 - so if it helps you with how you think
19:51 - about this you can imagine that it's
19:53 - similar to running a filter and then
19:56 - applying a function like we did here
19:57 - with a single country but when we group
19:59 - these using the group by function and
20:01 - then apply a
20:03 - function then it will combine those
20:05 - groups to give us the results for all of
20:07 - those unique countries so I think this
20:09 - will make sense once we just see this
20:11 - here so remember I called our group up
20:14 - here country group so if we come down
20:17 - here to the bottom then we can say okay
20:21 - for the country group now I want to look
20:25 - at the social media column and I want to
20:28 - grab the value counts for that column
20:31 - for that entire country group so if I
20:34 - run this then what this returns is a
20:37 - series with the most popular social
20:39 - media sites broken down by country now
20:42 - this actually cuts off a little early
20:43 - here so let me grab a larger chunk of
20:46 - this series to get a better idea of what
20:47 - this looks like so right here at the end
20:49 - I'm just gonna say dot head and look at
20:52 - the top 50 results or so so if we run
20:56 - this then we can see here that our first
20:58 - country is Afghanistan and we can look
21:00 - at the most popular social media for
21:02 - that and then go down the list Albania
21:05 - Algeria Argentina and so on now this is
21:08 - actually returning a series and this
21:11 - series has multiple indexes it has this
21:13 - country index and the social media index
21:16 - now we haven't discussed multiple
21:17 - indexes in this series yet but if anyone
21:20 - is curious about how this works then
21:22 - maybe just leave a comment in the
21:23 - description section below and maybe we
21:25 - can cover that topic in a future video
21:27 - but the country is the first index and
21:30 - we can grab these just like we would
21:33 - with any other series so again if I
21:36 - wanted to grab those most popular social
21:38 - media sites for India for example then I
21:42 - could just come up here and with that
21:45 - returned series actually let's take a
21:47 - look at this again so here's the index
21:50 - here I can grab that series just by
21:52 - saying dot lok and then looking for
21:56 - India and we can see that those are the
21:58 - same results that we got before now you
22:00 - might be wondering well hey if those are
22:03 - the same results that I got before then
22:05 - why is this even useful and it's useful
22:08 - because now we can see this result with
22:10 - any country without running a filter on
22:13 - each individual country in the world
22:15 - so for example if I wanted to see the
22:18 - most popular social media sites for the
22:20 - United States then now instead of you
22:23 - know changing a filter over and over I
22:25 - could just you know go here and look at
22:29 - the United States index for this return
22:31 - series and now we can see those results
22:33 - so I think it's really interesting
22:35 - being able to play around with your data
22:37 - like this and being able to explore I
22:40 - really like seeing the different results
22:42 - for different countries and a lot of
22:43 - these sites I've never heard of so for
22:46 - example if we look at the most popular
22:47 - social media sites and China or in
22:50 - Russia then let me look at China here so
22:54 - we can see that yeah it does look like
22:56 - that was a Chinese social media site
23:00 - this way chat or WeChat and then we have
23:03 - I'm assuming this is pronounced Weibo
23:06 - maybe but yeah I think that's very
23:08 - interesting
23:09 - if we want to look at Russia then we
23:14 - can't actually say just Russia in this
23:16 - survey Russia was called the Russian
23:19 - Federation I've made that mistake before
23:22 - where I just type in Russia and it'll
23:24 - tell you that it cannot find and index
23:27 - with that name so this is actually
23:29 - Russian Federation and if we search for
23:32 - that then we can see I don't know how to
23:34 - pronounce this but the one that I
23:37 - thought was Russian writing before it
23:39 - does look like that was in fact Russian
23:41 - and just remember if it makes more sense
23:43 - for you to look at percentages instead
23:47 - of just raw numbers here then you can
23:49 - always set normalize equal to true and
23:51 - it will give you percentage results
23:53 - instead of the raw numbers so we can see
23:55 - that this Russian social media site here
23:58 - has 30 percent or 30 percent of the
24:01 - people from Russia said that that was
24:03 - their most popular social network and if
24:07 - we go back to China then we can see that
24:10 - this one here at the top that has 67
24:13 - percent of the developers from China
24:15 - said that that was the social media site
24:18 - that they used the most so I just
24:19 - thought that was really interesting
24:21 - being able to play around with these
24:22 - numbers and seeing the different results
24:25 - for different countries and this is the
24:27 - kind of thing that we can do once we
24:28 - got these skills down within pandas and
24:31 - a lot of the times it's just fun being
24:33 - able to explore your data like this and
24:35 - finding things within your data that you
24:37 - might not have expected now bringing
24:39 - this back to what we were discussing at
24:41 - the beginning of the video we can also
24:43 - use this to run more traditional
24:44 - aggregate functions like mean median and
24:47 - things like that so before we looked at
24:49 - the median salaries for the entire
24:52 - survey but now let's break these down by
24:54 - country instead so just like we looked
24:57 - at the value counts of the social media
24:59 - column we can look at the median of the
25:01 - salary column and that salary column is
25:04 - labeled converted comp so to do this I
25:07 - can just grab our country group here and
25:10 - we want to look at this converted column
25:15 - and now we need to tell it what
25:16 - aggregate function we want to see for
25:18 - all these countries and I want to see
25:20 - the median salaries for all these
25:22 - countries so if I run this then we can
25:25 - see that our result here is that it says
25:27 - okay here is the median salary in
25:29 - Afghanistan here it is for Albania and
25:31 - so on so now if you wanted to for
25:34 - example see the median salary in a place
25:37 - like Germany then we can just simply
25:39 - come up here and this is the result that
25:42 - we get here and these are our indexes so
25:45 - the index the indexes are country name
25:48 - so if I want to grab a specific country
25:50 - then I can just use dot loke and type in
25:53 - the country name so if I run this then
25:56 - we can see that the median salary here
25:58 - in Germany is about 63,000 now maybe
26:01 - you're working on some analysis where
26:03 - you want to group your data but you also
26:05 - want to run multiple aggregate functions
26:07 - on your group so let's say that we just
26:10 - didn't want to see the median but we
26:12 - also wanted to see the mean as well so
26:14 - to do this we can use the AGG method AGG
26:18 - and pass in all of the aggregate
26:20 - functions that we want to use so to do
26:23 - this here I could just say let me grab
26:25 - where we ran our median here instead of
26:29 - running just the median aggregate
26:32 - function we're going to use this AGG
26:34 - method here AGG and now we're going to
26:37 - pass in a list of the aggregate
26:39 - functions so let's say that I want to
26:41 - get the median first
26:43 - and then I also want to be able to see
26:44 - the mean so if we run this then we can
26:48 - see that we get a data frame with the
26:50 - mean and the median salaries for every
26:52 - country and again just like we did
26:54 - before if I wanted to narrow this down
26:56 - by a specific country then we could
26:58 - easily do that just by grabbing one of
27:02 - these indexes here by country name so if
27:05 - we wanted to look at the mean and median
27:07 - salaries for Canada then I could just
27:10 - come up here and say dot Lok and then
27:13 - pass in Canada here let me spell that
27:16 - correctly and now we can see the median
27:19 - salary and the mean salary for Canada
27:22 - now depending on what you're trying to
27:23 - do you might run into some issues that
27:25 - you didn't quite expect so for example
27:28 - let's say that you're trying to figure
27:30 - out how many people in each country know
27:32 - how to use Python so before we do this
27:35 - to our group let's first look at how we
27:37 - do this with a single country using the
27:40 - filtering approach that we used earlier
27:42 - so I'm going to scroll up to where we
27:45 - had that filter and I'm going to copy
27:47 - that and paste that in down here and
27:50 - then I'm just going to get rid of this
27:51 - value counts section here so currently
27:55 - the filter that we have here is we are
27:58 - filtering the countries down to people
28:00 - who said that they were from India so
28:02 - now in order to figure out how many
28:04 - people said that they knew Python within
28:06 - this survey we're going to use the
28:08 - string methods that we've seen in
28:10 - previous videos and if you don't
28:11 - remember what these look like then we
28:14 - could do this by doing something like
28:16 - this we could say okay I want all of the
28:20 - responses for the people who said that
28:23 - they were from India and now when I get
28:26 - that result remember that this result
28:29 - here is just going to be a filtered
28:30 - version of our data frame our original
28:32 - data frame and now we can say okay
28:35 - I also want the language worked with is
28:40 - where they put the differences the
28:42 - different languages that they actually
28:43 - use so if we look at this language
28:46 - worked with column here then we can see
28:49 - that they list all of the languages that
28:50 - they said that they know and to see if
28:52 - Python is within this column here
28:56 - then I can say dot STR and use the
28:58 - string class on that return series and
29:01 - say okay we want where the STR dot
29:05 - contains Python so this will return true
29:09 - for the rows that have Python and the
29:12 - languages worked with and false for the
29:15 - responses that don't so if I run this
29:18 - then this just returns a series of true
29:21 - and false values where it tells us
29:23 - whether the language worked with column
29:25 - for each respondent contained that
29:27 - string of Python now if we want to
29:30 - actually count the number of people who
29:32 - know Python then we can use the sum
29:34 - function to add all of these up now
29:36 - normally you might think that some would
29:38 - only work with numerical data but some
29:41 - will also work on boolean x' it will
29:43 - count all of the truths as one and all
29:45 - the falses as zero so to find out how
29:48 - many people know Python then I could
29:51 - simply just do a dot sum here at the end
29:54 - and if I run this then we can see that
29:57 - around 30 100 people from India who
30:01 - answered the survey said that they knew
30:03 - Python as one of the languages that they
30:06 - work with now before when we wanted to
30:09 - run a similar aggregation function on
30:11 - our data frame group by object we simply
30:15 - took the same approach on our group by
30:17 - object so for example you might think
30:19 - that we could just do something like
30:21 - this to see all of these to see how many
30:24 - people knew Python from each country you
30:27 - might think that we could say okay well
30:29 - I should just be able to do this I could
30:32 - just say okay for this country group I
30:35 - want to look at this language worked
30:37 - with column and then see the strings
30:40 - that contain Python and sum those up but
30:44 - if I run this here then we can see that
30:46 - we get an error now like I said in a
30:49 - previous video sometimes it can be hard
30:51 - to read these pandas errors and
30:53 - understand exactly what we did wrong but
30:55 - in this case it actually gives us a
30:57 - pretty good clue as to what we did wrong
30:59 - it tells us that we cannot access the
31:02 - attribute string of a series group by
31:05 - object and then it says try using the
31:07 - apply method instead so the reason that
31:10 - we
31:10 - this error here is because this is no
31:12 - longer just a series instead this is a
31:15 - series group by object and it tells us
31:18 - to instead it used the apply method so
31:21 - when we run an apply method on a group
31:23 - object like this we're going to specify
31:25 - a function that we want to be run on
31:28 - every series in this group and I know
31:30 - that can sound a little bit confusing so
31:32 - let's actually see what this looks like
31:34 - and hopefully it'll clear this up a bit
31:37 - so instead of accessing this string
31:41 - class directly here I'm instead going to
31:45 - use the apply method and for anybody
31:47 - following along or who will download
31:49 - this I'm gonna go ahead and leave this
31:51 - cell with this error here so that you
31:55 - can run that and reproduce that error
31:57 - and then I'm gonna do the correct way in
31:59 - this cell so again instead of using it
32:02 - the string class directly on this series
32:06 - group object I'm instead going to use
32:08 - the apply method so let me just cut that
32:11 - out and I'll say dot apply and now we
32:16 - can apply a function that we want to run
32:18 - on each series in this group so if
32:22 - you've seen one of the previous videos
32:24 - then you'll know that if we just want a
32:27 - nice quick easy function then we can use
32:29 - a lambda function you could write
32:31 - another separate function if you wanted
32:33 - to but here I'm gonna use lambda so
32:36 - lambda here is going to be a series so
32:41 - now we can say okay well what do we want
32:43 - to return all right well I want to
32:45 - return X and then since this is a series
32:50 - we can say X dot string dot contains
32:54 - Python dot sum so again just one more
32:57 - time we are running the apply method on
33:00 - this series group and then we are
33:02 - passing in a function that is going to
33:04 - run on each one of these series and the
33:07 - function that we want or what we want
33:09 - returned from that function is the sum
33:11 - of any of the values in that series that
33:17 - contain the string Python and it's going
33:19 - to do that for every country since we're
33:21 - using this country group
33:23 - so if I run this then we can see here
33:27 - that we see okay in Afghanistan eight of
33:31 - the respondents said that they know
33:32 - Python Albania was twenty three and so
33:34 - on now seeing these numbers by itself
33:36 - isn't really that big of a help if we're
33:38 - trying to get an understanding of the
33:40 - percentage of people in each country who
33:43 - said that they know Python because with
33:44 - these results here we only see a single
33:47 - number we'd have to go back and forth
33:49 - and compare okay how many people answer
33:51 - the survey from each country and how
33:54 - many of them use Python and then we
33:56 - could do a calculation from there to
33:58 - figure out the percentage of people from
34:00 - that country who knew Python but we
34:02 - don't want to do that that is too much
34:04 - to do manually so we want to figure our
34:07 - way so that we can get Python and pandas
34:10 - to do this calculation for us now a lot
34:12 - of people have asked me to put together
34:13 - coding problems to practice what we
34:16 - learn in these videos so you can think
34:18 - of this as practice so I'll do this here
34:21 - so can any of you think of a way where
34:25 - we can figure out what percentage of
34:27 - people in each country know how to use
34:29 - Python if you think that you can figure
34:31 - that out then you can pause the video
34:33 - here and try to work through this
34:35 - yourself and it's going to combine a few
34:37 - topics that we've discussed in the
34:39 - series so far in order to do this but
34:41 - with that said I'm going to go ahead and
34:43 - move along with my solution so again if
34:45 - you want to try to figure that that out
34:47 - on your own then you can pause the video
34:49 - and try to work that out and if you did
34:52 - do that then I hope that you were able
34:54 - to get something figured out there but
34:56 - if not then no worries let's go ahead
34:58 - and walk through my solution here so
35:00 - that you can use this as practice to get
35:02 - better with pandas so that you can do
35:04 - this type of analysis in the future so
35:07 - like I said in order to get the
35:09 - percentage of developers who know Python
35:12 - for each country we're going to use a
35:14 - combination of a few different things
35:16 - that we've learned throughout this
35:17 - series so far now there are probably
35:19 - several different ways of answering this
35:21 - question and if you have a different way
35:24 - that you answered this question than me
35:26 - then definitely leave it in the
35:27 - description section below so that people
35:29 - can see different approaches to this you
35:31 - know it's absolutely possible that
35:33 - there's a more efficient way than how
35:35 - I'm about to do it here
35:36 - so if there is then I'll highlight that
35:38 - so others can see what the best approach
35:41 - is but here's how I'm gonna do this so
35:44 - first I'm gonna grab the total number of
35:46 - respondents from each country that way
35:49 - we know the total number of people from
35:51 - each country who responded to this
35:53 - survey so I will just call this country
35:57 - respondents and I will set this equal to
36:00 - we want to grab the value counts of the
36:06 - countries here so if I print out what we
36:11 - get here we've seen this before
36:13 - whoops and I got an error there because
36:15 - I put County I meant to put country so
36:17 - if I look at this then these are the
36:20 - total number of respondents who said
36:22 - that they were from each country and
36:23 - again we saw this earlier in the video
36:26 - so now I'm gonna grab the total number
36:29 - of people from each country who know
36:31 - Python and we just did this a second ago
36:33 - right here but I'll go ahead and do this
36:36 - again and set it as a variable so that
36:39 - we have all of these steps so I'm going
36:42 - to grab all of that that we just
36:45 - calculated and now I'm going to set this
36:47 - as a variable and I'm gonna call this
36:49 - you know country uses Python and then
36:53 - I'll set it equal to that and now let's
36:56 - print out that variable as well so let
37:00 - me go to the next line here my
37:03 - computer's kind of given me some grief
37:07 - okay so these are all the people from
37:09 - each country who said that they know how
37:11 - to use Python so now we have one
37:14 - variable that is a series that has the
37:17 - total number of people from each country
37:18 - right here called country respondents
37:21 - and then we have another variable that
37:22 - is a series that is the total number of
37:25 - people from each country who know Python
37:27 - so now we need to combine these two now
37:30 - I'm actually going to use a method here
37:31 - that we haven't discussed in this series
37:33 - yet so if you got stuck here then that's
37:36 - completely understandable I probably
37:38 - should have mentioned this in the video
37:39 - where we appended rows to a data frame
37:42 - but we can combine more than one series
37:45 - together using the pandas concat phone
37:48 - so let's see what this would look like
37:50 - so I can say and I'll just call this
37:54 - theta frame Python DF and now I'm going
37:57 - to create a data frame where we can cut
38:00 - those two series in the one so I can say
38:02 - PD dot concat and now I'm going to pass
38:06 - in a list of the series that we want to
38:08 - concatenate so I want this to be our
38:11 - country respondents and I also want to
38:15 - add in this country uses Python series
38:19 - and now we also want to set axis equal
38:23 - to columns because by default it's going
38:27 - to try to concatenate these on row but
38:31 - we want to match up the indexes here so
38:33 - that it can cuts it that way instead so
38:36 - we want to say axis is equal to columns
38:38 - and then finally I'm also going to put
38:41 - sort is equal to false now if you
38:44 - watched a previous video this isn't
38:46 - absolutely necessary but if you run it
38:50 - without sort equal to false then it'll
38:52 - give you a warning saying that in a
38:54 - future version of pandas that it'll sort
38:57 - by default or sort by false on default
39:00 - so it's better just to go ahead and
39:02 - specify if you want the resulting data
39:05 - frame sorted or not so now let's look at
39:08 - this concatenated data frame here okay
39:11 - so now we have a data frame here where
39:14 - these two series have been concatenated
39:16 - and match up on the same index so this
39:19 - is a lot more useful because now we can
39:21 - see okay there were about 20,000 or
39:25 - 21,000 people who said that they were
39:28 - from the United States and about 10,000
39:32 - people who said that they know Python so
39:33 - that's definitely a lot better and more
39:36 - useful information now one thing about
39:38 - this new data frame that we have is some
39:41 - columns that don't really relate to what
39:43 - we're talking about anymore we can see
39:46 - here that this one is just called
39:47 - country and this one is called languages
39:49 - worked with so let's rename these so
39:52 - that they make more sense in the context
39:54 - of what we're actually trying to do and
39:56 - we saw how to rename columns in a
39:58 - previous video as well but if you forgot
40:01 - then you can do this just by grabbing
40:03 - our data frame here and I'll say Python
40:07 - DF which is our data frame dot rename
40:10 - and now what do we want to rename we
40:13 - want to rename the columns and now I'm
40:15 - going to pass in a dictionary here where
40:18 - the key is the previous value and the
40:22 - value is going to be the updated value
40:25 - so I will call this number of
40:29 - respondents and then I also want to
40:31 - change this languages worked with column
40:34 - here and I want to change this to B
40:38 - let's call this num nose python and if i
40:44 - run this then we can see that this looks
40:46 - good we have number of respondents from
40:48 - the united states and number nose python
40:51 - from the united states so that looks
40:54 - good to me so since it looks good i'm
40:56 - going to say in place is equal to true
40:58 - so that it actually modifies our data
41:00 - frame so if I run that and then look at
41:04 - our data frame one more time then we can
41:08 - see that it has been updated with those
41:10 - new columns now we have the total number
41:13 - of respondents from each country and the
41:15 - number of people who know python from
41:18 - each country in one data frame so we
41:21 - have all the information that we need to
41:23 - calculate a percentage now all we need
41:25 - to do is create a new column and
41:27 - calculate this so if you remember in
41:30 - order to create a new column we can
41:33 - simply just assign it so I will call
41:36 - this column PCT for percentage knows
41:41 - Python and now what do we want this to
41:44 - be equal to well if you don't know how
41:46 - to calculate a percentage mathematically
41:49 - basically what you do is you take the
41:53 - part and then divide that by the whole
41:56 - and then you multiply that by 100 so our
42:01 - part here is the number of people who
42:03 - know Python so I will grab that and say
42:08 - python underscore DF and access that
42:12 - series access that column and then
42:15 - want to divide that by the hole and the
42:17 - hole are the total number of people from
42:20 - that country so that is Nam respondents
42:23 - and now if we want this to be a whole
42:25 - number percentage then we can multiply
42:27 - this by 100 okay so if I did all of this
42:32 - correctly and it's very possible I made
42:34 - a mistake but if I did all this
42:36 - correctly then we should have a data
42:39 - frame here with the percentage of people
42:42 - who know Python from each country and
42:44 - now we can work with this just like any
42:46 - other data frame so let's say that we
42:48 - wanted to sort these results now we
42:50 - learned this in a previous video on how
42:52 - to sort values in a series so let's say
42:56 - that we want to sort the countries by
42:57 - the largest percentage of respondents
42:59 - who know Python so to do this I can just
43:02 - say Python DF dot sort underscore values
43:07 - and if you forget how to do any of this
43:09 - then you can always go back to our
43:11 - pandas video where we learned about
43:12 - sorting so in order to sort by the
43:16 - people who know Python or the percentage
43:18 - we can say okay sort by what did I call
43:21 - this here percent knows Python and then
43:24 - I actually want this to be in ascending
43:27 - order equal to false because I want the
43:30 - largest percentage of people who know
43:32 - Python at the top and I was about to put
43:36 - in place equals true first but let's see
43:38 - what this looks like okay so it looks
43:39 - like that that sort worked and it looks
43:42 - good so now I'll say in place is equal
43:44 - to true so that it modifies our data
43:46 - frame and now we can look at our results
43:49 - here so we can see here that some of
43:51 - these are a little misleading here
43:53 - because you know a hundred percent of
43:55 - people from South Bay and príncipe know
43:59 - Python but we only had one person from
44:02 - the country who answered the survey and
44:04 - he happens to know Python or she so that
44:07 - is a hundred percent so instead let's
44:09 - look at the head here and grab see if we
44:13 - can find a country here with a larger
44:16 - number of respondents so okay we have 72
44:21 - people from Uganda and 47 of them knew
44:24 - Python so that's 65 percent that's
44:26 - pretty good we have
44:29 - oh okay so this is United States that's
44:31 - not bad either we have about 21,000 here
44:33 - about 10,000 new Python so that's 48
44:36 - percent so that's in the higher range
44:38 - that's pretty good so yeah I think this
44:40 - is a great way to practice working with
44:43 - pandas and also it's just fun being able
44:46 - to explore your information in this way
44:48 - and now that we have a data frame with
44:50 - all this information then we can also
44:52 - inspect a specific country to see what
44:54 - the percentage of developers are from a
44:57 - specific country who know Python so for
44:59 - example instead of looking through what
45:02 - if I wanted to see Japan instead of
45:04 - looking through all of these I could
45:06 - just say okay
45:08 - Python D F dot Lok and since our country
45:12 - names are our indexes here then we can
45:16 - just do a dot lok of japan and then we
45:18 - can see that we get these statistics for
45:20 - that specific country okay so I know
45:24 - that that may have been a lot to take in
45:26 - and that we covered a lot of ground in
45:27 - this video we definitely covered some
45:29 - more advanced topics here than we did in
45:31 - previous videos but I hope this kind of
45:33 - got you a little excited to learn what
45:35 - you can do with pandas and the types of
45:38 - problems that we can solve you know when
45:40 - you are exploring through your data like
45:41 - this you're probably going to make a ton
45:43 - of mistakes along the way you know I
45:45 - still make mistakes and pandas all the
45:47 - time even in these videos I've made some
45:49 - mistakes and I have these scripted out
45:52 - so it definitely happens but you know
45:55 - each problem that we work through it's
45:57 - similar to this just makes it easier and
46:00 - easier each time to work through
46:01 - additional problems so if you need to go
46:04 - back and re-watch some of these steps in
46:05 - order to work through these problems
46:07 - like this on your own then that's
46:09 - completely normal you know don't think
46:11 - that just because this may have seemed
46:14 - difficult that there's something wrong
46:16 - with you it's definitely normal for this
46:18 - stuff to be a lot of information to take
46:20 - in and also like I said before if you
46:23 - have some other ways of solving the
46:25 - problems that we answered here then like
46:27 - I said definitely leave a comment with
46:29 - your solution in the description section
46:31 - below and I'll take a look at those and
46:33 - I'll highlight some if they are better
46:35 - than what I did here okay so before we
46:38 - end here I would like to mention the
46:40 - sponsor of this video and that is
46:42 - brilliant
46:43 - so in this series we've been learning
46:45 - about pandas and how to analyze data and
46:47 - Python and brilliant would be an
46:49 - excellent way to supplement what you
46:50 - learn here with their hands-on courses
46:52 - they have some excellent courses and
46:54 - lessons that do a deep dive on how to
46:55 - think about and analyze data correctly
46:57 - for data analysis fundamentals I would
47:00 - really recommend checking out their
47:01 - statistics course which shows you how to
47:03 - analyze graphs and determine
47:05 - significance in the data and I would
47:06 - also recommend their machine learning
47:08 - course which takes data analysis to a
47:10 - new level while you'll learn about the
47:12 - techniques being used that allow
47:13 - machines to make decisions where there's
47:15 - just too many variables for a human to
47:17 - consider so to support my channel and
47:19 - learn more about brilliant you can go to
47:21 - brilliant org forge slash CMS to sign up
47:24 - for free and also the first 200 people
47:26 - they go to that link will get 20% off
47:28 - the annual premium subscription and you
47:31 - can find that link in the description
47:32 - section below
47:33 - again that's brilliant dot org forge
47:36 - slash CMS okay so I think that's going
47:39 - to do it for this pandas video I hope
47:41 - you feel like you got a good idea for
47:42 - how to use these aggregate functions and
47:45 - also how we can group our data so that
47:47 - we can explore our data in interesting
47:49 - ways
47:50 - I would really encourage you to take
47:51 - some time after this video and play
47:54 - around with the data a bit see if you
47:56 - can answer certain questions that
47:57 - someone might have about this data so
48:00 - for example what is the most common
48:02 - education level for people who answered
48:04 - the survey that's definitely something
48:06 - that we could answer by what we learned
48:08 - here so I hope you feel like you got a
48:10 - good introduction to being able to
48:12 - answer those types of questions now in
48:14 - the next video we're going to be
48:16 - learning about how to handle missing
48:18 - data and how to clean up your data it's
48:20 - very common for data to have missing
48:22 - values so knowing how to sanitize and
48:24 - clean our data is definitely going to be
48:26 - important but if anyone has any
48:28 - questions about what we covered in this
48:29 - video then feel free to ask in the
48:31 - comment section below and I'll do my
48:32 - best to answer those and if you enjoy
48:34 - these tutorials and would like to
48:35 - support them then there are several ways
48:37 - you can do that the easiest ways to
48:38 - simply like the video and give it a
48:40 - thumbs up and also it's a huge help to
48:41 - share these videos with anyone that you
48:43 - think would find them useful and if you
48:45 - have the means you can contribute
48:46 - through patreon and there's a link to
48:47 - that page into the scription section
48:48 - below be sure to subscribe for future
48:50 - videos and thank you all for watching
48:54 - you
49:03 - you