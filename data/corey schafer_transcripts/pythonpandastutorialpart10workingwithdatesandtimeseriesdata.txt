00:00 - hey there how's it going everybody in
00:01 - this video we're gonna be learning how
00:02 - to work with date and time series data
00:04 - within pandas now there's a ton of
00:06 - interesting stuff that we can do with
00:08 - date time data and we'll be learning
00:09 - about that here so we'll learn how to
00:11 - properly read in our data so that we can
00:14 - use date time functionality we'll also
00:16 - see how to filter by date times how to
00:19 - group dates by resampling the timeframes
00:22 - and we'll also take a look at doing some
00:24 - simple plotting with our time series
00:26 - data as well now I'd like to mention
00:28 - that we do have a sponsor for the series
00:30 - of videos and that is brilliant
00:32 - so I really want to thank brilliant for
00:33 - sponsoring this series and it would be
00:35 - great if y'all could check them out
00:36 - using the link in the description
00:38 - section below and support the sponsors
00:39 - and I'll talk more about their services
00:41 - and just a bit so with that said let's
00:43 - go ahead and get started okay so first
00:45 - of all I've been using the stack
00:47 - overflow survey data for this entire
00:49 - series so far but that data set doesn't
00:52 - actually have any date or time series
00:54 - data so I had to choose a different data
00:57 - set for this video I downloaded some
00:59 - historical cryptocurrency data that we
01:01 - can analyze for this video and as usual
01:03 - I'm gonna have links to download the
01:05 - data and the notebooks that I'm using in
01:07 - the description section below so I've
01:09 - got my notebook opened up here where I'm
01:11 - reading in this CSV file of data and
01:14 - let's go ahead and take a look at what
01:16 - this looks like so we can see here that
01:18 - I'm loading in this CSV file and I
01:21 - called this eth underscore 1h and that's
01:25 - because this is historical data for
01:27 - aetherium which is a cryptocurrency
01:29 - and i'm and this data is broken down on
01:33 - one hour segments so if we look down
01:35 - here at the head of this data we can see
01:37 - that we have some columns here the first
01:39 - one is a date column and these are
01:42 - broken down by the hour we also have
01:44 - some other information here like the
01:46 - symbols the open and closing values for
01:49 - these hours the highs and lows and also
01:52 - the volume so we can so all of this here
01:55 - is for let's see March 13th and this is
01:58 - for 8:00 p.m. 7:00 p.m. 6:00 p.m. and so
02:01 - on now remember if you want to see more
02:03 - information about your data frame so for
02:06 - example how many rows and columns there
02:08 - are I can run DFS shape and we can see
02:11 - that there are 23,000
02:13 - rose here almost 24,000 so a good bit of
02:16 - data for us to work with okay so now
02:18 - let's actually get into working with
02:20 - date/time data so we have this date
02:23 - column here and it looks like this is
02:25 - just giving us every hour of the day but
02:28 - right now this isn't actually a
02:29 - date/time object I can kind of tell this
02:33 - just because it's not in a format that
02:34 - date times usually display as but if you
02:37 - want to be sure you can always try
02:39 - running a panda's data and estate time
02:42 - method on this to see if it works so let
02:45 - me just grab the first row of this data
02:47 - frame and I'll grab that date value so
02:50 - and then I'll go ahead and try to run a
02:52 - date/time method so to grab that first
02:55 - nap value I'm just gonna say D F dot
02:57 - loke and we can see here that the index
03:00 - is just zero over here so I'm just gonna
03:02 - pass in a zero and I want to grab that
03:04 - date column there so if I run what we
03:07 - have now then we can see that I've
03:09 - plucked out that first date so now let's
03:12 - just try to run a date/time method on
03:14 - this so there's one method called day
03:16 - name that will give us the weekday that
03:19 - this date fell on but if I run this now
03:21 - and I say okay dot day name for this
03:26 - value here if I run this then we can see
03:29 - that we get an error and it says that a
03:32 - string object has no attribute day name
03:35 - and that's because we are reading this
03:36 - in as a string currently so how do we
03:39 - convert this to a date/time so there's a
03:42 - few different ways that we can do this
03:43 - and we'll go over some of those here now
03:46 - if you want to convert a column like we
03:47 - have here to a date/time then we can use
03:50 - the pandas to underscore date/time
03:53 - method so to do this I could simply say
03:56 - we'll access that date column and we'll
04:00 - set this date column equal to and then
04:03 - we'll just say PD if for what we
04:05 - imported pandas as to underscore
04:08 - date/time and now I want to pass in that
04:12 - same column to convert that to a
04:14 - date/time now I'm not gonna run this
04:17 - right now because if I run this as is
04:19 - then pandas would do its best to figure
04:22 - out the formatting of the date/time and
04:24 - converted accordingly but the day time
04:26 - that I have he
04:27 - is in a pretty different format so I
04:29 - doubt that this is gonna work but let's
04:31 - go ahead and try it out anyway
04:33 - okay so I expected to get an error if we
04:35 - scroll down and look at the error here
04:37 - we can see that it says unknown at
04:39 - string format so it did not know how to
04:41 - parse this date but like I said before
04:43 - depending on how your dates are
04:45 - formatted then that might actually work
04:47 - for you this just so happens to be
04:49 - format in a way that pandas can't
04:51 - convert this automatically without us
04:53 - telling it how our date is formatted so
04:56 - what we need to do here is pass in a
04:58 - format string specifying how dates are
05:01 - formatted so that it can parse this
05:03 - correctly now I went ahead and I created
05:06 - the correct string format ahead of time
05:08 - for this specific date but just to be
05:10 - clear I never really remember these
05:12 - formatting codes off the top of my head
05:14 - I always need to go and find these codes
05:17 - within the Python documentation so I
05:20 - have that page open here and I will
05:22 - leave a link to this in the description
05:24 - section as well but however your date is
05:28 - formatted here so ours started with the
05:30 - year so we can see that that is a
05:32 - percent Y and then we have the month day
05:35 - so we can find that and here another one
05:38 - is that we have like 8 p.m. and things
05:41 - like that so we can see here that these
05:44 - eyes here this eye is for a 12-hour
05:47 - clock which is what ours is doing and
05:49 - then this % P is for the local
05:53 - equivalent of a.m. or p.m. so those are
05:56 - going to be in our format string but
05:58 - I'll leave a link to this just in case
06:00 - your date formatting is different and
06:03 - you need to create your own so the
06:05 - format string that I need to pass in
06:07 - here and again this is basically just
06:09 - telling pandas how to parse our date
06:12 - we're gonna say that first we're gonna
06:14 - see the year and then a dash and then
06:17 - the month and then the day with a dash
06:20 - in between that and then a space and
06:22 - then % I was that 12 hour clock and then
06:27 - there is a dash and then it is % P so
06:32 - let me go ahead and run this and if I
06:33 - put this in correctly then this should
06:36 - work ok so we didn't get any errors
06:39 - there but let's go ahead and make
06:41 - so I'm going to go ahead and look at the
06:43 - date column here and we can see that now
06:46 - these look more like date/time objects
06:49 - that we might be used to seeing in
06:51 - programming so it converted 11 p.m.
06:54 - to 23 well I'm sorry I thought 11 p.m.
06:58 - was the first one no it's 8 p.m. okay so
07:00 - it converted 8 p.m. to 20 and 7 p.m. to
07:06 - 19 and so on and now that this is
07:09 - converted to a date/time we should be
07:11 - able to run these date/time methods that
07:12 - gave us an error before so up here where
07:15 - we got this error where we tried to grab
07:17 - the day name for these I'm just going to
07:19 - copy that and paste that in down here
07:21 - and now let's try to rerun this and we
07:25 - can see that now it's saying that that
07:27 - first date in our series here this March
07:31 - 13th was a Friday ok so that's nice so
07:34 - it looked like it works now the way that
07:36 - we did this here is that we converted
07:38 - this to a date after we loaded in our
07:41 - data with this line right here but if we
07:45 - wanted to convert this to a date as
07:46 - we're loading in our data then we can
07:48 - also do that as well so if I go up here
07:51 - to the top where we loaded this in at
07:53 - this read CSV line here then I can
07:56 - actually pass in some arguments to read
07:58 - CSV so that it loads in certain columns
08:00 - as date times and then we can pass in
08:03 - our formatting string as well so that it
08:06 - parses those as the data is read in so
08:10 - to do this we need to pass in this parse
08:13 - dates argument here and now I'm just
08:16 - going to pass in a list of the columns
08:18 - that are going to be dates we only have
08:20 - one here so it's just going to be a list
08:22 - of one item whoops and I meant to put
08:24 - date not dates and now just like with
08:28 - before if your dates are already
08:30 - formatted in a way that pandas can parse
08:32 - them then you don't need to add anything
08:34 - else here but we already saw before that
08:37 - we need to pass in a specific format so
08:40 - to do this here we can't just pass in a
08:41 - format string we instead need to pass in
08:44 - a function that converts each string to
08:47 - a date/time object so first let's create
08:50 - that function and we've seen lambda
08:52 - functions in this series before but
08:54 - just in case you're unfamiliar with
08:55 - those you can simply create a normal
08:58 - function instead if you are more
09:00 - comfortable with those but this is just
09:02 - a shorter way so to create this lambda
09:04 - function I'm just gonna call this D
09:07 - underscore parser I'm gonna set this
09:10 - equal to a lambda function and I'll just
09:14 - use X as the variable here and now what
09:17 - do we want to return so when we used PD
09:21 - to date time down here we actually
09:23 - passed in an entire series to PD to
09:26 - date/time but now this is actually just
09:29 - going to be each individual string and
09:32 - it's going to send each individual
09:33 - string through this function so in order
09:35 - to convert this we can use a function
09:39 - called PD dot date time dot STR P time
09:45 - that's how we convert a string to time
09:48 - and then we can just pass in our string
09:52 - that we want converted to a date time
09:54 - and then the format and I already had
09:57 - the format down here so I'll just go
09:58 - ahead and copy that and paste that in
10:01 - here and that's all we need for that
10:03 - date parser function so now the argument
10:06 - for the date parser is date underscore
10:10 - parser and I'm going to set that equal
10:12 - to that D parser variable there that is
10:16 - set to our lambda function okay so now
10:18 - if I run this cell here then we can see
10:22 - that we didn't get any errors so that's
10:23 - good and now if I run at this D F dot
10:27 - head here then we can see that now our
10:30 - data frame was already loaded in as a
10:33 - date time so we didn't have to do any
10:35 - conversions later on it just did it as
10:37 - it was reading in that CSV file okay so
10:40 - now let's look at some more useful
10:41 - things that we can do with date times so
10:44 - first I'm going to delete the cells that
10:46 - we have below here so that we are not
10:49 - converting these columns again since
10:51 - they're already loaded in as dates so
10:54 - I'll delete that one I will delete that
10:57 - one since that was what was converting
10:59 - it earlier I'll delete that as well and
11:01 - I'll keep this one here just for
11:03 - reference since I will have these up on
11:05 - my github afterwards okay so before
11:08 - actually right here we saw how to run a
11:11 - date/time method on a single value when
11:14 - we use this day name method but what if
11:17 - we want to run that method on our entire
11:19 - series so let's say that we wanted to
11:22 - view the day name of this entire date
11:24 - column here so to do this we can access
11:27 - the DT class on the series object and
11:30 - access the date/time methods that way so
11:33 - to do this we can just say we can first
11:37 - grab that series so that date column is
11:40 - going to return a series if I run that
11:42 - we can see that we get all those values
11:44 - and now if we wanted to access the DT
11:47 - class on the series object then we can
11:50 - just say dot DT and now the date/time
11:54 - method that we want to use so if I want
11:57 - to get the day name of all these values
11:58 - then I can just dude a name there and if
12:02 - I run that then we can see that we get
12:04 - the day of the week for each of the
12:06 - dates in this series so using the DT
12:08 - class on the series object is very
12:10 - similar to how we access the string
12:12 - class or the STR class for the string
12:15 - methods on an entire series and we saw
12:17 - that in previous videos so this can
12:20 - definitely be pretty useful so let's say
12:22 - that we wanted to you know create
12:24 - another column so that we can quickly
12:26 - reference what day all of these trades
12:28 - took place so to do that we could just
12:31 - grab what we have here and I could
12:33 - simply create a new column by simply
12:37 - like I'm accessing a column so I can
12:40 - call this column day of week and set
12:42 - this equal to and paste in that date
12:45 - time method there if I run this and then
12:48 - we look at our data frame then we can
12:51 - see that now we can quickly see over
12:53 - here on the right that okay the 13th was
12:56 - a Friday and then we have these dates
12:58 - down here towards the end this was a
13:00 - Saturday so it's nice to see about her
13:02 - be able to see what days these trades
13:04 - actually took place so now let's look at
13:06 - how we can explore our data a bit so we
13:09 - can see by looking at the indexes here
13:12 - on the far left that there are over
13:15 - 20,000 rows in this data set
13:17 - so let's see how we can view the
13:19 - earliest and latest dates in
13:21 - this data so to do this we can use the
13:24 - min and Max methods so to see the
13:26 - earliest date I could simply access this
13:31 - date series here and I could just run
13:33 - the min method on this if I run that
13:36 - then we can see that the earliest date
13:38 - that it gives us is 2017 zero seven zero
13:42 - one now if I wanted to see so what is
13:45 - that that's a July 1st of 2017 so to
13:48 - view the most recent date that I have
13:51 - and it should be the date that I
13:54 - downloaded this data then I can just
13:56 - look at the max value here and if I run
13:59 - this then we can see that this is March
14:02 - 13th 2020 which actually was the day
14:05 - that I downloaded this data and one
14:07 - really cool thing with date times is
14:09 - that we can actually subtract dates in
14:11 - order to view the time between those two
14:13 - dates and this is called a time Delta so
14:16 - to get the amount of time that spans
14:18 - between these two dates here then I
14:20 - could simply say take the max value and
14:23 - then subtract the min value and if I run
14:28 - this then we can see that we get this
14:30 - time Delta that says that there are
14:32 - almost a thousand days between the
14:34 - earliest date in our data set and the
14:36 - most recent so we have 986 days in this
14:40 - entire data set of cryptocurrency data
14:42 - almost a thousand so that would
14:45 - definitely be a lot of days to look
14:47 - through if we want to find some specific
14:49 - ranges so what if we wanted to do some
14:52 - filters by date so for example let's say
14:55 - that we just wanted to view the data for
14:57 - 2020 now that we have these converted to
15:00 - date times we can create filters just
15:02 - like we have in previous videos and we
15:04 - should be able to use strings that are
15:06 - formatted like date times or we can use
15:08 - actual date/time objects we'll take a
15:10 - look at both so let's see an example of
15:13 - this and some code so that it makes some
15:15 - more sense so first I'm going to create
15:17 - a filter and a separate variable like
15:20 - I've done in previous videos but you can
15:23 - also do this inline if you prefer to do
15:25 - it that way I just think that creating
15:27 - our filters separate is a little bit
15:29 - easier to read so let's say that I want
15:33 - our date series I want the objects or
15:38 - the rows that are greater than and then
15:42 - I'm just going to pass in a string here
15:43 - for now and I can just pass in a twenty
15:46 - twenty there and pandas will know that
15:48 - I'm talking about the year 2020 let's
15:50 - actually do a greater than or equal to
15:53 - here okay so now that I have that filter
15:55 - let's just do a D F dot Lok again we've
15:59 - seen this in previous videos and then
16:02 - I'll pass in that filter so if I run
16:05 - this then my bottom row here should be
16:08 - January 1st of 2020 and it is and we can
16:12 - see that we have 17,000 hours here of
16:15 - 20/20 data or I'm sorry that's 1700
16:19 - hours of 20/20 data okay so the reason
16:21 - that this doesn't go above 20/20 is
16:24 - simply because you know our latest data
16:26 - runs out so we're not getting 2021 since
16:30 - 2021 hasn't happened yet but what if we
16:33 - wanted data for 2019 well in order to do
16:36 - that we'd also have to put in an upper
16:38 - bound as well so to do that I'm gonna
16:41 - say okay we want our data to be greater
16:45 - than or equal to 2019 and and we just
16:50 - want to do an ampere sign there I'll go
16:52 - ahead and copy this here and then just
16:54 - replace this with a less-than and we'll
16:58 - say less than 2020 if I run this then we
17:02 - can see that our bottom row here we have
17:05 - January 1st of 2019 at midnight and then
17:09 - our top row here is December 31st at
17:12 - 11:00 p.m. of 2019 so that gives us all
17:16 - of the rows of data that we have for
17:17 - 2019 and right now we're just using
17:20 - strings up here for these comparisons
17:22 - but we can use actual date times as well
17:25 - so to do that we could actually say I
17:29 - could just say PD dot date time and then
17:33 - let me go ahead and pass in the month
17:36 - and day here as well so I'll say that I
17:38 - want this to be greater than 2019
17:41 - January first and then I'll just grab
17:45 - this here
17:46 - and replace this 2020 and then I'll say
17:49 - but I want this to be less than 20 20s
17:53 - January 1st so now if I run this whoops
17:57 - and I got an error here the it says you
18:01 - know integer is required got a string
18:03 - that might not make sense what I did
18:05 - here is I don't want PD date/time that
18:08 - was my mistake
18:08 - I want to do the same thing that we did
18:10 - before and do to date time so that it
18:13 - converts the string here to a day time
18:14 - so let's do PD to date time for both of
18:18 - those and run this and now we can see
18:20 - that we get those same results as before
18:22 - for all of the rows in 2019 now one nice
18:26 - feature about dates is that if we set
18:28 - our index so that it uses the date which
18:32 - would actually be a good idea for this
18:33 - data set since all of these date or time
18:36 - stamps are unique then we can actually
18:38 - do this same thing by using slicing
18:41 - instead so let's see what this looks
18:44 - like so that it makes more sense
18:45 - so first let's set our index so that
18:48 - it's using this date column here so here
18:52 - at the bottom I'm going to say DF dot
18:55 - set underscore index and then I'm going
18:58 - to pass in that we want to set the index
19:00 - to date and if I run this then that
19:04 - looks good we have set at our index to
19:07 - use date here and now that that looks
19:09 - good it actually didn't make that change
19:11 - I want to say in place is equal to true
19:13 - to make that change permanent so I'll
19:16 - run that and if we look at our data
19:19 - frame again then now we have that date
19:21 - as our index and now with that date
19:23 - index we can actually filter our dates
19:26 - just by passing them into our brackets
19:29 - so if we wanted the data for 2019 then I
19:33 - could literally just say that I want the
19:37 - data here for 2019 pass that into my
19:40 - brackets if I run that then we can see
19:43 - that we get the same thing here we get
19:45 - this value for January first and then
19:48 - the top value here is for December 31st
19:52 - so it's a bit easier to you know just
19:56 - access these within brackets when these
19:58 - are our indexes
20:00 - rather than creating a filter now if you
20:03 - want to grab dates for a specific range
20:05 - then you can use a slice so let's say
20:08 - that we wanted all of the data for
20:10 - January and February of 2020 so to do
20:14 - that using this slicing here then I
20:18 - could say okay I want from 20 2001 which
20:22 - would be January and then I could just
20:24 - do a slice here using that colon and
20:27 - then say okay well I want to go up to
20:30 - February of 2020 so if I run this the
20:35 - second value here is inclusive so we can
20:38 - see that we have January 1st of 2020
20:42 - down here at the bottom that slices all
20:44 - the way up to February 29th since this
20:47 - was a leap year now this can be really
20:49 - useful for analyzing our data because
20:52 - let's say that we wanted to get the
20:53 - average closing price for aetherium for
20:56 - all of our rows of these dates to do
20:59 - that we could simply grab this close
21:02 - column here and then grab that average
21:05 - or grab that mean so to do that we can
21:08 - just say let me copy this part here
21:11 - first let me just access that close
21:15 - series there that column if I run that
21:18 - then we can see that we get all of those
21:20 - closing values on each of those hours
21:23 - for all of those days and now to get the
21:26 - mean of that I can just say dot mean and
21:29 - that gives us the average closing price
21:31 - for all of those rows within that
21:33 - timeframe and remember each of those
21:35 - days is reporting by the hour but what
21:39 - if we wanted to see this data in a
21:41 - different way what if we instead wanted
21:44 - to look at this data on a daily basis
21:46 - instead of on an hourly basis well first
21:49 - we need to think about what would make
21:51 - sense to view on a daily basis so for
21:54 - example let's say that we wanted to you
21:57 - know view the highs for each day so
22:00 - right now we have all of these highs
22:02 - broken down by our let me actually look
22:06 - at the first let me grab this date range
22:10 - here and let's look at the first 20
22:13 - four of these so that we can get 24
22:15 - hours here so we can see that for
22:18 - February 29th we have all these
22:20 - different hours here and each hour has a
22:23 - different high value but what if we were
22:25 - like okay well we see all these
22:27 - different high values here but what was
22:29 - the highest value of the day so actually
22:31 - let me just grab a single day here and
22:34 - then we will look at the high values for
22:37 - that so instead of doing all of these
22:40 - dates here I'm just gonna grab January
22:42 - 1st of 2020 and then we will look at the
22:46 - high values for that day so again we
22:50 - don't really care what the highs are for
22:52 - each hour of each day we just want to
22:54 - know the high for the entire day so to
22:57 - do this all we need to do is grab the
23:00 - max value for this series and we saw how
23:05 - to do this it's just like running mean
23:07 - right here all we have to do is say dot
23:10 - Max and if I run that then we can see
23:12 - that the high value for that day was 132
23:16 - 0.68
23:17 - okay so let's remember this value here
23:19 - right now this one thirty two point six
23:21 - eight because we're going to see how we
23:24 - can resample our data so that we can get
23:27 - the highest trades for each day of our
23:30 - data and then we'll use this one here to
23:32 - compare for January 1st of 2020
23:35 - so again right now our data is broken
23:38 - down on an hourly basis so if we want to
23:41 - redo this so that it's instead broken
23:43 - down by day or week or month then we'll
23:47 - do this by doing something called
23:49 - resampling so let's see what this looks
23:51 - like so if I want to resample this and
23:54 - see the high value by day then I can
23:57 - simply access this high column here and
24:02 - then on that series I can say okay I
24:06 - want to resample this and now we have to
24:10 - tell resample how we want to resample
24:13 - this data right now it's hourly if I put
24:16 - in ad then it resamples it two days and
24:19 - I can do 1d or 2d you can do whatever
24:23 - there you can do a w4 week there's all
24:26 - kinds of different code
24:27 - here now just like with these date/time
24:30 - formats
24:31 - I hardly ever remember these so I always
24:33 - need to look them up in the
24:34 - documentation so I've got this pulled up
24:37 - in the pandas documentation here for
24:38 - these date offsets and I will leave a
24:41 - link to this page in the description
24:42 - section below as well if you all would
24:45 - like to try out some of these but we can
24:47 - see we have our minute second
24:50 - milliseconds microseconds all kinds of
24:52 - things if you're doing finances you can
24:54 - do quarterly and things like that so I
24:58 - want to do this on a daily basis so I'm
25:01 - gonna put ad there and now we have to
25:05 - tell it okay well what do we want to do
25:08 - what these resampling Ziff I'm looking
25:11 - at entire days here so if I take this
25:14 - entire day of the first what do I want
25:17 - to do with this high value and we're
25:18 - just saying well we want the max value
25:21 - for each of those days so if I run this
25:24 - then we can see that that gives us a
25:26 - series with all of the high values for
25:29 - each day so now let's save this series
25:32 - here as a new variable and look up the
25:35 - specific date that we used before so I'm
25:38 - going to save this as a variable and
25:40 - call that highs and then let's access
25:45 - that specific date of 20 2001 oh one for
25:51 - the highs now what we should get here
25:54 - since we're using the same date that we
25:56 - did here we should get this one value of
25:59 - 132 point six eight so if I run that
26:02 - then we can see that the high for that
26:04 - day was in fact equal to what we did
26:06 - here so that works but now instead of
26:09 - just getting one day at a time like we
26:10 - did here now that we've resampled this
26:13 - now we have those high values for every
26:16 - single day in our data okay so why would
26:19 - something like this be useful I mean you
26:21 - know that might be useful just because
26:23 - it's interesting but there are other
26:25 - things that we can do as well so let's
26:27 - say that maybe we wanted to plot this
26:29 - out but instead of you know viewing a
26:33 - plot that had these prices broken down
26:35 - hour by hour now we can just do a plot
26:38 - for the total price broken
26:41 - by day so within Jupiter notebooks it's
26:44 - extremely easy to plot out information
26:46 - I'm actually going to do an entire
26:48 - series on plotting with pandas so I'm
26:51 - not going to go into a ton of details in
26:53 - this video but we will see how we can do
26:55 - a you know very simple line plot here so
26:58 - to do this well we first need to use
27:00 - this special line within jupiter
27:02 - notebooks that allows our plots to
27:04 - display within the browser so all we
27:08 - have to do is say this is a % here then
27:12 - we can say mat plot lib in line now one
27:17 - thing that I do want to mention here is
27:18 - that I did have to go and install
27:21 - matplotlib
27:22 - in the virtual environment that I'm
27:24 - using so if you've only installed
27:27 - you know pandas or and that's it then
27:30 - you might want to go back and install
27:31 - matplotlib or else you will get an
27:33 - import error here but I went and
27:36 - installed that in my virtual environment
27:39 - so we can see that that worked there and
27:41 - with that one line of code there now we
27:43 - can display plots directly within our
27:46 - Jupiter notebook so I can simply run the
27:49 - plot method on this data frame variable
27:51 - that was resampled and get a plot of
27:53 - that so I'm just gonna say okay I want
27:56 - highs plotted out so heís dot plot
28:00 - I'll run that and we can see that we get
28:02 - a nice matplotlib plot here ok so that's
28:06 - you know pretty nice for you know just a
28:09 - few lines of code there now one thing
28:12 - that you might be wondering is if it's
28:14 - possible to resample multiple columns at
28:16 - once and we can do that by running the
28:20 - resample method on our entire data frame
28:22 - instead of one a single series so for
28:25 - example what do i mean by this
28:27 - ok so whenever I say you know resample
28:31 - multiple columns at once I mean that
28:33 - what if we wanted to resample this by
28:35 - day but so far we've only seen ok how we
28:38 - got the high value but what if we said
28:41 - okay well I want to resample this by day
28:43 - but I also want you know the average
28:46 - closing cost of that entire day I want
28:51 - the sum of all of these volumes for that
28:54 - entire day
28:55 - and then I want the you know the max
28:58 - high value and I want the men low value
29:00 - so the way that we've done that down
29:02 - here where we just access that single
29:05 - column we wouldn't be able to do it
29:07 - using this method that we did here so in
29:10 - order to resample and use multiple
29:14 - columns like that here's how we can do
29:17 - this so we can do this by running the
29:19 - resample method on our entire data frame
29:21 - so if you want to use the same
29:24 - aggregation method on all of your
29:26 - columns so for example let's say D F dot
29:30 - resample so now we're resampling our
29:33 - entire data frame object here and now
29:36 - we're gonna pass in what we want to
29:38 - resample on instead of day let's change
29:42 - it up and do week
29:43 - now we'll resample by the each week so
29:46 - if you want to use the same aggregation
29:48 - method on everything then you can just
29:50 - put in that aggregation method there so
29:53 - if I run this then this is going to give
29:55 - me the mean values for each of our
29:58 - columns on a weekly basis now this is
30:01 - cool that we can do this and sometimes
30:03 - you might want to do something like this
30:04 - but in this instance it doesn't really
30:07 - make sense to use mean to get the
30:10 - average of all of our columns so for
30:12 - example there's no real reason to get
30:14 - you know the average volume per hour or
30:16 - something like that
30:18 - you probably want to get the sum for the
30:20 - you know entire time period or for our
30:23 - high and low values here these are
30:25 - giving us the average highs and the
30:27 - average lows but the point of a high and
30:29 - low value is to know the high for that
30:31 - time period and the low for that time
30:33 - period so we probably don't want mean
30:35 - here either so how can we recess to
30:39 - where we can you know resample and use
30:42 - multiple columns but also use multiple
30:44 - aggregation methods now we've actually
30:46 - seen this in previous videos and use
30:48 - this method but what we want to use here
30:51 - is the AGG the AGG method and the AG
30:55 - method also accepts a map of columns and
30:58 - the aggregation functions that we want
31:00 - to run on that column so for example
31:03 - let's do this with the values for let's
31:07 - see we'll do the closing column
31:09 - we'll do the high and low columns and
31:10 - then we'll also do the volume here so
31:12 - I'm going to grab this from up here and
31:15 - then we'll do D F dot resample and we'll
31:19 - pass in a W for a weekly basis and now
31:22 - instead of passing in dot mean like we
31:25 - did up here I'm going to pass in dot AGG
31:28 - and now I can pass in a dictionary of
31:30 - the columns and/or the column names and
31:35 - then the values will be the aggregation
31:38 - function that we want to use on that
31:40 - column so for example let's say that for
31:42 - the closing value I do want to grab the
31:45 - mean of that and then I'll say for the
31:49 - hi column I want to use the max
31:55 - aggregation function for that since we
31:57 - want the max value for the low column I
32:01 - want to get the min and for volume I'll
32:08 - go ahead and just sum up all of the
32:11 - volume for that entire time period ok so
32:15 - again the keys here for this dictionary
32:18 - that we passed into AG the AG method
32:20 - this is the column name here then this
32:25 - is the aggregation function so we're
32:26 - taking the mean of clothes we're taking
32:29 - the max for this entire weekly period
32:31 - here for the highs the min for the low
32:34 - and then some for volume so if we run
32:37 - this then it gives us this nice weekly
32:39 - overview of the you know the weekly
32:41 - highs and the weekly lows here and also
32:44 - the average closing costs here and we
32:47 - also have the summation of the volume of
32:50 - trades so you know this really touches
32:53 - on what we can do with date times and
32:55 - time series data in pandas like I said a
32:58 - little bit ago I do plan on doing a full
33:01 - series on pandas plotting where we'll
33:04 - cover more advanced topics you know such
33:06 - as plotting plotting things out and
33:09 - having rolling averages for data and
33:11 - things like that now before we do end
33:14 - here I do want to thank the sponsor of
33:16 - this video and that is brilliant
33:18 - and I really enjoy the tutorials that
33:20 - brilliant provides and what
33:22 - recommend checking them out so in this
33:25 - series we've been learning about pandas
33:26 - and how to analyze data and Python and
33:29 - brilliant would be an excellent way to
33:30 - supplement what you learn here with
33:32 - their hands-on courses they have some
33:34 - excellent courses and lessons that do a
33:35 - deep dive on how to think about and
33:37 - analyze data correctly for data analysis
33:40 - fundamentals I would really recommend
33:41 - checking out their statistics course
33:43 - which shows you how to analyze graphs
33:45 - and determine significance in the data
33:46 - and I would also recommend their machine
33:48 - learning course which takes data
33:50 - analysis to a new level while you're
33:52 - learning about the techniques being used
33:54 - that allow machines to make decisions
33:55 - where there's just too many variables
33:57 - for a human to consider so to support my
33:59 - channel and learn more about brilliant
34:01 - you can go to brilliant org Forge slash
34:03 - CMS to sign up for free and also the
34:06 - first 200 people they go to that link
34:08 - will get 20% off the annual premium
34:10 - subscription and you can find that link
34:12 - in the description section below
34:14 - again that's brilliant dot org forge
34:17 - slash CMS okay so I think that's going
34:20 - to do it for this pay in this video I
34:22 - hope you feel like you got a good idea
34:24 - for how to work with date and time
34:25 - series data within pandas and like I
34:28 - said there's a lot more that we can
34:30 - cover with date time data but I feel
34:32 - like what we did here should definitely
34:34 - provide you with the basics of being
34:36 - able to convert analyze and resample
34:39 - your data so that you can do the exact
34:42 - analysis that you need now in the next
34:44 - video we're going to be learning how to
34:46 - read data in pandas from different
34:49 - sources so far in this series we've only
34:51 - covered CSV files but we're gonna learn
34:54 - how to read in data from excel from
34:57 - websites SQL databases and a few more so
35:00 - be sure to stick around for that but if
35:02 - anyone has any questions about what we
35:04 - covered in this video then feel free to
35:05 - ask in the comment section below and
35:07 - I'll do my best to answer those and if
35:09 - you enjoy these tutorials and would like
35:10 - to support them then there are some ways
35:12 - you can do that the easiest ways to
35:13 - simply like the video and give it a
35:15 - thumbs up and also it's a huge help to
35:17 - share these videos with anyone who you
35:18 - think would find them useful and if you
35:20 - have the means you can contribute the
35:21 - patreon and there's a link to that page
35:23 - and the description section below be
35:25 - sure to subscribe for future videos and
35:26 - thank you all for watching
35:38 - you