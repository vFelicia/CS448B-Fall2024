00:00 - hey there how's it going everybody in
00:01 - this video we're gonna be learning how
00:02 - to handle missing values and also how to
00:04 - clean up our data a bit
00:06 - now almost every dataset that you're
00:07 - going to be working with is likely going
00:09 - to have some missing data or data that
00:11 - we'd like to clean up or convert to a
00:13 - different data type so we'll learn how
00:15 - to do all of that here now towards the
00:17 - end of the video we'll combine what we
00:19 - learned here to be able to look at our
00:20 - Stack Overflow survey data and calculate
00:23 - the average years of experiences of
00:24 - developers who answered the survey so be
00:27 - sure to stay around for that and it's
00:29 - going to be great practice for what we
00:30 - learned here now I would like to mention
00:32 - that we do have a sponsor for this
00:34 - series of videos and that is brilliant
00:36 - so I really want to thank brilliant for
00:38 - sponsoring the series and it would be
00:39 - great if you all can check them out
00:40 - using the link in the description
00:42 - section below and support the sponsors
00:43 - and I'll talk more about their services
00:45 - in just a bit
00:46 - so with that said let's go ahead and get
00:48 - started ok so first let's talk about how
00:51 - to drop missing values so I have my
00:53 - snippets file open here and we've seen
00:55 - this in previous videos and again if
00:58 - anyone wants to follow along then I'll
01:00 - have a link to all of these notebooks
01:01 - and the data in the description section
01:03 - below and as we've seen in previous
01:05 - videos we'll learn how to do some of
01:07 - this and our smaller snippets data frame
01:09 - first and then we'll see how to do some
01:12 - interesting stuff on our larger Stack
01:14 - Overflow data set to get this working on
01:16 - some real world data so for this video
01:18 - I've added some null values here into
01:21 - our snippets data frame that we didn't
01:24 - have before so I added some extra first
01:27 - names here and we can see that I just
01:28 - have one that is a numpy dot n a n which
01:33 - is a not a number value I also imported
01:36 - numpy up here at the top this and this
01:39 - one here is just a none value and then I
01:41 - also have some custom missing values as
01:43 - well this one is just a string of n a
01:46 - and this one is just a string of missing
01:48 - so I'll have some NA ends some nuns and
01:51 - stuff like that thrown throughout this
01:53 - data so that we have some missing values
01:55 - so you're gonna see this a lot that when
01:57 - we work with pandas we're going to have
02:00 - some missing data and depending on what
02:02 - it is you're trying to do you might want
02:04 - to handle this in different ways so one
02:06 - thing you might want to do with missing
02:08 - data is to simply remove it so for our
02:10 - small data frame here let's say that
02:13 - we're going to
02:13 - do some analysis with these people in
02:15 - the data frame but if they don't have
02:17 - their first name last name and email
02:19 - address then we can't do what we're
02:22 - trying to do so we'll just remove the
02:24 - rows that don't have those values so in
02:26 - order to do this we can use the drop in
02:29 - a method so let's do this and then I'll
02:31 - explain the results and go over those so
02:35 - all I'm going to do down here with my
02:36 - data frame is I'm going to say DF drop
02:39 - in a and we're gonna run that without
02:41 - any arguments right now so when we run
02:43 - this we can see that now we only get
02:45 - four rows of data here and up here we
02:48 - had let's see four five six seven so we
02:52 - got these four rows here because they
02:55 - didn't have any missing values now we do
02:57 - still have our bottom row here which has
03:00 - some of our custom missing values but
03:02 - we'll see how to deal with these in just
03:04 - a second but for now let's go over what
03:07 - drop n/a is actually doing here now
03:09 - what's going on in the background is
03:11 - that drop in a is using some default
03:13 - arguments so I'm going to manually feel
03:16 - in these default arguments and it might
03:18 - make more sense why we got this specific
03:20 - result so by default I'll leave that one
03:24 - here and now I'm going to fill in drop
03:26 - in a again but I'm gonna put the default
03:28 - arguments that it already has and the
03:31 - default arguments of what this is doing
03:33 - in the background is it has an axis set
03:36 - to index and it has a Hal variable set
03:40 - to any so since this is what the method
03:43 - was using by default anyway we should
03:45 - just go ahead and get the same results
03:47 - here and we can see that we do we get
03:49 - the same results as we did when we ran
03:51 - this up here but now let me actually
03:53 - explain these arguments here so first we
03:55 - have the axis argument so this can
03:58 - either be set to index or set to columns
04:01 - that is going to tell pandas that we
04:03 - want to drop in a values when our rows
04:05 - are missing values when it's set here to
04:07 - index if we set this to columns then it
04:10 - would instead drop columns if they had
04:13 - missing values and we'll look at that in
04:15 - just a second
04:16 - now the second argument here is how we
04:19 - want to drop these or I guess a better
04:21 - way to frame that is this is the
04:23 - criteria that it uses for dropping a row
04:26 - or a column so
04:27 - by default this is set to any so we're
04:30 - looking over our rows since this is set
04:32 - to index and this is set to any here so
04:36 - it will drop rows with any missing
04:39 - values but this might not be what you
04:41 - want maybe with this kind of an analysis
04:44 - that we're doing it's okay to have you
04:46 - know missing email or last name or
04:48 - something like that but there just has
04:51 - to be something it can't just be an
04:53 - entire row of missing values so if
04:55 - that's the case then we can instead
04:57 - change this how argument to all and this
05:02 - will then only drop rows when all of the
05:04 - values in that row are missing so now if
05:08 - I run this then we can see that now we
05:10 - get back more rows than we did before
05:12 - because it kept the rows that had some
05:14 - missing values but not all missing
05:17 - values so we can see here we have an
05:19 - email missing but there were some other
05:22 - columns filled in and we can see that
05:24 - everything was missing here but they did
05:27 - have an email so all of the values have
05:29 - to be missing in order for this to
05:32 - actually drop those so it looks like we
05:34 - are missing index of four if I go up to
05:36 - a my original data frame here we can see
05:39 - that that index had all missing values
05:41 - there okay now if I instead change this
05:44 - axis to columns instead of index then it
05:47 - will drop columns that have all missing
05:49 - values we don't have any columns that
05:52 - have missing values all the way down so
05:54 - it should just return our original data
05:56 - frame so if I say columns here and run
05:59 - this then we can see that that's what we
06:01 - get because none of these columns have
06:03 - missing values all the way down now if I
06:06 - set this back to the default and drop
06:08 - columns with any missing values then
06:11 - we'll actually get an empty data frame
06:12 - return because we have one row that is
06:15 - completely empty that we saw here this
06:18 - index of four so for that row each
06:22 - column is going to have at least one
06:24 - missing value and if we set this to any
06:27 - than any column which is even a single
06:31 - missing value will be dropped which in
06:33 - this case is all of them so if I change
06:36 - this to any then since we have all
06:40 - missing value
06:41 - using one of these rows that's just
06:43 - going to give us an empty data frame now
06:45 - at this point you might be wondering
06:46 - okay well my data is a bit more
06:49 - complicated than this and I'm doing some
06:52 - internet analysis where I want to drop
06:54 - some missing values but I only want to
06:56 - drop rows that are missing values in a
06:58 - specific column so for example let's say
07:01 - that we're doing some analysis on our
07:03 - data and it's fine if they don't have a
07:05 - first name or a last name but we really
07:08 - need the email address and if they don't
07:10 - have an email address then we need to
07:13 - just drop those rows so in order to do
07:15 - this we can pass in a sub set argument
07:18 - so first I'm gonna set our access here
07:22 - back to index so that we're dropping
07:24 - rows and now we want to pass in a sub
07:29 - set argument and this subset will be the
07:31 - column names that were checking for
07:33 - missing values so in this case it's just
07:36 - going to be a single column so I'm gonna
07:38 - say sub set is equal to and I'm still
07:41 - going to pass in a list even though this
07:44 - is just a single column and I'll say
07:46 - email so if I run this then we can see
07:49 - that the data frame that we get back is
07:50 - full of rows that have at least their
07:53 - email address filled in and again this
07:55 - one down here with these n/a values that
07:57 - is our custom missing values and I'll
07:59 - show you how to treat those as missing
08:01 - values in just a bit now in this case
08:03 - here since we're only passing in a
08:05 - single column for our sub set our how
08:08 - argument here isn't really doing much
08:10 - because it's only going to look at the
08:12 - email address for missing values so if
08:15 - an email address isn't filled in then
08:17 - passing in either any or all for our
08:20 - argument here would trigger that row to
08:23 - be removed so even if I put this as all
08:26 - it should give us the same result
08:28 - because we're only checking one value
08:30 - but we can also pass in multiple columns
08:33 - to our subsets so what if we said okay
08:35 - well in order for my data to be useful
08:37 - I need either their last name or their
08:40 - email address but I don't need both so
08:43 - in order to do this we could just say
08:45 - okay they need all of the values in last
08:52 - name and an email or
08:55 - sorry there that I got that reversed
08:57 - they don't need their last name in the
08:59 - email it just can't be that all of those
09:01 - values are missing so as long as the
09:03 - last name or the email is there then it
09:06 - shouldn't drop those rows so if I run
09:08 - this then we can see that we get some
09:11 - values that don't have an email but they
09:13 - did have a last name
09:14 - and also we would get back some values
09:17 - that didn't have a last name but do have
09:19 - an email just like this anonymous one
09:21 - here it has an email but it doesn't have
09:23 - a last name and again that's because we
09:25 - passed in all for our Hal argument which
09:27 - means for a row to be dropped both of
09:30 - the subsets columns needed to be missing
09:33 - now like we've seen in previous videos
09:35 - this isn't permanently changing our data
09:38 - frame values if we want to permanently
09:39 - change our data frame then we'd have to
09:42 - add the in place argument and set that
09:44 - equal to true here within this method
09:47 - but we've seen that a bunch throughout
09:49 - the series so far so I don't think I'll
09:51 - go over that again here okay so now
09:53 - let's get to these custom missing values
09:55 - we can see down here that we have a row
09:58 - here that has some customized missing
10:01 - values so for example maybe the people
10:03 - who got our data from I didn't know what
10:06 - to do with missing values so instead
10:08 - they just passed in a string of n/a or
10:11 - they passed in you know a string of
10:13 - missing like we have here so how would
10:16 - we actually handle these well it depends
10:18 - on how we load in our data in this case
10:21 - we've created our data frame from
10:23 - scratch by creating a dictionary and
10:26 - then creating our data frame here so
10:28 - what we can do here is just simply
10:29 - replace those values with an N a n value
10:32 - now if we instead load it in our data
10:34 - from a CSV file then we could do
10:37 - something different but first I'll show
10:39 - this and then we'll take a look at the
10:41 - CSV file later whenever I go over to the
10:44 - stack overflow data so right here at the
10:46 - top where we created our data frame I'm
10:49 - going to replace these values with a
10:50 - proper numpy n a n value so to do this
10:54 - I'm just going to go a couple lines down
10:56 - here and we've seen this in previous
10:59 - videos but we can use this replace here
11:01 - and I'm replacing all the values in the
11:04 - entire data frame so anytime we see a
11:07 - string of n/a
11:08 - I'm going to replace that with numpy dot
11:12 - na in and again I am importing numpy up
11:16 - here as MP so that's where I'm getting
11:18 - I'm able to use numpy and then I want to
11:22 - say in place equal to true because we
11:24 - actually want to modify that data frame
11:26 - so if I run that then that would replace
11:30 - those values but I'm also going to place
11:32 - replace this string of missing as well
11:35 - with MP dot na n values and I want to do
11:39 - that in place as well so let's go ahead
11:42 - and run this that should replace those
11:44 - values and now if I look at our data
11:46 - frame here then we can see that we no
11:49 - longer have that string of missing or na
11:52 - these are now all na n values and now if
11:56 - we go back through and we run ourselves
11:58 - where we dropped in a values then these
12:01 - custom values should have been replaced
12:03 - and it should treat those as missing
12:05 - values so right here we can see what our
12:07 - previous result was where we got this
12:09 - index of 6 with those custom values if I
12:12 - rerun this now we can see that that's
12:14 - gone and the same with here if I rerun
12:17 - this then that is gone as well now if
12:20 - you don't actually want to make any
12:21 - changes and we just want to see if
12:23 - certain values would or wouldn't be
12:25 - treated as n/a values then we could just
12:27 - run the n/a or is in a method and get a
12:31 - mask of values as to whether or not
12:34 - these classify as n/a or not so let me
12:37 - just show you what I mean here
12:38 - so I could say DF dot is n/a and this is
12:42 - just going to give us a mask here of
12:44 - values that are whether or not they are
12:47 - classified as an n/a value so we can see
12:50 - that our row for here was all n/a values
12:54 - and so same thing with our row 6 and we
12:56 - can see some other missing values
12:58 - throughout here as well now sometimes
13:01 - especially when we're working with
13:02 - numerical data we might want to feel our
13:05 - NA values with a particular value now
13:09 - I'm working with string data here but
13:11 - sometimes it make might make sense to
13:13 - fill your n/a values with certain values
13:15 - with these as well
13:16 - so for example let's assume that we were
13:19 - calculating grades for assignments or
13:21 - some
13:22 - like that and you had some assignments
13:24 - that were in a because the student never
13:26 - turned in the assignment well at that
13:28 - point you could just decide if you
13:30 - wanted to score all missing assignments
13:33 - as zeros so that you could probably
13:35 - calculate up the grades so to do
13:38 - something like this we can use the fill
13:41 - in a method so for example I could say
13:44 - something like this if I do a DFS fill
13:47 - in a and then pass in a value just to
13:50 - show you exactly what this is doing I'm
13:53 - going to fill all of our missing values
13:55 - with this capitalized missing string
13:57 - here and if I run this then we can see
14:01 - that all of those missing or all of
14:03 - those n/a values were filled with this
14:06 - string capitalized as missing now like I
14:09 - said before I don't do this a lot with
14:11 - certain strings I found this to be most
14:14 - useful for numerical data depending on
14:17 - how you're doing your calculations but
14:19 - you might want to give n/a values a
14:21 - value of zero or negative one or
14:24 - something like that
14:25 - so if it would make sense with your data
14:28 - and you had numerical data to replace
14:30 - your missing values with a zero then you
14:32 - could just run DF dot fill in a zero and
14:35 - if I go ahead and run this then we can
14:37 - see that that works on our data frame as
14:39 - well and again just like with our other
14:41 - methods if you want those changes to
14:43 - your data frame to be permanent and
14:44 - carry over into other cells then simply
14:47 - just add that in place argument and set
14:49 - that to true to make that change
14:51 - permanent okay so now let's look at
14:53 - another common thing that we're likely
14:55 - going to need to do with a lot of our
14:57 - data and that is casting data types so I
15:00 - have another column and my snippets here
15:02 - that I didn't have in previous videos
15:05 - and I have up here if we look this is
15:08 - this age column so let's say that we
15:10 - wanted to get the average age of all the
15:13 - people in this sample data frame well
15:15 - right now these might look like numbers
15:17 - when we print them out in our data frame
15:19 - down here bTW these are actually strings
15:22 - and we can see this if we look at our
15:24 - data frame data types so to do this we
15:27 - can say D F dot d types and that's not a
15:30 - method it's just an attribute so if I
15:33 - run this here then we can see that it
15:35 - says all of
15:35 - these columns are objects and when it
15:38 - says it's an object it likely means it's
15:40 - a string or a mix of different things so
15:43 - in the latest version of Python or
15:45 - pandas I'm sorry
15:46 - they actually updated it so that there's
15:49 - actually a string datatype now but I'll
15:51 - do a video on those pandas version
15:54 - updates at the end of this series since
15:56 - they actually released that updated
15:57 - version as I was writing this course but
16:00 - don't worry there's not a lot that's
16:01 - changed to where what you learn here
16:03 - will be outdated or anything like that
16:05 - it's still mostly the same but we can
16:09 - see here that our age column is a string
16:12 - because it's this object data type so if
16:15 - we wanted the average age then it
16:18 - wouldn't work as it is now so let's just
16:21 - see what this error looks like so I'm
16:22 - going to grab the mean of that age
16:25 - column and if I run this then we can see
16:29 - that right now we get an error and if I
16:31 - scroll down to see what this air was it
16:33 - says can only concatenate STR not int to
16:37 - string now that might not be the most
16:39 - easy to understand air right there but
16:42 - basically it's telling us that because
16:43 - that column is strings and not integers
16:47 - so we need to convert that column to
16:49 - numbers instead of a string now there's
16:51 - a caveat when doing this and this might
16:54 - throw some people off so when we have na
16:57 - n values in a column that we're trying
16:59 - to convert to numbers then you need to
17:01 - use the float datatype and that's
17:04 - because the na n value is actually a
17:06 - float under the hood let me go ahead and
17:09 - show this just to show you what this
17:11 - looks like so I'm gonna look up the type
17:14 - of n P na in and we can see that that is
17:17 - a float so if we try to convert this
17:20 - column to integers then it's going to
17:22 - throw an error when it runs into those
17:24 - na n values because it can't convert
17:27 - those so if I was to say D F and of age
17:33 - is equal to and now let's try to convert
17:36 - these to integers so the way that you
17:38 - cast datatypes here is we can just say
17:41 - okay
17:41 - I want the age column as type and now we
17:46 - want to pass in the type that we want if
17:48 - I try to convert these
17:49 - two integers then this is going to give
17:51 - us an error because we have some na n
17:54 - values so we can see here int argument
17:57 - must be a string
17:59 - not none type so when you're trying to
18:01 - convert these two numbers and you have
18:03 - those na and values you basically have
18:05 - two options here if your column didn't
18:08 - have any missing values then this would
18:10 - just work fine we wouldn't even run into
18:12 - this error but if it does have missing
18:15 - values then you can either convert those
18:17 - missing values to something else like a
18:19 - zero using the fill and a method that we
18:22 - saw before or you can just cast that
18:24 - column to a float instead now I think
18:27 - this would be a bad idea to convert
18:29 - those missing values to a zero or some
18:32 - other number because we're trying to
18:33 - compute the average in this case but
18:36 - depending on your data that might be
18:38 - what you want to do but I'm going to go
18:40 - ahead and just convert these two floats
18:42 - so those na n values stay missing values
18:45 - so instead of an int here I'm just going
18:47 - to convert this to a float and if I run
18:49 - this then that seemed to have work so
18:52 - now we can look at the datatypes again
18:54 - so I'll say DF woops sorry I wasn't
18:57 - typing in that cell I can say D F dot d
19:00 - types and if we look at this then we can
19:02 - see that now our age is a float object
19:05 - here so now let's see what happens when
19:08 - we try to take the average of that
19:10 - column so I'll say D F dot mean and if I
19:14 - run that then we can see that we get the
19:16 - average value for those ages now if you
19:18 - have an entire data frame of numbers or
19:21 - something like that that you're trying
19:22 - to convert all at once then the data
19:25 - frame object has an as type method as
19:27 - well so you could just say D F dot as
19:31 - type and then pass in whatever data type
19:35 - you're trying to cast everything to and
19:37 - just convert everything in the data
19:39 - frame at once but we have some mixed
19:42 - columns here so we don't want to do that
19:44 - okay so we've been looking at our small
19:46 - data set right now to test this stuff
19:48 - out but now let's take what we learned
19:50 - here and learn how this applies to
19:51 - real-world data and do some analysis on
19:54 - our Stack Overflow survey data okay so
19:56 - first of all I mentioned earlier that if
19:59 - we had custom values for missing data
20:01 - then
20:02 - it's a little bit easier to handle these
20:04 - when loading in a CSV and what I'm
20:07 - talking about up here is up here at the
20:10 - top where we replaced these custom
20:13 - missing values let me show you how we
20:15 - would do this same thing but loading in
20:18 - a CSV instead so I'm going to switch
20:20 - over here to my stack overflow survey
20:23 - data let me go ahead and rerun this just
20:26 - to make sure that all of this stuff is
20:28 - running okay so this notebooks still
20:30 - running that's good and again this is
20:32 - that stack overflow data that we have
20:35 - been using throughout the series and if
20:36 - you'd like to follow along then I do
20:38 - have a download link for this in the
20:40 - description section below okay so if I
20:42 - wanted to ignore those custom values
20:44 - when loading in a CSV then we can simply
20:47 - pass in an argument of a list of values
20:50 - that we want to be treated as missing so
20:53 - here's how we would do this if we had
20:56 - some custom missing values here in this
20:59 - CSV file then I could simply create a
21:02 - list here of those missing values and I
21:06 - will just call this n/a valves and now
21:09 - I'll pass in a list of those so let's
21:12 - say that we have some values that are a
21:15 - string of n/a and a string of missing so
21:18 - now what we could do here is just add in
21:21 - an argument and say na values is equal
21:24 - to and then that list that we just
21:27 - created and if we run that then we
21:30 - shouldn't get any errors and when it
21:32 - reads in that CSV then it will treat
21:34 - that list of values as missing values
21:38 - and give them an n/a in result now in
21:41 - this survey here they did a good job of
21:43 - not having any weird occurrences like
21:44 - that so that actually shouldn't change
21:47 - anything okay so now let's look at an
21:49 - interesting problem with casting some
21:51 - values so let's say that for the
21:54 - developers who answered this survey we
21:57 - wanted to calculate the average number
21:58 - of years of coding experience among all
22:01 - of them now that might be a good thing
22:03 - to know to compare your experience
22:04 - against the average but let's look at
22:07 - what this or why this might be difficult
22:09 - to calculate with this data set and us
22:12 - calculating this solution is actually
22:14 - going to apply several concepts
22:16 - that we've learned through so far
22:17 - throughout this series so the column to
22:20 - view the answer for this question in the
22:23 - survey is called years code so let's
22:26 - look at some of these answers so I'm
22:28 - just going to look at the top ten
22:30 - answers for years code so I will do a
22:35 - dot head and let's look at the top ten
22:38 - so if I run this then at first glance
22:40 - this doesn't really look like it'll be a
22:42 - problem we just have a bunch of integers
22:45 - and the number of years that different
22:47 - respondents have been coding so you
22:49 - might think that we can just grab the
22:51 - mean by of this column simply by saying
22:54 - okay if we just have a bunch of integers
22:56 - here and some na n values that's fine
22:59 - let's just grab the mean of that column
23:01 - but if I run this then we get an error
23:04 - and if I scroll down here then it says
23:08 - can only concatenate string to string
23:11 - and we saw the same error in our smaller
23:14 - data set where the column was actually
23:16 - being loaded in as a string instead of
23:19 - numerical data and we should know how to
23:22 - handle this by now since we did it in
23:23 - the smaller data set so let's try that
23:26 - so let's try to convert this to floats
23:29 - and then take the average so let me go
23:32 - back up here to the top where we ran
23:34 - this and instead of running the mean
23:36 - here I'm going to say okay well let's
23:38 - convert this to a float so that we can
23:42 - grab that average so I will say as type
23:45 - and we want to convert this to a float
23:48 - since there are n a n values so if I run
23:51 - this then we still get an error so we
23:54 - didn't get an error in our smaller data
23:55 - set here so if I scroll down then it
23:58 - says could not convert string to float
24:00 - and the string that it couldn't convert
24:02 - was less than one year so this might be
24:05 - something that we didn't expect here so
24:07 - obviously we don't just have numbers and
24:10 - na n values in this column there is
24:12 - actually a string value that respondents
24:15 - could select that is equal to this
24:17 - string of less than one year for coding
24:20 - experience so let's look at all the
24:22 - unique values of this column so that we
24:25 - can see exactly what's in here in case
24:27 - there are more strings like this
24:29 - and I don't believe we've actually seen
24:32 - this in the series yet maybe we have I
24:34 - can't really remember but if we want to
24:37 - view unique values of a series then we
24:40 - can simply use the unique method so we
24:42 - could also use the value counts method
24:44 - that we've seen several times before if
24:46 - we want to count all the unique values
24:48 - but we don't really want to count them
24:50 - we just want to see all the unique
24:52 - values in this column so to do this I'm
24:55 - gonna say DF and then access that year's
25:00 - code column dot unique dot unique that
25:05 - is a method so if I run this whoops and
25:08 - I spelled this wrong sorry having a hard
25:10 - time typing today okay so if I run this
25:13 - then this gives us all the unique values
25:15 - of that column and as we'd expect there
25:18 - are a lot of numbers but we see that we
25:21 - also have some strings that are mixed
25:23 - throughout these numbers now we also
25:26 - have na n values here but we're not
25:28 - going to worry about those we just want
25:30 - to ignore the na n values because that's
25:33 - just people who didn't answer the
25:34 - question but we can see that the strings
25:36 - that we have throughout here are less
25:38 - than one year and more than 50 years of
25:42 - coding experience okay so those are our
25:45 - only string values so I'm going to
25:47 - replace those with numbers so that we
25:49 - can get an idea of the average years
25:51 - people have been coding so let's go
25:54 - ahead and replace less than one year
25:56 - here with a zero since that's basically
25:59 - the same thing if somebody's been coding
26:01 - for less than a year then they've been
26:02 - coding for basically zero years so to do
26:06 - this I can say D F dot years code and
26:11 - access that column and then we can just
26:14 - replace that value of less than one year
26:18 - and let's replace that with a zero and
26:22 - we also want these to be in place equal
26:25 - to true because we actually want to
26:28 - modify that data frame so if I run that
26:32 - then it should make that replacement and
26:33 - now I'm also going to replace the value
26:36 - for more than 50 years here and this is
26:40 - going to rescue our results a bit
26:42 - depending on how we want to
26:43 - this I'm simply going to replace this
26:46 - with the value of 51 there may be some
26:49 - people who have several more more years
26:52 - of coding experience than 51 years but I
26:55 - can't imagine that it would be that many
26:56 - people who have you know coded many
26:59 - years greater than 50 so I'm gonna just
27:02 - gonna fill this in with 51 but like I
27:04 - said depending on what we pick here
27:06 - it could affect our results slightly but
27:08 - not by a lot in this case so I'm just
27:12 - going to grab this same replace value
27:14 - here and instead I want to replace more
27:18 - than 50 years and I'm gonna replace that
27:21 - with a value of 51 so now let me go
27:24 - ahead and run this and if we want to
27:26 - look at these unique values again then
27:28 - we could look at these and now it
27:31 - doesn't look like we have any strings in
27:33 - here but we can see here that this is
27:35 - still a D type of object which means
27:37 - that it's not actually reading this in
27:40 - as floats so if we scroll back up here a
27:44 - bit Oh actually I think I over wrote
27:47 - that line yes I did so let's just try
27:50 - that again so what I want to do here is
27:54 - I want to convert this to a float and
27:56 - this is what gave us an error before
27:59 - because we had these strings in here and
28:02 - it didn't know how to convert these to a
28:04 - float but now we should just be able to
28:06 - see say okay I want to convert that as
28:09 - type set that to a float so let's run
28:13 - that and we didn't get an error this
28:15 - time so that's good and now we should be
28:17 - able to view the average numbers of our
28:19 - average number of years of coding
28:21 - experience of the developers who filled
28:24 - out this survey so to do this I'm just
28:27 - gonna say okay the data frame access
28:30 - that column grab the mean of that column
28:33 - so if I run this then we can see that
28:35 - now we get that average back so the
28:38 - average that we got here was about 11
28:40 - and a half years of coding experience as
28:43 - the average years for developers who
28:46 - answered this survey and now you can do
28:48 - other analysis on this as well so for
28:50 - example if we wanted to see the median
28:52 - then I could run that and the median
28:55 - comes back as nine years of code
28:57 - experience so hopefully that real-world
29:00 - example helped explain why it's
29:01 - important to know how to cast these
29:03 - values and understand what's going on
29:05 - there there's always going to be data
29:07 - that is messy or not in the format that
29:09 - we want it in so knowing how to handle
29:11 - these missing values and cast these
29:13 - values to different data types is really
29:16 - going to be essential when working with
29:18 - pandas okay so before we end here I'd
29:20 - like to thank the sponsor of this video
29:22 - and mentioned why I really enjoy their
29:24 - tutorials and that is brilliant so in
29:28 - this series we've been learning about
29:29 - pandas and how to analyze data and
29:31 - python and brilliant would be an
29:32 - excellent way to supplement what you
29:34 - learn here with their hands-on courses
29:36 - they have some excellent courses and
29:38 - lessons that do a deep dive on how to
29:39 - think about and analyze data correctly
29:41 - for data analysis fundamentals I would
29:44 - really recommend checking out their
29:45 - statistics course which shows you how to
29:47 - analyze graphs and determine
29:48 - significance in the data and I would
29:50 - also recommend their machine learning
29:52 - course which takes data analysis to a
29:54 - new level well you'll learn about the
29:56 - techniques being used that allow
29:57 - machines to make decisions where there's
29:59 - just too many variables for a human to
30:01 - consider so to support my channel and
30:03 - learn more about brilliant you can go to
30:05 - brilliant org Forge slash CMS to sign up
30:08 - for free and also the first 200 people
30:10 - they go to that link will get 20% off
30:12 - the annual premium subscription and you
30:14 - can find that link in the description
30:16 - section below
30:17 - again that's brilliant org forge slash
30:20 - CMS ok so I think that's going to do it
30:23 - for this pandas video I hope you feel
30:25 - like you got a good idea for how to
30:27 - handle these missing values and cast our
30:29 - data to different data types so that we
30:31 - can do exactly what we want to do in
30:33 - terms of analyzing our data now in the
30:36 - next video we're going to be learning
30:37 - how to work with dates and time series
30:40 - data now I've been using the Stack
30:42 - Overflow survey data for this entire
30:44 - series because I love being able to show
30:47 - you all real world examples of how these
30:49 - concepts apply but our Stack Overflow
30:51 - survey data doesn't have any time series
30:54 - data that we can actually work with so
30:56 - I'm going to be using a different data
30:58 - set for the next video and I still
31:00 - haven't narrowed down exactly what I'll
31:02 - be using but I'll be sure that it allows
31:04 - us to do some analysis on some real
31:06 - world data like we've been doing so
31:08 - maybe we'll use time series data to
31:11 - you know analyze cryptocurrency rates
31:13 - over time or something like that but if
31:15 - anyone has any questions about what we
31:17 - covered in this video then feel free to
31:19 - ask in the comment section below and
31:20 - I'll do my best to answer those and if
31:22 - you enjoy these tutorials and would like
31:23 - to support them then there are several
31:25 - ways you can do that the easiest ways to
31:27 - simply LIKE the video and give it a
31:28 - thumbs up and also it's a huge help to
31:30 - share these videos with anyone who you
31:31 - think would find them useful and if you
31:33 - have the means you can contribute
31:34 - through patreon and there's a link to
31:36 - that page in a description section below
31:37 - be sure to subscribe for future videos
31:39 - and thank you all for watching
31:52 - you