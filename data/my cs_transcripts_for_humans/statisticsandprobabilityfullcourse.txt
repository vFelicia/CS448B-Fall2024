With timestamps:

00:00 - hello and welcome to this video section
00:03 - 1.1 getting started
00:06 - in this section we're going to learn the
00:08 - vocabulary of Statistics the basic
00:10 - vocabulary at least we're going to
00:12 - distinguish between a population and a
00:14 - sample and we're going to distinguish
00:16 - between the two types of populations the
00:19 - target population and the sampled
00:22 - population
00:24 - so let's begin
00:26 - the goal of the branch of mathematics
00:28 - and these are the words from Hawks the
00:29 - goal of the branched mathematics called
00:30 - statistics is to provide information so
00:33 - that informed decisions can be made
00:36 - I disagree 100 with this statement
00:39 - statistics is not a branch of
00:41 - mathematics for sure we use numbers we
00:44 - use formulas but then so does chemistry
00:46 - and physics and they're not mathematics
00:49 - statistics is a science it proceeds
00:52 - forward based on inductive reasoning
00:55 - tries to learn about the larger whole
00:58 - based on your smaller sample
01:00 - mathematics on the other hand draws from
01:03 - the larger to bring down to the narrower
01:06 - deductive reasoning so statistics is
01:09 - actually a branch of science
01:12 - the goal of this text is to enable you
01:14 - to filter through the statistics you
01:16 - encounter so that you may be better
01:18 - prepared for the decisions that you make
01:20 - in daily life
01:23 - as befitting most things in statistics
01:25 - the word itself has a couple definitions
01:27 - first definition is that statistics is
01:30 - the science of gathering describing and
01:33 - analyzing data
01:34 - so it's the science of gathering
01:36 - describing analyzing data Hawks admits
01:40 - that it's a science
01:41 - we could also use the term statistics as
01:43 - the actual numerical descriptions of
01:46 - sample data
01:47 - in other words statistics are a function
01:50 - of your data a mathematical function of
01:53 - your data
01:56 - the target population is a particular
01:58 - group of Interest
02:01 - a sampled population is a group from
02:03 - which the sample is taken
02:05 - hopefully those two populations are the
02:08 - same they don't have to be
02:12 - if I would like to draw a conclusion
02:14 - about the population of Galesburg
02:17 - that's my target population
02:20 - if I contact People based on the numbers
02:23 - in the phone book the people in the
02:25 - phone book are the sampled population
02:27 - Target population and sampled population
02:29 - in that case are not the same
02:32 - hopefully however the sampled population
02:33 - is representative of the target
02:35 - population
02:38 - a sampling frame is a physical list of
02:40 - all members of the sampled population
02:41 - this may or may not actually exist in
02:45 - the example I just gave the phone book
02:47 - will be the sampling frame
02:50 - a sample is a subset of the population
02:52 - from which data are collected
02:57 - a subset of the target population
03:01 - no not from the target population
03:03 - from the sampled population
03:06 - from which the data are collected it's
03:08 - this sample
03:09 - that we're actually going to apply our
03:11 - statistics to so that we can learn about
03:14 - the target population
03:17 - now hopefully our sample is
03:19 - representative of the target population
03:22 - if the sample is not representative of
03:25 - the target population all the work that
03:27 - we're going to do is worth nothing
03:30 - the sample must be representative and
03:33 - we're going to see some ways of
03:35 - obtaining what are called quote
03:36 - representative samples in the future
03:41 - a census is a study from which the data
03:43 - are obtained from every member of the
03:45 - population
03:48 - every member of the population the
03:49 - United States is required to hold a
03:51 - census every 10 years
03:53 - that is not this type of census that
03:55 - we're talking about because the U.S
03:57 - Census Bureau is not able to contact
04:00 - every single resident of the United
04:02 - States
04:05 - it tries
04:06 - it does a good job of it but it never
04:09 - actually achieves their goal of a
04:11 - genuine census
04:16 - so now
04:18 - let's turn to an intra-lecture question
04:23 - these questions
04:24 - are you'll have to answer these
04:26 - questions in Moodle
04:28 - for section 1-1 is just to check that
04:31 - you are moving forward through these at
04:33 - a good pace
04:35 - so the first introelection question for
04:37 - this section
04:39 - what is the difference between a
04:40 - population
04:41 - and a sample
04:46 - what is the difference between a
04:47 - population
04:48 - in a sample
04:52 - remember you can pause this you've got
04:54 - that little double bar thing that you
04:56 - can push on pause it so you can do the
04:58 - answers in the Moodle right now
05:00 - I recommend that you actually write the
05:01 - question and the answer in your notes on
05:03 - the left hand side
05:05 - but regardless you can pause and I can
05:07 - go back to the lecture
05:14 - a variable is a value like four or a
05:18 - characteristic like blue that changes
05:21 - among members of the target population
05:23 - it's got to change among members of that
05:26 - population because if it is constant
05:28 - among all members that is if all members
05:31 - of the population have the same value
05:33 - such as blue
05:35 - there's really nothing to study
05:38 - it's just blue
05:41 - data are the counts measurements or
05:43 - observations gathered about a specific
05:44 - variable in a population in order to
05:46 - study it
05:47 - so data is what you actually mark down
05:50 - from your sample
05:54 - do not confuse a parameter with a sample
05:56 - statistic a parameter is a numerical
05:58 - description of a population
06:00 - characteristic
06:02 - whereas a sample statistic is a
06:04 - description of a sample
06:06 - characteristic so parameters are about
06:09 - the population
06:10 - statistics are about the sample
06:14 - P for population P for parameter
06:18 - s for sample s for statistic
06:22 - we want our sample statistics to be good
06:25 - estimates of the population parameters
06:27 - and that goal will actually dictate some
06:30 - of the formulas that we'll see in the
06:32 - future
06:35 - here's our second question
06:39 - what is the difference between a
06:41 - parameter and a statistic
06:43 - remember these go into your Moodle quiz
06:46 - but again I recommend on the left hand
06:48 - side of your notes write out the
06:50 - question and the answer and then later
06:52 - put it into Moodle
06:54 - again remember you can pause
06:56 - if I go too fast
07:04 - it is essential that you are mindful of
07:06 - the relationship between a population
07:07 - and a sample
07:10 - the next figure is the picture to help
07:12 - you visualize this relationship
07:14 - so that big blue oval is the target
07:17 - population that is the group we want to
07:20 - draw conclusions about
07:22 - the yellow oval is the sampled
07:24 - population notice the sampled population
07:26 - does not have to be a subset of the
07:28 - target population
07:31 - it hopefully is in fact it hopefully is
07:34 - the target population itself but it
07:35 - doesn't have to be
07:38 - but note that the sample does have to be
07:40 - a subset of the sampled population
07:46 - sample doesn't have to be a subset of
07:49 - the target population
07:51 - again we hope it does in fact we hope
07:54 - that the sample is representative of the
07:57 - target population
07:59 - but from the way that the data are
08:01 - collected it's not required
08:05 - strongly suggested
08:07 - hoped for that's the goal but it doesn't
08:09 - have to be
08:13 - so let's summarize the differences
08:14 - between a population
08:16 - and a sample
08:21 - population is about the whole group
08:24 - it's a group we want to know about
08:28 - characteristics of a population called
08:30 - parameters these parameters in reality
08:33 - are unknown we're trying to estimate
08:36 - those parameters
08:37 - those parameters are fixed they are
08:39 - about the entire population
08:42 - contrast that with a sample where the
08:44 - sample is part of the group
08:47 - is a group we know everything about
08:48 - because we've got it right in front of
08:50 - us we can measure anything we want in
08:52 - that group there is no Mysteries
08:54 - whatsoever
08:55 - characteristics of the sample are called
08:57 - statistics those statistics are always
08:59 - known because we can just measure it on
09:01 - this sample that we have in front of us
09:04 - and the statistics change with the
09:06 - sample
09:12 - a little bit ahead of the place on that
09:14 - example one one
09:16 - identifying population and Sample
09:21 - so please identify the population and
09:23 - the sample I will read through it
09:26 - both A and B
09:29 - I will expect you to hit pause and then
09:31 - I'll give you the answers
09:34 - I've got to identify the population and
09:36 - the sample so in this survey 359 college
09:38 - students at the University of Jackson
09:39 - were asked if they had tried the October
09:41 - flavor of the month at the campus coffee
09:43 - shop
09:44 - 83 of the student surveyed said yes
09:50 - I'm going to now give the answer to that
09:52 - go ahead and hit pause
09:55 - the population is going to be the
09:58 - University of Jackson students or the
09:59 - college students at the University of
10:01 - Jackson
10:02 - because that's the group we're trying to
10:03 - draw a conclusion about
10:06 - the sample is going to be those 359
10:09 - college students we actually contacted
10:15 - a survey b a survey of 1125 households
10:19 - in the United States found that 24
10:20 - subscribed to satellite radio
10:24 - identify the population and the sample
10:27 - hit pause
10:30 - the population is going to be all
10:34 - households in the United States all
10:37 - and the sample is going to be that 1125
10:39 - that we actually contacted
10:42 - those are the solutions
10:45 - move on to example 1.2 identifying the
10:48 - population the sample the parameters the
10:49 - statistics Etc
10:53 - for each of the following reports
10:55 - identify the population the target to
10:57 - population each of these cases the
11:00 - sample
11:01 - and whether or not the Highlight value
11:02 - is a parameter or a statistic so
11:05 - population sample and whether it's a
11:07 - parameter or statistics
11:09 - statistic
11:11 - so here we go numero one
11:14 - after an airline security scare on
11:17 - Christmas Day 2009 the Gap organization
11:19 - interviewed 542 American Air Travelers
11:22 - about the increased security measures at
11:24 - airports the reports say that 78 percent
11:27 - that must be the highlighted part of
11:29 - American Air Travelers are in favor
11:31 - of the United States airports using full
11:33 - body Scan Imaging on airline passengers
11:37 - population the target population the
11:39 - sample and what does that pink purple
11:43 - whatever color it is highlighted number
11:45 - actually indicate go ahead and hit pause
11:48 - the population here is all Americans
11:50 - because we are try all American Air
11:53 - Travelers if you wish because we're
11:55 - trying to draw conclusions about
11:57 - American Air Travelers in the United
11:59 - States
12:00 - two what is the sample
12:02 - it's those 542 people American Air
12:05 - Travelers that the Gallup organization
12:08 - contacted
12:10 - and this 78 percent is going to be the
12:12 - statistic because it was measured off of
12:15 - that sample
12:18 - two
12:19 - Rasmussen reports also conducted a
12:22 - survey in response to the airport
12:23 - security scare on Christmas Day 2009
12:26 - the national telephone survey of 1 000
12:29 - adult Americans found that 59 of
12:30 - Americans surveyed favor racial
12:32 - profiling as a means of determining
12:34 - which passengers to search at airport
12:35 - security checkpoints
12:38 - remember
12:39 - Target population
12:41 - sample
12:42 - and what does that 59 represent go ahead
12:45 - and hit pause
12:47 - the population is going to be all
12:49 - Americans
12:52 - because we're trying to draw conclusions
12:54 - about the Americans
12:56 - two the sample is going to be the house
12:58 - 1 000 adult Americans contacted
13:02 - and at 59 again is going to be a sample
13:04 - statistic it's 59 of those 1 000 people
13:13 - two branches of statistics
13:16 - the branch of Statistics called
13:18 - descriptive statistics is the science
13:21 - that gathers sorts summarizes displays
13:25 - the data doesn't try to draw conclusions
13:28 - about the data it just tries to better
13:30 - understand the data itself
13:33 - and the first four five six seven
13:36 - chapters of this book are going to cover
13:37 - descriptive statistics
13:41 - inferential statistics as the science
13:43 - evolves using these descriptive
13:44 - statistics to estimate population
13:46 - parameters
13:48 - and that'll be the last half of the
13:49 - course
13:50 - so inferential statistics takes our
13:52 - sample and tries to draw conclusions
13:54 - about the population
13:56 - descriptive statistics on their hand
13:58 - just takes our sample and learns about
14:00 - our sample which brings us to our next
14:03 - question
14:05 - what is the difference between
14:06 - descriptive and inferential statistics
14:10 - remember you can hit pause
14:19 - there are two types of analyzes one is
14:22 - called exploratory and one is called
14:24 - confirmatory
14:26 - exploratory analysis uses data to
14:30 - estimate parameters
14:32 - this will be akin to the confidence
14:34 - intervals in the second half of the
14:36 - course
14:37 - confirmatory analysis uses statistics to
14:39 - test stated claims about reality and
14:43 - these stage claims about reality we're
14:45 - going to call hypotheses and this will
14:47 - relate to the hypothesis testing or the
14:49 - p-values of the second half of the
14:50 - course
14:54 - so example 1.3 identifying descriptive
14:56 - and inferential statistics in a news
14:58 - report on the state of the media by Tom
15:00 - rosentile and Amy Mitchell and they
15:03 - write the following
15:05 - AOL had 900 journalists 500 of them at
15:09 - its local Patch News operation
15:11 - by the end of 2011 Bloomberg expects to
15:14 - have 150 journalists and analysts for
15:15 - its new Washington operation Bloomberg
15:18 - government
15:20 - so let's identify the descriptive and
15:22 - the inferential statistics used in this
15:25 - excerpt
15:27 - again remember hit the pause to answer
15:29 - that and then just go ahead and listen
15:31 - we talk
15:32 - and talk and talk and talk descriptive
15:35 - so the first descriptive is AOL had 900
15:38 - journalists so
15:40 - we're describing how many journalists
15:41 - AOL had and we describing 500 of them at
15:44 - its local news Operation
15:47 - so those are descriptive
15:50 - the future by the end of 2011 Bloomberg
15:53 - expects to have so expects to have tells
15:56 - us this is a feature event it's going to
15:58 - expect to have 150 journalists so that
16:00 - will be the inferential statistic
16:04 - so descriptive of that data that we have
16:07 - inferential is about the population
16:09 - which also translates into
16:11 - things that are happening in the future
16:16 - oops we've got that solution and that's
16:18 - the end of the slideshow and that's the
16:20 - end of section one one
16:21 - so expect these videos to look very
16:24 - similar to this that's slideshow me
16:26 - talking over it giving you questions me
16:29 - adding to the slideshows maybe telling a
16:31 - joke or two didn't tell any jokes today
16:33 - I'm sorry
16:34 - but expect them in the future
16:36 - so I hope this was helpful if not as
16:39 - always please make sure you email me
16:41 - with any questions or leave the
16:43 - questions in moodle's discussion forum
16:46 - for questions for chapter one for
16:48 - learning module one
16:50 - that's it thank you very much I
16:53 - appreciate it
16:57 - hello and welcome to section 1.2 data
17:00 - classifications
17:01 - in this section we're going to be
17:03 - looking at how to classify data that is
17:06 - how to determine what types of variable
17:08 - it is it can be either classified as
17:11 - qualitative or quantitative qualitative
17:15 - it's also called categorical
17:17 - quantitative is called numeric it can be
17:20 - classified as discrete continuous or
17:23 - neither I guess the book loves to talk
17:25 - about the neither we're going to look at
17:27 - discrete or continuous and it also adds
17:30 - nominal ordinal interval or ratio so
17:33 - so by the end of this slideshow you
17:36 - should be able to know what all of those
17:37 - terms mean and you should be able to
17:38 - classify a variable or data as in each
17:42 - of those three ways so let's begin
17:44 - qualitative data also known as
17:46 - categorical data consists of labels or
17:49 - descriptions of traits
17:51 - so qualitative variable example would be
17:55 - eye color
17:56 - hair color
18:00 - um
18:01 - favorite music
18:03 - things like that where you can if you
18:05 - want put a number to it but it's not a
18:07 - meaningful number say my eye color is
18:09 - one doesn't tell you what my eye color
18:11 - is say my eye color is blue does tell
18:13 - you
18:14 - those are qualitative those are
18:16 - categorical data
18:18 - quantitative on the other hand also
18:20 - known as numeric actually does have a
18:23 - number that does make fundamental sense
18:25 - to it usually counts measurements I
18:28 - object to the book using the term
18:29 - measurement because counts are also
18:32 - measurements in their own way examples
18:34 - of quantitative data would be height
18:37 - weight
18:39 - age
18:41 - gas mileage we're talking about cars now
18:44 - those would be quantitative because
18:46 - there are numbers associated with them
18:47 - and those numbers are inherently
18:49 - meaningful
18:54 - so here's a graphic we got the
18:55 - qualitative side we've got the
18:57 - quantitative side again qualitative is
18:58 - categorical quantitative is numeric
19:01 - qualitative data or descriptions and
19:04 - labels whereas quantitative tends to be
19:06 - counts and measurements or measurements
19:08 - in general
19:10 - yeah
19:12 - so here's an example
19:14 - classifying data is qualitative or
19:16 - quantitative classify the following data
19:19 - as either qualitative or quantitative
19:21 - what are you assuming about the
19:22 - described variables
19:23 - so a shades of red paint in a home
19:26 - improvement store
19:28 - is that qualitative or quantitative hit
19:31 - pause
19:32 - that is qualitative probably
19:36 - it depends on how you're actually
19:37 - measuring the shades of red paint
19:41 - if you're doing in terms of the names
19:43 - such as fire engine red apple red
19:47 - baby red I don't know what color is red
19:50 - paint are how does it just see red then
19:53 - those would be qualitative
19:56 - but if you're doing in terms of the
19:57 - percent of the paint that is red
20:00 - that is or how many squirts of red dye
20:03 - you have to put in the paint to get that
20:04 - then it would be quantitative
20:07 - because it's the number of squirts of
20:09 - red in the paint
20:11 - interesting so the Ampro answered
20:14 - depends on what you're trying to
20:17 - actually measure
20:19 - hmm
20:20 - B
20:21 - rankings are the most popular pink
20:23 - colors for the season
20:25 - again
20:26 - think about whether this is qualitative
20:28 - or quantitative and what you're assuming
20:30 - and go ahead and hit pause
20:33 - the rankings are counts there's numbers
20:35 - one through however many paint colors
20:37 - there are so this is going to be
20:39 - quantitative
20:42 - I don't think that there's much you're
20:44 - assuming about it unless you're thinking
20:47 - hey what if we're talking about rankings
20:49 - in terms of loved versus hate it well in
20:53 - that case that's qualitative
20:56 - but if you're looking at maybe number of
20:57 - stars that would be quali quantitative
21:00 - if you're looking at number of people
21:02 - who purchased it that would be
21:04 - quantitative again it's how are you
21:06 - actually measuring this ranking
21:09 - huh
21:11 - so these questions actually are kind of
21:13 - difficult
21:14 - C
21:16 - amount of red primary dye necessary to
21:19 - make one gallon of each shade of red
21:21 - paint
21:23 - qualitative or quantitative hit pause
21:26 - this is quantitative because it's an
21:28 - amount of
21:31 - I don't think I can think of any way of
21:33 - of interpreting this to make it a
21:35 - qualitative because it's very clearly an
21:37 - amount of red primary dye okay
21:41 - so that that's going to be quantitative
21:44 - D number of paint choices available at
21:47 - several stores again qualitative or
21:50 - quantitative hit pause
21:52 - you're right I hope that's quantitative
21:55 - because you're looking at the words
21:56 - number of paint choices
21:59 - I suppose you could make this
22:01 - qualitative by saying
22:04 - a lot of paint choices not so many paint
22:07 - choices almost no paint choices that
22:09 - would be a qualitative way of measuring
22:11 - this but it's pretty explicit with the
22:13 - numbers of so that would be quantitative
22:16 - okay
22:19 - yeah that's not what I wanted to switch
22:21 - over to that's what I wanted to switch
22:22 - over to
22:23 - intra lecture question number one
22:27 - again this goes in your Moodle quiz
22:30 - is the variable number of toes
22:33 - a qualitative or a quantitative variable
22:38 - and again I recommend you write the
22:40 - question in your notes on the left hand
22:41 - side your answer below it so that you
22:43 - can go to Moodle after this lecture and
22:45 - put the answers in
22:47 - is the variable number of toes a
22:49 - qualitative or a quantitative numeric
22:51 - variable
22:55 - and back to the lecture
23:00 - now we're looking at continuous versus
23:02 - discrete data discrete data are
23:04 - quantitative data that can take on only
23:07 - particular values
23:09 - and those values are usually counts but
23:11 - they don't have to be counts they could
23:12 - be ratios of two counts I mean hey that
23:15 - would also give you discrete data
23:19 - continuous data are quantitative data
23:22 - that can be taken that can take on any
23:24 - value in a given interval and are
23:26 - usually called measurements
23:29 - again I personally don't like the term
23:30 - measurement because any measurement for
23:32 - statistician is something that you
23:34 - measure so eye color would be a
23:36 - measurement for me because I have to
23:37 - measure eye color but the way that the
23:39 - book wants to frame this measurements
23:42 - would be things that you have on a scale
23:44 - like a ruler scale
23:48 - continuous versus discrete so discrete
23:51 - are counts or ratios of counts
23:55 - continuous data is is takes on any value
23:58 - in a given interval
24:02 - so again we've got the qualitative and
24:03 - the quantitative ovals quantitative can
24:06 - be broken down into the discrete which
24:08 - is usually counts so ratios of counts
24:10 - and then continuous which are usually
24:12 - measurements
24:15 - notice discrete and continuous don't
24:17 - speak at all towards qualitative because
24:19 - qualitative data is neither discrete nor
24:22 - continuous
24:27 - here's another example to determine
24:28 - whether the following data are
24:30 - continuous or discrete
24:32 - temperatures in Fahrenheit of cities in
24:35 - South Carolina
24:38 - is this continuous or discrete hit the
24:41 - pause button now
24:42 - welcome back assuming you hit the pause
24:45 - button now temperatures in Fahrenheit
24:47 - are going to be continuous
24:51 - what's reported on TV will be discrete
24:54 - but the actual temperatures will be
24:57 - continuous
24:59 - B number of houses in various
25:02 - neighborhoods in a city
25:03 - continuous or discrete hit pause now
25:07 - welcome back numbers of houses tells me
25:10 - it's going to be discrete the key word
25:12 - there is numbers you're counting how
25:15 - many houses are in a various
25:16 - neighborhood
25:18 - see numbers of elliptical machines in
25:21 - every YMCA in your state
25:24 - numbers of elliptical machines in every
25:27 - YMCA in your States
25:29 - is that continuous or discrete hit the
25:32 - pause button
25:33 - you're absolutely correct that also is
25:35 - discrete it's got the word numbers of
25:40 - D Heights of doors continuous or
25:43 - discrete
25:46 - oh hit the pause button welcome back
25:48 - whatever I'm supposed to say that's
25:50 - continuous Heights are continuous in the
25:52 - words of the book it's a measurement
25:55 - realize that the actual height of the
25:57 - door is going to be continuous
25:59 - the variable called Heights of doors is
26:02 - continuous
26:03 - what you actually write down as the
26:07 - height of the door is going to be
26:08 - discrete
26:09 - because eventually you're going to have
26:10 - to stop writing down digits
26:13 - and that gets back to something
26:15 - interesting the difference between the
26:17 - variable and the data
26:19 - the variable is what you're measuring
26:21 - the data is essentially what you're
26:23 - writing down for those measurements
26:26 - the variables are what you're measuring
26:28 - such as temperature such as height
26:31 - such as age
26:34 - and the data is what you're actually
26:35 - writing down such as 97.8 degrees such
26:40 - as
26:41 - six foot five such as I I forget what
26:44 - the other example was but those are
26:46 - things that are written down
26:49 - variables can be discrete or continuous
26:53 - what you write down has to be discrete
26:56 - because you have to stop writing at some
26:58 - point
27:00 - which brings us to the next
27:02 - intra-election question
27:05 - is the variable grade point average
27:07 - discrete or continuous
27:11 - again I recommend you write the question
27:12 - over on the left side of your notes
27:14 - write down your answer right after it
27:16 - think about it go back over the last
27:18 - five ten minutes of this lecture
27:21 - so that you're absolutely certain what
27:23 - the difference between continuous and
27:24 - discrete is
27:26 - hit pause
27:28 - fast forward rewind as you need you've
27:31 - got total control over this lecture
27:34 - except for what I say
27:37 - I mean you could even slow this down so
27:39 - I sound like this
27:43 - or speed it up sorry that's not like
27:45 - this
27:49 - levels of measurement four levels of
27:51 - measurement nominal ordinal interval
27:53 - ratio
27:54 - if you learned French you can think of
27:57 - this in terms of Noir black
28:01 - Noir nominal ordinal interval ratio that
28:05 - may help you remember the four levels
28:08 - the level of measurement of a variable
28:10 - describes the amount of information that
28:12 - that variable contains
28:14 - the four levels are nominal ordinal
28:16 - interval and ratio
28:18 - nominal the values are just descriptions
28:21 - ordinal which is your for ordered
28:23 - nominal you've got the description plus
28:25 - you've got an inherent ordering to it
28:29 - interval level
28:30 - the differences between levels are
28:32 - identical so you can subtract
28:35 - and ratio level not only do you have
28:38 - differences between two levels being the
28:40 - same but you've also got the value of
28:41 - zero meaning an absence of
28:44 - so data at the nominal level of
28:46 - measurement are qualitative
28:50 - consisting of labels or names so
28:53 - variable eye color will be nominal level
28:55 - because what are the eye colors blue
28:58 - brown green Hazel
29:03 - and I'm sure I'm missing some colors
29:05 - blue brown green Hazel yeah I could also
29:08 - order them as brown blue green Hazel it
29:10 - would mean the same thing
29:11 - they're just names attached
29:14 - so it's nominal
29:17 - the word nominal comes from the Latin
29:20 - nominus meaning name
29:25 - suppose all students in a stats class
29:26 - were asked what pizza topping is their
29:28 - favorite explain why these data are at
29:30 - the nominal level of measurement
29:33 - piece of cake
29:35 - favorite favorite
29:36 - pizza topping is I don't know pineapple
29:40 - pepperoni
29:42 - this
29:43 - sausage
29:45 - olives I can't think of any more because
29:47 - I only use pepperoni and pineapple
29:50 - those are just names I mean it doesn't
29:52 - matter that they're the individual
29:54 - person's favorite I'm asking everybody
29:57 - for one pizza topping
29:59 - I can call it pineapple I can call it
30:01 - pineapple Apple Pine means the same
30:03 - thing
30:06 - B suppose instead that you wish to know
30:07 - that the number of students whose
30:09 - favorite pizza topping is sausage the
30:11 - number of students explain why this data
30:13 - value is not nominal
30:16 - well it's the number of so you're
30:18 - measuring the number of students
30:19 - counting the number of students
30:21 - nominal has to be categorical
30:24 - if you're counting things that's that's
30:26 - numeric so it can't be nominal
30:32 - data at the ordinal level of measurement
30:34 - are qualitative data that can be
30:36 - arranged in a meaningful order
30:39 - but calculations such as addition or
30:41 - division do not make sense
30:43 - so we've got some examples of nominal
30:45 - data eye color hair color
30:48 - but we can also look at some examples of
30:51 - ordinal data
30:52 - such as
30:55 - and having trouble coming up with one
30:58 - off the top of my head
31:01 - um well let's think through this it's
31:02 - it's got to be categorical
31:06 - and they've got to be in some sort of
31:07 - inherent meaningful order
31:11 - categorical
31:13 - but some inherent meaningful order to it
31:17 - how about socioeconomic status
31:21 - it's categorical
31:23 - low medium high
31:25 - but there is an inherent meaningful
31:27 - order to it low medium high
31:30 - the fact that you could also do it high
31:32 - medium low is irrelevant you've got an
31:34 - ordering to it
31:35 - so socioeconomic status would be an
31:38 - ordinal level variable
31:40 - notice ordinal level variables have
31:42 - additional information to them that
31:44 - nominal level variables don't have that
31:46 - is position
31:49 - I can say high SES is larger or greater
31:52 - than low SES
31:55 - I can't say blue eyes are larger or
31:57 - greater than brown eyes
32:00 - doesn't make sense example
32:04 - determine whether the data are nominal
32:06 - or no seat numbers on your concert
32:07 - tickets such as a23 and a24
32:13 - go ahead and hit pause while you think
32:14 - about it
32:16 - I believe the answer to this will be
32:18 - it's ordinal
32:19 - although I haven't been to a concert
32:20 - 6080s
32:22 - I believe the a would be the row and 23
32:25 - would be the seat number so seat number
32:27 - 23 is one seat closer to the aisle than
32:30 - seat number 824.
32:32 - since it is closer to the aisle since we
32:35 - do have some sort of ordering to it
32:37 - we've got an ordinal variable
32:40 - genres of Music performed at performed
32:43 - at the 2013 Grammys
32:45 - that would be nominal
32:47 - because it'd be I assume country and and
32:50 - rock and Jazz and doo-wop roll done at
32:55 - the Grammys I assume and there's no
32:58 - inherent ordering to that I could do it
33:00 - alphabetical order it would have just as
33:02 - much meaning as doing it in terms of the
33:04 - order that I just gave you
33:06 - so a is ordinal B is nominal
33:13 - data at the interval level of
33:15 - measurement are quantitative data that
33:18 - can be arranged in a meaningful order
33:19 - and
33:21 - such that differences between data
33:23 - entries are meaningful
33:27 - difference between levels are meaningful
33:31 - such as shoe sizes
33:35 - the difference between a 5 and a six in
33:37 - shoe size is exactly the same as the
33:40 - difference between eight and a 9 in shoe
33:42 - size
33:43 - they both differ by one shoe size but
33:46 - also by 1.3 inches
33:51 - I think
33:53 - temperature in degrees Fahrenheit is
33:56 - interval
33:58 - because going from 45 to 46 degrees is
34:01 - exactly the same as going from 95 to 96
34:04 - degrees it's an increase of one degree
34:05 - Fahrenheit
34:07 - probably should say it's measured in
34:08 - degrees Fahrenheit
34:11 - birth years of your classmates are
34:13 - collected what level of measurement are
34:14 - these data well clearly it's going to be
34:16 - interval
34:19 - birth years
34:21 - 2000 I don't know what birth years you
34:23 - guys have 2001 2002 that differs by one
34:27 - year just like 1997 and 1998 different
34:29 - by one year
34:31 - the key for Interval level data is that
34:35 - subtraction and by extension addition
34:37 - actually makes sense
34:43 - and then the last level of measurement
34:45 - the one that has the most information in
34:46 - it is called the ratio level
34:49 - it's quantitative data
34:52 - it can be ordered
34:53 - this interval and the zero point
34:56 - indicates the lack of something
35:01 - so compare compare the year of your
35:03 - birth
35:05 - which we decided was interval from the
35:07 - last with your age which is ratio
35:12 - year the birth well a zero year of birth
35:15 - doesn't indicate a lack of anything just
35:18 - indicates it was two thousand
35:20 - some odd years ago
35:22 - but an age of zero would indicate you
35:24 - lack age
35:27 - now note that you don't have to be able
35:28 - to achieve a zero
35:32 - all that it means is that a value of 0
35:34 - would indicate an absence or a lack of
35:37 - something
35:38 - so the height of a person would also be
35:41 - ratio
35:42 - because a height of zero would indicate
35:44 - a lack of height
35:46 - can't achieve a height of zero but if it
35:49 - were achievable it would indicate you
35:51 - lack height
35:52 - weight would be another example of a
35:54 - ratio level variable you can't achieve
35:55 - zero weight but a zero weight would
35:58 - indicate a lack of weight
36:02 - now it's called ratio level because
36:05 - ratios
36:07 - actually do mean something
36:09 - so comparing a person of age 10 to a
36:12 - person of age 20
36:13 - that ratio actually does mean something
36:15 - the second person is twice as old as the
36:18 - first
36:19 - a four foot person versus an eight foot
36:22 - person
36:23 - that ratio of two
36:25 - actually does mean that the eight foot
36:27 - person is twice as tall as the four foot
36:29 - person
36:31 - compare that with your age of birth no
36:34 - just your year of birth sorry
36:37 - 1995 versus 2000
36:39 - that doesn't indicate that your year of
36:42 - birth is
36:44 - 0.5 percent higher
36:46 - is just your five years later
36:51 - which is interval
36:53 - so the word ratio comes from the fact
36:56 - that ratios divisions actually make
36:58 - sense
37:00 - so example consider the ages in whole
37:02 - years of U.S presidents when they were
37:04 - inaugurated what level of measurement or
37:06 - these data clearly they're going to be
37:07 - ratio because it's in this section but
37:09 - why are they because it's ages
37:13 - in whole years
37:16 - an age of zero would indicate a lack of
37:18 - age doesn't mean you can achieve an age
37:21 - of zero
37:22 - just means that if 0 is in the age
37:25 - column in your spreadsheet you lack h
37:31 - so here are the nice little stair step
37:33 - you got qualitative quantitative
37:35 - qualitative includes nominal and ordinal
37:38 - nominal means names ordinal means
37:40 - ordered nominal
37:43 - quantitative is interval and ratio
37:46 - in interval zero is just a placeholder
37:48 - it's just such as zero degrees
37:50 - Fahrenheit
37:52 - if zero degrees Fahrenheit doesn't
37:54 - indicate a lack of temperature
37:56 - whereas in ratio zero does mean the
37:58 - absence of something so zero Kelvin
38:01 - would be a ratio level variable
38:03 - but zero I'm sorry uh measuring
38:07 - temperature in Kelvin would be ratio
38:08 - level but measuring degrees in
38:10 - Fahrenheit would just be interval level
38:12 - because a zero fahrenheit just means
38:13 - it's cold not that it lacks temperature
38:16 - zero Kelvin means it actually lacks
38:18 - temperature
38:21 - here's the third one
38:25 - give an example of a ratio level
38:26 - variable not provided in the slides or
38:28 - the text
38:30 - again I would write this off on the left
38:32 - write the answer beneath it so you can
38:34 - go into Moodle and answer the questions
38:37 - and pause of course if you need to
38:43 - so final example classifying data
38:46 - classify these as qualitative or
38:48 - quantitative
38:49 - discrete or continuous or neither and
38:52 - the level of edge of measurement normal
38:55 - ordinal interval ratio
38:58 - finishing times for runners in the Labor
39:00 - Day 10K
39:01 - colors contained in a box of crayons
39:04 - boiling points on sale let's see a scale
39:07 - for various caramel candies
39:09 - and the top 10 spring break destinations
39:11 - is ranked by MTV
39:13 - go ahead and hit pause before I give you
39:15 - the answers
39:16 - the answer for a finishing times for
39:19 - runners will be a ratio level
39:21 - it will be continuous and it will be
39:24 - quantitative
39:26 - colors contained in a box of crayons
39:28 - will be qualitative
39:30 - it will be neither
39:32 - because qualitative is neither discrete
39:34 - nor continuous
39:36 - and it will be nominal
39:39 - note that number of colors would be
39:42 - different but we're looking at just the
39:43 - colors themselves
39:45 - boiling points on Celsius that's going
39:47 - to be quantitative
39:49 - continuous
39:51 - and interval
39:53 - because zero Celsius does not indicate a
39:55 - lack of temperature
39:58 - and finally the top 10 Spring Break
40:00 - destinations is ranked by MTV notice
40:03 - these are the destinations themselves so
40:05 - it's going to be categorical or
40:07 - qualitative
40:09 - and hence neither discrete nor
40:11 - continuous
40:13 - lever measurement is going to be ordinal
40:17 - and that's it for section one two
40:20 - again don't hesitate to send me comments
40:23 - questions and post them in Moodle for
40:26 - the discussion section thank you much
40:31 - hello welcome to section 1.3 the process
40:33 - of a statistical study the objectives
40:36 - for this section are to describe the
40:38 - process of a statistical study
40:41 - that's probably rather clear
40:45 - identify the various types of studies
40:46 - but most importantly to understand the
40:49 - primary sampling schemes and it's this
40:51 - understanding the primary sampling
40:52 - schemes that is key for this section if
40:55 - all you take out of the section is those
40:57 - sampling schemes and an understanding of
40:58 - them you're doing great
41:01 - so here's the process of a statistical
41:03 - study notes that there are four steps
41:05 - Step One is determine the design of the
41:07 - study there are classes taught in
41:09 - experimental design by the way but you
41:11 - need to State the question we studied
41:13 - determine the target population and the
41:15 - variables and the determine the sampling
41:18 - methods you're going to use it's one C
41:20 - is it's it's a subset of one but it's
41:23 - probably the most important thing here
41:25 - because remember your sample must be
41:28 - representative of your target population
41:30 - otherwise all the statistical methods in
41:33 - the world are useless
41:36 - your sample must be representative
41:39 - so we need to study how to draw a
41:42 - representative sample
41:44 - two you're going to collect the data
41:46 - according to that sampling method three
41:47 - you're going to organize the data
41:49 - and four you're going to analyze the
41:51 - data yeah 4 is going to be the second
41:54 - half of the class 3 is going to be most
41:56 - of the rest of the first half of the
41:58 - class
41:59 - so example 111 neurologists want to
42:03 - study the effect of vitamin C on nerve
42:05 - disorders the goal of the study is to
42:07 - see if taking an intravenous dose of
42:09 - vitamin C will reduce the amount of
42:11 - nerve pain associated reported by
42:13 - patients
42:16 - so identify the population of Interest
42:18 - and the variables in the study
42:21 - again pause
42:23 - and welcome back the population of
42:26 - interest is all
42:28 - notice all populations of interests have
42:30 - the word all in there somewhere
42:32 - all patients
42:37 - period
42:39 - and the variables in the study will be
42:41 - I'm sorry all patients with nerve
42:44 - disorders
42:45 - and the variables in the study will be
42:48 - the
42:49 - amount of vitamin C and amount of nerve
42:52 - pain
42:53 - those are the obvious ones perhaps
42:56 - others will need to be taken eventually
42:58 - to take care of some intervening issues
43:04 - observational study versus an experiment
43:06 - and observational study observes the
43:08 - data that already exists
43:11 - so the statistician will sit there and
43:13 - just collect data won't try to influence
43:16 - the outcome of anything whereas an
43:18 - experiment generates data to help
43:19 - identify the cause and effect
43:20 - relationships
43:23 - um
43:24 - yeah the the
43:26 - book emphasizes that it's cause and
43:28 - effect it's easier to do cause and
43:31 - effect analysis when you've got an
43:33 - experiment
43:34 - but observational studies can hint that
43:37 - as well
43:38 - now that these are the proper
43:40 - definitions as used by scientists a
43:43 - statistician word for any quote
43:44 - theoretical data collection as an
43:46 - experiment
43:48 - this difference in terminology comes to
43:50 - the fact that statisticians experiment
43:52 - to better understand their field of
43:54 - study just like biologists experiment to
43:57 - better understand biology and physicists
43:59 - experiment to better understand physics
44:01 - statisticians experiment to better
44:02 - understand statistics
44:05 - so here's an example which type of study
44:07 - would you conduct observational or
44:09 - experiment a you want to determine the
44:11 - average age of college students across
44:13 - the nation
44:15 - B researcher wishes to determine if flu
44:18 - shots actually help prevent severe cases
44:20 - of the flu
44:21 - go ahead and hit pause
44:24 - the first one is an observational study
44:25 - it's second is an experiment notice in
44:27 - the second one you're actually trying to
44:29 - determine if the flu shots do something
44:33 - whereas in the first one you're just
44:34 - observing ages of students across the
44:37 - nation
44:41 - a representative sample has the same
44:42 - relevant characteristics as the
44:45 - population and does not favor one group
44:47 - from the population over another
44:49 - note that a sample could be
44:51 - representative for one characteristic of
44:53 - the population but not for another
44:57 - so here's an interesting question
44:59 - how do you know if a sample is
45:01 - representative of the population
45:05 - that we're going to come back to time
45:08 - and time and time again because it's
45:10 - such an important question remember all
45:12 - those statistics is based on that sample
45:14 - you draw and that sample has to be
45:17 - representative of the population
45:20 - so how do you know if your sample
45:21 - actually is represented the population
45:25 - we got a red star at the bottom so let's
45:27 - move on to the first intro lecture
45:30 - question
45:31 - one
45:32 - my sample is all females in this class
45:35 - is it a representative sample
45:38 - remember hit pause write the question
45:40 - over on the left side you probably
45:43 - should listen to what I'm saying before
45:44 - you hit pause write the question on the
45:46 - left side of your notes write the answer
45:47 - down below it so you can put it into
45:49 - your middle quiz
45:51 - and then hit pause
45:56 - I really do want to come back to this
45:57 - how do we know if a sample is
45:59 - representative of the population
46:01 - maybe you should write that on the left
46:03 - hand side as well and start coming up
46:05 - with answers to that question because
46:06 - you're going to be seeing it several
46:08 - times in this course
46:11 - here's five sampling techniques simple
46:13 - Ram sampling notice the book also has
46:16 - something called random sampling we're
46:19 - going to conflate that with simple REM
46:20 - sampling
46:22 - second type is stratified then clustered
46:25 - and systematic and then convenience
46:26 - never ever use convenient sampling
46:28 - except when I give you permission to in
46:30 - class but we're not in class so
46:34 - simple random sample every sample from
46:36 - the population has an equal chance of
46:38 - being selected
46:40 - keyword there is actually two keywords
46:43 - it's sample
46:45 - and equal chance
46:49 - so if I want to draw a sample of size 50
46:52 - from
46:53 - everybody in the United States and they
46:56 - want to use simple random sampling that
46:57 - means that every possible combination of
47:00 - 50 people in the United States has an
47:03 - equal chance of being selected
47:07 - every possible combination of 50 people
47:10 - has an equal chance of being selected
47:13 - or we can bring it down to the class
47:14 - level let's say our our class is size 30
47:18 - so our population is size 30. I want to
47:22 - sample from this population sample of
47:24 - size 5
47:25 - simple random sampling would require
47:27 - that every possible combination of five
47:30 - people
47:32 - in this class would have an equal chance
47:34 - of being selected
47:39 - stratified sampling
47:42 - population is divided into subgroups
47:43 - called strata
47:45 - the grouping variable is correlated with
47:47 - a measurement variable
47:49 - and the sample is drawn from each
47:51 - stratum
47:52 - so in this example if I want to estimate
47:55 - the average GPA at Knox I probably would
47:59 - break it up into freshman sophomore
48:00 - junior seniors
48:02 - estimate the GPA and the Freshman
48:03 - estimate the GPA and the sophomores
48:05 - estimate the GPA and the changes
48:06 - estimate the GPA and the seniors and
48:08 - combine them together to get the
48:09 - estimated average and the reason I'd
48:12 - probably break it up into freshman
48:13 - sophomore junior seniors because GPA
48:16 - does seem to correlate with uh class
48:24 - contrast that with cluster sampling
48:26 - populations divided into subgroups
48:28 - called clusters not strata clusters the
48:32 - grouping variable is not correlated with
48:34 - the measurement variable
48:35 - and a sample is drawn from at least one
48:37 - of the Clusters
48:38 - so if we're going to go back and I want
48:40 - to determine the typical hair color at
48:43 - Knox breaking it up into freshman
48:45 - sophomore junior senior would not be
48:47 - useful in terms of stratified sampling
48:48 - but it would be useful in terms of
48:50 - cluster sampling so I highly doubt that
48:52 - level in school and hair color is
48:54 - correlated
48:56 - in this example we've got a humongous
48:59 - Rice Field one two three one two three
49:01 - four five six seven there's 21 subplots
49:04 - there I randomly select four of those
49:06 - subplots and estimate the
49:09 - amount of rice in each of them
49:11 - each of those subplots is is seems to be
49:15 - rather representative of the population
49:17 - as a whole
49:23 - systematic sampling
49:24 - every nth member of the population is
49:26 - selected
49:28 - so if I want to select 25 percent of the
49:32 - population I would select every fourth
49:33 - bottle in this example here I want to
49:36 - select every 10 percent of the
49:38 - population I'll select every tenth
49:40 - bottle
49:41 - if I want to get uh one percent of the
49:43 - population I'll select every hundredth
49:45 - the bottle
49:46 - but again it's it's systematic because
49:48 - it's selecting every nth member of the
49:50 - population
49:51 - as it comes down the conveyor belt or
49:54 - through the door
49:57 - convenient sampling
49:59 - it's just convenient for the researchers
50:00 - to select people will self-select into
50:03 - the poll perhaps it's very unethical to
50:06 - use
50:08 - all of the web polls that you see on
50:11 - newspaper sites that say hey how would
50:13 - you vote in this case those are all
50:15 - convenient sampling and they are highly
50:17 - unethical
50:19 - because it is trying to give information
50:21 - to the reader
50:23 - and that information is about the
50:25 - population of interest but you're
50:27 - drawing a sample from a very skewed
50:30 - population or a very biased population
50:33 - itself
50:38 - so here's some examples
50:40 - a poster surveys 50 people in each of
50:43 - the Senators 12 voting precincts
50:46 - so this sounds like stratified
50:49 - um
50:50 - Senators 12 voting precincts so the
50:52 - voting Precinct is the stratum
50:55 - there's 12 of them surveying 50 from
50:57 - each I would assume that voting
50:59 - precincts are internally more consistent
51:02 - than the population as a whole because
51:05 - like-minded people tend to live near
51:07 - each other
51:10 - the quality control department is zero
51:12 - manufacturer marries a weight of every
51:14 - 10th box so this will be systematic
51:19 - a female student walks down the halls in
51:21 - a door I'm asking students how much
51:22 - money they'd spend from the food court
51:24 - this is convenience
51:30 - an educator chooses five of the school
51:32 - districts in the Chicago area and ask
51:34 - his household in those District how many
51:36 - school age children are in the district
51:37 - this would be cluster
51:40 - probably
51:42 - the book says this is cluster this may
51:45 - actually be stratified because a school
51:48 - district the distribution of of
51:50 - school-aged children may not be the same
51:52 - across the school districts
51:54 - um
51:55 - one school district may have a higher
51:57 - proportion of children in each household
52:00 - than another school district if that's
52:02 - the case and this would be stratified
52:05 - to determine who'll win a hundred
52:06 - thousand dollar shopping spread them all
52:08 - manager draws the name out of a box of
52:09 - entries this will be simple round
52:10 - sampling
52:12 - technically this will be random sampling
52:14 - but we're conflating the two remember so
52:16 - it's simple random sampling
52:19 - there's the Red Box
52:22 - let's move on to question two
52:24 - this is a key one
52:27 - very very important what is the primary
52:30 - difference between cluster sampling and
52:32 - stratified sampling
52:37 - remember hit pause if you need to again
52:39 - write the question on the left hand side
52:41 - of your notebook and your answer
52:42 - underneath of it
52:47 - two types of observational studies is
52:50 - the cross-sectional study in the
52:51 - longitudinal study
52:53 - cross-sectional study data collected at
52:55 - a single point in time on a lot of
52:58 - members
52:59 - whereas in a longitudinal study it's
53:01 - over time on a few members
53:10 - a group of 220 patients is followed for
53:13 - 15 years in order to determine the okay
53:15 - right there I know this is going to be
53:17 - logic longitudinal because it's over
53:19 - time for a small group of of members
53:23 - be a gastroenterologist surveys 130 of
53:27 - his patients six months after okay this
53:29 - is going to be cross-sectional because
53:30 - it's done at a single point in time six
53:32 - months after having the gastric bypass
53:37 - if the gastroenterologist surveyed those
53:39 - 130 people six months one year 18 months
53:43 - two years three years four years five
53:45 - years and that would be a longitudinal
53:47 - study
53:50 - similar terminology
53:53 - treatment is some condition that is
53:55 - applied to a group of subjects in an
53:57 - experiment
54:00 - the subjects or the participants are the
54:01 - people or things being studied in
54:04 - experiment
54:06 - the response variable is the variable in
54:09 - an experiment that responds to the
54:12 - treatment
54:12 - we'll also refer to this as the
54:14 - dependent variable
54:17 - the explanatory variable is the variable
54:19 - in experiment that causes the change or
54:22 - it explains why that change took place
54:26 - we're going to refer to this as an
54:28 - independent variable
54:30 - there's another set of variables that
54:32 - are called independent
54:34 - it's not the next slide I guess called
54:37 - control variables
54:38 - control variables are variables that we
54:40 - know affect the dependent variable but
54:43 - we really don't care about them in terms
54:45 - of our research
54:47 - so independent variables are broken up
54:49 - into exploratory variables or research
54:51 - variables and then those control
54:53 - variables
54:55 - so here are three principles of
54:57 - experimental design or one you got to
54:59 - randomize the control in treatment
55:00 - groups because the goal is the only
55:03 - difference you want between the control
55:05 - and the treatment group is to be the
55:07 - treatment
55:08 - so you got to randomly put people into
55:10 - okay you don't have to you should
55:12 - randomly put people into the two groups
55:14 - control and treatment and then apply the
55:17 - treatment to the treatment group
55:19 - uh control for outside effects on the
55:21 - response variable those would be the
55:24 - control variables replicate the
55:26 - experiment a significant number of times
55:27 - to see meaningful patterns and I do want
55:29 - to emphasize here the word replicate
55:32 - remember back earlier we asked how do we
55:34 - know if our sample is actually
55:37 - representative
55:38 - the answer is we don't
55:41 - we have to replicate our studies over
55:43 - and over and over again
55:45 - and then the hope is that we don't make
55:47 - the same mistakes and get an
55:48 - unrepresentative sample in each of those
55:50 - replications
55:55 - okay control group versus treatment
55:56 - group
55:57 - is this is the group of subjects which
56:00 - no treatments given
56:02 - whereas the treatment group gets the
56:04 - treatment
56:05 - and again the structure of the
56:08 - experiment has to be such that the only
56:10 - meaningful difference between the
56:12 - control group and the treatment group
56:14 - is the treatment and hence the
56:17 - importance of the randomization
56:20 - confounding variables are unmeasured
56:22 - factors
56:23 - other than the treatment variable that
56:25 - cause and effect on those subjects
56:29 - I had to get rid of confounding
56:30 - variables you measure them add them to
56:32 - the model
56:33 - and there's a red star so let's go back
56:35 - and see
56:37 - question three how do we know if there
56:39 - are confounding variables in a
56:41 - statistical study
56:43 - move those over write that over on the
56:45 - left hand side of your notes put the
56:47 - answer there so that you can answer the
56:49 - quiz in Moodle
56:50 - how do we know if there are confounding
56:52 - variables in Oh missing an s in a
56:55 - statistical study
57:01 - a placebo is a substance that appears
57:03 - identical to the actual treatment but
57:05 - contains no intrinsic beneficial
57:07 - elements
57:08 - um placebos are used to ensure that the
57:12 - only difference between the control and
57:14 - the treatment group is the treatment
57:16 - itself placebos will be given to the
57:18 - control group so the control group
57:20 - doesn't know that they're in the control
57:21 - group
57:23 - the placebo effect is response to the
57:25 - power of suggestion rather than the
57:27 - treatment itself that's why we have to
57:29 - give a placebo to the control group
57:30 - because the very Act of receiving
57:32 - medicine will affect you
57:36 - hence we don't know if the treatment
57:37 - actually improves the the people or just
57:40 - them thinking they receive treatment
57:43 - so we got to give placebos to the
57:45 - control group and the actual treatment
57:47 - to the treatment group so they all think
57:49 - hey I received something
57:51 - so the placebo effect is going to be
57:53 - constant for the entire group and again
57:55 - remember
57:57 - the emphasis here
57:59 - the only allowable difference
58:01 - meaningful difference between the
58:03 - control group and the treatment group
58:05 - is that the treatment group receives the
58:08 - treatment
58:13 - in a single blind experiment the
58:16 - experimenter pokes out one person's eyes
58:18 - oh wait no that's not what it is
58:19 - subjects do not know that they are in
58:21 - the control group or the treatment group
58:24 - whereas In a double-blade experiment
58:26 - both the subject and the measurer
58:28 - doesn't know
58:31 - the measure is the person who measures
58:34 - the temperature of the patient measures
58:36 - whether or not the patient improved
58:39 - measures if the patient's foot fell off
58:41 - whatever whatever the treatment is
58:42 - supposed to fix
58:45 - the researcher
58:47 - a good researcher will move him or
58:49 - herself completely from this once the
58:52 - experiment is set up and is in motion
58:54 - researcher will just take the data that
58:57 - is collected and analyze it because the
59:01 - researcher needs to know everything
59:03 - but in a double-blind experiment it's
59:05 - the subject and the person doing the
59:08 - measuring who don't know
59:11 - so subject
59:14 - person doing measuring and researcher
59:16 - three different people in the experiment
59:19 - three different roles in the experiments
59:22 - sometimes
59:23 - the subject and the the measurer are the
59:26 - same person
59:31 - so here's the lengthy example consider
59:33 - the study from example 111
59:35 - in which a neurologist want to determine
59:36 - if taking the IV dose of vitamin C will
59:39 - reduce the amount of nerve pain reported
59:40 - by patients
59:42 - suppose that the study was narrowed to
59:44 - focus only on patients with the nerve
59:46 - disorder multiple sclerosis
59:50 - after study approval the neurologists
59:52 - solicit volunteers who are patients with
59:54 - MS
59:55 - and who are reporting nerve pain
59:59 - so Target population
60:02 - one I guess is all patients
60:09 - smaller Target population
60:11 - is patients with multiple sclerosis
60:16 - sampled population patients with
60:19 - multiple sclerosis who have nerve pain
60:25 - sampled population
60:29 - is the people in our
60:31 - after study approval the people in the
60:33 - study that I can pull from
60:35 - this doesn't specify what the sampled
60:37 - population is
60:39 - the sample is the actual 40 participants
60:43 - to one t in the control group and 20 in
60:47 - the treatment group
60:53 - a is the treatment group they're given
60:56 - the IV doses of vitamin C
61:01 - so the subjects are getting the doses
61:04 - somebody is measuring the nerve pain
61:08 - participants in group b are the control
61:11 - group
61:13 - they're given an IV dose of saline
61:16 - which is the placebo
61:18 - and somebody's measuring it
61:21 - the participants don't know which group
61:23 - they're in and the people measuring
61:24 - should not know which group they're in
61:29 - oh
61:31 - should not however the nurse is
61:33 - administrating the IVs are aware of the
61:35 - group assignments
61:38 - that's not necessarily a bad thing as
61:40 - long as the nurses are not measuring the
61:42 - nerve pain
61:45 - after predetermined length of time the
61:47 - amounts of pain reported by the separate
61:49 - groups are compared to determine if the
61:51 - IV dose reduces the amount of nerve pain
61:57 - interesting
62:00 - expandatory and response variables
62:03 - treatment
62:04 - which group is the treatment which is
62:06 - the control group
62:08 - what's the purpose of ministering saline
62:09 - to Group B is the single line single
62:12 - blind or double-blind
62:13 - hit pause
62:15 - welcome back
62:16 - explanatory and response response
62:18 - variable is going to be the amount of
62:19 - nerve pain the explanatory is the
62:21 - vitamin C saline I'm sorry the vitamin C
62:24 - IV drip
62:26 - the treatment is the IV
62:29 - uh of vitamin C
62:32 - which group is the treatment group and
62:34 - which is a control group the treatment
62:35 - groups the one that receives the vitamin
62:37 - C the control group is one that receives
62:39 - Saline
62:41 - what is the purpose of menstruating
62:43 - saline to Group B it's to control for
62:47 - the placebo effect remember group a and
62:49 - Group B have to be exactly the same
62:51 - except that group a receives the
62:53 - treatment
62:54 - so giving the IV also
62:57 - makes the groups more similar
63:00 - is this a single blind or a double-blind
63:03 - study it depends if the nurses are
63:05 - measuring the pain it's single blind
63:08 - if somebody else is measuring the pain
63:10 - that doesn't know who is when who is in
63:13 - which group then it's double-blind
63:20 - last page
63:22 - IRB is a group of people who review the
63:24 - design of a stage to make sure that it
63:25 - is appropriate and that no unnecessary
63:27 - harm will come to the subjects involved
63:29 - Knox College has an Institutional review
63:32 - board
63:33 - it meets infrequently to handle student
63:36 - and faculty research plans or research
63:40 - designs
63:42 - informed consent involves completely
63:45 - disclosing to the persistence pins the
63:47 - goals and procedures involved in the
63:48 - study and obtaining their agreement to
63:51 - participate
63:54 - there is a lot of question out there
63:55 - whether informed consent is
63:58 - ethical
64:01 - um
64:02 - whether it's even possible in some cases
64:07 - in all cases where it's reasonable
64:10 - informed consent should be obtained
64:13 - now the question comes down to
64:16 - when is it not reasonable
64:18 - and it's not reasonable when
64:22 - the purpose of the experiment is is
64:25 - given away by the
64:28 - by telling the participants the goals
64:30 - and procedures
64:32 - because remember
64:35 - um
64:36 - the the
64:38 - Placebo and the placebo effect if you
64:40 - know the outcome of an experiment or if
64:42 - you know what the researcher is looking
64:44 - for
64:45 - in the experiment is a much higher
64:47 - probability the researchers are going to
64:48 - find it
64:50 - so unfortunately informed consent in
64:53 - those cases could actually destroy the
64:55 - experiment which means well is informed
64:57 - consent ethical in those cases
65:00 - I don't know it's a big question
65:02 - things to think about
65:06 - but that brings us to the end of this
65:08 - section
65:09 - that's the end of this chapter however I
65:13 - encourage you to go through section one
65:15 - four but I won't be lecturing through it
65:18 - it's important stuff
65:20 - we just have more important things to
65:22 - cover
65:23 - so thank you much
65:28 - hello and welcome to section 2.1 we're
65:30 - going to be creating frequency
65:31 - distributions today
65:33 - it's unclear at this point why you're
65:36 - creating frequency distributions
65:38 - tomorrow or in the next lecture you're
65:41 - going to see how we use these frequency
65:42 - distributions and help create Graphics
65:44 - that tell the story of the data and
65:47 - we're going to look at how to create
65:48 - ungrouped frequency distribution and
65:50 - create a grouped frequency distribution
65:52 - the ungrouped is going to lead to charts
65:55 - such as a pie chart and a bar chart
65:57 - whereas the grouped is going to lead to
65:59 - such things as a histogram or a cement
66:02 - leaf plot so today is the foundations
66:05 - and the next lecture will be for what
66:07 - we're actually going to be using this
66:09 - for
66:10 - throughout this lecture I'm going to
66:12 - point out some things such as while this
66:14 - is a lot of work for not that much
66:16 - and I want you to take that seriously
66:18 - and think okay a lot of work we're not
66:20 - getting too much out of it is there a
66:23 - way of making this easier and of course
66:26 - the answer is yes
66:28 - so frequency distributions a
66:30 - distribution is a way to describe the
66:32 - structure of a particular data set or
66:34 - population
66:36 - we're going to look at sample
66:38 - distributions now
66:40 - when we get to chapter 5 and 6 we'll be
66:43 - looking at population distributions
66:46 - so keep this in my effect on the left
66:48 - hand side of your notes put a little
66:50 - star right
66:52 - C chapter 5 and 6 and another star just
66:57 - to draw your attention to it that this
66:59 - is the start of looking at distributions
67:03 - here for the sample later for the
67:06 - population
67:07 - a frequency distribution is a display or
67:10 - a table of the values that occur in a
67:13 - data set and how often each value
67:15 - or range of values occurs so it's how
67:18 - often that's where the frequency comes
67:20 - in
67:21 - frequencies which is a little f are the
67:24 - number of data values in that category
67:27 - if we're talking about categorical data
67:30 - or range of values if we're talking
67:33 - about numeric data
67:36 - in a class we're going to call that as a
67:39 - category of data in a frequency
67:41 - distribution
67:44 - for categorical variables that class
67:46 - most often is going to be where the
67:48 - levels in the variable
67:50 - for numeric data it's going to be some
67:52 - range of possible values for the data
67:57 - in order to raise an ordered list of the
68:00 - data from largest to smallest or
68:02 - smallest to largest
68:03 - probability distribution is a
68:05 - theoretical distribution
68:09 - used to predict the probabilities of
68:10 - particular data values occurring in a
68:12 - population
68:14 - probability distribution If all we're
68:16 - going to talk about is probability
68:17 - distribution is going to be about the
68:18 - population
68:20 - if we're talking about the distribution
68:21 - or a sample we'll call it a sample
68:23 - distribution
68:26 - an ungrouped frequency distribution is a
68:28 - frequency distribution where each
68:30 - category
68:31 - or class represents a single value or
68:35 - level in the variable
68:37 - these are used for categorical variables
68:40 - whereas a grouped frequency distribution
68:42 - is a frequency distribution where the
68:43 - classes are ranges of possible values
68:46 - these will most likely be used in
68:48 - numeric data
68:51 - the ungrouped frequency distribution
68:53 - will lead to bar charts and pie charts
68:55 - in the future whereas a grouped
68:57 - frequency distribution will lead to
68:58 - histograms in the future
69:02 - so here's the steps in constructing a
69:03 - frequency distribution for ungrouped for
69:06 - categorical data so to create an
69:08 - ungrouped frequency distribution to
69:10 - determine the levels of the categorical
69:11 - variable and I do want to emphasize that
69:13 - you would use this only for categorical
69:15 - variables
69:17 - and count the number of observed values
69:19 - in each level
69:22 - okay number a level is a possible
69:25 - outcome of a categorical variable
69:29 - example the iCloud of my research
69:30 - students in this term are as follows
69:32 - blue brown brown blue brown brown brown
69:34 - green
69:37 - so the blue brown brown blue brown brown
69:39 - brown green is the raw data the
69:42 - observations the measurements I make of
69:44 - my students eye colors
69:46 - blue is the eye color of my first
69:48 - research student Brown is the eye color
69:50 - my second research student
69:52 - Etc
69:54 - the frequency distribution just looks at
69:56 - all the possible levels that we observe
69:58 - blue brown
70:00 - and green
70:02 - and then counts the number of number of
70:05 - observed values in each of those levels
70:07 - I got one blue two blue only two
70:11 - Brown I got one two three four five
70:14 - green I got
70:16 - just one so the frequency distribution
70:19 - is 251. for blue brown green
70:23 - ooh red star
70:26 - I think that's what I want to do so
70:29 - question one
70:31 - okay and I encourage you to write these
70:33 - questions and your answers in your notes
70:36 - on the left hand side
70:37 - what is the difference between a grouped
70:39 - and an ungrouped frequency distribution
70:43 - pause
70:45 - and we're back
70:50 - so that was all about the ungrouped or
70:53 - for the categorical variables now we're
70:54 - going to look at grouped which is almost
70:56 - always applied to numeric variables
70:58 - remember the main difference between the
71:00 - ungrouped and the grouped is that the
71:03 - ungrouped each
71:05 - Row in the table corresponds to one
71:07 - level in the variable
71:09 - whereas for the grouped each row is
71:12 - going to correspond to a range of values
71:14 - of that variable
71:16 - so the first step in creating this
71:18 - grouped frequency distribution is
71:21 - decide how many classes should be in the
71:24 - distribution
71:25 - now we've got rules of thumb
71:28 - none of them are good because they're
71:29 - rules of thumb
71:30 - the reality is you should choose many
71:34 - different numbers of classes so the
71:36 - first time through you should choose
71:37 - five and then maybe 10. and see what the
71:42 - information given by these distributions
71:44 - how that changes
71:46 - Maybe 20. but you're going to realize
71:49 - that if we do 5 10 20 12 and 18 we do
71:53 - the we got to do these steps five times
71:55 - and it takes a lot of effort to do just
71:58 - one so you're going to fall in love with
72:00 - the computer because the computer can do
72:01 - it just like that I don't know if you
72:03 - could hear me snapping my fingers
72:07 - but I'm snapping my fingers so the first
72:09 - step is to decide how many classes
72:11 - should be in the distribution
72:16 - once you've decided that two
72:19 - choose an appropriate class width class
72:21 - width is usually going to be the highest
72:24 - value minus the lowest value divided by
72:26 - the number of groups
72:27 - sometimes you'll round down to something
72:31 - logical for the lowest value round up to
72:33 - something logical for the upper value
72:37 - and then divide by the the number of
72:39 - classes sometimes you'll Skip One go
72:42 - directly to two and and physically
72:44 - determine that class width
72:47 - what the classes should be based on the
72:50 - the problem itself and we'll see that
72:52 - later
72:54 - in those cases easy lends itself to
72:57 - Natural divisions
72:59 - such as decades or years or hundreds of
73:02 - dollars
73:06 - three find the class limits
73:09 - the lower class limit and this is for
73:11 - each of the classes by the way the lower
73:14 - class limit is the smallest number that
73:16 - can belong to that class
73:18 - and the upper class limit is the largest
73:20 - number that can belong to that class
73:25 - so all the values in that class
73:28 - are greater than the lower class limit
73:30 - and lower than the upper class limit
73:36 - and now you just count the frequency of
73:38 - each class 4 is the easy part by the way
73:41 - you just determine the frequency count
73:43 - how many are in each of those classes
73:48 - so here's some terminology class width
73:52 - is the difference between the lower
73:53 - limit and the upper limit of two
73:55 - consecutive classes
73:59 - lowers the smallest number that can
74:00 - belong to that class upper class limits
74:02 - the largest that can belong to that
74:04 - class
74:08 - here's the example
74:10 - here are 20
74:13 - television prices 3D TVs
74:16 - the first 3D TV price is one thousand
74:19 - five hundred ninety five dollars the
74:21 - second one was one thousand one hundred
74:22 - ninety nine dollars and sixteen eighty
74:24 - five Etc all the way up to 1999
74:26 - great song
74:28 - but that has nothing to do with class
74:30 - limits
74:32 - first step is determine the number of
74:33 - classes if we follow these three steps
74:36 - determine the number of classes there's
74:37 - 20 data points
74:39 - how many classes should we do
74:42 - or we could use the problem itself to
74:45 - hint what the classes themselves should
74:49 - be
74:51 - um these are TV prices
74:53 - we got the lowest TV price here of uh
74:57 - 15.95 the highest of 19.99
75:02 - these are in looks like class width
75:04 - should be probably be a hundred dollars
75:06 - that would make sense here so we do
75:08 - maybe 1400 to 1500 1500 to 1600 1600 to
75:14 - 1700 that would make sense
75:17 - remember the purpose of the graphics for
75:20 - you
75:21 - is so that you understand the
75:23 - distribution of the data
75:25 - for your reader or your client it's that
75:29 - they understand the story that the data
75:32 - is telling
75:39 - so to determine the class width we were
75:41 - told five by the way use five classes so
75:45 - maybe
75:46 - my idea doesn't quite work at least for
75:49 - this problem I think it would work for
75:51 - reality five classes
75:54 - so 19 this is the highest value the
75:57 - lowest value divided by 150 so about 81
75:59 - would give us a class width
76:03 - okay 81 dollars class width I guess if
76:07 - we're going to follow this without
76:08 - thinking that would be a good thing to
76:10 - do
76:13 - but if we actually want to think
76:15 - 81 dollars doesn't
76:17 - really seem reasonable in telling the
76:20 - story of the data a hundred dollars
76:22 - would make sense
76:24 - because there's something magical about
76:25 - round numbers in fact doubly round
76:27 - numbers
76:29 - there's two roundnesses here and there's
76:31 - none here oh I guess the eight if you
76:33 - turn on side it's got two circles but
76:35 - this never mind so a hundred dollars
76:38 - would make sense
76:41 - then we gotta choose 100 width so we
76:43 - gotta say how low to how high
76:46 - um
76:48 - beginning at 1500 makes sense if we want
76:52 - to just follow the directions without
76:53 - thinking at all 15.95 would make sense
76:56 - is the first class starting point 1500
76:59 - if you actually want to present this to
77:02 - a client would make much more sense
77:07 - so 1600 will be this class limit of the
77:09 - second
77:11 - 1700 1800 1900 2000 will be all the
77:15 - classes or the class boundaries
77:17 - otherwise known as the breaks
77:26 - should not be overlapped in the class
77:28 - boundaries really
77:30 - although in all reality it doesn't
77:32 - matter
77:33 - so here the 3D thinking that butt but
77:36 - they can't actually overlap so now we've
77:39 - got our classes 1500 15 99 1600 or 16.99
77:45 - and now we just calculate the
77:47 - frequencies
77:50 - two five four five four
77:55 - notice that if you add up to five four
77:57 - five and four you should get 20 which is
77:59 - our sample size
78:03 - now we're going to find something called
78:05 - a class boundary
78:08 - it is the value lies halfway between the
78:10 - upper limit at one class and the lower
78:11 - limit of the next class
78:13 - so the class boundary here would be 15
78:16 - 99.50
78:18 - 1699 50 17 99 50 18 99 50.
78:29 - or 15.99.5 if you want
78:36 - and the purpose of the class boundaries
78:37 - is to make very clear that there is no
78:40 - overlap and that everything no overlap
78:43 - and that every possible value fits into
78:45 - one and only one of the classes
78:52 - the midpoint is the upper limit plus the
78:55 - lower limit divided by two
79:01 - these midpoints are used
79:03 - um
79:04 - for estimating the average value in each
79:06 - class we'll see it in the next chapter
79:08 - in dealing with grouped data
79:13 - lower upper divided by 2 gives us a
79:16 - class midpoint of 1549.5
79:20 - although I don't know a statistician
79:22 - alive that wouldn't say 1550 would be
79:25 - the midpoint
79:31 - 1500 plus 1599 divided by 2 gives us
79:33 - this is the midpoint 1600 plus 16.99
79:37 - divided by 2 gives us this is the
79:38 - midpoint
79:44 - those are for frequencies if we want to
79:46 - do for relative frequencies we just
79:48 - divide the frequency by the sample size
79:49 - so 2 divided by 25 divided by 24 divided
79:52 - by 25 divided by 24 divided by 20.
79:58 - notice that adding up all the
80:00 - frequencies gives us the sample size
80:01 - which we're always going to symbolize as
80:04 - a lowercase n
80:08 - lowercase f is the frequency the
80:10 - subscript of the I is for class I so F
80:13 - sub I
80:15 - is the frequency in the I class
80:25 - so this calculates the relative
80:26 - frequency we first add up all the
80:28 - frequencies to determine the sample size
80:32 - now we just divide each of the class
80:34 - frequencies the F sub I's by 20
80:38 - so the first relative frequency is going
80:40 - to be F sub 1 divided by 20.
80:43 - second will be F sub 2 divided by 20.
80:46 - then F sub 3 divided by 20
80:50 - Etc
80:55 - cumulative frequency is just the sum of
80:57 - the frequencies of a given class added
81:00 - to all lower classes
81:02 - so if you are able to order your data
81:05 - then you can do a cumulative frequency
81:07 - if your data cannot be ordered that is
81:09 - if it's nominal and you cannot do a
81:12 - cumulative frequency it doesn't make
81:13 - sense to talk about less than or equal
81:15 - to if there's no ordering
81:20 - so here we got the frequencies and this
81:22 - last column is the cumulative frequency
81:25 - so the cumulative frequency for the
81:27 - first one is always the frequency
81:28 - cumulative frequency for the second is
81:30 - going to be five plus two
81:32 - because it's whatever is here Plus
81:34 - what's above
81:35 - four plus five plus two
81:38 - five plus four plus five plus two
81:40 - four plus five plus four plus five plus
81:42 - two
81:45 - notice the cumulative frequency always
81:47 - ends with our sample size little n
81:55 - here's an example
81:58 - data collected on the numbers of miles
81:59 - that professors strive to work daily are
82:01 - listed below clearly not below it's on
82:03 - the next side to use these data to
82:05 - create a frequency distribution
82:07 - that includes the class boundaries
82:08 - midpoint relative frequency and
82:10 - cumulative frequency of each class we're
82:12 - told we have to use six classes
82:19 - so here's the data
82:22 - the low is one
82:24 - the high is 11.9 we're told six
82:29 - so if we just follow the directions
82:31 - without thinking we're going to come up
82:32 - with 1.8 as our class width
82:36 - so we'll do class boundaries as being 1
82:40 - 2.8
82:42 - 4.6
82:44 - .4
82:46 - 8.2
82:48 - 11
82:52 - 12.9 I whatever they are
82:54 - I lost count already
82:58 - I'm just adding 1.8 where are we I'm
83:01 - just adding 1.8 to each of those
83:08 - notice that it may make sense to do it
83:11 - class width of 2 instead of 1.8 one it's
83:14 - a lot easier to do in your head and two
83:17 - it makes a lot more sense in terms of
83:19 - presenting the data
83:21 - so instead of 1.8 let's round this up to
83:23 - 2.
83:24 - so the boundaries will be 1 3 5 7 9 11
83:28 - 13.
83:30 - see much easier
83:35 - now the lower class limit is the limit
83:36 - of our first class
83:38 - one is the lowest so if we just want to
83:41 - follow this we can start with the one I
83:43 - would say let's start with zero that
83:46 - would kind of make sense
83:48 - so it'd be zero two four six eight ten
83:51 - twelve that would include all the data
83:53 - instead of one three five seven nine
83:56 - eleven thirteen which would also include
83:59 - all the data
84:01 - but maybe zero two four six eight Etc
84:04 - looks better to the client
84:10 - however we're going with one
84:13 - so one three five seven nine eleven
84:17 - three five seven nine eleven thirteen
84:23 - so we're backing It Off by point one
84:25 - now we do the frequencies
84:30 - there's our midpoints
84:37 - and again don't know any statistician
84:40 - that would say the midpoint is 1.95
84:42 - every statistic I know would say the
84:44 - midpoint is two four six eight ten
84:48 - twelve
84:49 - and the class boundaries and the class
84:51 - would be one to three three to four four
84:54 - to five five three to five five to seven
84:57 - seven to nine
84:59 - but notice in each case you include the
85:01 - lower but not the upper
85:05 - frequency is three three four two four
85:08 - two we get that just by going back to
85:09 - the data itself and Counting
85:13 - relative frequency is our frequency so F
85:16 - sub 1 divided by our sample size of 18 F
85:18 - sub 2 divided by 18 F sub 3 divided by
85:21 - 18.
85:22 - cumulative frequency is the frequency in
85:24 - this group plus all those lower
85:28 - so be 3
85:30 - 6
85:32 - 10
85:33 - 12.
85:35 - 16
85:37 - .
85:39 - and of course we could have a cumulative
85:41 - relative frequency
85:44 - which would be just each of these
85:46 - divided by 18.
85:53 - so here's the overview again notice
85:55 - there's a lot of work to this if we do
85:57 - it by hand and if we have a realistic
86:00 - data set and a lot of decisions we have
86:02 - to make and the the distribution itself
86:05 - depends on those decisions so we're
86:07 - going to want to do several of these
86:09 - hopefully not by hand so we got to
86:13 - decide on the number of classes I choose
86:15 - an appropriate class with find the class
86:16 - limits determine the frequency in each
86:18 - of the classes
86:21 - that's a lot of work for one frequency
86:24 - distribution and again remember this is
86:25 - for grouped frequency distributions
86:29 - being able to automate this would would
86:32 - make life so much easier and allow us to
86:34 - look at different uh the effect of
86:38 - different choices we make so instead of
86:40 - five we'd be able to figure out well
86:43 - what does the frequency distribution
86:44 - look like if we chose 10 classes what if
86:47 - we chose 50 classes what if we chose two
86:49 - classes what would it what would it look
86:50 - like
86:51 - and what stories do each of those
86:54 - frequency distributions tell us about
86:56 - the data
87:00 - here are the other characteristics we
87:01 - got the classes boundaries midpoints
87:03 - filter frequencies and cumulative
87:04 - frequencies
87:09 - and we got a red star
87:13 - it's good because that's the last page
87:14 - we got two questions
87:16 - question number two again put this over
87:19 - in your notebook on the left hand side
87:22 - write the question and your answer below
87:24 - it so you can transfer that into
87:25 - moodle's quiz what are the steps in
87:28 - constructing an ungrouped frequency
87:30 - distribution
87:34 - and then question number three
87:37 - what are the steps in constructing a
87:39 - grouped frequency distribution
87:41 - and a hint
87:43 - one of those two is going to be a lot
87:46 - easier than the other
87:48 - um
87:49 - and what I'm getting at in these two
87:51 - questions don't hit pause yet what I'm
87:53 - getting at least two questions is you'll
87:55 - see that
87:56 - the groups frequency distribution steps
87:59 - are exactly the same as the ungrouped
88:02 - except for you have to add a few things
88:05 - and taking your numeric variable and
88:07 - making it categorical
88:10 - take your numeric variable at making it
88:12 - categorical
88:13 - and that's what these classes are doing
88:15 - you're taking your numeric variable and
88:17 - you're classifying them you're making
88:19 - them categorical
88:22 - and that's it
88:24 - section 2 2 takes what we did today and
88:28 - makes pictures out of it okay makes
88:29 - Graphics out of it plus we get to see a
88:31 - lot of different types of graphics and
88:33 - we'll see how to do all of this really
88:36 - fast in r
88:38 - so
88:40 - hope this was helpful
88:42 - talk to you later
88:45 - hello and welcome to section 2.2
88:47 - graphical displays of data recall in the
88:50 - last lecture that we looked at how to
88:52 - create frequency distributions both
88:55 - ungrouped and grouped
88:57 - today we're in this lecture we're going
89:00 - to look at turning those frequency
89:02 - distributions into Graphics that tell
89:05 - the story of the data I do want to
89:08 - emphasize the purpose of Graphics is to
89:11 - tell the story of the data
89:14 - the graphics that we see today are going
89:16 - to tell the story of the data to you
89:18 - mainly because they look pretty ugly
89:21 - but we're going to see in some of the
89:23 - scas the statistical Computing
89:25 - activities how to pretty up those
89:27 - Graphics so that they tell the story in
89:29 - a pleasing way to your audience
89:32 - remember the graphics serve two purposes
89:35 - that are really the same purpose the
89:37 - basic Graphics tell you the story of the
89:40 - data
89:41 - and then the the the publication quality
89:44 - Graphics tell your audience the story
89:48 - the data that you learned
89:50 - so we're going to look at those
89:51 - utilitarian ugly Graphics today and in
89:54 - SCA I believe it's sca2 we're going to
89:57 - see how to create nice looking Graphics
89:59 - that can help convey that story to your
90:03 - reader
90:05 - um we're going to look at how to create
90:06 - some bar charts we have a pie chart
90:08 - before that yeah but bar charts uh both
90:11 - uh univariate bar chart that is for one
90:14 - categorical variable but also some
90:16 - bivariate stacked and side-by-side bar
90:19 - charts look at histogram and it's
90:21 - neighbor stem and leaf plot and then
90:24 - line graph we're going to look at
90:27 - different shapes of the distributions
90:28 - remember these these Graphics are going
90:30 - to tell us about the data
90:35 - here's some rules and what graphs or
90:37 - Graphics should have they should stand
90:39 - alone without the original data that is
90:42 - in your paper you provide a graphic
90:44 - instead of providing all of the data
90:47 - the graphics have to have labels and
90:49 - values for both axes
90:52 - when appropriate a legend of source and
90:54 - date should be included
90:56 - the source and the data are usually in
90:58 - the caption however
91:01 - and in a paper the graphic must contain
91:03 - a number and a caption so the first
91:04 - figure that you include is going to be
91:07 - called Figure one and then you're going
91:08 - to give it a caption a brief description
91:11 - of what that graphic says
91:13 - the graphic must also be described in
91:16 - the pros and you're in your writing in
91:18 - the paper itself ooh red star so let's
91:22 - go to our first question
91:26 - what do all Graphics need and remember
91:28 - you should write this over on the left
91:30 - hand side of your notes
91:32 - the question and your answer and you
91:34 - should hit pause to remember to gain
91:37 - some time here
91:39 - because I'm going to hop back to the
91:41 - lecture
91:44 - so we'll start with pie charts probably
91:46 - the worst creation of statisticians a
91:50 - pie chart shows how large each category
91:52 - is in relation to the whole the whole is
91:55 - represented by the entire circle the
91:57 - parts are slices of that pie
92:01 - he used to describe qualitative
92:03 - categorical data it's pretty
92:06 - straightforward you create your your
92:08 - frequency distribution for it you
92:10 - calculate the relative frequencies and
92:13 - then you multiply each of those relative
92:14 - frequencies by 360 degrees or 2 pi if if
92:18 - you prefer
92:20 - here's the example
92:24 - calculate the class housing types for
92:26 - students in this in a statistics class
92:29 - four types of housing these this is the
92:32 - frequency distribution we learned about
92:35 - how to calculate that in the last
92:36 - lecture
92:38 - 2015 nine and five 49 is our sample size
92:43 - so now we create relative frequencies
92:47 - so the first relative frequency is 20
92:50 - divided by 49 Second will be 15 over 49
92:53 - there will be 9 over 49 the last one
92:57 - will be 5 over 49.
93:00 - and then we create the central angle
93:02 - measures by multiplying those relative
93:03 - frequencies by 360 degrees
93:07 - so we know that the apartment has to
93:09 - cover 147 degrees of that 360 degree
93:12 - circle
93:14 - the dorm has to cover 31 which is 110
93:17 - degrees
93:19 - Etc
93:20 - and then we just create
93:22 - the histogram the pie chart drawing a
93:25 - circle a dot in the center
93:29 - and then
93:30 - measure that this 41 percent is 147
93:34 - degrees
93:36 - and this 31 percent corresponds to 110
93:39 - degrees and this 18 is 66 degrees
93:43 - Etc
93:46 - rather difficult to create
93:49 - you know reality
93:52 - it also suffers from the fact that
93:55 - slices have to be different colors and
93:57 - brighter colors tend to draw our
93:59 - attention
94:00 - more so the 10 percent slice the gold
94:04 - yellowish gold slice tends to look
94:06 - bigger than the 10 percent that is
94:08 - allotted to it
94:10 - furthermore it's kind of difficult
94:11 - without the 10 and 18 actually stated
94:14 - there to compare the size of this slice
94:16 - to the size of that slice our eyes our
94:18 - brains have trouble distinguishing
94:20 - central angles and the relative sizes of
94:23 - those
94:24 - so for all intents and purposes unless
94:27 - someone tells you you must make a pie
94:29 - chart avoid pie charts a much easier
94:32 - chart to create and better all-around
94:35 - chart to create is a bar chart which
94:37 - we'll see next but we got the R so let's
94:41 - go ahead and start r
94:43 - if I can figure out how to do that
94:50 - didn't do it
94:53 - that kind of did it
94:55 - so I'm going to
94:58 - just start r
95:00 - I've got our a link to our in several
95:03 - places this is one of them you will
95:06 - probably have two links on your desktop
95:07 - or one or two links on your desktop
95:11 - but it'll be here I've got several
95:14 - versions of our this is an old version
95:16 - of art but it works for me
95:19 - just started
95:21 - first thing always new script
95:24 - notice there's overlap between the two
95:26 - in PCS there's going to be that overlap
95:28 - in Max there may not be
95:30 - so I'm going to tile vertically notice I
95:32 - clicked on the console window first so
95:34 - the console window is over here the
95:36 - script window is over there
95:39 - so the first thing I'm going to do is
95:41 - load data
95:45 - these pound signs or hashtags
95:48 - indicate a comment
95:51 - once R comes across one of them it
95:54 - ignores everything else in that line
95:56 - so I use three to indep more
95:58 - notice I move things around I'm going to
96:01 - make this side bigger so you can see it
96:04 - I'm going to load data from the internet
96:06 - I'm going to read a CSV file from the
96:09 - internet
96:10 - function I'm going to use is read.csv it
96:13 - requires that you specify the path to
96:16 - that location
96:27 - what we're going to look at is the crime
96:29 - data set
96:32 - remember you've got the pause button so
96:34 - you can pause this and type it in
96:37 - um
96:38 - notice that if I just run this and by
96:40 - click run this that's control r on a PC
96:43 - or command enter on a Mac it's going to
96:47 - go to the Internet it's going to go to
96:48 - this particular URL
96:50 - and it's going to read it and it's going
96:52 - to spit it back out on over here on the
96:55 - console window so this is that data set
96:58 - it just printed off it didn't save it R
97:01 - doesn't know this data set now
97:04 - since we are going to be using this data
97:06 - set we need to save it into a variable
97:08 - by bad habit I call
97:11 - this variable DT DT for data a good
97:15 - habit and computer scientists would want
97:17 - to do this is to name it something
97:19 - descriptive so crime will be the crime
97:22 - data set
97:23 - or they may want to do
97:26 - crime data set if they like their camel
97:28 - case
97:30 - or Capital C crime data set
97:33 - I'm bad enough at typing that I just
97:34 - leave it as DT
97:37 - I run that notice what happens over on
97:39 - the left it just spits out DT equals
97:41 - read.csv of this URL
97:44 - no errors
97:45 - because I typed everything correctly
97:48 - and all this data set is now stored in
97:51 - the variable DT if I want to see it
97:54 - I'm going to type DT over on the console
97:56 - side
97:57 - to run a line on the console side you
97:59 - just have to enter and boom it spits out
98:02 - the entire data set
98:04 - [Applause]
98:08 - um
98:09 - we got a variable repub if I want to
98:12 - look at that variable repub I can type
98:14 - repub here
98:16 - on the on the script side if I'm going
98:18 - to save it and use it later
98:22 - or I can just explore over on this side
98:25 - type repub
98:27 - type it in hit the enter key and get an
98:29 - error
98:31 - error object repub not found
98:34 - um there's going to be two reasons for
98:35 - that error one misspelling two you
98:39 - didn't attach the data set
98:41 - R only sees one variable at the moment
98:44 - that one variable is DT the entire data
98:46 - set and there are ways of peeking into
98:49 - the data set for R name the data set
98:52 - dollar sign says Peak into
98:56 - and then the variable name
98:58 - and that will give us all the Republican
99:00 - values
99:02 - or if we're going to be doing this a lot
99:04 - and we don't want to type DT frequently
99:08 - we can just attach the data
99:11 - run the attach and then repub
99:14 - now gives us the data values
99:18 - so if I don't attach just type in the
99:21 - name of the variable gives me an error I
99:22 - have to do the data set name dollar sign
99:25 - repub to get those values
99:27 - or if I just attach the entire data set
99:29 - I can type repub or any of these
99:31 - variables
99:35 - to access them
99:38 - actually if all I want is the names of
99:40 - the variables
99:43 - I can use the names function
99:46 - similarly if I want a summary
99:50 - of
99:51 - each of the variables I can type summary
99:54 - this is very useful because it tells us
99:56 - how R sees the variables themselves if
100:00 - it gives a frequency distribution RC's
100:02 - this variable as being a categorical
100:04 - variable
100:05 - is it if it gives us the six number
100:07 - summary RCS this as being a numeric
100:10 - variable
100:11 - so back to repub the minimum value is a
100:14 - negative 0.25 the maximum is 0.24
100:17 - there's three missing values the median
100:20 - is 0.01 I mean this 0.005
100:23 - but that isn't that doesn't matter
100:26 - because we want to look at how to do a
100:28 - pie chart
100:29 - pie chart let's do a pie chart of census
100:32 - four
100:34 - there's our census for variable
100:37 - first thing we need to do is tabulate
100:39 - the values that is get a frequency
100:42 - distribution for it
100:44 - we do that with the table function
100:47 - and then to create the pie chart we just
100:49 - use the pi function on that frequency
100:51 - distribution
100:54 - and as you can see it's kind of
100:56 - difficult for me to tell which of these
100:58 - two is bigger Midwest or west if I look
101:00 - at it long enough I can tell that it's
101:02 - West but it took me a long time to
101:05 - figure that out I had to stop and think
101:07 - about it
101:09 - those aren't pretty colors but this is
101:11 - just for you to understand the data
101:13 - itself
101:14 - so the pi function when it's applied to
101:16 - a frequency distribution gives us a pie
101:18 - chart and to create that frequency
101:20 - distribution it's the table function
101:23 - let's see if I can get back to this
101:31 - okay
101:32 - moving on from a pie chart we go to the
101:34 - barge graphs or the bar charts a bar
101:37 - graph is just like a pie chart except
101:39 - instead of pieces of a circle it's bars
101:43 - of a certain height
101:44 - the height of those bars represents the
101:47 - frequency of the data of the values in
101:49 - the data set it's for qualitative data
101:52 - operator charts are for non-monal data
101:55 - it orders the bar Heights from lowest to
101:59 - highest or highest to lowest
102:02 - your choice
102:03 - stacked bar graph is for to looking at
102:06 - two different categorical variables
102:10 - um
102:11 - where the bars are stacked and then
102:14 - we've got the side by side it's where
102:16 - the bars are side by side clearly
102:20 - um of all of these if I've got the the
102:22 - bottom two or for for two categorical
102:26 - variables Prado is only phenomenal the
102:29 - bar is phenomenal and ordinal
102:32 - between the Stacked and the side by side
102:33 - I personally prefer the side by side
102:35 - it's a personal preference I don't know
102:37 - that there's any science behind it other
102:39 - than it's my personal preference
102:43 - so here we are creating a bar graph from
102:45 - our data that we've already found the
102:47 - frequency distribution for The Apartment
102:49 - bar is going to be 20 High the dorm bar
102:51 - is going to be 15 High the House Bar is
102:53 - going to be nine High the sorority
102:54 - fraternity bar is going to be 5 high
102:57 - because those are the frequencies that's
102:59 - all there is to this and now I don't
103:00 - have to squint and try to estimate
103:03 - which of these is bigger and which is
103:06 - smaller
103:07 - and by the way this happens to be a
103:09 - Pareto chart because it's going from
103:11 - largest to smallest or smallest to
103:12 - largest
103:14 - we've got the R so let's see how we can
103:16 - do a bar graph or a bar chart in r
103:24 - function is bar plot
103:27 - again it's applied to a frequency
103:29 - distribution
103:33 - and there's our bar plot
103:39 - the height of Midwest corresponds to the
103:41 - frequency of Midwest states
103:43 - height of Northeast to the frequency of
103:45 - northeast states Etc
103:47 - if I want to turn this into a Pareto I
103:51 - just have to sort
103:57 - and now we see that the South has the
103:59 - most frequent uh Northeast is the least
104:02 - frequent
104:04 - pretty straightforward
104:06 - by the way the space that I left here
104:08 - and here are optional those spaces are
104:11 - for me as the reader to better
104:13 - understand what I'm typing and to make
104:15 - sure I don't make mistakes if I don't
104:17 - include those spaces there's a good
104:19 - chance that I will forget the closing
104:21 - parentheses I'll run that line and I'll
104:24 - get a plus mark down here
104:26 - plus Mark indicates that R is waiting
104:30 - for
104:31 - some more input it's waiting for
104:33 - something for me usually it's a closing
104:35 - parenthesis or a closing brace
104:38 - in this case I know it's a closing
104:40 - parenthesis
104:43 - two ways of dealing with it if I know
104:46 - it's a closing parenthesis I can just
104:47 - highlight that parenthesis and run it
104:51 - it runs the line prints out the the bar
104:54 - chart and we get back to a less than
104:57 - sign
104:59 - if I wasn't sure what the problem was
105:02 - I click over on the console side and
105:05 - I'll hit the escape button
105:07 - and then enter and I get back to the
105:09 - lesson sign
105:10 - or maybe that's a greater than sign
105:13 - but the key is use spaces for you
105:17 - so that you can tell what you're typing
105:27 - here's how to do the prayer chart by
105:29 - hand you just order it
105:31 - and you get that
105:33 - notice this is a horizontal
105:35 - bar chart
105:37 - here's how to do horizontal and r
105:44 - h o r i z equals true
105:48 - there's a comma here
105:50 - is stands for horizontal
105:52 - there's true now we've got ourselves a
105:54 - horizontal Pareto chart
106:03 - here's stacked stacked bar charts are
106:06 - for looking at comparing two categorical
106:09 - variables
106:11 - um in this case one variable is the
106:13 - sample letter and the other is housing
106:16 - type
106:17 - so in this in the apartment housing type
106:20 - sample a came up with about 20
106:23 - of students who were in the department
106:26 - and Sample B came up with
106:29 - 33 I'm sorry 13 because this the height
106:33 - from here to here is 13.
106:38 - here's how to do this uh the Stacked
106:42 - side by side looks exactly the same
106:44 - except the heights these are stacked on
106:46 - top of each other they're side by side
106:48 - hence the name side by side
106:53 - let's see how to do side by side in r
106:57 - first we got to figure out that
106:59 - frequency distribution we'll use the
107:01 - table command tables used for frequency
107:03 - distributions of categorical variables I
107:06 - will do census for that's a comma let's
107:11 - do it by what names do we have available
107:15 - here we go looking through these there's
107:18 - lots and lots of variables
107:20 - I'm going to do Dom Paul culture as my
107:23 - second
107:24 - [Applause]
107:27 - so this is our two dimensional
107:31 - frequency distribution
107:33 - um
107:34 - this 5 here says that in our sample five
107:37 - states are in the Midwest and are
107:39 - individualistic
107:41 - this seven is seven states in the
107:43 - Midwest and are moralistic
107:45 - this zero is zero states in the midwest
107:48 - are traditionalistic
107:50 - seven western states are moralistic
107:55 - 15 southern states are traditionalistic
107:58 - so that's our frequency distribution
108:01 - and now we just apply our bar plot
108:05 - to this
108:08 - copy and paste is fantastic isn't it
108:15 - so here we are with this a stacked
108:18 - individualistic all together there's 17
108:20 - more holistic altogether they're 17
108:22 - traditionalistic altogether they're 17.
108:26 - um
108:27 - these colors correspond to the census 4
108:30 - region
108:33 - see if this works
108:35 - specify Legend equals true
108:39 - so the lightest is the West
108:43 - so this would be the western states that
108:45 - are individualistic this will be the
108:47 - southern that are individualistic this
108:49 - is the Northeast that are
108:50 - individualistic this is the Midwest that
108:52 - are individualistic
108:54 - this will be the South that are
108:55 - traditionalistic and the West that are
108:57 - traditionalistic
109:00 - that's one way of looking at it the data
109:07 - we could switch the order
109:14 - and now the base is going to be the
109:17 - region of the country and the bar
109:19 - Heights are going to be based on the
109:21 - um dominant political culture so in the
109:24 - midwest individualistics about five
109:27 - moralistic is
109:28 - about seven
109:31 - Northeast South West
109:34 - which of those two is better
109:38 - this or this the answer is it depends on
109:42 - what story you're trying to tell or to
109:44 - learn about the data
109:46 - if you're trying to learn about the data
109:47 - both are important to do
109:51 - if you have trouble with the colors you
109:53 - can specify different colors
109:56 - I'm going to have to specify four colors
109:58 - here with call col stands for colors
110:02 - and pretty easy we can just do one
110:04 - through four that's a colon it indicates
110:07 - through so this one colon four is the
110:10 - numbers one through four
110:13 - one two three four
110:16 - and now we run this
110:18 - and we can see much more clearly what
110:20 - the colors are the West is the blue the
110:23 - south is the green the Northeast is the
110:25 - red the Midwest it's the black
110:28 - for this second
110:30 - there's only three
110:33 - I'm going to do colors four through
110:34 - seven four through six four five and six
110:38 - would be the colors we use
110:41 - ew
110:42 - probably should have guessed what the
110:43 - colors were blue cyan and magenta
110:49 - I guess it could be worse
110:52 - so those are side by side I'm yeah those
110:55 - are stacked
110:57 - to do side by side
111:01 - we just specify
111:07 - we spell correctly beside equals true
111:10 - [Applause]
111:12 - so they get side by side we have to
111:13 - specify b side equals true
111:16 - and now we have side by side
111:21 - this may be helpful to look in the north
111:23 - Midwest and see that the moralistic
111:25 - outnumbers the individualistic
111:28 - in the Northeast the individualistic is
111:30 - about double the moralistic in the South
111:32 - strongly traditionalistic with just a
111:35 - couple individualistic and the West is
111:37 - much more spread out
111:42 - and again we could change the order
111:49 - but we have we now have four colors we
111:52 - need to deal with
111:53 - so the individualistics are kind of
111:55 - spread out over the four groups the
111:57 - moralistic it's totally missing the
111:59 - South and traditionalistic is Midwest
112:01 - and Northwest totally missing
112:03 - so again which of these two is better
112:06 - they both tell us a story about the data
112:09 - therefore they are both important
112:16 - frequency histogram or just a histogram
112:18 - is a bar graph of a frequency
112:20 - distribution of quantitative data
112:22 - frequency histogram is based on your
112:25 - groups
112:26 - distribution from last lecture
112:29 - a relative frequency histogram is also
112:31 - based on your grouped frequency
112:33 - distribution but it's the relative
112:34 - frequency distribution
112:37 - characteristics of histograms it's a bar
112:40 - graph of a frequency distribution
112:42 - horizontal axis is a real number line
112:45 - it's the values
112:47 - of your variable the vertical axis is
112:50 - going to be the frequency within each of
112:52 - those classes
112:54 - the width of the bars represents a class
112:56 - width the bars in the histogram should
112:59 - touch
113:00 - tell that to excel because Excel doesn't
113:02 - let them touch unless you know how to
113:04 - unless you know the tricks
113:06 - the height of each bar represents the
113:08 - frequency
113:09 - so here's the example remember this
113:11 - example from last time
113:14 - took a long time to come up with this
113:17 - table
113:18 - frequencies
113:20 - getting the graph is pretty
113:22 - straightforward though
113:23 - there's two in the first class five in
113:25 - the second class four and the third
113:27 - class five then four again
113:30 - this was the hard part
113:32 - creating bar charts off of it the easy
113:34 - part
113:35 - this is for the frequency histogram
113:39 - and doing it for the relative frequency
113:42 - you get exactly the same shape exactly
113:44 - the same shape you just got to call the
113:47 - vertical axis the relative frequency
113:50 - here's how to do a histogram and r
113:57 - we'll do it on the violent crime rate in
113:59 - 1990 the function is hist hist for
114:01 - histogram there's our basic histogram
114:05 - that was pretty fast
114:07 - by default in this case particularly R
114:10 - is going to have class widths of 500
114:12 - there's going to be one two three four
114:15 - five classes
114:17 - there's a there's an algorithm it goes
114:19 - through to figure out the quote optimal
114:22 - but there's nothing optional about these
114:28 - you need to actually specify or I'm
114:31 - sorry you need to do multiple histograms
114:33 - so you get a better feel for what the
114:35 - data are so here's how you specify it's
114:38 - with the breaks
114:40 - option
114:43 - this will give us about 11 breaks
114:49 - um breaks are very are going to be close
114:51 - to the class boundaries it can be
114:55 - similar to the class boundaries
114:57 - you want more than 11 breaks how about
114:59 - 21 breaks
115:01 - again the key is 51 breaks the key is
115:05 - what story are these telling us and
115:08 - what's the best story for it to tell
115:10 - there's a thousand and one breaks
115:14 - there's five breaks
115:17 - there's two breaks
115:20 - notice that
115:22 - we could also or realize that we can
115:24 - also instead of just say the number of
115:26 - breaks we can specify the actual class
115:28 - boundaries themselves
115:30 - also with the brakes slot
115:33 - we're going to use the sequence function
115:34 - seq for sequence
115:38 - the lowest is zero the highest I think
115:42 - was 2500 and then the third slot here
115:45 - belongs to the number of classes you
115:48 - want
115:49 - I'm sorry
115:51 - ignore that it doesn't belong to the
115:53 - classes you want a number of classes it
115:54 - belongs to the class width so if I
115:57 - wanted my class widths to be 100 that's
115:59 - what it's going to look like
116:02 - if I want the class widths to be 200
116:06 - oh I got an error down here
116:08 - some X not counted oh that's because
116:11 - this function will set up values
116:14 - I'm going to run that just that part at
116:17 - 0 200 400 600 I'm adding 200 each time
116:20 - until I get to 2400 because adding
116:22 - another 200 will put us Beyond
116:24 - and the Washington DC is actually
116:26 - sitting out there above 2400 it needs to
116:29 - be counted
116:31 - so
116:32 - 2600.
116:34 - and now it runs
116:37 - 250
116:42 - 10
116:45 - . notice how quickly we're able to
116:47 - create these histograms we don't have to
116:48 - focus on the particular calculations we
116:52 - can choose our class widths
116:55 - and allow that to tell us the story of
116:58 - the data much more quickly
117:01 - in all of these the story of the data
117:03 - says hey look at this outlier it's far
117:05 - from all the others this happens to be
117:07 - the District of Columbia everything else
117:09 - seems to be clumped
117:11 - nice little bell-shaped ish to it
117:15 - and that was true when it was a hundred
117:18 - for the class width again bell shaped
117:20 - not quite so smooth but a bell-shaped
117:21 - nonetheless
117:23 - when we use 2500
117:27 - when we use 200
117:30 - I mean
117:31 - this actually looks like the best of
117:33 - them but
117:34 - it looks like the best of me because
117:36 - I still got that outlier and I've got a
117:39 - nice curve over on the left
117:43 - so notice how quickly we did one two
117:46 - three four five six seven eight nine ten
117:48 - eleven twelve thirteen fourteen fifteen
117:50 - sixteen Seventeen histograms
117:53 - if we were stuck doing these by hand
117:55 - you'd still be doing the first one
118:02 - stamen leaf plot is an old time
118:04 - histogram
118:06 - that's also for quantitative data
118:13 - there's a lot of steps to it
118:16 - again it takes a long time to do it
118:19 - you're going to want to do one of these
118:21 - at least once but the reality is once
118:23 - you can do a histogram there's no reason
118:25 - to do a stem and leaf plot
118:30 - so here we've got the ACT scores
118:33 - the stem is going to be the tens place
118:35 - so this stem will be a one the leaf will
118:38 - be eight the stem is two the leaf is
118:40 - three this damage two the leaf is four
118:44 - and then we just collect on the stems
118:46 - and write out the leaves so
118:48 - we got 18 19 18 17 or HCT scores 23 24
118:54 - 27 26 22 27 29 Etc
118:57 - are the ACT scores
119:00 - it's better to do these in order by the
119:02 - way
119:03 - so
119:04 - the data values are 17 18 18 19 20 21 22
119:09 - 23 24 24 25
119:11 - Etc
119:13 - if you want
119:15 - and r
119:18 - function is stem
119:26 - and here's your stem and leaf plot
119:27 - here's the key decimal point is two to
119:31 - the right of it so
119:34 - this is going to be zero two decimal 2
119:38 - to the right of this is a decimal place
119:40 - so that'll be 70
119:43 - it'll be 130
119:45 - 130 140 160 160 170.
119:49 - because these are by twos
119:53 - 260 280 280 280 300 300 310 330 340 350
119:59 - 390 430 430 Etc this is 24.60 this is
120:04 - 1240 this is 10.50 1080.
120:13 - line graphs the last graph for the day
120:15 - is use the data our measurements over
120:17 - time
120:19 - it's just a scatter plot connected the
120:22 - dots where the horizontal axis is time
120:25 - the vertical axis is our variable of
120:27 - interest
120:29 - so here's the example with the Consumer
120:31 - Price Index CPI is a measure of the
120:33 - average change in the value over time
120:35 - for a quote basket of goods and services
120:37 - for a typical American
120:41 - it's an index calculated by the Bureau
120:43 - of Labor and statistics table below
120:44 - shows the actual values of the cpia
120:47 - from several years from 1920 to 2010
120:50 - actually every decade
120:52 - so in 1920 the CPI was 20.
120:56 - so it costs 20 bucks for that basket of
120:58 - goods in the year 2000 it costs
121:02 - 172.20 for that same basket of goods
121:05 - technically not the same basket
121:07 - something items were switched out and
121:09 - put in over time but they were
121:11 - equivalent when they were switched out
121:13 - and replaced
121:15 - here's how here's the line graph notice
121:18 - the horizontal axis is the year
121:21 - vertical axis is the CPI
121:25 - and you can look it was pretty
121:27 - consistent
121:28 - until 1970 and then the price increased
121:32 - dramatically over the next 40 years
121:36 - let's see how to do this in r
121:43 - um first we got to specify the variable
121:45 - values this was the years happened to be
121:49 - a sequence from 1920 to 2010 every 10
121:53 - years and the CPI itself is equal to C
121:59 - is a function that collects several
122:01 - values into one vector I'm going to be
122:04 - switching back and forth a lot 20 16.7
122:11 - 14 24.1
122:16 - 29.6
122:23 - 82.4
122:26 - 130.7
122:30 - 2
122:32 - 0.2
122:34 - finally 218 point
122:38 - 18.1
122:41 - those are the variables we got
122:44 - run them
122:45 - notice there's no errors over here and
122:48 - then we just do a plot
122:49 - X variable comes first that would be the
122:51 - year in this case the CPA the Y variable
122:54 - comes next which is a CPI
122:56 - and then this has to be a line and Dot
123:00 - [Applause]
123:02 - got lines and dots one way of doing that
123:05 - is to specify type is equal to lowercase
123:07 - b
123:09 - and there's our line graph
123:20 - x-axis or X variable y variable and to
123:23 - get both lines and dots you would do
123:25 - type equals B
123:30 - if you want lines that will be a
123:32 - lowercase l that is a lowercase l
123:36 - I notice the dots go away
123:39 - [Applause]
123:41 - but you can add them back with the
123:43 - points command
123:49 - now the dots are back
123:53 - you're going to learn how to make this
123:55 - graph look much more appealing
123:57 - but that is for a different slideshow
124:02 - so here's the summary for qualitative
124:04 - data that is for categorical data we've
124:06 - got pie charts and bar graphs
124:09 - we've got the Pareto chart if the data
124:10 - are nominal
124:12 - side by side bar and stacked bar when
124:15 - you've got two categorical variables
124:16 - that you're looking at
124:20 - for quantitative data you've got the
124:21 - histogram you've got the stamen leaf
124:24 - plot which if you turn your head
124:25 - sideways looks an awful lot like that
124:27 - histogram
124:28 - and then we've got the line graph if the
124:31 - horizontal axis or the X variable is
124:33 - time
124:36 - now we got our red star it's pretty good
124:38 - because that's the last page here
124:40 - let's see how bad I've messed it up aha
124:43 - we got it back
124:45 - so question number two and again
124:47 - I would recommend writing the question
124:49 - and your answer on the left hand side of
124:51 - your notes so question two which
124:53 - Graphics can only be used for a single
124:55 - categorical variable
124:57 - key is the word single and the word
124:59 - categorical
125:01 - go ahead and hit pause
125:04 - question number three which Graphics can
125:07 - only use for a single numeric variable
125:09 - single numeric
125:12 - and both question two and question three
125:14 - have multiple answers
125:16 - don't forget to submit these into Moodle
125:22 - and that's it
125:25 - I hope this was helpful
125:29 - take care
125:30 - hello and welcome to section 2.3
125:33 - an additional section on analyzing
125:35 - crafts this is not computational in
125:38 - structure this is more of an
125:40 - interpretation section and a better
125:43 - understanding of what makes a good graph
125:45 - and in many ways more importantly what
125:48 - makes a bad graph
125:49 - so the objective is to identify
125:51 - misleading characteristics of a graph
125:53 - with the hope that you will avoid them
125:56 - yourself
125:59 - there we go so title access Source this
126:02 - is how to properly label a graph the
126:05 - middle part really is the only thing
126:07 - done in your statistical program the
126:10 - title and the source and the caption and
126:12 - the numbering is done in your word
126:14 - processing or typesetting program
126:17 - so the bar part this middle part is all
126:20 - that's done in your statistical program
126:22 - the title and the source stuff is done
126:25 - in your word processor
126:29 - um
126:31 - a Time series graph is a line graph that
126:34 - is used to display a variable whose
126:35 - values change over time we've seen this
126:38 - already we called it a line graph in the
126:40 - last section
126:43 - um
126:43 - this should ring a bell as something
126:46 - like panel
126:48 - data
126:50 - from
126:51 - chapter one it should kind of should be
126:55 - able to go back over your notes press
126:57 - pause go back over your notes and see
127:00 - what panel data was
127:02 - this is a way of describing panel data
127:05 - for one or a very small number of people
127:08 - a cross-sectional graph is a way of
127:11 - displaying information collected at only
127:13 - one point in time again that should
127:15 - remind us of something we talked about
127:16 - in chapter one about cross-sectional
127:18 - data
127:20 - a pictograph is a bar graph that uses
127:23 - pictures of objects instead of bars
127:26 - they look really spiffy but they're
127:29 - really dangerous so let's go ahead and
127:31 - we got a red star so let's go to our
127:33 - first question
127:35 - have you picked a graphs differ from bar
127:37 - charts
127:39 - so again over on your left side of your
127:41 - notes right how do you pick the graphs
127:43 - differ from bar charts answer that hit
127:46 - pause right now
127:48 - and you're back so let's move on
127:52 - so here's an example of how pictographs
127:55 - can be deceiving
127:57 - the one that you're going to see in the
127:58 - newspapers or the one on the right and
128:01 - the one on the left is the quote correct
128:03 - monthly housing
128:05 - there are three at least three things
128:08 - wrong with the the incorrect pictograph
128:11 - for one I'm going to draw your attention
128:13 - to the axis values
128:17 - and compare here
128:19 - two I'm going to say
128:22 - what is the area of the 94 versus the
128:26 - area of the 2006 House on the Left
128:29 - well the 94 I'm sorry the 2006 is double
128:32 - the 94. which corresponds to it actually
128:36 - increasing by double
128:39 - hint it's a ratio level variable so we
128:42 - can meaningfully talk about doubling and
128:45 - having
128:47 - whereas in the incorrect going from 94
128:50 - to 2006 it doesn't double in size it
128:54 - quadruples in size because not only are
128:57 - you doubling the height but you're
128:58 - doubling the width to keep the right
129:00 - form so our eyes are saying oh my
129:02 - goodness it increased by a factor of
129:04 - four
129:05 - even though we try to focus on the scale
129:08 - on the left and it only increases by a
129:10 - factor of two our mind our our gut
129:13 - reaction to this is it increases by
129:15 - factor four
129:17 - so that's another way that this
129:19 - particular picture graph is deceiving
129:21 - I've given you two
129:27 - question number two what are three
129:30 - things misleading about this particular
129:32 - pictograph
129:33 - and I would recommend that again you
129:36 - right over on the left this question
129:38 - your answer also pause and go back and
129:40 - look at the that particular pictograph
129:43 - see if you can figure out another thing
129:45 - that's misleading about it
129:50 - scaling of graphs
129:52 - another important feature to look out
129:54 - for is that the graph is scaled properly
129:57 - if you stretch your strength shrink the
129:59 - scale on the y-axis the shape of the
130:01 - graph may change dramatically
130:03 - a line that raises gently on one scale
130:05 - may look very steep with a different
130:07 - scale
130:08 - make sure that you choose the correct
130:10 - scale and by correct I mean the scale
130:13 - that forces the graphic to show you
130:17 - this the story of the data
130:23 - in other words make sure that it
130:25 - represents the data well
130:27 - you don't want you
130:30 - to force the data to tell a story your
130:34 - graphic should allow the data to tell
130:36 - its story
130:38 - takes practice
130:41 - so here's an example
130:44 - looking at this quickly
130:46 - really doesn't it doesn't seem like
130:48 - there's anything wrong here consider the
130:50 - graph below the U.S federal minimum wage
130:52 - hour rates unadjusted for inflation what
130:54 - errors can you find in the graph how
130:56 - should they be fixed
130:59 - so the dates along the x-axis so this is
131:02 - a Time series plot
131:04 - also known from last time is a line
131:06 - graph
131:07 - vertical axis is the minimum hourly wage
131:10 - in dollars
131:15 - so as you over time things increase in
131:18 - terms of dollars and it looks pretty
131:20 - awesome because you're minimum hourly
131:22 - wage is increasing over time
131:26 - so what's wrong with this
131:30 - well
131:32 - the fact that in 1956 the dollar doesn't
131:35 - quite doesn't buy quite as much as a
131:37 - dollar did in 19 in 2008.
131:42 - so perhaps we should instead of
131:44 - unadjusting it we should adjust for
131:46 - inflation
131:50 - notice the x-axis does not have a
131:52 - consistent scale the ears are as few as
131:54 - one apart I didn't even notice that my
131:57 - goodness there's one A Part here there's
131:59 - six apart here there's five of Parts six
132:03 - five two four Etc so you should stretch
132:08 - this shrink this so that each
132:10 - inch along the horizontal axis
132:12 - corresponds to a single
132:15 - uh
132:16 - time limit
132:18 - so maybe each of these marks should be
132:21 - three years instead of
132:23 - something
132:27 - so change the x axis correct graph can
132:30 - be found in exercise 12 and the chapter
132:31 - 2 exercises
132:33 - but that's not the only thing maybe you
132:35 - should adjust this for inflation
132:42 - now we're going to talk about some
132:43 - shapes of graphs
132:44 - basic four basic shapes or uniform
132:46 - symmetric right skewed and left skewed
132:49 - the right skewed I will frequently
132:51 - called positively skewed and skewed to
132:54 - the leftover frequently called
132:56 - negatively skewed simply because I have
132:59 - trouble with left and right
133:01 - the the name outliers the data value
133:04 - that falls outside the quote normal
133:06 - shape of the graph or the typical shape
133:08 - of the graph we've already seen outliers
133:10 - back in section 2-2 the District of
133:12 - Columbia was an outlier in terms of the
133:14 - of the violent crime rate in 1990
133:17 - its little bar in the histogram is far
133:20 - away from the rest of them
133:23 - so here's what a uniform distribution
133:25 - looks like the frequency of each class
133:27 - is relatively the same
133:30 - if you're dealing with sample data
133:33 - it's unlikely that you'll get bars all
133:35 - the exact same height even if the
133:38 - population has a uniform distribution to
133:41 - it
133:44 - symmetrical
133:47 - data lie evenly in both sides of the
133:49 - distribution
133:51 - whatever that means
133:52 - I want to talk about the skewed two
133:55 - skews then come back to the symmetrical
133:56 - that might make this a little bit more
133:59 - meaningful
134:02 - skewed to the right or positively skewed
134:05 - the key for skew is to locate the tail
134:09 - the tail is the side where the data just
134:12 - keeps lingering on and on as opposed to
134:14 - the in this case as opposed to the left
134:16 - side where it just kind of stops quickly
134:18 - the right side just keeps going on and
134:20 - on so there's a right tail here
134:23 - so this is skewed to the right or skewed
134:25 - positively
134:27 - skewed right because the tail is on the
134:29 - right skewed positively because the tail
134:31 - is on the positive side of the middle
134:35 - contrast that was skewed to the left
134:36 - again the data just keeps going on and
134:40 - on and on it stops rather quickly here
134:42 - so this is left skewed because the tail
134:45 - is on the left or it's negatively skewed
134:47 - because it's on the negative side of the
134:49 - middle
134:51 - with that said symmetric data have
134:54 - neither a right skew nor a left skewed
134:58 - notice here it kind of stops at about
135:00 - the same distance it it's not perfectly
135:03 - symmetrical in the mathematical sense
135:06 - the key and here's the key that I tell
135:08 - people it's in order to if you want to
135:11 - conclude that the data seems symmetrical
135:14 - it's
135:15 - try to determine which side gives you
135:17 - the tail
135:18 - if you have to stop and think a little
135:20 - bit about it like at least a second then
135:23 - it's symmetrical enough
135:26 - we're actually going to in the next
135:28 - chapter come up with a definition of
135:30 - symmetrical and the way determining if
135:32 - the data are quote symmetrical enough
135:34 - but for now just look and see okay are
135:37 - the data obviously skewed right or
135:41 - obviously skewed left and if the answer
135:43 - to both of those is no then it's
135:45 - symmetrical enough
135:49 - so let's describe the overall shape of
135:51 - this distribution
135:54 - the middle seems to be somewhere around
135:55 - here
135:57 - it's
135:59 - doesn't seem to have a tail on either
136:02 - side I mean if I squint it hard I could
136:05 - probably figure out a tail but
136:07 - because I'm not seeing the obvious tail
136:09 - I'm going to say this is symmetric
136:12 - the average is somewhere around 80.
136:16 - I don't see any outliers
136:18 - if there is one bar way way over here
136:22 - I'd say yeah there's an outlier if
136:23 - there's a bar way way over here I'd say
136:25 - yeah there's an outlier
136:29 - um so there's the smooth curve that
136:32 - they're talking about the average seems
136:33 - to be about 80. yeah symmetric no
136:36 - outliers
136:39 - red star
136:43 - so the third one
136:45 - how does one determine the tail of a
136:47 - distribution of a graphic
136:49 - go ahead and hit pause
136:52 - hopefully your answer is written in your
136:54 - notes so that you can later transfer
136:56 - them to the Moodle quiz
137:01 - and that's the end of this rather short
137:03 - chapter or a short section it actually
137:07 - is the end of this chapter
137:09 - so make sure you keep sending me those
137:12 - questions post those questions to the
137:14 - discussion board and I will talk to you
137:17 - later
137:18 - take care
137:21 - hello and welcome to section 3.1 in your
137:23 - Hawks book this is going to talk about
137:26 - measures of center
137:28 - a measure of center is a single
137:31 - statistic that tries to summarize your
137:34 - entire data set so it takes your data
137:36 - set boils it down to one single value
137:42 - um next lecture we're going to talk
137:43 - about how to determine the quality of
137:45 - that single value as a representation of
137:47 - your entire data set but today we're
137:50 - just going to look at how to summarize
137:51 - your data set with one single value
137:54 - and as you can imagine that's going to
137:57 - depend upon what your data actually are
138:00 - what level
138:02 - your data are
138:03 - so topics today calculate the mean
138:06 - median mode determine the most
138:07 - appropriate measure of center so we're
138:09 - moving on to the mean
138:10 - then clearly the median then eventually
138:12 - the mode definition the mean is the
138:14 - arithmetic mean of a variable is one of
138:17 - the most important statistics calculated
138:19 - and provided because it is one of the
138:21 - most important statistics calculated and
138:23 - provided
138:26 - in other words there's nothing special
138:27 - about the mean above and beyond the
138:30 - median of the mode except for four
138:33 - things that I could come up with I mean
138:35 - I racked my brain and these are the four
138:38 - reasons that we should be looking at the
138:40 - mean
138:42 - Carl Friedrich Gauss thought it was
138:45 - important
138:47 - the sample mean is easily calculated
138:50 - at least it's more easily calculated
138:52 - than the other two measures of center
138:56 - um the sample mean can be used to
138:57 - calculate missing data values that is
139:00 - if you have 10 data values
139:03 - you're missing one of those 10 but you
139:05 - have the mean you can figure out what
139:07 - that missing value is I think that might
139:10 - be helpful
139:12 - probably not
139:14 - and the sample mean has a nice
139:16 - distribution which we're going to learn
139:18 - in chapter 7 when we talk about the
139:20 - central limit theorem I think this last
139:22 - one is the best reason of all
139:26 - other than the fact that it's important
139:28 - because we've determined it's important
139:32 - you'll hear this talk again when we get
139:35 - to What's called the normal distribution
139:37 - we'll discover that the normal
139:39 - distribution is entirely the most
139:41 - important distribution in all of
139:43 - Statistics because it's the most
139:45 - important distribution in all of
139:47 - Statistics anyway moving on here's how
139:51 - you'd calculate it is the sample mean
139:56 - X is just going to be a generic variable
139:58 - the bar is what tells us it's the sample
140:01 - mean so X bar is going to be the sum of
140:03 - all the X values
140:06 - divided by the number of X values
140:08 - so this could be the first X Value Plus
140:11 - the second X Value Plus the Third x
140:13 - value dot plus the nth or the last x
140:16 - value
140:18 - divided by n
140:20 - in other words you add up all the data
140:21 - to divide by the number of data values
140:24 - this capital sigma
140:27 - and it's actually a stylized Sigma
140:29 - indicates the summation you're adding up
140:32 - everything to the right with a subscript
140:34 - of I so you're adding up all the i x
140:37 - values and dividing by the sample size
140:42 - contrast this with the population mean
140:45 - okay there's not much to contrast the
140:47 - sample mean deals with the sample
140:49 - population mean deals with the
140:51 - population
140:52 - the sample mean you've got an X bar
140:56 - population mean you've got a mu
140:59 - it's what a Greek cow says mu you're
141:03 - adding up all the values of the of the
141:06 - variable in the population divided by
141:09 - the population size
141:11 - for the sample mean you divide by the
141:13 - sample size for the population mean you
141:16 - divided by the population size again
141:18 - we're adding up all the X values divided
141:21 - by these population size
141:24 - nothing special
141:26 - so we got an example
141:28 - students from our previous stat toner
141:30 - classroom surveyed find out the average
141:31 - number of hours they sleep per night
141:33 - during the term here's a sample of their
141:35 - self-reported responses
141:37 - 568-10469 I want to calculate the mean
141:40 - all we do is add them up and divide by
141:42 - seven
141:44 - y seven because there's seven data
141:46 - points so we're going to add them all up
141:48 - and divide by seven
141:51 - by the way 5 is x sub 1 6 is that sub 2
141:56 - 8 is x sub 3 10 is x sub 4
142:00 - 4 is x sub 5 6 is x sub 6.
142:04 - 9 is x sub seven
142:06 - so x sub we're adding up all the X's
142:10 - where I ranges from one through seven
142:12 - and dividing by n
142:15 - 48 over 7 is about 6.9 so the sample
142:19 - mean for the number of hours that the
142:20 - students reported sleeping per night
142:21 - during the semester is 6.9 hours
142:26 - wow that's pretty lazy embed 6.9 hours
142:30 - every day on average come on
142:34 - I'm kidding by the way
142:36 - um so here's how we do it in R we Define
142:38 - a variable sleep
142:40 - we could call this a if we wanted to or
142:43 - X if we wanted to I'm calling it sleep
142:45 - because sleep has some meaning
142:47 - there's the C function it combines
142:48 - everything to the right into a single
142:50 - Vector of values and then to calculate
142:53 - the mean of sleep we just calculate mean
142:55 - of parentheses sleep
142:58 - that's it
143:01 - for a small data set
143:04 - like this
143:06 - whether or not it's easier to calculate
143:08 - it by hand or using R that's up to you
143:12 - but the reality is most data sets that
143:14 - we care about are going to be of size 2
143:17 - 3 4 000 Maybe
143:19 - two three four million
143:22 - and we'll want the computer to do those
143:24 - calculations for us
143:27 - example 3.2
143:29 - I alluded to this earlier let's go ahead
143:31 - and do it we're going to use the mean to
143:33 - find a missing data value
143:35 - Rutherford who is a famous physicist
143:39 - downloaded five new songs from the
143:41 - internet he knows that on average the
143:45 - songs cost a buck 23. if four of the
143:48 - songs cost about 29 each was the price
143:50 - of the fifth song he downloaded
143:52 - in other words we're missing that the
143:55 - the price of the fifth song but we got
143:58 - the mean
143:59 - and we got the value of all the others
144:01 - let's see if we can determine or
144:03 - bring back that last price
144:08 - so here's the solution
144:11 - we're just going to substitute in the
144:13 - values we know
144:15 - here's the function for the population
144:16 - mean times
144:20 - substituting the values we know Buck 23
144:22 - was the average of those five values
144:26 - four of those values are a buck 29 we
144:28 - don't know the fifth
144:31 - doing some algebra
144:33 - we come up with
144:36 - that missing song was priced at 99 cents
144:41 - so the cost of Rutherford's fifth song
144:43 - was just 99 cents
144:48 - now this is one of the strengths and
144:50 - where the weaknesses to the sample mean
144:53 - or to the mean in general
144:56 - it's the same thing it equally depends
144:59 - on every single data point
145:02 - that's an observation that's a feature
145:05 - of the formula which could be a strength
145:08 - or weakness
145:09 - here is the strength because we're able
145:11 - to
145:13 - recover a missing value
145:16 - however if that missing value were a an
145:20 - outlier it the mean may not be
145:23 - representative of the data set as a
145:25 - whole
145:28 - sometimes we don't have the actual data
145:31 - but we have summarized data this would
145:34 - lead to us maybe perhaps wanting to
145:36 - calculate a weighted mean or the mean of
145:38 - the summarized data the weighted mean
145:42 - again this will be for the sample
145:43 - there's a bar on top of the variables so
145:45 - this this will be an X bar or a sample
145:48 - mean
145:49 - these are summations
145:52 - the W's represent the weights
145:54 - and the X's represent the observed
145:56 - values
145:58 - so the weighted mean is just going to be
146:00 - the sum of the values times the weights
146:04 - added up divided by the sum of the
146:07 - weights
146:10 - where might this be most important for
146:12 - you
146:12 - a lot of your classes are going to use
146:15 - weighted means for determining your
146:16 - final grade
146:17 - here's an example the syllabus my
146:19 - discrete mathematics class states that
146:21 - the final grade is determined by the
146:23 - midterm homeworks the discussions and
146:26 - the final grade
146:27 - sorry final exam
146:30 - midterm counts 40 towards the final
146:32 - homework 20 discussions 10 final exam 30
146:35 - towards the final grade
146:38 - so we're waiting the midterm grade by
146:41 - the term exam grade by 40 percent we're
146:44 - awaiting the homework grade by 20
146:46 - discussions by 10 final exam by 30
146:48 - percent
146:50 - so there's two students in the class Bob
146:52 - and Virginia want to calculate their
146:54 - final grades or in this specific
146:56 - instance estimate their final grades
146:59 - below are their average grades in each
147:01 - of the categories for the midterm
147:03 - homework discussions and they've also
147:05 - guessed at what they might score in the
147:07 - final exam
147:08 - so midterm homework and discussions they
147:10 - know their grades the final they're
147:12 - going to estimate
147:14 - here it is for Bob
147:16 - he got an 83 in the midterm and 98 on
147:19 - the homework discussions you got 90
147:20 - percent fantastic he thinks he's going
147:22 - to get an 87 on the final we'll see but
147:25 - he thinks he will
147:27 - if these numbers are correct then what
147:29 - is his final grade for the course
147:33 - this is for genius we'll come back to
147:35 - that later notice that Virginia did much
147:38 - worse than the homework and discussions
147:39 - she's going to think she's going to do
147:41 - much better in the tests
147:45 - so here it is for Bob
147:48 - first thing we need to do is determine
147:51 - which numbers are the values and which
147:53 - are the weights
147:56 - um the grade earned in each category is
147:58 - weighted by the percentage for that
147:59 - category
148:01 - so for instance Bob's test of 83 gets
148:04 - away to 40 percent
148:06 - so that 40 is going to be the weights
148:08 - and the 83 is going to be the values
148:12 - here's a nice little table
148:14 - so these are the scores that Bob got
148:17 - according to the syllabus these are the
148:19 - weights for each of those categories
148:21 - and technically the 87 he hasn't
148:23 - received yet
148:25 - he thinks he's going to get it
148:27 - he got an 83 on the midterm so a good
148:29 - chance of getting 87 at least on the
148:32 - final
148:35 - so to calculate the final grade down
148:37 - here we're just going to multiply the
148:38 - score by the weight
148:40 - we call that the contribution to the
148:42 - final score
148:44 - times weight score times weight Square
148:46 - Times weight add up this last column to
148:47 - get his final grade
148:50 - 83 times 40 percent is point is 33.2 98
148:55 - times 20 is 19.6
148:58 - and that should add up to 87.9 so if Bob
149:01 - is correct and he gets an 87 on the
149:04 - final
149:05 - he'll get an
149:06 - 80.87.9 percent for the course
149:10 - which is a B plus
149:12 - if we look at this in terms of
149:14 - mathematics
149:16 - we're adding up all the X's times the
149:18 - W's
149:19 - divided by the W's notice that the W's
149:22 - all add up to 1.4 plus 0.2 plus 0.1 plus
149:25 - 0.3
149:28 - this top is just each of those values in
149:31 - the last column adding them up gives us
149:34 - an 87.9
149:39 - there it is for Bob let's look at
149:41 - Virginia
149:43 - the X's are the genius grade in each
149:45 - category we got the the weights let's do
149:47 - this using r
149:50 - so the weights are 40 20 10 30.
149:54 - Virginia's actual grades are for 95 45
149:57 - 66 and she thinks she's getting a 90 on
150:00 - the final
150:01 - she got a 95 on the midterm there's a
150:04 - good chance she'll get a 90 on the final
150:07 - this formula
150:09 - and R corresponds the actual definition
150:12 - of the weighted mean
150:15 - so the sum
150:16 - of the values times the weights divided
150:20 - by the sum of the weights
150:25 - the sum of the values times the weights
150:28 - divided by the sum of the weights
150:33 - from this
150:35 - we decided we find out she Lambert
150:37 - within 80.6 for her final grade
150:42 - that is after we run it this line gives
150:44 - us 80.6 which is a B minus
150:48 - not bad considering she did so poorly on
150:50 - everything but the tests
150:55 - now as an aside from a teaching
150:58 - standpoint let's take a moment and
151:00 - consider the effect on her grade if her
151:04 - test score was low but her homework and
151:06 - quiz scores were high
151:08 - which should come out with the same
151:10 - grade
151:11 - in other words what lessons can you draw
151:13 - from this with respect to your own
151:15 - activity in the course
151:18 - hint hint
151:20 - hints
151:23 - and now extension here are some
151:25 - additional questions that we can answer
151:26 - for Virginia what's the highest grade
151:28 - she can earn what's the lowest grade she
151:31 - can earn and what does she need on her
151:33 - final to earn a 70 percent in the course
151:35 - which is the lowest passing grade
151:39 - well for the highest grade she can earn
151:41 - it occurs when she gets 100 on the final
151:45 - so you put a hundred percent in here you
151:47 - do these calculations and the highest
151:49 - that you can get is a b
151:51 - a low B but a b nonetheless
151:55 - the lowest grade she can earn happens
151:57 - when you put a zero in for the final
151:58 - grade and discover that lowest grade is
152:01 - a 53.6 percent
152:04 - which is an F
152:07 - now the likelihood she gets a zero on
152:09 - the final is is pretty low considering
152:12 - she got a 95 on the midterm
152:14 - so the next question is what does she
152:16 - need to earn on the final in order to
152:18 - pass the class
152:21 - 54.7 is the correct answer
152:24 - how did I determine that well I just
152:26 - kept changing this last number and
152:29 - re-running this code until I got this
152:32 - last quantity to be 70 percent
152:39 - we got a red star which means we need to
152:41 - go to our intra lecture question
152:45 - one
152:47 - what is the largest number of means that
152:51 - a variable can have
152:53 - again
152:54 - write the question on the left hand side
152:56 - of your notes answer it below so you can
152:58 - put it put it into Moodle later and
153:01 - pause and we're back
153:06 - moving on to the median here's the
153:09 - definition of the median the gut
153:10 - definition first the median is the quote
153:13 - middle value in the sense that about
153:15 - half of the data is above it and about
153:17 - half is below it
153:19 - yeah that's a nice gut definition
153:23 - doesn't work mathematically because the
153:25 - word about
153:26 - so mathematically here's the actual
153:29 - definition of median
153:30 - uh the median of a set of data is a
153:33 - value notice it's a value not V value so
153:37 - it could be more than one median in a
153:39 - data set
153:40 - a value such that at least half of the
153:43 - data is at least this value
153:46 - greater than or equal to
153:47 - and at least half the data is at most
153:51 - this value less than or equal to
153:57 - kind of complicated definition
154:02 - so we go back to the gut definition of
154:05 - okay it's it's the middle value about
154:07 - half's above and about halfs below
154:09 - if we need to calculate it this is the
154:12 - mathematical definition
154:14 - if we want to understand what it
154:15 - actually means we use the get definition
154:20 - so here's how you actually calculate the
154:22 - median of a data set by hand you list
154:24 - the data in ascending order
154:27 - from lowest to highest
154:29 - in other words you're making an ordered
154:31 - array
154:32 - if the sample size is odd the median
154:34 - submittal value in that ordered array if
154:36 - it's even that it's the mean of the two
154:39 - middle values in that ordered array
154:43 - note that this implies at least two
154:46 - things one
154:47 - the median like the mean may not be a
154:50 - value in the data set
154:53 - and two the medium may only be applied
154:56 - to ordinal data if n is odd or if
154:59 - there's if the middle two values are the
155:03 - same
155:04 - level
155:09 - so let's calculate the median
155:11 - given the number of absences for samples
155:14 - of students in two different classes
155:15 - find the median for each sample first
155:18 - step is order one two three four five
155:21 - six seven there's seven data points here
155:22 - so it's going to be the middle data
155:24 - point after we order the data and for B
155:27 - there's eight values so it's going to be
155:29 - the middle of the the arithmetic mean of
155:31 - the two
155:34 - so we put it in order
155:36 - it's going to be six
155:38 - is the median
155:40 - we put an order seven and eight are the
155:43 - two middle values the mean of seven and
155:45 - eight to seven point five
155:47 - that's the median
155:51 - again the median like the mean does not
155:54 - have to be a possible value in the
155:55 - variable
155:58 - here's how we do it in R the function is
156:01 - median and all you have to do is give it
156:04 - the data
156:08 - not too hard
156:10 - ooh red star so let's move on to intra
156:14 - lecture question number two
156:18 - what is the largest number of medians
156:20 - that a variable can have
156:23 - again write it to the left in your notes
156:25 - answer below so that later you can
156:28 - transpose it into Moodle
156:31 - pause
156:32 - and back we are
156:36 - third one is mode the mode is the most
156:39 - observed value if you're dealing with
156:41 - data it's the most observed value if
156:43 - you're dealing with the population
156:45 - it's the most observed value
156:49 - here's how we calculate by hand
156:51 - you just do a frequency distribution
156:54 - remember that from chapter two a
156:57 - frequency distribution the data and find
156:59 - the value that occurs most frequently
157:02 - some terminology if there is only one
157:05 - value that occurs most often it's called
157:07 - unimodal Data
157:09 - where the variable is termed unimodal
157:13 - if exactly two values occur equally
157:15 - option it's bimodal data or the variable
157:19 - is bimodal
157:20 - if it's more than two values that occur
157:22 - equally often it's multimodal
157:24 - however
157:27 - if the data values only occur once or an
157:29 - equal number of times each
157:31 - we say there is no mode
157:36 - so here's some examples
157:38 - finding the mode for the first one I see
157:40 - two sixes but I see three sevens so
157:43 - seven is the mode of a and a is unimodal
157:47 - because there's only one mode
157:49 - for B I don't see anything occurring
157:52 - more than once so there is no mode
157:57 - C I see seven and two both occurring
158:01 - twice
158:02 - so the modes are seven and two and it is
158:06 - bimodal
158:08 - and for D everything occurs twice so
158:11 - there is no mode
158:15 - which is what these Solutions say
158:22 - doing this in R note that the mode is
158:25 - not really a helpful measure of center
158:27 - for most data that we deal with
158:29 - therefore most
158:31 - statistical programs don't have a
158:34 - function for the mode
158:36 - I've created one for you it's called
158:39 - modal that's the function modal and it
158:42 - just takes the data
158:43 - however you need to Source this file
158:46 - before modal is an appropriate function
158:51 - so run these two lines and you'll get
158:53 - the mode of this data set which should
158:55 - be seven
158:59 - will be seven that should be
159:03 - in fact I would recommend that this line
159:05 - would be one of the first that you run
159:07 - in every script for this course
159:09 - and you'll see it more and more
159:11 - frequently
159:14 - example three seven let's bring it all
159:16 - together look at the mean the median the
159:18 - mode for the data here's the data
159:21 - eight data points
159:23 - here we've calculated in R can we load
159:27 - in this data I'm sorry we load in this
159:30 - functionality
159:31 - here's the data
159:33 - the mean the median and the modal of
159:35 - that of that variable
159:39 - remember that the pound sign or the
159:42 - hashtag or the octathorpe or whatever
159:44 - you want to call this indicates a
159:46 - comment R ignores everything to the
159:48 - right of it
159:50 - so this is just a comment to remind me
159:53 - what the mean age is what the median age
159:56 - is and what the modal age is
160:01 - notice the mean is far away from the
160:03 - median and the mode
160:05 - it's probably due to this outlier
160:09 - everything else seems to be really close
160:10 - to 80 except for this 42 year old who
160:14 - retired
160:16 - we can see this on a graph
160:19 - horizontal graph this is actually a Dot
160:21 - Plot we've got our outlier at 42 and
160:25 - it's an outlier because all the rest of
160:27 - the data is so far away from it
160:31 - the mode happens here at 80 only because
160:34 - two people
160:36 - retired at age 80
160:39 - the median is going to be pretty stable
160:40 - it's going to be really close to the
160:42 - middle of the data set
160:44 - and the mean is going to be really
160:46 - influenced by this outlier
160:52 - which brings us to a question of we've
160:54 - got three measures of center which one
160:56 - should we use
160:58 - for nominal data the mode should be used
161:02 - for ordinal data
161:04 - the mode should be used if it has to be
161:07 - the median should be used if it can be
161:10 - when can it be it's when the either the
161:14 - sample size is odd
161:17 - or the sample size is even and the two
161:20 - middle values are the same
161:23 - in other words we can use the the median
161:28 - for ordinal data when there's no need to
161:31 - actually take an average
161:33 - because average requires
161:36 - at least interval level data
161:40 - for numeric data the median should be
161:42 - used
161:44 - however if the numeric data is
161:46 - sufficiently symmetric the mean can be
161:48 - used
161:50 - and we like to use the mean because
161:52 - mathematically it behaves much more
161:54 - nicely than the median
161:57 - and really that is the only reason we
162:00 - want to use the mean
162:04 - now how do we determine if some data set
162:06 - is sufficiently symmetric we use What's
162:08 - called the Hildebrand rule
162:12 - the Hildebrand ratio is defined as the
162:15 - difference between the mean sample mean
162:17 - and the sample median
162:19 - divided by the sample standard deviation
162:21 - and you'll talk about the standard
162:23 - deviation next lecture
162:27 - if the size of the ratio is less than 20
162:31 - less than 0.20 and the data are
162:34 - sufficiently submits
162:35 - sufficiently symmetric
162:37 - if H itself is greater than 0.2 then it
162:41 - is positively skewed if H is less than
162:44 - negative 0.2 then it is negatively
162:47 - skewed
162:52 - the variable age is not sufficiently
162:55 - symmetric the ratio is negative 0.312
162:59 - we know that from running these three
163:01 - lines
163:03 - and actually if we have already run the
163:06 - first two lines in our R session we just
163:08 - need to run the Third
163:11 - since the data are not sufficiently
163:13 - symmetric we should not use the mean we
163:16 - should use the median
163:18 - as it is closer to most values AKA it's
163:21 - more typical of the data
163:23 - then is the mean
163:30 - at least that's what I'm supposed to
163:31 - tell you teaching introductory
163:33 - statistics the reality is you should
163:36 - give both values and interpret both
163:38 - values correctly
163:44 - sample 3 8 and that gives you data on
163:47 - these and just describing the variables
163:49 - which measure of center is appropriate
163:51 - T-shirt size is small medium large extra
163:54 - large well that's ordinal
163:57 - so at least mode
164:02 - if there's no incons no need to to take
164:07 - an average then the median would be
164:10 - appropriate
164:12 - salaries for a professional team of
164:13 - baseball players that would be median
164:15 - most likely
164:18 - um
164:18 - the star will be an outlier the rest of
164:22 - the players will tend to be at a little
164:24 - clump
164:25 - as it was back here the star will be
164:29 - not at the low end it'll be at the high
164:31 - end and everyone else will be clumped
164:33 - towards the middle
164:36 - so this would be a median
164:39 - see price of homes in a subdivision of
164:41 - similar homes since the homes are
164:43 - similar then everything's going to be
164:45 - clumped together so the mean would most
164:47 - likely be correct
164:49 - and Professor rankings from student
164:51 - evaluations of best average and worst
164:53 - that's ordinal so it's at least a mode
164:57 - hopefully we'd be able to use a median
165:00 - and we'd be able to use a meeting if we
165:02 - don't need to take a average of the
165:05 - middle two values
165:10 - oops so now we're looking at graphs
165:15 - a b and c a is the mode it is the most
165:19 - likely value
165:22 - B is going to be the median because
165:25 - about half of the shaded areas to the
165:27 - left and about half is to the right
165:31 - and C is going to be the mean notice
165:34 - that the mean is to the right of B
165:38 - because the data are right skewed
165:43 - if the data were left skewed then C
165:45 - would be to the left of B
165:50 - so here's the summary property is that
165:52 - the mean the median and the mode
165:56 - these are kind of important they just
165:58 - summarize what I've talked about for the
166:00 - last 20 minutes
166:02 - the mean is affected by outliers the
166:04 - median is not affected by outliers
166:08 - I mean there is a single value
166:11 - comedian there is usually a single value
166:14 - the way we calculate it
166:17 - but by the strict definition of the
166:20 - median it's a single value if n is odd
166:23 - and an infant number of values of n is
166:25 - even
166:26 - functions are mean median and modal
166:29 - remember you have to Source in that
166:32 - extra file in order to use modal
166:37 - so I guess that brings us to question
166:39 - three what is the largest number of
166:42 - modes that a variable can have
166:45 - again write this question on the left of
166:48 - your notes
166:49 - write your answer underneath of it so
166:51 - you can transfer that to the Moodle quiz
166:53 - and pause and we're back and we're done
167:00 - the key is not to be able to calculate
167:04 - these by hand
167:06 - you can get the computer to do that for
167:08 - you
167:08 - the key is much more difficult it's to
167:11 - be able to interpret these values
167:14 - what does it mean if the mean is 14 and
167:17 - the mode is 47.
167:19 - what does it mean if the mean is 14 and
167:22 - the median is 47.
167:24 - does it mean that the mean the median
167:26 - and mode are the same value
167:29 - what does it mean if the mean and the
167:31 - median are the same value and the mode
167:32 - the well there are two modes
167:37 - in the key here is to be able to
167:40 - interpret the values and you'll hear me
167:42 - say this over and over again as we move
167:44 - forward in this course the key is
167:47 - interpretation
167:48 - the computer can do the calculation for
167:50 - you you need to be able to interpret
167:52 - what the computer does
167:55 - and in many ways that's harder to do
168:01 - but it makes the calculations much more
168:05 - useful
168:07 - and so
168:09 - thank you and I will talk to you later
168:14 - hello and welcome to section 3.2
168:16 - measures of dispersion purpose of this
168:19 - section is clearly to determine ways of
168:21 - measuring the dispersion of a data set
168:24 - what do we mean by dispersion we mean
168:26 - how spread out the data are hence these
168:29 - are also called measures of spread we're
168:32 - going to look at four measures of spread
168:34 - in this lecture postponing the fifth one
168:37 - the interquartile range until the next
168:41 - um so here's the gut definition of a
168:44 - measure spread it's a measure of how
168:46 - much we can expect a value of the data
168:49 - to differ from the appropriate measure
168:52 - of center so because of that that means
168:55 - that we've got measures of spread for
168:57 - the mode we cut measures the spread for
168:59 - the median we've got measures of spread
169:01 - for the mean
169:03 - we've got lots of measures of spread all
169:05 - of them trying to Define this and trying
169:08 - to measure the same thing
169:10 - the spread of the data
169:12 - or we can also think of spread and
169:15 - dispersion as being the uncertainty in
169:17 - your data value
169:19 - that is if I've got a data data set with
169:21 - a very small level of dispersion you're
169:25 - much more certain about where that
169:26 - individual value is going to be than if
169:29 - you've got a very very large disperse
169:32 - largely dispersed data set where you got
169:35 - a value and you have no idea if it's to
169:37 - the left tail to the right tail close to
169:39 - the middle Million Miles Away
169:42 - note that there are many many measures
169:45 - of dispersion
169:46 - how one Define spread or dispersion and
169:49 - what properties one wants in such a
169:51 - measure determine the formula that is
169:53 - being used so be aware of that I see a
169:56 - red star so let's pop over to the
169:58 - intellecture question number one
170:01 - what is the main purpose of a measure of
170:03 - dispersion AKA a measure of spread again
170:07 - I recommend writing on the left hand
170:09 - side of your notebook the question your
170:11 - answer underneath of that so that when
170:13 - you do go into Moodle to finish this
170:15 - quiz you'll have the answers right there
170:17 - and of course hit pause
170:20 - and we're back
170:23 - the first measure of spread and the
170:25 - weakest one that we've got is just the
170:27 - range
170:28 - the range of your data is just the
170:30 - largest value minus the smallest value
170:33 - so the range is a single number it's not
170:36 - two numbers it's a single number so if
170:38 - we're looking at Heights of students in
170:40 - this stat 200 class the range is going
170:43 - to be 13 inches 13.2 inches actually
170:47 - so a single number
170:51 - the difference between the highest value
170:53 - and the lowest value
170:55 - that's it
170:57 - since it's based on two specific values
171:00 - the range is going to be very unstable
171:03 - that is if I send three people out to
171:06 - draw a sample from the same population
171:08 - you're going to get different samples by
171:10 - the way
171:11 - then the ranges have a very strong
171:13 - chance of varying quite drastically
171:16 - between the two samples that's what I
171:18 - mean by unstable or it's not robust
171:23 - a much better measure of dispersion is
171:26 - called the variance
171:29 - this one is for the population variance
171:33 - the population variance is the average
171:35 - squared distance of the population
171:38 - values from the mean
171:40 - so I've got my cursor so x i is a data
171:44 - value or a value in the population
171:46 - mu recall is the population mean capital
171:50 - N recall is the population size and
171:53 - we're going to call Sigma squared the
171:57 - population variance that will be the
171:59 - symbol for the population variance and
172:01 - what we're doing is just adding up every
172:03 - value in the population minus the mean
172:07 - we're going to square that
172:09 - add them all up and divide by the
172:12 - population size
172:16 - similar to the population variance is
172:18 - the sample variance this is the one
172:20 - that's actually going to be useful for
172:21 - us
172:22 - notice that the formula looks exactly
172:24 - the same except for two three parts
172:27 - let's call this three parts part one is
172:29 - the symbol for sample variances and S
172:32 - squared for the population variance it
172:34 - was Sigma squared recall that population
172:37 - parameters tend to be Greek letters and
172:40 - Sample parameters tend to be Latin
172:42 - letters so it's s squared for the sample
172:46 - variance Sigma squared for the
172:48 - population variance
172:49 - that's one difference the second
172:50 - difference is you're subtracting off X
172:52 - bar which is the sample mean
172:55 - for the population variance you're
172:57 - subtracting off mu the population mean
173:00 - here we're subtracting off X bar the
173:02 - sample mean
173:03 - and two instead of dividing by n we're
173:06 - dividing by n minus one and we'll
173:08 - explain why a little bit later
173:11 - so again it's each data value minus the
173:14 - mean of the data squared added up
173:17 - divided by m minus 1. and that will give
173:19 - you the sample variance
173:22 - so let's calculate some variances very
173:24 - small simple toy data set here so let's
173:27 - assume the data represent the actual
173:29 - weight changes for a sample
173:31 - of fitness club members so for a we're
173:33 - going to calculate the sample variance
173:36 - and for B assume that the data represent
173:38 - the actual weight changes of every
173:40 - member
173:41 - so for B we're going to calculate the
173:43 - population variance of these five values
173:46 - so again notice the numbers are
173:49 - meaningless unless you add context to
173:52 - them
173:53 - does three two five six four is that the
173:55 - sample or is that the population you
173:57 - need to know
173:59 - by the way in this class in less stated
174:01 - otherwise that's going to be a sample
174:03 - so let's do the calculations
174:06 - here's the function here's the formula
174:09 - for the sample variance
174:11 - we've got the values x sub I which are
174:14 - just the three two five six four
174:18 - X bar is going to be 4 because the
174:21 - sample mean of 32564 is 4.
174:25 - n is 5.
174:29 - which means the N minus 1 is going to be
174:31 - four
174:32 - so all we have to do is take each of
174:34 - those X values subtract off 4 Square it
174:37 - add them all together and then divide by
174:40 - 4.
174:42 - and that's what we're doing here data
174:44 - values
174:45 - the deviations which is the data value
174:47 - minus the sample mean
174:51 - notice if we add up the deviations we
174:53 - get zero which is a good thing
174:56 - the squared deviations the x i minus the
174:59 - X bars yeah the bars there somewhere
175:01 - squared notice we add these up we don't
175:04 - get zero
175:07 - and now if we add these up one four one
175:09 - four zero gives us ten
175:14 - and now all we have to do is divide by
175:15 - little n minus one
175:19 - 10 divided by 5 minus 1 is 2.5 so the
175:22 - sample variance is 2.5
175:25 - we can do this in r
175:28 - for example of size 5 you may not need
175:31 - to
175:32 - because it's very straightforward
175:34 - putting this table together and doing
175:36 - all the calculations
175:40 - but once we get into sample sizes of
175:42 - fifty hundred ten thousand
175:45 - you're going to want to be able to do
175:46 - this on a computer the function to
175:48 - calculate the sample variance is VAR VAR
175:51 - VAR for variance
175:55 - first line we Define a variable called
175:58 - weight
175:59 - we're going to put inside of weight five
176:02 - values a 3 a 2 a 5 a 6 and a 4. notice
176:06 - you've got the little C to combine all
176:09 - of these values into one variable
176:11 - and then we calculate the variance of
176:13 - weight
176:15 - that's it
176:18 - so now B remember was for a population
176:20 - variance
176:22 - we can do this the long way we would
176:26 - essentially just end up dividing by n
176:29 - five instead of n minus 1 or we can do
176:32 - this on a computer
176:34 - for a sample size 5 and not a big deal
176:37 - for a sample of size 5000 that is a big
176:40 - deal so again here's our data
176:43 - to calculate the population variance you
176:46 - could do the sample variance times four
176:47 - over five
176:49 - n minus 1 over n
176:51 - y n minus 1 over n well let's go back to
176:55 - the formulas
176:57 - there we go
176:59 - we want to calculate this number
177:02 - we've got this number notice the
177:04 - difference is the denominator
177:07 - so we're going to multiply this number
177:08 - the sample variance by n minus 1 here
177:12 - it'll cancel out the N minus ones n
177:15 - minus 1 over capital N so what you'll be
177:17 - left with is just the sum of the x i
177:19 - minus the mean squared over capital n
177:27 - so four over five
177:29 - if you're going to calculate population
177:31 - variance a lot you may want to create a
177:33 - function for it this is the first place
177:35 - that we see the extensibility of r
177:38 - will create a function called varpop the
177:41 - variance of the population I guess we're
177:43 - going to set it equal to this keyword
177:45 - called function
177:47 - we're going to give it just one variable
177:49 - X that'll be the data
177:51 - that's a brace an open brace and that's
177:54 - a closed brace down here
177:56 - and then the actual calculation is just
177:59 - x minus the mean of the x is squared
178:04 - and the average of those it's the
178:05 - average because you're dividing by the
178:07 - number capital n
178:09 - and you would use it as VAR pop of
178:11 - weights
178:14 - you would have to create this function
178:16 - in every script that you need the
178:19 - population variance in
178:22 - thing is we rarely have the entire
178:24 - population
178:26 - we tend to deal with just the sample
178:28 - populations hence the VAR is the sample
178:31 - variance
178:33 - so here's the big question where does
178:34 - the formula actually come from what does
178:36 - it tell us what does it do for us so we
178:38 - can think of variance as an average
178:41 - distance that the value is going to set
178:43 - Lie from the mean
178:45 - maybe they lay from the mean I think
178:47 - they lie from the mean
178:49 - so it's while it's not an actual average
178:51 - when we're thinking of the sample
178:52 - variance here and the variance is
178:55 - usually very close to the average and
178:56 - conceptually it's a good way to think
178:58 - about what this variance is actually
178:59 - calculating
179:01 - um so
179:03 - on your notes I'm sure you got the the
179:05 - variance formula there so to derive this
179:08 - formula for variance we're going to use
179:09 - a method similar to finding average
179:10 - distances or squared deviations from the
179:12 - mean
179:14 - so first we must know the actual
179:16 - deviation from the mean well that's just
179:18 - x i minus X bar that's a deviation
179:21 - that's how far that individual value is
179:23 - from the mean
179:26 - then we're going to square those
179:28 - to get a distance
179:30 - or technically a squared distance
179:33 - and then we're going to find the average
179:34 - of all those how do you find an average
179:36 - well you add them out divide by the
179:37 - sample size
179:40 - well we're not entirely dividing by the
179:43 - sample size we're dividing by the sample
179:45 - size minus one
179:47 - but essentially this is just the
179:49 - deviances squared so it's a squared
179:51 - distance
179:52 - and this part will just be an average
179:54 - squared distance from the data point to
179:57 - the mean
180:00 - so y n minus 1
180:03 - and this is pretty important the
180:04 - fundamental purpose of sample statistics
180:07 - is to estimate a population parameter
180:12 - so the N minus 1 is there so that s
180:15 - squared is a good estimator of Sigma
180:18 - squared
180:20 - now that's the reason for n minus 1. and
180:24 - then mathematics is the other reason for
180:27 - that
180:28 - um
180:29 - laboratory activity d d as in dog looks
180:34 - at estimators and what makes a good
180:37 - estimator and what do we mean by a good
180:39 - estimator
180:40 - and at that point you'll see oh yeah
180:41 - unbiased estimators are all things
180:45 - equal are good things
180:47 - and dividing by n minus 1 gives us an
180:50 - unbiased estimator of Sigma squared
180:57 - the standard deviation is closely
180:59 - related to the variance
181:01 - standard deviation is also a measure of
181:04 - how much we might expect a typical
181:05 - member of the data to differ from the
181:07 - mean
181:08 - in words definition seems very very
181:11 - similar in fact it's probably exactly
181:13 - the same I like to copy and paste things
181:15 - the only difference is the formula
181:19 - and what we mean by how much we might
181:22 - expect it to differ
181:26 - Sigma squared is the variance Sigma is
181:29 - the standard deviation those are for the
181:30 - population by the way so the population
181:33 - standard deviation is just the square
181:34 - root of the population variance
181:38 - and the sample standard deviation is
181:41 - equal to the square root of the sample
181:43 - variance
181:45 - so the big question comes up if the
181:48 - sample standard deviation the sample
181:50 - variance or the the population standard
181:52 - deviation and the population variance
181:54 - really tell us the same information why
181:56 - are we
181:57 - pardon me why are we introducing the
181:59 - standard deviation
182:01 - and the reason is the units of the
182:04 - standard deviation are going to be
182:05 - exactly the same as the units of your
182:07 - data so if your data are inches your
182:10 - standard deviation units are going to be
182:11 - inches if your data are years your
182:15 - standard deviation data standard
182:17 - deviation units will be years
182:19 - for the variance it'll be the square of
182:21 - that unit
182:23 - which is much more difficult to see
182:26 - on a histogram
182:29 - I can see things that are in the same
182:30 - units of the data because the histogram
182:33 - is in the units of the data but see in
182:35 - terms of the square that's much more
182:37 - difficult to see
182:40 - thus standard deviation is much more
182:42 - interpretable and should be used in all
182:44 - of your papers instead of the variance
182:48 - which brings up a related question of
182:50 - why do we cover the variance first
182:53 - the reason we cover the variance first
182:55 - is because it's much more
182:58 - intuitive as to what it's trying to
183:00 - measure
183:01 - the square rooting of the variance just
183:03 - brings the the units down into what
183:06 - we're used to
183:08 - also mathematicians love to use the
183:10 - variance because variances add standard
183:13 - deviations do not add
183:16 - so mathematicians like the variance
183:18 - everybody else on the face of this
183:21 - planet love the standard deviations
183:23 - ooh we got a red star so
183:26 - question two why should one report the
183:29 - standard deviation instead of the
183:31 - variance
183:32 - again to write the question on the left
183:34 - hand side answer it underneath go ahead
183:36 - and pause and we're back
183:41 - so let's calculate the standard
183:43 - deviation
183:44 - so let's find the stand sample standard
183:46 - deviation of the data shown below
183:49 - yeah
183:51 - I challenge you to do to do this by hand
183:56 - here it is an R
183:58 - just take the data
184:00 - wrap it in the C function send it to the
184:03 - variable
184:04 - since there's no context to what these
184:06 - numbers actually mean we'll just say x
184:08 - is equal to those values
184:11 - and then the function to calculate the
184:12 - sample standard deviation is just SD
184:16 - that's it
184:19 - if you need to calculate the population
184:21 - standard deviation where you're going to
184:23 - have to go back to the function you
184:24 - created
184:26 - varpop calculate the population variance
184:30 - and then take the square root of that
184:32 - sqrt is the square root function
184:36 - but again statisticians rarely calculate
184:40 - population parameters they estimate them
184:42 - but they estimate them with the sample
184:44 - statistics hence SD is the sample
184:48 - standard deviation and VAR VAR is the
184:52 - sample variance
184:55 - here's a use or I guess we're going to
184:58 - say it's an application of the standard
184:59 - deviation
185:01 - um Financial people love to use variance
185:04 - and standard deviation because they
185:06 - indicate risk of an investment
185:09 - so we're looking into investing a
185:10 - portion of recent bonus into the stock
185:12 - market I guess Mark's doing it we're not
185:14 - while researching different companies he
185:16 - discovers the following standard
185:17 - deviations of one year of daily stock
185:19 - closing prices
185:22 - the standard deviation is a dollar two
185:25 - yardsmith
185:27 - it's 9.67
185:30 - in other words the standard deviation of
185:32 - yardsmith is much larger than that of
185:34 - profacto so all things being equal there
185:38 - is much more variability in yardsmith
185:40 - than there is in profacto so if you want
185:42 - a nice stable
185:45 - stock investment you'll go with profacto
185:47 - instead of yard Smith
185:51 - which is what this solution says
185:54 - hence stable
185:57 - note that looking at standard deviations
186:00 - is just one component of evaluating
186:02 - market prices or youth or plans or
186:05 - things like that
186:08 - another measure of spread is called the
186:11 - coefficient variation the coefficient of
186:13 - variation is just the standard deviation
186:15 - divided by the mean
186:17 - times 100 percent
186:19 - if you're looking at the population
186:20 - coefficient of variation it'll be Sigma
186:23 - over mu
186:24 - if you're looking at the sample
186:26 - coefficient of variation will be S over
186:28 - X bar
186:30 - why do we introduce yet another
186:32 - coefficient variations because
186:33 - coefficient variation is used compared
186:35 - to versions for two variables
186:38 - hence
186:39 - our previous example
186:41 - which I don't want to say in front of
186:43 - anyone from Hawks
186:44 - this is completely useless
186:46 - because if profacto has a standard
186:50 - deviation of a dollar two but it trades
186:52 - at a dollar or that's an incredible
186:55 - change uh incredible width of it whereas
186:59 - yard Smith it's 967 but if it's trading
187:02 - at a thousand dollars a share that's
187:04 - really not much of a variation
187:08 - so while we do like to look at the
187:10 - standard deviations being a measure of
187:11 - risk really it should be the coefficient
187:14 - of variation
187:17 - which this gets to example 314 we got
187:20 - two graphs we're trying to figure out
187:21 - which has that greater standard
187:24 - deviation relative to its mean in other
187:26 - words which has a greater coefficient
187:28 - variation
187:29 - if we ignore the bottom numbers which of
187:33 - these two Graphics is more spread out
187:36 - and it's very clear that the price of
187:38 - U.S Farmland is much more variable if we
187:41 - ignore the bottom numbers much more
187:43 - variable than is the annual average
187:44 - rainfall
187:47 - and that's what the coefficient of
187:48 - variation really does it ignores the
187:51 - bottom numbers and just looks at the
187:53 - shape of the graphs
187:55 - or quantifies how spread out the graph
187:58 - is
188:00 - we can do the actual calculations
188:04 - find out the coefficient of variation
188:06 - for data set a is 28.9 percent and for B
188:09 - it's 36.7 percent that really doesn't
188:12 - surprise us because we just decided that
188:14 - b is much more spread out than a is
188:18 - and that's what again that's what the
188:19 - coefficient of variation is measuring
188:21 - it's how spread out is the data if we
188:23 - ignore the actual values at the bottom
188:27 - and dividing by that X bar allows us to
188:30 - ignore those values at the bottom
188:33 - so those are measures of of spread or
188:36 - measures of dispersion
188:39 - the last two topics for this section
188:41 - really don't sit well with that that
188:43 - idea of measures of center measures to
188:45 - spread
188:46 - they really probably should be on their
188:48 - own section but this is where Hawks is
188:50 - putting it and between the two of us
188:54 - good as places any to put them
188:56 - we're going to look at the empirical
188:57 - rule and chebyshev's Theorem
189:01 - the empirical rule says that when the
189:03 - data follow a bell-shaped distribution
189:06 - an interesting pattern emerges in the
189:08 - data values about 68 of the data is
189:11 - within one standard deviation of the
189:14 - mean
189:16 - that is about 68 is between the values
189:18 - of mu minus Sigma and mu plus Sigma
189:23 - about 95 percent is within two standard
189:26 - deviations the mean that is about 95 is
189:29 - between mu minus two Sigma and mu plus
189:31 - two Sigma
189:33 - and almost all of it 99.7 within three
189:36 - standard deviations of the mean
189:40 - and that's all the empirical rule says
189:41 - notice it's when the data follow a
189:44 - bell-shaped distribution these are good
189:46 - estimates
189:47 - if the data do not follow a bell-shaped
189:49 - distribution these are not necessarily
189:51 - good estimates in fact they tend to be
189:53 - pretty poor
189:56 - so if the data follow a nice little bell
189:58 - shape here then six to eight percents
190:00 - within one standard deviation the mean
190:04 - 95 within two
190:07 - 99.7 within three
190:10 - and almost all is within four
190:14 - so here's an application of it
190:16 - distribution of Weights of newborn
190:18 - babies is bell-shaped with a mean of 3
190:21 - 000 grams wow those are heavy babies
190:23 - three thousand grams and a standard
190:25 - deviation of 500 grams
190:28 - so we know that 68 percent of the babies
190:31 - are between three thousand minus five
190:33 - hundred and three thousand plus five
190:35 - hundred
190:36 - 95 within three thousand minus two times
190:40 - five hundred and three thousand plus two
190:43 - times five hundred
190:44 - and ninety nine point seven percent is
190:46 - within three thousand minus three times
190:48 - five hundred and three thousand plus
190:50 - three times five hundred
190:52 - so what the empirical rule says
190:55 - hey what percentage of the newborn
190:56 - babies weigh between two thousand and
190:58 - four thousand grams
191:00 - that's 95 percent because two thousand
191:03 - is two standard deviations below three
191:06 - thousand and four thousands to standard
191:08 - deviations above it so it's within two
191:10 - standard deviations of three thousand
191:13 - what percentage of newborn babies weigh
191:15 - less than 3 500 grams that one's not
191:17 - going to be so easy
191:19 - we do know that 68 are between 2500 and
191:23 - 500 I'm sorry twenty five hundred and
191:26 - thirty five hundred
191:28 - which tells us that 34 is between three
191:31 - thousand and thirty five hundred
191:34 - we also know that half is less than
191:36 - three thousand
191:38 - so we can use that calculation to get
191:40 - the answer
191:41 - and we can calculate the range of birth
191:44 - rates that would contain the middle 68
191:45 - percent so it's between 2500 and 3 500.
191:49 - now here it is written out
191:52 - since we know the distribution the data
191:54 - is spell shaped we can apply the
191:55 - empirical rule
191:57 - we need to know how many standard
191:58 - deviations two thousand grams and four
192:00 - thousand grams are from the mean
192:04 - here are the calculations it's two below
192:07 - to two above
192:11 - according to the empirical rule
192:12 - approximately 95 of the values lie
192:14 - within two standard deviations the mean
192:16 - so it's 95 percent
192:21 - B takes a little bit more work
192:23 - how many standard deviations the weight
192:25 - of 3500 is away from the mean
192:28 - that's one above
192:30 - so it's one standard deviation above the
192:33 - mean
192:36 - says that 68 of the data values lie
192:38 - within one standard deviation the mean
192:39 - that means that 34 percent lie between
192:42 - the mean and one above
192:45 - and half is below 3000 because it's a
192:51 - bell-shaped curve symmetric so the mean
192:54 - and the median are the same
192:56 - so if the mean is 3000 then half will be
192:58 - less than or equal to three thousand
193:01 - so the 34 and the 50 add together to
193:03 - give us an 84 percent of the newborn
193:05 - babies weigh less than 3 500 grams
193:12 - here's a picture of it
193:15 - 2500 to 3500 contains 68 percent
193:19 - since it's symmetric that means between
193:20 - 3000 and 3 500 we've got 34 percent
193:26 - and a systematic distribution so half of
193:29 - the data is less than three thousand
193:31 - so the part that's not yellow is just 50
193:34 - plus 34 which gave us the 84 percent
193:42 - and then C also an easy calculation we
193:44 - know 68 of the data lie within one
193:46 - standard deviation the mean
193:48 - that's between 2500 and 3 500.
193:54 - and the last topic is chebyshev's
193:57 - theorem
193:58 - whereas the empirical rule is a nice
194:00 - rule of thumb if the data comes from a
194:02 - bell-shaped distribution gets you really
194:03 - good estimates chebyshev's theorem is a
194:06 - mathematical theorem that is always
194:08 - correct it's not always helpful
194:11 - but it's always correct
194:13 - and the theorem statement is the
194:15 - proportion of data that lie within K
194:17 - standard deviations the mean is at least
194:20 - one minus 1 over K squared
194:23 - and K's got to be greater than one
194:26 - so if K is 2
194:28 - then within two standard deviations the
194:31 - mean is at least three-fourths
194:35 - could be more than that could be a lot
194:38 - more than that but it's at least
194:39 - three-fourths
194:42 - and the proportion of the data within
194:44 - three standard deviations at the mean
194:46 - set k equal to three
194:48 - is about 89 percent
194:51 - so at least 89 of the data is within
194:54 - three standard deviations of the mean
194:56 - could be a lot more than that
194:59 - could be just
195:01 - 88.88889 percent
195:05 - so shabby chefs always works but it
195:07 - doesn't necessarily give us a good
195:09 - estimate
195:11 - empirical rule doesn't always work but
195:13 - when it does is applied to bell-shaped
195:15 - data it does give us a good estimate
195:18 - so here we'll apply Chevy Chef's theorem
195:20 - suppose that in one small town the
195:23 - average household income is three
195:24 - thousand thirty four thousand two
195:26 - hundred dollars with a standard
195:27 - deviation of two thousand two hundred
195:30 - dollars
195:31 - what percentage of households earn
195:33 - between 27 6 and 40 800 dollars
195:37 - oh we just have to figure out how many
195:39 - standard Devi deviations both below the
195:42 - mean are 27 6 and how many both mean are
195:46 - forty thousand eight hundred
195:48 - that will tell us what the value of K is
195:52 - and then we need Chevy chev's theorem to
195:54 - determine the minimum
195:56 - or the at least as much
196:01 - so 6600 is the distance we know that the
196:05 - standard deviation is 2200 so that's
196:07 - negative three
196:09 - so 27 600 is three standard deviations
196:13 - below the mean
196:17 - similarly
196:18 - forty thousand eight hundred is three
196:20 - standard deviations above the mean
196:24 - thus K is three and we know that at
196:28 - least eighty eight points nine percent
196:30 - of the data is within those bounds
196:33 - could be a lot more
196:34 - can't be less it absolutely cannot be
196:37 - less than 88.9 percent
196:44 - so now let's compare the empirical rule
196:46 - and chebyshev
196:48 - here the difference is Shelby Chef's
196:50 - theorem always works that is it's always
196:52 - correct
196:54 - an empirical rule requires the
196:55 - distribution is bell-shaped
196:58 - the empirical rule tends to give better
197:00 - estimates than does Chevy Chef's theorem
197:02 - Chevy chefs just gives a lower bound
197:04 - empirical rule
197:07 - tries to estimate the actual proportion
197:10 - it's going to be off if the data is not
197:12 - bell shaped but if the data are
197:14 - bell-shaped it's going to be a really
197:16 - good estimate especially compared to
197:17 - championships theorem
197:20 - and while chebyshev's theorem always
197:22 - works it only serves as a lower bound
197:24 - hence the at least in the theorem
197:27 - statements
197:28 - and we got a red star so let's move on
197:30 - to question three
197:33 - which is more helpful the empirical rule
197:35 - or chebyshev's theorem again left hand
197:39 - side of your notes answered in your
197:40 - notes so that you can eventually put it
197:42 - into the Moodle quiz go ahead and pause
197:45 - and back
197:47 - and that's it for this chapter I'm sorry
197:50 - that's it for this section measures of
197:52 - dispersion
197:54 - as we move forward in this class the
197:56 - measure dispersion that's going to be
197:58 - most useful for statisticians
198:00 - for some odd reason is the variance or
198:04 - the standard deviation
198:08 - that's just how it is the range note we
198:11 - just got one small page dedicated to it
198:13 - then we kind of left it you can go ahead
198:15 - and ignore the range it's not helpful at
198:18 - all
198:19 - um
198:20 - empirical rule we're going to see pop up
198:22 - again in chapters six seven eight nine
198:27 - ten eleven
198:30 - 12. so you may want to spend a little
198:32 - bit of time learning the empirical rule
198:35 - shabby Chev we're not going to see it
198:38 - again after this chapter
198:40 - so well except on yeah we won't see it
198:43 - again on the chapter so now you know the
198:45 - most important things from this section
198:47 - but again I want you to focus on
198:50 - how to get the computer to calculate
198:52 - these and how to interpret them
198:55 - I also want you to figure out why
198:57 - measures of spread are important
198:59 - especially if we talked about measures
199:01 - of center last lecture
199:03 - how do those two relate how are they
199:06 - different why would one be more useful
199:09 - than the other those are important
199:10 - questions to answer in any class but
199:14 - especially in statistics
199:16 - so that's it I wish you a good day
199:20 - hello and welcome to section three three
199:22 - measures of relative position in some
199:24 - ways this should be section 2 2 and the
199:27 - measures of spread would be section
199:28 - three three because measures relative
199:31 - position are measures of some point in
199:33 - your data or in your distribution very
199:36 - similar to section 31 where we talked
199:38 - about a point in your data or in your
199:40 - distribution but the point for section
199:42 - three one was the mean the median or the
199:44 - mode here it's going to be some other
199:46 - point within your data
199:49 - and so topics from a Saturday you're
199:51 - going to be able to calculate
199:52 - percentiles sometimes these are called
199:55 - quantiles q-u-a-n-t-i-l-e-s
199:59 - so you're going to be able to calculate
200:00 - the quartiles then the five number
200:02 - summary which is just all the quartiles
200:04 - together calculate the interquartile
200:07 - range the IQR which is also a measure of
200:09 - spread
200:10 - so that kind of ties into the last
200:12 - lecture we're going to be able to create
200:13 - a box plot and calculate some z-scores
200:16 - and that Z scores may not sound too
200:18 - interesting now but those z-scores will
200:20 - pop up again in chapters 6 7 8 9 10 11.
200:24 - 12.
200:26 - without z-scores that understanding what
200:28 - z-scores do we kind of lose a lot of
200:31 - this second half of the course
200:34 - okay so we'll start with percentiles
200:35 - definition of percentile is
200:38 - those hundred divisions
200:40 - in order to calculate a values relative
200:43 - position we can divide the data into
200:45 - equal parts and state in which part the
200:47 - value lies
200:49 - that kind of underscores all of these
200:51 - measures the relative position
200:52 - we may choose to divide the data up into
200:54 - any number of parts for percentiles
200:57 - we're dividing the data up into a
200:59 - hundred parts
201:00 - 100 cents
201:04 - and those divisions when you divide up
201:05 - into 100 Parts is called a percentile
201:09 - here's how you calculate percentiles
201:11 - we're going to locate the data value for
201:13 - the P percentile so you're going to be
201:14 - given the percentile such as the 45th
201:17 - percentile the 97th percentile the 1.75
201:21 - eighth percentile
201:24 - those values are going to be the p
201:27 - n will be the sample size and this
201:28 - believe it or not is a lowercase l l for
201:31 - location
201:35 - and this is the formula to calculate the
201:36 - location notice it's the location not
201:38 - the data value so the location is just
201:41 - the sample size times the percentile
201:44 - over 100 well that percentile over 100
201:47 - is just the proportion of the way
201:49 - through the data so it kind of makes
201:51 - sense that this would be a location spot
201:56 - um don't miss something nope so when
201:58 - using the formula to find the location
201:59 - for the percentile's value in the data
202:01 - set you must make sure of the following
202:03 - two rules if the formula results in a
202:06 - decimal value for l
202:08 - the location is the next larger whole
202:10 - number
202:12 - if the formula results in a whole number
202:15 - then the percentile's value is the
202:17 - arithmetic mean of the data value that
202:19 - is located at that location and at the
202:21 - next larger
202:22 - Hmm this kind of sounds vaguely like how
202:26 - we calculate the median
202:28 - which shouldn't surprise us because the
202:31 - median we're going to find out is the
202:32 - 50th percentile
202:35 - so here's an example to see how to do
202:37 - this
202:38 - car manufacturers studying the highway
202:39 - MPG for a wide range of makes and models
202:42 - of vehicles
202:43 - stem and leaf plots given in the next
202:45 - slides there's a lot of it there's 135
202:47 - data points we need to find the 10th
202:50 - percentile and the 20th percentile
202:53 - so here's the data here's the second
202:55 - page of the data
202:57 - so we got one vehicle gets 12.1 percent
203:01 - one mile per gallon another vehicle gets
203:03 - 13.3 another is 14.1 then 15.5 and 15.6
203:08 - so that's what statement leaf plot tells
203:10 - us
203:12 - so here's the solution first we need to
203:16 - notice that the data are in an ordered
203:18 - array
203:19 - that is the lowest is 12.1 mile per
203:22 - gallon and the highest is 35.9 miles per
203:24 - gallon
203:28 - there are 135 values so n is 135.
203:33 - we want to calculate the 10th percentile
203:34 - so p is 10
203:36 - substituting the values we get L is
203:39 - equal to 13.5
203:40 - notice 13.5 is not a whole number
203:44 - therefore
203:47 - the next larger we round that up to 14
203:49 - and so the data point that is at
203:52 - position 14
203:55 - will be the 10th percentile
204:00 - and that data value is 17.3
204:06 - so let's go back to the data to see that
204:08 - one more time
204:09 - the 14th data value one two three four
204:12 - five six seven eight nine ten eleven
204:15 - twelve thirteen fourteen Seventeen point
204:17 - three is the 14th value
204:20 - therefore 17.3 is the tenth percentile
204:24 - in other words approximately 10 percent
204:26 - of the values in the data set are less
204:28 - than or equal to 17.3
204:31 - more importantly
204:33 - if we gathered an infinite amount of
204:36 - data
204:37 - 10 of the data values would be less than
204:39 - or equal to about 17.3
204:45 - for B we're looking at the 20th
204:47 - percentile we still have n is equal to
204:49 - 135 now we're given p is equal to 20
204:52 - because it's 20th percentile
204:54 - we get 27 as value for l
204:58 - this is a whole number therefore we
205:00 - average the 27th and the 28th data
205:03 - values
205:05 - let's go back to the data
205:07 - 27th and 28th data values one two three
205:10 - four five six seven eight nine
205:14 - 20 19 20 21 22 23 24 25 26 27th is 19.2
205:19 - 28 is 19.3 therefore the 20th percentile
205:24 - would be the arithmetic mean of those
205:25 - two or 19.25
205:31 - so approximately 20 of the values in the
205:33 - data set are less than or equal to 19.25
205:41 - and now we need to figure out a way of
205:44 - calculating the percentile of a given
205:47 - data value so now we're given the data
205:49 - value
205:50 - and we need to determine its percentile
205:53 - so for instance we're given 18.9 miles
205:56 - per gallon we need to determine which
205:58 - percentile that corresponds to
206:01 - it's just inverts the previous uh
206:05 - previous equation L is the location of
206:09 - that data value n is the sample size so
206:12 - L Over N is actually the proportion of
206:14 - the way through the data
206:16 - that holds that value
206:18 - multiply by 100 that's the percent of
206:20 - the way through the data that holds the
206:21 - value and by gosh that's the percentile
206:25 - p
206:27 - so if the data from the previous example
206:29 - of the Nissan Xterra average 21.1 miles
206:31 - per gallon what is its percentile
206:34 - well here we go we got to figure out
206:36 - what position 21.1 miles per gallon is
206:39 - in the data set so let's scroll back
206:43 - 21.1 is way down here or way up here I'm
206:47 - not going to count that but I'll let you
206:49 - count it
206:54 - happens to be the 49th value
206:58 - so the value of 21.1 miles per gallon is
207:01 - 49 out of 135 of the way through the
207:04 - data
207:05 - 49 out of 135 is 0.36 multiply that by
207:09 - 100 so that 21.1 is about 36. is about
207:14 - the 20 30. is about the 36th percentile
207:22 - approximately 36 percent of the data
207:24 - values are less than or equal to
207:28 - that is the value 21.1 miles per gallon
207:32 - is in the 36 percentile of the data set
207:36 - oh we got a red star
207:40 - here we go this will be the intro
207:42 - lecture question the first one of this
207:44 - lecture
207:45 - question one what is the percentile
207:47 - again write the question on your
207:48 - notebook answer it so that you can
207:50 - transfer though that question and answer
207:52 - or just that answer to Moodle for the
207:55 - lecture quiz
207:56 - pause
207:58 - and we're back
208:02 - now we're going to look at quartiles
208:04 - recall that the percentiles divide the
208:06 - data of into a hundred
208:09 - whereas the quartiles are going to
208:10 - divide it up into fourths
208:12 - four for court
208:15 - first quartile is about 25 of the data
208:18 - is less than or equal to it second is 50
208:20 - is less than or equal
208:22 - third quartile is three quarters or 75
208:24 - of the data is less than or equal to
208:26 - in other words the first quartile is the
208:29 - 25th percentile
208:31 - the second quartile is the 50th
208:33 - percentile and the third quartile is the
208:36 - 75th percentile
208:38 - also recall that the second quartile
208:41 - which is also the 50th percentile is
208:44 - also the median
208:47 - here's a couple ways of calculating or
208:50 - shall I say estimating the quartiles
208:56 - we're going to use a percentile method
208:58 - which we just got through doing a couple
209:01 - examples on the percentile we're going
209:02 - to use the approximation method
209:04 - going this way approximation method is
209:06 - you find the median drop the median you
209:08 - find the median of the lower half to get
209:10 - q1 median of the upper half to get Q3
209:14 - and look at how these values compare
209:17 - for a large
209:18 - data set
209:20 - the value is going to be very close
209:23 - we already know the data are in order
209:25 - from smallest to largest and we already
209:26 - know that n is 135
209:28 - . here's the data set
209:32 - so using the percentile method we want
209:33 - to find the 25th percentile so p is 25.
209:37 - do these calculations to find that the
209:39 - location is 33.75
209:42 - because that is not a whole number we
209:44 - just look at the 34th value which is
209:46 - 19.8 miles per gallon that will be the
209:49 - first quartile
209:50 - which is identical to the 25th
209:52 - percentile
209:55 - second quartile is the median or the
209:57 - 50th percentile thus n is equal to 135 p
210:01 - is equal to 50.
210:03 - that brings us to 67.5 round that up to
210:07 - 68. the median will be the 68th value
210:13 - which is 23.6
210:15 - the third quartile is to the 75th
210:18 - percentile so p is 75 crack put into the
210:22 - formula 101 rounded up to 102 so it'll
210:26 - be the 102nd value
210:28 - which is 25.3 so the third quartile is
210:32 - 25.3
210:36 - we can also do the approximation method
210:38 - divide the data in half
210:41 - we get the 68th position
210:44 - which is the median is 23.6 calculate
210:47 - that many times
210:49 - so the second quartile is 23.6
210:52 - first quartile is going to be the median
210:54 - of the lower half which comes out to be
210:57 - 19.8
210:59 - and the third quartile is the median of
211:01 - the upper half which is 25.3
211:06 - they look they're very very close and
211:09 - for large data sets they are going to
211:11 - tend to be close
211:14 - and remember all the times I've said
211:16 - it's approximately this or about a
211:19 - certain percent of the data is less than
211:21 - or equal to
211:22 - when you've got data the best you can do
211:24 - is just approximates or about
211:27 - if you got the entire population you can
211:29 - get exact
211:32 - ly next example finding the quartiles of
211:34 - a data set so we got two data sets we
211:37 - got data set a and data set B
211:42 - we can use the approximation method
211:45 - there's the quartile I I'm sorry there's
211:48 - the median second quartile
211:51 - 70.5
211:56 - q1 and Q3 can be estimated
212:00 - 65 and 78
212:04 - . so three of the five number summary is
212:06 - going to be 65 70.5 and 78.
212:11 - using this second set of data
212:14 - can we start the median
212:17 - go through the exact same calculations
212:20 - to get estimated values so three of the
212:23 - five numbers for the B data set are 67
212:26 - 75 and 79.
212:31 - of course we can use R to do these
212:33 - rather quickly give it the data do
212:36 - summary of that data set
212:38 - and this will give you the first and
212:41 - third quartiles it'll give you the
212:43 - median
212:44 - it also gives you the Min and the max
212:45 - which are the other two numbers in the
212:47 - five number summary and it will also
212:49 - give you a mean
212:52 - now that these values are not the same
212:54 - as those received when the approximation
212:55 - method is used
212:57 - so which is correct
213:00 - they both are
213:02 - because remember medians
213:04 - don't have to be unique and that extends
213:08 - to all the quartiles and percentiles
213:10 - they don't have to be unique
213:15 - oops red star
213:18 - question two again write this over on
213:20 - the left hand side answer it beneath how
213:23 - do percentiles and quartiles differ
213:26 - hit pause
213:28 - and we're back
213:32 - five number summary and box plots five
213:35 - number summary consists of the five
213:37 - quartiles
213:38 - the minimum value which is q0
213:41 - first quartile which is q1 second
213:43 - quartile which is Q2 third quartile Q3
213:46 - and the maximum which is Q4
213:50 - five number summaries may put these five
213:52 - numbers listed in order from smallest to
213:54 - largest
213:56 - why do we need the five number summary
213:57 - well it gives us a good feel for the
213:59 - distribution of the data
214:01 - so write the five number summary for the
214:03 - date an example of 3.2
214:06 - we've done we've calculated the
214:08 - quartiles already
214:09 - all we have to do is figure out what the
214:10 - minimum 12.1 and the maximum 35.9 values
214:15 - are and there's our five number summary
214:18 - minimum q1 Q2 Q3 maximum
214:26 - we are going to illustrate the five
214:28 - number summary using a box plot
214:30 - technically it's called a box and
214:32 - whiskers plot
214:35 - box and whiskers plot this case it's a
214:37 - horizontal box and whiskers plot looks
214:39 - like this
214:41 - it specifies with the minimum value and
214:43 - the maximum values are those are the
214:44 - endpoints of the whiskers and the Box
214:46 - endpoints are q1 and Q3
214:49 - and the median is indicated by a thick
214:52 - line inside that box
214:55 - the range between q1 and Q3 is called
214:59 - the interquartile range
215:01 - it's just Q3 minus q1
215:05 - it's a single number
215:07 - about half the data occurs Inside the
215:09 - Box
215:11 - about a quarter of the data occurs above
215:13 - the box and about a quarter occurs below
215:20 - and this is what I just said
215:23 - here's the actual formula for the
215:25 - interquartile range it's Q3 minus q1
215:31 - here's how we create the box plot
215:35 - we're going to begin with
215:37 - the five number summary
215:40 - we're going to determine a nice little
215:41 - scale horizontal axis that fits all
215:43 - those values nicely
215:46 - we're now going to Mark those five
215:48 - numbers on the on the graphic the
215:52 - minimum q1 Q2 Q3 the maximum
215:57 - we're going to draw the Box in q1 to Q3
216:01 - the thick horizontal vertical line at
216:03 - the median
216:05 - and then we show the whiskers going out
216:08 - for the box to the Min and the max
216:12 - we can do this in r with the box plot
216:15 - command load in the data and just supply
216:18 - box plot to the data
216:20 - if you want something that looks like
216:21 - this graphic
216:23 - you have to specify horizontal equals
216:26 - true a vertical box plot is the default
216:28 - and the color is orange col for color
216:32 - and that will color the Box
216:38 - slot faster and a lot more accurate than
216:40 - doing it by hand
216:45 - we're going to interpret box plots now
216:48 - we have four sub basins
216:50 - we got the Upper Mississippi the Ohio
216:52 - Tennessee the Missouri and the Arkansas
216:54 - Red River sub basins
216:58 - notice this is not a typical box plot
217:00 - because the top whisker and the bottom
217:02 - whisker are at 90th percentile and the
217:05 - 10th percentile
217:06 - which we see over here
217:09 - we also see the mean as a DOT
217:12 - that will help us see the skew of the
217:13 - data set
217:15 - so for instance Missouri seems to be
217:17 - skewed up
217:19 - because the mean is above the median
217:23 - the Upper Mississippi and the Ohio
217:24 - Tennessee seem to be rather symmetric
217:26 - and the Arkansas Red River seems to be
217:29 - skewed up as well
217:36 - so here are the questions what did the
217:37 - top and bottom bars represent in these
217:39 - box plots according to the key they
217:41 - represent the 90th and the 10th
217:42 - percentiles
217:44 - which sub Basin had the highest median
217:46 - average
217:48 - highest median average the median is the
217:50 - solid horizontal line so it looks like
217:53 - it's the Missouri
217:57 - which set Basin had the lowest average
217:59 - spring
218:01 - lowest average
218:07 - so it looks like the lowest average is
218:10 - going to be the Ohio Tennessee
218:14 - if we're looking at the average it looks
218:16 - like that's going to be less than the
218:17 - lower than the Arkansas Red River
218:20 - if we're looking to medians however the
218:21 - Arkansas Red River is definitely below
218:23 - the Ohio Tennessee
218:27 - and which sub Basin had the largest
218:29 - interquartile range in other words which
218:32 - said Basin had the largest spread to the
218:35 - data that's clearly in Missouri because
218:38 - the box is the largest of all the other
218:41 - three
218:47 - last topic today is our z-scores or our
218:50 - standard scores
218:53 - when we can compare two data values from
218:56 - two completely different populations by
218:58 - comparing their relative or their
219:00 - respective percentiles we could also
219:03 - determine how the values relate to the
219:04 - respective means of their data sets
219:06 - z-scores do this latter
219:09 - it's called the standard score or the
219:12 - z-score again we're going to see the
219:14 - z-score pop up in chapter 6 7 8 9 10 11
219:18 - 12.
219:20 - the standard Square tells us how far a
219:22 - value is from the mean
219:26 - specifically how many standard
219:27 - deviations it is from the mean
219:31 - the formula for the population standard
219:33 - score that is if you are given the
219:35 - population mean and standard deviation
219:37 - it's just the value you have minus the
219:40 - population mean
219:42 - divided by the population standard
219:44 - deviation
219:46 - for the sample standard score it's x
219:50 - minus X bar the sample mean divided by S
219:52 - which is the sample standard deviation
219:56 - so it's always x minus a mean divided by
219:58 - a standard deviation
220:02 - example
220:04 - mean square of the math section of the
220:05 - SAT test is 500 with a standard
220:07 - deviation of when it's 50.
220:09 - what is the standard score for a student
220:11 - who scored 630 so 630 is X
220:15 - 500 is Mu
220:18 - and 150 is Sigma so the Z is just going
220:21 - to be 630 minus 500.
220:24 - divided by 150.
220:31 - so a person who scored 500 on the SAT
220:33 - now I'm sorry a person who scored 630 on
220:36 - an SAT has a z-score of 0.87
220:42 - . in other words this student
220:44 - 's test score is about 0.87 standard
220:47 - deviations above the mean
220:55 - Jody scored an 87 on her calculus test
220:57 - and was bragging to her breasts friend
220:59 - about how well she'd done
221:01 - poor Jody
221:02 - she said that her class had a mean of 80
221:04 - and a standard deviation of 5.
221:06 - therefore she had done better than the
221:08 - class average which is true Jody got an
221:11 - 87 though the average was 80 so
221:13 - definitely above
221:15 - her best friend Ashley was disappointed
221:17 - she'd scored only in 82 on her calculus
221:20 - test however the mean for her class was
221:22 - 73 with a standard deviation of six
221:27 - so who really did better on her test
221:30 - Jody or Ashley
221:32 - and we're going to do this compared to
221:33 - the rest of the class
221:36 - so Jody's z-score is 87 minus 80. divide
221:40 - by 5 and Ashley's will be 82 minus 73
221:43 - divided by 6.
221:50 - so for Jody it's 1.4
221:53 - Ashley it's 1.5
221:55 - thus compared to everyone else in the
221:58 - class Ashley actually did better
222:00 - compare it to everybody else in the
222:02 - class Ashley actually did better
222:06 - Ashley's score was actually 1.5 standard
222:09 - deviations above average
222:11 - whereas Jody's score was only 1.4
222:14 - standard deviations above average
222:20 - he and here we are calculating z-scores
222:23 - using R let's go back to the A
222:25 - let's calculate the z-score
222:27 - corresponding to our first value
222:30 - now there is no z-score function native
222:32 - to R
222:34 - but sourcing this file will give us one
222:37 - we just give it the data and we specify
222:38 - z-score of the data and then in Brackets
222:41 - which data value do you want to get the
222:43 - z-score for if we want to get the first
222:45 - z-score we put a one in Brackets
222:48 - if we leave the brackets off completely
222:51 - and just have z-score of a then we get
222:53 - all the z-scores
222:56 - the output for this tells us that the
222:58 - first value has a z-score of negative
223:00 - 1.47196
223:02 - thus it is about 1.47 standard
223:05 - deviations below average about one half
223:07 - standard deviations below average
223:10 - below because the z-score is negative
223:13 - below average because all Z scores are
223:16 - done with respect to average
223:18 - if the z-score is zero then that person
223:21 - scored average
223:27 - yeah there's one more question
223:30 - question three
223:32 - I must miss the red star again write the
223:36 - question in your notes on the left
223:37 - answer it below when should one use a
223:40 - box plot instead of a histogram
223:42 - this one's going to take you some some
223:44 - thought
223:45 - think about what the histogram tells you
223:47 - think about what the box plot tells you
223:51 - think about the examples that we gave in
223:53 - this class in this lecture for box plots
223:57 - spend some time this is the last part of
224:00 - this lecture by the way spend some time
224:02 - thinking about the strengths of
224:03 - histograms the strengths of box plots
224:06 - and now when should a box plot be used
224:08 - instead of a histogram
224:12 - go ahead and hit pause but we are done
224:17 - and so that's the end of chapter three
224:19 - call in chapter three we're now
224:21 - summarizing our data using numbers in
224:24 - chapter two we summarized our data using
224:26 - Graphics chapter 3 it's using numbers we
224:29 - had some several we had several
224:30 - important numbers that we looked at the
224:32 - mean the median the mode standard
224:34 - deviation the interquartile range we
224:37 - also looked at measures of position the
224:40 - z-score don't forget the empirical rule
224:43 - and chebyshev's Theorem although you
224:46 - could forget Chevy Chef's theorem life
224:48 - would go on the empirical rule is
224:50 - actually very important as is z-score
224:53 - understanding both of those will help us
224:54 - when we get to chapter seven
224:56 - and that's it hopefully this was helpful
224:58 - if not drop me in line
225:02 - hello and welcome to section 4.1 this
225:04 - will be the first section of the chapter
225:06 - four purpose of chapter four is to
225:08 - introduce you to probability Theory the
225:10 - vast majority of chapter four is going
225:12 - to review
225:13 - um it's going to be review of stuff that
225:15 - you learned back in middle school and
225:17 - high school
225:18 - um
225:20 - there are no intra-lecture questions
225:23 - posted however there are quizzes in
225:26 - Moodle so the answers for the Moodle
225:30 - quizzes for each of these chapter four
225:32 - sections will be not in the lecture
225:38 - got it
225:39 - you write not in the lecture for each of
225:41 - those you get full credit if you don't
225:43 - write it you don't get full credit
225:46 - so the objectives for section four one
225:48 - identify the sample space of a
225:49 - probability event calculate basic
225:52 - probabilities determine if two of answer
225:53 - mutually exclusive and determinive to
225:56 - advance our independent
225:57 - mutually exclusive and independent are
226:00 - two things that you're going to be need
226:02 - to be very careful of if there's a lot
226:04 - of confusion between the two if two
226:08 - events are mutually exclusive then both
226:10 - cannot happen at the same time
226:13 - if two events are independent then
226:15 - having one happen doesn't affect the
226:18 - other
226:19 - so here's some terminology
226:22 - a probability experiment or a trial is a
226:25 - process with a result determined by
226:27 - chance such as flipping a coin
226:30 - each individual result that is possible
226:32 - for a probability experiment is an
226:34 - outcome
226:37 - so there are two outcomes for the coin
226:40 - flipping outcome one is ahead outcome
226:42 - two is a tail
226:44 - the sample space is the set of all
226:46 - possible outcomes for any given
226:48 - probability experiment therefore the
226:50 - sample space is heads and tails or heads
226:53 - comma tails
226:55 - an event is a subset of outcomes from a
226:57 - sample space so an event could be just
227:00 - hit
227:02 - an event could be just tail
227:05 - an event could be head or tail
227:07 - or an event here could be none of the
227:10 - above
227:14 - examples that probably experiments
227:16 - include 50 coin tossing a pair of die
227:18 - drawing a raffle ticket those are the
227:20 - basics in each of these examples there
227:23 - is more than one possible result and the
227:26 - result is determined at random
227:30 - example four one consider an experiment
227:33 - in which a coin is tossed and then a
227:36 - six-sided die is rolled
227:38 - so a we need to list the outcomes of the
227:40 - sample space for the experiment
227:42 - see coin toss then six-sided die is
227:45 - rolled okay list the outcomes in the
227:47 - event tossing a tail then rolling an odd
227:50 - number
227:52 - okay
227:53 - so here are the solutions
227:55 - a each outcome consists of a coin toss
227:57 - and a die roll
227:59 - for example you can get a head and then
228:01 - a three we're going to denote a head
228:03 - followed by 3 as H3
228:06 - using this notation we've got 12
228:09 - outcomes
228:11 - 12 possible outcomes
228:13 - and the set of those 12 possible
228:15 - outcomes is going to be the sample space
228:18 - you can get ahead and then a One a head
228:20 - and then a two a head then a three dot
228:23 - dot dot a tail then a five then a tail
228:26 - and a six
228:27 - it's
228:29 - 2 times 6 possible outcomes
228:32 - so the sample space is just those 12
228:35 - possible outcomes
228:38 - so B we're going to choose the members
228:39 - of the sample space that fit the event
228:41 - quote tossing a tail then rolling an odd
228:44 - number
228:46 - tossing your tail and then rolling out
228:47 - number there's only three odd numbers
228:49 - one three and five so the three possible
228:51 - outcomes of this sample space are T1 T3
228:54 - T5
229:01 - next example let's consider the
229:04 - experiment in which a red six-sided die
229:06 - and a blue six-sided die are rolled
229:08 - together
229:10 - use a pattern to help list the outcomes
229:12 - in the sample space
229:15 - with the outcomes in the event the sum
229:16 - of the numbers rolled on the two dice
229:18 - equals six
229:23 - okay many patterns that we could use
229:27 - this is one that they're looking at
229:30 - why would we use a quote pattern here
229:33 - instead of listing all of them out
229:34 - there's 36 possible outcomes
229:37 - six for the first six for the second six
229:40 - times six gives us 36. then we'd have to
229:42 - list out 36 outcomes for the entire
229:45 - sample space
229:46 - here
229:48 - we can just give a pattern for what
229:50 - they're going to be
229:52 - or we can list them all out if we look
229:55 - if we'd like to
229:57 - have these drawings
230:00 - notice the sample space is one and one
230:02 - two and one three and one four and one
230:04 - five and one six and one Etc
230:07 - if all we care about is the sum of the
230:11 - uh numbers that come up then the sample
230:14 - space would be 2 3 4 5 6 7 8 9 10 11 12.
230:17 - because 2 through 12 is the sums
230:21 - I'll keep the sample space in mind
230:25 - note that rolling one on the red dying
230:27 - or two on the blue die is different
230:28 - there are only a two and a one if we
230:30 - care about the actual outcomes however
230:32 - if we care about just the sum then those
230:35 - two are going to be identical outcomes
230:40 - Part B I'm going to list the outcomes
230:42 - the event quote the sum of the numbers
230:44 - rolled on the two dice equals six
230:47 - so we're going to go back and look to
230:49 - see there's one here five one four two
230:52 - three three two four one five those all
230:56 - give us a sum of six
230:58 - so we could actually say the probability
231:00 - of getting a six when rolling two dice
231:02 - is six one two three four five sorry
231:05 - it's five one two three four five five
231:08 - out of 36.
231:13 - a tree diagram can be used to represent
231:16 - the outcomes of an experiment
231:18 - especially in the early phases when
231:20 - you're just trying to learn why it's 6
231:22 - times 6 instead of six plus six
231:26 - the tree begins with the possible
231:27 - outcomes of the first stage and then
231:29 - branches for each additional possibility
231:31 - we're going to see tree diagrams in the
231:34 - future so it would be good to be able to
231:36 - write these out eventually
231:38 - number of possibilities in the bottom
231:40 - row the tree is equal to the number of
231:42 - outcomes in the sample space
231:44 - so let's consider a family with three
231:45 - children
231:46 - use a tree diagram to find the sample
231:49 - space for the gender of each child in
231:51 - regard to birth order so again ordering
231:54 - here does matter we're doing it oldest
231:56 - middle youngest
231:59 - so here it is the first child can either
232:02 - be a girl or a boy
232:06 - second can be girl boy
232:11 - or girl and boy and then girl boy girl
232:14 - boy girl boy girl boy
232:17 - so the bottom here GGG would indicate
232:21 - that all three children were girls
232:23 - ggp would be a girl girl followed by a
232:26 - boy
232:26 - gbg would be a girl than a boy then a
232:29 - girl notice that this looks like a tree
232:31 - especially if you turn it upside down
232:37 - and so using this tree diagram as a
232:40 - guide we can see there's going to be
232:41 - eight outcomes
232:43 - two times two times two
232:46 - three children so two to the power of
232:47 - three
232:51 - and now let's look at three methods for
232:53 - calculating the probability of an
232:54 - outcome
232:56 - this is also usually thought of in terms
232:59 - of three ways of understanding
233:01 - probability they're subjective the
233:04 - experimental and there's classical what
233:06 - we've used so far is classical what the
233:09 - course is going to be using the future
233:10 - will be mostly experimental sometimes
233:13 - it's called frequency probability or
233:16 - sometimes relative frequency probability
233:19 - in a feature course subjective
233:22 - probability will be renamed Bayesian
233:24 - probability and we'll see that
233:26 - subjective probability is actually the
233:27 - most useful of the three
233:30 - but that's for a
233:32 - a future course way down the line
233:36 - subjective probability according to
233:38 - Hawks is simply an educated guess
233:39 - regarding the chance an event will occur
233:41 - this is Hawks
233:43 - this is not reality but
233:47 - you won't understand the reality until
233:49 - you get into a future stat course
233:51 - and again subjective probability or
233:53 - Bayesian probability will be the most
233:55 - useful
233:57 - but we're not there yet
233:59 - experimental probability or relative
234:01 - frequency probability
234:03 - talks about using data to estimate a
234:07 - probability in an event
234:09 - so if e is an event then P of e the
234:12 - probability that e occurs is given by F
234:14 - over n
234:16 - where f is the frequency and N is the
234:18 - total number times the experiment is
234:20 - performed
234:23 - so if I want to estimate or find the
234:26 - probability of randomly selecting a
234:28 - sophomore
234:30 - from campus I could ask 500 people on
234:33 - campus if they're a sophomore and of
234:36 - those 500 295 said they are a sophomore
234:39 - and would be 500 the number of people I
234:41 - asked
234:42 - and therefore I would estimate the
234:44 - probability of a sophomore of being 295
234:46 - or 500.
234:50 - and that's going to be the Cornerstone
234:52 - this experimental probability or this
234:54 - relative frequency probability will be
234:56 - the Cornerstone of
234:58 - the second half of the course
235:01 - mainly because
235:03 - I'm going to go the classical
235:06 - probability
235:07 - very quickly indicates a very the
235:12 - classical probability very quickly will
235:14 - become useless we don't know the actual
235:17 - probability of selecting a student who
235:19 - is a sophomore
235:21 - so we just have to estimate it and that
235:24 - estimation and the using of the
235:26 - estimated probabilities is this
235:27 - experimental probability
235:30 - the benefit or the theoretical
235:33 - background that we can use the
235:34 - experimental probability is called the
235:36 - law of large numbers as the sample size
235:38 - increases the proportion or the means of
235:43 - your sample approaches the means or the
235:45 - the proportions of the population
235:52 - and they have classical probability
235:54 - if all outcomes are equally likely and
235:57 - this is actually a very important
235:58 - requirement that the outcomes are
235:59 - equally likely then the probability of e
236:01 - is equal to the number of elements in
236:04 - the e
236:05 - divide by the number of elements in the
236:07 - sample space
236:09 - hence when we were talking about what's
236:11 - the probability of getting a 6 when
236:13 - rolling 2 Die by getting six I mean
236:15 - adding up the dice together of getting
236:18 - it was just five the number of ways of
236:21 - the dice adding up to six divided by 36
236:24 - the possible number of outcomes
236:33 - example four is identifying the types of
236:35 - probability
236:36 - determine whether each probability is
236:38 - subjective experimental or classical
236:41 - again experimental is also also referred
236:43 - to as relative frequency or just as
236:45 - frequentest classical is also referred
236:48 - to as axiomatic ax iom a t i c
236:53 - probability of selecting the Queen of
236:54 - Spades out of a well-shuffled standard
236:56 - deck of cards is 1 over 52 that's
236:58 - clearly classical there's only one Queen
237:00 - of Spades out of the 52 cards in the
237:02 - deck
237:05 - we know this we don't have to estimate
237:07 - it
237:08 - we're not guessing at it
237:10 - Economist predicts a 20 percent chance
237:13 - that technology stocks will decrease in
237:15 - value over the next year
237:18 - from the information given that will be
237:20 - subjective
237:21 - although this economist may have used
237:25 - data to come up with that in which case
237:26 - it would be experimental
237:29 - police officer wishes to know the
237:31 - probability that a driver chosen at
237:32 - random will be driving under the
237:34 - influence on a Friday night
237:36 - so he records the number of drivers at a
237:39 - roadblock
237:40 - a number of drivers drinking with ba
237:42 - blood alcohol levels of the legal limit
237:44 - yet determines the probability is three
237:45 - percent that is very clearly a relative
237:48 - frequency or an experimental probability
237:50 - because it's based on the experiment
237:53 - that he performed
237:58 - we just discussed that
238:00 - Beck is allergic to peanuts for the next
238:03 - example poorbeck at a large dinner party
238:05 - one evening he notices that the
238:07 - cheesecake options on the dessert table
238:08 - contain the following flavors 10 slices
238:11 - of chocolate 12 slices of caramel 12
238:13 - slices of peanut butter chocolate and
238:16 - eight slices of strawberry
238:18 - assuming that the desserts are served to
238:20 - a guest at random what's the probability
238:23 - that Beck's cheesecake contains peanuts
238:26 - it's 12
238:28 - divided by 10 plus 12 plus 12 plus 8.
238:33 - and what's the probability that X
238:34 - dessert does not contain chocolate
238:37 - that's going to be 12
238:41 - Plus 8 divided by 10 plus 12 plus 12
238:45 - plus 8.
238:48 - in the first case it's because 12 of
238:51 - those the numerator 12 contain peanut
238:54 - butter
238:55 - and in the second case it's a numerator
238:58 - of 12 plus 8 does not contain chocolate
239:07 - however were I back I would need
239:10 - cheesecake at all because
239:13 - even a
239:14 - 28 chance it's not worth it
239:22 - consider a beginning Archer who only
239:24 - manages to hit the target half the time
239:26 - what's the probability that in three
239:27 - shots the Archer will hit the target all
239:30 - three times
239:32 - that's going to be 1 over 8 or 12.5
239:35 - percent here's y
239:38 - each time
239:42 - she has a 50 chance of hitting the
239:44 - target
239:45 - so 50 chance to hit the first time fifty
239:47 - the second 50 the third to hit it all
239:50 - three times it's got to hit it the first
239:53 - and the second and the third time
239:58 - here's the tart here's the tree diagram
240:00 - notice that only one of these eight
240:02 - cases
240:03 - does she hit the target all three times
240:09 - so it's one over eighth
240:10 - or 12.5 percent
240:18 - consider a family with six boys what's
240:21 - the probability that the seventh child
240:22 - will also be a boy
240:25 - okay
240:27 - um
240:30 - families do to have a girl
240:32 - but reality States just the opposite if
240:36 - you've got six boys it's most more
240:37 - likely that you'll have a seventh boy
240:39 - however we're supposed to pretend that
240:42 - each time it's a fifty percent chance of
240:44 - getting a boy or a girl
240:48 - um so it's you got six boys followed by
240:52 - girl or six boys followed by a boy of
240:54 - these one out of the two
240:58 - um is a boy so 50 chance again this
241:02 - assumes that the outcomes are equally
241:04 - likely the reality is that if you've had
241:07 - six boys it's more likely that you'll
241:09 - have a seventh boy
241:11 - um it's not 50 chance and also even on
241:15 - your first child it's not a 50 chance
241:18 - that you'll have a boy it's actually a
241:20 - less than 50 chance that you'll have a
241:22 - boy
241:23 - that's a 49 point something percent
241:25 - chance
241:26 - but we're simplifying things here so we
241:29 - because we can multiply and by half very
241:32 - easily
241:36 - um in biology we learned that many
241:38 - diseases are genetic one example as such
241:40 - is Huntington's disease which causes a
241:42 - neurological disorder as person ages
241:46 - each person has two Huntington genes
241:50 - one inherited from each parent if the
241:53 - individual inherits a mutated Huntington
241:55 - genes
241:56 - from
241:59 - from either of his or her parents that
242:02 - person will develop the disease notice
242:04 - it's from either
242:06 - of the parents
242:09 - uh TV show House the character who Dr
242:11 - house calls 13. inherited the disease
242:15 - from her mother
242:17 - so if 13 has a child with a person who
242:19 - has two healthy Huntington G disease
242:23 - what's the probability your child will
242:24 - develop Huntington's
242:27 - quite simply the answer is going to be
242:32 - one half
242:34 - because gonna get a good Gene from the
242:38 - parent from the father and a bad Gene
242:42 - from her 50 percent
242:48 - and that's it for section four one
242:51 - remember
242:52 - go to Moodle take the quiz and for each
242:55 - of your answers write something like
242:57 - this was not asked in the lecture
243:01 - and that's it
243:03 - hope you all have a good day
243:06 - hello and welcome to section 4.5 where
243:08 - we're learning about the addition rules
243:10 - of probability in other words we're
243:11 - looking at probability of the Union of
243:14 - two events so we're going to use
243:16 - addition rules to calculate probability
243:18 - that's the objective for today there are
243:21 - three properties of probability
243:24 - um for any event e is going to be our
243:26 - generic event and the probability V is
243:28 - going to be between 0 and 1 inclusive
243:32 - for any sample space the generic sample
243:35 - spaces can be in a cursive S the
243:37 - probability of being an element of that
243:39 - sample space is one because remember the
243:42 - sample space is a set of all possible
243:43 - outcomes
243:45 - and three for an empty set
243:47 - the probability of empty set is zero so
243:49 - the probability of nothing happening is
243:51 - zero
243:54 - um
243:54 - the complement is a very important
243:57 - concept that you're going to be using in
243:59 - chapter five
244:00 - chapter four really only has two
244:04 - Concepts that are extremely important
244:06 - one is complement
244:08 - the other is going to be the concept of
244:10 - Independence Independence we're going to
244:12 - be using in chapter 11 but complement
244:15 - we're going to be using quite a bit in
244:17 - chapters five and six
244:19 - um the complement of event e has denoted
244:21 - is denoted as e to the power of C it's
244:25 - not really a to a power of it's just
244:27 - superscripted C
244:30 - in some sources this will be a prime
244:34 - in some sources it'll be bar over the e
244:38 - I like the C it's pretty nice it's the
244:42 - complement of e is the set of all
244:44 - outcomes that are not in e
244:47 - so describe the complement of each of
244:49 - the following events a red card out of a
244:52 - standard deck of cards the event is
244:54 - choose a red card so the complement will
244:57 - be choose a not red card which means the
245:01 - component is going to be choose a black
245:03 - card out of a standard deck of cards
245:08 - pardon me out of 31 students in your
245:11 - stat class 15 are out sick with the flu
245:14 - so the event e is being out sick with
245:17 - the flu
245:18 - the complement of e is going to be not
245:21 - being out sick with the flu so in this
245:23 - case e complement will be able to attend
245:26 - class or not being sick
245:29 - your area 91 percent of phone customers
245:31 - use phone South
245:34 - so the event is customer using phone
245:37 - South the complement will be customer
245:39 - not using phone South
245:43 - so be the complement e complement will
245:45 - be customer using some
245:48 - something other than phone South
245:52 - that's what all of the solutions tell us
245:58 - the complement rule for probability and
246:00 - this is the most important part is that
246:02 - the probability of an event plus the
246:04 - probability of its complement is one
246:08 - this is kind of like put a star next to
246:10 - it in your notes Circle it happy face
246:12 - around it because this we're going to be
246:15 - using a lot in chapter five
246:19 - for some problems and this is actually
246:21 - why we're going to use it on chapter
246:23 - five for some problems the probability
246:24 - of complement is much easier to
246:25 - calculate than the probability of the
246:27 - vent itself
246:28 - this is especially true when you're
246:30 - dealing with infinite sets or infinite
246:32 - sample spaces for these problems you can
246:34 - calculate the probability of the
246:35 - complement and subtract that value from
246:37 - one
246:40 - here's an example you're worried that
246:42 - there are there is a 35 chance you'll
246:44 - fail your upcoming test what's the
246:47 - probability that you will pass the test
246:49 - it's going to be the complement of
246:51 - failing so it's going to be 1 minus 35
246:52 - percent
246:55 - if there is a five percent chance that
246:56 - none of the items on a scratch off will
246:58 - be a winner what's the probability that
246:59 - at least one
247:02 - who will win well at least one means the
247:05 - probability of a one or a zero
247:07 - I'm sorry at least one that's the
247:09 - probability of a one a two a three a
247:11 - four or five all the way up there
247:13 - so that's going to be 1 minus the
247:15 - probability of zero
247:18 - so here's the solution for the first
247:21 - 65 percent pretty straightforward the
247:24 - second is much more complicated
247:28 - probability of at least one winner is
247:30 - one minus the probability of no winners
247:33 - and we're told the probability of no
247:35 - winners is five percent
247:37 - which means that the probability of at
247:39 - least one
247:40 - is going to be 95 percent
247:44 - I do want to emphasize here make sure
247:46 - you understand that
247:48 - no winners and at least one winners are
247:51 - complementary events
247:54 - so pause until you understand that
247:57 - role of pair is standard six-sided dice
247:59 - what is the probability that neither die
248:02 - is a three
248:03 - that's gonna be pretty straightforward
248:05 - to calculate if you use complements if
248:07 - you do it Brute Force then you're going
248:10 - to have to list out all 36 possibilities
248:13 - and determine how many of those
248:16 - neither has a three
248:18 - or we can just use compliments
248:23 - so these are the outcomes that have a
248:26 - three
248:27 - remember e is not having a three so e
248:31 - complement will be having a three
248:35 - so there are 11 elements in E
248:37 - complements so the probability of being
248:39 - an e complement is 11 over 36.
248:42 - therefore the probability of being an e
248:44 - is 25 over 36.
248:54 - which is about 70 percent
248:56 - so you have about a seventy percent
248:57 - chance of rolling two dice and not
249:00 - getting a three on either one
249:08 - the original addition rules for
249:09 - probability this is the addition rule
249:13 - for probability
249:15 - there is a simplification to it that may
249:18 - be useful or may be allowed but this is
249:21 - the rule and the probability of given to
249:24 - events the probability of being in event
249:26 - e or in event F or both is just the
249:30 - probability of being an e plus the
249:31 - probability of being an F minus the
249:34 - probability of being both
249:40 - Cerise is looking for a new condo to
249:43 - rent her realtor provided with the
249:45 - following list of amenities for 17
249:47 - available properties
249:49 - there's a list close to the subway with
249:52 - six seven were low maintenance fee five
249:55 - had green space two were newly renovated
249:59 - close to the subway and low maintenance
250:01 - were two
250:02 - so one and two there were two in that
250:06 - which meant that there were yeah
250:10 - um
250:11 - Green Space and Lira innovated was just
250:13 - one
250:14 - if cerise's realtor selects the first
250:17 - condo they visit at random what's
250:19 - probably the property is either close to
250:21 - a Subway or has a low maintenance fee
250:25 - close to assembly we'll call e low
250:27 - maintenance V we'll call F
250:32 - so let's verify that the realtor has
250:34 - accurately counted the total number of
250:36 - properties
250:42 - remember there were 17 properties
250:45 - six where if type one seven were a type
250:48 - 2 5 or type three two were type four
250:52 - which gave us 20 but we know that there
250:55 - is a overlap of three
250:57 - we were told what this overlap was to
251:01 - are both low maintenance and close to
251:03 - Subway and one is both newly renovated
251:05 - and green spaced so
251:08 - 17 individual properties
251:11 - we're going to use the or that tells us
251:14 - we'll be using the addition rule
251:17 - e or F
251:20 - that's equal to probability of e plus a
251:23 - probability of f minus the probability
251:24 - of both
251:26 - There are 16 that are close to a Subway
251:29 - seven that are low fee and we're also
251:32 - told that two are both
251:35 - so six plus seven minus 2 is 11. so
251:38 - about a 64 chance
251:46 - suppose that after a vote in the U.S
251:48 - Senate on a proposed health care bill
251:49 - the following table shows the breakdown
251:51 - of the votes by party
251:55 - 23 versus 21 43 versus 7 2 versus 4.
252:03 - if a lobbyist stops a random Senator
252:05 - after the vote what's the probability of
252:07 - the senator will be either a Republican
252:10 - or voted against the bill
252:13 - so either a Republican
252:15 - or evaluate against the bill
252:18 - that's just going to equal the
252:19 - probability of it being a Republican 50.
252:22 - plus the probability of voting against
252:24 - the bill 32
252:27 - minus the seven that were counted twice
252:32 - divided by 100 or whatever
252:41 - Republican voted against the bill
252:44 - counted twice gives us a 75 chance the
252:47 - lobbyists will
252:49 - have dinner with the senator he wants
252:57 - roll a pair of dice what's the
252:59 - probability of rolling either a total
253:00 - less than four
253:02 - or a total equal to 10
253:07 - total less than 4 will be e total equal
253:10 - to 10 would be f
253:12 - we want the probability of e or F
253:15 - that's just the probability of e plus
253:16 - the probability of f minus the
253:18 - probability of E and F
253:21 - less than 4 plus probability of 10 minus
253:25 - the probability of less than 4 and 10.
253:26 - well what's the probability of it being
253:29 - both less than 4 and equal to 10. well
253:33 - that's zero because this is the empty
253:35 - set
253:36 - and from our uh three requirements for
253:39 - probability that means that this
253:41 - probability is equal to zero and we just
253:43 - pay attention to the first two
253:47 - in other words
253:49 - less than four and ten are mutually
253:53 - exclusive events
253:55 - one can happen the other could happen
253:58 - but both cannot happen
254:04 - so there's three that are less than four
254:06 - there's three that are ten so it's going
254:09 - to be six out of 36.
254:17 - again the key these two events are
254:19 - mutually exclusive
254:21 - and that leads to a zero percent chance
254:23 - of both occurring
254:30 - if events E and F are exclusive and the
254:33 - addition rule is much easier
254:37 - and apparently Hawks didn't give us
254:40 - the formula
254:42 - it's just the probability of e plus the
254:44 - probability of f there's no need to
254:47 - subtract off anything because what
254:48 - you're subtracting off is just zero
254:54 - Caleb is very excited that it's finally
254:57 - time to purchase his first new car after
255:00 - much thought he's narrowed his choices
255:01 - down to four because it's taken him so
255:03 - long to make his mind his friends have
255:05 - started to bet on which car he will
255:06 - choose
255:08 - they've given each car a probability
255:10 - based on How likely they think Caleb is
255:12 - to choose that car
255:16 - Devin's betting that Caleb will choose
255:18 - either a Toyota or a Jeep
255:20 - find the probability that difference
255:22 - right
255:24 - and here's the probabilities that
255:26 - they've assigned
255:28 - probability of a Toyota or a Jeep
255:30 - it's just going to be the probability of
255:31 - a Toyota plus the probability of a Jeep
255:34 - minus the probability of a Toyota and a
255:36 - Jeep but that probability of both is
255:39 - zero because he's purchasing his first
255:42 - new car only one car
255:54 - so Devin has a 75 chance of correctly
255:57 - picking which car Caleb will buy
256:02 - example 416
256:04 - we're going to extend the addition rule
256:07 - to more than just two events
256:11 - we got these probabilities
256:13 - and we're asked what's the probability
256:14 - that the driver will refuel at Shell
256:16 - Exxon or Chevron
256:18 - shell
256:19 - Exxon or Chevron
256:24 - these are mutually exclusive events
256:26 - therefore the probability is just going
256:28 - to be the sum of the individual
256:29 - probabilities
256:31 - the probability of the genuine is the
256:33 - sum of the probabilities
256:35 - if the events are mutually exclusive
256:39 - and that's the end of first chapter
256:41 - section 4-2
256:43 - again don't forget to complete the quiz
256:46 - in Moodle
256:48 - and for each of the questions write
256:49 - something like this was not covered on
256:52 - the lecture
256:54 - and that's it
256:55 - take care
256:57 - hello and welcome to section 4.3 this is
257:00 - the third section in chapter four
257:02 - probability Theory here we're looking at
257:04 - the multiplication rules for probability
257:06 - which eventually will lead to
257:08 - conditional probability and Independence
257:11 - so we're going to use multiplication
257:13 - rules to calculate probability
257:15 - um
257:16 - multi-stage experiment is experiment
257:18 - with two or more steps or stages we've
257:21 - seen examples of this the roll the die
257:22 - and then flip a coin is a multi-stage
257:25 - experiment the flipping the coin three
257:27 - times is a multi-stage experiment in
257:29 - fact it's a three-stage experiment
257:31 - an experiment performed with replacement
257:33 - refers to placing an object back into
257:36 - consideration before performing the next
257:38 - stage of the experiment
257:39 - those examples they just gave you were
257:41 - quote with replacement because you could
257:43 - end up with actually the first one with
257:46 - roll the die and then flip a coin it's
257:48 - not with experiment because
257:50 - once you roll the die you can't get a
257:51 - one two three four five or six for the
257:53 - second term
257:54 - we did have an earlier example of the
257:56 - probability of getting a Queen of Spades
257:59 - If all we're doing is drawing one card
258:01 - then we don't need to talk about with or
258:04 - without replacement but if we're talking
258:06 - about drawing two cards
258:08 - what's the probability that either one
258:09 - is a Queen of Spades well now we got to
258:11 - think about am I putting the first card
258:13 - back after I draw the second first card
258:15 - or do I keep that first card in my hand
258:18 - if I put the card back it's with
258:20 - replacement
258:21 - and the calculations tend to be a little
258:22 - bit easier when you're doing things with
258:24 - replacement if I hold on to that first
258:27 - card then it's without replacement and
258:30 - the calculations get a little bit more
258:31 - difficult not too much two events are
258:34 - independent if one event happening does
258:36 - not influence the probability of the
258:37 - other event happening
258:40 - um so that's independent multiplication
258:42 - rule for probability of independent
258:44 - events and I want to emphasize this is
258:45 - for independent events right now
258:48 - if enf are independent then the
258:50 - probability of E and F occurring is
258:53 - equal to probability of e times the
258:55 - probability of f
258:58 - that's it I suppose two cards from a
259:00 - standard deck I choose two cards from a
259:02 - standard deck with replacement that
259:04 - means I draw a card look at it put the
259:06 - card back reshuffle draw a second card
259:08 - what's the probability of choosing a
259:10 - king and then a queen
259:13 - well since I put the card back
259:17 - the outcome of the first doesn't impact
259:20 - the outcome of the second draw so these
259:22 - are going to be two independent events
259:24 - so it's just going to be the probability
259:26 - of choosing a king times the probability
259:28 - of choosing a queen
259:32 - so that's going to be 4 out of 52 times
259:34 - 4 out of 52
259:36 - or 1 13 times 1 13 which is about
259:41 - 0.6 percent chance
259:46 - assume that a study by human resources
259:48 - has found that the probabilities of an
259:50 - employee being written up for the
259:51 - following infractions are the value
259:52 - shown in the following table
259:55 - so this is probability of being written
259:58 - up at work
259:59 - um
260:02 - and we're going to assume that each
260:04 - infraction is independent of the others
260:07 - this is given information to us we would
260:09 - have to know that they're independent we
260:11 - can't just look at the problem and
260:13 - understand hey they're independent we
260:15 - have to be told they're independent
260:17 - what's the probability that a given
260:18 - employee will be written up for being
260:20 - late to work
260:21 - taking unauthorized breaks and leaving
260:23 - early
260:25 - late to work
260:27 - unauthorized breaks leaving early
260:30 - so we're looking at the probability of
260:32 - late work and unauthorized breaks and
260:35 - leaving early
260:36 - so it's just the product of those three
260:39 - probabilities because these are
260:41 - independent events
260:44 - so about one percent chance
260:49 - set a different way about one percent of
260:52 - the of the people at that company
260:55 - get rid of for being late to work taking
260:57 - on authorized breaks and leaving early
260:59 - that seems rather
261:02 - seems rather High to me
261:06 - um
261:07 - an experiment performed without replace
261:09 - it means that the objects are not placed
261:11 - back into consideration
261:13 - that means that the two draws are going
261:16 - to be dependent most likely to events
261:18 - are dependent if one event happening
261:19 - affects the probability of the other
261:21 - event happening
261:23 - so we're looking back at our king and
261:26 - queen example we want to know the
261:28 - probability of drawing a king and then a
261:29 - queen if the cards are drawn without
261:32 - replacement
261:34 - so this is going to be a good one
261:36 - the situation is essentially the same as
261:38 - drawing two cards from a standard deck
261:42 - so instead instead of thinking let's
261:44 - draw one card and then draw the other
261:47 - you can think of this as just drawing
261:49 - two cards
261:50 - and you're asking what's the probability
261:52 - of the first card I drew being a king
261:54 - and the second being a queen
261:57 - by determining the probability of
261:59 - drawing a king from a snare deck of
262:01 - cards Begin by doing that it's just one
262:04 - out of 13.
262:07 - now let's assume that when I drew the
262:10 - first card it was a king
262:12 - so given the first card is a king what's
262:14 - the probability that the second one is a
262:16 - queen
262:18 - huh it's there's four Queens in the deck
262:22 - but the deck only has 51 cards left in
262:25 - it
262:27 - because I didn't put that King back
262:29 - there's only 51 cards left in the deck
262:32 - thus the probability of a queen given
262:34 - that the King was drawn first without
262:36 - replacement is 4 out of 51.
262:40 - so the probability of getting a king and
262:42 - then a queen is just gonna be the
262:44 - product
262:47 - of about 0.06
262:49 - I'm Sorry by about point six percent
262:56 - now what we've actually done is we've
262:58 - started talking about conditional
262:59 - probability without telling you that
263:01 - we're talking about conditional
263:02 - probability
263:04 - here's the the key that this is
263:06 - conditional probability it's the word
263:08 - given
263:10 - conditional probability denoted by
263:12 - probability of f given e that vertical
263:15 - bar is read as quote given
263:18 - is a probability of event F occurring
263:20 - given that the event e occurs first
263:25 - is event E and F are independent then
263:29 - the probability of f given e is just
263:30 - probability of f which leads to a a nice
263:34 - definition of independence for us
263:37 - one card's already been chosen from a
263:39 - standard deck without replace and what's
263:41 - the probability of now choosing a second
263:42 - card and it being red given the first
263:46 - card was a diamond
263:48 - so we're asked what's the probability of
263:50 - being second being read given the first
263:52 - was a diamond
263:58 - red given diamonds just 25 over 51
264:01 - because we didn't put the card back
264:04 - there's only 25 cards that are red
264:09 - when the first card was a diamond which
264:12 - is also Red by the way
264:17 - so now here's the actual multiplication
264:20 - rule
264:21 - for probability
264:24 - given to events E and F
264:26 - the probability of E and F occurring is
264:28 - just the probability of e times the
264:30 - probability of f given e
264:33 - which is identical to the probability of
264:36 - f times the probability of e given f
264:41 - these two formulas are
264:43 - exactly the same
264:46 - the one you use depends on the data
264:48 - that's given to you
264:50 - so it's a probability of choosing two
264:52 - face cards in a row
264:54 - we're going to assume the cards are
264:56 - chosen without replacement
264:58 - here we're dealing with dependent events
265:00 - so we'll use the multiplication rule
265:04 - and the first card is picked all 12 face
265:06 - cards are available at a 52 so the
265:08 - probability the first one is a face card
265:09 - is 12 out of 52.
265:12 - now given that the first one is
265:15 - a face card
265:17 - then there's only 11 left that are face
265:19 - cards out of the 52 out of the 51 cards
265:22 - remaining in the deck
265:24 - so the total probability will be 12 over
265:26 - 52 times 11 over 51.
265:33 - which is about a five percent chance
265:40 - assume that there are 17 men 24 women in
265:42 - the Rotary Club two members are chosen
265:44 - at random each year to serve on the
265:45 - scholarship committee what's the
265:47 - probability of choosing two members at
265:49 - random the first being a man and the
265:52 - second being a woman
265:56 - we're choosing two members the first
265:58 - choice will influence the probability of
265:59 - the second
266:03 - there's 41 people all together
266:08 - so the probability of a man and woman is
266:10 - just a probability of man times the
266:12 - probability of a woman given the first
266:13 - was a man
266:17 - probability of man is 17 out of 41.
266:21 - so given the first one was a man the
266:23 - probability of the second one being a
266:25 - woman
266:25 - is just 24 out of 40.
266:29 - it's 24 because there's 24 women in the
266:31 - Rotary Club it's 40 because there are 40
266:34 - that are remaining after the first one
266:36 - was chosen
266:41 - so there's about a 1 4 chance
266:49 - we can also write conditional
266:50 - probability and this is sometimes
266:52 - referred to as the definition of
266:53 - conditional probability
266:55 - the probability of f given e is the
266:57 - probability of E and F
266:59 - divided by the probability of E
267:05 - if E and F are independent
267:09 - from the probability of E and F is just
267:11 - the probability of e times the
267:12 - probability of f
267:14 - probability of E's cancel out so we're
267:16 - given the probability of f given e is
267:18 - just equal to the probability of f
267:22 - and that's a very nice definition of
267:24 - Independence
267:27 - out of 300 applicants for a job 212 were
267:29 - female and 110 are female and have a
267:31 - graduate degree
267:33 - what's the probability that a randomly
267:35 - chosen applicant has a graduate degree
267:37 - given she's female
267:40 - so now we're told the person who God is
267:42 - female what's the probability that she
267:44 - has a graduate degree
267:46 - 212 female 110 female graduate degree so
267:50 - this probability is just going to be 110
267:52 - over 212.
267:56 - if 152 of the applicants have graduate
267:58 - degrees
268:00 - what's the probability that a rambly
268:02 - chosen applicant is female
268:04 - given the applicant has a graduate
268:06 - degree
268:08 - 152 is the denominator
268:11 - 110 is the numerator
268:14 - so the probability is just going to be
268:15 - 110 over 152.
268:31 - he is female and graduate degree divided
268:34 - by female
268:46 - female and graduate degrees divided by
268:48 - graduate degree
268:53 - the emphasis here is that probability F
268:55 - given e is not necessarily the same as
268:57 - probability of e given F order matters
269:00 - with conditional probability
269:02 - the reason order matters is you're given
269:04 - different information
269:06 - probability F given e you're given that
269:08 - e is true and you need to calculate the
269:10 - probability of f
269:11 - or as in probability of e given F you're
269:13 - given f is true and you need to
269:15 - calculate a completely different
269:16 - probability probability of E
269:22 - and now for the fundamental counting
269:24 - principle for a multi-stage experiment
269:27 - with n stages where the first stage has
269:29 - K1 outcomes the second stage has K2
269:32 - outcomes the third stage has K3 outcomes
269:34 - and so on
269:35 - the total number of possible outcomes
269:38 - is k1 times K2 times K3 times dot dot
269:42 - dot times kn
269:44 - we've already seen this
269:47 - um with the roller die then flip a coin
269:49 - K1 was 6 K2 was two the total number of
269:53 - outcomes was 12. 6 times 2. flipping the
269:57 - coin three times okay one was two k two
270:00 - is two k three was two two times two
270:02 - times two is eight there are eight
270:04 - possible outcomes
270:06 - rolling two dice K1 with six K two was
270:09 - six six and six is 36 there are 36
270:11 - possible outcomes so we've already
270:13 - experienced this we're just giving you a
270:15 - nice name for it
270:17 - Kilby begins her first year in an online
270:19 - degree program in July the first
270:21 - semester she'll randomly be assigned to
270:23 - one section for each of four different
270:25 - core courses if there are eight English
270:27 - one sections 12 college algebra sections
270:31 - 11 American history sections and five
270:33 - phys Ed physical science sections
270:36 - how many different options are there for
270:38 - Kilby's schedule for her first semester
270:41 - oh it's just eight times twelve times
270:43 - eleven times five
270:46 - from the fundamental accounting
270:48 - principle
270:57 - fifty two eighty
271:00 - the governing board at a local charity
271:02 - Mission Stateville is electing a new
271:04 - vice president and secretary to replace
271:07 - outgoing board members if the board
271:09 - consists of 11 members who don't already
271:11 - hold an office how many different ways
271:13 - can the two positions be filled if no
271:15 - one may hold more than one office
271:18 - go ahead and hit pause
271:20 - to figure this out
271:22 - and you're back
271:26 - two slots to fill
271:28 - it's going to be without replacement
271:30 - because you can't hold both offices
271:33 - there are 11 choices for the first
271:35 - position
271:37 - 10 choices for the second that leaves
271:39 - 110 possible ways to elect the new
271:42 - officers
271:45 - example Robin is preparing an afternoon
271:47 - snack for her twins Matthew and Laney
271:50 - she wants to give each child one item
271:52 - she has the following snacks on hand
271:54 - carrots raisins crackers grapes apples
271:56 - yogurt and granola bars
271:59 - if she randomly chooses one snack for
272:01 - Matthew and one snack for Laney what's
272:03 - the probability each child gets the same
272:05 - snack as yesterday
272:09 - here's the solution
272:11 - probably want to hit pause to come up
272:13 - with the solution yourself
272:15 - before you read the solution here
272:19 - you need to count the number of ways in
272:21 - which Robin can randomly choose a snack
272:22 - for her twins
272:24 - again we can think of this as two slots
272:26 - to fill one for each twin
272:29 - seven possibilities for each child
272:35 - there's no requirement that the twins
272:36 - have different snacks
272:38 - or conversely have the same snack so the
272:42 - total number of
272:44 - ways she can prepare the snacks is seven
272:46 - times seven
272:49 - now we need to count the number of ways
272:51 - that she can choose the same afternoon
272:52 - snack as yesterday
272:55 - there's only one thing they had
272:57 - yesterday
272:59 - so the probability is going to be one
273:01 - divided by the 49 total number of snacks
273:04 - so about two percent chance
273:12 - and that's it
273:15 - thank you very much
273:20 - hello and welcome to section 4.4 this is
273:22 - the fourth section of chapter four we're
273:25 - going to look at combinations and
273:26 - permutations these are just functions
273:29 - that allow you to quickly calculate
273:31 - total number of possible outcomes
273:35 - um so the objective calculate numbers of
273:36 - permutations and combinations
273:39 - before we get to that we have to Define
273:40 - what a factorial is
273:42 - a factorial of n a positive integer
273:46 - denoted by n exclamation point or called
273:50 - n factorial is just the product of n
273:53 - times n minus 1 times n minus 2 all the
273:55 - way down to one
273:57 - so one factorial is equal to one
274:00 - two factorial is equal to two times one
274:02 - or two three factorials equal three
274:05 - times two times one which is six
274:07 - by agreement 0 factorial is equal to one
274:12 - you might want to
274:13 - be careful on that one
274:17 - so let's calculate the following
274:18 - factorial expressions
274:20 - a is 7 factorial it's just going to be
274:23 - seven times six times five times four
274:24 - times three times two times one
274:27 - B is 4 factorial over zero factorial
274:30 - that's numerator is going to be four
274:32 - times three times two times one
274:33 - denominator is going to be just one
274:37 - C 95 factorial over 93 factorial that's
274:41 - just going to be 95 times 94
274:45 - y
274:46 - because 95 factorial is equal to 95
274:49 - times 94 times 93 factorial
274:53 - and then you're dividing off by that 93
274:55 - factorial so you're left with just 95
274:57 - times 94.
275:00 - D the numerators can be 5 times 4 times
275:02 - 3 times 2 times 1 denominator well 5
275:05 - minus 5 minus 3 is 2. so the denominator
275:09 - is just going to be 2 times 1.
275:11 - 2 factorial
275:15 - e numerator is going to be 6 times 5
275:17 - times 4 times 3 times 2 times 1.
275:19 - denominator is going to be 2 times 1
275:22 - times
275:23 - 4 times 3 times 2 times 1 because 6
275:25 - minus 2 is 4
275:30 - . so 7 factorial is 50 40.
275:35 - 4 factorial over 0 factorial is 24.
275:42 - 95 factorial divided by 93 factorial
275:47 - is 95 times 94.
275:55 - 5 factorial over 2 factorial is just 60.
276:02 - and 6 factorial over two factorial times
276:05 - four factorial is 15.
276:10 - now we're going to define the definite
276:12 - difference between a combination and a
276:14 - permutation
276:16 - a combination let's do with permutation
276:18 - first a permutation is a selection of
276:20 - objects from a group with the
276:21 - arrangement matters
276:24 - um
276:25 - combination the arrangement doesn't
276:27 - matter it's an example when you would
276:29 - use a permutation is you want to select
276:30 - a president vice president secretary
276:33 - from a group of people
276:35 - the actual positions are named and they
276:37 - matter
276:38 - combination would be I want to select a
276:41 - a
276:42 - group of three people from a larger
276:45 - class of 50.
276:47 - I'm just selecting three people in this
276:49 - group by not naming the positions it's
276:52 - just three
276:56 - so the arrangement matters that leads to
276:58 - permutations the arrangement is
277:00 - irrelevant that leads to a combination
277:03 - so the calculations are very similar ish
277:08 - and when order is not important the
277:10 - following formula is used to calculate
277:11 - the number of combinations
277:15 - um it's NCR or n choose r
277:18 - is just n factorial the larger number
277:21 - divided by R factorial times n minus r
277:24 - factorial
277:25 - so if I want to choose a group of three
277:29 - people out of our class of 30 and this
277:32 - 30 R is 3.
277:34 - so it'll be 30 factorial divided by 3
277:36 - factorial times 27 factorial
277:40 - when the order is important the
277:42 - following formula is used to calculate
277:43 - the number of permutations
277:45 - so if I want to choose a president vice
277:47 - president and secretary from our class
277:50 - of 30.
277:51 - and it's again going to be 30 R again is
277:55 - going to be 3 but the number of
277:56 - permutations just n factorial over 27
277:59 - factorial
278:00 - notice the number of combinations cannot
278:03 - be larger than the number of
278:04 - permutations
278:06 - and the number of combinations will be
278:07 - equal only when R is equal to
278:10 - 1 or 0.
278:14 - in other words when R is one or zero
278:18 - order is not important versus order is
278:20 - important that means the same thing
278:21 - because there's only one or no position
278:23 - to fill
278:25 - it's like choosing a president from our
278:27 - class of 30.
278:30 - whether or not the order matters within
278:32 - that one position it's not a question to
278:35 - ask because there's only that one
278:36 - position
278:42 - given a group of three friends
278:44 - how many ways can you arrange the way
278:46 - that they stand in line for the movies
278:48 - standing in line matters the order
278:50 - matters that so it'll be three factorial
278:53 - divided by three minus three factorial
278:57 - how many ways can I choose two of them
278:59 - to write in a car together I'm just
279:01 - choosing two I'm not saying one get
279:03 - shotgun the other doesn't I'm just
279:05 - choosing two so a is going to be
279:07 - permutations B will be combinations
279:09 - that'll be three choose two
279:20 - so there are six ways that they can
279:22 - stand in line
279:25 - here's the actual listing of those six
279:27 - ways
279:29 - as for the second part we're just
279:32 - choosing two from a group of three so
279:34 - it's called three choose two
279:37 - 3 factorial 2 factorial and then three
279:40 - minus 2 factorial
279:42 - so you can also think of this as the
279:45 - total number of way a total number of of
279:47 - people factorial
279:50 - divided by the number of people in the
279:52 - group you care about factorial times the
279:55 - number of people in the rest of the
279:57 - group
280:00 - we start with three friends we want two
280:02 - in the car this is what's not in the car
280:09 - and here are the possible options for
280:11 - who gets to ride
280:15 - class of 18 fifth graders is holding
280:18 - elections for class president vice
280:19 - president secretary how many different
280:21 - ways can the officers be elected since
280:22 - we've actually named these positions
280:24 - this is going to be permutations
280:27 - R is 3 n is 18.
280:34 - it's almost five thousand ways
280:41 - consider that a cafeteria is serving the
280:43 - following vegetables carrots green beans
280:45 - lima beans celery corn broccoli and
280:47 - spinach
280:48 - Bill what wishes to order a vegetable
280:51 - plate with three different vegetables
280:52 - how many ways can this plate be prepared
280:55 - well this is one where order doesn't
280:57 - matter
280:58 - this is going to be one two three four
281:03 - one two three four five six seven will
281:06 - be n
281:08 - 3 will be R so this will be
281:12 - seven choose three
281:19 - thirty-five
281:26 - suppose that a little league baseball
281:28 - coach is randomly listing the nine
281:30 - starting baseball players in batting
281:32 - order for their second game
281:35 - at this level the batting order is
281:37 - randomly chosen to give all players an
281:38 - opportunity to experience different
281:39 - batting positions what's the probability
281:41 - that the order chosen for the second
281:42 - game is exactly the same as that of the
281:44 - first game since we're focusing on the
281:46 - order itself this will be permutations
281:54 - and and R are both nine
282:00 - and we're looking at the probability so
282:02 - the denominator is going to be n permute
282:04 - nine I'm sorry 9 permute nine and the
282:08 - numerator is going to be 1 because there
282:10 - is only one ordering that we had
282:12 - yesterday
282:25 - 9 9 is equal to 9 times 8 times 7 times
282:28 - 6 times 5 times 4 times 3 times 2 times
282:30 - 1 which is 9 factorial
282:32 - and zero factorial is one so this is
282:36 - nine factorial or nine permute nine
282:41 - so there are that many possible batting
282:43 - orders
282:45 - the probability of getting the exact one
282:46 - from yesterday is just one divided by
282:48 - that
282:52 - so pretty low probability
283:02 - Maya has a bag of 15 blocks Each of
283:05 - which is a different color including red
283:06 - blue and yellow
283:08 - I reaches into the bag and pulls out
283:10 - three blocks what's the probability of
283:12 - the block she has chosen or red blue and
283:14 - yellow
283:17 - from the way this is stated this looks
283:20 - like a combination
283:22 - because it doesn't say that the blocks
283:24 - have to be in that order that she first
283:26 - pulls out a red then a blue then a
283:28 - yellow just that there are red blue and
283:31 - yellow
283:34 - out of those 15 so it's going to be 15
283:36 - choose 3.
283:42 - 455.
283:47 - how many combinations contain the red
283:49 - blue and yellow blocks
283:52 - order does not matter if there's only
283:54 - one way to choose those colors that's
283:56 - the probability that my choose is red
283:58 - blue and yellow is calculated and such
284:00 - just one over that
284:10 - last topic will be special permutations
284:14 - special permutations involve objects
284:17 - that are identical
284:19 - number of distinguishable permutations
284:21 - of n objects of which K1 are alike K2
284:24 - are alike and so forth is given by this
284:27 - we've actually already dealt with this
284:29 - in terms of permutations
284:32 - we start out with n in our permutations
284:35 - and we divide by K that belong to the
284:37 - group we care about and N minus K belong
284:41 - to the group we didn't care about
284:45 - if we only have two groups then K2 is
284:47 - just going to be n minus K1
284:49 - or n minus r
284:53 - make sure that your K1 K2 all the way as
284:56 - up to n
284:57 - Tennessee good example Mississippi is
285:00 - the other usual example how many
285:02 - different ways can you arrange the
285:03 - letters of the word Tennessee notice
285:05 - that the n's the E's the s's are all the
285:08 - same as each other
285:10 - there's one two three four five six
285:12 - seven eight nine letters in Tennessee
285:16 - there's one t four e's two n's and two
285:21 - s's
285:25 - which means the total number of ways is
285:27 - going to be 9 factorial divided by 1
285:30 - factorial times 4 factorial times two
285:33 - factorial times two factorial
285:42 - 3780
285:46 - and that's it again Mississippi is the
285:49 - other usual example
285:52 - how many ways can you arrange the
285:54 - letters in Mississippi if the I's the
285:56 - S's and the P's all look the same as the
285:57 - other eyes S's and P's
286:00 - definitely something to think about
286:02 - so that's the end of this section I
286:05 - think there's one more in the course I
286:07 - mean one more in the chapter
286:09 - enjoy
286:12 - hello and welcome to section 4.5 this
286:14 - will be the last section of chapter four
286:17 - and probability theory in this section
286:20 - we're going to combine the probability
286:22 - techniques
286:24 - of the first two and the counting
286:26 - techniques of the last sections into one
286:29 - nice little ball of of probability
286:34 - so we're going to use the basic counting
286:36 - rules to calculate probabilities
286:39 - in this section we'll look at counting
286:40 - problems that have more complicated
286:42 - Solutions than just one over the total
286:44 - number of possibilities a helpful trick
286:46 - for these problems look first to look
286:48 - for certain key terms
286:51 - the key terms are important because they
286:53 - Define the method to be used to find the
286:55 - correct answer words to look for include
286:58 - at least at most greater than less than
287:00 - between
287:01 - Etc
287:04 - so here we'll start out with a nice
287:07 - little example
287:08 - a group of 12 tourists is visiting
287:11 - London at one particular Museum a
287:14 - discounted admission is given to groups
287:15 - of at least 10.
287:19 - so a how many combinations of tourists
287:22 - can be made for the museum visit so that
287:25 - the group receives the discounted rate
287:27 - in other words we want to find the
287:29 - number of ways of having 10
287:33 - and 11.
287:35 - and 12 chosen out of that group of 12.
287:40 - and B suppose that a group of tourists
287:42 - does get the discount
287:44 - what's probability was made up of 11
287:46 - tourists
287:48 - in other words in a we got the total
287:50 - number of ways that you can get the
287:51 - discount and now we're given that they
287:53 - got the discount what's probability that
287:55 - it was made up of 11.
287:59 - so here's the solution keywords in this
288:01 - problem are at least
288:03 - so if at least 10 are required in the
288:05 - group
288:06 - that gets the discount has to be 10 11
288:09 - or 12.
288:12 - now we calculate the number of
288:13 - combinations to get 10 number of
288:15 - combinations to get 11 and number of
288:17 - combinations to get 12.
288:20 - then add them to back add them together
288:24 - and notice that these are combinations
288:26 - because
288:27 - we aren't putting the tourists in any
288:29 - line we're just getting groups of those
288:32 - tourists
288:33 - so here's how to get the group of 10
288:35 - tourists
288:36 - it's 12 shoes 10.
288:38 - group of 11.
288:40 - 12 choose 11.
288:42 - remember that 12 by the way
288:44 - and 12 choose 12 which is just one
288:52 - as we know the question applies that the
288:53 - group will get a discount if 11 a 10 11
288:56 - or 12 so the number of ways is just the
288:59 - sum of those 66 plus 12 plus 1
289:02 - so there is 79 ways that this group can
289:05 - get that discount
289:11 - now B we need to calculate this
289:13 - probability so we're given the group
289:15 - receive the discount what's the
289:16 - probability that it's a group of 11.
289:20 - so the numerator is going to be the
289:22 - number of ways of getting groups of 11
289:23 - and the denominators are going to be the
289:25 - number of ways of getting the discount
289:27 - well we've already figured that ways of
289:29 - getting a discount is 79
289:31 - and you figure out the ways of getting a
289:33 - group of 11 is 12. so the probability is
289:34 - going to be 12 over 79.
289:46 - Jack is setting a password on his
289:47 - computer he's told that his password
289:49 - must contain at least three but no more
289:51 - than five characters
289:53 - he may use either letters or numbers
289:56 - at least three but no more than five
289:58 - characters wow
290:00 - so a how many different possibilities
290:02 - are there for his password if each
290:04 - character can only be used once
290:07 - notice that this is without replacement
290:13 - and then suppose that Jack's computer
290:15 - randomly sets his password using all the
290:17 - restrictions given above what's the
290:19 - probability that his password is an
290:20 - arrangement of the letters in his name
290:23 - Jack
290:25 - so here's the solution for a
290:28 - it's got to be at least and no more
290:30 - those are some keywords there at least
290:33 - three no more than five so it's a number
290:34 - it's got to be three four or five
290:38 - so we gotta count the number of ways you
290:39 - can get three a number of ways you can
290:41 - get four and the number of ways you can
290:42 - get five
290:44 - the order of characters is important so
290:47 - this will be permutations
290:50 - there's 36 characters to choose from 26
290:53 - letters 10 digits
290:56 - so here's the calculation for three it's
291:00 - 36 permute three
291:03 - for four characters it's 36 permute four
291:07 - and for five characters it's 36 permute
291:10 - five
291:11 - so the total number of possible
291:12 - passwords is just the sum of those
291:16 - which is about
291:17 - 46.7 million possibilities
291:27 - to find the probability of a randomly
291:29 - chosen password would include only the
291:30 - four letters from Jack's name
291:36 - find the uh
291:37 - we got we use the number from part A is
291:39 - the denominator that's the N of s that's
291:43 - our sample space
291:44 - I'm going to find the numerator the
291:46 - number of ways you can get the event
291:49 - calculate the number of permutations of
291:51 - four from a set of four
291:57 - four is Just 4 factorial which is 24 so
292:02 - the actual probability is that 24
292:03 - divided by the total number of possible
292:07 - passwords
292:08 - so it's rather small
292:12 - now the way that they calculated this
292:18 - is never mind
292:20 - um it's n of e divided by n of s
292:25 - we don't need to know what e and s are
292:27 - except into calculating what n of e and
292:29 - N of s are
292:31 - Tina is packing your suitcase to go on a
292:33 - weekend trip she wants to pack three
292:34 - shirts two pairs of pants and two pairs
292:37 - of shoes
292:39 - she has nine shirts five pants and four
292:41 - pairs of shoes to choose from how many
292:43 - ways contain a pack or suitcase
292:46 - we do need to assume that everything
292:48 - matches
292:49 - which means that we are in the
292:51 - independent Realm
292:54 - this will be nine choose three
292:58 - times five choose two
293:01 - times four choose two
293:04 - because in the nine charts she's
293:05 - choosing three of the five pairs of
293:07 - pants she's using two
293:09 - and of the pairs of four pairs of shoes
293:11 - she's also choosing two
293:17 - so there's the number of ways that she
293:19 - can select her shirts
293:21 - number of ways that she can get her
293:23 - pants and every way she can get her
293:25 - shoes so the total number of ways from
293:28 - the fundamental accounting principle is
293:30 - just that product 84 times 10 times 6.
293:35 - I do want to emphasize the word and here
293:37 - implies multiply
293:39 - it's and being you have to choose this
293:42 - and choose those pants and choose those
293:45 - shoes
293:48 - in probability or indicates adding
293:51 - and indicates multiplying
293:59 - here's an interesting little drawing
294:00 - that might help see this shirts pants
294:03 - shoes they're independent of each other
294:05 - because we've specified everything
294:06 - matches
294:08 - there's 84 ways of getting her shirts 10
294:12 - of her pants six of her shoes
294:14 - so by the fundamental accounting
294:15 - principle it's just the product of those
294:22 - an elementary school principal is
294:24 - putting together a committee of six
294:25 - teachers a committee of six teachers at
294:28 - the Springs Festival
294:30 - there are eight first grade
294:33 - nine second grade and seven third grade
294:37 - teachers at the school
294:39 - how many ways can the committee be
294:41 - formed
294:43 - notice we're not specifying that any
294:45 - that the committee has to contain a
294:47 - certain number of any of the grades
294:50 - there's eight 17 24 t-shirts held
294:54 - together so the number of ways the
294:55 - committee can be formed is just 24
294:57 - choose six
295:00 - how many ways can the committee be
295:01 - formed if there must be two teachers
295:03 - chosen from each grade
295:06 - it's going to be eight choose two times
295:07 - nine choose two times seven choose two
295:09 - for the same reason of Tina's suitcase
295:14 - and suppose the committee's chosen at
295:16 - random with no restrictions so we're in
295:19 - the case of a what's the probability
295:21 - that two teachers from each grade are
295:23 - represented
295:24 - so the denominator the bottom is going
295:27 - to be a and the top is going to be B
295:31 - because a is the number of possible
295:34 - outcomes and B is going to be the number
295:36 - of possible outcomes in the events
295:44 - 24 choose six
295:49 - eight choose two
295:52 - times nine choose two
295:55 - times seven choose two
295:59 - and then the probability will be just be
296:01 - this 21 168.
296:04 - divided by the total number n of s of
296:07 - 134 596.
296:10 - so even if you're not focusing on
296:14 - making sure that the committee consists
296:16 - of two first grader teachers two second
296:18 - grade teachers to third grade teachers
296:20 - there's still a 15.73 chance that it's
296:24 - going to happen naturally just out of
296:26 - randomness
296:31 - and that's it notice how this this
296:34 - section took all of our combinations and
296:37 - permutation stuff and used it to
296:41 - calculate probabilities of events and it
296:43 - comes back to n of e divided by n of s
296:47 - of course we are assuming Independence
296:49 - and all of those outcomes are equally
296:51 - likely but it's n of e Over N of s n of
296:55 - e Over N of s
296:57 - and that's it
296:58 - the end of chapter four
297:01 - the next lecture will be on specific
297:03 - probability distributions and things
297:05 - that we can do with discrete
297:06 - distributions so stay tuned
297:10 - hello and welcome to section five one
297:12 - this is the chapter or this section
297:14 - introduces the chapter that deals with
297:16 - discrete distributions
297:18 - this chapter is going to cover some
297:20 - named and therefore important discrete
297:22 - distributions this section is just
297:25 - looking at some basics of discrete
297:28 - distributions so by the end of the
297:30 - selection you should be able to
297:31 - understand the difference between
297:32 - discrete and continuous distributions or
297:35 - discrete and continuous random variables
297:38 - chapter 6 will deal with the continuous
297:40 - ones
297:41 - to know the purpose of the probability
297:44 - Mass function
297:46 - explain the three requirements for a
297:48 - function to be a probability Mass
297:50 - function that's that's an important one
297:52 - calculate probabilities using the
297:54 - probability Mass function determine
297:56 - sample space sample space also very
297:58 - important because it helps give you a
298:00 - better understanding of which
298:01 - distributions the data could actually
298:03 - come from
298:04 - and then calculate the expected value
298:06 - and variance of a distribution
298:08 - expected value and standard deviation
298:10 - will be the important ones the standard
298:11 - deviation is just the square root of the
298:13 - variance
298:15 - so let's start with the definition of
298:17 - what a random variable is
298:19 - a random variable is a variable whose
298:22 - numeric value is determined by the
298:24 - outcome of a probability experiment you
298:27 - can think of this as an outcome
298:29 - from the future
298:32 - some examples statistician's favorite
298:34 - flavor of ice cream you don't know that
298:37 - until you actually measure it
298:39 - ask the statistician a student's level
298:43 - of approval of a congressional decision
298:45 - you don't know that until you measure it
298:48 - AKA ask the student the Euro Knox
298:51 - college professor is born you don't know
298:53 - that until you ask until you perform
298:54 - that experiment so all of these are
298:56 - random variables
298:57 - Iran variables have or they follow
299:00 - probability distributions
299:02 - it's this fact that they follow
299:04 - probability distributions that allows us
299:06 - to understand the randomness of a random
299:08 - variable
299:10 - and I I want to say that just because
299:12 - it's random doesn't mean we don't know
299:15 - anything about the possible outcomes
299:17 - in fact we know
299:19 - a lot about those possible outcomes we
299:22 - don't know what the exact outcome is
299:24 - going to be
299:25 - but we know everything about it we know
299:27 - the expected value we know the
299:29 - uncertainty in that estimate we know the
299:32 - medians we know probabilities of
299:33 - specific outcomes so we know everything
299:36 - about that future outcome except for
299:38 - what that future outcome is going to be
299:42 - um these are the three requirements for
299:44 - a function to be a pmf
299:47 - all the probabilities have to be between
299:48 - zero and one this is true in general
299:51 - probabilities can't be greater than one
299:54 - can't be less than zero
299:57 - the sum of the probabilities of the
299:59 - sample space is one
300:02 - um
300:03 - cursive S is called the sample space
300:05 - it's the set of all values of x
300:07 - that can happen that have a non-zero
300:10 - probability of happening
300:12 - um
300:13 - so this notation this is summation over
300:16 - all the values of X that are in the
300:18 - sample space
300:20 - of that probability of the value
300:23 - the little X is going to be the value
300:25 - the big X is just going to be big X's
300:27 - these are random variables
300:30 - sometimes we'll have Big Y for a
300:32 - different random variable
300:34 - so the sum over the entire sample space
300:36 - of the probabilities will be one
300:39 - and the probability of a union is no
300:41 - more than the sum of the individual
300:42 - probabilities
300:44 - this comes from chapter four so the
300:48 - probability of a union B is less than or
300:49 - equal to probability of a plus the
300:51 - probability of B for events A and B
300:54 - they are equal if the intersection of A
300:57 - and B is empty
300:59 - it's less than if the intersection is
301:01 - not empty
301:05 - so let's create a probability Mass
301:08 - function for this experiment
301:12 - flip a coin three times
301:14 - count the number of heads flipped
301:17 - so the outcome or the random variable is
301:20 - going to be the number of heads flipped
301:21 - in those three coins
301:24 - from this we know that the sample space
301:26 - is going to be 0 1 2 and 3.
301:31 - how do we know that it's 0 1 2 and 3
301:34 - well we're flipping the coin three times
301:36 - so the largest number of heads we can
301:38 - get is going to be three hits
301:40 - and the smallest will be zero and
301:42 - they're going to be counts this is
301:44 - discrete so we can expect it to be some
301:46 - sort of counts
301:48 - so the sample space is the set 0 1 2 3.
301:55 - second step is to determine the
301:56 - probability of each of those four
301:58 - outcomes
302:00 - we're going to rely on two assumptions I
302:02 - mean three the coin has two sides
302:05 - that'll be one two the coin is fair and
302:07 - the flips are independent
302:10 - coin is fair indicates that the
302:12 - probability of a hit is one half
302:14 - flips are independent means that the
302:16 - outcome of one flip is not going to
302:18 - influence the outcome of another
302:23 - so if these are true there are eight
302:25 - possible outcomes of the flips
302:29 - not of the random variable but of the
302:31 - flips you can get three tails
302:34 - and get Telltale head tail head tail
302:36 - head tail tail
302:37 - you can get head tail head tail head
302:39 - tail head head
302:41 - or you can get three hits
302:45 - the random variable is the number of
302:47 - heads so if you get Telltale tail you
302:50 - have zero
302:51 - if you get hit head you get three
302:55 - if you get tail head tail you get one
303:00 - probability of getting three tails is
303:02 - one half times one-half times one-half
303:05 - which is 1 8.
303:08 - so the probability of the outcome 0 is 1
303:10 - 8.
303:12 - similarly the probability of the outcome
303:14 - three
303:16 - is 1 8 because the only way to get three
303:18 - is head head one-half times one half
303:21 - times one-half
303:27 - these three outcomes from the flips give
303:30 - you the same random variable value of
303:33 - one because they all have one head
303:36 - probability of tail tail head is 1 8.
303:40 - probability of tail head tail is 1 8
303:42 - probability of head tail tail is 1 8. so
303:45 - the probability of getting one head is 1
303:47 - 8 plus 1 8 plus 1 8.
303:53 - probability of getting two tails I'm
303:55 - sorry of getting two heads is just head
303:58 - head tail head tail head tattoo head
304:01 - 1 8 plus 1 8 plus 1 8 3 8. in other
304:06 - words there's three ways
304:08 - of arranging these outcomes so you get
304:10 - two heads
304:13 - times
304:15 - one half times one-half times one-half
304:21 - here's another way that we can represent
304:23 - this this will be graphically
304:27 - sample space is listed along the bottom
304:29 - the height of the bar corresponds to the
304:32 - probability of that specific outcome
304:38 - we could represent the probability Mass
304:40 - function in this way as well
304:44 - probability of the random variable
304:45 - equaling some value some specified value
304:50 - is 0.125 if this x is zero or three
304:55 - it's 0.375 if this little X is one or
304:57 - two
304:59 - and at zero otherwise
305:03 - notice this formula is also not unique
305:05 - we could also represent it as 3 choose x
305:08 - times 0.125
305:15 - and keep this in mind when we get to the
305:17 - binomial distribution which will be in
305:19 - the next section
305:22 - remember that population parameter is a
305:24 - function of the population
305:26 - contrast this with a statistic being a
305:29 - sample of the data
305:31 - we want to use those statistics estimate
305:33 - the population usually and we'll do that
305:35 - in the second half of the course
305:37 - here we're going to look at ways of
305:39 - calculating those population parameters
305:41 - when we know what the distribution is
305:44 - some population parameters that we care
305:46 - most about will be the mean
305:48 - the standard deviation which is the
305:50 - square root of the variance and the
305:52 - median
305:57 - the definition of the expected value
306:01 - or the mean of a discrete random
306:04 - variable
306:06 - so the expected value of x where X is
306:09 - our random variable is just the sum over
306:12 - all values of X in the sample space
306:16 - of the quantity x times its probability
306:25 - recall from chapter 3 mu which is the
306:28 - population mean
306:30 - when we're dealing with distributions we
306:32 - can also use expected value of x just
306:34 - equal to the sum
306:35 - of x times 1 over n of x times well
306:39 - if each outcome in the population is
306:42 - equally likely to be observed then the
306:45 - probability of observing that person is
306:47 - just one over n
306:50 - and I suppose that should be a capital N
306:53 - expected value is just a long run
306:55 - average of those outcomes
307:02 - variance of a distribution is a measure
307:04 - of uncertainty in each outcome
307:06 - it's the opposite of the word precision
307:10 - I like the word uncertainty
307:14 - the variance of discrete random variable
307:16 - is given by that's a v of x
307:20 - we could also use Sigma squared
307:23 - just the sum over all the x's in the
307:25 - sample space
307:26 - of the quantity x minus mu squared
307:30 - times the probability of that outcome
307:34 - population variance remember was the
307:37 - same thing except one over n
307:40 - and this is perfect if every outcome is
307:43 - equally likely
307:44 - here we're dealing with unequally likely
307:48 - outcomes
307:52 - here's the wonderful definition the
307:54 - median
307:55 - notice that this explains why a hand
307:57 - waved saying it's uh it's about half is
307:59 - below and about half is above because
308:02 - dealing with the actual definition the
308:03 - median gives me a headache sometimes
308:08 - um
308:09 - when the distribution is continuous
308:13 - and we can just deal with the
308:14 - probability of X being less than or
308:15 - equal to the median that's tilde on top
308:18 - utilities are located to the left of the
308:20 - one as is just equal to 0.5
308:27 - note that this implies that for a
308:29 - discrete distribution the median is not
308:31 - necessarily unique
308:33 - whereas for continuous distributions it
308:36 - will be
308:38 - here's an example
308:40 - three coins
308:42 - flipping through a coin three times the
308:44 - probability Mass function remember we
308:46 - came up with was this
308:49 - let's calculate the mean the variance
308:50 - and the median
308:52 - from definitions
308:57 - expected value of x is just the sum over
309:00 - all X that are in the sample space
309:03 - of the value times its probability
309:08 - there are four things in the sample
309:10 - space zero one two and three
309:14 - so it's zero times the probability of
309:16 - zero plus one times the probability of a
309:19 - one plus two times the probability of a
309:22 - two plus three times the probability of
309:25 - a three
309:26 - which gives us 1.5
309:29 - so the expected number of heads on three
309:31 - flips of a fair coin is 1.5
309:34 - doesn't surprise us
309:39 - variance
309:41 - same thing
309:42 - adding up overall X is in the sample
309:44 - space of the value minus the mean
309:47 - squared times the probability of the
309:48 - value
309:50 - so zero minus the mean squared times the
309:52 - probability of a zero
309:55 - plus one minus the mean squared times
309:56 - the probability of a one
309:59 - plus two minus the mean squared times
310:01 - the probability of a two
310:04 - plus three minus the mean squared times
310:06 - the probability of a three
310:08 - add them all up you get 0.75
310:11 - standard deviation therefore is 0.866
310:20 - that's the definition of the median
310:24 - I think the best way of using it is
310:26 - using cumulative sums of the
310:28 - probabilities
310:30 - so you order it from zero outcomes from
310:33 - lowest to highest
310:35 - start with the lowest outcome
310:36 - probability see if it's greater than or
310:38 - equal to 0.5 if it's not go to the next
310:41 - one and add the probability onto it
310:46 - so here x equals zero probability of
310:49 - that is 0.125 it's not greater than or
310:51 - equal to 0.5 so zero is not a median
310:55 - probability of a one was .375
310:58 - so the cumulative probability at 1 is
311:01 - 0.125 plus 0.375 and that is greater
311:05 - than equal to 0.5 so 1 is a median
311:10 - note that since it comes out exactly
311:12 - equal to 0.5 the next value is also the
311:16 - median 2 is also a median and every
311:19 - number between one and two so 1.5 is a
311:23 - median 1.6 is a median
311:29 - here's the r code the X and P lines are
311:32 - going to be common to all the
311:33 - calculations it's the last one that
311:35 - shows you how to perform the calculation
311:37 - to get the the parameter
311:40 - so we've seen things like this P line so
311:43 - we're defining the variable p as equal
311:45 - to and we're using the C function to
311:47 - combine all of these values into one
311:49 - vector it's 0.125 0.375.375.125
311:55 - so writing that second line P will now
311:58 - contain those four values
312:00 - the first line this is a new construct
312:03 - for us we're saying X is equal to the
312:05 - values from zero to three
312:08 - that's a colon so this will be 0 1 2 and
312:11 - 3.
312:13 - in some computer languages it'll you'll
312:18 - have instead of a colon three lower dots
312:21 - kind of like this to indicate zero
312:24 - through three but for R it's just a
312:27 - colon
312:29 - and now that we've got the sample space
312:31 - and the probability of each of those
312:33 - outcomes we can calculate the mean is
312:35 - just the sum of all those X's times
312:37 - their probabilities
312:41 - 1.5
312:43 - for the variance
312:45 - is just the sum of the value minus the
312:48 - mean squared
312:50 - this is a carrot
312:52 - this is the thing on top of the six
312:55 - it's read as quote to the power of
312:59 - so this carrot 2 will mean to the power
313:03 - of 2 or squared
313:06 - times p
313:07 - added up
313:11 - and the standard deviation is just the
313:13 - square root of that
313:14 - function to take the square root in R is
313:16 - sqrt
313:21 - for the median we use the function
313:23 - cumulative sum or sum
313:26 - and we look for the first place where it
313:29 - gets to be 0.5 or greater
313:32 - and that first place remember the sample
313:35 - space is 0 1 2 3 0 1 2 3 0 1 so 1 is a
313:40 - median
313:43 - remember since this is exactly 0.5 then
313:47 - not only is one a median but so is 2 and
313:50 - every number between one and two
313:57 - example two ice hockey my stat 225
314:00 - course we did a project where the
314:02 - students predicted the outcome of a
314:03 - future ice hockey game between the
314:06 - Portland Winterhawks and the Prince
314:08 - George cougars go Hawks
314:12 - I took all of their probabilities and
314:15 - averaged them for the entire class
314:18 - for the number of points scored by the
314:20 - Winterhawks so the class as a whole said
314:23 - the probability of them scoring zero
314:25 - points was 0.1
314:27 - point was 0.1 two points is 0.2 3.6.4
314:32 - [Music]
314:33 - and four points and five points or point
314:35 - one each
314:38 - so let's calculate the expected number
314:40 - of goals the uncertainty of that aka the
314:42 - variance and the median
314:47 - so here's a graph of that
314:52 - using the formula for the expected value
314:54 - we're adding up over all the x's in the
314:57 - sample space
314:58 - sample space zero one two three four
315:00 - five of the value times its probability
315:05 - and we get 2.6 so the expected number of
315:08 - goals to be made by the winter hawks is
315:10 - 2.6
315:12 - you do not round that to the nearest
315:14 - integer
315:15 - the expected value is a long run average
315:19 - 's don't have to be in the sample space
315:23 - so the expected value is 2.6 it's not
315:26 - two it's not 3 it's 2.6
315:33 - variance going to use the formula again
315:37 - plug and chug
315:39 - get 1.84
315:43 - standard deviation therefore is 1.356
315:48 - we could use the standard deviation with
315:51 - the empirical rule
315:53 - say that the probability of the
315:56 - Winterhawk scoring between 1.2 and 3.9
315:58 - is about 68 percent
316:02 - we can actually calculate it exactly
316:05 - it's 60 percent
316:11 - the median
316:15 - start with zero
316:17 - probability is 0.1
316:19 - that's not at least 0.5
316:22 - add the probability of a one so we're at
316:25 - point two now that's not at least 0.5
316:29 - I have the probability of two goals
316:32 - that's 0.4 that's not at least 0.5
316:36 - add the probability of three goals
316:38 - a that comes up to be point a
316:42 - that is greater than or equal to 0.5 so
316:45 - 3 is a median
316:48 - in fact since the probability of three
316:50 - or less is greater than 0.5 we know that
316:54 - 3 is only is I'm sorry that 3 is the
316:57 - only median
317:01 - if these had added up to exactly 0.5
317:03 - we'd know that 3 and 4 were both medians
317:06 - as well as everything between three and
317:08 - four
317:09 - but
317:10 - since it came out to be something
317:12 - greater than 0.5 we know that 3 is the
317:15 - only median
317:20 - we can do it in R quickly
317:23 - X Line This is the value 0 through 5.
317:27 - again there's that colon notation
317:29 - so run that first line X is now the
317:32 - vector 0 1 2 3 4 5.
317:35 - p
317:37 - this combined function the C function of
317:41 - 0.1.1.2.4.1.4
317:44 - and the mean is just the sum of the X's
317:46 - times their probabilities
317:49 - 2.6
317:52 - the variance is just the sum of the x
317:56 - minus mu squared times P's
318:00 - the standard deviation is just the
318:02 - square of that variance
318:08 - and the sum function r
318:12 - that's Q sum Q sum for x equals zero Q
318:17 - sum for x equals one for x equals two
318:20 - for x equals three boom x equals three
318:24 - is the median
318:29 - can sum is the cumulative sum of
318:32 - those probabilities
318:35 - so these values are probability of X
318:38 - being less than or equal to
318:40 - zero one two three four five
318:48 - so here's a summary of what we did today
318:49 - in this introduction to discrete
318:51 - distributions
318:53 - we looked at probability Mass functions
318:55 - and saw how they describe the
318:56 - probabilities of each outcome in the
318:58 - sample space
319:02 - we saw probabilities can be calculated
319:03 - by adding the individual probabilities
319:06 - this reflects chapter 4 very clearly
319:10 - the expected value is a weighted sum of
319:12 - the outcomes
319:15 - weighted by the probabilities
319:18 - expected value and mean are synonymous
319:23 - the variance is weight some of the
319:25 - distances between the outcomes and the
319:27 - mean
319:29 - again weighted by that probability
319:32 - and the median is the value again not
319:34 - necessarily in the sample space
319:36 - so it's just that at least half is less
319:38 - than or equal to it and at least half is
319:40 - greater than or equal to it
319:45 - in the future we're going to look at
319:46 - some named discrete distributions
319:49 - Bernoulli binomial on the next poisson
319:52 - later hypergeometric later
319:56 - I misspelled hypergeometric I guess
319:59 - we're going to use R to calculate those
320:00 - probabilities expected values if we wish
320:03 - it to
320:04 - find many uses for these probability
320:06 - distributions and modeling the real
320:07 - world events around us
320:10 - and then when we get to chapter six
320:11 - we'll repeat this for some continuous
320:13 - distributions
320:15 - and we're going to continue
320:16 - understanding how the data generating
320:19 - process helps us to better estimate the
320:22 - parameters
320:25 - the readings will be section 5-1 at
320:27 - Hawks
320:28 - appendix A1 and R for starters
320:33 - here are the intellecture questions not
320:35 - entirely intra lecture but
320:38 - we'll pretend
320:40 - question one what is a random variable
320:41 - again I would suggest in your notes
320:43 - write the question on the left hand side
320:45 - and answer it so that you can transfer
320:48 - that into moodle's quizzes
320:50 - what is a random variable
320:53 - question two is what is a sample space
320:57 - and question three is what is the
321:00 - difference between an expected value and
321:02 - a mean
321:06 - and that's it
321:09 - hello and welcome to section five two
321:11 - the binomial distribution
321:13 - in this lecture we're going to learn
321:16 - about two named distributions the
321:19 - Bernoulli distribution and the binomial
321:22 - you'll see that the Bernoulli
321:23 - distribution is just a special case of
321:25 - the binomial but the Bernoulli
321:27 - distribution does make a nice entrance
321:29 - into the binomial so here are today's
321:32 - objective again calculate expected value
321:34 - variance median probabilities associated
321:37 - with a Bernoulli random variable we're
321:39 - going to determine what random variable
321:40 - follows a Bernoulli binomial
321:43 - distribution using its definition and
321:45 - you do want to memorize the definition
321:46 - of a binomial random variable
321:49 - calculate the mean the variance of
321:51 - binomial determine if Bernoulli and
321:52 - binomial distributions are skewed and
321:55 - calculate probabilities from a binomial
321:57 - distribution
321:58 - and we're going to do a lot of that in R
322:00 - just to show you how easy it is to do so
322:03 - definition of a Bernoulli experiment
322:06 - a random variable with only two possible
322:08 - outcomes follows a Bernoulli
322:10 - distribution and we're going to define
322:12 - the success probability p
322:16 - so any random variable that has two
322:18 - possible outcomes is going to be a
322:21 - Bernoulli random variable
322:22 - some examples heads on a single flip the
322:25 - coin a correct choice and a true false
322:27 - question me stopping at Starbucks in the
322:30 - morning
322:31 - in a given single morning
322:34 - each of those two possible outcomes a
322:37 - success or a failure either
322:39 - I get ahead I get a correct choice and a
322:42 - true false I do stop at Starbucks or a
322:45 - failure
322:47 - the probability Mass function for the
322:50 - Bernoulli note that the sample space is
322:52 - 0 1
322:55 - so and P is defined as the success
322:58 - probability
323:00 - so probability of X equaling 1 in other
323:04 - words having a success is p
323:07 - and we know from our rules the
323:08 - probability that if there's only two
323:10 - possible outcomes and the probability of
323:12 - one event is p the probability of the
323:14 - other is going to be one minus P the
323:16 - complement of it
323:18 - so the probability of a failure is one
323:20 - minus p
323:24 - um
323:24 - for reasons that will become clear when
323:26 - we get to the binomial distribution this
323:29 - can also be written as P to the power of
323:30 - X and one minus P to the power of one
323:33 - minus X
323:38 - let's calculate expected value and
323:40 - variance for Bernoulli's in general
323:43 - remember the definition the expected
323:44 - value is just the sum over all the X
323:47 - values in the sample space
323:49 - the quantity x times the probability of
323:51 - that X
323:52 - sample space is just 0 1 recall
323:55 - so this is zero times the probability of
323:58 - zero plus one times the probability of a
324:00 - one
324:02 - probability of a failure is one minus p
324:05 - probability of a success is p
324:08 - the expected value is p
324:12 - remember expected value just the long
324:15 - run average
324:17 - if my Bernoulli event is flipping a coin
324:21 - getting heads
324:22 - as a Fair coin then we would expect that
324:25 - I flipped that coin 10 billion times
324:28 - half of them will be ahead
324:31 - I'll get successes on half of them and
324:34 - that's what this expected value of x
324:36 - equal and P tells you
324:38 - similarly we can calculate the variance
324:40 - again it's sum over all the x's in the
324:42 - sample space if x minus mean
324:45 - squared times the probability of that X
324:48 - 0 minus p squared times the probability
324:50 - of a zero plus one minus p squared times
324:53 - the probability of a p
324:55 - algebra shows us that this is just P
324:57 - times 1 minus p
324:59 - you may see this is p times q and some
325:01 - sources
325:03 - Q would be the failure probability but
325:05 - let's keep it as 1 minus p
325:10 - so here's the pmf
325:13 - let's look at the median
325:16 - CDF function is what we're going to use
325:18 - for the calculator the median or call
325:20 - the CDF functions find as probability of
325:22 - the random variable being less than or
325:24 - equal to
325:26 - some value
325:28 - the cumulative part is the less than or
325:31 - equal to contrast that with the equals
325:33 - part
325:34 - cumulative will be for the less than or
325:36 - equal to CDF of a Bernoulli
325:40 - is zero when X is less than or equal to
325:42 - zero once you get to zero however once
325:44 - you get to that failure
325:46 - it hops up to 1 minus p
325:48 - stays at 1 minus P until you get to one
325:51 - then it's 1 minus P plus p
325:54 - from then on
325:56 - here's a
325:58 - that helps us calculate the median
326:01 - here's the CDF
326:03 - and we need to determine when that CDF
326:04 - is at least five at least 0.5 for the
326:08 - first time
326:13 - so the median is zero
326:16 - if success probability is less than a
326:19 - half it's one it's greater than a half
326:20 - and it's every number between 0 and 1
326:23 - inclusive if p is equal to one-half
326:27 - now the next two slides will show that
326:29 - this actually makes sense so here's the
326:31 - CDF
326:33 - for the Bernoulli when the success
326:35 - probability p is larger than one half
326:40 - this is a CDF because the vertical axis
326:42 - is the cumulative probability
326:46 - it's zero zero zero until you get to one
326:49 - to get to zero and then it pops up to
326:51 - one minus p
326:53 - stays at 1 minus P until you get to one
326:56 - that hops up to one
326:58 - so where is the first time that the
327:00 - cumulative probability is at least 0.5
327:05 - right there one
327:07 - so the median when p is greater than 0.5
327:10 - is 1.
327:13 - now when success probability is small
327:16 - when it's less than a half
327:18 - cdf000 pop-up to 1 minus P ooh we just
327:23 - popped up Beyond point five so zero is
327:26 - going to be our median
327:30 - so in other words when the success
327:32 - probability is
327:34 - large
327:36 - the median is one when it's small
327:38 - meeting zero
327:40 - make sure that that makes sense to you
327:42 - that should be very clear
327:46 - going from the Bernoulli to the binomial
327:51 - binomial random variable is defined
327:53 - statistically defined as the sum
327:56 - of n
327:58 - independent and identically distributed
328:00 - for newly random variables that's the
328:03 - definition of the binomial
328:05 - that actually comes down to this
328:08 - definition of five requirements
328:10 - and this you'll want to memorize
328:12 - by knowing random variable meets the
328:14 - following five requirements the number
328:15 - of Trials is known
328:18 - so we know what n is
328:21 - each trial has two possible outcomes
328:23 - that is each trial is a Bernoulli random
328:26 - variable itself
328:28 - the success probability p is constant
328:30 - from trial to trial
328:33 - identically distributed note that we
328:36 - don't have to know what p is we just
328:37 - have to know that it's constant
328:39 - for the trials are independent well it's
328:42 - up here
328:43 - trials are independent and the random
328:45 - variable or what we're measuring is the
328:48 - number of successes
328:50 - notice that one and five together tells
328:53 - us that sample space is 0 1 2 3 all the
328:56 - way up to n
329:01 - here's some examples no C's reflect the
329:03 - examples we had with the Bernoulli
329:06 - example uh so it's the times getting
329:09 - heads on N flips of a coin
329:12 - for Bernoulli it was just one flip of a
329:14 - coin times getting a six on N rolls of
329:17 - dice for Bernoulli it was one roll of a
329:19 - die
329:20 - time stopping at Starbucks and N
329:22 - mornings for the Bernoulli it was just
329:24 - one morning
329:25 - so again notice that the binomial random
329:28 - variable is just a generalization of the
329:31 - Bernoulli
329:32 - or you can think of the Bernoulli just
329:34 - as being a simplification or a special
329:36 - case of the binomial
329:41 - here's the probability Mass function for
329:43 - the binomial
329:44 - I like this version of it's in three
329:46 - parts
329:47 - this P to the power of X is the
329:50 - probability of getting X successes
329:53 - the 1 minus p
329:55 - to the power of n minus X is the
329:57 - probability of getting those n minus X
329:59 - failures
330:01 - and then the combinations part
330:03 - is just the number of ways that you can
330:06 - arrange those X successes in the end
330:08 - trials
330:09 - if we think back to section five one
330:11 - with the flipping the coin three times
330:13 - it was this combinations part that gave
330:16 - us the one the three the three and the
330:18 - one
330:20 - whereas the 1 8 part was just these two
330:24 - together
330:29 - binomial parameters
330:31 - the expected value of binomial is easily
330:34 - calculated from its statistical
330:36 - definition
330:37 - so this is the statistical definition
330:39 - the binomial
330:41 - let x sub I be a Bernoulli random
330:44 - variable
330:46 - define y as the sum of those X's
330:52 - this leads us to Y being a binomial
330:56 - with parameters n
330:58 - the number of bernoullis added together
331:01 - and P the success probability for each
331:04 - of those bernoullis notice there's no
331:07 - subscript on the P so this p is constant
331:09 - for all of those Bernoulli's
331:13 - here's the expected value
331:16 - expected value of y well we just
331:18 - substituted and what Y is this is y so
331:20 - we substitute it in
331:23 - we know that the expected value of a sum
331:25 - is just the sum of the expected values
331:30 - so we can switch the expected value and
331:32 - the summation
331:35 - we know the expected value of x sub I
331:37 - expect value for Bernoulli is just p
331:41 - and so we're n adding up the value p n
331:46 - times
331:47 - which is just n times p
331:50 - so the expected value of a binomial is
331:53 - just n times p
331:54 - as always stop and make sure that this
331:56 - works makes sense think of this in terms
331:59 - of flipping coins
332:04 - variance we can do the same thing
332:06 - variance of Y just substitute in what Y
332:08 - is
332:10 - the variance in the summation chain can
332:12 - switch positions because the X's are
332:15 - independent
332:16 - the variance of a Bernoulli is just P
332:19 - times 1 minus p
332:21 - we're adding up P times 1 minus p n
332:23 - times so there's our variance
332:28 - and a mathematical note this step
332:31 - requires Independence of the Bernoulli
332:35 - experiments
332:37 - which is a requirement of binomial so it
332:40 - works
332:44 - we'll do an example
332:47 - um I'll let you do the other four
332:49 - examples on your own
332:52 - and I really do suggest you do those
332:54 - work through them make sure they all
332:57 - make sense do them by hand and do them
333:00 - in r
333:02 - um so example one Fair coins let X be
333:04 - the number of heads flipped in 10 flips
333:06 - of a coin
333:08 - if the coin is fair then it's clear that
333:10 - X follows a binomial distribution
333:13 - with 10 trials and success probability
333:16 - of one-half
333:21 - X is distributed as a binomial or X
333:24 - follows a binomial
333:26 - let's calculate these three things the
333:28 - probability of getting three heads
333:31 - probability getting three tails the
333:33 - probability of getting at most three
333:34 - heads or at least seven hits
333:38 - otherwise known as at least sorry at
333:41 - most three heads or at most three tails
333:48 - probability of getting three heads we
333:50 - want to calculate the probability that X
333:52 - is three
333:54 - this is a simple application of the
333:56 - probability Mass function here's the pmf
334:00 - we're given Little X is three n is 10 p
334:03 - is one half
334:05 - plug in
334:07 - chug
334:08 - get 0.1171875
334:12 - 10 choose 3 recall from chapter four
334:15 - it's just 10 factorial divided by 3
334:17 - factorial divided by seven factorial
334:21 - which gives you
334:22 - 120.
334:28 - in R that's this one line
334:31 - notice three four things about I guess
334:35 - we're talking about a binomial
334:37 - distribution so the stem is binom
334:42 - we're asked to calculate the probability
334:43 - that X is equal to a value so that's
334:46 - what the prefix will be a d
334:50 - we're asked to calculate the probability
334:52 - X is equal to three
334:54 - so the first number is three
334:57 - then everything else defines that
334:59 - particular binomial distribution
335:01 - there are ten trials so size is equal to
335:04 - 10.
335:06 - and each trial has a probability of
335:08 - success of 0.5 so prob is 0.5
335:17 - probability of getting exactly three
335:20 - tails
335:22 - if you get three tails
335:24 - we've got two options notice that this x
335:26 - is the number of heads flipped
335:29 - so we could redefine a new random
335:32 - variable for the distribution of tails
335:35 - or we can just note that if it's not a
335:37 - head it's a tail so the probability of
335:39 - three Tails suggests the probability of
335:40 - heads being seven
335:46 - again this simple application the
335:47 - probability Mass function plug and chug
335:50 - Little X is seven n is 10 p is one half
335:54 - probability is 0.1171875
336:01 - before I go to the next slide what would
336:03 - this be an R
336:08 - binom because it's the binomial D
336:11 - because we're asked to calculate the
336:13 - probability that X is equal to something
336:16 - because we're asked to calculate
336:18 - probability X is equal to seven
336:20 - ten trials success probability of 0.5
336:27 - this is a lot faster than this
336:33 - because I don't know 10 choose 7 in my
336:34 - head I'd have to use a calculator to do
336:36 - that
336:39 - finally we need to calculate the
336:41 - probability of getting at most three
336:42 - heads
336:44 - or at most three tails
336:49 - and most three heads are at least seven
336:52 - heads
336:54 - using the notation from chapter four
336:55 - this is probability of X being less than
336:57 - or equal to three Union
336:59 - X greater than equal to seven
337:01 - since there's no overlap between this
337:03 - event and this event
337:05 - probability of the Union just assuming
337:07 - the individual probabilities
337:11 - so to do this by hand we need to
337:13 - calculate the probability of X being
337:14 - less than or equal to three and add it
337:15 - to the probability of X greater than or
337:17 - equal to seven
337:18 - X less than or equal to three as X is
337:20 - zero x equals one x equals two x equals
337:23 - three
337:24 - and the event X greater than seven is
337:26 - just seven eight nine and ten
337:28 - have to calculate all those individual
337:30 - probabilities add them together and we
337:33 - get the answer
337:36 - if you do it by hand there's no
337:37 - shortcuts on this no special formula
337:41 - it's just you got to use this formula
337:46 - properly changed for each of these
337:50 - now if we want to do this in r
337:53 - it's pretty straightforward you would
337:56 - use the CDF function in r
338:00 - CDF remember is always less than or
338:03 - equal to
338:04 - so we've got to change our less than or
338:06 - equal to three Union greater than or
338:08 - equal to seven into just two less than
338:10 - or equal to's
338:12 - from chapter four we got the complements
338:14 - rule
338:16 - the probability of X being greater than
338:18 - or equal to seven is one minus the
338:19 - probability of X less than or equal to
338:20 - six
338:23 - which means the probability that we need
338:25 - to calculate is just part of X less than
338:27 - or equal to three plus this one minus
338:29 - probability of X less than or equal to
338:31 - six
338:34 - notice now all of our probabilities are
338:36 - cumulative they're all less than or
338:38 - equal to
338:41 - which makes things rather easy
338:43 - in R this would be P binom three
338:47 - and then the definition the binom
338:49 - plus one minus P binom six
338:54 - plus the
338:55 - trials and success probability
338:59 - notice we went from D binomial to p
339:02 - binom
339:04 - P is for cumulative probabilities D is
339:07 - for what I call Point probabilities
339:10 - P is for less than or equal D is for
339:13 - equal
339:19 - the interstate word problem
339:22 - remember I'm not going to do these I
339:25 - will encourage you to do these yourself
339:27 - I will require them as much as possible
339:34 - really not too much to say on these
339:37 - they're more activities for you
339:40 - illustrating calculations
339:45 - some examples from the life of my or
339:48 - time for my life as a
339:51 - consultant
339:54 - so here's what we learned from this
339:55 - slice deck definition the Bernoulli
339:58 - definition of the binomial
340:00 - expected value of a Bernoulli and a
340:02 - binomial distribution
340:04 - the variance of both of those
340:06 - applications a lot of examples got five
340:09 - examples again you should go through
340:11 - those how to use R to calculate the
340:14 - point probabilities the equals
340:16 - probabilities
340:17 - cumulative probability is less than or
340:19 - equal to
340:22 - for the equals probabilities you use d
340:26 - for the less than or equal to use p
340:31 - you'll want to download the all
340:32 - probabilities file
340:38 - we're going to do the same thing with
340:39 - the poisson in the future
340:42 - we're going to see that these
340:43 - distributions do describe real world
340:45 - events
340:47 - we're going to repeat all this with
340:48 - continuous distributions
340:51 - and we're going to realize that
340:53 - understanding the distribution of the
340:54 - data helps us to describe the underlying
340:57 - process better
340:59 - and that will allow us to estimate
341:01 - parameters of Interest
341:06 - covered two of these R functions in this
341:09 - slide deck but I'm going to give you
341:10 - four of them we looked at the D and the
341:13 - P versions
341:16 - the D for the equal the P for the less
341:18 - than or equal to both of these calculate
341:19 - probabilities
341:21 - you've seen the r version before
341:24 - you saw that in a lab
341:26 - R generates a random sample from that
341:30 - distribution
341:34 - so D and P give probabilities R gives a
341:37 - sample from the distribution and Q will
341:40 - give you the quantile or the percentile
341:43 - this will be useful for calculating
341:46 - calculating medians
341:48 - first quartiles
341:50 - 97.5 percentiles
341:52 - what have you so the Q will calculate a
341:55 - data value an X corresponding to a given
341:58 - probability
342:02 - and notice if you want to do Bernoulli
342:04 - random variables just make sure size is
342:07 - equal to one
342:09 - because really that's the only
342:10 - difference between a binomial and a
342:12 - Bernoulli
342:14 - R for starters appendix A2 and A3 will
342:17 - help
342:18 - Wikipedia's got a couple good pages on
342:21 - the Bernoulli and the binomial
342:24 - and so here
342:26 - are the intellectual questions not so
342:29 - intro lecture for this
342:32 - section but they're here
342:35 - again on the left hand side write the
342:37 - question underneath it write your answer
342:40 - so you can transfer that to your Moodle
342:43 - quiz
342:45 - what are the five requirements of a
342:46 - binomial distribution again you must
342:48 - memorize these
342:52 - what is the difference between a
342:53 - binomial and a Bernoulli
342:58 - and what is the formula for the expected
342:59 - value of a binomial random variable
343:06 - and that's it
343:11 - hello and welcome to section 5.3 the
343:14 - poisson distribution here we introduce a
343:16 - second discrete distribution and I'd
343:18 - like you to keep comparing it to the
343:19 - binomial to see what the similarities
343:21 - and the differences really are so by the
343:24 - end of this lecture you should be able
343:25 - to determine what random variables
343:27 - follow a poisson distribution by using
343:30 - the definition of a poisson
343:32 - calculate the probabilities from a
343:34 - poisson distribution both by hand and by
343:38 - R and calculate the expected value of
343:40 - variance median probabilities associated
343:42 - with that
343:44 - random variable
343:46 - um
343:47 - so here's the definition
343:49 - a random variable that is a count of
343:52 - successes over an area or a time period
343:56 - follows the poisson distribution with an
343:58 - average rate parameter of Lambda
344:02 - contrast this with the binomial
344:04 - distribution which was this count of
344:06 - successes over a certain number of
344:08 - Trials
344:09 - here it's over a time period
344:12 - so examples heads flipped in an hour as
344:15 - opposed to heads flipped in 10 flips
344:17 - which would be binomial here it's heads
344:19 - flipped in an hour
344:20 - number of dents on a car
344:24 - number of errors on a page number of
344:28 - terrorist attacks in a year number of
344:30 - bacteria in a swimming pool influenza
344:32 - cases in a week Wars in a year
344:35 - the binomial would be the number of
344:38 - states
344:40 - or countries at war in a year
344:44 - notice there's an upper bound to that
344:47 - um
344:48 - or binomial would be the the number of
344:51 - people in this class with influenza in a
344:55 - week
344:55 - that also has an upper bound
344:58 - notice in each of those cases you're
345:00 - you're measuring number of successes out
345:03 - of a total population of n
345:07 - bacteria swimming pool would be a
345:09 - poisson
345:11 - a number of swimming pools in Galesburg
345:14 - with bacteria would be binomial
345:19 - notice what I'm getting at here is that
345:22 - for a binomial it's the number of
345:25 - successes out of a number of Trials
345:27 - so the sample space is 0 through n
345:31 - whereas for poisson distribution it's
345:34 - not the number of successes over trials
345:36 - this number of successes in a time
345:38 - period or an area which case there is no
345:40 - upper bound
345:43 - therefore the sample space for poisson
345:45 - is 0 1 2 3 4 dot dot
345:50 - it can be proven AKA is beyond the scope
345:53 - of this course the probability Mass
345:55 - function for poisson is
345:59 - for probably Mass function it's got the
346:01 - equals part random variable is X the
346:04 - value is Little X is equal to e to the
346:07 - power of negative Lambda
346:09 - times Lambda to the power of x
346:12 - all over X factorial
346:15 - this e is the natural log base it's
346:18 - 2.7182
346:20 - Etc
346:21 - Lambda is the average
346:24 - that's the parameter that we have to
346:25 - give it and X is the value that we want
346:28 - to find the probability of
346:34 - it can be shown
346:38 - that the expected value in the variance
346:40 - for a poisson are both Lambda
346:42 - that's kind of cool
346:45 - um
346:47 - we'll go with an example
346:49 - let X be the number of heads flipped in
346:52 - a minute
346:54 - I'm given that I average 24 flips per
346:57 - minute
346:58 - I've noticed I didn't say I am flipping
347:01 - the coin guaranteed 22 times 24 times in
347:04 - this minute I'm saying average 24 flips
347:08 - per minute
347:09 - if the coin is fair then it's clear that
347:11 - X follows a poisson distribution with
347:14 - Lambda equal to 12.
347:20 - probability of getting no heads
347:22 - probability of getting at most 20 heads
347:25 - and the probability of getting at least
347:27 - one head in the first six seconds
347:33 - probability of getting no heads in that
347:34 - minute so we're asking what's probably X
347:36 - is equal to zero
347:37 - we're given Lambda is equal to 12. so
347:40 - it's a plug and chug e to the negative
347:42 - 12 12 to the zero all over zero
347:46 - factorial recall from chapter four zero
347:48 - factorial is one
347:51 - so this is equal to point zero zero zero
347:53 - zero zero six one one four
347:56 - in other words if I actually do this
347:59 - experiment flip the coin for a minute
348:01 - and come up with zero heads then either
348:05 - I don't average 24 flips per minute or
348:08 - the coin is not fair
348:10 - or I should go out and buy a couple
348:12 - lottery tickets
348:13 - because this outcome is extremely rare
348:20 - on R this would be D plus zero Lambda
348:24 - equals 12. again D is for the
348:27 - probability of calculating the
348:28 - probabilities of equals
348:30 - here plus is for the poisson
348:33 - distribution
348:34 - zero we're calculating the probability X
348:37 - is equal to zero and we have to specify
348:39 - that Lambda is 12.
348:43 - because we're given Lambda is 12.
348:47 - part two what's the probability of
348:48 - getting at most 20 heads in that minute
348:50 - so probability of X being less than or
348:53 - equal to 20. of the random variable
348:55 - being less than or equal to 20.
348:58 - this is a quote simple application of
349:01 - the probability Mass function
349:03 - this is equal to the probability x
349:04 - equals zero plus probability x equals
349:06 - one plus the probability of x equals two
349:08 - plus the probability of x equals three
349:10 - okay I'll stop there until we get to 20
349:13 - here's the pmf we're just adding up for
349:15 - all those various values of x
349:17 - from 0 to 20.
349:22 - easy way since this is a less than or
349:24 - equal to is to use the P form
349:27 - again the stem is pois for the poisson
349:30 - or calculate calculating the probability
349:32 - of X being less than or equal to 20. and
349:35 - Lambda is still 12.
349:42 - the probability of getting at least one
349:44 - head in the first six seconds at least
349:46 - notice X was flips heads in a minute we
349:51 - want to do this in SEC six seconds so we
349:54 - have to define a new random variable
349:57 - um six seconds is a tenth of a minute
350:00 - so the expected number of heads is just
350:03 - going to be 12 divided by 10
350:06 - the number of heads in a minute divided
350:08 - by 10 because now we're in terms of six
350:10 - seconds
350:14 - plug and chug
350:18 - y greater than or equal to one so we're
350:21 - I guess this should be a y so we're
350:22 - summing up from Y is equal to one all
350:24 - the way up to Infinity this is going to
350:26 - take a long time if we do it this way
350:29 - but recall the complements rule
350:31 - probability of Y being greater than or
350:33 - equal to one is equal to one minus the
350:35 - probability of Y equaling 0.
350:39 - and this part here is the probability
350:41 - that Y is equal to zero
350:43 - one minus that it's y equals two we'll
350:46 - use a d
350:48 - for the equals and a zero
350:52 - so there's about a seventy percent
350:53 - chance that I get
350:57 - at least one head in SEC six seconds
351:04 - complements rule comes in really handy
351:06 - when dealing with poissons
351:11 - let X be the number of words in a year
351:14 - from the binomial slide deck we know
351:16 - that the average rate of wars in a year
351:17 - is one over
351:19 - 0.8462141
351:23 - 5 Section 5 1 gave us this
351:28 - this one divided that be 1.181734
351:32 - so that's Lambda for one year
351:35 - I want to know what's probability that
351:36 - we have five years without a war
351:40 - if this is Lambda for one year the
351:42 - Lambda for five years will be just five
351:44 - times that
351:45 - boom
351:50 - and so probability of Y is equal to zero
351:55 - it's just .002715796
352:00 - this is about 1 over 500 so we'd expect
352:04 - um a five-year P streak to happen about
352:06 - once every 500 years
352:09 - which seems to be borne out by reality
352:21 - real estate developer in Galesburg is
352:23 - looking to determine if it would be
352:24 - profitable to renovate the ferris
352:26 - building downtown into a boutique hotel
352:31 - to help determine this he has the
352:32 - student of mine who graduated last year
352:35 - to help estimate it
352:39 - John the student estimated the rate
352:41 - would be about 150 people per week
352:46 - the developer decided that a minimum of
352:48 - 130 per week would be needed to turn a
352:51 - profit so if John is right what
352:53 - proportional weeks will not be
352:54 - profitable
352:56 - so we're given Lambda is equal to 150
353:00 - we want to know the probability of being
353:02 - 130 or less
353:09 - or I'm sorry less than 130.
353:13 - so this is a probability
353:16 - this is not a cumulative probability
353:17 - because accumulative be less than or
353:19 - equal to
353:21 - so we make it a less than or equal to by
353:23 - dropping the 130 by 1.
353:27 - it's now a cumulative probability
353:30 - so we use p
353:32 - uas because it's hipposan 129
353:37 - and we're given that Lambda is equal to
353:39 - 150.
353:42 - so about four and a half percent of the
353:44 - weeks will not be profitable
353:54 - another example Galesburg crime
353:57 - there's actually a few examples in part
353:59 - of example four
354:01 - um reportedly the number of crimes in
354:03 - Galesburg was 96 last year
354:08 - so we're going to use that for our
354:09 - Lambda
354:11 - if there were four violent crimes last
354:13 - week
354:15 - is there significant evidence of an
354:16 - uptick in the crime rate
354:20 - okay so Lambda is 96 per year
354:25 - I got four violent crimes last week
354:29 - so we could make that Lambda in terms of
354:31 - weeks by dividing 96 by 52.
354:36 - so if the number of violent crimes in a
354:38 - week
354:39 - if x is that then we're asked to
354:41 - calculate probability of X being greater
354:43 - than or equal to four and interpreting
354:45 - that
354:47 - we're given X follows a poisson with
354:48 - Lambda equal to 96. this is crimes in a
354:51 - year divided by 52 so that would be
354:54 - crimes in a week
354:57 - this is the wrong direction it needs to
354:59 - be less than or equal to
355:02 - so this be one minus X less than or
355:04 - equal to three
355:07 - it's a less than or equal to so we can
355:09 - use the P form of poisson
355:11 - 3 Lambda is 96 out of 52.
355:15 - and we get 0.116
355:18 - so what this means is if the crime rate
355:22 - stayed the same
355:24 - that is 96 per year then we'd observe
355:27 - four crimes a week
355:30 - about 11 of the time
355:33 - that's rather large really
355:35 - taking everything into consideration so
355:38 - it's not really evidence
355:40 - that the crime rate's gone up
355:42 - that isn't evidence crime rate may have
355:45 - gone up
355:47 - but this doesn't provide sufficient
355:50 - evidence for that
355:54 - so let's say there were 16 violent
355:56 - crimes last month
355:59 - notice part A it was four in a week
356:03 - so we'd expect 16 in a month if it's
356:06 - four in a week for four weeks so this
356:09 - really is in some ways the same question
356:11 - as last
356:13 - but we're holding that increase of crime
356:16 - for instead of over a week we're doing
356:17 - it over a month
356:19 - so really we're asking does increasing
356:21 - the sample size affect the probabilities
356:25 - so we're asked what's the probability
356:26 - and we're supposed to interpret the
356:28 - probability of X being greater than or
356:29 - equal to 16.
356:31 - given that Lambda is 96 over 12 because
356:34 - it's we're in terms of 12 months
356:39 - greater than or equal to 16 is 1 minus
356:42 - less than or equal to 15.
356:44 - we're now less than or equal to so we
356:46 - can use
356:47 - P form of poisson 15 Lambda and now we
356:53 - got probability of being .00823
356:57 - that is very small
356:59 - in other words if the crime rate stayed
357:02 - the same we would expect to see 16
357:05 - crimes in a month
357:07 - less than one percent of the time
357:11 - so this would constitute some evidence
357:13 - that the crime rate did go up this year
357:18 - now the good question that you're
357:20 - thinking is okay the last one it was
357:22 - Point 11 and this one's .008
357:26 - 0.11 it was relatively large interpreted
357:30 - as being relatively large .008 is
357:33 - relatively small
357:35 - the 0.11 doesn't give us sufficient
357:37 - evidence the .008 does where's the
357:40 - separation between doesn't give us the
357:43 - evidence and does give us the evidence
357:46 - I'm not answering that now
357:49 - but keep this in my effect over on the
357:51 - left hand side of your notes
357:54 - write something like
357:56 - what's the difference or what's the
357:59 - separation between not enough evidence
358:01 - and enough evidence
358:05 - Circle it
358:07 - make it stand out a little bit maybe put
358:09 - Alpha around it a few times just to
358:13 - so that we can refer back to that in the
358:16 - future
358:17 - so here's a summary of the poisson
358:21 - poisson distribution model is the number
358:23 - of successes or over a time period or an
358:25 - area
358:28 - a parameter of a poisson distribution is
358:30 - Lambda
358:31 - this Lambda is both the mean and the
358:34 - variance
358:37 - we know the probability Mass function of
358:39 - the poisson that allows us to calculate
358:41 - probabilities from a poisson random
358:43 - variable
358:45 - the future we're going to look at the
358:46 - hypergeometric
358:48 - notice the hypergeometric is spelled
358:50 - correctly here and also hypergeometric
358:53 - we've bumped into that before in in lab
358:56 - B hybrid or about ready to hyper
358:59 - geometric like the binomial model's
359:03 - number of successes given a number of
359:04 - Trials
359:07 - so you've got to figure out what is the
359:09 - key difference between the binomial and
359:11 - the hypergeometric and then chapter 6
359:13 - will be the continuous distributions
359:16 - here are the r functions two of these we
359:18 - dealt with two of those are implied they
359:21 - all have the stem pois and they all
359:24 - require specifying Lambda because those
359:26 - are the keys to a poisson distribution
359:29 - the D form is for calculating
359:33 - probability of X equaling a value
359:36 - the P form is for the CDF calculating
359:40 - probability of X being less than or
359:41 - equal to a value
359:44 - the r generates random values from a
359:47 - poisson
359:48 - and the Q form calculates the quantiles
359:51 - so if I want to calculate the median I
359:54 - do Q Plus of 0.5
359:57 - and specifying whatever value of Lambda
360:00 - it is
360:01 - if I want to calculate the 10th
360:04 - percentile I put in 0.1 for p
360:10 - uh the readings Hawks was Section 5 3 R
360:14 - for starters this is appendix A7
360:17 - and as usual Wikipedia's got a pretty
360:19 - good page on the poisson distribution
360:23 - gives you a nice flavor of how far you
360:25 - can go with these probability
360:27 - distributions
360:29 - so here's the intra lecture question
360:32 - for five three our questions for five
360:35 - three
360:36 - question one what is the parameter of a
360:38 - poisson distribution
360:40 - it is the parameter for poisson hint
360:42 - it's called Lambda
360:44 - question two what's the difference
360:46 - between a binomial and a poisson
360:47 - variable
360:50 - I won't give a hint on that one except
360:52 - to say go back over your notes to make
360:54 - sure that you have this written down
360:56 - this is this is important
360:59 - and what's the formula for the expected
361:01 - value of a poisson
361:03 - that's also pretty easy one
361:05 - so of these three the sequence going to
361:08 - be the tough one
361:10 - and that brings us to the end of the
361:12 - poisson distribution
361:14 - take care
361:16 - hello and welcome to the last section of
361:19 - chapter five this will be the last named
361:22 - discrete distribution we'll be working
361:24 - with
361:25 - it's called the hypergeometric this feat
361:27 - is featured in lab B
361:32 - um so by the end of the lecture you
361:33 - should be able to determine which random
361:34 - variables follow hypergeometric
361:36 - distribution using its definition
361:38 - identify the three parameters of a
361:40 - hypergeometric distribution and
361:42 - calculate the expected value variance
361:43 - being probabilities associated with
361:44 - hypergeometric random variable
361:47 - hypergeometric is rather difficult to
361:50 - work with
361:51 - so don't expect too much work using the
361:55 - hypergeometric doing things by hand
361:58 - you'll see the probability Mass function
362:00 - and understand what I mean when you do
362:02 - see that the hypergeometric distribution
362:05 - describes the probability of obtaining X
362:07 - successes
362:09 - in K draws or trials without replacement
362:16 - from a finite population
362:18 - that contains exactly M successes and N
362:21 - failures see I told you it was kind of
362:24 - complicated
362:26 - um
362:27 - Keys two keys to this are it's without
362:30 - replacement so you're drawing and you're
362:33 - not putting it back and the population's
362:35 - finite
362:37 - if you are
362:39 - doing it with replacement or if the
362:43 - population is infinite then you've got a
362:45 - binomial random variable here so this is
362:48 - a binomial constrained to a finite
362:50 - population without replacement
362:54 - it's well when you get to the lab you'll
362:57 - understand that it's better but as you
363:00 - work through this uh
363:03 - slide deck you'll see why we tend to
363:05 - focus on the binomial
363:07 - examples number of Hearts drawn from a
363:10 - deck without replacement so
363:12 - I draw five cards what's the probability
363:15 - three are Hearts if I don't replace
363:17 - finite population Capital n's 50.
363:21 - two
363:23 - um
363:24 - and it's without replacement
363:26 - a number of sophomores counted in the
363:28 - Library without recounting somebody
363:31 - um
363:32 - number of Parolees that returned to the
363:34 - Hill Correctional Center in Galesburg
363:37 - um in each of these cases it's without
363:39 - replacement and the population is finite
363:43 - so key
363:45 - um
363:46 - so hyper geometric finite and no
363:49 - replacement
363:51 - now you'll you'll find out and I think
363:53 - the lab helps with this that if your
363:56 - population is sufficiently large but not
363:59 - infinite then the advantage of using the
364:02 - hypergeometric is very small it might as
364:05 - well stick with the binomial
364:09 - recall that the probability Mass
364:10 - function provides the probability of
364:12 - each element for the sample space for a
364:15 - hypergeometric random variable
364:17 - this is the sample space
364:21 - see notice automatically that we see
364:24 - that the hyper geometric is rather
364:26 - complicated to work with
364:29 - for the binomial it's just from 0 to n
364:31 - here it's the larger of zero and K minus
364:35 - n
364:36 - all the way up to the minimum of K and M
364:41 - how do we parameterize this there's lots
364:44 - of ways of parameterizing the
364:45 - hypergeometric that's another
364:47 - frustrating thing with binomial it's
364:48 - always n and p or n in pi for the
364:51 - hypergeometric
364:52 - there's at least a half dozen ways in
364:56 - fact Hawks uses a way that's different
364:58 - from either r or the forsberg text
365:02 - um
365:04 - the uh we can do it by successes and
365:07 - total or by successes and failures
365:10 - or by combination of the two
365:15 - but fundamentally the probability Mass
365:18 - function here at the bottom
365:20 - is just a number of ways to succeed
365:23 - times the number of ways to fail divided
365:26 - by the number of ways you can have those
365:29 - end trials
365:31 - and these are combinations so review of
365:34 - chapter four would not be unwise
365:39 - um
365:41 - so here's the probability Mass function
365:44 - it's m choose X
365:46 - n choose K minus X
365:49 - divided by M plus n over choose k
365:53 - X is the number of successes you care
365:55 - about
365:56 - m is the number of successes in the
365:59 - population
366:01 - in this case little n is the number of
366:03 - failures in the population
366:06 - so X is the number of successes in your
366:09 - sample
366:11 - K minus X will be the number of failures
366:13 - in your sample
366:16 - and then the denominator is just going
366:18 - to be the total population size divided
366:20 - by the total sample size
366:22 - and notice the switch here K is now the
366:24 - sample size not n
366:29 - and trust me this is one of the better
366:31 - parameterizations of the hypergeometric
366:36 - here's another one
366:38 - here K will be the population successes
366:40 - instead of a little m n will be the
366:44 - population size which means n minus K
366:46 - will be the population failures
366:49 - X will be the successes in your sample n
366:52 - minus X will be the six failures in your
366:55 - sample here n will be the sample size so
366:58 - n choose n will be the denominator
367:01 - they say the same things they're using
367:04 - different letters to represent what
367:06 - those things are but they're saying the
367:08 - same thing this is the number of ways of
367:10 - getting X successes from the population
367:13 - this is the number of ways of getting
367:14 - your specific number of failures in your
367:17 - population and this is the number of
367:19 - possible samples you can draw from that
367:21 - population
367:22 - same as it was for the previous
367:28 - expected value of a hypergeometric does
367:30 - not need to be memorized you've got this
367:33 - in your notes
367:34 - so make sure you know where you can
367:36 - locate it it's just k
367:38 - ick your sample size
367:41 - p
367:42 - the probability of a success in the
367:44 - population
367:47 - m is the number of successes n is the
367:49 - number of failures so n plus m is a
367:51 - population size so this M over n plus m
367:54 - is just the probability of a success in
367:57 - the population
368:00 - and that's the sample size so it's
368:02 - essentially n times P but we're using
368:05 - different letters
368:06 - here's the variance
368:09 - it's your sample size times the
368:12 - probability of a success in the
368:14 - population times the probability of a
368:17 - failure in the population times an
368:19 - adjustment Factor
368:21 - notice this is
368:23 - thinking back to binomial this would be
368:26 - n times P times 1 minus p
368:30 - times some other number
368:32 - notice this number is always going to be
368:34 - between 0 and 1.
368:36 - which means that the variance of a hyper
368:38 - geometric is always going to be smaller
368:40 - than that of a binomial
368:45 - that's important
368:47 - variance of a hypergeometric is always
368:50 - going to be less than
368:51 - or equal to for extremely large sample
368:53 - sizes that of a binomial
368:58 - as long as K is not equal to one
369:01 - if K is equal to one then there is
369:04 - absolutely no difference between a
369:05 - binomial and a hypergeometric
369:11 - go back to the definition of a
369:13 - hypergeometric to see why that's the
369:15 - case
369:17 - the big difference is with replacement
369:19 - or without if you're only drawing one
369:21 - thing it doesn't matter if you replace
369:23 - it or not it's the same probability
369:29 - this is another way of saying exactly
369:31 - what I did
369:32 - understand what the parameters represent
369:35 - don't get hung up on the formulas
369:37 - themselves
369:39 - understand that the expected value is
369:41 - just the sample size times the
369:43 - probability of a success in the
369:44 - population
369:46 - the variance is n times P times 1 minus
369:49 - P times some adjustment Factor
369:55 - so here's a couple examples okay here's
369:58 - a few examples
369:59 - I'll let X be the number of Spades drawn
370:01 - at a four car uh four draws from a deck
370:04 - of cards without replacement
370:07 - if the deck is fair
370:09 - and it's clear that X follows a
370:11 - hypergeometric
370:14 - with m equal 13 the number of successes
370:17 - in the population is 13.
370:20 - and the number of failures in the
370:22 - population is 39
370:24 - and our sample size K is 4.
370:30 - success failure sample size
370:34 - so here are the probabilities or here
370:36 - are the questions what's the probability
370:37 - of getting one Spade what's the
370:39 - probability of getting at most three
370:40 - Spades and what's the expected number of
370:43 - Spades
370:47 - probability of eating one Spade
370:49 - plug and chug
370:53 - probably X is equal to one
370:55 - we're given m is 13 x is one n is 39 K
371:00 - is 4 4 minus 1 is 3.
371:06 - a little check here
371:09 - 13 plus 39 has to be 52. 1 plus 3 has to
371:14 - be 4.
371:15 - why is that a check this first refers to
371:19 - the number of ways of getting those
371:20 - successes the second is the number of
371:23 - ways of getting those failures
371:25 - and the total is just failures plus
371:28 - successes
371:31 - so the probability of getting one Spade
371:33 - is 0.438847539
371:39 - here it is in r
371:44 - probability that X is equal to one so
371:47 - it's D
371:49 - and a 1 there this is a hyper geometric
371:51 - so the stem is hyper
371:54 - parameters m is 13 and 39K is four
371:58 - and by the way this is the
372:00 - parameterization that R uses mnk
372:06 - probability of getting at most three
372:08 - Spades this is a less than or equal to
372:10 - question
372:12 - less than or equal to three
372:14 - since it is a cumulative a less than or
372:17 - equal to we can use the P version
372:20 - 3
372:22 - m n and K
372:27 - so the probability of getting at most
372:29 - three Spades is 0.9973589
372:34 - in other words pretty good chance that
372:36 - you'll get at most three
372:42 - what's the expected number of Spades
372:44 - sample size times success is over
372:48 - trials
372:52 - so the expected number of Spades it's
372:54 - going to be one
372:56 - make sure this result makes sense
373:00 - especially you think about lab B
373:04 - a quarter of the deck is Spades I draw
373:07 - four so I'd expect one
373:13 - example two I wonder if stat 200
373:15 - attracts third tier students at a
373:16 - greater rate than other courses
373:18 - so X will be the number of third years
373:20 - in stat 200
373:23 - we know that the number of third year
373:25 - students at Knox is 356.
373:29 - we know the number of non-thirds years
373:31 - is 978 we got that from the registrar's
373:35 - website therefore those are population
373:37 - numbers
373:39 - in this stat course there's 41 students
373:44 - 18 or 30 years
373:47 - so we need to calculate the probability
373:48 - X is greater than or equal to 18.
373:53 - given m is 356 n is 978 and K is 41.
374:02 - this is in the wrong direction it's got
374:03 - to be a less than or equal to
374:05 - so using the complements rule
374:08 - this is one minus probability of X is
374:10 - less than or equal to 17.
374:12 - this is what it is in r 1 minus
374:16 - probability under hypergeometric of X
374:19 - being less than or equal to
374:23 - oops that should be a 17.
374:28 - so this number here should be 17 not 18.
374:32 - m n and K are given to us previously
374:36 - this is probably around .005
374:40 - very small probability of this happening
374:43 - therefore it appears as though third
374:45 - years are over represented in this
374:47 - course
374:51 - or
374:53 - I'm wrong about m n and K
374:57 - I trust the registrar to give me these
374:59 - two numbers
375:01 - I know I can count to 41. therefore I'm
375:04 - going to conclude that third years are
375:06 - indeed over represented
375:11 - if we were to Pretend This was a
375:13 - binomial we get a probability of .00537
375:18 - not much of a difference between these
375:20 - two
375:22 - the reason why there's not much of a
375:24 - difference between the two is because
375:26 - the population size is quote rather
375:29 - large
375:30 - it's thirteen hundred or so
375:36 - so once you get a large population
375:37 - hypergeometric and binomial are
375:39 - essentially the same thing
375:43 - I wonder if my math 121 attracts fourth
375:45 - year students at a lower rate than other
375:47 - courses
375:49 - let X be the number of fourth years in
375:51 - my math 121 course
375:53 - according to the registrar's website we
375:55 - know the number of fourth year is at
375:56 - Knox is 278 non-fourth years is 10 56.
376:01 - in my 121 there are 30 students
376:05 - three or fourth years
376:07 - so I need to calculate the probability
376:09 - of X being less than or equal to three
376:12 - where X follows a hypergeometric
376:14 - distribution with m n and k equal to 278
376:19 - 1056 and 30.
376:25 - P hyper
376:27 - P because it's less than or equal to
376:30 - we added at the 3 I mean it right this
376:32 - time MN and K given to us by the
376:35 - registrar's website and me being able to
376:37 - count the class
376:38 - this is 10 percent
376:41 - because the probability is not that
376:43 - small there does not seem to be much
376:45 - evidence that math 121 attracts fourth
376:47 - years at a lower rate than other courses
376:50 - 10 percent not that small again this
376:53 - goes back to
376:55 - um
376:56 - I think was the binomial slide deck
376:59 - where we started talking about okay
377:00 - what's the difference between no it was
377:01 - a poisson and the crimes what's the
377:04 - difference between not that much
377:05 - evidence and sufficient evidence
377:08 - so you might want to write in the in the
377:10 - margin again Arrow there of asking the
377:13 - question difference between sufficient
377:16 - evidence and not sufficient evidence
377:23 - by the way 0.099 versus 0.102 Hyper
377:27 - geometric versus binomial
377:30 - don't get me wrong the hyper geometric
377:32 - is the correct distribution
377:35 - and doing this by using R is pretty darn
377:40 - easy
377:42 - so there's no reason to use the binomial
377:44 - approximation
377:46 - but if you're doing it by hand or using
377:50 - some things that we will be doing in
377:51 - this class in the future
377:53 - that are predicated in the binomial as
377:56 - long as you're set plus size is large
377:57 - enough you can use those things those
378:00 - future things for both the binomial and
378:02 - the hypergeometric
378:06 - so the summary hybrid geometric models
378:08 - the number of successes out of a
378:10 - specific number of Trials when the
378:13 - population is finite and
378:16 - the elements cannot be selected multiple
378:18 - times
378:20 - there are multiple ways of
378:22 - parameterizing this distribution I gave
378:24 - you two of them
378:25 - but they all specify the sample size
378:28 - number of successes the failures in the
378:30 - population in some way
378:33 - you saw the pmfs of the hyper geometric
378:37 - you know the mean and the variance the
378:39 - hyper geometric again
378:41 - you should not spend your time
378:43 - memorizing those
378:47 - have them in your notes and make sure
378:49 - you're able to access them and use them
378:52 - the future today was or this lecture is
378:55 - the end of the discrete distributions
378:58 - the future will be continuous
379:00 - distributions
379:02 - and that'll start with chapter six
379:04 - we've got the four R functions all of
379:07 - them have the stem of hyper
379:09 - for the hypergeometric
379:12 - they all require you to specify M and K
379:16 - again the D is for the equals
379:19 - probabilities the P is for less than or
379:22 - equal
379:23 - the r is for generating a random sample
379:27 - from that distribution
379:29 - and the Q is for the quantile
379:33 - so if I want the median I'll put 0.5 for
379:37 - p and I'll be able to get the median if
379:39 - I want the third quartile I would put
379:42 - 0.75 in here for p and get the third
379:45 - quartile
379:48 - course readings Section 5 4 and Hawks
379:51 - this is appendix A6 in R for starters
379:55 - Wikipedia's got an interesting page on
379:57 - the hyper geometric again it'll give you
380:00 - a taste of where you can find the
380:03 - formulas if you forget them
380:05 - and it'll show you a lot of things that
380:08 - we can do with this probability stuff
380:11 - which brings us to the three
380:13 - interlection questions
380:15 - one what are the five requirements for a
380:17 - random variable to follow a binomial
380:19 - distribution
380:20 - yes this is for the binomial
380:22 - distribution
380:24 - you must know those five
380:28 - two what's the difference between a
380:29 - binomial and a hypergeometric variable
380:35 - and question three what's the difference
380:37 - between a binomial and a poisson random
380:39 - variable
380:41 - so a little hint here seems like we're
380:44 - focusing on the binomial as being the
380:46 - most important distribution in this
380:48 - chapter
380:49 - if it seems that way then good because
380:52 - it is
380:54 - but also you're seeing how close the
380:56 - poisson and the higher pot geometric are
380:58 - to the binomial when they are close
381:01 - and by extension when they're not close
381:07 - and that's it
381:09 - take care
381:11 - hello and welcome to chapter six in
381:13 - chapter six we're talking about
381:15 - continuous distributions contrast this
381:18 - with the discrete distributions in
381:20 - chapter five
381:21 - chapter 6 and Hawks goes directly to the
381:24 - normal distribution introduces some
381:26 - things that just kind of pop out of
381:28 - nowhere to help see where those things
381:31 - come from I'm introducing two other
381:33 - distributions the uniform distribution
381:35 - which will be this lecture and the
381:38 - exponential distribution which will be
381:39 - the next
381:42 - so by the end of this lecture you should
381:44 - be able to understand the difference
381:45 - between discrete and continuous random
381:47 - variables
381:48 - know the purpose of the probability
381:50 - density function notice this is the
381:53 - probability density function not the
381:55 - probability Mass function
381:57 - be able to prove that the density is not
382:00 - a probability
382:01 - you know the purpose of the cumulative
382:04 - distribution function CDF
382:06 - hint it's the same as the purpose of the
382:10 - CDF with discrete random variables be
382:13 - able to calculate probabilities using
382:14 - geometry or the CDF in fact we're going
382:18 - to figure out an easy way of calculating
382:20 - or creating the CDF
382:23 - for the uniform and then understand the
382:25 - uniform distribution it's two parameters
382:28 - it's sample space it's expected value
382:31 - its variance standard deviation uh
382:35 - median things like that
382:37 - so here's a definition of continuous
382:40 - random variable it's a random variable
382:42 - with a sample space consisting of an
382:44 - interval of values that means for
382:47 - continuous random variables there is
382:49 - always a value between two other values
382:52 - in it
382:53 - so there is no next two with continuous
382:56 - random variables
382:58 - for discrete random variables you could
383:00 - list them off there's a next two after
383:02 - one came two after two came three for a
383:05 - continuous random variable between any
383:07 - two values there's always going to be
383:08 - another value
383:10 - examples of continuous of actually
383:12 - continuous random variables student
383:15 - height
383:16 - age of a car time spent at a stoplight
383:19 - distance of golf ball goes those are all
383:22 - continuous measures
383:26 - there's also a category
383:29 - of random variables that are essentially
383:33 - continuous they're they're not
383:35 - continuous they're discrete but we can
383:37 - pretend they're continuous and not lose
383:40 - too much
383:41 - not create too much error in much the
383:44 - same way that when we were dealing with
383:46 - a hypergeometric distribution if the
383:49 - population was quote large enough we
383:52 - could use the simpler binomial and we
383:54 - wouldn't introduce too many errors
383:57 - such examples of quote near continuous
384:00 - random variables would be GPA
384:02 - definitely a discrete distribution but
384:06 - the distance between two levels of it is
384:10 - so small compared to the range that we
384:13 - can just pretend that it's continuous
384:15 - and not introduced too much by way of
384:17 - error annual salary my salary is paid
384:21 - down to the penny
384:22 - uh
384:24 - so the the the grid is a penny but my
384:29 - salary is in tens of thousands of
384:31 - dollars so that that
384:33 - close enough to being continuous that we
384:35 - can pretend it's continuous
384:37 - same with gross domestic product GDP per
384:39 - capita crime rate number of years of
384:42 - corn grown in Iowa that's definitely a
384:44 - discrete distribution but the distance
384:46 - between one ear and the next year that
384:49 - that that grid is so small compared to
384:52 - the number of ears grown that we it
384:55 - we can pretend that it's continuous and
384:58 - not lose too much
384:59 - what I'm getting at here is just like
385:01 - with the binomial there are benefits to
385:04 - using the binomial when you can
385:05 - Simplicity is is the best
385:08 - that even though the data are generated
385:11 - from hyper hypergeometric
385:14 - if this the population is large enough
385:16 - we can pretend it's binomial and not
385:19 - introduce too much error and then reap
385:23 - the reward of everything that comes from
385:24 - it being a binomial
385:26 - same thing here with these quote near
385:30 - continuous Ram variables they're not
385:32 - continuous they're discrete
385:34 - but they're so close to being continuous
385:37 - that we can pretend they're continuous
385:40 - and reap all the benefits of a
385:42 - continuous random variable and there are
385:44 - several
385:46 - um
385:47 - the first continuous random variable
385:49 - that we're going to talk about is the
385:52 - uniform distribution
385:54 - the uniform distribution is a continuous
385:56 - distribution that describes random
385:57 - variables whose likelihood of occurring
386:00 - is constant across a specified interval
386:06 - notice the word probability is missing
386:08 - from that definition
386:11 - when we start talking about continuous
386:14 - distributions we have to start talking
386:16 - about likelihoods of events we'll be
386:20 - able to retrieve probabilities in a lot
386:22 - of cases but we have to start talking
386:24 - about likelihoods
386:26 - if the ram variable X has a uniform
386:29 - distribution we're going to write X
386:32 - and there's that tilde is distributed as
386:35 - a uniform with parameters A and B A is
386:39 - the lower bound and B is the upper bound
386:44 - if x is a uniform a b distribution
386:47 - and then the sample space is is all
386:50 - values between a and b
386:53 - the expected value is just a plus b over
386:56 - 2.
386:57 - the variance is just the width of the
386:59 - interval squared over 12.
387:02 - standard deviation is just the width of
387:04 - the interval divided by the square root
387:05 - of 12 in other words it's the square
387:07 - root of the variance
387:11 - this is the probability density function
387:18 - for all X that's in the interval A to B
387:21 - the likelihood is 1 over B minus a
387:25 - or the density is 1 over B minus a
387:30 - outside the interval the density or the
387:33 - likelihood is zero
387:37 - it's a uniform distribution because each
387:40 - of the likelihoods is the same
387:44 - in a probability density function you're
387:46 - measuring the likelihood or the
387:49 - probability density
387:50 - for the uniform it's a constant
387:53 - between a and b
387:57 - the height is going to be 1 over B minus
388:00 - a
388:02 - Y is the height 1 over B minus a
388:06 - this is a graph of the likelihood let me
388:08 - be clear this is the graph of the
388:09 - density or the likelihood
388:13 - this is not a graph of probabilities
388:17 - we know it's not a graph of
388:19 - probabilities because if a is zero for
388:22 - instance and B is one half
388:25 - this width is going to be one-half and
388:28 - the height is going to be two
388:31 - and you can't have probabilities that
388:34 - are greater than one so this is clearly
388:36 - not a probability
388:38 - to obtain a probability
388:41 - you just have to find the area
388:45 - corresponding to what you're trying to
388:47 - the event you're finding the probability
388:48 - of
388:50 - so probabilities are just areas of the
388:55 - PDF functions
388:58 - for most continuous distributions that
389:01 - means we have to use calculus
389:04 - for the uniform distribution we can use
389:06 - high school algebra or we can use sixth
389:09 - grade algebra it's just squares and
389:11 - rectangles
389:15 - note now we know why it has to be have a
389:17 - height of 1 over B minus a
389:20 - because the interval length is a to B
389:24 - it's the length is b minus a
389:27 - and we know that the probability of
389:28 - something in this interval happening has
389:30 - to be one
389:31 - so the probability that X is between A
389:34 - and B has to equal one
389:36 - it's a rectangle so
389:39 - B minus a which is this width times the
389:42 - height has to equal one
389:46 - because rectangles are width times
389:48 - Heights
389:50 - and therefore the height has to be one
389:52 - over B minus a
389:58 - um
389:59 - note that the PDF has two purposes one
390:01 - it's to help the researcher understand
390:03 - the probability for a continuous
390:04 - distribution
390:05 - and it's to help the researcher
390:07 - calculate probabilities of a continuous
390:08 - distribution
390:10 - and in red probabilities are areas under
390:13 - the density curve
390:15 - another thing that these PDFs can tell
390:17 - us is
390:19 - where the outcome is most likely
390:23 - here the the likelihood is flat
390:25 - therefore every value between A and B is
390:27 - equally likely
390:30 - um
390:31 - if there was a mound to this then those
390:35 - things near the those values around the
390:38 - highest around the peak that's the word
390:40 - Peak will be more likely
390:44 - so let's see how we do this there's only
390:47 - one stop light between home and school
390:49 - in the morning my home and my school it
390:52 - regularly Cycles among green
390:55 - 175 seconds green five seconds yellow
390:57 - and 180 seconds red
391:00 - there's only one stoplight
391:02 - it's at Maine and Academy it's it's a
391:05 - doozy given that I stop at the light in
391:08 - other words given that the lights red
391:10 - what's the probability that I wait at
391:12 - most 60 Seconds
391:16 - so let's define the random variable t
391:20 - as the time I spend waiting
391:22 - and I want to find the probability that
391:25 - I weighed at most 60 seconds that t is
391:28 - less than or equal to 60.
391:32 - because the time I wait does not depend
391:33 - on when I get there
391:35 - notice it's the only light if there were
391:38 - several lights I had to stop through
391:39 - then
391:40 - it wouldn't be a uniform distribution
391:44 - but because it's the only light it is a
391:47 - uniform distribution
391:50 - because there's a definite lower and
391:52 - upper bound
391:53 - the lowest that I will stop there will
391:56 - be for zero seconds and the highest that
391:59 - I'll stop there will be 180 seconds
392:02 - I'm definite upper and lower bound
392:04 - uniform distribution
392:07 - we now have that t the time I spend
392:10 - waiting at the light in seconds is
392:11 - distributed according to uniform
392:13 - with parameter 0 and 180. or with
392:17 - minimum zero and maximum 180 seconds
392:22 - and again we have to calculate
392:23 - probability T is less than or equal to
392:25 - 60 that I wait at most 60 Seconds
392:34 - the outside rectangle is the
392:37 - distribution of t
392:40 - specifically it's the probability
392:42 - density function of t
392:45 - the dark blue is the area that I want to
392:48 - calculate
392:49 - it's the probability T is less than or
392:52 - equal to 60 Seconds
392:56 - to find the probability it's just the
392:58 - area under the Curve
393:01 - of the density curve corresponding to
393:04 - this width
393:06 - Heights 1 over 180
393:09 - the width of the dark blue is 60 so the
393:12 - probability of waiting at most 60
393:14 - Seconds
393:15 - is just 60 over 180.
393:20 - this should ring some Bells from chapter
393:22 - four when you were talking about equally
393:24 - likely events
393:28 - because the PDF of the uniform is just a
393:30 - rectangle and because areas of PDFs are
393:32 - probabilities and I want to emphasize
393:34 - that again areas in PDFs are the
393:37 - probabilities
393:38 - we just need to calculate the region of
393:40 - T less than or equal to 60.
393:42 - height times width
393:44 - so 33.3 percent chance that I wait at
393:48 - most 60 Seconds
393:51 - for the uniform this is easy to do in
393:54 - our heads
393:55 - if we want to use r
393:58 - here's how we use r
394:00 - again we're looking for a probability of
394:01 - T being less than or equal to something
394:03 - so we use the P version
394:06 - the stem for the uniform is uni f
394:09 - we're looking for the probability T is
394:11 - less than or equal to 60. then we
394:13 - specify the parameters of the uniform
394:15 - Min equals zero Max equals 180. and the
394:20 - keywords are Min and Max Not A and B
394:27 - the CDF of a probability distribution
394:30 - recall is defined as a probability of
394:32 - the random variable is less than or
394:33 - equal to some value
394:37 - traditionally we give this a capital f
394:42 - for continuous distributions without
394:44 - knowledge of calculus this is frequently
394:47 - rather difficult to calculate
394:50 - however for uniform we can just rely on
394:52 - middle school geometry
394:55 - so we're back to our generic uniform
394:58 - distribution ranges from A to B
395:01 - which means the height is 1 over B minus
395:03 - a
395:05 - we're going to let X move between a and
395:08 - b and we need to calculate this area
395:10 - here that's pretty easy the area of this
395:13 - is just x minus a
395:15 - divided by B minus a
395:19 - in surprise you just calculated CDF
395:22 - function
395:27 - this is the CDF function
395:30 - so if I want to calculate the
395:32 - probability probability that X is less
395:34 - than or equal to
395:36 - 6. I'd put 6 in here for x
395:41 - 6 in here for x and I would have to know
395:44 - A and B from the definition of the
395:46 - uniform
395:51 - I want to be clear calculating CDF
395:53 - functions usually requires calculus
395:56 - usually requires integration
395:58 - but for those who've had calculus
396:00 - integration is just a fast way of
396:02 - finding areas
396:04 - and in this case we've got geometry to
396:07 - help us find the area
396:12 - the quantile function is the inverse of
396:15 - the CDF
396:18 - we've bumped into the quantile function
396:20 - several times in the past
396:23 - um
396:24 - it's the quantile function is a function
396:27 - of P
396:30 - the CDF is a function of X it's the same
396:33 - relationship here by the way
396:35 - CDF is a function of X that calculates p
396:39 - quantile function is a function of P
396:42 - that calculates X they're inverses
396:48 - so from the CDF we know that P
396:51 - is equal to x minus A over B minus a
396:54 - this is for the uniform
396:55 - all we have to do is solve for x
397:00 - so X is equal to P times the width plus
397:03 - a the starting point
397:07 - so this thing on the left is the
397:09 - quantile function
397:12 - you're given P you solve it for x
397:15 - the first line the thing on the right
397:17 - was the CDF you're given X to calculate
397:20 - p
397:23 - inverse functions
397:27 - so here's the quantile function we can
397:29 - use a capital Q to symbolize it the
397:31 - quantile function for the uniform is
397:32 - just P times the width plus a at the
397:34 - starting point
397:43 - so here's what we've learned in this
397:45 - slide deck
397:47 - actually let's go back a page
397:50 - so how can I use this quantile function
397:53 - I can use it to calculate the median
397:56 - remember the median is the 50th
397:58 - percentile
398:00 - so to calculate the median of this
398:02 - uniform distribution we put 0.5 in for p
398:06 - 0.5 in for p
398:09 - and solve
398:11 - if we know what b and a are we can come
398:13 - up with a number
398:15 - if we don't know what b and a are then
398:17 - we just get an expression
398:20 - I will leave it as an exercise for you
398:22 - to show that the median of a uniform
398:26 - distribution is equal to the mean of the
398:28 - uniform distribution
398:30 - all it takes is understanding what the
398:32 - quantile function is and a little simple
398:35 - algebra
398:40 - so here's what we learned in this slide
398:42 - deck
398:43 - actually before we get to this let's go
398:45 - to our intra lecture questions
398:49 - there's three of them
398:52 - again write the question over on the
398:53 - left hand side of your notes answer
398:55 - below it so you can transfer that into
398:57 - Moodle
398:58 - question one what is the difference
398:59 - between a discrete and a continuous
399:01 - random variable
399:05 - question two what is the definition of a
399:09 - uniform random variable
399:13 - and question three what are the two
399:16 - parameters of a uniform distribution
399:21 - remember you do have the pause button to
399:23 - use
399:24 - okay let's go into the uniform summary
399:26 - now here's what we learned in this slide
399:28 - deck continuous random variables
399:30 - describe different phenomena than
399:31 - discrete random variables
399:33 - they describe things that are continuous
399:36 - instead of counts
399:38 - in a discrete random variable you talk
399:40 - about the probability Mass function
399:42 - which actually does give you a
399:44 - probability
399:45 - in continuous you've talked about a
399:48 - probability density function which gives
399:50 - you a likelihood and you have to
399:52 - calculate areas in order to turn that
399:54 - likelihood into a probability
399:59 - probability is the area under the PDF
400:01 - curve
400:03 - the CDF is probability that the random
400:06 - variable is less than or equal to some
400:07 - value
400:09 - uniform distribution describes a random
400:11 - variable where all values are equally
400:13 - likely
400:16 - the mean of a uniform random variable is
400:18 - a plus b over 2.
400:20 - turns out that that's also the median
400:22 - and I gave you that as a fun exercise
400:26 - future we're going to look at the
400:27 - exponential and normal distributions
400:30 - we're going to practice calculating
400:31 - probabilities using formulas tables and
400:33 - r
400:35 - I'm going to continue thinking about the
400:36 - relationship between random variables
400:38 - around us and their distributions
400:41 - the key part for chapters four five and
400:45 - six
400:46 - is starting to look at the world around
400:48 - us and think in terms of probability
400:51 - distributions
400:53 - here's the for our functions notice the
400:55 - stem for each is u n i f for uniform
401:00 - for the density function
401:02 - the PDF it's just d
401:05 - uniform now we see what the D actually
401:08 - means
401:09 - for the cumulative probability it's p
401:13 - for a random
401:15 - sample from this distribution it's r
401:19 - and then the quantile function is q
401:23 - and while we were here we calculated the
401:25 - P the CDF
401:28 - we calculate the PDF as well and we
401:30 - calculate the quantile function
401:33 - by hand we realize it's not that
401:35 - difficult
401:37 - but let's be clear it's not that
401:40 - difficult for
401:42 - this distribution
401:44 - once you go beyond something as simple
401:46 - as the uniform it begins to get much
401:48 - more difficult and sometimes you can't
401:50 - even determine what the quantile
401:52 - function or the CDF function is it may
401:55 - not exist
401:58 - the readings are for started appendix B1
402:01 - and B2
402:03 - there's nothing in Hawks
402:05 - and uniform distribution continuous in
402:08 - Wikipedia
402:12 - now this is as far as you have to go if
402:16 - you've taken some calculus and you want
402:18 - to see how to use it
402:19 - I give you the calculus extra
402:22 - but if you don't know calculus skip over
402:25 - this and you will lose nothing
402:28 - so how does one directly calculate the
402:30 - expected value for continuous
402:31 - distribution you use calculus
402:35 - refer back to the discrete case for the
402:38 - expected value
402:40 - it was x times the probability of X
402:43 - added up over everything in the sample
402:45 - space
402:46 - in the continuous case you're
402:50 - integrating over the sample space of x
402:52 - times the probability density function
402:56 - so this is the definition of the
402:58 - expected value in the continuous case
403:02 - similarly this is the formula for the
403:06 - variance
403:07 - in the continuous case notice again
403:09 - there's an x minus mu squared
403:12 - and this f of x isn't a probability
403:15 - but it sure looks like that's where the
403:17 - probability was in the discrete case
403:19 - instead of adding your integrating
403:23 - there's another formula for the variance
403:25 - that's equivalent and sometimes it's
403:27 - easier to use
403:32 - sometimes it's not
403:34 - so for let's see how to use these
403:36 - formulas
403:38 - um
403:38 - expected value of x is the integral over
403:40 - the sample space of x times the density
403:43 - of DX
403:46 - sample space is all values between a and
403:48 - b
403:50 - of x times the density the density just
403:52 - one over B minus a
403:54 - DX
403:56 - 1 over B minus a is a constant so it can
403:58 - be pulled out
404:00 - integral of x DX is x squared over two
404:05 - x squared over two there's that one
404:07 - minus B one over B minus a
404:09 - and it's evaluated between B and A
404:13 - between x equals a and x equals B
404:19 - x equals B this is B squared over two
404:23 - times one minus B over a subtracting off
404:26 - the evaluation at the lower bound a
404:28 - squared
404:30 - which gives us this next line
404:34 - and now we can do some calculus not
404:37 - calculus algebra stuff
404:38 - B squared minus a squared is B minus a
404:41 - times B plus a
404:43 - why did I know to do that well there's a
404:45 - B minus a out here and I knew this was
404:47 - the difference of squares so the B minus
404:49 - a is cancel and I'm left with a plus b
404:52 - over 2.
404:58 - variance same idea
405:01 - I'm using the second formula for the
405:03 - variance calculating What's called the
405:05 - second moment
405:07 - just the integral
405:09 - over the sample space of x squared times
405:12 - f of x DX and then I'll subtract off the
405:15 - mean squared
405:17 - integral of x squared DX is X cubed over
405:20 - three
405:22 - times one over B minus a
405:24 - evaluating between B and A
405:27 - so that's B cubed minus a cubed over
405:29 - three
405:31 - and B cubed minus a cubed has a B minus
405:34 - a term to it
405:38 - B cubed minus a cubed who knows B minus
405:40 - a times B squared plus a B plus a
405:43 - squared
405:45 - and we got some canceling there going
405:46 - away
405:48 - I combine these two fractions a 1 3 and
405:52 - 1 4 common denominator is the twelve
405:55 - Ah that's where the 12 comes from
405:59 - common denominator is 12 so this becomes
406:02 - 4 over 12 and this is 3 over 12.
406:05 - there's four there's the three cancels
406:09 - distribute the four distribute the 3.
406:13 - combine like terms
406:15 - B squared minus two a B plus a squared
406:18 - we know is equal to B minus a squared
406:21 - it's amazing how much high school
406:22 - algebra does come in handy sometimes
406:26 - which gives us our formula for the
406:28 - variance
406:30 - that makes sense the B minus a squared I
406:33 - mean the the wider the interval the more
406:36 - uncertain we are in the outcome so it
406:37 - makes sense that the variance would
406:39 - depend on B minus a squared now we see
406:41 - where the 12 comes from
406:44 - comes from combining these two parts
406:48 - and that's it for the calculus extra
406:49 - again if you haven't had calculus you
406:52 - shouldn't have watched that
406:54 - and if you have had calculus you see why
406:56 - you've had calculus
406:58 - take care
407:00 - hello welcome to the second lecture of
407:03 - chapter six this one covers the
407:05 - exponential distribution and the
407:07 - previous one covered the uniform
407:09 - distribution the uniform distribution
407:11 - was very good at modeling stuff that had
407:13 - a definite lower and a definite upper
407:15 - bound
407:16 - and was continuous and you knew nothing
407:20 - else about it
407:22 - other than it had a definite lower a
407:24 - definite upper and was a continuous
407:25 - random variable an exponential
407:27 - distribution will be very useful when
407:29 - you've got a continuous random variable
407:31 - that has a definite lower bound of zero
407:34 - and no hard upper bound
407:37 - so it's bounded on one side examples of
407:41 - this would be wait times
407:43 - um so by the end of this lecture you
407:44 - should be able to determine which random
407:45 - variables may follow an exponential
407:47 - distribution specify the characteristics
407:49 - of an exponential distribution compare
407:51 - and contrast exponential with uniform
407:52 - calculate probabilities using the CDF
407:55 - and quantiles using the CDF
407:58 - so here's the characteristics it's a
408:01 - continuous distribution describes a
408:03 - random variable whose probability of
408:04 - occurring decreases with time
408:08 - it's frequently used to model quote time
408:11 - until something occurs when there is no
408:14 - upper bound we say x follows or has a
408:18 - distribution of an exponential
408:20 - the parameter for an exponential is
408:22 - Lambda
408:23 - Lambda is the rate of the thing
408:25 - happening
408:28 - sample space is from 0 to Infinity there
408:30 - is no upper bound there is a lower bound
408:33 - of zero but there's no upper bound
408:35 - expected value is 1 over Lambda the
408:38 - variance is one over Lambda squared
408:40 - which means that the standard deviation
408:42 - and the expected value of the
408:44 - exponential are identical one over
408:46 - Lambda
408:51 - the rate of something happening
408:54 - well if we remember our physics the rate
408:57 - of one divided by the rate is just the
408:59 - frequency
409:02 - so it makes sense that the expected
409:04 - value would be 1 over the rate because
409:07 - it's an average frequency of it
409:09 - happening
409:10 - this is what the exponential
409:12 - distribution looks like Smooth declining
409:16 - the height happens at Lambda
409:19 - when x equals zero
409:21 - the probability density function is
409:23 - Lambda
409:24 - times e to the power of negative Lambda
409:27 - X
409:30 - for X greater than zero and zero
409:32 - otherwise
409:37 - and again it's important probabilities
409:38 - are areas under the density curve
409:41 - notice there's no rectangles here for us
409:43 - to play with so calculating those areas
409:46 - is going to be a bit more difficult than
409:48 - it was in the uniform case
409:52 - let's go into two examples
409:55 - time between when I fill my bird feeder
409:57 - with seed and when chunky the squirrel
409:59 - starts eating from the feeder follows an
410:01 - exponential distribution with an average
410:03 - time of 10 seconds
410:06 - in other words if we Define t as the
410:08 - time in seconds until chunky raised the
410:10 - bird feeder T follows an exponential
410:12 - distribution with Lambda equal to one
410:14 - over ten
410:18 - Lambda is 1 over the mean
410:22 - the mean is one over Lambda
410:26 - Lambda is one over the mean we're given
410:28 - the average time is the mean time is 10
410:31 - seconds
410:32 - so what's the probability that a weights
410:34 - at most a half minute before chowing
410:36 - down on some awesome seed
410:38 - at most a half minute so we're looking
410:40 - at the probability of T being less than
410:42 - or equal to 30.
410:46 - dark color is what we need to calculate
410:48 - remember again
410:49 - for continuous random variables areas
410:52 - under the PDF curve
410:54 - are the probabilities
410:59 - no rectangles to use
411:04 - can't use the tricks from last lecture
411:08 - are going to have to jump directly to
411:10 - the CDF function
411:13 - I leave it as a proof for those who
411:15 - enjoy calculus
411:17 - to prove that this is indeed the CDF for
411:20 - the exponential distribution
411:25 - what we did with the uniform remember is
411:27 - we created this ourselves
411:30 - for the exponential it's given to us
411:33 - and we use it
411:35 - for the normal which is the next
411:37 - distribution in the next lecture we
411:39 - realize there is no function and we have
411:41 - to use something else
411:45 - so we want to find the probability less
411:47 - than equal to 30 that's just the CDF at
411:50 - 30.
411:52 - Lambda is 0.1 x is 30. here's the CDF
411:56 - function
411:58 - 95 percent
411:59 - probability that chunky will wait no
412:02 - more than 30 seconds to raid the feeder
412:07 - this is a little bit more involved if we
412:09 - got R open might as well use R to do the
412:12 - calculations
412:14 - it's a less than or equal to so this is
412:16 - a p
412:18 - the stem for the exponential is EXP
412:22 - we want to find the probability T is
412:23 - less than or equal to 30.
412:25 - and we're told Lambda is equal to 1 10.
412:28 - we're not calling it Lambda here we're
412:31 - calling it rate so be aware that this
412:33 - has to be rate
412:39 - example two
412:41 - time I wait until the gold express bus
412:43 - comes follows an exponential
412:45 - distribution
412:46 - my average wait time is five minutes
412:50 - in this cold of winter it takes 10
412:52 - minutes for Forest bite to start
412:55 - so given this what's the probability
412:57 - that I will have frostbite before the
412:59 - bus arrives in other words
413:01 - given that 1 over Lambda is five what's
413:05 - the probability that t is greater than
413:07 - 10.
413:14 - 10. this is not a CDF it has to be in
413:16 - the form of a less than or equal to
413:19 - but using the complements rule we can
413:22 - easily change this into from p is from T
413:26 - greater than 10 to 1 minus probability
413:28 - of T being less than or equal to 10.
413:32 - chapter 4 pops up every once in a while
413:34 - this is probably the most important
413:37 - thing from chapter four the compliments
413:39 - rule
413:42 - so now all you do who is calculate the
413:45 - probability is less than or equal to 10
413:46 - that's the CDF at 10 given that Lambda
413:49 - is 20. I'm sorry 0.20
413:52 - 1 over 5 is 0.20
413:55 - plug chug 13.5 percent chance that I get
413:58 - frostbite
414:02 - or it's 1 minus P EXP
414:06 - of 10
414:08 - rate equals 0.2
414:11 - again it's p because we're looking at
414:14 - the CDF function it's exp because it's
414:17 - an exponential distribution
414:22 - we can also calculate the median
414:29 - remember the median is the value X tilde
414:32 - such that the CDF at X tilde is one half
414:40 - on the left is the CDF function
414:44 - evaluated X tilde
414:47 - which is just 1 minus E to the negative
414:49 - Lambda X tilde
414:52 - 0.5 stays on the right
414:55 - now what we're doing now is solving for
414:57 - x tilde
415:00 - subtract
415:02 - 1 from both sides and then multiply
415:05 - through by negative one
415:06 - gets us here
415:09 - take the natural log of both sides
415:12 - gets us here
415:14 - divide by negative 0.2
415:17 - gets us here
415:21 - remember the log of one-half is negative
415:24 - log of 2.
415:27 - so the median is 3.466 minutes
415:32 - and we can show in general that the
415:34 - median is just mu the mean Times log 2.
415:38 - and this is the natural log of 2.
415:43 - or instead of doing all this calculation
415:47 - and R is just q q because we look at the
415:50 - quantile at 0.5 this will give us the
415:53 - median
415:54 - exp because it's an exponential and the
415:57 - rate is one over five it's one over mu
416:06 - we could also calculate the 90th
416:08 - percentile
416:12 - set the CDF equal to 0.9
416:15 - and solve for x star
416:19 - or just put 0.9 in the quantile function
416:21 - and solve
416:25 - before we get to the summary let's go
416:27 - ahead and put in the intra lecture
416:30 - questions there's three of them as usual
416:32 - and again as usual write these on the
416:35 - left side of your notes answer them so
416:37 - you can transfer them into Moodle
416:41 - one give an example of a random variable
416:44 - that follows an exponential distribution
416:46 - something other than the two examples
416:47 - that we did today
416:51 - two what is the parameter of an
416:53 - exponential distribution
416:56 - and three
416:58 - what are the mean and standard deviation
417:00 - of an exponential distribution
417:03 - give me the formula for those
417:06 - I did say mean and standard deviation
417:10 - the reason I'm asking this is to set
417:12 - something in your mind so that you can
417:13 - contrast the exponential with the
417:16 - poisson because exponential poisson both
417:18 - use Lambda as the parameter
417:21 - um so here's a summary we reminded
417:24 - ourselves that probabilities area under
417:25 - the PDF curve the CDF function is X
417:29 - probability of X less than or equal to X
417:32 - in this case we needed calculus to find
417:36 - the CDF
417:37 - although the calculus extra is where we
417:39 - prove it I just threw the formula at you
417:44 - exponential distribution describes a
417:46 - white wait time random variable the time
417:49 - until something happens
417:51 - either being a variance of an
417:53 - exponential random variable or one over
417:54 - Lambda and one over Lambda squared
417:56 - respectively meaning that the standard
417:58 - deviation is going to be the square root
418:00 - of the variance
418:01 - in other words the standard deviation is
418:03 - one over Lambda
418:07 - last distribution is the normal
418:09 - distribution
418:12 - normal distribution is the key
418:14 - distribution from chapter six when we
418:18 - get to chapter seven and talk about the
418:19 - central limit theorem I'll drive this
418:21 - home as to why the normal distribution
418:22 - is the most important
418:24 - we cannot there is no formula to
418:27 - calculate the normal distribution
418:28 - probabilities
418:30 - we can either use the table at the end
418:31 - of the book no no no no no or we can use
418:34 - R to do it we're going to look at
418:36 - quantiles Otherwise Known percentiles
418:38 - and we're going to again understand the
418:40 - difference between discrete and
418:41 - continuous random variables
418:43 - here are the r functions again all of
418:45 - them have the stem exp for the expected
418:48 - value and all of them require that you
418:50 - specify what the rate is and the rate
418:52 - here is Lambda
418:54 - D for the density the little f of x p
418:58 - for the cumulative probability the
419:00 - capital f of x r to generate a random
419:03 - value Q for the quantile
419:06 - so the median we'd use the Q exp and
419:09 - then put in 0.5 for the 10th percentile
419:11 - we put in point one for the 99th
419:14 - percentile we put in 0.99
419:20 - nothing in Hawks about this R for
419:22 - starters it's appendix B5
419:25 - how Wikipedia's got the exponential
419:27 - distribution
419:28 - in the Wikipedia page you'll see that
419:31 - there's actually two parameterizations
419:32 - for the exponential
419:34 - the one that we're using in class which
419:36 - is Lambda and the one that
419:39 - the engineers
419:41 - tend to use Theta
419:44 - if memory serves me right
419:48 - that's kind of the end again if you are
419:50 - not a calculus person you can stop here
419:53 - if you are a calculus person you want to
419:55 - see the how to actually create that CDF
419:58 - function continue on
420:02 - so here's a proof of the exponential CDF
420:06 - remember the definition of the CDF it's
420:08 - the probability
420:09 - otherwise an area under this under the
420:13 - PDF curve
420:14 - other random variable being less than or
420:16 - equal to X for the exponential that
420:19 - means that you're integrating from zero
420:21 - to that x value
420:24 - F of T is the exponential PDF
420:27 - probability density function DT
420:32 - substitute F of T is just Lambda e to
420:35 - the negative Lambda t
420:38 - this Lambda doesn't depend upon T so it
420:40 - can be pulled out
420:42 - so we're integrating from zero to X of e
420:44 - to the negative Lambda T DT
420:48 - not too difficult
420:51 - the integral of e to the negative Lambda
420:53 - T is just negative one over Lambda times
420:56 - e to the minus Lambda t
421:00 - evaluated from T is zero to X
421:05 - this Lambda and this Lambda cancel so
421:07 - we're left with just a negative e to the
421:09 - negative Lambda t
421:13 - evaluated at X subtract off this thing
421:16 - evaluated at zero
421:18 - here it is evaluated at X subtract off
421:21 - evaluated at zero notice e to the zero
421:24 - is equal to one
421:26 - subtracting off a negative one means
421:28 - you're adding one
421:31 - no simplification the first term
421:34 - so we're left with CDF being one minus E
421:37 - to the negative Lambda X
421:41 - again
421:42 - this is for those who want to see that
421:44 - their calculus time was well spent that
421:46 - it's used a lot in statistics and
421:49 - probability Theory and I'm sure it's
421:51 - used other places but really I don't
421:53 - care it's all about the statistics and
421:55 - probability and how we're using the
421:57 - mathematics so I'm done I have a good
422:01 - one
422:02 - hello and welcome to chapter six from
422:04 - Hawks this lecture will cover check
422:07 - section six one through six four all
422:10 - four of those discuss the normal
422:12 - distribution because we spent some time
422:15 - building up things with uniform
422:18 - distribution the exponential
422:19 - distribution we'll be able to cover the
422:21 - normal distribution in one rather
422:23 - lengthy lecture but still just one
422:25 - lecture
422:26 - by the end of this lecture you should be
422:27 - able to discuss the differences between
422:29 - the uniform exponential and normal
422:31 - distributions
422:32 - know the expected value and the mean of
422:35 - a normally distributed random variable
422:37 - it's kind of tricky because remember the
422:39 - mean and the expected value are the same
422:41 - thing
422:43 - sketch the graph of a normal
422:44 - distribution
422:46 - realize that capital N normal and
422:49 - lowercase and normal mean different
422:50 - things
422:52 - the capital N normal refers to the
422:54 - distribution
422:55 - and lowercase and normal just means
422:58 - typical
422:59 - or not surprising
423:02 - to be able to calculate probabilities
423:04 - normal random variables and quantiles of
423:06 - normal random variables
423:10 - so here's the arc that we've been
423:11 - working on
423:13 - chapter five we looked at discrete
423:15 - distributions which included probability
423:18 - Mass functions pmf calculated mean the
423:21 - variances sample spaces for those
423:23 - discrete distributions
423:25 - the discrete distributions we looked at
423:28 - were the Bernoulli the binomial the
423:32 - poisson and the hypergeometric we also
423:35 - looked at generic discrete distributions
423:37 - in the first section of chapter five
423:41 - and chapter six looked at looks at
423:44 - continuous distributions
423:46 - I introduced the uniform distribution so
423:48 - you get an idea that probabilities for
423:51 - continuous distributions were actually
423:53 - areas under the PDF curve
423:56 - which meant the PDF curve was not
423:58 - probabilities they were probability
424:00 - densities or likelihoods
424:03 - we use the uniform to illustrate how to
424:06 - calculate the CDF the cumulative
424:08 - distribution function
424:10 - using simple high school geometry
424:13 - and then we moved on to look at how to
424:15 - calculate the mean the variance the
424:17 - sample space the median other quantiles
424:19 - on the uniform
424:21 - so that section on the uniform just laid
424:24 - the foundation for all the other
424:26 - important features of continuous
424:29 - distributions
424:31 - the second section was the exponential
424:33 - where we did the same thing but then we
424:35 - realized
424:36 - creating that CDF function that
424:38 - cumulative distribution function was not
424:40 - easy and in that case we had to use
424:43 - calculus to get it
424:45 - or It came it was given to us pop fully
424:48 - formed out of the sea foam your choice
424:52 - so we went from the uniform where we
424:54 - created it ourselves and got a good feel
424:56 - for how it was created to the
424:58 - exponential where we had to use Calculus
425:00 - if we wanted to see how it was created
425:02 - to this lecture it's normal and there is
425:05 - no CDF function
425:09 - which means that in the uniform well you
425:11 - really didn't have to pay attention to
425:13 - the CDF we could calculate those
425:14 - probabilities in our head exponential we
425:17 - need to use the CDF but it was a nice
425:19 - simple form that we could use
425:21 - for the normal we got to go to a table
425:23 - or to a computer
425:25 - and for much of this second half of the
425:28 - 19th century
425:30 - a lot of effort was spent trying to
425:33 - create those tables it was pretty easy
425:36 - in the early 19th century create
425:39 - CDF function for for values of we're
425:43 - going to see Z between negative 1 and
425:46 - positive one that was pretty easy to do
425:47 - once you got outside that negative one
425:49 - to positive one realm it got harder and
425:52 - harder and harder to get good estimates
425:54 - of those probabilities
425:57 - with the Advent of computers it's much
426:00 - easier I mean we could even do it
426:03 - and today we're going to look at the
426:04 - normal probability density function for
426:06 - it the CDF function for it the mean
426:08 - variance sample space median quantiles
426:11 - AKA percentiles same stuff
426:14 - we're going to see the function the PDF
426:16 - function we're going to realize okay we
426:18 - don't need to memorize that
426:20 - we're going to see that we can't write
426:23 - out the CDF function in any meaningful
426:26 - way but we'll find out ways of
426:28 - calculating
426:30 - the gaussian distribution apparently was
426:33 - named or no apparently about it it was
426:37 - named after Carl Friedrich Gauss who
426:40 - advocated for its use AKA created it
426:45 - um
426:46 - or at least we think he created those
426:48 - who stem from the German school of
426:50 - thought think he created it
426:52 - his first paper though was 1809.
426:56 - it was LaPlace who published a paper
426:59 - earlier than that using this
427:00 - distribution
427:02 - that's why the French
427:05 - descendants call this the Gauss LaPlace
427:08 - or just the LaPlace distribution
427:10 - but we're going to call it the normal
427:11 - distribution
427:14 - um gal said hey wait a minute LaPlace I
427:16 - was working on this way back in 1794. no
427:20 - evidence of it except gauss's word
427:24 - normal distribution arises from modeling
427:26 - observed Randomness in astronomical and
427:28 - geodetic data
427:31 - it arises in variables that have a
427:33 - specific expected value but demonstrate
427:36 - some minor random variation above and
427:38 - below so for instance this pop that I'm
427:41 - drinking now says that contains 100
427:43 - milligrams of caffeine
427:46 - so I would not be surprised if the
427:49 - distribution of caffeine in these
427:51 - bottles followed a normal curve that is
427:54 - the mean would be a hundred and there
427:55 - would be some variation above and below
427:57 - because you can't get exactly 100
427:59 - milligrams of caffeine every time
428:03 - there are two parameters to the gaussian
428:05 - distribution or to the normal
428:06 - distribution the mean and the standard
428:08 - deviation mu and sigma
428:13 - most important distribution in
428:14 - statistics because of the central limit
428:16 - theorem which we'll see in chapter seven
428:20 - so for the record if x a random variable
428:22 - follows or is distributed according to a
428:25 - normal distribution with mean mu and
428:28 - standard deviation Sigma
428:30 - then the PDF is lowercase f of x is this
428:34 - thing
428:39 - notice that the PDF depends on mu and
428:42 - sigma
428:44 - and some Sigma in a couple places
428:48 - however recall to chapter 3 when we
428:50 - looked at the standardized score
428:53 - we can subtract off mu divided by Sigma
428:56 - and we actually come up with another
428:57 - normal distribution it's a normal
429:00 - distribution that does not depend upon
429:02 - mu and sigma it's a standard normal
429:06 - distribution
429:08 - with mean zero standard deviation of one
429:11 - so the normal distribution has two
429:14 - parameters mu and sigma the standard
429:17 - normal distribution has no parameters
429:21 - and it's the standard normal that is
429:23 - tabulated in books
429:29 - here's a graph of the standard normal
429:30 - PDF notice that it goes off in both
429:32 - sides forever
429:34 - this should raise some memories of the
429:37 - empirical rule
429:40 - um
429:40 - you will see sometimes that the PDF is
429:45 - symbolized with a lowercase fee
429:48 - Greek letter Phi
429:50 - lowercase fee lowercase f
429:54 - because it's a PDF and the CDF will be
429:57 - at uppercase fee
430:00 - I don't think I have an uppercase fee on
430:01 - here
430:05 - here's the mean
430:07 - notice it's also the median
430:09 - normal distribution to symmetric
430:12 - so median median will be the same
430:15 - notice also it's the most likely value
430:18 - because the distribute or the the
430:21 - likelihood function
430:23 - the the PDF is highest there so this is
430:27 - the highest likely or the most likely
430:30 - value from this distribution
430:35 - so the normal distribution the mean
430:36 - median and mode are the same value
430:41 - here are the standard deviations
430:44 - at this point you should be able to say
430:46 - hey what I know what percent of the data
430:49 - or of this distribution is between
430:51 - negative 1 and positive 1. and what
430:54 - percent is that
430:56 - no no no no oh I'm sorry I miss heard
430:59 - you yeah 68 percent
431:01 - and what percent is between negative 2
431:04 - and positive 2.
431:07 - yep about 95 percent
431:09 - and between negative 3 and positive 3 is
431:11 - 99.7 percent ish
431:17 - call the normal distribution is
431:18 - continuous but it's not rectangular and
431:21 - it can be proven that the CDF cannot be
431:24 - written out
431:26 - hence late 19th century early 20th
431:30 - century as well working on estimating
431:32 - those probabilities
431:35 - so you have to use a table or a computer
431:38 - table one calc tabulates this CDF
431:42 - computer calculations are easier to
431:44 - perform however
431:46 - they're more accurate and they are more
431:49 - precise
431:51 - the stem is Norm you have to specify the
431:55 - mean M and the standard deviation s
432:00 - P for the cumulative probability Q for
432:03 - the quantile r for generating random
432:06 - values
432:10 - pqr is the same as it has been with all
432:13 - the other probability distributions
432:16 - stem is Norm and you have to specify M
432:18 - and S
432:21 - so some examples
432:24 - intelligence quotient by Design
432:27 - the IQ measures in the United States
432:30 - follow the normal distribution with mu
432:32 - equal to 100 and sigma equal to 15.
432:36 - this is by Design This Is How They
432:38 - create this test
432:40 - What proportion of the U.S population
432:42 - has an IQ less than 90.
432:45 - in other words if
432:47 - what Rand variable do I use in other
432:49 - words if this is the distribution of IQs
432:52 - in the United States I want to find the
432:55 - area under the curve less than 90.
433:00 - the purple area
433:03 - I want to calculate the probability that
433:05 - X is less than or equal to 90.
433:08 - give that X follows a normal
433:09 - distribution with mean
433:11 - expected value of 100 and standard
433:14 - deviation of 15.
433:17 - here it is in r
433:19 - P for a cumulative probability Norm is
433:23 - the stem for the normal 90 is the value
433:26 - we're specifying m is equal to 100 and S
433:29 - is equal to 15.
433:31 - this comes out to be about a quarter of
433:33 - the population
433:37 - wait a minute
433:38 - I can hear you saying
433:40 - I thought this P only worked when it was
433:42 - less than or equal to
433:45 - what is going on here are you trying to
433:48 - pull something over on us the answer is
433:50 - no
433:52 - remember that X follows a continuous
433:54 - distribution
433:56 - in this case it's a normal distribution
433:58 - that means that the probability of X
434:00 - equaling 90 exactly
434:03 - is zero
434:05 - think back to the uniform distribution
434:09 - it's an area so it's the base times the
434:11 - height
434:12 - if the base consists of a single point
434:15 - then the width is zero
434:18 - and a width of zero times whatever
434:19 - height is going to give you a
434:21 - probability of zero
434:22 - so for continuous distributions
434:26 - less than or less than or equal to are
434:29 - going to be exactly the same
434:36 - what IQ value separates the lower 30
434:39 - from the upper 70 percent
434:42 - so we're looking for an IQ value
434:45 - that tells me I'm going to be using a
434:47 - quantile
434:49 - I'm looking for the 30th quantile or the
434:52 - 30th percentile
434:55 - so I'm looking for this value here such
434:58 - that 30 percent is in purple
435:04 - Q for quantile
435:06 - we're looking at the 30th percentile so
435:09 - it's 0.3 here
435:10 - and we specify the mean and standard
435:12 - deviation
435:14 - 92.13399
435:18 - so approximately 30 percent of Americans
435:21 - have an IQ score less than 92.13399
435:26 - and approximately 70 percent of
435:27 - Americans that's everyone else have an
435:30 - IQ of more than 92.13399
435:36 - 30 it's pretty frequent still
435:40 - so in other words IQ of 90 and no big
435:42 - deal
435:45 - by Design IQs Etc what's the proportion
435:48 - of Americans that with an IQ score
435:50 - between we haven't dealt with any
435:52 - between calculations yet between 87 and
435:56 - 122.
435:57 - so we want this purple area between 87
436:01 - and 122.
436:04 - for calculating cumulative probabilities
436:06 - it's got to be in the form of less than
436:08 - or equal twos
436:10 - so I can do less than or equal to 122
436:13 - but that covers everything here
436:16 - not just what we want
436:18 - but notice like that this region is less
436:22 - than 122
436:24 - minus less than 87.
436:31 - looking for this
436:33 - this is just the probability commutative
436:35 - probability at the upper minus the
436:37 - cumulative probability at the lower
436:41 - so approximately 75 percent of Americans
436:43 - have an IQ between 87 and 122.
436:49 - that's a lot
436:53 - and now we'll go for above we did a less
436:56 - than between now we'll do an above
436:58 - probability of a proportion of Americans
437:00 - have an IQ above 90.
437:03 - so this blue purple area
437:07 - complements rule this this purple area
437:10 - is just one minus the white area
437:14 - 1 minus the white area
437:18 - so and again about a three quarters of
437:20 - Americans have an IQ above 90.
437:27 - so let's do a learning check I'm going
437:29 - to ask some questions I'll pause you
437:31 - answer them
437:33 - on your own out loud because
437:36 - dogs watching you and you really do want
437:39 - to talk to it
437:40 - what type of random variable will have a
437:42 - normal distribution
437:46 - a random variable with a definite Target
437:49 - and some random variation added to it
437:52 - will tend to have a normal distribution
437:56 - what's the difference between capital
437:57 - and normal and lower end normal
438:05 - capital N normal is refers to the
438:08 - distribution itself
438:09 - lower end normal refers to the fact that
438:12 - something is not unexpected
438:16 - what is the sample space of a normal
438:19 - random variable
438:25 - all real values there is neither a lower
438:27 - bound nor an upper bound
438:30 - what is the expected value of a normal
438:32 - random variable
438:36 - very good that's mu
438:39 - it's one of the parameters
438:40 - what is the variance of a normal random
438:42 - variable
438:45 - yes
438:46 - Sigma squared it's the square of one of
438:48 - the other parameters remember there are
438:50 - two parameters mu and sigma the variance
438:53 - is just the square of Sigma
438:55 - what R function is used to calculate
438:57 - cumulative probability for normal
439:02 - yep cumulative probability so it starts
439:05 - with a P for normal random variables so
439:08 - it's p Norm
439:11 - and what our functions use to calculate
439:13 - the quantiles
439:15 - Q Norm Q for quantiles Norm for normal
439:17 - random variable
439:20 - the future we're going to keep working
439:22 - with the normal distribution simply
439:23 - because of the central limit theorem
439:26 - and we're going to use this normal
439:27 - distribution to estimate population
439:29 - parameters
439:30 - but this this last one will be the
439:33 - second half of the course
439:37 - here are the r functions
439:39 - none too surprising the stem is Norm
439:42 - then we got the DPR and Q
439:47 - notice when we were dealing with the
439:48 - discrete we used the d a lot
439:51 - now that we've moved on continuous we
439:53 - really don't use the D
439:56 - I use the D Norm to create those graphs
439:59 - but really you don't use the D Norm
440:02 - function at all you focus on the P norm
440:04 - and the Q norm and the r Norm
440:08 - here are some readings for you
440:11 - are for starters appendix B3 C1 and C2
440:16 - and Hawk section six one to six four
440:19 - normal distribution in Wikipedia is a
440:21 - good one
440:24 - oh I haven't done the intra lecture quiz
440:29 - or questions
440:30 - here we go
440:32 - question one give an example of a random
440:34 - variable that follows a normal
440:35 - distribution
440:39 - question two what are the two parameters
440:42 - of a normal distribution
440:47 - and question three what are the mean and
440:49 - standard deviation of a normal
440:50 - distribution
440:52 - three again will be really easy
440:55 - two will be pretty easy
440:58 - one
440:59 - that'll take some thought but it'll be
441:01 - easy
441:03 - oops went too far so I guess I'm done
441:07 - hello and welcome to this last section
441:09 - of chapter six this is where we're going
441:11 - to use the normal distribution to
441:13 - approximate the binomial distribution
441:17 - notice this is rather interesting
441:19 - because the normal distribution is
441:21 - continuous and the binomial distribution
441:23 - is discrete
441:25 - so indeed we can approximate discrete
441:28 - distributions using these the The
441:30 - Continuous distributions
441:33 - so by the end of the selection you're
441:34 - going to be able to describe the normal
441:35 - and the binomial distributions
441:38 - you're going to see how the normal can
441:40 - be used to approximate the binomial
441:41 - you're going to calculate the
441:43 - approximate binomial probabilities and
441:45 - understanding why the binomial is useful
441:47 - even if we have a computer to calculate
441:49 - them exactly
441:52 - um
441:54 - so here's the arc
441:57 - we've examined recently discrete
441:59 - distributions of which the binomial is
442:02 - one
442:03 - and continuous distributions of which
442:05 - the normal is one
442:07 - we looked at probability Mass functions
442:09 - means variances sample spaces
442:13 - for the discrete we've looked at
442:15 - probability density functions means
442:17 - variance of sample spaces
442:19 - the CDF can be applied to both discrete
442:22 - and continuous as can median and
442:24 - quantiles
442:25 - today we're going to approximate one
442:28 - distribution with another
442:30 - so the first thing we have to do is
442:32 - explain what we mean by
442:35 - uh
442:36 - approximating one distribution with
442:38 - another
442:39 - short answer is that the cumulative
442:42 - probabilities are close
442:45 - that is if X1 X2 are different
442:46 - distributions that are approximately the
442:49 - same then this relationship holds for
442:52 - all values of little X
442:56 - of course there is a whole lot of detail
442:59 - hidden in that little tilde that I'm
443:01 - sorry that approximation sign
443:05 - um
443:07 - if X1 is approximately the same as X2
443:11 - how close is close enough
443:14 - how approximate is necessary it's a
443:17 - question of precision and we leave that
443:19 - up to the scientist
443:21 - I've got the statistician scientist
443:23 - comes to me and say that says I need the
443:26 - probability speed within .003 of each
443:29 - other
443:29 - from that information I can say okay I
443:32 - need a sample size of
443:34 - 1300.
443:37 - or something like that
443:39 - um
443:41 - so we're going to approximate the
443:43 - binomial X1 with the normal X2
443:47 - that is we'd like to determine some
443:49 - rules on NP mu and sigma
443:53 - to ensure that this relationship this
443:55 - approximation relationship is true
443:58 - now surprisingly it's not as difficult
444:00 - as it seems to get a good first order
444:02 - approximation
444:06 - what do we remember about the binomial
444:08 - we remember these five requirements and
444:10 - again you need to have these memorized
444:13 - the number of Trials is known each trial
444:16 - is a Bernoulli trial success probability
444:18 - is constant trials are independent and
444:22 - the random variable is the number of
444:23 - successes in those trials
444:25 - and this led to expected value and
444:27 - variance of NP and np1 minus p
444:31 - so from this it's next natural for us to
444:33 - see just how well
444:36 - this X2 distribution which is normal how
444:40 - well this approximates the binomial
444:42 - where the mean is and P and the variance
444:45 - is np1 minus p
444:48 - in other words we want to see how far
444:49 - apart those cdfs are
444:52 - so here's the CDF of a binomial 5.1
445:00 - n is 5 p is 0.1
445:03 - this is CDF
445:07 - here's the graph of a normal
445:10 - 0.5 and 0.45
445:14 - so this normally is equating to the rule
445:17 - this will be the X2 distribution
445:19 - and this should illustrate how close
445:21 - they are
445:25 - midway between the numbers one two three
445:28 - four five
445:29 - it's right on or really really close
445:32 - the closer you are to one of the
445:34 - integers the worse off it is
445:39 - in fact here is a animated graphic of
445:42 - the difference in cdfs between the
445:45 - binomial and the normal
445:46 - sample size is up here
445:50 - um
445:51 - this is the value of x
445:55 - and the the height here for any value of
445:59 - x is binomial CDF minus normalcdf
446:04 - notice we have spikes at the integer
446:07 - values
446:09 - now I would love to press play but
446:13 - unfortunately this P this Adobe Acrobat
446:17 - type program isn't Adobe Acrobat
446:21 - so I'm going to have to open up this
446:23 - file
446:25 - separately
446:30 - the sample size is changing
446:35 - notice also the worst
446:37 - is getting smaller and smaller and
446:40 - smaller so here we are at about 70. it's
446:43 - the worst is 0.05
446:47 - we're coming up on a hundred
446:50 - the worst looks to be about 0.045
446:53 - let me just keep this running and notice
446:55 - that
446:56 - the error the worst error tends to go to
446:59 - zero as the sample size increases
447:03 - but notice that it doesn't go to zero
447:04 - too quickly here we are at the sample
447:07 - size of about 200
447:10 - coming up on 200 we're going to see
447:12 - that's only about 0.03
447:14 - where's the area is 0.03 and we start
447:17 - over again
447:18 - and I don't know how to stop that from
447:20 - continuing
447:23 - 0.03
447:25 - as the difference between the normal
447:28 - binomial CDF that's 0.03 which is a
447:32 - probability so that's a large difference
447:34 - so if the correct probability is
447:37 - 0.57
447:40 - then this error could be anywhere from
447:42 - or this
447:43 - would be anywhere from 0.54 to 0.60 and
447:47 - that's a big range big error
447:50 - introduced but as you saw it's a sample
447:53 - size increase that
447:55 - the worst absolute error went to zero
448:00 - so large sample size
448:02 - we can go ahead and
448:04 - say the normal and the binomial are
448:07 - close enough
448:08 - that also kind of leads to the physicist
448:11 - who tells me I need to be within 0.03
448:14 - 0.03 I can come back and say okay sample
448:17 - size needs to be at least 200. or the
448:19 - physicist comes to me and she says I
448:21 - want it to be within .003 I'll come back
448:24 - with okay sample size needs to be at
448:26 - least twenty thousand or whatever
448:31 - so one conclusion is the approximation
448:33 - increases as values of n increase
448:36 - we're going to see that again when we
448:37 - get to the central limit theorem in
448:39 - chapter seven
448:40 - um
448:41 - with additional exploration we could
448:43 - come to a second conclusion that the
448:45 - approximation is better for values of P
448:47 - close to 0.5
448:50 - the example I gave you P was 0.1
448:54 - so if p is close to 0.5 we don't need as
448:57 - large as sample size as we do if p p is
449:01 - 0.1
449:03 - so combining these two observations
449:05 - leads to the following quote rule of
449:07 - thumb
449:08 - and this rule of thumbs tends to change
449:11 - from source to source
449:14 - um
449:15 - the normal distribution is sufficiently
449:17 - close to the binomial if both NP
449:20 - and N times 1 minus P are at least five
449:25 - I happen to think 15 or 20 is a better
449:29 - rule for that
449:33 - and if I need a lot of precision that
449:37 - I'll need NP to be at least
449:39 - 1000 or maybe two thousand
449:43 - and the approximation is improved by
449:45 - using quote a continuity correction of
449:47 - 0.5 added to or subtracted from the
449:50 - number value
449:52 - we'll have an example of that shortly
449:56 - this is not the best cracking but it's a
449:58 - good correction
450:00 - so we're going to use a con continuity
450:02 - correction factor to describe the area
450:04 - under the normal curve that approximates
450:06 - the probability that at least two people
450:08 - in a math class of 50
450:10 - regularly cheat on their tests
450:13 - assume the number of people in the math
450:15 - class 50
450:16 - who consistently cheated on their test
450:18 - has a binomial distribution with a mean
450:20 - of five and a standard deviation of 2.12
450:26 - . so in other words if x is the number
450:28 - of students cheating in the class X
450:30 - follows the binomial of 50 with a p of
450:32 - 0.1
450:34 - we need to calculate probability that X
450:37 - is greater than 2.
450:41 - so
450:43 - we're going to begin by converting the
450:45 - discrete number 2 into an interval
450:47 - by adding 0.5 and subtracting 0.5 from
450:50 - it
450:51 - I have discreet number two is changed to
450:53 - a continuous interval from point from
450:56 - 1.5 to 2.5
451:00 - here's a picture of that
451:05 - so for the discrete we could just use
451:08 - two
451:10 - for the continuous
451:13 - because the probability of x equal to is
451:16 - zero just like x equal to any single
451:19 - number we need to change it into an
451:21 - interval
451:22 - so that 2 will be replaced by the
451:24 - interval from 1.5 to 2.5
451:29 - now we're going to draw a normal curve
451:31 - with the mean of 5 and a standard
451:32 - deviation of 2.12 those were given to us
451:35 - and indicate the interval from 1.5 to
451:38 - 2.5 to represent the number two that's
451:40 - what this is the red curves the normal
451:43 - of mean five standard deviation of 2.12
451:47 - and I've got the value of 2 in the
451:50 - double shaded or the Shaded and hashed
451:55 - now we're going to shade the area
451:57 - corresponding to the phrase quote at
452:00 - least two
452:01 - because we need to calculate the
452:03 - probability of at least two
452:09 - and that's the blue shaded area
452:12 - it's this hashed part which is 2 and
452:16 - above
452:21 - so in other words if x is binomial and Y
452:24 - is the normal approximation
452:26 - probability of X greater than 2 is about
452:29 - probability of y greater than or equal
452:31 - to 1.5
452:33 - because it's everything blue
452:35 - if I were calculating the probability of
452:38 - X being less than 2 it would be y less
452:42 - than or equal to 2.5
452:47 - use a normal distribution to estimate
452:49 - the probability of more than 55 girls
452:51 - being born in 100 births
452:54 - so exactly we need to calculate
452:56 - probability X is greater than 55. given
452:59 - X follows a binomial distribution out of
453:02 - 100 P of 0.5
453:06 - using a normal approximation that means
453:09 - we're going to define a y variable to
453:11 - follow a normal distribution y follow a
453:14 - normal distribution with the expected
453:16 - value of 50
453:18 - 100
453:19 - times 0.5
453:21 - and variance of 100 times 0.5 times 1
453:25 - minus 0.5
453:28 - and we're going to calculate the
453:29 - probability that Y is greater than
453:33 - 40 uh I'm sorry that Y is greater than
453:36 - 54.5
453:47 - there we go
453:49 - now we could calculate the exact answer
453:51 - of 13 percent
453:55 - 13 5 6 2 65.
453:59 - notice how close the approximation is
454:01 - though
454:04 - this is using the p binom
454:07 - and this is using the P Norm
454:10 - very close and the sample size is only a
454:12 - hundred
454:14 - I say it's very close we're off on the
454:16 - fourth digit fifth digit
454:20 - so if I need Precision beyond the fifth
454:22 - digit
454:22 - then
454:24 - I'm going to have to have a larger
454:25 - sample size
454:27 - if I did not use the continuity
454:29 - correction I'd be much further off
454:35 - after many hours of studying you believe
454:37 - you have a 90 or 90 probability of
454:40 - answering any given question correctly
454:43 - test is 50 true false questions oh that
454:45 - would be horrible if I gave you 50 true
454:47 - false questions for the test
454:50 - or would it now I wouldn't do that
454:52 - assuming that your estimate is the true
454:55 - probability that you will answer a
454:57 - question correctly let's estimate the
454:59 - probability that you'll miss no more
455:01 - than four questions
455:03 - so each of the 50 true false has a
455:06 - probability of 90 being right
455:09 - and we want to say missing no more than
455:12 - four questions so the probability of
455:13 - missing will be 0.1
455:16 - 50 questions this is the exact
455:19 - distribution of X this is the exact
455:21 - probability to calculate
455:24 - we can approximate this binomial with a
455:26 - normal
455:28 - y will follow a normal distribution of
455:31 - expected value 50 times 0.1 and variance
455:34 - of 50 times 0.1 times 0.9
455:38 - and we'd calculate the probability that
455:40 - Y is less than or equal to 4.5
455:44 - it's too far
455:47 - 40 percent
455:49 - the real answer is 43 percent so I'm off
455:53 - by two and a half percent
455:59 - many toothpaste commercials advertised
456:01 - at three out of four dentists recommend
456:03 - their brand of toothpaste using a normal
456:05 - distribution to estimate the probability
456:08 - that in a random survey of 400 dentists
456:12 - so we'll be using random sampling simple
456:15 - random sampling
456:17 - exactly 300 will recommend Brand X
456:20 - toothpaste
456:22 - we're going to assume the commercials
456:23 - are correct and therefore there's a 75
456:25 - percent chance that any given dentist
456:27 - will recommend Brand X
456:30 - so if x is the number in that sample of
456:32 - 400 x follows exactly a binomial
456:37 - and a 400 P of 0.75
456:41 - and we need to calculate exactly x
456:42 - equals 300.
456:45 - the approximation y will follow the
456:47 - normal distribution expected value of
456:50 - 400 times 0.75
456:52 - variance of 400 times 0.75 times 0.25
456:56 - and we need to calculate the probability
456:58 - that Y is between
457:00 - 299.5 and 300.5
457:08 - and we get 0.046
457:12 - 0403
457:15 - and the real answer is 0.046
457:19 - O2
457:21 - 432
457:23 - so the normal distribution with the
457:25 - continuity correction did a very good
457:27 - job here as well
457:31 - without the continuity correction this
457:33 - would be terrible
457:35 - in fact without the continuity
457:36 - correction
457:37 - the probability estimated would be zero
457:40 - because you'd be calculating the
457:41 - probability that y was equal to 300.
457:51 - so learning check
457:53 - what's meant by the normal distribution
457:54 - approximates the binomial
458:01 - correct their cdfs are quote close
458:06 - what are two requirements for the
458:08 - approximation to be quote good
458:14 - right n times p is at least five and N
458:19 - times 1 minus p is at least five
458:21 - again I would recommend higher numbers
458:23 - for those if I really really care about
458:26 - the results
458:28 - how do we make the approximation better
458:34 - yes larger sample size you'll find in
458:37 - statistics at a larger sample size
458:39 - solves a whole lot of problems
458:42 - from the economic standpoint however
458:44 - larger sample size causes problems
458:47 - because it costs time money other
458:51 - resources to collect that larger sample
458:54 - size
458:56 - what is the continuity correction why is
458:58 - it used
459:02 - very good continuity correction is used
459:05 - to help ensure that the normal
459:07 - approximation to the binomial is quote
459:09 - better
459:10 - continuity correction itself is to treat
459:13 - an equals as adding 0.5 and subtracting
459:17 - 0.5 and calculating that
459:19 - interval
459:23 - in the future we're going to explore the
459:24 - normal integrator detail we're going to
459:27 - use the central limit theorem which will
459:28 - be chapter 7 and discover that the
459:31 - binomial is not special the normal
459:33 - distribution is
459:36 - um
459:37 - here's a question that you may have
459:40 - we have a computer
459:43 - we can easily calculate these binomial
459:46 - probabilities exactly why do we still
459:48 - need to approximate a binomial with the
459:50 - normal
459:52 - and here here's the answer the examples
459:55 - that we're given today
459:57 - we're given where you could calculate
460:00 - exactly and approximately and compared
460:02 - just so you get a feel for how how good
460:04 - this approximation is
460:07 - when we get to trying to estimate a
460:10 - single proportion we're going to stick
460:12 - with the exact way of doing it using the
460:15 - binomial distribution and everything we
460:17 - know about the binomial
460:19 - however when we start to look at
460:21 - comparing two proportions
460:25 - or trying to estimate the difference
460:27 - between two proportions to population
460:29 - proportions
460:31 - we won't be able to use an exact
460:33 - distribution because we would need to
460:35 - find the distribution of the sum of two
460:37 - binomials
460:39 - and that doesn't exist
460:42 - however we do know the distribution of
460:45 - the sum of two normals that's the normal
460:47 - distribution
460:48 - so we would use we would have to use the
460:52 - normal approximation when we start
460:53 - comparing two proportions until we get
460:56 - there we can use the exact form
461:01 - now with that said some books and I
461:03 - think Hawks does this
461:05 - Hawks will use the normal approximation
461:08 - for even the one population parameter s
461:11 - population proportion estimating
461:14 - so there is reason to look at this
461:16 - approximately in the binomial
461:18 - not just it's the next section in the
461:20 - chapter
461:22 - here are the r functions we've seen
461:24 - these already d by Nom P by Nom and P
461:27 - Norm
461:29 - here's some readings appendix C and R
461:32 - for starters and Hawks section six five
461:36 - and then Wikipedia Central limit theorem
461:39 - appendix C and the central limit theorem
461:42 - and Wikipedia will give you a good
461:44 - background for chapter seven and this
461:48 - approximating the normal approximating
461:51 - the binomial distribution with the
461:52 - normal
461:53 - is actually a result of the central
461:56 - limit theorem
461:59 - so questions where are those questions
462:02 - there they go
462:04 - question one
462:06 - again I recommend writing these
462:07 - questions in the left hand side of your
462:08 - notes answering them below so you can
462:10 - transform the trans put them into Moodle
462:13 - quiz what does it mean for one
462:15 - distribution to approximate another
462:18 - question two
462:20 - what are the mean and the standard
462:21 - deviation of a binomial
462:24 - question three what are the mean and
462:27 - standard deviation of a normal
462:28 - distribution
462:33 - and that's it
462:35 - take care
462:37 - hello and welcome to section 7.1 where
462:39 - we introduce the central limit theorem
462:42 - Central limit theorem I would argue is
462:44 - the most important theorem in all of
462:45 - introductory statistics in fact I'd
462:47 - argue it's the most important theorem in
462:49 - all of Statistics it helps to explain
462:51 - that
462:52 - when we have data that comes from a
462:56 - continuous distribution
462:58 - and we're trying to estimate the
463:00 - population mean
463:02 - we don't really care that much about the
463:04 - distribution of the data we just quote
463:06 - pretend that the data comes from a
463:09 - normal distribution because the central
463:10 - limit theorem says Hey regardless of
463:13 - where the data comes from the sample
463:15 - means follow a normal distribution and
463:18 - it's the sample means that we're going
463:19 - to use to understand our population mean
463:21 - in the future so from this point forward
463:24 - when we're dealing with continuous data
463:26 - the sample means are going to follow a
463:28 - normal distribution or it's going to
463:29 - look like a normal distribution and we
463:31 - see that in lab C so by the end of this
463:34 - lecture you're going to be able to State
463:35 - the central limit theorem say the
463:37 - requirements for applying it and state
463:39 - its consequences
463:41 - so in recent Computing activities we've
463:44 - examined drawing random samples from a
463:46 - known distribution we've looked at
463:48 - calculating sample means from subsets of
463:50 - those distributions we've created
463:51 - histograms of the distributions of those
463:53 - sample means today we're going to
463:55 - examine the central limit theorem which
463:56 - kind of gives a mathematical explanation
464:00 - for everything that we've observed about
464:02 - those sample mean distributions
464:05 - so here's the statement of the central
464:06 - limit theorem let X be a random variable
464:10 - with a mean
464:11 - we're going to call that mean mu and
464:13 - finite variance Sigma squared and we're
464:17 - going to draw a random sample of size
464:19 - and from this distribution
464:22 - so that first paragraph is all about the
464:26 - data itself the data comes from some
464:29 - distribution with a mean and a variance
464:32 - second paragraph then the distribution
464:36 - of the sample sums converges to a normal
464:39 - distribution as n gets larger
464:42 - so the first paragraph said we don't
464:44 - care about the distribution of the data
464:46 - second paragraph says therefore
464:49 - the distribution of the sample sums
464:51 - converges to a normal distribution so
464:54 - the data could be exponential the data
464:56 - could be uniform the data could be
464:57 - binomial for All We Care
464:59 - it's the sample sums that are going to
465:02 - be normally distributed as n gets larger
465:05 - specifically
465:07 - this thing on the left is just how we
465:09 - represent sample sums we're adding up
465:11 - this is a big summation adding up over
465:14 - all n values in our sample those values
465:18 - it converges in distribution that's the
465:21 - D on top of the arrow means it converges
465:23 - in distribution to a normal
465:25 - with the expected value of n times mu
465:28 - and a variance of n times Sigma squared
465:32 - we've actually kind of seen something
465:35 - like this in the past but let's keep
465:38 - working forward
465:39 - the proof of this theorem is beyond the
465:41 - scope of this course I think the first
465:43 - time you get to see it is in math 321
465:45 - happens at the end of math 321 after
465:48 - you've learned about these things called
465:50 - moment generating functions
465:53 - um so our first intra lecture
465:56 - question
465:58 - is
466:00 - here
466:02 - again on the left hand side of your
466:04 - notes write the question write the
466:06 - answers
466:08 - so that you can come back to it later
466:09 - remember you got the pause button
466:15 - so there are three main consequences
466:18 - um
466:19 - Central limit theorem tells us the
466:20 - following one it tells us that the sum
466:22 - of independent random variables is more
466:25 - normal than the distribution the
466:26 - variable itself
466:28 - of course if the data do come from a
466:30 - normal distribution you already start at
466:32 - normal
466:33 - so you don't have any place to go but if
466:36 - you start at uniform it takes a little
466:38 - bit to get to normal if you start
466:39 - exponential it takes a little bit longer
466:41 - to get to normal
466:43 - um
466:44 - recall that the binomial is the sum of
466:46 - independent or newly random variables we
466:49 - use that fact to prove things about the
466:50 - binomial specifically the expected value
466:52 - and the and the
466:54 - select the value in the variance poisson
466:57 - happens to be the sum of independent
466:58 - poissons we've alluded to that in the
467:01 - past
467:02 - but that means that as n increases the
467:06 - binomial becomes more normal
467:09 - and as Lambda increases the poisson
467:12 - becomes more normal it's because the
467:13 - poisson is just the sum of independent
467:15 - poissons and binomial is the sum of
467:17 - independent bernoullis
467:20 - kind of powerful right there
467:23 - a second consequence is because the
467:26 - sample mean is just the sample sum
467:27 - divided by constant
467:30 - the central limit theorem tells us that
467:31 - the distribution of sample means will
467:33 - tend towards normal
467:36 - and again if the data starts with the
467:37 - normal this tinting towards happens
467:39 - immediately
467:41 - if it starts with the uniform it takes
467:43 - like 10 to get there if it's an
467:45 - exponential it takes like 50 or so
467:48 - but eventually those sample means will
467:50 - become as close to normally distributed
467:53 - as if you want them to be you just have
467:55 - to get a larger n sometimes
467:58 - and the third the speed of convergence
468:01 - depends on how close the data are
468:03 - distributed to normal
468:05 - the closer they are to normal the faster
468:07 - thus the normal if the data actually do
468:09 - come from a normal distribution then the
468:11 - sample means are immediately normal
468:15 - the poisson is
468:17 - closer to normal than the
468:21 - since many of the binomials so you'd
468:24 - need a Lambda smaller than the and for
468:26 - binomial
468:27 - so poissons converge faster
468:32 - let's look at the next introduction
468:34 - question it's going to ask you about the
468:37 - uniform and the exponential
468:39 - oh no it won't well it will pretty soon
468:42 - so question two is State the second
468:44 - consequence of the central limit theorem
468:49 - and then I'm going to pause
468:53 - then I'm going to go directly to
468:54 - question three which talks about the
468:57 - third consequence
468:58 - since the speed of convergence depends
469:00 - on how closely the data are to normal
469:03 - as the missing L which of these two will
469:05 - converge the fastest if the data follows
469:08 - a uniform well that converge well the
469:10 - sample means converge faster to the
469:12 - normal than if the data follow an
469:14 - exponential
469:17 - remember to write down the question on
469:19 - the left answer it so that you can
469:21 - transfer it into Moodle
469:23 - um
469:24 - again the data follower uniform well the
469:26 - sample means converge to normal faster
469:29 - than if the data Fallen exponential
469:33 - pause moving back
469:37 - um so here's one of the big uses that is
469:40 - used sometimes
469:42 - let X follow a binomial distribution
469:44 - remember there are two parameters for
469:46 - binomial n and p
469:48 - we're going to use the central limit
469:50 - theorem to approximate the distribution
469:51 - of X we actually did this back in
469:55 - section
469:57 - I believe it was section six five
470:00 - but here we're going to see why it
470:02 - actually works or we're going to apply a
470:05 - mathematical reasoning for why it works
470:07 - remember the binomial is just the sum of
470:09 - N independent bernoullis
470:12 - that is if y sub I follows that
470:14 - Bernoulli
470:16 - then X being the sum of those y's
470:18 - follows a binomial of NP
470:21 - so according to the central limit
470:22 - theorem since it's the sum of
470:24 - independent
470:26 - distributions
470:28 - the x is going to be approximately
470:30 - normal
470:32 - with a mean and a variance
470:35 - that mean is just going to be the mean
470:37 - of this binomial
470:38 - well what's the mean of a binomial it's
470:40 - n times p
470:41 - and the variance is just going to be the
470:43 - variance of this binomial well what's
470:45 - variance of a binomial n times P times 1
470:47 - minus p
470:50 - so this gets us directly to the
470:52 - approximation of the binomial with the
470:54 - normal in about one step
471:00 - more importantly for us
471:03 - okay never mind
471:05 - um we're going to let the X for another
471:06 - example follow a uniform
471:09 - a b a is the minimum value B is the
471:12 - maximum value we're going to use the
471:13 - central limit theorem to estimate the
471:15 - distribution the mean of a sample of
471:17 - size n
471:19 - remember that X bar the mean is just one
471:22 - over n times the sample total
471:26 - and we represent the sample total this
471:28 - way
471:29 - and so by the central limit theorem T
471:32 - follows approximately the dot on top of
471:34 - the tilde means approximately follows a
471:37 - normal distribution
471:39 - n times mu and N times Sigma squared
471:42 - where mu is the mean of the underlying
471:45 - distribution
471:47 - of the X distribution
471:50 - and sigma squared is the variance of
471:52 - that underlying distribution
471:54 - of the X distribution
471:57 - from our knowledge of the uniform we
471:59 - know that the mean of the uniform is
472:01 - just a plus b over 2 and the variance is
472:03 - B minus a squared over 2.
472:08 - so the sample sum
472:11 - is approximately normally distributed
472:13 - with expected value n
472:16 - times a plus b over 2 and variance of n
472:18 - times B minus a squared over two over
472:22 - twelve
472:25 - remember the question was originally
472:27 - about this distribution the sample mean
472:31 - and X bar is just one over n times t
472:35 - that means the that X bar approximately
472:38 - again dot on top means approximately
472:40 - follows a normal distribution
472:43 - expected value of
472:46 - mu n times mu divided by n
472:50 - and N times Sigma squared divided by N
472:53 - squared
472:56 - so the expected value of x bar is a plus
472:59 - b over two
473:01 - which happens to be mu
473:04 - and the variance of X bar is B minus a
473:07 - squared over 12n
473:10 - now notice something that we didn't
473:12 - notice before
473:14 - the expected value of x bar is Mu
473:18 - the variance of X bar is smaller than
473:21 - the variance of x
473:22 - so as n increases the variance decreases
473:26 - to zero
473:27 - which means these X bars that we
473:29 - actually measure from a sample The
473:31 - observed X bars are going to tend to be
473:34 - closer to the MU
473:39 - so in other words as n increases the
473:42 - Precision of our little X bar increases
473:50 - so the big question that comes in here
473:52 - is why is there an n in the denominator
473:53 - of the variance
473:56 - recall that the variance is find as the
473:58 - sum of x minus mu squared times the
474:01 - probability of that x value
474:05 - so the variance of a times x is just the
474:08 - sum of a times x minus a times mu
474:11 - squared times the probability of the x
474:12 - value
474:16 - factor out an a from both of these out
474:18 - front that gives us an a squared times x
474:21 - minus mu squared
474:23 - times the probability of the x value
474:26 - this is just a constant has nothing to
474:28 - do with the I pull it out so at a
474:30 - squared times the sum of the x minus mu
474:33 - squared times P of x
474:35 - so the variance of ax is just a squared
474:38 - times the variance of x
474:41 - in other words where does that squaring
474:43 - come from comes from the fact that the
474:45 - random variable itself is squared in a
474:47 - variance
474:51 - so when we found the variance of X bar
474:52 - that was just the variance of one over n
474:54 - times t
474:56 - pull the 1 over n out front it's 1 over
474:58 - N squared
475:00 - times the variance of t
475:03 - there's one over N squared this n times
475:05 - B minus a squared over 12 that's the
475:08 - variance of t
475:09 - the n on top cancels one of the ends of
475:11 - the bottom that gets us our variance of
475:13 - X bar
475:18 - so a learning check
475:21 - what is the main consequence for the
475:23 - central limit theorem
475:25 - I'll just pause for a little bit you can
475:27 - go ahead and hit the pause button to
475:29 - give yourself some more time
475:31 - the main consequence for the central
475:33 - limit theorem is that the sample means
475:36 - will become more and more normal
475:40 - what are the requirements for the
475:42 - central limit theorem
475:44 - the expected value and variance both
475:46 - have to be
475:47 - finite
475:50 - and
475:51 - the X's have to be independent
475:57 - why does this mean we should focus on
475:59 - the normal distribution as we move
476:00 - forward in the course
476:02 - when we care to estimate the population
476:04 - mean
476:06 - we're going to use the sample mean
476:09 - and since Central limit theorem tells us
476:12 - the sample means are going to eventually
476:13 - follow the normal distribution as long
476:15 - as n is large enough then we should just
476:18 - focus on this normal distribution
476:24 - R for starters this is appendix C3 Hawks
476:27 - this is section 7 1
476:30 - um demov LaPlace theorem is interesting
476:32 - of course Wikipedia's got a really good
476:34 - entry on the central limit theorem
476:37 - and that's the N for Section seven one
476:42 - um
476:43 - the central limit theorem and by the way
476:45 - you will see the central limit theorem
476:47 - at least in the next two lectures but in
476:49 - undergirding all lectures from this
476:51 - point forward in this course
476:53 - so enjoy
476:55 - hello and welcome to section 7 2 last
476:58 - section we looked at the central limit
477:00 - theorem here we're going to be applying
477:01 - the central limit theorem to the
477:02 - distribution of sample means
477:05 - um so by the end of this lecture you
477:07 - should be able to State the central
477:08 - limit theorem and to apply the central
477:10 - limit theorem to the problem of sampling
477:11 - distribution of the mean or what most
477:14 - books just refer to as the sampling
477:16 - distribution
477:17 - so here's the arc the way what we've
477:20 - been talking about so recently we've
477:22 - examined drawing random samples from a
477:24 - known distribution
477:25 - we've calculated sample means from
477:27 - subsets to those distributions we've
477:29 - created histograms of those
477:30 - distributions we've seen that those
477:32 - histograms tell us that the sample mean
477:35 - distributions look pretty darn normal
477:38 - in the last slide deck we introduced the
477:40 - center limit theorem that says yes they
477:43 - do look pretty darn normal and that's
477:44 - not that's not an accident
477:46 - we've looked at Central limit theorem
477:48 - and its requirements and its
477:50 - consequences and the main consequence is
477:51 - the distribution the sample means tends
477:54 - towards normal as the sample size
477:55 - increases So today we're going to
477:58 - examine the center limit theorem in
478:00 - terms of what it tells us specifically
478:02 - about the distribution the sample mean
478:04 - in the next lecture it'll be about the
478:06 - distribution the sample proportion but
478:09 - if you think back to section 7 1 in fact
478:11 - if you think back to when we first
478:13 - introduced the binomial distribution we
478:15 - realize that the sample proportion and
478:17 - the sample mean are not that different
478:20 - at all
478:22 - mind blown
478:24 - so here's the statement of the central
478:27 - limit theorem again let X be a random
478:29 - variable with a finite mean and finite
478:31 - variance let us draw a random sample of
478:33 - size n from this distribution remember
478:35 - random sample means it's an independent
478:37 - sample each of those X values are
478:39 - independent of the others and again note
478:41 - that that first paragraph talks about
478:43 - the distribution of the data
478:46 - so the data have a mean and a variance
478:49 - and the data were generated from some
478:51 - sort of independent process
478:54 - that's all we need from the data the
478:56 - data is the prerequisite to the central
478:58 - limit theorem
479:00 - so then and here this part is the
479:02 - consequence of the central limit theorem
479:03 - the distribution of the sample sums
479:06 - converges to a normal distribution
479:10 - notice the only thing we need from the
479:12 - data from paragraph one is that it has a
479:15 - mean of variance in this from a random
479:16 - sample that tells us paragraph two that
479:20 - the distribution of the sample sums
479:21 - converges to a normal distribution
479:26 - so here's our first intra lecture
479:30 - question
479:31 - State the central limit theorem as I
479:33 - have stated it in this lecture
479:40 - that question is pretty familiar to us
479:44 - again
479:45 - write the question on the left hand side
479:46 - answer it on the left hand side of your
479:49 - notes therefore when you get to Moodle
479:51 - you can write it in nice and easily
479:57 - here's a corollary of it it's the
479:59 - distribution the sample mean
480:02 - so we're going to start with X being a
480:03 - random variable mean mu finite variant
480:06 - Sigma squared draw random sample size n
480:08 - from this distribution in other words
480:09 - that first paragraph notice is exactly
480:12 - the same as the first paragraph of the
480:13 - central limit theorem therefore those
480:16 - tell us that we can now apply the
480:18 - central limit theorem
480:20 - consequence of this is the distribution
480:22 - the sample mean is
480:25 - approximately normal
480:27 - here's some notation X bar is the sample
480:30 - mean sub n we're going to index it by n
480:33 - to indicate the sample size
480:36 - this converges in distribution to a
480:38 - normal expected value of mu variance of
480:41 - one over n times the variance of the
480:43 - original data
480:52 - here's the proof
480:54 - from the central limit theorem we know
480:56 - this is true
480:57 - we get this from that first paragraph
480:58 - this first paragraph same as in the
481:01 - central limit theorem so this is the
481:02 - consequence that first paragraph
481:04 - boom
481:06 - now we wonder about the distribution of
481:08 - X bar well X bar so that is just one
481:10 - over n times the sum of those X I's
481:13 - okay look at this sum of the x i is what
481:15 - distribution does that have
481:17 - normal
481:18 - boom
481:20 - expected value of some of the X sizes n
481:23 - mu expected value of one over n times
481:25 - the sum of the X I's is Mu
481:30 - the variance of the sum of the X I's is
481:32 - n Sigma squared the variance of one over
481:35 - n times those X Out of the sum of the X
481:37 - I's is one over n times Sigma squared
481:43 - why are we dividing by n instead of
481:45 - having just Sigma squared well set proof
481:48 - one this is y the expected value of x
481:51 - bar is Mu
481:55 - I'll find the expected value of x bar
481:57 - sub n substitutes pull out because
482:00 - expected value is a linear operation
482:04 - expected value of ax is a times the
482:07 - expected value of x in this case a is at
482:10 - one over n
482:11 - we know the expected value of T is just
482:13 - n mu the ends cancel that was easy
482:17 - set proof two with the variances if we
482:20 - want to find the variance of X bars of n
482:21 - substitutes
482:23 - 1 over n times t
482:26 - pull out that one over n it's 1 over N
482:29 - squared
482:30 - why is it now squared look back to the
482:34 - lecture on Section 7 1.
482:39 - because there's a square in the
482:41 - calculation of the variance of the
482:42 - random variable
482:44 - variance of T is just n Sigma squared
482:46 - that n and one of these cancel out so
482:48 - we're left with one over n times Sigma
482:50 - squared
482:54 - and that brings us to the conclusion of
482:56 - the Corollary
482:59 - and this is exactly what we just showed
483:02 - so let's have a couple intra lecture
483:05 - questions
483:06 - two
483:08 - how does the distribution the sample
483:10 - mean depend on the distribution of the
483:11 - data
483:19 - and might as well do question three now
483:22 - while we're at it
483:24 - since the speed of convergence depends
483:26 - on how closely the data are to normal I
483:28 - got the L in there finally I edited it
483:29 - which of these two will converge fastest
483:32 - in the uniform or the exponential
483:38 - yep question one and three are from the
483:40 - last slide deck
483:42 - question two
483:46 - it's kind of the Q is the most important
483:48 - part of this slide deck
483:51 - so let's go back to some examples
483:54 - example one let's say I draw a sample of
483:57 - size 14 from a population or that has a
484:00 - mean of 126 and a variance of 42
484:04 - what's the approximate distribution the
484:06 - sample means notice I didn't tell you
484:09 - that X followed a normal distribution I
484:11 - didn't tell you X followed a uniform
484:12 - distribution I didn't tell you X
484:14 - followed an exponential distribution it
484:16 - doesn't matter
484:17 - I know X has a finite variance therefore
484:20 - I can apply the central limit theorem
484:23 - whether or not n equals 14 is large
484:25 - enough for this approximation to be good
484:27 - that's another question
484:30 - all I want is an approximate
484:31 - distribution the sample means
484:34 - so this is a straightforward application
484:36 - of the main corollary from this section
484:43 - approximate distribution the sample
484:44 - means X bar is approximately normally
484:47 - distributed with expected value mu of X
484:50 - bar being 126 the same as the expected
484:53 - value of the original distribution
484:56 - and variance of being one over n times
484:59 - 42.
485:01 - 42 is the original variance or I'm sorry
485:04 - the variance of the original
485:05 - distribution
485:07 - it's 1 over n times that Sigma squared
485:10 - 42 over 14 is 3 so X bar approximately
485:14 - normal mean of 126 variance of 3.
485:24 - example two
485:26 - I've been told that the average adult
485:28 - height for males in the United States
485:29 - has mean of 69 inches in standard
485:32 - deviation of 3
485:35 - what's the probability of having the
485:36 - mean of a sample of size 2 being less
485:39 - than 65 inches
485:42 - so in the last example straightforward
485:44 - application we didn't do anything with
485:46 - the distribution of X bar here we're
485:49 - going to do something with that X bar
485:50 - we're going to ask what's the
485:51 - probability of observing an X bar being
485:54 - less than 65 inches
485:58 - um so we're asked ultimately what is the
486:01 - probability of having the mean being
486:02 - less than 65 we're asked to calculate
486:04 - this
486:05 - now if you want to do a sub 2 here that
486:07 - would be fantastic
486:09 - so we're asked to calculate the
486:10 - probability of X bar sub 2 being less
486:12 - than 65. that means we need to know the
486:14 - distribution of X bar sub 2.
486:17 - we have we're trying to solve a
486:19 - probability statement about X bar sub 2
486:21 - we really need to know the distribution
486:23 - of X bar sub 2. so to calculate this
486:26 - we're going to use the central limit
486:27 - theorem X bar sub 2 is just is
486:31 - approximately normal expected value the
486:33 - same as the original distribution and
486:36 - variance equal to one over n times the
486:38 - original variance
486:40 - notice I told you the standard deviation
486:42 - was three therefore the variance is nine
486:45 - so we know X bar sub 2 approximately
486:48 - normal mean of 69 variance of 4.5
486:55 - now we use that to answer our original
486:57 - question about the probability of X bar
486:59 - being less than 65.
487:04 - using R this is p Norm remember it's got
487:07 - to be a less than part 65 is from here
487:11 - m is equal to 69 s which for R needs to
487:15 - be the standard deviation that's just
487:18 - the square root of the variance
487:20 - the variance is 3 squared over two
487:24 - so this our code will get us 0.0297
487:28 - in other words there's about a three
487:31 - percent chance that I will observe at
487:34 - average being less than 65.
487:37 - given that the mean of the population is
487:40 - 69 and the variance of the population is
487:43 - 9.
487:44 - in other words this property this this
487:47 - probability is small
487:50 - and therefore my beliefs about the
487:53 - original population are unlikely
487:57 - that's the probability of observing this
487:59 - event given our assumptions are correct
488:00 - is quite small so either I did not
488:02 - observe this event or my assumptions are
488:05 - not true
488:11 - so example one straightforward
488:13 - application of the central limit theorem
488:15 - example two okay now that we've got the
488:17 - distribution of X bar let's see how we
488:19 - can use this
488:21 - okay
488:23 - now my sample is size 10.
488:26 - same mean and variance of the
488:30 - adult height for males
488:33 - um
488:35 - but now I'm doing a sample of size 10. I
488:38 - want to know what's the probability of
488:39 - that sample of size 10 being less than
488:41 - 65.
488:42 - same statement it's probability of X bar
488:44 - being less than 65 except in this case n
488:47 - is now 10.
488:49 - so from the central limit theorem X bar
488:51 - follows an approximately normal
488:52 - distribution with mean of 69.
488:55 - from the original distribution and
488:57 - variance of one over n
489:00 - times the original variance
489:02 - and the original variance is 3 squared
489:05 - so X bar now follows this distribution
489:11 - notice the expected value the same the
489:13 - variance is smaller
489:14 - so as the sample size increases the
489:16 - variance of X bar gets smaller
489:21 - so again I want to calculate probability
489:23 - of X bar being less than 65 this is p
489:25 - Norm of 65.
489:28 - mu m is equal to 69
489:31 - s standard deviation is just the square
489:34 - root of the variance of 0.9
489:38 - this is .000124
489:42 - so again
489:43 - very small probability
489:46 - meaning
489:47 - if everything I said is true
489:50 - then the probability of me observing
489:53 - this is small incredibly small it's one
489:56 - and ten to the negative fifth it's one
489:58 - in ten thousand
490:01 - since the probability of me observing
490:03 - this is so small I no longer really
490:05 - believe all of my assumptions
490:08 - maybe mu for the population isn't 69.
490:12 - maybe the variance of the population
490:14 - isn't nine
490:17 - maybe the distribution of the population
490:19 - is so non-normal that the central limit
490:21 - theorem is not helping when n is equal
490:23 - to 10.
490:28 - but still there is a one in ten thousand
490:30 - chance that my assumptions are correct
490:32 - and I observe this event so it is
490:35 - possible
490:36 - but it really draws brings into
490:40 - really makes you doubt the assumptions
490:49 - fourth example crime
490:52 - uh the 2000 crime rate for the 50 states
490:55 - plus DC are given in the data file crime
490:58 - we need to find a 95 confidence Central
491:00 - confidence interval for the mean violent
491:02 - crime rate
491:05 - this is not the first time you've seen
491:06 - the word confidence interval
491:09 - um
491:10 - that would be one of the labs
491:15 - so here we're asked to calculate the
491:17 - 2.5th and the 97.5 percentiles
491:20 - of the sample means
491:23 - drawn from the 2000 violent crime rate
491:29 - why is it the 2.5th and 97.5th
491:34 - note the difference is 95
491:37 - and no she got 2.5 percent below and 2.5
491:40 - percent above so that gives us the
491:42 - central part
491:44 - so those two quantiles will give us the
491:47 - central the endpoints of that Central 95
491:49 - confidence interval
491:52 - it's a confidence interval so it's going
491:54 - to be an interval on the means
492:00 - the confidence interval so it's going to
492:02 - be an interval on the means
492:04 - probably want to put a star there some
492:06 - googly eyes staring at it and big
492:07 - expression of wow
492:10 - one way of estimating this confidence
492:12 - interval is to play the corollary to a
492:14 - central limit theorem
492:17 - from the data we've got a mean of 441.55
492:20 - and a standard deviation of 241.45
492:25 - thus by the corollary Central limit
492:27 - theorem we have X bar is approximately
492:30 - normal
492:31 - expected value of 441.55 and variance of
492:35 - 1 over n times the variance of the data
492:37 - itself
492:40 - and so the endpoints for the 95
492:42 - confidence interval of the 2.5 the 97.5
492:45 - quantiles of that distribution
492:48 - which in R we can do this way
492:50 - let's look at all the parts that's a q q
492:53 - Norm is for the quantiles of a normal
492:55 - distribution
492:58 - these are the two percents that you want
493:00 - or the two proportions you want the
493:02 - 2.5th percentile and the 97.5 percentile
493:07 - got a put them together in a c function
493:11 - followed by a comma followed by how
493:13 - you're defining this distribution mean
493:16 - of 441.55 standard deviation of
493:20 - 241.45 divided by the square root of 51.
493:23 - I could have written this as the square
493:25 - root of
493:27 - 241.45 squared over 51 but the square
493:30 - root of a square you're just pulling
493:31 - that 241 through 45 out front
493:34 - so in other words this is really just s
493:36 - over square root of n
493:40 - so we're 95 confident that the actual
493:42 - mean is between 375 and 508.
493:46 - and that's finite crimes per 100 000.
493:51 - confidence interval is on the mean
493:56 - um
493:57 - review back what an observation interval
494:00 - is that's on all the observations you've
494:02 - seen
494:02 - recall back with a prediction interval
494:04 - it's on all the observations you will
494:06 - see in the future confidence interval is
494:10 - about the means the sample means
494:15 - we could also in this part is
494:17 - bootstrapping but also estimate the
494:19 - confidence interval from the data itself
494:22 - using a process that's called
494:23 - bootstrapping
494:25 - here's the code to do it and this will
494:28 - give us a 95 confidence interval from
494:30 - 380 to 510.
494:33 - I'm going to go through the line to this
494:35 - code very carefully shortly but notice
494:38 - that the confidence interval you get
494:40 - through bootstrapping is 385 10 but what
494:42 - we got from using a normal approximation
494:45 - was 375 to 508.
494:48 - in other words not too different
494:53 - Okay so
494:55 - five lines six lines
494:58 - here are the two key lines
495:01 - they're inside the for Loop actually
495:03 - this line is key and this line is key
495:08 - so let's look at the x equals line
495:10 - we're drawing a sample
495:12 - for the violent crime rate in 2000.
495:16 - it's going to be a random sample that's
495:17 - what the sample command does draws it
495:19 - and allows you and it does replace
495:21 - equals true which means it allows you to
495:24 - draw a state's crime rate more than once
495:28 - if replace equals true isn't there then
495:31 - this is just going to give you the same
495:33 - 51 values in a different order each time
495:36 - but the same 51 values and if it's the
495:38 - same 51 values the mean of those values
495:40 - will be the exactly the same each time
495:42 - so we need to replace equals true to do
495:45 - bootstrapping
495:48 - so this x equals line will draw a random
495:51 - sample from the violent crime rate in
495:53 - 2000 allowing for states to be chosen
495:55 - multiple times
495:57 - stores it in the variable X
495:59 - second line
496:00 - second important line we calculate the
496:03 - mean of that sample
496:07 - we're going to store it in the variable
496:08 - called MN
496:11 - MN is for mean but we're going to store
496:13 - it in this variable
496:14 - bracket I bracket
496:17 - that is done to allow us to
496:21 - remember or to allow R to remember each
496:24 - of those sample means
496:26 - so x equals that gives us the sample
496:29 - this line gives us the sample mean of
496:31 - that sample storing it in the variable m
496:34 - n in the ith position
496:36 - and bracket I bracket will mean that
496:38 - this is stored in the ith position
496:40 - we're going to do that 10 000 times
496:43 - that's what the for Loop
496:45 - does
496:47 - first time through I is equal to 1
496:49 - because 1 colon 1e4 is one two three
496:53 - four all the way up to ten thousand
496:55 - first time through I is equal to 1 draws
496:58 - a sample calculates the mean of that
497:00 - Sample Stores it in the variable MN in
497:02 - the first position
497:04 - closing brace goes back up here I is now
497:07 - equal to 2.
497:09 - draws another sample calculates the mean
497:12 - of that stores in the variable M and in
497:14 - position two
497:17 - closing brace goes back up here I is now
497:20 - equal to three
497:22 - draws a ram sample calculates the mean
497:24 - stories in position three this is
497:27 - continued ten thousand times
497:29 - so at the end of this at the end of this
497:31 - loop at the end of these four lines the
497:34 - variable m n will con contain 10 000
497:37 - values
497:38 - each of those ten thousand values will
497:40 - be a sample beam from this distribution
497:47 - the bottom line gives us
497:49 - the endpoints of that 95 confidence
497:52 - interval remember this is sample means
497:55 - so an interval on Sample means will be
497:57 - the confidence interval
497:58 - quantile function you give it the vector
498:01 - first and then you specify it's the
498:04 - percentiles you want
498:09 - so we've explained every single line
498:10 - there except the first
498:13 - in R when you're building a vector such
498:16 - as the way that we're doing it here you
498:18 - need to tell R to set aside memory for
498:20 - that vector
498:21 - this line
498:23 - tells our hey we're going to create the
498:25 - vector MN it's going to be a numeric
498:27 - Vector it's going to hold numbers
498:29 - and you just need to set aside memory so
498:32 - that we can build it and then this Loop
498:35 - will build it and then at the end we're
498:36 - going to use that MN
498:41 - I strongly suggest at this point
498:44 - you type this in you run it and you see
498:47 - that it actually does work
498:49 - your confidence interval will be
498:50 - slightly different
498:52 - why will your confidence interval be
498:54 - slightly different or why could it be
498:55 - slightly different
498:57 - because this is a random sample
498:59 - we're drawing a random sample so your
499:01 - random sample will be different from my
499:03 - random sample the fact that we're doing
499:05 - this ten thousand times means that the
499:07 - interval endpoints are probably going to
499:09 - be really really close
499:12 - if you want to make them even closer we
499:14 - do this a million times one E6
499:23 - so here's the question why is there a
499:24 - difference between the two confidence
499:25 - intervals doing it bootstrap it's three
499:28 - five ten
499:30 - doing it using the normal approximations
499:31 - 375-508
499:34 - the reason it's different
499:37 - is here we're assuming that the sample
499:40 - means follow normal distribution
499:44 - and here we're just we're not making
499:46 - that assumption here we're saying okay
499:48 - data I don't care what distribution you
499:50 - have I don't care what the distribution
499:52 - of the sample means actually is I want
499:55 - to see the I want to have a good
499:58 - estimate for that confidence interval
500:02 - since the endpoints are so close to each
500:04 - other
500:05 - that tells me that this normal
500:07 - approximation is not a bad approximation
500:10 - when the sample size is 51.
500:13 - if the sample size were 5000 I would
500:16 - expect the this interval to be the same
500:19 - as this interval
500:21 - if the sample size were 5 I would not be
500:24 - surprised if this interval and this
500:25 - interval were very different
500:32 - they give you all of them I did give you
500:34 - all of them okay so learning check
500:36 - what's the main consequence of the
500:38 - central limit theorem you can hit pause
500:40 - come up with an answer and then I'll
500:42 - give you the answer
500:44 - the distribution of the sample means is
500:46 - approximately normal
500:50 - what is the sampling distribution for
500:52 - the mean
500:53 - sampling distribution for the mean is
500:55 - normal
500:56 - with expected value of mu
500:58 - and variance of Sigma squared over n
501:04 - how does the sampling distribution for
501:06 - the mean depend on the distribution of
501:08 - the data
501:09 - if the data are normal
501:11 - then the sampling distribution for the
501:13 - mean is exactly normal
501:15 - if it is not normal
501:17 - then the farther the data distribution
501:20 - from normal is the larger the sample
501:23 - size needs to be for this to be a good
501:25 - approximation
501:28 - what is bootstrapping and what is its
501:30 - greatest use
501:31 - bootstrapping is using the data under
501:33 - repeated
501:35 - sampling
501:36 - to estimate a population parameter
501:39 - and I would argue its greatest use is to
501:41 - estimate the confidence interval for the
501:42 - mean
501:47 - section 72 in Hawks appendix C3
501:52 - and of course Central limit theorem you
501:54 - may want to also look at Wikipedia
501:56 - bootstrapping if you've got questions on
501:58 - bootstrapping
502:00 - and that's it
502:02 - see you in section 7-3
502:05 - hello and welcome to the last section of
502:07 - chapter seven chapter seven covered the
502:09 - central limit theorem or covers the
502:11 - central limit theorem of two of its most
502:13 - important applications
502:14 - section seven one you were introduced
502:17 - the central limit theorem 72 you saw its
502:19 - application to sample means in this
502:22 - section you're going to see it's uh its
502:24 - application to sample proportions
502:29 - so by the end of this lecture you should
502:30 - be able to State the central limit
502:31 - theorem apply the central limit theorem
502:34 - to the problem of the sampling
502:35 - distribution of the proportion
502:38 - estimate the sampling distribution for
502:41 - the proportion
502:42 - and understand the relationship between
502:44 - the sample size and the Precision of the
502:46 - estimate we've been hinting at facts
502:50 - that there is a a strong relationship
502:53 - between sample size and precision for a
502:55 - long time here we're going to actually
502:57 - see it in the form or in the guise
502:59 - something we call power
503:03 - here's the Arc in recent computer
503:05 - activities the scas we've examined
503:08 - random draws a calculating sample means
503:10 - from subsets histograms from those we've
503:13 - seen that those histograms illustrate
503:14 - that the sample means look pretty normal
503:17 - and then we looked at the central limit
503:19 - theorem and saw oh that makes sense
503:20 - because the central limit theorem says
503:23 - sample means are going to tend to be
503:25 - normal especially if the sample size is
503:26 - large
503:27 - and the last slide deck we saw this
503:29 - application to the sample mean so boom
503:32 - today we're going to examine the central
503:35 - limit theorem in terms of what it tells
503:36 - us about the distribution the sample
503:37 - proportion
503:39 - aka the sampling distribution of the
503:41 - proportion
503:43 - so last time it was the mean this time
503:46 - it's the proportion
503:48 - so here's the the central limit theorem
503:51 - again
503:52 - the first paragraph is the premise these
503:55 - are things that must be met before you
503:56 - can apply the center limit theorem
503:59 - notice that the premises premises
504:01 - premises speak about the distribution of
504:05 - the data
504:07 - the consequence of the central limit
504:08 - theorem the second paragraph talks about
504:10 - the distribution of the sample totals
504:14 - so let X be a random variable with being
504:16 - mu finite variance Sigma squared let us
504:18 - draw a random sample in other words all
504:20 - the X's are independent of size n from
504:24 - this distribution
504:26 - notice we didn't specify that the data
504:28 - have to be normal or exponential or
504:31 - uniform or Gamma or beta or whatever
504:36 - we just said it has to have a mean and
504:38 - finite variance and the data have to be
504:40 - random draw from this distribution
504:44 - consequence then the distribution the
504:46 - sample sums
504:47 - converges to a normal distribution
504:50 - specifically T which is the sum of the X
504:53 - I's converges in distribution to a
504:55 - normal
504:57 - with expected value and mu and variance
505:00 - n Sigma squared
505:03 - which brings us to our first
505:05 - intra-lecture question
505:09 - State the central limit theorem as I
505:11 - have stated it in this lecture
505:13 - so again write this over on the left
505:15 - hand side answer it below
505:18 - this is really important because a lot
505:20 - of students in the past have thought
505:22 - Central limit theorem says the data
505:24 - become more normal that's not true the
505:27 - data central limit theorem doesn't tell
505:29 - us anything about the distribution of
505:30 - the data it requires that the data has a
505:33 - finite variance and a mean but it
505:35 - doesn't say therefore the distribution
505:37 - of the data becomes more normal no it
505:39 - says the distribution of the sums
505:41 - becomes more normal
505:47 - um
505:48 - there we are
505:50 - so there's the theorem statement
505:53 - here's the corollary for the sample
505:55 - proportion we saw one of these with the
505:57 - sample mean we're going to see it for
505:58 - the sample proportion
506:00 - let X follow a binomial distribution
506:06 - with parameter values n and p
506:10 - remember N is a sample size and P is the
506:13 - probability of success on each of the
506:15 - trials
506:16 - that the X be a random sample from n
506:18 - Bernoulli random variables
506:22 - notice again remember back to 7 1 I
506:25 - framed the binomial as being just the
506:27 - sum of independent Bernoulli's we're
506:29 - using that here
506:31 - then the distribution of the sample
506:33 - proportion which I'm going to call big p
506:37 - is which is defined as 1 over n times x
506:40 - the number of successes from that
506:42 - binomial or the number of successes from
506:44 - those and Bernoulli's so this is 1 over
506:47 - n times x
506:48 - I've seen one over n times x before I
506:51 - forget what that was converges in
506:54 - distribution to a normal
506:55 - with expected value Little P
506:58 - hmm
507:01 - and variance of P times 1 minus p over n
507:07 - so here we are for interlection question
507:09 - number two
507:11 - if I can find it
507:14 - what is the distribution of the sample
507:16 - proportion
507:20 - this is an important one
507:23 - also while you're here look at how this
507:26 - distribution compares to the
507:27 - distribution of the sample mean
507:31 - going back
507:39 - so here's the proof
507:41 - from the central limit theorem we know X
507:43 - follows in proximate normal distribution
507:45 - expected value NP
507:48 - variance np1 minus p
507:52 - that's straight out application the
507:54 - central limit theorem and P is the
507:56 - expected value of x and P1 minus p is
507:58 - the variance of x
508:02 - remember we want to find the
508:04 - distribution or the approximation
508:05 - distribution of P the sample proportions
508:08 - so expected value of P
508:11 - which is expected value of x over n
508:14 - pulling out that one over n because
508:16 - expectation is a linear operator we get
508:19 - expected value of x over n we know the
508:21 - expected value of x
508:24 - from a previous slide is just NP
508:28 - the ends cancel out we're left with p
508:31 - so the expected value of p is Little P
508:36 - so the expected value of your sample
508:38 - proportions is your population
508:40 - proportion
508:46 - normal distribution has an expected
508:48 - value and a variance so let's calculate
508:49 - the variance of your sample proportions
508:53 - again for step substitution we're going
508:55 - to factor out that n as an N squared
509:00 - check back to section seven one why
509:02 - we're factoring out an N squared has to
509:05 - do with the fact that the variance you
509:06 - square the random variable
509:08 - variance of X is np1 minus p
509:12 - this n and one of these cancels out P
509:14 - times 1 minus p over n
509:17 - therefore
509:18 - sample proportions are approximately
509:20 - normally distributed expected value of P
509:23 - variance of P times 1 minus p over n
509:28 - notice the expected value of p is Little
509:30 - P
509:32 - since the expected value of p is Little
509:34 - P this is an unbiased estimator
509:40 - and notice that as the sample size
509:42 - increases the variance also I'm sorry
509:45 - sample size increases the variance
509:47 - decreases
509:49 - in other words as n increases the
509:51 - Precision
509:53 - will increase as well
509:55 - the variance decreases Precision
509:57 - increases
510:03 - so there's our result
510:07 - some examples
510:10 - according to the U.S census 18 of
510:12 - Americans are below the poverty line
510:15 - if I randomly sample 10 people from the
510:17 - United States what's the probability
510:18 - that more than 20 of them are below the
510:20 - poverty line
510:22 - so notice I'm asking a probability
510:25 - question it's what is the probability
510:27 - that more than 20 percent are below the
510:30 - poverty line
510:31 - so I'm asked probability of P being
510:33 - greater than 20. I'm sorry 0.20
510:38 - since we're doing a pro a uh trying to
510:40 - calculate a probability about the random
510:42 - variable P we need to know the
510:44 - distribution of P
510:48 - from the corollary to the central limit
510:50 - theorem that we covered today P
510:53 - approximate is approximately normal
510:54 - expected value of 0.18
510:58 - because that's me that's p in the
511:00 - population
511:01 - and variance of P times 1 minus p over n
511:08 - so P big p is approximately normal mean
511:13 - at 0.18 variance to 0.01476
511:18 - I need to calculate the probability that
511:20 - P is greater than 0.2
511:23 - remember in order to use cumulative
511:25 - probabilities this has to be a less less
511:28 - than
511:30 - complements rule from chapter four says
511:32 - probability P being greater than
511:33 - something is one minus the probability
511:35 - of P being less than or equal to it
511:38 - we've got a less than or equal here so
511:40 - we can use the P Norm
511:42 - 0.2
511:43 - 0.18
511:46 - and this is standard deviation so it's
511:48 - the square root of the variance
511:50 - that gives us a probability of 0.4346
511:56 - that's not small
511:58 - not at all so it wouldn't shock me if
512:00 - more than 20 of my sample is below the
512:02 - poverty line
512:04 - assuming
512:07 - that the the population me uh population
512:11 - proportion is 0.18
512:14 - then the probability of me observing
512:16 - more than 20 percent
512:18 - being under the poverty line of is 0.43
512:23 - doesn't give me any evidence against my
512:26 - assumption of the 0.18
512:31 - it's a coin flip essentially
512:33 - about half the time I'll be above 20
512:35 - percent about half the time I went below
512:43 - same assumption
512:44 - of the population proportion is 18
512:47 - percent
512:49 - I'm now going to sample a hundred people
512:51 - instead of what I did in the previous
512:53 - example
512:55 - increasing the sample size
512:58 - and I want to calculate the same
513:00 - probability probability more than 20 or
513:02 - below the poverty line
513:04 - so again we're asked to calculate the
513:06 - probability that P is greater than 0.2
513:08 - we need to know the distribution of P
513:11 - from the central limit theorem
513:13 - it's 18 18 times 1 minus 18 over n and
513:19 - here is a hundred
513:23 - this gives us that distribution
513:27 - I need to calculate the probability P
513:28 - being being greater than 0.2 it's one
513:31 - minus probability of P being less than
513:32 - or equal to 0.2
513:34 - and I get a probability of 0.3
513:39 - so also not a small value
513:42 - it's about a third of the time I'll get
513:43 - a a sample proportion being greater than
513:46 - 20 percent
513:48 - if our assumption about the population
513:51 - is true that is if the population
513:55 - poverty rate indeed is 0.18
514:02 - notice the probability did go down
514:04 - though
514:05 - the previous example when n was 10 this
514:08 - was 0.46
514:09 - 4.47
514:11 - it's gone down it's 0.30 which makes
514:14 - sense because
514:16 - I collect more and more data I would
514:18 - expect my observations to average out
514:21 - being closer to the real average
514:29 - now my sample size is a thousand I'm
514:31 - going to ask a thousand people
514:33 - the first it was 10 then 100 now a
514:36 - thousand
514:38 - we're going to look at a 95 confidence
514:40 - interval
514:42 - for the sample proportion
514:46 - confidence interval is on those sample
514:47 - statistics not on observations that
514:49 - we've seen which is an observation
514:51 - interval not on observations we're going
514:54 - to see in the future which would be a
514:55 - prediction interval but on the sample
514:59 - statistic in this case sample proportion
515:06 - so with a thousand I would expect to see
515:08 - 95 of the time somewhere between 0.156
515:12 - and 0.204
515:18 - . so if reality is correct I'm sorry if
515:22 - my assumption about reality if 18 is
515:25 - correct
515:26 - and I collect a sample of size a
515:28 - thousand I expect 95 of the time the
515:30 - sample proportion is going to be between
515:32 - 15 and 20 percent
515:37 - how to get that I've got the proportions
515:40 - normally distributed so I'll use the
515:41 - norm stem
515:43 - I'm looking at the quantiles so I'll use
515:46 - Q Norm
515:48 - I'm doing the two point fifth percentile
515:50 - the 97.5 percentile
515:52 - and the rest just specifies that
515:54 - particular normal distribution
516:02 - now I'm going to ask a hundred thousand
516:03 - Americans
516:09 - and I want to know the probability that
516:10 - more than 20 of them are below the
516:12 - poverty line
516:13 - so I did ten hundred thousand now a
516:15 - hundred thousand
516:20 - and my confidence interval is 17.8 to
516:23 - 18.2
516:26 - so I would expect 95 of the time my
516:29 - sample of a hundred thousand to be
516:30 - between 17.8 and 18 point
516:40 - got the two and the eight switched
516:42 - between 17.2 and 18.8
516:48 - so if I fell out of the interval of 17.2
516:51 - to 18.8 I'd say well
516:53 - there's something wrong with my
516:55 - assumption of 18 percent
517:00 - here's a graph of the probability that
517:04 - the observed
517:06 - sample proportion is greater than 0.2
517:08 - as graphed against the sample size
517:14 - our first example is 10
517:18 - n equals 10 our probability of observing
517:21 - it was way up here at about 0.46 I think
517:24 - the second n was a hundred and the
517:26 - probability we got was about 0.30
517:29 - so as the sample size increases the
517:32 - probability of observing a extreme event
517:35 - decreases
517:39 - doesn't go away
517:41 - I mean here we are with n equal to 1000
517:43 - we've still got a probability of about
517:45 - 0.08
517:48 - so about eight percent of the time that
517:50 - sample of a thousand will give me a
517:52 - sample proportion greater than 0.2
517:58 - we can also look at this in terms of
518:00 - confidence intervals
518:02 - as the sample size increases the upper
518:06 - confidence bound and the lower
518:08 - confidence bound get closer and closer
518:10 - together
518:13 - they never become the same
518:15 - even out here at n equal to a thousand
518:17 - there's still a sizeable
518:21 - gap between the two
518:28 - so this is where power comes in
518:32 - power is the ability to distinguish
518:34 - between a true and a false null
518:39 - hypothesis
518:40 - in other words we are assuming that the
518:44 - sample I'm sorry we're assuming that the
518:47 - population proportion is 0.18
518:51 - power is the ability to say no it's not
518:53 - given our observations so if we observe
518:57 - P being greater than 0.2
519:00 - the probability of us saying no that
519:02 - 0.18 is wrong gets bigger and bigger as
519:06 - sample size increases
519:11 - in terms of confidence intervals
519:14 - remember the width of the confidence
519:16 - interval is the Precision the smaller
519:18 - the width the higher the Precision
519:20 - as the sample size increases our
519:23 - Precision increases
519:28 - most of the benefit happens in the first
519:31 - couple hundred
519:36 - what this means is as the sample size
519:39 - increases we're going to observe sample
519:43 - proportions closer and closer to our
519:46 - population proportion
519:49 - we still get some above and some below
519:51 - but there will be much more concentrated
519:53 - around the sample proportion
520:00 - which brings us to intra-lecture
520:03 - question number three
520:06 - what is power
520:12 - when we get to chapter 10 we'll have a
520:14 - much better understanding of power but
520:18 - you gotta dip your toes in here
520:25 - so learning check what's the main
520:27 - consequence for the central limit
520:28 - theorem
520:30 - the main consequence for the central
520:31 - limit theorem is that that means a
520:33 - sample means and Sample proportions are
520:35 - much more normally distributed than the
520:37 - data itself
520:39 - what is sampling distribution for the
520:41 - proportion
520:43 - the sampling distribution proportion
520:45 - capital P is approximately normal
520:48 - with expected value of Little P and
520:51 - variance of P times 1 minus p over n
520:56 - would affect the sample size have for
520:59 - precision and estimating the sample
521:00 - proportion as the sample size increases
521:03 - the Precision increases as well
521:10 - that was section three you may also want
521:13 - to glance through appendix C3 and
521:15 - Central limit theorem
521:17 - you may also want to look at Power in
521:19 - statistics on Wikipedia
521:23 - and that's it for chapter seven
521:25 - in chapter 8 we're going to use all of
521:28 - this Central limit theorem stuff to
521:30 - start creating confidence intervals and
521:33 - in chapter 10 we're going to look at
521:35 - hypothesis testing
521:38 - um but in all reality we understand what
521:40 - confidence intervals are at this point
521:44 - but when we get to chapter eight we're
521:46 - going to get a much deeper appreciation
521:48 - for confidence intervals and that's it
521:51 - hello and welcome to the video lecture
521:53 - on chapter 8 where we introduce the
521:55 - theory of confidence intervals and how
521:58 - to calculate them by hand which we
522:00 - should never want to do because
522:03 - calculating things by hand introduces
522:05 - errors in many many places it's much
522:08 - better to learn how to do these with a
522:10 - computer which will be the next lecture
522:13 - learning how to do confidence intervals
522:15 - on a computer specifically using the r
522:18 - statistical environment
522:21 - um
522:22 - I guess we get to start by the end of
522:25 - this lecture you should be able to State
522:26 - what a confidence interval concerns what
522:28 - it is State the theory behind
522:31 - calculating those confidence intervals
522:32 - and understand that they represent a
522:34 - proportion not a probability but a
522:37 - proportion
522:39 - um so here's the entire Arc of the
522:41 - course or the story of the course thus
522:43 - far
522:45 - the second examination would have taken
522:47 - place right last week if we had a
522:50 - typical course so before that back when
522:54 - we were talking about probabilities we
522:56 - calculated three types of intervals uh
522:58 - We've calculated the observation
522:59 - intervals the confidence intervals and
523:01 - the prediction intervals
523:03 - and these three were actually intervals
523:06 - about different things even though it's
523:08 - all based on the data they all were
523:10 - intervals about different things the
523:11 - observation interval is intervals on
523:13 - observations that we have already had
523:16 - prediction interval is on observations
523:18 - that we will have in the future
523:21 - so a 95 prediction or role tells us that
523:24 - 95 in the future we will be within this
523:27 - range whereas a 95 observation interval
523:30 - tells us that of everything that we've
523:32 - observed so far 95 of those observations
523:34 - are in this range
523:36 - now contrast both of those with the
523:38 - confidence interval recall that the
523:40 - confidence interval was about the
523:41 - population I'm sorry about the the
523:43 - sample statistic so we had confidence
523:45 - intervals looking at the sample mean or
523:49 - confidence intervals looking at the
523:51 - sample proportion or confidence
523:53 - intervals looking at the sample variance
523:55 - and chapter 8 onward we're going to be
523:59 - using these confidence intervals in a
524:01 - much more mathematically balanced way
524:06 - um laying out the theory today we also
524:08 - examine the central limit theorem when
524:10 - we looked at the normal distribution a
524:11 - lot and I when we talked about the
524:14 - central limit theorem I said this tells
524:16 - us this this theorem is so important
524:19 - that it tells us that we can just focus
524:20 - on the normal distribution for a good
524:23 - first approximation anytime in the
524:25 - future
524:26 - so that's why the normal distribution is
524:27 - so important it's also why the central
524:29 - limit theorem is so important and we're
524:31 - going to use both of those today to
524:33 - create these this mathematical
524:35 - confidence interval
524:38 - um so today we'll look at confidence
524:40 - intervals that deal with population
524:42 - parameters and the confidence intervals
524:44 - we've dealt with in the past have looked
524:45 - at the sample statistics now we're going
524:47 - to do it for the population parameters
524:49 - and remember the goal of inferential
524:52 - statistics is to take our sample
524:55 - and draw conclusions about the
524:57 - population parameter
524:59 - so now we're going to take our sample
525:01 - and from that sample we're going to
525:02 - calculate a confidence interval and that
525:05 - confidence interval is going to tell us
525:06 - about reasonable values for this
525:09 - population parameter so let's start with
525:11 - the definition
525:13 - a confidence interval is a set of values
525:16 - that theoretically contain the
525:18 - population parameter given proportion of
525:20 - the time when the experiment is
525:22 - performed many many many many many many
525:24 - many times
525:25 - so it's a set of values
525:29 - usually it's an interval so we have a
525:31 - lower bound and an upper bound and we
525:33 - assume every value between those two
525:35 - lower and the upper constitute the
525:37 - interval
525:38 - it contains the population parameter
525:41 - that we're interested in a given
525:43 - proportion of the time so if we're
525:45 - talking about like a 95 percent
525:46 - confidence interval then theoretically
525:49 - the population parameter is about 95
525:51 - percent of those confidence intervals
525:53 - when we do this experiment over and over
525:56 - and over
525:58 - when we do it for just once then we just
526:01 - have to hope
526:04 - that the population parameter is in the
526:06 - interval
526:07 - it and be confident at a certain level
526:11 - that it is in the interval
526:13 - and this is why it's important to
526:15 - replicate experiments over and over
526:17 - again
526:18 - because one confidence interval could be
526:20 - based on biased data or data that's not
526:24 - representative of the population
526:26 - and even we could use the best sampling
526:31 - method on the world and we would get a a
526:34 - set of a set of data a sample of that
526:37 - data that's not represented of the
526:38 - population we'd never know that if all
526:41 - we do this if all we do is do this once
526:43 - that's why replication is so important
526:45 - so that we know that whether or not our
526:47 - first sample was representative or not
526:50 - because if it's unlikely that the sample
526:53 - is unrepresentative
526:55 - then it's going to be doubly so if you
526:58 - have two samples
527:00 - and that they're both unrepresentative
527:04 - so note that it gives popular
527:05 - information about the population
527:06 - parameter it's a set of reasonable
527:08 - values for that parameter some
527:10 - population parameters we'll be looking
527:11 - at will be the mean the variance the
527:13 - proportion
527:15 - it's a set of reasonable values
527:19 - it's a function of the data
527:22 - you collect data and from that data you
527:25 - calculate the endpoints of that
527:26 - confidence interval
527:28 - and since there's a function of the data
527:29 - it's a random variable
527:32 - and it is a result of a probability
527:34 - distribution calculation and we'll see
527:36 - that later
527:39 - um so if LCL and UCL LCL for lower
527:42 - confidence limit and uclb upper
527:46 - confidence limit
527:47 - you have to say one minus Alpha times
527:49 - 100 percent confidence interval
527:51 - for parameter Theta and Theta could be
527:54 - mu or Sigma squared or p
527:57 - and then about 1 minus Alpha times 100
527:59 - of the intervals under repeated
528:02 - experiments will contain that parameter
528:05 - by default we'll use Alpha of .05
528:10 - which means that we'll be talking about
528:12 - 95 confidence intervals
528:15 - just by default
528:20 - which I say here the confidence level C
528:22 - is 1 minus our Alpha by default we're
528:25 - going to use Alpha of 0.05 in this
528:27 - course
528:28 - and the value of alpha will be
528:30 - reintroduced in chapter 10 when we talk
528:32 - about hypothesis testing it's going to
528:34 - be the theoretical or the nominal type 1
528:36 - error rate and those words will make
528:39 - sense when we get to chapter 10 when we
528:41 - start talking about hypothesis testing
528:44 - note that Alpha is selected by the
528:47 - researcher therefore the confidence
528:48 - level is selected by the researcher
528:51 - uh so unless it's stated otherwise
528:54 - we're going to have Alpha 0.05 in this
528:57 - course
529:00 - so let's get to our first intro lecture
529:02 - question
529:03 - that interlection question will be
529:05 - Define the confidence interval
529:08 - and again I suggest writing down the
529:11 - question on the left hand side of your
529:13 - notebook answer it below that
529:16 - and this is especially important because
529:18 - understanding what a confidence interval
529:20 - is
529:21 - is extremely important for understanding
529:23 - what you can learn from your data
529:31 - okay so that's where we were
529:34 - let's move on to something called
529:36 - bootstrapping
529:37 - in a previous class we introduced
529:39 - bootstrapping it's in
529:42 - sca5b I believe
529:44 - and obtaining a confidence interval for
529:46 - a population parameter so let's look at
529:49 - the code again
529:51 - this is how you would load in data from
529:54 - this
529:56 - a path
529:58 - it's geography.csv
530:01 - these are the results of a previous
530:04 - class taking a geography quiz six is the
530:07 - highest possible score on the quiz zero
530:10 - is the lowest
530:12 - um load it in attach the data set if you
530:15 - want to you can do a do summary of DT to
530:20 - see what the mean and the median are
530:22 - see what the Min and the max are see
530:25 - what the first and the third quartile
530:27 - are
530:27 - you could also calculate the standard
530:29 - deviation doing SD
530:32 - you can do the IQR interquartile range
530:34 - using the IQR function
530:39 - then we performed these two lines
530:43 - multiple times the first line draws a
530:46 - random sample from the variable score
530:49 - that's the score that the students made
530:52 - on the quiz
530:53 - with replacement
530:55 - so each time through X is going to be a
530:59 - vector of scores
531:02 - on the second line you find the mean of
531:05 - that so this will calculate the sample
531:07 - mean of that particular sample of scores
531:10 - and store it into the variable St at the
531:14 - ith position
531:16 - this is a for Loop so it will be doing
531:18 - this these two lines or everything
531:20 - between the two braces 1 times 10 to the
531:24 - fourth times
531:26 - so at the end of running this Loop
531:29 - you're going to have 10 000 sample beans
531:34 - doing a histogram of those will show you
531:36 - the distribution of those sample means
531:39 - and during the quantiles from these two
531:41 - positions these two points or these two
531:43 - levels
531:45 - will give you the endpoints of a 95
531:47 - confidence interval for the mean
531:52 - we've done this in the past
531:55 - here's what the histogram looks like
531:56 - notice the histogram looks rather normal
532:01 - considering that the actual data looked
532:03 - nothing like a normal distribution the
532:06 - distribution the sample means definitely
532:08 - does look normal
532:09 - that does not surprise us or it should
532:12 - not surprise us if we actually
532:13 - understand the central limit theorem
532:15 - because what does the central limit
532:17 - theorem tell us
532:18 - sample means will be much more normally
532:21 - distributed than the data itself
532:25 - and these are the endpoints of the 95
532:27 - confidence interval
532:29 - so we're 95 confident that the
532:31 - population mean mu is between 1.63 and
532:34 - 2.57
532:37 - 1.63333 and 2.56667.
532:41 - again these are out of six
532:44 - so the mean is definitely going to be
532:47 - less than 50 percent
532:50 - because fifty percent is three and
532:52 - that's way way over here and just about
532:55 - all of the histogram is to the left of
532:58 - three so we're almost positive that the
533:02 - mean understanding of geography is less
533:04 - than 50 percent
533:12 - this is fine if all we have is the data
533:15 - and that's all we did here we just
533:16 - looked at the data we drew from the data
533:18 - we dealt with the data as being
533:20 - representative of the population
533:23 - we didn't say the data was normally
533:25 - distributed we didn't say the data
533:27 - followed a binomial distribution we just
533:28 - said here's the data
533:31 - if we can make an additional assumption
533:33 - about the data specifically that the
533:36 - data are normally distributed then we
533:39 - can go a little bit further we don't
533:41 - have to do bootstrapping we can just run
533:43 - a simple test
533:48 - so recall that for large n from the
533:51 - central limit theorem X bar is
533:52 - approximately normal
533:55 - with mean mu and variance Sigma squared
533:58 - over n
534:00 - where Sigma squared's the variance of
534:02 - the data
534:04 - or variance of the data generating
534:05 - process and mu is the expected value of
534:08 - the data generating process
534:10 - so this should look familiar from
534:12 - chapter 7 section 2.
534:15 - now if we apply the Z transform from
534:17 - back in chapter 3
534:20 - we're going to subtract off mu from both
534:22 - sides
534:23 - and we're going to divide by the square
534:25 - root of Sigma squared over n
534:27 - we're going to get that X bar minus mu
534:30 - over root Sigma squared over n is
534:32 - approximately normal 0 1.
534:35 - this is why it's called the z-score this
534:38 - distribution normal 0 1 it's also called
534:39 - the Z distribution
534:42 - this is normalizing the X bar
534:45 - distribution
534:50 - so we have X bar minus mu over square
534:52 - root of Sigma squared over n is
534:54 - approximately normal it's approximately
534:57 - standard normal
535:02 - that means the expression on the left is
535:06 - between the values of negative 1.96 and
535:09 - positive not 1.96 about 95 of the time
535:15 - how do I know that because the 2.5
535:18 - percentile and the 97.5 percentile of
535:21 - the standard normal are negative 1.96
535:23 - and positive 1.96
535:27 - . how do we know that
535:29 - Q Norm of 0.025 will give us the
535:32 - negative 1.96 and Q Norm of 0.975 will
535:37 - give us the 1.96
535:42 - and so we would say that a theoretical
535:45 - 95 confidence interval I'm sorry a
535:48 - theoretical 95 interval for this
535:50 - quantity
535:52 - this thing on the on the left X bar
535:54 - minus mu over square root of Sigma
535:55 - squared over n
535:57 - is between negative 1.96 and 1.96 by
536:01 - definition
536:02 - so this bottom statement would be a
536:04 - definitional statement
536:07 - that is if I have X that's normally
536:09 - distributed
536:10 - with mean mu and variance Sigma squared
536:13 - and I draw a sample of size n from the X
536:16 - distribution and calculate this
536:19 - there's a 95 probability that it's going
536:22 - to be between negative 1.96 and positive
536:24 - 1.96
536:27 - because 95 of the time it's between this
536:31 - quantity is between negative 1.96 and
536:33 - positive 1.96
536:36 - about two and a half percent of the time
536:38 - it's above 1.96 and about two and a half
536:40 - percent of the time it's less than
536:42 - negative 1.96
536:48 - . now we really don't care about the
536:50 - interval for this quantity we want it
536:52 - for Mu we want a confidence interval for
536:55 - Mu
536:57 - well all we have to do is solve this
536:59 - equation this equals negative 1.96 for
537:03 - Mu
537:04 - and here's what we get
537:07 - here's the quantity equal to negative
537:10 - 1.96 we're solving for Mu remember
537:13 - so we multiply both sides by the square
537:15 - root of Sigma squared over n
537:17 - and this is kind of important this
537:19 - quantity square root of Sigma squared
537:21 - over n is called the standard error
537:27 - now we got X bar minus mu is equal to
537:29 - negative 1.96 times the square standard
537:32 - error we subtract an X bar from both
537:34 - sides remember we're solving for this
537:36 - meal so we're subtracting X bar from
537:38 - both sides we'll f with the negative mu
537:40 - on the right multiply through by
537:42 - negative one we get that this is the
537:44 - upper Bound for Mu
537:50 - so this we're going to call the upper
537:53 - bound on the 95 confidence interval from
537:57 - U
537:59 - I leave it as an exercise to show that
538:01 - this with a negative sign here is going
538:03 - to be the lower bound
538:05 - how would you do that
538:07 - this quantity equal to 1.96 and so from
538:10 - U
538:13 - so we got it X bar minus 1.96 times the
538:17 - standard error and X bar plus 1.96 times
538:20 - the standard error of the two endpoints
538:22 - we can symbolize it in just one
538:25 - expression X bar plus and minus
538:29 - Z of alpha over 2 times the standard
538:32 - error
538:33 - if Alpha is .05 then this will be a 1.96
538:37 - out front
538:46 - um
538:47 - so we would call this oddly enough we
538:50 - would call this the confidence interval
538:51 - from U
538:53 - technically this would just provide the
538:55 - two endpoints of the confidence interval
538:57 - from U
539:04 - foreign
539:08 - question at this point
539:12 - oops what is the distribution of the
539:15 - sample mean in this lecture
539:19 - so what did we require the distribution
539:21 - of the sample mean to be notice not the
539:24 - distribution of the sample
539:27 - we're looking at the distribution of the
539:28 - sample mean so what was that in this
539:31 - lecture
539:33 - and again write the question on the left
539:35 - side your answer below it
539:43 - okay so this confidence interval that we
539:45 - just calculated
539:46 - is for Mu
539:48 - and it works this confidence interval is
539:50 - from U and it works when you're
539:52 - estimating mu which makes sense if
539:54 - you're trying to estimate the variance
539:56 - you wouldn't use a confidence interval
539:58 - from mu
539:59 - if you're able to calculate X bar and N
540:02 - well if you can't calculate X bar and N
540:05 - from the data then you I don't know what
540:08 - to say
540:09 - this goes better calculating X bar from
540:11 - the data is pretty straightforward and
540:13 - as you're just counting the data size
540:16 - and if you know Sigma squared remember
540:18 - Sigma squared is the population variance
540:23 - um
540:24 - I don't know that we would ever know the
540:26 - population variance
540:29 - and then last but not least the data are
540:31 - generated from a normal process or
540:33 - n is large enough that the central limit
540:35 - theorem says at X bar is approximately
540:38 - normal
540:40 - and again the reality here for four is
540:42 - we just need X bar to be approximately
540:44 - normal at no point in the last few
540:46 - slides did I say x has to follow any
540:49 - specific distribution
540:51 - every distributional statement we made
540:53 - was that X bar was normal or
540:55 - approximately so
540:59 - call the going back to number three the
541:02 - you have to know Sigma squared the
541:03 - population variance
541:05 - recode the definition of the population
541:07 - variance in order to calculate it you
541:09 - need to know what mu is
541:11 - however since we're trying to estimate
541:13 - mu we don't want to know what mu is
541:15 - therefore how do we know what Sigma
541:16 - squared is
541:18 - and in general if we don't know Sigma
541:20 - squared then what we just did doesn't
541:22 - work
541:24 - so your question is your question to me
541:26 - is
541:27 - why did we do it if we never know Sigma
541:30 - squared and therefore what we just did
541:31 - doesn't doesn't do anything for us
541:34 - and the answer is
541:37 - this process that we just did
541:40 - gave us an opportunity to show how
541:42 - confidence intervals are constructed
541:45 - what they actually indicates
541:49 - and how they can be useful
541:53 - kind of shaky on that last one
541:56 - but now that we know the process in
541:58 - creating confidence intervals we can fix
542:00 - this quote error
542:02 - or try to figure out some way that we
542:03 - can get by without having to know Sigma
542:05 - squared and create a confidence interval
542:07 - from that
542:08 - using the same theory behind it
542:12 - I mean specifically since we can't use
542:15 - the Z procedure which we just did it's
542:17 - also called the Wald w-a-l-d named after
542:21 - uh I think it was Benjamin Walt could
542:24 - have been Abraham Wald I probably should
542:27 - have looked that up we're going to use a
542:28 - different procedure if we know Sigma
542:31 - squared we can use this procedure we
542:33 - just did if we don't know Sigma squared
542:36 - we can't
542:37 - because we need to know Sigma squared to
542:40 - do this procedure
542:42 - instead of the Z procedure we'll use the
542:45 - T procedure
542:47 - here's the function that we will
542:49 - calculate it's X bar minus mu the
542:52 - numerator is the same as it was for the
542:54 - Z procedure denominator we're looking at
542:56 - s squared over n and the Z procedure
542:59 - that was Sigma squared here it's s
543:01 - squared s squared would be the sample
543:03 - variance since if we've got the sample
543:04 - we can calculate s squared
543:07 - and this quantity follows approximately
543:09 - a t distribution
543:11 - T distribution has a parameter called
543:14 - the degrees of freedom
543:17 - in for one sample cases the degrees of
543:20 - freedom is just equal to n minus 1.
543:24 - now with this said we can go through the
543:27 - same procedure we did before with the
543:29 - Z's to come up with the endpoints of a
543:31 - confidence interval using the T
543:34 - is X bar plus or minus this
543:36 - distributional multiplier times the
543:39 - standard error just in this case the
543:41 - standard error is the square root of s
543:43 - squared over n
543:45 - comparing this confidence interval to
543:48 - the one we calculated just earlier
543:55 - so here capital T sub Alpha over 2 is
543:58 - the alpha over second quantity over the
544:00 - T distribution with n minus 1 degrees of
544:02 - freedom
544:03 - table two by tradition table two is the
544:06 - T distribution table
544:09 - so in the back of the book there's
544:11 - probably a table two
544:14 - that you would use if you had to
544:19 - I'm this slide Compares with the Z
544:22 - distribution the standard normal
544:23 - distribution the blue distribution how
544:26 - it compares to the T distribution
544:28 - notice the T distribution is much wider
544:31 - than the standard normal there's more
544:33 - measurements out here in the Tails far
544:35 - away from zero
544:37 - in other words the T distribution is
544:39 - much more variable than the Z
544:41 - why does that make sense
544:44 - it's because when the Z distribution
544:46 - when we were using the Z distribution we
544:48 - only had one source of uncertainty that
544:50 - was in The X bar
544:53 - the denominator was a sigma squared a
544:55 - population variance
544:57 - with the T we've got more sources of
545:00 - uncertainty we've got the X bar but
545:01 - we've also got a s squared that's a
545:04 - random variable also so we've got two
545:05 - sources of randomness so we would expect
545:08 - the T distribution to be much wider than
545:10 - the Z
545:13 - now this confidence interval from U
545:15 - works if you're trying to estimate mu
545:18 - if you're able to calculate X bar and n
545:21 - and S squared which again those are from
545:23 - the data so you should be able to
545:25 - calculate them
545:26 - and if the data are generated from a
545:28 - normal process or if n is large enough
545:30 - again we're doing this for X bar has to
545:32 - be normally distributed
545:35 - and that happens in two ways if the X's
545:37 - are normally distributed or if n is
545:40 - large enough and the central limit
545:41 - theorem works
545:44 - uh let's do question three
545:50 - why do we use a t distribution when the
545:52 - population variance is unknown
545:54 - so that should be two or three sentences
545:56 - or maybe one or two sentences again
545:58 - write the question on the left hand side
545:59 - answer it below
546:05 - so we've got our
546:07 - procedure to estimate mu
546:11 - okay just one View
546:13 - as we go through this this section this
546:16 - this these two chapters eight and nine
546:18 - we're going to get more and more
546:20 - procedures to estimate things we're
546:23 - going to get a procedure to estimate P
546:25 - we're going to get a procedure to
546:27 - estimate Sigma squared we're going to
546:29 - get a procedure to estimate two mu's or
546:31 - the difference in two mu's the
546:33 - difference in two proportions that the
546:35 - ratio of two variances we're going to
546:38 - get procedures that work when the sample
546:42 - is not normal enough
546:44 - so make sure that you can
546:49 - match the hypoth the what you're trying
546:52 - to estimate the parameter with the
546:54 - correct procedure and the requirements
546:56 - for that procedure if we go back one who
546:59 - uh
547:00 - these are the requirements to the T
547:02 - distribution
547:03 - or I'm sorry these are the requirements
547:05 - of the t-test for the T procedure if you
547:08 - don't meet these then you've got to use
547:09 - something else
547:12 - here's a list of several others go to
547:15 - the all procedures.pdf handout that's
547:17 - located online
547:19 - that will give you all the procedures
547:21 - we'll be looking at
547:23 - and we're going to be able to even have
547:25 - a procedure for mutilda the the
547:27 - population median
547:32 - so in today into today's slide deck we
547:35 - cover the central limit theorem again
547:37 - we looked at the definition of
547:39 - confidence intervals
547:41 - we look for confidence intervals for Mu
547:45 - when Sigma squared is known
547:47 - uh from U when Sigma squared wasn't
547:49 - known and both of those cases X bar had
547:51 - to be approximately normal
547:54 - we looked at confidence intervals for
547:55 - other instances or I just mentioned them
547:58 - and most importantly the idea or the
548:01 - theory behind confidence intervals this
548:03 - is the key for today understand what
548:05 - confidence intervals tell us
548:08 - once you understand this then you can
548:10 - just hop on the computer and have the
548:12 - computer calculate those confidence
548:14 - intervals for you
548:17 - it's the theory that you need to
548:19 - understand
548:20 - in the next slide deck we're going to
548:22 - show how to do these confidence
548:24 - intervals calculations in r
548:30 - tomorrow we're going to review a little
548:31 - bit of today we're going to look at some
548:33 - processes to create confidence intervals
548:35 - for one and two means medians
548:37 - proportions variances
548:39 - but we're going to do it in r
548:40 - so instead of having to actually do
548:42 - those calculations by hand it's going to
548:45 - be
548:47 - one line of code and interpret the
548:49 - results
548:52 - so in the future we're going to see a
548:54 - lot of these procedures make sure you
548:56 - keep a separate sheet I would have it in
548:57 - the back of your notebook
549:00 - and for every procedure you should State
549:03 - how to do it in R what the procedure is
549:07 - for is it for a single mean is it for
549:09 - two proportions it is it a ratio of
549:11 - variances Etc
549:13 - what the requirements are otherwise
549:15 - known as the assumptions of it and then
549:17 - how to interpret
549:22 - so chapter 8 and 5 and then Wikipedia
549:25 - all of those will be helpful
549:28 - hope this was helpful
549:30 - I'll see you in the next lecture
549:33 - hello and welcome to chapter 10. this is
549:36 - the chapter where we introduce
549:37 - hypothesis testing and see several
549:40 - different types of tests so this lecture
549:44 - is going to be about the theory of
549:45 - hypothesis testing
549:47 - so this will leave the groundwork tie it
549:50 - into previous
549:52 - topics that we've discussed the next
549:55 - lecture will be how to actually do this
549:57 - using R so you don't have to worry about
549:59 - all these probability calculations so by
550:02 - the end of this lecture you should be
550:03 - able to identify the research null and
550:06 - alternative hypotheses
550:08 - the research hypothesis is given to you
550:10 - by the researcher the null on the
550:12 - alternative you have to come up with
550:15 - you have to calculate the p-value for a
550:17 - given alternative hypothesis and again
550:20 - this slide deck will be about doing it
550:23 - by hand that'll let you know exactly
550:25 - what the p-value means and what it
550:27 - doesn't mean
550:28 - which ties into the third objective the
550:31 - next lecture will be how to do this in R
550:33 - so all you have to do is interpret the
550:36 - p-value that R gives you
550:38 - so previously we've calculated
550:39 - confidence intervals and we've
550:40 - interpreted confidence intervals today
550:43 - we're going to look at the other side of
550:45 - the statistical inference coin and
550:47 - looking at hypothesis testing and
550:49 - p-values the key one of the key one of
550:53 - the key differences is in hypothesis
550:55 - testing you create the hypothesis before
550:59 - you collect the data
551:02 - and then you test that hypothesis with
551:06 - your data so a hypothesis comes first
551:09 - collect your data then test the
551:11 - hypothesis whereas in confidence
551:13 - intervals you walk into it with no idea
551:15 - what the parameter value is supposed to
551:17 - be and you estimate the parameter value
551:19 - from the data and only from the data so
551:23 - confidence intervals solely from the
551:25 - data hypothesis testing is from the data
551:27 - and from a claimed hypothesis
551:30 - so here's the theory the theory behind
551:33 - hypothesis testing is one state the
551:36 - research hypothesis and the null
551:38 - hypothesis
551:40 - two
551:41 - somehow determine how much the data
551:43 - support the hypothesis and by the
551:46 - hypothesis I mean the null hypothesis
551:49 - to do that you need to determine the
551:52 - parameter being tested turn the
551:55 - determine the appropriate statistic or
551:57 - estimator how to determine the
551:59 - distribution of that statistic under the
552:01 - null hypothesis determine How likely it
552:04 - is to observe that statistic if the null
552:06 - hypothesis is true and that last thing
552:09 - is called the p-value and then finally
552:11 - interpret all of that specifically
552:13 - interpret that level of support
552:19 - so let's go through a few definitions
552:21 - we're going to talk about that the three
552:23 - hypotheses types
552:25 - um hypothesis is a testable Claim about
552:28 - reality
552:31 - that's all it is since it's a claim
552:33 - about reality it's going to concern some
552:35 - aspect of the population and this aspect
552:38 - of the population is going to be a
552:40 - parameter that's we've been dealing with
552:41 - parameters from the second chapter
552:43 - onward
552:45 - we're going to start testing hypotheses
552:47 - about that population parameter based on
552:49 - our sample now
552:50 - the usual parameters hypothesize about
552:53 - at this level are just the mean the
552:56 - proportion the variance but we can
552:58 - hypothesize about any aspects of the
553:01 - population
553:02 - and we're going to symbolize this
553:04 - generic parameter using the Greek letter
553:07 - Theta
553:09 - so Theta can represent the mean the
553:11 - proportion the variance
553:13 - since it's a claim about reality since
553:15 - this hypothesis is a claim about reality
553:17 - it separates all possible realities into
553:21 - those that are consistent with
553:23 - hypothesis and those that are
553:25 - incompatible with it
553:27 - so every single reality will either be
553:30 - consistent with a hypothesis or
553:33 - incompatible with it and we have to
553:35 - determine which reality we fall into
553:38 - if we fall into the reality that is
553:40 - consistent with hypothesis we say the
553:42 - data support the hypothesis if we fall
553:45 - into the reality where the the uh the
553:48 - we fall into the reality that is
553:50 - incompatible with hypothesis we reject
553:53 - the hypothesis we say that the data are
553:56 - not supporting it
554:00 - I'm the most absolutely without question
554:02 - the most important hypothesis is the one
554:04 - made by the researcher and that is the
554:07 - research hypothesis it's a testable
554:10 - Claim about reality that's hypothesis
554:12 - part made by the scientist or the
554:14 - researcher and that's the research part
554:17 - this is the one that the statistician
554:19 - must eventually come to a conclusion
554:21 - about
554:24 - we are going to create two additional
554:26 - hypotheses to help us with coming to a
554:28 - conclusion about the research hypothesis
554:30 - but but ultimately we have to come back
554:33 - to the scientists and say yes the data
554:35 - supports the research hypothesis or know
554:37 - the data do not support the research
554:39 - hypothesis
554:42 - because we're using statistics we create
554:44 - two statistics specific hypotheses these
554:47 - two hypotheses divide all possible
554:49 - realities into two groups those that are
554:51 - part of the research and those that are
554:53 - not up that are not a part of it
554:56 - um
554:57 - so here's a scary table
555:00 - pay attention solely to the First Column
555:02 - the column headed HR that's going to be
555:06 - the different types of research
555:08 - hypotheses
555:10 - recall that Theta is the parameter of
555:13 - Interest it could be mu it could be P
555:15 - could be Sigma squared it could be I
555:18 - can't think of it it could be Lambda
555:19 - remember Lambda from the poisson and the
555:21 - Lambda from the exponential it could be
555:23 - a could be B from the uniform but we're
555:26 - focusing solely on mu and P and sigma
555:29 - squared the mean the proportion and the
555:32 - variance so those thetas represent one
555:34 - of those three
555:36 - the value Theta 0 or the the simple
555:39 - Theta zero represents a hypothesized
555:42 - value claimed by the researcher the
555:44 - Theta naught is just a a number that the
555:47 - researcher thinks is interesting
555:51 - so now we got these six symbols between
555:54 - the Theta the parameter and the Theta
555:57 - not the value
556:00 - the relationship between Theta and Theta
556:03 - naught has to be one of these six
556:06 - Theta the hypothe the researcher could
556:09 - hypothesize that Theta is less than some
556:11 - value equal to some value greater than
556:13 - some value less than or equal to some
556:15 - value not equal to some value or greater
556:17 - than some value
556:19 - and again the research hypothesis is
556:22 - given to us by the stat by the scientist
556:26 - from that research hypothesis
556:29 - we statisticians create a null and an
556:33 - alternative
556:35 - now let's look at the null and
556:36 - alternative hypo columns starting with
556:39 - the null column
556:41 - notice the null only has three types of
556:44 - of symbols
556:46 - greater than or equal to equal to or
556:48 - less than or equal to
556:51 - greater than or equal to equal two or
556:53 - less than or equal to
556:54 - the null hypothesis must always have the
556:57 - equal to part and that's because we are
557:01 - going to create a probability
557:03 - distribution for the parameter or for
557:05 - the statistic based on that equals part
557:12 - so the null hypothesis always has to
557:14 - have the equals part
557:16 - contrast this with the alternative there
557:19 - is never an equals part
557:21 - in fact if you notice the null and the
557:24 - alternative are complete opposites
557:27 - if the null is greater than or equal to
557:28 - the alternative is less than
557:32 - if the null is equal to the alternative
557:34 - is not equal to
557:37 - if the null is less than or equal to the
557:39 - alternative is not less than or equal to
557:42 - otherwise known as greater than
557:44 - it's because the null and the
557:46 - alternative have to divide reality up
557:48 - into two parts
557:50 - one part that supports the research
557:52 - hypothesis and one part that doesn't
557:56 - now which part supports the research
557:58 - hypothesis the answer is well it depends
558:01 - what is the symbol in the research
558:03 - hypothesis If the symbol is less than
558:06 - that cannot be a null hypothesis so
558:09 - that's got to be the alternative
558:13 - If the symbol is equal to well that's
558:15 - one of the allowable symbols for the
558:17 - null so that's going to be the null
558:20 - and the alternative is the opposite
558:23 - if the research hypothesis uses greater
558:25 - than well that has to be the alternative
558:27 - because it can't be the null and the
558:30 - null is going to be the opposite
558:33 - less than or equal to will be the null
558:36 - greater than or equal to will be the
558:37 - null
558:38 - not equal to will be the alternative
558:42 - so this the HR column is given to us by
558:45 - the researcher
558:47 - and from that we create a null and an
558:49 - alternative hypothesis
558:51 - that split the world up into a a reality
558:56 - that supports the research
558:58 - hypothesis and a reality that does not
559:00 - we have to determine which reality we're
559:02 - in
559:08 - don't memorize the table understand it
559:11 - learn what it says about relationships
559:14 - these research hypotheses are given to
559:16 - us we just have to figure out what the
559:18 - null and the alternative is
559:19 - first step is to determine if the equals
559:22 - part is a part of the research
559:23 - hypothesis if it is then the research
559:26 - hypothesis and the null hypothesis are
559:28 - the same
559:29 - otherwise the research hypothesis and
559:31 - the alternative hypothesis are the same
559:35 - and then the null and the alternative
559:36 - are always opposites
559:45 - what do all the null hypotheses have in
559:47 - common
559:49 - already said that
559:53 - what's the relationship between the null
559:55 - and the alternative
559:57 - said that
560:01 - why does that relationship have to exist
560:08 - okay now we know what the hypotheses are
560:11 - the research the null the alternative
560:14 - now let's look at this thing called the
560:16 - p-value
560:17 - the p-value is the probability of
560:20 - observing a test statistic this extreme
560:22 - or more so given the null hypothesis is
560:26 - true
560:28 - some key points there
560:30 - it's a probability
560:32 - it relates to the test statistic
560:35 - it's a probability of observing such a
560:37 - test statistic that is this extreme
560:40 - or more so
560:44 - given the null hypothesis is true so we
560:47 - assume the null hypothesis is true and
560:50 - we calculate the p-value based on that
560:54 - now this is the stat statistical
560:57 - definition of the p-value this is the
560:59 - one that's usually in the books p-value
561:02 - is the probability of observing data
561:04 - this extreme or more so given the null
561:06 - hypothesis is true
561:11 - while the first definition makes me feel
561:14 - all warm and fuzzy inside the second one
561:15 - actually ties it more closely to the
561:18 - data that you've observed
561:21 - and the one I really liked one that
561:23 - makes me feel all nice and warm and
561:25 - fuzzy inside is the last one it's not a
561:28 - mathematical definition but it's a gut
561:29 - level definition of what the p-value
561:31 - actually tells us p-value is the amount
561:33 - of support in the data for the null
561:35 - hypothesis and again it's for the null
561:38 - hypothesis in other words if the p-value
561:41 - is large then there is a lot of support
561:43 - at the data for the null hypothesis if
561:46 - the p-value is small then there is very
561:48 - little support in the data for the null
561:51 - hypothesis
561:54 - the third definition comes from the
561:56 - second definition and the second comes
561:58 - from the first
562:00 - and they're all equivalent assuming that
562:02 - your test is appropriate and you've
562:04 - collected the data well
562:07 - so now we know to look at the p-value
562:10 - and to interpret the p-value it's just a
562:13 - matter of the p-value is how much
562:15 - support is in the data for that null
562:18 - hypothesis
562:20 - and remember we got to tie it eventually
562:22 - back to the research hypothesis
562:27 - okay now we've got two of the three
562:29 - parts of the hypothesis testing Theory
562:31 - we laid out earlier the last part is
562:33 - making a decision about a research
562:35 - hypothesis based on the data so the
562:37 - fundamental question is do the data
562:39 - sufficiently support the research
562:41 - hypothesis
562:44 - do the data sufficiently support the
562:47 - research hypothesis and note that this
562:49 - is a binary yes or no yes the data do
562:52 - know the data do not
562:56 - this goes back to the p-value but we
562:58 - need to somehow change that p-value of
563:00 - probability that ranges from zero to one
563:02 - we need to change that into a yes or no
563:04 - and to do that we just have to determine
563:06 - some sort of cutoff between what
563:09 - supports the research hypothesis and
563:11 - what does not this cut off boundary
563:13 - we're going to refer to as the alpha
563:15 - value
563:17 - the typical Alpha value is .05
563:21 - my defaults .05
563:25 - if the level of support is less than
563:27 - Alpha we reject the null hypothesis
563:30 - the data do not sufficiently support the
563:33 - null hypothesis
563:35 - if the day if the if the level support
563:37 - is greater than Alpha if the p-value is
563:39 - greater than Alpha we don't reject the
563:42 - null hypothesis
563:43 - the data do not tell us that the null
563:46 - hypothesis is wrong
563:50 - notice that there is information in the
563:52 - p-value that goes above and beyond just
563:54 - the yes or no answer
563:57 - so not only should we interpret the
564:00 - p-value in terms of yes the data
564:02 - supported to know the data don't but we
564:05 - should also provide the p-value because
564:06 - that will also tell us how much it
564:09 - supports it or how much it doesn't
564:14 - continue on to and continue on with
564:16 - decision Theory
564:18 - this Alpha value
564:20 - the alpha value is the type 1 error rate
564:22 - that's claimed by the statistician
564:26 - a type 1 error occurs in the researcher
564:29 - rejects a true null hypothesis
564:32 - so there is that null hypothesis if it's
564:35 - true then Alpha then the type 1 type 1
564:38 - error occurs when we reject that true
564:40 - null hypothesis
564:41 - now the thing is we never know if a null
564:43 - hypothesis is true in real real life
564:48 - which can be a problem but then if we
564:51 - knew a null hypothesis was true in real
564:54 - life then why would we do statistics to
564:56 - test the null hypothesis I mean we'd
564:58 - already know that it was true
565:02 - the type 1 error rate is the proportion
565:05 - the type 1 error rate is a proportion of
565:07 - the time that the researcher commits a
565:08 - type 1 error
565:10 - so the rate part means it's the
565:14 - proportion of the times that the
565:16 - researcher commits that type 1 error
565:18 - note that the actual type 1 error rate
565:21 - may not be Alpha
565:23 - the alpha is claimed by the statistician
565:26 - but the test may have some flaws to it
565:29 - where the actual type 1 error rate is
565:32 - not Alpha and laboratory F explores this
565:35 - in much greater detail
565:38 - however we tend to just pretend that
565:41 - Alpha actually is the type 1 error rate
565:44 - how
565:45 - it's a lot of good pretending in
565:47 - statistics
565:49 - if there is a type 1 error then there
565:51 - must at least be a type 2 error
565:53 - otherwise why would we call it a type 1
565:55 - error a type 2 error occurs in the
565:58 - research researcher fails to reject a
565:59 - false null hypothesis
566:02 - type 1
566:04 - occurred when the researcher rejected a
566:07 - true null hypothesis a type 2 occurs
566:10 - when the researcher
566:13 - fails to reject a false null hypothesis
566:16 - or we could also think of it as a type 2
566:18 - error occurs when the researcher rejects
566:20 - a true alternative
566:23 - although we never frame it in terms of
566:24 - rejecting a true alternative
566:27 - the type 1 error rate that slipped in
566:31 - the type 2 error rate is symbolized with
566:34 - beta
566:36 - just like the type 1 error rate is
566:38 - implies by Alpha the type 2 error rate
566:40 - symbolized by Beta
566:42 - both are errors and error rates so we
566:46 - would like to minimize both however
566:49 - is reducing one increases the other
566:53 - furthermore if we want to choose if we
566:55 - want to make either one of them zero
566:57 - then the other one goes all the way up
566:59 - to one
567:02 - so let's we're kind of stuck there
567:06 - we tradition in statistics is to set
567:10 - Alpha
567:11 - and then beta just happens as it happens
567:15 - beta is going to be a function of alpha
567:18 - as described here but it's also going to
567:20 - be a function the sample size it's also
567:22 - going to be a function of how wrong the
567:25 - null hypothesis is
567:28 - um and so if we want to reduce the type
567:31 - 2 error rate then increasing the sample
567:33 - size is guaranteed to do that but
567:36 - unfortunately as we've seen in the past
567:37 - increasing the sample size tends to be a
567:40 - little bit expensive at times
567:47 - here's an aside this is not statistical
567:50 - here well it is kind of statistical but
567:52 - it's
567:53 - if there's a type 1 error rate then
567:55 - there must be at least a type 2 error
567:56 - rate a terror error there are in fact a
567:59 - couple more these are non-standard by
568:01 - the way
568:03 - but they do give us some insight into
568:05 - what can go wrong in a statistical
568:07 - analysis so we'll Define a Type 3 error
568:10 - which I want to introduce here but I
568:14 - only want to introduce this to you so
568:16 - that you think more through the process
568:18 - and where errors can pop in not so that
568:21 - you can replicate this on a test a type
568:23 - three error occurs in the researcher
568:25 - rejects a false null hypothesis that's
568:27 - good but for the wrong reason that's the
568:30 - error part is you're doing it for the
568:31 - wrong reason
568:33 - um
568:33 - type 3 so here are some causes of type 3
568:37 - errors One is using the wrong test
568:41 - um
568:42 - two is aggregation bias in other words
568:45 - you're measuring at a lot at uh
568:47 - measuring something about groups and
568:50 - you're trying to draw conclusions about
568:52 - the individuals in the groups
568:54 - and there is ecological fallacy which is
568:57 - just about the opposite of that
568:59 - collinearity among predictors it just
569:01 - means you're using two independent
569:03 - variables that are highly correlated and
569:05 - you're not able to determine which is
569:07 - causing the the effect that you're
569:09 - looking at
569:11 - so the key to avoiding the type 3 error
569:13 - rate is to fully understand the
569:15 - statistical tests
569:16 - the data collection the relationship
569:19 - amongst the independent variables in
569:21 - other words the key is to understanding
569:24 - your model and your data
569:28 - the wrong test the wrong interpretation
569:31 - the wrong assumptions All Leads errors
569:35 - but also realize that type 3 errors can
569:39 - be eliminated it just requires that the
569:42 - statistician is careful
569:45 - type 1 and type 2 errors cannot be
569:47 - eliminated
569:50 - let's go through three simple
569:53 - the CCD examples I'd like to test if my
569:57 - coin is biased in favor of getting heads
569:59 - specifically using physical language my
570:01 - research hypothesis is
570:04 - HR
570:06 - for research hypothesis that's colon p
570:09 - is our parameter so it's a population
570:11 - proportion
570:12 - my research hypothesis is that P is
570:15 - greater than one-half
570:19 - I would like to test if my coin is
570:21 - biased in favor of hits
570:23 - so P must be the probability of getting
570:25 - hits
570:26 - so this will be our research hypothesis
570:29 - going back to the table if this is our
570:32 - research hypothesis
570:33 - where will this also be the null or will
570:37 - this also be the alternative hypothesis
570:41 - the key is looking at the symbol in the
570:43 - middle there's no equals part so this
570:46 - will also be the alternative hypothesis
570:49 - since this is the alternative
570:52 - what is the null
570:53 - I recall that the null hypothesis is The
570:56 - Logical opposite
570:57 - what is the opposite of greater than
571:01 - correct it's less than or equal to
571:05 - and as always if we're going to test
571:06 - hypothesis we collect data in this case
571:09 - I flip the coin 100 times and I get 58
571:12 - hits
571:17 - from the way the problem is set up we
571:19 - know that the number of heads is follows
571:22 - a binomial distribution
571:25 - N is a hundred it's the number of Trials
571:28 - the number of times I flip the coin
571:30 - p is 0.5 because that's my null that's
571:35 - the equals part of my null hypothesis
571:37 - that P is equal to 0.5
571:41 - note that if the number of heads
571:42 - observed is too large then the
571:44 - alternative hypothesis is more likely to
571:46 - be correct
571:48 - if the number of observed heads is too
571:50 - large because we're research hypothesis
571:53 - is greater than one-half
571:58 - so let's calculate the p-value from the
572:00 - definition
572:02 - p-value is a probability of observing
572:04 - data this extreme or more so given the
572:06 - null hypothesis is true
572:08 - so here's the p-value it's the
572:11 - probability of of observing
572:15 - more than or equal to 58 heads
572:21 - because I observed 58
572:23 - the p-value is the probability of
572:25 - observing data this extreme or more so
572:29 - greater than or equal to 58 given the
572:32 - null hypothesis is true given the null
572:35 - hypothesis is true and this is the null
572:37 - hypothesis in distribution form
572:42 - and 100
572:43 - p is 0.5
572:45 - and guess what we know how to calculate
572:48 - this from back in chapter five
572:51 - we also know how to do it rather quickly
572:54 - it's a greater than or equal to 58 x is
572:57 - eight discrete so we actually do have to
572:59 - pay attention to the 58 this is 1 minus
573:03 - X less than or equal to 57.
573:10 - ends 100
573:11 - piece 0.5
573:14 - so the p-value is .0660531
573:25 - so how do we interpret that p-value
573:32 - let's go ahead and look at the
573:34 - distribution of the number of heads
573:37 - so this is the probability Mass function
573:39 - for our null hypothesis
573:43 - the dark blue
573:45 - is the greater than or equal to 58
573:49 - so the dark blue is going to be the
573:51 - p-value
573:53 - and technically the sum of the area
573:55 - under the dark blue will be our p-value
574:04 - since the p-value is greater than our
574:06 - usual Alpha of 0.05 remember it's 0.06
574:09 - something
574:11 - we do not reject the null hypothesis
574:16 - the data are not sufficiently against
574:18 - the null hypothesis
574:21 - therefore we actually cannot
574:25 - conclude anything about the research
574:28 - hypothesis
574:30 - P may be greater than 0.5 P may be less
574:33 - than 0.5 P may be exactly equal to 0.5
574:39 - if you look at the confidence interval
574:40 - for p by doing the binomial procedure
574:44 - you'll see that the confidence interval
574:46 - includes 0.5
574:48 - it includes some numbers less than 0.5
574:51 - it includes some numbers greater than
574:53 - 0.5
574:54 - and as such we know that that entire
574:58 - confidence interval is a set of
574:59 - reasonable values for p
575:02 - since it includes things above below and
575:05 - equal to 0.5 all of those are reasonable
575:07 - realities
575:12 - so we fail to reject the null hypothesis
575:15 - p-value is too large
575:20 - example two
575:21 - cards
575:23 - I believe that my blackjack dealer is
575:25 - cheating if everything is fairer than I
575:27 - expect to have a blackjack 4.75 the time
575:30 - you can calculate that using chapter 4
575:32 - stuff
575:33 - I have played 132 hands and got a
575:36 - blackjack only once
575:38 - do I have evidence that the black deck
575:41 - blackjack dealer is cheating
575:43 - in other words using statistical
575:45 - language my research hypothesis my claim
575:47 - about reality is that P is less than
575:51 - 4.75 percent
575:55 - why less than
575:57 - why would my blackjack dealer cheat and
575:59 - give me a higher probability of winning
576:02 - so less than
576:04 - will this also be the alternative or the
576:07 - null
576:08 - correct this will also be the
576:10 - alternative this has no equals part to
576:12 - it
576:13 - so what will the null hypothesis be
576:17 - very good greater than or equal to
576:21 - from the way the problem is set up we
576:22 - know the number of blackjacks X follows
576:25 - a binomial
576:26 - n of 132 the number of hands I've played
576:29 - P of 0.0475 which is 4.75 percent
576:34 - which is the equals part in the null
576:36 - hypothesis
576:40 - also we know that if the number of
576:41 - blackjacks is too small
576:44 - then the alternative hypothesis is more
576:46 - likely to be correct and we would reject
576:49 - the null
576:52 - so let's calculate the p-value
576:55 - I observed one so the p-value is going
576:58 - to be X less the probability of X being
577:00 - less than or equal to one
577:03 - given X follows this distribution
577:09 - so as P binom one size 132 prob 0.0475
577:13 - that gives us a p-value of .0123027
577:18 - because the p-value is so small I reject
577:22 - the null hypothesis
577:23 - I have evidence that the blackjack
577:25 - dealer is cheating
577:28 - notice that I say I have evidence I
577:31 - don't say this is proof
577:33 - statisticians deal in evidence not in
577:36 - proof
577:41 - so this will be the distribution
577:44 - or part of it continues on to the right
577:47 - a lot
577:48 - a distribution under the null hypothesis
577:51 - the p-value will be the sum of these two
577:53 - heights
577:55 - notice that it is very small very low
577:58 - P naught is our claimed
578:06 - well oh yeah P naught is our
578:11 - it's the 4.75 percent
578:14 - sorry it threw me off there for a moment
578:16 - P naught is our 4.75 percent four point
578:20 - yeah 4.75
578:23 - what we observed was way down here
578:26 - and remember the p-value is what we
578:28 - observed or more so
578:31 - observed more so
578:40 - next example the D in CCD
578:44 - I believe that this dye is biased
578:46 - against getting a six specifically using
578:48 - statistical language my research
578:50 - hypothesis Claim about reality
578:53 - is hrp less than 1 6
578:57 - which is 0.11 I'm sorry 0.1667
579:03 - to test this I roll the die 100 times
579:06 - and get six a total of nine times
579:09 - because this is less than that's the
579:12 - alternative
579:14 - phenel would be greater than or equal to
579:17 - this is also binomial
579:19 - n of a hundred
579:21 - P of 0.1667
579:26 - I need to calculate let's see I got a
579:28 - total of nine times so I need to
579:30 - calculate the probability of X being
579:32 - less than or equal to 9.
579:35 - the equal to is data this extreme
579:39 - and the less than is or more so
579:43 - we know how to calculate this this comes
579:45 - out to be 0.02124964
579:50 - if we need to make a decision since this
579:53 - is less than our Alpha of 0.05 we reject
579:55 - the null hypothesis
579:57 - we have evidence that the dye is biased
580:00 - against the number six
580:05 - here's the probability Mass function for
580:08 - that particular binomial
580:10 - I observed 9 which is right here
580:13 - p-value would be the sum of the heights
580:17 - for nine and less
580:25 - those are pretty straightforward those
580:27 - are the binomial examples
580:29 - let's look at some means examples these
580:32 - will go back to the Z distribution
580:36 - um
580:37 - some fast food restaurant in town claims
580:40 - that the weight of a quarter pounder
580:42 - hamburger before cooking is four ounces
580:45 - with a standard deviation of Sigma
580:47 - equals one
580:48 - ounce
580:50 - notice we are given the population
580:52 - standard deviation is one
580:56 - the research hypothesis or the claim
580:59 - bioacdonalds is Mu is equal to four
581:04 - since this is an equals this will also
581:06 - be the null hypothesis
581:09 - the alternative hypothesis will be not
581:12 - equal to that's no we haven't done or
581:14 - not equal to
581:15 - test this weigh a stack of 25 patties
581:18 - and find that the total weight is only
581:19 - 94 ounces
581:21 - we would expect
581:23 - it to be a hundred four times 25 but
581:26 - it's only 94.
581:28 - is there sufficient evidence that whack
581:30 - the nulls is incorrect
581:35 - so we're trying to calculate probability
581:38 - that t remember capital t is the
581:40 - statistic total or the sum of the
581:44 - observations
581:47 - probability that t is less than or equal
581:48 - to 94 or t is greater than or equal to
581:50 - 106.
581:52 - well the less than or equal to 94 is
581:54 - pretty obvious we observed 94
581:57 - that's less than what we would expect
581:59 - therefore we'd have to calculate the
582:01 - probability of P being
582:02 - as extreme equals part or more so
582:07 - but remember the alternative is not
582:10 - equal to
582:11 - therefore being too high would also be
582:14 - as extreme or more so
582:17 - 94 is 6 less than what we would expect
582:21 - so the upper end will be six more than
582:23 - what we expect
582:27 - and then greater than equal to that 106.
582:31 - uh T follows a normal distribution with
582:34 - expected value of 100 and variance of
582:37 - one times square root of 25 and where
582:40 - this is for the sample total and the
582:42 - statement more or less comes from the
582:43 - central limit theorem
582:45 - in fact forget the more or less the
582:48 - statement does come from the central
582:49 - limit theorem specifically since I
582:51 - didn't tell you the actual distribution
582:53 - of the Patty weights
582:57 - one times the square root of 25 is just
582:59 - five so keep that in mind
583:03 - this is just two times probability
583:07 - of X being less than or equal to 94
583:11 - given mu is equal to 100 and S is equal
583:13 - to 5.
583:15 - why is it two times this well notice
583:18 - we're only calculating the lower tail
583:21 - and we actually have to calculate both
583:22 - the lower and the upper
583:25 - but the normal distribution is symmetric
583:28 - so I can just double the lower
583:31 - gives me a p-value of 0.23
583:34 - because this p-value is greater than
583:36 - Alpha we cannot reject the null
583:38 - hypothesis
583:40 - if we do a confidence interval we'll see
583:42 - that the confidence interval does
583:43 - include 100
583:45 - as well as values above it and values
583:47 - below it
583:50 - so we don't actually know if mu is 4
583:54 - or greater than 4 or less than four
583:58 - all values work according to our data
584:03 - which means we probably should go back
584:04 - there and collect some more data
584:07 - here's the distribution of t
584:11 - notice we observed 94
584:14 - this area is less than or equal to 94.
584:18 - this is the as extreme or more so on the
584:22 - bottom end
584:24 - is this 106 so this will be the as
584:26 - extreme or more so on the upper end
584:31 - normal distribution is symmetric so this
584:34 - dark blue area will be exactly the same
584:36 - as this dark blue area
584:38 - so we can get away with just doubling
584:40 - this lower tail
584:45 - so the total area or double the lower is
584:48 - the p-value
584:52 - uh McDonald's claims that the weight of
584:55 - a half pounder hamburger patty before
584:58 - cooking is eight ounces with a standard
585:00 - deviation of Sigma equals one
585:02 - again we know we're given Sigma the
585:05 - population standard deviation
585:09 - in symbols this will be mu is equal to
585:11 - eight
585:13 - because they're claiming that the
585:15 - average weight is eight ounces
585:18 - to test this we weigh a stack of 25
585:19 - patties and find that the average
585:22 - now we're looking at the average is only
585:25 - 7.5 ounces
585:27 - sufficient evidence that wax Donald's
585:30 - incorrect
585:32 - since the research is equal that will
585:34 - also be the null
585:36 - alternative will be not equal to
585:41 - we observe 7.5 ounces so we need to
585:44 - calculate the probability of being less
585:46 - than or equals seven point ounces and
585:49 - greater than or equal to 8.5 ounces
585:52 - it because the alternative is not equal
585:54 - to
585:57 - here x bar follows a normal mean eight
586:00 - standard deviation one over five
586:04 - 1 over the square root of n
586:07 - and again this comes from the center
586:08 - limit theorem
586:12 - 2 times P Norm of 7.5
586:15 - mean of eight standard deviation of 0.2
586:18 - gives us a p-value of .01241933
586:24 - how should we interpret this result
586:28 - because the p because the p-value is
586:31 - less than Alpha we reject the null
586:33 - hypothesis
586:34 - we have sufficient evidence that mu is
586:37 - less than eight
586:41 - here's the distribution of X bar
586:44 - we observed way down here
586:47 - whack Donald's claims way up here but we
586:50 - observe down here
586:52 - p-value is this area
586:55 - plus this area
586:58 - because both are as extreme from U or
587:02 - more so
587:03 - we can just double the lower tail
587:06 - that will give us the p-value that we
587:08 - saw it very small probability we're way
587:11 - out here in the tails
587:15 - whack the nulls claims at the weight of
587:17 - a pounder hamburger patty before cooking
587:20 - is at least 16 ounces with the standard
587:23 - deviation of Sigma equal one ounce
587:26 - and sybils this is HR
587:29 - mu that's the average
587:34 - is greater than or equal to 16 y greater
587:38 - than or equal to because it's at least
587:40 - that's what greater than or equal to
587:42 - means
587:45 - this will also be the null hypothesis
587:47 - because it has the equals part
587:50 - the alternative will be the opposite of
587:52 - this it'll be less than
587:55 - to test this we weigh a stack of 100
587:56 - patties and find that the average weight
587:58 - is 15.9 ounces
588:03 - this sufficient evidence that whack the
588:05 - nulls is incorrect
588:07 - let's calculate the p-value
588:10 - I want to calculate the p-value this is
588:12 - just probability of X bar being less
588:13 - than or equal to 15.9
588:17 - why only One Direction that's because
588:19 - the research hypothesis only has one
588:22 - direction
588:24 - notice we're calculating less than or
588:25 - equal to
588:27 - the alternative was less than
588:30 - and we know that X bar follows a normal
588:32 - mean of 16 standard deviation of one
588:35 - over ten
588:38 - because N is a hundred
588:40 - and this is a really good weekend for me
588:45 - here's how we calculate the p-value
588:47 - comes out to be 0.1586553
588:51 - this value is greater than Alpha
588:53 - therefore we cannot reject the null
588:56 - hypothesis
588:57 - there is not sufficient evidence that
589:00 - McDonald's is wrong I mean whack the
589:01 - nulls is wrong
589:05 - they could be wrong but we don't have
589:07 - the evidence that they're wrong
589:09 - because the p-value is so high
589:12 - here's that distribution of X bar
589:15 - this is what we observed remember the
589:18 - alternative was less than
589:21 - because the null was greater than or
589:23 - equal to the alternative is less than so
589:25 - we shade less than
589:27 - what we observe this will be the p-value
589:34 - one more example after this
589:39 - number of calories in a mcpork is at
589:42 - most 350.
589:46 - with the standard deviation of 50
589:47 - calories and symbols this is less than
589:49 - or equal to
589:50 - at most is less than or equal to to test
589:54 - this we perform a calorimetry tests on a
589:56 - stack of 100 with porks what a waste of
589:59 - good food and find that the average
590:01 - number of calories is 343.
590:06 - note the alternative is greater than
590:08 - because this is less than or equal to
590:11 - this will be the null the or equal to
590:13 - part is key there the alternative will
590:16 - be the opposite
590:17 - so now we ask what's the p-value is
590:19 - there sufficient evidence that whack
590:21 - Donald's is incorrect
590:24 - we need to calculate the probability of
590:26 - X bar being greater than or equal to 343
590:30 - we observe 343
590:32 - the alternative is greater than
590:35 - so the greater than part will be the or
590:38 - more so extreme
590:41 - and we got X bar follows a normal 350
590:44 - because that's our claim
590:46 - with Sigma equal to 50. boom boom boom
590:49 - divided by the square root of n
590:54 - is just P Norm 343
590:58 - 55 gives us a p-value 0.9192433
591:03 - because the p-value is greater than
591:05 - Alpha we cannot reject the null
591:08 - hypothesis there is no sufficient
591:10 - evidence that whack the nulls is wrong
591:13 - about its calorie statement
591:19 - they could be wrong we just don't have
591:21 - the evidence
591:23 - there is the distribution remember the
591:26 - alternative was greater than so we shade
591:28 - above what we observe
591:32 - and that's a lot of dark blue so the
591:34 - p-value really looks really big
591:42 - so short summary we looked at stating
591:44 - hypotheses testing hypotheses
591:46 - calculating p-value from the definition
591:50 - we looked at two R functions these are
591:52 - things that we've seen in the past
591:55 - download the all procedures PDF file
591:58 - that lists all the statistical
591:59 - procedures we're going to use in R we're
592:01 - going to see how to use those in the
592:03 - next lecture
592:05 - because in the next lecture we're going
592:07 - to review today and use R to perform the
592:10 - calculations really easy
592:14 - I guess that should be p-values
592:17 - um again we're going to cover a lot of
592:19 - tests
592:21 - so keep separate sheets for each of the
592:23 - tests
592:24 - there's an example of a flow chart on
592:26 - how to test me a single mean
592:31 - on the module on the module 4 Page
592:34 - follow it create your own for the rest
592:38 - of them
592:39 - here are some readings are for starters
592:41 - chapter 4 Hawks learning chapter 10. of
592:45 - course Wikipedia hypothesis tests
592:48 - which brings us to the intra-electric
592:50 - questions there's three of them
592:53 - question one again write question one
592:56 - the question on the left hand side of
592:59 - your notes answer it below so you can
593:02 - easily transfer it into Moodle
593:04 - what is the relationship between the
593:06 - null and the alternative hypotheses
593:09 - question two what are the three allowed
593:13 - signs in a null hypothesis
593:16 - question three
593:19 - when is the research hypothesis the same
593:21 - as the alternative hypothesis
593:23 - and all three of these get back at a
593:26 - common problem in intro stats of trying
593:30 - to figure out okay I've got a research
593:31 - hypothesis where do I go from here you
593:35 - may want to review the table
593:37 - um or he may not
593:40 - so those are the three
593:42 - and that's the end of this
593:45 - again next lecture we're going to see
593:47 - how to do this in r
593:49 - thank you much
593:51 - hello and welcome to section 10 4
593:53 - chapter 10 is all about hypothesis
593:55 - testing section 10 4 is about handling
593:58 - proportions one sample and two sample
594:01 - proportions so by the end of this
594:04 - lecture you should be able to understand
594:05 - the theory behind and test hypotheses
594:08 - about a single population proportion and
594:11 - the difference between two population
594:12 - proportions you should also better
594:14 - understand that the better understand
594:17 - what the p-value means and how to test
594:19 - hypotheses and clearly specify why
594:22 - confidence intervals and p-values both
594:24 - give important informations about the
594:26 - population parameter
594:30 - so one parameter procedures
594:33 - this will be about population
594:36 - proportion
594:38 - p
594:40 - on the parametric procedure is the
594:42 - binomial procedure
594:44 - the usual graphic is the binomial plots
594:46 - that seems to be a lot like what we did
594:50 - back with confidence intervals and one
594:52 - parameter
594:54 - uh one population proportion hmm
594:58 - requires the data being generated from
595:00 - the binomial distribution that really
595:03 - sounds familiar the r function is
595:05 - binom.tests X comma n x is the number of
595:08 - successes and is the number of Trials i
595:11 - s I guarantee we've seen this somewhere
595:13 - before haven't we
595:15 - note that this is not the procedure that
595:18 - Hawks covers they use something called
595:20 - the walled tests World DOT test it also
595:23 - takes X comma n
595:26 - so that should help you with the Hox
595:28 - assignments
595:31 - here's the tests Theory and we've
595:33 - actually covered this back when we
595:35 - introduced hypothesis testing it was a
595:38 - whole bunch of binomial stuff
595:39 - this is what led to the binomial test
595:42 - and we're trying to draw conclusions
595:44 - about a single population proportion we
595:46 - should use a test statistic based on the
595:48 - sample proportion
595:50 - or on what we observe the number of of
595:53 - successes if we do it based on the
595:55 - sample proportion we'll use the walled
595:57 - test if we do it on the number of
595:59 - successes we use the binomial test
596:02 - again the binomial test is the exact
596:04 - test and the wall test is the
596:06 - approximate test
596:08 - um
596:09 - we're given X follows a binomial
596:11 - distribution recall that the binomial
596:13 - has two parameters and NP
596:16 - um the number of observations serves as
596:19 - a particularly fine test statistic
596:21 - because we know it's distribution
596:23 - exactly
596:24 - it's binomial
596:26 - we only know the distribution of the
596:28 - proportions approximately x divided by n
596:32 - is only approximately normal
596:34 - and that only comes about because of the
596:37 - central limit theorem if we go all the
596:40 - way back to section six
596:43 - point five approximating the binomial
596:47 - with the normal we see oh there's a lot
596:49 - of error that can pop in there because
596:51 - it is a an approximation and the smaller
596:54 - the sample size the more the error
596:59 - the following are three examples showing
597:01 - how to perform these calculations
597:05 - I have a coin that I think is fair to
597:07 - test this F of it 10 times and count the
597:09 - number of heads in those 10 flips
597:11 - total of three heads actually came up is
597:14 - this sufficient evidence of the coin
597:15 - that's not fair
597:18 - so the claim is p is equal to one-half
597:22 - that's the research hypothesis
597:25 - since it contains equal sign this is
597:27 - also the null hypothesis
597:30 - since the equals is the null the
597:32 - alternative will be not equal
597:38 - we're trying to make a conclusion about
597:39 - P where the data are generated from a
597:41 - binomial
597:43 - thus under the null hypothesis X follows
597:45 - the binomial n of tan P of 0.5
597:48 - we observed x equals three
597:53 - p-value is defined as the probability of
597:55 - observing data this extreme or more so
597:58 - given that the null hypothesis is true
598:02 - in other words the p-value is the
598:04 - probability of X being less than or
598:05 - equal to three
598:07 - plus X greater than a probability of X
598:10 - greater than equal to seven
598:12 - data this extreme would be x equal three
598:15 - or or more so is less than three
598:20 - where'd the 7 come from
598:22 - well expected value of x which is n
598:25 - times p is five
598:27 - three is two less than five just as
598:31 - extreme would be two more than five
598:34 - and more extreme than that would be
598:35 - greater than two plus five
598:43 - there's the distribution of x
598:47 - distribution of the number of heads
598:49 - we observed three
598:52 - that's the red thing
598:55 - 7 is equally
598:57 - extreme
599:01 - less than or equal to 3 is or more so on
599:04 - the left and greater than or equal to
599:06 - seven is or more so on the right
599:09 - so the red Heights added up will give us
599:12 - our p-value
599:17 - so the p-value is
599:18 - 0.34375 simply using calculations that
599:22 - we did back in chapter five
599:25 - section two
599:26 - and in fact we did these calculations
599:29 - back in chapter five section two
599:32 - and we interpreted it correctly as
599:35 - evidence in favor of or against the
599:38 - claim
599:40 - we're just giving it some more
599:42 - terminology and some more meaning
599:48 - from the all procedures handout and the
599:50 - sca examples that we've looked at we
599:52 - know we can also do binom.test x equals
599:55 - three n equals 10 and P equals .050
599:59 - line tells us that the p-value is 0.3438
600:02 - since this is greater than Alpha we fail
600:05 - to reject the null hypothesis the coin
600:08 - may be fair the coin may not be fair we
600:11 - have no evidence that can tell us either
600:13 - way
600:15 - a 95 confidence interval for the
600:18 - probability of a flip Landing head is
600:20 - between 6.7 percent and 65.2 percent
600:30 - that is an extremely wide confidence
600:32 - interval
600:33 - how possibly could we make it narrower
600:38 - right
600:39 - collect more data instead of flipping
600:41 - the coin 10 times flip the coin
600:45 - a hundred times
600:47 - ten hundred times
600:50 - ten thousand times
600:52 - the larger the sample size the narrower
600:55 - the confidence interval
600:58 - example two this goes back to
601:01 - a a lab that we did
601:05 - I contend that more than a quarter of
601:07 - students at Knox or Juniors to test this
601:09 - I randomly sample from the student body
601:11 - asking class here
601:13 - in my sample of 100 students 30 stated
601:15 - they were juniors
601:18 - here the claim is p is greater than 0.25
601:22 - because I contend that more than
601:25 - a quarter so it's more than
601:28 - since it contains the greater than sign
601:30 - this will be the alternative
601:33 - the no will be the opposite of greater
601:35 - than which is not greater than otherwise
601:38 - known as less than or equal to
601:41 - that means the two statistical
601:42 - hypotheses are given below
601:48 - here's the distribution of the number of
601:51 - Juniors under the null hypothesis
601:54 - remember the null hypothesis contains
601:57 - the equals part of a quarter
601:59 - I asked 100 people
602:01 - so this would be the distribution of the
602:03 - number of Juniors that I experience
602:08 - p-value will be the probability of
602:09 - remember I got 30 of them so this is
602:12 - what I observed
602:15 - right here
602:20 - since the alternative was greater than
602:22 - the p-value is going to be the
602:24 - probability of X being greater than or
602:26 - equal to 30.
602:28 - the equal to is as extreme and the
602:31 - greater than is the or more so
602:35 - we get 0.1495
602:39 - because the p-value is greater than
602:40 - Alpha we fail to reject the null
602:42 - hypothesis we do not have sufficient
602:45 - evidence that the proportion of Genius
602:47 - is greater than a quarter
602:53 - in fact
602:54 - we can calculate the confidence interval
602:56 - directly
602:57 - and find that the proportion of generous
602:59 - is greater than 0.2249 based on this
603:02 - data
603:04 - or we can use a binomial test
603:08 - X is 30
603:10 - because that's what we observe the
603:11 - number of successes N is a hundred
603:13 - that's the sample size p is 0.25 that's
603:18 - my value of Interest that's my Theta
603:20 - naught
603:21 - the alternative is greater because I
603:23 - claim that mu is I'm sorry I claim that
603:25 - P is greater than 0.25
603:31 - if you get a p-value of Point 1495
603:34 - we get the confidence interval from 0.23
603:37 - up
603:39 - so I'm 95 percent that the proportion of
603:41 - Genius is at least 0.2249232
603:54 - one thing that we can definitely use
603:56 - this binomial test for is to check if a
603:59 - data set is representative on its face
604:03 - the data file is some college
604:06 - this is real data I had to change the
604:09 - name of the college to protect the
604:12 - innocent
604:14 - it was sent to me by the Registrar of
604:16 - some college to do some statistical
604:18 - analysis I did a little check sent it
604:21 - right back and said the data are not
604:23 - representative
604:24 - she said yes it is I said okay I'll go
604:29 - ahead and do the analysis and it'll be
604:31 - worthless but I will tell you it's
604:33 - worthless she kind of got upset with me
604:36 - um
604:38 - I was supposed to model the success the
604:40 - high enough GPA given some of the other
604:42 - variables let's perform a quick check to
604:44 - see if the data are reasonably
604:45 - representative of the population at that
604:47 - College
604:51 - so my provided sample consisted of 661
604:54 - students she claimed a random sample
604:57 - from the College of which 22 were
604:59 - freshmen
605:02 - given that the proportion of freshmen at
605:04 - SCU some College University is 28
605:08 - percent
605:09 - is the day or the data representative in
605:12 - terms of freshmen
605:15 - so here the claim is p is equal to 0.280
605:19 - since it contains equal sign it's the
605:21 - null
605:22 - that means that the two hypotheses are
605:24 - equal and then not equal for the
605:26 - alternative
605:31 - under the null hypothesis
605:34 - the number of freshmen in my sample
605:36 - should follow this binomial distribution
605:39 - n of 661 P of 0.28
605:48 - remember I observed 22.
605:52 - so I observed something down here
605:57 - something just as extreme is going to be
605:59 - located way up here
606:02 - or we can simply just double
606:04 - the probability of X being less than or
606:06 - equal to 22.
606:09 - probability of X being less than or
606:10 - equal to 22 is zero
606:13 - I'll go ahead and double that 2 times 0
606:15 - is 0.
606:17 - we could also add zero to that
606:20 - um
606:22 - we could multiply by one if we want to
606:24 - stretch this out
606:26 - but it comes down the p-value is zero
606:29 - since the p-value is less than or Alpha
606:31 - 0.05 we reject the null hypothesis in
606:34 - favor of the alternative
606:36 - in terms of freshmen the sample is not
606:38 - represented of the population of SCU
606:42 - Simpson I'm trying to model the GPA
606:45 - it is very reasonable that freshman GPA
606:48 - will be different from sophomores
606:50 - Juniors and seniors therefore in order
606:53 - for my analysis to be worth the weight
606:55 - that it's
606:57 - to be worth its weight in gold I really
607:00 - do need more freshmen in my sample
607:05 - or we can just use power of r
607:07 - binom.tests x is 22 number of successes
607:10 - n is 661 p is 0.28
607:14 - I got the 0.28 from there uh
607:17 - website
607:19 - got a p-value of being less than 2.2
607:22 - times 10 to the negative 16th
607:27 - this data would be representative if
607:31 - the proportion of freshmen at SCU is
607:33 - between two percent and five percent
607:39 - now that I talk about this I realize I
607:42 - never did get paid for this so yeah
607:45 - it's okay I didn't do anything other
607:47 - than
607:48 - say that they didn't give me the data
607:55 - next types of tests the two parameter
607:57 - procedures we're going to compare P1
608:01 - minus P2 or we're going to compare P1
608:04 - and P2 the parameter of interest will be
608:07 - P1 minus P2 this would be the
608:10 - proportions procedure or the two
608:13 - proportions procedure the graphic will
608:16 - be the binomial plot it requires that
608:18 - the expected number of successes is at
608:20 - least five in each group
608:23 - as some would say at least 10 in each
608:26 - group
608:27 - uh if no one's dying from you being
608:30 - wrong then at least 10 should be fine
608:32 - if you really do need to make sure that
608:35 - you're right then at least 50 or at
608:37 - least 100 in each group would be
608:39 - preferred
608:40 - the proportions test is based on the
608:43 - normality approximation to the binomial
608:46 - distribution
608:48 - therefore large sample size is required
608:52 - the r function is
608:55 - pardon me is prop.tests
609:00 - you give it the number of successes X
609:03 - and the number of Trials n since it's 2
609:07 - um
609:08 - two proportions we're comparing you got
609:10 - to give it two sets of x's or two x's
609:13 - and two ends
609:15 - wrapped in C's
609:20 - they use something close to this but
609:23 - this procedure actually makes
609:24 - adjustments for the fact that the
609:25 - binomial distribution is discrete and
609:27 - the normal is not
609:28 - as such you'll need to use the wall test
609:31 - to perform Hawk's homework estimating P1
609:33 - minus P2
609:35 - so again when you're using proportion
609:38 - stuff with Hawks use the walled test
609:41 - the wall test will act will require both
609:44 - the X and the and just as laid out here
609:46 - with prop test
609:48 - um sample sizes are large the wall test
609:52 - and the proportions test will give you
609:53 - essentially the same answers
609:56 - it's only when the sample size is small
609:58 - that they're going to be different
610:02 - so here's the theory behind the
610:04 - proportions test
610:06 - actually here's the theory behind the
610:08 - walled test
610:09 - since we're trying to draw conclusions
610:11 - about the difference between two
610:13 - population proportions we're going to
610:15 - use a test statistic based on the
610:16 - difference into sample proportions
610:19 - in other words we're going to use sample
610:21 - proportion one minus sample proportion
610:23 - two
610:24 - we do know that X and Y both follow
610:26 - binomial distributions each with their
610:29 - own sample sizes each with their own
610:31 - probabilities PX and py
610:35 - one of the biggest problems is x minus y
610:38 - is not going to work because we don't
610:41 - know the distribution of x minus y it's
610:43 - not binomial
610:46 - um
610:48 - we but we do know that the sample
610:50 - proportion for x minus the sample
610:52 - proportion for y is a good estimator of
610:55 - p x minus py
610:57 - and the sample proportion is just the
610:59 - number of successes divided by the
611:00 - number of Trials
611:02 - so that's going to be the successes over
611:03 - trials for x minus successes over trial
611:06 - for y
611:07 - all we have to do now is figure out the
611:09 - distribution of this statistic
611:15 - if we're okay with approximation we know
611:18 - that X is approximately normal and Y is
611:21 - approximately normal which means that X
611:23 - over n x is approximately normal and Y
611:26 - over n y will be approximately normal
611:28 - and X over n x minus y over n y will
611:31 - also be approximately normal
611:33 - Central limit theorem
611:36 - is just so awesome
611:37 - I mean we can say all of that simply
611:40 - because of the central limit theorem
611:42 - and the approximation increases I mean
611:44 - the approximation gets better as the
611:46 - sample size is individually get better
611:52 - which is what we're saying here remember
611:53 - that the if x follows a binomial then it
611:57 - also approximately follows a normal with
612:00 - expected value of NP and variance of NP
612:03 - 1 minus p
612:06 - and we'll subscript all the X stuff with
612:08 - X and all the Y stuff with y
612:12 - standardizing
612:14 - or I'm sorry dividing by x and x and and
612:17 - Y will get us this as our normal
612:20 - distributions
612:21 - subtracting X over N X and Y over n y
612:24 - gets us its own normal distribution
612:28 - notice that this p x minus p y is what
612:30 - we're trying to estimate
612:35 - in other words
612:37 - our sample proportion for x minus the
612:39 - sample proportion for y is unbiased for
612:43 - our population proportion x minus
612:45 - population proportion y That's good
612:47 - here's the variance though
612:50 - and we'll do the typical standardization
612:53 - we're going to subtract off the expected
612:55 - value divide by the square root of the
612:56 - variance and that means that it will
612:59 - follow this this test statistic here
613:01 - will follow a normal distribution with
613:04 - mean zero standard deviation one
613:07 - and the standardization comes directly
613:09 - out of either chapter 2 or chapter 3
613:11 - when we did the Z scores or the
613:14 - standardized scores
613:20 - if p x equals p y is our null hypothesis
613:24 - then that means that the second half
613:26 - just goes to zero or is equal to zero
613:28 - and this is the usual Z procedure
613:31 - version of the test
613:36 - we can do an equivalent test to this
613:40 - this isn't a side
613:42 - if we Square this thing on the left and
613:46 - that means we have to square this thing
613:48 - on the right
613:49 - squaring this thing on the left leads us
613:51 - to this
613:53 - squaring the thing on the right brings
613:54 - up a new distribution it's the
613:56 - chi-squared distribution this
614:00 - probability statement and this
614:02 - probability statement are identical
614:09 - there is no information contained in one
614:11 - that's not contained in the other
614:13 - there is no difference in power
614:16 - there is no difference in Precision
614:18 - there is no difference in accuracy the
614:21 - two tests are mathematically equivalent
614:25 - so the one you use if you use this one
614:27 - you might as well use this one if you
614:29 - use this one go ahead and use this one
614:31 - they're the same
614:33 - the proof of that is left for later
614:35 - class 225 or 321.
614:41 - so let's see how to do this I'd like to
614:44 - determine the proportion of males who
614:45 - wear hats is the same as a proportion of
614:47 - females who wear hats notice now we're
614:50 - dealing with proportions in two separate
614:52 - populations
614:53 - proportion of hat wearers of males and
614:56 - proportion of hat wearers of females
615:00 - to test this I sample 100 males 100
615:02 - females
615:04 - 10 males and 16 females were wearing
615:06 - hats
615:08 - so those are the hypotheses
615:15 - step one we're going to assemble the
615:17 - information we have
615:19 - P sub X is what we observe
615:23 - P sub y sample proportion I guess NX and
615:27 - Y we asked 100 males 100 females Alphas
615:30 - 0.05 which means our Z sub Alpha over
615:33 - two is plus or minus 1.96
615:37 - this is the test statistic we have to
615:39 - calculate
615:41 - I'll give you a hint there's a fast way
615:43 - of doing this in R and you already know
615:45 - it but let's go ahead and churn through
615:47 - this
615:50 - so here's the formula for the test
615:52 - statistic this is the plug this is the
615:54 - Chug and we're chugging some more and
615:57 - more chugging and chugging we're done
615:58 - chugging so our test statistic is
616:00 - negative 1.26601
616:07 - this is this distribution is our
616:10 - standard normal distribution this is
616:12 - what we observe
616:13 - negative 1.26601 it's right here
616:18 - that value is as extreme so it's
616:20 - positive 1.26601
616:23 - or more so
616:24 - is the Shaded area on both sides
616:29 - so these two areas add up give you the
616:31 - p-value
616:33 - because the normal is symmetric twice
616:35 - this area is the p-value
616:38 - so we can calculate the p-value
616:41 - it's 0.2053 because the p-value is
616:44 - greater than Alpha we failed to reject
616:46 - the null hypothesis
616:48 - we do not have evidence that men wear
616:50 - hats at a rate different than women
616:55 - which is what this says there is no
616:58 - evidence to claim that
617:01 - they may they may not we just don't know
617:05 - we don't have the evidence
617:09 - um what I just described is the walled
617:12 - test
617:16 - doing this in R is just one line as
617:20 - opposed to doing all this calculation by
617:22 - hand all this fun calculation
617:25 - it's just one line This is the output
617:30 - 10 successes for males 16 successes for
617:33 - females out of 100 trials for men and
617:35 - 100 trials for females
617:39 - gives us back the data
617:43 - here's the p-value
617:46 - 0.2071
617:49 - greater than Alpha failed reject the
617:52 - null hypothesis no evidence of a
617:54 - difference
617:57 - and that's it how how much faster is
618:00 - this than all of this
618:05 - and I'll tell you this when I was typing
618:07 - this up you know how many times I had to
618:09 - go through this and make sure I didn't
618:10 - make a mistake in the calculations I'm
618:12 - not talking about in the typing unit I'm
618:14 - talking about the calculations I think I
618:16 - even made a mistake in the numerator
618:18 - here I think I made this as plus 0.06
618:22 - so allowing the computer to do all of
618:25 - this in one line is just amazing
618:30 - so in today's slide deck we covered
618:32 - procedures for estimating a single
618:34 - population proportion and for estimating
618:37 - the difference in two population
618:38 - proportions
618:42 - if you're doing this in R I'm sorry if
618:44 - you're doing this for Hawks walled test
618:47 - works for both of these
618:50 - and you should use the wall test if
618:52 - you're doing Hawks if you're doing this
618:54 - for real
618:55 - binomial test for the first and prop
618:58 - test for the second
619:02 - in the future we're going to see many
619:03 - many many many many many many many many
619:05 - many many many many many more procedures
619:08 - more tests again create that section in
619:12 - your notebook dedicated to test the
619:13 - assumptions of the tests
619:15 - we looked at binom.test and prop.test
619:20 - wold.test behaves the exact same as
619:22 - either one of these except instead of
619:25 - binom.test you'll do Wald DOT test
619:27 - and instead of prop.test you'll do
619:29 - walt.test but the the parameters that
619:32 - you put over here are going to be the
619:33 - same
619:34 - here are some available scas to help you
619:37 - work through these
619:40 - SCA 12 and 22
619:44 - the 2 means it's for proportions the one
619:46 - is the number of samples so this sca-12
619:49 - is for one sample proportions
619:51 - and SCA 22 is for two sample proportions
619:55 - and they're all located in the usual
619:57 - place
619:59 - um
620:00 - so now I'm going to ask the
620:01 - intra-lecture questions
620:03 - one of these will be very familiar
620:06 - question one when is the research
620:09 - hypothesis the same as the alternative
620:14 - I encourage you to go back and look it
620:16 - up again
620:17 - the rest of the the remaining two
620:20 - questions will be different but similar
620:22 - to each other for what types of
620:24 - hypotheses do we use the binomial tests
620:30 - and what I mean by that is I'll just
620:32 - give you the answer because it's fun the
620:34 - types of hypotheses for which we use
620:36 - binomial tests are those where we're
620:38 - trying to test hypotheses about one
620:41 - single
620:42 - population proportion
620:47 - and question three is what types
620:49 - hypotheses do we use the proportions
620:51 - test
620:52 - I can give you that answer too for
620:54 - hypotheses about comparing two
620:58 - population proportions
621:05 - I don't mind giving the answer sometimes
621:09 - chapter 8 and R for starters sections 10
621:11 - 4 and 11 4 in Hawks
621:15 - go ahead and read up on hypothesis tests
621:17 - in Wikipedia
621:20 - all procedures at PDF you should have
621:22 - that printed out and available to you
621:24 - when you're working on stats and that's
621:28 - the end handling proportions
621:32 - um hope this was helpful
621:35 - I will see you or talk to you later
621:39 - hello and welcome to chapter 10 where we
621:41 - cover lots and lots of tests for
621:43 - hypothesis testing here we're going to
621:46 - look at section 10 6 where we look at
621:47 - discrete distribution matching we
621:50 - introduce the
621:52 - chi-squared goodness of fit test and by
621:55 - the end of this lecture you should be
621:56 - able to understand the theory behind and
621:58 - test hypotheses about
622:01 - just one thing comparing an observed
622:03 - categorical distribution to a
622:05 - hypothesized one
622:07 - also the usual better understand the
622:09 - p-value and how to test hypotheses but
622:11 - this one is about goodness of fit tests
622:16 - so here's the parametric procedure it's
622:18 - the only procedure we've got the
622:20 - chi-squared goodness of fit procedure
622:22 - the null hypothesis is that the data are
622:25 - generally generated by the hypothesized
622:27 - distribution
622:29 - so the research hypothesis is going to
622:33 - be in the null hypothesis is going to be
622:36 - that hypothesized distribution
622:39 - the alternative hypothesis will be the
622:42 - data are not generated by that
622:43 - hypothesized distribution
622:46 - the graphic is going to be the usual
622:48 - binomial plot notice that it's expanded
622:50 - to go from not one not two but K
622:54 - different values or different groups
622:57 - these will be successes the x's and the
623:00 - NS will be the number of Trials
623:03 - requires that the number of successes
623:05 - and expected number of successes in each
623:07 - group is at least five
623:10 - sometimes it's required to be 10 it
623:14 - really does come down to how precise or
623:18 - accurate do you want your values to be
623:20 - or your estimates to be
623:22 - um more data is always better as long as
623:25 - it's good data of course
623:28 - if your expected number of successes is
623:31 - not at least five in each group
623:33 - it's really not anything you can do
623:36 - except collect more data
623:39 - and if you can't collect more data then
623:40 - you've got to conclude whatever you can
623:42 - conclude from this test but you must
623:45 - specify the sample the sample size is
623:48 - too small to properly use this
623:50 - chi-squared goodness of fit test
623:52 - therefore these results are Highly
623:54 - Questionable
623:56 - the r function is that tests what does
624:01 - stand for it's Chi Squared and notice
624:05 - again we got a DOT test thing we get a
624:07 - whole lot of DOT test things and the Dot
624:09 - Plot things we got a lot of Dot Plot
624:11 - things
624:13 - note that this function this DOT test
624:16 - function is not Hawks
624:19 - they use something close to this but the
624:22 - case not test function actually makes
624:24 - adjustment for the fact that the
624:25 - binomial distribution is discrete and
624:27 - the normal is not
624:29 - the hand calculations that we have do
624:31 - agree with Hawks however
624:34 - um so pay attention to the quote hand
624:36 - calculations and how to do them in R so
624:40 - all you have to do is just substitute a
624:42 - few change a few numbers and
624:44 - it'll be good for you
624:47 - so we'll start with a framing example
624:50 - that is a picture of a three-sided die
624:55 - I would like to test if it is fair
624:59 - to do this I roll it 600 times and
625:02 - tabulate the observed frequency
625:03 - distribution
625:05 - in those 600 rolls I got 181s
625:10 - 215 twos and 205 threes
625:15 - note that I would expect to have 200
625:18 - ones 200 twos and 203s if the die were
625:24 - Fair
625:27 - I didn't I got 181s 215 twos and 205
625:31 - threes
625:32 - now the question is is the 180 215 and
625:35 - 205
625:37 - just due to the random fluctuation in a
625:41 - fair die or are those values too far
625:43 - away from what we would expect if the
625:46 - diver fare
625:48 - and that's really what all of the
625:50 - statistical testing is coming down to
625:52 - it's
625:54 - what I expect
625:56 - versus what I observe and is what I
626:00 - observe too far away from what I expect
626:02 - given expected random fluctuation
626:09 - so we're given this information I'm just
626:11 - abstracting it The observed counts are
626:13 - 180 to 15 205 the expected counts are
626:17 - 200 200 200
626:19 - now where did I get the 200
626:21 - sample size is 600 I rolled it 600 times
626:24 - I want to test if the three-sided die is
626:27 - fair
626:28 - if it is fair then I would expect
626:31 - the number of ones to equal the number
626:32 - of twos to equal the number of Threes
626:34 - each of them to be a third of six
626:36 - hundred
626:42 - note what the information above actually
626:44 - gives to us it gives us this a set of
626:47 - observed counts and a set of expected
626:50 - counts we're going to call the observed
626:52 - counts x's and the expected counts Muse
626:57 - and in this there are K groups
627:02 - that's what the k stands for in the
627:04 - example that we're working on K is equal
627:06 - to 3.
627:08 - so our goal is to create a test
627:10 - statistic that measures how far apart
627:12 - the observed is from the expected
627:17 - while creating well still having a
627:19 - distribution that we know
627:22 - and we're going to use Define we in this
627:25 - case as statisticians
627:28 - we in this class don't know this
627:31 - distribution yet although technically we
627:33 - have bumped into it once when we were
627:36 - doing onevar Dot tests
627:41 - so it can be shown but not in this class
627:44 - that the test statistic approximately
627:47 - follows a chi-squared distribution with
627:49 - K minus 1 degrees of freedom
627:52 - the approximation gets better
627:55 - as U gets larger
627:59 - so that statement the TS equals the sum
628:03 - and the probability statement is
628:05 - something that is
628:07 - to be shown in a future course
628:10 - in other words it's beyond the scope of
628:12 - this course I like that phrase beyond
628:14 - the scope of this course
628:16 - uh TS represents test statistic it's the
628:19 - sum over all of the groups The observed
628:23 - minus the expected squared I divided by
628:26 - the expected
628:29 - y the squaring
628:32 - we're squaring it because if we don't
628:33 - Square then the sum of the x i minus the
628:36 - MU I is always going to give us zero
628:39 - so this squaring allows us to avoid the
628:43 - zero problem and to indicate that larger
628:47 - values of there are
628:49 - could be too high or too low they're
628:52 - just farther away from what you would
628:54 - expect
628:57 - so recall the observed counts were 182
629:00 - 15 205 the expecteds were 200 200 200
629:04 - expected values again came from NP
629:07 - n is our number of Trials the p is the
629:12 - probability of success for each of those
629:14 - categories
629:16 - the probability of getting one is
629:17 - one-third so the expected number ones is
629:20 - 600 times one-third respective number
629:23 - twos is 600 times one-thirds respect to
629:25 - number three is the 600 times one-third
629:29 - yes binomial
629:31 - mu is equal to NP
629:36 - so let's go ahead and do these
629:37 - calculations out
629:39 - for fun
629:43 - TS to test statistic is defined as this
629:45 - if we expand that summation that's what
629:48 - this means
629:49 - first time through I is equal to one so
629:51 - it's x sub 1 minus mu sub 1 squared over
629:54 - mu sub 1.
629:55 - plus because it's a summation x times
629:58 - through I is equal to two
630:00 - x sub 2 minus mu sub 2 squared divided
630:05 - by mu sub 2.
630:06 - third time through and again we're going
630:08 - to add on the third term which is x sub
630:11 - 3 minus mu sub 3 squared over mu sub 3.
630:17 - so this is the expansion of the
630:18 - summation
630:19 - and now all we're doing is
630:22 - just substituting in the values we've
630:24 - got
630:25 - x sub 1 is when a to the x sub 2 is 215
630:28 - x sub 3 is 205 those are what we
630:31 - experienced
630:32 - or observed
630:35 - the expected values were 200 each
630:39 - now we do the calculations calculations
630:41 - calculations calculations so our test
630:44 - statistic is 3.250
630:50 - this is the distribution of the
630:52 - chi-squared distribution with two
630:54 - degrees of freedom
630:56 - this is what we observed for our test
630:58 - statistic 3.250 3.250
631:03 - shaded area is 3.25 is the probability
631:06 - of having an a test statistic of 3.250
631:10 - or greater
631:12 - so this shaded area is the p-value
631:16 - and that p-value actually comes out to
631:18 - be 0.1969
631:21 - here we are with a p-value we know how
631:23 - to interpret that p-value so it's been
631:24 - the same since we introduced p-values
631:27 - p-value is greater than Alpha failed
631:29 - reject the null hypothesis we don't have
631:32 - evidence that the die is unfair
631:36 - p-value greater than Alpha we failed to
631:39 - reject the null hypothesis we don't have
631:41 - evidence that the die is unfair
631:47 - p-value greater than 0.05
631:49 - we failed reject the null hypothesis we
631:53 - don't have evidence the die is unfair
631:56 - I think three is enough
631:58 - so there's the conclusion we don't have
632:01 - evidence that the die is unfair it could
632:03 - be unfair I don't know
632:06 - but the data doesn't tell us it's unfair
632:10 - note that the data doesn't tell us it's
632:12 - fair either
632:14 - the data leaves it up to a big question
632:16 - mark of we don't know could be fair may
632:18 - not be fair
632:19 - we don't have enough data to say
632:26 - so here's the way of getting it in uh
632:31 - for Hawks calculations
632:34 - OBS will be the observed counts
632:38 - EXP
632:39 - is the expected counts
632:43 - so these will need to change
632:46 - for your problem
632:49 - TS this is how you quickly calculate
632:51 - that test statistic that doesn't need to
632:54 - change
632:56 - just running TS will give you the value
632:58 - of the test statistic that won't need to
633:00 - change
633:01 - 1 minus P Kai's doesn't change of TS
633:06 - doesn't change but we do need to change
633:07 - the degrees of freedom
633:09 - this 2 needs to be K minus 1.
633:13 - the number of groups minus one so if
633:15 - we've got 10 groups here change that to
633:18 - a 9. if you've got two groups here
633:20 - change that to a one
633:23 - this will get you the Hawk's answers
633:27 - so we're going to test statistic of 3.25
633:29 - a p-value of 0.1969
633:35 - here it is using r
633:41 - kaisk.test so you give it the observed
633:44 - distribution of counts
633:47 - and this is important this is a
633:49 - distribution of counts
633:51 - and you also give the hypothesized
633:53 - distribution but these are going to be
633:56 - probabilities
634:01 - so instead of 200 200 200 you give it
634:04 - one-third one-third one-third
634:08 - just so happens in this case you do get
634:10 - the same chi-squared test statistic
634:12 - degrees of freedom and p-value as if you
634:16 - did it by hand using the Hawks method
634:24 - example two
634:26 - my friend claims that the proportion of
634:28 - cars in the Knox campus that are
634:29 - American is the same as the proportion
634:31 - that our European and as the proportion
634:34 - that are Asian
634:36 - okay
634:37 - so to test this I went to the parking
634:40 - lot across burying from smack and
634:42 - counted the cars and their Origins
634:45 - and
634:47 - had a conversation with a couple people
634:49 - who were wondering what I was doing over
634:51 - there
634:53 - I on that day I there were 19 American
634:57 - 23 Asian and two European cars
635:01 - so the observed and expected values are
635:04 - 19 The observed is 1923 and 2. the
635:08 - expected is 44 thirds 44 30s 44 thirds
635:13 - those are expected counts
635:16 - why is it thirds that's because they're
635:19 - the same proportions so the proportion
635:22 - of Americans equals the proportion of
635:23 - Europeans equals a proportion of Asians
635:26 - one-third one-third one-third
635:28 - where the 44 come from there are 44 cars
635:35 - so let's go ahead and calculate this out
635:38 - three groups so the test statistic this
635:41 - is the formula which is expanded to this
635:43 - plug and chug and chug and chug are we
635:47 - done chugging there we go test statistic
635:49 - is 16.954
635:55 - here's the distribution here the
635:57 - chi-squared distribution with two
635:58 - degrees of freedom
636:00 - there are three groups so it's three
636:02 - minus one two degrees of freedom
636:04 - whereas the p-value
636:07 - wait where's the test statistic test
636:09 - statistic was almost 17 so I guess test
636:11 - to sits way over here
636:14 - p-value .0002
636:18 - because the p-value is less than Alpha
636:20 - we reject the null hypothesis
636:23 - the proportion of cars on campus is not
636:26 - the same for American Asian and European
636:28 - that's all we can say is it's not the
636:30 - same this is not the distribution
636:35 - because the null hypothesis in the
636:37 - research hypothesis for chi-squared
636:40 - goodness fit test is this is the
636:43 - distribution
636:44 - the alternative would be this is not the
636:46 - distribution
636:49 - um
636:51 - what does this conclusion assume and
636:54 - it's time we start thinking about this
636:55 - again notice how I collected the data I
636:58 - went across the street from smack and
637:01 - went through the went through the
637:03 - parking lot
637:05 - for this to be an appropriate conclusion
637:09 - the data I collected has to be
637:12 - representative of the cars on campus
637:17 - cars in that one parking lot has to be
637:20 - representative of all the cars on campus
637:22 - the cluster sampling I did has to be
637:26 - appropriate
637:29 - remembering back to chapter one
637:31 - since it's cluster sampling
637:34 - I have to somehow assert or check that
637:39 - the proportion of cars that are American
637:41 - European Asian
637:43 - is independent of the parking lot
637:49 - huh how could we test that and I'll give
637:54 - you a hint we don't know yet
637:56 - because we would be testing for
637:59 - Independence between a categorical
638:02 - variable
638:03 - car type and a categorical variable
638:06 - parking lot
638:09 - so
638:11 - be aware that'll be a future test
638:13 - here is the code
638:16 - again three groups
638:19 - degrees of freedom is three minus one
638:22 - we can also do this with the test notice
638:26 - I did not specify comma P equals
638:30 - one-third one-third one-third
638:33 - here's why
638:34 - by default R assumes equal probabilities
638:41 - so if you are testing equal
638:43 - probabilities you don't actually have to
638:45 - specify that for r
638:47 - you can I encourage it because it
638:51 - increases the readability of your code
638:52 - but you don't have to
638:55 - in fact I strongly encourage it because
638:58 - it does make very explicit that you are
639:01 - testing equal probability amongst those
639:03 - three groups
639:12 - sample three Department of Mathematics
639:15 - claims that the proportion of its
639:16 - graduates who went to grad school is
639:19 - twice the proportion of any other
639:21 - postback path
639:24 - to test this department sent out a
639:26 - questionnaire to all of the alums who
639:29 - for whom they had current addresses
639:31 - here's a table of the results
639:37 - that includes the count and the expected
639:43 - we had 35 that we could get in touch
639:45 - with
639:46 - um
639:48 - notice we said twice the proportion of
639:50 - any other post back so
639:52 - 14 expected for grad school but seven
639:55 - for business seven for Education seven
639:56 - for unemployed
639:59 - by the way this is fake data we actually
640:02 - counted 14 grad school seven business 10
640:05 - education five unemployed
640:11 - so this will be the vector of counts
640:14 - observed and this will be a vector of
640:17 - counts expected
640:20 - this is how we calculate it by hand
640:24 - notice we now have four groups
640:27 - here's the chi-squared distribution with
640:30 - three degrees of freedom
640:32 - K is 4 K minus 1 is 3 degrees of freedom
640:36 - here's our value of the test statistic
640:39 - we got
640:41 - the Shaded region is our p-value p-value
640:44 - of 0.5874
640:47 - because the p-value is greater than
640:49 - Alpha we fail to reject the null
640:51 - hypothesis we do not have evidence that
640:55 - the data do not follow the distribution
641:00 - the claimed distribution
641:04 - the claim made by the math department
641:06 - that twice as many of its graduates go
641:08 - to grad school is in any other category
641:09 - is reasonable
641:12 - note that we did not prove that the math
641:14 - department is correct we just said their
641:16 - claim is reasonable given this data
641:21 - given additional data it may not be
641:23 - reasonable but given this particular
641:24 - data it is
641:27 - and then of course I'm going to ask what
641:28 - does this conclusion actually assume
641:32 - so I want to start tying back to chapter
641:34 - one more and more
641:36 - not just subtly but very explicitly
641:41 - how did the math department get this
641:43 - data
641:46 - is this data representative of all math
641:49 - majors
641:51 - in other words who would we have kept in
641:53 - touch with or who would have kept in
641:55 - touch with us
641:57 - is it equally likely that any graduate
642:00 - of the math department would have kept
642:01 - in touch with us or are certain types of
642:04 - students
642:05 - like grad students more likely to keep
642:09 - in touch with us
642:12 - or like education students more likely
642:15 - to keep in touch with us
642:19 - notice that we send out a questionnaire
642:22 - to all of the alums for whom we had
642:23 - current addresses
642:27 - Who Are we more likely to keep current
642:29 - addresses for
642:31 - who is more likely to fall off the grid
642:40 - this is how you do it for Hawks notice
642:42 - we got four groups so the degrees of
642:45 - freedom will be three
642:55 - notice I
642:57 - did the p is equal to C of 1477 divided
643:01 - by 35.
643:03 - I could have done CF 14 over 35 comma 7
643:06 - over 35 comma 7 over 35 comma 7 over 35
643:10 - this is a little bit faster
643:19 - one last example
643:22 - one of the initiatives of Knox College
643:24 - has become more representative of the US
643:25 - population
643:28 - this raises the question of whether we
643:29 - have succeeded in terms of numbers
643:32 - according to the fall 2019 domestic
643:35 - numbers the data are
643:37 - observed expected and the expected is
643:40 - due to its Compares aha let me start
643:44 - that again
643:45 - expected proportions are from the Census
643:50 - Bureau
643:52 - so we have 109 71 194 and 635
644:00 - we'd expect
644:01 - 163.58 73.10
644:04 - 238.37 and 555.61
644:08 - if we perfectly followed
644:14 - here's a graph
644:18 - notice
644:22 - what the the boxes with the lines are
644:25 - what we actually are
644:28 - and the dots are the population of the
644:32 - United States
644:34 - notice we are below average here we're
644:37 - dead on here
644:39 - we're below here and we're way above
644:41 - here
644:47 - by the way the boxes indicate confidence
644:49 - intervals
644:54 - the horizontal line is the proportions
644:56 - the sample proportions
645:04 - so here we are
645:07 - using r
645:09 - chi-square tests
645:11 - that's the observed counts
645:14 - this is proportions according to the
645:16 - Census Bureau I run it got an error
645:20 - the error is probabilities must sum to
645:22 - one
645:25 - what's the error well these
645:26 - probabilities don't add up to one
645:29 - why don't they add up to one well
645:31 - there's a couple possibilities one is I
645:33 - may have dropped a couple of the smaller
645:35 - groups two there could be rounding
645:37 - errors
645:39 - but for R to do this these probabilities
645:41 - have to add up to one
645:45 - so the question comes down to how do we
645:46 - fix it if we actually did leave out some
645:48 - of the smaller groups or if there is
645:50 - rounding how do we fix this
645:53 - R does have a parameter rescale dot p
645:59 - we set it equal to True it'll rescale
646:02 - these so that they do add up to 100
646:04 - percent
646:10 - so this will fix the error
646:15 - gives us a p-value that is significant
646:17 - is much much less than Alpha
646:19 - we have evidence that the distribution
646:22 - of races and ethnicities in Knox is does
646:26 - not match that in the population at
646:28 - large of the United States
646:32 - have her realize that this p-value
646:34 - corresponds to a snapshot in time
646:38 - it doesn't compare where we were 10
646:41 - years ago it just says where we were
646:43 - fall of 2019.
646:45 - so we weren't there we didn't match the
646:48 - distribution in the United States in
646:49 - 2019
646:51 - but we actually are getting much closer
646:53 - to Ria to the population in the United
646:56 - States
646:57 - much closer
646:59 - so the p-values are getting larger
647:01 - the test statistics are getting smaller
647:09 - and that's what this conclusion assumes
647:14 - so let's go ahead and do the introductor
647:16 - questions there we go
647:19 - huh this looks familiar question one
647:22 - when is the research hypothesis the same
647:24 - as the alternative hypothesis
647:29 - question two
647:31 - I like question two and question three
647:32 - so they're the same question essentially
647:35 - but I I want two examples question two
647:38 - is Give an example where you would use
647:40 - the chi-squared goodness of fit test in
647:42 - your area of Interest so if your major
647:45 - is or will be political science give me
647:48 - a time when you would use the goodness
647:49 - fit test in political science if your
647:51 - area of interest is or will be biology
647:53 - give me an example where to use the
647:55 - goodness of fit test in biology
647:58 - or in physics if you're going to be a
648:00 - physicist or in uh
648:03 - I can't think of any other areas and so
648:06 - if you're going to be an anthropologist
648:08 - a sociologist
648:09 - that's question two is give one example
648:12 - question three is to give another
648:14 - example
648:15 - so question two and three are give
648:17 - examples of where you would actually use
648:19 - this chi-square goodness of it in your
648:22 - discipline
648:23 - and this actually could be in classes
648:25 - that you've taken in your discipline
648:28 - for instance in chemistry
648:30 - reaction rates be
648:33 - no that wouldn't work for instance
648:35 - chemistry we've had classes in chemistry
648:38 - you may have needed to use a chi-squared
648:40 - goodness fit test
648:45 - so in today day Slide Attack ah let me
648:48 - start that slide again so in today's
648:51 - slide deck we covered procedures for
648:53 - testing if the observed categorical data
648:55 - came from the hypothesized distribution
648:59 - that's it
649:01 - I'm in the future we're going to look at
649:02 - the chi-square tests of Independence
649:04 - we're going to look at analysis of
649:06 - variance or you do linear regression
649:08 - those are the three remaining tests
649:12 - um all procedures take advantage of the
649:14 - sca's
649:16 - please take advantage of the scas for
649:18 - work for practice
649:20 - we only had one r function.test
649:23 - remember X and P are the numbers of
649:27 - successes and the probability of
649:30 - successes in each of the groups
649:32 - respectively
649:35 - SCA 32 is going to be very helpful here
649:38 - two for proportions which goodness fit
649:42 - test does talk about proportions and
649:45 - three more than two samples
649:50 - there's nothing in R for starters for
649:52 - this Hawks learning this is section 10
649:54 - 6.
649:56 - that brings us to the end
649:59 - um
650:01 - that's it
650:03 - um
650:04 - so bye
650:06 - hello and welcome to chapter 10 where we
650:09 - cover a lot of the hypothesis testing
650:11 - not all of it but a lot of it and here
650:13 - we're looking at categorical
650:14 - Independence section seven uh
650:16 - categorical Independence means that
650:18 - you're testing for Independence between
650:19 - two categorical variables
650:23 - um contrast this with testing
650:25 - Independence between two numeric
650:26 - variables which will be linear
650:27 - regression which will be in the future
650:29 - this lecture is about two categorical
650:32 - variables
650:33 - and that's really what we're doing still
650:35 - understanding the theory behind in
650:37 - hypotheses about determining if two
650:39 - categorical variables are independent
650:41 - and better understanding the p-value and
650:43 - how to test hypotheses
650:46 - parametric procedure is the chi-squared
650:49 - test of Independence it's called a
650:52 - chi-squared test because the test
650:53 - statistic follows a chi-squared
650:55 - distribution
650:57 - they're called t-tests because things in
651:00 - the past that we've covered are called
651:01 - t-test because their test statistic
651:03 - follows a t distribution things in the
651:05 - past have been called z-tests because
651:07 - their test statistics follow a z
651:10 - distribution
651:11 - we saw an F test at one point because
651:14 - the test statistic follows an F
651:15 - distribution here this is one of many
651:18 - chi-squared tests and they're called
651:20 - chi-square tests because the test
651:21 - statistic follows a chi-squared
651:23 - distribution
651:24 - the null hypothesis is that the two
651:27 - categorical variables are independent
651:30 - the graphics a matrix plot or a graphic
651:33 - is a matrix plot
651:35 - resign yourself to the fact that
651:37 - graphics for chi-square tests of
651:39 - Independence all look ugly and are
651:43 - difficult to interpret
651:46 - um this requires expected number of
651:48 - successes to be at least five in each
651:49 - cell
651:50 - uh or 10 in each cell or I mean the
651:53 - reason for this is It's a normal
651:55 - approximation of the binomial
651:56 - distribution
651:58 - and so larger sample sizes means that
652:00 - that normal approximation improves
652:02 - Central limit theorem
652:05 - Section 5 6 no 6 5
652:10 - . this function is indeed what hawks
652:13 - covers so you won't have to change
652:16 - anything to to your hux homework with
652:19 - this
652:19 - however for R to give you Hawks results
652:22 - you'll need to use correct equals false
652:24 - in the function and I'll show you where
652:26 - to do that
652:27 - so let's go ahead and look at a framing
652:29 - example I would like to test if there's
652:31 - a relationship between whether a person
652:33 - has blue eyes and whether that person is
652:35 - a math natural science major
652:38 - yeah to do this I asked 100 people at
652:40 - Knox College so I'm dealing with one
652:43 - large sample of 100 people and I'm
652:46 - measuring two things on each person one
652:49 - if that person has blue eyes or not blue
652:52 - eyes and two if their major is in MNS or
652:56 - not in m s
652:58 - so here's the contingency table
653:02 - notice that each of these is a count so
653:04 - in my sample seven people had blue eyes
653:07 - and were M and S majors
653:10 - 20 people 7 plus 13 20 people in my
653:13 - sample had blue eyes
653:16 - the 52 people did not have blue eyes and
653:19 - were not math Natural Science majors
653:23 - 80 people 28 plus 52 did not have blue
653:26 - eyes
653:27 - 35 people were math Natural Science
653:30 - majors
653:31 - and whatever 13 plus 52 is I think
653:34 - that's 65 were not math Natural Science
653:37 - majors
653:39 - and adding them all together gives you a
653:41 - hundred the number of people that I
653:43 - asked
653:46 - so contingency table is a table of
653:48 - counts
653:51 - so let's us let's ask ourselves this
653:54 - question
653:55 - what does it mean for two categorical
653:58 - variables to be independent
654:02 - recall back to chapter four
654:05 - that really fun chapter where it covered
654:08 - Independence at one point
654:10 - that is exactly what we're going to be
654:13 - using to create our expected counts
654:17 - so we'll have observe counts expected
654:20 - counts and when you've got those two
654:22 - it's just a hop skip and a jump to your
654:24 - chi-square test
654:27 - um so what does it mean it means that
654:28 - the value of one does not affect the
654:30 - value of the other
654:34 - particularly for this it means that the
654:36 - probability of a math natural science
654:37 - major does not depend on whether you are
654:40 - blue-eyed
654:43 - we can also frame this as the
654:46 - distribution of M and S Majors is the
654:48 - same for blue-eyed people as it is for
654:50 - non-blue-eyed people
654:52 - but you could also frame this as the
654:55 - distribution of blue-eyedness is the
654:57 - same for mathematical science Majors as
654:59 - it is for non-m S majors
655:04 - let's check with the data for this
655:07 - interpretation
655:09 - here's the counts so what's the
655:11 - proportion of blue eyed people in this
655:13 - sample
655:14 - 7 plus 13 over 100 so 20 percent
655:18 - what's the proportion of blue eyed
655:20 - people who are mathematical science
655:21 - Majors it's seven
655:23 - divided by 35. this is the number of
655:27 - blue-eyed people who are math Natural
655:28 - Science majors and this is the number of
655:31 - math Natural Science majors
655:33 - 20 it's a match so if I tell you I'm
655:36 - blue-eyed
655:39 - that does not affect my knowledge about
655:42 - you being a math natural science major
655:48 - notice we could do the same thing
655:50 - proportion of blue eyes in the sample
655:52 - versus proportion of blue-eyed people
655:54 - who are not math Natural Science Majors
655:56 - again it's 20 percent so if I tell you
655:59 - I'm blue-eyed that should give you no
656:00 - information or that does give you no
656:02 - information about whether I am
656:04 - a math natural science major because the
656:06 - probability to do the same or whether
656:08 - I'm not a math natural science major
656:10 - probability is the same
656:17 - we can do this with non-blloid people
656:20 - proportion of non-bloid people in the
656:21 - sample is
656:23 - 20 plus 28 plus 52 which is 80 divided
656:26 - by our sample size of 180 percent
656:31 - proportion of non-bloid people who are m
656:33 - s Majors is also 80
656:35 - 28 divided by 35.
656:38 - so if I tell you I'm not blue-eyed I'm
656:41 - not giving you any information about
656:42 - whether I'm a math natural science major
656:45 - X page or not
656:47 - because the proportions or the percents
656:49 - are the same
656:53 - so me giving you information about my ID
656:55 - eye color doesn't give you any
656:57 - information about my major
657:03 - in other words my eye color is
657:05 - independent of major
657:13 - now that we've seen a small example
657:14 - let's go ahead and generalize this
657:16 - here's our data
657:19 - variable one is a variable two is B
657:25 - variable one has two
657:28 - levels A1 and A2
657:32 - the B variable has two levels B1 and B2
657:37 - x 1 1 is the number of people the actual
657:40 - counts who were A1 and B1
657:44 - X12 is the actual counts of people who
657:48 - are B1 and A2
657:51 - x21 were the actual counts of people who
657:54 - were B2 and A1 and x22 is B2 and A2
657:57 - count
657:59 - I'm going to scroll back to our data
658:03 - I only have to go back to here this
658:05 - would be a this would be X11
658:08 - x 1 2 x 1 3 x 1 no I got that wrong X1 1
658:14 - x 1 2 x 2 1 x 2 2 there we go
658:18 - the variable a is eye color
658:21 - the level A1 is blue
658:24 - level A2 is not blue
658:27 - the variable B is Major
658:30 - level B1 is math the natural science
658:32 - major level B2 is not math natural
658:35 - science major
658:39 - now we're going to create column and row
658:41 - sums
658:43 - we'll number the row sums R1 and R2
658:46 - R1 will be the number of people who are
658:50 - B1
658:51 - R2 will be the number of people who are
658:54 - B2
658:55 - column sums will be the number of people
658:57 - who are A1 and C2 is a number of people
658:59 - who are A2 going back a slide
659:03 - so R1 will be 7 plus 28 there are 35 B
659:09 - ones
659:11 - R2 will be 13 plus 52
659:15 - there's 65 b2s
659:20 - C1 is 7 plus 13 so there's 20 a ones and
659:25 - there's 80 a2s the total sample size
659:28 - will be the sum of everything here
659:31 - or it'll be the sum of the row sums or
659:34 - it'll be the sum of the column sums
659:36 - n's 100.
659:39 - so here's the data that we were working
659:40 - with here's abstraction of it
659:48 - in other words we've got our counts this
659:51 - is observed now we've got to figure out
659:52 - how to get
659:53 - expected
659:58 - if the two variables are perfectly
660:00 - independent and again refer back to
660:02 - section 4 3 the expected values would be
660:06 - although their expected values to be n
660:09 - times p
660:10 - so we got the ends common for all of
660:13 - these p is the probability of being in
660:16 - this cell
660:17 - if
660:18 - the two variables are independent that
660:20 - would be the probability of being in row
660:23 - one
660:24 - times the probability of being in column
660:26 - one
660:29 - if they're independent
660:32 - here this would be the probability of
660:34 - being in row two
660:35 - times the probability being in column
660:37 - one
660:38 - row two column two Row one column two
660:41 - those would be the probabilities
660:43 - multiplied by n to get NP which is the
660:46 - expected counts
660:54 - so this will be the table of expected
660:57 - counts
660:59 - you've got the table of observed
661:01 - accounts table of expected counts and we
661:05 - know what to do with that for a
661:07 - chi-squared goodness for chi-squared
661:09 - distribution
661:11 - observed minus expected squared over
661:14 - expected
661:17 - we're going to call this X2 instead of
661:19 - TS
661:21 - chi-squared distribution as one
661:23 - parameter called the degrees of freedom
661:25 - for the goodness of fit test it was
661:28 - groups minus one for the test of
661:31 - Independence it's going to be rows minus
661:34 - 1 times columns minus one
661:41 - rows minus 1 times columns minus one in
661:45 - this example we've been working with
661:47 - there are two rows two columns the
661:49 - degrees of freedom will be one
661:51 - two minus one times two minus one
662:00 - example
662:01 - one of the actual examples we'd like to
662:03 - determine the proportion of males to
662:04 - wear hats is the same as a proportion of
662:06 - females who wear hats to test this I
662:08 - sample 100 males 100 females you know
662:10 - this is sounding familiar uh 10 males 16
662:12 - females were wearing hats oh yeah we
662:15 - have seen this comparing two proportions
662:18 - let's also look at this in terms of a
662:20 - test of Independence Independence
662:22 - between gender and hat weariness
662:28 - so doing it the long way here's the
662:31 - table of observed values
662:34 - 16 females had a hat 84 did not 10 males
662:39 - had a hat 90 did not row sums are 100
662:42 - and 100 columns sums are 26 and 174.
662:48 - if the two are independent then we would
662:51 - expect the proportions of females with
662:53 - Hats the proportion of males with Hats
662:55 - to be close to each other and close to
662:58 - 26 over 200
663:02 - here's the table of expected values
663:04 - again in all the painful Glory
663:09 - n
663:10 - times R1 Over N times C1 over n
663:16 - times r 1 over n times C2 over n
663:24 - Etc
663:26 - so these are observed counts and these
663:31 - should be expected counts
663:35 - doing the math it actually comes out
663:37 - kind of nice
663:42 - it took a lot of effort to get it to
663:44 - come out so nice
663:47 - um
663:48 - so here's the test statistic value
663:52 - observed my suspected squared over
663:55 - expected we observed 16 we expected 13
664:00 - we observed 84 we expected 87 observe 10
664:04 - observed 90. we had a chi-squared test
664:08 - statistic of 1.5915
664:12 - we need to compare this test statistic
664:14 - to the chi-square distribution with one
664:17 - degree of freedom
664:19 - R is 2 C is 2.
664:23 - here's how we'd actually calculate that
664:28 - so the p-value is one minus because it's
664:30 - greater than or equal to
664:32 - 1.5915 degrees of freedom is 1.
664:35 - gives us a p-value of 0.2071
664:40 - after all this fund calculation because
664:43 - remember we also had to determine the
664:44 - expected values here
664:47 - or we can use r
664:50 - if we add correct equals
664:52 - false then we'll get the Hawks result
664:56 - if we leave the correct equals false out
664:58 - we'll get a better result
665:00 - the first step is to create the Matrix
665:03 - of observed values
665:06 - The Matrix of observed values
665:09 - here's how we do that we use the
665:11 - function Matrix the first thing we give
665:14 - the Matrix function is the counts and
665:17 - then we specify how many columns
665:21 - here notice that the counts are going in
665:24 - by row
665:25 - let me go back
665:27 - 1610 84.90
665:31 - 16 10 84.90 so this line will actually
665:35 - give you the observed Matrix going back
665:38 - this thing right here
665:40 - as a matrix
665:43 - and then all we have to do is a chi got
665:45 - tests of that observed Matrix
665:49 - if you're doing it for Hawks we do comma
665:51 - correct equals false
665:57 - this first line that is the most
665:59 - difficult
666:01 - but it's just the Matrix function
666:04 - this the the values by row and then you
666:07 - specify the number of columns
666:12 - if we do the r we leave off the correct
666:15 - equals false
666:17 - here's the output for r
666:21 - got a p-value 0.2931
666:24 - degrees of freedom of one
666:26 - The observed value of 1.1052
666:31 - because the p-value is greater than
666:32 - Alpha we fail to reject the null
666:34 - hypothesis we do not have evidence that
666:37 - the hat wearing rate for males differs
666:39 - from the hat wearing rate for females
666:42 - similarly we could say we have no
666:45 - evidence that the gender ratio for hat
666:46 - wearers differs from that for non-hat
666:49 - wearers
666:51 - both are actually equivalent
666:54 - interpretations
666:58 - equivalent interpretations
667:00 - if one is true the other is true
667:05 - notice the difference between the two
667:07 - tests is minor when the sample size is
667:09 - large
667:10 - but we have 100 or so sample size pretty
667:13 - large
667:14 - so this is with the continuity
667:17 - correction if we did it with correct
667:18 - equals false we have it without
667:26 - example three
667:30 - I'd like to determine if females have a
667:32 - different grade distribution my stat
667:33 - tone of course is than males
667:35 - this is data from I forget what uh
667:39 - I think my first three years here
667:42 - so gender this is A's b c's D's and F in
667:46 - the in the class
667:50 - yes it is fake data don't worry
667:55 - um so if all my past students 57 were
667:58 - females who got A's 68 were males who
668:01 - got B's 40 were females who got these
668:04 - and 22 were females who got F's so I
668:07 - would like to determine if the grade
668:09 - distribution for females is this is
668:11 - different than that for males
668:14 - in other words
668:17 - what I want to determine is
668:19 - is the grade distribution dependent on
668:22 - gender
668:26 - is the grade this grades a categorical
668:30 - variable independent of gender
668:32 - categorical variable
668:39 - there are a lot of ways of interpreting
668:41 - this and they're all logically
668:43 - equivalent
668:46 - so to do this we just got to put get
668:49 - these numbers into a matrix 57 49 7868
668:53 - Etc and do a test on it it will
668:57 - automatically tell us there are four
668:58 - degrees of freedom
669:00 - how did I get four degrees freedom
669:03 - two rows five columns R minus one times
669:07 - C minus 1 gives us four
669:12 - putting the Matrix in
669:14 - specify the number of columns is five we
669:17 - did it by column
669:18 - this is the Hawks result because I
669:20 - specified correct equals false
669:23 - p-value is 0.9851 because the p-value is
669:27 - greater than our usual Alpha of 0.05 we
669:30 - cannot reject the null hypothesis there
669:33 - is no evidence that the grade
669:34 - distribution differs between females and
669:36 - males
669:38 - similarly I could say hey I have this
669:41 - student the student got AC
669:44 - well guess what I didn't give you any
669:47 - information about the student's gender
669:51 - because information about the grade is
669:55 - independent of information about the
669:56 - gender
670:01 - so let's go to the intra lecture
670:03 - questions
670:06 - this looks familiar when is the research
670:09 - hypothesis the same as the alternative
670:11 - hypothesis
670:13 - should have this by now question two I
670:17 - want you to give an example where you
670:19 - would need to test independence of two
670:20 - categorical variables in your area of
670:23 - Interest
670:25 - two categorical variables
670:30 - and this is
670:32 - for our lecture today
670:34 - you would be able to use the chi-square
670:36 - test for Independence for this example
670:39 - question three is Give an example where
670:43 - would you need to use tests for
670:45 - independence of two numeric variables in
670:47 - your area of interest you do not know
670:50 - how to do this test yet
670:52 - this is not from today today was just
670:55 - too categorical independents
670:58 - in the future we'll learn how to do two
671:00 - numeric Independence
671:03 - but I want you to start thinking about
671:05 - okay where would I need to test for
671:07 - independence of two numeric variables
671:13 - so we learned how to test if two
671:16 - categorical variables are independent
671:18 - today
671:19 - future we got Anova and linear
671:20 - regression
671:23 - kaisk test
671:25 - performs a chi-square test of
671:27 - Independence m is a matrix
671:30 - the sca is number 41
671:36 - there are no readings in our starters
671:38 - Hawks has section 10 7.
671:43 - and the usual don't forget the all
671:45 - procedures make sure you have a page for
671:48 - each of these
671:50 - and that's it
671:53 - hello and welcome to the analysis of
671:55 - variance this is the most important part
671:58 - of chapter 11. analysis the variance is
672:00 - used to test for independence of a
672:03 - categorical and a numeric variable
672:06 - that's one use of Anova the usual use of
672:10 - Anova is testing for equality of
672:12 - population means amongst more than two
672:14 - groups
672:16 - so by the end of this lecture you should
672:18 - be able to understand the theory behind
672:19 - testing the means of more than two
672:21 - populations
672:22 - and the independence between America and
672:24 - a categorical variable
672:26 - and again better understand p-values and
672:28 - how to test hypotheses so let's do a
672:31 - frame example here I like to test if the
672:33 - average GPA
672:35 - of a student is the same for the four
672:37 - types of majors at Knox m s is math and
672:39 - natural science HSS is Humanities and
672:43 - social sciences hum
672:46 - is Humanities
672:48 - because it's history and social sciences
672:50 - and art is all the art stuff
672:52 - so to do this I asked 200 students at
672:55 - Knox College 50 of each major type and
672:57 - asked two questions one what is your
672:59 - major type two what is your GPA so note
673:03 - that we have one population
673:05 - students at Knox College
673:08 - and on every member of that population I
673:11 - asked two questions one what is your
673:14 - major type and two what is your GPA
673:17 - if we're looking at a relationship
673:18 - between the two we're actually testing
673:20 - for Independence between those two
673:22 - variables
673:24 - major type is categorical and the GPA is
673:27 - numeric
673:30 - so here's a box plot side by side box
673:33 - plot of the data I collected
673:36 - each dot represents a student that I
673:38 - talk to
673:40 - Green Dots or MNS majors
673:43 - horizontally corresponds to the reported
673:46 - GPA
673:48 - the dots are included as is or as are
673:52 - the box plots for each of the four major
673:55 - types
674:00 - so there's actually a few equivalent
674:03 - ways of looking at this question and as
674:05 - you've learned from your math courses
674:06 - throughout the years the ability to look
674:09 - at a single question from multiple
674:10 - standpoints is always a strength and
674:14 - that strength leads leads us to be able
674:17 - to in this case determine what the
674:20 - actual test should be
674:22 - so one way is do the means in each group
674:24 - significantly differ
674:25 - in other words
674:27 - is the mean for math and natural science
674:30 - which is probably located somewhere
674:32 - around here is that significantly
674:34 - different than the mean for history and
674:37 - social sciences which is probably down
674:39 - here somewhere
674:40 - and for humanities which looks like it's
674:42 - here somewhere and art which looks like
674:45 - it's here somewhere
674:47 - so the first way of looking at this is
674:49 - comparing the means of the individual
674:51 - groups
674:54 - second way of looking this is are the
674:56 - group and the GPA independent
674:59 - these two questions are logically
675:02 - identical are the group and GPA
675:05 - independent
675:06 - and the Third Way is does including the
675:08 - group identifier improve our ability to
675:10 - estimate a person's GPA
675:14 - and since we're looking at the mean the
675:16 - expected value what we're looking at is
675:18 - actually testing okay does the
675:21 - information of group improve our
675:24 - understanding of GPA
675:27 - thinking back to chapter four that's
675:29 - equivalent to saying our group and GPA
675:32 - dependent or independent
675:34 - think back to the category the
675:38 - conditional probability definition of
675:40 - Independence
675:44 - and it's this last one that gives us
675:45 - some insight into the test statistic
675:48 - what improving predictions implies is
675:51 - that we reduce the uncertainty in those
675:53 - predictions
675:54 - and reducing uncertainty means we reduce
675:57 - the variance
675:58 - and those predictions
676:01 - and this is the idea behind the analysis
676:04 - of variance procedure
676:07 - first thing you do is measure the
676:08 - variance of the original data
676:10 - to measure the variance that is left
676:13 - over after you include the model and by
676:16 - model I mean the group identifier
676:20 - and then you look at the ratio between
676:22 - the two from the explained to the
676:23 - Unexplained and it's this last ratio
676:26 - that's actually the test statistic now
676:29 - let's think about that for a second
676:32 - if the explained variance the variance
676:36 - is explained by the model are contained
676:38 - in the model or taken care of by the
676:40 - model is large
676:43 - compared to what's left over the
676:45 - Unexplained variance then the model is
676:47 - good
676:49 - because the remaining variance is small
676:52 - compared to what you started with
676:54 - the model is explaining a lot of the
676:57 - uncertainty in the dependent variable
677:01 - if on the other hand
677:02 - the explained variance is small in other
677:06 - words if the model doesn't explain much
677:08 - of that dependent variable then the
677:10 - model is is virtually worthless
677:13 - and it's that ratio which is an F ratio
677:17 - it's called an F ratio because the test
677:19 - statistic follows an F distribution
677:22 - it's this F ratio that leads us to a
677:25 - p-value which leads us to an
677:27 - interpretation of those results
677:30 - so that was the theory let's go through
677:32 - the calculations by hand
677:35 - probably the only I think we get one
677:37 - more time when we do it by hand but so
677:40 - with that background let's calculate the
677:42 - test statistic and the p-value using the
677:44 - Anova table and here's the blank Anova
677:47 - table
677:47 - this is a this is what an anova table
677:50 - looks like
677:52 - there's three sources the model what's
677:55 - remaining after you apply the model and
677:58 - then what's originally there
678:01 - so model is the model that you apply
678:03 - which is the
678:05 - independent variable of the grouping
678:07 - variable error is what remains after
678:10 - applying the model and the total is what
678:13 - you start with
678:16 - SS is sum of squares so this box will
678:20 - contain the sum of squares for the model
678:23 - this box will be the sum of squares that
678:26 - remains
678:27 - and this box will be the total sum of
678:29 - squares or the original sum of squares
678:31 - note that this box plus this box will
678:34 - give you this box
678:37 - this will be the degrees of freedom for
678:38 - the model degrees of freedom for the
678:40 - error and then the total degrees of
678:42 - freedom and the total degrees of freedom
678:43 - will be just the usual the total sample
678:46 - size minus one
678:48 - and again note that this box and this
678:50 - box will add to this box
678:53 - Ms stands for mean squared
678:57 - so this would be the mean squared error
678:59 - for the model
679:00 - sorry this will be the mean squared for
679:02 - the model this will be mean squared for
679:04 - the error
679:06 - the mean square is just the ratio of the
679:07 - sum of squares to the degrees of freedom
679:11 - I mean we saw that back in chapter two
679:13 - or chapter three when we were looking at
679:15 - the variance calculations it was the sum
679:17 - of the squares divided by the degrees of
679:20 - freedom we just didn't use term degrees
679:22 - of freedom back then it was n minus 1.
679:27 - this F ratio is going to be this box
679:29 - divided by this box
679:31 - and this p-value will be a function of
679:33 - this F ratio according to the F
679:35 - distribution
679:37 - so let's fill it in and you get to see
679:39 - all the calculations
679:42 - the column marked SS contains the sum of
679:45 - squares for those three sources the sum
679:48 - of squares is just the sum of the
679:49 - deviation between the observation and
679:51 - the mean
679:53 - notice the structure of all of these is
679:55 - the same you're adding up over all the
679:57 - data values that's what the double
679:58 - summation is
680:00 - you're adding up within the group to the
680:03 - grand mean so this is what's explained
680:05 - by the model the the mean within the
680:09 - group
680:10 - difference with the grand mean or the
680:13 - mean of all the data values
680:16 - what's remaining is the variation within
680:18 - the group the data value to the group
680:20 - mean
680:23 - and this is the original this is the
680:25 - data values to the group mean
680:28 - back in the day this was x minus X bar
680:31 - squared add it up
680:35 - here we're just renaming x and x bar
680:40 - so again this is what's explained by the
680:42 - model
680:43 - this is the group mean minus the grand
680:46 - mean
680:48 - this is the what's left unexplained the
680:52 - variance within each of the groups
680:54 - the data value to that group mean
680:58 - and this is what you started with
681:04 - because each of these calculations
681:06 - require 50 sums differences in squares
681:09 - calculating by hand it's not really
681:10 - realistic so I'm just going to give you
681:13 - the answers for this data notice I
681:15 - didn't actually give you the data itself
681:18 - here's a sum of squares here's what's
681:20 - here's the error that's remaining here's
681:22 - the total
681:23 - notice the sum of squares for the model
681:25 - plus the sum of squares error is going
681:27 - to add up to the total
681:31 - the important reason for that is that
681:34 - means that the SS model
681:36 - or the
681:37 - the sum of squares for the model and the
681:40 - sum of squares for the error are
681:41 - independent of each other
681:42 - doesn't mean much for us in stat 200 but
681:45 - it will be important in later set
681:47 - courses
681:51 - call Mark DF contains the degrees of
681:54 - freedom for the three sources
681:56 - there are the parameters that reflect
681:58 - the amount of information contributed by
682:00 - each Source probably believe the best
682:02 - definition of degrees of freedom
682:06 - the degrees of freedom tool is just the
682:08 - total sample size minus one
682:12 - so it'd be 200 minus one because I asked
682:14 - 200 people
682:16 - the F model is the number of groups
682:18 - minus one there's four groups four
682:21 - different major types so DF model will
682:23 - be three
682:24 - and then the way I calculated is just DF
682:26 - total minus DF model
682:30 - that'll give you the if error
682:35 - 3 and 199 and the DF error will be 196.
682:41 - the mean squared will be the sum of
682:43 - squares divided by the degrees of
682:45 - freedom
682:47 - now this should look should refresh or
682:50 - should
682:52 - look very similar to what we got back in
682:54 - chapter two with the variance sample
682:57 - variance it was 1 over n minus 1 times
682:59 - the sum of x minus X bar
683:02 - squared
683:04 - calculating the same thing here this is
683:07 - an estimator of a variance just like s
683:10 - squared was an estimator of a variance
683:15 - so mean squared model is 2.6104 mean
683:18 - squared error is 0.4718
683:22 - and this F statistic is going to be the
683:24 - mean squared model divided by the mean
683:25 - squared error
683:31 - as you can as you should guess by now
683:33 - the distribution of the F statistic IS F
683:36 - that's the name of a distribution do you
683:39 - know what the name that distribution is
683:40 - named after
683:41 - Fisher John Fisher one of The Luminaries
683:45 - of early statistics in the 20th century
683:48 - there is f it's 5.533
683:52 - last box we'll calculate the p-value
683:55 - p-value is calculated in exactly the
683:57 - same way
683:59 - it's a probability of being of the test
684:01 - statistic
684:02 - being this extreme or more so
684:06 - given the null hypothesis is true
684:09 - if the null hypothesis is true F will be
684:11 - zero I mean if the null hypothesis is
684:14 - perfectly true this F ratio will be zero
684:18 - because there will be no uh this number
684:21 - will be zero
684:23 - this number would be zero
684:27 - remember the null hypothesis is there is
684:30 - no difference in the means
684:32 - or the null hypothesis is that the two
684:36 - variables are independent
684:39 - or
684:41 - the null hypothesis is the model
684:44 - explains none of the variation
684:47 - the model explains another variation
684:49 - this is going to be zero
684:52 - which means mean squared is going to be
684:53 - zero
684:55 - which means the F ratio will be zero
684:57 - over something which is zero
685:00 - so this is the probability of the
685:03 - distribution being greater than or equal
685:04 - to 5.53
685:10 - which looks like this
685:14 - here's the F distribution for this
685:16 - particular problem
685:18 - p-value is way out here it's this area
685:21 - that's hard to see
685:25 - 0.00115
685:27 - how do we interpret the p-value
685:32 - absolutely correct same as always
685:34 - compare it to Alpha
685:36 - if the p-value is less than Alpha as in
685:38 - this case we reject the null hypothesis
685:42 - that means
685:44 - all of the following three things
685:48 - the two variables are not independent
685:52 - and that is GP average that is GPA and
685:56 - um
685:57 - GPA and major type
685:59 - those are not independent because p is
686:02 - too small it means the average GPA in
686:06 - the four groups is not the same
686:10 - and it means if you want to model the
686:12 - GPA if you want to explain the GPA then
686:16 - including the major type will help with
686:19 - that
686:25 - okay
686:26 - let's look at the rice yields I'll give
686:28 - you the data
686:30 - so we can see how all the calculations
686:32 - are done
686:34 - does rice variety influence the average
686:37 - yield amongst these four varieties
686:40 - here's the yield in each of the four
686:42 - plots for each of the four varieties so
686:44 - we got 16 plots
686:46 - in each plot we planted one of the four
686:49 - varieties of rice
686:52 - so there's the raw data
686:58 - here's a graph
687:04 - sure seems like variety D is much higher
687:06 - than the rest I don't know about variety
687:09 - a
687:11 - but it certainly seems as though at
687:14 - least one of the four varieties has a
687:17 - different average
687:22 - here's the blank Anova table let's
687:24 - calculate this all together
687:26 - just so we can see oh wow we really
687:28 - don't want to do this by hand
687:33 - so here are the formulas for SS model SS
687:35 - error and SS total
687:37 - from here
687:41 - so here's SS model we're adding up over
687:43 - all the data values
687:45 - the average in the group minus the grand
687:47 - mean
687:49 - squared
687:51 - but this actually translates to is this
687:54 - because there's 12 data points so we've
687:56 - got 12 terms
688:01 - from the data this is what we get the
688:03 - grand mean is
688:05 - 9981.9375 and the individual group means
688:09 - or variety means are those
688:11 - now we just plug and chug this term
688:13 - becomes 984.50 minus
688:18 - 991.9375
688:19 - squared plus
688:22 - Etc
688:25 - substitution
688:30 - do a lot of calculations and we get 89
688:32 - 931.
688:34 - so the sum of squared for the model is
688:36 - 89 931.
688:42 - we can do the same thing with the error
688:43 - and total
688:46 - we could just calculate the total which
688:48 - is the variance of the data and subtract
688:52 - off the model to get the error
688:58 - degrees of freedom we've got 16 data
689:01 - points so DF total will be 16 minus 1.
689:04 - we've got four groups so DF model will
689:07 - be four minus one
689:09 - and then DF error will be 15 minus three
689:18 - mean squared is just the sum of squares
689:20 - divided by the degrees of freedom
689:29 - that 2 should not be there
689:35 - the F ratio is just the ratio of the
689:37 - mean squared of the model to the mean
689:38 - square root of the error
689:40 - so this would be about 30 divided by 4
689:43 - should be about seven and a half
689:48 - this is the F ratio
689:50 - and it follows the F distribution
689:55 - that's why it's called an F statistic it
689:57 - follows the F distribution
690:03 - and now this p-value is a probability of
690:05 - this distribution being greater than or
690:07 - equal to 7.212
690:12 - . there are 7.212 this shaded area is
690:15 - the p-value
690:18 - 0.00503
690:22 - interpret the P bet value in the usual
690:24 - way
690:25 - compare it to Alpha if the p-value is
690:28 - less than Alpha reject the null
690:29 - hypothesis
690:30 - in this case the null hypothesis is that
690:33 - the four means are equal
690:36 - or
690:38 - that the yield variable and the variety
690:42 - variable are independent
690:44 - or if we're trying to model the yield of
690:50 - wheat was it wheat or corn if we're
690:52 - trying to model the yield of whatever
690:54 - grain this is
690:56 - including the including the variety will
690:59 - help
691:02 - those three are equivalent
691:08 - the one you use depends on what you're
691:10 - trying to understand about rice I guess
691:13 - it's rice instead of corn or wheat
691:20 - now I'll do it in r
691:22 - three lines to get the data in
691:25 - it's the rice data set summary just to
691:28 - make sure that you loaded it in
691:29 - correctly and attach it
691:33 - that's what the box plot looks like
691:35 - after putting it up a little
691:38 - here's the code for the fancy box plot
691:41 - box plot line is actually way down here
691:45 - this will just get you a nice plain box
691:48 - plot
691:51 - here's the two lines of code to do all
691:53 - of the Anova calculations that you need
691:55 - to do
691:58 - we'll call it rice mod it's the model
692:00 - about rice the function is aov analysis
692:04 - of variance
692:06 - in parentheses this is the dependent
692:08 - variable
692:09 - that's the tilde to the left of the one
692:12 - and this is the independent variable
692:15 - the dependent variable here is the
692:17 - numeric variable and the independent
692:19 - variable is the grouping variable or the
692:21 - categorical variable
692:23 - this rice mod line doesn't do much of
692:25 - anything except everything
692:29 - inside
692:30 - it's the summary that takes it from
692:33 - what's inside R to actually put it in a
692:35 - form that is Meaningful for you
692:39 - notice it doesn't give the total stuff
692:41 - but we can figure out degrees of freedom
692:43 - total is 15.
692:46 - the sum of squares total is just the sum
692:48 - of those two
692:50 - there are four varieties so the degrees
692:52 - of freedom for variety is four
692:55 - there's a sum of squares the mean
692:56 - squared is the sum of squares over the
692:58 - degrees of freedom
692:59 - the F value is the ratio of the mean
693:01 - squared variety to the mean squared
693:03 - residuals
693:05 - which is often called error instead of
693:08 - residuals here's our p-value
693:11 - notice we've got two stars on it two
693:13 - stars is down here so that p-value is
693:15 - between .001 and .01
693:20 - between U because we got the p-value
693:22 - there
693:24 - and that's it just those two lines once
693:26 - you get the data in just those two lines
693:28 - interpret
693:32 - same conclusion
693:36 - Ronald Fisher introduced the Anova
693:39 - procedure in his 1925 book
693:44 - and to illustrate Anova he came up with
693:48 - this experiment
693:49 - collect a sample of pond water okay so
693:53 - you take your five gallon bucket and dip
693:55 - it in the pond and you've got a sample
693:57 - of pond water
693:59 - now divide that water in that same
694:02 - bucket amongst four different beakers
694:07 - now separate the beakers to ensure that
694:09 - there's no cross contamination and from
694:11 - each Beaker take four samples and
694:15 - counter record the number of amoeba
694:17 - present
694:21 - so you got this sample of pond water
694:24 - dip four beakers in that same sample so
694:27 - you would expect the amoeba
694:29 - concentration be the same in those four
694:31 - beakers
694:33 - and now from each speaker take four
694:36 - small samples and count and record the
694:38 - number of amoeba so you'd expect the
694:40 - averages in each of those four samples
694:42 - from each of the four beakers to be
694:44 - about the same
694:48 - so we would expect the p-value of this
694:50 - experiment to be rather large
694:54 - data is available in the Fischer 38 data
694:56 - file
694:58 - this loads it attaches it this does the
695:01 - analysis of variance and the summary
695:03 - gives us these results
695:06 - we got a p-value that's rather large
695:09 - notice that when the F value is small
695:11 - the p-value is large and when the F
695:13 - value is large the p-value is small
695:16 - it's because the larger the F value the
695:19 - more extreme your observations are if
695:22 - the null hypothesis is true
695:26 - and the null hypothesis is either all
695:30 - the means are equal
695:32 - or
695:33 - the the two variables the numeric
695:36 - variable and the grouping variable are
695:38 - independent
695:39 - or that grouping variable gives us no
695:42 - information about the numeric variable
695:49 - there's that F distribution there's the
695:52 - p-value in Darker blue
695:55 - it's not small
695:57 - we didn't expect it to be small
696:01 - there's the conclusion
696:04 - and at this point we should be asking do
696:07 - these results make sense for every
696:10 - single analysis
696:13 - the results either make sense or
696:17 - we are learning something new or we did
696:20 - something wrong
696:25 - so here's the summary slides but I'm
696:27 - going to throw in the intro lecture
696:29 - questions here
696:32 - so the first introduction question
696:35 - does a large value of f correspond to a
696:39 - large p-value or to a small p-value
696:43 - does a large value of f correspond to a
696:46 - large p-value or a small p-value
696:51 - and again I would write the question
696:53 - your notes on the left answer below
696:56 - question two what is the null hypothesis
697:00 - in Anova
697:01 - notice I've given you three options for
697:03 - this give me any of those three
697:10 - and three here's a given example where
697:12 - you would need to test for the
697:13 - independence of one numeric variable and
697:15 - one categorical variable
697:21 - any example will work any good example
697:23 - will work
697:30 - so here's what we did in today's slide
697:32 - deck we covered Anova
697:34 - this procedure helps us determine three
697:37 - things
697:38 - if the mean of several groups are the
697:40 - same
697:42 - if a numeric and a categorical variable
697:44 - are independent
697:46 - and if knowing a group membership helps
697:48 - with estimation of the dependent
697:49 - variable
697:50 - all three of those are logically and
697:53 - statistically equivalent
697:56 - the way that you frame your results will
697:59 - depend on what the original research
698:00 - hypothesis and or research question are
698:04 - if the researcher is asking about the
698:06 - means of separate groups then that's the
698:08 - one you're going to use to interpret
698:10 - your Anova
698:15 - in the future we're going to do linear
698:16 - regression
698:18 - same reminders as always create that
698:21 - section in the notebook dedicated to the
698:23 - tests and the assumptions of that test
698:24 - take advantage of the scas and use the
698:27 - all procedures handout
698:30 - um
698:33 - there's some functions that we are
698:36 - hinting at the aov function we did in
698:39 - the summary function we did you've
698:40 - already seen the Shapiro
698:43 - test function
698:45 - the flickner test function is what we're
698:47 - going to use in the next set of slides
698:50 - notice that all we were able to conclude
698:52 - was the mains are all the same or at
698:56 - least one mean is different
698:58 - we weren't able to determine which mean
699:00 - was different
699:02 - next set of the next set of slides will
699:05 - cover that
699:06 - we looked at we could also conclude that
699:09 - the two variables are independent or
699:11 - they are dependent we weren't able to
699:14 - classify what type of dependents
699:17 - the next set of slides will help with
699:19 - that
699:20 - however the fligner test will need to be
699:23 - used to test one of the assumptions of
699:25 - Anova
699:26 - but that will be for next time
699:30 - and there are this the readings
699:33 - and that's the end and I hope this was
699:35 - fun
699:36 - or at least helpful

Cleaned transcript:

hello and welcome to this video section 1.1 getting started in this section we're going to learn the vocabulary of Statistics the basic vocabulary at least we're going to distinguish between a population and a sample and we're going to distinguish between the two types of populations the target population and the sampled population so let's begin the goal of the branch of mathematics and these are the words from Hawks the goal of the branched mathematics called statistics is to provide information so that informed decisions can be made I disagree 100 with this statement statistics is not a branch of mathematics for sure we use numbers we use formulas but then so does chemistry and physics and they're not mathematics statistics is a science it proceeds forward based on inductive reasoning tries to learn about the larger whole based on your smaller sample mathematics on the other hand draws from the larger to bring down to the narrower deductive reasoning so statistics is actually a branch of science the goal of this text is to enable you to filter through the statistics you encounter so that you may be better prepared for the decisions that you make in daily life as befitting most things in statistics the word itself has a couple definitions first definition is that statistics is the science of gathering describing and analyzing data so it's the science of gathering describing analyzing data Hawks admits that it's a science we could also use the term statistics as the actual numerical descriptions of sample data in other words statistics are a function of your data a mathematical function of your data the target population is a particular group of Interest a sampled population is a group from which the sample is taken hopefully those two populations are the same they don't have to be if I would like to draw a conclusion about the population of Galesburg that's my target population if I contact People based on the numbers in the phone book the people in the phone book are the sampled population Target population and sampled population in that case are not the same hopefully however the sampled population is representative of the target population a sampling frame is a physical list of all members of the sampled population this may or may not actually exist in the example I just gave the phone book will be the sampling frame a sample is a subset of the population from which data are collected a subset of the target population no not from the target population from the sampled population from which the data are collected it's this sample that we're actually going to apply our statistics to so that we can learn about the target population now hopefully our sample is representative of the target population if the sample is not representative of the target population all the work that we're going to do is worth nothing the sample must be representative and we're going to see some ways of obtaining what are called quote representative samples in the future a census is a study from which the data are obtained from every member of the population every member of the population the United States is required to hold a census every 10 years that is not this type of census that we're talking about because the U.S Census Bureau is not able to contact every single resident of the United States it tries it does a good job of it but it never actually achieves their goal of a genuine census so now let's turn to an intralecture question these questions are you'll have to answer these questions in Moodle for section 11 is just to check that you are moving forward through these at a good pace so the first introelection question for this section what is the difference between a population and a sample what is the difference between a population in a sample remember you can pause this you've got that little double bar thing that you can push on pause it so you can do the answers in the Moodle right now I recommend that you actually write the question and the answer in your notes on the left hand side but regardless you can pause and I can go back to the lecture a variable is a value like four or a characteristic like blue that changes among members of the target population it's got to change among members of that population because if it is constant among all members that is if all members of the population have the same value such as blue there's really nothing to study it's just blue data are the counts measurements or observations gathered about a specific variable in a population in order to study it so data is what you actually mark down from your sample do not confuse a parameter with a sample statistic a parameter is a numerical description of a population characteristic whereas a sample statistic is a description of a sample characteristic so parameters are about the population statistics are about the sample P for population P for parameter s for sample s for statistic we want our sample statistics to be good estimates of the population parameters and that goal will actually dictate some of the formulas that we'll see in the future here's our second question what is the difference between a parameter and a statistic remember these go into your Moodle quiz but again I recommend on the left hand side of your notes write out the question and the answer and then later put it into Moodle again remember you can pause if I go too fast it is essential that you are mindful of the relationship between a population and a sample the next figure is the picture to help you visualize this relationship so that big blue oval is the target population that is the group we want to draw conclusions about the yellow oval is the sampled population notice the sampled population does not have to be a subset of the target population it hopefully is in fact it hopefully is the target population itself but it doesn't have to be but note that the sample does have to be a subset of the sampled population sample doesn't have to be a subset of the target population again we hope it does in fact we hope that the sample is representative of the target population but from the way that the data are collected it's not required strongly suggested hoped for that's the goal but it doesn't have to be so let's summarize the differences between a population and a sample population is about the whole group it's a group we want to know about characteristics of a population called parameters these parameters in reality are unknown we're trying to estimate those parameters those parameters are fixed they are about the entire population contrast that with a sample where the sample is part of the group is a group we know everything about because we've got it right in front of us we can measure anything we want in that group there is no Mysteries whatsoever characteristics of the sample are called statistics those statistics are always known because we can just measure it on this sample that we have in front of us and the statistics change with the sample a little bit ahead of the place on that example one one identifying population and Sample so please identify the population and the sample I will read through it both A and B I will expect you to hit pause and then I'll give you the answers I've got to identify the population and the sample so in this survey 359 college students at the University of Jackson were asked if they had tried the October flavor of the month at the campus coffee shop 83 of the student surveyed said yes I'm going to now give the answer to that go ahead and hit pause the population is going to be the University of Jackson students or the college students at the University of Jackson because that's the group we're trying to draw a conclusion about the sample is going to be those 359 college students we actually contacted a survey b a survey of 1125 households in the United States found that 24 subscribed to satellite radio identify the population and the sample hit pause the population is going to be all households in the United States all and the sample is going to be that 1125 that we actually contacted those are the solutions move on to example 1.2 identifying the population the sample the parameters the statistics Etc for each of the following reports identify the population the target to population each of these cases the sample and whether or not the Highlight value is a parameter or a statistic so population sample and whether it's a parameter or statistics statistic so here we go numero one after an airline security scare on Christmas Day 2009 the Gap organization interviewed 542 American Air Travelers about the increased security measures at airports the reports say that 78 percent that must be the highlighted part of American Air Travelers are in favor of the United States airports using full body Scan Imaging on airline passengers population the target population the sample and what does that pink purple whatever color it is highlighted number actually indicate go ahead and hit pause the population here is all Americans because we are try all American Air Travelers if you wish because we're trying to draw conclusions about American Air Travelers in the United States two what is the sample it's those 542 people American Air Travelers that the Gallup organization contacted and this 78 percent is going to be the statistic because it was measured off of that sample two Rasmussen reports also conducted a survey in response to the airport security scare on Christmas Day 2009 the national telephone survey of 1 000 adult Americans found that 59 of Americans surveyed favor racial profiling as a means of determining which passengers to search at airport security checkpoints remember Target population sample and what does that 59 represent go ahead and hit pause the population is going to be all Americans because we're trying to draw conclusions about the Americans two the sample is going to be the house 1 000 adult Americans contacted and at 59 again is going to be a sample statistic it's 59 of those 1 000 people two branches of statistics the branch of Statistics called descriptive statistics is the science that gathers sorts summarizes displays the data doesn't try to draw conclusions about the data it just tries to better understand the data itself and the first four five six seven chapters of this book are going to cover descriptive statistics inferential statistics as the science evolves using these descriptive statistics to estimate population parameters and that'll be the last half of the course so inferential statistics takes our sample and tries to draw conclusions about the population descriptive statistics on their hand just takes our sample and learns about our sample which brings us to our next question what is the difference between descriptive and inferential statistics remember you can hit pause there are two types of analyzes one is called exploratory and one is called confirmatory exploratory analysis uses data to estimate parameters this will be akin to the confidence intervals in the second half of the course confirmatory analysis uses statistics to test stated claims about reality and these stage claims about reality we're going to call hypotheses and this will relate to the hypothesis testing or the pvalues of the second half of the course so example 1.3 identifying descriptive and inferential statistics in a news report on the state of the media by Tom rosentile and Amy Mitchell and they write the following AOL had 900 journalists 500 of them at its local Patch News operation by the end of 2011 Bloomberg expects to have 150 journalists and analysts for its new Washington operation Bloomberg government so let's identify the descriptive and the inferential statistics used in this excerpt again remember hit the pause to answer that and then just go ahead and listen we talk and talk and talk and talk descriptive so the first descriptive is AOL had 900 journalists so we're describing how many journalists AOL had and we describing 500 of them at its local news Operation so those are descriptive the future by the end of 2011 Bloomberg expects to have so expects to have tells us this is a feature event it's going to expect to have 150 journalists so that will be the inferential statistic so descriptive of that data that we have inferential is about the population which also translates into things that are happening in the future oops we've got that solution and that's the end of the slideshow and that's the end of section one one so expect these videos to look very similar to this that's slideshow me talking over it giving you questions me adding to the slideshows maybe telling a joke or two didn't tell any jokes today I'm sorry but expect them in the future so I hope this was helpful if not as always please make sure you email me with any questions or leave the questions in moodle's discussion forum for questions for chapter one for learning module one that's it thank you very much I appreciate it hello and welcome to section 1.2 data classifications in this section we're going to be looking at how to classify data that is how to determine what types of variable it is it can be either classified as qualitative or quantitative qualitative it's also called categorical quantitative is called numeric it can be classified as discrete continuous or neither I guess the book loves to talk about the neither we're going to look at discrete or continuous and it also adds nominal ordinal interval or ratio so so by the end of this slideshow you should be able to know what all of those terms mean and you should be able to classify a variable or data as in each of those three ways so let's begin qualitative data also known as categorical data consists of labels or descriptions of traits so qualitative variable example would be eye color hair color um favorite music things like that where you can if you want put a number to it but it's not a meaningful number say my eye color is one doesn't tell you what my eye color is say my eye color is blue does tell you those are qualitative those are categorical data quantitative on the other hand also known as numeric actually does have a number that does make fundamental sense to it usually counts measurements I object to the book using the term measurement because counts are also measurements in their own way examples of quantitative data would be height weight age gas mileage we're talking about cars now those would be quantitative because there are numbers associated with them and those numbers are inherently meaningful so here's a graphic we got the qualitative side we've got the quantitative side again qualitative is categorical quantitative is numeric qualitative data or descriptions and labels whereas quantitative tends to be counts and measurements or measurements in general yeah so here's an example classifying data is qualitative or quantitative classify the following data as either qualitative or quantitative what are you assuming about the described variables so a shades of red paint in a home improvement store is that qualitative or quantitative hit pause that is qualitative probably it depends on how you're actually measuring the shades of red paint if you're doing in terms of the names such as fire engine red apple red baby red I don't know what color is red paint are how does it just see red then those would be qualitative but if you're doing in terms of the percent of the paint that is red that is or how many squirts of red dye you have to put in the paint to get that then it would be quantitative because it's the number of squirts of red in the paint interesting so the Ampro answered depends on what you're trying to actually measure hmm B rankings are the most popular pink colors for the season again think about whether this is qualitative or quantitative and what you're assuming and go ahead and hit pause the rankings are counts there's numbers one through however many paint colors there are so this is going to be quantitative I don't think that there's much you're assuming about it unless you're thinking hey what if we're talking about rankings in terms of loved versus hate it well in that case that's qualitative but if you're looking at maybe number of stars that would be quali quantitative if you're looking at number of people who purchased it that would be quantitative again it's how are you actually measuring this ranking huh so these questions actually are kind of difficult C amount of red primary dye necessary to make one gallon of each shade of red paint qualitative or quantitative hit pause this is quantitative because it's an amount of I don't think I can think of any way of of interpreting this to make it a qualitative because it's very clearly an amount of red primary dye okay so that that's going to be quantitative D number of paint choices available at several stores again qualitative or quantitative hit pause you're right I hope that's quantitative because you're looking at the words number of paint choices I suppose you could make this qualitative by saying a lot of paint choices not so many paint choices almost no paint choices that would be a qualitative way of measuring this but it's pretty explicit with the numbers of so that would be quantitative okay yeah that's not what I wanted to switch over to that's what I wanted to switch over to intra lecture question number one again this goes in your Moodle quiz is the variable number of toes a qualitative or a quantitative variable and again I recommend you write the question in your notes on the left hand side your answer below it so that you can go to Moodle after this lecture and put the answers in is the variable number of toes a qualitative or a quantitative numeric variable and back to the lecture now we're looking at continuous versus discrete data discrete data are quantitative data that can take on only particular values and those values are usually counts but they don't have to be counts they could be ratios of two counts I mean hey that would also give you discrete data continuous data are quantitative data that can be taken that can take on any value in a given interval and are usually called measurements again I personally don't like the term measurement because any measurement for statistician is something that you measure so eye color would be a measurement for me because I have to measure eye color but the way that the book wants to frame this measurements would be things that you have on a scale like a ruler scale continuous versus discrete so discrete are counts or ratios of counts continuous data is is takes on any value in a given interval so again we've got the qualitative and the quantitative ovals quantitative can be broken down into the discrete which is usually counts so ratios of counts and then continuous which are usually measurements notice discrete and continuous don't speak at all towards qualitative because qualitative data is neither discrete nor continuous here's another example to determine whether the following data are continuous or discrete temperatures in Fahrenheit of cities in South Carolina is this continuous or discrete hit the pause button now welcome back assuming you hit the pause button now temperatures in Fahrenheit are going to be continuous what's reported on TV will be discrete but the actual temperatures will be continuous B number of houses in various neighborhoods in a city continuous or discrete hit pause now welcome back numbers of houses tells me it's going to be discrete the key word there is numbers you're counting how many houses are in a various neighborhood see numbers of elliptical machines in every YMCA in your state numbers of elliptical machines in every YMCA in your States is that continuous or discrete hit the pause button you're absolutely correct that also is discrete it's got the word numbers of D Heights of doors continuous or discrete oh hit the pause button welcome back whatever I'm supposed to say that's continuous Heights are continuous in the words of the book it's a measurement realize that the actual height of the door is going to be continuous the variable called Heights of doors is continuous what you actually write down as the height of the door is going to be discrete because eventually you're going to have to stop writing down digits and that gets back to something interesting the difference between the variable and the data the variable is what you're measuring the data is essentially what you're writing down for those measurements the variables are what you're measuring such as temperature such as height such as age and the data is what you're actually writing down such as 97.8 degrees such as six foot five such as I I forget what the other example was but those are things that are written down variables can be discrete or continuous what you write down has to be discrete because you have to stop writing at some point which brings us to the next intraelection question is the variable grade point average discrete or continuous again I recommend you write the question over on the left side of your notes write down your answer right after it think about it go back over the last five ten minutes of this lecture so that you're absolutely certain what the difference between continuous and discrete is hit pause fast forward rewind as you need you've got total control over this lecture except for what I say I mean you could even slow this down so I sound like this or speed it up sorry that's not like this levels of measurement four levels of measurement nominal ordinal interval ratio if you learned French you can think of this in terms of Noir black Noir nominal ordinal interval ratio that may help you remember the four levels the level of measurement of a variable describes the amount of information that that variable contains the four levels are nominal ordinal interval and ratio nominal the values are just descriptions ordinal which is your for ordered nominal you've got the description plus you've got an inherent ordering to it interval level the differences between levels are identical so you can subtract and ratio level not only do you have differences between two levels being the same but you've also got the value of zero meaning an absence of so data at the nominal level of measurement are qualitative consisting of labels or names so variable eye color will be nominal level because what are the eye colors blue brown green Hazel and I'm sure I'm missing some colors blue brown green Hazel yeah I could also order them as brown blue green Hazel it would mean the same thing they're just names attached so it's nominal the word nominal comes from the Latin nominus meaning name suppose all students in a stats class were asked what pizza topping is their favorite explain why these data are at the nominal level of measurement piece of cake favorite favorite pizza topping is I don't know pineapple pepperoni this sausage olives I can't think of any more because I only use pepperoni and pineapple those are just names I mean it doesn't matter that they're the individual person's favorite I'm asking everybody for one pizza topping I can call it pineapple I can call it pineapple Apple Pine means the same thing B suppose instead that you wish to know that the number of students whose favorite pizza topping is sausage the number of students explain why this data value is not nominal well it's the number of so you're measuring the number of students counting the number of students nominal has to be categorical if you're counting things that's that's numeric so it can't be nominal data at the ordinal level of measurement are qualitative data that can be arranged in a meaningful order but calculations such as addition or division do not make sense so we've got some examples of nominal data eye color hair color but we can also look at some examples of ordinal data such as and having trouble coming up with one off the top of my head um well let's think through this it's it's got to be categorical and they've got to be in some sort of inherent meaningful order categorical but some inherent meaningful order to it how about socioeconomic status it's categorical low medium high but there is an inherent meaningful order to it low medium high the fact that you could also do it high medium low is irrelevant you've got an ordering to it so socioeconomic status would be an ordinal level variable notice ordinal level variables have additional information to them that nominal level variables don't have that is position I can say high SES is larger or greater than low SES I can't say blue eyes are larger or greater than brown eyes doesn't make sense example determine whether the data are nominal or no seat numbers on your concert tickets such as a23 and a24 go ahead and hit pause while you think about it I believe the answer to this will be it's ordinal although I haven't been to a concert 6080s I believe the a would be the row and 23 would be the seat number so seat number 23 is one seat closer to the aisle than seat number 824. since it is closer to the aisle since we do have some sort of ordering to it we've got an ordinal variable genres of Music performed at performed at the 2013 Grammys that would be nominal because it'd be I assume country and and rock and Jazz and doowop roll done at the Grammys I assume and there's no inherent ordering to that I could do it alphabetical order it would have just as much meaning as doing it in terms of the order that I just gave you so a is ordinal B is nominal data at the interval level of measurement are quantitative data that can be arranged in a meaningful order and such that differences between data entries are meaningful difference between levels are meaningful such as shoe sizes the difference between a 5 and a six in shoe size is exactly the same as the difference between eight and a 9 in shoe size they both differ by one shoe size but also by 1.3 inches I think temperature in degrees Fahrenheit is interval because going from 45 to 46 degrees is exactly the same as going from 95 to 96 degrees it's an increase of one degree Fahrenheit probably should say it's measured in degrees Fahrenheit birth years of your classmates are collected what level of measurement are these data well clearly it's going to be interval birth years 2000 I don't know what birth years you guys have 2001 2002 that differs by one year just like 1997 and 1998 different by one year the key for Interval level data is that subtraction and by extension addition actually makes sense and then the last level of measurement the one that has the most information in it is called the ratio level it's quantitative data it can be ordered this interval and the zero point indicates the lack of something so compare compare the year of your birth which we decided was interval from the last with your age which is ratio year the birth well a zero year of birth doesn't indicate a lack of anything just indicates it was two thousand some odd years ago but an age of zero would indicate you lack age now note that you don't have to be able to achieve a zero all that it means is that a value of 0 would indicate an absence or a lack of something so the height of a person would also be ratio because a height of zero would indicate a lack of height can't achieve a height of zero but if it were achievable it would indicate you lack height weight would be another example of a ratio level variable you can't achieve zero weight but a zero weight would indicate a lack of weight now it's called ratio level because ratios actually do mean something so comparing a person of age 10 to a person of age 20 that ratio actually does mean something the second person is twice as old as the first a four foot person versus an eight foot person that ratio of two actually does mean that the eight foot person is twice as tall as the four foot person compare that with your age of birth no just your year of birth sorry 1995 versus 2000 that doesn't indicate that your year of birth is 0.5 percent higher is just your five years later which is interval so the word ratio comes from the fact that ratios divisions actually make sense so example consider the ages in whole years of U.S presidents when they were inaugurated what level of measurement or these data clearly they're going to be ratio because it's in this section but why are they because it's ages in whole years an age of zero would indicate a lack of age doesn't mean you can achieve an age of zero just means that if 0 is in the age column in your spreadsheet you lack h so here are the nice little stair step you got qualitative quantitative qualitative includes nominal and ordinal nominal means names ordinal means ordered nominal quantitative is interval and ratio in interval zero is just a placeholder it's just such as zero degrees Fahrenheit if zero degrees Fahrenheit doesn't indicate a lack of temperature whereas in ratio zero does mean the absence of something so zero Kelvin would be a ratio level variable but zero I'm sorry uh measuring temperature in Kelvin would be ratio level but measuring degrees in Fahrenheit would just be interval level because a zero fahrenheit just means it's cold not that it lacks temperature zero Kelvin means it actually lacks temperature here's the third one give an example of a ratio level variable not provided in the slides or the text again I would write this off on the left write the answer beneath it so you can go into Moodle and answer the questions and pause of course if you need to so final example classifying data classify these as qualitative or quantitative discrete or continuous or neither and the level of edge of measurement normal ordinal interval ratio finishing times for runners in the Labor Day 10K colors contained in a box of crayons boiling points on sale let's see a scale for various caramel candies and the top 10 spring break destinations is ranked by MTV go ahead and hit pause before I give you the answers the answer for a finishing times for runners will be a ratio level it will be continuous and it will be quantitative colors contained in a box of crayons will be qualitative it will be neither because qualitative is neither discrete nor continuous and it will be nominal note that number of colors would be different but we're looking at just the colors themselves boiling points on Celsius that's going to be quantitative continuous and interval because zero Celsius does not indicate a lack of temperature and finally the top 10 Spring Break destinations is ranked by MTV notice these are the destinations themselves so it's going to be categorical or qualitative and hence neither discrete nor continuous lever measurement is going to be ordinal and that's it for section one two again don't hesitate to send me comments questions and post them in Moodle for the discussion section thank you much hello welcome to section 1.3 the process of a statistical study the objectives for this section are to describe the process of a statistical study that's probably rather clear identify the various types of studies but most importantly to understand the primary sampling schemes and it's this understanding the primary sampling schemes that is key for this section if all you take out of the section is those sampling schemes and an understanding of them you're doing great so here's the process of a statistical study notes that there are four steps Step One is determine the design of the study there are classes taught in experimental design by the way but you need to State the question we studied determine the target population and the variables and the determine the sampling methods you're going to use it's one C is it's it's a subset of one but it's probably the most important thing here because remember your sample must be representative of your target population otherwise all the statistical methods in the world are useless your sample must be representative so we need to study how to draw a representative sample two you're going to collect the data according to that sampling method three you're going to organize the data and four you're going to analyze the data yeah 4 is going to be the second half of the class 3 is going to be most of the rest of the first half of the class so example 111 neurologists want to study the effect of vitamin C on nerve disorders the goal of the study is to see if taking an intravenous dose of vitamin C will reduce the amount of nerve pain associated reported by patients so identify the population of Interest and the variables in the study again pause and welcome back the population of interest is all notice all populations of interests have the word all in there somewhere all patients period and the variables in the study will be I'm sorry all patients with nerve disorders and the variables in the study will be the amount of vitamin C and amount of nerve pain those are the obvious ones perhaps others will need to be taken eventually to take care of some intervening issues observational study versus an experiment and observational study observes the data that already exists so the statistician will sit there and just collect data won't try to influence the outcome of anything whereas an experiment generates data to help identify the cause and effect relationships um yeah the the book emphasizes that it's cause and effect it's easier to do cause and effect analysis when you've got an experiment but observational studies can hint that as well now that these are the proper definitions as used by scientists a statistician word for any quote theoretical data collection as an experiment this difference in terminology comes to the fact that statisticians experiment to better understand their field of study just like biologists experiment to better understand biology and physicists experiment to better understand physics statisticians experiment to better understand statistics so here's an example which type of study would you conduct observational or experiment a you want to determine the average age of college students across the nation B researcher wishes to determine if flu shots actually help prevent severe cases of the flu go ahead and hit pause the first one is an observational study it's second is an experiment notice in the second one you're actually trying to determine if the flu shots do something whereas in the first one you're just observing ages of students across the nation a representative sample has the same relevant characteristics as the population and does not favor one group from the population over another note that a sample could be representative for one characteristic of the population but not for another so here's an interesting question how do you know if a sample is representative of the population that we're going to come back to time and time and time again because it's such an important question remember all those statistics is based on that sample you draw and that sample has to be representative of the population so how do you know if your sample actually is represented the population we got a red star at the bottom so let's move on to the first intro lecture question one my sample is all females in this class is it a representative sample remember hit pause write the question over on the left side you probably should listen to what I'm saying before you hit pause write the question on the left side of your notes write the answer down below it so you can put it into your middle quiz and then hit pause I really do want to come back to this how do we know if a sample is representative of the population maybe you should write that on the left hand side as well and start coming up with answers to that question because you're going to be seeing it several times in this course here's five sampling techniques simple Ram sampling notice the book also has something called random sampling we're going to conflate that with simple REM sampling second type is stratified then clustered and systematic and then convenience never ever use convenient sampling except when I give you permission to in class but we're not in class so simple random sample every sample from the population has an equal chance of being selected keyword there is actually two keywords it's sample and equal chance so if I want to draw a sample of size 50 from everybody in the United States and they want to use simple random sampling that means that every possible combination of 50 people in the United States has an equal chance of being selected every possible combination of 50 people has an equal chance of being selected or we can bring it down to the class level let's say our our class is size 30 so our population is size 30. I want to sample from this population sample of size 5 simple random sampling would require that every possible combination of five people in this class would have an equal chance of being selected stratified sampling population is divided into subgroups called strata the grouping variable is correlated with a measurement variable and the sample is drawn from each stratum so in this example if I want to estimate the average GPA at Knox I probably would break it up into freshman sophomore junior seniors estimate the GPA and the Freshman estimate the GPA and the sophomores estimate the GPA and the changes estimate the GPA and the seniors and combine them together to get the estimated average and the reason I'd probably break it up into freshman sophomore junior seniors because GPA does seem to correlate with uh class contrast that with cluster sampling populations divided into subgroups called clusters not strata clusters the grouping variable is not correlated with the measurement variable and a sample is drawn from at least one of the Clusters so if we're going to go back and I want to determine the typical hair color at Knox breaking it up into freshman sophomore junior senior would not be useful in terms of stratified sampling but it would be useful in terms of cluster sampling so I highly doubt that level in school and hair color is correlated in this example we've got a humongous Rice Field one two three one two three four five six seven there's 21 subplots there I randomly select four of those subplots and estimate the amount of rice in each of them each of those subplots is is seems to be rather representative of the population as a whole systematic sampling every nth member of the population is selected so if I want to select 25 percent of the population I would select every fourth bottle in this example here I want to select every 10 percent of the population I'll select every tenth bottle if I want to get uh one percent of the population I'll select every hundredth the bottle but again it's it's systematic because it's selecting every nth member of the population as it comes down the conveyor belt or through the door convenient sampling it's just convenient for the researchers to select people will selfselect into the poll perhaps it's very unethical to use all of the web polls that you see on newspaper sites that say hey how would you vote in this case those are all convenient sampling and they are highly unethical because it is trying to give information to the reader and that information is about the population of interest but you're drawing a sample from a very skewed population or a very biased population itself so here's some examples a poster surveys 50 people in each of the Senators 12 voting precincts so this sounds like stratified um Senators 12 voting precincts so the voting Precinct is the stratum there's 12 of them surveying 50 from each I would assume that voting precincts are internally more consistent than the population as a whole because likeminded people tend to live near each other the quality control department is zero manufacturer marries a weight of every 10th box so this will be systematic a female student walks down the halls in a door I'm asking students how much money they'd spend from the food court this is convenience an educator chooses five of the school districts in the Chicago area and ask his household in those District how many school age children are in the district this would be cluster probably the book says this is cluster this may actually be stratified because a school district the distribution of of schoolaged children may not be the same across the school districts um one school district may have a higher proportion of children in each household than another school district if that's the case and this would be stratified to determine who'll win a hundred thousand dollar shopping spread them all manager draws the name out of a box of entries this will be simple round sampling technically this will be random sampling but we're conflating the two remember so it's simple random sampling there's the Red Box let's move on to question two this is a key one very very important what is the primary difference between cluster sampling and stratified sampling remember hit pause if you need to again write the question on the left hand side of your notebook and your answer underneath of it two types of observational studies is the crosssectional study in the longitudinal study crosssectional study data collected at a single point in time on a lot of members whereas in a longitudinal study it's over time on a few members a group of 220 patients is followed for 15 years in order to determine the okay right there I know this is going to be logic longitudinal because it's over time for a small group of of members be a gastroenterologist surveys 130 of his patients six months after okay this is going to be crosssectional because it's done at a single point in time six months after having the gastric bypass if the gastroenterologist surveyed those 130 people six months one year 18 months two years three years four years five years and that would be a longitudinal study similar terminology treatment is some condition that is applied to a group of subjects in an experiment the subjects or the participants are the people or things being studied in experiment the response variable is the variable in an experiment that responds to the treatment we'll also refer to this as the dependent variable the explanatory variable is the variable in experiment that causes the change or it explains why that change took place we're going to refer to this as an independent variable there's another set of variables that are called independent it's not the next slide I guess called control variables control variables are variables that we know affect the dependent variable but we really don't care about them in terms of our research so independent variables are broken up into exploratory variables or research variables and then those control variables so here are three principles of experimental design or one you got to randomize the control in treatment groups because the goal is the only difference you want between the control and the treatment group is to be the treatment so you got to randomly put people into okay you don't have to you should randomly put people into the two groups control and treatment and then apply the treatment to the treatment group uh control for outside effects on the response variable those would be the control variables replicate the experiment a significant number of times to see meaningful patterns and I do want to emphasize here the word replicate remember back earlier we asked how do we know if our sample is actually representative the answer is we don't we have to replicate our studies over and over and over again and then the hope is that we don't make the same mistakes and get an unrepresentative sample in each of those replications okay control group versus treatment group is this is the group of subjects which no treatments given whereas the treatment group gets the treatment and again the structure of the experiment has to be such that the only meaningful difference between the control group and the treatment group is the treatment and hence the importance of the randomization confounding variables are unmeasured factors other than the treatment variable that cause and effect on those subjects I had to get rid of confounding variables you measure them add them to the model and there's a red star so let's go back and see question three how do we know if there are confounding variables in a statistical study move those over write that over on the left hand side of your notes put the answer there so that you can answer the quiz in Moodle how do we know if there are confounding variables in Oh missing an s in a statistical study a placebo is a substance that appears identical to the actual treatment but contains no intrinsic beneficial elements um placebos are used to ensure that the only difference between the control and the treatment group is the treatment itself placebos will be given to the control group so the control group doesn't know that they're in the control group the placebo effect is response to the power of suggestion rather than the treatment itself that's why we have to give a placebo to the control group because the very Act of receiving medicine will affect you hence we don't know if the treatment actually improves the the people or just them thinking they receive treatment so we got to give placebos to the control group and the actual treatment to the treatment group so they all think hey I received something so the placebo effect is going to be constant for the entire group and again remember the emphasis here the only allowable difference meaningful difference between the control group and the treatment group is that the treatment group receives the treatment in a single blind experiment the experimenter pokes out one person's eyes oh wait no that's not what it is subjects do not know that they are in the control group or the treatment group whereas In a doubleblade experiment both the subject and the measurer doesn't know the measure is the person who measures the temperature of the patient measures whether or not the patient improved measures if the patient's foot fell off whatever whatever the treatment is supposed to fix the researcher a good researcher will move him or herself completely from this once the experiment is set up and is in motion researcher will just take the data that is collected and analyze it because the researcher needs to know everything but in a doubleblind experiment it's the subject and the person doing the measuring who don't know so subject person doing measuring and researcher three different people in the experiment three different roles in the experiments sometimes the subject and the the measurer are the same person so here's the lengthy example consider the study from example 111 in which a neurologist want to determine if taking the IV dose of vitamin C will reduce the amount of nerve pain reported by patients suppose that the study was narrowed to focus only on patients with the nerve disorder multiple sclerosis after study approval the neurologists solicit volunteers who are patients with MS and who are reporting nerve pain so Target population one I guess is all patients smaller Target population is patients with multiple sclerosis sampled population patients with multiple sclerosis who have nerve pain sampled population is the people in our after study approval the people in the study that I can pull from this doesn't specify what the sampled population is the sample is the actual 40 participants to one t in the control group and 20 in the treatment group a is the treatment group they're given the IV doses of vitamin C so the subjects are getting the doses somebody is measuring the nerve pain participants in group b are the control group they're given an IV dose of saline which is the placebo and somebody's measuring it the participants don't know which group they're in and the people measuring should not know which group they're in oh should not however the nurse is administrating the IVs are aware of the group assignments that's not necessarily a bad thing as long as the nurses are not measuring the nerve pain after predetermined length of time the amounts of pain reported by the separate groups are compared to determine if the IV dose reduces the amount of nerve pain interesting expandatory and response variables treatment which group is the treatment which is the control group what's the purpose of ministering saline to Group B is the single line single blind or doubleblind hit pause welcome back explanatory and response response variable is going to be the amount of nerve pain the explanatory is the vitamin C saline I'm sorry the vitamin C IV drip the treatment is the IV uh of vitamin C which group is the treatment group and which is a control group the treatment groups the one that receives the vitamin C the control group is one that receives Saline what is the purpose of menstruating saline to Group B it's to control for the placebo effect remember group a and Group B have to be exactly the same except that group a receives the treatment so giving the IV also makes the groups more similar is this a single blind or a doubleblind study it depends if the nurses are measuring the pain it's single blind if somebody else is measuring the pain that doesn't know who is when who is in which group then it's doubleblind last page IRB is a group of people who review the design of a stage to make sure that it is appropriate and that no unnecessary harm will come to the subjects involved Knox College has an Institutional review board it meets infrequently to handle student and faculty research plans or research designs informed consent involves completely disclosing to the persistence pins the goals and procedures involved in the study and obtaining their agreement to participate there is a lot of question out there whether informed consent is ethical um whether it's even possible in some cases in all cases where it's reasonable informed consent should be obtained now the question comes down to when is it not reasonable and it's not reasonable when the purpose of the experiment is is given away by the by telling the participants the goals and procedures because remember um the the Placebo and the placebo effect if you know the outcome of an experiment or if you know what the researcher is looking for in the experiment is a much higher probability the researchers are going to find it so unfortunately informed consent in those cases could actually destroy the experiment which means well is informed consent ethical in those cases I don't know it's a big question things to think about but that brings us to the end of this section that's the end of this chapter however I encourage you to go through section one four but I won't be lecturing through it it's important stuff we just have more important things to cover so thank you much hello and welcome to section 2.1 we're going to be creating frequency distributions today it's unclear at this point why you're creating frequency distributions tomorrow or in the next lecture you're going to see how we use these frequency distributions and help create Graphics that tell the story of the data and we're going to look at how to create ungrouped frequency distribution and create a grouped frequency distribution the ungrouped is going to lead to charts such as a pie chart and a bar chart whereas the grouped is going to lead to such things as a histogram or a cement leaf plot so today is the foundations and the next lecture will be for what we're actually going to be using this for throughout this lecture I'm going to point out some things such as while this is a lot of work for not that much and I want you to take that seriously and think okay a lot of work we're not getting too much out of it is there a way of making this easier and of course the answer is yes so frequency distributions a distribution is a way to describe the structure of a particular data set or population we're going to look at sample distributions now when we get to chapter 5 and 6 we'll be looking at population distributions so keep this in my effect on the left hand side of your notes put a little star right C chapter 5 and 6 and another star just to draw your attention to it that this is the start of looking at distributions here for the sample later for the population a frequency distribution is a display or a table of the values that occur in a data set and how often each value or range of values occurs so it's how often that's where the frequency comes in frequencies which is a little f are the number of data values in that category if we're talking about categorical data or range of values if we're talking about numeric data in a class we're going to call that as a category of data in a frequency distribution for categorical variables that class most often is going to be where the levels in the variable for numeric data it's going to be some range of possible values for the data in order to raise an ordered list of the data from largest to smallest or smallest to largest probability distribution is a theoretical distribution used to predict the probabilities of particular data values occurring in a population probability distribution If all we're going to talk about is probability distribution is going to be about the population if we're talking about the distribution or a sample we'll call it a sample distribution an ungrouped frequency distribution is a frequency distribution where each category or class represents a single value or level in the variable these are used for categorical variables whereas a grouped frequency distribution is a frequency distribution where the classes are ranges of possible values these will most likely be used in numeric data the ungrouped frequency distribution will lead to bar charts and pie charts in the future whereas a grouped frequency distribution will lead to histograms in the future so here's the steps in constructing a frequency distribution for ungrouped for categorical data so to create an ungrouped frequency distribution to determine the levels of the categorical variable and I do want to emphasize that you would use this only for categorical variables and count the number of observed values in each level okay number a level is a possible outcome of a categorical variable example the iCloud of my research students in this term are as follows blue brown brown blue brown brown brown green so the blue brown brown blue brown brown brown green is the raw data the observations the measurements I make of my students eye colors blue is the eye color of my first research student Brown is the eye color my second research student Etc the frequency distribution just looks at all the possible levels that we observe blue brown and green and then counts the number of number of observed values in each of those levels I got one blue two blue only two Brown I got one two three four five green I got just one so the frequency distribution is 251. for blue brown green ooh red star I think that's what I want to do so question one okay and I encourage you to write these questions and your answers in your notes on the left hand side what is the difference between a grouped and an ungrouped frequency distribution pause and we're back so that was all about the ungrouped or for the categorical variables now we're going to look at grouped which is almost always applied to numeric variables remember the main difference between the ungrouped and the grouped is that the ungrouped each Row in the table corresponds to one level in the variable whereas for the grouped each row is going to correspond to a range of values of that variable so the first step in creating this grouped frequency distribution is decide how many classes should be in the distribution now we've got rules of thumb none of them are good because they're rules of thumb the reality is you should choose many different numbers of classes so the first time through you should choose five and then maybe 10. and see what the information given by these distributions how that changes Maybe 20. but you're going to realize that if we do 5 10 20 12 and 18 we do the we got to do these steps five times and it takes a lot of effort to do just one so you're going to fall in love with the computer because the computer can do it just like that I don't know if you could hear me snapping my fingers but I'm snapping my fingers so the first step is to decide how many classes should be in the distribution once you've decided that two choose an appropriate class width class width is usually going to be the highest value minus the lowest value divided by the number of groups sometimes you'll round down to something logical for the lowest value round up to something logical for the upper value and then divide by the the number of classes sometimes you'll Skip One go directly to two and and physically determine that class width what the classes should be based on the the problem itself and we'll see that later in those cases easy lends itself to Natural divisions such as decades or years or hundreds of dollars three find the class limits the lower class limit and this is for each of the classes by the way the lower class limit is the smallest number that can belong to that class and the upper class limit is the largest number that can belong to that class so all the values in that class are greater than the lower class limit and lower than the upper class limit and now you just count the frequency of each class 4 is the easy part by the way you just determine the frequency count how many are in each of those classes so here's some terminology class width is the difference between the lower limit and the upper limit of two consecutive classes lowers the smallest number that can belong to that class upper class limits the largest that can belong to that class here's the example here are 20 television prices 3D TVs the first 3D TV price is one thousand five hundred ninety five dollars the second one was one thousand one hundred ninety nine dollars and sixteen eighty five Etc all the way up to 1999 great song but that has nothing to do with class limits first step is determine the number of classes if we follow these three steps determine the number of classes there's 20 data points how many classes should we do or we could use the problem itself to hint what the classes themselves should be um these are TV prices we got the lowest TV price here of uh 15.95 the highest of 19.99 these are in looks like class width should be probably be a hundred dollars that would make sense here so we do maybe 1400 to 1500 1500 to 1600 1600 to 1700 that would make sense remember the purpose of the graphics for you is so that you understand the distribution of the data for your reader or your client it's that they understand the story that the data is telling so to determine the class width we were told five by the way use five classes so maybe my idea doesn't quite work at least for this problem I think it would work for reality five classes so 19 this is the highest value the lowest value divided by 150 so about 81 would give us a class width okay 81 dollars class width I guess if we're going to follow this without thinking that would be a good thing to do but if we actually want to think 81 dollars doesn't really seem reasonable in telling the story of the data a hundred dollars would make sense because there's something magical about round numbers in fact doubly round numbers there's two roundnesses here and there's none here oh I guess the eight if you turn on side it's got two circles but this never mind so a hundred dollars would make sense then we gotta choose 100 width so we gotta say how low to how high um beginning at 1500 makes sense if we want to just follow the directions without thinking at all 15.95 would make sense is the first class starting point 1500 if you actually want to present this to a client would make much more sense so 1600 will be this class limit of the second 1700 1800 1900 2000 will be all the classes or the class boundaries otherwise known as the breaks should not be overlapped in the class boundaries really although in all reality it doesn't matter so here the 3D thinking that butt but they can't actually overlap so now we've got our classes 1500 15 99 1600 or 16.99 and now we just calculate the frequencies two five four five four notice that if you add up to five four five and four you should get 20 which is our sample size now we're going to find something called a class boundary it is the value lies halfway between the upper limit at one class and the lower limit of the next class so the class boundary here would be 15 99.50 1699 50 17 99 50 18 99 50. or 15.99.5 if you want and the purpose of the class boundaries is to make very clear that there is no overlap and that everything no overlap and that every possible value fits into one and only one of the classes the midpoint is the upper limit plus the lower limit divided by two these midpoints are used um for estimating the average value in each class we'll see it in the next chapter in dealing with grouped data lower upper divided by 2 gives us a class midpoint of 1549.5 although I don't know a statistician alive that wouldn't say 1550 would be the midpoint 1500 plus 1599 divided by 2 gives us this is the midpoint 1600 plus 16.99 divided by 2 gives us this is the midpoint those are for frequencies if we want to do for relative frequencies we just divide the frequency by the sample size so 2 divided by 25 divided by 24 divided by 25 divided by 24 divided by 20. notice that adding up all the frequencies gives us the sample size which we're always going to symbolize as a lowercase n lowercase f is the frequency the subscript of the I is for class I so F sub I is the frequency in the I class so this calculates the relative frequency we first add up all the frequencies to determine the sample size now we just divide each of the class frequencies the F sub I's by 20 so the first relative frequency is going to be F sub 1 divided by 20. second will be F sub 2 divided by 20. then F sub 3 divided by 20 Etc cumulative frequency is just the sum of the frequencies of a given class added to all lower classes so if you are able to order your data then you can do a cumulative frequency if your data cannot be ordered that is if it's nominal and you cannot do a cumulative frequency it doesn't make sense to talk about less than or equal to if there's no ordering so here we got the frequencies and this last column is the cumulative frequency so the cumulative frequency for the first one is always the frequency cumulative frequency for the second is going to be five plus two because it's whatever is here Plus what's above four plus five plus two five plus four plus five plus two four plus five plus four plus five plus two notice the cumulative frequency always ends with our sample size little n here's an example data collected on the numbers of miles that professors strive to work daily are listed below clearly not below it's on the next side to use these data to create a frequency distribution that includes the class boundaries midpoint relative frequency and cumulative frequency of each class we're told we have to use six classes so here's the data the low is one the high is 11.9 we're told six so if we just follow the directions without thinking we're going to come up with 1.8 as our class width so we'll do class boundaries as being 1 2.8 4.6 .4 8.2 11 12.9 I whatever they are I lost count already I'm just adding 1.8 where are we I'm just adding 1.8 to each of those notice that it may make sense to do it class width of 2 instead of 1.8 one it's a lot easier to do in your head and two it makes a lot more sense in terms of presenting the data so instead of 1.8 let's round this up to 2. so the boundaries will be 1 3 5 7 9 11 13. see much easier now the lower class limit is the limit of our first class one is the lowest so if we just want to follow this we can start with the one I would say let's start with zero that would kind of make sense so it'd be zero two four six eight ten twelve that would include all the data instead of one three five seven nine eleven thirteen which would also include all the data but maybe zero two four six eight Etc looks better to the client however we're going with one so one three five seven nine eleven three five seven nine eleven thirteen so we're backing It Off by point one now we do the frequencies there's our midpoints and again don't know any statistician that would say the midpoint is 1.95 every statistic I know would say the midpoint is two four six eight ten twelve and the class boundaries and the class would be one to three three to four four to five five three to five five to seven seven to nine but notice in each case you include the lower but not the upper frequency is three three four two four two we get that just by going back to the data itself and Counting relative frequency is our frequency so F sub 1 divided by our sample size of 18 F sub 2 divided by 18 F sub 3 divided by 18. cumulative frequency is the frequency in this group plus all those lower so be 3 6 10 12. 16 . and of course we could have a cumulative relative frequency which would be just each of these divided by 18. so here's the overview again notice there's a lot of work to this if we do it by hand and if we have a realistic data set and a lot of decisions we have to make and the the distribution itself depends on those decisions so we're going to want to do several of these hopefully not by hand so we got to decide on the number of classes I choose an appropriate class with find the class limits determine the frequency in each of the classes that's a lot of work for one frequency distribution and again remember this is for grouped frequency distributions being able to automate this would would make life so much easier and allow us to look at different uh the effect of different choices we make so instead of five we'd be able to figure out well what does the frequency distribution look like if we chose 10 classes what if we chose 50 classes what if we chose two classes what would it what would it look like and what stories do each of those frequency distributions tell us about the data here are the other characteristics we got the classes boundaries midpoints filter frequencies and cumulative frequencies and we got a red star it's good because that's the last page we got two questions question number two again put this over in your notebook on the left hand side write the question and your answer below it so you can transfer that into moodle's quiz what are the steps in constructing an ungrouped frequency distribution and then question number three what are the steps in constructing a grouped frequency distribution and a hint one of those two is going to be a lot easier than the other um and what I'm getting at in these two questions don't hit pause yet what I'm getting at least two questions is you'll see that the groups frequency distribution steps are exactly the same as the ungrouped except for you have to add a few things and taking your numeric variable and making it categorical take your numeric variable at making it categorical and that's what these classes are doing you're taking your numeric variable and you're classifying them you're making them categorical and that's it section 2 2 takes what we did today and makes pictures out of it okay makes Graphics out of it plus we get to see a lot of different types of graphics and we'll see how to do all of this really fast in r so hope this was helpful talk to you later hello and welcome to section 2.2 graphical displays of data recall in the last lecture that we looked at how to create frequency distributions both ungrouped and grouped today we're in this lecture we're going to look at turning those frequency distributions into Graphics that tell the story of the data I do want to emphasize the purpose of Graphics is to tell the story of the data the graphics that we see today are going to tell the story of the data to you mainly because they look pretty ugly but we're going to see in some of the scas the statistical Computing activities how to pretty up those Graphics so that they tell the story in a pleasing way to your audience remember the graphics serve two purposes that are really the same purpose the basic Graphics tell you the story of the data and then the the the publication quality Graphics tell your audience the story the data that you learned so we're going to look at those utilitarian ugly Graphics today and in SCA I believe it's sca2 we're going to see how to create nice looking Graphics that can help convey that story to your reader um we're going to look at how to create some bar charts we have a pie chart before that yeah but bar charts uh both uh univariate bar chart that is for one categorical variable but also some bivariate stacked and sidebyside bar charts look at histogram and it's neighbor stem and leaf plot and then line graph we're going to look at different shapes of the distributions remember these these Graphics are going to tell us about the data here's some rules and what graphs or Graphics should have they should stand alone without the original data that is in your paper you provide a graphic instead of providing all of the data the graphics have to have labels and values for both axes when appropriate a legend of source and date should be included the source and the data are usually in the caption however and in a paper the graphic must contain a number and a caption so the first figure that you include is going to be called Figure one and then you're going to give it a caption a brief description of what that graphic says the graphic must also be described in the pros and you're in your writing in the paper itself ooh red star so let's go to our first question what do all Graphics need and remember you should write this over on the left hand side of your notes the question and your answer and you should hit pause to remember to gain some time here because I'm going to hop back to the lecture so we'll start with pie charts probably the worst creation of statisticians a pie chart shows how large each category is in relation to the whole the whole is represented by the entire circle the parts are slices of that pie he used to describe qualitative categorical data it's pretty straightforward you create your your frequency distribution for it you calculate the relative frequencies and then you multiply each of those relative frequencies by 360 degrees or 2 pi if if you prefer here's the example calculate the class housing types for students in this in a statistics class four types of housing these this is the frequency distribution we learned about how to calculate that in the last lecture 2015 nine and five 49 is our sample size so now we create relative frequencies so the first relative frequency is 20 divided by 49 Second will be 15 over 49 there will be 9 over 49 the last one will be 5 over 49. and then we create the central angle measures by multiplying those relative frequencies by 360 degrees so we know that the apartment has to cover 147 degrees of that 360 degree circle the dorm has to cover 31 which is 110 degrees Etc and then we just create the histogram the pie chart drawing a circle a dot in the center and then measure that this 41 percent is 147 degrees and this 31 percent corresponds to 110 degrees and this 18 is 66 degrees Etc rather difficult to create you know reality it also suffers from the fact that slices have to be different colors and brighter colors tend to draw our attention more so the 10 percent slice the gold yellowish gold slice tends to look bigger than the 10 percent that is allotted to it furthermore it's kind of difficult without the 10 and 18 actually stated there to compare the size of this slice to the size of that slice our eyes our brains have trouble distinguishing central angles and the relative sizes of those so for all intents and purposes unless someone tells you you must make a pie chart avoid pie charts a much easier chart to create and better allaround chart to create is a bar chart which we'll see next but we got the R so let's go ahead and start r if I can figure out how to do that didn't do it that kind of did it so I'm going to just start r I've got our a link to our in several places this is one of them you will probably have two links on your desktop or one or two links on your desktop but it'll be here I've got several versions of our this is an old version of art but it works for me just started first thing always new script notice there's overlap between the two in PCS there's going to be that overlap in Max there may not be so I'm going to tile vertically notice I clicked on the console window first so the console window is over here the script window is over there so the first thing I'm going to do is load data these pound signs or hashtags indicate a comment once R comes across one of them it ignores everything else in that line so I use three to indep more notice I move things around I'm going to make this side bigger so you can see it I'm going to load data from the internet I'm going to read a CSV file from the internet function I'm going to use is read.csv it requires that you specify the path to that location what we're going to look at is the crime data set remember you've got the pause button so you can pause this and type it in um notice that if I just run this and by click run this that's control r on a PC or command enter on a Mac it's going to go to the Internet it's going to go to this particular URL and it's going to read it and it's going to spit it back out on over here on the console window so this is that data set it just printed off it didn't save it R doesn't know this data set now since we are going to be using this data set we need to save it into a variable by bad habit I call this variable DT DT for data a good habit and computer scientists would want to do this is to name it something descriptive so crime will be the crime data set or they may want to do crime data set if they like their camel case or Capital C crime data set I'm bad enough at typing that I just leave it as DT I run that notice what happens over on the left it just spits out DT equals read.csv of this URL no errors because I typed everything correctly and all this data set is now stored in the variable DT if I want to see it I'm going to type DT over on the console side to run a line on the console side you just have to enter and boom it spits out the entire data set um we got a variable repub if I want to look at that variable repub I can type repub here on the on the script side if I'm going to save it and use it later or I can just explore over on this side type repub type it in hit the enter key and get an error error object repub not found um there's going to be two reasons for that error one misspelling two you didn't attach the data set R only sees one variable at the moment that one variable is DT the entire data set and there are ways of peeking into the data set for R name the data set dollar sign says Peak into and then the variable name and that will give us all the Republican values or if we're going to be doing this a lot and we don't want to type DT frequently we can just attach the data run the attach and then repub now gives us the data values so if I don't attach just type in the name of the variable gives me an error I have to do the data set name dollar sign repub to get those values or if I just attach the entire data set I can type repub or any of these variables to access them actually if all I want is the names of the variables I can use the names function similarly if I want a summary of each of the variables I can type summary this is very useful because it tells us how R sees the variables themselves if it gives a frequency distribution RC's this variable as being a categorical variable is it if it gives us the six number summary RCS this as being a numeric variable so back to repub the minimum value is a negative 0.25 the maximum is 0.24 there's three missing values the median is 0.01 I mean this 0.005 but that isn't that doesn't matter because we want to look at how to do a pie chart pie chart let's do a pie chart of census four there's our census for variable first thing we need to do is tabulate the values that is get a frequency distribution for it we do that with the table function and then to create the pie chart we just use the pi function on that frequency distribution and as you can see it's kind of difficult for me to tell which of these two is bigger Midwest or west if I look at it long enough I can tell that it's West but it took me a long time to figure that out I had to stop and think about it those aren't pretty colors but this is just for you to understand the data itself so the pi function when it's applied to a frequency distribution gives us a pie chart and to create that frequency distribution it's the table function let's see if I can get back to this okay moving on from a pie chart we go to the barge graphs or the bar charts a bar graph is just like a pie chart except instead of pieces of a circle it's bars of a certain height the height of those bars represents the frequency of the data of the values in the data set it's for qualitative data operator charts are for nonmonal data it orders the bar Heights from lowest to highest or highest to lowest your choice stacked bar graph is for to looking at two different categorical variables um where the bars are stacked and then we've got the side by side it's where the bars are side by side clearly um of all of these if I've got the the bottom two or for for two categorical variables Prado is only phenomenal the bar is phenomenal and ordinal between the Stacked and the side by side I personally prefer the side by side it's a personal preference I don't know that there's any science behind it other than it's my personal preference so here we are creating a bar graph from our data that we've already found the frequency distribution for The Apartment bar is going to be 20 High the dorm bar is going to be 15 High the House Bar is going to be nine High the sorority fraternity bar is going to be 5 high because those are the frequencies that's all there is to this and now I don't have to squint and try to estimate which of these is bigger and which is smaller and by the way this happens to be a Pareto chart because it's going from largest to smallest or smallest to largest we've got the R so let's see how we can do a bar graph or a bar chart in r function is bar plot again it's applied to a frequency distribution and there's our bar plot the height of Midwest corresponds to the frequency of Midwest states height of Northeast to the frequency of northeast states Etc if I want to turn this into a Pareto I just have to sort and now we see that the South has the most frequent uh Northeast is the least frequent pretty straightforward by the way the space that I left here and here are optional those spaces are for me as the reader to better understand what I'm typing and to make sure I don't make mistakes if I don't include those spaces there's a good chance that I will forget the closing parentheses I'll run that line and I'll get a plus mark down here plus Mark indicates that R is waiting for some more input it's waiting for something for me usually it's a closing parenthesis or a closing brace in this case I know it's a closing parenthesis two ways of dealing with it if I know it's a closing parenthesis I can just highlight that parenthesis and run it it runs the line prints out the the bar chart and we get back to a less than sign if I wasn't sure what the problem was I click over on the console side and I'll hit the escape button and then enter and I get back to the lesson sign or maybe that's a greater than sign but the key is use spaces for you so that you can tell what you're typing here's how to do the prayer chart by hand you just order it and you get that notice this is a horizontal bar chart here's how to do horizontal and r h o r i z equals true there's a comma here is stands for horizontal there's true now we've got ourselves a horizontal Pareto chart here's stacked stacked bar charts are for looking at comparing two categorical variables um in this case one variable is the sample letter and the other is housing type so in this in the apartment housing type sample a came up with about 20 of students who were in the department and Sample B came up with 33 I'm sorry 13 because this the height from here to here is 13. here's how to do this uh the Stacked side by side looks exactly the same except the heights these are stacked on top of each other they're side by side hence the name side by side let's see how to do side by side in r first we got to figure out that frequency distribution we'll use the table command tables used for frequency distributions of categorical variables I will do census for that's a comma let's do it by what names do we have available here we go looking through these there's lots and lots of variables I'm going to do Dom Paul culture as my second so this is our two dimensional frequency distribution um this 5 here says that in our sample five states are in the Midwest and are individualistic this seven is seven states in the Midwest and are moralistic this zero is zero states in the midwest are traditionalistic seven western states are moralistic 15 southern states are traditionalistic so that's our frequency distribution and now we just apply our bar plot to this copy and paste is fantastic isn't it so here we are with this a stacked individualistic all together there's 17 more holistic altogether they're 17 traditionalistic altogether they're 17. um these colors correspond to the census 4 region see if this works specify Legend equals true so the lightest is the West so this would be the western states that are individualistic this will be the southern that are individualistic this is the Northeast that are individualistic this is the Midwest that are individualistic this will be the South that are traditionalistic and the West that are traditionalistic that's one way of looking at it the data we could switch the order and now the base is going to be the region of the country and the bar Heights are going to be based on the um dominant political culture so in the midwest individualistics about five moralistic is about seven Northeast South West which of those two is better this or this the answer is it depends on what story you're trying to tell or to learn about the data if you're trying to learn about the data both are important to do if you have trouble with the colors you can specify different colors I'm going to have to specify four colors here with call col stands for colors and pretty easy we can just do one through four that's a colon it indicates through so this one colon four is the numbers one through four one two three four and now we run this and we can see much more clearly what the colors are the West is the blue the south is the green the Northeast is the red the Midwest it's the black for this second there's only three I'm going to do colors four through seven four through six four five and six would be the colors we use ew probably should have guessed what the colors were blue cyan and magenta I guess it could be worse so those are side by side I'm yeah those are stacked to do side by side we just specify we spell correctly beside equals true so they get side by side we have to specify b side equals true and now we have side by side this may be helpful to look in the north Midwest and see that the moralistic outnumbers the individualistic in the Northeast the individualistic is about double the moralistic in the South strongly traditionalistic with just a couple individualistic and the West is much more spread out and again we could change the order but we have we now have four colors we need to deal with so the individualistics are kind of spread out over the four groups the moralistic it's totally missing the South and traditionalistic is Midwest and Northwest totally missing so again which of these two is better they both tell us a story about the data therefore they are both important frequency histogram or just a histogram is a bar graph of a frequency distribution of quantitative data frequency histogram is based on your groups distribution from last lecture a relative frequency histogram is also based on your grouped frequency distribution but it's the relative frequency distribution characteristics of histograms it's a bar graph of a frequency distribution horizontal axis is a real number line it's the values of your variable the vertical axis is going to be the frequency within each of those classes the width of the bars represents a class width the bars in the histogram should touch tell that to excel because Excel doesn't let them touch unless you know how to unless you know the tricks the height of each bar represents the frequency so here's the example remember this example from last time took a long time to come up with this table frequencies getting the graph is pretty straightforward though there's two in the first class five in the second class four and the third class five then four again this was the hard part creating bar charts off of it the easy part this is for the frequency histogram and doing it for the relative frequency you get exactly the same shape exactly the same shape you just got to call the vertical axis the relative frequency here's how to do a histogram and r we'll do it on the violent crime rate in 1990 the function is hist hist for histogram there's our basic histogram that was pretty fast by default in this case particularly R is going to have class widths of 500 there's going to be one two three four five classes there's a there's an algorithm it goes through to figure out the quote optimal but there's nothing optional about these you need to actually specify or I'm sorry you need to do multiple histograms so you get a better feel for what the data are so here's how you specify it's with the breaks option this will give us about 11 breaks um breaks are very are going to be close to the class boundaries it can be similar to the class boundaries you want more than 11 breaks how about 21 breaks again the key is 51 breaks the key is what story are these telling us and what's the best story for it to tell there's a thousand and one breaks there's five breaks there's two breaks notice that we could also or realize that we can also instead of just say the number of breaks we can specify the actual class boundaries themselves also with the brakes slot we're going to use the sequence function seq for sequence the lowest is zero the highest I think was 2500 and then the third slot here belongs to the number of classes you want I'm sorry ignore that it doesn't belong to the classes you want a number of classes it belongs to the class width so if I wanted my class widths to be 100 that's what it's going to look like if I want the class widths to be 200 oh I got an error down here some X not counted oh that's because this function will set up values I'm going to run that just that part at 0 200 400 600 I'm adding 200 each time until I get to 2400 because adding another 200 will put us Beyond and the Washington DC is actually sitting out there above 2400 it needs to be counted so 2600. and now it runs 250 10 . notice how quickly we're able to create these histograms we don't have to focus on the particular calculations we can choose our class widths and allow that to tell us the story of the data much more quickly in all of these the story of the data says hey look at this outlier it's far from all the others this happens to be the District of Columbia everything else seems to be clumped nice little bellshaped ish to it and that was true when it was a hundred for the class width again bell shaped not quite so smooth but a bellshaped nonetheless when we use 2500 when we use 200 I mean this actually looks like the best of them but it looks like the best of me because I still got that outlier and I've got a nice curve over on the left so notice how quickly we did one two three four five six seven eight nine ten eleven twelve thirteen fourteen fifteen sixteen Seventeen histograms if we were stuck doing these by hand you'd still be doing the first one stamen leaf plot is an old time histogram that's also for quantitative data there's a lot of steps to it again it takes a long time to do it you're going to want to do one of these at least once but the reality is once you can do a histogram there's no reason to do a stem and leaf plot so here we've got the ACT scores the stem is going to be the tens place so this stem will be a one the leaf will be eight the stem is two the leaf is three this damage two the leaf is four and then we just collect on the stems and write out the leaves so we got 18 19 18 17 or HCT scores 23 24 27 26 22 27 29 Etc are the ACT scores it's better to do these in order by the way so the data values are 17 18 18 19 20 21 22 23 24 24 25 Etc if you want and r function is stem and here's your stem and leaf plot here's the key decimal point is two to the right of it so this is going to be zero two decimal 2 to the right of this is a decimal place so that'll be 70 it'll be 130 130 140 160 160 170. because these are by twos 260 280 280 280 300 300 310 330 340 350 390 430 430 Etc this is 24.60 this is 1240 this is 10.50 1080. line graphs the last graph for the day is use the data our measurements over time it's just a scatter plot connected the dots where the horizontal axis is time the vertical axis is our variable of interest so here's the example with the Consumer Price Index CPI is a measure of the average change in the value over time for a quote basket of goods and services for a typical American it's an index calculated by the Bureau of Labor and statistics table below shows the actual values of the cpia from several years from 1920 to 2010 actually every decade so in 1920 the CPI was 20. so it costs 20 bucks for that basket of goods in the year 2000 it costs 172.20 for that same basket of goods technically not the same basket something items were switched out and put in over time but they were equivalent when they were switched out and replaced here's how here's the line graph notice the horizontal axis is the year vertical axis is the CPI and you can look it was pretty consistent until 1970 and then the price increased dramatically over the next 40 years let's see how to do this in r um first we got to specify the variable values this was the years happened to be a sequence from 1920 to 2010 every 10 years and the CPI itself is equal to C is a function that collects several values into one vector I'm going to be switching back and forth a lot 20 16.7 14 24.1 29.6 82.4 130.7 2 0.2 finally 218 point 18.1 those are the variables we got run them notice there's no errors over here and then we just do a plot X variable comes first that would be the year in this case the CPA the Y variable comes next which is a CPI and then this has to be a line and Dot got lines and dots one way of doing that is to specify type is equal to lowercase b and there's our line graph xaxis or X variable y variable and to get both lines and dots you would do type equals B if you want lines that will be a lowercase l that is a lowercase l I notice the dots go away but you can add them back with the points command now the dots are back you're going to learn how to make this graph look much more appealing but that is for a different slideshow so here's the summary for qualitative data that is for categorical data we've got pie charts and bar graphs we've got the Pareto chart if the data are nominal side by side bar and stacked bar when you've got two categorical variables that you're looking at for quantitative data you've got the histogram you've got the stamen leaf plot which if you turn your head sideways looks an awful lot like that histogram and then we've got the line graph if the horizontal axis or the X variable is time now we got our red star it's pretty good because that's the last page here let's see how bad I've messed it up aha we got it back so question number two and again I would recommend writing the question and your answer on the left hand side of your notes so question two which Graphics can only be used for a single categorical variable key is the word single and the word categorical go ahead and hit pause question number three which Graphics can only use for a single numeric variable single numeric and both question two and question three have multiple answers don't forget to submit these into Moodle and that's it I hope this was helpful take care hello and welcome to section 2.3 an additional section on analyzing crafts this is not computational in structure this is more of an interpretation section and a better understanding of what makes a good graph and in many ways more importantly what makes a bad graph so the objective is to identify misleading characteristics of a graph with the hope that you will avoid them yourself there we go so title access Source this is how to properly label a graph the middle part really is the only thing done in your statistical program the title and the source and the caption and the numbering is done in your word processing or typesetting program so the bar part this middle part is all that's done in your statistical program the title and the source stuff is done in your word processor um a Time series graph is a line graph that is used to display a variable whose values change over time we've seen this already we called it a line graph in the last section um this should ring a bell as something like panel data from chapter one it should kind of should be able to go back over your notes press pause go back over your notes and see what panel data was this is a way of describing panel data for one or a very small number of people a crosssectional graph is a way of displaying information collected at only one point in time again that should remind us of something we talked about in chapter one about crosssectional data a pictograph is a bar graph that uses pictures of objects instead of bars they look really spiffy but they're really dangerous so let's go ahead and we got a red star so let's go to our first question have you picked a graphs differ from bar charts so again over on your left side of your notes right how do you pick the graphs differ from bar charts answer that hit pause right now and you're back so let's move on so here's an example of how pictographs can be deceiving the one that you're going to see in the newspapers or the one on the right and the one on the left is the quote correct monthly housing there are three at least three things wrong with the the incorrect pictograph for one I'm going to draw your attention to the axis values and compare here two I'm going to say what is the area of the 94 versus the area of the 2006 House on the Left well the 94 I'm sorry the 2006 is double the 94. which corresponds to it actually increasing by double hint it's a ratio level variable so we can meaningfully talk about doubling and having whereas in the incorrect going from 94 to 2006 it doesn't double in size it quadruples in size because not only are you doubling the height but you're doubling the width to keep the right form so our eyes are saying oh my goodness it increased by a factor of four even though we try to focus on the scale on the left and it only increases by a factor of two our mind our our gut reaction to this is it increases by factor four so that's another way that this particular picture graph is deceiving I've given you two question number two what are three things misleading about this particular pictograph and I would recommend that again you right over on the left this question your answer also pause and go back and look at the that particular pictograph see if you can figure out another thing that's misleading about it scaling of graphs another important feature to look out for is that the graph is scaled properly if you stretch your strength shrink the scale on the yaxis the shape of the graph may change dramatically a line that raises gently on one scale may look very steep with a different scale make sure that you choose the correct scale and by correct I mean the scale that forces the graphic to show you this the story of the data in other words make sure that it represents the data well you don't want you to force the data to tell a story your graphic should allow the data to tell its story takes practice so here's an example looking at this quickly really doesn't it doesn't seem like there's anything wrong here consider the graph below the U.S federal minimum wage hour rates unadjusted for inflation what errors can you find in the graph how should they be fixed so the dates along the xaxis so this is a Time series plot also known from last time is a line graph vertical axis is the minimum hourly wage in dollars so as you over time things increase in terms of dollars and it looks pretty awesome because you're minimum hourly wage is increasing over time so what's wrong with this well the fact that in 1956 the dollar doesn't quite doesn't buy quite as much as a dollar did in 19 in 2008. so perhaps we should instead of unadjusting it we should adjust for inflation notice the xaxis does not have a consistent scale the ears are as few as one apart I didn't even notice that my goodness there's one A Part here there's six apart here there's five of Parts six five two four Etc so you should stretch this shrink this so that each inch along the horizontal axis corresponds to a single uh time limit so maybe each of these marks should be three years instead of something so change the x axis correct graph can be found in exercise 12 and the chapter 2 exercises but that's not the only thing maybe you should adjust this for inflation now we're going to talk about some shapes of graphs basic four basic shapes or uniform symmetric right skewed and left skewed the right skewed I will frequently called positively skewed and skewed to the leftover frequently called negatively skewed simply because I have trouble with left and right the the name outliers the data value that falls outside the quote normal shape of the graph or the typical shape of the graph we've already seen outliers back in section 22 the District of Columbia was an outlier in terms of the of the violent crime rate in 1990 its little bar in the histogram is far away from the rest of them so here's what a uniform distribution looks like the frequency of each class is relatively the same if you're dealing with sample data it's unlikely that you'll get bars all the exact same height even if the population has a uniform distribution to it symmetrical data lie evenly in both sides of the distribution whatever that means I want to talk about the skewed two skews then come back to the symmetrical that might make this a little bit more meaningful skewed to the right or positively skewed the key for skew is to locate the tail the tail is the side where the data just keeps lingering on and on as opposed to the in this case as opposed to the left side where it just kind of stops quickly the right side just keeps going on and on so there's a right tail here so this is skewed to the right or skewed positively skewed right because the tail is on the right skewed positively because the tail is on the positive side of the middle contrast that was skewed to the left again the data just keeps going on and on and on it stops rather quickly here so this is left skewed because the tail is on the left or it's negatively skewed because it's on the negative side of the middle with that said symmetric data have neither a right skew nor a left skewed notice here it kind of stops at about the same distance it it's not perfectly symmetrical in the mathematical sense the key and here's the key that I tell people it's in order to if you want to conclude that the data seems symmetrical it's try to determine which side gives you the tail if you have to stop and think a little bit about it like at least a second then it's symmetrical enough we're actually going to in the next chapter come up with a definition of symmetrical and the way determining if the data are quote symmetrical enough but for now just look and see okay are the data obviously skewed right or obviously skewed left and if the answer to both of those is no then it's symmetrical enough so let's describe the overall shape of this distribution the middle seems to be somewhere around here it's doesn't seem to have a tail on either side I mean if I squint it hard I could probably figure out a tail but because I'm not seeing the obvious tail I'm going to say this is symmetric the average is somewhere around 80. I don't see any outliers if there is one bar way way over here I'd say yeah there's an outlier if there's a bar way way over here I'd say yeah there's an outlier um so there's the smooth curve that they're talking about the average seems to be about 80. yeah symmetric no outliers red star so the third one how does one determine the tail of a distribution of a graphic go ahead and hit pause hopefully your answer is written in your notes so that you can later transfer them to the Moodle quiz and that's the end of this rather short chapter or a short section it actually is the end of this chapter so make sure you keep sending me those questions post those questions to the discussion board and I will talk to you later take care hello and welcome to section 3.1 in your Hawks book this is going to talk about measures of center a measure of center is a single statistic that tries to summarize your entire data set so it takes your data set boils it down to one single value um next lecture we're going to talk about how to determine the quality of that single value as a representation of your entire data set but today we're just going to look at how to summarize your data set with one single value and as you can imagine that's going to depend upon what your data actually are what level your data are so topics today calculate the mean median mode determine the most appropriate measure of center so we're moving on to the mean then clearly the median then eventually the mode definition the mean is the arithmetic mean of a variable is one of the most important statistics calculated and provided because it is one of the most important statistics calculated and provided in other words there's nothing special about the mean above and beyond the median of the mode except for four things that I could come up with I mean I racked my brain and these are the four reasons that we should be looking at the mean Carl Friedrich Gauss thought it was important the sample mean is easily calculated at least it's more easily calculated than the other two measures of center um the sample mean can be used to calculate missing data values that is if you have 10 data values you're missing one of those 10 but you have the mean you can figure out what that missing value is I think that might be helpful probably not and the sample mean has a nice distribution which we're going to learn in chapter 7 when we talk about the central limit theorem I think this last one is the best reason of all other than the fact that it's important because we've determined it's important you'll hear this talk again when we get to What's called the normal distribution we'll discover that the normal distribution is entirely the most important distribution in all of Statistics because it's the most important distribution in all of Statistics anyway moving on here's how you'd calculate it is the sample mean X is just going to be a generic variable the bar is what tells us it's the sample mean so X bar is going to be the sum of all the X values divided by the number of X values so this could be the first X Value Plus the second X Value Plus the Third x value dot plus the nth or the last x value divided by n in other words you add up all the data to divide by the number of data values this capital sigma and it's actually a stylized Sigma indicates the summation you're adding up everything to the right with a subscript of I so you're adding up all the i x values and dividing by the sample size contrast this with the population mean okay there's not much to contrast the sample mean deals with the sample population mean deals with the population the sample mean you've got an X bar population mean you've got a mu it's what a Greek cow says mu you're adding up all the values of the of the variable in the population divided by the population size for the sample mean you divide by the sample size for the population mean you divided by the population size again we're adding up all the X values divided by these population size nothing special so we got an example students from our previous stat toner classroom surveyed find out the average number of hours they sleep per night during the term here's a sample of their selfreported responses 56810469 I want to calculate the mean all we do is add them up and divide by seven y seven because there's seven data points so we're going to add them all up and divide by seven by the way 5 is x sub 1 6 is that sub 2 8 is x sub 3 10 is x sub 4 4 is x sub 5 6 is x sub 6. 9 is x sub seven so x sub we're adding up all the X's where I ranges from one through seven and dividing by n 48 over 7 is about 6.9 so the sample mean for the number of hours that the students reported sleeping per night during the semester is 6.9 hours wow that's pretty lazy embed 6.9 hours every day on average come on I'm kidding by the way um so here's how we do it in R we Define a variable sleep we could call this a if we wanted to or X if we wanted to I'm calling it sleep because sleep has some meaning there's the C function it combines everything to the right into a single Vector of values and then to calculate the mean of sleep we just calculate mean of parentheses sleep that's it for a small data set like this whether or not it's easier to calculate it by hand or using R that's up to you but the reality is most data sets that we care about are going to be of size 2 3 4 000 Maybe two three four million and we'll want the computer to do those calculations for us example 3.2 I alluded to this earlier let's go ahead and do it we're going to use the mean to find a missing data value Rutherford who is a famous physicist downloaded five new songs from the internet he knows that on average the songs cost a buck 23. if four of the songs cost about 29 each was the price of the fifth song he downloaded in other words we're missing that the the price of the fifth song but we got the mean and we got the value of all the others let's see if we can determine or bring back that last price so here's the solution we're just going to substitute in the values we know here's the function for the population mean times substituting the values we know Buck 23 was the average of those five values four of those values are a buck 29 we don't know the fifth doing some algebra we come up with that missing song was priced at 99 cents so the cost of Rutherford's fifth song was just 99 cents now this is one of the strengths and where the weaknesses to the sample mean or to the mean in general it's the same thing it equally depends on every single data point that's an observation that's a feature of the formula which could be a strength or weakness here is the strength because we're able to recover a missing value however if that missing value were a an outlier it the mean may not be representative of the data set as a whole sometimes we don't have the actual data but we have summarized data this would lead to us maybe perhaps wanting to calculate a weighted mean or the mean of the summarized data the weighted mean again this will be for the sample there's a bar on top of the variables so this this will be an X bar or a sample mean these are summations the W's represent the weights and the X's represent the observed values so the weighted mean is just going to be the sum of the values times the weights added up divided by the sum of the weights where might this be most important for you a lot of your classes are going to use weighted means for determining your final grade here's an example the syllabus my discrete mathematics class states that the final grade is determined by the midterm homeworks the discussions and the final grade sorry final exam midterm counts 40 towards the final homework 20 discussions 10 final exam 30 towards the final grade so we're waiting the midterm grade by the term exam grade by 40 percent we're awaiting the homework grade by 20 discussions by 10 final exam by 30 percent so there's two students in the class Bob and Virginia want to calculate their final grades or in this specific instance estimate their final grades below are their average grades in each of the categories for the midterm homework discussions and they've also guessed at what they might score in the final exam so midterm homework and discussions they know their grades the final they're going to estimate here it is for Bob he got an 83 in the midterm and 98 on the homework discussions you got 90 percent fantastic he thinks he's going to get an 87 on the final we'll see but he thinks he will if these numbers are correct then what is his final grade for the course this is for genius we'll come back to that later notice that Virginia did much worse than the homework and discussions she's going to think she's going to do much better in the tests so here it is for Bob first thing we need to do is determine which numbers are the values and which are the weights um the grade earned in each category is weighted by the percentage for that category so for instance Bob's test of 83 gets away to 40 percent so that 40 is going to be the weights and the 83 is going to be the values here's a nice little table so these are the scores that Bob got according to the syllabus these are the weights for each of those categories and technically the 87 he hasn't received yet he thinks he's going to get it he got an 83 on the midterm so a good chance of getting 87 at least on the final so to calculate the final grade down here we're just going to multiply the score by the weight we call that the contribution to the final score times weight score times weight Square Times weight add up this last column to get his final grade 83 times 40 percent is point is 33.2 98 times 20 is 19.6 and that should add up to 87.9 so if Bob is correct and he gets an 87 on the final he'll get an 80.87.9 percent for the course which is a B plus if we look at this in terms of mathematics we're adding up all the X's times the W's divided by the W's notice that the W's all add up to 1.4 plus 0.2 plus 0.1 plus 0.3 this top is just each of those values in the last column adding them up gives us an 87.9 there it is for Bob let's look at Virginia the X's are the genius grade in each category we got the the weights let's do this using r so the weights are 40 20 10 30. Virginia's actual grades are for 95 45 66 and she thinks she's getting a 90 on the final she got a 95 on the midterm there's a good chance she'll get a 90 on the final this formula and R corresponds the actual definition of the weighted mean so the sum of the values times the weights divided by the sum of the weights the sum of the values times the weights divided by the sum of the weights from this we decided we find out she Lambert within 80.6 for her final grade that is after we run it this line gives us 80.6 which is a B minus not bad considering she did so poorly on everything but the tests now as an aside from a teaching standpoint let's take a moment and consider the effect on her grade if her test score was low but her homework and quiz scores were high which should come out with the same grade in other words what lessons can you draw from this with respect to your own activity in the course hint hint hints and now extension here are some additional questions that we can answer for Virginia what's the highest grade she can earn what's the lowest grade she can earn and what does she need on her final to earn a 70 percent in the course which is the lowest passing grade well for the highest grade she can earn it occurs when she gets 100 on the final so you put a hundred percent in here you do these calculations and the highest that you can get is a b a low B but a b nonetheless the lowest grade she can earn happens when you put a zero in for the final grade and discover that lowest grade is a 53.6 percent which is an F now the likelihood she gets a zero on the final is is pretty low considering she got a 95 on the midterm so the next question is what does she need to earn on the final in order to pass the class 54.7 is the correct answer how did I determine that well I just kept changing this last number and rerunning this code until I got this last quantity to be 70 percent we got a red star which means we need to go to our intra lecture question one what is the largest number of means that a variable can have again write the question on the left hand side of your notes answer it below so you can put it put it into Moodle later and pause and we're back moving on to the median here's the definition of the median the gut definition first the median is the quote middle value in the sense that about half of the data is above it and about half is below it yeah that's a nice gut definition doesn't work mathematically because the word about so mathematically here's the actual definition of median uh the median of a set of data is a value notice it's a value not V value so it could be more than one median in a data set a value such that at least half of the data is at least this value greater than or equal to and at least half the data is at most this value less than or equal to kind of complicated definition so we go back to the gut definition of okay it's it's the middle value about half's above and about halfs below if we need to calculate it this is the mathematical definition if we want to understand what it actually means we use the get definition so here's how you actually calculate the median of a data set by hand you list the data in ascending order from lowest to highest in other words you're making an ordered array if the sample size is odd the median submittal value in that ordered array if it's even that it's the mean of the two middle values in that ordered array note that this implies at least two things one the median like the mean may not be a value in the data set and two the medium may only be applied to ordinal data if n is odd or if there's if the middle two values are the same level so let's calculate the median given the number of absences for samples of students in two different classes find the median for each sample first step is order one two three four five six seven there's seven data points here so it's going to be the middle data point after we order the data and for B there's eight values so it's going to be the middle of the the arithmetic mean of the two so we put it in order it's going to be six is the median we put an order seven and eight are the two middle values the mean of seven and eight to seven point five that's the median again the median like the mean does not have to be a possible value in the variable here's how we do it in R the function is median and all you have to do is give it the data not too hard ooh red star so let's move on to intra lecture question number two what is the largest number of medians that a variable can have again write it to the left in your notes answer below so that later you can transpose it into Moodle pause and back we are third one is mode the mode is the most observed value if you're dealing with data it's the most observed value if you're dealing with the population it's the most observed value here's how we calculate by hand you just do a frequency distribution remember that from chapter two a frequency distribution the data and find the value that occurs most frequently some terminology if there is only one value that occurs most often it's called unimodal Data where the variable is termed unimodal if exactly two values occur equally option it's bimodal data or the variable is bimodal if it's more than two values that occur equally often it's multimodal however if the data values only occur once or an equal number of times each we say there is no mode so here's some examples finding the mode for the first one I see two sixes but I see three sevens so seven is the mode of a and a is unimodal because there's only one mode for B I don't see anything occurring more than once so there is no mode C I see seven and two both occurring twice so the modes are seven and two and it is bimodal and for D everything occurs twice so there is no mode which is what these Solutions say doing this in R note that the mode is not really a helpful measure of center for most data that we deal with therefore most statistical programs don't have a function for the mode I've created one for you it's called modal that's the function modal and it just takes the data however you need to Source this file before modal is an appropriate function so run these two lines and you'll get the mode of this data set which should be seven will be seven that should be in fact I would recommend that this line would be one of the first that you run in every script for this course and you'll see it more and more frequently example three seven let's bring it all together look at the mean the median the mode for the data here's the data eight data points here we've calculated in R can we load in this data I'm sorry we load in this functionality here's the data the mean the median and the modal of that of that variable remember that the pound sign or the hashtag or the octathorpe or whatever you want to call this indicates a comment R ignores everything to the right of it so this is just a comment to remind me what the mean age is what the median age is and what the modal age is notice the mean is far away from the median and the mode it's probably due to this outlier everything else seems to be really close to 80 except for this 42 year old who retired we can see this on a graph horizontal graph this is actually a Dot Plot we've got our outlier at 42 and it's an outlier because all the rest of the data is so far away from it the mode happens here at 80 only because two people retired at age 80 the median is going to be pretty stable it's going to be really close to the middle of the data set and the mean is going to be really influenced by this outlier which brings us to a question of we've got three measures of center which one should we use for nominal data the mode should be used for ordinal data the mode should be used if it has to be the median should be used if it can be when can it be it's when the either the sample size is odd or the sample size is even and the two middle values are the same in other words we can use the the median for ordinal data when there's no need to actually take an average because average requires at least interval level data for numeric data the median should be used however if the numeric data is sufficiently symmetric the mean can be used and we like to use the mean because mathematically it behaves much more nicely than the median and really that is the only reason we want to use the mean now how do we determine if some data set is sufficiently symmetric we use What's called the Hildebrand rule the Hildebrand ratio is defined as the difference between the mean sample mean and the sample median divided by the sample standard deviation and you'll talk about the standard deviation next lecture if the size of the ratio is less than 20 less than 0.20 and the data are sufficiently submits sufficiently symmetric if H itself is greater than 0.2 then it is positively skewed if H is less than negative 0.2 then it is negatively skewed the variable age is not sufficiently symmetric the ratio is negative 0.312 we know that from running these three lines and actually if we have already run the first two lines in our R session we just need to run the Third since the data are not sufficiently symmetric we should not use the mean we should use the median as it is closer to most values AKA it's more typical of the data then is the mean at least that's what I'm supposed to tell you teaching introductory statistics the reality is you should give both values and interpret both values correctly sample 3 8 and that gives you data on these and just describing the variables which measure of center is appropriate Tshirt size is small medium large extra large well that's ordinal so at least mode if there's no incons no need to to take an average then the median would be appropriate salaries for a professional team of baseball players that would be median most likely um the star will be an outlier the rest of the players will tend to be at a little clump as it was back here the star will be not at the low end it'll be at the high end and everyone else will be clumped towards the middle so this would be a median see price of homes in a subdivision of similar homes since the homes are similar then everything's going to be clumped together so the mean would most likely be correct and Professor rankings from student evaluations of best average and worst that's ordinal so it's at least a mode hopefully we'd be able to use a median and we'd be able to use a meeting if we don't need to take a average of the middle two values oops so now we're looking at graphs a b and c a is the mode it is the most likely value B is going to be the median because about half of the shaded areas to the left and about half is to the right and C is going to be the mean notice that the mean is to the right of B because the data are right skewed if the data were left skewed then C would be to the left of B so here's the summary property is that the mean the median and the mode these are kind of important they just summarize what I've talked about for the last 20 minutes the mean is affected by outliers the median is not affected by outliers I mean there is a single value comedian there is usually a single value the way we calculate it but by the strict definition of the median it's a single value if n is odd and an infant number of values of n is even functions are mean median and modal remember you have to Source in that extra file in order to use modal so I guess that brings us to question three what is the largest number of modes that a variable can have again write this question on the left of your notes write your answer underneath of it so you can transfer that to the Moodle quiz and pause and we're back and we're done the key is not to be able to calculate these by hand you can get the computer to do that for you the key is much more difficult it's to be able to interpret these values what does it mean if the mean is 14 and the mode is 47. what does it mean if the mean is 14 and the median is 47. does it mean that the mean the median and mode are the same value what does it mean if the mean and the median are the same value and the mode the well there are two modes in the key here is to be able to interpret the values and you'll hear me say this over and over again as we move forward in this course the key is interpretation the computer can do the calculation for you you need to be able to interpret what the computer does and in many ways that's harder to do but it makes the calculations much more useful and so thank you and I will talk to you later hello and welcome to section 3.2 measures of dispersion purpose of this section is clearly to determine ways of measuring the dispersion of a data set what do we mean by dispersion we mean how spread out the data are hence these are also called measures of spread we're going to look at four measures of spread in this lecture postponing the fifth one the interquartile range until the next um so here's the gut definition of a measure spread it's a measure of how much we can expect a value of the data to differ from the appropriate measure of center so because of that that means that we've got measures of spread for the mode we cut measures the spread for the median we've got measures of spread for the mean we've got lots of measures of spread all of them trying to Define this and trying to measure the same thing the spread of the data or we can also think of spread and dispersion as being the uncertainty in your data value that is if I've got a data data set with a very small level of dispersion you're much more certain about where that individual value is going to be than if you've got a very very large disperse largely dispersed data set where you got a value and you have no idea if it's to the left tail to the right tail close to the middle Million Miles Away note that there are many many measures of dispersion how one Define spread or dispersion and what properties one wants in such a measure determine the formula that is being used so be aware of that I see a red star so let's pop over to the intellecture question number one what is the main purpose of a measure of dispersion AKA a measure of spread again I recommend writing on the left hand side of your notebook the question your answer underneath of that so that when you do go into Moodle to finish this quiz you'll have the answers right there and of course hit pause and we're back the first measure of spread and the weakest one that we've got is just the range the range of your data is just the largest value minus the smallest value so the range is a single number it's not two numbers it's a single number so if we're looking at Heights of students in this stat 200 class the range is going to be 13 inches 13.2 inches actually so a single number the difference between the highest value and the lowest value that's it since it's based on two specific values the range is going to be very unstable that is if I send three people out to draw a sample from the same population you're going to get different samples by the way then the ranges have a very strong chance of varying quite drastically between the two samples that's what I mean by unstable or it's not robust a much better measure of dispersion is called the variance this one is for the population variance the population variance is the average squared distance of the population values from the mean so I've got my cursor so x i is a data value or a value in the population mu recall is the population mean capital N recall is the population size and we're going to call Sigma squared the population variance that will be the symbol for the population variance and what we're doing is just adding up every value in the population minus the mean we're going to square that add them all up and divide by the population size similar to the population variance is the sample variance this is the one that's actually going to be useful for us notice that the formula looks exactly the same except for two three parts let's call this three parts part one is the symbol for sample variances and S squared for the population variance it was Sigma squared recall that population parameters tend to be Greek letters and Sample parameters tend to be Latin letters so it's s squared for the sample variance Sigma squared for the population variance that's one difference the second difference is you're subtracting off X bar which is the sample mean for the population variance you're subtracting off mu the population mean here we're subtracting off X bar the sample mean and two instead of dividing by n we're dividing by n minus one and we'll explain why a little bit later so again it's each data value minus the mean of the data squared added up divided by m minus 1. and that will give you the sample variance so let's calculate some variances very small simple toy data set here so let's assume the data represent the actual weight changes for a sample of fitness club members so for a we're going to calculate the sample variance and for B assume that the data represent the actual weight changes of every member so for B we're going to calculate the population variance of these five values so again notice the numbers are meaningless unless you add context to them does three two five six four is that the sample or is that the population you need to know by the way in this class in less stated otherwise that's going to be a sample so let's do the calculations here's the function here's the formula for the sample variance we've got the values x sub I which are just the three two five six four X bar is going to be 4 because the sample mean of 32564 is 4. n is 5. which means the N minus 1 is going to be four so all we have to do is take each of those X values subtract off 4 Square it add them all together and then divide by 4. and that's what we're doing here data values the deviations which is the data value minus the sample mean notice if we add up the deviations we get zero which is a good thing the squared deviations the x i minus the X bars yeah the bars there somewhere squared notice we add these up we don't get zero and now if we add these up one four one four zero gives us ten and now all we have to do is divide by little n minus one 10 divided by 5 minus 1 is 2.5 so the sample variance is 2.5 we can do this in r for example of size 5 you may not need to because it's very straightforward putting this table together and doing all the calculations but once we get into sample sizes of fifty hundred ten thousand you're going to want to be able to do this on a computer the function to calculate the sample variance is VAR VAR VAR for variance first line we Define a variable called weight we're going to put inside of weight five values a 3 a 2 a 5 a 6 and a 4. notice you've got the little C to combine all of these values into one variable and then we calculate the variance of weight that's it so now B remember was for a population variance we can do this the long way we would essentially just end up dividing by n five instead of n minus 1 or we can do this on a computer for a sample size 5 and not a big deal for a sample of size 5000 that is a big deal so again here's our data to calculate the population variance you could do the sample variance times four over five n minus 1 over n y n minus 1 over n well let's go back to the formulas there we go we want to calculate this number we've got this number notice the difference is the denominator so we're going to multiply this number the sample variance by n minus 1 here it'll cancel out the N minus ones n minus 1 over capital N so what you'll be left with is just the sum of the x i minus the mean squared over capital n so four over five if you're going to calculate population variance a lot you may want to create a function for it this is the first place that we see the extensibility of r will create a function called varpop the variance of the population I guess we're going to set it equal to this keyword called function we're going to give it just one variable X that'll be the data that's a brace an open brace and that's a closed brace down here and then the actual calculation is just x minus the mean of the x is squared and the average of those it's the average because you're dividing by the number capital n and you would use it as VAR pop of weights you would have to create this function in every script that you need the population variance in thing is we rarely have the entire population we tend to deal with just the sample populations hence the VAR is the sample variance so here's the big question where does the formula actually come from what does it tell us what does it do for us so we can think of variance as an average distance that the value is going to set Lie from the mean maybe they lay from the mean I think they lie from the mean so it's while it's not an actual average when we're thinking of the sample variance here and the variance is usually very close to the average and conceptually it's a good way to think about what this variance is actually calculating um so on your notes I'm sure you got the the variance formula there so to derive this formula for variance we're going to use a method similar to finding average distances or squared deviations from the mean so first we must know the actual deviation from the mean well that's just x i minus X bar that's a deviation that's how far that individual value is from the mean then we're going to square those to get a distance or technically a squared distance and then we're going to find the average of all those how do you find an average well you add them out divide by the sample size well we're not entirely dividing by the sample size we're dividing by the sample size minus one but essentially this is just the deviances squared so it's a squared distance and this part will just be an average squared distance from the data point to the mean so y n minus 1 and this is pretty important the fundamental purpose of sample statistics is to estimate a population parameter so the N minus 1 is there so that s squared is a good estimator of Sigma squared now that's the reason for n minus 1. and then mathematics is the other reason for that um laboratory activity d d as in dog looks at estimators and what makes a good estimator and what do we mean by a good estimator and at that point you'll see oh yeah unbiased estimators are all things equal are good things and dividing by n minus 1 gives us an unbiased estimator of Sigma squared the standard deviation is closely related to the variance standard deviation is also a measure of how much we might expect a typical member of the data to differ from the mean in words definition seems very very similar in fact it's probably exactly the same I like to copy and paste things the only difference is the formula and what we mean by how much we might expect it to differ Sigma squared is the variance Sigma is the standard deviation those are for the population by the way so the population standard deviation is just the square root of the population variance and the sample standard deviation is equal to the square root of the sample variance so the big question comes up if the sample standard deviation the sample variance or the the population standard deviation and the population variance really tell us the same information why are we pardon me why are we introducing the standard deviation and the reason is the units of the standard deviation are going to be exactly the same as the units of your data so if your data are inches your standard deviation units are going to be inches if your data are years your standard deviation data standard deviation units will be years for the variance it'll be the square of that unit which is much more difficult to see on a histogram I can see things that are in the same units of the data because the histogram is in the units of the data but see in terms of the square that's much more difficult to see thus standard deviation is much more interpretable and should be used in all of your papers instead of the variance which brings up a related question of why do we cover the variance first the reason we cover the variance first is because it's much more intuitive as to what it's trying to measure the square rooting of the variance just brings the the units down into what we're used to also mathematicians love to use the variance because variances add standard deviations do not add so mathematicians like the variance everybody else on the face of this planet love the standard deviations ooh we got a red star so question two why should one report the standard deviation instead of the variance again to write the question on the left hand side answer it underneath go ahead and pause and we're back so let's calculate the standard deviation so let's find the stand sample standard deviation of the data shown below yeah I challenge you to do to do this by hand here it is an R just take the data wrap it in the C function send it to the variable since there's no context to what these numbers actually mean we'll just say x is equal to those values and then the function to calculate the sample standard deviation is just SD that's it if you need to calculate the population standard deviation where you're going to have to go back to the function you created varpop calculate the population variance and then take the square root of that sqrt is the square root function but again statisticians rarely calculate population parameters they estimate them but they estimate them with the sample statistics hence SD is the sample standard deviation and VAR VAR is the sample variance here's a use or I guess we're going to say it's an application of the standard deviation um Financial people love to use variance and standard deviation because they indicate risk of an investment so we're looking into investing a portion of recent bonus into the stock market I guess Mark's doing it we're not while researching different companies he discovers the following standard deviations of one year of daily stock closing prices the standard deviation is a dollar two yardsmith it's 9.67 in other words the standard deviation of yardsmith is much larger than that of profacto so all things being equal there is much more variability in yardsmith than there is in profacto so if you want a nice stable stock investment you'll go with profacto instead of yard Smith which is what this solution says hence stable note that looking at standard deviations is just one component of evaluating market prices or youth or plans or things like that another measure of spread is called the coefficient variation the coefficient of variation is just the standard deviation divided by the mean times 100 percent if you're looking at the population coefficient of variation it'll be Sigma over mu if you're looking at the sample coefficient of variation will be S over X bar why do we introduce yet another coefficient variations because coefficient variation is used compared to versions for two variables hence our previous example which I don't want to say in front of anyone from Hawks this is completely useless because if profacto has a standard deviation of a dollar two but it trades at a dollar or that's an incredible change uh incredible width of it whereas yard Smith it's 967 but if it's trading at a thousand dollars a share that's really not much of a variation so while we do like to look at the standard deviations being a measure of risk really it should be the coefficient of variation which this gets to example 314 we got two graphs we're trying to figure out which has that greater standard deviation relative to its mean in other words which has a greater coefficient variation if we ignore the bottom numbers which of these two Graphics is more spread out and it's very clear that the price of U.S Farmland is much more variable if we ignore the bottom numbers much more variable than is the annual average rainfall and that's what the coefficient of variation really does it ignores the bottom numbers and just looks at the shape of the graphs or quantifies how spread out the graph is we can do the actual calculations find out the coefficient of variation for data set a is 28.9 percent and for B it's 36.7 percent that really doesn't surprise us because we just decided that b is much more spread out than a is and that's what again that's what the coefficient of variation is measuring it's how spread out is the data if we ignore the actual values at the bottom and dividing by that X bar allows us to ignore those values at the bottom so those are measures of of spread or measures of dispersion the last two topics for this section really don't sit well with that that idea of measures of center measures to spread they really probably should be on their own section but this is where Hawks is putting it and between the two of us good as places any to put them we're going to look at the empirical rule and chebyshev's Theorem the empirical rule says that when the data follow a bellshaped distribution an interesting pattern emerges in the data values about 68 of the data is within one standard deviation of the mean that is about 68 is between the values of mu minus Sigma and mu plus Sigma about 95 percent is within two standard deviations the mean that is about 95 is between mu minus two Sigma and mu plus two Sigma and almost all of it 99.7 within three standard deviations of the mean and that's all the empirical rule says notice it's when the data follow a bellshaped distribution these are good estimates if the data do not follow a bellshaped distribution these are not necessarily good estimates in fact they tend to be pretty poor so if the data follow a nice little bell shape here then six to eight percents within one standard deviation the mean 95 within two 99.7 within three and almost all is within four so here's an application of it distribution of Weights of newborn babies is bellshaped with a mean of 3 000 grams wow those are heavy babies three thousand grams and a standard deviation of 500 grams so we know that 68 percent of the babies are between three thousand minus five hundred and three thousand plus five hundred 95 within three thousand minus two times five hundred and three thousand plus two times five hundred and ninety nine point seven percent is within three thousand minus three times five hundred and three thousand plus three times five hundred so what the empirical rule says hey what percentage of the newborn babies weigh between two thousand and four thousand grams that's 95 percent because two thousand is two standard deviations below three thousand and four thousands to standard deviations above it so it's within two standard deviations of three thousand what percentage of newborn babies weigh less than 3 500 grams that one's not going to be so easy we do know that 68 are between 2500 and 500 I'm sorry twenty five hundred and thirty five hundred which tells us that 34 is between three thousand and thirty five hundred we also know that half is less than three thousand so we can use that calculation to get the answer and we can calculate the range of birth rates that would contain the middle 68 percent so it's between 2500 and 3 500. now here it is written out since we know the distribution the data is spell shaped we can apply the empirical rule we need to know how many standard deviations two thousand grams and four thousand grams are from the mean here are the calculations it's two below to two above according to the empirical rule approximately 95 of the values lie within two standard deviations the mean so it's 95 percent B takes a little bit more work how many standard deviations the weight of 3500 is away from the mean that's one above so it's one standard deviation above the mean says that 68 of the data values lie within one standard deviation the mean that means that 34 percent lie between the mean and one above and half is below 3000 because it's a bellshaped curve symmetric so the mean and the median are the same so if the mean is 3000 then half will be less than or equal to three thousand so the 34 and the 50 add together to give us an 84 percent of the newborn babies weigh less than 3 500 grams here's a picture of it 2500 to 3500 contains 68 percent since it's symmetric that means between 3000 and 3 500 we've got 34 percent and a systematic distribution so half of the data is less than three thousand so the part that's not yellow is just 50 plus 34 which gave us the 84 percent and then C also an easy calculation we know 68 of the data lie within one standard deviation the mean that's between 2500 and 3 500. and the last topic is chebyshev's theorem whereas the empirical rule is a nice rule of thumb if the data comes from a bellshaped distribution gets you really good estimates chebyshev's theorem is a mathematical theorem that is always correct it's not always helpful but it's always correct and the theorem statement is the proportion of data that lie within K standard deviations the mean is at least one minus 1 over K squared and K's got to be greater than one so if K is 2 then within two standard deviations the mean is at least threefourths could be more than that could be a lot more than that but it's at least threefourths and the proportion of the data within three standard deviations at the mean set k equal to three is about 89 percent so at least 89 of the data is within three standard deviations of the mean could be a lot more than that could be just 88.88889 percent so shabby chefs always works but it doesn't necessarily give us a good estimate empirical rule doesn't always work but when it does is applied to bellshaped data it does give us a good estimate so here we'll apply Chevy Chef's theorem suppose that in one small town the average household income is three thousand thirty four thousand two hundred dollars with a standard deviation of two thousand two hundred dollars what percentage of households earn between 27 6 and 40 800 dollars oh we just have to figure out how many standard Devi deviations both below the mean are 27 6 and how many both mean are forty thousand eight hundred that will tell us what the value of K is and then we need Chevy chev's theorem to determine the minimum or the at least as much so 6600 is the distance we know that the standard deviation is 2200 so that's negative three so 27 600 is three standard deviations below the mean similarly forty thousand eight hundred is three standard deviations above the mean thus K is three and we know that at least eighty eight points nine percent of the data is within those bounds could be a lot more can't be less it absolutely cannot be less than 88.9 percent so now let's compare the empirical rule and chebyshev here the difference is Shelby Chef's theorem always works that is it's always correct an empirical rule requires the distribution is bellshaped the empirical rule tends to give better estimates than does Chevy Chef's theorem Chevy chefs just gives a lower bound empirical rule tries to estimate the actual proportion it's going to be off if the data is not bell shaped but if the data are bellshaped it's going to be a really good estimate especially compared to championships theorem and while chebyshev's theorem always works it only serves as a lower bound hence the at least in the theorem statements and we got a red star so let's move on to question three which is more helpful the empirical rule or chebyshev's theorem again left hand side of your notes answered in your notes so that you can eventually put it into the Moodle quiz go ahead and pause and back and that's it for this chapter I'm sorry that's it for this section measures of dispersion as we move forward in this class the measure dispersion that's going to be most useful for statisticians for some odd reason is the variance or the standard deviation that's just how it is the range note we just got one small page dedicated to it then we kind of left it you can go ahead and ignore the range it's not helpful at all um empirical rule we're going to see pop up again in chapters six seven eight nine ten eleven 12. so you may want to spend a little bit of time learning the empirical rule shabby Chev we're not going to see it again after this chapter so well except on yeah we won't see it again on the chapter so now you know the most important things from this section but again I want you to focus on how to get the computer to calculate these and how to interpret them I also want you to figure out why measures of spread are important especially if we talked about measures of center last lecture how do those two relate how are they different why would one be more useful than the other those are important questions to answer in any class but especially in statistics so that's it I wish you a good day hello and welcome to section three three measures of relative position in some ways this should be section 2 2 and the measures of spread would be section three three because measures relative position are measures of some point in your data or in your distribution very similar to section 31 where we talked about a point in your data or in your distribution but the point for section three one was the mean the median or the mode here it's going to be some other point within your data and so topics from a Saturday you're going to be able to calculate percentiles sometimes these are called quantiles quantiles so you're going to be able to calculate the quartiles then the five number summary which is just all the quartiles together calculate the interquartile range the IQR which is also a measure of spread so that kind of ties into the last lecture we're going to be able to create a box plot and calculate some zscores and that Z scores may not sound too interesting now but those zscores will pop up again in chapters 6 7 8 9 10 11. 12. without zscores that understanding what zscores do we kind of lose a lot of this second half of the course okay so we'll start with percentiles definition of percentile is those hundred divisions in order to calculate a values relative position we can divide the data into equal parts and state in which part the value lies that kind of underscores all of these measures the relative position we may choose to divide the data up into any number of parts for percentiles we're dividing the data up into a hundred parts 100 cents and those divisions when you divide up into 100 Parts is called a percentile here's how you calculate percentiles we're going to locate the data value for the P percentile so you're going to be given the percentile such as the 45th percentile the 97th percentile the 1.75 eighth percentile those values are going to be the p n will be the sample size and this believe it or not is a lowercase l l for location and this is the formula to calculate the location notice it's the location not the data value so the location is just the sample size times the percentile over 100 well that percentile over 100 is just the proportion of the way through the data so it kind of makes sense that this would be a location spot um don't miss something nope so when using the formula to find the location for the percentile's value in the data set you must make sure of the following two rules if the formula results in a decimal value for l the location is the next larger whole number if the formula results in a whole number then the percentile's value is the arithmetic mean of the data value that is located at that location and at the next larger Hmm this kind of sounds vaguely like how we calculate the median which shouldn't surprise us because the median we're going to find out is the 50th percentile so here's an example to see how to do this car manufacturers studying the highway MPG for a wide range of makes and models of vehicles stem and leaf plots given in the next slides there's a lot of it there's 135 data points we need to find the 10th percentile and the 20th percentile so here's the data here's the second page of the data so we got one vehicle gets 12.1 percent one mile per gallon another vehicle gets 13.3 another is 14.1 then 15.5 and 15.6 so that's what statement leaf plot tells us so here's the solution first we need to notice that the data are in an ordered array that is the lowest is 12.1 mile per gallon and the highest is 35.9 miles per gallon there are 135 values so n is 135. we want to calculate the 10th percentile so p is 10 substituting the values we get L is equal to 13.5 notice 13.5 is not a whole number therefore the next larger we round that up to 14 and so the data point that is at position 14 will be the 10th percentile and that data value is 17.3 so let's go back to the data to see that one more time the 14th data value one two three four five six seven eight nine ten eleven twelve thirteen fourteen Seventeen point three is the 14th value therefore 17.3 is the tenth percentile in other words approximately 10 percent of the values in the data set are less than or equal to 17.3 more importantly if we gathered an infinite amount of data 10 of the data values would be less than or equal to about 17.3 for B we're looking at the 20th percentile we still have n is equal to 135 now we're given p is equal to 20 because it's 20th percentile we get 27 as value for l this is a whole number therefore we average the 27th and the 28th data values let's go back to the data 27th and 28th data values one two three four five six seven eight nine 20 19 20 21 22 23 24 25 26 27th is 19.2 28 is 19.3 therefore the 20th percentile would be the arithmetic mean of those two or 19.25 so approximately 20 of the values in the data set are less than or equal to 19.25 and now we need to figure out a way of calculating the percentile of a given data value so now we're given the data value and we need to determine its percentile so for instance we're given 18.9 miles per gallon we need to determine which percentile that corresponds to it's just inverts the previous uh previous equation L is the location of that data value n is the sample size so L Over N is actually the proportion of the way through the data that holds that value multiply by 100 that's the percent of the way through the data that holds the value and by gosh that's the percentile p so if the data from the previous example of the Nissan Xterra average 21.1 miles per gallon what is its percentile well here we go we got to figure out what position 21.1 miles per gallon is in the data set so let's scroll back 21.1 is way down here or way up here I'm not going to count that but I'll let you count it happens to be the 49th value so the value of 21.1 miles per gallon is 49 out of 135 of the way through the data 49 out of 135 is 0.36 multiply that by 100 so that 21.1 is about 36. is about the 20 30. is about the 36th percentile approximately 36 percent of the data values are less than or equal to that is the value 21.1 miles per gallon is in the 36 percentile of the data set oh we got a red star here we go this will be the intro lecture question the first one of this lecture question one what is the percentile again write the question on your notebook answer it so that you can transfer though that question and answer or just that answer to Moodle for the lecture quiz pause and we're back now we're going to look at quartiles recall that the percentiles divide the data of into a hundred whereas the quartiles are going to divide it up into fourths four for court first quartile is about 25 of the data is less than or equal to it second is 50 is less than or equal third quartile is three quarters or 75 of the data is less than or equal to in other words the first quartile is the 25th percentile the second quartile is the 50th percentile and the third quartile is the 75th percentile also recall that the second quartile which is also the 50th percentile is also the median here's a couple ways of calculating or shall I say estimating the quartiles we're going to use a percentile method which we just got through doing a couple examples on the percentile we're going to use the approximation method going this way approximation method is you find the median drop the median you find the median of the lower half to get q1 median of the upper half to get Q3 and look at how these values compare for a large data set the value is going to be very close we already know the data are in order from smallest to largest and we already know that n is 135 . here's the data set so using the percentile method we want to find the 25th percentile so p is 25. do these calculations to find that the location is 33.75 because that is not a whole number we just look at the 34th value which is 19.8 miles per gallon that will be the first quartile which is identical to the 25th percentile second quartile is the median or the 50th percentile thus n is equal to 135 p is equal to 50. that brings us to 67.5 round that up to 68. the median will be the 68th value which is 23.6 the third quartile is to the 75th percentile so p is 75 crack put into the formula 101 rounded up to 102 so it'll be the 102nd value which is 25.3 so the third quartile is 25.3 we can also do the approximation method divide the data in half we get the 68th position which is the median is 23.6 calculate that many times so the second quartile is 23.6 first quartile is going to be the median of the lower half which comes out to be 19.8 and the third quartile is the median of the upper half which is 25.3 they look they're very very close and for large data sets they are going to tend to be close and remember all the times I've said it's approximately this or about a certain percent of the data is less than or equal to when you've got data the best you can do is just approximates or about if you got the entire population you can get exact ly next example finding the quartiles of a data set so we got two data sets we got data set a and data set B we can use the approximation method there's the quartile I I'm sorry there's the median second quartile 70.5 q1 and Q3 can be estimated 65 and 78 . so three of the five number summary is going to be 65 70.5 and 78. using this second set of data can we start the median go through the exact same calculations to get estimated values so three of the five numbers for the B data set are 67 75 and 79. of course we can use R to do these rather quickly give it the data do summary of that data set and this will give you the first and third quartiles it'll give you the median it also gives you the Min and the max which are the other two numbers in the five number summary and it will also give you a mean now that these values are not the same as those received when the approximation method is used so which is correct they both are because remember medians don't have to be unique and that extends to all the quartiles and percentiles they don't have to be unique oops red star question two again write this over on the left hand side answer it beneath how do percentiles and quartiles differ hit pause and we're back five number summary and box plots five number summary consists of the five quartiles the minimum value which is q0 first quartile which is q1 second quartile which is Q2 third quartile Q3 and the maximum which is Q4 five number summaries may put these five numbers listed in order from smallest to largest why do we need the five number summary well it gives us a good feel for the distribution of the data so write the five number summary for the date an example of 3.2 we've done we've calculated the quartiles already all we have to do is figure out what the minimum 12.1 and the maximum 35.9 values are and there's our five number summary minimum q1 Q2 Q3 maximum we are going to illustrate the five number summary using a box plot technically it's called a box and whiskers plot box and whiskers plot this case it's a horizontal box and whiskers plot looks like this it specifies with the minimum value and the maximum values are those are the endpoints of the whiskers and the Box endpoints are q1 and Q3 and the median is indicated by a thick line inside that box the range between q1 and Q3 is called the interquartile range it's just Q3 minus q1 it's a single number about half the data occurs Inside the Box about a quarter of the data occurs above the box and about a quarter occurs below and this is what I just said here's the actual formula for the interquartile range it's Q3 minus q1 here's how we create the box plot we're going to begin with the five number summary we're going to determine a nice little scale horizontal axis that fits all those values nicely we're now going to Mark those five numbers on the on the graphic the minimum q1 Q2 Q3 the maximum we're going to draw the Box in q1 to Q3 the thick horizontal vertical line at the median and then we show the whiskers going out for the box to the Min and the max we can do this in r with the box plot command load in the data and just supply box plot to the data if you want something that looks like this graphic you have to specify horizontal equals true a vertical box plot is the default and the color is orange col for color and that will color the Box slot faster and a lot more accurate than doing it by hand we're going to interpret box plots now we have four sub basins we got the Upper Mississippi the Ohio Tennessee the Missouri and the Arkansas Red River sub basins notice this is not a typical box plot because the top whisker and the bottom whisker are at 90th percentile and the 10th percentile which we see over here we also see the mean as a DOT that will help us see the skew of the data set so for instance Missouri seems to be skewed up because the mean is above the median the Upper Mississippi and the Ohio Tennessee seem to be rather symmetric and the Arkansas Red River seems to be skewed up as well so here are the questions what did the top and bottom bars represent in these box plots according to the key they represent the 90th and the 10th percentiles which sub Basin had the highest median average highest median average the median is the solid horizontal line so it looks like it's the Missouri which set Basin had the lowest average spring lowest average so it looks like the lowest average is going to be the Ohio Tennessee if we're looking at the average it looks like that's going to be less than the lower than the Arkansas Red River if we're looking to medians however the Arkansas Red River is definitely below the Ohio Tennessee and which sub Basin had the largest interquartile range in other words which said Basin had the largest spread to the data that's clearly in Missouri because the box is the largest of all the other three last topic today is our zscores or our standard scores when we can compare two data values from two completely different populations by comparing their relative or their respective percentiles we could also determine how the values relate to the respective means of their data sets zscores do this latter it's called the standard score or the zscore again we're going to see the zscore pop up in chapter 6 7 8 9 10 11 12. the standard Square tells us how far a value is from the mean specifically how many standard deviations it is from the mean the formula for the population standard score that is if you are given the population mean and standard deviation it's just the value you have minus the population mean divided by the population standard deviation for the sample standard score it's x minus X bar the sample mean divided by S which is the sample standard deviation so it's always x minus a mean divided by a standard deviation example mean square of the math section of the SAT test is 500 with a standard deviation of when it's 50. what is the standard score for a student who scored 630 so 630 is X 500 is Mu and 150 is Sigma so the Z is just going to be 630 minus 500. divided by 150. so a person who scored 500 on the SAT now I'm sorry a person who scored 630 on an SAT has a zscore of 0.87 . in other words this student 's test score is about 0.87 standard deviations above the mean Jody scored an 87 on her calculus test and was bragging to her breasts friend about how well she'd done poor Jody she said that her class had a mean of 80 and a standard deviation of 5. therefore she had done better than the class average which is true Jody got an 87 though the average was 80 so definitely above her best friend Ashley was disappointed she'd scored only in 82 on her calculus test however the mean for her class was 73 with a standard deviation of six so who really did better on her test Jody or Ashley and we're going to do this compared to the rest of the class so Jody's zscore is 87 minus 80. divide by 5 and Ashley's will be 82 minus 73 divided by 6. so for Jody it's 1.4 Ashley it's 1.5 thus compared to everyone else in the class Ashley actually did better compare it to everybody else in the class Ashley actually did better Ashley's score was actually 1.5 standard deviations above average whereas Jody's score was only 1.4 standard deviations above average he and here we are calculating zscores using R let's go back to the A let's calculate the zscore corresponding to our first value now there is no zscore function native to R but sourcing this file will give us one we just give it the data and we specify zscore of the data and then in Brackets which data value do you want to get the zscore for if we want to get the first zscore we put a one in Brackets if we leave the brackets off completely and just have zscore of a then we get all the zscores the output for this tells us that the first value has a zscore of negative 1.47196 thus it is about 1.47 standard deviations below average about one half standard deviations below average below because the zscore is negative below average because all Z scores are done with respect to average if the zscore is zero then that person scored average yeah there's one more question question three I must miss the red star again write the question in your notes on the left answer it below when should one use a box plot instead of a histogram this one's going to take you some some thought think about what the histogram tells you think about what the box plot tells you think about the examples that we gave in this class in this lecture for box plots spend some time this is the last part of this lecture by the way spend some time thinking about the strengths of histograms the strengths of box plots and now when should a box plot be used instead of a histogram go ahead and hit pause but we are done and so that's the end of chapter three call in chapter three we're now summarizing our data using numbers in chapter two we summarized our data using Graphics chapter 3 it's using numbers we had some several we had several important numbers that we looked at the mean the median the mode standard deviation the interquartile range we also looked at measures of position the zscore don't forget the empirical rule and chebyshev's Theorem although you could forget Chevy Chef's theorem life would go on the empirical rule is actually very important as is zscore understanding both of those will help us when we get to chapter seven and that's it hopefully this was helpful if not drop me in line hello and welcome to section 4.1 this will be the first section of the chapter four purpose of chapter four is to introduce you to probability Theory the vast majority of chapter four is going to review um it's going to be review of stuff that you learned back in middle school and high school um there are no intralecture questions posted however there are quizzes in Moodle so the answers for the Moodle quizzes for each of these chapter four sections will be not in the lecture got it you write not in the lecture for each of those you get full credit if you don't write it you don't get full credit so the objectives for section four one identify the sample space of a probability event calculate basic probabilities determine if two of answer mutually exclusive and determinive to advance our independent mutually exclusive and independent are two things that you're going to be need to be very careful of if there's a lot of confusion between the two if two events are mutually exclusive then both cannot happen at the same time if two events are independent then having one happen doesn't affect the other so here's some terminology a probability experiment or a trial is a process with a result determined by chance such as flipping a coin each individual result that is possible for a probability experiment is an outcome so there are two outcomes for the coin flipping outcome one is ahead outcome two is a tail the sample space is the set of all possible outcomes for any given probability experiment therefore the sample space is heads and tails or heads comma tails an event is a subset of outcomes from a sample space so an event could be just hit an event could be just tail an event could be head or tail or an event here could be none of the above examples that probably experiments include 50 coin tossing a pair of die drawing a raffle ticket those are the basics in each of these examples there is more than one possible result and the result is determined at random example four one consider an experiment in which a coin is tossed and then a sixsided die is rolled so a we need to list the outcomes of the sample space for the experiment see coin toss then sixsided die is rolled okay list the outcomes in the event tossing a tail then rolling an odd number okay so here are the solutions a each outcome consists of a coin toss and a die roll for example you can get a head and then a three we're going to denote a head followed by 3 as H3 using this notation we've got 12 outcomes 12 possible outcomes and the set of those 12 possible outcomes is going to be the sample space you can get ahead and then a One a head and then a two a head then a three dot dot dot a tail then a five then a tail and a six it's 2 times 6 possible outcomes so the sample space is just those 12 possible outcomes so B we're going to choose the members of the sample space that fit the event quote tossing a tail then rolling an odd number tossing your tail and then rolling out number there's only three odd numbers one three and five so the three possible outcomes of this sample space are T1 T3 T5 next example let's consider the experiment in which a red sixsided die and a blue sixsided die are rolled together use a pattern to help list the outcomes in the sample space with the outcomes in the event the sum of the numbers rolled on the two dice equals six okay many patterns that we could use this is one that they're looking at why would we use a quote pattern here instead of listing all of them out there's 36 possible outcomes six for the first six for the second six times six gives us 36. then we'd have to list out 36 outcomes for the entire sample space here we can just give a pattern for what they're going to be or we can list them all out if we look if we'd like to have these drawings notice the sample space is one and one two and one three and one four and one five and one six and one Etc if all we care about is the sum of the uh numbers that come up then the sample space would be 2 3 4 5 6 7 8 9 10 11 12. because 2 through 12 is the sums I'll keep the sample space in mind note that rolling one on the red dying or two on the blue die is different there are only a two and a one if we care about the actual outcomes however if we care about just the sum then those two are going to be identical outcomes Part B I'm going to list the outcomes the event quote the sum of the numbers rolled on the two dice equals six so we're going to go back and look to see there's one here five one four two three three two four one five those all give us a sum of six so we could actually say the probability of getting a six when rolling two dice is six one two three four five sorry it's five one two three four five five out of 36. a tree diagram can be used to represent the outcomes of an experiment especially in the early phases when you're just trying to learn why it's 6 times 6 instead of six plus six the tree begins with the possible outcomes of the first stage and then branches for each additional possibility we're going to see tree diagrams in the future so it would be good to be able to write these out eventually number of possibilities in the bottom row the tree is equal to the number of outcomes in the sample space so let's consider a family with three children use a tree diagram to find the sample space for the gender of each child in regard to birth order so again ordering here does matter we're doing it oldest middle youngest so here it is the first child can either be a girl or a boy second can be girl boy or girl and boy and then girl boy girl boy girl boy girl boy so the bottom here GGG would indicate that all three children were girls ggp would be a girl girl followed by a boy gbg would be a girl than a boy then a girl notice that this looks like a tree especially if you turn it upside down and so using this tree diagram as a guide we can see there's going to be eight outcomes two times two times two three children so two to the power of three and now let's look at three methods for calculating the probability of an outcome this is also usually thought of in terms of three ways of understanding probability they're subjective the experimental and there's classical what we've used so far is classical what the course is going to be using the future will be mostly experimental sometimes it's called frequency probability or sometimes relative frequency probability in a feature course subjective probability will be renamed Bayesian probability and we'll see that subjective probability is actually the most useful of the three but that's for a a future course way down the line subjective probability according to Hawks is simply an educated guess regarding the chance an event will occur this is Hawks this is not reality but you won't understand the reality until you get into a future stat course and again subjective probability or Bayesian probability will be the most useful but we're not there yet experimental probability or relative frequency probability talks about using data to estimate a probability in an event so if e is an event then P of e the probability that e occurs is given by F over n where f is the frequency and N is the total number times the experiment is performed so if I want to estimate or find the probability of randomly selecting a sophomore from campus I could ask 500 people on campus if they're a sophomore and of those 500 295 said they are a sophomore and would be 500 the number of people I asked and therefore I would estimate the probability of a sophomore of being 295 or 500. and that's going to be the Cornerstone this experimental probability or this relative frequency probability will be the Cornerstone of the second half of the course mainly because I'm going to go the classical probability very quickly indicates a very the classical probability very quickly will become useless we don't know the actual probability of selecting a student who is a sophomore so we just have to estimate it and that estimation and the using of the estimated probabilities is this experimental probability the benefit or the theoretical background that we can use the experimental probability is called the law of large numbers as the sample size increases the proportion or the means of your sample approaches the means or the the proportions of the population and they have classical probability if all outcomes are equally likely and this is actually a very important requirement that the outcomes are equally likely then the probability of e is equal to the number of elements in the e divide by the number of elements in the sample space hence when we were talking about what's the probability of getting a 6 when rolling 2 Die by getting six I mean adding up the dice together of getting it was just five the number of ways of the dice adding up to six divided by 36 the possible number of outcomes example four is identifying the types of probability determine whether each probability is subjective experimental or classical again experimental is also also referred to as relative frequency or just as frequentest classical is also referred to as axiomatic ax iom a t i c probability of selecting the Queen of Spades out of a wellshuffled standard deck of cards is 1 over 52 that's clearly classical there's only one Queen of Spades out of the 52 cards in the deck we know this we don't have to estimate it we're not guessing at it Economist predicts a 20 percent chance that technology stocks will decrease in value over the next year from the information given that will be subjective although this economist may have used data to come up with that in which case it would be experimental police officer wishes to know the probability that a driver chosen at random will be driving under the influence on a Friday night so he records the number of drivers at a roadblock a number of drivers drinking with ba blood alcohol levels of the legal limit yet determines the probability is three percent that is very clearly a relative frequency or an experimental probability because it's based on the experiment that he performed we just discussed that Beck is allergic to peanuts for the next example poorbeck at a large dinner party one evening he notices that the cheesecake options on the dessert table contain the following flavors 10 slices of chocolate 12 slices of caramel 12 slices of peanut butter chocolate and eight slices of strawberry assuming that the desserts are served to a guest at random what's the probability that Beck's cheesecake contains peanuts it's 12 divided by 10 plus 12 plus 12 plus 8. and what's the probability that X dessert does not contain chocolate that's going to be 12 Plus 8 divided by 10 plus 12 plus 12 plus 8. in the first case it's because 12 of those the numerator 12 contain peanut butter and in the second case it's a numerator of 12 plus 8 does not contain chocolate however were I back I would need cheesecake at all because even a 28 chance it's not worth it consider a beginning Archer who only manages to hit the target half the time what's the probability that in three shots the Archer will hit the target all three times that's going to be 1 over 8 or 12.5 percent here's y each time she has a 50 chance of hitting the target so 50 chance to hit the first time fifty the second 50 the third to hit it all three times it's got to hit it the first and the second and the third time here's the tart here's the tree diagram notice that only one of these eight cases does she hit the target all three times so it's one over eighth or 12.5 percent consider a family with six boys what's the probability that the seventh child will also be a boy okay um families do to have a girl but reality States just the opposite if you've got six boys it's most more likely that you'll have a seventh boy however we're supposed to pretend that each time it's a fifty percent chance of getting a boy or a girl um so it's you got six boys followed by girl or six boys followed by a boy of these one out of the two um is a boy so 50 chance again this assumes that the outcomes are equally likely the reality is that if you've had six boys it's more likely that you'll have a seventh boy um it's not 50 chance and also even on your first child it's not a 50 chance that you'll have a boy it's actually a less than 50 chance that you'll have a boy that's a 49 point something percent chance but we're simplifying things here so we because we can multiply and by half very easily um in biology we learned that many diseases are genetic one example as such is Huntington's disease which causes a neurological disorder as person ages each person has two Huntington genes one inherited from each parent if the individual inherits a mutated Huntington genes from from either of his or her parents that person will develop the disease notice it's from either of the parents uh TV show House the character who Dr house calls 13. inherited the disease from her mother so if 13 has a child with a person who has two healthy Huntington G disease what's the probability your child will develop Huntington's quite simply the answer is going to be one half because gonna get a good Gene from the parent from the father and a bad Gene from her 50 percent and that's it for section four one remember go to Moodle take the quiz and for each of your answers write something like this was not asked in the lecture and that's it hope you all have a good day hello and welcome to section 4.5 where we're learning about the addition rules of probability in other words we're looking at probability of the Union of two events so we're going to use addition rules to calculate probability that's the objective for today there are three properties of probability um for any event e is going to be our generic event and the probability V is going to be between 0 and 1 inclusive for any sample space the generic sample spaces can be in a cursive S the probability of being an element of that sample space is one because remember the sample space is a set of all possible outcomes and three for an empty set the probability of empty set is zero so the probability of nothing happening is zero um the complement is a very important concept that you're going to be using in chapter five chapter four really only has two Concepts that are extremely important one is complement the other is going to be the concept of Independence Independence we're going to be using in chapter 11 but complement we're going to be using quite a bit in chapters five and six um the complement of event e has denoted is denoted as e to the power of C it's not really a to a power of it's just superscripted C in some sources this will be a prime in some sources it'll be bar over the e I like the C it's pretty nice it's the complement of e is the set of all outcomes that are not in e so describe the complement of each of the following events a red card out of a standard deck of cards the event is choose a red card so the complement will be choose a not red card which means the component is going to be choose a black card out of a standard deck of cards pardon me out of 31 students in your stat class 15 are out sick with the flu so the event e is being out sick with the flu the complement of e is going to be not being out sick with the flu so in this case e complement will be able to attend class or not being sick your area 91 percent of phone customers use phone South so the event is customer using phone South the complement will be customer not using phone South so be the complement e complement will be customer using some something other than phone South that's what all of the solutions tell us the complement rule for probability and this is the most important part is that the probability of an event plus the probability of its complement is one this is kind of like put a star next to it in your notes Circle it happy face around it because this we're going to be using a lot in chapter five for some problems and this is actually why we're going to use it on chapter five for some problems the probability of complement is much easier to calculate than the probability of the vent itself this is especially true when you're dealing with infinite sets or infinite sample spaces for these problems you can calculate the probability of the complement and subtract that value from one here's an example you're worried that there are there is a 35 chance you'll fail your upcoming test what's the probability that you will pass the test it's going to be the complement of failing so it's going to be 1 minus 35 percent if there is a five percent chance that none of the items on a scratch off will be a winner what's the probability that at least one who will win well at least one means the probability of a one or a zero I'm sorry at least one that's the probability of a one a two a three a four or five all the way up there so that's going to be 1 minus the probability of zero so here's the solution for the first 65 percent pretty straightforward the second is much more complicated probability of at least one winner is one minus the probability of no winners and we're told the probability of no winners is five percent which means that the probability of at least one is going to be 95 percent I do want to emphasize here make sure you understand that no winners and at least one winners are complementary events so pause until you understand that role of pair is standard sixsided dice what is the probability that neither die is a three that's gonna be pretty straightforward to calculate if you use complements if you do it Brute Force then you're going to have to list out all 36 possibilities and determine how many of those neither has a three or we can just use compliments so these are the outcomes that have a three remember e is not having a three so e complement will be having a three so there are 11 elements in E complements so the probability of being an e complement is 11 over 36. therefore the probability of being an e is 25 over 36. which is about 70 percent so you have about a seventy percent chance of rolling two dice and not getting a three on either one the original addition rules for probability this is the addition rule for probability there is a simplification to it that may be useful or may be allowed but this is the rule and the probability of given to events the probability of being in event e or in event F or both is just the probability of being an e plus the probability of being an F minus the probability of being both Cerise is looking for a new condo to rent her realtor provided with the following list of amenities for 17 available properties there's a list close to the subway with six seven were low maintenance fee five had green space two were newly renovated close to the subway and low maintenance were two so one and two there were two in that which meant that there were yeah um Green Space and Lira innovated was just one if cerise's realtor selects the first condo they visit at random what's probably the property is either close to a Subway or has a low maintenance fee close to assembly we'll call e low maintenance V we'll call F so let's verify that the realtor has accurately counted the total number of properties remember there were 17 properties six where if type one seven were a type 2 5 or type three two were type four which gave us 20 but we know that there is a overlap of three we were told what this overlap was to are both low maintenance and close to Subway and one is both newly renovated and green spaced so 17 individual properties we're going to use the or that tells us we'll be using the addition rule e or F that's equal to probability of e plus a probability of f minus the probability of both There are 16 that are close to a Subway seven that are low fee and we're also told that two are both so six plus seven minus 2 is 11. so about a 64 chance suppose that after a vote in the U.S Senate on a proposed health care bill the following table shows the breakdown of the votes by party 23 versus 21 43 versus 7 2 versus 4. if a lobbyist stops a random Senator after the vote what's the probability of the senator will be either a Republican or voted against the bill so either a Republican or evaluate against the bill that's just going to equal the probability of it being a Republican 50. plus the probability of voting against the bill 32 minus the seven that were counted twice divided by 100 or whatever Republican voted against the bill counted twice gives us a 75 chance the lobbyists will have dinner with the senator he wants roll a pair of dice what's the probability of rolling either a total less than four or a total equal to 10 total less than 4 will be e total equal to 10 would be f we want the probability of e or F that's just the probability of e plus the probability of f minus the probability of E and F less than 4 plus probability of 10 minus the probability of less than 4 and 10. well what's the probability of it being both less than 4 and equal to 10. well that's zero because this is the empty set and from our uh three requirements for probability that means that this probability is equal to zero and we just pay attention to the first two in other words less than four and ten are mutually exclusive events one can happen the other could happen but both cannot happen so there's three that are less than four there's three that are ten so it's going to be six out of 36. again the key these two events are mutually exclusive and that leads to a zero percent chance of both occurring if events E and F are exclusive and the addition rule is much easier and apparently Hawks didn't give us the formula it's just the probability of e plus the probability of f there's no need to subtract off anything because what you're subtracting off is just zero Caleb is very excited that it's finally time to purchase his first new car after much thought he's narrowed his choices down to four because it's taken him so long to make his mind his friends have started to bet on which car he will choose they've given each car a probability based on How likely they think Caleb is to choose that car Devin's betting that Caleb will choose either a Toyota or a Jeep find the probability that difference right and here's the probabilities that they've assigned probability of a Toyota or a Jeep it's just going to be the probability of a Toyota plus the probability of a Jeep minus the probability of a Toyota and a Jeep but that probability of both is zero because he's purchasing his first new car only one car so Devin has a 75 chance of correctly picking which car Caleb will buy example 416 we're going to extend the addition rule to more than just two events we got these probabilities and we're asked what's the probability that the driver will refuel at Shell Exxon or Chevron shell Exxon or Chevron these are mutually exclusive events therefore the probability is just going to be the sum of the individual probabilities the probability of the genuine is the sum of the probabilities if the events are mutually exclusive and that's the end of first chapter section 42 again don't forget to complete the quiz in Moodle and for each of the questions write something like this was not covered on the lecture and that's it take care hello and welcome to section 4.3 this is the third section in chapter four probability Theory here we're looking at the multiplication rules for probability which eventually will lead to conditional probability and Independence so we're going to use multiplication rules to calculate probability um multistage experiment is experiment with two or more steps or stages we've seen examples of this the roll the die and then flip a coin is a multistage experiment the flipping the coin three times is a multistage experiment in fact it's a threestage experiment an experiment performed with replacement refers to placing an object back into consideration before performing the next stage of the experiment those examples they just gave you were quote with replacement because you could end up with actually the first one with roll the die and then flip a coin it's not with experiment because once you roll the die you can't get a one two three four five or six for the second term we did have an earlier example of the probability of getting a Queen of Spades If all we're doing is drawing one card then we don't need to talk about with or without replacement but if we're talking about drawing two cards what's the probability that either one is a Queen of Spades well now we got to think about am I putting the first card back after I draw the second first card or do I keep that first card in my hand if I put the card back it's with replacement and the calculations tend to be a little bit easier when you're doing things with replacement if I hold on to that first card then it's without replacement and the calculations get a little bit more difficult not too much two events are independent if one event happening does not influence the probability of the other event happening um so that's independent multiplication rule for probability of independent events and I want to emphasize this is for independent events right now if enf are independent then the probability of E and F occurring is equal to probability of e times the probability of f that's it I suppose two cards from a standard deck I choose two cards from a standard deck with replacement that means I draw a card look at it put the card back reshuffle draw a second card what's the probability of choosing a king and then a queen well since I put the card back the outcome of the first doesn't impact the outcome of the second draw so these are going to be two independent events so it's just going to be the probability of choosing a king times the probability of choosing a queen so that's going to be 4 out of 52 times 4 out of 52 or 1 13 times 1 13 which is about 0.6 percent chance assume that a study by human resources has found that the probabilities of an employee being written up for the following infractions are the value shown in the following table so this is probability of being written up at work um and we're going to assume that each infraction is independent of the others this is given information to us we would have to know that they're independent we can't just look at the problem and understand hey they're independent we have to be told they're independent what's the probability that a given employee will be written up for being late to work taking unauthorized breaks and leaving early late to work unauthorized breaks leaving early so we're looking at the probability of late work and unauthorized breaks and leaving early so it's just the product of those three probabilities because these are independent events so about one percent chance set a different way about one percent of the of the people at that company get rid of for being late to work taking on authorized breaks and leaving early that seems rather seems rather High to me um an experiment performed without replace it means that the objects are not placed back into consideration that means that the two draws are going to be dependent most likely to events are dependent if one event happening affects the probability of the other event happening so we're looking back at our king and queen example we want to know the probability of drawing a king and then a queen if the cards are drawn without replacement so this is going to be a good one the situation is essentially the same as drawing two cards from a standard deck so instead instead of thinking let's draw one card and then draw the other you can think of this as just drawing two cards and you're asking what's the probability of the first card I drew being a king and the second being a queen by determining the probability of drawing a king from a snare deck of cards Begin by doing that it's just one out of 13. now let's assume that when I drew the first card it was a king so given the first card is a king what's the probability that the second one is a queen huh it's there's four Queens in the deck but the deck only has 51 cards left in it because I didn't put that King back there's only 51 cards left in the deck thus the probability of a queen given that the King was drawn first without replacement is 4 out of 51. so the probability of getting a king and then a queen is just gonna be the product of about 0.06 I'm Sorry by about point six percent now what we've actually done is we've started talking about conditional probability without telling you that we're talking about conditional probability here's the the key that this is conditional probability it's the word given conditional probability denoted by probability of f given e that vertical bar is read as quote given is a probability of event F occurring given that the event e occurs first is event E and F are independent then the probability of f given e is just probability of f which leads to a a nice definition of independence for us one card's already been chosen from a standard deck without replace and what's the probability of now choosing a second card and it being red given the first card was a diamond so we're asked what's the probability of being second being read given the first was a diamond red given diamonds just 25 over 51 because we didn't put the card back there's only 25 cards that are red when the first card was a diamond which is also Red by the way so now here's the actual multiplication rule for probability given to events E and F the probability of E and F occurring is just the probability of e times the probability of f given e which is identical to the probability of f times the probability of e given f these two formulas are exactly the same the one you use depends on the data that's given to you so it's a probability of choosing two face cards in a row we're going to assume the cards are chosen without replacement here we're dealing with dependent events so we'll use the multiplication rule and the first card is picked all 12 face cards are available at a 52 so the probability the first one is a face card is 12 out of 52. now given that the first one is a face card then there's only 11 left that are face cards out of the 52 out of the 51 cards remaining in the deck so the total probability will be 12 over 52 times 11 over 51. which is about a five percent chance assume that there are 17 men 24 women in the Rotary Club two members are chosen at random each year to serve on the scholarship committee what's the probability of choosing two members at random the first being a man and the second being a woman we're choosing two members the first choice will influence the probability of the second there's 41 people all together so the probability of a man and woman is just a probability of man times the probability of a woman given the first was a man probability of man is 17 out of 41. so given the first one was a man the probability of the second one being a woman is just 24 out of 40. it's 24 because there's 24 women in the Rotary Club it's 40 because there are 40 that are remaining after the first one was chosen so there's about a 1 4 chance we can also write conditional probability and this is sometimes referred to as the definition of conditional probability the probability of f given e is the probability of E and F divided by the probability of E if E and F are independent from the probability of E and F is just the probability of e times the probability of f probability of E's cancel out so we're given the probability of f given e is just equal to the probability of f and that's a very nice definition of Independence out of 300 applicants for a job 212 were female and 110 are female and have a graduate degree what's the probability that a randomly chosen applicant has a graduate degree given she's female so now we're told the person who God is female what's the probability that she has a graduate degree 212 female 110 female graduate degree so this probability is just going to be 110 over 212. if 152 of the applicants have graduate degrees what's the probability that a rambly chosen applicant is female given the applicant has a graduate degree 152 is the denominator 110 is the numerator so the probability is just going to be 110 over 152. he is female and graduate degree divided by female female and graduate degrees divided by graduate degree the emphasis here is that probability F given e is not necessarily the same as probability of e given F order matters with conditional probability the reason order matters is you're given different information probability F given e you're given that e is true and you need to calculate the probability of f or as in probability of e given F you're given f is true and you need to calculate a completely different probability probability of E and now for the fundamental counting principle for a multistage experiment with n stages where the first stage has K1 outcomes the second stage has K2 outcomes the third stage has K3 outcomes and so on the total number of possible outcomes is k1 times K2 times K3 times dot dot dot times kn we've already seen this um with the roller die then flip a coin K1 was 6 K2 was two the total number of outcomes was 12. 6 times 2. flipping the coin three times okay one was two k two is two k three was two two times two times two is eight there are eight possible outcomes rolling two dice K1 with six K two was six six and six is 36 there are 36 possible outcomes so we've already experienced this we're just giving you a nice name for it Kilby begins her first year in an online degree program in July the first semester she'll randomly be assigned to one section for each of four different core courses if there are eight English one sections 12 college algebra sections 11 American history sections and five phys Ed physical science sections how many different options are there for Kilby's schedule for her first semester oh it's just eight times twelve times eleven times five from the fundamental accounting principle fifty two eighty the governing board at a local charity Mission Stateville is electing a new vice president and secretary to replace outgoing board members if the board consists of 11 members who don't already hold an office how many different ways can the two positions be filled if no one may hold more than one office go ahead and hit pause to figure this out and you're back two slots to fill it's going to be without replacement because you can't hold both offices there are 11 choices for the first position 10 choices for the second that leaves 110 possible ways to elect the new officers example Robin is preparing an afternoon snack for her twins Matthew and Laney she wants to give each child one item she has the following snacks on hand carrots raisins crackers grapes apples yogurt and granola bars if she randomly chooses one snack for Matthew and one snack for Laney what's the probability each child gets the same snack as yesterday here's the solution probably want to hit pause to come up with the solution yourself before you read the solution here you need to count the number of ways in which Robin can randomly choose a snack for her twins again we can think of this as two slots to fill one for each twin seven possibilities for each child there's no requirement that the twins have different snacks or conversely have the same snack so the total number of ways she can prepare the snacks is seven times seven now we need to count the number of ways that she can choose the same afternoon snack as yesterday there's only one thing they had yesterday so the probability is going to be one divided by the 49 total number of snacks so about two percent chance and that's it thank you very much hello and welcome to section 4.4 this is the fourth section of chapter four we're going to look at combinations and permutations these are just functions that allow you to quickly calculate total number of possible outcomes um so the objective calculate numbers of permutations and combinations before we get to that we have to Define what a factorial is a factorial of n a positive integer denoted by n exclamation point or called n factorial is just the product of n times n minus 1 times n minus 2 all the way down to one so one factorial is equal to one two factorial is equal to two times one or two three factorials equal three times two times one which is six by agreement 0 factorial is equal to one you might want to be careful on that one so let's calculate the following factorial expressions a is 7 factorial it's just going to be seven times six times five times four times three times two times one B is 4 factorial over zero factorial that's numerator is going to be four times three times two times one denominator is going to be just one C 95 factorial over 93 factorial that's just going to be 95 times 94 y because 95 factorial is equal to 95 times 94 times 93 factorial and then you're dividing off by that 93 factorial so you're left with just 95 times 94. D the numerators can be 5 times 4 times 3 times 2 times 1 denominator well 5 minus 5 minus 3 is 2. so the denominator is just going to be 2 times 1. 2 factorial e numerator is going to be 6 times 5 times 4 times 3 times 2 times 1. denominator is going to be 2 times 1 times 4 times 3 times 2 times 1 because 6 minus 2 is 4 . so 7 factorial is 50 40. 4 factorial over 0 factorial is 24. 95 factorial divided by 93 factorial is 95 times 94. 5 factorial over 2 factorial is just 60. and 6 factorial over two factorial times four factorial is 15. now we're going to define the definite difference between a combination and a permutation a combination let's do with permutation first a permutation is a selection of objects from a group with the arrangement matters um combination the arrangement doesn't matter it's an example when you would use a permutation is you want to select a president vice president secretary from a group of people the actual positions are named and they matter combination would be I want to select a a group of three people from a larger class of 50. I'm just selecting three people in this group by not naming the positions it's just three so the arrangement matters that leads to permutations the arrangement is irrelevant that leads to a combination so the calculations are very similar ish and when order is not important the following formula is used to calculate the number of combinations um it's NCR or n choose r is just n factorial the larger number divided by R factorial times n minus r factorial so if I want to choose a group of three people out of our class of 30 and this 30 R is 3. so it'll be 30 factorial divided by 3 factorial times 27 factorial when the order is important the following formula is used to calculate the number of permutations so if I want to choose a president vice president and secretary from our class of 30. and it's again going to be 30 R again is going to be 3 but the number of permutations just n factorial over 27 factorial notice the number of combinations cannot be larger than the number of permutations and the number of combinations will be equal only when R is equal to 1 or 0. in other words when R is one or zero order is not important versus order is important that means the same thing because there's only one or no position to fill it's like choosing a president from our class of 30. whether or not the order matters within that one position it's not a question to ask because there's only that one position given a group of three friends how many ways can you arrange the way that they stand in line for the movies standing in line matters the order matters that so it'll be three factorial divided by three minus three factorial how many ways can I choose two of them to write in a car together I'm just choosing two I'm not saying one get shotgun the other doesn't I'm just choosing two so a is going to be permutations B will be combinations that'll be three choose two so there are six ways that they can stand in line here's the actual listing of those six ways as for the second part we're just choosing two from a group of three so it's called three choose two 3 factorial 2 factorial and then three minus 2 factorial so you can also think of this as the total number of way a total number of of people factorial divided by the number of people in the group you care about factorial times the number of people in the rest of the group we start with three friends we want two in the car this is what's not in the car and here are the possible options for who gets to ride class of 18 fifth graders is holding elections for class president vice president secretary how many different ways can the officers be elected since we've actually named these positions this is going to be permutations R is 3 n is 18. it's almost five thousand ways consider that a cafeteria is serving the following vegetables carrots green beans lima beans celery corn broccoli and spinach Bill what wishes to order a vegetable plate with three different vegetables how many ways can this plate be prepared well this is one where order doesn't matter this is going to be one two three four one two three four five six seven will be n 3 will be R so this will be seven choose three thirtyfive suppose that a little league baseball coach is randomly listing the nine starting baseball players in batting order for their second game at this level the batting order is randomly chosen to give all players an opportunity to experience different batting positions what's the probability that the order chosen for the second game is exactly the same as that of the first game since we're focusing on the order itself this will be permutations and and R are both nine and we're looking at the probability so the denominator is going to be n permute nine I'm sorry 9 permute nine and the numerator is going to be 1 because there is only one ordering that we had yesterday 9 9 is equal to 9 times 8 times 7 times 6 times 5 times 4 times 3 times 2 times 1 which is 9 factorial and zero factorial is one so this is nine factorial or nine permute nine so there are that many possible batting orders the probability of getting the exact one from yesterday is just one divided by that so pretty low probability Maya has a bag of 15 blocks Each of which is a different color including red blue and yellow I reaches into the bag and pulls out three blocks what's the probability of the block she has chosen or red blue and yellow from the way this is stated this looks like a combination because it doesn't say that the blocks have to be in that order that she first pulls out a red then a blue then a yellow just that there are red blue and yellow out of those 15 so it's going to be 15 choose 3. 455. how many combinations contain the red blue and yellow blocks order does not matter if there's only one way to choose those colors that's the probability that my choose is red blue and yellow is calculated and such just one over that last topic will be special permutations special permutations involve objects that are identical number of distinguishable permutations of n objects of which K1 are alike K2 are alike and so forth is given by this we've actually already dealt with this in terms of permutations we start out with n in our permutations and we divide by K that belong to the group we care about and N minus K belong to the group we didn't care about if we only have two groups then K2 is just going to be n minus K1 or n minus r make sure that your K1 K2 all the way as up to n Tennessee good example Mississippi is the other usual example how many different ways can you arrange the letters of the word Tennessee notice that the n's the E's the s's are all the same as each other there's one two three four five six seven eight nine letters in Tennessee there's one t four e's two n's and two s's which means the total number of ways is going to be 9 factorial divided by 1 factorial times 4 factorial times two factorial times two factorial 3780 and that's it again Mississippi is the other usual example how many ways can you arrange the letters in Mississippi if the I's the S's and the P's all look the same as the other eyes S's and P's definitely something to think about so that's the end of this section I think there's one more in the course I mean one more in the chapter enjoy hello and welcome to section 4.5 this will be the last section of chapter four and probability theory in this section we're going to combine the probability techniques of the first two and the counting techniques of the last sections into one nice little ball of of probability so we're going to use the basic counting rules to calculate probabilities in this section we'll look at counting problems that have more complicated Solutions than just one over the total number of possibilities a helpful trick for these problems look first to look for certain key terms the key terms are important because they Define the method to be used to find the correct answer words to look for include at least at most greater than less than between Etc so here we'll start out with a nice little example a group of 12 tourists is visiting London at one particular Museum a discounted admission is given to groups of at least 10. so a how many combinations of tourists can be made for the museum visit so that the group receives the discounted rate in other words we want to find the number of ways of having 10 and 11. and 12 chosen out of that group of 12. and B suppose that a group of tourists does get the discount what's probability was made up of 11 tourists in other words in a we got the total number of ways that you can get the discount and now we're given that they got the discount what's probability that it was made up of 11. so here's the solution keywords in this problem are at least so if at least 10 are required in the group that gets the discount has to be 10 11 or 12. now we calculate the number of combinations to get 10 number of combinations to get 11 and number of combinations to get 12. then add them to back add them together and notice that these are combinations because we aren't putting the tourists in any line we're just getting groups of those tourists so here's how to get the group of 10 tourists it's 12 shoes 10. group of 11. 12 choose 11. remember that 12 by the way and 12 choose 12 which is just one as we know the question applies that the group will get a discount if 11 a 10 11 or 12 so the number of ways is just the sum of those 66 plus 12 plus 1 so there is 79 ways that this group can get that discount now B we need to calculate this probability so we're given the group receive the discount what's the probability that it's a group of 11. so the numerator is going to be the number of ways of getting groups of 11 and the denominators are going to be the number of ways of getting the discount well we've already figured that ways of getting a discount is 79 and you figure out the ways of getting a group of 11 is 12. so the probability is going to be 12 over 79. Jack is setting a password on his computer he's told that his password must contain at least three but no more than five characters he may use either letters or numbers at least three but no more than five characters wow so a how many different possibilities are there for his password if each character can only be used once notice that this is without replacement and then suppose that Jack's computer randomly sets his password using all the restrictions given above what's the probability that his password is an arrangement of the letters in his name Jack so here's the solution for a it's got to be at least and no more those are some keywords there at least three no more than five so it's a number it's got to be three four or five so we gotta count the number of ways you can get three a number of ways you can get four and the number of ways you can get five the order of characters is important so this will be permutations there's 36 characters to choose from 26 letters 10 digits so here's the calculation for three it's 36 permute three for four characters it's 36 permute four and for five characters it's 36 permute five so the total number of possible passwords is just the sum of those which is about 46.7 million possibilities to find the probability of a randomly chosen password would include only the four letters from Jack's name find the uh we got we use the number from part A is the denominator that's the N of s that's our sample space I'm going to find the numerator the number of ways you can get the event calculate the number of permutations of four from a set of four four is Just 4 factorial which is 24 so the actual probability is that 24 divided by the total number of possible passwords so it's rather small now the way that they calculated this is never mind um it's n of e divided by n of s we don't need to know what e and s are except into calculating what n of e and N of s are Tina is packing your suitcase to go on a weekend trip she wants to pack three shirts two pairs of pants and two pairs of shoes she has nine shirts five pants and four pairs of shoes to choose from how many ways contain a pack or suitcase we do need to assume that everything matches which means that we are in the independent Realm this will be nine choose three times five choose two times four choose two because in the nine charts she's choosing three of the five pairs of pants she's using two and of the pairs of four pairs of shoes she's also choosing two so there's the number of ways that she can select her shirts number of ways that she can get her pants and every way she can get her shoes so the total number of ways from the fundamental accounting principle is just that product 84 times 10 times 6. I do want to emphasize the word and here implies multiply it's and being you have to choose this and choose those pants and choose those shoes in probability or indicates adding and indicates multiplying here's an interesting little drawing that might help see this shirts pants shoes they're independent of each other because we've specified everything matches there's 84 ways of getting her shirts 10 of her pants six of her shoes so by the fundamental accounting principle it's just the product of those an elementary school principal is putting together a committee of six teachers a committee of six teachers at the Springs Festival there are eight first grade nine second grade and seven third grade teachers at the school how many ways can the committee be formed notice we're not specifying that any that the committee has to contain a certain number of any of the grades there's eight 17 24 tshirts held together so the number of ways the committee can be formed is just 24 choose six how many ways can the committee be formed if there must be two teachers chosen from each grade it's going to be eight choose two times nine choose two times seven choose two for the same reason of Tina's suitcase and suppose the committee's chosen at random with no restrictions so we're in the case of a what's the probability that two teachers from each grade are represented so the denominator the bottom is going to be a and the top is going to be B because a is the number of possible outcomes and B is going to be the number of possible outcomes in the events 24 choose six eight choose two times nine choose two times seven choose two and then the probability will be just be this 21 168. divided by the total number n of s of 134 596. so even if you're not focusing on making sure that the committee consists of two first grader teachers two second grade teachers to third grade teachers there's still a 15.73 chance that it's going to happen naturally just out of randomness and that's it notice how this this section took all of our combinations and permutation stuff and used it to calculate probabilities of events and it comes back to n of e divided by n of s of course we are assuming Independence and all of those outcomes are equally likely but it's n of e Over N of s n of e Over N of s and that's it the end of chapter four the next lecture will be on specific probability distributions and things that we can do with discrete distributions so stay tuned hello and welcome to section five one this is the chapter or this section introduces the chapter that deals with discrete distributions this chapter is going to cover some named and therefore important discrete distributions this section is just looking at some basics of discrete distributions so by the end of the selection you should be able to understand the difference between discrete and continuous distributions or discrete and continuous random variables chapter 6 will deal with the continuous ones to know the purpose of the probability Mass function explain the three requirements for a function to be a probability Mass function that's that's an important one calculate probabilities using the probability Mass function determine sample space sample space also very important because it helps give you a better understanding of which distributions the data could actually come from and then calculate the expected value and variance of a distribution expected value and standard deviation will be the important ones the standard deviation is just the square root of the variance so let's start with the definition of what a random variable is a random variable is a variable whose numeric value is determined by the outcome of a probability experiment you can think of this as an outcome from the future some examples statistician's favorite flavor of ice cream you don't know that until you actually measure it ask the statistician a student's level of approval of a congressional decision you don't know that until you measure it AKA ask the student the Euro Knox college professor is born you don't know that until you ask until you perform that experiment so all of these are random variables Iran variables have or they follow probability distributions it's this fact that they follow probability distributions that allows us to understand the randomness of a random variable and I I want to say that just because it's random doesn't mean we don't know anything about the possible outcomes in fact we know a lot about those possible outcomes we don't know what the exact outcome is going to be but we know everything about it we know the expected value we know the uncertainty in that estimate we know the medians we know probabilities of specific outcomes so we know everything about that future outcome except for what that future outcome is going to be um these are the three requirements for a function to be a pmf all the probabilities have to be between zero and one this is true in general probabilities can't be greater than one can't be less than zero the sum of the probabilities of the sample space is one um cursive S is called the sample space it's the set of all values of x that can happen that have a nonzero probability of happening um so this notation this is summation over all the values of X that are in the sample space of that probability of the value the little X is going to be the value the big X is just going to be big X's these are random variables sometimes we'll have Big Y for a different random variable so the sum over the entire sample space of the probabilities will be one and the probability of a union is no more than the sum of the individual probabilities this comes from chapter four so the probability of a union B is less than or equal to probability of a plus the probability of B for events A and B they are equal if the intersection of A and B is empty it's less than if the intersection is not empty so let's create a probability Mass function for this experiment flip a coin three times count the number of heads flipped so the outcome or the random variable is going to be the number of heads flipped in those three coins from this we know that the sample space is going to be 0 1 2 and 3. how do we know that it's 0 1 2 and 3 well we're flipping the coin three times so the largest number of heads we can get is going to be three hits and the smallest will be zero and they're going to be counts this is discrete so we can expect it to be some sort of counts so the sample space is the set 0 1 2 3. second step is to determine the probability of each of those four outcomes we're going to rely on two assumptions I mean three the coin has two sides that'll be one two the coin is fair and the flips are independent coin is fair indicates that the probability of a hit is one half flips are independent means that the outcome of one flip is not going to influence the outcome of another so if these are true there are eight possible outcomes of the flips not of the random variable but of the flips you can get three tails and get Telltale head tail head tail head tail tail you can get head tail head tail head tail head head or you can get three hits the random variable is the number of heads so if you get Telltale tail you have zero if you get hit head you get three if you get tail head tail you get one probability of getting three tails is one half times onehalf times onehalf which is 1 8. so the probability of the outcome 0 is 1 8. similarly the probability of the outcome three is 1 8 because the only way to get three is head head onehalf times one half times onehalf these three outcomes from the flips give you the same random variable value of one because they all have one head probability of tail tail head is 1 8. probability of tail head tail is 1 8 probability of head tail tail is 1 8. so the probability of getting one head is 1 8 plus 1 8 plus 1 8. probability of getting two tails I'm sorry of getting two heads is just head head tail head tail head tattoo head 1 8 plus 1 8 plus 1 8 3 8. in other words there's three ways of arranging these outcomes so you get two heads times one half times onehalf times onehalf here's another way that we can represent this this will be graphically sample space is listed along the bottom the height of the bar corresponds to the probability of that specific outcome we could represent the probability Mass function in this way as well probability of the random variable equaling some value some specified value is 0.125 if this x is zero or three it's 0.375 if this little X is one or two and at zero otherwise notice this formula is also not unique we could also represent it as 3 choose x times 0.125 and keep this in mind when we get to the binomial distribution which will be in the next section remember that population parameter is a function of the population contrast this with a statistic being a sample of the data we want to use those statistics estimate the population usually and we'll do that in the second half of the course here we're going to look at ways of calculating those population parameters when we know what the distribution is some population parameters that we care most about will be the mean the standard deviation which is the square root of the variance and the median the definition of the expected value or the mean of a discrete random variable so the expected value of x where X is our random variable is just the sum over all values of X in the sample space of the quantity x times its probability recall from chapter 3 mu which is the population mean when we're dealing with distributions we can also use expected value of x just equal to the sum of x times 1 over n of x times well if each outcome in the population is equally likely to be observed then the probability of observing that person is just one over n and I suppose that should be a capital N expected value is just a long run average of those outcomes variance of a distribution is a measure of uncertainty in each outcome it's the opposite of the word precision I like the word uncertainty the variance of discrete random variable is given by that's a v of x we could also use Sigma squared just the sum over all the x's in the sample space of the quantity x minus mu squared times the probability of that outcome population variance remember was the same thing except one over n and this is perfect if every outcome is equally likely here we're dealing with unequally likely outcomes here's the wonderful definition the median notice that this explains why a hand waved saying it's uh it's about half is below and about half is above because dealing with the actual definition the median gives me a headache sometimes um when the distribution is continuous and we can just deal with the probability of X being less than or equal to the median that's tilde on top utilities are located to the left of the one as is just equal to 0.5 note that this implies that for a discrete distribution the median is not necessarily unique whereas for continuous distributions it will be here's an example three coins flipping through a coin three times the probability Mass function remember we came up with was this let's calculate the mean the variance and the median from definitions expected value of x is just the sum over all X that are in the sample space of the value times its probability there are four things in the sample space zero one two and three so it's zero times the probability of zero plus one times the probability of a one plus two times the probability of a two plus three times the probability of a three which gives us 1.5 so the expected number of heads on three flips of a fair coin is 1.5 doesn't surprise us variance same thing adding up overall X is in the sample space of the value minus the mean squared times the probability of the value so zero minus the mean squared times the probability of a zero plus one minus the mean squared times the probability of a one plus two minus the mean squared times the probability of a two plus three minus the mean squared times the probability of a three add them all up you get 0.75 standard deviation therefore is 0.866 that's the definition of the median I think the best way of using it is using cumulative sums of the probabilities so you order it from zero outcomes from lowest to highest start with the lowest outcome probability see if it's greater than or equal to 0.5 if it's not go to the next one and add the probability onto it so here x equals zero probability of that is 0.125 it's not greater than or equal to 0.5 so zero is not a median probability of a one was .375 so the cumulative probability at 1 is 0.125 plus 0.375 and that is greater than equal to 0.5 so 1 is a median note that since it comes out exactly equal to 0.5 the next value is also the median 2 is also a median and every number between one and two so 1.5 is a median 1.6 is a median here's the r code the X and P lines are going to be common to all the calculations it's the last one that shows you how to perform the calculation to get the the parameter so we've seen things like this P line so we're defining the variable p as equal to and we're using the C function to combine all of these values into one vector it's 0.125 0.375.375.125 so writing that second line P will now contain those four values the first line this is a new construct for us we're saying X is equal to the values from zero to three that's a colon so this will be 0 1 2 and 3. in some computer languages it'll you'll have instead of a colon three lower dots kind of like this to indicate zero through three but for R it's just a colon and now that we've got the sample space and the probability of each of those outcomes we can calculate the mean is just the sum of all those X's times their probabilities 1.5 for the variance is just the sum of the value minus the mean squared this is a carrot this is the thing on top of the six it's read as quote to the power of so this carrot 2 will mean to the power of 2 or squared times p added up and the standard deviation is just the square root of that function to take the square root in R is sqrt for the median we use the function cumulative sum or sum and we look for the first place where it gets to be 0.5 or greater and that first place remember the sample space is 0 1 2 3 0 1 2 3 0 1 so 1 is a median remember since this is exactly 0.5 then not only is one a median but so is 2 and every number between one and two example two ice hockey my stat 225 course we did a project where the students predicted the outcome of a future ice hockey game between the Portland Winterhawks and the Prince George cougars go Hawks I took all of their probabilities and averaged them for the entire class for the number of points scored by the Winterhawks so the class as a whole said the probability of them scoring zero points was 0.1 point was 0.1 two points is 0.2 3.6.4 and four points and five points or point one each so let's calculate the expected number of goals the uncertainty of that aka the variance and the median so here's a graph of that using the formula for the expected value we're adding up over all the x's in the sample space sample space zero one two three four five of the value times its probability and we get 2.6 so the expected number of goals to be made by the winter hawks is 2.6 you do not round that to the nearest integer the expected value is a long run average 's don't have to be in the sample space so the expected value is 2.6 it's not two it's not 3 it's 2.6 variance going to use the formula again plug and chug get 1.84 standard deviation therefore is 1.356 we could use the standard deviation with the empirical rule say that the probability of the Winterhawk scoring between 1.2 and 3.9 is about 68 percent we can actually calculate it exactly it's 60 percent the median start with zero probability is 0.1 that's not at least 0.5 add the probability of a one so we're at point two now that's not at least 0.5 I have the probability of two goals that's 0.4 that's not at least 0.5 add the probability of three goals a that comes up to be point a that is greater than or equal to 0.5 so 3 is a median in fact since the probability of three or less is greater than 0.5 we know that 3 is only is I'm sorry that 3 is the only median if these had added up to exactly 0.5 we'd know that 3 and 4 were both medians as well as everything between three and four but since it came out to be something greater than 0.5 we know that 3 is the only median we can do it in R quickly X Line This is the value 0 through 5. again there's that colon notation so run that first line X is now the vector 0 1 2 3 4 5. p this combined function the C function of 0.1.1.2.4.1.4 and the mean is just the sum of the X's times their probabilities 2.6 the variance is just the sum of the x minus mu squared times P's the standard deviation is just the square of that variance and the sum function r that's Q sum Q sum for x equals zero Q sum for x equals one for x equals two for x equals three boom x equals three is the median can sum is the cumulative sum of those probabilities so these values are probability of X being less than or equal to zero one two three four five so here's a summary of what we did today in this introduction to discrete distributions we looked at probability Mass functions and saw how they describe the probabilities of each outcome in the sample space we saw probabilities can be calculated by adding the individual probabilities this reflects chapter 4 very clearly the expected value is a weighted sum of the outcomes weighted by the probabilities expected value and mean are synonymous the variance is weight some of the distances between the outcomes and the mean again weighted by that probability and the median is the value again not necessarily in the sample space so it's just that at least half is less than or equal to it and at least half is greater than or equal to it in the future we're going to look at some named discrete distributions Bernoulli binomial on the next poisson later hypergeometric later I misspelled hypergeometric I guess we're going to use R to calculate those probabilities expected values if we wish it to find many uses for these probability distributions and modeling the real world events around us and then when we get to chapter six we'll repeat this for some continuous distributions and we're going to continue understanding how the data generating process helps us to better estimate the parameters the readings will be section 51 at Hawks appendix A1 and R for starters here are the intellecture questions not entirely intra lecture but we'll pretend question one what is a random variable again I would suggest in your notes write the question on the left hand side and answer it so that you can transfer that into moodle's quizzes what is a random variable question two is what is a sample space and question three is what is the difference between an expected value and a mean and that's it hello and welcome to section five two the binomial distribution in this lecture we're going to learn about two named distributions the Bernoulli distribution and the binomial you'll see that the Bernoulli distribution is just a special case of the binomial but the Bernoulli distribution does make a nice entrance into the binomial so here are today's objective again calculate expected value variance median probabilities associated with a Bernoulli random variable we're going to determine what random variable follows a Bernoulli binomial distribution using its definition and you do want to memorize the definition of a binomial random variable calculate the mean the variance of binomial determine if Bernoulli and binomial distributions are skewed and calculate probabilities from a binomial distribution and we're going to do a lot of that in R just to show you how easy it is to do so definition of a Bernoulli experiment a random variable with only two possible outcomes follows a Bernoulli distribution and we're going to define the success probability p so any random variable that has two possible outcomes is going to be a Bernoulli random variable some examples heads on a single flip the coin a correct choice and a true false question me stopping at Starbucks in the morning in a given single morning each of those two possible outcomes a success or a failure either I get ahead I get a correct choice and a true false I do stop at Starbucks or a failure the probability Mass function for the Bernoulli note that the sample space is 0 1 so and P is defined as the success probability so probability of X equaling 1 in other words having a success is p and we know from our rules the probability that if there's only two possible outcomes and the probability of one event is p the probability of the other is going to be one minus P the complement of it so the probability of a failure is one minus p um for reasons that will become clear when we get to the binomial distribution this can also be written as P to the power of X and one minus P to the power of one minus X let's calculate expected value and variance for Bernoulli's in general remember the definition the expected value is just the sum over all the X values in the sample space the quantity x times the probability of that X sample space is just 0 1 recall so this is zero times the probability of zero plus one times the probability of a one probability of a failure is one minus p probability of a success is p the expected value is p remember expected value just the long run average if my Bernoulli event is flipping a coin getting heads as a Fair coin then we would expect that I flipped that coin 10 billion times half of them will be ahead I'll get successes on half of them and that's what this expected value of x equal and P tells you similarly we can calculate the variance again it's sum over all the x's in the sample space if x minus mean squared times the probability of that X 0 minus p squared times the probability of a zero plus one minus p squared times the probability of a p algebra shows us that this is just P times 1 minus p you may see this is p times q and some sources Q would be the failure probability but let's keep it as 1 minus p so here's the pmf let's look at the median CDF function is what we're going to use for the calculator the median or call the CDF functions find as probability of the random variable being less than or equal to some value the cumulative part is the less than or equal to contrast that with the equals part cumulative will be for the less than or equal to CDF of a Bernoulli is zero when X is less than or equal to zero once you get to zero however once you get to that failure it hops up to 1 minus p stays at 1 minus P until you get to one then it's 1 minus P plus p from then on here's a that helps us calculate the median here's the CDF and we need to determine when that CDF is at least five at least 0.5 for the first time so the median is zero if success probability is less than a half it's one it's greater than a half and it's every number between 0 and 1 inclusive if p is equal to onehalf now the next two slides will show that this actually makes sense so here's the CDF for the Bernoulli when the success probability p is larger than one half this is a CDF because the vertical axis is the cumulative probability it's zero zero zero until you get to one to get to zero and then it pops up to one minus p stays at 1 minus P until you get to one that hops up to one so where is the first time that the cumulative probability is at least 0.5 right there one so the median when p is greater than 0.5 is 1. now when success probability is small when it's less than a half cdf000 popup to 1 minus P ooh we just popped up Beyond point five so zero is going to be our median so in other words when the success probability is large the median is one when it's small meeting zero make sure that that makes sense to you that should be very clear going from the Bernoulli to the binomial binomial random variable is defined statistically defined as the sum of n independent and identically distributed for newly random variables that's the definition of the binomial that actually comes down to this definition of five requirements and this you'll want to memorize by knowing random variable meets the following five requirements the number of Trials is known so we know what n is each trial has two possible outcomes that is each trial is a Bernoulli random variable itself the success probability p is constant from trial to trial identically distributed note that we don't have to know what p is we just have to know that it's constant for the trials are independent well it's up here trials are independent and the random variable or what we're measuring is the number of successes notice that one and five together tells us that sample space is 0 1 2 3 all the way up to n here's some examples no C's reflect the examples we had with the Bernoulli example uh so it's the times getting heads on N flips of a coin for Bernoulli it was just one flip of a coin times getting a six on N rolls of dice for Bernoulli it was one roll of a die time stopping at Starbucks and N mornings for the Bernoulli it was just one morning so again notice that the binomial random variable is just a generalization of the Bernoulli or you can think of the Bernoulli just as being a simplification or a special case of the binomial here's the probability Mass function for the binomial I like this version of it's in three parts this P to the power of X is the probability of getting X successes the 1 minus p to the power of n minus X is the probability of getting those n minus X failures and then the combinations part is just the number of ways that you can arrange those X successes in the end trials if we think back to section five one with the flipping the coin three times it was this combinations part that gave us the one the three the three and the one whereas the 1 8 part was just these two together binomial parameters the expected value of binomial is easily calculated from its statistical definition so this is the statistical definition the binomial let x sub I be a Bernoulli random variable define y as the sum of those X's this leads us to Y being a binomial with parameters n the number of bernoullis added together and P the success probability for each of those bernoullis notice there's no subscript on the P so this p is constant for all of those Bernoulli's here's the expected value expected value of y well we just substituted and what Y is this is y so we substitute it in we know that the expected value of a sum is just the sum of the expected values so we can switch the expected value and the summation we know the expected value of x sub I expect value for Bernoulli is just p and so we're n adding up the value p n times which is just n times p so the expected value of a binomial is just n times p as always stop and make sure that this works makes sense think of this in terms of flipping coins variance we can do the same thing variance of Y just substitute in what Y is the variance in the summation chain can switch positions because the X's are independent the variance of a Bernoulli is just P times 1 minus p we're adding up P times 1 minus p n times so there's our variance and a mathematical note this step requires Independence of the Bernoulli experiments which is a requirement of binomial so it works we'll do an example um I'll let you do the other four examples on your own and I really do suggest you do those work through them make sure they all make sense do them by hand and do them in r um so example one Fair coins let X be the number of heads flipped in 10 flips of a coin if the coin is fair then it's clear that X follows a binomial distribution with 10 trials and success probability of onehalf X is distributed as a binomial or X follows a binomial let's calculate these three things the probability of getting three heads probability getting three tails the probability of getting at most three heads or at least seven hits otherwise known as at least sorry at most three heads or at most three tails probability of getting three heads we want to calculate the probability that X is three this is a simple application of the probability Mass function here's the pmf we're given Little X is three n is 10 p is one half plug in chug get 0.1171875 10 choose 3 recall from chapter four it's just 10 factorial divided by 3 factorial divided by seven factorial which gives you 120. in R that's this one line notice three four things about I guess we're talking about a binomial distribution so the stem is binom we're asked to calculate the probability that X is equal to a value so that's what the prefix will be a d we're asked to calculate the probability X is equal to three so the first number is three then everything else defines that particular binomial distribution there are ten trials so size is equal to 10. and each trial has a probability of success of 0.5 so prob is 0.5 probability of getting exactly three tails if you get three tails we've got two options notice that this x is the number of heads flipped so we could redefine a new random variable for the distribution of tails or we can just note that if it's not a head it's a tail so the probability of three Tails suggests the probability of heads being seven again this simple application the probability Mass function plug and chug Little X is seven n is 10 p is one half probability is 0.1171875 before I go to the next slide what would this be an R binom because it's the binomial D because we're asked to calculate the probability that X is equal to something because we're asked to calculate probability X is equal to seven ten trials success probability of 0.5 this is a lot faster than this because I don't know 10 choose 7 in my head I'd have to use a calculator to do that finally we need to calculate the probability of getting at most three heads or at most three tails and most three heads are at least seven heads using the notation from chapter four this is probability of X being less than or equal to three Union X greater than equal to seven since there's no overlap between this event and this event probability of the Union just assuming the individual probabilities so to do this by hand we need to calculate the probability of X being less than or equal to three and add it to the probability of X greater than or equal to seven X less than or equal to three as X is zero x equals one x equals two x equals three and the event X greater than seven is just seven eight nine and ten have to calculate all those individual probabilities add them together and we get the answer if you do it by hand there's no shortcuts on this no special formula it's just you got to use this formula properly changed for each of these now if we want to do this in r it's pretty straightforward you would use the CDF function in r CDF remember is always less than or equal to so we've got to change our less than or equal to three Union greater than or equal to seven into just two less than or equal to's from chapter four we got the complements rule the probability of X being greater than or equal to seven is one minus the probability of X less than or equal to six which means the probability that we need to calculate is just part of X less than or equal to three plus this one minus probability of X less than or equal to six notice now all of our probabilities are cumulative they're all less than or equal to which makes things rather easy in R this would be P binom three and then the definition the binom plus one minus P binom six plus the trials and success probability notice we went from D binomial to p binom P is for cumulative probabilities D is for what I call Point probabilities P is for less than or equal D is for equal the interstate word problem remember I'm not going to do these I will encourage you to do these yourself I will require them as much as possible really not too much to say on these they're more activities for you illustrating calculations some examples from the life of my or time for my life as a consultant so here's what we learned from this slice deck definition the Bernoulli definition of the binomial expected value of a Bernoulli and a binomial distribution the variance of both of those applications a lot of examples got five examples again you should go through those how to use R to calculate the point probabilities the equals probabilities cumulative probability is less than or equal to for the equals probabilities you use d for the less than or equal to use p you'll want to download the all probabilities file we're going to do the same thing with the poisson in the future we're going to see that these distributions do describe real world events we're going to repeat all this with continuous distributions and we're going to realize that understanding the distribution of the data helps us to describe the underlying process better and that will allow us to estimate parameters of Interest covered two of these R functions in this slide deck but I'm going to give you four of them we looked at the D and the P versions the D for the equal the P for the less than or equal to both of these calculate probabilities you've seen the r version before you saw that in a lab R generates a random sample from that distribution so D and P give probabilities R gives a sample from the distribution and Q will give you the quantile or the percentile this will be useful for calculating calculating medians first quartiles 97.5 percentiles what have you so the Q will calculate a data value an X corresponding to a given probability and notice if you want to do Bernoulli random variables just make sure size is equal to one because really that's the only difference between a binomial and a Bernoulli R for starters appendix A2 and A3 will help Wikipedia's got a couple good pages on the Bernoulli and the binomial and so here are the intellectual questions not so intro lecture for this section but they're here again on the left hand side write the question underneath it write your answer so you can transfer that to your Moodle quiz what are the five requirements of a binomial distribution again you must memorize these what is the difference between a binomial and a Bernoulli and what is the formula for the expected value of a binomial random variable and that's it hello and welcome to section 5.3 the poisson distribution here we introduce a second discrete distribution and I'd like you to keep comparing it to the binomial to see what the similarities and the differences really are so by the end of this lecture you should be able to determine what random variables follow a poisson distribution by using the definition of a poisson calculate the probabilities from a poisson distribution both by hand and by R and calculate the expected value of variance median probabilities associated with that random variable um so here's the definition a random variable that is a count of successes over an area or a time period follows the poisson distribution with an average rate parameter of Lambda contrast this with the binomial distribution which was this count of successes over a certain number of Trials here it's over a time period so examples heads flipped in an hour as opposed to heads flipped in 10 flips which would be binomial here it's heads flipped in an hour number of dents on a car number of errors on a page number of terrorist attacks in a year number of bacteria in a swimming pool influenza cases in a week Wars in a year the binomial would be the number of states or countries at war in a year notice there's an upper bound to that um or binomial would be the the number of people in this class with influenza in a week that also has an upper bound notice in each of those cases you're you're measuring number of successes out of a total population of n bacteria swimming pool would be a poisson a number of swimming pools in Galesburg with bacteria would be binomial notice what I'm getting at here is that for a binomial it's the number of successes out of a number of Trials so the sample space is 0 through n whereas for poisson distribution it's not the number of successes over trials this number of successes in a time period or an area which case there is no upper bound therefore the sample space for poisson is 0 1 2 3 4 dot dot it can be proven AKA is beyond the scope of this course the probability Mass function for poisson is for probably Mass function it's got the equals part random variable is X the value is Little X is equal to e to the power of negative Lambda times Lambda to the power of x all over X factorial this e is the natural log base it's 2.7182 Etc Lambda is the average that's the parameter that we have to give it and X is the value that we want to find the probability of it can be shown that the expected value in the variance for a poisson are both Lambda that's kind of cool um we'll go with an example let X be the number of heads flipped in a minute I'm given that I average 24 flips per minute I've noticed I didn't say I am flipping the coin guaranteed 22 times 24 times in this minute I'm saying average 24 flips per minute if the coin is fair then it's clear that X follows a poisson distribution with Lambda equal to 12. probability of getting no heads probability of getting at most 20 heads and the probability of getting at least one head in the first six seconds probability of getting no heads in that minute so we're asking what's probably X is equal to zero we're given Lambda is equal to 12. so it's a plug and chug e to the negative 12 12 to the zero all over zero factorial recall from chapter four zero factorial is one so this is equal to point zero zero zero zero zero six one one four in other words if I actually do this experiment flip the coin for a minute and come up with zero heads then either I don't average 24 flips per minute or the coin is not fair or I should go out and buy a couple lottery tickets because this outcome is extremely rare on R this would be D plus zero Lambda equals 12. again D is for the probability of calculating the probabilities of equals here plus is for the poisson distribution zero we're calculating the probability X is equal to zero and we have to specify that Lambda is 12. because we're given Lambda is 12. part two what's the probability of getting at most 20 heads in that minute so probability of X being less than or equal to 20. of the random variable being less than or equal to 20. this is a quote simple application of the probability Mass function this is equal to the probability x equals zero plus probability x equals one plus the probability of x equals two plus the probability of x equals three okay I'll stop there until we get to 20 here's the pmf we're just adding up for all those various values of x from 0 to 20. easy way since this is a less than or equal to is to use the P form again the stem is pois for the poisson or calculate calculating the probability of X being less than or equal to 20. and Lambda is still 12. the probability of getting at least one head in the first six seconds at least notice X was flips heads in a minute we want to do this in SEC six seconds so we have to define a new random variable um six seconds is a tenth of a minute so the expected number of heads is just going to be 12 divided by 10 the number of heads in a minute divided by 10 because now we're in terms of six seconds plug and chug y greater than or equal to one so we're I guess this should be a y so we're summing up from Y is equal to one all the way up to Infinity this is going to take a long time if we do it this way but recall the complements rule probability of Y being greater than or equal to one is equal to one minus the probability of Y equaling 0. and this part here is the probability that Y is equal to zero one minus that it's y equals two we'll use a d for the equals and a zero so there's about a seventy percent chance that I get at least one head in SEC six seconds complements rule comes in really handy when dealing with poissons let X be the number of words in a year from the binomial slide deck we know that the average rate of wars in a year is one over 0.8462141 5 Section 5 1 gave us this this one divided that be 1.181734 so that's Lambda for one year I want to know what's probability that we have five years without a war if this is Lambda for one year the Lambda for five years will be just five times that boom and so probability of Y is equal to zero it's just .002715796 this is about 1 over 500 so we'd expect um a fiveyear P streak to happen about once every 500 years which seems to be borne out by reality real estate developer in Galesburg is looking to determine if it would be profitable to renovate the ferris building downtown into a boutique hotel to help determine this he has the student of mine who graduated last year to help estimate it John the student estimated the rate would be about 150 people per week the developer decided that a minimum of 130 per week would be needed to turn a profit so if John is right what proportional weeks will not be profitable so we're given Lambda is equal to 150 we want to know the probability of being 130 or less or I'm sorry less than 130. so this is a probability this is not a cumulative probability because accumulative be less than or equal to so we make it a less than or equal to by dropping the 130 by 1. it's now a cumulative probability so we use p uas because it's hipposan 129 and we're given that Lambda is equal to 150. so about four and a half percent of the weeks will not be profitable another example Galesburg crime there's actually a few examples in part of example four um reportedly the number of crimes in Galesburg was 96 last year so we're going to use that for our Lambda if there were four violent crimes last week is there significant evidence of an uptick in the crime rate okay so Lambda is 96 per year I got four violent crimes last week so we could make that Lambda in terms of weeks by dividing 96 by 52. so if the number of violent crimes in a week if x is that then we're asked to calculate probability of X being greater than or equal to four and interpreting that we're given X follows a poisson with Lambda equal to 96. this is crimes in a year divided by 52 so that would be crimes in a week this is the wrong direction it needs to be less than or equal to so this be one minus X less than or equal to three it's a less than or equal to so we can use the P form of poisson 3 Lambda is 96 out of 52. and we get 0.116 so what this means is if the crime rate stayed the same that is 96 per year then we'd observe four crimes a week about 11 of the time that's rather large really taking everything into consideration so it's not really evidence that the crime rate's gone up that isn't evidence crime rate may have gone up but this doesn't provide sufficient evidence for that so let's say there were 16 violent crimes last month notice part A it was four in a week so we'd expect 16 in a month if it's four in a week for four weeks so this really is in some ways the same question as last but we're holding that increase of crime for instead of over a week we're doing it over a month so really we're asking does increasing the sample size affect the probabilities so we're asked what's the probability and we're supposed to interpret the probability of X being greater than or equal to 16. given that Lambda is 96 over 12 because it's we're in terms of 12 months greater than or equal to 16 is 1 minus less than or equal to 15. we're now less than or equal to so we can use P form of poisson 15 Lambda and now we got probability of being .00823 that is very small in other words if the crime rate stayed the same we would expect to see 16 crimes in a month less than one percent of the time so this would constitute some evidence that the crime rate did go up this year now the good question that you're thinking is okay the last one it was Point 11 and this one's .008 0.11 it was relatively large interpreted as being relatively large .008 is relatively small the 0.11 doesn't give us sufficient evidence the .008 does where's the separation between doesn't give us the evidence and does give us the evidence I'm not answering that now but keep this in my effect over on the left hand side of your notes write something like what's the difference or what's the separation between not enough evidence and enough evidence Circle it make it stand out a little bit maybe put Alpha around it a few times just to so that we can refer back to that in the future so here's a summary of the poisson poisson distribution model is the number of successes or over a time period or an area a parameter of a poisson distribution is Lambda this Lambda is both the mean and the variance we know the probability Mass function of the poisson that allows us to calculate probabilities from a poisson random variable the future we're going to look at the hypergeometric notice the hypergeometric is spelled correctly here and also hypergeometric we've bumped into that before in in lab B hybrid or about ready to hyper geometric like the binomial model's number of successes given a number of Trials so you've got to figure out what is the key difference between the binomial and the hypergeometric and then chapter 6 will be the continuous distributions here are the r functions two of these we dealt with two of those are implied they all have the stem pois and they all require specifying Lambda because those are the keys to a poisson distribution the D form is for calculating probability of X equaling a value the P form is for the CDF calculating probability of X being less than or equal to a value the r generates random values from a poisson and the Q form calculates the quantiles so if I want to calculate the median I do Q Plus of 0.5 and specifying whatever value of Lambda it is if I want to calculate the 10th percentile I put in 0.1 for p uh the readings Hawks was Section 5 3 R for starters this is appendix A7 and as usual Wikipedia's got a pretty good page on the poisson distribution gives you a nice flavor of how far you can go with these probability distributions so here's the intra lecture question for five three our questions for five three question one what is the parameter of a poisson distribution it is the parameter for poisson hint it's called Lambda question two what's the difference between a binomial and a poisson variable I won't give a hint on that one except to say go back over your notes to make sure that you have this written down this is this is important and what's the formula for the expected value of a poisson that's also pretty easy one so of these three the sequence going to be the tough one and that brings us to the end of the poisson distribution take care hello and welcome to the last section of chapter five this will be the last named discrete distribution we'll be working with it's called the hypergeometric this feat is featured in lab B um so by the end of the lecture you should be able to determine which random variables follow hypergeometric distribution using its definition identify the three parameters of a hypergeometric distribution and calculate the expected value variance being probabilities associated with hypergeometric random variable hypergeometric is rather difficult to work with so don't expect too much work using the hypergeometric doing things by hand you'll see the probability Mass function and understand what I mean when you do see that the hypergeometric distribution describes the probability of obtaining X successes in K draws or trials without replacement from a finite population that contains exactly M successes and N failures see I told you it was kind of complicated um Keys two keys to this are it's without replacement so you're drawing and you're not putting it back and the population's finite if you are doing it with replacement or if the population is infinite then you've got a binomial random variable here so this is a binomial constrained to a finite population without replacement it's well when you get to the lab you'll understand that it's better but as you work through this uh slide deck you'll see why we tend to focus on the binomial examples number of Hearts drawn from a deck without replacement so I draw five cards what's the probability three are Hearts if I don't replace finite population Capital n's 50. two um and it's without replacement a number of sophomores counted in the Library without recounting somebody um number of Parolees that returned to the Hill Correctional Center in Galesburg um in each of these cases it's without replacement and the population is finite so key um so hyper geometric finite and no replacement now you'll you'll find out and I think the lab helps with this that if your population is sufficiently large but not infinite then the advantage of using the hypergeometric is very small it might as well stick with the binomial recall that the probability Mass function provides the probability of each element for the sample space for a hypergeometric random variable this is the sample space see notice automatically that we see that the hyper geometric is rather complicated to work with for the binomial it's just from 0 to n here it's the larger of zero and K minus n all the way up to the minimum of K and M how do we parameterize this there's lots of ways of parameterizing the hypergeometric that's another frustrating thing with binomial it's always n and p or n in pi for the hypergeometric there's at least a half dozen ways in fact Hawks uses a way that's different from either r or the forsberg text um the uh we can do it by successes and total or by successes and failures or by combination of the two but fundamentally the probability Mass function here at the bottom is just a number of ways to succeed times the number of ways to fail divided by the number of ways you can have those end trials and these are combinations so review of chapter four would not be unwise um so here's the probability Mass function it's m choose X n choose K minus X divided by M plus n over choose k X is the number of successes you care about m is the number of successes in the population in this case little n is the number of failures in the population so X is the number of successes in your sample K minus X will be the number of failures in your sample and then the denominator is just going to be the total population size divided by the total sample size and notice the switch here K is now the sample size not n and trust me this is one of the better parameterizations of the hypergeometric here's another one here K will be the population successes instead of a little m n will be the population size which means n minus K will be the population failures X will be the successes in your sample n minus X will be the six failures in your sample here n will be the sample size so n choose n will be the denominator they say the same things they're using different letters to represent what those things are but they're saying the same thing this is the number of ways of getting X successes from the population this is the number of ways of getting your specific number of failures in your population and this is the number of possible samples you can draw from that population same as it was for the previous expected value of a hypergeometric does not need to be memorized you've got this in your notes so make sure you know where you can locate it it's just k ick your sample size p the probability of a success in the population m is the number of successes n is the number of failures so n plus m is a population size so this M over n plus m is just the probability of a success in the population and that's the sample size so it's essentially n times P but we're using different letters here's the variance it's your sample size times the probability of a success in the population times the probability of a failure in the population times an adjustment Factor notice this is thinking back to binomial this would be n times P times 1 minus p times some other number notice this number is always going to be between 0 and 1. which means that the variance of a hyper geometric is always going to be smaller than that of a binomial that's important variance of a hypergeometric is always going to be less than or equal to for extremely large sample sizes that of a binomial as long as K is not equal to one if K is equal to one then there is absolutely no difference between a binomial and a hypergeometric go back to the definition of a hypergeometric to see why that's the case the big difference is with replacement or without if you're only drawing one thing it doesn't matter if you replace it or not it's the same probability this is another way of saying exactly what I did understand what the parameters represent don't get hung up on the formulas themselves understand that the expected value is just the sample size times the probability of a success in the population the variance is n times P times 1 minus P times some adjustment Factor so here's a couple examples okay here's a few examples I'll let X be the number of Spades drawn at a four car uh four draws from a deck of cards without replacement if the deck is fair and it's clear that X follows a hypergeometric with m equal 13 the number of successes in the population is 13. and the number of failures in the population is 39 and our sample size K is 4. success failure sample size so here are the probabilities or here are the questions what's the probability of getting one Spade what's the probability of getting at most three Spades and what's the expected number of Spades probability of eating one Spade plug and chug probably X is equal to one we're given m is 13 x is one n is 39 K is 4 4 minus 1 is 3. a little check here 13 plus 39 has to be 52. 1 plus 3 has to be 4. why is that a check this first refers to the number of ways of getting those successes the second is the number of ways of getting those failures and the total is just failures plus successes so the probability of getting one Spade is 0.438847539 here it is in r probability that X is equal to one so it's D and a 1 there this is a hyper geometric so the stem is hyper parameters m is 13 and 39K is four and by the way this is the parameterization that R uses mnk probability of getting at most three Spades this is a less than or equal to question less than or equal to three since it is a cumulative a less than or equal to we can use the P version 3 m n and K so the probability of getting at most three Spades is 0.9973589 in other words pretty good chance that you'll get at most three what's the expected number of Spades sample size times success is over trials so the expected number of Spades it's going to be one make sure this result makes sense especially you think about lab B a quarter of the deck is Spades I draw four so I'd expect one example two I wonder if stat 200 attracts third tier students at a greater rate than other courses so X will be the number of third years in stat 200 we know that the number of third year students at Knox is 356. we know the number of nonthirds years is 978 we got that from the registrar's website therefore those are population numbers in this stat course there's 41 students 18 or 30 years so we need to calculate the probability X is greater than or equal to 18. given m is 356 n is 978 and K is 41. this is in the wrong direction it's got to be a less than or equal to so using the complements rule this is one minus probability of X is less than or equal to 17. this is what it is in r 1 minus probability under hypergeometric of X being less than or equal to oops that should be a 17. so this number here should be 17 not 18. m n and K are given to us previously this is probably around .005 very small probability of this happening therefore it appears as though third years are over represented in this course or I'm wrong about m n and K I trust the registrar to give me these two numbers I know I can count to 41. therefore I'm going to conclude that third years are indeed over represented if we were to Pretend This was a binomial we get a probability of .00537 not much of a difference between these two the reason why there's not much of a difference between the two is because the population size is quote rather large it's thirteen hundred or so so once you get a large population hypergeometric and binomial are essentially the same thing I wonder if my math 121 attracts fourth year students at a lower rate than other courses let X be the number of fourth years in my math 121 course according to the registrar's website we know the number of fourth year is at Knox is 278 nonfourth years is 10 56. in my 121 there are 30 students three or fourth years so I need to calculate the probability of X being less than or equal to three where X follows a hypergeometric distribution with m n and k equal to 278 1056 and 30. P hyper P because it's less than or equal to we added at the 3 I mean it right this time MN and K given to us by the registrar's website and me being able to count the class this is 10 percent because the probability is not that small there does not seem to be much evidence that math 121 attracts fourth years at a lower rate than other courses 10 percent not that small again this goes back to um I think was the binomial slide deck where we started talking about okay what's the difference between no it was a poisson and the crimes what's the difference between not that much evidence and sufficient evidence so you might want to write in the in the margin again Arrow there of asking the question difference between sufficient evidence and not sufficient evidence by the way 0.099 versus 0.102 Hyper geometric versus binomial don't get me wrong the hyper geometric is the correct distribution and doing this by using R is pretty darn easy so there's no reason to use the binomial approximation but if you're doing it by hand or using some things that we will be doing in this class in the future that are predicated in the binomial as long as you're set plus size is large enough you can use those things those future things for both the binomial and the hypergeometric so the summary hybrid geometric models the number of successes out of a specific number of Trials when the population is finite and the elements cannot be selected multiple times there are multiple ways of parameterizing this distribution I gave you two of them but they all specify the sample size number of successes the failures in the population in some way you saw the pmfs of the hyper geometric you know the mean and the variance the hyper geometric again you should not spend your time memorizing those have them in your notes and make sure you're able to access them and use them the future today was or this lecture is the end of the discrete distributions the future will be continuous distributions and that'll start with chapter six we've got the four R functions all of them have the stem of hyper for the hypergeometric they all require you to specify M and K again the D is for the equals probabilities the P is for less than or equal the r is for generating a random sample from that distribution and the Q is for the quantile so if I want the median I'll put 0.5 for p and I'll be able to get the median if I want the third quartile I would put 0.75 in here for p and get the third quartile course readings Section 5 4 and Hawks this is appendix A6 in R for starters Wikipedia's got an interesting page on the hyper geometric again it'll give you a taste of where you can find the formulas if you forget them and it'll show you a lot of things that we can do with this probability stuff which brings us to the three interlection questions one what are the five requirements for a random variable to follow a binomial distribution yes this is for the binomial distribution you must know those five two what's the difference between a binomial and a hypergeometric variable and question three what's the difference between a binomial and a poisson random variable so a little hint here seems like we're focusing on the binomial as being the most important distribution in this chapter if it seems that way then good because it is but also you're seeing how close the poisson and the higher pot geometric are to the binomial when they are close and by extension when they're not close and that's it take care hello and welcome to chapter six in chapter six we're talking about continuous distributions contrast this with the discrete distributions in chapter five chapter 6 and Hawks goes directly to the normal distribution introduces some things that just kind of pop out of nowhere to help see where those things come from I'm introducing two other distributions the uniform distribution which will be this lecture and the exponential distribution which will be the next so by the end of this lecture you should be able to understand the difference between discrete and continuous random variables know the purpose of the probability density function notice this is the probability density function not the probability Mass function be able to prove that the density is not a probability you know the purpose of the cumulative distribution function CDF hint it's the same as the purpose of the CDF with discrete random variables be able to calculate probabilities using geometry or the CDF in fact we're going to figure out an easy way of calculating or creating the CDF for the uniform and then understand the uniform distribution it's two parameters it's sample space it's expected value its variance standard deviation uh median things like that so here's a definition of continuous random variable it's a random variable with a sample space consisting of an interval of values that means for continuous random variables there is always a value between two other values in it so there is no next two with continuous random variables for discrete random variables you could list them off there's a next two after one came two after two came three for a continuous random variable between any two values there's always going to be another value examples of continuous of actually continuous random variables student height age of a car time spent at a stoplight distance of golf ball goes those are all continuous measures there's also a category of random variables that are essentially continuous they're they're not continuous they're discrete but we can pretend they're continuous and not lose too much not create too much error in much the same way that when we were dealing with a hypergeometric distribution if the population was quote large enough we could use the simpler binomial and we wouldn't introduce too many errors such examples of quote near continuous random variables would be GPA definitely a discrete distribution but the distance between two levels of it is so small compared to the range that we can just pretend that it's continuous and not introduced too much by way of error annual salary my salary is paid down to the penny uh so the the the grid is a penny but my salary is in tens of thousands of dollars so that that close enough to being continuous that we can pretend it's continuous same with gross domestic product GDP per capita crime rate number of years of corn grown in Iowa that's definitely a discrete distribution but the distance between one ear and the next year that that that grid is so small compared to the number of ears grown that we it we can pretend that it's continuous and not lose too much what I'm getting at here is just like with the binomial there are benefits to using the binomial when you can Simplicity is is the best that even though the data are generated from hyper hypergeometric if this the population is large enough we can pretend it's binomial and not introduce too much error and then reap the reward of everything that comes from it being a binomial same thing here with these quote near continuous Ram variables they're not continuous they're discrete but they're so close to being continuous that we can pretend they're continuous and reap all the benefits of a continuous random variable and there are several um the first continuous random variable that we're going to talk about is the uniform distribution the uniform distribution is a continuous distribution that describes random variables whose likelihood of occurring is constant across a specified interval notice the word probability is missing from that definition when we start talking about continuous distributions we have to start talking about likelihoods of events we'll be able to retrieve probabilities in a lot of cases but we have to start talking about likelihoods if the ram variable X has a uniform distribution we're going to write X and there's that tilde is distributed as a uniform with parameters A and B A is the lower bound and B is the upper bound if x is a uniform a b distribution and then the sample space is is all values between a and b the expected value is just a plus b over 2. the variance is just the width of the interval squared over 12. standard deviation is just the width of the interval divided by the square root of 12 in other words it's the square root of the variance this is the probability density function for all X that's in the interval A to B the likelihood is 1 over B minus a or the density is 1 over B minus a outside the interval the density or the likelihood is zero it's a uniform distribution because each of the likelihoods is the same in a probability density function you're measuring the likelihood or the probability density for the uniform it's a constant between a and b the height is going to be 1 over B minus a Y is the height 1 over B minus a this is a graph of the likelihood let me be clear this is the graph of the density or the likelihood this is not a graph of probabilities we know it's not a graph of probabilities because if a is zero for instance and B is one half this width is going to be onehalf and the height is going to be two and you can't have probabilities that are greater than one so this is clearly not a probability to obtain a probability you just have to find the area corresponding to what you're trying to the event you're finding the probability of so probabilities are just areas of the PDF functions for most continuous distributions that means we have to use calculus for the uniform distribution we can use high school algebra or we can use sixth grade algebra it's just squares and rectangles note now we know why it has to be have a height of 1 over B minus a because the interval length is a to B it's the length is b minus a and we know that the probability of something in this interval happening has to be one so the probability that X is between A and B has to equal one it's a rectangle so B minus a which is this width times the height has to equal one because rectangles are width times Heights and therefore the height has to be one over B minus a um note that the PDF has two purposes one it's to help the researcher understand the probability for a continuous distribution and it's to help the researcher calculate probabilities of a continuous distribution and in red probabilities are areas under the density curve another thing that these PDFs can tell us is where the outcome is most likely here the the likelihood is flat therefore every value between A and B is equally likely um if there was a mound to this then those things near the those values around the highest around the peak that's the word Peak will be more likely so let's see how we do this there's only one stop light between home and school in the morning my home and my school it regularly Cycles among green 175 seconds green five seconds yellow and 180 seconds red there's only one stoplight it's at Maine and Academy it's it's a doozy given that I stop at the light in other words given that the lights red what's the probability that I wait at most 60 Seconds so let's define the random variable t as the time I spend waiting and I want to find the probability that I weighed at most 60 seconds that t is less than or equal to 60. because the time I wait does not depend on when I get there notice it's the only light if there were several lights I had to stop through then it wouldn't be a uniform distribution but because it's the only light it is a uniform distribution because there's a definite lower and upper bound the lowest that I will stop there will be for zero seconds and the highest that I'll stop there will be 180 seconds I'm definite upper and lower bound uniform distribution we now have that t the time I spend waiting at the light in seconds is distributed according to uniform with parameter 0 and 180. or with minimum zero and maximum 180 seconds and again we have to calculate probability T is less than or equal to 60 that I wait at most 60 Seconds the outside rectangle is the distribution of t specifically it's the probability density function of t the dark blue is the area that I want to calculate it's the probability T is less than or equal to 60 Seconds to find the probability it's just the area under the Curve of the density curve corresponding to this width Heights 1 over 180 the width of the dark blue is 60 so the probability of waiting at most 60 Seconds is just 60 over 180. this should ring some Bells from chapter four when you were talking about equally likely events because the PDF of the uniform is just a rectangle and because areas of PDFs are probabilities and I want to emphasize that again areas in PDFs are the probabilities we just need to calculate the region of T less than or equal to 60. height times width so 33.3 percent chance that I wait at most 60 Seconds for the uniform this is easy to do in our heads if we want to use r here's how we use r again we're looking for a probability of T being less than or equal to something so we use the P version the stem for the uniform is uni f we're looking for the probability T is less than or equal to 60. then we specify the parameters of the uniform Min equals zero Max equals 180. and the keywords are Min and Max Not A and B the CDF of a probability distribution recall is defined as a probability of the random variable is less than or equal to some value traditionally we give this a capital f for continuous distributions without knowledge of calculus this is frequently rather difficult to calculate however for uniform we can just rely on middle school geometry so we're back to our generic uniform distribution ranges from A to B which means the height is 1 over B minus a we're going to let X move between a and b and we need to calculate this area here that's pretty easy the area of this is just x minus a divided by B minus a in surprise you just calculated CDF function this is the CDF function so if I want to calculate the probability probability that X is less than or equal to 6. I'd put 6 in here for x 6 in here for x and I would have to know A and B from the definition of the uniform I want to be clear calculating CDF functions usually requires calculus usually requires integration but for those who've had calculus integration is just a fast way of finding areas and in this case we've got geometry to help us find the area the quantile function is the inverse of the CDF we've bumped into the quantile function several times in the past um it's the quantile function is a function of P the CDF is a function of X it's the same relationship here by the way CDF is a function of X that calculates p quantile function is a function of P that calculates X they're inverses so from the CDF we know that P is equal to x minus A over B minus a this is for the uniform all we have to do is solve for x so X is equal to P times the width plus a the starting point so this thing on the left is the quantile function you're given P you solve it for x the first line the thing on the right was the CDF you're given X to calculate p inverse functions so here's the quantile function we can use a capital Q to symbolize it the quantile function for the uniform is just P times the width plus a at the starting point so here's what we've learned in this slide deck actually let's go back a page so how can I use this quantile function I can use it to calculate the median remember the median is the 50th percentile so to calculate the median of this uniform distribution we put 0.5 in for p 0.5 in for p and solve if we know what b and a are we can come up with a number if we don't know what b and a are then we just get an expression I will leave it as an exercise for you to show that the median of a uniform distribution is equal to the mean of the uniform distribution all it takes is understanding what the quantile function is and a little simple algebra so here's what we learned in this slide deck actually before we get to this let's go to our intra lecture questions there's three of them again write the question over on the left hand side of your notes answer below it so you can transfer that into Moodle question one what is the difference between a discrete and a continuous random variable question two what is the definition of a uniform random variable and question three what are the two parameters of a uniform distribution remember you do have the pause button to use okay let's go into the uniform summary now here's what we learned in this slide deck continuous random variables describe different phenomena than discrete random variables they describe things that are continuous instead of counts in a discrete random variable you talk about the probability Mass function which actually does give you a probability in continuous you've talked about a probability density function which gives you a likelihood and you have to calculate areas in order to turn that likelihood into a probability probability is the area under the PDF curve the CDF is probability that the random variable is less than or equal to some value uniform distribution describes a random variable where all values are equally likely the mean of a uniform random variable is a plus b over 2. turns out that that's also the median and I gave you that as a fun exercise future we're going to look at the exponential and normal distributions we're going to practice calculating probabilities using formulas tables and r I'm going to continue thinking about the relationship between random variables around us and their distributions the key part for chapters four five and six is starting to look at the world around us and think in terms of probability distributions here's the for our functions notice the stem for each is u n i f for uniform for the density function the PDF it's just d uniform now we see what the D actually means for the cumulative probability it's p for a random sample from this distribution it's r and then the quantile function is q and while we were here we calculated the P the CDF we calculate the PDF as well and we calculate the quantile function by hand we realize it's not that difficult but let's be clear it's not that difficult for this distribution once you go beyond something as simple as the uniform it begins to get much more difficult and sometimes you can't even determine what the quantile function or the CDF function is it may not exist the readings are for started appendix B1 and B2 there's nothing in Hawks and uniform distribution continuous in Wikipedia now this is as far as you have to go if you've taken some calculus and you want to see how to use it I give you the calculus extra but if you don't know calculus skip over this and you will lose nothing so how does one directly calculate the expected value for continuous distribution you use calculus refer back to the discrete case for the expected value it was x times the probability of X added up over everything in the sample space in the continuous case you're integrating over the sample space of x times the probability density function so this is the definition of the expected value in the continuous case similarly this is the formula for the variance in the continuous case notice again there's an x minus mu squared and this f of x isn't a probability but it sure looks like that's where the probability was in the discrete case instead of adding your integrating there's another formula for the variance that's equivalent and sometimes it's easier to use sometimes it's not so for let's see how to use these formulas um expected value of x is the integral over the sample space of x times the density of DX sample space is all values between a and b of x times the density the density just one over B minus a DX 1 over B minus a is a constant so it can be pulled out integral of x DX is x squared over two x squared over two there's that one minus B one over B minus a and it's evaluated between B and A between x equals a and x equals B x equals B this is B squared over two times one minus B over a subtracting off the evaluation at the lower bound a squared which gives us this next line and now we can do some calculus not calculus algebra stuff B squared minus a squared is B minus a times B plus a why did I know to do that well there's a B minus a out here and I knew this was the difference of squares so the B minus a is cancel and I'm left with a plus b over 2. variance same idea I'm using the second formula for the variance calculating What's called the second moment just the integral over the sample space of x squared times f of x DX and then I'll subtract off the mean squared integral of x squared DX is X cubed over three times one over B minus a evaluating between B and A so that's B cubed minus a cubed over three and B cubed minus a cubed has a B minus a term to it B cubed minus a cubed who knows B minus a times B squared plus a B plus a squared and we got some canceling there going away I combine these two fractions a 1 3 and 1 4 common denominator is the twelve Ah that's where the 12 comes from common denominator is 12 so this becomes 4 over 12 and this is 3 over 12. there's four there's the three cancels distribute the four distribute the 3. combine like terms B squared minus two a B plus a squared we know is equal to B minus a squared it's amazing how much high school algebra does come in handy sometimes which gives us our formula for the variance that makes sense the B minus a squared I mean the the wider the interval the more uncertain we are in the outcome so it makes sense that the variance would depend on B minus a squared now we see where the 12 comes from comes from combining these two parts and that's it for the calculus extra again if you haven't had calculus you shouldn't have watched that and if you have had calculus you see why you've had calculus take care hello welcome to the second lecture of chapter six this one covers the exponential distribution and the previous one covered the uniform distribution the uniform distribution was very good at modeling stuff that had a definite lower and a definite upper bound and was continuous and you knew nothing else about it other than it had a definite lower a definite upper and was a continuous random variable an exponential distribution will be very useful when you've got a continuous random variable that has a definite lower bound of zero and no hard upper bound so it's bounded on one side examples of this would be wait times um so by the end of this lecture you should be able to determine which random variables may follow an exponential distribution specify the characteristics of an exponential distribution compare and contrast exponential with uniform calculate probabilities using the CDF and quantiles using the CDF so here's the characteristics it's a continuous distribution describes a random variable whose probability of occurring decreases with time it's frequently used to model quote time until something occurs when there is no upper bound we say x follows or has a distribution of an exponential the parameter for an exponential is Lambda Lambda is the rate of the thing happening sample space is from 0 to Infinity there is no upper bound there is a lower bound of zero but there's no upper bound expected value is 1 over Lambda the variance is one over Lambda squared which means that the standard deviation and the expected value of the exponential are identical one over Lambda the rate of something happening well if we remember our physics the rate of one divided by the rate is just the frequency so it makes sense that the expected value would be 1 over the rate because it's an average frequency of it happening this is what the exponential distribution looks like Smooth declining the height happens at Lambda when x equals zero the probability density function is Lambda times e to the power of negative Lambda X for X greater than zero and zero otherwise and again it's important probabilities are areas under the density curve notice there's no rectangles here for us to play with so calculating those areas is going to be a bit more difficult than it was in the uniform case let's go into two examples time between when I fill my bird feeder with seed and when chunky the squirrel starts eating from the feeder follows an exponential distribution with an average time of 10 seconds in other words if we Define t as the time in seconds until chunky raised the bird feeder T follows an exponential distribution with Lambda equal to one over ten Lambda is 1 over the mean the mean is one over Lambda Lambda is one over the mean we're given the average time is the mean time is 10 seconds so what's the probability that a weights at most a half minute before chowing down on some awesome seed at most a half minute so we're looking at the probability of T being less than or equal to 30. dark color is what we need to calculate remember again for continuous random variables areas under the PDF curve are the probabilities no rectangles to use can't use the tricks from last lecture are going to have to jump directly to the CDF function I leave it as a proof for those who enjoy calculus to prove that this is indeed the CDF for the exponential distribution what we did with the uniform remember is we created this ourselves for the exponential it's given to us and we use it for the normal which is the next distribution in the next lecture we realize there is no function and we have to use something else so we want to find the probability less than equal to 30 that's just the CDF at 30. Lambda is 0.1 x is 30. here's the CDF function 95 percent probability that chunky will wait no more than 30 seconds to raid the feeder this is a little bit more involved if we got R open might as well use R to do the calculations it's a less than or equal to so this is a p the stem for the exponential is EXP we want to find the probability T is less than or equal to 30. and we're told Lambda is equal to 1 10. we're not calling it Lambda here we're calling it rate so be aware that this has to be rate example two time I wait until the gold express bus comes follows an exponential distribution my average wait time is five minutes in this cold of winter it takes 10 minutes for Forest bite to start so given this what's the probability that I will have frostbite before the bus arrives in other words given that 1 over Lambda is five what's the probability that t is greater than 10. 10. this is not a CDF it has to be in the form of a less than or equal to but using the complements rule we can easily change this into from p is from T greater than 10 to 1 minus probability of T being less than or equal to 10. chapter 4 pops up every once in a while this is probably the most important thing from chapter four the compliments rule so now all you do who is calculate the probability is less than or equal to 10 that's the CDF at 10 given that Lambda is 20. I'm sorry 0.20 1 over 5 is 0.20 plug chug 13.5 percent chance that I get frostbite or it's 1 minus P EXP of 10 rate equals 0.2 again it's p because we're looking at the CDF function it's exp because it's an exponential distribution we can also calculate the median remember the median is the value X tilde such that the CDF at X tilde is one half on the left is the CDF function evaluated X tilde which is just 1 minus E to the negative Lambda X tilde 0.5 stays on the right now what we're doing now is solving for x tilde subtract 1 from both sides and then multiply through by negative one gets us here take the natural log of both sides gets us here divide by negative 0.2 gets us here remember the log of onehalf is negative log of 2. so the median is 3.466 minutes and we can show in general that the median is just mu the mean Times log 2. and this is the natural log of 2. or instead of doing all this calculation and R is just q q because we look at the quantile at 0.5 this will give us the median exp because it's an exponential and the rate is one over five it's one over mu we could also calculate the 90th percentile set the CDF equal to 0.9 and solve for x star or just put 0.9 in the quantile function and solve before we get to the summary let's go ahead and put in the intra lecture questions there's three of them as usual and again as usual write these on the left side of your notes answer them so you can transfer them into Moodle one give an example of a random variable that follows an exponential distribution something other than the two examples that we did today two what is the parameter of an exponential distribution and three what are the mean and standard deviation of an exponential distribution give me the formula for those I did say mean and standard deviation the reason I'm asking this is to set something in your mind so that you can contrast the exponential with the poisson because exponential poisson both use Lambda as the parameter um so here's a summary we reminded ourselves that probabilities area under the PDF curve the CDF function is X probability of X less than or equal to X in this case we needed calculus to find the CDF although the calculus extra is where we prove it I just threw the formula at you exponential distribution describes a white wait time random variable the time until something happens either being a variance of an exponential random variable or one over Lambda and one over Lambda squared respectively meaning that the standard deviation is going to be the square root of the variance in other words the standard deviation is one over Lambda last distribution is the normal distribution normal distribution is the key distribution from chapter six when we get to chapter seven and talk about the central limit theorem I'll drive this home as to why the normal distribution is the most important we cannot there is no formula to calculate the normal distribution probabilities we can either use the table at the end of the book no no no no no or we can use R to do it we're going to look at quantiles Otherwise Known percentiles and we're going to again understand the difference between discrete and continuous random variables here are the r functions again all of them have the stem exp for the expected value and all of them require that you specify what the rate is and the rate here is Lambda D for the density the little f of x p for the cumulative probability the capital f of x r to generate a random value Q for the quantile so the median we'd use the Q exp and then put in 0.5 for the 10th percentile we put in point one for the 99th percentile we put in 0.99 nothing in Hawks about this R for starters it's appendix B5 how Wikipedia's got the exponential distribution in the Wikipedia page you'll see that there's actually two parameterizations for the exponential the one that we're using in class which is Lambda and the one that the engineers tend to use Theta if memory serves me right that's kind of the end again if you are not a calculus person you can stop here if you are a calculus person you want to see the how to actually create that CDF function continue on so here's a proof of the exponential CDF remember the definition of the CDF it's the probability otherwise an area under this under the PDF curve other random variable being less than or equal to X for the exponential that means that you're integrating from zero to that x value F of T is the exponential PDF probability density function DT substitute F of T is just Lambda e to the negative Lambda t this Lambda doesn't depend upon T so it can be pulled out so we're integrating from zero to X of e to the negative Lambda T DT not too difficult the integral of e to the negative Lambda T is just negative one over Lambda times e to the minus Lambda t evaluated from T is zero to X this Lambda and this Lambda cancel so we're left with just a negative e to the negative Lambda t evaluated at X subtract off this thing evaluated at zero here it is evaluated at X subtract off evaluated at zero notice e to the zero is equal to one subtracting off a negative one means you're adding one no simplification the first term so we're left with CDF being one minus E to the negative Lambda X again this is for those who want to see that their calculus time was well spent that it's used a lot in statistics and probability Theory and I'm sure it's used other places but really I don't care it's all about the statistics and probability and how we're using the mathematics so I'm done I have a good one hello and welcome to chapter six from Hawks this lecture will cover check section six one through six four all four of those discuss the normal distribution because we spent some time building up things with uniform distribution the exponential distribution we'll be able to cover the normal distribution in one rather lengthy lecture but still just one lecture by the end of this lecture you should be able to discuss the differences between the uniform exponential and normal distributions know the expected value and the mean of a normally distributed random variable it's kind of tricky because remember the mean and the expected value are the same thing sketch the graph of a normal distribution realize that capital N normal and lowercase and normal mean different things the capital N normal refers to the distribution and lowercase and normal just means typical or not surprising to be able to calculate probabilities normal random variables and quantiles of normal random variables so here's the arc that we've been working on chapter five we looked at discrete distributions which included probability Mass functions pmf calculated mean the variances sample spaces for those discrete distributions the discrete distributions we looked at were the Bernoulli the binomial the poisson and the hypergeometric we also looked at generic discrete distributions in the first section of chapter five and chapter six looked at looks at continuous distributions I introduced the uniform distribution so you get an idea that probabilities for continuous distributions were actually areas under the PDF curve which meant the PDF curve was not probabilities they were probability densities or likelihoods we use the uniform to illustrate how to calculate the CDF the cumulative distribution function using simple high school geometry and then we moved on to look at how to calculate the mean the variance the sample space the median other quantiles on the uniform so that section on the uniform just laid the foundation for all the other important features of continuous distributions the second section was the exponential where we did the same thing but then we realized creating that CDF function that cumulative distribution function was not easy and in that case we had to use calculus to get it or It came it was given to us pop fully formed out of the sea foam your choice so we went from the uniform where we created it ourselves and got a good feel for how it was created to the exponential where we had to use Calculus if we wanted to see how it was created to this lecture it's normal and there is no CDF function which means that in the uniform well you really didn't have to pay attention to the CDF we could calculate those probabilities in our head exponential we need to use the CDF but it was a nice simple form that we could use for the normal we got to go to a table or to a computer and for much of this second half of the 19th century a lot of effort was spent trying to create those tables it was pretty easy in the early 19th century create CDF function for for values of we're going to see Z between negative 1 and positive one that was pretty easy to do once you got outside that negative one to positive one realm it got harder and harder and harder to get good estimates of those probabilities with the Advent of computers it's much easier I mean we could even do it and today we're going to look at the normal probability density function for it the CDF function for it the mean variance sample space median quantiles AKA percentiles same stuff we're going to see the function the PDF function we're going to realize okay we don't need to memorize that we're going to see that we can't write out the CDF function in any meaningful way but we'll find out ways of calculating the gaussian distribution apparently was named or no apparently about it it was named after Carl Friedrich Gauss who advocated for its use AKA created it um or at least we think he created those who stem from the German school of thought think he created it his first paper though was 1809. it was LaPlace who published a paper earlier than that using this distribution that's why the French descendants call this the Gauss LaPlace or just the LaPlace distribution but we're going to call it the normal distribution um gal said hey wait a minute LaPlace I was working on this way back in 1794. no evidence of it except gauss's word normal distribution arises from modeling observed Randomness in astronomical and geodetic data it arises in variables that have a specific expected value but demonstrate some minor random variation above and below so for instance this pop that I'm drinking now says that contains 100 milligrams of caffeine so I would not be surprised if the distribution of caffeine in these bottles followed a normal curve that is the mean would be a hundred and there would be some variation above and below because you can't get exactly 100 milligrams of caffeine every time there are two parameters to the gaussian distribution or to the normal distribution the mean and the standard deviation mu and sigma most important distribution in statistics because of the central limit theorem which we'll see in chapter seven so for the record if x a random variable follows or is distributed according to a normal distribution with mean mu and standard deviation Sigma then the PDF is lowercase f of x is this thing notice that the PDF depends on mu and sigma and some Sigma in a couple places however recall to chapter 3 when we looked at the standardized score we can subtract off mu divided by Sigma and we actually come up with another normal distribution it's a normal distribution that does not depend upon mu and sigma it's a standard normal distribution with mean zero standard deviation of one so the normal distribution has two parameters mu and sigma the standard normal distribution has no parameters and it's the standard normal that is tabulated in books here's a graph of the standard normal PDF notice that it goes off in both sides forever this should raise some memories of the empirical rule um you will see sometimes that the PDF is symbolized with a lowercase fee Greek letter Phi lowercase fee lowercase f because it's a PDF and the CDF will be at uppercase fee I don't think I have an uppercase fee on here here's the mean notice it's also the median normal distribution to symmetric so median median will be the same notice also it's the most likely value because the distribute or the the likelihood function the the PDF is highest there so this is the highest likely or the most likely value from this distribution so the normal distribution the mean median and mode are the same value here are the standard deviations at this point you should be able to say hey what I know what percent of the data or of this distribution is between negative 1 and positive 1. and what percent is that no no no no oh I'm sorry I miss heard you yeah 68 percent and what percent is between negative 2 and positive 2. yep about 95 percent and between negative 3 and positive 3 is 99.7 percent ish call the normal distribution is continuous but it's not rectangular and it can be proven that the CDF cannot be written out hence late 19th century early 20th century as well working on estimating those probabilities so you have to use a table or a computer table one calc tabulates this CDF computer calculations are easier to perform however they're more accurate and they are more precise the stem is Norm you have to specify the mean M and the standard deviation s P for the cumulative probability Q for the quantile r for generating random values pqr is the same as it has been with all the other probability distributions stem is Norm and you have to specify M and S so some examples intelligence quotient by Design the IQ measures in the United States follow the normal distribution with mu equal to 100 and sigma equal to 15. this is by Design This Is How They create this test What proportion of the U.S population has an IQ less than 90. in other words if what Rand variable do I use in other words if this is the distribution of IQs in the United States I want to find the area under the curve less than 90. the purple area I want to calculate the probability that X is less than or equal to 90. give that X follows a normal distribution with mean expected value of 100 and standard deviation of 15. here it is in r P for a cumulative probability Norm is the stem for the normal 90 is the value we're specifying m is equal to 100 and S is equal to 15. this comes out to be about a quarter of the population wait a minute I can hear you saying I thought this P only worked when it was less than or equal to what is going on here are you trying to pull something over on us the answer is no remember that X follows a continuous distribution in this case it's a normal distribution that means that the probability of X equaling 90 exactly is zero think back to the uniform distribution it's an area so it's the base times the height if the base consists of a single point then the width is zero and a width of zero times whatever height is going to give you a probability of zero so for continuous distributions less than or less than or equal to are going to be exactly the same what IQ value separates the lower 30 from the upper 70 percent so we're looking for an IQ value that tells me I'm going to be using a quantile I'm looking for the 30th quantile or the 30th percentile so I'm looking for this value here such that 30 percent is in purple Q for quantile we're looking at the 30th percentile so it's 0.3 here and we specify the mean and standard deviation 92.13399 so approximately 30 percent of Americans have an IQ score less than 92.13399 and approximately 70 percent of Americans that's everyone else have an IQ of more than 92.13399 30 it's pretty frequent still so in other words IQ of 90 and no big deal by Design IQs Etc what's the proportion of Americans that with an IQ score between we haven't dealt with any between calculations yet between 87 and 122. so we want this purple area between 87 and 122. for calculating cumulative probabilities it's got to be in the form of less than or equal twos so I can do less than or equal to 122 but that covers everything here not just what we want but notice like that this region is less than 122 minus less than 87. looking for this this is just the probability commutative probability at the upper minus the cumulative probability at the lower so approximately 75 percent of Americans have an IQ between 87 and 122. that's a lot and now we'll go for above we did a less than between now we'll do an above probability of a proportion of Americans have an IQ above 90. so this blue purple area complements rule this this purple area is just one minus the white area 1 minus the white area so and again about a three quarters of Americans have an IQ above 90. so let's do a learning check I'm going to ask some questions I'll pause you answer them on your own out loud because dogs watching you and you really do want to talk to it what type of random variable will have a normal distribution a random variable with a definite Target and some random variation added to it will tend to have a normal distribution what's the difference between capital and normal and lower end normal capital N normal is refers to the distribution itself lower end normal refers to the fact that something is not unexpected what is the sample space of a normal random variable all real values there is neither a lower bound nor an upper bound what is the expected value of a normal random variable very good that's mu it's one of the parameters what is the variance of a normal random variable yes Sigma squared it's the square of one of the other parameters remember there are two parameters mu and sigma the variance is just the square of Sigma what R function is used to calculate cumulative probability for normal yep cumulative probability so it starts with a P for normal random variables so it's p Norm and what our functions use to calculate the quantiles Q Norm Q for quantiles Norm for normal random variable the future we're going to keep working with the normal distribution simply because of the central limit theorem and we're going to use this normal distribution to estimate population parameters but this this last one will be the second half of the course here are the r functions none too surprising the stem is Norm then we got the DPR and Q notice when we were dealing with the discrete we used the d a lot now that we've moved on continuous we really don't use the D I use the D Norm to create those graphs but really you don't use the D Norm function at all you focus on the P norm and the Q norm and the r Norm here are some readings for you are for starters appendix B3 C1 and C2 and Hawk section six one to six four normal distribution in Wikipedia is a good one oh I haven't done the intra lecture quiz or questions here we go question one give an example of a random variable that follows a normal distribution question two what are the two parameters of a normal distribution and question three what are the mean and standard deviation of a normal distribution three again will be really easy two will be pretty easy one that'll take some thought but it'll be easy oops went too far so I guess I'm done hello and welcome to this last section of chapter six this is where we're going to use the normal distribution to approximate the binomial distribution notice this is rather interesting because the normal distribution is continuous and the binomial distribution is discrete so indeed we can approximate discrete distributions using these the The Continuous distributions so by the end of the selection you're going to be able to describe the normal and the binomial distributions you're going to see how the normal can be used to approximate the binomial you're going to calculate the approximate binomial probabilities and understanding why the binomial is useful even if we have a computer to calculate them exactly um so here's the arc we've examined recently discrete distributions of which the binomial is one and continuous distributions of which the normal is one we looked at probability Mass functions means variances sample spaces for the discrete we've looked at probability density functions means variance of sample spaces the CDF can be applied to both discrete and continuous as can median and quantiles today we're going to approximate one distribution with another so the first thing we have to do is explain what we mean by uh approximating one distribution with another short answer is that the cumulative probabilities are close that is if X1 X2 are different distributions that are approximately the same then this relationship holds for all values of little X of course there is a whole lot of detail hidden in that little tilde that I'm sorry that approximation sign um if X1 is approximately the same as X2 how close is close enough how approximate is necessary it's a question of precision and we leave that up to the scientist I've got the statistician scientist comes to me and say that says I need the probability speed within .003 of each other from that information I can say okay I need a sample size of 1300. or something like that um so we're going to approximate the binomial X1 with the normal X2 that is we'd like to determine some rules on NP mu and sigma to ensure that this relationship this approximation relationship is true now surprisingly it's not as difficult as it seems to get a good first order approximation what do we remember about the binomial we remember these five requirements and again you need to have these memorized the number of Trials is known each trial is a Bernoulli trial success probability is constant trials are independent and the random variable is the number of successes in those trials and this led to expected value and variance of NP and np1 minus p so from this it's next natural for us to see just how well this X2 distribution which is normal how well this approximates the binomial where the mean is and P and the variance is np1 minus p in other words we want to see how far apart those cdfs are so here's the CDF of a binomial 5.1 n is 5 p is 0.1 this is CDF here's the graph of a normal 0.5 and 0.45 so this normally is equating to the rule this will be the X2 distribution and this should illustrate how close they are midway between the numbers one two three four five it's right on or really really close the closer you are to one of the integers the worse off it is in fact here is a animated graphic of the difference in cdfs between the binomial and the normal sample size is up here um this is the value of x and the the height here for any value of x is binomial CDF minus normalcdf notice we have spikes at the integer values now I would love to press play but unfortunately this P this Adobe Acrobat type program isn't Adobe Acrobat so I'm going to have to open up this file separately the sample size is changing notice also the worst is getting smaller and smaller and smaller so here we are at about 70. it's the worst is 0.05 we're coming up on a hundred the worst looks to be about 0.045 let me just keep this running and notice that the error the worst error tends to go to zero as the sample size increases but notice that it doesn't go to zero too quickly here we are at the sample size of about 200 coming up on 200 we're going to see that's only about 0.03 where's the area is 0.03 and we start over again and I don't know how to stop that from continuing 0.03 as the difference between the normal binomial CDF that's 0.03 which is a probability so that's a large difference so if the correct probability is 0.57 then this error could be anywhere from or this would be anywhere from 0.54 to 0.60 and that's a big range big error introduced but as you saw it's a sample size increase that the worst absolute error went to zero so large sample size we can go ahead and say the normal and the binomial are close enough that also kind of leads to the physicist who tells me I need to be within 0.03 0.03 I can come back and say okay sample size needs to be at least 200. or the physicist comes to me and she says I want it to be within .003 I'll come back with okay sample size needs to be at least twenty thousand or whatever so one conclusion is the approximation increases as values of n increase we're going to see that again when we get to the central limit theorem in chapter seven um with additional exploration we could come to a second conclusion that the approximation is better for values of P close to 0.5 the example I gave you P was 0.1 so if p is close to 0.5 we don't need as large as sample size as we do if p p is 0.1 so combining these two observations leads to the following quote rule of thumb and this rule of thumbs tends to change from source to source um the normal distribution is sufficiently close to the binomial if both NP and N times 1 minus P are at least five I happen to think 15 or 20 is a better rule for that and if I need a lot of precision that I'll need NP to be at least 1000 or maybe two thousand and the approximation is improved by using quote a continuity correction of 0.5 added to or subtracted from the number value we'll have an example of that shortly this is not the best cracking but it's a good correction so we're going to use a con continuity correction factor to describe the area under the normal curve that approximates the probability that at least two people in a math class of 50 regularly cheat on their tests assume the number of people in the math class 50 who consistently cheated on their test has a binomial distribution with a mean of five and a standard deviation of 2.12 . so in other words if x is the number of students cheating in the class X follows the binomial of 50 with a p of 0.1 we need to calculate probability that X is greater than 2. so we're going to begin by converting the discrete number 2 into an interval by adding 0.5 and subtracting 0.5 from it I have discreet number two is changed to a continuous interval from point from 1.5 to 2.5 here's a picture of that so for the discrete we could just use two for the continuous because the probability of x equal to is zero just like x equal to any single number we need to change it into an interval so that 2 will be replaced by the interval from 1.5 to 2.5 now we're going to draw a normal curve with the mean of 5 and a standard deviation of 2.12 those were given to us and indicate the interval from 1.5 to 2.5 to represent the number two that's what this is the red curves the normal of mean five standard deviation of 2.12 and I've got the value of 2 in the double shaded or the Shaded and hashed now we're going to shade the area corresponding to the phrase quote at least two because we need to calculate the probability of at least two and that's the blue shaded area it's this hashed part which is 2 and above so in other words if x is binomial and Y is the normal approximation probability of X greater than 2 is about probability of y greater than or equal to 1.5 because it's everything blue if I were calculating the probability of X being less than 2 it would be y less than or equal to 2.5 use a normal distribution to estimate the probability of more than 55 girls being born in 100 births so exactly we need to calculate probability X is greater than 55. given X follows a binomial distribution out of 100 P of 0.5 using a normal approximation that means we're going to define a y variable to follow a normal distribution y follow a normal distribution with the expected value of 50 100 times 0.5 and variance of 100 times 0.5 times 1 minus 0.5 and we're going to calculate the probability that Y is greater than 40 uh I'm sorry that Y is greater than 54.5 there we go now we could calculate the exact answer of 13 percent 13 5 6 2 65. notice how close the approximation is though this is using the p binom and this is using the P Norm very close and the sample size is only a hundred I say it's very close we're off on the fourth digit fifth digit so if I need Precision beyond the fifth digit then I'm going to have to have a larger sample size if I did not use the continuity correction I'd be much further off after many hours of studying you believe you have a 90 or 90 probability of answering any given question correctly test is 50 true false questions oh that would be horrible if I gave you 50 true false questions for the test or would it now I wouldn't do that assuming that your estimate is the true probability that you will answer a question correctly let's estimate the probability that you'll miss no more than four questions so each of the 50 true false has a probability of 90 being right and we want to say missing no more than four questions so the probability of missing will be 0.1 50 questions this is the exact distribution of X this is the exact probability to calculate we can approximate this binomial with a normal y will follow a normal distribution of expected value 50 times 0.1 and variance of 50 times 0.1 times 0.9 and we'd calculate the probability that Y is less than or equal to 4.5 it's too far 40 percent the real answer is 43 percent so I'm off by two and a half percent many toothpaste commercials advertised at three out of four dentists recommend their brand of toothpaste using a normal distribution to estimate the probability that in a random survey of 400 dentists so we'll be using random sampling simple random sampling exactly 300 will recommend Brand X toothpaste we're going to assume the commercials are correct and therefore there's a 75 percent chance that any given dentist will recommend Brand X so if x is the number in that sample of 400 x follows exactly a binomial and a 400 P of 0.75 and we need to calculate exactly x equals 300. the approximation y will follow the normal distribution expected value of 400 times 0.75 variance of 400 times 0.75 times 0.25 and we need to calculate the probability that Y is between 299.5 and 300.5 and we get 0.046 0403 and the real answer is 0.046 O2 432 so the normal distribution with the continuity correction did a very good job here as well without the continuity correction this would be terrible in fact without the continuity correction the probability estimated would be zero because you'd be calculating the probability that y was equal to 300. so learning check what's meant by the normal distribution approximates the binomial correct their cdfs are quote close what are two requirements for the approximation to be quote good right n times p is at least five and N times 1 minus p is at least five again I would recommend higher numbers for those if I really really care about the results how do we make the approximation better yes larger sample size you'll find in statistics at a larger sample size solves a whole lot of problems from the economic standpoint however larger sample size causes problems because it costs time money other resources to collect that larger sample size what is the continuity correction why is it used very good continuity correction is used to help ensure that the normal approximation to the binomial is quote better continuity correction itself is to treat an equals as adding 0.5 and subtracting 0.5 and calculating that interval in the future we're going to explore the normal integrator detail we're going to use the central limit theorem which will be chapter 7 and discover that the binomial is not special the normal distribution is um here's a question that you may have we have a computer we can easily calculate these binomial probabilities exactly why do we still need to approximate a binomial with the normal and here here's the answer the examples that we're given today we're given where you could calculate exactly and approximately and compared just so you get a feel for how how good this approximation is when we get to trying to estimate a single proportion we're going to stick with the exact way of doing it using the binomial distribution and everything we know about the binomial however when we start to look at comparing two proportions or trying to estimate the difference between two proportions to population proportions we won't be able to use an exact distribution because we would need to find the distribution of the sum of two binomials and that doesn't exist however we do know the distribution of the sum of two normals that's the normal distribution so we would use we would have to use the normal approximation when we start comparing two proportions until we get there we can use the exact form now with that said some books and I think Hawks does this Hawks will use the normal approximation for even the one population parameter s population proportion estimating so there is reason to look at this approximately in the binomial not just it's the next section in the chapter here are the r functions we've seen these already d by Nom P by Nom and P Norm here's some readings appendix C and R for starters and Hawks section six five and then Wikipedia Central limit theorem appendix C and the central limit theorem and Wikipedia will give you a good background for chapter seven and this approximating the normal approximating the binomial distribution with the normal is actually a result of the central limit theorem so questions where are those questions there they go question one again I recommend writing these questions in the left hand side of your notes answering them below so you can transform the trans put them into Moodle quiz what does it mean for one distribution to approximate another question two what are the mean and the standard deviation of a binomial question three what are the mean and standard deviation of a normal distribution and that's it take care hello and welcome to section 7.1 where we introduce the central limit theorem Central limit theorem I would argue is the most important theorem in all of introductory statistics in fact I'd argue it's the most important theorem in all of Statistics it helps to explain that when we have data that comes from a continuous distribution and we're trying to estimate the population mean we don't really care that much about the distribution of the data we just quote pretend that the data comes from a normal distribution because the central limit theorem says Hey regardless of where the data comes from the sample means follow a normal distribution and it's the sample means that we're going to use to understand our population mean in the future so from this point forward when we're dealing with continuous data the sample means are going to follow a normal distribution or it's going to look like a normal distribution and we see that in lab C so by the end of this lecture you're going to be able to State the central limit theorem say the requirements for applying it and state its consequences so in recent Computing activities we've examined drawing random samples from a known distribution we've looked at calculating sample means from subsets of those distributions we've created histograms of the distributions of those sample means today we're going to examine the central limit theorem which kind of gives a mathematical explanation for everything that we've observed about those sample mean distributions so here's the statement of the central limit theorem let X be a random variable with a mean we're going to call that mean mu and finite variance Sigma squared and we're going to draw a random sample of size and from this distribution so that first paragraph is all about the data itself the data comes from some distribution with a mean and a variance second paragraph then the distribution of the sample sums converges to a normal distribution as n gets larger so the first paragraph said we don't care about the distribution of the data second paragraph says therefore the distribution of the sample sums converges to a normal distribution so the data could be exponential the data could be uniform the data could be binomial for All We Care it's the sample sums that are going to be normally distributed as n gets larger specifically this thing on the left is just how we represent sample sums we're adding up this is a big summation adding up over all n values in our sample those values it converges in distribution that's the D on top of the arrow means it converges in distribution to a normal with the expected value of n times mu and a variance of n times Sigma squared we've actually kind of seen something like this in the past but let's keep working forward the proof of this theorem is beyond the scope of this course I think the first time you get to see it is in math 321 happens at the end of math 321 after you've learned about these things called moment generating functions um so our first intra lecture question is here again on the left hand side of your notes write the question write the answers so that you can come back to it later remember you got the pause button so there are three main consequences um Central limit theorem tells us the following one it tells us that the sum of independent random variables is more normal than the distribution the variable itself of course if the data do come from a normal distribution you already start at normal so you don't have any place to go but if you start at uniform it takes a little bit to get to normal if you start exponential it takes a little bit longer to get to normal um recall that the binomial is the sum of independent or newly random variables we use that fact to prove things about the binomial specifically the expected value and the and the select the value in the variance poisson happens to be the sum of independent poissons we've alluded to that in the past but that means that as n increases the binomial becomes more normal and as Lambda increases the poisson becomes more normal it's because the poisson is just the sum of independent poissons and binomial is the sum of independent bernoullis kind of powerful right there a second consequence is because the sample mean is just the sample sum divided by constant the central limit theorem tells us that the distribution of sample means will tend towards normal and again if the data starts with the normal this tinting towards happens immediately if it starts with the uniform it takes like 10 to get there if it's an exponential it takes like 50 or so but eventually those sample means will become as close to normally distributed as if you want them to be you just have to get a larger n sometimes and the third the speed of convergence depends on how close the data are distributed to normal the closer they are to normal the faster thus the normal if the data actually do come from a normal distribution then the sample means are immediately normal the poisson is closer to normal than the since many of the binomials so you'd need a Lambda smaller than the and for binomial so poissons converge faster let's look at the next introduction question it's going to ask you about the uniform and the exponential oh no it won't well it will pretty soon so question two is State the second consequence of the central limit theorem and then I'm going to pause then I'm going to go directly to question three which talks about the third consequence since the speed of convergence depends on how closely the data are to normal as the missing L which of these two will converge the fastest if the data follows a uniform well that converge well the sample means converge faster to the normal than if the data follow an exponential remember to write down the question on the left answer it so that you can transfer it into Moodle um again the data follower uniform well the sample means converge to normal faster than if the data Fallen exponential pause moving back um so here's one of the big uses that is used sometimes let X follow a binomial distribution remember there are two parameters for binomial n and p we're going to use the central limit theorem to approximate the distribution of X we actually did this back in section I believe it was section six five but here we're going to see why it actually works or we're going to apply a mathematical reasoning for why it works remember the binomial is just the sum of N independent bernoullis that is if y sub I follows that Bernoulli then X being the sum of those y's follows a binomial of NP so according to the central limit theorem since it's the sum of independent distributions the x is going to be approximately normal with a mean and a variance that mean is just going to be the mean of this binomial well what's the mean of a binomial it's n times p and the variance is just going to be the variance of this binomial well what's variance of a binomial n times P times 1 minus p so this gets us directly to the approximation of the binomial with the normal in about one step more importantly for us okay never mind um we're going to let the X for another example follow a uniform a b a is the minimum value B is the maximum value we're going to use the central limit theorem to estimate the distribution the mean of a sample of size n remember that X bar the mean is just one over n times the sample total and we represent the sample total this way and so by the central limit theorem T follows approximately the dot on top of the tilde means approximately follows a normal distribution n times mu and N times Sigma squared where mu is the mean of the underlying distribution of the X distribution and sigma squared is the variance of that underlying distribution of the X distribution from our knowledge of the uniform we know that the mean of the uniform is just a plus b over 2 and the variance is B minus a squared over 2. so the sample sum is approximately normally distributed with expected value n times a plus b over 2 and variance of n times B minus a squared over two over twelve remember the question was originally about this distribution the sample mean and X bar is just one over n times t that means the that X bar approximately again dot on top means approximately follows a normal distribution expected value of mu n times mu divided by n and N times Sigma squared divided by N squared so the expected value of x bar is a plus b over two which happens to be mu and the variance of X bar is B minus a squared over 12n now notice something that we didn't notice before the expected value of x bar is Mu the variance of X bar is smaller than the variance of x so as n increases the variance decreases to zero which means these X bars that we actually measure from a sample The observed X bars are going to tend to be closer to the MU so in other words as n increases the Precision of our little X bar increases so the big question that comes in here is why is there an n in the denominator of the variance recall that the variance is find as the sum of x minus mu squared times the probability of that x value so the variance of a times x is just the sum of a times x minus a times mu squared times the probability of the x value factor out an a from both of these out front that gives us an a squared times x minus mu squared times the probability of the x value this is just a constant has nothing to do with the I pull it out so at a squared times the sum of the x minus mu squared times P of x so the variance of ax is just a squared times the variance of x in other words where does that squaring come from comes from the fact that the random variable itself is squared in a variance so when we found the variance of X bar that was just the variance of one over n times t pull the 1 over n out front it's 1 over N squared times the variance of t there's one over N squared this n times B minus a squared over 12 that's the variance of t the n on top cancels one of the ends of the bottom that gets us our variance of X bar so a learning check what is the main consequence for the central limit theorem I'll just pause for a little bit you can go ahead and hit the pause button to give yourself some more time the main consequence for the central limit theorem is that the sample means will become more and more normal what are the requirements for the central limit theorem the expected value and variance both have to be finite and the X's have to be independent why does this mean we should focus on the normal distribution as we move forward in the course when we care to estimate the population mean we're going to use the sample mean and since Central limit theorem tells us the sample means are going to eventually follow the normal distribution as long as n is large enough then we should just focus on this normal distribution R for starters this is appendix C3 Hawks this is section 7 1 um demov LaPlace theorem is interesting of course Wikipedia's got a really good entry on the central limit theorem and that's the N for Section seven one um the central limit theorem and by the way you will see the central limit theorem at least in the next two lectures but in undergirding all lectures from this point forward in this course so enjoy hello and welcome to section 7 2 last section we looked at the central limit theorem here we're going to be applying the central limit theorem to the distribution of sample means um so by the end of this lecture you should be able to State the central limit theorem and to apply the central limit theorem to the problem of sampling distribution of the mean or what most books just refer to as the sampling distribution so here's the arc the way what we've been talking about so recently we've examined drawing random samples from a known distribution we've calculated sample means from subsets to those distributions we've created histograms of those distributions we've seen that those histograms tell us that the sample mean distributions look pretty darn normal in the last slide deck we introduced the center limit theorem that says yes they do look pretty darn normal and that's not that's not an accident we've looked at Central limit theorem and its requirements and its consequences and the main consequence is the distribution the sample means tends towards normal as the sample size increases So today we're going to examine the center limit theorem in terms of what it tells us specifically about the distribution the sample mean in the next lecture it'll be about the distribution the sample proportion but if you think back to section 7 1 in fact if you think back to when we first introduced the binomial distribution we realize that the sample proportion and the sample mean are not that different at all mind blown so here's the statement of the central limit theorem again let X be a random variable with a finite mean and finite variance let us draw a random sample of size n from this distribution remember random sample means it's an independent sample each of those X values are independent of the others and again note that that first paragraph talks about the distribution of the data so the data have a mean and a variance and the data were generated from some sort of independent process that's all we need from the data the data is the prerequisite to the central limit theorem so then and here this part is the consequence of the central limit theorem the distribution of the sample sums converges to a normal distribution notice the only thing we need from the data from paragraph one is that it has a mean of variance in this from a random sample that tells us paragraph two that the distribution of the sample sums converges to a normal distribution so here's our first intra lecture question State the central limit theorem as I have stated it in this lecture that question is pretty familiar to us again write the question on the left hand side answer it on the left hand side of your notes therefore when you get to Moodle you can write it in nice and easily here's a corollary of it it's the distribution the sample mean so we're going to start with X being a random variable mean mu finite variant Sigma squared draw random sample size n from this distribution in other words that first paragraph notice is exactly the same as the first paragraph of the central limit theorem therefore those tell us that we can now apply the central limit theorem consequence of this is the distribution the sample mean is approximately normal here's some notation X bar is the sample mean sub n we're going to index it by n to indicate the sample size this converges in distribution to a normal expected value of mu variance of one over n times the variance of the original data here's the proof from the central limit theorem we know this is true we get this from that first paragraph this first paragraph same as in the central limit theorem so this is the consequence that first paragraph boom now we wonder about the distribution of X bar well X bar so that is just one over n times the sum of those X I's okay look at this sum of the x i is what distribution does that have normal boom expected value of some of the X sizes n mu expected value of one over n times the sum of the X I's is Mu the variance of the sum of the X I's is n Sigma squared the variance of one over n times those X Out of the sum of the X I's is one over n times Sigma squared why are we dividing by n instead of having just Sigma squared well set proof one this is y the expected value of x bar is Mu I'll find the expected value of x bar sub n substitutes pull out because expected value is a linear operation expected value of ax is a times the expected value of x in this case a is at one over n we know the expected value of T is just n mu the ends cancel that was easy set proof two with the variances if we want to find the variance of X bars of n substitutes 1 over n times t pull out that one over n it's 1 over N squared why is it now squared look back to the lecture on Section 7 1. because there's a square in the calculation of the variance of the random variable variance of T is just n Sigma squared that n and one of these cancel out so we're left with one over n times Sigma squared and that brings us to the conclusion of the Corollary and this is exactly what we just showed so let's have a couple intra lecture questions two how does the distribution the sample mean depend on the distribution of the data and might as well do question three now while we're at it since the speed of convergence depends on how closely the data are to normal I got the L in there finally I edited it which of these two will converge fastest in the uniform or the exponential yep question one and three are from the last slide deck question two it's kind of the Q is the most important part of this slide deck so let's go back to some examples example one let's say I draw a sample of size 14 from a population or that has a mean of 126 and a variance of 42 what's the approximate distribution the sample means notice I didn't tell you that X followed a normal distribution I didn't tell you X followed a uniform distribution I didn't tell you X followed an exponential distribution it doesn't matter I know X has a finite variance therefore I can apply the central limit theorem whether or not n equals 14 is large enough for this approximation to be good that's another question all I want is an approximate distribution the sample means so this is a straightforward application of the main corollary from this section approximate distribution the sample means X bar is approximately normally distributed with expected value mu of X bar being 126 the same as the expected value of the original distribution and variance of being one over n times 42. 42 is the original variance or I'm sorry the variance of the original distribution it's 1 over n times that Sigma squared 42 over 14 is 3 so X bar approximately normal mean of 126 variance of 3. example two I've been told that the average adult height for males in the United States has mean of 69 inches in standard deviation of 3 what's the probability of having the mean of a sample of size 2 being less than 65 inches so in the last example straightforward application we didn't do anything with the distribution of X bar here we're going to do something with that X bar we're going to ask what's the probability of observing an X bar being less than 65 inches um so we're asked ultimately what is the probability of having the mean being less than 65 we're asked to calculate this now if you want to do a sub 2 here that would be fantastic so we're asked to calculate the probability of X bar sub 2 being less than 65. that means we need to know the distribution of X bar sub 2. we have we're trying to solve a probability statement about X bar sub 2 we really need to know the distribution of X bar sub 2. so to calculate this we're going to use the central limit theorem X bar sub 2 is just is approximately normal expected value the same as the original distribution and variance equal to one over n times the original variance notice I told you the standard deviation was three therefore the variance is nine so we know X bar sub 2 approximately normal mean of 69 variance of 4.5 now we use that to answer our original question about the probability of X bar being less than 65. using R this is p Norm remember it's got to be a less than part 65 is from here m is equal to 69 s which for R needs to be the standard deviation that's just the square root of the variance the variance is 3 squared over two so this our code will get us 0.0297 in other words there's about a three percent chance that I will observe at average being less than 65. given that the mean of the population is 69 and the variance of the population is 9. in other words this property this this probability is small and therefore my beliefs about the original population are unlikely that's the probability of observing this event given our assumptions are correct is quite small so either I did not observe this event or my assumptions are not true so example one straightforward application of the central limit theorem example two okay now that we've got the distribution of X bar let's see how we can use this okay now my sample is size 10. same mean and variance of the adult height for males um but now I'm doing a sample of size 10. I want to know what's the probability of that sample of size 10 being less than 65. same statement it's probability of X bar being less than 65 except in this case n is now 10. so from the central limit theorem X bar follows an approximately normal distribution with mean of 69. from the original distribution and variance of one over n times the original variance and the original variance is 3 squared so X bar now follows this distribution notice the expected value the same the variance is smaller so as the sample size increases the variance of X bar gets smaller so again I want to calculate probability of X bar being less than 65 this is p Norm of 65. mu m is equal to 69 s standard deviation is just the square root of the variance of 0.9 this is .000124 so again very small probability meaning if everything I said is true then the probability of me observing this is small incredibly small it's one and ten to the negative fifth it's one in ten thousand since the probability of me observing this is so small I no longer really believe all of my assumptions maybe mu for the population isn't 69. maybe the variance of the population isn't nine maybe the distribution of the population is so nonnormal that the central limit theorem is not helping when n is equal to 10. but still there is a one in ten thousand chance that my assumptions are correct and I observe this event so it is possible but it really draws brings into really makes you doubt the assumptions fourth example crime uh the 2000 crime rate for the 50 states plus DC are given in the data file crime we need to find a 95 confidence Central confidence interval for the mean violent crime rate this is not the first time you've seen the word confidence interval um that would be one of the labs so here we're asked to calculate the 2.5th and the 97.5 percentiles of the sample means drawn from the 2000 violent crime rate why is it the 2.5th and 97.5th note the difference is 95 and no she got 2.5 percent below and 2.5 percent above so that gives us the central part so those two quantiles will give us the central the endpoints of that Central 95 confidence interval it's a confidence interval so it's going to be an interval on the means the confidence interval so it's going to be an interval on the means probably want to put a star there some googly eyes staring at it and big expression of wow one way of estimating this confidence interval is to play the corollary to a central limit theorem from the data we've got a mean of 441.55 and a standard deviation of 241.45 thus by the corollary Central limit theorem we have X bar is approximately normal expected value of 441.55 and variance of 1 over n times the variance of the data itself and so the endpoints for the 95 confidence interval of the 2.5 the 97.5 quantiles of that distribution which in R we can do this way let's look at all the parts that's a q q Norm is for the quantiles of a normal distribution these are the two percents that you want or the two proportions you want the 2.5th percentile and the 97.5 percentile got a put them together in a c function followed by a comma followed by how you're defining this distribution mean of 441.55 standard deviation of 241.45 divided by the square root of 51. I could have written this as the square root of 241.45 squared over 51 but the square root of a square you're just pulling that 241 through 45 out front so in other words this is really just s over square root of n so we're 95 confident that the actual mean is between 375 and 508. and that's finite crimes per 100 000. confidence interval is on the mean um review back what an observation interval is that's on all the observations you've seen recall back with a prediction interval it's on all the observations you will see in the future confidence interval is about the means the sample means we could also in this part is bootstrapping but also estimate the confidence interval from the data itself using a process that's called bootstrapping here's the code to do it and this will give us a 95 confidence interval from 380 to 510. I'm going to go through the line to this code very carefully shortly but notice that the confidence interval you get through bootstrapping is 385 10 but what we got from using a normal approximation was 375 to 508. in other words not too different Okay so five lines six lines here are the two key lines they're inside the for Loop actually this line is key and this line is key so let's look at the x equals line we're drawing a sample for the violent crime rate in 2000. it's going to be a random sample that's what the sample command does draws it and allows you and it does replace equals true which means it allows you to draw a state's crime rate more than once if replace equals true isn't there then this is just going to give you the same 51 values in a different order each time but the same 51 values and if it's the same 51 values the mean of those values will be the exactly the same each time so we need to replace equals true to do bootstrapping so this x equals line will draw a random sample from the violent crime rate in 2000 allowing for states to be chosen multiple times stores it in the variable X second line second important line we calculate the mean of that sample we're going to store it in the variable called MN MN is for mean but we're going to store it in this variable bracket I bracket that is done to allow us to remember or to allow R to remember each of those sample means so x equals that gives us the sample this line gives us the sample mean of that sample storing it in the variable m n in the ith position and bracket I bracket will mean that this is stored in the ith position we're going to do that 10 000 times that's what the for Loop does first time through I is equal to 1 because 1 colon 1e4 is one two three four all the way up to ten thousand first time through I is equal to 1 draws a sample calculates the mean of that Sample Stores it in the variable MN in the first position closing brace goes back up here I is now equal to 2. draws another sample calculates the mean of that stores in the variable M and in position two closing brace goes back up here I is now equal to three draws a ram sample calculates the mean stories in position three this is continued ten thousand times so at the end of this at the end of this loop at the end of these four lines the variable m n will con contain 10 000 values each of those ten thousand values will be a sample beam from this distribution the bottom line gives us the endpoints of that 95 confidence interval remember this is sample means so an interval on Sample means will be the confidence interval quantile function you give it the vector first and then you specify it's the percentiles you want so we've explained every single line there except the first in R when you're building a vector such as the way that we're doing it here you need to tell R to set aside memory for that vector this line tells our hey we're going to create the vector MN it's going to be a numeric Vector it's going to hold numbers and you just need to set aside memory so that we can build it and then this Loop will build it and then at the end we're going to use that MN I strongly suggest at this point you type this in you run it and you see that it actually does work your confidence interval will be slightly different why will your confidence interval be slightly different or why could it be slightly different because this is a random sample we're drawing a random sample so your random sample will be different from my random sample the fact that we're doing this ten thousand times means that the interval endpoints are probably going to be really really close if you want to make them even closer we do this a million times one E6 so here's the question why is there a difference between the two confidence intervals doing it bootstrap it's three five ten doing it using the normal approximations 375508 the reason it's different is here we're assuming that the sample means follow normal distribution and here we're just we're not making that assumption here we're saying okay data I don't care what distribution you have I don't care what the distribution of the sample means actually is I want to see the I want to have a good estimate for that confidence interval since the endpoints are so close to each other that tells me that this normal approximation is not a bad approximation when the sample size is 51. if the sample size were 5000 I would expect the this interval to be the same as this interval if the sample size were 5 I would not be surprised if this interval and this interval were very different they give you all of them I did give you all of them okay so learning check what's the main consequence of the central limit theorem you can hit pause come up with an answer and then I'll give you the answer the distribution of the sample means is approximately normal what is the sampling distribution for the mean sampling distribution for the mean is normal with expected value of mu and variance of Sigma squared over n how does the sampling distribution for the mean depend on the distribution of the data if the data are normal then the sampling distribution for the mean is exactly normal if it is not normal then the farther the data distribution from normal is the larger the sample size needs to be for this to be a good approximation what is bootstrapping and what is its greatest use bootstrapping is using the data under repeated sampling to estimate a population parameter and I would argue its greatest use is to estimate the confidence interval for the mean section 72 in Hawks appendix C3 and of course Central limit theorem you may want to also look at Wikipedia bootstrapping if you've got questions on bootstrapping and that's it see you in section 73 hello and welcome to the last section of chapter seven chapter seven covered the central limit theorem or covers the central limit theorem of two of its most important applications section seven one you were introduced the central limit theorem 72 you saw its application to sample means in this section you're going to see it's uh its application to sample proportions so by the end of this lecture you should be able to State the central limit theorem apply the central limit theorem to the problem of the sampling distribution of the proportion estimate the sampling distribution for the proportion and understand the relationship between the sample size and the Precision of the estimate we've been hinting at facts that there is a a strong relationship between sample size and precision for a long time here we're going to actually see it in the form or in the guise something we call power here's the Arc in recent computer activities the scas we've examined random draws a calculating sample means from subsets histograms from those we've seen that those histograms illustrate that the sample means look pretty normal and then we looked at the central limit theorem and saw oh that makes sense because the central limit theorem says sample means are going to tend to be normal especially if the sample size is large and the last slide deck we saw this application to the sample mean so boom today we're going to examine the central limit theorem in terms of what it tells us about the distribution the sample proportion aka the sampling distribution of the proportion so last time it was the mean this time it's the proportion so here's the the central limit theorem again the first paragraph is the premise these are things that must be met before you can apply the center limit theorem notice that the premises premises premises speak about the distribution of the data the consequence of the central limit theorem the second paragraph talks about the distribution of the sample totals so let X be a random variable with being mu finite variance Sigma squared let us draw a random sample in other words all the X's are independent of size n from this distribution notice we didn't specify that the data have to be normal or exponential or uniform or Gamma or beta or whatever we just said it has to have a mean and finite variance and the data have to be random draw from this distribution consequence then the distribution the sample sums converges to a normal distribution specifically T which is the sum of the X I's converges in distribution to a normal with expected value and mu and variance n Sigma squared which brings us to our first intralecture question State the central limit theorem as I have stated it in this lecture so again write this over on the left hand side answer it below this is really important because a lot of students in the past have thought Central limit theorem says the data become more normal that's not true the data central limit theorem doesn't tell us anything about the distribution of the data it requires that the data has a finite variance and a mean but it doesn't say therefore the distribution of the data becomes more normal no it says the distribution of the sums becomes more normal um there we are so there's the theorem statement here's the corollary for the sample proportion we saw one of these with the sample mean we're going to see it for the sample proportion let X follow a binomial distribution with parameter values n and p remember N is a sample size and P is the probability of success on each of the trials that the X be a random sample from n Bernoulli random variables notice again remember back to 7 1 I framed the binomial as being just the sum of independent Bernoulli's we're using that here then the distribution of the sample proportion which I'm going to call big p is which is defined as 1 over n times x the number of successes from that binomial or the number of successes from those and Bernoulli's so this is 1 over n times x I've seen one over n times x before I forget what that was converges in distribution to a normal with expected value Little P hmm and variance of P times 1 minus p over n so here we are for interlection question number two if I can find it what is the distribution of the sample proportion this is an important one also while you're here look at how this distribution compares to the distribution of the sample mean going back so here's the proof from the central limit theorem we know X follows in proximate normal distribution expected value NP variance np1 minus p that's straight out application the central limit theorem and P is the expected value of x and P1 minus p is the variance of x remember we want to find the distribution or the approximation distribution of P the sample proportions so expected value of P which is expected value of x over n pulling out that one over n because expectation is a linear operator we get expected value of x over n we know the expected value of x from a previous slide is just NP the ends cancel out we're left with p so the expected value of p is Little P so the expected value of your sample proportions is your population proportion normal distribution has an expected value and a variance so let's calculate the variance of your sample proportions again for step substitution we're going to factor out that n as an N squared check back to section seven one why we're factoring out an N squared has to do with the fact that the variance you square the random variable variance of X is np1 minus p this n and one of these cancels out P times 1 minus p over n therefore sample proportions are approximately normally distributed expected value of P variance of P times 1 minus p over n notice the expected value of p is Little P since the expected value of p is Little P this is an unbiased estimator and notice that as the sample size increases the variance also I'm sorry sample size increases the variance decreases in other words as n increases the Precision will increase as well the variance decreases Precision increases so there's our result some examples according to the U.S census 18 of Americans are below the poverty line if I randomly sample 10 people from the United States what's the probability that more than 20 of them are below the poverty line so notice I'm asking a probability question it's what is the probability that more than 20 percent are below the poverty line so I'm asked probability of P being greater than 20. I'm sorry 0.20 since we're doing a pro a uh trying to calculate a probability about the random variable P we need to know the distribution of P from the corollary to the central limit theorem that we covered today P approximate is approximately normal expected value of 0.18 because that's me that's p in the population and variance of P times 1 minus p over n so P big p is approximately normal mean at 0.18 variance to 0.01476 I need to calculate the probability that P is greater than 0.2 remember in order to use cumulative probabilities this has to be a less less than complements rule from chapter four says probability P being greater than something is one minus the probability of P being less than or equal to it we've got a less than or equal here so we can use the P Norm 0.2 0.18 and this is standard deviation so it's the square root of the variance that gives us a probability of 0.4346 that's not small not at all so it wouldn't shock me if more than 20 of my sample is below the poverty line assuming that the the population me uh population proportion is 0.18 then the probability of me observing more than 20 percent being under the poverty line of is 0.43 doesn't give me any evidence against my assumption of the 0.18 it's a coin flip essentially about half the time I'll be above 20 percent about half the time I went below same assumption of the population proportion is 18 percent I'm now going to sample a hundred people instead of what I did in the previous example increasing the sample size and I want to calculate the same probability probability more than 20 or below the poverty line so again we're asked to calculate the probability that P is greater than 0.2 we need to know the distribution of P from the central limit theorem it's 18 18 times 1 minus 18 over n and here is a hundred this gives us that distribution I need to calculate the probability P being being greater than 0.2 it's one minus probability of P being less than or equal to 0.2 and I get a probability of 0.3 so also not a small value it's about a third of the time I'll get a a sample proportion being greater than 20 percent if our assumption about the population is true that is if the population poverty rate indeed is 0.18 notice the probability did go down though the previous example when n was 10 this was 0.46 4.47 it's gone down it's 0.30 which makes sense because I collect more and more data I would expect my observations to average out being closer to the real average now my sample size is a thousand I'm going to ask a thousand people the first it was 10 then 100 now a thousand we're going to look at a 95 confidence interval for the sample proportion confidence interval is on those sample statistics not on observations that we've seen which is an observation interval not on observations we're going to see in the future which would be a prediction interval but on the sample statistic in this case sample proportion so with a thousand I would expect to see 95 of the time somewhere between 0.156 and 0.204 . so if reality is correct I'm sorry if my assumption about reality if 18 is correct and I collect a sample of size a thousand I expect 95 of the time the sample proportion is going to be between 15 and 20 percent how to get that I've got the proportions normally distributed so I'll use the norm stem I'm looking at the quantiles so I'll use Q Norm I'm doing the two point fifth percentile the 97.5 percentile and the rest just specifies that particular normal distribution now I'm going to ask a hundred thousand Americans and I want to know the probability that more than 20 of them are below the poverty line so I did ten hundred thousand now a hundred thousand and my confidence interval is 17.8 to 18.2 so I would expect 95 of the time my sample of a hundred thousand to be between 17.8 and 18 point got the two and the eight switched between 17.2 and 18.8 so if I fell out of the interval of 17.2 to 18.8 I'd say well there's something wrong with my assumption of 18 percent here's a graph of the probability that the observed sample proportion is greater than 0.2 as graphed against the sample size our first example is 10 n equals 10 our probability of observing it was way up here at about 0.46 I think the second n was a hundred and the probability we got was about 0.30 so as the sample size increases the probability of observing a extreme event decreases doesn't go away I mean here we are with n equal to 1000 we've still got a probability of about 0.08 so about eight percent of the time that sample of a thousand will give me a sample proportion greater than 0.2 we can also look at this in terms of confidence intervals as the sample size increases the upper confidence bound and the lower confidence bound get closer and closer together they never become the same even out here at n equal to a thousand there's still a sizeable gap between the two so this is where power comes in power is the ability to distinguish between a true and a false null hypothesis in other words we are assuming that the sample I'm sorry we're assuming that the population proportion is 0.18 power is the ability to say no it's not given our observations so if we observe P being greater than 0.2 the probability of us saying no that 0.18 is wrong gets bigger and bigger as sample size increases in terms of confidence intervals remember the width of the confidence interval is the Precision the smaller the width the higher the Precision as the sample size increases our Precision increases most of the benefit happens in the first couple hundred what this means is as the sample size increases we're going to observe sample proportions closer and closer to our population proportion we still get some above and some below but there will be much more concentrated around the sample proportion which brings us to intralecture question number three what is power when we get to chapter 10 we'll have a much better understanding of power but you gotta dip your toes in here so learning check what's the main consequence for the central limit theorem the main consequence for the central limit theorem is that that means a sample means and Sample proportions are much more normally distributed than the data itself what is sampling distribution for the proportion the sampling distribution proportion capital P is approximately normal with expected value of Little P and variance of P times 1 minus p over n would affect the sample size have for precision and estimating the sample proportion as the sample size increases the Precision increases as well that was section three you may also want to glance through appendix C3 and Central limit theorem you may also want to look at Power in statistics on Wikipedia and that's it for chapter seven in chapter 8 we're going to use all of this Central limit theorem stuff to start creating confidence intervals and in chapter 10 we're going to look at hypothesis testing um but in all reality we understand what confidence intervals are at this point but when we get to chapter eight we're going to get a much deeper appreciation for confidence intervals and that's it hello and welcome to the video lecture on chapter 8 where we introduce the theory of confidence intervals and how to calculate them by hand which we should never want to do because calculating things by hand introduces errors in many many places it's much better to learn how to do these with a computer which will be the next lecture learning how to do confidence intervals on a computer specifically using the r statistical environment um I guess we get to start by the end of this lecture you should be able to State what a confidence interval concerns what it is State the theory behind calculating those confidence intervals and understand that they represent a proportion not a probability but a proportion um so here's the entire Arc of the course or the story of the course thus far the second examination would have taken place right last week if we had a typical course so before that back when we were talking about probabilities we calculated three types of intervals uh We've calculated the observation intervals the confidence intervals and the prediction intervals and these three were actually intervals about different things even though it's all based on the data they all were intervals about different things the observation interval is intervals on observations that we have already had prediction interval is on observations that we will have in the future so a 95 prediction or role tells us that 95 in the future we will be within this range whereas a 95 observation interval tells us that of everything that we've observed so far 95 of those observations are in this range now contrast both of those with the confidence interval recall that the confidence interval was about the population I'm sorry about the the sample statistic so we had confidence intervals looking at the sample mean or confidence intervals looking at the sample proportion or confidence intervals looking at the sample variance and chapter 8 onward we're going to be using these confidence intervals in a much more mathematically balanced way um laying out the theory today we also examine the central limit theorem when we looked at the normal distribution a lot and I when we talked about the central limit theorem I said this tells us this this theorem is so important that it tells us that we can just focus on the normal distribution for a good first approximation anytime in the future so that's why the normal distribution is so important it's also why the central limit theorem is so important and we're going to use both of those today to create these this mathematical confidence interval um so today we'll look at confidence intervals that deal with population parameters and the confidence intervals we've dealt with in the past have looked at the sample statistics now we're going to do it for the population parameters and remember the goal of inferential statistics is to take our sample and draw conclusions about the population parameter so now we're going to take our sample and from that sample we're going to calculate a confidence interval and that confidence interval is going to tell us about reasonable values for this population parameter so let's start with the definition a confidence interval is a set of values that theoretically contain the population parameter given proportion of the time when the experiment is performed many many many many many many many times so it's a set of values usually it's an interval so we have a lower bound and an upper bound and we assume every value between those two lower and the upper constitute the interval it contains the population parameter that we're interested in a given proportion of the time so if we're talking about like a 95 percent confidence interval then theoretically the population parameter is about 95 percent of those confidence intervals when we do this experiment over and over and over when we do it for just once then we just have to hope that the population parameter is in the interval it and be confident at a certain level that it is in the interval and this is why it's important to replicate experiments over and over again because one confidence interval could be based on biased data or data that's not representative of the population and even we could use the best sampling method on the world and we would get a a set of a set of data a sample of that data that's not represented of the population we'd never know that if all we do this if all we do is do this once that's why replication is so important so that we know that whether or not our first sample was representative or not because if it's unlikely that the sample is unrepresentative then it's going to be doubly so if you have two samples and that they're both unrepresentative so note that it gives popular information about the population parameter it's a set of reasonable values for that parameter some population parameters we'll be looking at will be the mean the variance the proportion it's a set of reasonable values it's a function of the data you collect data and from that data you calculate the endpoints of that confidence interval and since there's a function of the data it's a random variable and it is a result of a probability distribution calculation and we'll see that later um so if LCL and UCL LCL for lower confidence limit and uclb upper confidence limit you have to say one minus Alpha times 100 percent confidence interval for parameter Theta and Theta could be mu or Sigma squared or p and then about 1 minus Alpha times 100 of the intervals under repeated experiments will contain that parameter by default we'll use Alpha of .05 which means that we'll be talking about 95 confidence intervals just by default which I say here the confidence level C is 1 minus our Alpha by default we're going to use Alpha of 0.05 in this course and the value of alpha will be reintroduced in chapter 10 when we talk about hypothesis testing it's going to be the theoretical or the nominal type 1 error rate and those words will make sense when we get to chapter 10 when we start talking about hypothesis testing note that Alpha is selected by the researcher therefore the confidence level is selected by the researcher uh so unless it's stated otherwise we're going to have Alpha 0.05 in this course so let's get to our first intro lecture question that interlection question will be Define the confidence interval and again I suggest writing down the question on the left hand side of your notebook answer it below that and this is especially important because understanding what a confidence interval is is extremely important for understanding what you can learn from your data okay so that's where we were let's move on to something called bootstrapping in a previous class we introduced bootstrapping it's in sca5b I believe and obtaining a confidence interval for a population parameter so let's look at the code again this is how you would load in data from this a path it's geography.csv these are the results of a previous class taking a geography quiz six is the highest possible score on the quiz zero is the lowest um load it in attach the data set if you want to you can do a do summary of DT to see what the mean and the median are see what the Min and the max are see what the first and the third quartile are you could also calculate the standard deviation doing SD you can do the IQR interquartile range using the IQR function then we performed these two lines multiple times the first line draws a random sample from the variable score that's the score that the students made on the quiz with replacement so each time through X is going to be a vector of scores on the second line you find the mean of that so this will calculate the sample mean of that particular sample of scores and store it into the variable St at the ith position this is a for Loop so it will be doing this these two lines or everything between the two braces 1 times 10 to the fourth times so at the end of running this Loop you're going to have 10 000 sample beans doing a histogram of those will show you the distribution of those sample means and during the quantiles from these two positions these two points or these two levels will give you the endpoints of a 95 confidence interval for the mean we've done this in the past here's what the histogram looks like notice the histogram looks rather normal considering that the actual data looked nothing like a normal distribution the distribution the sample means definitely does look normal that does not surprise us or it should not surprise us if we actually understand the central limit theorem because what does the central limit theorem tell us sample means will be much more normally distributed than the data itself and these are the endpoints of the 95 confidence interval so we're 95 confident that the population mean mu is between 1.63 and 2.57 1.63333 and 2.56667. again these are out of six so the mean is definitely going to be less than 50 percent because fifty percent is three and that's way way over here and just about all of the histogram is to the left of three so we're almost positive that the mean understanding of geography is less than 50 percent this is fine if all we have is the data and that's all we did here we just looked at the data we drew from the data we dealt with the data as being representative of the population we didn't say the data was normally distributed we didn't say the data followed a binomial distribution we just said here's the data if we can make an additional assumption about the data specifically that the data are normally distributed then we can go a little bit further we don't have to do bootstrapping we can just run a simple test so recall that for large n from the central limit theorem X bar is approximately normal with mean mu and variance Sigma squared over n where Sigma squared's the variance of the data or variance of the data generating process and mu is the expected value of the data generating process so this should look familiar from chapter 7 section 2. now if we apply the Z transform from back in chapter 3 we're going to subtract off mu from both sides and we're going to divide by the square root of Sigma squared over n we're going to get that X bar minus mu over root Sigma squared over n is approximately normal 0 1. this is why it's called the zscore this distribution normal 0 1 it's also called the Z distribution this is normalizing the X bar distribution so we have X bar minus mu over square root of Sigma squared over n is approximately normal it's approximately standard normal that means the expression on the left is between the values of negative 1.96 and positive not 1.96 about 95 of the time how do I know that because the 2.5 percentile and the 97.5 percentile of the standard normal are negative 1.96 and positive 1.96 . how do we know that Q Norm of 0.025 will give us the negative 1.96 and Q Norm of 0.975 will give us the 1.96 and so we would say that a theoretical 95 confidence interval I'm sorry a theoretical 95 interval for this quantity this thing on the on the left X bar minus mu over square root of Sigma squared over n is between negative 1.96 and 1.96 by definition so this bottom statement would be a definitional statement that is if I have X that's normally distributed with mean mu and variance Sigma squared and I draw a sample of size n from the X distribution and calculate this there's a 95 probability that it's going to be between negative 1.96 and positive 1.96 because 95 of the time it's between this quantity is between negative 1.96 and positive 1.96 about two and a half percent of the time it's above 1.96 and about two and a half percent of the time it's less than negative 1.96 . now we really don't care about the interval for this quantity we want it for Mu we want a confidence interval for Mu well all we have to do is solve this equation this equals negative 1.96 for Mu and here's what we get here's the quantity equal to negative 1.96 we're solving for Mu remember so we multiply both sides by the square root of Sigma squared over n and this is kind of important this quantity square root of Sigma squared over n is called the standard error now we got X bar minus mu is equal to negative 1.96 times the square standard error we subtract an X bar from both sides remember we're solving for this meal so we're subtracting X bar from both sides we'll f with the negative mu on the right multiply through by negative one we get that this is the upper Bound for Mu so this we're going to call the upper bound on the 95 confidence interval from U I leave it as an exercise to show that this with a negative sign here is going to be the lower bound how would you do that this quantity equal to 1.96 and so from U so we got it X bar minus 1.96 times the standard error and X bar plus 1.96 times the standard error of the two endpoints we can symbolize it in just one expression X bar plus and minus Z of alpha over 2 times the standard error if Alpha is .05 then this will be a 1.96 out front um so we would call this oddly enough we would call this the confidence interval from U technically this would just provide the two endpoints of the confidence interval from U foreign question at this point oops what is the distribution of the sample mean in this lecture so what did we require the distribution of the sample mean to be notice not the distribution of the sample we're looking at the distribution of the sample mean so what was that in this lecture and again write the question on the left side your answer below it okay so this confidence interval that we just calculated is for Mu and it works this confidence interval is from U and it works when you're estimating mu which makes sense if you're trying to estimate the variance you wouldn't use a confidence interval from mu if you're able to calculate X bar and N well if you can't calculate X bar and N from the data then you I don't know what to say this goes better calculating X bar from the data is pretty straightforward and as you're just counting the data size and if you know Sigma squared remember Sigma squared is the population variance um I don't know that we would ever know the population variance and then last but not least the data are generated from a normal process or n is large enough that the central limit theorem says at X bar is approximately normal and again the reality here for four is we just need X bar to be approximately normal at no point in the last few slides did I say x has to follow any specific distribution every distributional statement we made was that X bar was normal or approximately so call the going back to number three the you have to know Sigma squared the population variance recode the definition of the population variance in order to calculate it you need to know what mu is however since we're trying to estimate mu we don't want to know what mu is therefore how do we know what Sigma squared is and in general if we don't know Sigma squared then what we just did doesn't work so your question is your question to me is why did we do it if we never know Sigma squared and therefore what we just did doesn't doesn't do anything for us and the answer is this process that we just did gave us an opportunity to show how confidence intervals are constructed what they actually indicates and how they can be useful kind of shaky on that last one but now that we know the process in creating confidence intervals we can fix this quote error or try to figure out some way that we can get by without having to know Sigma squared and create a confidence interval from that using the same theory behind it I mean specifically since we can't use the Z procedure which we just did it's also called the Wald wald named after uh I think it was Benjamin Walt could have been Abraham Wald I probably should have looked that up we're going to use a different procedure if we know Sigma squared we can use this procedure we just did if we don't know Sigma squared we can't because we need to know Sigma squared to do this procedure instead of the Z procedure we'll use the T procedure here's the function that we will calculate it's X bar minus mu the numerator is the same as it was for the Z procedure denominator we're looking at s squared over n and the Z procedure that was Sigma squared here it's s squared s squared would be the sample variance since if we've got the sample we can calculate s squared and this quantity follows approximately a t distribution T distribution has a parameter called the degrees of freedom in for one sample cases the degrees of freedom is just equal to n minus 1. now with this said we can go through the same procedure we did before with the Z's to come up with the endpoints of a confidence interval using the T is X bar plus or minus this distributional multiplier times the standard error just in this case the standard error is the square root of s squared over n comparing this confidence interval to the one we calculated just earlier so here capital T sub Alpha over 2 is the alpha over second quantity over the T distribution with n minus 1 degrees of freedom table two by tradition table two is the T distribution table so in the back of the book there's probably a table two that you would use if you had to I'm this slide Compares with the Z distribution the standard normal distribution the blue distribution how it compares to the T distribution notice the T distribution is much wider than the standard normal there's more measurements out here in the Tails far away from zero in other words the T distribution is much more variable than the Z why does that make sense it's because when the Z distribution when we were using the Z distribution we only had one source of uncertainty that was in The X bar the denominator was a sigma squared a population variance with the T we've got more sources of uncertainty we've got the X bar but we've also got a s squared that's a random variable also so we've got two sources of randomness so we would expect the T distribution to be much wider than the Z now this confidence interval from U works if you're trying to estimate mu if you're able to calculate X bar and n and S squared which again those are from the data so you should be able to calculate them and if the data are generated from a normal process or if n is large enough again we're doing this for X bar has to be normally distributed and that happens in two ways if the X's are normally distributed or if n is large enough and the central limit theorem works uh let's do question three why do we use a t distribution when the population variance is unknown so that should be two or three sentences or maybe one or two sentences again write the question on the left hand side answer it below so we've got our procedure to estimate mu okay just one View as we go through this this section this this these two chapters eight and nine we're going to get more and more procedures to estimate things we're going to get a procedure to estimate P we're going to get a procedure to estimate Sigma squared we're going to get a procedure to estimate two mu's or the difference in two mu's the difference in two proportions that the ratio of two variances we're going to get procedures that work when the sample is not normal enough so make sure that you can match the hypoth the what you're trying to estimate the parameter with the correct procedure and the requirements for that procedure if we go back one who uh these are the requirements to the T distribution or I'm sorry these are the requirements of the ttest for the T procedure if you don't meet these then you've got to use something else here's a list of several others go to the all procedures.pdf handout that's located online that will give you all the procedures we'll be looking at and we're going to be able to even have a procedure for mutilda the the population median so in today into today's slide deck we cover the central limit theorem again we looked at the definition of confidence intervals we look for confidence intervals for Mu when Sigma squared is known uh from U when Sigma squared wasn't known and both of those cases X bar had to be approximately normal we looked at confidence intervals for other instances or I just mentioned them and most importantly the idea or the theory behind confidence intervals this is the key for today understand what confidence intervals tell us once you understand this then you can just hop on the computer and have the computer calculate those confidence intervals for you it's the theory that you need to understand in the next slide deck we're going to show how to do these confidence intervals calculations in r tomorrow we're going to review a little bit of today we're going to look at some processes to create confidence intervals for one and two means medians proportions variances but we're going to do it in r so instead of having to actually do those calculations by hand it's going to be one line of code and interpret the results so in the future we're going to see a lot of these procedures make sure you keep a separate sheet I would have it in the back of your notebook and for every procedure you should State how to do it in R what the procedure is for is it for a single mean is it for two proportions it is it a ratio of variances Etc what the requirements are otherwise known as the assumptions of it and then how to interpret so chapter 8 and 5 and then Wikipedia all of those will be helpful hope this was helpful I'll see you in the next lecture hello and welcome to chapter 10. this is the chapter where we introduce hypothesis testing and see several different types of tests so this lecture is going to be about the theory of hypothesis testing so this will leave the groundwork tie it into previous topics that we've discussed the next lecture will be how to actually do this using R so you don't have to worry about all these probability calculations so by the end of this lecture you should be able to identify the research null and alternative hypotheses the research hypothesis is given to you by the researcher the null on the alternative you have to come up with you have to calculate the pvalue for a given alternative hypothesis and again this slide deck will be about doing it by hand that'll let you know exactly what the pvalue means and what it doesn't mean which ties into the third objective the next lecture will be how to do this in R so all you have to do is interpret the pvalue that R gives you so previously we've calculated confidence intervals and we've interpreted confidence intervals today we're going to look at the other side of the statistical inference coin and looking at hypothesis testing and pvalues the key one of the key one of the key differences is in hypothesis testing you create the hypothesis before you collect the data and then you test that hypothesis with your data so a hypothesis comes first collect your data then test the hypothesis whereas in confidence intervals you walk into it with no idea what the parameter value is supposed to be and you estimate the parameter value from the data and only from the data so confidence intervals solely from the data hypothesis testing is from the data and from a claimed hypothesis so here's the theory the theory behind hypothesis testing is one state the research hypothesis and the null hypothesis two somehow determine how much the data support the hypothesis and by the hypothesis I mean the null hypothesis to do that you need to determine the parameter being tested turn the determine the appropriate statistic or estimator how to determine the distribution of that statistic under the null hypothesis determine How likely it is to observe that statistic if the null hypothesis is true and that last thing is called the pvalue and then finally interpret all of that specifically interpret that level of support so let's go through a few definitions we're going to talk about that the three hypotheses types um hypothesis is a testable Claim about reality that's all it is since it's a claim about reality it's going to concern some aspect of the population and this aspect of the population is going to be a parameter that's we've been dealing with parameters from the second chapter onward we're going to start testing hypotheses about that population parameter based on our sample now the usual parameters hypothesize about at this level are just the mean the proportion the variance but we can hypothesize about any aspects of the population and we're going to symbolize this generic parameter using the Greek letter Theta so Theta can represent the mean the proportion the variance since it's a claim about reality since this hypothesis is a claim about reality it separates all possible realities into those that are consistent with hypothesis and those that are incompatible with it so every single reality will either be consistent with a hypothesis or incompatible with it and we have to determine which reality we fall into if we fall into the reality that is consistent with hypothesis we say the data support the hypothesis if we fall into the reality where the the uh the we fall into the reality that is incompatible with hypothesis we reject the hypothesis we say that the data are not supporting it I'm the most absolutely without question the most important hypothesis is the one made by the researcher and that is the research hypothesis it's a testable Claim about reality that's hypothesis part made by the scientist or the researcher and that's the research part this is the one that the statistician must eventually come to a conclusion about we are going to create two additional hypotheses to help us with coming to a conclusion about the research hypothesis but but ultimately we have to come back to the scientists and say yes the data supports the research hypothesis or know the data do not support the research hypothesis because we're using statistics we create two statistics specific hypotheses these two hypotheses divide all possible realities into two groups those that are part of the research and those that are not up that are not a part of it um so here's a scary table pay attention solely to the First Column the column headed HR that's going to be the different types of research hypotheses recall that Theta is the parameter of Interest it could be mu it could be P could be Sigma squared it could be I can't think of it it could be Lambda remember Lambda from the poisson and the Lambda from the exponential it could be a could be B from the uniform but we're focusing solely on mu and P and sigma squared the mean the proportion and the variance so those thetas represent one of those three the value Theta 0 or the the simple Theta zero represents a hypothesized value claimed by the researcher the Theta naught is just a a number that the researcher thinks is interesting so now we got these six symbols between the Theta the parameter and the Theta not the value the relationship between Theta and Theta naught has to be one of these six Theta the hypothe the researcher could hypothesize that Theta is less than some value equal to some value greater than some value less than or equal to some value not equal to some value or greater than some value and again the research hypothesis is given to us by the stat by the scientist from that research hypothesis we statisticians create a null and an alternative now let's look at the null and alternative hypo columns starting with the null column notice the null only has three types of of symbols greater than or equal to equal to or less than or equal to greater than or equal to equal two or less than or equal to the null hypothesis must always have the equal to part and that's because we are going to create a probability distribution for the parameter or for the statistic based on that equals part so the null hypothesis always has to have the equals part contrast this with the alternative there is never an equals part in fact if you notice the null and the alternative are complete opposites if the null is greater than or equal to the alternative is less than if the null is equal to the alternative is not equal to if the null is less than or equal to the alternative is not less than or equal to otherwise known as greater than it's because the null and the alternative have to divide reality up into two parts one part that supports the research hypothesis and one part that doesn't now which part supports the research hypothesis the answer is well it depends what is the symbol in the research hypothesis If the symbol is less than that cannot be a null hypothesis so that's got to be the alternative If the symbol is equal to well that's one of the allowable symbols for the null so that's going to be the null and the alternative is the opposite if the research hypothesis uses greater than well that has to be the alternative because it can't be the null and the null is going to be the opposite less than or equal to will be the null greater than or equal to will be the null not equal to will be the alternative so this the HR column is given to us by the researcher and from that we create a null and an alternative hypothesis that split the world up into a a reality that supports the research hypothesis and a reality that does not we have to determine which reality we're in don't memorize the table understand it learn what it says about relationships these research hypotheses are given to us we just have to figure out what the null and the alternative is first step is to determine if the equals part is a part of the research hypothesis if it is then the research hypothesis and the null hypothesis are the same otherwise the research hypothesis and the alternative hypothesis are the same and then the null and the alternative are always opposites what do all the null hypotheses have in common already said that what's the relationship between the null and the alternative said that why does that relationship have to exist okay now we know what the hypotheses are the research the null the alternative now let's look at this thing called the pvalue the pvalue is the probability of observing a test statistic this extreme or more so given the null hypothesis is true some key points there it's a probability it relates to the test statistic it's a probability of observing such a test statistic that is this extreme or more so given the null hypothesis is true so we assume the null hypothesis is true and we calculate the pvalue based on that now this is the stat statistical definition of the pvalue this is the one that's usually in the books pvalue is the probability of observing data this extreme or more so given the null hypothesis is true while the first definition makes me feel all warm and fuzzy inside the second one actually ties it more closely to the data that you've observed and the one I really liked one that makes me feel all nice and warm and fuzzy inside is the last one it's not a mathematical definition but it's a gut level definition of what the pvalue actually tells us pvalue is the amount of support in the data for the null hypothesis and again it's for the null hypothesis in other words if the pvalue is large then there is a lot of support at the data for the null hypothesis if the pvalue is small then there is very little support in the data for the null hypothesis the third definition comes from the second definition and the second comes from the first and they're all equivalent assuming that your test is appropriate and you've collected the data well so now we know to look at the pvalue and to interpret the pvalue it's just a matter of the pvalue is how much support is in the data for that null hypothesis and remember we got to tie it eventually back to the research hypothesis okay now we've got two of the three parts of the hypothesis testing Theory we laid out earlier the last part is making a decision about a research hypothesis based on the data so the fundamental question is do the data sufficiently support the research hypothesis do the data sufficiently support the research hypothesis and note that this is a binary yes or no yes the data do know the data do not this goes back to the pvalue but we need to somehow change that pvalue of probability that ranges from zero to one we need to change that into a yes or no and to do that we just have to determine some sort of cutoff between what supports the research hypothesis and what does not this cut off boundary we're going to refer to as the alpha value the typical Alpha value is .05 my defaults .05 if the level of support is less than Alpha we reject the null hypothesis the data do not sufficiently support the null hypothesis if the day if the if the level support is greater than Alpha if the pvalue is greater than Alpha we don't reject the null hypothesis the data do not tell us that the null hypothesis is wrong notice that there is information in the pvalue that goes above and beyond just the yes or no answer so not only should we interpret the pvalue in terms of yes the data supported to know the data don't but we should also provide the pvalue because that will also tell us how much it supports it or how much it doesn't continue on to and continue on with decision Theory this Alpha value the alpha value is the type 1 error rate that's claimed by the statistician a type 1 error occurs in the researcher rejects a true null hypothesis so there is that null hypothesis if it's true then Alpha then the type 1 type 1 error occurs when we reject that true null hypothesis now the thing is we never know if a null hypothesis is true in real real life which can be a problem but then if we knew a null hypothesis was true in real life then why would we do statistics to test the null hypothesis I mean we'd already know that it was true the type 1 error rate is the proportion the type 1 error rate is a proportion of the time that the researcher commits a type 1 error so the rate part means it's the proportion of the times that the researcher commits that type 1 error note that the actual type 1 error rate may not be Alpha the alpha is claimed by the statistician but the test may have some flaws to it where the actual type 1 error rate is not Alpha and laboratory F explores this in much greater detail however we tend to just pretend that Alpha actually is the type 1 error rate how it's a lot of good pretending in statistics if there is a type 1 error then there must at least be a type 2 error otherwise why would we call it a type 1 error a type 2 error occurs in the research researcher fails to reject a false null hypothesis type 1 occurred when the researcher rejected a true null hypothesis a type 2 occurs when the researcher fails to reject a false null hypothesis or we could also think of it as a type 2 error occurs when the researcher rejects a true alternative although we never frame it in terms of rejecting a true alternative the type 1 error rate that slipped in the type 2 error rate is symbolized with beta just like the type 1 error rate is implies by Alpha the type 2 error rate symbolized by Beta both are errors and error rates so we would like to minimize both however is reducing one increases the other furthermore if we want to choose if we want to make either one of them zero then the other one goes all the way up to one so let's we're kind of stuck there we tradition in statistics is to set Alpha and then beta just happens as it happens beta is going to be a function of alpha as described here but it's also going to be a function the sample size it's also going to be a function of how wrong the null hypothesis is um and so if we want to reduce the type 2 error rate then increasing the sample size is guaranteed to do that but unfortunately as we've seen in the past increasing the sample size tends to be a little bit expensive at times here's an aside this is not statistical here well it is kind of statistical but it's if there's a type 1 error rate then there must be at least a type 2 error rate a terror error there are in fact a couple more these are nonstandard by the way but they do give us some insight into what can go wrong in a statistical analysis so we'll Define a Type 3 error which I want to introduce here but I only want to introduce this to you so that you think more through the process and where errors can pop in not so that you can replicate this on a test a type three error occurs in the researcher rejects a false null hypothesis that's good but for the wrong reason that's the error part is you're doing it for the wrong reason um type 3 so here are some causes of type 3 errors One is using the wrong test um two is aggregation bias in other words you're measuring at a lot at uh measuring something about groups and you're trying to draw conclusions about the individuals in the groups and there is ecological fallacy which is just about the opposite of that collinearity among predictors it just means you're using two independent variables that are highly correlated and you're not able to determine which is causing the the effect that you're looking at so the key to avoiding the type 3 error rate is to fully understand the statistical tests the data collection the relationship amongst the independent variables in other words the key is to understanding your model and your data the wrong test the wrong interpretation the wrong assumptions All Leads errors but also realize that type 3 errors can be eliminated it just requires that the statistician is careful type 1 and type 2 errors cannot be eliminated let's go through three simple the CCD examples I'd like to test if my coin is biased in favor of getting heads specifically using physical language my research hypothesis is HR for research hypothesis that's colon p is our parameter so it's a population proportion my research hypothesis is that P is greater than onehalf I would like to test if my coin is biased in favor of hits so P must be the probability of getting hits so this will be our research hypothesis going back to the table if this is our research hypothesis where will this also be the null or will this also be the alternative hypothesis the key is looking at the symbol in the middle there's no equals part so this will also be the alternative hypothesis since this is the alternative what is the null I recall that the null hypothesis is The Logical opposite what is the opposite of greater than correct it's less than or equal to and as always if we're going to test hypothesis we collect data in this case I flip the coin 100 times and I get 58 hits from the way the problem is set up we know that the number of heads is follows a binomial distribution N is a hundred it's the number of Trials the number of times I flip the coin p is 0.5 because that's my null that's the equals part of my null hypothesis that P is equal to 0.5 note that if the number of heads observed is too large then the alternative hypothesis is more likely to be correct if the number of observed heads is too large because we're research hypothesis is greater than onehalf so let's calculate the pvalue from the definition pvalue is a probability of observing data this extreme or more so given the null hypothesis is true so here's the pvalue it's the probability of of observing more than or equal to 58 heads because I observed 58 the pvalue is the probability of observing data this extreme or more so greater than or equal to 58 given the null hypothesis is true given the null hypothesis is true and this is the null hypothesis in distribution form and 100 p is 0.5 and guess what we know how to calculate this from back in chapter five we also know how to do it rather quickly it's a greater than or equal to 58 x is eight discrete so we actually do have to pay attention to the 58 this is 1 minus X less than or equal to 57. ends 100 piece 0.5 so the pvalue is .0660531 so how do we interpret that pvalue let's go ahead and look at the distribution of the number of heads so this is the probability Mass function for our null hypothesis the dark blue is the greater than or equal to 58 so the dark blue is going to be the pvalue and technically the sum of the area under the dark blue will be our pvalue since the pvalue is greater than our usual Alpha of 0.05 remember it's 0.06 something we do not reject the null hypothesis the data are not sufficiently against the null hypothesis therefore we actually cannot conclude anything about the research hypothesis P may be greater than 0.5 P may be less than 0.5 P may be exactly equal to 0.5 if you look at the confidence interval for p by doing the binomial procedure you'll see that the confidence interval includes 0.5 it includes some numbers less than 0.5 it includes some numbers greater than 0.5 and as such we know that that entire confidence interval is a set of reasonable values for p since it includes things above below and equal to 0.5 all of those are reasonable realities so we fail to reject the null hypothesis pvalue is too large example two cards I believe that my blackjack dealer is cheating if everything is fairer than I expect to have a blackjack 4.75 the time you can calculate that using chapter 4 stuff I have played 132 hands and got a blackjack only once do I have evidence that the black deck blackjack dealer is cheating in other words using statistical language my research hypothesis my claim about reality is that P is less than 4.75 percent why less than why would my blackjack dealer cheat and give me a higher probability of winning so less than will this also be the alternative or the null correct this will also be the alternative this has no equals part to it so what will the null hypothesis be very good greater than or equal to from the way the problem is set up we know the number of blackjacks X follows a binomial n of 132 the number of hands I've played P of 0.0475 which is 4.75 percent which is the equals part in the null hypothesis also we know that if the number of blackjacks is too small then the alternative hypothesis is more likely to be correct and we would reject the null so let's calculate the pvalue I observed one so the pvalue is going to be X less the probability of X being less than or equal to one given X follows this distribution so as P binom one size 132 prob 0.0475 that gives us a pvalue of .0123027 because the pvalue is so small I reject the null hypothesis I have evidence that the blackjack dealer is cheating notice that I say I have evidence I don't say this is proof statisticians deal in evidence not in proof so this will be the distribution or part of it continues on to the right a lot a distribution under the null hypothesis the pvalue will be the sum of these two heights notice that it is very small very low P naught is our claimed well oh yeah P naught is our it's the 4.75 percent sorry it threw me off there for a moment P naught is our 4.75 percent four point yeah 4.75 what we observed was way down here and remember the pvalue is what we observed or more so observed more so next example the D in CCD I believe that this dye is biased against getting a six specifically using statistical language my research hypothesis Claim about reality is hrp less than 1 6 which is 0.11 I'm sorry 0.1667 to test this I roll the die 100 times and get six a total of nine times because this is less than that's the alternative phenel would be greater than or equal to this is also binomial n of a hundred P of 0.1667 I need to calculate let's see I got a total of nine times so I need to calculate the probability of X being less than or equal to 9. the equal to is data this extreme and the less than is or more so we know how to calculate this this comes out to be 0.02124964 if we need to make a decision since this is less than our Alpha of 0.05 we reject the null hypothesis we have evidence that the dye is biased against the number six here's the probability Mass function for that particular binomial I observed 9 which is right here pvalue would be the sum of the heights for nine and less those are pretty straightforward those are the binomial examples let's look at some means examples these will go back to the Z distribution um some fast food restaurant in town claims that the weight of a quarter pounder hamburger before cooking is four ounces with a standard deviation of Sigma equals one ounce notice we are given the population standard deviation is one the research hypothesis or the claim bioacdonalds is Mu is equal to four since this is an equals this will also be the null hypothesis the alternative hypothesis will be not equal to that's no we haven't done or not equal to test this weigh a stack of 25 patties and find that the total weight is only 94 ounces we would expect it to be a hundred four times 25 but it's only 94. is there sufficient evidence that whack the nulls is incorrect so we're trying to calculate probability that t remember capital t is the statistic total or the sum of the observations probability that t is less than or equal to 94 or t is greater than or equal to 106. well the less than or equal to 94 is pretty obvious we observed 94 that's less than what we would expect therefore we'd have to calculate the probability of P being as extreme equals part or more so but remember the alternative is not equal to therefore being too high would also be as extreme or more so 94 is 6 less than what we would expect so the upper end will be six more than what we expect and then greater than equal to that 106. uh T follows a normal distribution with expected value of 100 and variance of one times square root of 25 and where this is for the sample total and the statement more or less comes from the central limit theorem in fact forget the more or less the statement does come from the central limit theorem specifically since I didn't tell you the actual distribution of the Patty weights one times the square root of 25 is just five so keep that in mind this is just two times probability of X being less than or equal to 94 given mu is equal to 100 and S is equal to 5. why is it two times this well notice we're only calculating the lower tail and we actually have to calculate both the lower and the upper but the normal distribution is symmetric so I can just double the lower gives me a pvalue of 0.23 because this pvalue is greater than Alpha we cannot reject the null hypothesis if we do a confidence interval we'll see that the confidence interval does include 100 as well as values above it and values below it so we don't actually know if mu is 4 or greater than 4 or less than four all values work according to our data which means we probably should go back there and collect some more data here's the distribution of t notice we observed 94 this area is less than or equal to 94. this is the as extreme or more so on the bottom end is this 106 so this will be the as extreme or more so on the upper end normal distribution is symmetric so this dark blue area will be exactly the same as this dark blue area so we can get away with just doubling this lower tail so the total area or double the lower is the pvalue uh McDonald's claims that the weight of a half pounder hamburger patty before cooking is eight ounces with a standard deviation of Sigma equals one again we know we're given Sigma the population standard deviation in symbols this will be mu is equal to eight because they're claiming that the average weight is eight ounces to test this we weigh a stack of 25 patties and find that the average now we're looking at the average is only 7.5 ounces sufficient evidence that wax Donald's incorrect since the research is equal that will also be the null alternative will be not equal to we observe 7.5 ounces so we need to calculate the probability of being less than or equals seven point ounces and greater than or equal to 8.5 ounces it because the alternative is not equal to here x bar follows a normal mean eight standard deviation one over five 1 over the square root of n and again this comes from the center limit theorem 2 times P Norm of 7.5 mean of eight standard deviation of 0.2 gives us a pvalue of .01241933 how should we interpret this result because the p because the pvalue is less than Alpha we reject the null hypothesis we have sufficient evidence that mu is less than eight here's the distribution of X bar we observed way down here whack Donald's claims way up here but we observe down here pvalue is this area plus this area because both are as extreme from U or more so we can just double the lower tail that will give us the pvalue that we saw it very small probability we're way out here in the tails whack the nulls claims at the weight of a pounder hamburger patty before cooking is at least 16 ounces with the standard deviation of Sigma equal one ounce and sybils this is HR mu that's the average is greater than or equal to 16 y greater than or equal to because it's at least that's what greater than or equal to means this will also be the null hypothesis because it has the equals part the alternative will be the opposite of this it'll be less than to test this we weigh a stack of 100 patties and find that the average weight is 15.9 ounces this sufficient evidence that whack the nulls is incorrect let's calculate the pvalue I want to calculate the pvalue this is just probability of X bar being less than or equal to 15.9 why only One Direction that's because the research hypothesis only has one direction notice we're calculating less than or equal to the alternative was less than and we know that X bar follows a normal mean of 16 standard deviation of one over ten because N is a hundred and this is a really good weekend for me here's how we calculate the pvalue comes out to be 0.1586553 this value is greater than Alpha therefore we cannot reject the null hypothesis there is not sufficient evidence that McDonald's is wrong I mean whack the nulls is wrong they could be wrong but we don't have the evidence that they're wrong because the pvalue is so high here's that distribution of X bar this is what we observed remember the alternative was less than because the null was greater than or equal to the alternative is less than so we shade less than what we observe this will be the pvalue one more example after this number of calories in a mcpork is at most 350. with the standard deviation of 50 calories and symbols this is less than or equal to at most is less than or equal to to test this we perform a calorimetry tests on a stack of 100 with porks what a waste of good food and find that the average number of calories is 343. note the alternative is greater than because this is less than or equal to this will be the null the or equal to part is key there the alternative will be the opposite so now we ask what's the pvalue is there sufficient evidence that whack Donald's is incorrect we need to calculate the probability of X bar being greater than or equal to 343 we observe 343 the alternative is greater than so the greater than part will be the or more so extreme and we got X bar follows a normal 350 because that's our claim with Sigma equal to 50. boom boom boom divided by the square root of n is just P Norm 343 55 gives us a pvalue 0.9192433 because the pvalue is greater than Alpha we cannot reject the null hypothesis there is no sufficient evidence that whack the nulls is wrong about its calorie statement they could be wrong we just don't have the evidence there is the distribution remember the alternative was greater than so we shade above what we observe and that's a lot of dark blue so the pvalue really looks really big so short summary we looked at stating hypotheses testing hypotheses calculating pvalue from the definition we looked at two R functions these are things that we've seen in the past download the all procedures PDF file that lists all the statistical procedures we're going to use in R we're going to see how to use those in the next lecture because in the next lecture we're going to review today and use R to perform the calculations really easy I guess that should be pvalues um again we're going to cover a lot of tests so keep separate sheets for each of the tests there's an example of a flow chart on how to test me a single mean on the module on the module 4 Page follow it create your own for the rest of them here are some readings are for starters chapter 4 Hawks learning chapter 10. of course Wikipedia hypothesis tests which brings us to the intraelectric questions there's three of them question one again write question one the question on the left hand side of your notes answer it below so you can easily transfer it into Moodle what is the relationship between the null and the alternative hypotheses question two what are the three allowed signs in a null hypothesis question three when is the research hypothesis the same as the alternative hypothesis and all three of these get back at a common problem in intro stats of trying to figure out okay I've got a research hypothesis where do I go from here you may want to review the table um or he may not so those are the three and that's the end of this again next lecture we're going to see how to do this in r thank you much hello and welcome to section 10 4 chapter 10 is all about hypothesis testing section 10 4 is about handling proportions one sample and two sample proportions so by the end of this lecture you should be able to understand the theory behind and test hypotheses about a single population proportion and the difference between two population proportions you should also better understand that the better understand what the pvalue means and how to test hypotheses and clearly specify why confidence intervals and pvalues both give important informations about the population parameter so one parameter procedures this will be about population proportion p on the parametric procedure is the binomial procedure the usual graphic is the binomial plots that seems to be a lot like what we did back with confidence intervals and one parameter uh one population proportion hmm requires the data being generated from the binomial distribution that really sounds familiar the r function is binom.tests X comma n x is the number of successes and is the number of Trials i s I guarantee we've seen this somewhere before haven't we note that this is not the procedure that Hawks covers they use something called the walled tests World DOT test it also takes X comma n so that should help you with the Hox assignments here's the tests Theory and we've actually covered this back when we introduced hypothesis testing it was a whole bunch of binomial stuff this is what led to the binomial test and we're trying to draw conclusions about a single population proportion we should use a test statistic based on the sample proportion or on what we observe the number of of successes if we do it based on the sample proportion we'll use the walled test if we do it on the number of successes we use the binomial test again the binomial test is the exact test and the wall test is the approximate test um we're given X follows a binomial distribution recall that the binomial has two parameters and NP um the number of observations serves as a particularly fine test statistic because we know it's distribution exactly it's binomial we only know the distribution of the proportions approximately x divided by n is only approximately normal and that only comes about because of the central limit theorem if we go all the way back to section six point five approximating the binomial with the normal we see oh there's a lot of error that can pop in there because it is a an approximation and the smaller the sample size the more the error the following are three examples showing how to perform these calculations I have a coin that I think is fair to test this F of it 10 times and count the number of heads in those 10 flips total of three heads actually came up is this sufficient evidence of the coin that's not fair so the claim is p is equal to onehalf that's the research hypothesis since it contains equal sign this is also the null hypothesis since the equals is the null the alternative will be not equal we're trying to make a conclusion about P where the data are generated from a binomial thus under the null hypothesis X follows the binomial n of tan P of 0.5 we observed x equals three pvalue is defined as the probability of observing data this extreme or more so given that the null hypothesis is true in other words the pvalue is the probability of X being less than or equal to three plus X greater than a probability of X greater than equal to seven data this extreme would be x equal three or or more so is less than three where'd the 7 come from well expected value of x which is n times p is five three is two less than five just as extreme would be two more than five and more extreme than that would be greater than two plus five there's the distribution of x distribution of the number of heads we observed three that's the red thing 7 is equally extreme less than or equal to 3 is or more so on the left and greater than or equal to seven is or more so on the right so the red Heights added up will give us our pvalue so the pvalue is 0.34375 simply using calculations that we did back in chapter five section two and in fact we did these calculations back in chapter five section two and we interpreted it correctly as evidence in favor of or against the claim we're just giving it some more terminology and some more meaning from the all procedures handout and the sca examples that we've looked at we know we can also do binom.test x equals three n equals 10 and P equals .050 line tells us that the pvalue is 0.3438 since this is greater than Alpha we fail to reject the null hypothesis the coin may be fair the coin may not be fair we have no evidence that can tell us either way a 95 confidence interval for the probability of a flip Landing head is between 6.7 percent and 65.2 percent that is an extremely wide confidence interval how possibly could we make it narrower right collect more data instead of flipping the coin 10 times flip the coin a hundred times ten hundred times ten thousand times the larger the sample size the narrower the confidence interval example two this goes back to a a lab that we did I contend that more than a quarter of students at Knox or Juniors to test this I randomly sample from the student body asking class here in my sample of 100 students 30 stated they were juniors here the claim is p is greater than 0.25 because I contend that more than a quarter so it's more than since it contains the greater than sign this will be the alternative the no will be the opposite of greater than which is not greater than otherwise known as less than or equal to that means the two statistical hypotheses are given below here's the distribution of the number of Juniors under the null hypothesis remember the null hypothesis contains the equals part of a quarter I asked 100 people so this would be the distribution of the number of Juniors that I experience pvalue will be the probability of remember I got 30 of them so this is what I observed right here since the alternative was greater than the pvalue is going to be the probability of X being greater than or equal to 30. the equal to is as extreme and the greater than is the or more so we get 0.1495 because the pvalue is greater than Alpha we fail to reject the null hypothesis we do not have sufficient evidence that the proportion of Genius is greater than a quarter in fact we can calculate the confidence interval directly and find that the proportion of generous is greater than 0.2249 based on this data or we can use a binomial test X is 30 because that's what we observe the number of successes N is a hundred that's the sample size p is 0.25 that's my value of Interest that's my Theta naught the alternative is greater because I claim that mu is I'm sorry I claim that P is greater than 0.25 if you get a pvalue of Point 1495 we get the confidence interval from 0.23 up so I'm 95 percent that the proportion of Genius is at least 0.2249232 one thing that we can definitely use this binomial test for is to check if a data set is representative on its face the data file is some college this is real data I had to change the name of the college to protect the innocent it was sent to me by the Registrar of some college to do some statistical analysis I did a little check sent it right back and said the data are not representative she said yes it is I said okay I'll go ahead and do the analysis and it'll be worthless but I will tell you it's worthless she kind of got upset with me um I was supposed to model the success the high enough GPA given some of the other variables let's perform a quick check to see if the data are reasonably representative of the population at that College so my provided sample consisted of 661 students she claimed a random sample from the College of which 22 were freshmen given that the proportion of freshmen at SCU some College University is 28 percent is the day or the data representative in terms of freshmen so here the claim is p is equal to 0.280 since it contains equal sign it's the null that means that the two hypotheses are equal and then not equal for the alternative under the null hypothesis the number of freshmen in my sample should follow this binomial distribution n of 661 P of 0.28 remember I observed 22. so I observed something down here something just as extreme is going to be located way up here or we can simply just double the probability of X being less than or equal to 22. probability of X being less than or equal to 22 is zero I'll go ahead and double that 2 times 0 is 0. we could also add zero to that um we could multiply by one if we want to stretch this out but it comes down the pvalue is zero since the pvalue is less than or Alpha 0.05 we reject the null hypothesis in favor of the alternative in terms of freshmen the sample is not represented of the population of SCU Simpson I'm trying to model the GPA it is very reasonable that freshman GPA will be different from sophomores Juniors and seniors therefore in order for my analysis to be worth the weight that it's to be worth its weight in gold I really do need more freshmen in my sample or we can just use power of r binom.tests x is 22 number of successes n is 661 p is 0.28 I got the 0.28 from there uh website got a pvalue of being less than 2.2 times 10 to the negative 16th this data would be representative if the proportion of freshmen at SCU is between two percent and five percent now that I talk about this I realize I never did get paid for this so yeah it's okay I didn't do anything other than say that they didn't give me the data next types of tests the two parameter procedures we're going to compare P1 minus P2 or we're going to compare P1 and P2 the parameter of interest will be P1 minus P2 this would be the proportions procedure or the two proportions procedure the graphic will be the binomial plot it requires that the expected number of successes is at least five in each group as some would say at least 10 in each group uh if no one's dying from you being wrong then at least 10 should be fine if you really do need to make sure that you're right then at least 50 or at least 100 in each group would be preferred the proportions test is based on the normality approximation to the binomial distribution therefore large sample size is required the r function is pardon me is prop.tests you give it the number of successes X and the number of Trials n since it's 2 um two proportions we're comparing you got to give it two sets of x's or two x's and two ends wrapped in C's they use something close to this but this procedure actually makes adjustments for the fact that the binomial distribution is discrete and the normal is not as such you'll need to use the wall test to perform Hawk's homework estimating P1 minus P2 so again when you're using proportion stuff with Hawks use the walled test the wall test will act will require both the X and the and just as laid out here with prop test um sample sizes are large the wall test and the proportions test will give you essentially the same answers it's only when the sample size is small that they're going to be different so here's the theory behind the proportions test actually here's the theory behind the walled test since we're trying to draw conclusions about the difference between two population proportions we're going to use a test statistic based on the difference into sample proportions in other words we're going to use sample proportion one minus sample proportion two we do know that X and Y both follow binomial distributions each with their own sample sizes each with their own probabilities PX and py one of the biggest problems is x minus y is not going to work because we don't know the distribution of x minus y it's not binomial um we but we do know that the sample proportion for x minus the sample proportion for y is a good estimator of p x minus py and the sample proportion is just the number of successes divided by the number of Trials so that's going to be the successes over trials for x minus successes over trial for y all we have to do now is figure out the distribution of this statistic if we're okay with approximation we know that X is approximately normal and Y is approximately normal which means that X over n x is approximately normal and Y over n y will be approximately normal and X over n x minus y over n y will also be approximately normal Central limit theorem is just so awesome I mean we can say all of that simply because of the central limit theorem and the approximation increases I mean the approximation gets better as the sample size is individually get better which is what we're saying here remember that the if x follows a binomial then it also approximately follows a normal with expected value of NP and variance of NP 1 minus p and we'll subscript all the X stuff with X and all the Y stuff with y standardizing or I'm sorry dividing by x and x and and Y will get us this as our normal distributions subtracting X over N X and Y over n y gets us its own normal distribution notice that this p x minus p y is what we're trying to estimate in other words our sample proportion for x minus the sample proportion for y is unbiased for our population proportion x minus population proportion y That's good here's the variance though and we'll do the typical standardization we're going to subtract off the expected value divide by the square root of the variance and that means that it will follow this this test statistic here will follow a normal distribution with mean zero standard deviation one and the standardization comes directly out of either chapter 2 or chapter 3 when we did the Z scores or the standardized scores if p x equals p y is our null hypothesis then that means that the second half just goes to zero or is equal to zero and this is the usual Z procedure version of the test we can do an equivalent test to this this isn't a side if we Square this thing on the left and that means we have to square this thing on the right squaring this thing on the left leads us to this squaring the thing on the right brings up a new distribution it's the chisquared distribution this probability statement and this probability statement are identical there is no information contained in one that's not contained in the other there is no difference in power there is no difference in Precision there is no difference in accuracy the two tests are mathematically equivalent so the one you use if you use this one you might as well use this one if you use this one go ahead and use this one they're the same the proof of that is left for later class 225 or 321. so let's see how to do this I'd like to determine the proportion of males who wear hats is the same as a proportion of females who wear hats notice now we're dealing with proportions in two separate populations proportion of hat wearers of males and proportion of hat wearers of females to test this I sample 100 males 100 females 10 males and 16 females were wearing hats so those are the hypotheses step one we're going to assemble the information we have P sub X is what we observe P sub y sample proportion I guess NX and Y we asked 100 males 100 females Alphas 0.05 which means our Z sub Alpha over two is plus or minus 1.96 this is the test statistic we have to calculate I'll give you a hint there's a fast way of doing this in R and you already know it but let's go ahead and churn through this so here's the formula for the test statistic this is the plug this is the Chug and we're chugging some more and more chugging and chugging we're done chugging so our test statistic is negative 1.26601 this is this distribution is our standard normal distribution this is what we observe negative 1.26601 it's right here that value is as extreme so it's positive 1.26601 or more so is the Shaded area on both sides so these two areas add up give you the pvalue because the normal is symmetric twice this area is the pvalue so we can calculate the pvalue it's 0.2053 because the pvalue is greater than Alpha we failed to reject the null hypothesis we do not have evidence that men wear hats at a rate different than women which is what this says there is no evidence to claim that they may they may not we just don't know we don't have the evidence um what I just described is the walled test doing this in R is just one line as opposed to doing all this calculation by hand all this fun calculation it's just one line This is the output 10 successes for males 16 successes for females out of 100 trials for men and 100 trials for females gives us back the data here's the pvalue 0.2071 greater than Alpha failed reject the null hypothesis no evidence of a difference and that's it how how much faster is this than all of this and I'll tell you this when I was typing this up you know how many times I had to go through this and make sure I didn't make a mistake in the calculations I'm not talking about in the typing unit I'm talking about the calculations I think I even made a mistake in the numerator here I think I made this as plus 0.06 so allowing the computer to do all of this in one line is just amazing so in today's slide deck we covered procedures for estimating a single population proportion and for estimating the difference in two population proportions if you're doing this in R I'm sorry if you're doing this for Hawks walled test works for both of these and you should use the wall test if you're doing Hawks if you're doing this for real binomial test for the first and prop test for the second in the future we're going to see many many many many many many many many many many many many many many more procedures more tests again create that section in your notebook dedicated to test the assumptions of the tests we looked at binom.test and prop.test wold.test behaves the exact same as either one of these except instead of binom.test you'll do Wald DOT test and instead of prop.test you'll do walt.test but the the parameters that you put over here are going to be the same here are some available scas to help you work through these SCA 12 and 22 the 2 means it's for proportions the one is the number of samples so this sca12 is for one sample proportions and SCA 22 is for two sample proportions and they're all located in the usual place um so now I'm going to ask the intralecture questions one of these will be very familiar question one when is the research hypothesis the same as the alternative I encourage you to go back and look it up again the rest of the the remaining two questions will be different but similar to each other for what types of hypotheses do we use the binomial tests and what I mean by that is I'll just give you the answer because it's fun the types of hypotheses for which we use binomial tests are those where we're trying to test hypotheses about one single population proportion and question three is what types hypotheses do we use the proportions test I can give you that answer too for hypotheses about comparing two population proportions I don't mind giving the answer sometimes chapter 8 and R for starters sections 10 4 and 11 4 in Hawks go ahead and read up on hypothesis tests in Wikipedia all procedures at PDF you should have that printed out and available to you when you're working on stats and that's the end handling proportions um hope this was helpful I will see you or talk to you later hello and welcome to chapter 10 where we cover lots and lots of tests for hypothesis testing here we're going to look at section 10 6 where we look at discrete distribution matching we introduce the chisquared goodness of fit test and by the end of this lecture you should be able to understand the theory behind and test hypotheses about just one thing comparing an observed categorical distribution to a hypothesized one also the usual better understand the pvalue and how to test hypotheses but this one is about goodness of fit tests so here's the parametric procedure it's the only procedure we've got the chisquared goodness of fit procedure the null hypothesis is that the data are generally generated by the hypothesized distribution so the research hypothesis is going to be in the null hypothesis is going to be that hypothesized distribution the alternative hypothesis will be the data are not generated by that hypothesized distribution the graphic is going to be the usual binomial plot notice that it's expanded to go from not one not two but K different values or different groups these will be successes the x's and the NS will be the number of Trials requires that the number of successes and expected number of successes in each group is at least five sometimes it's required to be 10 it really does come down to how precise or accurate do you want your values to be or your estimates to be um more data is always better as long as it's good data of course if your expected number of successes is not at least five in each group it's really not anything you can do except collect more data and if you can't collect more data then you've got to conclude whatever you can conclude from this test but you must specify the sample the sample size is too small to properly use this chisquared goodness of fit test therefore these results are Highly Questionable the r function is that tests what does stand for it's Chi Squared and notice again we got a DOT test thing we get a whole lot of DOT test things and the Dot Plot things we got a lot of Dot Plot things note that this function this DOT test function is not Hawks they use something close to this but the case not test function actually makes adjustment for the fact that the binomial distribution is discrete and the normal is not the hand calculations that we have do agree with Hawks however um so pay attention to the quote hand calculations and how to do them in R so all you have to do is just substitute a few change a few numbers and it'll be good for you so we'll start with a framing example that is a picture of a threesided die I would like to test if it is fair to do this I roll it 600 times and tabulate the observed frequency distribution in those 600 rolls I got 181s 215 twos and 205 threes note that I would expect to have 200 ones 200 twos and 203s if the die were Fair I didn't I got 181s 215 twos and 205 threes now the question is is the 180 215 and 205 just due to the random fluctuation in a fair die or are those values too far away from what we would expect if the diver fare and that's really what all of the statistical testing is coming down to it's what I expect versus what I observe and is what I observe too far away from what I expect given expected random fluctuation so we're given this information I'm just abstracting it The observed counts are 180 to 15 205 the expected counts are 200 200 200 now where did I get the 200 sample size is 600 I rolled it 600 times I want to test if the threesided die is fair if it is fair then I would expect the number of ones to equal the number of twos to equal the number of Threes each of them to be a third of six hundred note what the information above actually gives to us it gives us this a set of observed counts and a set of expected counts we're going to call the observed counts x's and the expected counts Muse and in this there are K groups that's what the k stands for in the example that we're working on K is equal to 3. so our goal is to create a test statistic that measures how far apart the observed is from the expected while creating well still having a distribution that we know and we're going to use Define we in this case as statisticians we in this class don't know this distribution yet although technically we have bumped into it once when we were doing onevar Dot tests so it can be shown but not in this class that the test statistic approximately follows a chisquared distribution with K minus 1 degrees of freedom the approximation gets better as U gets larger so that statement the TS equals the sum and the probability statement is something that is to be shown in a future course in other words it's beyond the scope of this course I like that phrase beyond the scope of this course uh TS represents test statistic it's the sum over all of the groups The observed minus the expected squared I divided by the expected y the squaring we're squaring it because if we don't Square then the sum of the x i minus the MU I is always going to give us zero so this squaring allows us to avoid the zero problem and to indicate that larger values of there are could be too high or too low they're just farther away from what you would expect so recall the observed counts were 182 15 205 the expecteds were 200 200 200 expected values again came from NP n is our number of Trials the p is the probability of success for each of those categories the probability of getting one is onethird so the expected number ones is 600 times onethird respective number twos is 600 times onethirds respect to number three is the 600 times onethird yes binomial mu is equal to NP so let's go ahead and do these calculations out for fun TS to test statistic is defined as this if we expand that summation that's what this means first time through I is equal to one so it's x sub 1 minus mu sub 1 squared over mu sub 1. plus because it's a summation x times through I is equal to two x sub 2 minus mu sub 2 squared divided by mu sub 2. third time through and again we're going to add on the third term which is x sub 3 minus mu sub 3 squared over mu sub 3. so this is the expansion of the summation and now all we're doing is just substituting in the values we've got x sub 1 is when a to the x sub 2 is 215 x sub 3 is 205 those are what we experienced or observed the expected values were 200 each now we do the calculations calculations calculations calculations so our test statistic is 3.250 this is the distribution of the chisquared distribution with two degrees of freedom this is what we observed for our test statistic 3.250 3.250 shaded area is 3.25 is the probability of having an a test statistic of 3.250 or greater so this shaded area is the pvalue and that pvalue actually comes out to be 0.1969 here we are with a pvalue we know how to interpret that pvalue so it's been the same since we introduced pvalues pvalue is greater than Alpha failed reject the null hypothesis we don't have evidence that the die is unfair pvalue greater than Alpha we failed to reject the null hypothesis we don't have evidence that the die is unfair pvalue greater than 0.05 we failed reject the null hypothesis we don't have evidence the die is unfair I think three is enough so there's the conclusion we don't have evidence that the die is unfair it could be unfair I don't know but the data doesn't tell us it's unfair note that the data doesn't tell us it's fair either the data leaves it up to a big question mark of we don't know could be fair may not be fair we don't have enough data to say so here's the way of getting it in uh for Hawks calculations OBS will be the observed counts EXP is the expected counts so these will need to change for your problem TS this is how you quickly calculate that test statistic that doesn't need to change just running TS will give you the value of the test statistic that won't need to change 1 minus P Kai's doesn't change of TS doesn't change but we do need to change the degrees of freedom this 2 needs to be K minus 1. the number of groups minus one so if we've got 10 groups here change that to a 9. if you've got two groups here change that to a one this will get you the Hawk's answers so we're going to test statistic of 3.25 a pvalue of 0.1969 here it is using r kaisk.test so you give it the observed distribution of counts and this is important this is a distribution of counts and you also give the hypothesized distribution but these are going to be probabilities so instead of 200 200 200 you give it onethird onethird onethird just so happens in this case you do get the same chisquared test statistic degrees of freedom and pvalue as if you did it by hand using the Hawks method example two my friend claims that the proportion of cars in the Knox campus that are American is the same as the proportion that our European and as the proportion that are Asian okay so to test this I went to the parking lot across burying from smack and counted the cars and their Origins and had a conversation with a couple people who were wondering what I was doing over there I on that day I there were 19 American 23 Asian and two European cars so the observed and expected values are 19 The observed is 1923 and 2. the expected is 44 thirds 44 30s 44 thirds those are expected counts why is it thirds that's because they're the same proportions so the proportion of Americans equals the proportion of Europeans equals a proportion of Asians onethird onethird onethird where the 44 come from there are 44 cars so let's go ahead and calculate this out three groups so the test statistic this is the formula which is expanded to this plug and chug and chug and chug are we done chugging there we go test statistic is 16.954 here's the distribution here the chisquared distribution with two degrees of freedom there are three groups so it's three minus one two degrees of freedom whereas the pvalue wait where's the test statistic test statistic was almost 17 so I guess test to sits way over here pvalue .0002 because the pvalue is less than Alpha we reject the null hypothesis the proportion of cars on campus is not the same for American Asian and European that's all we can say is it's not the same this is not the distribution because the null hypothesis in the research hypothesis for chisquared goodness fit test is this is the distribution the alternative would be this is not the distribution um what does this conclusion assume and it's time we start thinking about this again notice how I collected the data I went across the street from smack and went through the went through the parking lot for this to be an appropriate conclusion the data I collected has to be representative of the cars on campus cars in that one parking lot has to be representative of all the cars on campus the cluster sampling I did has to be appropriate remembering back to chapter one since it's cluster sampling I have to somehow assert or check that the proportion of cars that are American European Asian is independent of the parking lot huh how could we test that and I'll give you a hint we don't know yet because we would be testing for Independence between a categorical variable car type and a categorical variable parking lot so be aware that'll be a future test here is the code again three groups degrees of freedom is three minus one we can also do this with the test notice I did not specify comma P equals onethird onethird onethird here's why by default R assumes equal probabilities so if you are testing equal probabilities you don't actually have to specify that for r you can I encourage it because it increases the readability of your code but you don't have to in fact I strongly encourage it because it does make very explicit that you are testing equal probability amongst those three groups sample three Department of Mathematics claims that the proportion of its graduates who went to grad school is twice the proportion of any other postback path to test this department sent out a questionnaire to all of the alums who for whom they had current addresses here's a table of the results that includes the count and the expected we had 35 that we could get in touch with um notice we said twice the proportion of any other post back so 14 expected for grad school but seven for business seven for Education seven for unemployed by the way this is fake data we actually counted 14 grad school seven business 10 education five unemployed so this will be the vector of counts observed and this will be a vector of counts expected this is how we calculate it by hand notice we now have four groups here's the chisquared distribution with three degrees of freedom K is 4 K minus 1 is 3 degrees of freedom here's our value of the test statistic we got the Shaded region is our pvalue pvalue of 0.5874 because the pvalue is greater than Alpha we fail to reject the null hypothesis we do not have evidence that the data do not follow the distribution the claimed distribution the claim made by the math department that twice as many of its graduates go to grad school is in any other category is reasonable note that we did not prove that the math department is correct we just said their claim is reasonable given this data given additional data it may not be reasonable but given this particular data it is and then of course I'm going to ask what does this conclusion actually assume so I want to start tying back to chapter one more and more not just subtly but very explicitly how did the math department get this data is this data representative of all math majors in other words who would we have kept in touch with or who would have kept in touch with us is it equally likely that any graduate of the math department would have kept in touch with us or are certain types of students like grad students more likely to keep in touch with us or like education students more likely to keep in touch with us notice that we send out a questionnaire to all of the alums for whom we had current addresses Who Are we more likely to keep current addresses for who is more likely to fall off the grid this is how you do it for Hawks notice we got four groups so the degrees of freedom will be three notice I did the p is equal to C of 1477 divided by 35. I could have done CF 14 over 35 comma 7 over 35 comma 7 over 35 comma 7 over 35 this is a little bit faster one last example one of the initiatives of Knox College has become more representative of the US population this raises the question of whether we have succeeded in terms of numbers according to the fall 2019 domestic numbers the data are observed expected and the expected is due to its Compares aha let me start that again expected proportions are from the Census Bureau so we have 109 71 194 and 635 we'd expect 163.58 73.10 238.37 and 555.61 if we perfectly followed here's a graph notice what the the boxes with the lines are what we actually are and the dots are the population of the United States notice we are below average here we're dead on here we're below here and we're way above here by the way the boxes indicate confidence intervals the horizontal line is the proportions the sample proportions so here we are using r chisquare tests that's the observed counts this is proportions according to the Census Bureau I run it got an error the error is probabilities must sum to one what's the error well these probabilities don't add up to one why don't they add up to one well there's a couple possibilities one is I may have dropped a couple of the smaller groups two there could be rounding errors but for R to do this these probabilities have to add up to one so the question comes down to how do we fix it if we actually did leave out some of the smaller groups or if there is rounding how do we fix this R does have a parameter rescale dot p we set it equal to True it'll rescale these so that they do add up to 100 percent so this will fix the error gives us a pvalue that is significant is much much less than Alpha we have evidence that the distribution of races and ethnicities in Knox is does not match that in the population at large of the United States have her realize that this pvalue corresponds to a snapshot in time it doesn't compare where we were 10 years ago it just says where we were fall of 2019. so we weren't there we didn't match the distribution in the United States in 2019 but we actually are getting much closer to Ria to the population in the United States much closer so the pvalues are getting larger the test statistics are getting smaller and that's what this conclusion assumes so let's go ahead and do the introductor questions there we go huh this looks familiar question one when is the research hypothesis the same as the alternative hypothesis question two I like question two and question three so they're the same question essentially but I I want two examples question two is Give an example where you would use the chisquared goodness of fit test in your area of Interest so if your major is or will be political science give me a time when you would use the goodness fit test in political science if your area of interest is or will be biology give me an example where to use the goodness of fit test in biology or in physics if you're going to be a physicist or in uh I can't think of any other areas and so if you're going to be an anthropologist a sociologist that's question two is give one example question three is to give another example so question two and three are give examples of where you would actually use this chisquare goodness of it in your discipline and this actually could be in classes that you've taken in your discipline for instance in chemistry reaction rates be no that wouldn't work for instance chemistry we've had classes in chemistry you may have needed to use a chisquared goodness fit test so in today day Slide Attack ah let me start that slide again so in today's slide deck we covered procedures for testing if the observed categorical data came from the hypothesized distribution that's it I'm in the future we're going to look at the chisquare tests of Independence we're going to look at analysis of variance or you do linear regression those are the three remaining tests um all procedures take advantage of the sca's please take advantage of the scas for work for practice we only had one r function.test remember X and P are the numbers of successes and the probability of successes in each of the groups respectively SCA 32 is going to be very helpful here two for proportions which goodness fit test does talk about proportions and three more than two samples there's nothing in R for starters for this Hawks learning this is section 10 6. that brings us to the end um that's it um so bye hello and welcome to chapter 10 where we cover a lot of the hypothesis testing not all of it but a lot of it and here we're looking at categorical Independence section seven uh categorical Independence means that you're testing for Independence between two categorical variables um contrast this with testing Independence between two numeric variables which will be linear regression which will be in the future this lecture is about two categorical variables and that's really what we're doing still understanding the theory behind in hypotheses about determining if two categorical variables are independent and better understanding the pvalue and how to test hypotheses parametric procedure is the chisquared test of Independence it's called a chisquared test because the test statistic follows a chisquared distribution they're called ttests because things in the past that we've covered are called ttest because their test statistic follows a t distribution things in the past have been called ztests because their test statistics follow a z distribution we saw an F test at one point because the test statistic follows an F distribution here this is one of many chisquared tests and they're called chisquare tests because the test statistic follows a chisquared distribution the null hypothesis is that the two categorical variables are independent the graphics a matrix plot or a graphic is a matrix plot resign yourself to the fact that graphics for chisquare tests of Independence all look ugly and are difficult to interpret um this requires expected number of successes to be at least five in each cell uh or 10 in each cell or I mean the reason for this is It's a normal approximation of the binomial distribution and so larger sample sizes means that that normal approximation improves Central limit theorem Section 5 6 no 6 5 . this function is indeed what hawks covers so you won't have to change anything to to your hux homework with this however for R to give you Hawks results you'll need to use correct equals false in the function and I'll show you where to do that so let's go ahead and look at a framing example I would like to test if there's a relationship between whether a person has blue eyes and whether that person is a math natural science major yeah to do this I asked 100 people at Knox College so I'm dealing with one large sample of 100 people and I'm measuring two things on each person one if that person has blue eyes or not blue eyes and two if their major is in MNS or not in m s so here's the contingency table notice that each of these is a count so in my sample seven people had blue eyes and were M and S majors 20 people 7 plus 13 20 people in my sample had blue eyes the 52 people did not have blue eyes and were not math Natural Science majors 80 people 28 plus 52 did not have blue eyes 35 people were math Natural Science majors and whatever 13 plus 52 is I think that's 65 were not math Natural Science majors and adding them all together gives you a hundred the number of people that I asked so contingency table is a table of counts so let's us let's ask ourselves this question what does it mean for two categorical variables to be independent recall back to chapter four that really fun chapter where it covered Independence at one point that is exactly what we're going to be using to create our expected counts so we'll have observe counts expected counts and when you've got those two it's just a hop skip and a jump to your chisquare test um so what does it mean it means that the value of one does not affect the value of the other particularly for this it means that the probability of a math natural science major does not depend on whether you are blueeyed we can also frame this as the distribution of M and S Majors is the same for blueeyed people as it is for nonblueeyed people but you could also frame this as the distribution of blueeyedness is the same for mathematical science Majors as it is for nonm S majors let's check with the data for this interpretation here's the counts so what's the proportion of blue eyed people in this sample 7 plus 13 over 100 so 20 percent what's the proportion of blue eyed people who are mathematical science Majors it's seven divided by 35. this is the number of blueeyed people who are math Natural Science majors and this is the number of math Natural Science majors 20 it's a match so if I tell you I'm blueeyed that does not affect my knowledge about you being a math natural science major notice we could do the same thing proportion of blue eyes in the sample versus proportion of blueeyed people who are not math Natural Science Majors again it's 20 percent so if I tell you I'm blueeyed that should give you no information or that does give you no information about whether I am a math natural science major because the probability to do the same or whether I'm not a math natural science major probability is the same we can do this with nonblloid people proportion of nonbloid people in the sample is 20 plus 28 plus 52 which is 80 divided by our sample size of 180 percent proportion of nonbloid people who are m s Majors is also 80 28 divided by 35. so if I tell you I'm not blueeyed I'm not giving you any information about whether I'm a math natural science major X page or not because the proportions or the percents are the same so me giving you information about my ID eye color doesn't give you any information about my major in other words my eye color is independent of major now that we've seen a small example let's go ahead and generalize this here's our data variable one is a variable two is B variable one has two levels A1 and A2 the B variable has two levels B1 and B2 x 1 1 is the number of people the actual counts who were A1 and B1 X12 is the actual counts of people who are B1 and A2 x21 were the actual counts of people who were B2 and A1 and x22 is B2 and A2 count I'm going to scroll back to our data I only have to go back to here this would be a this would be X11 x 1 2 x 1 3 x 1 no I got that wrong X1 1 x 1 2 x 2 1 x 2 2 there we go the variable a is eye color the level A1 is blue level A2 is not blue the variable B is Major level B1 is math the natural science major level B2 is not math natural science major now we're going to create column and row sums we'll number the row sums R1 and R2 R1 will be the number of people who are B1 R2 will be the number of people who are B2 column sums will be the number of people who are A1 and C2 is a number of people who are A2 going back a slide so R1 will be 7 plus 28 there are 35 B ones R2 will be 13 plus 52 there's 65 b2s C1 is 7 plus 13 so there's 20 a ones and there's 80 a2s the total sample size will be the sum of everything here or it'll be the sum of the row sums or it'll be the sum of the column sums n's 100. so here's the data that we were working with here's abstraction of it in other words we've got our counts this is observed now we've got to figure out how to get expected if the two variables are perfectly independent and again refer back to section 4 3 the expected values would be although their expected values to be n times p so we got the ends common for all of these p is the probability of being in this cell if the two variables are independent that would be the probability of being in row one times the probability of being in column one if they're independent here this would be the probability of being in row two times the probability being in column one row two column two Row one column two those would be the probabilities multiplied by n to get NP which is the expected counts so this will be the table of expected counts you've got the table of observed accounts table of expected counts and we know what to do with that for a chisquared goodness for chisquared distribution observed minus expected squared over expected we're going to call this X2 instead of TS chisquared distribution as one parameter called the degrees of freedom for the goodness of fit test it was groups minus one for the test of Independence it's going to be rows minus 1 times columns minus one rows minus 1 times columns minus one in this example we've been working with there are two rows two columns the degrees of freedom will be one two minus one times two minus one example one of the actual examples we'd like to determine the proportion of males to wear hats is the same as a proportion of females who wear hats to test this I sample 100 males 100 females you know this is sounding familiar uh 10 males 16 females were wearing hats oh yeah we have seen this comparing two proportions let's also look at this in terms of a test of Independence Independence between gender and hat weariness so doing it the long way here's the table of observed values 16 females had a hat 84 did not 10 males had a hat 90 did not row sums are 100 and 100 columns sums are 26 and 174. if the two are independent then we would expect the proportions of females with Hats the proportion of males with Hats to be close to each other and close to 26 over 200 here's the table of expected values again in all the painful Glory n times R1 Over N times C1 over n times r 1 over n times C2 over n Etc so these are observed counts and these should be expected counts doing the math it actually comes out kind of nice it took a lot of effort to get it to come out so nice um so here's the test statistic value observed my suspected squared over expected we observed 16 we expected 13 we observed 84 we expected 87 observe 10 observed 90. we had a chisquared test statistic of 1.5915 we need to compare this test statistic to the chisquare distribution with one degree of freedom R is 2 C is 2. here's how we'd actually calculate that so the pvalue is one minus because it's greater than or equal to 1.5915 degrees of freedom is 1. gives us a pvalue of 0.2071 after all this fund calculation because remember we also had to determine the expected values here or we can use r if we add correct equals false then we'll get the Hawks result if we leave the correct equals false out we'll get a better result the first step is to create the Matrix of observed values The Matrix of observed values here's how we do that we use the function Matrix the first thing we give the Matrix function is the counts and then we specify how many columns here notice that the counts are going in by row let me go back 1610 84.90 16 10 84.90 so this line will actually give you the observed Matrix going back this thing right here as a matrix and then all we have to do is a chi got tests of that observed Matrix if you're doing it for Hawks we do comma correct equals false this first line that is the most difficult but it's just the Matrix function this the the values by row and then you specify the number of columns if we do the r we leave off the correct equals false here's the output for r got a pvalue 0.2931 degrees of freedom of one The observed value of 1.1052 because the pvalue is greater than Alpha we fail to reject the null hypothesis we do not have evidence that the hat wearing rate for males differs from the hat wearing rate for females similarly we could say we have no evidence that the gender ratio for hat wearers differs from that for nonhat wearers both are actually equivalent interpretations equivalent interpretations if one is true the other is true notice the difference between the two tests is minor when the sample size is large but we have 100 or so sample size pretty large so this is with the continuity correction if we did it with correct equals false we have it without example three I'd like to determine if females have a different grade distribution my stat tone of course is than males this is data from I forget what uh I think my first three years here so gender this is A's b c's D's and F in the in the class yes it is fake data don't worry um so if all my past students 57 were females who got A's 68 were males who got B's 40 were females who got these and 22 were females who got F's so I would like to determine if the grade distribution for females is this is different than that for males in other words what I want to determine is is the grade distribution dependent on gender is the grade this grades a categorical variable independent of gender categorical variable there are a lot of ways of interpreting this and they're all logically equivalent so to do this we just got to put get these numbers into a matrix 57 49 7868 Etc and do a test on it it will automatically tell us there are four degrees of freedom how did I get four degrees freedom two rows five columns R minus one times C minus 1 gives us four putting the Matrix in specify the number of columns is five we did it by column this is the Hawks result because I specified correct equals false pvalue is 0.9851 because the pvalue is greater than our usual Alpha of 0.05 we cannot reject the null hypothesis there is no evidence that the grade distribution differs between females and males similarly I could say hey I have this student the student got AC well guess what I didn't give you any information about the student's gender because information about the grade is independent of information about the gender so let's go to the intra lecture questions this looks familiar when is the research hypothesis the same as the alternative hypothesis should have this by now question two I want you to give an example where you would need to test independence of two categorical variables in your area of Interest two categorical variables and this is for our lecture today you would be able to use the chisquare test for Independence for this example question three is Give an example where would you need to use tests for independence of two numeric variables in your area of interest you do not know how to do this test yet this is not from today today was just too categorical independents in the future we'll learn how to do two numeric Independence but I want you to start thinking about okay where would I need to test for independence of two numeric variables so we learned how to test if two categorical variables are independent today future we got Anova and linear regression kaisk test performs a chisquare test of Independence m is a matrix the sca is number 41 there are no readings in our starters Hawks has section 10 7. and the usual don't forget the all procedures make sure you have a page for each of these and that's it hello and welcome to the analysis of variance this is the most important part of chapter 11. analysis the variance is used to test for independence of a categorical and a numeric variable that's one use of Anova the usual use of Anova is testing for equality of population means amongst more than two groups so by the end of this lecture you should be able to understand the theory behind testing the means of more than two populations and the independence between America and a categorical variable and again better understand pvalues and how to test hypotheses so let's do a frame example here I like to test if the average GPA of a student is the same for the four types of majors at Knox m s is math and natural science HSS is Humanities and social sciences hum is Humanities because it's history and social sciences and art is all the art stuff so to do this I asked 200 students at Knox College 50 of each major type and asked two questions one what is your major type two what is your GPA so note that we have one population students at Knox College and on every member of that population I asked two questions one what is your major type and two what is your GPA if we're looking at a relationship between the two we're actually testing for Independence between those two variables major type is categorical and the GPA is numeric so here's a box plot side by side box plot of the data I collected each dot represents a student that I talk to Green Dots or MNS majors horizontally corresponds to the reported GPA the dots are included as is or as are the box plots for each of the four major types so there's actually a few equivalent ways of looking at this question and as you've learned from your math courses throughout the years the ability to look at a single question from multiple standpoints is always a strength and that strength leads leads us to be able to in this case determine what the actual test should be so one way is do the means in each group significantly differ in other words is the mean for math and natural science which is probably located somewhere around here is that significantly different than the mean for history and social sciences which is probably down here somewhere and for humanities which looks like it's here somewhere and art which looks like it's here somewhere so the first way of looking at this is comparing the means of the individual groups second way of looking this is are the group and the GPA independent these two questions are logically identical are the group and GPA independent and the Third Way is does including the group identifier improve our ability to estimate a person's GPA and since we're looking at the mean the expected value what we're looking at is actually testing okay does the information of group improve our understanding of GPA thinking back to chapter four that's equivalent to saying our group and GPA dependent or independent think back to the category the conditional probability definition of Independence and it's this last one that gives us some insight into the test statistic what improving predictions implies is that we reduce the uncertainty in those predictions and reducing uncertainty means we reduce the variance and those predictions and this is the idea behind the analysis of variance procedure first thing you do is measure the variance of the original data to measure the variance that is left over after you include the model and by model I mean the group identifier and then you look at the ratio between the two from the explained to the Unexplained and it's this last ratio that's actually the test statistic now let's think about that for a second if the explained variance the variance is explained by the model are contained in the model or taken care of by the model is large compared to what's left over the Unexplained variance then the model is good because the remaining variance is small compared to what you started with the model is explaining a lot of the uncertainty in the dependent variable if on the other hand the explained variance is small in other words if the model doesn't explain much of that dependent variable then the model is is virtually worthless and it's that ratio which is an F ratio it's called an F ratio because the test statistic follows an F distribution it's this F ratio that leads us to a pvalue which leads us to an interpretation of those results so that was the theory let's go through the calculations by hand probably the only I think we get one more time when we do it by hand but so with that background let's calculate the test statistic and the pvalue using the Anova table and here's the blank Anova table this is a this is what an anova table looks like there's three sources the model what's remaining after you apply the model and then what's originally there so model is the model that you apply which is the independent variable of the grouping variable error is what remains after applying the model and the total is what you start with SS is sum of squares so this box will contain the sum of squares for the model this box will be the sum of squares that remains and this box will be the total sum of squares or the original sum of squares note that this box plus this box will give you this box this will be the degrees of freedom for the model degrees of freedom for the error and then the total degrees of freedom and the total degrees of freedom will be just the usual the total sample size minus one and again note that this box and this box will add to this box Ms stands for mean squared so this would be the mean squared error for the model sorry this will be the mean squared for the model this will be mean squared for the error the mean square is just the ratio of the sum of squares to the degrees of freedom I mean we saw that back in chapter two or chapter three when we were looking at the variance calculations it was the sum of the squares divided by the degrees of freedom we just didn't use term degrees of freedom back then it was n minus 1. this F ratio is going to be this box divided by this box and this pvalue will be a function of this F ratio according to the F distribution so let's fill it in and you get to see all the calculations the column marked SS contains the sum of squares for those three sources the sum of squares is just the sum of the deviation between the observation and the mean notice the structure of all of these is the same you're adding up over all the data values that's what the double summation is you're adding up within the group to the grand mean so this is what's explained by the model the the mean within the group difference with the grand mean or the mean of all the data values what's remaining is the variation within the group the data value to the group mean and this is the original this is the data values to the group mean back in the day this was x minus X bar squared add it up here we're just renaming x and x bar so again this is what's explained by the model this is the group mean minus the grand mean this is the what's left unexplained the variance within each of the groups the data value to that group mean and this is what you started with because each of these calculations require 50 sums differences in squares calculating by hand it's not really realistic so I'm just going to give you the answers for this data notice I didn't actually give you the data itself here's a sum of squares here's what's here's the error that's remaining here's the total notice the sum of squares for the model plus the sum of squares error is going to add up to the total the important reason for that is that means that the SS model or the the sum of squares for the model and the sum of squares for the error are independent of each other doesn't mean much for us in stat 200 but it will be important in later set courses call Mark DF contains the degrees of freedom for the three sources there are the parameters that reflect the amount of information contributed by each Source probably believe the best definition of degrees of freedom the degrees of freedom tool is just the total sample size minus one so it'd be 200 minus one because I asked 200 people the F model is the number of groups minus one there's four groups four different major types so DF model will be three and then the way I calculated is just DF total minus DF model that'll give you the if error 3 and 199 and the DF error will be 196. the mean squared will be the sum of squares divided by the degrees of freedom now this should look should refresh or should look very similar to what we got back in chapter two with the variance sample variance it was 1 over n minus 1 times the sum of x minus X bar squared calculating the same thing here this is an estimator of a variance just like s squared was an estimator of a variance so mean squared model is 2.6104 mean squared error is 0.4718 and this F statistic is going to be the mean squared model divided by the mean squared error as you can as you should guess by now the distribution of the F statistic IS F that's the name of a distribution do you know what the name that distribution is named after Fisher John Fisher one of The Luminaries of early statistics in the 20th century there is f it's 5.533 last box we'll calculate the pvalue pvalue is calculated in exactly the same way it's a probability of being of the test statistic being this extreme or more so given the null hypothesis is true if the null hypothesis is true F will be zero I mean if the null hypothesis is perfectly true this F ratio will be zero because there will be no uh this number will be zero this number would be zero remember the null hypothesis is there is no difference in the means or the null hypothesis is that the two variables are independent or the null hypothesis is the model explains none of the variation the model explains another variation this is going to be zero which means mean squared is going to be zero which means the F ratio will be zero over something which is zero so this is the probability of the distribution being greater than or equal to 5.53 which looks like this here's the F distribution for this particular problem pvalue is way out here it's this area that's hard to see 0.00115 how do we interpret the pvalue absolutely correct same as always compare it to Alpha if the pvalue is less than Alpha as in this case we reject the null hypothesis that means all of the following three things the two variables are not independent and that is GP average that is GPA and um GPA and major type those are not independent because p is too small it means the average GPA in the four groups is not the same and it means if you want to model the GPA if you want to explain the GPA then including the major type will help with that okay let's look at the rice yields I'll give you the data so we can see how all the calculations are done does rice variety influence the average yield amongst these four varieties here's the yield in each of the four plots for each of the four varieties so we got 16 plots in each plot we planted one of the four varieties of rice so there's the raw data here's a graph sure seems like variety D is much higher than the rest I don't know about variety a but it certainly seems as though at least one of the four varieties has a different average here's the blank Anova table let's calculate this all together just so we can see oh wow we really don't want to do this by hand so here are the formulas for SS model SS error and SS total from here so here's SS model we're adding up over all the data values the average in the group minus the grand mean squared but this actually translates to is this because there's 12 data points so we've got 12 terms from the data this is what we get the grand mean is 9981.9375 and the individual group means or variety means are those now we just plug and chug this term becomes 984.50 minus 991.9375 squared plus Etc substitution do a lot of calculations and we get 89 931. so the sum of squared for the model is 89 931. we can do the same thing with the error and total we could just calculate the total which is the variance of the data and subtract off the model to get the error degrees of freedom we've got 16 data points so DF total will be 16 minus 1. we've got four groups so DF model will be four minus one and then DF error will be 15 minus three mean squared is just the sum of squares divided by the degrees of freedom that 2 should not be there the F ratio is just the ratio of the mean squared of the model to the mean square root of the error so this would be about 30 divided by 4 should be about seven and a half this is the F ratio and it follows the F distribution that's why it's called an F statistic it follows the F distribution and now this pvalue is a probability of this distribution being greater than or equal to 7.212 . there are 7.212 this shaded area is the pvalue 0.00503 interpret the P bet value in the usual way compare it to Alpha if the pvalue is less than Alpha reject the null hypothesis in this case the null hypothesis is that the four means are equal or that the yield variable and the variety variable are independent or if we're trying to model the yield of wheat was it wheat or corn if we're trying to model the yield of whatever grain this is including the including the variety will help those three are equivalent the one you use depends on what you're trying to understand about rice I guess it's rice instead of corn or wheat now I'll do it in r three lines to get the data in it's the rice data set summary just to make sure that you loaded it in correctly and attach it that's what the box plot looks like after putting it up a little here's the code for the fancy box plot box plot line is actually way down here this will just get you a nice plain box plot here's the two lines of code to do all of the Anova calculations that you need to do we'll call it rice mod it's the model about rice the function is aov analysis of variance in parentheses this is the dependent variable that's the tilde to the left of the one and this is the independent variable the dependent variable here is the numeric variable and the independent variable is the grouping variable or the categorical variable this rice mod line doesn't do much of anything except everything inside it's the summary that takes it from what's inside R to actually put it in a form that is Meaningful for you notice it doesn't give the total stuff but we can figure out degrees of freedom total is 15. the sum of squares total is just the sum of those two there are four varieties so the degrees of freedom for variety is four there's a sum of squares the mean squared is the sum of squares over the degrees of freedom the F value is the ratio of the mean squared variety to the mean squared residuals which is often called error instead of residuals here's our pvalue notice we've got two stars on it two stars is down here so that pvalue is between .001 and .01 between U because we got the pvalue there and that's it just those two lines once you get the data in just those two lines interpret same conclusion Ronald Fisher introduced the Anova procedure in his 1925 book and to illustrate Anova he came up with this experiment collect a sample of pond water okay so you take your five gallon bucket and dip it in the pond and you've got a sample of pond water now divide that water in that same bucket amongst four different beakers now separate the beakers to ensure that there's no cross contamination and from each Beaker take four samples and counter record the number of amoeba present so you got this sample of pond water dip four beakers in that same sample so you would expect the amoeba concentration be the same in those four beakers and now from each speaker take four small samples and count and record the number of amoeba so you'd expect the averages in each of those four samples from each of the four beakers to be about the same so we would expect the pvalue of this experiment to be rather large data is available in the Fischer 38 data file this loads it attaches it this does the analysis of variance and the summary gives us these results we got a pvalue that's rather large notice that when the F value is small the pvalue is large and when the F value is large the pvalue is small it's because the larger the F value the more extreme your observations are if the null hypothesis is true and the null hypothesis is either all the means are equal or the the two variables the numeric variable and the grouping variable are independent or that grouping variable gives us no information about the numeric variable there's that F distribution there's the pvalue in Darker blue it's not small we didn't expect it to be small there's the conclusion and at this point we should be asking do these results make sense for every single analysis the results either make sense or we are learning something new or we did something wrong so here's the summary slides but I'm going to throw in the intro lecture questions here so the first introduction question does a large value of f correspond to a large pvalue or to a small pvalue does a large value of f correspond to a large pvalue or a small pvalue and again I would write the question your notes on the left answer below question two what is the null hypothesis in Anova notice I've given you three options for this give me any of those three and three here's a given example where you would need to test for the independence of one numeric variable and one categorical variable any example will work any good example will work so here's what we did in today's slide deck we covered Anova this procedure helps us determine three things if the mean of several groups are the same if a numeric and a categorical variable are independent and if knowing a group membership helps with estimation of the dependent variable all three of those are logically and statistically equivalent the way that you frame your results will depend on what the original research hypothesis and or research question are if the researcher is asking about the means of separate groups then that's the one you're going to use to interpret your Anova in the future we're going to do linear regression same reminders as always create that section in the notebook dedicated to the tests and the assumptions of that test take advantage of the scas and use the all procedures handout um there's some functions that we are hinting at the aov function we did in the summary function we did you've already seen the Shapiro test function the flickner test function is what we're going to use in the next set of slides notice that all we were able to conclude was the mains are all the same or at least one mean is different we weren't able to determine which mean was different next set of the next set of slides will cover that we looked at we could also conclude that the two variables are independent or they are dependent we weren't able to classify what type of dependents the next set of slides will help with that however the fligner test will need to be used to test one of the assumptions of Anova but that will be for next time and there are this the readings and that's the end and I hope this was fun or at least helpful
