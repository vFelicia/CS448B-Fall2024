With timestamps:

00:06 - so the purpose of this course and the
00:09 - place that this course started from
00:11 - was
00:12 - we were noticing at the university that
00:15 - there were lots and lots of people
00:16 - applying for grant money wanting to
00:18 - start new projects getting interested in
00:20 - trying to solve new problems
00:22 - and there was this buzzword flying
00:24 - around of deep learning and machine
00:25 - learning and we realized that a lot of
00:27 - people wanted to take part in this new
00:30 - fad but didn't really know what it
00:32 - actually involved they would just write
00:34 - it down in their grant application and
00:36 - say well i'm going to do some deep
00:37 - learning as part of this and then when
00:38 - actually came to it they didn't really
00:40 - necessarily know the full extent of what
00:42 - was possible with it and so this course
00:44 - started from my attempt to
00:47 - educate and inform people about what
00:48 - deep learning really is how it works
00:50 - from the lowest levels and the kinds of
00:53 - problems that you might be able to use
00:54 - it to apply to so this really is an
00:56 - introduction to deep learning
00:58 - we're not going to be assuming any
00:59 - knowledge of neural networks any
01:01 - knowledge of how that kind of data
01:03 - science stuff works and we're trying to
01:04 - cover all of that
01:06 - if you came to my course yesterday
01:08 - applied data analysis in python and then
01:10 - we're going to be covering a few of the
01:11 - same topics but
01:13 - 90 of it is going to be completely
01:15 - separate
01:18 - so
01:19 - let's start from the basics
01:21 - this course is called
01:22 - introduction to deep learning but before
01:24 - we can get to deep learning we need to
01:25 - understand the context about how that
01:27 - fits into the larger scheme of things
01:30 - so machine learning is where i'm going
01:31 - to start and that's a lot of what we
01:33 - covered in the course yesterday but at
01:35 - its core machine learning is any
01:36 - technique which uses computers to
01:39 - discover patterns of information
01:41 - so it's what would traditionally have
01:43 - been called statistics but because using
01:45 - computers to do it it allows you to get
01:47 - greater reproducibility and most
01:49 - importantly for some of the stuff we're
01:50 - going to be talking about today
01:52 - much greater scale you can go to much
01:55 - larger amounts of data
01:56 - much larger depths of information you're
01:59 - trying to seek out and that's what
02:01 - differs machine learning from
02:03 - what would traditionally be called
02:04 - statistics is this scale of what you can
02:06 - do with it
02:09 - now machine learning sits inside the
02:10 - wider field of artificial intelligence
02:12 - artificial intelligence is a
02:14 - very poorly defined term it basically
02:17 - means anything where computers are
02:19 - trying to make decisions so that can be
02:21 - something as simple as
02:23 - trying to make a decision as to what the
02:26 - value of y should be given x that's a
02:28 - very very narrow form of artificial
02:30 - intelligence all the way up to
02:32 - self-driving cars and potentially
02:35 - according to sci-fi anyway well beyond
02:37 - inter-generalized artificial
02:38 - intelligence
02:40 - but within artificial intelligence this
02:42 - is machine learning which is a better
02:43 - defined field where it's about
02:45 - statistics and data science and things
02:46 - like this
02:48 - but of course
02:49 - even machine learning being a narrower
02:51 - field than artificial intelligence is
02:53 - still incredibly broad and is full of
02:55 - loads and loads of different techniques
02:57 - and concepts
02:59 - in the course yesterday we covered
03:01 - things like k-means clustering for doing
03:03 - uh unsupervised clustering algorithms as
03:06 - well as linear regression and
03:07 - correlation studies so these were some
03:09 - of the tools you'll apply to your data
03:12 - and what we're learning about today on a
03:13 - much larger scale is deep learning and
03:16 - neural networks around that
03:20 - so inside machine learning
03:22 - which remember is sitting inside
03:24 - artificial intelligence it's a whole
03:25 - bunch of different things
03:26 - yesterday one of those we looked at is a
03:28 - linear regression it is a very simple
03:30 - concept it got across you've got a
03:31 - scatter plot you draw a straight line
03:33 - through it that is nonetheless machine
03:35 - learning because you're using a computer
03:37 - to discover what the values of the
03:40 - gradient and the y-intercept are going
03:41 - to be
03:43 - slightly more advanced technique is
03:44 - something like k-means clustering it's
03:46 - trying to discover things about your
03:47 - data without you having to explicitly
03:50 - tell it where you want to end up
03:52 - there are
03:53 - hundreds maybe even thousands of
03:55 - different algorithms that sit inside
03:56 - machine learning variations therein
03:59 - obviously we don't have time to go
04:00 - through all of those
04:02 - possibly one of the most famous examples
04:05 - of machine learning algorithm
04:06 - particularly over the last 10 15 years
04:08 - or so are neural networks now neural
04:10 - networks have been around a long time
04:12 - they've
04:13 - decades and decades new networks have
04:14 - been around and at their core they're a
04:16 - relatively simple concept but over the
04:19 - decades as computers have got more
04:20 - powerful and the kind of questions that
04:22 - we want to scale up have got more
04:24 - difficult we've had to become clever
04:26 - about where how we apply neural networks
04:29 - to try and solve the problem
04:31 - and so for a long time neural networks
04:33 - were relatively constrained in the kinds
04:34 - of complicated problems they could solve
04:37 - but since about 2010 the last 10 years
04:39 - or so there's been a second wave or
04:42 - maybe even third or fourth wave
04:43 - depending on how you measure your
04:45 - history about
04:47 - how you can create new networks which
04:49 - are what is called deep now i'm going to
04:51 - come across shortly what i mean by a
04:53 - deep neural network but it's a specific
04:56 - structural shape of a neural network
04:58 - which has been managed to be solved by
05:01 - applying more advanced statistical
05:02 - techniques by very very clever
05:04 - mathematicians
05:06 - and it's this resurgence of neural
05:08 - networks which are deep which has led
05:10 - into the modern fad last five years or
05:12 - so perhaps in research of deep learning
05:15 - so deep learning is applying a deep
05:17 - neural network to
05:18 - some kind of problem
05:23 - so
05:24 - i've given you artificial intelligence
05:26 - i've drilled down into machine learning
05:28 - and from there we've drilled down into
05:30 - neural networks so this course is titled
05:32 - an introduction to deep learning but
05:34 - before i can really give you a sense of
05:36 - what deep learning is and why it's
05:38 - interesting
05:39 - we first need to make sure we
05:40 - understanding understand what a neural
05:42 - network is at their most basic
05:44 - elementary level how do they work
05:46 - because the terminology
05:48 - and the few bits of maths that i'm going
05:50 - to be putting in today are important to
05:52 - understand how the problem scales up and
05:55 - when you're wanting to use these things
05:57 - how you can solve the problems that
05:58 - you're going to come across when you're
05:59 - trying to get stuff done
06:02 - so a neural network is a collection of
06:04 - artificial neurons
06:06 - so the place we need to start before we
06:08 - think about a network is to think about
06:09 - the subset the things that make up that
06:11 - network the nodes that connect together
06:13 - and those are neurons
06:15 - and before i go any further i need to
06:17 - apologize and say that i am not a brain
06:19 - scientist i have no idea how the human
06:21 - or animal brain works so if anyone in
06:23 - the chat here does you might want to
06:25 - cover your ears for a second
06:27 - but at their core a neural network is
06:29 - modeled after a real animal brain
06:33 - a brain is constructed by having a whole
06:35 - bunch of neuron cells all connected
06:37 - together and all connected together but
06:38 - connected together in some kind of
06:40 - network
06:41 - and as
06:42 - inputs and impetuses come in from the
06:44 - outside world
06:45 - electrical signals are triggered off in
06:46 - some of the neurons and then depending
06:48 - on which neurons they're connected to
06:50 - and the conditions under which those
06:52 - following neurons are going to activate
06:54 - you get a kind of ripple effect going
06:55 - through with different scales and
06:57 - sections firing and other parts not and
06:59 - it's that complex interplay of all these
07:02 - different neurons which is what gives us
07:04 - intelligence
07:06 - and now the idea within your networks
07:07 - was to take a
07:08 - simplified model of how that works some
07:10 - kind of mathematical description and see
07:12 - if we can try and solve some of those
07:14 - same problems that the human brain can
07:16 - solve
07:18 - so from this point on you can forget all
07:20 - about the animal brain pretty much
07:22 - because everything i'm saying isn't
07:24 - really how nature works but it is how
07:26 - our mathematical model
07:28 - inspired model is going to work
07:32 - so an artificial neuron which is
07:34 - designed after the idea of what a real
07:37 - animal neuron is like has multiple
07:39 - inputs coming into it and it passes
07:41 - output out you can think of it as like
07:43 - being a little function with a bunch of
07:45 - inputs and one output
07:48 - at their most simplest
07:49 - the function that a newer one
07:51 - encompasses is adding together all the
07:54 - inputs coming into it
07:56 - and then putting the output out the far
07:58 - side
08:00 - it's slightly more complicated than that
08:01 - because it doesn't just take each input
08:03 - it takes the first input and multiplies
08:05 - it by a number
08:07 - adds that to the second input multiplied
08:09 - by a different number and adds that to
08:11 - the third input multiplied by a
08:12 - different number and so on and so on
08:14 - depending on how many inputs a
08:16 - particular neuron has
08:17 - neurons can have
08:19 - potentially zero inputs but
08:20 - realistically the minimum would be one
08:22 - they can have thousands of inputs
08:25 - but at its core it is this bit of maths
08:28 - here which is what is going on inside a
08:30 - newer one when you're passing
08:32 - information through the network
08:35 - it's the sum over the input values and
08:37 - their corresponding weights
08:39 - and notice that each input value x i has
08:42 - a w i
08:44 - each input has its own weight if there
08:46 - are 10 inputs to a neuron there are
08:48 - going to be 10 weights
08:51 - also bear in mind that every neuron is
08:53 - going to have its own set of weights so
08:55 - if there's another neuron somewhere else
08:57 - in the network its w1 will be different
09:00 - to this neurons w1
09:03 - so you can imagine if you've got a
09:04 - network full of 100 different neurons
09:07 - and they're connected together
09:09 - you're going to end up with millions of
09:11 - these weights all connecting together
09:13 - and trying to
09:15 - give you some kind of realistic output
09:20 - the one other thing to be aware of in
09:22 - this simplified model i just described
09:23 - is there's an extra step which happens
09:26 - after this sum here is calculated p
09:29 - it then gets passed through an
09:31 - activation function
09:32 - now an activation function in nature
09:34 - will be something like don't trigger the
09:37 - output until the sum here is above a
09:39 - certain threshold but because we're
09:41 - doing maths here we can do absolutely
09:43 - any activation function we might want
09:46 - there are various reasons for choosing
09:48 - one activation function over another but
09:50 - often on the whole they'll do something
09:51 - like if the sum is negative then just
09:54 - ignore it and if it's positive pass it
09:57 - on through
10:00 - so before i go any further i'm going to
10:01 - ask if there are any questions about how
10:03 - this part of it works how a single newer
10:06 - one is interacting before we then think
10:08 - about how we connect it through into a
10:09 - network
10:12 - right so there's a question there from
10:13 - yan
10:14 - is there reason for using a linear
10:16 - combinations of inputs only
10:18 - i think because it is the easiest thing
10:21 - to do mathematically
10:23 - it is a simple function you can apply
10:25 - which gives you a an effective result
10:28 - you do want to pass it through an
10:30 - activation function because activation
10:32 - functions can give you extra bits of
10:34 - magic
10:36 - so if your whole network was just linear
10:38 - combinations of inputs and weights
10:40 - then if you add a whole bunch of linear
10:43 - sums together what you get at the end is
10:45 - a linear sum
10:46 - and if that was the case the most
10:48 - powerful thing a network would be able
10:50 - to do would be to evaluate some kind of
10:52 - linear relationship between the inputs
10:54 - and the outputs
10:55 - by having the activation function at the
10:57 - end that allows you to encapsulate
11:00 - non-linearity in the thing you're trying
11:02 - to model
11:03 - so you can keep the core of your model
11:04 - simple
11:06 - and then allow for non-linearity by
11:08 - passing it through this activation
11:09 - function and that gives you the space to
11:11 - explore more interesting models
11:15 - yvette asks so the weights associated
11:17 - with the neurons not the inputs yes
11:21 - that's the easiest way to think about it
11:22 - think about each neuron having a bunch
11:24 - of pipes coming into it and on each of
11:26 - those pipes it's got a valve which it
11:28 - can use to multiply the input or divide
11:30 - the input by so each neuron has inputs
11:33 - coming into it and it has its own
11:36 - uh
11:37 - value that is being multiplied by to
11:39 - decide how that neuron is going to treat
11:41 - that particular input
11:48 - sharia asks can we use non-linear
11:49 - combination as well in in principle a
11:52 - neural network can do whatever you want
11:54 - there's no reason why a neural network
11:55 - has to have a simple function like that
11:57 - and in fact in reality there might be
11:59 - neural networks out there which have
12:00 - more complicated um
12:03 - summing functions inside there's no
12:04 - reason to think that it's necessarily
12:06 - this but at least
12:08 - conceptually it's the easiest way to
12:10 - think about it reality might be more
12:12 - messy but conceptually consider it as a
12:15 - sum over the inputs and the weights
12:17 - multiplied together
12:18 - reality might be more difficult but
12:21 - until you start having a full degree in
12:24 - neural networks and deep learning you
12:26 - won't have to worry about anything more
12:27 - complicated than that in order to get
12:28 - your job done
12:30 - at the end of the day here we want to
12:31 - learn enough to be able to use these
12:32 - things and understand how to fix them we
12:34 - don't necessarily for today at least
12:37 - need to understand the full mathematical
12:38 - description of how it all works
12:48 - okay so i'm going to carry on to the
12:49 - next slide now so as we go through do
12:51 - feel free to pipe up and ask any
12:53 - questions you might have even if it's on
12:55 - a slide that we've already been through
12:56 - that's fine i'm happy to reiterate over
12:58 - things because you might think oh yeah i
13:00 - understand this and then i mentioned
13:02 - something in two slides time and you
13:03 - realize i really didn't understand that
13:05 - i obviously misunderstood so do feel
13:08 - free to ask as we go
13:12 - so now we've got our description of a
13:14 - single neuron we're now going to see how
13:16 - we can connect them all together now
13:18 - again it is possible in principle to
13:20 - connect your network together however
13:22 - you want you could just turn it into a
13:23 - big net of things connected together in
13:25 - all sorts of directions
13:27 - but
13:28 - because we want our networks to be able
13:30 - to be understood by humans to some
13:31 - extent we want to be able to design them
13:33 - and understand what the different
13:34 - elements of it do
13:36 - there is a general structure which most
13:38 - neural networks follow
13:40 - now most simple neural networks anyway
13:42 - the complicated stuff comes from people
13:44 - breaking these rules and discovering new
13:46 - and interesting ways of structuring net
13:48 - networks
13:49 - but certainly for example today our
13:50 - network's all going to be this kind of
13:52 - structure which i'm going to describe
13:54 - now
13:55 - and that is you start on the left hand
13:56 - side of this picture on the input layer
14:00 - and this is where you take your values
14:02 - that you want to put into your model
14:04 - so uh an example that we used in the
14:06 - course yesterday was looking at the
14:08 - price of a house so you might go out and
14:11 - measure how many bedrooms a house has
14:13 - how large its garden is and how close it
14:15 - is to the nearest school for example so
14:17 - those are three quantitative
14:19 - measurements that you can make so those
14:21 - would be the values that you put into
14:23 - each of these three neurons on the left
14:25 - hand side
14:26 - now note that these three new ones on
14:27 - the left-hand side kind of don't have
14:30 - inputs that's because that's where the
14:31 - data is going to start from so the
14:33 - left-hand side is where we put our data
14:35 - in
14:36 - now each of these neurons on the
14:37 - left-hand side is connected to every one
14:40 - of the neurons in the next layer
14:42 - that doesn't necessarily have to be the
14:43 - case
14:44 - a new a neural network where everything
14:46 - is connected to every other one is
14:47 - called fully connected
14:49 - but it's perfectly reasonable to have
14:51 - some neurons on the first layer
14:53 - connected to only some on the next in
14:55 - whatever combination you see fit
14:58 - but you can see looking at this um
15:01 - second layer here the first one that's
15:02 - got two neurons on it it has each of
15:05 - them have three inputs coming into it
15:07 - and they have an output coming out
15:09 - now looking at the output coming out of
15:11 - the top neuron here you see it's
15:13 - connected to two neurons in the third
15:15 - layer
15:16 - that's not to say that this neuron has
15:18 - two outputs it's simply that that one
15:20 - single output is being passed on to both
15:23 - of the neurons in the following layer
15:26 - and each of those neurons in the
15:27 - following layer will have their own
15:28 - weight that's being applied to the
15:30 - output of the first mural on here
15:34 - this process just gets applied all the
15:36 - way through you work out the first layer
15:38 - you work out the sums and the activation
15:39 - function you pass into the second layer
15:41 - you add up the sums use the activation
15:43 - function you pass it through to the
15:44 - third until eventually you get through
15:46 - to the output layer when you get to the
15:48 - output layer this is your result this
15:51 - will give you some kind of number
15:53 - because at the end of the day we're
15:54 - doing maths it's all going to be numbers
15:55 - we'll end up with a number at the end
15:57 - and it's up to us to then interpret what
15:59 - this number means in the context of the
16:01 - question we're trying to ask
16:04 - but overall this is the structure that
16:05 - they all have they have an input layer
16:06 - on the left hand side i say left hand
16:08 - side it's all virtual but i think of it
16:10 - going left to right
16:11 - if you find it easier to think right to
16:13 - left that's perfectly fine
16:14 - input layer on the left hand side
16:16 - a single output layer on the right hand
16:18 - side and then some number of hidden
16:21 - layers in between
16:22 - in this example here we have two hidden
16:24 - layers each of which have two neurons
16:27 - in principle you can have any number of
16:29 - hidden layers you like with any number
16:31 - of neurons in each
16:34 - the thing that makes a deep neural
16:35 - network deep is having lots of hidden
16:38 - layers that's pretty much the definition
16:40 - of a deep neural network and we'll see
16:43 - later on why that's a useful thing to
16:45 - have
16:52 - great so i was asked if anyone's got any
16:53 - questions there's already one from sumet
16:55 - so how many layers and how many neurons
16:57 - should we have
16:58 - it depends on the problem you're trying
16:59 - to solve
17:00 - so
17:02 - i'll get on to how you go about
17:03 - designing your networks later so i'll go
17:05 - through this in a bit more detail but
17:07 - the number one rule i would suggest is
17:09 - find someone who knows what they're
17:10 - doing who has solved a similar problem
17:12 - to what you're trying to do and use the
17:14 - same structure as they have
17:16 - that is going to get you
17:18 - 90 percent of the way after that you can
17:21 - feel free to tweak it but by using
17:22 - pre-existing pre-existing and published
17:25 - network structures that's going to help
17:27 - you
17:30 - shuya asks how many layers is considered
17:32 - deep
17:33 - it's that's one of those questions where
17:34 - there's no real answer i would say as
17:36 - soon as you've got
17:38 - four or five you're getting to the point
17:40 - where networks would have struggled with
17:42 - in the past once you're at 100 layers
17:44 - you're definitely in the situation where
17:46 - you've got a deep neural network
17:48 - going much beyond that you're going to
17:49 - have to start bringing real firepower to
17:51 - be able to evaluate anything to do with
17:53 - the network
17:54 - but i would say rule of thumb you can
17:56 - start asking answering those really
17:58 - interesting deep learning questions
18:00 - by the time you get uh
18:02 - sort of six seven eight nine ten and
18:04 - beyond
18:06 - though as with all things the benchmark
18:08 - kind of shifts over time what might have
18:10 - been considered a deep neural network
18:12 - ten years ago might not be considered
18:14 - quite so deep these days
18:22 - and tim asked what's the precise
18:23 - definition of a layer here
18:26 - so
18:27 - there's no one universal definition of a
18:29 - layer which can capture every single
18:32 - different situation
18:33 - but under the structure of a network we
18:35 - have here
18:36 - where we take all of our neurons and we
18:38 - put them into boxes where they are lined
18:40 - up on top of each other
18:42 - such that in one layer in the in one box
18:46 - on the left hand side here they are only
18:47 - connected to neurons in the next layer
18:50 - and in this next box here these are only
18:52 - connected to neurons in the next box
18:54 - that allows you to have a
18:57 - an absolute definition of the ordering
18:59 - of the neurons which lets you decide
19:02 - absolutely
19:04 - what layer each neuron is in
19:06 - and therefore how many layers you have
19:08 - of course because these are just
19:10 - networks and you can in principle do
19:11 - whatever you want you could take the
19:13 - output of the third layer and connect it
19:15 - back to the input layer if you wanted
19:16 - and do something
19:18 - with that at which point it's harder to
19:20 - define what's a layer
19:22 - however you can do a lot of really
19:24 - interesting stuff without having to
19:26 - cycle through
19:28 - so the networks we're working today are
19:30 - what are called a cyclic ie they don't
19:32 - have cycles they are simply feed forward
19:35 - from the left hand side over to the
19:36 - right
19:42 - exactly tim that's a good way of putting
19:44 - it
19:45 - it's only it's at the neurons on layer n
19:47 - only have inputs from layer n minus one
19:52 - knitting asks more layers lead to
19:53 - overfitting
19:54 - absolutely we're going to cover one of
19:56 - the main techniques well actually two
19:58 - techniques which you can use to reduce
20:00 - the chances of overfitting as we go
20:02 - through
20:04 - okay i'm going to carry on
20:07 - so
20:08 - these are some of the questions you've
20:09 - been asking so it's good you've been um
20:12 - you've been uh asking intelligent
20:14 - questions before i've even got to it
20:18 - so
20:18 - a question you've been asking is what
20:19 - shape should it be so there is some art
20:21 - and some science to this there are there
20:22 - are rules of thumb
20:24 - if you've got a certain number of input
20:26 - layers if you've got 10 input parameters
20:29 - then your hidden layers should
20:31 - to an order of magnitude be about 10.
20:34 - there's no point in having an input with
20:36 - 10 inputs and then the first hidden
20:38 - layer being 100 million different
20:41 - neurons that's not going to extract any
20:43 - useful information
20:44 - if you've got 10 inputs you're not going
20:46 - to want more than 100 in the next layer
20:48 - realistically anyway
20:52 - this is one of the things which you can
20:53 - assess using mathematical techniques
20:55 - which we're not going to cover in this
20:56 - course because they're very complicated
20:57 - and also a certain amount of trial and
20:59 - error and data science applied to the
21:01 - shape of the network
21:02 - also as i think someone mentioned the
21:04 - chat the number of hidden layers kind of
21:06 - relates to how complicated a question
21:08 - you can answer
21:10 - every hidden layer provides a layer of
21:12 - abstraction
21:14 - so if you're trying to start from some
21:16 - very um generalized information and to
21:18 - pull out a really specific answer you're
21:20 - going to need lots and lots of hidden
21:22 - layers to work through the layers of
21:24 - questions that the network is implicitly
21:27 - answering
21:28 - so if you've got a hard question you're
21:30 - gonna need a deeper network if you've
21:32 - got an easy question you can get away
21:33 - with one hidden layer for simple things
21:35 - it doesn't need to be a complicated
21:37 - situation
21:39 - the flip side of this is the deeper the
21:41 - network and the larger the network the
21:43 - harder it is to train
21:45 - that means it's going to take longer to
21:47 - train you're going to be more risk from
21:49 - overfitting and we're going to see how
21:50 - this sort of balance comes through as we
21:52 - go through the course today
21:56 - the next question that you might have
21:58 - had sitting in the back of your head but
21:59 - you haven't worked out how to formalize
22:01 - it yet is i've been talking about the
22:03 - neurons and the inputs coming in and how
22:04 - they're going to be multiplied by some
22:06 - kind of weight
22:08 - and the question you hopefully have is
22:10 - how do you know what that weight should
22:12 - be
22:12 - because at its core there are two things
22:14 - which describe
22:16 - how the network is going to answer your
22:18 - question
22:19 - the first is the structure of it which
22:22 - neurons are connected to which how many
22:24 - layers how many neurons these kinds of
22:26 - things
22:26 - and the second part of the problem is
22:29 - the weights that are being applied on
22:31 - each individual neurons input
22:35 - so the first part we decide and we
22:36 - design upfront we describe a network we
22:39 - say this is the thing that's going to
22:40 - solve our problem
22:42 - but for the weights we have no way as a
22:44 - human of working out what these millions
22:45 - of different values should be and so
22:47 - this is where we use computers this is
22:49 - what makes it a machine learning
22:51 - algorithm because we are going to use a
22:53 - mathematical process
22:54 - on the computer to discover what the
22:56 - values of those weights should be
22:58 - for the particular problem that we're
23:00 - trying to solve
23:01 - this process is called training
23:03 - and it effectively works by showing it
23:05 - loads of examples
23:06 - and it ends up giving you something
23:08 - which can replicate the effect of those
23:10 - examples
23:16 - so question there from ali how can we
23:17 - decide how many layers we need
23:20 - again i would say that the number one
23:22 - way you should discover how many layers
23:25 - you need and the shape of your network
23:26 - is to think about the problem you're
23:27 - solving find someone else who solved a
23:30 - similar problem and start from their
23:32 - example
23:34 - if you're doing something where you're
23:35 - trying to identify
23:37 - um
23:38 - heart disease in uh ct scans of a heart
23:42 - tissue
23:43 - then you might start from someone else
23:44 - who's done the same thing with
23:46 - liver disease or something like that so
23:48 - it's something working with 3d data
23:50 - you're looking for certain features in
23:52 - the data you don't have to start from
23:54 - scratch design your network you start
23:55 - from someone else who solved a similar
23:56 - problem and has analyzed how it works
23:59 - and you start by using those if you can
24:01 - show that it performs well
24:03 - then you've succeeded and you can use
24:04 - that to do the things you want to do
24:06 - if it doesn't perform well then at that
24:08 - point you have to start talking to data
24:10 - scientists and experts and trying to
24:12 - work out how to go from there
24:17 - so to dive into that question there how
24:19 - do we go about training the neural
24:21 - networks what is the process by which
24:23 - the values of the weights get assigned
24:26 - now again there's lots of potential
24:28 - algorithms you could use here there's
24:30 - lots of different ways that you can
24:32 - decide to
24:34 - use a computer to work out what those
24:36 - weights should be
24:37 - but the main technique that's used is
24:39 - something called back propagation
24:41 - now again back propagation is a
24:43 - technique that was
24:44 - applied to neural networks quite a long
24:46 - time ago and in the preceding times in
24:48 - the time since there's been lots and
24:50 - lots of nuances and changes to it and
24:53 - replacement techniques but that
24:55 - propagation at its core as to how it
24:57 - works in principle
24:58 - is still pretty much how neural networks
25:00 - are trained even if there's lots and
25:02 - lots of clever hacks applied on top of
25:03 - it
25:05 - and so to do this training technique you
25:07 - need
25:08 - three different things pretty much
25:10 - you only need two of them but the third
25:12 - one isn't going to do very good data
25:14 - science without it
25:15 - so first of all you need a training data
25:18 - set you need a bunch of examples where
25:20 - you've got the inputs to it
25:22 - using the examples from before a bunch
25:24 - of houses where you've measured the size
25:26 - of the garden counter the number of
25:27 - bedrooms and measure the distance to the
25:29 - nearest school
25:30 - and for each of those examples you have
25:32 - a label assigned to it you've said how
25:35 - much is this house worth
25:37 - so that is what we mean when we say a
25:39 - labeled training data set
25:44 - for checking how well our network is
25:46 - performing we also need a labeled test
25:49 - or evaluation data set
25:51 - now it's important that your training
25:52 - data set and your test data set are
25:55 - distinct and disjoint
25:57 - to make sure you avoid overfitting now
25:59 - we're going to cover exactly how
26:00 - overfitting feeds in a little bit but we
26:03 - did cover it in our course yesterday as
26:05 - well
26:06 - but you want to make sure you have two
26:07 - distinct subsets of training data they
26:10 - can look the same but you should choose
26:12 - randomly about 80 percent
26:14 - of your data to be used for training and
26:16 - then keep aside about 20 to check how
26:19 - well your network is performing
26:22 - and for the back propagation algorithm
26:23 - to work you need to start off with some
26:25 - set of initial weights
26:27 - so now i'm going to go through and show
26:28 - you how these different things can be
26:31 - designed collected and evaluated
26:40 - before i do this two questions so
26:42 - prataps asking
26:43 - that the same output can be achieved by
26:45 - multiple combinations of inputs across
26:47 - layers
26:48 - how is precise connectivity and weights
26:50 - decided so you're right you can have
26:53 - repeated information existing inside
26:55 - network you might have two parts of your
26:57 - network which are deciding the same
26:58 - piece of information in which case
27:00 - you've got more neurons and weights than
27:03 - you really need
27:04 - there are techniques you can use to try
27:06 - and find correlations between weights as
27:08 - they progress
27:09 - and that gives you some sense as if you
27:10 - might be able to prune your network or
27:12 - reduce your network down
27:14 - if it's affecting the ability of your
27:18 - results to be accurate then you're going
27:19 - to want to do that pruning if it's not
27:21 - then you don't so the precise
27:23 - connectivity
27:24 - is
27:25 - hard to do connectivity is generally
27:27 - done on a course layer it's called
27:29 - course level although it's of course
27:31 - possible to apply specific techniques to
27:33 - prune parts of it
27:34 - as for weights we're going to show you
27:36 - now how this particular weights are
27:38 - decided and it does feed into that
27:39 - question you were just asking
27:42 - and yan asks is training always
27:43 - mandatory with neural networks
27:46 - realistically yes if you have an
27:49 - off-the-shelf neural network which has
27:50 - been trained to identify the difference
27:52 - between cats and dogs you don't have to
27:54 - do any more training on it you can just
27:56 - use it
27:57 - but if you start with the empty
27:58 - structure you need to do something to
28:00 - work out what the values of the weights
28:02 - should be
28:03 - and to work out what the weight should
28:05 - be you have to train the network
28:09 - so starting with the initial weights
28:12 - lots of different opinions and
28:13 - techniques as you can use to start off
28:15 - your network with some kind of values
28:17 - but the easiest way to do and the
28:19 - conceptually the thing that we're going
28:21 - to do is setting all of your weights
28:23 - randomly
28:24 - there are ways to be smarter about it
28:26 - but
28:27 - for today let's just assume that they're
28:28 - all being set completely randomly so
28:30 - every neuron is having inputs coming
28:33 - into it
28:34 - and it's multiplying each of those
28:35 - numbers initially by a random number
28:38 - and we're going to train it to try and
28:40 - fine-tune those random numbers in the
28:41 - direction of something which is useful
28:46 - and then for the test and training test
28:49 - data sets so we're going to need two so
28:50 - we need one as i said to train the
28:52 - network on so we can learn about stuff
28:54 - and the other one is going to tell us
28:56 - how well it did
28:58 - it's important to separate your training
29:00 - and test data sets to avoid overfitting
29:02 - and to give a bit more information about
29:05 - why that's the case
29:06 - it's because on your network inside it
29:08 - has got
29:09 - tens hundreds millions potentially of
29:12 - different weights
29:13 - and if your data has
29:16 - fewer
29:17 - degrees of freedom than that
29:19 - then your network is able to learn
29:21 - potentially every single piece of
29:22 - information about your data that you're
29:25 - showing it when it's learning it'll be
29:27 - possible for every single newer one to
29:29 - each take their turn and remember one
29:31 - particular input into the network
29:33 - and so as you're going through it's
29:34 - going to start looking like your network
29:36 - is able to perfectly replicate the
29:38 - training data that you're showing it and
29:39 - that's because it's remembered every
29:41 - single little nuance up and down of the
29:43 - data you've been sharing it
29:45 - by keeping some of that data aside for
29:47 - training for testing at the end
29:50 - even if your network has learned every
29:51 - single little detail about your train
29:54 - data set
29:55 - it's not going to have learned any of
29:56 - the nuances of your testing data set
30:00 - your testing data set is therefore not
30:02 - going to have those same ups and downs
30:04 - and so when you show it to the network
30:06 - it's going to behave very very poorly
30:08 - it's not going to do a good job
30:10 - and so the dance you need to get is the
30:12 - balance between getting your training
30:15 - data set behaving well
30:16 - while also keeping your testing data set
30:19 - still also performing well
30:24 - the common split you generally see is
30:26 - about 80 for training and uh 20 for test
30:29 - but depending on how much data you have
30:31 - you can jiggle those numbers around and
30:33 - there are also more advanced techniques
30:35 - things like k-folds where you end up
30:37 - using all of your data for training but
30:39 - you split it into different subsets and
30:40 - you repeat it over and over again and
30:42 - then you aggregate it at the end in a
30:44 - clever mathematical way
30:47 - um says by disjoint do you mean
30:49 - statistically
30:50 - what i mean is you want of all the
30:53 - samples you've collected
30:55 - when you
30:56 - divide your set into two halves you
30:57 - don't want any overlap between the two
31:00 - they should be sampled from the same
31:01 - distribution is one way of thinking
31:03 - about it but they should be separate
31:05 - samples from that same distribution
31:08 - trey asks is it ideal to set weights
31:10 - randomly if there's repetition will it
31:11 - not be wasteful
31:13 - possibly yes you random setting your
31:15 - initial weight isn't necessarily the
31:17 - thing to do there are clever ways you
31:18 - can work out what your initial weight
31:20 - should be set to
31:21 - but as far as us as researchers using
31:24 - neural networks to solve problems we're
31:26 - going to leave that to the software to
31:27 - decide and they're going to do a better
31:29 - job than we would be able to decide
31:32 - and a vet suggests that you need a
31:33 - massive data set to do useful stuff
31:36 - you generally do you do need a lot of
31:38 - data to train in your network
31:40 - however if you're answering a simpler
31:42 - question and therefore you've got fewer
31:43 - neurons and fewer weights you can get
31:46 - away with less data we'll see an example
31:48 - in a minute where we only have around
31:50 - 150 examples and that's enough on that
31:53 - particular example to answer the
31:55 - question well
31:58 - on later examples you might need
32:00 - i i i generally think a thousand
32:02 - examples is a good kind of rule of thumb
32:03 - for neural network for most problems but
32:05 - having more than that is always going to
32:07 - help
32:08 - one of the problems with data science
32:10 - and your networks is having that data
32:12 - that's nice and clean and well prepared
32:18 - so i'm going to talk briefly about the
32:20 - matsy bit and this is probably one of
32:22 - the last bits of math on the screen so
32:25 - i'm not going to be using any maths
32:27 - that's beyond
32:28 - a level but if anyone wants a bit of
32:30 - clarification i'm happy to explain it in
32:32 - more detail
32:33 - but back propagation is the process by
32:36 - which we assign weights to the network
32:38 - it's an algorithm which we use to decide
32:40 - what those weights should be
32:42 - it's an iterative algorithm so we're
32:43 - going to apply this over and over again
32:45 - and over that time the weights are going
32:47 - to get closer and closer to that which
32:49 - the algorithm considers to be optimal in
32:52 - some way
32:53 - but before you can apply the algorithm
32:55 - you need to start off with your network
32:56 - structure so we've decided how many
32:58 - inputs we've got and outputs
33:00 - how many hidden layers and how many va
33:02 - neurons we have in each hidden layer
33:05 - we need to have some initial weights we
33:07 - need to have somewhere to start our
33:09 - iterative algorithm from
33:12 - and you need to have a training data set
33:14 - you need to have some examples you can
33:15 - show to it and those examples are going
33:17 - to inform how the weights progress as
33:20 - the algorithm is applied
33:22 - now there's lots and lots of different
33:24 - ways you can do the training but the
33:26 - core of how it's generally done these
33:28 - days is using backpropagation and this
33:30 - is a very simplified description of how
33:32 - backpropagation works
33:35 - so
33:36 - you need to start off by looking at the
33:39 - structure of your network looking at the
33:41 - inputs and at the outputs
33:43 - i need to look at every single weight in
33:46 - your network so every single input to a
33:48 - neuron is going to have its own
33:49 - individual weight you're going to have
33:51 - maybe thousands of potential weights
33:53 - and you need to work out how much do you
33:56 - need to change each weight by to affect
33:58 - the output of the whole network by a
34:01 - certain amount
34:02 - if i change a particular weight by a
34:04 - small amount
34:05 - is the output of the network going to
34:06 - only change by a small amount or is it
34:08 - going to change by a relatively large
34:10 - amount
34:11 - if a weight has a disproportionately
34:13 - large effect on the output of a network
34:15 - then the derivative of that weight is a
34:17 - large number
34:19 - if that weight only has a small effect
34:21 - on the eventual output of the network
34:23 - then it's said to have a small
34:24 - derivative and it's only going to have a
34:26 - smaller effect
34:29 - so
34:30 - working out this derivative
34:31 - independently for every single weight
34:33 - and assuming they are independent which
34:35 - largely works in the situations that
34:37 - we're talking about here
34:38 - this gives you a list of derivatives one
34:41 - for every single weight in your whole
34:42 - network every weight will have its own
34:45 - dn its own derivative
34:49 - so this is telling us how much you need
34:50 - to change that weight by to have a
34:52 - certain effect on the output
34:55 - note also that these derivatives are
34:57 - signed so it could be that if you make
34:59 - your weight slightly larger the output
35:01 - goes down
35:02 - and that would be a negative derivative
35:04 - for example so all this information is
35:06 - captured in a big list of derivatives
35:08 - that's stored alongside the network
35:12 - so that's the first step that happens
35:14 - once up front before you start doing any
35:16 - training because
35:17 - the networks we're looking at those
35:18 - derivatives don't change over time
35:22 - what you then do is start the training
35:24 - process and this is where you repeatedly
35:26 - do these steps over and over again
35:29 - and the core of how this works is
35:31 - you take a training entry which has for
35:34 - example three values the number of rooms
35:36 - the size of the garden and the distance
35:38 - near a school three numbers you put it
35:40 - in the left hand side of your network
35:43 - you then work out what the output to
35:45 - that layer is going to be
35:46 - pass it into the next layer that layer
35:48 - is then going to take if you take each
35:50 - of the inputs
35:51 - it's going to multiply them by its
35:53 - initially random weight
35:56 - it's going to do that all the way
35:57 - through right on the other side so the
35:58 - weights are initially set randomly and
36:00 - you're going to get out the far side a
36:02 - random number effectively you're going
36:04 - to get some kind of
36:05 - result it's going to be a numerical
36:06 - value between minus infinity and
36:08 - infinity but you're going to get a
36:09 - number out
36:11 - what you then need to do is look at how
36:13 - far away that random number you got is
36:16 - from the true number
36:18 - maybe the true value of the house was
36:20 - two hundred thousand pounds your network
36:22 - gave you a random number of
36:24 - zero 000 pounds you know you're there
36:26 - for 200 000 away
36:29 - so you then need to look at all of the
36:31 - weights and adjust them by how much they
36:33 - need to change
36:34 - to make the nut the result you've got
36:36 - out that network move in the direction
36:38 - of the correct answer
36:41 - now you don't move it all the way in one
36:42 - go you don't jump the result up by two
36:44 - hundred thousand you move it by a very
36:46 - small amount so that you can slowly move
36:48 - in the direction of something which is
36:50 - going to get you the right answer
36:53 - so after this you go through all of your
36:54 - weights and you update each of them by a
36:56 - small amount by looking at what their
36:58 - derivative is with respect to what the
37:00 - how wrong you were was
37:02 - scale that down a little bit to try and
37:04 - slow things down
37:05 - and then you do the same thing again you
37:07 - show it another example
37:09 - and this means that the more wrong the
37:10 - weights are the more they work towards
37:12 - the answer and as they get close they
37:14 - slow down and start propagating towards
37:16 - a settled result
37:18 - the idea being after this magic if you
37:21 - put in
37:22 - three values which represent a house it
37:24 - is going to give you back something
37:25 - which is going to make a good guess as
37:27 - to what the value of that house should
37:29 - be
37:31 - so i'm going to answer a few questions
37:33 - from chat before i move on
37:34 - nissan asks how's it going to stop and
37:36 - what's ideal you keep on running until
37:39 - your training data set is looking your
37:42 - testing data set sorry you keep on going
37:44 - until your test data set looks like it's
37:46 - giving you a good result
37:48 - at its core there's more nuance than
37:49 - that but that's the basic answer you
37:51 - keep going until your test data set your
37:54 - validation set is saying this is looking
37:56 - good this is answering the question
37:58 - quite well
38:01 - connor asks an insightful question in a
38:03 - network with a huge number of weights
38:04 - wouldn't analyzing them individually be
38:06 - an inefficient method of tweaking
38:08 - indeed so the way i'm describing it here
38:09 - is how you would do it if you were doing
38:11 - this on paper
38:12 - in reality
38:14 - uh modern techniques represent the
38:16 - network as a huge multi-dimensional
38:18 - tensor
38:19 - and you just do tensor arithmetic on it
38:21 - and it has the effect of doing this
38:23 - simplified maths
38:25 - and by doing this tensor arithmetic you
38:26 - get nice
38:28 - fast results because hardware can be
38:30 - designed to do tensor arithmetic very
38:32 - very quickly
38:34 - canal asks how should we determine the
38:36 - learning rate
38:37 - use the defaults of the software that
38:39 - you have in front of you they will
38:41 - generally have a default learning rate
38:42 - which works well
38:44 - if you find that it's behaving poorly as
38:46 - it's training you can tweak that
38:47 - learning rate
38:48 - and there are in fact algorithms and in
38:50 - fact we're going to be using one today
38:51 - where the learning rate adapts
38:53 - automatically as it goes through
38:57 - and roy white asks is this essentially
38:59 - manual optimization
39:01 - in a way yes but we're doing it using a
39:03 - computer and so it can do it a lot
39:05 - faster than we can you can imagine
39:06 - looking at each individual thing
39:08 - tweaking up and down seeing how it
39:09 - affects the answer tweaking the next one
39:11 - and so on and so on we can do it all in
39:13 - one go using the network and it becomes
39:16 - scalable
39:18 - and michael asks does this always
39:19 - converge no it doesn't there's no reason
39:22 - to assume it's always going to converge
39:23 - it's an extremely complicated
39:24 - mathematical object and even if it does
39:26 - converge it won't necessarily converge
39:28 - where you want it to
39:31 - at its core most neural network training
39:34 - methods are doing a form of gradient
39:37 - descent
39:38 - and you can imagine that in
39:39 - two-dimensional way on a big plane
39:41 - covered in hills and valleys and you're
39:44 - trying to find the lowest point in that
39:46 - plane so you start by rolling a ball
39:47 - down a hill it's very possible in that
39:49 - situation to end up in a valley that's
39:51 - near where you started that is not as
39:53 - deep as a valley which is further away
39:56 - so that you've ended up in a local
39:57 - minimum
39:58 - and you haven't ended up in the true
40:00 - result
40:01 - so there are techniques you try and use
40:02 - using randomness and adaptive learning
40:04 - rates to try and avoid getting stuck in
40:06 - a local minimum
40:12 - so to reiterate what we were just
40:13 - talking about there
40:15 - here is a much
40:16 - uh cuter example than a house here is a
40:19 - lovely picture of a dog
40:21 - imagine you can extract some properties
40:22 - of this photograph of the dog maybe we
40:25 - add up all the pixels and look at their
40:26 - red green and blue values and so we end
40:28 - up with
40:30 - 20 000 input neurons for example one for
40:32 - each pixel however you want to describe
40:34 - the picture
40:36 - we start with a random network we put
40:38 - all the numbers going through so
40:41 - all the pixels end up in this input
40:42 - layer on the left hand side the three
40:44 - new ones
40:45 - we then pass it through to the next
40:46 - layer and the next layer the next layer
40:47 - of course we need a more complicated
40:49 - network than this but we would still get
40:51 - an answer with this network
40:53 - we run this example through and we get
40:55 - some kind of answer
40:57 - it's randomly weighted to start with and
40:58 - so it says something like
41:00 - 37 or 0.37 which we decide means is 30
41:04 - set 37 dog and 63 cat
41:08 - we know that the truth in this example
41:10 - because all of our examples are labeled
41:12 - is that it is 100 dog and a zero percent
41:15 - cat
41:16 - and that means that we are 100 minus
41:18 - thirty seven percent
41:20 - are the
41:21 - 0.63 percent
41:23 - wrong
41:24 - and so we use that 0.63
41:26 - to work out how much we should change
41:29 - all of our weights by it's a positive
41:31 - number so we should generally multiply
41:33 - all of our weights in that equation by a
41:35 - positive number and that's going to
41:37 - slowly tweak our network towards an
41:40 - answer
41:40 - we're going to do this thousands of
41:42 - times each time taking a really really
41:44 - small step and that's hopefully going to
41:46 - over time push the network into
41:48 - configuration where it's able to answer
41:50 - questions similar to that one that we
41:53 - started with
41:55 - uh alastair asks are there ways of
41:56 - assessing the rate of conversion as a
41:58 - proxy of the confidence that the
41:59 - conversion is generally
42:01 - a general rather than a local optima
42:04 - in principle you can but it could just
42:06 - be that you happen to be in a
42:08 - flat bit of the terrain at that
42:10 - particular time you could start off in
42:12 - hill country out in the distance then
42:14 - have to go through a large flat area
42:15 - before then ending up in a really
42:17 - valley-ish area in the middle of the
42:19 - country so you can't necessarily assume
42:21 - that because it's converging slowly that
42:23 - you've ended up in the right place
42:25 - you have to use clever techniques you
42:27 - have to be careful to
42:30 - not end up assuming that you've ended
42:32 - you've solved the problem before you
42:33 - have
42:36 - and as lester says there you also start
42:37 - off in lots and lots of different
42:38 - initial places and you see if any of
42:41 - those are going to push you towards an
42:42 - answer
42:44 - i've been talking through so far
42:46 - effectively how you would do this by
42:48 - hand you can imagine that you could sit
42:50 - down with a bit of paper
42:52 - lovely picture of your man good job um
42:54 - you imagine you could sit down with a
42:55 - piece of paper draw your network out
42:57 - work out what the derivative of all the
42:58 - weights are
43:00 - and
43:00 - look at the numbers of the inputs do the
43:02 - maths do the inversion and keep going
43:04 - and keep going as i said the way this
43:06 - actually works is by
43:10 - doing this using very very large tensors
43:12 - using gpu accelerated hardware all this
43:14 - kind of thing
43:15 - because the scale to which you can get
43:17 - is not is not coverable obviously if
43:20 - you're doing it by hand you have to use
43:21 - computers which is why it's a classic
43:23 - machine learning algorithm
43:26 - again we also don't write the software
43:28 - by hand to describe the network and to
43:31 - do the back propagation and to do all
43:33 - these things we use software that exists
43:35 - already to do that work for us we're not
43:38 - going to spend five years trying to
43:40 - compete with google or facebook with
43:42 - their techniques we're gonna use
43:44 - software they provided which is shown by
43:46 - the
43:46 - whole industry to work really well
43:49 - now there are lots and lots of different
43:50 - pieces of software out there which can
43:53 - provide a way of describing a network of
43:55 - training a network and evaluating a
43:57 - network
43:58 - and some of the most popular ones are
44:00 - well the two most popular are pi torch
44:02 - and tensorflow so pytorch i believe was
44:04 - originally produced by facebook and
44:06 - tensorflow was produced by google they
44:08 - are both fantastic neural network
44:10 - libraries which do all the bells and
44:12 - whistles you'll need for any kind of
44:13 - complicated network
44:15 - in the course today we're going to be
44:16 - using tensorflow but um pytorch works
44:20 - brilliantly well so i'm not going to say
44:21 - anything bad against fighters
44:23 - there's also a package called karas
44:26 - which isn't itself containing any
44:28 - algorithms for doing the training it is
44:30 - a wrapper on top of the other packages
44:32 - to give a nicer way of describing the
44:34 - problems you're trying to solve it's a
44:36 - bunch of niceties and add-ons basically
44:38 - and so what we're going to be using
44:39 - today is keras's nice add-on layer on
44:43 - top of tensorflow
44:45 - there's also a package called cafe 2
44:46 - which isn't getting quite as much
44:47 - publicity these days but
44:49 - it and its derivatives are still thought
44:51 - of very well and they do a good job
44:54 - and finally psychic learn so yesterday
44:57 - in the applied data analysis course we
44:58 - use psychic learn for doing other
45:00 - machine learning algorithms
45:02 - it does have a neural network a whole
45:04 - set of neural network packages and
45:06 - modules built in
45:07 - but it's not going to be anywhere near
45:09 - as performant as things like pytorch or
45:11 - tensorflow it's not going to perform
45:12 - very well for your very very deep neural
45:14 - networks
45:15 - and it's not going to take advantage of
45:17 - the gpus in the way the other packages
45:19 - would
45:20 - but for exploring and playing around and
45:21 - getting started psychic learn works
45:23 - perfectly well and you get the advantage
45:25 - of being able to easily compare to other
45:27 - machine learning libraries
45:29 - so i said we're going to be using
45:30 - tensorflow and we're gonna be using a
45:32 - bit of a chaos layer on top of it
45:35 - so let's get on and actually see how we
45:39 - can use some code to describe train and
45:42 - evaluate on your networks with a a real
45:44 - example
45:46 - so the example we're going to use is a
45:48 - famous one if you've ever done a machine
45:50 - learning course before and by in this i
45:52 - include our course we gave yesterday you
45:54 - will have seen the ios example
45:57 - so this is a data set that was collected
45:59 - quite a way back
46:01 - and it is
46:02 - been used for decades for giving machine
46:05 - learning training
46:07 - and the data set at its core is
46:08 - information collected about three
46:10 - different species of iris flower
46:13 - so the three pictures here are three
46:14 - different species
46:16 - and we are going to try and design a
46:18 - network which based on measurements made
46:21 - of these flowers not made not based on
46:23 - photos but based on measurements made of
46:25 - the flowers we are going to try and
46:26 - decide which species the flower belongs
46:29 - to
46:31 - so these three are iris setoza i was
46:34 - verticolor and i was virginica that is
46:36 - the extent about which i
46:39 - know anything about these flowers
46:42 - however i do know about the data set
46:44 - that describes the flowers so there are
46:46 - 150 examples in the dataset and the link
46:49 - there i think takes you to the wikipedia
46:52 - page
46:53 - let's have a look yes it's famous enough
46:55 - that it's got a wikipedia page so if
46:56 - you're interested do have a look at that
46:58 - page later on and learn about the
47:00 - nuances of this
47:02 - data set
47:05 - so each flower that was measured by a
47:07 - particular person back in the day
47:09 - they went and measured four different
47:11 - properties of each flower they took a
47:12 - ruler
47:13 - they wrote down their notebook in a
47:15 - table and they measured these four
47:18 - things that we see in the table in front
47:19 - of us the length and the width of the
47:21 - sepal and the length and the width of
47:23 - the main petal
47:25 - and their idea was that based on just
47:27 - those four measurements they should be
47:28 - able to distinguish
47:30 - which species the flower belongs to
47:32 - and remember that we need to have this
47:34 - being a
47:35 - labeled data set we need to know what
47:37 - the truth is so that we can nudge our
47:39 - network in the right direction as it's
47:41 - training and so we also record the
47:43 - species so species 2 1 2 0 etc
47:48 - remember that we're dealing with maths
47:50 - here all of neural networks are maths
47:52 - and so anything that we come up with
47:53 - that describes something human or
47:56 - physical we have to convert that into
47:58 - something numerical in some way
48:00 - and so in this data set here we are
48:02 - using the number zero to represent the
48:04 - setoza one to represent the vertical and
48:07 - two to represent the virginica
48:11 - and so you can imagine that we want to
48:14 - try and train our network so that if we
48:15 - show it a set of sequels and petrol
48:17 - measurements which represent acetosa we
48:20 - want the network to output a number
48:22 - which is near to zero
48:24 - so now we've seen what the data set
48:26 - looks like we now think about how we can
48:28 - design a network which can take these
48:31 - first four columns
48:32 - now the inputs to a network
48:34 - are often referred to as the features
48:36 - we want to take something which can take
48:38 - these four features and give us out our
48:40 - label
48:42 - and do that consistently even for
48:44 - examples that it's never seen before
48:51 - okay
48:52 - so to see a bit more visual idea of how
48:55 - this works this picture was taken from
48:56 - the wikipedia page
48:59 - we have here
49:00 - a multi-scatter plot of the four
49:03 - different features
49:04 - so
49:05 - along the
49:07 - uh along the rows we have the length
49:08 - width of the sepals and then the length
49:10 - and width of the petals and likewise
49:12 - down the columns we have the length and
49:13 - width of the sequels and the length and
49:15 - the width of the petals and so in each
49:17 - off diagonal we have a scatter plot of
49:20 - one of those features against the other
49:23 - so in the second row in the first column
49:25 - we have here a scatter plot of sepal
49:27 - length against sepal width
49:29 - and we have each of the dots in that
49:31 - colored by what the true label of that
49:34 - particular flower is and so you see here
49:36 - there is a fairly distinct cluster of
49:38 - the red which is the setoza whereas the
49:40 - blue and the green are relatively
49:42 - intermingled with each other so based
49:44 - purely on this scatter plot you would
49:46 - probably be able to make a good guess
49:47 - about whether a flower is a setoza or
49:49 - not but you wouldn't be able to easily
49:51 - distinguish between a verticolor and a
49:53 - virginica
49:55 - and if we go back to slides
49:57 - you'll see that you can probably guess
49:58 - which one's which the satosa is this one
50:00 - on the left and the other two flowers do
50:02 - look very similar to each other to my
50:04 - untrained eye i would struggle to tell
50:06 - those two apart
50:10 - however if we look in other projections
50:12 - some of them do have better separations
50:14 - but there's no one projection which has
50:17 - perfect separation between the data sets
50:19 - they all have at least some overlap
50:21 - between the blue and the green and i do
50:22 - apologize if you're colorblind
50:24 - um we i'll try and do a better plot for
50:27 - next time with better color blind
50:29 - friendly colors
50:31 - the idea however here is that even
50:33 - though no
50:34 - one single projection can distinguish
50:36 - the two we're hoping that by combining
50:38 - together all six projections because
50:41 - those are the four features um combined
50:44 - together all six of those different
50:45 - projections we can come up with some
50:46 - kind of complex multi-dimensional
50:49 - description of what's going on
50:52 - in a way you can imagine that the
50:55 - network is going to divide these things
50:57 - with kind of dividing areas in the
50:59 - different planes with certain
51:00 - probabilities and then some to get those
51:02 - probabilities at the end and give us an
51:04 - estimate of which flower species each
51:07 - example is
51:09 - that's not exactly how a neural network
51:11 - works um there are other machine
51:12 - learning techniques which effectively do
51:14 - that and they would also behave quite
51:16 - well on this
51:17 - this isn't the kind of problem that you
51:19 - need a neural network to solve this is
51:22 - something which can be solved with other
51:23 - relatively simple techniques but this is
51:25 - a good place to start when learning
51:27 - about how we can design a network and
51:30 - train it and go through that mechanical
51:32 - process
51:35 - so the code for this is all linked at
51:37 - this link here so on this page in the
51:39 - notes click on iris dot i pi nb
51:43 - i'll click on it as well to show you
51:44 - what it looks like
51:45 - it will take you to a page on google
51:47 - collab
51:48 - which looks like this
51:50 - i'll show you how to run through it in a
51:52 - bit so if you want to follow along and
51:53 - keep track of where i am feel free to
51:55 - have this page open in another tab but
51:57 - i'm going to have all the code samples
51:59 - on the slides and i'll talk through it
52:01 - bit by bit to explain what the different
52:03 - parts of the code are being used for
52:08 - so the code we're using today is using
52:11 - tensorflow and keras and it's being
52:12 - written in python but all the concepts
52:14 - and ideas i'm going through will apply
52:17 - for any other kind of
52:20 - programming language or tool so the same
52:22 - steps we're doing here we would have to
52:23 - do in pi torch we'd have to do if we're
52:25 - using r or we'd have to do it using
52:27 - julia or some other language so it's not
52:29 - necessary today about learning about the
52:31 - python it's learning about the steps you
52:33 - go through and as a side effect we're
52:35 - learning how to use tensorflow
52:38 - i'm going to jump into the question or
52:39 - two in chat before coming on this slide
52:41 - so tim asks how come the categorical
52:43 - output are incident coded rather than
52:44 - one hot encoded
52:46 - in reality tim they are one hot encoded
52:49 - it's just that in the original data set
52:52 - they are integer encoded when it comes
52:54 - to the network it is going to treat them
52:56 - automatically as being one hot and
52:58 - that's one of the features that kevas
53:00 - and tensorflow just handled for us we
53:03 - tell it it's categorical and it does
53:04 - that work
53:06 - marius asks when working with
53:07 - categorical features should we convert
53:09 - those into numerical features
53:11 - more or less yes and so um
53:14 - tim's and maurice's questions are
53:15 - related now i don't cover in detail what
53:17 - one hot encoding is here but let's just
53:20 - post it there in the chat but at its
53:22 - core
53:23 - you can imagine that we have three
53:25 - different features here we've got sorry
53:27 - three different labels here we've got is
53:29 - it flower one flower sorry is it flower
53:31 - zero flower one or flower two
53:34 - now when we're training a network it's
53:35 - going to be trying to
53:37 - aim for flower zero one or two
53:40 - if it's flower
53:43 - two and it's
53:44 - actually giving a value of three for
53:46 - example then we need to bring it down
53:48 - but also if it's below we need to bring
53:49 - it up and so it's got an ambiguity about
53:52 - if you're between two flowers which one
53:53 - it's going to be and other flowers even
53:55 - if it's likely probability they are
53:57 - necessarily further away because of the
53:59 - inevitable ordering of integers
54:03 - so what you can do instead is take those
54:05 - three different features and turn it
54:06 - into a three-bit binary number so you
54:09 - have a zero or one followed by zero or
54:12 - one followed by a zero or one
54:14 - and you use the first
54:16 - digit to describe
54:18 - how much of it is the first species how
54:21 - much of the second speed is in the
54:22 - second digit and how much of the third
54:23 - species in the third digit
54:25 - so if it is a
54:28 - flower species zero the number would be
54:30 - one zero zero
54:32 - if it's a flower species one it would be
54:34 - zero one zero
54:36 - if it's a flower species two it would be
54:39 - zero zero one so the place where the one
54:42 - is tells you which category the
54:44 - particular measurement is in and that's
54:46 - why it's called one hot because the
54:47 - number one labels the hot place in that
54:50 - measurement and this allows you to find
54:54 - scalable probabilities for all of the
54:55 - different classes in a clever way
54:57 - but the nice thing about tensorflow and
54:59 - coax is they hide that for us we don't
55:01 - have to worry about binary digits it's
55:02 - just going to do the right thing for us
55:07 - so the first thing you need to do is
55:08 - load in our data so
55:11 - to be
55:13 - to cut to the chase with this
55:14 - psychic learn which isn't the package
55:16 - we're going to use to do our training
55:18 - with
55:18 - but it does provide us with some data
55:20 - loading facilities has a function called
55:22 - load iris and this gives us
55:25 - the table that we saw on the previous
55:27 - slide as a piece of code it's something
55:30 - which we can use and analyze and pass
55:32 - through the system
55:33 - so we are going to load into our x
55:35 - parameter
55:36 - the data of the
55:38 - load iris data set and the y value we
55:41 - are going to load the target so x is
55:44 - going to contain the values which were
55:46 - measured with a ruler it is going to be
55:48 - the length and width of the sepals and
55:49 - the length and width of the petals four
55:52 - columns of this data
55:54 - and then the y parameter is going to
55:56 - contain which species each of those
55:58 - samples relates to
56:00 - so we're keeping our
56:02 - training data and our labels associated
56:04 - with it in separate variables and that
56:06 - makes sure you don't accidentally leak
56:08 - some of your labels into your training
56:10 - set and skew the whole thing
56:13 - once we've got it loaded in the next
56:15 - thing we need to do is turn it into our
56:17 - training and test data set and just
56:19 - split those apart and again for this
56:21 - there is a psychic learn function called
56:23 - train test split which we give it our x
56:25 - and y and it gives us back our
56:28 - train and our test for x and our train
56:30 - and our test for y
56:33 - so this has done it randomly we can
56:34 - trust this to do it well pseudo randomly
56:36 - we can trust this to do our job for us
56:38 - by default this does a
56:40 - 25
56:42 - split for testing and therefore a 75
56:45 - split for training
56:48 - if we look at our x train value we see
56:51 - we have
56:52 - an array of lists or way of arrays
56:55 - two-dimensional thing
56:56 - each sample is a row and each sample has
56:59 - four measurements one for each of those
57:01 - things we measured with a ruler on the
57:03 - flower so this is one sample
57:05 - you one flower this is the second flower
57:07 - and this is the third flower and these
57:09 - are the numbers that are going to be put
57:10 - into the input layer of our network
57:13 - these four
57:14 - numerical values
57:16 - if we look at the y thing
57:19 - and the first three that corresponded to
57:21 - it we see we have the number zero one
57:22 - and zero so that's saying this first
57:24 - sample here is flower zero flower
57:26 - species zero this here is flower species
57:29 - one this here is flower species zero
57:33 - if we look at the shape of these two
57:34 - things we see that our x data has 112
57:37 - samples and four columns
57:39 - and our
57:40 - labels there's just 112 numbers it's
57:43 - just a one-dimensional thing
57:45 - so our x is a table of data our y is a
57:48 - list of numbers
57:50 - and in general this kind of shape of
57:52 - things is going to help
57:55 - to ask is the capital letter for input
57:57 - data and lowercase outputs a convention
57:59 - there's a bunch of different conventions
58:01 - you often see x capital x being used
58:03 - because it's representing something
58:04 - which is
58:06 - a vector of samples
58:08 - so you've got something that's got
58:09 - multiple dimensions to it the fact that
58:11 - this is two dimensional is it's capital
58:14 - lettered same reason when you're doing a
58:15 - vector maths you sometimes do an arrow
58:17 - over it to sort of designate it as being
58:19 - a vector also sometimes use capital
58:21 - letters for matrices and this is
58:22 - effectively a matrix
58:24 - the conventions are a bit wobbly but
58:26 - it's more or less the convention that
58:28 - i'm using in the notes here
58:29 - and i think i'm consistent if i'm not
58:31 - please do ask
58:33 - this is our data we've got this for the
58:34 - training we've got
58:35 - equivalent shape stuff for the test data
58:38 - set
58:39 - we've got 112 training samples and
58:42 - therefore we've got 38
58:44 - testing samples
58:50 - simply having your data in a
58:52 - two-dimensional table isn't itself
58:53 - enough we need to do some prep work on
58:55 - it so that tensorflow understands what
58:58 - the data means
59:00 - we have our training sample but we want
59:02 - to be able to show them to the network
59:04 - repeatedly over and over again
59:05 - randomizing the order so that it doesn't
59:08 - do things like remembering the order of
59:09 - the samples for example it's not going
59:12 - to buy us towards remembering the ones
59:14 - at the beginning or the end more
59:15 - strongly so we're going to randomly
59:17 - shuffle them together
59:18 - and by doing over and over again each
59:20 - sample will contribute a small amount
59:22 - towards a result but at different points
59:24 - in the training and so hopefully you'll
59:26 - get a nice balanced result
59:28 - so the two bits of code here again the
59:30 - code details aren't important but it's
59:32 - good to know the kind of thing you need
59:34 - to do
59:34 - we take our
59:36 - data sets our training data set x and y
59:39 - and we turn it into what tensorflow
59:41 - calls a
59:42 - ten a data set
59:45 - the data set in tensorflow kind of
59:46 - encapsulates all the stuff that the
59:48 - network needs to know to train from it
59:50 - and that includes things like the fact
59:52 - that the data should be repeated over
59:54 - and over again those 112 examples should
59:56 - be
59:57 - 112 way through and then show them again
59:59 - and then again and then again
60:02 - they should all be shuffled together
60:03 - here we're shuffling them in batches of
60:04 - a thousand so the ordering that they
60:06 - were in the original sample isn't going
60:08 - to factor in
60:10 - and finally we do a thing called
60:11 - batching and this is another way that
60:13 - you get a
60:14 - more generalized smooth approximation of
60:17 - the answer and that is by showing it not
60:20 - just one example at a time as i was
60:22 - explaining we do before
60:24 - in fact we actually show it 32 examples
60:26 - all at once and because this thing's
60:29 - being represented as a
60:30 - big complicated tensor it just adds
60:33 - another dimension to the tensor which we
60:35 - end up inverting and multiplying and
60:36 - doing math stuff too and so we can show
60:38 - it multiple at once and effectively the
60:40 - average effect of this batch is what
60:43 - gets applied to the training weights
60:45 - and that smooths things out stops you
60:47 - ending up jumping around too much and
60:48 - makes it more likely you're going to
60:50 - find a generalized answer
60:52 - the numbers we use here 1000 or 32 you
60:55 - find them by
60:57 - tweaking the numbers messing around with
60:58 - it seeing what works
61:00 - we then do
61:01 - the same thing with our test data set
61:04 - except in our test data set we don't
61:07 - need to repeat it and we don't need to
61:08 - shuffle it because those things aren't
61:09 - affecting anything and we just batch it
61:11 - into batches of one because again we
61:13 - don't need to worry about
61:15 - this data set having any effect on the
61:17 - network it's only being used to measure
61:19 - it
61:20 - does batching affect the degree of
61:22 - overfitting
61:24 - it's possible by having larger batches
61:26 - you can reduce overfitting you'd have to
61:28 - fit the network for a lot longer to get
61:30 - the same overfitting effect
61:32 - but it's not the primary use of it the
61:34 - primary use of it is to smooth out the
61:36 - training and to stop you ending up in
61:38 - weird local minima too much
61:41 - exactly um we show it 32 examples and
61:44 - then based on the effect of those 32
61:46 - samples we then update the weights all
61:48 - in one go
61:53 - i need to ask is it similar to
61:55 - cross-validation
61:57 - in the
61:58 - with the test and train data sets this
62:00 - is effectively a form of
62:02 - cross-validation we're doing it's not
62:04 - there are more details and nuances to
62:06 - advanced cross-validation to really
62:09 - avoid the nitty-gritty of overfitting
62:11 - but this at its most basic is the first
62:13 - step towards doing cross-validation to
62:16 - avoid overfitting
62:18 - and then carter asks does batching help
62:20 - the time taken for training it can do as
62:23 - long as the computer hardware you're
62:25 - using is able to hold
62:28 - four
62:29 - samples times 32
62:33 - samples in a batch so four times 32
62:35 - numbers in its little cash register to
62:38 - do the the maths to then it's going to
62:41 - make it faster if you make this number
62:43 - too large then it's not going to be able
62:44 - to fit in the memory of the machine and
62:46 - it's then not going to be able to train
62:47 - at all you'd often go the approach of
62:50 - making this number as large as possible
62:52 - such that it fits into the memory of the
62:54 - machine
62:56 - in the example we're doing here we've
62:57 - only got four features and so you're not
62:59 - really ever going to have problems but
63:00 - i've dealt with networks where you've
63:02 - got
63:03 - million features or something like that
63:04 - and then when you start having a batch
63:06 - of 100 or something you start
63:07 - potentially pushing into memory limits
63:09 - of some computers
63:14 - right so we've got our data set up we're
63:16 - all ready to go with that it is ready to
63:18 - be shown to the network it just needs a
63:20 - network to be shown to
63:22 - and so we need to design a network which
63:24 - is able to do this evaluation
63:26 - and the network we're going to design
63:27 - here is a very very similar structure to
63:29 - that which i showed in the earlier
63:30 - slides it's got a input layer at the far
63:34 - left an output layer at the far right
63:36 - and it has two hidden layers
63:39 - we use keras here because keras is what
63:41 - allows us to write layer sorry
63:45 - layer after layer after layer after
63:47 - layer just in a python list
63:52 - and so we start off with our input layer
63:54 - we say our input has four features and
63:57 - so we've got four newer ones in our
63:59 - input layer
64:00 - that's all we need to do to tell this
64:02 - how it's going to work based on this
64:04 - it's going to know to ask for our data
64:05 - set for things which have
64:07 - a dimensionality of four in one of the
64:10 - dimensions so a size of four in one of
64:12 - the dimensions
64:13 - then we're going to have a hidden layer
64:16 - both of these layers here are the hidden
64:17 - layers because they're between the input
64:18 - and the output
64:19 - and our input layer here is going to
64:21 - have 10 neurons in it followed by
64:23 - another hidden layer with 10 neurons in
64:25 - it
64:26 - and the final output of our network is
64:28 - going to be a layer which has three
64:31 - neurons on it we choose the number three
64:33 - here because there are three different
64:35 - categories that we want to put our iris
64:38 - samples into
64:40 - what this is actually going to do
64:42 - is instead of having one single output
64:44 - neuron which has a number and we try and
64:46 - push that number towards the result we
64:48 - actually end up with three different
64:49 - neurons each of which is going to
64:51 - represent the probability
64:54 - of a particular measurement being one of
64:56 - the particular flowers so if neuron one
64:59 - has a high value and neuron two and
65:01 - three you've got low values that's
65:02 - saying it's likely to be neuron one is
65:05 - the answer and we'll say that neuron one
65:07 - will decide in advance is flower zero
65:09 - for example
65:12 - we apply the soft max activation
65:14 - function to the output of those neurons
65:16 - because that is what turns it into a
65:18 - probability
65:19 - it basically works out the
65:22 - it normalizes it so the sum of them add
65:24 - up to one and so we end up with an
65:26 - effective probability being the output
65:27 - of each neuron of it being each
65:29 - particular species
65:31 - i mentioned activation functions earlier
65:33 - and it was more or less in passing and
65:36 - here you see we have to specify what the
65:38 - activation function is we have to say
65:40 - based on what gets added up and
65:42 - multiplied inside the newer one what
65:44 - function do we apply to that number
65:45 - before passing it on
65:48 - and if you don't know any better a good
65:50 - place to start is using the relu
65:52 - which is let's have a look at the
65:54 - wikipedia page for it
65:59 - it's the rectifier
66:00 - and this is a particular form of the
66:02 - rectifier the rectifier linear unit
66:05 - it's a function which looks like this
66:08 - it's the blue line there you see that if
66:11 - the
66:11 - output from the neuron which is x is
66:14 - negative it sets it to zero if the
66:16 - output is positive it just sets it to be
66:19 - the number
66:20 - now this particular activation function
66:22 - has some nice properties which mean that
66:24 - it works well at describing
66:26 - non-linearity and things like that
66:29 - if in doubt go ahead and use the value
66:31 - or spend a few weeks reading the the
66:33 - literature out there to try and decide
66:35 - on a better activation function relu for
66:37 - most purposes is going to do the job
66:39 - quite well
66:41 - okay so i'm just going to check for any
66:43 - questions uh
66:44 - lester's answered a bunch of them
66:48 - so we may ask why do we need the comma
66:50 - after four for input neurons four comma
66:52 - that is a particular pythonism
66:54 - which is saying that this input function
66:57 - here
66:58 - takes arguments and the argument it
67:00 - takes has to be a list of dimensions
67:04 - in this case in python this is how we
67:06 - make a python tuple which only has one
67:09 - element inside it which is the number
67:10 - four
67:13 - nitin asks do we have to do feature
67:15 - selection before or does the model do it
67:16 - for us so if you want to do feature
67:18 - selection and think about what features
67:19 - are important and not you should do that
67:21 - before the network
67:23 - you can however
67:24 - query how the network is training to try
67:27 - and discover which features are
67:28 - interesting or not
67:30 - but i always recommend the best thing to
67:31 - do is to use your scientific intuition
67:33 - to think about the different features
67:36 - and decide which ones are important
67:37 - which ones are not look at correlations
67:39 - between them look at the effect that
67:41 - different ones have based on other
67:42 - machine learning models and decide which
67:44 - features are important you can also do
67:46 - various post processing to your features
67:48 - if you think that's a useful thing to do
67:51 - otherwise one of the nice things about
67:52 - neural networks in many situations is if
67:55 - you put in a useless
67:57 - input feature then it will just get
68:00 - pushed to the side all the weights
68:01 - coming out of that feature are going to
68:02 - get set to zero because they're going to
68:05 - have no effect on the output they're
68:06 - going to kind of get ignored
68:08 - now that's not necessarily a situation
68:10 - you want to be in but it does mean you
68:11 - can sometimes get away with it if you've
68:13 - just got a weak feature in your data set
68:16 - but obviously the more features you have
68:17 - the slower your network is to train
68:24 - so we've described our training data set
68:26 - and we've described our network so in
68:28 - principle we're ready to go now now we
68:30 - just need to describe how we're going to
68:32 - mesh the two of those things together
68:36 - so before we can actually train it we
68:38 - need to tell the algorithm how it's
68:40 - going to work and remember this is more
68:41 - or less a back propagation thing being
68:42 - applied but there was some
68:44 - unspoken details that went into that
68:47 - back propagation algorithm which i
68:49 - didn't cover at the time
68:50 - because uh we need a real example to
68:53 - actually understand what's going on
68:56 - so remember to forward that propagation
68:57 - about you show the
68:59 - the network a sample of data you push it
69:01 - through forward through the network and
69:02 - you see what result you get and based on
69:05 - how wrong that answer is
69:07 - you need to decide how much to change
69:09 - the weights by
69:11 - and you can see if you've got three
69:13 - different outputs
69:15 - and you've got a
69:17 - class you want to end up at you need
69:18 - some way of describing how wrong that is
69:22 - so if for example the three neurons on
69:24 - the output were 0.4 0.3 and 0.3
69:27 - and the real answer is zero
69:30 - naively there's no obvious way to be
69:32 - able to say well i need a single
69:33 - numerical value to describe what how
69:35 - wrong i am
69:37 - so we need to come up with a
69:38 - mathematical function which can take
69:41 - the three neuron outputs and the place
69:43 - we want to be and give a number out of
69:45 - it for example we might want to get a
69:47 - number like 1.13 we need a single
69:49 - numerical value
69:52 - and this is what the loss function is
69:54 - for
69:55 - now there is an art choosing loss
69:58 - functions but mostly it comes down to
69:59 - what kind of problem are you trying to
70:01 - solve in our case we're trying to put
70:03 - things into categories so we want to use
70:05 - some kind of categorical loss function
70:07 - and in the situation where we are where
70:09 - we have multiple neurons as outputs with
70:11 - values
70:12 - and we have a single integer describing
70:14 - the class that we want to assign it to
70:17 - then tensorflow comes with a loss
70:20 - function called sparse categorical cross
70:22 - entropy
70:23 - if you really want to you can read the
70:24 - documentation page and the paper about
70:26 - it and learn about it but for our
70:28 - purposes it's good to know that it does
70:30 - the job well and in most similar cases
70:32 - it's going to do the job well
70:34 - almost every network i've trained in my
70:36 - career so far has used this fast
70:38 - categorical cross-entropy loss function
70:41 - or a very similar variant on it
70:45 - so that's going to tell us how wrong we
70:47 - are so we can use that we can feed that
70:48 - into our back propagation
70:50 - now the other thing in the back
70:52 - propagation is the particular function
70:54 - that we use to accept all these
70:55 - arguments and decide how much to change
70:58 - the weight by
70:59 - on the earlier slide i had that uh
71:02 - sum over the differentials and then you
71:04 - shift the weight by that amount with a
71:06 - delta delta omega
71:08 - that was a simplified version of how
71:10 - gradient descent works of course when we
71:12 - actually come to doing the real job we
71:14 - have to have a specific real answer
71:16 - about how the back propagation is going
71:17 - to work
71:18 - and so we need to tell the network what
71:20 - algorithm should you use and here we are
71:22 - going to use the adam algorithm which
71:24 - does something very similar to how i
71:26 - described it before except it's a little
71:28 - bit smarter
71:29 - one of the things that the adam
71:30 - algorithm does is it has an adaptive
71:33 - learning rate
71:34 - so if going back to our example of being
71:36 - out on the planes with some mountains
71:38 - and hills and valleys and stuff around
71:40 - when it's on really flat land
71:42 - it will move more quickly it will
71:45 - move across flatland really quickly but
71:47 - as soon as it gets to the hills it's
71:48 - going to slow down to make sure it
71:49 - doesn't jump over any particular local
71:52 - minima
71:53 - in the in the whole domain
71:56 - the last thing we have is the metrics
71:58 - this isn't used to inform the training
72:00 - per se
72:01 - it's only used to tell us how the
72:03 - training is progressing like how well
72:05 - it's doing so we can keep an eye on it
72:07 - as it's going through
72:10 - so we've prepared all the pieces we have
72:12 - our data we have our network we've
72:14 - described how they're going to fit
72:15 - together then we just need to actually
72:17 - kick off the algorithm which is going to
72:18 - do the job
72:20 - and so we've created our model which was
72:21 - made up from these different layers
72:23 - and every model in tensorflow and keras
72:25 - has a fit function
72:27 - so we just call that fit function we
72:30 - show it our training data and it's going
72:32 - to go away and do the fitting that's
72:34 - pretty much all we have to do
72:37 - the nuance on top of what i just said is
72:40 - that as well as showing it the training
72:41 - data we also want to give you access to
72:44 - the test data so that while it's
72:46 - training it can be printing out onto the
72:47 - screen how the network at the point it's
72:50 - got to is working on the validation data
72:52 - set
72:53 - and so as it's printing these things out
72:55 - and it's training on the trained data
72:56 - set and evaluating in a side channel on
72:59 - the validation data set we should see
73:02 - the
73:03 - progression of the network improving for
73:05 - example one of the things that we asked
73:06 - it to tell us about if i go back to the
73:08 - previous slide
73:09 - we asked it to tell us about the
73:10 - accuracy
73:11 - that is of the samples that you have
73:13 - there how many are you getting right and
73:14 - how many are you getting wrong
73:17 - we want that number to be 100 and
73:19 - anything short of that is less good and
73:21 - zero is terrible
73:24 - so what we'll see as the network
73:26 - progresses is that the accuracy by
73:28 - looking at the training data set is
73:30 - going to improve over time and the
73:32 - accuracy on the validation data set is
73:34 - going to hopefully also improve over
73:37 - time
73:38 - the way that you spot whether your
73:40 - network is overfitting is whether the
73:42 - accuracy of your training data set keeps
73:44 - on getting better and better because it
73:45 - keeps on learning more and more nuances
73:47 - about the training data that it's seeing
73:49 - but then your validation data starts
73:51 - getting worse all of a sudden because
73:53 - your network is no longer generalizing
73:55 - well it's learning specifically about
73:57 - the training data set
73:59 - and so it's the
74:00 - combination of looking at the accuracy
74:03 - on the training data and the accuracy on
74:05 - the test data that tell you about
74:07 - overfitting and things like this
74:10 - the very last thing we need to do is
74:11 - tell us tell it how long to train for so
74:13 - we say for each
74:15 - big loop of training data you should do
74:17 - 150 look at 150 examples
74:20 - this is actually 150 batches
74:23 - so it's going to be 150 times 32
74:26 - different flower examples it's going to
74:27 - look at
74:29 - and it's then it's going to evaluate
74:31 - against the test data and then it's
74:32 - going to do that 10 times it's going to
74:34 - do over and over again 10 times until
74:36 - it's finished
74:37 - now i've chosen these numbers because i
74:38 - know that by the time it's done that 10
74:40 - times it's going to
74:42 - give a nice answer but in reality there
74:46 - are more advanced techniques to work out
74:47 - how long you should train your network
74:49 - for
74:50 - so when we run this function it's going
74:51 - to go off do the machine learning do the
74:52 - back propagation and it's going to
74:54 - output the information to the screen and
74:56 - it should only take a second or two with
74:58 - the data that we're looking at
75:00 - newton asks a question how is the model
75:03 - handled and balanced data set
75:05 - um
75:06 - and
75:07 - there are techniques there by choosing
75:09 - the right metrics is one of the main
75:11 - ways that you do so
75:13 - you choose a metric which rather than
75:15 - looking at accuracy which if you have a
75:16 - very imbalanced data set is going to
75:18 - tell you that you're often doing quite
75:19 - well because one of your classes only
75:21 - crops up one percent of the time so you
75:23 - have to think of a different metric to
75:25 - accuracy to describe it or you do some
75:28 - clever stuff to rebalance your data set
75:30 - i know this is something that lester who
75:32 - is working on a machine learning project
75:33 - at the moment is currently fighting with
75:35 - and it's not always very fun
75:37 - ume asks what steps for epoch's doing so
75:40 - steps for epoch is how many batches
75:42 - should it look at before it
75:44 - prints some statistics to the screen
75:46 - historically steps for epoch would be
75:50 - how many samples should i look at or
75:52 - other how many samples do i have
75:54 - and an epoch would be how many times
75:57 - should i look at the full data set
75:59 - when you're repeating and randomizing
76:02 - and batching your data those definitions
76:04 - fall apart a little bit so mostly you
76:07 - can think of it looking at all the
76:08 - batches
76:10 - one and a half thousand times 150 times
76:12 - 100 times 10.
76:14 - however i break it down into 150 and 10
76:17 - so that it's going to print out summary
76:18 - statistics as it goes along so mostly
76:20 - that's a hack to make tensorflow
76:22 - describe what it's doing as it
76:23 - progresses
76:27 - so we call fit it goes away and does it
76:28 - and then as it's going through it's
76:30 - going to print this stuff and i'm going
76:31 - to just describe what you see on the
76:32 - screen and then we're actually going to
76:33 - run this stuff for ourselves
76:34 - so for example on epoch 10 the last
76:37 - epoch it's going to print out the value
76:40 - of the loss function remember that
76:41 - sparse categorical cross entropy thing
76:43 - that has a number we're trying to make
76:45 - that number small we're trying to reduce
76:48 - how much it's lost
76:49 - and we're also trying to measure the
76:50 - accuracy which will make the accuracy
76:52 - large we want it to be accurate when
76:54 - assessing the data that it's training
76:56 - over
76:57 - as well as the loss and accuracy on the
76:59 - training data set we're also looking at
77:01 - the loss and accuracy on the validation
77:04 - or the test data set
77:06 - so we want these numbers to be moving
77:07 - down at the same kind of rate as those
77:10 - two numbers
77:11 - they should have similar values and they
77:13 - should progress at a similar rate if
77:16 - they start diverging from each other
77:18 - that's a sign that you're probably over
77:19 - fitting on your data set and you need to
77:21 - reevaluate how you're balancing things
77:22 - up
77:24 - so this is telling us that at the end of
77:25 - this it's got a 75 accuracy
77:32 - once we've trained our network and we've
77:34 - got
77:35 - um everything all working then we have
77:36 - to actually use the model
77:39 - so
77:40 - this is after the model's trained it's
77:41 - learned everything the weights are set
77:42 - and they are fixed at this point the
77:44 - weights don't change anymore they are
77:45 - just going to be used we're no longer
77:47 - going to do any back propagation we no
77:49 - longer have any true labels we're
77:50 - comparing with we're no longer doing
77:52 - loss functions any of that stuff we have
77:54 - a fixed network which is designed to
77:56 - answer questions so we're going to
77:58 - pretend that we've since training the
78:00 - network gone out into the garden and
78:01 - measured some more flowers
78:02 - using my brain i know which one is which
78:05 - i know i've got one of each flowers
78:07 - and the four features of the first
78:09 - flower are those four four feet to the
78:10 - second foul are those and the four
78:12 - features are those
78:16 - we take this data this prediction this
78:19 - prediction this data we want to predict
78:21 - over these three samples and we pass it
78:24 - to the predict function of the model
78:26 - it takes some data that's the same shape
78:28 - as the training data and it's going to
78:29 - give us back some predictions this
78:31 - predictions is going to contain the
78:33 - values of the output neurons
78:36 - for each of the three samples that we've
78:38 - shown it
78:40 - we then do a bit of python magic where
78:42 - it takes
78:44 - those predictions loops over them find
78:46 - the one that has the maximum probability
78:50 - and then grabs the actual name of the
78:52 - flower out of the look up table which
78:54 - made zero be one flower one be the other
78:56 - flower and two of the other flower
78:58 - we run that bit of code and it gives us
78:59 - back its prediction of what the species
79:02 - of those three flowers are
79:04 - and if we look we see that it matches
79:06 - what we had at the beginning
79:09 - it's also worth noting that during this
79:11 - process while we up front wrote what the
79:13 - three flowers were we never actually
79:15 - used that in the prediction that was
79:16 - just for our human purposes it wasn't
79:19 - used by the code at all it's not
79:20 - cheating and looking the answer it's
79:22 - only looking at
79:23 - these data here
79:27 - umai asks where did it convert the
79:29 - binary number zero one or two like in y
79:31 - test
79:32 - it did that automatically by us
79:36 - describing
79:37 - the loss function
79:39 - as being the sparse categorical cross
79:41 - entropy
79:42 - that function there understands how to
79:44 - take an integer value like this and turn
79:46 - it into a one hot encoding which can be
79:48 - compared to the three output neurons
79:51 - this function is designed to kind of do
79:53 - that one hot thing automatically for us
79:57 - and it is just a function which you can
79:59 - call with the value zero and a list of
80:01 - numbers and it will give you back
80:03 - a loss value
80:07 - right enough talking for me let's
80:09 - actually go ahead and run it so uh go
80:11 - ahead and click on that link there
80:13 - when you've i'll go through this in a
80:15 - second so um just click on this link
80:17 - here
80:19 - if you're not logged into your google
80:20 - account already then you'll have to sign
80:22 - in and to do that you'll click on the
80:24 - sign in button up here in the top right
80:30 - once you're logged in go to run time up
80:33 - here at the top and click run all
80:37 - once you click that it's going to run
80:38 - all the code and it's going to run all
80:40 - the way through and while that's running
80:41 - i'm going to answer some questions
80:43 - it will warn you that this code is
80:45 - coming from github
80:46 - it is coming from my github account so
80:48 - if you trust me go ahead and run it if
80:50 - you don't trust me
80:51 - press cancel but i do hope that you
80:53 - trust me and you can see the code here
80:54 - and understand everything we've just
80:55 - done
80:59 - annika asks how would you look at the
81:01 - layers and see how the network makes
81:02 - decisions either planes separating the
81:04 - three species
81:05 - there are
81:07 - things you can apply on top of the data
81:09 - the network to try and do this
81:11 - tensorflow comes with some tools to do
81:12 - these kind of things but it is a whole
81:15 - whole field of research to try and
81:16 - understand what's going on
81:18 - to see the planes here for example you
81:19 - could just sample the face space and do
81:21 - a 3d plot and try and see these clusters
81:24 - but more advanced techniques would take
81:25 - more than this course to apply but the
81:27 - tensorflow documentation does have some
81:29 - information about this
81:31 - so this has now worked
81:33 - it has loaded the data it's done all the
81:35 - um
81:37 - shuffling of the data we've designed our
81:39 - network and my screen a little bit
81:41 - bigger that's much better
81:43 - and then it's on the training and so you
81:44 - see here we've started out and the loss
81:46 - function was off the first epoch quite
81:48 - large and the accuracy was quite small
81:51 - only 20
81:53 - notice at the very beginning here while
81:55 - the training accuracy was 20
81:57 - the validation accuracy was 40
82:00 - and that's just random chance after the
82:02 - first iteration you could easily expect
82:03 - them to be the other way around
82:06 - what we want to see as we progress
82:07 - however is this accuracy increasing over
82:10 - time
82:10 - and that's it saying that based on the
82:12 - examples that it's being shown it is
82:14 - doing a better and better job of
82:15 - representing them
82:17 - but remember as well as that's
82:18 - increasing over time we want to make
82:20 - sure that the test data set the
82:22 - validation data set is also increasing
82:24 - and we see here it does and it quickly
82:26 - kind of starts aligning with the values
82:28 - 79
82:29 - 81
82:32 - after a while the network has got to the
82:33 - point where the validation accuracy has
82:35 - topped out it's no longer getting any
82:37 - higher because 97
82:40 - is probably of the 40 or so examples
82:43 - probably only one example that it's no
82:45 - longer
82:46 - getting it
82:49 - it's failing to um it's it's it's not
82:53 - able to resolve that one last example
82:54 - but it's doing all the rest of them
82:56 - quite well and from that point on it
82:57 - never gets any better on the validation
82:59 - data set
83:00 - we might need more validation examples
83:02 - to get a better idea of why
83:04 - but it's not necessarily a problem that
83:05 - the validation accuracy tops out
83:07 - what would matter was if this validation
83:09 - accuracy started going down
83:11 - we also note that the violation loss is
83:13 - still reducing which is a good sign that
83:15 - it's continuing to go down in concert
83:18 - with the
83:19 - uh
83:20 - loss function of the training data set
83:24 - so and right at the bottom you see that
83:25 - example and it says the sosa
83:27 - is 94 98 sure that it's correct the
83:31 - versacle is 97 sure that's correct and
83:33 - the virginity is 92 percent sure it's
83:34 - correct
83:35 - and so you wouldn't necessarily expect
83:37 - to see the exactly the same numbers as
83:38 - that because there's a random element
83:40 - but you should expect to see numbers in
83:41 - the 90s
83:47 - right i was going to go through a few
83:48 - questions here i've seen some things
83:49 - cropping up
83:50 - yan's asking about the role of the test
83:53 - and training data set so
83:55 - i'll go back to the beginning and
83:56 - briefly show what's going on here so we
83:59 - have our all our data we split it into
84:02 - training and testing and from that point
84:04 - on those two data sets are completely
84:06 - treated in independently our training
84:08 - data set gets shuffled and blocked up
84:11 - and our test data set just gets left how
84:12 - it is pretty much
84:15 - when we come to the fitting
84:17 - we show it the train data and the
84:19 - training data is what's going to be used
84:21 - during the back propagation process
84:23 - that's the one that's going to be having
84:25 - the loss function applied to it it's
84:26 - going to have the weights being
84:28 - recalculated from it and it's the one
84:29 - that's going to cause the network to
84:32 - collect towards a result
84:34 - so the network is only going to learn
84:35 - from the things that are in the trained
84:36 - data set it is never going to see
84:39 - for its training purposes anything from
84:41 - the test data set
84:43 - the only thing that the validation data
84:45 - test is used for is for calculating
84:47 - these numbers
84:48 - it's not used to inform the network in
84:50 - any way it's just used to give us this
84:52 - output so that we as humans
84:54 - can assess how well the network is
84:56 - working
84:58 - tim asks if you restart the window and
85:00 - start again it will by default in this
85:02 - case
85:04 - start again from scratch entirely or
85:05 - wipe all the weights and run it all if
85:07 - you run just the fit function again i
85:09 - think it will carry on from the previous
85:11 - weights
85:13 - and ozalpas asks can we access model
85:15 - metrics and model selection comparison
85:17 - yes so keras and tensorflow have a whole
85:18 - bunch of functions built in to extract
85:20 - information from these models for doing
85:22 - comparisons and whole kind of
85:24 - cross-validation high performance tuning
85:26 - setups so that's all possible in
85:28 - tensorflow we're not going to cover it
85:30 - all in this course because it's a large
85:31 - topic we're trying to focus on as simple
85:33 - an example as i can come up with
85:37 - okay looks like i can update the notes
85:38 - and thank you lester for posting those
85:39 - links
85:41 - i'm going to close that page
85:45 - so that's the end of the ios example
85:47 - that is the
85:49 - arguably the simplest neural network you
85:51 - can think of designing and you see that
85:52 - even though it's a simple example
85:53 - there's still a lot of questions you
85:55 - have to ask and answer along the way you
85:58 - have to think about structure of data
85:59 - about tests and training data splits
86:01 - you've got to think about your loss
86:02 - functions and how you're going to repeat
86:05 - your data you've got to think about your
86:07 - optimization functions there's lots of
86:09 - choices you make along the way
86:10 - but the first thing to think about or to
86:13 - remember in that situation is you don't
86:15 - always have to make those decisions up
86:17 - front
86:18 - i'll say this again because i think it's
86:19 - worth reiterating the best place to
86:21 - start with the network is what someone
86:23 - else has created before you you can go
86:25 - online you can find pre-designed and
86:28 - set up models in tensorflow to solve a
86:30 - problem you just import that code show
86:32 - your data and see how it performs that
86:34 - is going to get you a lot of the way
86:36 - there
86:37 - by understanding what we've gone through
86:38 - today that's going to give you the
86:39 - ability to tune and tweak what you're
86:41 - seeing to slowly start understand how to
86:44 - apply it to your particular situation
86:46 - so thomas is asking why the validation
86:48 - accuracy didn't keep increasing as the
86:50 - loss decreased on the validation data
86:52 - set and that's because
86:54 - it the accuracy is a course measure it's
86:57 - measuring of all the samples add up how
87:00 - many i got right and wrong and give that
87:02 - as a fraction
87:03 - the loss function as leicester mentioned
87:05 - earlier is differentiable it is smooth
87:08 - and so it can keep on increasing
87:10 - and so while the accuracy wasn't going
87:12 - up the degree to which it was sure about
87:15 - each of its answers
87:16 - would have kept on going up
87:19 - it might have once it got to that that
87:21 - threshold been saying i've got the right
87:23 - answer but i'm only 53 sure
87:25 - by the end it might have been saying
87:27 - i've got that same answer but now i'm 90
87:30 - sure for example so that's why the loss
87:32 - can keep on improving while the accuracy
87:34 - doesn't necessarily change
87:39 - so i'm going to start covering this
87:41 - section here and then we're going to
87:42 - have a break and come back through the
87:43 - last example
87:45 - so this should only be maybe 10 minutes
87:47 - and then we'll have a break
87:49 - and so for this we're going to go
87:50 - through some image analysis stuff
87:52 - because we want to learn how we can
87:54 - apply neural networks to pictures
87:56 - as well as how we can apply neural
87:58 - networks to plain old numbers
88:01 - but before we can dive into applying
88:03 - neural networks to images we need to
88:05 - understand how computers treat images
88:08 - because images are more complex things
88:10 - than
88:11 - measurements made with a ruler
88:15 - so
88:16 - the flip side of this is that pictures
88:17 - are also much easier to collect it's
88:19 - much easier to grind your garden and
88:20 - take a photograph of a bunch of flowers
88:22 - than it is to go out with a ruler and
88:23 - measure all of their parameters it also
88:25 - requires less skill so you can collect
88:27 - larger data sets by using crowdsourcing
88:29 - and so on
88:31 - so i'm going to go through now and
88:32 - explain how image analysis techniques
88:34 - work so to get a sense from the class
88:36 - could people just post in the chat what
88:38 - kind of experience you have with image
88:40 - analysis using kernels and convolutions
88:43 - and things like that is it completely
88:45 - new to you or have you done some of this
88:46 - before i'm assuming it's completely new
88:48 - to you which is why i'm going to go
88:49 - through it all
88:52 - i'm seeing lots of news and nuns
88:54 - wonderful
88:55 - i'm sure there's gonna be some people in
88:56 - the room who've done this before and
88:58 - hopefully i'll be able to answer
88:59 - questions you have as well but let's
89:00 - start right from the basics
89:03 - so the idea of applying neural networks
89:05 - to pictures
89:07 - starts with
89:08 - the
89:09 - maths and computer science approach to
89:10 - dealing with pictures
89:12 - and so the place you have to start
89:14 - because we're going to be doing maths to
89:15 - this stuff because neural networks at
89:16 - their core are multiplications and sums
89:19 - and function applications we have to
89:21 - turn our picture into numbers
89:24 - i mean a consistent way of representing
89:26 - images as numbers
89:29 - and so i hope
89:30 - it you're comfortable with the idea of
89:32 - taking a gray set grayscale image just a
89:35 - black and white image
89:36 - and assigning for every pixel in that
89:38 - image
89:39 - a number and that number represents how
89:41 - bright that pixel is
89:43 - commonly these numbers are between 0 and
89:45 - 255 you've seen these sort of binary
89:48 - power of two numbers before so a pixel
89:50 - being 255 would be completely white and
89:54 - a pixel being zero would be completely
89:56 - black
89:57 - a number like 105 is slightly under
90:00 - halfway so that's a darkish gray
90:03 - but all the numbers on this little
90:04 - snippet here are a darkish gray
90:08 - the dots here are to represent that
90:09 - these pictures can be very large the
90:11 - numbers we have here is only a five by
90:13 - five sample in the top left corner
90:16 - but
90:17 - these pictures can be hundreds or
90:19 - thousands of pixels across in each
90:20 - dimension you know megapixels means
90:22 - millions of pixels which means millions
90:25 - of numbers
90:26 - which in our case corresponds to
90:28 - millions of input neurons and that's a
90:32 - lot of data you need to deal with
90:35 - so we have a grid of numbers
90:37 - sometimes people refer to these as
90:38 - matrixes matrices of numbers but don't
90:40 - think of these as mathematical matrices
90:43 - as you might have done in school or
90:44 - undergrad these are a grid it's a it's a
90:46 - grid of numbers and nothing more than
90:48 - that
90:51 - so once we've got our grid of numbers we
90:52 - need to decide describe some kind of
90:54 - mathematical process we can do to that
90:56 - grid
90:57 - and the way this is commonly done is by
91:00 - using a technique called kernel
91:02 - convolution and i'm going to explain the
91:04 - two parts of that term as i go through
91:06 - let's start with the first first part a
91:08 - kernel
91:09 - now a kernel in
91:12 - image analysis is a another smaller grid
91:15 - of numbers
91:16 - usually they're two by two three by
91:18 - three five by five that kind of size and
91:21 - they contain inside them a bunch of
91:23 - numbers
91:24 - and the magic of kernel convolution is
91:26 - depending on which numbers you put
91:28 - inside that kernel
91:30 - when you apply it to the image and we'll
91:32 - see how it gets applied in a moment when
91:34 - you apply it to the image different
91:36 - things happen
91:37 - so for example these particular numbers
91:39 - here
91:40 - will sharpen the image they'll make it
91:42 - less blurry
91:44 - you might have come across the sharpened
91:46 - mask or the unsharp mask if you've
91:48 - played around in photoshop or something
91:49 - like that
91:51 - a different selection of numbers
91:53 - with like one which didn't have the
91:54 - negatives here for example would blur
91:56 - the image you apply a particular small
91:59 - kernel to the image and it blurs it this
92:01 - is how
92:02 - all image blurring works in any kind of
92:04 - computer sense
92:06 - there's other kernels you can choose
92:08 - which do edge detection and edge
92:09 - detection is a very common computer
92:11 - vision technique for example working out
92:13 - outlines of things or it's also the core
92:15 - of what we're going to be seeing today
92:17 - now you might be wondering how can you
92:19 - know what particular numbers are going
92:21 - to give particular outputs and that's
92:22 - because some clever mathematicians have
92:24 - done that work for us they are all
92:26 - published and online you just choose the
92:28 - one that does the job for you if you
92:29 - want to blur the image you go on the
92:31 - wikipedia page you find the blurring
92:32 - kernel and you apply it
92:35 - that's all you have to do you don't have
92:36 - to think about what these numbers should
92:38 - do schweitz is analogous to
92:41 - normalization
92:42 - in a sense so um one of the things you
92:44 - might notice is that the sum of all the
92:46 - numbers in here
92:47 - add up to one
92:49 - and so after applying this kernel
92:51 - you
92:52 - expect it to have a sort of a normal
92:54 - effect
92:55 - normalization can be applied in lots of
92:57 - different ways so there might not be a
92:59 - an image kernel which can perform it but
93:01 - it's going to be a similar technique
93:02 - that's going to be done to it
93:04 - so the idea here is we have a set of
93:06 - predefined kernels designed by computer
93:08 - scientists and mathematicians which have
93:11 - effects which we can describe with words
93:13 - that's how these kernels are described
93:16 - the next question then is once we've got
93:17 - a kernel how can we use it to do
93:20 - something to our image and what we do is
93:23 - we take our image which here on the left
93:24 - hand side
93:25 - so you've got the purple section and the
93:27 - lighter blue out going into the distance
93:30 - we take our kernel matrix i say it's not
93:33 - again it's not really a matrix it's just
93:34 - a grid and we overlay it over those top
93:37 - three squares our kernel is three by
93:39 - three and so we overlay it over the top
93:41 - three by three grid
93:43 - and then we kind of shine a light
93:45 - through it
93:46 - we look at the top left number that's
93:48 - being shadowed and the top left number
93:50 - in the kernel matrix
93:51 - we multiply those two numbers together
93:53 - so 105 times zero
93:55 - and then we add that to the next two
93:58 - pairs that are over each other so 102
94:00 - times -1
94:02 - 100 times zero and so on and so on we
94:05 - add up all of those
94:07 - total numbers and we put that in where
94:09 - the middle
94:10 - of that kernel is currently sitting
94:14 - you should ask why do we apply a kernel
94:16 - that's a tricky question so we apply a
94:19 - kernel because it has the effect that we
94:21 - want it to
94:22 - we apply a kernel because for example we
94:24 - want to blur our image and if we want to
94:26 - blur our image we choose a blurring
94:28 - kernel
94:29 - and we apply it to the image and we end
94:31 - up with a blurred image
94:34 - but at its core all kernels work the
94:35 - same regardless of what the numbers in
94:37 - the kernel itself are
94:39 - the process that's applied is the same
94:41 - and if we change the numbers the effect
94:43 - in the output image is also going to be
94:45 - different you'll end up with a blurred
94:47 - image or a sharpened image or an image
94:48 - which represents where the edges are for
94:50 - example
94:54 - so to take that picture and kind of turn
94:56 - it on its side a little bit we have the
94:57 - picture on the left hand side we have
94:59 - our kernel and we've done 105 times zero
95:02 - 102 times -1 100 times zero
95:05 - adding these up as i go 103 times minus
95:07 - 1 et cetera and so we see this in the
95:09 - sum at the bottom you see that's the
95:11 - calculation it's done
95:13 - and that gives us the number 89 and
95:15 - because this kernel was 3x3 and it was
95:17 - overlaid on the 3x3 purple area the
95:19 - result of that calculation goes in the
95:21 - middle of the kernel so the 99 gets
95:23 - replaced with an
95:25 - 89
95:31 - so what we do after we applied the
95:32 - kernel the first time is we shuffle it
95:34 - across by just one square and we do the
95:35 - same thing again so it's overlapping
95:37 - with the original one but the middle of
95:39 - the kernel is now in the next space so
95:40 - now we're gonna do the same maths again
95:42 - but we've got different numbers so now
95:43 - the 103 gets maps to a 111 for example
95:48 - and we keep on doing that all the way
95:49 - along the row and on the next row and
95:50 - the next row and the next row filling up
95:52 - the whole image
95:54 - now what you might have noticed is that
95:56 - in that situation you're not filling in
95:58 - the edge values at all there's no way to
96:00 - represent what the number in the top
96:02 - left-hand corner should be because five
96:05 - of the numbers in the kernel won't have
96:07 - corresponding um image values to
96:09 - multiply against
96:11 - and so we have to make a choice how do
96:13 - we deal with that
96:14 - and that's just a choice that we make
96:17 - in our case here we can decide to just
96:19 - pad the edges with zeros that's
96:22 - perfectly valid you get slightly weird
96:24 - effects of the edge of the image but
96:26 - it's
96:26 - mathematically valid and it's consistent
96:28 - at least
96:29 - alternatively we could replace we could
96:32 - pad the outside with a repeat of the
96:34 - value so this top row would be well
96:36 - undefined but then it'll be 10502 197.96
96:40 - repeating above the actual image there
96:43 - so depending on whether you want to zero
96:44 - pad or same pad it gives you a choice
96:47 - about how you want to deal with this
96:49 - edge condition
96:52 - you do this to an image by repeating
96:54 - that process over and over again
96:55 - obviously use computer if you use the
96:57 - right kernel for example there's a
96:58 - kernel that does sobel edge detection
97:01 - you start with the image on the left and
97:02 - you end up with the image on the right
97:05 - anywhere where the image is smooth just
97:06 - stays black anywhere where the sharp
97:08 - change ends up white so we've got an
97:10 - image now which shows where the edges
97:12 - are
97:13 - and that's a useful thing to have
97:14 - because edges are often where
97:16 - interesting stuff happens in pictures
97:23 - so we saw just before the break how
97:25 - at its core
97:27 - image analysis is done
97:29 - most image analysis algorithms have some
97:32 - elements of kernel convolution being
97:35 - done to them now the kernel is that
97:36 - little three by three matrix we saw and
97:39 - the convolution part is the bit that
97:41 - applies it to the picture and gives some
97:43 - kind of result
97:45 - so in principle it would be possible to
97:47 - take a whole bunch of kernels
97:50 - really carefully design them such that
97:52 - you can show them to an image one after
97:54 - another layering them on top of each
97:55 - other
97:57 - to get some kind of answer for example
97:59 - you could start off with one kernel
98:01 - which does the edge detection
98:03 - and then based on the result of that
98:05 - edge section you could have another
98:06 - kernel which says are there two edges
98:08 - next to each other here so they're
98:10 - parallel at which point you say okay the
98:12 - result of this is a image which
98:14 - represents where parallel edges are from
98:16 - that you have another kernel which looks
98:18 - for circles
98:20 - and then based on that you have another
98:21 - kernel which looks first circles next to
98:23 - each other and you start building up
98:25 - these questions getting more and more
98:26 - abstracted away from the picture but
98:28 - more and more towards human concepts
98:30 - like is this a person
98:32 - and at its core that's what
98:34 - convolutional neural networks are going
98:36 - to do
98:37 - now i alluded before to the fact that
98:40 - kernels
98:41 - in convolutional image analysis
98:44 - are designed by mathematicians and
98:45 - computer scientists
98:48 - and the example i just gave there i said
98:49 - if you choose your kernels really
98:51 - carefully you can do such and such a
98:53 - thing
98:54 - so what we're going to do is try and
98:55 - make those two things meet in the middle
98:57 - using your networks we're going to try
98:59 - and train a neural network to
99:00 - automatically discover
99:03 - what the values of the kernel should be
99:06 - we are going to design brand new kernels
99:08 - and a whole slew of them we're going to
99:10 - layer them on top of each other
99:12 - and through the magic of the back
99:14 - propagation stuff we saw before this is
99:16 - going to give us an answer which can
99:18 - based on a picture say is it a cat or a
99:21 - dog for example
99:23 - so the process is going to be the same
99:24 - we're going to be showing it examples
99:26 - checking how far away we are from the
99:28 - answer
99:29 - doing the same back propagation thing to
99:31 - the weights
99:32 - but the difference here
99:34 - is that the weights in the network are
99:36 - going to be representing the values in
99:40 - the kernel
99:41 - so going back to the previous section
99:45 - this matrix here for example has got
99:47 - nine values inside it you can imagine in
99:50 - our network that we have a weight
99:52 - associated with each of these numbers
99:55 - if this zero was a different number you
99:57 - can expect that the output of the
99:58 - network would be different
100:00 - and therefore this
100:02 - number here this weight has an effect on
100:04 - the output
100:05 - and therefore we can do the same thing
100:06 - we did before we work out what the
100:07 - derivative of that is
100:09 - and therefore work out how much this
100:11 - number in the top left of the kernel
100:13 - needs to be changed in order to make our
100:15 - answer more correct
100:18 - so it's the same idea as before but in
100:20 - this situation the weights are the
100:23 - values inside the convolution kernels
100:28 - the way we apply it ends up working
100:30 - exactly the same and that's the beauty
100:31 - of convolutional neural networks that
100:33 - the process is the same but the way that
100:35 - you treat those weights and the way you
100:37 - apply those weights you get a very
100:39 - different kind of idea
100:43 - a convolution on your network or a cnn
100:46 - is a classic example of a deep neural
100:48 - network they're deep because we're going
100:50 - to need lots of layers
100:52 - we need lots of layers because we need
100:54 - those levels of obstruction we need to
100:56 - start at one end with is there an edge
100:59 - here
101:00 - why is it the other end saying is this a
101:02 - cat there's lots of questions you have
101:03 - to ask along the way like where are the
101:06 - ears with respect to the nose what shape
101:08 - of the ears how do you describe an ear
101:10 - how do you find the edges that go into
101:12 - that where are those edges etcetera
101:14 - there's lots of questions you have to
101:15 - ask and so you need lots of layers
101:18 - in the situation where we're asking
101:20 - things about the location of something
101:22 - on an image with respect to the pixels
101:24 - around it on the image
101:25 - we have to use what are called
101:27 - convolutional layers
101:28 - now a convolutional layer doesn't
101:30 - connect every neuron in one layer with
101:33 - every neuron on the left on the next
101:35 - layer
101:36 - it only connects the neurons in one
101:39 - layer with the neurons in the next which
101:41 - map to the same part of the image so
101:45 - each neuron is representing a pixel
101:47 - b only connect to the neuron on the
101:49 - output with the neurons that are near to
101:51 - it on the previous layer
101:53 - as well as these convolutional layers
101:55 - which are the layers that are basically
101:56 - doing the same convolutional kernel
101:59 - thing we saw before just it's kind of
102:01 - encoded into a a neural network language
102:05 - we also have pooling layers and our
102:06 - pooling layer is a much simpler thing a
102:08 - pooling layer takes a picture which is a
102:10 - thousand by a thousand and makes it
102:11 - smaller so it takes a thousand by a
102:13 - thousand image and for example takes
102:15 - every two by two pixel and squishes it
102:17 - into a one by one pixel and therefore it
102:19 - goes from being one thousand by one
102:20 - thousand into
102:22 - five hundred by five hundred
102:24 - and we do that because by
102:26 - applying convolutional layers and then
102:28 - pooling layers we create layers of
102:30 - abstraction
102:31 - so in the output a particular neuron is
102:34 - having a larger area of effect on the
102:35 - earlier layers than it is on the later
102:37 - layers and so you get global information
102:40 - as well as local information
102:42 - the classic pooling algorithm is max
102:44 - pooling so based on a two by two little
102:46 - grid you find the largest of the four
102:48 - numbers inside there and that's the
102:49 - number that survives and goes on to the
102:51 - next layer
102:53 - and the third kind of layer we have in a
102:54 - cnn is the dense layers now dense layers
102:57 - are the same stuff as we saw before
102:58 - these are just the traditional neural
103:00 - network layers where everything is
103:01 - connected to everything and information
103:03 - just flows on through
103:07 - so here we see
103:09 - an example of what this is doing so we
103:12 - have an input image
103:14 - this feature maps blob here this is a
103:17 - layer this is one layer of the neural
103:18 - network now that layer has a bunch of
103:21 - feature maps each of those feature maps
103:22 - is encoding a kernel being applied to
103:25 - the image
103:26 - so by the first layer here we've got in
103:28 - this situation four different kernels
103:30 - being applied to the input image
103:32 - so if this input image was a 10 by 10
103:34 - image
103:35 - then in the output we'd have four
103:37 - 10 by 10 images
103:39 - and each of those feature maps would be
103:40 - representing a different thing one of
103:42 - them might be looking for horizontal
103:43 - edges one might be looking for vertical
103:45 - edges one might be looking for
103:47 - dots that are by themselves who knows
103:49 - what they're actually looking for the
103:51 - point is each of the feature maps are
103:52 - likely to be different
103:54 - we then do this sub-sampling this
103:56 - pooling layer we make things smaller we
103:58 - do some more convolutions to make more
103:59 - feature maps with different algorithms
104:01 - being applied to them
104:03 - we then sub sample again to get a whole
104:05 - bunch of smaller feature maps which have
104:08 - loads and loads of different pieces of
104:09 - information inside them
104:11 - and at this point we treat each of these
104:13 - last very very small feature maps as
104:16 - inputs to a standard neural network like
104:18 - we had before
104:19 - by this point each of these can be
104:21 - represented as a single number we just
104:22 - put them in as the input layer to a
104:25 - traditional neural network
104:27 - so this is kind of in two halves the
104:28 - first part is finding sort of a
104:31 - graphical and spatial information about
104:34 - where things are in the image
104:35 - and then the second part takes where
104:37 - those things have been found and makes a
104:39 - decision based on that as to what the
104:41 - image is of
104:46 - we can do some amazing stuff with
104:48 - convolutional neural networks and i'm
104:50 - sure lester's got some examples you
104:51 - might post as we go through these
104:53 - sections one example i really like is
104:54 - image segmentation so here you can train
104:57 - a network so that it can identify
104:59 - objects in the image and it can overlay
105:01 - each image each pixel in that image with
105:03 - what kind of thing it is and this is how
105:05 - some self-driving cars work so we can
105:07 - identify that as a blue the car over
105:09 - here is a car and it's been outlined
105:11 - with blue we've got the road we've got
105:12 - the arrows on the road we've got traffic
105:14 - lights we've got all sorts of things
105:16 - this is a convolutional neural network
105:19 - admittedly a complicated one but
105:20 - nonetheless a convolutional neural
105:22 - network
105:25 - you can also do things like style
105:27 - transfer now style transfer is a really
105:29 - fun thing to google for because since
105:31 - this example here there's been much
105:32 - better examples but this is a fun one
105:33 - nonetheless
105:34 - you show it an example of a picture on
105:36 - the left hand side by a particular
105:37 - artist
105:38 - you can then add things into that image
105:40 - just based on photographs
105:42 - and the network has learnt how this
105:44 - artist draws and so it's able to draw
105:47 - the things you've added in the style of
105:48 - the artist so you end up with this
105:50 - mcdonald's balloon being inserted right
105:52 - into a streetscape in new york in a
105:54 - style which looks consistent with the
105:56 - rest of the image and in this case the
105:58 - network has learned how the artist draws
106:00 - that's what the kernel convolutional
106:02 - layers are representing in a very
106:04 - complicated way nonetheless
106:07 - and finally again apologies to any
106:09 - people actually understand this is
106:11 - approximately how animal vision works
106:15 - so animal vision does largely work by
106:17 - looking for basic features it looks for
106:20 - horizontal and vertical lines simple
106:21 - patterns like that
106:23 - then it takes those simple features and
106:25 - your brain and your optical nerves and
106:27 - all this processing system in your head
106:29 - takes those things and consecutively
106:31 - applies more and more abstracted ideas
106:34 - on top of it until it ends up at the end
106:36 - with your brain able to identify this as
106:38 - a picture of a cat for example
106:41 - and most animal vision and brain systems
106:43 - work in a similar way to this although
106:46 - of course because these are evolved and
106:47 - not designed they are much more complex
106:50 - and complicated and also much cleverer
106:52 - in some ways
106:55 - so that's what convolutional neural
106:56 - networks
106:57 - are and how they work it is the same as
106:59 - we had before but we
107:01 - design a network so that it can
107:03 - calculate what the weights of these
107:05 - kernels should be and it's going to be
107:06 - going to say give me uh 60 different
107:10 - kernels
107:11 - work out what the weights in those
107:12 - kernels should be in order to answer
107:13 - this particular question and through the
107:14 - magic of that propagation it's going to
107:16 - manage to do that it really is quite
107:19 - amazing
107:24 - the thing that convolutional neural
107:25 - networks are for
107:27 - is for images
107:29 - that's at least 95 percent true so if
107:31 - you've got image analysis you almost
107:33 - always want a convolutional neural
107:34 - network of some kind if you're using
107:36 - convolution on your networks it's almost
107:38 - always because you're dealing with
107:40 - images they are designed for each other
107:42 - effectively
107:44 - so we have an example here where we're
107:46 - going to be dealing with this data set
107:48 - called the mnist data
107:49 - it's again a classic data set based on
107:51 - numbers i think written for automatic
107:54 - check processing in the us
107:57 - this is a data set which is freely
107:59 - available of 70 000 pictures of
108:01 - handwritten digits by all sorts of
108:03 - different people from the wild from
108:05 - actual real people writing checks to
108:07 - people
108:08 - each picture is 28 by 28 pixels so it's
108:11 - quite small and that's useful for us
108:13 - because remember what i said before that
108:15 - the number each pixel is going to end up
108:17 - being an input layer and then we're
108:19 - going to be doing our feature maps and
108:20 - so by starting with a small picture you
108:22 - manage to keep your network under
108:23 - control
108:24 - so we want to design a network where we
108:26 - can show it a picture one of these
108:28 - examples and it's going to say that's a
108:30 - 4. that's a zero that's a nine
108:33 - that's our task that we've got ahead of
108:35 - us
108:39 - so we're gonna go through the similar
108:40 - process that we did with the iris data
108:41 - set so again feel free to click on this
108:43 - link here
108:44 - it's going to take you to the
108:46 - um actually that one takes you to my
108:48 - github page but at the end there'll be a
108:50 - link to the code app where you can
108:51 - follow through the notes if you want to
108:54 - so we're going to do the same things
108:55 - we're going to specify the shape of the
108:56 - network we're going to say how it should
108:58 - be trained we're going to specify our
109:00 - training data set
109:02 - same process before this is process you
109:04 - always go through with your networks
109:06 - so
109:07 - design the network now
109:09 - we've had the question a few times about
109:10 - how do you know how many neurons how
109:12 - many layers how many how big the layer
109:13 - should be
109:14 - this layout of this network
109:16 - is
109:17 - a standard layout of a network for an
109:21 - image classification problem if you've
109:23 - got a relatively simple image
109:24 - classification problem this layout of a
109:27 - network is going to do a decent job
109:29 - as long as you haven't got 10 000
109:31 - different categories as long as you're
109:32 - not trying to
109:34 - identify multiple objects in one image
109:36 - things like that then this is going to
109:38 - do a good job of classifying the images
109:40 - and the way it works is
109:42 - by starting with a convolutional layer
109:44 - now this is going to
109:46 - describe and design
109:48 - 16 different filters
109:50 - each filter is going to be a five by
109:52 - five kernel so a filter on a kernel is
109:54 - terminology means the same thing so you
109:56 - have 16 different kernels each of them
109:58 - are going to be five by five
109:59 - and it's going to make a new layer which
110:01 - is therefore the same size as the input
110:03 - but there's had this layer this this
110:06 - kernel applied to it
110:07 - but it's gonna do that 16 times all in
110:09 - parallel
110:10 - so if we had
110:11 - 10 input neurons we're going to have 160
110:14 - neurons in the in the first layer we've
110:16 - actually got 28 by 28 input neurons and
110:19 - so we can end up with 28 times 28 times
110:21 - 16 neurons in the next layer um objects
110:24 - in the next layer
110:27 - then we have a pooling layer which makes
110:28 - our image half a size so it goes from 28
110:30 - by 28 to 14 by 14 and then another
110:32 - convolutional layer and another pooling
110:34 - layer to do convolution pooling
110:36 - convolution and pooling
110:38 - that's a common sort of technique you
110:40 - get so you've got two layers of
110:41 - abstraction over the spatial information
110:44 - in the image
110:45 - then we finish off with a dense layer
110:48 - which combines together all of the
110:49 - outputs of the
110:50 - convolutions and
110:53 - tries to sort of answer ask the
110:54 - questions about how those different
110:56 - features that it's discovered relate to
110:58 - each other in the same way as we were
111:00 - evaluating how the four different
111:02 - features of the iris flowers related to
111:05 - each other to give us the answer we
111:06 - wanted
111:07 - the final output is the
111:10 - ten neurons which represent one for each
111:12 - digit so is it a zero one two three four
111:14 - five six seven eight or nine
111:16 - there's ten possible categories so we
111:18 - have ten neurons in the final image
111:21 - in the honest example we had three
111:22 - categories and so we had three neurons
111:25 - in the final image
111:27 - there's some details in here but i'm
111:28 - going to go through that as we go
111:30 - through
111:31 - so this structure works really well for
111:32 - mnist you could actually mist is a
111:34 - simpler problem you could get away with
111:36 - a smaller network for mnist but
111:38 - this is a nice generalizable
111:40 - classification
111:41 - network so you can use this in your work
111:43 - if you were doing classification stuff
111:47 - so we're going to do the same thing we
111:48 - did before we're going to build up our
111:49 - network we're using keras and we're
111:51 - going to use a sequential thing which
111:53 - means we're going to pass a list of
111:54 - layers they're going to start from the
111:56 - input and work their way through
111:59 - so the first layer is a convolutional
112:01 - layer it's two-dimensional so it
112:02 - understands sort of built into these
112:04 - functions is the understanding of what
112:06 - images are and how they relate it's kind
112:07 - of wrapping that stuff up for us so all
112:10 - we have to tell it is that we want 16
112:12 - different kernels 16 different filters
112:15 - each filter should be a five by five
112:18 - there was that thing before where at the
112:20 - edge we had to decide are we going to
112:22 - pad it with zeros or pad it with the
112:23 - same number here we tell it that we need
112:25 - to pad with the same
112:28 - and then again because this is
112:29 - eventually actually doing a mathematical
112:31 - operation of some kind we have to have
112:33 - an activation function and like before
112:35 - we use our our
112:37 - um our value function and that does the
112:39 - job well
112:40 - it lets us represent our non-linearity
112:42 - in our model
112:44 - so we have 16 5x5 filters
112:47 - the layer will be 28 by 28 but we've got
112:50 - 16 filters so the overall size of this
112:52 - layer is now 12 and a half thousand so
112:55 - we now have 12 and a half thousand
112:57 - neurons in our next layer so a lot of
113:00 - neurons and each of those are going to
113:01 - have a whole bunch of weights they're
113:03 - each going to have 25 weights
113:07 - a lot of numbers to deal with already as
113:09 - you can see
113:10 - but nothing that modern computers can't
113:11 - handle
113:13 - we then
113:15 - into this list here we add a pooling
113:16 - layer
113:17 - and i've pulled out available here
113:19 - because we're going to be repeating this
113:20 - in a little bit and all we say here is
113:22 - we want a 2x2
113:23 - pooling so it's going to make our image
113:25 - half as big in every direction
113:27 - and this one here is saying how much you
113:29 - want to jump along by each time
113:30 - and the same here is the same padding as
113:32 - we had before
113:34 - so we have now shrunk this down instead
113:36 - of being 28 by 28 by 16 it's now 14 by
113:38 - 14 by 16. so we've only got 3 000
113:41 - neurons in the next layer
113:46 - then we do the same dance again we have
113:47 - another convolutional layer and another
113:49 - pooling layer so remember we have
113:50 - convolution pool convolution pool
113:53 - same bit of code only difference is at
113:55 - this time we have more filters
113:58 - we've more abstracted away so we're
114:00 - further away from the original image but
114:01 - we want to be able to represent more
114:03 - different types of things we're looking
114:05 - for
114:06 - particular curls and curves in the in
114:08 - the handwriting of the original image is
114:10 - what this layer here is going to
114:11 - represent
114:12 - the first convolutional layer is going
114:13 - to represent more things to do with
114:15 - where the lines are and if there's two
114:18 - lines next to each other this part here
114:20 - is going to be describing more precisely
114:22 - where different circles and curves and
114:25 - spaces are in the image as a whole
114:28 - and you see this number here is again
114:29 - getting smaller the layers are getting
114:31 - smaller as we go because we are
114:32 - abstracting away the information we are
114:34 - reducing down our face space
114:39 - so um each neuron have a different
114:41 - kernel so um it's a bit more complex
114:44 - than i was making out uh each there
114:46 - won't be the thing times 25 i misspoke
114:49 - when i said that each of the filters
114:52 - each of the 16 or 32 filters will have a
114:56 - um
114:57 - 5x5 filter associated with it so
115:00 - there'll be 32 5x5 filters and therefore
115:03 - there'll be that many weights so some of
115:04 - the weights are going to be sort of
115:05 - correlated with each other in some ways
115:08 - so it reduces the total number of
115:09 - weights you actually have to analyze
115:10 - which is helpful
115:13 - then we have our same thing as we had
115:15 - before this is the same set in the iris
115:16 - we have a dense layer now a dense layer
115:18 - means connect everything to everything
115:21 - 128 neurons so this is a number chosen
115:24 - which is
115:25 - big enough to be able to represent the
115:27 - variation in the images but small enough
115:29 - that it's going to be able to train
115:31 - quickly
115:34 - we end with a dense layer with 10
115:35 - neurons and we have this soft max thing
115:37 - again to turn it into probabilities
115:40 - the only tricky part in all of this is
115:43 - this layer in here which i didn't
115:44 - mention before called dropout layer
115:48 - and what a dropout layer does is not
115:50 - actually a layer of neurons per se it's
115:52 - simply a thing that gets applied to the
115:53 - process
115:55 - and what this layer does
115:56 - is as the network is training once after
115:59 - each set of examples it will randomly
116:02 - delete
116:03 - 40 percent of the connections it won't
116:06 - set the weights to zero it just for this
116:08 - example
116:09 - will not use those inputs on the
116:11 - training
116:12 - and this is used to avoid overfitting it
116:15 - spreads around the information stops you
116:18 - getting stuck in local minima
116:20 - and make sure you end up with a more
116:21 - generalized model so you're artificially
116:23 - snipping connections and then
116:24 - reconnecting the next time so that no
116:26 - one connection becomes too powerful and
116:29 - too important so this dropout thing is
116:31 - often used to smooth the network out and
116:34 - allow it to be more general
116:39 - this bit of code here is exactly the
116:41 - same as we had last time we're doing the
116:43 - same classification task we're trying to
116:46 - put things into is it this thing this
116:48 - thing this thing on this thing is it one
116:49 - of these flowers is it one of these
116:51 - numbers
116:52 - same optimizers before and the same
116:54 - metrics
116:55 - so we are doing the same task as last
116:58 - time the way that it works the way the
116:59 - network's training is the same it's one
117:01 - of the nice things that once you've done
117:02 - this a few times you start seeing the
117:04 - commonalities and you stop having to
117:06 - worry about how you should make these
117:08 - decisions because you just use the same
117:10 - stuff as as last time which makes your
117:12 - life a lot easier
117:14 - so as i said before if you're doing a
117:16 - classification problem and the labels
117:18 - are the integer index of the class that
117:20 - you want to get to then sparse
117:22 - categorical cross entropy is the
117:24 - tensorflow solution to your problem use
117:26 - that one and you're going to be fine
117:28 - there's also a sparse i think binary
117:30 - cross entropy if you want to have a yes
117:32 - no answer at the end rather than a is it
117:35 - one of these 10 20 categories
117:42 - we've designed our network and we've
117:43 - said how it's going to be trained the
117:44 - order's a little bit different this time
117:45 - we are going to load in our data
117:47 - and for this tensorflow comes with a
117:49 - tensorflow dataset function which gives
117:52 - us our mnist data it splits it into test
117:54 - and train for us it shuffles it for us
117:56 - it does all the magic well much of the
117:59 - magic automatically for us
118:02 - so after this function's been called ds
118:04 - train and ds test that's our test and
118:06 - train data set they are both sequences
118:09 - of 128 by 128 by 1 matrices
118:13 - so it's a 3d matrix but one of the
118:14 - dimensions is only one big because it's
118:16 - only got gray it's not red green and
118:18 - blue
118:20 - and the values in each of those are the
118:21 - numbers from 0 to 255 so the same as we
118:24 - were seeing before
118:30 - there's a few things we do have to do
118:31 - with that data to make it
118:32 - you know suitable for our network to
118:34 - train one of them is that in most
118:37 - networks you end up working best if you
118:39 - have
118:40 - numbers between 0 and 1 rather numbers
118:43 - between 0 and 255 so we
118:46 - apply a function which divides every
118:48 - pixel by 255 and make sure it's a
118:50 - floating point number
118:52 - so that just turns them from zero to
118:54 - five five into zero to one
118:57 - then we do the same thing before we
118:58 - cache it and shuffle it and this kind of
119:00 - stuff details here don't matter too much
119:02 - this is just the process it's going
119:04 - through to get stuff in the right shape
119:07 - you would start with if you were doing
119:08 - this for yourself you'd start with these
119:09 - things and you play around with them and
119:10 - read the documentation and see where it
119:11 - takes you from there
119:16 - you then do the same thing to the test
119:17 - data set but simpler you need to
119:19 - normalize it but you don't have to do
119:20 - all the shuffling and stuff like we had
119:22 - last time the test data set analysis is
119:24 - a simpler process
119:26 - and then we call the fitting same as
119:29 - before we call the fit function and we
119:31 - give it the training data we're not
119:33 - giving the validation data this time
119:35 - because
119:36 - um we
119:37 - actually we do do so we do do underneath
119:39 - so the same process before but we're
119:41 - only going to do it through once
120:00 - so this notebook that i've got here on
120:02 - the screen that is exactly the same code
120:04 - as i've just gone through
120:06 - we set up the data we shuffle it all
120:09 - we do the
120:11 - normalization
120:12 - add all our layers together
120:15 - and call the fitting and you'll see here
120:17 - after the end of the fitting step the
120:19 - accuracy on the training data set is 94
120:22 - percent and the accuracy on the
120:24 - validation data set is 98
120:27 - so those are both large
120:29 - um but uh it's got there very very
120:32 - quickly with only one epoch it's already
120:34 - got a very good accuracy if we kept on
120:36 - training it could increase further but
120:38 - that would be something to have a
120:40 - playground with after session these
120:42 - collab notebooks will stay up for as
120:44 - long as you need them to
120:46 - the last thing we get printed out and i
120:48 - want to switch back to the notes of this
120:49 - is a table of
120:52 - predictions on certain images and what
120:54 - it thinks they are so let's have a look
120:56 - at what it looks like
120:59 - ah this is a good example uh training
121:01 - networks can take a long time sometimes
121:03 - so uh it's a good excuse to pretend
121:05 - you're working while your computer's
121:06 - doing all the hard work for you
121:08 - but it gives you a table that looks
121:10 - something like this
121:11 - it's got an image which is a number one
121:13 - uh it thinks it's a three because it's
121:16 - got it wrong we've given it a picture of
121:17 - a number two oh we've got that one right
121:19 - it thinks it's a two i'm not going to go
121:20 - any further because it's a bad way to
121:22 - look at data this is a much better way
121:25 - to look at data
121:26 - so in this table here down the left-hand
121:28 - side we have a bunch of images which
121:30 - once our network has finished training
121:32 - we are showing to the network and saying
121:34 - what do you think of this
121:36 - can you get anywhere with this kind of
121:38 - image and we'll see that it's done some
121:39 - of them quite well but many of them have
121:41 - done a terrible job at
121:43 - and this is on purpose we're going to
121:45 - get to solving this in a moment this
121:47 - isn't just a demonstration of how bad
121:48 - your networks are it is however a
121:50 - demonstration of how you've got to be
121:52 - careful about the difference between the
121:54 - data you train your network on
121:56 - and the data you apply it to
121:58 - so the data we had up at the beginning
122:00 - was all 28 by 28 pictures which were
122:03 - white ink on the black background
122:05 - now the only one that's whiting on a
122:07 - black background is number five and it's
122:08 - got that one successfully correct
122:11 - but the other images are mostly
122:14 - from a different source they're not
122:15 - handwritten they're different inverted
122:17 - contrast things like that and some of
122:19 - them it's got right but many of them
122:20 - it's got wrong
122:22 - so we've trained the network to identify
122:24 - white on black
122:26 - handwritten digits
122:28 - and it doesn't really behave well
122:30 - outside of that it gives an answer but
122:33 - it doesn't always give you the right
122:34 - answer so it's not reliable
122:36 - i would also importantly draw your
122:38 - attention to the very last row there we
122:41 - have that little dog we saw earlier so
122:43 - if we put a picture of a dog into this
122:45 - network it's going to give us back an
122:46 - answer it's not going to say none of the
122:49 - above
122:50 - it's going to say its best guess is that
122:52 - this looks like a number two
122:54 - maybe its nose has got the right kind of
122:56 - curve to it or something and so this is
122:58 - something you really need to be aware of
123:00 - you need to make sure when you're
123:01 - training your network it's been trained
123:03 - on the kind of data that it's going to
123:05 - be applied to
123:07 - there's a classic example
123:08 - which was mentioned in the course
123:10 - yesterday which i'm just going to post a
123:12 - picture to
123:13 - came up recently and this is the
123:15 - covid cat
123:16 - where a similar kind of situation
123:18 - network trained to look for kovind 19 if
123:20 - you show it a cat
123:22 - a bunch of the networks say that yes
123:24 - that's definitely got covered and even
123:26 - though they have no information really
123:27 - about what's going on inside
123:29 - they were trained on one thing and they
123:31 - are making spurious guesses about data
123:33 - outside of that so you have to be
123:34 - careful about the data you train it on
123:37 - and the environment in which you apply
123:39 - your network think about how general or
123:41 - not general it is
123:43 - jen asks is there a way to get a none of
123:45 - it answer
123:46 - with some networks you can have a go
123:48 - with that and that's what the um writers
123:50 - of the the publishers of that covered
123:51 - cat image have tried to do they've got a
123:53 - none of it example you've got to be
123:55 - quite careful about what you consider to
123:57 - be none of it and how that blends in
123:59 - with the categories you do care about
124:02 - it's a tricky problem to solve the
124:03 - important thing to do is think about it
124:05 - from the beginning think about the scope
124:06 - of data you're training on and think
124:08 - about how that relates to the um
124:13 - data that uh you're going to be using
124:14 - the network on
124:16 - regardless it's it's giving an answer
124:18 - for every single thing it's always going
124:19 - to tell us something and that's a common
124:20 - problem machine learning machine
124:22 - learning doesn't tend to be well
124:24 - designed to give you an i don't know
124:25 - answer it's a hard problem to solve
124:29 - but we can in this situation for these
124:31 - numbers do an attempt to improve that
124:34 - domain that we care about
124:36 - and i think the main difference between
124:37 - the numbers that worked well and those
124:39 - that didn't is that this was white on
124:40 - black and these were black on white
124:43 - the answer to this is doing something
124:45 - called data augmentation
124:48 - basically adding in more training data
124:50 - to represent a larger range of
124:52 - possibilities so the network learns a
124:54 - more general model of what the world
124:56 - looks like
124:57 - if you're doing something to identify
124:58 - dogs you don't just want really nice
125:00 - framed dogs with maybe the good camera
125:03 - you want pictures of dogs made with
125:04 - really bad cameras in dark conditions
125:06 - and snowy conditions all these different
125:08 - things you will have all of those the
125:09 - network can learn the full scope of the
125:12 - world
125:13 - in our case we've got some black on
125:16 - white on black images so we want to just
125:18 - put in some black on white images as
125:20 - well and that's hopefully going to make
125:22 - it able to understand the full scope of
125:25 - handwritten images handwritten digits
125:27 - sorry
125:29 - in general you're going to want to do
125:30 - data augmentation on any network it's a
125:32 - really really powerful thing to do
125:34 - if you can't collect more data then fake
125:37 - it by blurring rotating scaling the data
125:39 - that you do have
125:42 - in our case we are going to invert the
125:44 - images we're going to take our original
125:45 - data set and we're going to add on to it
125:48 - the same data set again but with black
125:50 - turn into white and white turn into
125:52 - black
125:52 - so we multiply it by -1 and then add one
125:55 - to it so that turns one to zero and zero
125:57 - into one
125:58 - and 0.5 stays the same
126:03 - so let's have a go and see what effect
126:04 - that has so go back to that notebook
126:06 - and at the very top in the top cell
126:09 - i made it super easy for you there's a
126:12 - variable that says invert equals false
126:14 - change that into invert equals true and
126:16 - go to runtime run all again and see what
126:19 - effect that has on it you do need to run
126:21 - all the cells again
126:22 - um so make sure it's true with a capsule
126:24 - t because that's what python expects
126:27 - then run time run all
126:31 - while that's running uh mia says this
126:33 - raises the issue of implicit bias
126:35 - absolutely and this is something i'm
126:36 - going to cover on my very last slide
126:38 - about being careful about
126:40 - the kind of things you train on and the
126:42 - kinds of questions that you're
126:45 - asking your computer to answer it's not
126:47 - as simple as throwing the numbers in and
126:48 - get a number out you have to think about
126:50 - the human in the loop for example
126:54 - so that's all run through it's currently
126:55 - fitting
126:58 - it's finished
126:59 - and it's doing the analysis
127:01 - and this is again going to print out
127:04 - our table of results
127:07 - and i'm going to skip that table of
127:09 - results
127:10 - and show you in a better format
127:13 - umea asks how to evaluate bias of a
127:14 - network well it depends on what kind of
127:16 - bias you're talking about
127:18 - one of the primary biases you get is
127:21 - sort of a selection bias where you've
127:23 - only trained your network on a subset of
127:25 - data and so it's only going to know
127:27 - things about that subset of data or
127:29 - sampling bias as well i suppose it could
127:30 - be as well
127:32 - so
127:33 - the way to do that is to apply try using
127:36 - your network on data that represents
127:38 - where it's actually going to be used and
127:40 - make sure it's performing well on that
127:42 - whole set
127:43 - but we see here it's worked well
127:46 - simply by showing it inverted color
127:48 - images it's able to generalize well
127:50 - across data which wasn't even part of
127:52 - the original data set the number one is
127:55 - an inverted image but two three and four
127:57 - are computer generated numbers they are
127:59 - you know printed to the screen they
128:01 - aren't hand written but it's still able
128:03 - to recognize them we trained it on
128:05 - handwriting it can recognize
128:07 - non-handwriting which is
128:09 - perhaps surprising and it's not
128:10 - something you should rely on we should
128:12 - add into our training
128:14 - examples like these if we want it to
128:16 - perform well
128:17 - we see how see here however that it is
128:19 - performing reliably on these examples
128:22 - it's not only saying that there's a high
128:23 - probability of it being a two
128:25 - it's saying there's a low probability of
128:27 - it being anything else and that's a
128:28 - useful thing to see
128:30 - all the way through it's doing very well
128:32 - 97 is one of the lowest we get 71 there
128:36 - probably because of the low contrast on
128:38 - the number one so we want to add in
128:40 - maybe some more images with you know
128:41 - washed out colors and that's going to
128:42 - help the training
128:44 - the number nine here isn't handwritten
128:46 - or computer generated there's noise
128:48 - around the edge because this is actually
128:49 - a photograph of someone's door number
128:52 - and so it does a bad job it thinks it's
128:53 - a zero because he's getting confused
128:55 - again if we want it to recognize door
128:56 - numbers we need to train it on door
128:58 - numbers
129:00 - once again we see the dog has given us a
129:03 - wrong answer
129:05 - you see the probabilities are better
129:06 - spread out so you might not in this
129:08 - situation say well 23 is the highest and
129:11 - so therefore it's a number eight you
129:12 - might want to say
129:14 - none of them stand more than a certain
129:16 - amount amongst their peers and therefore
129:18 - it's an i don't know that's one of the
129:20 - ways you can say i don't know because
129:22 - there's no one conclusive answer but
129:24 - it's up to you to think about the
129:25 - thresholds and the statistics and the
129:27 - probabilities of these things to make a
129:28 - decision about where that threshold
129:30 - should be put but this hopefully shows
129:33 - you the power of data augmentation the
129:34 - stuff that you can do just by using the
129:36 - same data tweaking it a bit adding it
129:39 - into the mix you suddenly get a network
129:41 - which is way more powerful and way more
129:43 - general
129:46 - so in summary
129:47 - these are the things you need to think
129:49 - about when you're training the data so
129:50 - here we had on our first example we had
129:52 - the number nine the door number it
129:54 - doesn't represent the training data set
129:55 - so if we want it to we need to get some
129:57 - pictures that do we need to add them in
129:59 - if we want to make our network perform
130:01 - better we want to do more data
130:03 - augmentation we may want to rotate
130:06 - images around or flip them or blur them
130:08 - or
130:09 - change the colors or delete parts of
130:11 - them you mess around with them to
130:13 - represent lossy data and hope
130:16 - that the lossy data we apply is a good
130:18 - approximation of the kind of lottiness
130:20 - that exists in reality
130:23 - if you can
130:24 - get a larger base training data set if
130:26 - you can't if you don't if you
130:29 - if you want to do the best job then
130:30 - getting more actual data collecting real
130:33 - data is going to do the best job if you
130:34 - go out and collect more photographs and
130:36 - get more pictures of handwriting that is
130:38 - going to help you a lot if you can't get
130:41 - real data then data augmentation is
130:42 - going to help on top of that
130:45 - if it's taking a while to give you a
130:47 - good network then just train it for
130:48 - longer you can in general keep on
130:50 - training and it will keep on getting
130:51 - better
130:52 - but bear in mind keep an eye on the
130:54 - validation accuracy and the loss to make
130:57 - sure that it's not over fitting against
130:59 - the training data set
131:01 - the more data you have that you're
131:02 - training on the less likely it is to
131:04 - overfit as well so that's also a benefit
131:06 - you get
131:07 - and also there's loads of numbers that
131:08 - we've included in this process the batch
131:10 - size the learning rates the dropouts the
131:12 - kernel size all these things we put in
131:13 - there
131:14 - play around with those numbers make your
131:15 - network larger or smaller see how they
131:17 - affect things understand the effect that
131:20 - they have on your eventual result
131:25 - so the last thing i want to talk about
131:27 - is something which comes up a lot when
131:29 - you talk about machine learning i think
131:30 - in a research context it's very
131:32 - important as well because as researchers
131:34 - we have a sort of responsibility to be
131:37 - honest and
131:39 - good about how we're approaching things
131:41 - now machine learning and your networks
131:43 - in particular do have a bit of this
131:44 - black box problem you put numbers in you
131:47 - do an algorithm you get a number out and
131:49 - it always gives you some kind of answer
131:52 - and it's very easy to end up in a
131:53 - computer says no situation here where
131:55 - you're creating a network to decide
131:57 - whether someone is deserving of getting
131:58 - a loan for example so you put in the
132:01 - description of them to do with their
132:03 - social socioeconomic background and
132:05 - where they live and how what their
132:07 - income is and it gives a number like yes
132:09 - or no a result like this one no
132:11 - and
132:12 - you don't have any recourse there it's
132:14 - just a machine giving you an answer so
132:16 - you need to think about is that a
132:17 - an appropriate thing to do as a society
132:21 - but inevitably machine learning is
132:23 - becoming more important it's taking over
132:24 - everything it's cropping up all over the
132:26 - place and so you need to think about
132:28 - how you can do this stuff in an honest
132:30 - way now google have a set of ai
132:32 - principles and i've grabbed the few of
132:34 - them from here
132:36 - i also wouldn't say that necessarily
132:37 - google follow their own ai principles
132:39 - but it's nice to see them written down
132:41 - somewhere and we can choose to follow
132:42 - them
132:44 - so the first thing i think one really
132:46 - useful for researchers especially those
132:47 - that publicly funded is to be socially
132:48 - beneficial don't design a new network
132:51 - which is going to
132:53 - not give loans to people of color or
132:55 - that's going to make a missile kill
132:57 - someone or anything like this design
132:59 - stuff that is going to be good
133:01 - be good people be nice be helpful
133:05 - avoid creating or reinforcing unfair
133:07 - bias there's a classic example with the
133:09 - um
133:10 - microsoft connect their their xbox
133:12 - controller where they design some
133:14 - machine learning and networks to be able
133:16 - to identify people standing in positions
133:17 - they can dance around and control the
133:19 - machine
133:20 - when they took it on tour to all the
133:21 - game shows people from the public were
133:23 - coming along and having a go and it
133:24 - wasn't seeing them it wasn't seeing
133:26 - their faces and it turned out it was
133:28 - doing a bad job of seeing them because
133:29 - they hadn't trained the networks on
133:31 - people with a diverse set of skin colors
133:34 - they trained it on the people who were
133:35 - the engineers at that particular group
133:37 - in the company and they were mostly
133:38 - white and they'd failed to identify the
133:41 - broad spectrum of people that would be
133:42 - using the technology
133:44 - and that is an unfair bias for sure
133:47 - make sure stuff's designed for safety so
133:49 - have things which fail safely
133:51 - if it's unsure about something er on the
133:53 - side of asking a human and
133:56 - including people in the loop as well
133:59 - make sure you don't fall on the side of
134:00 - i'm not sure so let's not give them a
134:02 - loan and you know subject them to a
134:04 - lifetime of misery or whatever think
134:06 - about how it's going to be affecting the
134:08 - humans in the process
134:10 - be private with things it's easy when
134:11 - you're designing a network to
134:13 - accidentally have the eventual network
134:14 - containing personal information
134:17 - you might have a weight in there which
134:18 - accidentally represents the age of a
134:20 - particular person you've trained it on
134:22 - and there are techniques statistical
134:23 - techniques which you can use to extract
134:24 - that out so be really careful about the
134:26 - data that you're training it on and how
134:28 - you're going to incorporate privacy into
134:30 - that
134:31 - and as many of us are researchers or
134:33 - scientists
134:35 - uphold high standards of scientific
134:36 - excellence this means be honest about
134:39 - how you're training your data set be
134:41 - honest about the testing you've done on
134:42 - it make sure you honestly split your
134:44 - tests and training data sets make sure
134:47 - you've analyzed the results in a
134:49 - thorough and comprehensive way be good
134:51 - citizens and be good scientists is a big
134:54 - summary coming out of this
134:56 - but that's the end of the session today
134:58 - thank you all for your attention i know
135:00 - it's a lot of information in three hours
135:01 - and i really appreciate all the
135:02 - brilliant questions we've had so thank
135:04 - you all very much and i hopefully will
135:06 - see you again soon

Cleaned transcript:

so the purpose of this course and the place that this course started from was we were noticing at the university that there were lots and lots of people applying for grant money wanting to start new projects getting interested in trying to solve new problems and there was this buzzword flying around of deep learning and machine learning and we realized that a lot of people wanted to take part in this new fad but didn't really know what it actually involved they would just write it down in their grant application and say well i'm going to do some deep learning as part of this and then when actually came to it they didn't really necessarily know the full extent of what was possible with it and so this course started from my attempt to educate and inform people about what deep learning really is how it works from the lowest levels and the kinds of problems that you might be able to use it to apply to so this really is an introduction to deep learning we're not going to be assuming any knowledge of neural networks any knowledge of how that kind of data science stuff works and we're trying to cover all of that if you came to my course yesterday applied data analysis in python and then we're going to be covering a few of the same topics but 90 of it is going to be completely separate so let's start from the basics this course is called introduction to deep learning but before we can get to deep learning we need to understand the context about how that fits into the larger scheme of things so machine learning is where i'm going to start and that's a lot of what we covered in the course yesterday but at its core machine learning is any technique which uses computers to discover patterns of information so it's what would traditionally have been called statistics but because using computers to do it it allows you to get greater reproducibility and most importantly for some of the stuff we're going to be talking about today much greater scale you can go to much larger amounts of data much larger depths of information you're trying to seek out and that's what differs machine learning from what would traditionally be called statistics is this scale of what you can do with it now machine learning sits inside the wider field of artificial intelligence artificial intelligence is a very poorly defined term it basically means anything where computers are trying to make decisions so that can be something as simple as trying to make a decision as to what the value of y should be given x that's a very very narrow form of artificial intelligence all the way up to selfdriving cars and potentially according to scifi anyway well beyond intergeneralized artificial intelligence but within artificial intelligence this is machine learning which is a better defined field where it's about statistics and data science and things like this but of course even machine learning being a narrower field than artificial intelligence is still incredibly broad and is full of loads and loads of different techniques and concepts in the course yesterday we covered things like kmeans clustering for doing uh unsupervised clustering algorithms as well as linear regression and correlation studies so these were some of the tools you'll apply to your data and what we're learning about today on a much larger scale is deep learning and neural networks around that so inside machine learning which remember is sitting inside artificial intelligence it's a whole bunch of different things yesterday one of those we looked at is a linear regression it is a very simple concept it got across you've got a scatter plot you draw a straight line through it that is nonetheless machine learning because you're using a computer to discover what the values of the gradient and the yintercept are going to be slightly more advanced technique is something like kmeans clustering it's trying to discover things about your data without you having to explicitly tell it where you want to end up there are hundreds maybe even thousands of different algorithms that sit inside machine learning variations therein obviously we don't have time to go through all of those possibly one of the most famous examples of machine learning algorithm particularly over the last 10 15 years or so are neural networks now neural networks have been around a long time they've decades and decades new networks have been around and at their core they're a relatively simple concept but over the decades as computers have got more powerful and the kind of questions that we want to scale up have got more difficult we've had to become clever about where how we apply neural networks to try and solve the problem and so for a long time neural networks were relatively constrained in the kinds of complicated problems they could solve but since about 2010 the last 10 years or so there's been a second wave or maybe even third or fourth wave depending on how you measure your history about how you can create new networks which are what is called deep now i'm going to come across shortly what i mean by a deep neural network but it's a specific structural shape of a neural network which has been managed to be solved by applying more advanced statistical techniques by very very clever mathematicians and it's this resurgence of neural networks which are deep which has led into the modern fad last five years or so perhaps in research of deep learning so deep learning is applying a deep neural network to some kind of problem so i've given you artificial intelligence i've drilled down into machine learning and from there we've drilled down into neural networks so this course is titled an introduction to deep learning but before i can really give you a sense of what deep learning is and why it's interesting we first need to make sure we understanding understand what a neural network is at their most basic elementary level how do they work because the terminology and the few bits of maths that i'm going to be putting in today are important to understand how the problem scales up and when you're wanting to use these things how you can solve the problems that you're going to come across when you're trying to get stuff done so a neural network is a collection of artificial neurons so the place we need to start before we think about a network is to think about the subset the things that make up that network the nodes that connect together and those are neurons and before i go any further i need to apologize and say that i am not a brain scientist i have no idea how the human or animal brain works so if anyone in the chat here does you might want to cover your ears for a second but at their core a neural network is modeled after a real animal brain a brain is constructed by having a whole bunch of neuron cells all connected together and all connected together but connected together in some kind of network and as inputs and impetuses come in from the outside world electrical signals are triggered off in some of the neurons and then depending on which neurons they're connected to and the conditions under which those following neurons are going to activate you get a kind of ripple effect going through with different scales and sections firing and other parts not and it's that complex interplay of all these different neurons which is what gives us intelligence and now the idea within your networks was to take a simplified model of how that works some kind of mathematical description and see if we can try and solve some of those same problems that the human brain can solve so from this point on you can forget all about the animal brain pretty much because everything i'm saying isn't really how nature works but it is how our mathematical model inspired model is going to work so an artificial neuron which is designed after the idea of what a real animal neuron is like has multiple inputs coming into it and it passes output out you can think of it as like being a little function with a bunch of inputs and one output at their most simplest the function that a newer one encompasses is adding together all the inputs coming into it and then putting the output out the far side it's slightly more complicated than that because it doesn't just take each input it takes the first input and multiplies it by a number adds that to the second input multiplied by a different number and adds that to the third input multiplied by a different number and so on and so on depending on how many inputs a particular neuron has neurons can have potentially zero inputs but realistically the minimum would be one they can have thousands of inputs but at its core it is this bit of maths here which is what is going on inside a newer one when you're passing information through the network it's the sum over the input values and their corresponding weights and notice that each input value x i has a w i each input has its own weight if there are 10 inputs to a neuron there are going to be 10 weights also bear in mind that every neuron is going to have its own set of weights so if there's another neuron somewhere else in the network its w1 will be different to this neurons w1 so you can imagine if you've got a network full of 100 different neurons and they're connected together you're going to end up with millions of these weights all connecting together and trying to give you some kind of realistic output the one other thing to be aware of in this simplified model i just described is there's an extra step which happens after this sum here is calculated p it then gets passed through an activation function now an activation function in nature will be something like don't trigger the output until the sum here is above a certain threshold but because we're doing maths here we can do absolutely any activation function we might want there are various reasons for choosing one activation function over another but often on the whole they'll do something like if the sum is negative then just ignore it and if it's positive pass it on through so before i go any further i'm going to ask if there are any questions about how this part of it works how a single newer one is interacting before we then think about how we connect it through into a network right so there's a question there from yan is there reason for using a linear combinations of inputs only i think because it is the easiest thing to do mathematically it is a simple function you can apply which gives you a an effective result you do want to pass it through an activation function because activation functions can give you extra bits of magic so if your whole network was just linear combinations of inputs and weights then if you add a whole bunch of linear sums together what you get at the end is a linear sum and if that was the case the most powerful thing a network would be able to do would be to evaluate some kind of linear relationship between the inputs and the outputs by having the activation function at the end that allows you to encapsulate nonlinearity in the thing you're trying to model so you can keep the core of your model simple and then allow for nonlinearity by passing it through this activation function and that gives you the space to explore more interesting models yvette asks so the weights associated with the neurons not the inputs yes that's the easiest way to think about it think about each neuron having a bunch of pipes coming into it and on each of those pipes it's got a valve which it can use to multiply the input or divide the input by so each neuron has inputs coming into it and it has its own uh value that is being multiplied by to decide how that neuron is going to treat that particular input sharia asks can we use nonlinear combination as well in in principle a neural network can do whatever you want there's no reason why a neural network has to have a simple function like that and in fact in reality there might be neural networks out there which have more complicated um summing functions inside there's no reason to think that it's necessarily this but at least conceptually it's the easiest way to think about it reality might be more messy but conceptually consider it as a sum over the inputs and the weights multiplied together reality might be more difficult but until you start having a full degree in neural networks and deep learning you won't have to worry about anything more complicated than that in order to get your job done at the end of the day here we want to learn enough to be able to use these things and understand how to fix them we don't necessarily for today at least need to understand the full mathematical description of how it all works okay so i'm going to carry on to the next slide now so as we go through do feel free to pipe up and ask any questions you might have even if it's on a slide that we've already been through that's fine i'm happy to reiterate over things because you might think oh yeah i understand this and then i mentioned something in two slides time and you realize i really didn't understand that i obviously misunderstood so do feel free to ask as we go so now we've got our description of a single neuron we're now going to see how we can connect them all together now again it is possible in principle to connect your network together however you want you could just turn it into a big net of things connected together in all sorts of directions but because we want our networks to be able to be understood by humans to some extent we want to be able to design them and understand what the different elements of it do there is a general structure which most neural networks follow now most simple neural networks anyway the complicated stuff comes from people breaking these rules and discovering new and interesting ways of structuring net networks but certainly for example today our network's all going to be this kind of structure which i'm going to describe now and that is you start on the left hand side of this picture on the input layer and this is where you take your values that you want to put into your model so uh an example that we used in the course yesterday was looking at the price of a house so you might go out and measure how many bedrooms a house has how large its garden is and how close it is to the nearest school for example so those are three quantitative measurements that you can make so those would be the values that you put into each of these three neurons on the left hand side now note that these three new ones on the lefthand side kind of don't have inputs that's because that's where the data is going to start from so the lefthand side is where we put our data in now each of these neurons on the lefthand side is connected to every one of the neurons in the next layer that doesn't necessarily have to be the case a new a neural network where everything is connected to every other one is called fully connected but it's perfectly reasonable to have some neurons on the first layer connected to only some on the next in whatever combination you see fit but you can see looking at this um second layer here the first one that's got two neurons on it it has each of them have three inputs coming into it and they have an output coming out now looking at the output coming out of the top neuron here you see it's connected to two neurons in the third layer that's not to say that this neuron has two outputs it's simply that that one single output is being passed on to both of the neurons in the following layer and each of those neurons in the following layer will have their own weight that's being applied to the output of the first mural on here this process just gets applied all the way through you work out the first layer you work out the sums and the activation function you pass into the second layer you add up the sums use the activation function you pass it through to the third until eventually you get through to the output layer when you get to the output layer this is your result this will give you some kind of number because at the end of the day we're doing maths it's all going to be numbers we'll end up with a number at the end and it's up to us to then interpret what this number means in the context of the question we're trying to ask but overall this is the structure that they all have they have an input layer on the left hand side i say left hand side it's all virtual but i think of it going left to right if you find it easier to think right to left that's perfectly fine input layer on the left hand side a single output layer on the right hand side and then some number of hidden layers in between in this example here we have two hidden layers each of which have two neurons in principle you can have any number of hidden layers you like with any number of neurons in each the thing that makes a deep neural network deep is having lots of hidden layers that's pretty much the definition of a deep neural network and we'll see later on why that's a useful thing to have great so i was asked if anyone's got any questions there's already one from sumet so how many layers and how many neurons should we have it depends on the problem you're trying to solve so i'll get on to how you go about designing your networks later so i'll go through this in a bit more detail but the number one rule i would suggest is find someone who knows what they're doing who has solved a similar problem to what you're trying to do and use the same structure as they have that is going to get you 90 percent of the way after that you can feel free to tweak it but by using preexisting preexisting and published network structures that's going to help you shuya asks how many layers is considered deep it's that's one of those questions where there's no real answer i would say as soon as you've got four or five you're getting to the point where networks would have struggled with in the past once you're at 100 layers you're definitely in the situation where you've got a deep neural network going much beyond that you're going to have to start bringing real firepower to be able to evaluate anything to do with the network but i would say rule of thumb you can start asking answering those really interesting deep learning questions by the time you get uh sort of six seven eight nine ten and beyond though as with all things the benchmark kind of shifts over time what might have been considered a deep neural network ten years ago might not be considered quite so deep these days and tim asked what's the precise definition of a layer here so there's no one universal definition of a layer which can capture every single different situation but under the structure of a network we have here where we take all of our neurons and we put them into boxes where they are lined up on top of each other such that in one layer in the in one box on the left hand side here they are only connected to neurons in the next layer and in this next box here these are only connected to neurons in the next box that allows you to have a an absolute definition of the ordering of the neurons which lets you decide absolutely what layer each neuron is in and therefore how many layers you have of course because these are just networks and you can in principle do whatever you want you could take the output of the third layer and connect it back to the input layer if you wanted and do something with that at which point it's harder to define what's a layer however you can do a lot of really interesting stuff without having to cycle through so the networks we're working today are what are called a cyclic ie they don't have cycles they are simply feed forward from the left hand side over to the right exactly tim that's a good way of putting it it's only it's at the neurons on layer n only have inputs from layer n minus one knitting asks more layers lead to overfitting absolutely we're going to cover one of the main techniques well actually two techniques which you can use to reduce the chances of overfitting as we go through okay i'm going to carry on so these are some of the questions you've been asking so it's good you've been um you've been uh asking intelligent questions before i've even got to it so a question you've been asking is what shape should it be so there is some art and some science to this there are there are rules of thumb if you've got a certain number of input layers if you've got 10 input parameters then your hidden layers should to an order of magnitude be about 10. there's no point in having an input with 10 inputs and then the first hidden layer being 100 million different neurons that's not going to extract any useful information if you've got 10 inputs you're not going to want more than 100 in the next layer realistically anyway this is one of the things which you can assess using mathematical techniques which we're not going to cover in this course because they're very complicated and also a certain amount of trial and error and data science applied to the shape of the network also as i think someone mentioned the chat the number of hidden layers kind of relates to how complicated a question you can answer every hidden layer provides a layer of abstraction so if you're trying to start from some very um generalized information and to pull out a really specific answer you're going to need lots and lots of hidden layers to work through the layers of questions that the network is implicitly answering so if you've got a hard question you're gonna need a deeper network if you've got an easy question you can get away with one hidden layer for simple things it doesn't need to be a complicated situation the flip side of this is the deeper the network and the larger the network the harder it is to train that means it's going to take longer to train you're going to be more risk from overfitting and we're going to see how this sort of balance comes through as we go through the course today the next question that you might have had sitting in the back of your head but you haven't worked out how to formalize it yet is i've been talking about the neurons and the inputs coming in and how they're going to be multiplied by some kind of weight and the question you hopefully have is how do you know what that weight should be because at its core there are two things which describe how the network is going to answer your question the first is the structure of it which neurons are connected to which how many layers how many neurons these kinds of things and the second part of the problem is the weights that are being applied on each individual neurons input so the first part we decide and we design upfront we describe a network we say this is the thing that's going to solve our problem but for the weights we have no way as a human of working out what these millions of different values should be and so this is where we use computers this is what makes it a machine learning algorithm because we are going to use a mathematical process on the computer to discover what the values of those weights should be for the particular problem that we're trying to solve this process is called training and it effectively works by showing it loads of examples and it ends up giving you something which can replicate the effect of those examples so question there from ali how can we decide how many layers we need again i would say that the number one way you should discover how many layers you need and the shape of your network is to think about the problem you're solving find someone else who solved a similar problem and start from their example if you're doing something where you're trying to identify um heart disease in uh ct scans of a heart tissue then you might start from someone else who's done the same thing with liver disease or something like that so it's something working with 3d data you're looking for certain features in the data you don't have to start from scratch design your network you start from someone else who solved a similar problem and has analyzed how it works and you start by using those if you can show that it performs well then you've succeeded and you can use that to do the things you want to do if it doesn't perform well then at that point you have to start talking to data scientists and experts and trying to work out how to go from there so to dive into that question there how do we go about training the neural networks what is the process by which the values of the weights get assigned now again there's lots of potential algorithms you could use here there's lots of different ways that you can decide to use a computer to work out what those weights should be but the main technique that's used is something called back propagation now again back propagation is a technique that was applied to neural networks quite a long time ago and in the preceding times in the time since there's been lots and lots of nuances and changes to it and replacement techniques but that propagation at its core as to how it works in principle is still pretty much how neural networks are trained even if there's lots and lots of clever hacks applied on top of it and so to do this training technique you need three different things pretty much you only need two of them but the third one isn't going to do very good data science without it so first of all you need a training data set you need a bunch of examples where you've got the inputs to it using the examples from before a bunch of houses where you've measured the size of the garden counter the number of bedrooms and measure the distance to the nearest school and for each of those examples you have a label assigned to it you've said how much is this house worth so that is what we mean when we say a labeled training data set for checking how well our network is performing we also need a labeled test or evaluation data set now it's important that your training data set and your test data set are distinct and disjoint to make sure you avoid overfitting now we're going to cover exactly how overfitting feeds in a little bit but we did cover it in our course yesterday as well but you want to make sure you have two distinct subsets of training data they can look the same but you should choose randomly about 80 percent of your data to be used for training and then keep aside about 20 to check how well your network is performing and for the back propagation algorithm to work you need to start off with some set of initial weights so now i'm going to go through and show you how these different things can be designed collected and evaluated before i do this two questions so prataps asking that the same output can be achieved by multiple combinations of inputs across layers how is precise connectivity and weights decided so you're right you can have repeated information existing inside network you might have two parts of your network which are deciding the same piece of information in which case you've got more neurons and weights than you really need there are techniques you can use to try and find correlations between weights as they progress and that gives you some sense as if you might be able to prune your network or reduce your network down if it's affecting the ability of your results to be accurate then you're going to want to do that pruning if it's not then you don't so the precise connectivity is hard to do connectivity is generally done on a course layer it's called course level although it's of course possible to apply specific techniques to prune parts of it as for weights we're going to show you now how this particular weights are decided and it does feed into that question you were just asking and yan asks is training always mandatory with neural networks realistically yes if you have an offtheshelf neural network which has been trained to identify the difference between cats and dogs you don't have to do any more training on it you can just use it but if you start with the empty structure you need to do something to work out what the values of the weights should be and to work out what the weight should be you have to train the network so starting with the initial weights lots of different opinions and techniques as you can use to start off your network with some kind of values but the easiest way to do and the conceptually the thing that we're going to do is setting all of your weights randomly there are ways to be smarter about it but for today let's just assume that they're all being set completely randomly so every neuron is having inputs coming into it and it's multiplying each of those numbers initially by a random number and we're going to train it to try and finetune those random numbers in the direction of something which is useful and then for the test and training test data sets so we're going to need two so we need one as i said to train the network on so we can learn about stuff and the other one is going to tell us how well it did it's important to separate your training and test data sets to avoid overfitting and to give a bit more information about why that's the case it's because on your network inside it has got tens hundreds millions potentially of different weights and if your data has fewer degrees of freedom than that then your network is able to learn potentially every single piece of information about your data that you're showing it when it's learning it'll be possible for every single newer one to each take their turn and remember one particular input into the network and so as you're going through it's going to start looking like your network is able to perfectly replicate the training data that you're showing it and that's because it's remembered every single little nuance up and down of the data you've been sharing it by keeping some of that data aside for training for testing at the end even if your network has learned every single little detail about your train data set it's not going to have learned any of the nuances of your testing data set your testing data set is therefore not going to have those same ups and downs and so when you show it to the network it's going to behave very very poorly it's not going to do a good job and so the dance you need to get is the balance between getting your training data set behaving well while also keeping your testing data set still also performing well the common split you generally see is about 80 for training and uh 20 for test but depending on how much data you have you can jiggle those numbers around and there are also more advanced techniques things like kfolds where you end up using all of your data for training but you split it into different subsets and you repeat it over and over again and then you aggregate it at the end in a clever mathematical way um says by disjoint do you mean statistically what i mean is you want of all the samples you've collected when you divide your set into two halves you don't want any overlap between the two they should be sampled from the same distribution is one way of thinking about it but they should be separate samples from that same distribution trey asks is it ideal to set weights randomly if there's repetition will it not be wasteful possibly yes you random setting your initial weight isn't necessarily the thing to do there are clever ways you can work out what your initial weight should be set to but as far as us as researchers using neural networks to solve problems we're going to leave that to the software to decide and they're going to do a better job than we would be able to decide and a vet suggests that you need a massive data set to do useful stuff you generally do you do need a lot of data to train in your network however if you're answering a simpler question and therefore you've got fewer neurons and fewer weights you can get away with less data we'll see an example in a minute where we only have around 150 examples and that's enough on that particular example to answer the question well on later examples you might need i i i generally think a thousand examples is a good kind of rule of thumb for neural network for most problems but having more than that is always going to help one of the problems with data science and your networks is having that data that's nice and clean and well prepared so i'm going to talk briefly about the matsy bit and this is probably one of the last bits of math on the screen so i'm not going to be using any maths that's beyond a level but if anyone wants a bit of clarification i'm happy to explain it in more detail but back propagation is the process by which we assign weights to the network it's an algorithm which we use to decide what those weights should be it's an iterative algorithm so we're going to apply this over and over again and over that time the weights are going to get closer and closer to that which the algorithm considers to be optimal in some way but before you can apply the algorithm you need to start off with your network structure so we've decided how many inputs we've got and outputs how many hidden layers and how many va neurons we have in each hidden layer we need to have some initial weights we need to have somewhere to start our iterative algorithm from and you need to have a training data set you need to have some examples you can show to it and those examples are going to inform how the weights progress as the algorithm is applied now there's lots and lots of different ways you can do the training but the core of how it's generally done these days is using backpropagation and this is a very simplified description of how backpropagation works so you need to start off by looking at the structure of your network looking at the inputs and at the outputs i need to look at every single weight in your network so every single input to a neuron is going to have its own individual weight you're going to have maybe thousands of potential weights and you need to work out how much do you need to change each weight by to affect the output of the whole network by a certain amount if i change a particular weight by a small amount is the output of the network going to only change by a small amount or is it going to change by a relatively large amount if a weight has a disproportionately large effect on the output of a network then the derivative of that weight is a large number if that weight only has a small effect on the eventual output of the network then it's said to have a small derivative and it's only going to have a smaller effect so working out this derivative independently for every single weight and assuming they are independent which largely works in the situations that we're talking about here this gives you a list of derivatives one for every single weight in your whole network every weight will have its own dn its own derivative so this is telling us how much you need to change that weight by to have a certain effect on the output note also that these derivatives are signed so it could be that if you make your weight slightly larger the output goes down and that would be a negative derivative for example so all this information is captured in a big list of derivatives that's stored alongside the network so that's the first step that happens once up front before you start doing any training because the networks we're looking at those derivatives don't change over time what you then do is start the training process and this is where you repeatedly do these steps over and over again and the core of how this works is you take a training entry which has for example three values the number of rooms the size of the garden and the distance near a school three numbers you put it in the left hand side of your network you then work out what the output to that layer is going to be pass it into the next layer that layer is then going to take if you take each of the inputs it's going to multiply them by its initially random weight it's going to do that all the way through right on the other side so the weights are initially set randomly and you're going to get out the far side a random number effectively you're going to get some kind of result it's going to be a numerical value between minus infinity and infinity but you're going to get a number out what you then need to do is look at how far away that random number you got is from the true number maybe the true value of the house was two hundred thousand pounds your network gave you a random number of zero 000 pounds you know you're there for 200 000 away so you then need to look at all of the weights and adjust them by how much they need to change to make the nut the result you've got out that network move in the direction of the correct answer now you don't move it all the way in one go you don't jump the result up by two hundred thousand you move it by a very small amount so that you can slowly move in the direction of something which is going to get you the right answer so after this you go through all of your weights and you update each of them by a small amount by looking at what their derivative is with respect to what the how wrong you were was scale that down a little bit to try and slow things down and then you do the same thing again you show it another example and this means that the more wrong the weights are the more they work towards the answer and as they get close they slow down and start propagating towards a settled result the idea being after this magic if you put in three values which represent a house it is going to give you back something which is going to make a good guess as to what the value of that house should be so i'm going to answer a few questions from chat before i move on nissan asks how's it going to stop and what's ideal you keep on running until your training data set is looking your testing data set sorry you keep on going until your test data set looks like it's giving you a good result at its core there's more nuance than that but that's the basic answer you keep going until your test data set your validation set is saying this is looking good this is answering the question quite well connor asks an insightful question in a network with a huge number of weights wouldn't analyzing them individually be an inefficient method of tweaking indeed so the way i'm describing it here is how you would do it if you were doing this on paper in reality uh modern techniques represent the network as a huge multidimensional tensor and you just do tensor arithmetic on it and it has the effect of doing this simplified maths and by doing this tensor arithmetic you get nice fast results because hardware can be designed to do tensor arithmetic very very quickly canal asks how should we determine the learning rate use the defaults of the software that you have in front of you they will generally have a default learning rate which works well if you find that it's behaving poorly as it's training you can tweak that learning rate and there are in fact algorithms and in fact we're going to be using one today where the learning rate adapts automatically as it goes through and roy white asks is this essentially manual optimization in a way yes but we're doing it using a computer and so it can do it a lot faster than we can you can imagine looking at each individual thing tweaking up and down seeing how it affects the answer tweaking the next one and so on and so on we can do it all in one go using the network and it becomes scalable and michael asks does this always converge no it doesn't there's no reason to assume it's always going to converge it's an extremely complicated mathematical object and even if it does converge it won't necessarily converge where you want it to at its core most neural network training methods are doing a form of gradient descent and you can imagine that in twodimensional way on a big plane covered in hills and valleys and you're trying to find the lowest point in that plane so you start by rolling a ball down a hill it's very possible in that situation to end up in a valley that's near where you started that is not as deep as a valley which is further away so that you've ended up in a local minimum and you haven't ended up in the true result so there are techniques you try and use using randomness and adaptive learning rates to try and avoid getting stuck in a local minimum so to reiterate what we were just talking about there here is a much uh cuter example than a house here is a lovely picture of a dog imagine you can extract some properties of this photograph of the dog maybe we add up all the pixels and look at their red green and blue values and so we end up with 20 000 input neurons for example one for each pixel however you want to describe the picture we start with a random network we put all the numbers going through so all the pixels end up in this input layer on the left hand side the three new ones we then pass it through to the next layer and the next layer the next layer of course we need a more complicated network than this but we would still get an answer with this network we run this example through and we get some kind of answer it's randomly weighted to start with and so it says something like 37 or 0.37 which we decide means is 30 set 37 dog and 63 cat we know that the truth in this example because all of our examples are labeled is that it is 100 dog and a zero percent cat and that means that we are 100 minus thirty seven percent are the 0.63 percent wrong and so we use that 0.63 to work out how much we should change all of our weights by it's a positive number so we should generally multiply all of our weights in that equation by a positive number and that's going to slowly tweak our network towards an answer we're going to do this thousands of times each time taking a really really small step and that's hopefully going to over time push the network into configuration where it's able to answer questions similar to that one that we started with uh alastair asks are there ways of assessing the rate of conversion as a proxy of the confidence that the conversion is generally a general rather than a local optima in principle you can but it could just be that you happen to be in a flat bit of the terrain at that particular time you could start off in hill country out in the distance then have to go through a large flat area before then ending up in a really valleyish area in the middle of the country so you can't necessarily assume that because it's converging slowly that you've ended up in the right place you have to use clever techniques you have to be careful to not end up assuming that you've ended you've solved the problem before you have and as lester says there you also start off in lots and lots of different initial places and you see if any of those are going to push you towards an answer i've been talking through so far effectively how you would do this by hand you can imagine that you could sit down with a bit of paper lovely picture of your man good job um you imagine you could sit down with a piece of paper draw your network out work out what the derivative of all the weights are and look at the numbers of the inputs do the maths do the inversion and keep going and keep going as i said the way this actually works is by doing this using very very large tensors using gpu accelerated hardware all this kind of thing because the scale to which you can get is not is not coverable obviously if you're doing it by hand you have to use computers which is why it's a classic machine learning algorithm again we also don't write the software by hand to describe the network and to do the back propagation and to do all these things we use software that exists already to do that work for us we're not going to spend five years trying to compete with google or facebook with their techniques we're gonna use software they provided which is shown by the whole industry to work really well now there are lots and lots of different pieces of software out there which can provide a way of describing a network of training a network and evaluating a network and some of the most popular ones are well the two most popular are pi torch and tensorflow so pytorch i believe was originally produced by facebook and tensorflow was produced by google they are both fantastic neural network libraries which do all the bells and whistles you'll need for any kind of complicated network in the course today we're going to be using tensorflow but um pytorch works brilliantly well so i'm not going to say anything bad against fighters there's also a package called karas which isn't itself containing any algorithms for doing the training it is a wrapper on top of the other packages to give a nicer way of describing the problems you're trying to solve it's a bunch of niceties and addons basically and so what we're going to be using today is keras's nice addon layer on top of tensorflow there's also a package called cafe 2 which isn't getting quite as much publicity these days but it and its derivatives are still thought of very well and they do a good job and finally psychic learn so yesterday in the applied data analysis course we use psychic learn for doing other machine learning algorithms it does have a neural network a whole set of neural network packages and modules built in but it's not going to be anywhere near as performant as things like pytorch or tensorflow it's not going to perform very well for your very very deep neural networks and it's not going to take advantage of the gpus in the way the other packages would but for exploring and playing around and getting started psychic learn works perfectly well and you get the advantage of being able to easily compare to other machine learning libraries so i said we're going to be using tensorflow and we're gonna be using a bit of a chaos layer on top of it so let's get on and actually see how we can use some code to describe train and evaluate on your networks with a a real example so the example we're going to use is a famous one if you've ever done a machine learning course before and by in this i include our course we gave yesterday you will have seen the ios example so this is a data set that was collected quite a way back and it is been used for decades for giving machine learning training and the data set at its core is information collected about three different species of iris flower so the three pictures here are three different species and we are going to try and design a network which based on measurements made of these flowers not made not based on photos but based on measurements made of the flowers we are going to try and decide which species the flower belongs to so these three are iris setoza i was verticolor and i was virginica that is the extent about which i know anything about these flowers however i do know about the data set that describes the flowers so there are 150 examples in the dataset and the link there i think takes you to the wikipedia page let's have a look yes it's famous enough that it's got a wikipedia page so if you're interested do have a look at that page later on and learn about the nuances of this data set so each flower that was measured by a particular person back in the day they went and measured four different properties of each flower they took a ruler they wrote down their notebook in a table and they measured these four things that we see in the table in front of us the length and the width of the sepal and the length and the width of the main petal and their idea was that based on just those four measurements they should be able to distinguish which species the flower belongs to and remember that we need to have this being a labeled data set we need to know what the truth is so that we can nudge our network in the right direction as it's training and so we also record the species so species 2 1 2 0 etc remember that we're dealing with maths here all of neural networks are maths and so anything that we come up with that describes something human or physical we have to convert that into something numerical in some way and so in this data set here we are using the number zero to represent the setoza one to represent the vertical and two to represent the virginica and so you can imagine that we want to try and train our network so that if we show it a set of sequels and petrol measurements which represent acetosa we want the network to output a number which is near to zero so now we've seen what the data set looks like we now think about how we can design a network which can take these first four columns now the inputs to a network are often referred to as the features we want to take something which can take these four features and give us out our label and do that consistently even for examples that it's never seen before okay so to see a bit more visual idea of how this works this picture was taken from the wikipedia page we have here a multiscatter plot of the four different features so along the uh along the rows we have the length width of the sepals and then the length and width of the petals and likewise down the columns we have the length and width of the sequels and the length and the width of the petals and so in each off diagonal we have a scatter plot of one of those features against the other so in the second row in the first column we have here a scatter plot of sepal length against sepal width and we have each of the dots in that colored by what the true label of that particular flower is and so you see here there is a fairly distinct cluster of the red which is the setoza whereas the blue and the green are relatively intermingled with each other so based purely on this scatter plot you would probably be able to make a good guess about whether a flower is a setoza or not but you wouldn't be able to easily distinguish between a verticolor and a virginica and if we go back to slides you'll see that you can probably guess which one's which the satosa is this one on the left and the other two flowers do look very similar to each other to my untrained eye i would struggle to tell those two apart however if we look in other projections some of them do have better separations but there's no one projection which has perfect separation between the data sets they all have at least some overlap between the blue and the green and i do apologize if you're colorblind um we i'll try and do a better plot for next time with better color blind friendly colors the idea however here is that even though no one single projection can distinguish the two we're hoping that by combining together all six projections because those are the four features um combined together all six of those different projections we can come up with some kind of complex multidimensional description of what's going on in a way you can imagine that the network is going to divide these things with kind of dividing areas in the different planes with certain probabilities and then some to get those probabilities at the end and give us an estimate of which flower species each example is that's not exactly how a neural network works um there are other machine learning techniques which effectively do that and they would also behave quite well on this this isn't the kind of problem that you need a neural network to solve this is something which can be solved with other relatively simple techniques but this is a good place to start when learning about how we can design a network and train it and go through that mechanical process so the code for this is all linked at this link here so on this page in the notes click on iris dot i pi nb i'll click on it as well to show you what it looks like it will take you to a page on google collab which looks like this i'll show you how to run through it in a bit so if you want to follow along and keep track of where i am feel free to have this page open in another tab but i'm going to have all the code samples on the slides and i'll talk through it bit by bit to explain what the different parts of the code are being used for so the code we're using today is using tensorflow and keras and it's being written in python but all the concepts and ideas i'm going through will apply for any other kind of programming language or tool so the same steps we're doing here we would have to do in pi torch we'd have to do if we're using r or we'd have to do it using julia or some other language so it's not necessary today about learning about the python it's learning about the steps you go through and as a side effect we're learning how to use tensorflow i'm going to jump into the question or two in chat before coming on this slide so tim asks how come the categorical output are incident coded rather than one hot encoded in reality tim they are one hot encoded it's just that in the original data set they are integer encoded when it comes to the network it is going to treat them automatically as being one hot and that's one of the features that kevas and tensorflow just handled for us we tell it it's categorical and it does that work marius asks when working with categorical features should we convert those into numerical features more or less yes and so um tim's and maurice's questions are related now i don't cover in detail what one hot encoding is here but let's just post it there in the chat but at its core you can imagine that we have three different features here we've got sorry three different labels here we've got is it flower one flower sorry is it flower zero flower one or flower two now when we're training a network it's going to be trying to aim for flower zero one or two if it's flower two and it's actually giving a value of three for example then we need to bring it down but also if it's below we need to bring it up and so it's got an ambiguity about if you're between two flowers which one it's going to be and other flowers even if it's likely probability they are necessarily further away because of the inevitable ordering of integers so what you can do instead is take those three different features and turn it into a threebit binary number so you have a zero or one followed by zero or one followed by a zero or one and you use the first digit to describe how much of it is the first species how much of the second speed is in the second digit and how much of the third species in the third digit so if it is a flower species zero the number would be one zero zero if it's a flower species one it would be zero one zero if it's a flower species two it would be zero zero one so the place where the one is tells you which category the particular measurement is in and that's why it's called one hot because the number one labels the hot place in that measurement and this allows you to find scalable probabilities for all of the different classes in a clever way but the nice thing about tensorflow and coax is they hide that for us we don't have to worry about binary digits it's just going to do the right thing for us so the first thing you need to do is load in our data so to be to cut to the chase with this psychic learn which isn't the package we're going to use to do our training with but it does provide us with some data loading facilities has a function called load iris and this gives us the table that we saw on the previous slide as a piece of code it's something which we can use and analyze and pass through the system so we are going to load into our x parameter the data of the load iris data set and the y value we are going to load the target so x is going to contain the values which were measured with a ruler it is going to be the length and width of the sepals and the length and width of the petals four columns of this data and then the y parameter is going to contain which species each of those samples relates to so we're keeping our training data and our labels associated with it in separate variables and that makes sure you don't accidentally leak some of your labels into your training set and skew the whole thing once we've got it loaded in the next thing we need to do is turn it into our training and test data set and just split those apart and again for this there is a psychic learn function called train test split which we give it our x and y and it gives us back our train and our test for x and our train and our test for y so this has done it randomly we can trust this to do it well pseudo randomly we can trust this to do our job for us by default this does a 25 split for testing and therefore a 75 split for training if we look at our x train value we see we have an array of lists or way of arrays twodimensional thing each sample is a row and each sample has four measurements one for each of those things we measured with a ruler on the flower so this is one sample you one flower this is the second flower and this is the third flower and these are the numbers that are going to be put into the input layer of our network these four numerical values if we look at the y thing and the first three that corresponded to it we see we have the number zero one and zero so that's saying this first sample here is flower zero flower species zero this here is flower species one this here is flower species zero if we look at the shape of these two things we see that our x data has 112 samples and four columns and our labels there's just 112 numbers it's just a onedimensional thing so our x is a table of data our y is a list of numbers and in general this kind of shape of things is going to help to ask is the capital letter for input data and lowercase outputs a convention there's a bunch of different conventions you often see x capital x being used because it's representing something which is a vector of samples so you've got something that's got multiple dimensions to it the fact that this is two dimensional is it's capital lettered same reason when you're doing a vector maths you sometimes do an arrow over it to sort of designate it as being a vector also sometimes use capital letters for matrices and this is effectively a matrix the conventions are a bit wobbly but it's more or less the convention that i'm using in the notes here and i think i'm consistent if i'm not please do ask this is our data we've got this for the training we've got equivalent shape stuff for the test data set we've got 112 training samples and therefore we've got 38 testing samples simply having your data in a twodimensional table isn't itself enough we need to do some prep work on it so that tensorflow understands what the data means we have our training sample but we want to be able to show them to the network repeatedly over and over again randomizing the order so that it doesn't do things like remembering the order of the samples for example it's not going to buy us towards remembering the ones at the beginning or the end more strongly so we're going to randomly shuffle them together and by doing over and over again each sample will contribute a small amount towards a result but at different points in the training and so hopefully you'll get a nice balanced result so the two bits of code here again the code details aren't important but it's good to know the kind of thing you need to do we take our data sets our training data set x and y and we turn it into what tensorflow calls a ten a data set the data set in tensorflow kind of encapsulates all the stuff that the network needs to know to train from it and that includes things like the fact that the data should be repeated over and over again those 112 examples should be 112 way through and then show them again and then again and then again they should all be shuffled together here we're shuffling them in batches of a thousand so the ordering that they were in the original sample isn't going to factor in and finally we do a thing called batching and this is another way that you get a more generalized smooth approximation of the answer and that is by showing it not just one example at a time as i was explaining we do before in fact we actually show it 32 examples all at once and because this thing's being represented as a big complicated tensor it just adds another dimension to the tensor which we end up inverting and multiplying and doing math stuff too and so we can show it multiple at once and effectively the average effect of this batch is what gets applied to the training weights and that smooths things out stops you ending up jumping around too much and makes it more likely you're going to find a generalized answer the numbers we use here 1000 or 32 you find them by tweaking the numbers messing around with it seeing what works we then do the same thing with our test data set except in our test data set we don't need to repeat it and we don't need to shuffle it because those things aren't affecting anything and we just batch it into batches of one because again we don't need to worry about this data set having any effect on the network it's only being used to measure it does batching affect the degree of overfitting it's possible by having larger batches you can reduce overfitting you'd have to fit the network for a lot longer to get the same overfitting effect but it's not the primary use of it the primary use of it is to smooth out the training and to stop you ending up in weird local minima too much exactly um we show it 32 examples and then based on the effect of those 32 samples we then update the weights all in one go i need to ask is it similar to crossvalidation in the with the test and train data sets this is effectively a form of crossvalidation we're doing it's not there are more details and nuances to advanced crossvalidation to really avoid the nittygritty of overfitting but this at its most basic is the first step towards doing crossvalidation to avoid overfitting and then carter asks does batching help the time taken for training it can do as long as the computer hardware you're using is able to hold four samples times 32 samples in a batch so four times 32 numbers in its little cash register to do the the maths to then it's going to make it faster if you make this number too large then it's not going to be able to fit in the memory of the machine and it's then not going to be able to train at all you'd often go the approach of making this number as large as possible such that it fits into the memory of the machine in the example we're doing here we've only got four features and so you're not really ever going to have problems but i've dealt with networks where you've got million features or something like that and then when you start having a batch of 100 or something you start potentially pushing into memory limits of some computers right so we've got our data set up we're all ready to go with that it is ready to be shown to the network it just needs a network to be shown to and so we need to design a network which is able to do this evaluation and the network we're going to design here is a very very similar structure to that which i showed in the earlier slides it's got a input layer at the far left an output layer at the far right and it has two hidden layers we use keras here because keras is what allows us to write layer sorry layer after layer after layer after layer just in a python list and so we start off with our input layer we say our input has four features and so we've got four newer ones in our input layer that's all we need to do to tell this how it's going to work based on this it's going to know to ask for our data set for things which have a dimensionality of four in one of the dimensions so a size of four in one of the dimensions then we're going to have a hidden layer both of these layers here are the hidden layers because they're between the input and the output and our input layer here is going to have 10 neurons in it followed by another hidden layer with 10 neurons in it and the final output of our network is going to be a layer which has three neurons on it we choose the number three here because there are three different categories that we want to put our iris samples into what this is actually going to do is instead of having one single output neuron which has a number and we try and push that number towards the result we actually end up with three different neurons each of which is going to represent the probability of a particular measurement being one of the particular flowers so if neuron one has a high value and neuron two and three you've got low values that's saying it's likely to be neuron one is the answer and we'll say that neuron one will decide in advance is flower zero for example we apply the soft max activation function to the output of those neurons because that is what turns it into a probability it basically works out the it normalizes it so the sum of them add up to one and so we end up with an effective probability being the output of each neuron of it being each particular species i mentioned activation functions earlier and it was more or less in passing and here you see we have to specify what the activation function is we have to say based on what gets added up and multiplied inside the newer one what function do we apply to that number before passing it on and if you don't know any better a good place to start is using the relu which is let's have a look at the wikipedia page for it it's the rectifier and this is a particular form of the rectifier the rectifier linear unit it's a function which looks like this it's the blue line there you see that if the output from the neuron which is x is negative it sets it to zero if the output is positive it just sets it to be the number now this particular activation function has some nice properties which mean that it works well at describing nonlinearity and things like that if in doubt go ahead and use the value or spend a few weeks reading the the literature out there to try and decide on a better activation function relu for most purposes is going to do the job quite well okay so i'm just going to check for any questions uh lester's answered a bunch of them so we may ask why do we need the comma after four for input neurons four comma that is a particular pythonism which is saying that this input function here takes arguments and the argument it takes has to be a list of dimensions in this case in python this is how we make a python tuple which only has one element inside it which is the number four nitin asks do we have to do feature selection before or does the model do it for us so if you want to do feature selection and think about what features are important and not you should do that before the network you can however query how the network is training to try and discover which features are interesting or not but i always recommend the best thing to do is to use your scientific intuition to think about the different features and decide which ones are important which ones are not look at correlations between them look at the effect that different ones have based on other machine learning models and decide which features are important you can also do various post processing to your features if you think that's a useful thing to do otherwise one of the nice things about neural networks in many situations is if you put in a useless input feature then it will just get pushed to the side all the weights coming out of that feature are going to get set to zero because they're going to have no effect on the output they're going to kind of get ignored now that's not necessarily a situation you want to be in but it does mean you can sometimes get away with it if you've just got a weak feature in your data set but obviously the more features you have the slower your network is to train so we've described our training data set and we've described our network so in principle we're ready to go now now we just need to describe how we're going to mesh the two of those things together so before we can actually train it we need to tell the algorithm how it's going to work and remember this is more or less a back propagation thing being applied but there was some unspoken details that went into that back propagation algorithm which i didn't cover at the time because uh we need a real example to actually understand what's going on so remember to forward that propagation about you show the the network a sample of data you push it through forward through the network and you see what result you get and based on how wrong that answer is you need to decide how much to change the weights by and you can see if you've got three different outputs and you've got a class you want to end up at you need some way of describing how wrong that is so if for example the three neurons on the output were 0.4 0.3 and 0.3 and the real answer is zero naively there's no obvious way to be able to say well i need a single numerical value to describe what how wrong i am so we need to come up with a mathematical function which can take the three neuron outputs and the place we want to be and give a number out of it for example we might want to get a number like 1.13 we need a single numerical value and this is what the loss function is for now there is an art choosing loss functions but mostly it comes down to what kind of problem are you trying to solve in our case we're trying to put things into categories so we want to use some kind of categorical loss function and in the situation where we are where we have multiple neurons as outputs with values and we have a single integer describing the class that we want to assign it to then tensorflow comes with a loss function called sparse categorical cross entropy if you really want to you can read the documentation page and the paper about it and learn about it but for our purposes it's good to know that it does the job well and in most similar cases it's going to do the job well almost every network i've trained in my career so far has used this fast categorical crossentropy loss function or a very similar variant on it so that's going to tell us how wrong we are so we can use that we can feed that into our back propagation now the other thing in the back propagation is the particular function that we use to accept all these arguments and decide how much to change the weight by on the earlier slide i had that uh sum over the differentials and then you shift the weight by that amount with a delta delta omega that was a simplified version of how gradient descent works of course when we actually come to doing the real job we have to have a specific real answer about how the back propagation is going to work and so we need to tell the network what algorithm should you use and here we are going to use the adam algorithm which does something very similar to how i described it before except it's a little bit smarter one of the things that the adam algorithm does is it has an adaptive learning rate so if going back to our example of being out on the planes with some mountains and hills and valleys and stuff around when it's on really flat land it will move more quickly it will move across flatland really quickly but as soon as it gets to the hills it's going to slow down to make sure it doesn't jump over any particular local minima in the in the whole domain the last thing we have is the metrics this isn't used to inform the training per se it's only used to tell us how the training is progressing like how well it's doing so we can keep an eye on it as it's going through so we've prepared all the pieces we have our data we have our network we've described how they're going to fit together then we just need to actually kick off the algorithm which is going to do the job and so we've created our model which was made up from these different layers and every model in tensorflow and keras has a fit function so we just call that fit function we show it our training data and it's going to go away and do the fitting that's pretty much all we have to do the nuance on top of what i just said is that as well as showing it the training data we also want to give you access to the test data so that while it's training it can be printing out onto the screen how the network at the point it's got to is working on the validation data set and so as it's printing these things out and it's training on the trained data set and evaluating in a side channel on the validation data set we should see the progression of the network improving for example one of the things that we asked it to tell us about if i go back to the previous slide we asked it to tell us about the accuracy that is of the samples that you have there how many are you getting right and how many are you getting wrong we want that number to be 100 and anything short of that is less good and zero is terrible so what we'll see as the network progresses is that the accuracy by looking at the training data set is going to improve over time and the accuracy on the validation data set is going to hopefully also improve over time the way that you spot whether your network is overfitting is whether the accuracy of your training data set keeps on getting better and better because it keeps on learning more and more nuances about the training data that it's seeing but then your validation data starts getting worse all of a sudden because your network is no longer generalizing well it's learning specifically about the training data set and so it's the combination of looking at the accuracy on the training data and the accuracy on the test data that tell you about overfitting and things like this the very last thing we need to do is tell us tell it how long to train for so we say for each big loop of training data you should do 150 look at 150 examples this is actually 150 batches so it's going to be 150 times 32 different flower examples it's going to look at and it's then it's going to evaluate against the test data and then it's going to do that 10 times it's going to do over and over again 10 times until it's finished now i've chosen these numbers because i know that by the time it's done that 10 times it's going to give a nice answer but in reality there are more advanced techniques to work out how long you should train your network for so when we run this function it's going to go off do the machine learning do the back propagation and it's going to output the information to the screen and it should only take a second or two with the data that we're looking at newton asks a question how is the model handled and balanced data set um and there are techniques there by choosing the right metrics is one of the main ways that you do so you choose a metric which rather than looking at accuracy which if you have a very imbalanced data set is going to tell you that you're often doing quite well because one of your classes only crops up one percent of the time so you have to think of a different metric to accuracy to describe it or you do some clever stuff to rebalance your data set i know this is something that lester who is working on a machine learning project at the moment is currently fighting with and it's not always very fun ume asks what steps for epoch's doing so steps for epoch is how many batches should it look at before it prints some statistics to the screen historically steps for epoch would be how many samples should i look at or other how many samples do i have and an epoch would be how many times should i look at the full data set when you're repeating and randomizing and batching your data those definitions fall apart a little bit so mostly you can think of it looking at all the batches one and a half thousand times 150 times 100 times 10. however i break it down into 150 and 10 so that it's going to print out summary statistics as it goes along so mostly that's a hack to make tensorflow describe what it's doing as it progresses so we call fit it goes away and does it and then as it's going through it's going to print this stuff and i'm going to just describe what you see on the screen and then we're actually going to run this stuff for ourselves so for example on epoch 10 the last epoch it's going to print out the value of the loss function remember that sparse categorical cross entropy thing that has a number we're trying to make that number small we're trying to reduce how much it's lost and we're also trying to measure the accuracy which will make the accuracy large we want it to be accurate when assessing the data that it's training over as well as the loss and accuracy on the training data set we're also looking at the loss and accuracy on the validation or the test data set so we want these numbers to be moving down at the same kind of rate as those two numbers they should have similar values and they should progress at a similar rate if they start diverging from each other that's a sign that you're probably over fitting on your data set and you need to reevaluate how you're balancing things up so this is telling us that at the end of this it's got a 75 accuracy once we've trained our network and we've got um everything all working then we have to actually use the model so this is after the model's trained it's learned everything the weights are set and they are fixed at this point the weights don't change anymore they are just going to be used we're no longer going to do any back propagation we no longer have any true labels we're comparing with we're no longer doing loss functions any of that stuff we have a fixed network which is designed to answer questions so we're going to pretend that we've since training the network gone out into the garden and measured some more flowers using my brain i know which one is which i know i've got one of each flowers and the four features of the first flower are those four four feet to the second foul are those and the four features are those we take this data this prediction this prediction this data we want to predict over these three samples and we pass it to the predict function of the model it takes some data that's the same shape as the training data and it's going to give us back some predictions this predictions is going to contain the values of the output neurons for each of the three samples that we've shown it we then do a bit of python magic where it takes those predictions loops over them find the one that has the maximum probability and then grabs the actual name of the flower out of the look up table which made zero be one flower one be the other flower and two of the other flower we run that bit of code and it gives us back its prediction of what the species of those three flowers are and if we look we see that it matches what we had at the beginning it's also worth noting that during this process while we up front wrote what the three flowers were we never actually used that in the prediction that was just for our human purposes it wasn't used by the code at all it's not cheating and looking the answer it's only looking at these data here umai asks where did it convert the binary number zero one or two like in y test it did that automatically by us describing the loss function as being the sparse categorical cross entropy that function there understands how to take an integer value like this and turn it into a one hot encoding which can be compared to the three output neurons this function is designed to kind of do that one hot thing automatically for us and it is just a function which you can call with the value zero and a list of numbers and it will give you back a loss value right enough talking for me let's actually go ahead and run it so uh go ahead and click on that link there when you've i'll go through this in a second so um just click on this link here if you're not logged into your google account already then you'll have to sign in and to do that you'll click on the sign in button up here in the top right once you're logged in go to run time up here at the top and click run all once you click that it's going to run all the code and it's going to run all the way through and while that's running i'm going to answer some questions it will warn you that this code is coming from github it is coming from my github account so if you trust me go ahead and run it if you don't trust me press cancel but i do hope that you trust me and you can see the code here and understand everything we've just done annika asks how would you look at the layers and see how the network makes decisions either planes separating the three species there are things you can apply on top of the data the network to try and do this tensorflow comes with some tools to do these kind of things but it is a whole whole field of research to try and understand what's going on to see the planes here for example you could just sample the face space and do a 3d plot and try and see these clusters but more advanced techniques would take more than this course to apply but the tensorflow documentation does have some information about this so this has now worked it has loaded the data it's done all the um shuffling of the data we've designed our network and my screen a little bit bigger that's much better and then it's on the training and so you see here we've started out and the loss function was off the first epoch quite large and the accuracy was quite small only 20 notice at the very beginning here while the training accuracy was 20 the validation accuracy was 40 and that's just random chance after the first iteration you could easily expect them to be the other way around what we want to see as we progress however is this accuracy increasing over time and that's it saying that based on the examples that it's being shown it is doing a better and better job of representing them but remember as well as that's increasing over time we want to make sure that the test data set the validation data set is also increasing and we see here it does and it quickly kind of starts aligning with the values 79 81 after a while the network has got to the point where the validation accuracy has topped out it's no longer getting any higher because 97 is probably of the 40 or so examples probably only one example that it's no longer getting it it's failing to um it's it's it's not able to resolve that one last example but it's doing all the rest of them quite well and from that point on it never gets any better on the validation data set we might need more validation examples to get a better idea of why but it's not necessarily a problem that the validation accuracy tops out what would matter was if this validation accuracy started going down we also note that the violation loss is still reducing which is a good sign that it's continuing to go down in concert with the uh loss function of the training data set so and right at the bottom you see that example and it says the sosa is 94 98 sure that it's correct the versacle is 97 sure that's correct and the virginity is 92 percent sure it's correct and so you wouldn't necessarily expect to see the exactly the same numbers as that because there's a random element but you should expect to see numbers in the 90s right i was going to go through a few questions here i've seen some things cropping up yan's asking about the role of the test and training data set so i'll go back to the beginning and briefly show what's going on here so we have our all our data we split it into training and testing and from that point on those two data sets are completely treated in independently our training data set gets shuffled and blocked up and our test data set just gets left how it is pretty much when we come to the fitting we show it the train data and the training data is what's going to be used during the back propagation process that's the one that's going to be having the loss function applied to it it's going to have the weights being recalculated from it and it's the one that's going to cause the network to collect towards a result so the network is only going to learn from the things that are in the trained data set it is never going to see for its training purposes anything from the test data set the only thing that the validation data test is used for is for calculating these numbers it's not used to inform the network in any way it's just used to give us this output so that we as humans can assess how well the network is working tim asks if you restart the window and start again it will by default in this case start again from scratch entirely or wipe all the weights and run it all if you run just the fit function again i think it will carry on from the previous weights and ozalpas asks can we access model metrics and model selection comparison yes so keras and tensorflow have a whole bunch of functions built in to extract information from these models for doing comparisons and whole kind of crossvalidation high performance tuning setups so that's all possible in tensorflow we're not going to cover it all in this course because it's a large topic we're trying to focus on as simple an example as i can come up with okay looks like i can update the notes and thank you lester for posting those links i'm going to close that page so that's the end of the ios example that is the arguably the simplest neural network you can think of designing and you see that even though it's a simple example there's still a lot of questions you have to ask and answer along the way you have to think about structure of data about tests and training data splits you've got to think about your loss functions and how you're going to repeat your data you've got to think about your optimization functions there's lots of choices you make along the way but the first thing to think about or to remember in that situation is you don't always have to make those decisions up front i'll say this again because i think it's worth reiterating the best place to start with the network is what someone else has created before you you can go online you can find predesigned and set up models in tensorflow to solve a problem you just import that code show your data and see how it performs that is going to get you a lot of the way there by understanding what we've gone through today that's going to give you the ability to tune and tweak what you're seeing to slowly start understand how to apply it to your particular situation so thomas is asking why the validation accuracy didn't keep increasing as the loss decreased on the validation data set and that's because it the accuracy is a course measure it's measuring of all the samples add up how many i got right and wrong and give that as a fraction the loss function as leicester mentioned earlier is differentiable it is smooth and so it can keep on increasing and so while the accuracy wasn't going up the degree to which it was sure about each of its answers would have kept on going up it might have once it got to that that threshold been saying i've got the right answer but i'm only 53 sure by the end it might have been saying i've got that same answer but now i'm 90 sure for example so that's why the loss can keep on improving while the accuracy doesn't necessarily change so i'm going to start covering this section here and then we're going to have a break and come back through the last example so this should only be maybe 10 minutes and then we'll have a break and so for this we're going to go through some image analysis stuff because we want to learn how we can apply neural networks to pictures as well as how we can apply neural networks to plain old numbers but before we can dive into applying neural networks to images we need to understand how computers treat images because images are more complex things than measurements made with a ruler so the flip side of this is that pictures are also much easier to collect it's much easier to grind your garden and take a photograph of a bunch of flowers than it is to go out with a ruler and measure all of their parameters it also requires less skill so you can collect larger data sets by using crowdsourcing and so on so i'm going to go through now and explain how image analysis techniques work so to get a sense from the class could people just post in the chat what kind of experience you have with image analysis using kernels and convolutions and things like that is it completely new to you or have you done some of this before i'm assuming it's completely new to you which is why i'm going to go through it all i'm seeing lots of news and nuns wonderful i'm sure there's gonna be some people in the room who've done this before and hopefully i'll be able to answer questions you have as well but let's start right from the basics so the idea of applying neural networks to pictures starts with the maths and computer science approach to dealing with pictures and so the place you have to start because we're going to be doing maths to this stuff because neural networks at their core are multiplications and sums and function applications we have to turn our picture into numbers i mean a consistent way of representing images as numbers and so i hope it you're comfortable with the idea of taking a gray set grayscale image just a black and white image and assigning for every pixel in that image a number and that number represents how bright that pixel is commonly these numbers are between 0 and 255 you've seen these sort of binary power of two numbers before so a pixel being 255 would be completely white and a pixel being zero would be completely black a number like 105 is slightly under halfway so that's a darkish gray but all the numbers on this little snippet here are a darkish gray the dots here are to represent that these pictures can be very large the numbers we have here is only a five by five sample in the top left corner but these pictures can be hundreds or thousands of pixels across in each dimension you know megapixels means millions of pixels which means millions of numbers which in our case corresponds to millions of input neurons and that's a lot of data you need to deal with so we have a grid of numbers sometimes people refer to these as matrixes matrices of numbers but don't think of these as mathematical matrices as you might have done in school or undergrad these are a grid it's a it's a grid of numbers and nothing more than that so once we've got our grid of numbers we need to decide describe some kind of mathematical process we can do to that grid and the way this is commonly done is by using a technique called kernel convolution and i'm going to explain the two parts of that term as i go through let's start with the first first part a kernel now a kernel in image analysis is a another smaller grid of numbers usually they're two by two three by three five by five that kind of size and they contain inside them a bunch of numbers and the magic of kernel convolution is depending on which numbers you put inside that kernel when you apply it to the image and we'll see how it gets applied in a moment when you apply it to the image different things happen so for example these particular numbers here will sharpen the image they'll make it less blurry you might have come across the sharpened mask or the unsharp mask if you've played around in photoshop or something like that a different selection of numbers with like one which didn't have the negatives here for example would blur the image you apply a particular small kernel to the image and it blurs it this is how all image blurring works in any kind of computer sense there's other kernels you can choose which do edge detection and edge detection is a very common computer vision technique for example working out outlines of things or it's also the core of what we're going to be seeing today now you might be wondering how can you know what particular numbers are going to give particular outputs and that's because some clever mathematicians have done that work for us they are all published and online you just choose the one that does the job for you if you want to blur the image you go on the wikipedia page you find the blurring kernel and you apply it that's all you have to do you don't have to think about what these numbers should do schweitz is analogous to normalization in a sense so um one of the things you might notice is that the sum of all the numbers in here add up to one and so after applying this kernel you expect it to have a sort of a normal effect normalization can be applied in lots of different ways so there might not be a an image kernel which can perform it but it's going to be a similar technique that's going to be done to it so the idea here is we have a set of predefined kernels designed by computer scientists and mathematicians which have effects which we can describe with words that's how these kernels are described the next question then is once we've got a kernel how can we use it to do something to our image and what we do is we take our image which here on the left hand side so you've got the purple section and the lighter blue out going into the distance we take our kernel matrix i say it's not again it's not really a matrix it's just a grid and we overlay it over those top three squares our kernel is three by three and so we overlay it over the top three by three grid and then we kind of shine a light through it we look at the top left number that's being shadowed and the top left number in the kernel matrix we multiply those two numbers together so 105 times zero and then we add that to the next two pairs that are over each other so 102 times 1 100 times zero and so on and so on we add up all of those total numbers and we put that in where the middle of that kernel is currently sitting you should ask why do we apply a kernel that's a tricky question so we apply a kernel because it has the effect that we want it to we apply a kernel because for example we want to blur our image and if we want to blur our image we choose a blurring kernel and we apply it to the image and we end up with a blurred image but at its core all kernels work the same regardless of what the numbers in the kernel itself are the process that's applied is the same and if we change the numbers the effect in the output image is also going to be different you'll end up with a blurred image or a sharpened image or an image which represents where the edges are for example so to take that picture and kind of turn it on its side a little bit we have the picture on the left hand side we have our kernel and we've done 105 times zero 102 times 1 100 times zero adding these up as i go 103 times minus 1 et cetera and so we see this in the sum at the bottom you see that's the calculation it's done and that gives us the number 89 and because this kernel was 3x3 and it was overlaid on the 3x3 purple area the result of that calculation goes in the middle of the kernel so the 99 gets replaced with an 89 so what we do after we applied the kernel the first time is we shuffle it across by just one square and we do the same thing again so it's overlapping with the original one but the middle of the kernel is now in the next space so now we're gonna do the same maths again but we've got different numbers so now the 103 gets maps to a 111 for example and we keep on doing that all the way along the row and on the next row and the next row and the next row filling up the whole image now what you might have noticed is that in that situation you're not filling in the edge values at all there's no way to represent what the number in the top lefthand corner should be because five of the numbers in the kernel won't have corresponding um image values to multiply against and so we have to make a choice how do we deal with that and that's just a choice that we make in our case here we can decide to just pad the edges with zeros that's perfectly valid you get slightly weird effects of the edge of the image but it's mathematically valid and it's consistent at least alternatively we could replace we could pad the outside with a repeat of the value so this top row would be well undefined but then it'll be 10502 197.96 repeating above the actual image there so depending on whether you want to zero pad or same pad it gives you a choice about how you want to deal with this edge condition you do this to an image by repeating that process over and over again obviously use computer if you use the right kernel for example there's a kernel that does sobel edge detection you start with the image on the left and you end up with the image on the right anywhere where the image is smooth just stays black anywhere where the sharp change ends up white so we've got an image now which shows where the edges are and that's a useful thing to have because edges are often where interesting stuff happens in pictures so we saw just before the break how at its core image analysis is done most image analysis algorithms have some elements of kernel convolution being done to them now the kernel is that little three by three matrix we saw and the convolution part is the bit that applies it to the picture and gives some kind of result so in principle it would be possible to take a whole bunch of kernels really carefully design them such that you can show them to an image one after another layering them on top of each other to get some kind of answer for example you could start off with one kernel which does the edge detection and then based on the result of that edge section you could have another kernel which says are there two edges next to each other here so they're parallel at which point you say okay the result of this is a image which represents where parallel edges are from that you have another kernel which looks for circles and then based on that you have another kernel which looks first circles next to each other and you start building up these questions getting more and more abstracted away from the picture but more and more towards human concepts like is this a person and at its core that's what convolutional neural networks are going to do now i alluded before to the fact that kernels in convolutional image analysis are designed by mathematicians and computer scientists and the example i just gave there i said if you choose your kernels really carefully you can do such and such a thing so what we're going to do is try and make those two things meet in the middle using your networks we're going to try and train a neural network to automatically discover what the values of the kernel should be we are going to design brand new kernels and a whole slew of them we're going to layer them on top of each other and through the magic of the back propagation stuff we saw before this is going to give us an answer which can based on a picture say is it a cat or a dog for example so the process is going to be the same we're going to be showing it examples checking how far away we are from the answer doing the same back propagation thing to the weights but the difference here is that the weights in the network are going to be representing the values in the kernel so going back to the previous section this matrix here for example has got nine values inside it you can imagine in our network that we have a weight associated with each of these numbers if this zero was a different number you can expect that the output of the network would be different and therefore this number here this weight has an effect on the output and therefore we can do the same thing we did before we work out what the derivative of that is and therefore work out how much this number in the top left of the kernel needs to be changed in order to make our answer more correct so it's the same idea as before but in this situation the weights are the values inside the convolution kernels the way we apply it ends up working exactly the same and that's the beauty of convolutional neural networks that the process is the same but the way that you treat those weights and the way you apply those weights you get a very different kind of idea a convolution on your network or a cnn is a classic example of a deep neural network they're deep because we're going to need lots of layers we need lots of layers because we need those levels of obstruction we need to start at one end with is there an edge here why is it the other end saying is this a cat there's lots of questions you have to ask along the way like where are the ears with respect to the nose what shape of the ears how do you describe an ear how do you find the edges that go into that where are those edges etcetera there's lots of questions you have to ask and so you need lots of layers in the situation where we're asking things about the location of something on an image with respect to the pixels around it on the image we have to use what are called convolutional layers now a convolutional layer doesn't connect every neuron in one layer with every neuron on the left on the next layer it only connects the neurons in one layer with the neurons in the next which map to the same part of the image so each neuron is representing a pixel b only connect to the neuron on the output with the neurons that are near to it on the previous layer as well as these convolutional layers which are the layers that are basically doing the same convolutional kernel thing we saw before just it's kind of encoded into a a neural network language we also have pooling layers and our pooling layer is a much simpler thing a pooling layer takes a picture which is a thousand by a thousand and makes it smaller so it takes a thousand by a thousand image and for example takes every two by two pixel and squishes it into a one by one pixel and therefore it goes from being one thousand by one thousand into five hundred by five hundred and we do that because by applying convolutional layers and then pooling layers we create layers of abstraction so in the output a particular neuron is having a larger area of effect on the earlier layers than it is on the later layers and so you get global information as well as local information the classic pooling algorithm is max pooling so based on a two by two little grid you find the largest of the four numbers inside there and that's the number that survives and goes on to the next layer and the third kind of layer we have in a cnn is the dense layers now dense layers are the same stuff as we saw before these are just the traditional neural network layers where everything is connected to everything and information just flows on through so here we see an example of what this is doing so we have an input image this feature maps blob here this is a layer this is one layer of the neural network now that layer has a bunch of feature maps each of those feature maps is encoding a kernel being applied to the image so by the first layer here we've got in this situation four different kernels being applied to the input image so if this input image was a 10 by 10 image then in the output we'd have four 10 by 10 images and each of those feature maps would be representing a different thing one of them might be looking for horizontal edges one might be looking for vertical edges one might be looking for dots that are by themselves who knows what they're actually looking for the point is each of the feature maps are likely to be different we then do this subsampling this pooling layer we make things smaller we do some more convolutions to make more feature maps with different algorithms being applied to them we then sub sample again to get a whole bunch of smaller feature maps which have loads and loads of different pieces of information inside them and at this point we treat each of these last very very small feature maps as inputs to a standard neural network like we had before by this point each of these can be represented as a single number we just put them in as the input layer to a traditional neural network so this is kind of in two halves the first part is finding sort of a graphical and spatial information about where things are in the image and then the second part takes where those things have been found and makes a decision based on that as to what the image is of we can do some amazing stuff with convolutional neural networks and i'm sure lester's got some examples you might post as we go through these sections one example i really like is image segmentation so here you can train a network so that it can identify objects in the image and it can overlay each image each pixel in that image with what kind of thing it is and this is how some selfdriving cars work so we can identify that as a blue the car over here is a car and it's been outlined with blue we've got the road we've got the arrows on the road we've got traffic lights we've got all sorts of things this is a convolutional neural network admittedly a complicated one but nonetheless a convolutional neural network you can also do things like style transfer now style transfer is a really fun thing to google for because since this example here there's been much better examples but this is a fun one nonetheless you show it an example of a picture on the left hand side by a particular artist you can then add things into that image just based on photographs and the network has learnt how this artist draws and so it's able to draw the things you've added in the style of the artist so you end up with this mcdonald's balloon being inserted right into a streetscape in new york in a style which looks consistent with the rest of the image and in this case the network has learned how the artist draws that's what the kernel convolutional layers are representing in a very complicated way nonetheless and finally again apologies to any people actually understand this is approximately how animal vision works so animal vision does largely work by looking for basic features it looks for horizontal and vertical lines simple patterns like that then it takes those simple features and your brain and your optical nerves and all this processing system in your head takes those things and consecutively applies more and more abstracted ideas on top of it until it ends up at the end with your brain able to identify this as a picture of a cat for example and most animal vision and brain systems work in a similar way to this although of course because these are evolved and not designed they are much more complex and complicated and also much cleverer in some ways so that's what convolutional neural networks are and how they work it is the same as we had before but we design a network so that it can calculate what the weights of these kernels should be and it's going to be going to say give me uh 60 different kernels work out what the weights in those kernels should be in order to answer this particular question and through the magic of that propagation it's going to manage to do that it really is quite amazing the thing that convolutional neural networks are for is for images that's at least 95 percent true so if you've got image analysis you almost always want a convolutional neural network of some kind if you're using convolution on your networks it's almost always because you're dealing with images they are designed for each other effectively so we have an example here where we're going to be dealing with this data set called the mnist data it's again a classic data set based on numbers i think written for automatic check processing in the us this is a data set which is freely available of 70 000 pictures of handwritten digits by all sorts of different people from the wild from actual real people writing checks to people each picture is 28 by 28 pixels so it's quite small and that's useful for us because remember what i said before that the number each pixel is going to end up being an input layer and then we're going to be doing our feature maps and so by starting with a small picture you manage to keep your network under control so we want to design a network where we can show it a picture one of these examples and it's going to say that's a 4. that's a zero that's a nine that's our task that we've got ahead of us so we're gonna go through the similar process that we did with the iris data set so again feel free to click on this link here it's going to take you to the um actually that one takes you to my github page but at the end there'll be a link to the code app where you can follow through the notes if you want to so we're going to do the same things we're going to specify the shape of the network we're going to say how it should be trained we're going to specify our training data set same process before this is process you always go through with your networks so design the network now we've had the question a few times about how do you know how many neurons how many layers how many how big the layer should be this layout of this network is a standard layout of a network for an image classification problem if you've got a relatively simple image classification problem this layout of a network is going to do a decent job as long as you haven't got 10 000 different categories as long as you're not trying to identify multiple objects in one image things like that then this is going to do a good job of classifying the images and the way it works is by starting with a convolutional layer now this is going to describe and design 16 different filters each filter is going to be a five by five kernel so a filter on a kernel is terminology means the same thing so you have 16 different kernels each of them are going to be five by five and it's going to make a new layer which is therefore the same size as the input but there's had this layer this this kernel applied to it but it's gonna do that 16 times all in parallel so if we had 10 input neurons we're going to have 160 neurons in the in the first layer we've actually got 28 by 28 input neurons and so we can end up with 28 times 28 times 16 neurons in the next layer um objects in the next layer then we have a pooling layer which makes our image half a size so it goes from 28 by 28 to 14 by 14 and then another convolutional layer and another pooling layer to do convolution pooling convolution and pooling that's a common sort of technique you get so you've got two layers of abstraction over the spatial information in the image then we finish off with a dense layer which combines together all of the outputs of the convolutions and tries to sort of answer ask the questions about how those different features that it's discovered relate to each other in the same way as we were evaluating how the four different features of the iris flowers related to each other to give us the answer we wanted the final output is the ten neurons which represent one for each digit so is it a zero one two three four five six seven eight or nine there's ten possible categories so we have ten neurons in the final image in the honest example we had three categories and so we had three neurons in the final image there's some details in here but i'm going to go through that as we go through so this structure works really well for mnist you could actually mist is a simpler problem you could get away with a smaller network for mnist but this is a nice generalizable classification network so you can use this in your work if you were doing classification stuff so we're going to do the same thing we did before we're going to build up our network we're using keras and we're going to use a sequential thing which means we're going to pass a list of layers they're going to start from the input and work their way through so the first layer is a convolutional layer it's twodimensional so it understands sort of built into these functions is the understanding of what images are and how they relate it's kind of wrapping that stuff up for us so all we have to tell it is that we want 16 different kernels 16 different filters each filter should be a five by five there was that thing before where at the edge we had to decide are we going to pad it with zeros or pad it with the same number here we tell it that we need to pad with the same and then again because this is eventually actually doing a mathematical operation of some kind we have to have an activation function and like before we use our our um our value function and that does the job well it lets us represent our nonlinearity in our model so we have 16 5x5 filters the layer will be 28 by 28 but we've got 16 filters so the overall size of this layer is now 12 and a half thousand so we now have 12 and a half thousand neurons in our next layer so a lot of neurons and each of those are going to have a whole bunch of weights they're each going to have 25 weights a lot of numbers to deal with already as you can see but nothing that modern computers can't handle we then into this list here we add a pooling layer and i've pulled out available here because we're going to be repeating this in a little bit and all we say here is we want a 2x2 pooling so it's going to make our image half as big in every direction and this one here is saying how much you want to jump along by each time and the same here is the same padding as we had before so we have now shrunk this down instead of being 28 by 28 by 16 it's now 14 by 14 by 16. so we've only got 3 000 neurons in the next layer then we do the same dance again we have another convolutional layer and another pooling layer so remember we have convolution pool convolution pool same bit of code only difference is at this time we have more filters we've more abstracted away so we're further away from the original image but we want to be able to represent more different types of things we're looking for particular curls and curves in the in the handwriting of the original image is what this layer here is going to represent the first convolutional layer is going to represent more things to do with where the lines are and if there's two lines next to each other this part here is going to be describing more precisely where different circles and curves and spaces are in the image as a whole and you see this number here is again getting smaller the layers are getting smaller as we go because we are abstracting away the information we are reducing down our face space so um each neuron have a different kernel so um it's a bit more complex than i was making out uh each there won't be the thing times 25 i misspoke when i said that each of the filters each of the 16 or 32 filters will have a um 5x5 filter associated with it so there'll be 32 5x5 filters and therefore there'll be that many weights so some of the weights are going to be sort of correlated with each other in some ways so it reduces the total number of weights you actually have to analyze which is helpful then we have our same thing as we had before this is the same set in the iris we have a dense layer now a dense layer means connect everything to everything 128 neurons so this is a number chosen which is big enough to be able to represent the variation in the images but small enough that it's going to be able to train quickly we end with a dense layer with 10 neurons and we have this soft max thing again to turn it into probabilities the only tricky part in all of this is this layer in here which i didn't mention before called dropout layer and what a dropout layer does is not actually a layer of neurons per se it's simply a thing that gets applied to the process and what this layer does is as the network is training once after each set of examples it will randomly delete 40 percent of the connections it won't set the weights to zero it just for this example will not use those inputs on the training and this is used to avoid overfitting it spreads around the information stops you getting stuck in local minima and make sure you end up with a more generalized model so you're artificially snipping connections and then reconnecting the next time so that no one connection becomes too powerful and too important so this dropout thing is often used to smooth the network out and allow it to be more general this bit of code here is exactly the same as we had last time we're doing the same classification task we're trying to put things into is it this thing this thing this thing on this thing is it one of these flowers is it one of these numbers same optimizers before and the same metrics so we are doing the same task as last time the way that it works the way the network's training is the same it's one of the nice things that once you've done this a few times you start seeing the commonalities and you stop having to worry about how you should make these decisions because you just use the same stuff as as last time which makes your life a lot easier so as i said before if you're doing a classification problem and the labels are the integer index of the class that you want to get to then sparse categorical cross entropy is the tensorflow solution to your problem use that one and you're going to be fine there's also a sparse i think binary cross entropy if you want to have a yes no answer at the end rather than a is it one of these 10 20 categories we've designed our network and we've said how it's going to be trained the order's a little bit different this time we are going to load in our data and for this tensorflow comes with a tensorflow dataset function which gives us our mnist data it splits it into test and train for us it shuffles it for us it does all the magic well much of the magic automatically for us so after this function's been called ds train and ds test that's our test and train data set they are both sequences of 128 by 128 by 1 matrices so it's a 3d matrix but one of the dimensions is only one big because it's only got gray it's not red green and blue and the values in each of those are the numbers from 0 to 255 so the same as we were seeing before there's a few things we do have to do with that data to make it you know suitable for our network to train one of them is that in most networks you end up working best if you have numbers between 0 and 1 rather numbers between 0 and 255 so we apply a function which divides every pixel by 255 and make sure it's a floating point number so that just turns them from zero to five five into zero to one then we do the same thing before we cache it and shuffle it and this kind of stuff details here don't matter too much this is just the process it's going through to get stuff in the right shape you would start with if you were doing this for yourself you'd start with these things and you play around with them and read the documentation and see where it takes you from there you then do the same thing to the test data set but simpler you need to normalize it but you don't have to do all the shuffling and stuff like we had last time the test data set analysis is a simpler process and then we call the fitting same as before we call the fit function and we give it the training data we're not giving the validation data this time because um we actually we do do so we do do underneath so the same process before but we're only going to do it through once so this notebook that i've got here on the screen that is exactly the same code as i've just gone through we set up the data we shuffle it all we do the normalization add all our layers together and call the fitting and you'll see here after the end of the fitting step the accuracy on the training data set is 94 percent and the accuracy on the validation data set is 98 so those are both large um but uh it's got there very very quickly with only one epoch it's already got a very good accuracy if we kept on training it could increase further but that would be something to have a playground with after session these collab notebooks will stay up for as long as you need them to the last thing we get printed out and i want to switch back to the notes of this is a table of predictions on certain images and what it thinks they are so let's have a look at what it looks like ah this is a good example uh training networks can take a long time sometimes so uh it's a good excuse to pretend you're working while your computer's doing all the hard work for you but it gives you a table that looks something like this it's got an image which is a number one uh it thinks it's a three because it's got it wrong we've given it a picture of a number two oh we've got that one right it thinks it's a two i'm not going to go any further because it's a bad way to look at data this is a much better way to look at data so in this table here down the lefthand side we have a bunch of images which once our network has finished training we are showing to the network and saying what do you think of this can you get anywhere with this kind of image and we'll see that it's done some of them quite well but many of them have done a terrible job at and this is on purpose we're going to get to solving this in a moment this isn't just a demonstration of how bad your networks are it is however a demonstration of how you've got to be careful about the difference between the data you train your network on and the data you apply it to so the data we had up at the beginning was all 28 by 28 pictures which were white ink on the black background now the only one that's whiting on a black background is number five and it's got that one successfully correct but the other images are mostly from a different source they're not handwritten they're different inverted contrast things like that and some of them it's got right but many of them it's got wrong so we've trained the network to identify white on black handwritten digits and it doesn't really behave well outside of that it gives an answer but it doesn't always give you the right answer so it's not reliable i would also importantly draw your attention to the very last row there we have that little dog we saw earlier so if we put a picture of a dog into this network it's going to give us back an answer it's not going to say none of the above it's going to say its best guess is that this looks like a number two maybe its nose has got the right kind of curve to it or something and so this is something you really need to be aware of you need to make sure when you're training your network it's been trained on the kind of data that it's going to be applied to there's a classic example which was mentioned in the course yesterday which i'm just going to post a picture to came up recently and this is the covid cat where a similar kind of situation network trained to look for kovind 19 if you show it a cat a bunch of the networks say that yes that's definitely got covered and even though they have no information really about what's going on inside they were trained on one thing and they are making spurious guesses about data outside of that so you have to be careful about the data you train it on and the environment in which you apply your network think about how general or not general it is jen asks is there a way to get a none of it answer with some networks you can have a go with that and that's what the um writers of the the publishers of that covered cat image have tried to do they've got a none of it example you've got to be quite careful about what you consider to be none of it and how that blends in with the categories you do care about it's a tricky problem to solve the important thing to do is think about it from the beginning think about the scope of data you're training on and think about how that relates to the um data that uh you're going to be using the network on regardless it's it's giving an answer for every single thing it's always going to tell us something and that's a common problem machine learning machine learning doesn't tend to be well designed to give you an i don't know answer it's a hard problem to solve but we can in this situation for these numbers do an attempt to improve that domain that we care about and i think the main difference between the numbers that worked well and those that didn't is that this was white on black and these were black on white the answer to this is doing something called data augmentation basically adding in more training data to represent a larger range of possibilities so the network learns a more general model of what the world looks like if you're doing something to identify dogs you don't just want really nice framed dogs with maybe the good camera you want pictures of dogs made with really bad cameras in dark conditions and snowy conditions all these different things you will have all of those the network can learn the full scope of the world in our case we've got some black on white on black images so we want to just put in some black on white images as well and that's hopefully going to make it able to understand the full scope of handwritten images handwritten digits sorry in general you're going to want to do data augmentation on any network it's a really really powerful thing to do if you can't collect more data then fake it by blurring rotating scaling the data that you do have in our case we are going to invert the images we're going to take our original data set and we're going to add on to it the same data set again but with black turn into white and white turn into black so we multiply it by 1 and then add one to it so that turns one to zero and zero into one and 0.5 stays the same so let's have a go and see what effect that has so go back to that notebook and at the very top in the top cell i made it super easy for you there's a variable that says invert equals false change that into invert equals true and go to runtime run all again and see what effect that has on it you do need to run all the cells again um so make sure it's true with a capsule t because that's what python expects then run time run all while that's running uh mia says this raises the issue of implicit bias absolutely and this is something i'm going to cover on my very last slide about being careful about the kind of things you train on and the kinds of questions that you're asking your computer to answer it's not as simple as throwing the numbers in and get a number out you have to think about the human in the loop for example so that's all run through it's currently fitting it's finished and it's doing the analysis and this is again going to print out our table of results and i'm going to skip that table of results and show you in a better format umea asks how to evaluate bias of a network well it depends on what kind of bias you're talking about one of the primary biases you get is sort of a selection bias where you've only trained your network on a subset of data and so it's only going to know things about that subset of data or sampling bias as well i suppose it could be as well so the way to do that is to apply try using your network on data that represents where it's actually going to be used and make sure it's performing well on that whole set but we see here it's worked well simply by showing it inverted color images it's able to generalize well across data which wasn't even part of the original data set the number one is an inverted image but two three and four are computer generated numbers they are you know printed to the screen they aren't hand written but it's still able to recognize them we trained it on handwriting it can recognize nonhandwriting which is perhaps surprising and it's not something you should rely on we should add into our training examples like these if we want it to perform well we see how see here however that it is performing reliably on these examples it's not only saying that there's a high probability of it being a two it's saying there's a low probability of it being anything else and that's a useful thing to see all the way through it's doing very well 97 is one of the lowest we get 71 there probably because of the low contrast on the number one so we want to add in maybe some more images with you know washed out colors and that's going to help the training the number nine here isn't handwritten or computer generated there's noise around the edge because this is actually a photograph of someone's door number and so it does a bad job it thinks it's a zero because he's getting confused again if we want it to recognize door numbers we need to train it on door numbers once again we see the dog has given us a wrong answer you see the probabilities are better spread out so you might not in this situation say well 23 is the highest and so therefore it's a number eight you might want to say none of them stand more than a certain amount amongst their peers and therefore it's an i don't know that's one of the ways you can say i don't know because there's no one conclusive answer but it's up to you to think about the thresholds and the statistics and the probabilities of these things to make a decision about where that threshold should be put but this hopefully shows you the power of data augmentation the stuff that you can do just by using the same data tweaking it a bit adding it into the mix you suddenly get a network which is way more powerful and way more general so in summary these are the things you need to think about when you're training the data so here we had on our first example we had the number nine the door number it doesn't represent the training data set so if we want it to we need to get some pictures that do we need to add them in if we want to make our network perform better we want to do more data augmentation we may want to rotate images around or flip them or blur them or change the colors or delete parts of them you mess around with them to represent lossy data and hope that the lossy data we apply is a good approximation of the kind of lottiness that exists in reality if you can get a larger base training data set if you can't if you don't if you if you want to do the best job then getting more actual data collecting real data is going to do the best job if you go out and collect more photographs and get more pictures of handwriting that is going to help you a lot if you can't get real data then data augmentation is going to help on top of that if it's taking a while to give you a good network then just train it for longer you can in general keep on training and it will keep on getting better but bear in mind keep an eye on the validation accuracy and the loss to make sure that it's not over fitting against the training data set the more data you have that you're training on the less likely it is to overfit as well so that's also a benefit you get and also there's loads of numbers that we've included in this process the batch size the learning rates the dropouts the kernel size all these things we put in there play around with those numbers make your network larger or smaller see how they affect things understand the effect that they have on your eventual result so the last thing i want to talk about is something which comes up a lot when you talk about machine learning i think in a research context it's very important as well because as researchers we have a sort of responsibility to be honest and good about how we're approaching things now machine learning and your networks in particular do have a bit of this black box problem you put numbers in you do an algorithm you get a number out and it always gives you some kind of answer and it's very easy to end up in a computer says no situation here where you're creating a network to decide whether someone is deserving of getting a loan for example so you put in the description of them to do with their social socioeconomic background and where they live and how what their income is and it gives a number like yes or no a result like this one no and you don't have any recourse there it's just a machine giving you an answer so you need to think about is that a an appropriate thing to do as a society but inevitably machine learning is becoming more important it's taking over everything it's cropping up all over the place and so you need to think about how you can do this stuff in an honest way now google have a set of ai principles and i've grabbed the few of them from here i also wouldn't say that necessarily google follow their own ai principles but it's nice to see them written down somewhere and we can choose to follow them so the first thing i think one really useful for researchers especially those that publicly funded is to be socially beneficial don't design a new network which is going to not give loans to people of color or that's going to make a missile kill someone or anything like this design stuff that is going to be good be good people be nice be helpful avoid creating or reinforcing unfair bias there's a classic example with the um microsoft connect their their xbox controller where they design some machine learning and networks to be able to identify people standing in positions they can dance around and control the machine when they took it on tour to all the game shows people from the public were coming along and having a go and it wasn't seeing them it wasn't seeing their faces and it turned out it was doing a bad job of seeing them because they hadn't trained the networks on people with a diverse set of skin colors they trained it on the people who were the engineers at that particular group in the company and they were mostly white and they'd failed to identify the broad spectrum of people that would be using the technology and that is an unfair bias for sure make sure stuff's designed for safety so have things which fail safely if it's unsure about something er on the side of asking a human and including people in the loop as well make sure you don't fall on the side of i'm not sure so let's not give them a loan and you know subject them to a lifetime of misery or whatever think about how it's going to be affecting the humans in the process be private with things it's easy when you're designing a network to accidentally have the eventual network containing personal information you might have a weight in there which accidentally represents the age of a particular person you've trained it on and there are techniques statistical techniques which you can use to extract that out so be really careful about the data that you're training it on and how you're going to incorporate privacy into that and as many of us are researchers or scientists uphold high standards of scientific excellence this means be honest about how you're training your data set be honest about the testing you've done on it make sure you honestly split your tests and training data sets make sure you've analyzed the results in a thorough and comprehensive way be good citizens and be good scientists is a big summary coming out of this but that's the end of the session today thank you all for your attention i know it's a lot of information in three hours and i really appreciate all the brilliant questions we've had so thank you all very much and i hopefully will see you again soon
