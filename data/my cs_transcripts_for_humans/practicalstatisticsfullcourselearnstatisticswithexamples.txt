With timestamps:

00:05 - here we'll do a quick example
00:06 - where we identify the sample and the
00:08 - population in a survey
00:11 - question asks a survey asked 2 000 u.s
00:13 - households if they currently own at
00:15 - least one pet
00:16 - the results show that 69 of households
00:19 - do own at least one pet
00:20 - identify the sample and the population
00:22 - in this situation
00:24 - remember with a survey like this the
00:27 - population is the group that we're
00:29 - interested in knowing something about
00:31 - but it's usually not feasible to study
00:34 - the entire population
00:35 - so we gather data from a small subset of
00:38 - that group
00:39 - so in this case we're interested in
00:41 - knowing about all us households
00:43 - so that's the population notice that
00:45 - that wasn't explicitly stated but it's
00:47 - clear from the problem statement that
00:48 - that's what we're interested in knowing
00:49 - about
00:50 - because it's infeasible to study all
00:53 - households in the u.s we take this
00:54 - sample
00:55 - of 2000 households
01:01 - we gather data from them and we use that
01:03 - to draw inferences
01:04 - about the entire population here we're
01:08 - looking at the idea of representative
01:09 - samples
01:10 - so if we're looking to measure something
01:12 - about a population
01:14 - we want to gather a sample to measure
01:17 - and we want to make sure that when we do
01:18 - that the sample represents
01:20 - the population that it looks similar to
01:23 - the population as a whole
01:25 - so we'll look at a couple examples here
01:26 - first of all to find the average
01:28 - annual income of all adults in the
01:30 - united states suppose we sampled
01:32 - representatives in the congress of the
01:34 - united states
01:36 - it turns out this is not a very
01:37 - representative sample first of all the
01:39 - salary for representatives in congress
01:41 - is set at a fixed number
01:43 - and that number is relatively high
01:44 - compared to
01:46 - the average income for all adults in the
01:49 - united states
01:50 - so it's not representative because if
01:52 - you look at the whole population there
01:53 - are some people who make
01:54 - very little and some people who make a
01:56 - lot and
01:58 - in the congress there's a fixed value
02:00 - that's unlikely to be
02:02 - similar to the average value of all
02:05 - adults in the us so it's not a very good
02:07 - representative sample
02:08 - we would say no this is not
02:09 - representative
02:11 - the second example says to find out the
02:13 - most popular cereal among children under
02:15 - the age of 10
02:16 - you could stand outside a large
02:18 - supermarket one day and pull every 20th
02:20 - child
02:21 - under the age of 10 who enters the
02:22 - supermarket
02:24 - it's not clear that there's any bias in
02:26 - this one this seems like a pretty good
02:28 - way
02:29 - to find an answer to this question if
02:32 - you pull
02:33 - children coming into a supermarket of
02:35 - the right age group
02:36 - you're likely to get a pretty
02:38 - representative sample for all children
02:39 - to do this
02:40 - now you may want to pick different areas
02:42 - of the country for instance
02:43 - there could be differences depending on
02:45 - where you look but
02:47 - without going any deeper it doesn't look
02:49 - like there are any obvious
02:50 - red flags that this would not be
02:52 - representative for all
02:54 - the answers you're looking for so this
02:55 - one looks fairly good and the lesson
02:57 - from these is just that when you're
02:58 - gathering a sample it's important to
03:00 - look for a representative one
03:02 - one that's likely to look similar to
03:04 - your population you don't want a sample
03:06 - that's chosen too narrowly or that's
03:08 - chosen with some sort of obvious bias
03:11 - this is a simple example that
03:13 - illustrates a way that a sample can be
03:15 - biased
03:16 - here a coach is interested in how many
03:18 - cartwheels the average college freshman
03:20 - can do at his university
03:21 - eight volunteers from the freshman class
03:23 - step forward after observing their
03:25 - performance
03:26 - the coach concludes the college freshman
03:28 - can do an average of 16 cartwheels in a
03:30 - row
03:30 - without stopping is this sample random
03:33 - and representative
03:34 - in general a good sample is random and
03:37 - representative
03:39 - a simple rule of thumb for deciding
03:41 - whether a sample is random or not
03:42 - is just to think about whether or not
03:45 - every member of the population
03:47 - is equally likely to be selected if so
03:50 - there's randomness involved
03:51 - to decide whether or not the sample is
03:52 - representative think about whether the
03:54 - sample
03:55 - looks similar to the population here the
03:58 - biggest source of bias that we observe
04:00 - and bias means that the results will be
04:02 - skewed
04:03 - is this voluntary response bias
04:10 - voluntary response bias means that
04:12 - rather than
04:13 - picking people to ask the coach asks for
04:16 - volunteers
04:18 - in this case people that are able to do
04:20 - more cartwheels are more likely to step
04:21 - forward and volunteer for the study
04:23 - because of that we conclude that this
04:25 - probably isn't a very good sample
04:27 - to do this study a voluntary response
04:30 - bias also comes into play
04:32 - in surveys that have questions
04:35 - where certain responses are more
04:37 - favorable than others
04:40 - in this example we're going to decide
04:42 - which type of sampling is being used
04:44 - in each description the first situation
04:47 - we have a soccer coach
04:48 - who selects six players from a group of
04:51 - boys
04:51 - aged eight to ten then seven players
04:55 - from the group of boys aged 11 to 12 and
04:58 - finally three players from a group of
05:00 - boys aged 13 to 14
05:02 - to form a rec team notice the key here
05:04 - which is that the coach has divided the
05:06 - group
05:07 - into segments based on their ages so
05:09 - there's a segment from eight to ten
05:11 - a segment from 11 to 12 and then a
05:13 - segment from 13 to 14.
05:14 - and from each segment the coach has
05:16 - selected
05:18 - several players so after dividing into
05:21 - segments
05:22 - the sampling could be either stratified
05:24 - or clustered depending on what happens
05:26 - next
05:26 - but the fact that we're choosing a
05:28 - couple from each group
05:30 - makes it stratified if we chose a couple
05:32 - of groups
05:33 - all together that would look more like
05:35 - cluster sampling
05:36 - so this first one is stratified sampling
05:39 - now generally with stratified sampling
05:40 - we select the same number from each
05:42 - group
05:43 - in this case the coach didn't do that he
05:44 - selected six from one group
05:46 - seven from another and three from
05:48 - another but generally speaking with
05:49 - stratified sampling we select the same
05:52 - number from each group in the second
05:54 - part there's a pollster who interviews
05:57 - all human resource personnel in five
05:59 - different high tech companies
06:01 - this may be hard to see at first but the
06:04 - fact that
06:04 - this pollster selected five companies
06:08 - leads you to think that they looked at
06:10 - all the companies that were out there
06:12 - and thought of each company as a group
06:15 - and they selected all human resource
06:18 - personnel
06:19 - from a few of these groups so by
06:21 - dividing them into groups
06:22 - again we can think about either
06:23 - stratified or cluster sampling starting
06:25 - that way
06:26 - and then because the pollster selected
06:28 - everyone
06:29 - from a couple of groups that's cluster
06:32 - sampling if they had selected a few
06:34 - from all the groups that would be
06:36 - stratified sampling
06:37 - but the fact that they selected a few
06:39 - full groups makes it
06:41 - cluster sampling the third one a high
06:43 - school educational counselor
06:45 - interviews 50 female teachers and 50
06:47 - male teachers
06:49 - notice again that there's a separation
06:51 - of categories
06:52 - and so the teachers have been separated
06:54 - into male teachers and female teachers
06:56 - they've been divided into groups and
06:58 - then from those groups
07:00 - some have been selected an equal number
07:02 - from each
07:03 - which again looks like stratified
07:05 - sampling
07:08 - next a medical researcher interviews
07:10 - every third cancer patient
07:12 - from a list of cancer patients at a
07:13 - local hospital and the key here
07:16 - is that term every third which
07:19 - is what identifies this as systematic
07:21 - sampling systematic sampling is where we
07:22 - have a list like this
07:24 - and we pick some step like this like
07:27 - three
07:28 - and we check every third or we could
07:31 - pick every fifth
07:32 - or every tenth whatever it is that
07:34 - systematic
07:35 - moving through the list is what makes
07:38 - this
07:39 - systematic sampling the next one a high
07:41 - school counselor uses a computer to
07:43 - generate 50 random numbers
07:45 - and then pick students whose names
07:46 - correspond to the numbers
07:48 - notice how there's no division into
07:50 - groups there's no systematic process
07:53 - this is the full population of students
07:56 - and we just
07:57 - select random numbers from that full
07:59 - group
08:00 - and that's what makes this simple random
08:02 - sampling that's kind of the simplest
08:04 - version where we're looking at the full
08:06 - population
08:07 - and using a random number generator to
08:10 - just select
08:11 - without any division into groups or
08:13 - anything else
08:15 - lastly a student interviews classmates
08:17 - in his algebra class to determine how
08:19 - many pairs of jeans a student at a
08:20 - school owns
08:21 - on the average notice here that this
08:25 - student
08:25 - is looking for information about the
08:27 - whole school
08:29 - but rather than looking at a full
08:30 - student list and selecting randomly from
08:32 - them
08:33 - or selecting every third student or even
08:35 - dividing them into groups
08:37 - the student just asks the students
08:39 - nearby
08:41 - the students that are in this class next
08:43 - to him
08:44 - which makes this a convenient sample
08:48 - in this example we'll see how to select
08:50 - a simple random sample
08:51 - from a population the population we're
08:53 - given is a set of six
08:55 - quiz scores and there are 10 students
08:58 - in this example we'll draw a dot plot
09:00 - and we're given data that represents
09:02 - the ages of 30 randomly chosen mba
09:05 - players
09:06 - the first thing we need to do is draw an
09:07 - axis that will cover the full range of
09:10 - the data
09:10 - so just kind of scanning through it
09:12 - looks like the lowest value here
09:14 - is around 20 and the highest value
09:18 - is about 36. so let's make sure our
09:21 - range will at least cover those values
09:24 - let's draw an axis here
09:26 - with values from 20 up to 36.
09:29 - now that we have our axis all we need to
09:30 - do is read through each of these data
09:32 - points and put a dot for each one
09:34 - so for the first value at 22 we just put
09:36 - a dot above the 22
09:40 - then we have 28 so we'll put a dot at
09:42 - the 28.
09:43 - notice it hovers a little bit above the
09:45 - marker but it doesn't really matter
09:47 - how high we put them as long as we place
09:49 - them at consistent heights
09:51 - just so we can visualize the final
09:52 - result that'll make more sense as we
09:54 - draw more of these
09:56 - the next value is at 20
09:59 - then at 24
10:03 - then at 26 then 21
10:08 - 27 and then we get another 28 so
10:11 - the second time we've seen 28 so we
10:14 - won't draw the second dot at the same
10:15 - place as the first one
10:17 - but we'll put it right above it so now
10:19 - there's a second dot
10:20 - 28 then we go to 31
10:24 - and 29 and just continue on entering
10:26 - these
10:27 - we hit another 24 here and again we just
10:30 - put this one
10:31 - above the first one at that location
10:34 - then we have another 22
10:36 - another 21 then a 25
10:40 - another 22 another 25. 30
10:47 - another 29 another 20
10:52 - and so on there's our 36
10:56 - another 24 the first 23
11:01 - another 36 another 24
11:06 - another 29
11:10 - and we'll just finish this out
11:15 - so there we see our dot plot where each
11:18 - time we run into
11:19 - a value we've already drawn a dot for we
11:21 - just draw a one a little bit higher
11:23 - so notice that the height of these
11:25 - stacks
11:26 - tells us how frequent an age is so the
11:30 - more frequent ones are over here
11:32 - and then the 36s there's kind of what we
11:34 - would call an outlier
11:36 - they're far out from the other data
11:38 - points and for instance
11:40 - 30 is relatively rare there's only one
11:42 - of those
11:43 - and so on so there's a lot we can tell
11:45 - from this plot and we'll draw other
11:47 - types of plots later on
11:48 - that fit this same kind of pattern of
11:51 - looking for where the data is clustered
11:53 - and where it's spread out but a dot plot
11:55 - is a very simple way
11:56 - to observe that at first
11:59 - here we're going to build a simple
12:01 - frequency table the problem states that
12:03 - 19 people were asked how many miles to
12:06 - the nearest mile
12:07 - they commute to work each day and their
12:08 - responses were recorded in this data set
12:11 - the frequency table lists each possible
12:14 - data value
12:15 - and the number of times that data value
12:17 - occurs so we put two columns like this
12:20 - one for the data values and one for
12:21 - their frequencies
12:23 - now we'll go through the data set and
12:25 - for each unique data value that we see
12:28 - we'll list that in the left hand column
12:30 - once we've filled in all these data
12:32 - values
12:32 - now we just need to count how many times
12:34 - they occur so for instance i notice that
12:36 - 2 appears twice in the data set
12:38 - so it has a frequency of 2. 3
12:42 - appears only once so it has a frequency
12:44 - of 1
12:45 - and so on and i fill in the rest of the
12:47 - table and it's really as simple as that
12:50 - once we've filled in the frequency table
12:52 - a quick check that we can do is
12:54 - add up all the frequencies and they
12:56 - should add up to how many data points we
12:57 - have
12:58 - and here if we add up these frequencies
13:00 - we do find a total of 19.
13:04 - in this example we'll build a frequency
13:05 - table for categorical data
13:08 - we're given a sample of mba players with
13:11 - their
13:12 - position which is a categorical variable
13:15 - it divides them into categories
13:17 - point guards shooting guards small
13:19 - forwards power forwards and centers
13:21 - and we're going to build a frequency
13:22 - table to go along with this
13:24 - so the categories will be the divisions
13:28 - and then we'll count the frequency in
13:30 - each category so our frequency table
13:32 - will start with
13:34 - two columns one for the position
13:37 - and one for the frequency
13:41 - and then we'll add a third column for
13:43 - relative frequency
13:45 - it's a good thing to include when you
13:47 - draw a frequency table
13:48 - it's not entirely necessary every time
13:50 - but it's a good idea
13:51 - to include it when you can so the
13:53 - positions we have
13:55 - point guard shooting guard small forward
13:59 - power forward and center as our five
14:01 - categories
14:02 - and then we just go through and count
14:04 - how many there are of each one so for
14:05 - point guards for instance we have one
14:07 - two three four of them
14:11 - so the frequency is four for shooting
14:13 - guards you can count one
14:14 - two three four
14:17 - five six seven eight nine
14:21 - for small fours we have one two three
14:24 - four five six seven
14:30 - for power forwards we have one two three
14:33 - four and five
14:36 - and then for centers we have one two
14:38 - three four and five
14:40 - and notice if we add up those
14:41 - frequencies we should get the total
14:44 - number that we have
14:45 - which is 30. so we should get 30 if we
14:46 - add those up and you can check that by
14:48 - adding those frequencies
14:50 - for the relative frequency we need to
14:51 - divide each frequency by
14:53 - the total number that we have in our
14:55 - sample which is again 30.
14:56 - so for the relative frequency of point
14:58 - guards we would divide
15:00 - 4 by 30 which we could write as a
15:02 - fraction or if we wanted to
15:04 - we could write that as a decimal which
15:06 - comes out to about 0.133
15:08 - or 13.3 percent then for the next one we
15:12 - would divide
15:13 - nine by thirty which works out to thirty
15:16 - percent
15:18 - seven out of thirty is twenty three
15:21 - point three percent
15:23 - and then five out of thirty it's about
15:25 - 16.7
15:28 - and that's for both the last two
15:29 - categories
15:32 - so again a frequency table is pretty
15:34 - easy to
15:35 - construct all you have to do is count
15:38 - how many fall into each category
15:40 - in this case the categories were the
15:41 - positions of the players
15:43 - here we're going to build a grouped
15:45 - frequency table using the data set of
15:47 - nba players with their points per game
15:50 - shown below so we have 30 values
15:52 - ranging from 1.4 up to about 24
15:59 - and we're told to use a class width of 5
16:02 - for our group frequency table
16:04 - so we want to start low enough that we
16:05 - can cover all of them
16:07 - and rather than starting at one let's
16:09 - start at zero just to get a nice round
16:11 - number
16:12 - and make it easy for ourselves so in our
16:14 - frequency table
16:15 - the first column will be points per game
16:18 - and our first category the first class
16:22 - will start at zero and we'll go up to
16:25 - five but remember we won't go all the
16:27 - way to five because we don't want to
16:30 - overlap our classes so the first one's
16:33 - gonna go up to just less than five
16:35 - let's use 4.9 to represent just less
16:37 - than five
16:38 - and the next one will start at 5.0 and
16:41 - go up to 10
16:42 - but just below 10 so we'll stop at 9.9
16:46 - and then we'll go from 10 to 14.9
16:50 - 15 to 19.9
16:53 - and 20 to 24.9
16:56 - we don't need any more classes because
16:58 - that's as high as
16:59 - we need to go to cover everyone in this
17:02 - data set
17:03 - so the next column will be the frequency
17:07 - and then lastly we'll have the relative
17:09 - frequency
17:10 - since there are 30 values here in our
17:13 - data set
17:14 - once we have the frequencies we'll just
17:15 - divide each one by 30
17:17 - to get the relative frequency we won't
17:20 - show this in detail for all these
17:22 - classes but just for the first class
17:24 - from 0 to 4.9
17:25 - we'll go through and select all the ones
17:27 - that fit into that category
17:29 - so between 0.0 and 4.9 in the first row
17:33 - we find 4.9 in the second row we find
17:37 - 1.4
17:38 - and 2.0 and in the third row we find
17:41 - 3.0 4.3 and that looks like all of them
17:45 - so there are a total of five
17:47 - that fall into that range and then we
17:49 - can continue this on
17:50 - for all the others but i won't show the
17:53 - counting
17:54 - you can go through and do that yourself
17:56 - and i'll show you the results
17:57 - here so once you count all the others
17:59 - you should get 11 for the second class
18:02 - 5 for the third class 4
18:06 - and then 5 for the final class then if
18:08 - we divide each of these by 30 to get the
18:10 - relative frequency
18:11 - 5 divided by 30 is
18:15 - 0.16 repeating so you can round that to
18:19 - 0.167
18:20 - or 16.7 percent
18:23 - and then 11 out of 30.
18:26 - we could round to 36.7 percent
18:32 - 5 out of 30 again is 16.7 percent
18:35 - and then 4 out of 30.
18:39 - we could round to 13.3
18:43 - to get the relative frequency for each
18:45 - so it's really just counting
18:46 - the only thing to keep track of for
18:48 - grouped frequency tables
18:50 - is to make sure your classes are all
18:52 - evenly wide
18:54 - and that none of them overlap that's the
18:55 - important piece
18:57 - here we'll do the histogram using the
19:00 - data set
19:01 - for the players in the mba with their
19:03 - points per game listed here
19:05 - in the last example we built a grouped
19:08 - frequency table
19:09 - and now we're going to build a histogram
19:10 - that matches it so we're going to take
19:12 - the
19:12 - frequency table we built earlier and
19:14 - just draw this
19:16 - histogram to represent the same picture
19:18 - so due to that we'll start with a grid
19:20 - and the x values will range from 0 up to
19:23 - 24.9 or
19:24 - up to 25 so we'll have 0 here
19:28 - 5 10 15 20 and 25
19:32 - and each of these classes will fit
19:34 - between two of those values so the first
19:36 - class goes between 0 and 5
19:38 - the next one goes between 5 and 10 and
19:40 - so on and of course we know that it goes
19:42 - just up to 5 but not including 5
19:44 - and so on but we'll draw it as if it
19:46 - goes all the way to 5 just to
19:48 - make the picture as simple as possible
19:52 - then on the vertical axis we have the
19:54 - frequency so here we have
19:55 - the points here we have the frequency
20:01 - and the highest frequency we see is 11
20:03 - so we need to at least go up to 11.
20:05 - so let's have 10 here and five
20:14 - and then we just draw a bar to the right
20:16 - height for each category
20:18 - in the first class the frequency is five
20:20 - so we'll draw a bar up to five
20:24 - ranging from zero to five then from five
20:26 - to ten the frequency was eleven so that
20:28 - one goes all the way up
20:30 - to the top of the graph
20:34 - then from 10 to 15 that frequency was
20:37 - again five
20:39 - from 15 to 20 it goes down to four so
20:41 - we'll draw it a little shorter
20:43 - down at the tick mark for four
20:47 - and the last one goes back up to five
20:51 - so each class each category gets a bar
20:55 - with the height representing the
20:57 - frequency it's relatively simple
20:58 - once you've drawn the frequency table
21:01 - which just consists of counting the ones
21:03 - that fit into each class
21:05 - here we'll build a bar chart for this
21:07 - data set which is the
21:09 - positions for the players in the mba
21:11 - sample
21:13 - again we have 30 observations
21:16 - falling into one of five categories
21:19 - point guard shooting guards
21:20 - small forwards power forwards and
21:21 - centers first we need to find the
21:24 - frequency of each category
21:26 - which means building the frequency table
21:28 - but since we've already done that in a
21:29 - previous example
21:30 - i won't go through in detail and count
21:32 - those
21:33 - rather we'll just put the results here
21:37 - in this frequency table we have the five
21:39 - positions
21:40 - and the frequencies there are four point
21:42 - guards
21:43 - nine shooting guards seven small
21:45 - forwards and five power forwards and
21:47 - five centers
21:48 - now just like with the histogram we
21:50 - start with our grid
21:52 - and this time the horizontal axis
21:55 - will represent again our categories so
21:58 - we'll have
22:00 - five spots for our bars to go
22:08 - and then the vertical axis will again
22:10 - represent
22:12 - frequency so the horizontal axis is
22:14 - position
22:16 - the vertical axis is frequency the
22:18 - highest frequency we see is nine
22:20 - so we need to make sure we go up to at
22:21 - least nine
22:24 - once again go to ten and now we're ready
22:25 - to draw a bar for each position
22:28 - so rather than having the bars connect
22:30 - like they would with a histogram
22:32 - with a bar chart since we're thinking of
22:33 - these as separate categories
22:36 - they're not ones that flow into one
22:38 - another we'll draw the bars with some
22:39 - separation
22:40 - so at the point guard position we'll
22:42 - draw a bar that goes up to four
22:44 - since there are four of those
22:47 - and then at the shooting card position
22:49 - we'll draw one that goes up to nine
22:57 - small forwards have a frequency of seven
23:03 - and power forwards and centers each have
23:04 - a frequency of five
23:12 - so again we're drawing a bar where the
23:15 - height represents the frequency of that
23:16 - category
23:17 - but unlike with a histogram these bars
23:19 - are separated because we're indicating
23:21 - that these are separate categories
23:23 - and there's not a flow from one to the
23:25 - next
23:27 - let's build a stem-and-leaf plot the
23:29 - question says suppose you gathered data
23:31 - on how long it took you to get ready in
23:32 - the morning
23:33 - for 40 days you measured the amount of
23:34 - time between when your alarm went off
23:36 - and when you left the house the results
23:38 - are below rounded to the nearest minute
23:41 - and we want to build a stem-and-leaf
23:43 - plot for this
23:44 - for a stem-and-leaf plot we divide
23:47 - each value into its stem
23:50 - and its leaf and the stems are generally
23:54 - the tens place although you can tweak
23:56 - this and make it for instance the ones
23:58 - place
23:59 - but in this case since our values are
24:00 - two digit values
24:02 - the first digit will represent the stem
24:04 - and the second digit will represent the
24:06 - leaf
24:07 - so if you look through you'll notice
24:08 - that all these numbers start with either
24:11 - one two or three that's the
24:12 - the first digit so our stems could be
24:15 - one two
24:16 - or three and now we'll go through and
24:19 - for each value
24:20 - we'll place that leaf
24:24 - in the correct category so the first
24:26 - value is 35
24:27 - so we'll put a 5 under the leaves
24:30 - category
24:32 - in the third row then for 28
24:35 - we'll put an 8 here then 25
24:39 - we'll put a 5 here then 23
24:42 - and so on we'll put another 3 then 32
24:46 - means we'll put a 2 in the third column
24:49 - then 29 19
24:54 - 21 13 and so on
24:58 - and i'll go through and fill all these
24:59 - out in detail i'll go through and show
25:02 - the final result
25:03 - in just a moment now what you often find
25:05 - with the stem and leaf plot is that we
25:06 - write the leaves
25:07 - in the order of their value rather the
25:10 - order in which they appear in the data
25:11 - set so instead of writing 8
25:13 - five three three nine one we would write
25:15 - the one first and then the threes and
25:17 - the fives and so on
25:19 - now let's take some extra work because
25:20 - we have to do some sorting but when i
25:22 - show the results in a second
25:24 - it'll be shown in that way with them
25:26 - sorted
25:27 - by value now that we have all the stems
25:30 - and leaves written
25:31 - notice how it looks it looks almost like
25:33 - a histogram or a bar chart
25:35 - turned sideways where the length of each
25:38 - string of leaves
25:39 - represents the frequency of that
25:41 - category so it divides it into
25:42 - categories by tens
25:44 - and we can see that the most frequent
25:46 - category is the
25:48 - range from 20 minutes to 29 minutes and
25:51 - yet unlike
25:52 - a normal grouped frequency by grouping
25:54 - them we haven't lost any information
25:56 - we could reconstruct the entire data set
25:59 - from this stem-and-leaf plot
26:00 - if we had to in this example we'll
26:03 - construct a scatter plot
26:05 - for some data that we're given we're
26:07 - given the sizes
26:08 - of several tvs and the price that goes
26:11 - along with each one
26:12 - now our data is separated into three
26:14 - categories just so the table could fit
26:16 - easily here but side by side you'll see
26:19 - a size and a price
26:21 - where the size is given in inches and
26:22 - the price is given in dollars
26:24 - and we want to construct a scatter plot
26:26 - where we compare these two variables
26:29 - now when we construct a scatter plot we
26:30 - have to pick which variable will be x
26:32 - and which one will be y
26:34 - it's not crucial that we pick it in the
26:36 - right order because it turns out if they
26:37 - get
26:38 - switched much of the analysis we can do
26:40 - is still
26:41 - the same but if we can it would be nice
26:44 - to pick x and y
26:46 - in a reasonable way and usually we want
26:48 - to think about how
26:49 - x determines why in other words is there
26:52 - one of the variables that seems to
26:54 - control the other one
26:56 - would we say that the size determines
26:58 - the price of the tv
26:59 - or would we say that the price
27:01 - determines the size it's probably more
27:03 - likely that we would say
27:04 - that the price depends on the size or
27:07 - the size determines the price
27:09 - so we could call x the size and y the
27:12 - price so let's say our x-axis
27:14 - represents size and our y-axis
27:16 - represents price
27:18 - again we'll try to be consistent with
27:20 - these where the first column is x
27:22 - the second column is y but it turns out
27:24 - that if you switch the order
27:25 - it doesn't change too much at least at
27:27 - this point so now we need to put a scale
27:29 - on each axis
27:30 - so for the sizes the sizes go up to
27:33 - about
27:34 - 60 inches so let's make sure we include
27:37 - at least up to 60.
27:38 - let's say we go up to 70 here
27:42 - and if we start down at zero that means
27:44 - we need to divide this
27:46 - evenly so that we get to 70. so halfway
27:49 - there would be 35
27:51 - and then if we divide these by
27:54 - fives
28:00 - every 5 10 and so on
28:03 - and the prices range from 200 to about 2
28:07 - 800.
28:08 - so let's say we include all the way up
28:09 - to 3 000.
28:11 - and let's mark it in increments of 500.
28:15 - now for each
28:16 - value in our list let's take the first
28:18 - one for instance
28:20 - the size is 43 inches the price is 500
28:23 - so on the horizontal axis the size axis
28:27 - will go up to 43 which is around here
28:30 - between 40 and 45
28:32 - and then on the price side we'll be
28:35 - right at 500
28:36 - so we want to find out where those two
28:38 - cross
28:40 - which is right around here so our first
28:42 - point
28:43 - will be right there at 43 inches
28:47 - and 500
28:50 - for the next one we'll go to 55 and 900
28:53 - so 55 is right here between 50 and 60.
28:56 - 900 is right here right under a thousand
29:00 - so those two look like they cross right
29:01 - around here which gives us our second
29:03 - point
29:04 - and then we'll continue for the rest of
29:06 - them without
29:08 - showing each one in detail i'll just
29:10 - show the final picture here in a moment
29:12 - after we've drawn all these points so
29:15 - there's the final result and even though
29:16 - it's hand-drawn and imperfect
29:18 - we can see the general trend which is
29:20 - that larger tvs tend to cost
29:22 - more so there's this general upward
29:25 - trend
29:26 - as you scan through this picture that's
29:29 - really the value of a scatter plot is to
29:30 - look for an association or a connection
29:33 - between two variables
29:34 - to see if there's a relationship and
29:36 - this one looks like there's this upward
29:38 - trend
29:39 - later on we'll talk about how to draw a
29:42 - line
29:43 - or some other curve that represents this
29:46 - shape but for now just notice that
29:48 - there's that relationship between the
29:50 - two
29:52 - find the median of the salaries listed
29:54 - below so these are the salaries from the
29:56 - mba data set we have 30 values and we
29:59 - want to find the median
30:01 - remember that the median is the middle
30:03 - data point
30:04 - now helpfully these salaries have been
30:06 - listed for us in order
30:08 - from smallest to largest so notice we
30:10 - have the smallest value here
30:11 - and then they increase along this row
30:14 - and then down to the next row
30:15 - until we reach the largest value at the
30:17 - bottom right and that's important when
30:19 - you're finding the median
30:20 - to order them from least to greatest or
30:24 - from greatest to least
30:25 - so that when we look in the middle of
30:26 - that list the middle value is truly
30:30 - in the middle between the highest and
30:31 - lowest so that half of the data points
30:33 - fall below it
30:34 - and half of them fall above it so since
30:37 - it's listed like this for us we can look
30:39 - halfway through since there are six rows
30:42 - at the end of the third row and the
30:45 - beginning of the fourth row
30:47 - we have the middle so the middle is
30:49 - right between
30:51 - this value and this value
30:54 - since there are two values in the middle
30:56 - which will happen every time we have an
30:58 - even number of data points
30:59 - we need to find the number halfway in
31:01 - between those two
31:02 - which we do by averaging them together
31:04 - so find the average of those two numbers
31:06 - in the middle
31:07 - that will be the median if we had an odd
31:09 - number of data points like say we had 29
31:11 - players
31:12 - we would find the middle value and that
31:15 - would be our median
31:16 - we wouldn't have to find any average but
31:18 - anytime there's an even number
31:19 - there will be two in the middle so we
31:21 - need to find whatever's halfway between
31:23 - those two
31:23 - so the median will be halfway between
31:27 - four million four hundred and sixty nine
31:28 - thousand one hundred sixty and
31:32 - 4 million 767 000.
31:35 - so if we add those together and divide
31:36 - by two
31:38 - what we get is four million six hundred
31:41 - and eighteen
31:41 - thousand eighty and that's the median
31:45 - half of the players make less than four
31:48 - million six hundred eighteen thousand
31:49 - half of the players make more so that's
31:51 - the middle data point
31:53 - and it's a good measure of the center
31:57 - here we'll find a weighted average using
32:00 - a
32:00 - student's score in a class so we have
32:03 - several assignments
32:04 - three tests homework project and final
32:06 - exam and we're given the student's score
32:08 - on each of these assignments
32:10 - as well as a weight for each assignment
32:13 - or a number of points for them
32:14 - now notice that usually we'd be given
32:16 - either the weight
32:18 - or the points in this case we're given
32:20 - both and if you look closely
32:22 - there are a total of a thousand points
32:24 - that they could earn if you add up all
32:26 - those points
32:27 - you should get a thousand and if you
32:30 - divide each of those point values by a
32:32 - thousand
32:33 - you'll get the percentages that are
32:35 - listed here so really
32:36 - the same information is given in the
32:38 - weight column or in the points column
32:40 - and you may see for some of your classes
32:43 - the scores are given with weights
32:45 - sometimes they are given with points
32:46 - it's really the same thing
32:48 - it's just written in a different way so
32:50 - we're going to show the calculation with
32:51 - both
32:52 - just to compare but we'll get the same
32:54 - answer either way for part a then
32:56 - let's use the weights
33:00 - so for the weights we can multiply each
33:03 - score the student got
33:05 - by the weight that's associated with it
33:08 - and once we multiply those and add them
33:10 - all together
33:11 - the answer we'll get at the end is the
33:14 - average score
33:15 - so the weights are nice because they're
33:17 - already
33:18 - scaled for us so that just by
33:20 - multiplying the score
33:21 - times the weight and adding those
33:23 - together we'll get the final
33:25 - weighted average so i have to do is take
33:28 - 85 percent
33:31 - times 20
33:35 - plus 92 percent times twenty percent
33:40 - plus eighty-seven percent and so on so
33:43 - if we multiply all those
33:44 - and add them all together the answer we
33:47 - get at the end is about point eight
33:48 - nine nine which works out to 89.9
33:53 - so on a standard 10-point scale this
33:55 - student will be very close to an a
33:57 - just under 90 now let's do the same
34:00 - thing with a point system
34:02 - and we're going to get the same answer
34:03 - of course but this time we have to do
34:05 - two steps
34:07 - where first we multiply their score so
34:10 - for the first category they got
34:11 - 85 percent times the number of points
34:14 - they got
34:15 - and then do that for all of them and
34:18 - once we multiply and add all those up
34:20 - we get 899 points so they earned a total
34:24 - of 899 points
34:26 - and then when you divide that out of the
34:28 - total of number of points they could
34:29 - have earned which was a thousand
34:31 - they get .899 which again works out to
34:35 - 89.9
34:37 - so notice you're doing the same work
34:38 - both ways it's just that when you use
34:40 - the weights
34:41 - you've already divided by a thousand
34:43 - before you start
34:44 - when you use the points you have to
34:46 - divide by a thousand at the end
34:48 - to get the final score the same way you
34:50 - did with the weights so either way you
34:51 - do it
34:52 - you're using the same values and this is
34:54 - how you do a weighted average
34:55 - whether you're given the weights as
34:57 - percentages
34:58 - or as points out of a total that you
35:01 - could
35:02 - earn here we'll find the mode of a data
35:04 - set that's summarized for us
35:06 - we're already given the frequency table
35:08 - rather than just the raw data
35:10 - so most of the work is done for us the
35:12 - mode is the most
35:13 - frequent data value and the nice thing
35:16 - about having the data given to us as a
35:18 - frequency table
35:19 - is all we have to do is look through the
35:20 - frequencies and find the highest
35:22 - frequency
35:23 - notice the highest frequency is four and
35:26 - occurs
35:27 - three times there are three values which
35:30 - occur
35:31 - four times and are tied for most
35:33 - frequent and those are
35:34 - 21 22 and 24 so there are actually three
35:38 - modes
35:38 - and this can happen a lot in our case
35:41 - the modes are 21
35:42 - 22 and 24.
35:46 - here we'll find the range of a data set
35:48 - that we're given and the data set
35:50 - is the heights of the players given in
35:53 - the mba data set
35:55 - the range is really simple it's just the
35:57 - difference between
35:58 - the smallest and the largest so all we
36:00 - have to do is look through this list
36:02 - and find the smallest number and the
36:03 - largest number that occur
36:05 - if you scan through you should be able
36:07 - to pick out that the lowest number
36:08 - is 1.78 1.78 meters
36:12 - is the shortest player in the sample
36:14 - that we listed
36:15 - and the tallest player is 2.13 meters
36:19 - so the range is just the difference
36:20 - between those two if we take
36:22 - 2.13 minus 1.78
36:26 - the range is the difference or 0.35
36:29 - meters
36:32 - that's a really simple way of measuring
36:35 - how spread out the data is
36:36 - if the range is larger it's more spread
36:38 - out if the range is smaller
36:40 - it's more tightly clustered together so
36:42 - 0.35
36:44 - when compared to the values in this data
36:46 - set is a fairly small range so they're
36:48 - fairly
36:49 - tightly clustered together all of these
36:50 - players are relatively tall
36:52 - here we'll calculate the standard
36:54 - deviation by hand using the formula
36:56 - instead of using the built-in function
36:58 - in the calculator
36:58 - just to illustrate how the formula works
37:01 - we have
37:02 - five data points and we want to
37:03 - calculate the standard deviation
37:05 - the first thing we need to do is to
37:07 - calculate the mean
37:08 - so the mean remember is the sum of the
37:11 - data points
37:12 - divided by the number of data points so
37:15 - we
37:16 - add them up and divide by 5 and we find
37:18 - that the mean
37:19 - is 10.2 now the standard deviation of
37:23 - this data set
37:24 - is the square root of the sum
37:27 - of the squared deviations divided by
37:31 - n minus 1 and all that's inside the
37:33 - square root so we'll need to calculate
37:35 - these things
37:36 - individually we need to find each
37:37 - deviation
37:39 - the difference between each data point
37:41 - and the mean
37:42 - square those add them up divide that
37:45 - answer by n minus 1
37:47 - and then take the square root of that
37:48 - answer so this can get kind of tedious
37:51 - which is why we only do this with small
37:53 - data sets and from this point on we'll
37:55 - use
37:55 - the built-in function in the calculator
37:57 - but here we'll just illustrate
37:59 - the formula once we'll use this table to
38:02 - organize everything
38:03 - we have the data values listed and we'll
38:05 - calculate the deviation for each one by
38:07 - simply subtracting
38:09 - this number 20 for instance minus
38:13 - 10.2 and then 4 minus 10.2 and 15 and so
38:16 - on
38:17 - and we'll check the deviation for each
38:18 - data value
38:20 - once we've done that we square them all
38:22 - and remember
38:23 - the reason that we square them is
38:25 - because if we tried to average these
38:27 - deviations
38:28 - we would get zero because the positive
38:31 - ones and the negative ones would cancel
38:32 - each other out
38:33 - but by squaring them we end up with all
38:35 - positive numbers so when we average
38:37 - those
38:38 - they don't cancel each other then here
38:40 - all the squared deviations are filled in
38:42 - now we need to add those squared
38:44 - deviations
38:46 - divide by n minus 1 or 4 in this case
38:49 - and then take the square root of the
38:50 - answer adding these all up we get
38:53 - 210.8 and then
38:56 - if we divide that
39:00 - by 4 we get
39:03 - 52.7 but we're still not done the last
39:07 - step is to take the square root of that
39:10 - the square root of 52.7 is
39:13 - 7.26 so the standard deviation of this
39:16 - data set
39:17 - is 7.26 or the distance that a typical
39:22 - data point is from the center again this
39:25 - example mostly illustrates why we don't
39:27 - calculate the standard deviation by hand
39:29 - usually
39:30 - even for a small data set it gets pretty
39:31 - tedious
39:34 - a random sample of 11 statistics
39:36 - students produce the following data
39:38 - where x is the third exam score out of
39:40 - 80
39:41 - and y is the final exam score out of 200
39:44 - here we'll use a graphing calculator to
39:47 - find the equation of the least squares
39:48 - regression line
39:50 - so that we can predict the final exam
39:52 - performance which is why
39:54 - based on the third test score which is x
39:58 - so to use the calculator we first need
39:59 - to enter the data
40:01 - if we enter the stat menu and edit
40:04 - we can enter the data here which i've
40:05 - already done the first
40:07 - list we've entered the x's and in the
40:09 - second list we've entered the y's
40:11 - now if we go back under the stat menu we
40:14 - want to calculate so we'll scroll over
40:16 - to the calc menu
40:17 - and we want linear regression the form
40:20 - we've been using
40:20 - is this number 4 reg ax plus b
40:25 - so a is the slope and b is the
40:27 - y-intercept
40:28 - if we select that we don't need to
40:31 - change anything here because we entered
40:32 - the x's in list 1
40:34 - and the y's in list 2. so nothing needs
40:36 - to be changed we can just scroll down to
40:39 - calculate and it gives us the equation
40:43 - the form of it is y equals ax plus b and
40:46 - we get the values for a and b
40:48 - as well as values for r squared and r
40:51 - and now we have a better sense of what
40:53 - this r represents
40:54 - that's the correlation coefficient so
40:56 - the correlation coefficient is about
40:57 - 0.66
40:59 - which means there's a moderate linear
41:01 - relationship
41:02 - and it's positive so there we go there's
41:05 - our regression line
41:06 - and there's the r value that goes with
41:07 - it
41:09 - here we're given a regression equation
41:11 - which we developed in a previous example
41:14 - to predict the price of a house based on
41:17 - its square footage
41:18 - so if x represents the square footage we
41:21 - can
41:21 - predict the price in thousands of
41:24 - dollars so if we entered a thousand for
41:26 - the square footage for x
41:27 - we could calculate a value for y that
41:30 - would be the predicted price
41:32 - for houses with that square footage and
41:35 - we're going to use this equation to
41:36 - predict the price of homes with two
41:38 - different square footage values
41:39 - so all we have to do in each case is
41:42 - replace x
41:43 - with the given value for square footage
41:46 - in the first case we have y hat equals
41:50 - 0.099 times 2700
41:54 - plus 160.8 which works out to
41:58 - 428.1 so that
42:01 - corresponds to 428 thousand
42:04 - one hundred dollars and that technically
42:07 - means that the
42:07 - average house price we would expect for
42:10 - all the houses that are twenty seven
42:11 - hundred square feet
42:12 - would be four hundred and twenty eight
42:14 - thousand dollars approximately
42:16 - and then in the second case we can make
42:20 - a similar prediction
42:23 - for houses with 4 500 square feet
42:31 - and that works out to just over six
42:33 - hundred thousand
42:36 - then the question asks which prediction
42:38 - do you expect to be more reliable
42:40 - now for this we'd really have to go back
42:41 - and look at the data which we don't have
42:43 - in front of us
42:44 - but if you go back in the textbook and
42:45 - look at the data you'll notice
42:47 - that in the range of houses that we have
42:49 - data for
42:51 - 2700 square feet falls within that
42:54 - range and 4 500 square feet does not
42:58 - none of the houses we have in our data
42:59 - set are nearly as big
43:02 - as 4 500 square feet so it turns out
43:05 - that the first one is more likely to be
43:07 - reliable
43:08 - without external information for all we
43:09 - know that now it seems more likely that
43:12 - the first prediction would be more
43:13 - reliable
43:14 - because we've seen houses that are
43:16 - similar to it in our data set
43:18 - the first example we call interpolation
43:21 - where we are predicting
43:23 - within our data range the second one we
43:25 - call
43:26 - extrapolation where we're predicting
43:28 - beyond our data range
43:29 - and in general extrapolation is
43:31 - dangerous because we don't know
43:32 - quite what could be happening beyond the
43:35 - range of data we've actually looked at
43:37 - so in general interpolation is more
43:40 - reliable than extrapolation
43:42 - so the first prediction is more likely
43:44 - to be reliable
43:45 - even though we don't have a lot of
43:47 - information about it at the moment
43:50 - here we'll do a linear regression
43:52 - problem with several steps
43:54 - first we're given some data which is a
43:57 - set of quarterbacks in the nfl during
43:59 - the 2019 season
44:01 - and for each quarterback we're given
44:02 - their height and their weight we're
44:04 - going to compare these two
44:05 - now without any other information at the
44:08 - moment we're going to assume that height
44:10 - is going to represent x and weight will
44:12 - represent y just because they're ordered
44:14 - that way
44:15 - later on in the problem we might have
44:16 - more information that'll tell us which
44:18 - one should be x and which one should be
44:20 - y
44:20 - but for now we'll just assume that and
44:22 - change if necessary
44:24 - so first we want to calculate the
44:25 - correlation coefficient and to do this
44:27 - we'll use the calculator
44:29 - so first we need to go to the calculator
44:31 - and enter this data
44:32 - to enter the data we'll hit the stat
44:34 - button and hit enter to get into the
44:36 - edit menu
44:37 - and under list one we'll enter the x
44:40 - values so the heights
44:41 - that we saw the first quarterback had a
44:43 - height of 75 inches
44:45 - then 74 74
44:50 - 70 71
44:55 - 74 76
45:01 - 77 72 and 74.
45:07 - and then we can scroll over to the
45:09 - second list for their weight
45:12 - and we'll enter these as well once we
45:14 - have all the data entered we can go back
45:15 - into the stat menu
45:17 - and scroll over to the calc down to the
45:20 - linear regression option
45:23 - again we've entered x's in list one and
45:25 - y's and list two so we don't need to
45:26 - change anything
45:27 - if those were switched it wouldn't
45:29 - change the correlation coefficient
45:30 - anyway
45:31 - so for this first part it didn't really
45:33 - matter which way we entered them
45:34 - but later on it will be significant so
45:37 - we'll just hit calculate
45:39 - and it goes ahead and gives us the
45:40 - equation for the line
45:42 - which we don't need just yet all we're
45:44 - looking for at the moment is the value
45:46 - of
45:46 - r which is about 0.6 the first part of
45:49 - the question just asked for the value of
45:51 - r so now that we have that we can go
45:53 - back to the notes and enter that
45:55 - now for the second part of the question
45:56 - we want to know is there a strong linear
45:58 - relationship so based on the value of r
46:01 - we know first of all that there is a
46:02 - positive relationship since r is
46:04 - positive
46:05 - and we wouldn't necessarily say it's a
46:07 - strong relationship because it's less
46:09 - than 0.8
46:10 - which again is not a magical number but
46:12 - it's sort of an agreed upon
46:14 - level for a strong relationship but 0.6
46:17 - we might say there is a moderate
46:19 - relationship and often a moderate
46:20 - relationship is good enough
46:22 - to continue on and we'll continue to do
46:25 - the rest of the problem
46:26 - so we wouldn't necessarily say it's a
46:27 - strong relationship but it is a moderate
46:29 - linear relationship the next part of the
46:32 - question asks for the regression line
46:35 - which we've already calculated now
46:36 - notice the direction here the regression
46:38 - line is going to predict
46:39 - the weight from the height in other
46:42 - words the weight is going to depend
46:43 - on height which means that
46:46 - weight should be y and height should be
46:49 - x which is the way that we already
46:51 - set it up so it's good that we did so
46:53 - and the regression line that we got from
46:54 - the calculator earlier
46:56 - is the one that we'll write here so if
46:59 - we go back to the calculator we can see
47:00 - what we had there
47:02 - we have a is about 3.01
47:05 - and b is negative 4.43 so now that we
47:09 - have that we can move on to the rest of
47:10 - the problem
47:11 - here we want to graph the data as well
47:13 - as the regression equation
47:15 - and i'll actually use the calculator to
47:16 - do this we'll graph both the data
47:18 - and the equation on the same window we
47:21 - already have the data entered
47:23 - so if we turn on the stat plot under
47:25 - second
47:26 - y equals we turn on this first stat plot
47:29 - and leave it as a scatter plot the way
47:30 - it is
47:31 - x and y are in the right order now when
47:34 - we graph we'll see those points
47:35 - as long as we're scaled to the right
47:38 - window
47:39 - and then also if we hit y equals we can
47:41 - graph the equation
47:43 - which was 3.01 x
47:48 - minus 4.43
47:54 - now before we graph we should go to the
47:56 - window and make sure that our x values
47:58 - and y values are in the right range
48:00 - the x values the heights range from 70
48:03 - up to 77
48:04 - so we should make sure that x covers at
48:07 - least that range
48:08 - so let's say we make x min 68
48:12 - and x max 79
48:18 - and then the y values the weights range
48:20 - from 200
48:22 - up to 233 so let's say we go from 190
48:28 - to 240.
48:34 - when we graph this we see those heights
48:37 - as well as the line that passes through
48:39 - them now notice what this graph gives us
48:41 - first of all it points out that
48:44 - outlier on the lower right hand side and
48:47 - that's the first entry lamar jackson
48:49 - is on the upper end of the height scale
48:52 - he's at 75 inches he's one of the taller
48:54 - players
48:55 - but he's actually the lightest of all of
48:57 - them at 200 pounds
48:58 - so if we actually removed him from the
49:00 - dataset our r value will be much much
49:02 - higher
49:03 - because he's what's making the data
49:05 - points not
49:06 - follow a straight line since he's kind
49:09 - of out in space by himself
49:10 - that messes up some of the strength of
49:13 - the linear trend that would be there
49:14 - otherwise
49:15 - so sometimes there are outliers like
49:17 - this that will
49:19 - lower the value of r but if you remove
49:21 - them the value of r will be better
49:23 - of course you can't just throw away
49:24 - outliers because your data would look
49:26 - better without them
49:27 - but keep that in mind next we can
49:29 - predict the weight of a quarterback who
49:31 - is 73 inches tall so given a height we
49:33 - can predict
49:34 - their weight using this equation so a
49:38 - is 3.01 and then if we plug in 73 for x
49:44 - we can predict that y hat their weight
49:48 - for a quarterback of this height would
49:50 - be about 215 pounds
49:56 - lastly we get this question does drew
49:58 - brees weigh more or less
49:59 - than the weight predicted by the
50:01 - regression line based on his height
50:03 - so we can do the same thing we did in
50:04 - the last part where we predict
50:07 - what someone who is the height of drew
50:09 - brees 72 inches
50:13 - would tend to weigh according to this
50:15 - equation
50:16 - according to this equation that comes
50:17 - out to 212 pounds approximately
50:21 - so the question asks does he weigh more
50:23 - or less than the predicted value
50:25 - the predicted value is 212 pounds and he
50:28 - only weighs 209 pounds so he weighs
50:31 - less than what's predicted so here are
50:34 - the answers to all the parts of this
50:35 - question
50:36 - we found the correlation coefficient we
50:38 - interpreted it
50:39 - we found the regression line and graphed
50:41 - it alongside the data
50:42 - and then we made a couple of predictions
50:44 - based on that regression line
50:47 - the scores in a college entrance exam
50:49 - are normally distributed with a mean of
50:50 - 52 points
50:51 - and a standard deviation of 11 points
50:54 - and we're asked to find
50:55 - what two scores encompass 95 percent
50:59 - of the test takers since the data is
51:02 - normally distributed
51:03 - the empirical rule tells us that 95 of
51:05 - the data
51:06 - will be within two standard deviations
51:09 - of the mean
51:16 - since one standard deviation is 11
51:17 - points two standard deviations is 22
51:20 - points
51:20 - twice that so 52
51:23 - minus that and 52 plus that will form
51:28 - the boundaries of this range that
51:30 - includes 95 percent of the data
51:32 - therefore we decide that 95 of the
51:35 - values
51:36 - lie between 30 and 74.
51:39 - iq scores are normally distributed with
51:42 - a mean of 100 and a standard deviation
51:44 - of 15.
51:45 - here we're asked to use the empirical
51:47 - rule to find the data that is within
51:49 - one two and three standard deviations of
51:52 - the mean
51:54 - remember the empirical rule states that
51:57 - sixty-eight percent of the data
52:00 - is within one standard deviation of the
52:03 - mean
52:09 - meaning that if we go one standard
52:10 - deviation below the mean and one
52:12 - standard deviation above the mean
52:14 - that range will hold 68 of the data
52:18 - 95 percent of the data falls within two
52:21 - standard deviations
52:23 - and almost all the data or around 99.7
52:26 - percent of the data
52:27 - falls within three standard deviations
52:30 - so if we work from the mean
52:32 - down three standard deviations and up
52:33 - three standard deviations will encompass
52:36 - almost all the data in this case with a
52:38 - mean of 100 and a standard deviation of
52:40 - 15
52:41 - one standard deviation below will be 85
52:44 - and one standard deviation above
52:46 - will be 115 100 minus 1500 plus 15.
52:50 - so 68 of the data in other words
52:53 - 68 of people will have an iq score
52:56 - between
52:57 - 85 and 115. 95
53:00 - of people will have an iq score between
53:03 - 70 and 130
53:05 - and almost everyone will have an iq
53:07 - score of between
53:09 - 55 and 145. here we have a picture that
53:12 - illustrates
53:12 - this situation where each unit on the
53:15 - axis
53:16 - is one standard deviation
53:19 - suppose you know that the prices paid
53:21 - for cars are normally distributed
53:22 - with a mean of seventeen thousand and a
53:25 - standard deviation of five hundred
53:27 - we want to use the 68 95 99.7 rule
53:30 - or the empirical rule to find the
53:33 - percentage of buyers who paid
53:35 - in any given range the first thing to do
53:38 - when working with a problem like this is
53:39 - to draw a picture and here's the picture
53:41 - for this example
53:43 - the center is the mean and then each
53:46 - unit on the axis is the standard
53:49 - deviation
53:50 - so we have it centered at seventeen
53:52 - thousand we go up to seventeen thousand
53:53 - five hundred
53:54 - eighteen thousand eighteen thousand five
53:56 - hundred and we could keep going but
53:57 - that's all we need
53:58 - and then on the lower side we go down to
54:00 - sixteen thousand five hundred
54:02 - sixteen thousand and fifteen thousand
54:04 - five hundred here i've also filled in
54:06 - all the percentages for each range
54:09 - notice that the empirical rule tells us
54:11 - that 68
54:12 - of the data falls within one standard
54:14 - deviation of the mean
54:16 - since everything is symmetric each half
54:18 - of that
54:19 - holds half of that 68 percent and that's
54:21 - where those 34
54:23 - numbers came from then i know that
54:25 - within two standard deviations of the
54:27 - mean
54:28 - i have 95 percent of the data
54:34 - if i have 68 in the middle and by going
54:37 - out another standard deviation
54:39 - i get the 95 percent that means the
54:42 - yellow regions together
54:43 - must make up that 27 percent that gets
54:46 - us from 68
54:47 - to 95 so if the two yellow regions
54:50 - together
54:51 - hold 27 percent each of them holds half
54:54 - of that
54:54 - or 13.5 percent we can repeat this
54:57 - process for the green regions
54:59 - again going out to a third standard
55:01 - deviation
55:02 - we know that that holds 99.7 percent of
55:05 - the data
55:07 - so the green regions together must hold
55:10 - 4.7 percent of the data since everything
55:13 - up through the yellow regions held 95
55:15 - and then just by adding the two green
55:16 - regions we got 99.7
55:19 - the green regions must be that 4.7
55:22 - percent
55:22 - therefore each of them holds half of
55:24 - that or 2.35 percent
55:28 - and then outside the 99.7
55:31 - is 0.3 percent of the data and again
55:33 - because it's symmetric
55:34 - each half of that or each tail contains
55:37 - 0.15 percent again the goal is not to
55:40 - memorize these percentages
55:41 - but just to realize how we got them and
55:44 - be able to rederive them at any point
55:46 - but now that we have them we can use
55:48 - them to solve the problem
55:50 - so part a asks what percentage of buyers
55:54 - paid between sixteen thousand five
55:55 - hundred and seventeen thousand five
55:57 - hundred
55:57 - so we find those two points
56:01 - on our picture and between them we add
56:03 - up the blocks
56:04 - and find that sixty-eight percent of
56:06 - buyers were in that area
56:08 - similarly for part b between seventeen
56:11 - thousand five hundred and eighteen
56:12 - thousand
56:13 - we locate those points and between them
56:15 - there's just one region
56:17 - with thirteen point five percent for
56:19 - part c
56:20 - we look between sixteen thousand and
56:21 - seventeen thousand and there's two
56:23 - blocks there
56:24 - adding them up we get forty seven point
56:27 - five percent
56:28 - for part d we're looking between sixteen
56:30 - thousand five hundred
56:31 - and eighteen thousand and adding up
56:33 - those three blocks we get eighty one
56:35 - point five percent
56:36 - so eighty one point five percent of
56:38 - people paid somewhere between sixteen
56:40 - thousand five hundred
56:41 - and eighteen thousand dollars for their
56:42 - car part e asks what percentage paid
56:45 - below
56:46 - sixteen thousand so that's right here
56:49 - and below sixteen thousand we have
56:51 - two blocks to take care of two point
56:54 - three five percent
56:54 - and point one five percent adding them
56:56 - together we get 2.5 percent
56:59 - so 2.5 percent of buyers paid less than
57:02 - 16 000
57:02 - for their car lastly the final part asks
57:06 - what percentage paid above 18 500 and
57:09 - again there's
57:10 - only one block up there so 0.15 percent
57:13 - of buyers paid above that amount
57:16 - and here we have all the answers
57:18 - summarized
57:19 - here we're told that female adult height
57:21 - in some population
57:23 - is normally distributed with a mean of
57:24 - 65 inches
57:26 - and a standard deviation of 3.5 inches
57:29 - and we're asked to find the z-scores
57:31 - of two heights remember that a z-score
57:36 - is the data value
57:42 - minus the mean
57:45 - divided by the standard deviation
57:48 - and this z score represents how many
57:52 - standard deviations this data point is
57:55 - above or below the mean
57:56 - if the z-score is positive it's above
57:58 - the mean if the z-score is negative
58:00 - it's below the mean for the first
58:02 - example then
58:04 - to calculate z we take 58
58:07 - minus the mean 65 and divide by the
58:11 - standard deviation
58:12 - 3.5 58 minus 65 is negative 7
58:17 - and that divided by 3.5 is negative 2.
58:20 - so what that means is that first data
58:23 - value
58:23 - is two standard deviations below the
58:26 - mean
58:28 - for the second point we do the same
58:30 - thing
58:31 - now taking 71 minus 65 and dividing by
58:34 - 3.5
58:36 - 71 minus 65 is 6 and that divided by 3.5
58:42 - is 1.71 so the second data point
58:45 - is between 1 and 2 standard deviations
58:48 - above the mean
58:50 - scores on an iq test are normally
58:52 - distributed with a mean of 100
58:54 - and a standard deviation of 15. we're
58:57 - asked to find the iq score
58:59 - that corresponds to each of the
59:00 - following z-scores so here we're given
59:03 - z values and we're asked to work
59:05 - backward from them
59:06 - to the data values again remember though
59:09 - that a z-score
59:11 - is simply a given data value
59:15 - minus the mean divided by the standard
59:18 - deviation
59:24 - here we're given the z-score the mean
59:26 - and the standard deviation
59:28 - and asked to solve for the data value
59:30 - for the first one for example
59:32 - we know that z is negative 1.5
59:35 - the data value is the unknown part that
59:37 - we're going to find
59:39 - so the z-score is x minus mean
59:43 - 100 divided by the standard deviation of
59:45 - 15.
59:46 - now to solve for x i'm going to multiply
59:48 - both sides by 15
59:50 - and we find that negative 22.5
59:54 - equals x minus 100
59:58 - then add 100 to both sides to get x
60:01 - equals 77.5
60:04 - so a z-score of negative 1.5 corresponds
60:07 - to an iq score
60:08 - of 77.5 for part b
60:11 - we do the same process the z-score
60:15 - equals the data value minus the mean
60:18 - divided by the standard deviation so we
60:20 - multiply both sides by 15
60:22 - and find that 30.75
60:26 - equals x minus 100 and then adding 100
60:29 - to both sides
60:30 - x equals 130.75 so
60:34 - a z score of 2.05 or
60:37 - just over two standard deviations above
60:39 - the mean
60:40 - corresponds to an iq score of around
60:43 - 130.
60:45 - this question asks what is the margin of
60:48 - error on a poll with a sample size of a
60:50 - thousand people
60:51 - with the simplifying assumptions that
60:52 - we've made the margin of error
60:55 - is one divided by the square root of the
60:57 - sample size
60:59 - as a percentage so this will give us a
61:01 - decimal
61:02 - and then we'll write it as a percentage
61:04 - to do that some people write
61:06 - that this times a hundred percent is the
61:10 - margin of error and that times 100
61:11 - percent just converts
61:13 - that decimal into a percentage
61:16 - again there's some simplifying
61:17 - assumptions that lie behind this but for
61:19 - our purposes this is good enough
61:21 - in this example n is a thousand so the
61:23 - margin of error is one
61:25 - divided by the square root of a thousand
61:28 - times a hundred percent
61:30 - one divided by the square root of a
61:33 - thousand
61:36 - is point zero three one six etcetera
61:39 - and by multiplying by a hundred percent
61:42 - or converting to a percentage
61:43 - gives us three point one six
61:47 - percent approximately it's fairly common
61:50 - for a pole to have a margin of error of
61:52 - around three percent
61:53 - and when you see that you can tell the
61:55 - sample size is around a thousand people
61:58 - in this example we're going to find a
62:00 - sample size that corresponds to a given
62:03 - margin of error
62:04 - so if we want a poll to have a margin of
62:06 - error of 2 percent or less
62:08 - what's the minimum sample size needed to
62:10 - make that happen
62:12 - remember the margin of error looks like
62:14 - 1 over the square root of n
62:16 - as a percentage so we can write times
62:19 - percent just to indicate that we're
62:20 - making it a percentage
62:22 - so if we want this to equal two percent
62:26 - then really what we want is we want one
62:28 - over the square root of n
62:29 - to equal zero point zero two so as a
62:32 - decimal we want one over the square root
62:33 - of n to equal point zero two
62:35 - or as a percentage we want it to equal
62:37 - two percent
62:39 - now we need to solve for n and it takes
62:41 - a little bit of algebra to do so
62:43 - there are a few ways to observe this one
62:45 - if we want to get
62:46 - n by itself we could move it out of the
62:49 - denominator by multiplying it on both
62:51 - sides
62:52 - so if we multiply it over there we get 1
62:55 - equals 0.02 times the square root of n
62:59 - and then again we're trying to get this
63:01 - part by itself
63:02 - so we'll divide both sides by .02
63:08 - which gives us 1 over 0.02 equals the
63:11 - square root of n
63:13 - now there's a shortcut to this at this
63:16 - step
63:16 - you could say we have 1 over the square
63:18 - root of n equals 0.02
63:20 - you can actually flip both sides of that
63:22 - equation upside down
63:24 - and get square root of n over 1 equals 1
63:27 - over .02
63:29 - which gets you to the same place that we
63:30 - already did just by a shortcut
63:32 - don't get too lost in that step if that
63:34 - doesn't make sense
63:36 - but there are shortcuts to some of this
63:37 - algebra once we get to this point though
63:40 - we just have one step left which is that
63:43 - we need to
63:44 - square both sides now before i do that
63:46 - i'm going to simplify
63:47 - 1 divided by 0.02
63:50 - and that simplifies to 50
63:53 - but now to get rid of that square root
63:55 - we just need to square both sides
63:58 - and the square and the square root will
63:59 - cancel each other
64:01 - when we square 50 we get 2500
64:05 - equals n so that's our answer that if we
64:08 - sampled
64:08 - at least 2500 people we would be
64:11 - guaranteed to have a margin of error of
64:12 - 2
64:13 - or less and if we sampled more than 2500
64:16 - our margin of error would be less than 2
64:18 - percent but as long as we
64:20 - sample 2500 will be guaranteed to have a
64:23 - margin of error of 2 percent

Cleaned transcript:

here we'll do a quick example where we identify the sample and the population in a survey question asks a survey asked 2 000 u.s households if they currently own at least one pet the results show that 69 of households do own at least one pet identify the sample and the population in this situation remember with a survey like this the population is the group that we're interested in knowing something about but it's usually not feasible to study the entire population so we gather data from a small subset of that group so in this case we're interested in knowing about all us households so that's the population notice that that wasn't explicitly stated but it's clear from the problem statement that that's what we're interested in knowing about because it's infeasible to study all households in the u.s we take this sample of 2000 households we gather data from them and we use that to draw inferences about the entire population here we're looking at the idea of representative samples so if we're looking to measure something about a population we want to gather a sample to measure and we want to make sure that when we do that the sample represents the population that it looks similar to the population as a whole so we'll look at a couple examples here first of all to find the average annual income of all adults in the united states suppose we sampled representatives in the congress of the united states it turns out this is not a very representative sample first of all the salary for representatives in congress is set at a fixed number and that number is relatively high compared to the average income for all adults in the united states so it's not representative because if you look at the whole population there are some people who make very little and some people who make a lot and in the congress there's a fixed value that's unlikely to be similar to the average value of all adults in the us so it's not a very good representative sample we would say no this is not representative the second example says to find out the most popular cereal among children under the age of 10 you could stand outside a large supermarket one day and pull every 20th child under the age of 10 who enters the supermarket it's not clear that there's any bias in this one this seems like a pretty good way to find an answer to this question if you pull children coming into a supermarket of the right age group you're likely to get a pretty representative sample for all children to do this now you may want to pick different areas of the country for instance there could be differences depending on where you look but without going any deeper it doesn't look like there are any obvious red flags that this would not be representative for all the answers you're looking for so this one looks fairly good and the lesson from these is just that when you're gathering a sample it's important to look for a representative one one that's likely to look similar to your population you don't want a sample that's chosen too narrowly or that's chosen with some sort of obvious bias this is a simple example that illustrates a way that a sample can be biased here a coach is interested in how many cartwheels the average college freshman can do at his university eight volunteers from the freshman class step forward after observing their performance the coach concludes the college freshman can do an average of 16 cartwheels in a row without stopping is this sample random and representative in general a good sample is random and representative a simple rule of thumb for deciding whether a sample is random or not is just to think about whether or not every member of the population is equally likely to be selected if so there's randomness involved to decide whether or not the sample is representative think about whether the sample looks similar to the population here the biggest source of bias that we observe and bias means that the results will be skewed is this voluntary response bias voluntary response bias means that rather than picking people to ask the coach asks for volunteers in this case people that are able to do more cartwheels are more likely to step forward and volunteer for the study because of that we conclude that this probably isn't a very good sample to do this study a voluntary response bias also comes into play in surveys that have questions where certain responses are more favorable than others in this example we're going to decide which type of sampling is being used in each description the first situation we have a soccer coach who selects six players from a group of boys aged eight to ten then seven players from the group of boys aged 11 to 12 and finally three players from a group of boys aged 13 to 14 to form a rec team notice the key here which is that the coach has divided the group into segments based on their ages so there's a segment from eight to ten a segment from 11 to 12 and then a segment from 13 to 14. and from each segment the coach has selected several players so after dividing into segments the sampling could be either stratified or clustered depending on what happens next but the fact that we're choosing a couple from each group makes it stratified if we chose a couple of groups all together that would look more like cluster sampling so this first one is stratified sampling now generally with stratified sampling we select the same number from each group in this case the coach didn't do that he selected six from one group seven from another and three from another but generally speaking with stratified sampling we select the same number from each group in the second part there's a pollster who interviews all human resource personnel in five different high tech companies this may be hard to see at first but the fact that this pollster selected five companies leads you to think that they looked at all the companies that were out there and thought of each company as a group and they selected all human resource personnel from a few of these groups so by dividing them into groups again we can think about either stratified or cluster sampling starting that way and then because the pollster selected everyone from a couple of groups that's cluster sampling if they had selected a few from all the groups that would be stratified sampling but the fact that they selected a few full groups makes it cluster sampling the third one a high school educational counselor interviews 50 female teachers and 50 male teachers notice again that there's a separation of categories and so the teachers have been separated into male teachers and female teachers they've been divided into groups and then from those groups some have been selected an equal number from each which again looks like stratified sampling next a medical researcher interviews every third cancer patient from a list of cancer patients at a local hospital and the key here is that term every third which is what identifies this as systematic sampling systematic sampling is where we have a list like this and we pick some step like this like three and we check every third or we could pick every fifth or every tenth whatever it is that systematic moving through the list is what makes this systematic sampling the next one a high school counselor uses a computer to generate 50 random numbers and then pick students whose names correspond to the numbers notice how there's no division into groups there's no systematic process this is the full population of students and we just select random numbers from that full group and that's what makes this simple random sampling that's kind of the simplest version where we're looking at the full population and using a random number generator to just select without any division into groups or anything else lastly a student interviews classmates in his algebra class to determine how many pairs of jeans a student at a school owns on the average notice here that this student is looking for information about the whole school but rather than looking at a full student list and selecting randomly from them or selecting every third student or even dividing them into groups the student just asks the students nearby the students that are in this class next to him which makes this a convenient sample in this example we'll see how to select a simple random sample from a population the population we're given is a set of six quiz scores and there are 10 students in this example we'll draw a dot plot and we're given data that represents the ages of 30 randomly chosen mba players the first thing we need to do is draw an axis that will cover the full range of the data so just kind of scanning through it looks like the lowest value here is around 20 and the highest value is about 36. so let's make sure our range will at least cover those values let's draw an axis here with values from 20 up to 36. now that we have our axis all we need to do is read through each of these data points and put a dot for each one so for the first value at 22 we just put a dot above the 22 then we have 28 so we'll put a dot at the 28. notice it hovers a little bit above the marker but it doesn't really matter how high we put them as long as we place them at consistent heights just so we can visualize the final result that'll make more sense as we draw more of these the next value is at 20 then at 24 then at 26 then 21 27 and then we get another 28 so the second time we've seen 28 so we won't draw the second dot at the same place as the first one but we'll put it right above it so now there's a second dot 28 then we go to 31 and 29 and just continue on entering these we hit another 24 here and again we just put this one above the first one at that location then we have another 22 another 21 then a 25 another 22 another 25. 30 another 29 another 20 and so on there's our 36 another 24 the first 23 another 36 another 24 another 29 and we'll just finish this out so there we see our dot plot where each time we run into a value we've already drawn a dot for we just draw a one a little bit higher so notice that the height of these stacks tells us how frequent an age is so the more frequent ones are over here and then the 36s there's kind of what we would call an outlier they're far out from the other data points and for instance 30 is relatively rare there's only one of those and so on so there's a lot we can tell from this plot and we'll draw other types of plots later on that fit this same kind of pattern of looking for where the data is clustered and where it's spread out but a dot plot is a very simple way to observe that at first here we're going to build a simple frequency table the problem states that 19 people were asked how many miles to the nearest mile they commute to work each day and their responses were recorded in this data set the frequency table lists each possible data value and the number of times that data value occurs so we put two columns like this one for the data values and one for their frequencies now we'll go through the data set and for each unique data value that we see we'll list that in the left hand column once we've filled in all these data values now we just need to count how many times they occur so for instance i notice that 2 appears twice in the data set so it has a frequency of 2. 3 appears only once so it has a frequency of 1 and so on and i fill in the rest of the table and it's really as simple as that once we've filled in the frequency table a quick check that we can do is add up all the frequencies and they should add up to how many data points we have and here if we add up these frequencies we do find a total of 19. in this example we'll build a frequency table for categorical data we're given a sample of mba players with their position which is a categorical variable it divides them into categories point guards shooting guards small forwards power forwards and centers and we're going to build a frequency table to go along with this so the categories will be the divisions and then we'll count the frequency in each category so our frequency table will start with two columns one for the position and one for the frequency and then we'll add a third column for relative frequency it's a good thing to include when you draw a frequency table it's not entirely necessary every time but it's a good idea to include it when you can so the positions we have point guard shooting guard small forward power forward and center as our five categories and then we just go through and count how many there are of each one so for point guards for instance we have one two three four of them so the frequency is four for shooting guards you can count one two three four five six seven eight nine for small fours we have one two three four five six seven for power forwards we have one two three four and five and then for centers we have one two three four and five and notice if we add up those frequencies we should get the total number that we have which is 30. so we should get 30 if we add those up and you can check that by adding those frequencies for the relative frequency we need to divide each frequency by the total number that we have in our sample which is again 30. so for the relative frequency of point guards we would divide 4 by 30 which we could write as a fraction or if we wanted to we could write that as a decimal which comes out to about 0.133 or 13.3 percent then for the next one we would divide nine by thirty which works out to thirty percent seven out of thirty is twenty three point three percent and then five out of thirty it's about 16.7 and that's for both the last two categories so again a frequency table is pretty easy to construct all you have to do is count how many fall into each category in this case the categories were the positions of the players here we're going to build a grouped frequency table using the data set of nba players with their points per game shown below so we have 30 values ranging from 1.4 up to about 24 and we're told to use a class width of 5 for our group frequency table so we want to start low enough that we can cover all of them and rather than starting at one let's start at zero just to get a nice round number and make it easy for ourselves so in our frequency table the first column will be points per game and our first category the first class will start at zero and we'll go up to five but remember we won't go all the way to five because we don't want to overlap our classes so the first one's gonna go up to just less than five let's use 4.9 to represent just less than five and the next one will start at 5.0 and go up to 10 but just below 10 so we'll stop at 9.9 and then we'll go from 10 to 14.9 15 to 19.9 and 20 to 24.9 we don't need any more classes because that's as high as we need to go to cover everyone in this data set so the next column will be the frequency and then lastly we'll have the relative frequency since there are 30 values here in our data set once we have the frequencies we'll just divide each one by 30 to get the relative frequency we won't show this in detail for all these classes but just for the first class from 0 to 4.9 we'll go through and select all the ones that fit into that category so between 0.0 and 4.9 in the first row we find 4.9 in the second row we find 1.4 and 2.0 and in the third row we find 3.0 4.3 and that looks like all of them so there are a total of five that fall into that range and then we can continue this on for all the others but i won't show the counting you can go through and do that yourself and i'll show you the results here so once you count all the others you should get 11 for the second class 5 for the third class 4 and then 5 for the final class then if we divide each of these by 30 to get the relative frequency 5 divided by 30 is 0.16 repeating so you can round that to 0.167 or 16.7 percent and then 11 out of 30. we could round to 36.7 percent 5 out of 30 again is 16.7 percent and then 4 out of 30. we could round to 13.3 to get the relative frequency for each so it's really just counting the only thing to keep track of for grouped frequency tables is to make sure your classes are all evenly wide and that none of them overlap that's the important piece here we'll do the histogram using the data set for the players in the mba with their points per game listed here in the last example we built a grouped frequency table and now we're going to build a histogram that matches it so we're going to take the frequency table we built earlier and just draw this histogram to represent the same picture so due to that we'll start with a grid and the x values will range from 0 up to 24.9 or up to 25 so we'll have 0 here 5 10 15 20 and 25 and each of these classes will fit between two of those values so the first class goes between 0 and 5 the next one goes between 5 and 10 and so on and of course we know that it goes just up to 5 but not including 5 and so on but we'll draw it as if it goes all the way to 5 just to make the picture as simple as possible then on the vertical axis we have the frequency so here we have the points here we have the frequency and the highest frequency we see is 11 so we need to at least go up to 11. so let's have 10 here and five and then we just draw a bar to the right height for each category in the first class the frequency is five so we'll draw a bar up to five ranging from zero to five then from five to ten the frequency was eleven so that one goes all the way up to the top of the graph then from 10 to 15 that frequency was again five from 15 to 20 it goes down to four so we'll draw it a little shorter down at the tick mark for four and the last one goes back up to five so each class each category gets a bar with the height representing the frequency it's relatively simple once you've drawn the frequency table which just consists of counting the ones that fit into each class here we'll build a bar chart for this data set which is the positions for the players in the mba sample again we have 30 observations falling into one of five categories point guard shooting guards small forwards power forwards and centers first we need to find the frequency of each category which means building the frequency table but since we've already done that in a previous example i won't go through in detail and count those rather we'll just put the results here in this frequency table we have the five positions and the frequencies there are four point guards nine shooting guards seven small forwards and five power forwards and five centers now just like with the histogram we start with our grid and this time the horizontal axis will represent again our categories so we'll have five spots for our bars to go and then the vertical axis will again represent frequency so the horizontal axis is position the vertical axis is frequency the highest frequency we see is nine so we need to make sure we go up to at least nine once again go to ten and now we're ready to draw a bar for each position so rather than having the bars connect like they would with a histogram with a bar chart since we're thinking of these as separate categories they're not ones that flow into one another we'll draw the bars with some separation so at the point guard position we'll draw a bar that goes up to four since there are four of those and then at the shooting card position we'll draw one that goes up to nine small forwards have a frequency of seven and power forwards and centers each have a frequency of five so again we're drawing a bar where the height represents the frequency of that category but unlike with a histogram these bars are separated because we're indicating that these are separate categories and there's not a flow from one to the next let's build a stemandleaf plot the question says suppose you gathered data on how long it took you to get ready in the morning for 40 days you measured the amount of time between when your alarm went off and when you left the house the results are below rounded to the nearest minute and we want to build a stemandleaf plot for this for a stemandleaf plot we divide each value into its stem and its leaf and the stems are generally the tens place although you can tweak this and make it for instance the ones place but in this case since our values are two digit values the first digit will represent the stem and the second digit will represent the leaf so if you look through you'll notice that all these numbers start with either one two or three that's the the first digit so our stems could be one two or three and now we'll go through and for each value we'll place that leaf in the correct category so the first value is 35 so we'll put a 5 under the leaves category in the third row then for 28 we'll put an 8 here then 25 we'll put a 5 here then 23 and so on we'll put another 3 then 32 means we'll put a 2 in the third column then 29 19 21 13 and so on and i'll go through and fill all these out in detail i'll go through and show the final result in just a moment now what you often find with the stem and leaf plot is that we write the leaves in the order of their value rather the order in which they appear in the data set so instead of writing 8 five three three nine one we would write the one first and then the threes and the fives and so on now let's take some extra work because we have to do some sorting but when i show the results in a second it'll be shown in that way with them sorted by value now that we have all the stems and leaves written notice how it looks it looks almost like a histogram or a bar chart turned sideways where the length of each string of leaves represents the frequency of that category so it divides it into categories by tens and we can see that the most frequent category is the range from 20 minutes to 29 minutes and yet unlike a normal grouped frequency by grouping them we haven't lost any information we could reconstruct the entire data set from this stemandleaf plot if we had to in this example we'll construct a scatter plot for some data that we're given we're given the sizes of several tvs and the price that goes along with each one now our data is separated into three categories just so the table could fit easily here but side by side you'll see a size and a price where the size is given in inches and the price is given in dollars and we want to construct a scatter plot where we compare these two variables now when we construct a scatter plot we have to pick which variable will be x and which one will be y it's not crucial that we pick it in the right order because it turns out if they get switched much of the analysis we can do is still the same but if we can it would be nice to pick x and y in a reasonable way and usually we want to think about how x determines why in other words is there one of the variables that seems to control the other one would we say that the size determines the price of the tv or would we say that the price determines the size it's probably more likely that we would say that the price depends on the size or the size determines the price so we could call x the size and y the price so let's say our xaxis represents size and our yaxis represents price again we'll try to be consistent with these where the first column is x the second column is y but it turns out that if you switch the order it doesn't change too much at least at this point so now we need to put a scale on each axis so for the sizes the sizes go up to about 60 inches so let's make sure we include at least up to 60. let's say we go up to 70 here and if we start down at zero that means we need to divide this evenly so that we get to 70. so halfway there would be 35 and then if we divide these by fives every 5 10 and so on and the prices range from 200 to about 2 800. so let's say we include all the way up to 3 000. and let's mark it in increments of 500. now for each value in our list let's take the first one for instance the size is 43 inches the price is 500 so on the horizontal axis the size axis will go up to 43 which is around here between 40 and 45 and then on the price side we'll be right at 500 so we want to find out where those two cross which is right around here so our first point will be right there at 43 inches and 500 for the next one we'll go to 55 and 900 so 55 is right here between 50 and 60. 900 is right here right under a thousand so those two look like they cross right around here which gives us our second point and then we'll continue for the rest of them without showing each one in detail i'll just show the final picture here in a moment after we've drawn all these points so there's the final result and even though it's handdrawn and imperfect we can see the general trend which is that larger tvs tend to cost more so there's this general upward trend as you scan through this picture that's really the value of a scatter plot is to look for an association or a connection between two variables to see if there's a relationship and this one looks like there's this upward trend later on we'll talk about how to draw a line or some other curve that represents this shape but for now just notice that there's that relationship between the two find the median of the salaries listed below so these are the salaries from the mba data set we have 30 values and we want to find the median remember that the median is the middle data point now helpfully these salaries have been listed for us in order from smallest to largest so notice we have the smallest value here and then they increase along this row and then down to the next row until we reach the largest value at the bottom right and that's important when you're finding the median to order them from least to greatest or from greatest to least so that when we look in the middle of that list the middle value is truly in the middle between the highest and lowest so that half of the data points fall below it and half of them fall above it so since it's listed like this for us we can look halfway through since there are six rows at the end of the third row and the beginning of the fourth row we have the middle so the middle is right between this value and this value since there are two values in the middle which will happen every time we have an even number of data points we need to find the number halfway in between those two which we do by averaging them together so find the average of those two numbers in the middle that will be the median if we had an odd number of data points like say we had 29 players we would find the middle value and that would be our median we wouldn't have to find any average but anytime there's an even number there will be two in the middle so we need to find whatever's halfway between those two so the median will be halfway between four million four hundred and sixty nine thousand one hundred sixty and 4 million 767 000. so if we add those together and divide by two what we get is four million six hundred and eighteen thousand eighty and that's the median half of the players make less than four million six hundred eighteen thousand half of the players make more so that's the middle data point and it's a good measure of the center here we'll find a weighted average using a student's score in a class so we have several assignments three tests homework project and final exam and we're given the student's score on each of these assignments as well as a weight for each assignment or a number of points for them now notice that usually we'd be given either the weight or the points in this case we're given both and if you look closely there are a total of a thousand points that they could earn if you add up all those points you should get a thousand and if you divide each of those point values by a thousand you'll get the percentages that are listed here so really the same information is given in the weight column or in the points column and you may see for some of your classes the scores are given with weights sometimes they are given with points it's really the same thing it's just written in a different way so we're going to show the calculation with both just to compare but we'll get the same answer either way for part a then let's use the weights so for the weights we can multiply each score the student got by the weight that's associated with it and once we multiply those and add them all together the answer we'll get at the end is the average score so the weights are nice because they're already scaled for us so that just by multiplying the score times the weight and adding those together we'll get the final weighted average so i have to do is take 85 percent times 20 plus 92 percent times twenty percent plus eightyseven percent and so on so if we multiply all those and add them all together the answer we get at the end is about point eight nine nine which works out to 89.9 so on a standard 10point scale this student will be very close to an a just under 90 now let's do the same thing with a point system and we're going to get the same answer of course but this time we have to do two steps where first we multiply their score so for the first category they got 85 percent times the number of points they got and then do that for all of them and once we multiply and add all those up we get 899 points so they earned a total of 899 points and then when you divide that out of the total of number of points they could have earned which was a thousand they get .899 which again works out to 89.9 so notice you're doing the same work both ways it's just that when you use the weights you've already divided by a thousand before you start when you use the points you have to divide by a thousand at the end to get the final score the same way you did with the weights so either way you do it you're using the same values and this is how you do a weighted average whether you're given the weights as percentages or as points out of a total that you could earn here we'll find the mode of a data set that's summarized for us we're already given the frequency table rather than just the raw data so most of the work is done for us the mode is the most frequent data value and the nice thing about having the data given to us as a frequency table is all we have to do is look through the frequencies and find the highest frequency notice the highest frequency is four and occurs three times there are three values which occur four times and are tied for most frequent and those are 21 22 and 24 so there are actually three modes and this can happen a lot in our case the modes are 21 22 and 24. here we'll find the range of a data set that we're given and the data set is the heights of the players given in the mba data set the range is really simple it's just the difference between the smallest and the largest so all we have to do is look through this list and find the smallest number and the largest number that occur if you scan through you should be able to pick out that the lowest number is 1.78 1.78 meters is the shortest player in the sample that we listed and the tallest player is 2.13 meters so the range is just the difference between those two if we take 2.13 minus 1.78 the range is the difference or 0.35 meters that's a really simple way of measuring how spread out the data is if the range is larger it's more spread out if the range is smaller it's more tightly clustered together so 0.35 when compared to the values in this data set is a fairly small range so they're fairly tightly clustered together all of these players are relatively tall here we'll calculate the standard deviation by hand using the formula instead of using the builtin function in the calculator just to illustrate how the formula works we have five data points and we want to calculate the standard deviation the first thing we need to do is to calculate the mean so the mean remember is the sum of the data points divided by the number of data points so we add them up and divide by 5 and we find that the mean is 10.2 now the standard deviation of this data set is the square root of the sum of the squared deviations divided by n minus 1 and all that's inside the square root so we'll need to calculate these things individually we need to find each deviation the difference between each data point and the mean square those add them up divide that answer by n minus 1 and then take the square root of that answer so this can get kind of tedious which is why we only do this with small data sets and from this point on we'll use the builtin function in the calculator but here we'll just illustrate the formula once we'll use this table to organize everything we have the data values listed and we'll calculate the deviation for each one by simply subtracting this number 20 for instance minus 10.2 and then 4 minus 10.2 and 15 and so on and we'll check the deviation for each data value once we've done that we square them all and remember the reason that we square them is because if we tried to average these deviations we would get zero because the positive ones and the negative ones would cancel each other out but by squaring them we end up with all positive numbers so when we average those they don't cancel each other then here all the squared deviations are filled in now we need to add those squared deviations divide by n minus 1 or 4 in this case and then take the square root of the answer adding these all up we get 210.8 and then if we divide that by 4 we get 52.7 but we're still not done the last step is to take the square root of that the square root of 52.7 is 7.26 so the standard deviation of this data set is 7.26 or the distance that a typical data point is from the center again this example mostly illustrates why we don't calculate the standard deviation by hand usually even for a small data set it gets pretty tedious a random sample of 11 statistics students produce the following data where x is the third exam score out of 80 and y is the final exam score out of 200 here we'll use a graphing calculator to find the equation of the least squares regression line so that we can predict the final exam performance which is why based on the third test score which is x so to use the calculator we first need to enter the data if we enter the stat menu and edit we can enter the data here which i've already done the first list we've entered the x's and in the second list we've entered the y's now if we go back under the stat menu we want to calculate so we'll scroll over to the calc menu and we want linear regression the form we've been using is this number 4 reg ax plus b so a is the slope and b is the yintercept if we select that we don't need to change anything here because we entered the x's in list 1 and the y's in list 2. so nothing needs to be changed we can just scroll down to calculate and it gives us the equation the form of it is y equals ax plus b and we get the values for a and b as well as values for r squared and r and now we have a better sense of what this r represents that's the correlation coefficient so the correlation coefficient is about 0.66 which means there's a moderate linear relationship and it's positive so there we go there's our regression line and there's the r value that goes with it here we're given a regression equation which we developed in a previous example to predict the price of a house based on its square footage so if x represents the square footage we can predict the price in thousands of dollars so if we entered a thousand for the square footage for x we could calculate a value for y that would be the predicted price for houses with that square footage and we're going to use this equation to predict the price of homes with two different square footage values so all we have to do in each case is replace x with the given value for square footage in the first case we have y hat equals 0.099 times 2700 plus 160.8 which works out to 428.1 so that corresponds to 428 thousand one hundred dollars and that technically means that the average house price we would expect for all the houses that are twenty seven hundred square feet would be four hundred and twenty eight thousand dollars approximately and then in the second case we can make a similar prediction for houses with 4 500 square feet and that works out to just over six hundred thousand then the question asks which prediction do you expect to be more reliable now for this we'd really have to go back and look at the data which we don't have in front of us but if you go back in the textbook and look at the data you'll notice that in the range of houses that we have data for 2700 square feet falls within that range and 4 500 square feet does not none of the houses we have in our data set are nearly as big as 4 500 square feet so it turns out that the first one is more likely to be reliable without external information for all we know that now it seems more likely that the first prediction would be more reliable because we've seen houses that are similar to it in our data set the first example we call interpolation where we are predicting within our data range the second one we call extrapolation where we're predicting beyond our data range and in general extrapolation is dangerous because we don't know quite what could be happening beyond the range of data we've actually looked at so in general interpolation is more reliable than extrapolation so the first prediction is more likely to be reliable even though we don't have a lot of information about it at the moment here we'll do a linear regression problem with several steps first we're given some data which is a set of quarterbacks in the nfl during the 2019 season and for each quarterback we're given their height and their weight we're going to compare these two now without any other information at the moment we're going to assume that height is going to represent x and weight will represent y just because they're ordered that way later on in the problem we might have more information that'll tell us which one should be x and which one should be y but for now we'll just assume that and change if necessary so first we want to calculate the correlation coefficient and to do this we'll use the calculator so first we need to go to the calculator and enter this data to enter the data we'll hit the stat button and hit enter to get into the edit menu and under list one we'll enter the x values so the heights that we saw the first quarterback had a height of 75 inches then 74 74 70 71 74 76 77 72 and 74. and then we can scroll over to the second list for their weight and we'll enter these as well once we have all the data entered we can go back into the stat menu and scroll over to the calc down to the linear regression option again we've entered x's in list one and y's and list two so we don't need to change anything if those were switched it wouldn't change the correlation coefficient anyway so for this first part it didn't really matter which way we entered them but later on it will be significant so we'll just hit calculate and it goes ahead and gives us the equation for the line which we don't need just yet all we're looking for at the moment is the value of r which is about 0.6 the first part of the question just asked for the value of r so now that we have that we can go back to the notes and enter that now for the second part of the question we want to know is there a strong linear relationship so based on the value of r we know first of all that there is a positive relationship since r is positive and we wouldn't necessarily say it's a strong relationship because it's less than 0.8 which again is not a magical number but it's sort of an agreed upon level for a strong relationship but 0.6 we might say there is a moderate relationship and often a moderate relationship is good enough to continue on and we'll continue to do the rest of the problem so we wouldn't necessarily say it's a strong relationship but it is a moderate linear relationship the next part of the question asks for the regression line which we've already calculated now notice the direction here the regression line is going to predict the weight from the height in other words the weight is going to depend on height which means that weight should be y and height should be x which is the way that we already set it up so it's good that we did so and the regression line that we got from the calculator earlier is the one that we'll write here so if we go back to the calculator we can see what we had there we have a is about 3.01 and b is negative 4.43 so now that we have that we can move on to the rest of the problem here we want to graph the data as well as the regression equation and i'll actually use the calculator to do this we'll graph both the data and the equation on the same window we already have the data entered so if we turn on the stat plot under second y equals we turn on this first stat plot and leave it as a scatter plot the way it is x and y are in the right order now when we graph we'll see those points as long as we're scaled to the right window and then also if we hit y equals we can graph the equation which was 3.01 x minus 4.43 now before we graph we should go to the window and make sure that our x values and y values are in the right range the x values the heights range from 70 up to 77 so we should make sure that x covers at least that range so let's say we make x min 68 and x max 79 and then the y values the weights range from 200 up to 233 so let's say we go from 190 to 240. when we graph this we see those heights as well as the line that passes through them now notice what this graph gives us first of all it points out that outlier on the lower right hand side and that's the first entry lamar jackson is on the upper end of the height scale he's at 75 inches he's one of the taller players but he's actually the lightest of all of them at 200 pounds so if we actually removed him from the dataset our r value will be much much higher because he's what's making the data points not follow a straight line since he's kind of out in space by himself that messes up some of the strength of the linear trend that would be there otherwise so sometimes there are outliers like this that will lower the value of r but if you remove them the value of r will be better of course you can't just throw away outliers because your data would look better without them but keep that in mind next we can predict the weight of a quarterback who is 73 inches tall so given a height we can predict their weight using this equation so a is 3.01 and then if we plug in 73 for x we can predict that y hat their weight for a quarterback of this height would be about 215 pounds lastly we get this question does drew brees weigh more or less than the weight predicted by the regression line based on his height so we can do the same thing we did in the last part where we predict what someone who is the height of drew brees 72 inches would tend to weigh according to this equation according to this equation that comes out to 212 pounds approximately so the question asks does he weigh more or less than the predicted value the predicted value is 212 pounds and he only weighs 209 pounds so he weighs less than what's predicted so here are the answers to all the parts of this question we found the correlation coefficient we interpreted it we found the regression line and graphed it alongside the data and then we made a couple of predictions based on that regression line the scores in a college entrance exam are normally distributed with a mean of 52 points and a standard deviation of 11 points and we're asked to find what two scores encompass 95 percent of the test takers since the data is normally distributed the empirical rule tells us that 95 of the data will be within two standard deviations of the mean since one standard deviation is 11 points two standard deviations is 22 points twice that so 52 minus that and 52 plus that will form the boundaries of this range that includes 95 percent of the data therefore we decide that 95 of the values lie between 30 and 74. iq scores are normally distributed with a mean of 100 and a standard deviation of 15. here we're asked to use the empirical rule to find the data that is within one two and three standard deviations of the mean remember the empirical rule states that sixtyeight percent of the data is within one standard deviation of the mean meaning that if we go one standard deviation below the mean and one standard deviation above the mean that range will hold 68 of the data 95 percent of the data falls within two standard deviations and almost all the data or around 99.7 percent of the data falls within three standard deviations so if we work from the mean down three standard deviations and up three standard deviations will encompass almost all the data in this case with a mean of 100 and a standard deviation of 15 one standard deviation below will be 85 and one standard deviation above will be 115 100 minus 1500 plus 15. so 68 of the data in other words 68 of people will have an iq score between 85 and 115. 95 of people will have an iq score between 70 and 130 and almost everyone will have an iq score of between 55 and 145. here we have a picture that illustrates this situation where each unit on the axis is one standard deviation suppose you know that the prices paid for cars are normally distributed with a mean of seventeen thousand and a standard deviation of five hundred we want to use the 68 95 99.7 rule or the empirical rule to find the percentage of buyers who paid in any given range the first thing to do when working with a problem like this is to draw a picture and here's the picture for this example the center is the mean and then each unit on the axis is the standard deviation so we have it centered at seventeen thousand we go up to seventeen thousand five hundred eighteen thousand eighteen thousand five hundred and we could keep going but that's all we need and then on the lower side we go down to sixteen thousand five hundred sixteen thousand and fifteen thousand five hundred here i've also filled in all the percentages for each range notice that the empirical rule tells us that 68 of the data falls within one standard deviation of the mean since everything is symmetric each half of that holds half of that 68 percent and that's where those 34 numbers came from then i know that within two standard deviations of the mean i have 95 percent of the data if i have 68 in the middle and by going out another standard deviation i get the 95 percent that means the yellow regions together must make up that 27 percent that gets us from 68 to 95 so if the two yellow regions together hold 27 percent each of them holds half of that or 13.5 percent we can repeat this process for the green regions again going out to a third standard deviation we know that that holds 99.7 percent of the data so the green regions together must hold 4.7 percent of the data since everything up through the yellow regions held 95 and then just by adding the two green regions we got 99.7 the green regions must be that 4.7 percent therefore each of them holds half of that or 2.35 percent and then outside the 99.7 is 0.3 percent of the data and again because it's symmetric each half of that or each tail contains 0.15 percent again the goal is not to memorize these percentages but just to realize how we got them and be able to rederive them at any point but now that we have them we can use them to solve the problem so part a asks what percentage of buyers paid between sixteen thousand five hundred and seventeen thousand five hundred so we find those two points on our picture and between them we add up the blocks and find that sixtyeight percent of buyers were in that area similarly for part b between seventeen thousand five hundred and eighteen thousand we locate those points and between them there's just one region with thirteen point five percent for part c we look between sixteen thousand and seventeen thousand and there's two blocks there adding them up we get forty seven point five percent for part d we're looking between sixteen thousand five hundred and eighteen thousand and adding up those three blocks we get eighty one point five percent so eighty one point five percent of people paid somewhere between sixteen thousand five hundred and eighteen thousand dollars for their car part e asks what percentage paid below sixteen thousand so that's right here and below sixteen thousand we have two blocks to take care of two point three five percent and point one five percent adding them together we get 2.5 percent so 2.5 percent of buyers paid less than 16 000 for their car lastly the final part asks what percentage paid above 18 500 and again there's only one block up there so 0.15 percent of buyers paid above that amount and here we have all the answers summarized here we're told that female adult height in some population is normally distributed with a mean of 65 inches and a standard deviation of 3.5 inches and we're asked to find the zscores of two heights remember that a zscore is the data value minus the mean divided by the standard deviation and this z score represents how many standard deviations this data point is above or below the mean if the zscore is positive it's above the mean if the zscore is negative it's below the mean for the first example then to calculate z we take 58 minus the mean 65 and divide by the standard deviation 3.5 58 minus 65 is negative 7 and that divided by 3.5 is negative 2. so what that means is that first data value is two standard deviations below the mean for the second point we do the same thing now taking 71 minus 65 and dividing by 3.5 71 minus 65 is 6 and that divided by 3.5 is 1.71 so the second data point is between 1 and 2 standard deviations above the mean scores on an iq test are normally distributed with a mean of 100 and a standard deviation of 15. we're asked to find the iq score that corresponds to each of the following zscores so here we're given z values and we're asked to work backward from them to the data values again remember though that a zscore is simply a given data value minus the mean divided by the standard deviation here we're given the zscore the mean and the standard deviation and asked to solve for the data value for the first one for example we know that z is negative 1.5 the data value is the unknown part that we're going to find so the zscore is x minus mean 100 divided by the standard deviation of 15. now to solve for x i'm going to multiply both sides by 15 and we find that negative 22.5 equals x minus 100 then add 100 to both sides to get x equals 77.5 so a zscore of negative 1.5 corresponds to an iq score of 77.5 for part b we do the same process the zscore equals the data value minus the mean divided by the standard deviation so we multiply both sides by 15 and find that 30.75 equals x minus 100 and then adding 100 to both sides x equals 130.75 so a z score of 2.05 or just over two standard deviations above the mean corresponds to an iq score of around 130. this question asks what is the margin of error on a poll with a sample size of a thousand people with the simplifying assumptions that we've made the margin of error is one divided by the square root of the sample size as a percentage so this will give us a decimal and then we'll write it as a percentage to do that some people write that this times a hundred percent is the margin of error and that times 100 percent just converts that decimal into a percentage again there's some simplifying assumptions that lie behind this but for our purposes this is good enough in this example n is a thousand so the margin of error is one divided by the square root of a thousand times a hundred percent one divided by the square root of a thousand is point zero three one six etcetera and by multiplying by a hundred percent or converting to a percentage gives us three point one six percent approximately it's fairly common for a pole to have a margin of error of around three percent and when you see that you can tell the sample size is around a thousand people in this example we're going to find a sample size that corresponds to a given margin of error so if we want a poll to have a margin of error of 2 percent or less what's the minimum sample size needed to make that happen remember the margin of error looks like 1 over the square root of n as a percentage so we can write times percent just to indicate that we're making it a percentage so if we want this to equal two percent then really what we want is we want one over the square root of n to equal zero point zero two so as a decimal we want one over the square root of n to equal point zero two or as a percentage we want it to equal two percent now we need to solve for n and it takes a little bit of algebra to do so there are a few ways to observe this one if we want to get n by itself we could move it out of the denominator by multiplying it on both sides so if we multiply it over there we get 1 equals 0.02 times the square root of n and then again we're trying to get this part by itself so we'll divide both sides by .02 which gives us 1 over 0.02 equals the square root of n now there's a shortcut to this at this step you could say we have 1 over the square root of n equals 0.02 you can actually flip both sides of that equation upside down and get square root of n over 1 equals 1 over .02 which gets you to the same place that we already did just by a shortcut don't get too lost in that step if that doesn't make sense but there are shortcuts to some of this algebra once we get to this point though we just have one step left which is that we need to square both sides now before i do that i'm going to simplify 1 divided by 0.02 and that simplifies to 50 but now to get rid of that square root we just need to square both sides and the square and the square root will cancel each other when we square 50 we get 2500 equals n so that's our answer that if we sampled at least 2500 people we would be guaranteed to have a margin of error of 2 or less and if we sampled more than 2500 our margin of error would be less than 2 percent but as long as we sample 2500 will be guaranteed to have a margin of error of 2 percent
