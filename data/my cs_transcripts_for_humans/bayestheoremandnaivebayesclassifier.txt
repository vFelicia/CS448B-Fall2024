With timestamps:

00:00 - conditional and joint probabilities are
00:03 - concepts in probability theory that are
00:06 - that are quite useful when multiple
00:09 - events are occurring a joint probability
00:16 - is quite easy to describe it's the
00:19 - probability that an event a and an event
00:23 - B occur at the same time
00:27 - so let's take for example that event a
00:31 - is the probability of landing let's say
00:38 - that a is defined as we have to define
00:41 - the event first a is the that a coin
00:44 - flip lands as heads and let's say that B
00:54 - is the event that a die rolls as a 2 we
01:12 - can easily calculate the probability of
01:14 - a this is simply equal to 1/2 we can
01:20 - also equally calculate the probability
01:22 - of B this is equal to 1 and 6 right fair
01:29 - die now let's ask the question what's
01:31 - the probability of a and B occurring
01:33 - together this is the joint probability
01:36 - well the probability that you both land
01:41 - as ahead and roll it to 1 these are
01:46 - independent events one doesn't influence
01:48 - the other so you have to have both coin
01:51 - and landing as ahead an or a rolling a
01:53 - die these are just these two multiplied
01:55 - together right it's 1/2 times 1/6 which
01:58 - is equal to 1/12
02:08 - that doesn't seem particularly
02:10 - insightful but things get a little more
02:14 - interesting
02:15 - when we talk about events that are not
02:22 - independent let's define another event
02:27 - event C as the die roll being even so
02:41 - now what's the probability of B and C
02:47 - occurring at the same time well if it
02:52 - turns out that the Dyer comes out as a 2
02:54 - then we know that the roll is also even
02:59 - so this is just 1 over 6 because it
03:05 - doesn't constrain it anymore similarly
03:12 - the probability that probability of C
03:13 - all by itself right is equal to 1/2 but
03:19 - here we did not multiply 1/6 by 1/2
03:22 - because we know that there's an
03:24 - interaction effect these are not
03:26 - independent events
03:38 - now let's ask a different question let's
03:41 - ask the likelihood let's ask for the
03:44 - probability of two events occurring but
03:48 - knowing that one of the events already
03:51 - occurred so let's ask what the
03:57 - probability of B and C occurring is
04:04 - right this event of rolling a dice a two
04:08 - and the diving even but we know that
04:13 - this actually occurred we know that C
04:16 - occurred let's then divide it by the
04:18 - probability that the die roll was even
04:27 - we know that I'm telling you that this
04:29 - occurred we're asking for the
04:31 - probability that these two events
04:32 - occurred at the same time but we know
04:34 - that I'm just we're just speculating I'm
04:37 - speculating but we're just defining
04:38 - we're saying that C occurred the
04:41 - probability of the die roll B even
04:42 - happened we know that it is even so if
04:46 - we know that it's even what is the
04:49 - probability that it lands as a - well
04:54 - that's this expression is 1 over 3
05:01 - because if we know it's even then it's
05:04 - either two or four or six and it's one
05:08 - of those three so it's 1 and 3 this is
05:10 - the same write as 1 over 6 which was the
05:19 - joint the joint probability divided by
05:22 - probability of C which was 1 over 2
05:25 - which is equal to 2 over 6 which is
05:29 - equal to 1 over 3 right it's the same
05:33 - that's quite cool and we define this
05:36 - expression
05:39 - as the joint as the conditional
05:42 - probability what's the probability that
05:46 - B occurs
05:47 - knowing that C occurs and this is equal
05:50 - to
05:52 - he joins C over P see I'm just rewriting
05:58 - this again so in so we have it in green
06:01 - in fact what I'm gonna do is I'm gonna
06:03 - write it with a and B the probability
06:09 - that some event a occurred knowing that
06:12 - some event B occurred is equal to the
06:15 - joint probability of a and B divided by
06:22 - the probability of B now it gets a
06:33 - little
06:33 - more interesting because this idea of
06:38 - two events occurring together is not by
06:44 - any means selective for a coming in
06:48 - front of B two events occurring
06:50 - simultaneously there's two events
06:52 - occurring simultaneously so there's
06:53 - actually two ways of writing this I mean
06:57 - that this let's look at this if we have
07:00 - the probability of an A intersecting
07:06 - with probability to be that these two
07:07 - occur together in fact what I'm gonna do
07:11 - is I'm going to write this in the middle
07:14 - and show it interpret it in two ways
07:21 - then on one hand write this is equal to
07:26 - we can divide this by by B as we did
07:30 - before and over here show our
07:35 - conditional probability so this is the
07:37 - conditional probability of a conditioned
07:42 - on B and normally I would have this
07:46 - right in the previous in the previous
07:47 - slide I had this divided by B but we'll
07:49 - just multiply that over here times the
07:51 - probability of B same thing I haven't
07:53 - changed the equation but I needed this
07:56 - is the same like the probability of a
07:58 - and B happening together is also the
08:00 - same probability of B and a happening
08:02 - together so there's another way that we
08:03 - can write this on this side such that
08:07 - this is also the probability of B
08:08 - conditioned on a time's the probability
08:11 - of a
08:19 - and if this is true if this is equal on
08:23 - both sides that means this and this are
08:25 - equal and if that's the case then let's
08:30 - throw this B over out here and rewrite
08:33 - this a B by itself and we'll do that one
08:37 - in yellow
08:38 - so here I'm gonna write the probability
08:41 - of a conditioned on B times the
08:45 - probability divided and I'm dividing
08:48 - this on the other end that's let me set
08:49 - this equal to this guy here probability
08:53 - of B conditioned on a time's the
08:56 - probability of a and I'm gonna pull this
08:58 - guy and just divided over here divided
09:00 - by probability of B and this equation is
09:08 - bayes's theorem
09:16 - and it is the basis for all statistical
09:20 - inference the conditional probability of
09:23 - a conditioned on B is equal to the
09:30 - probability of B conditioned on a times
09:32 - the probability of a divided by the
09:34 - probability of B these ideas actually
09:41 - have very common names that are often
09:46 - thrown around this value here is the
09:49 - posterior this value here is the prior
09:56 - this value here is the likelihood this
10:04 - value here this term over here is the
10:06 - evidence you'll also see these two terms
10:12 - called the marginal probabilities
10:23 - the marginal probability is just the
10:25 - probability of an event by itself so
10:28 - these are both marginal probabilities
10:30 - but if you have an event that you're
10:34 - trying to figure out the probability of
10:35 - and you know some other event occurred
10:38 - already right you have some point of
10:40 - evidence about it that you know these
10:42 - two are related then you can condition
10:45 - the likelihood of a occurring based on
10:48 - the evidence you've seen in B and
10:49 - rewrite it this way because here what
10:54 - you're asking you're saying is well how
10:56 - it's the probability that the evidence
10:59 - that I'm seeing is what it is if I
11:01 - assume that a is the event of interest
11:07 - that I'm looking at you multiply that by
11:08 - how likely a event is a by itself
11:10 - divided by the probability that you see
11:14 - the evidence B that you got and you
11:16 - actually will get your posterior and it
11:18 - all comes out of this relationship that
11:20 - it that the that the joint probability
11:23 - can be written in two different ways of
11:25 - conditional probability the naive Bayes
11:33 - classifier is a very simple statistical
11:37 - learning technique that leverage is the
11:41 - relationship of bayes's theorem to
11:44 - perform classification task that is
11:47 - otherwise difficult to simply estimate
11:51 - when there is lots of evidence available
11:54 - from different different features of
11:57 - data then you can make an estimate here
12:01 - of what the likelihood of the event is
12:04 - interested you know of interest is again
12:07 - this is our posterior this is our
12:09 - likelihood this is our prior and this is
12:12 - our evidence the naive Bayes classifier
12:17 - was one of the very first successful
12:21 - classifiers that could detect for
12:24 - example whether or not a piece of mail
12:27 - with spam versus legitimate so it was
12:30 - actually one of the very original spam
12:32 - busting algorithms that was deployed in
12:35 - the net
12:36 - and the Internet in the early days now
12:38 - we have much more sophisticated
12:39 - algorithms but the naive Bayes
12:41 - classifier still does a decent job and
12:45 - the way it works in general is that you
12:50 - want to for example a would be write the
12:53 - probability if we're talking about the
12:55 - spam example the probability that a
12:58 - piece of mail is spam and B here would
13:11 - be the evidence that we obtained right
13:16 - and the evidence could be all sorts of
13:17 - things it could be you know the text of
13:21 - the email it could be where it comes
13:25 - from it could be the time of day it came
13:40 - from it could be the length
13:48 - oops
13:52 - of the email it could be the presence or
14:03 - absence or the presence of certain words
14:07 - in the email or it could just be all of
14:21 - the different words the most classic
14:23 - version of this is simply the collection
14:25 - of all the words that are present in the
14:28 - eye so this is text all words the most
14:34 - the most classic version of this simply
14:37 - looked at just that I didn't do any of
14:40 - this stuff just took all the words that
14:44 - were in there and treated every word
14:47 - totally independently one of the key
14:49 - features of the naive Bayes classifier
14:51 - is that it assumes every piece of
14:52 - evidence that you're using is fully
14:54 - independent if your if your evidence is
14:57 - not independent then that can cause
14:59 - problems for the accuracy of the
15:01 - classifier so that's called a model
15:02 - mismatch but the naive Bayes classifier
15:04 - fundamentally assumes that all of the
15:07 - different pieces of evidence the
15:10 - different features the dimensions and
15:12 - the features in this case every word
15:13 - that's present is independent that may
15:16 - or may not be true but that's what the
15:18 - assumption of this D of this classifier
15:20 - does so that's our evidence B and so now
15:25 - let's take a look at what we're doing
15:27 - here what we're doing is we're saying
15:29 - well what is the probability we're gonna
15:31 - dam we're gonna we're gonna look at this
15:32 - what's the probability that we got that
15:34 - evidence all right so the probability of
15:37 - this piece of mail is BAM given the
15:38 - evidence we got right the text the words
15:41 - in the text is equal to the probability
15:45 - that we got this piece of evidence
15:47 - conditioned on this being a piece of
15:52 - spam would we get these words if what's
15:55 - the likelihood of it what's the
15:56 - probability of us getting those those
15:58 - words together seeing those words in
16:01 - this email can
16:02 - on the fact that this we're assuming
16:04 - this is either a piece of spam or
16:06 - condition on the fact that it's not a
16:08 - piece of spam that would be the
16:11 - probability this piece of mail is not
16:12 - spam
16:13 - all right so if a is the probability
16:14 - that it's not a spam then if this is
16:18 - this is just a binary classification
16:19 - then not a write the inverse of a
16:22 - opposite of a would be the probability
16:25 - of this piece of mail is not spam and
16:27 - thus that's equal to the probability
16:28 - that we see this piece of evidence
16:30 - conditioned on the assumption that it is
16:31 - not right not a not a piece of spam then
16:36 - we multiply this by the prior which is a
16:38 - probability that any piece of arbitrary
16:41 - mail that comes in is spam and so your
16:44 - prior might be very very high because in
16:46 - most mail systems the bulk of mail that
16:49 - goes through is just spam so this number
16:51 - and the probability that your mail is
16:54 - spam could be very high initially for a
16:56 - and the probability that it's not a it
16:58 - could be extremely low what does this
17:00 - mean this means that it helps push the
17:02 - system to make I things as spam because
17:05 - if for a thousand pieces of email that
17:08 - come in if 998 of them are spam and only
17:13 - two of them are real ajith omit then
17:16 - your prior right knowing nothing else
17:18 - other than a piece of email has come in
17:21 - you want to just assume that you know
17:24 - you your chances of that being spam is
17:27 - extremely high and so your prior right
17:29 - your your non evidence driven
17:32 - probability of this thing being spam is
17:34 - already very high and so you need to
17:36 - have overwhelming evidence in order to
17:40 - convince you that it is not spam right
17:43 - this number the probability of seeing
17:45 - those were conditioned on conditioned on
17:51 - this being spam needs to be extremely
17:54 - small
17:55 - to counteract something with the lard
17:57 - bias and that's that's exactly what
18:00 - these distributions are designed to do
18:02 - and then you divide everything by the
18:03 - probability of seeing that slice of
18:05 - evidence now for classification tasks
18:08 - this is not something that you tend to
18:10 - worry about very much because even
18:13 - though technically that will give you
18:14 - the actual probability here
18:16 - what you are really doing a naive Bayes
18:18 - classifier and this is in this two
18:20 - version example is pairing this
18:22 - probability with the opposite
18:25 - probability the probability of the other
18:28 - term all right so let's draw a line here
18:31 - you're comparing it to the probability
18:34 - that it's not spam given that we saw
18:39 - this piece of evidence and that's equal
18:41 - to the probability of B conditioned on
18:45 - not a times the probability of not a
18:50 - divided by the probability of B well
18:55 - this is great because if this is the
18:57 - probability that a piece of mail is not
18:59 - spam given our evidence and this is the
19:01 - probability that is spam given our
19:03 - evidence we're dividing by the
19:05 - probability of B in both cases let's not
19:07 - even bother because we don't want I
19:09 - think about that that's just the term
19:12 - that is the same in both so we can just
19:14 - get rid of it wonderful now we just have
19:18 - these two terms the probability of B
19:21 - conditioned on a and the probability of
19:23 - a or the probability of a not now in the
19:26 - case that the probability of the events
19:28 - is the same guess what then this term
19:32 - and this term would be the same and you
19:34 - could cancel these out as well when
19:36 - would this occur this would occur in the
19:38 - case where if you in a thousand emails
19:40 - half of them are legit and half of them
19:43 - are spam the probability or priors right
19:45 - the probability that any arbitrary piece
19:47 - of mail is spam is half and the
19:50 - probability that any arbitrary piece of
19:52 - mail is not spam is also half and thus
19:55 - these two probabilities these priors
19:57 - would be equal and you can cancel them
19:58 - out of the calculation for the spam
20:01 - example you cannot do that because we
20:05 - just said in a thousand emails 998 of
20:08 - them are going to be spam thus these
20:10 - probabilities need to be kept so that
20:13 - you can you can properly account for
20:16 - that other examples may have an equal
20:18 - distribution of events for a and not a
20:22 - or all the different different type of
20:23 - events that it could be and thus in
20:25 - those cases you can cancel this out
20:29 - but it's not a given you have to be very
20:31 - careful about what-what this prior is
20:34 - telling you let's again label our terms
20:38 - just so we know this is posterior and
20:40 - I'll do this in some green here so this
20:43 - is the posterior this is our evidence
20:51 - this is our prior and this is our
20:55 - likelihood and so now we're task right
21:04 - so it's obvious that we can calculate
21:06 - these these probabilities for our priors
21:08 - they're easy right because you just look
21:10 - at all of the different males you get
21:11 - and you know for some training set
21:13 - whether or not they're spam or not you
21:15 - just count those probabilities and thus
21:16 - you have your priors no problem now we
21:18 - have to think about how do they get this
21:20 - likelihood this is now the challenge to
21:24 - model what is the probability of seeing
21:26 - a particular evidence distribution that
21:28 - we just saw conditioned on the fact that
21:30 - we are assuming that this is spam or
21:33 - what is the probability of seeing this
21:35 - probability distribution conditioned on
21:36 - the fact that it is not spam that is not
21:40 - something that you can calculate
21:42 - explicitly however it is something that
21:45 - can be modeled and we can model it
21:49 - because we have prior data telling us so
21:53 - in order to build and execute an EIN
21:55 - based classifier you need a training set
21:57 - you need prior data and examples with
22:01 - which to build this likelihood estimate
22:04 - you need to be able to take some
22:07 - training set label some of them as spam
22:10 - some of them as not spam manually
22:13 - someone has to do this and then if you
22:16 - are looking for evidence and your
22:18 - evidence is going to be this text and
22:20 - all of the words in the text then you
22:23 - need to go in and find the probabilities
22:26 - that any given words showed up in spam
22:32 - versus not spam emails and you need to
22:34 - have a dictionary a table of all the
22:37 - different probabilities for all the
22:39 - different words
22:42 - when they are spam versus not spam and
22:47 - so if you had for example you were
22:54 - building this model if you had the word
22:56 - for example offer right or money then
23:04 - these terms Knight or opportunity or
23:12 - virus or hack right these are often
23:20 - terms that are usually in if they're in
23:24 - an email then you know or no other term
23:27 - Pro terms like MoneyGram right right
23:36 - these terms are more often associated
23:42 - with spam emails than not this the
23:49 - probability of seeing a word like this
23:52 - conditioned on it being spam is higher
23:56 - for virus hack and offer than it would
23:59 - be for not spam just as examples further
24:03 - because the naive based classifier
24:06 - treats everything at all the features
24:08 - all the words in this case since these
24:10 - are features as independent you can
24:12 - separately multiply the probabilities
24:15 - for every single one these words
24:16 - independently because you're not
24:17 - modeling any correlation between them
24:19 - that is often a limitation because if an
24:22 - email has the word hack in it probably
24:24 - also has the word virus in it or if it
24:26 - has the word offer and it probably also
24:27 - has the word money in it and if these
24:29 - terms are present together that could
24:32 - probably even strengthen right your
24:34 - ability to determine whether or not this
24:36 - is a piece of spam or not but that's not
24:38 - within the scope of this classifier
24:39 - classifier doesn't do this then I based
24:41 - classifier simply assumes that each of
24:43 - the features are independent and thus
24:44 - will independently look at the
24:46 - probability of the whatever words are in
24:49 - the the email and look up in its table
24:53 - look up in its model what the
24:55 - probability of see
24:56 - that word is conditioned on looking up
24:59 - in the in the spam table versus a non
25:00 - spam table and give you that probability
25:03 - as an output and then you do that for
25:05 - all different words that are in the
25:06 - email and you have a string of
25:08 - multiplications one for every single
25:10 - word and that if you look it up from the
25:13 - probability of the of the spam table of
25:17 - the spam spam train table gives you some
25:20 - probability and if you look it up for
25:21 - the table of words that are in the non
25:23 - spam category it gives you a different
25:25 - probability of multiplicative property
25:27 - and then you multiply those by the prior
25:29 - and whichever of these two are higher
25:31 - you then classify that piece of email as
25:36 - spam or not spam accordingly it's
25:39 - actually a very clever and simple idea
25:40 - but it requires being able to build
25:43 - these likelihood estimates and that
25:45 - requires previous training data but
25:48 - that's really all that's going on that's
25:50 - the process of a naive Bayes classifier
25:53 - it simply looks at prior examples builds
25:56 - up some likelihood build some prior and
25:59 - for all the different conditions that a
26:01 - can take on in this case it's only two
26:03 - because it's spam versus not spam it
26:06 - will estimate what the probability of
26:09 - that event occurring given the evidence
26:11 - that it is seen the same evidence in all
26:14 - cases right we're using the same
26:16 - evidence in it to evaluate whether or
26:18 - not a piece of email a piece of email
26:19 - because that's what we're evaluating
26:20 - right the email is there it is spam or
26:24 - not spam and so we're smashing it
26:25 - through the different likelihood models
26:27 - for some given evidence to see whether
26:29 - or not this conditional probability or
26:32 - this conditional probability is going to
26:33 - be higher and if you have three
26:35 - different possible outcomes of a right
26:38 - if it's if it's a decision you're making
26:40 - whether you have to go forward left or
26:42 - right then you have three different
26:44 - likelihood tables you have to look up or
26:46 - three different likelihood models you
26:47 - have to consult then you end up with
26:48 - three different probabilities and you
26:50 - take the highest of those and that's
26:51 - what the classifier would classify
26:54 - that's all that's going on in our Bayes
26:56 - classifier very simple but also
26:58 - extremely powerful is if you can get
27:00 - this likelihood model correct if you
27:03 - model your evidence correctly against
27:05 - the different conditions then it can be
27:07 - extremely accurate assuming the
27:10 - there is a difference of abilities in
27:13 - across the event conditions what does
27:18 - this mean in you when would about naive
27:21 - Bayes classifier fail for this example
27:23 - if a night anion based classifier would
27:25 - feel if spam and real emails used the
27:28 - same words right if the same words were
27:32 - used for both spam and non-spam emails
27:35 - in exactly the same frequencies and all
27:37 - that kind of stuff then this decoder
27:39 - this classifier wouldn't work at all it
27:40 - would not be able to tell the difference
27:42 - between spam and not spam because that's
27:45 - all it's using to come up with these
27:47 - these likelihood estimates but
27:51 - mercifully right what can what is very
27:55 - salient what's very important in
27:57 - determining whether a piece of email is
27:59 - spam or not is the actual words that are
28:01 - used and thus the likelihood of seeing a
28:05 - particular set of words when something
28:07 - is spam are going to be very different
28:08 - and the likelihood of seeing those same
28:10 - words if that if that email is not spam
28:12 - and this is how a naive Bayes classifier
28:15 - works this is how the earliest spam
28:17 - detectors worked by simply pulling out
28:19 - these models with all of the different
28:21 - emails that had previously been banked
28:23 - labeling them manually and building
28:25 - these banks of likely residents

Cleaned transcript:

conditional and joint probabilities are concepts in probability theory that are that are quite useful when multiple events are occurring a joint probability is quite easy to describe it's the probability that an event a and an event B occur at the same time so let's take for example that event a is the probability of landing let's say that a is defined as we have to define the event first a is the that a coin flip lands as heads and let's say that B is the event that a die rolls as a 2 we can easily calculate the probability of a this is simply equal to 1/2 we can also equally calculate the probability of B this is equal to 1 and 6 right fair die now let's ask the question what's the probability of a and B occurring together this is the joint probability well the probability that you both land as ahead and roll it to 1 these are independent events one doesn't influence the other so you have to have both coin and landing as ahead an or a rolling a die these are just these two multiplied together right it's 1/2 times 1/6 which is equal to 1/12 that doesn't seem particularly insightful but things get a little more interesting when we talk about events that are not independent let's define another event event C as the die roll being even so now what's the probability of B and C occurring at the same time well if it turns out that the Dyer comes out as a 2 then we know that the roll is also even so this is just 1 over 6 because it doesn't constrain it anymore similarly the probability that probability of C all by itself right is equal to 1/2 but here we did not multiply 1/6 by 1/2 because we know that there's an interaction effect these are not independent events now let's ask a different question let's ask the likelihood let's ask for the probability of two events occurring but knowing that one of the events already occurred so let's ask what the probability of B and C occurring is right this event of rolling a dice a two and the diving even but we know that this actually occurred we know that C occurred let's then divide it by the probability that the die roll was even we know that I'm telling you that this occurred we're asking for the probability that these two events occurred at the same time but we know that I'm just we're just speculating I'm speculating but we're just defining we're saying that C occurred the probability of the die roll B even happened we know that it is even so if we know that it's even what is the probability that it lands as a well that's this expression is 1 over 3 because if we know it's even then it's either two or four or six and it's one of those three so it's 1 and 3 this is the same write as 1 over 6 which was the joint the joint probability divided by probability of C which was 1 over 2 which is equal to 2 over 6 which is equal to 1 over 3 right it's the same that's quite cool and we define this expression as the joint as the conditional probability what's the probability that B occurs knowing that C occurs and this is equal to he joins C over P see I'm just rewriting this again so in so we have it in green in fact what I'm gonna do is I'm gonna write it with a and B the probability that some event a occurred knowing that some event B occurred is equal to the joint probability of a and B divided by the probability of B now it gets a little more interesting because this idea of two events occurring together is not by any means selective for a coming in front of B two events occurring simultaneously there's two events occurring simultaneously so there's actually two ways of writing this I mean that this let's look at this if we have the probability of an A intersecting with probability to be that these two occur together in fact what I'm gonna do is I'm going to write this in the middle and show it interpret it in two ways then on one hand write this is equal to we can divide this by by B as we did before and over here show our conditional probability so this is the conditional probability of a conditioned on B and normally I would have this right in the previous in the previous slide I had this divided by B but we'll just multiply that over here times the probability of B same thing I haven't changed the equation but I needed this is the same like the probability of a and B happening together is also the same probability of B and a happening together so there's another way that we can write this on this side such that this is also the probability of B conditioned on a time's the probability of a and if this is true if this is equal on both sides that means this and this are equal and if that's the case then let's throw this B over out here and rewrite this a B by itself and we'll do that one in yellow so here I'm gonna write the probability of a conditioned on B times the probability divided and I'm dividing this on the other end that's let me set this equal to this guy here probability of B conditioned on a time's the probability of a and I'm gonna pull this guy and just divided over here divided by probability of B and this equation is bayes's theorem and it is the basis for all statistical inference the conditional probability of a conditioned on B is equal to the probability of B conditioned on a times the probability of a divided by the probability of B these ideas actually have very common names that are often thrown around this value here is the posterior this value here is the prior this value here is the likelihood this value here this term over here is the evidence you'll also see these two terms called the marginal probabilities the marginal probability is just the probability of an event by itself so these are both marginal probabilities but if you have an event that you're trying to figure out the probability of and you know some other event occurred already right you have some point of evidence about it that you know these two are related then you can condition the likelihood of a occurring based on the evidence you've seen in B and rewrite it this way because here what you're asking you're saying is well how it's the probability that the evidence that I'm seeing is what it is if I assume that a is the event of interest that I'm looking at you multiply that by how likely a event is a by itself divided by the probability that you see the evidence B that you got and you actually will get your posterior and it all comes out of this relationship that it that the that the joint probability can be written in two different ways of conditional probability the naive Bayes classifier is a very simple statistical learning technique that leverage is the relationship of bayes's theorem to perform classification task that is otherwise difficult to simply estimate when there is lots of evidence available from different different features of data then you can make an estimate here of what the likelihood of the event is interested you know of interest is again this is our posterior this is our likelihood this is our prior and this is our evidence the naive Bayes classifier was one of the very first successful classifiers that could detect for example whether or not a piece of mail with spam versus legitimate so it was actually one of the very original spam busting algorithms that was deployed in the net and the Internet in the early days now we have much more sophisticated algorithms but the naive Bayes classifier still does a decent job and the way it works in general is that you want to for example a would be write the probability if we're talking about the spam example the probability that a piece of mail is spam and B here would be the evidence that we obtained right and the evidence could be all sorts of things it could be you know the text of the email it could be where it comes from it could be the time of day it came from it could be the length oops of the email it could be the presence or absence or the presence of certain words in the email or it could just be all of the different words the most classic version of this is simply the collection of all the words that are present in the eye so this is text all words the most the most classic version of this simply looked at just that I didn't do any of this stuff just took all the words that were in there and treated every word totally independently one of the key features of the naive Bayes classifier is that it assumes every piece of evidence that you're using is fully independent if your if your evidence is not independent then that can cause problems for the accuracy of the classifier so that's called a model mismatch but the naive Bayes classifier fundamentally assumes that all of the different pieces of evidence the different features the dimensions and the features in this case every word that's present is independent that may or may not be true but that's what the assumption of this D of this classifier does so that's our evidence B and so now let's take a look at what we're doing here what we're doing is we're saying well what is the probability we're gonna dam we're gonna we're gonna look at this what's the probability that we got that evidence all right so the probability of this piece of mail is BAM given the evidence we got right the text the words in the text is equal to the probability that we got this piece of evidence conditioned on this being a piece of spam would we get these words if what's the likelihood of it what's the probability of us getting those those words together seeing those words in this email can on the fact that this we're assuming this is either a piece of spam or condition on the fact that it's not a piece of spam that would be the probability this piece of mail is not spam all right so if a is the probability that it's not a spam then if this is this is just a binary classification then not a write the inverse of a opposite of a would be the probability of this piece of mail is not spam and thus that's equal to the probability that we see this piece of evidence conditioned on the assumption that it is not right not a not a piece of spam then we multiply this by the prior which is a probability that any piece of arbitrary mail that comes in is spam and so your prior might be very very high because in most mail systems the bulk of mail that goes through is just spam so this number and the probability that your mail is spam could be very high initially for a and the probability that it's not a it could be extremely low what does this mean this means that it helps push the system to make I things as spam because if for a thousand pieces of email that come in if 998 of them are spam and only two of them are real ajith omit then your prior right knowing nothing else other than a piece of email has come in you want to just assume that you know you your chances of that being spam is extremely high and so your prior right your your non evidence driven probability of this thing being spam is already very high and so you need to have overwhelming evidence in order to convince you that it is not spam right this number the probability of seeing those were conditioned on conditioned on this being spam needs to be extremely small to counteract something with the lard bias and that's that's exactly what these distributions are designed to do and then you divide everything by the probability of seeing that slice of evidence now for classification tasks this is not something that you tend to worry about very much because even though technically that will give you the actual probability here what you are really doing a naive Bayes classifier and this is in this two version example is pairing this probability with the opposite probability the probability of the other term all right so let's draw a line here you're comparing it to the probability that it's not spam given that we saw this piece of evidence and that's equal to the probability of B conditioned on not a times the probability of not a divided by the probability of B well this is great because if this is the probability that a piece of mail is not spam given our evidence and this is the probability that is spam given our evidence we're dividing by the probability of B in both cases let's not even bother because we don't want I think about that that's just the term that is the same in both so we can just get rid of it wonderful now we just have these two terms the probability of B conditioned on a and the probability of a or the probability of a not now in the case that the probability of the events is the same guess what then this term and this term would be the same and you could cancel these out as well when would this occur this would occur in the case where if you in a thousand emails half of them are legit and half of them are spam the probability or priors right the probability that any arbitrary piece of mail is spam is half and the probability that any arbitrary piece of mail is not spam is also half and thus these two probabilities these priors would be equal and you can cancel them out of the calculation for the spam example you cannot do that because we just said in a thousand emails 998 of them are going to be spam thus these probabilities need to be kept so that you can you can properly account for that other examples may have an equal distribution of events for a and not a or all the different different type of events that it could be and thus in those cases you can cancel this out but it's not a given you have to be very careful about whatwhat this prior is telling you let's again label our terms just so we know this is posterior and I'll do this in some green here so this is the posterior this is our evidence this is our prior and this is our likelihood and so now we're task right so it's obvious that we can calculate these these probabilities for our priors they're easy right because you just look at all of the different males you get and you know for some training set whether or not they're spam or not you just count those probabilities and thus you have your priors no problem now we have to think about how do they get this likelihood this is now the challenge to model what is the probability of seeing a particular evidence distribution that we just saw conditioned on the fact that we are assuming that this is spam or what is the probability of seeing this probability distribution conditioned on the fact that it is not spam that is not something that you can calculate explicitly however it is something that can be modeled and we can model it because we have prior data telling us so in order to build and execute an EIN based classifier you need a training set you need prior data and examples with which to build this likelihood estimate you need to be able to take some training set label some of them as spam some of them as not spam manually someone has to do this and then if you are looking for evidence and your evidence is going to be this text and all of the words in the text then you need to go in and find the probabilities that any given words showed up in spam versus not spam emails and you need to have a dictionary a table of all the different probabilities for all the different words when they are spam versus not spam and so if you had for example you were building this model if you had the word for example offer right or money then these terms Knight or opportunity or virus or hack right these are often terms that are usually in if they're in an email then you know or no other term Pro terms like MoneyGram right right these terms are more often associated with spam emails than not this the probability of seeing a word like this conditioned on it being spam is higher for virus hack and offer than it would be for not spam just as examples further because the naive based classifier treats everything at all the features all the words in this case since these are features as independent you can separately multiply the probabilities for every single one these words independently because you're not modeling any correlation between them that is often a limitation because if an email has the word hack in it probably also has the word virus in it or if it has the word offer and it probably also has the word money in it and if these terms are present together that could probably even strengthen right your ability to determine whether or not this is a piece of spam or not but that's not within the scope of this classifier classifier doesn't do this then I based classifier simply assumes that each of the features are independent and thus will independently look at the probability of the whatever words are in the the email and look up in its table look up in its model what the probability of see that word is conditioned on looking up in the in the spam table versus a non spam table and give you that probability as an output and then you do that for all different words that are in the email and you have a string of multiplications one for every single word and that if you look it up from the probability of the of the spam table of the spam spam train table gives you some probability and if you look it up for the table of words that are in the non spam category it gives you a different probability of multiplicative property and then you multiply those by the prior and whichever of these two are higher you then classify that piece of email as spam or not spam accordingly it's actually a very clever and simple idea but it requires being able to build these likelihood estimates and that requires previous training data but that's really all that's going on that's the process of a naive Bayes classifier it simply looks at prior examples builds up some likelihood build some prior and for all the different conditions that a can take on in this case it's only two because it's spam versus not spam it will estimate what the probability of that event occurring given the evidence that it is seen the same evidence in all cases right we're using the same evidence in it to evaluate whether or not a piece of email a piece of email because that's what we're evaluating right the email is there it is spam or not spam and so we're smashing it through the different likelihood models for some given evidence to see whether or not this conditional probability or this conditional probability is going to be higher and if you have three different possible outcomes of a right if it's if it's a decision you're making whether you have to go forward left or right then you have three different likelihood tables you have to look up or three different likelihood models you have to consult then you end up with three different probabilities and you take the highest of those and that's what the classifier would classify that's all that's going on in our Bayes classifier very simple but also extremely powerful is if you can get this likelihood model correct if you model your evidence correctly against the different conditions then it can be extremely accurate assuming the there is a difference of abilities in across the event conditions what does this mean in you when would about naive Bayes classifier fail for this example if a night anion based classifier would feel if spam and real emails used the same words right if the same words were used for both spam and nonspam emails in exactly the same frequencies and all that kind of stuff then this decoder this classifier wouldn't work at all it would not be able to tell the difference between spam and not spam because that's all it's using to come up with these these likelihood estimates but mercifully right what can what is very salient what's very important in determining whether a piece of email is spam or not is the actual words that are used and thus the likelihood of seeing a particular set of words when something is spam are going to be very different and the likelihood of seeing those same words if that if that email is not spam and this is how a naive Bayes classifier works this is how the earliest spam detectors worked by simply pulling out these models with all of the different emails that had previously been banked labeling them manually and building these banks of likely residents
