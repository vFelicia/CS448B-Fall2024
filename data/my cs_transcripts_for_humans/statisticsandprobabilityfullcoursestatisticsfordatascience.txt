With timestamps:

00:04 - well it looks like it's time to get
00:05 - started
00:06 - so let's start with section one of
00:08 - chapter one
00:09 - discussing uh descriptive statistics and
00:13 - some basic terminology
00:14 - of statistics so all right let's uh
00:18 - get to work then so uh
00:22 - so in this chapter in this section we're
00:24 - first going to start out
00:26 - uh with some basic terminology uh let's
00:28 - start let's start out with the
00:30 - data data is what statistics is
00:32 - interested in
00:34 - we're going to say that data is
00:35 - basically a collection of facts
00:37 - we have generally a population
00:41 - or some group of interest for which
00:44 - we want to make some statements so
00:48 - if we were to collect data for the
00:50 - entire population we would have
00:51 - conducted a census
00:52 - uh let's let's have some examples
00:54 - there's the classic example of the
00:56 - united states because
00:58 - right now we're actually in a census
01:01 - the 2020 census being conducted by the
01:03 - census bureau
01:04 - and this is a massive
01:08 - undertaking by the u.s government
01:10 - mandated by the constitution
01:12 - to count every single individual
01:15 - currently living in the united states
01:18 - and with that get
01:22 - to get a count that is supposed to be a
01:23 - complete count not an estimate but a
01:25 - count this becomes the facts
01:27 - of how many people are currently living
01:28 - in the united states
01:31 - on a smaller scale there is
01:34 - this class and i presumably have the
01:39 - grades for this class
01:40 - so if i were interested in the grades of
01:42 - this class i could conduct a census by
01:44 - looking at the grades of every single
01:46 - student
01:47 - and with that i would have the entire
01:50 - data set the entire population where
01:52 - i've defined the population to be
01:54 - this class now that said
01:58 - you may not necessarily have access to
02:00 - the entire population for example
02:03 - in the case of the united states the
02:05 - census is
02:07 - an extremely expensive and complex
02:10 - undertaking that can only be done every
02:12 - 10 years
02:15 - and in the case of this class well i can
02:17 - conduct a census
02:18 - you yourself as a student may not be
02:20 - able to conduct a census
02:22 - so if you were interested in how your
02:25 - other
02:25 - fellow students were doing in the class
02:28 - you would be forced to collect a sample
02:30 - a sample is a subset of the population
02:34 - for which you managed to collect data so
02:38 - in the case of the class if you were to
02:41 - talk to some of your friends which may
02:43 - not be the greatest idea anyway
02:45 - but if you were to talk to some of your
02:46 - friends about uh
02:48 - how and ask them how they were doing in
02:50 - the class then you would have conducted
02:51 - this that you would have
02:53 - uh selected a sample uh whereas um
02:57 - uh in the united states we're regularly
03:00 - uh subjected to surveys where a subset
03:03 - of the american population
03:05 - is going to be examined and you're going
03:08 - to ask questions about them and in the
03:10 - case of the census bureau
03:11 - they have a lesser intensity
03:16 - project such as the american community
03:18 - survey which
03:19 - isn't aimed at getting the entire
03:21 - population i think
03:23 - every year they try to get one percent
03:26 - of the population
03:27 - which is big in and of itself all right
03:30 - so uh in a sample we have
03:34 - observations so maybe these are
03:37 - people but in principle they could be
03:40 - just about anything like they could be
03:41 - petri dishes they could be
03:44 - the price of a stock on a particular day
03:46 - something like that
03:47 - and uh with those observations we have
03:50 - variables so in the case of our little
03:53 - person over here who is an observation
03:54 - in our sample
03:55 - we may have tracked their age we'll say
03:58 - this person is
04:00 - 22 we may track their gender
04:04 - uh we'll say that this person uh
04:07 - identifies
04:08 - as male or maybe we are tracking i don't
04:11 - know their
04:12 - uh occupation
04:15 - and we'll just say that this person is a
04:17 - student
04:19 - all right so univariate data is a data
04:23 - where there's only
04:24 - one variable being tracked so
04:28 - in this case if all we did was track age
04:30 - then this would be a univariate data set
04:32 - but this is actually a multivariate data
04:34 - set because we're not
04:35 - only tracking age we're also tracking
04:37 - gender and occupation
04:38 - so that makes this uh data set
04:40 - multivariate presumably
04:42 - there's more than just this individual
04:45 - in our sample uh maybe there are some
04:48 - under
04:49 - other individuals too for whom we've
04:51 - collected some data in this case this
04:53 - would be a multivariate data set
04:55 - there is kind of a special case for the
04:57 - multivariate case which is the bivariate
04:59 - case
04:59 - and bivariate data is a data where
05:02 - there's two variables
05:03 - and when you have only two variables at
05:05 - least in this class we can start talking
05:07 - about
05:08 - ideas such as correlation how are two
05:11 - uh variables related to each other uh
05:13 - you can do this for
05:15 - general multivariate data too but that's
05:17 - kind of going
05:18 - uh beyond the scope of this course and
05:20 - even then you kind of start out with the
05:22 - bivariate case first
05:24 - so data itself
05:27 - the variables can come in different
05:29 - classes such as there can be categorical
05:31 - variables
05:32 - so an example of categorical would be
05:35 - gender and occupation these would be
05:38 - categorical variables because there
05:40 - are a finite number of categories that
05:42 - these could be in
05:45 - compare that to say age which we would
05:47 - consider
05:49 - quantitative because what's being
05:51 - tracked is
05:53 - a number presumably an unbounded number
05:55 - now you might think well what about
05:57 - say uh the roll of a dice if you're
06:00 - rolling a dice
06:01 - then there's only a finite number of
06:03 - numbers one through six
06:04 - this is pretty much a simplification
06:06 - more with categorical we're thinking
06:08 - with things where
06:10 - there may or may not be an ordering but
06:11 - we really don't want to view categorical
06:13 - data necessarily as
06:15 - numbers per se
06:18 - whereas quantitative are numbers right
06:21 - and there's also
06:22 - alternative formulations alternative
06:24 - breakdowns uh with maybe some more
06:26 - granularity
06:27 - on how you're going to break down data
06:29 - like for example you might say
06:31 - there's nominal data where it's just
06:34 - categories that have no relation amongst
06:36 - each other
06:37 - you might then progress to ordinal data
06:40 - where you do add a little bit more
06:41 - structure
06:42 - where one observation where one value
06:45 - could be considered
06:45 - greater than another maybe we could talk
06:48 - about a person's year
06:50 - in college in which case you would say
06:52 - that sophomore
06:53 - is less than in some sense junior which
06:56 - is less than
06:57 - a senior
07:00 - although you don't necessarily want to
07:02 - attach numbers to
07:04 - uh sophomore freshman sophomore junior
07:07 - senior
07:08 - and then you could have maybe interval
07:10 - where
07:11 - you're allowed to do addition and ratio
07:14 - where you're allowed to do division
07:16 - so uh but but yeah that's
07:20 - an alternative breakdown don't worry too
07:22 - much about it
07:23 - in modern statistics we depend very
07:26 - heavily on probability theory
07:28 - probability theory is a field of
07:30 - mathematics that describes
07:31 - the behavior of objects in the presence
07:34 - of uncertainty
07:35 - the mathematical study of probability
07:37 - we've known about probability as a
07:38 - species for a very long time at least
07:40 - since the greeks
07:41 - but it's only until around the time
07:43 - calculus was being developed and
07:45 - probability
07:46 - uh started to take a mathematical
07:48 - interest and then it was around
07:50 - the beginning of the 20th century where
07:52 - probability became its own
07:54 - proper uh field in mathematics
07:57 - so why do statisticians care about
08:00 - probability how's probability used in
08:02 - statistics
08:03 - well we have this relationship um
08:07 - we have a population and we have
08:13 - a sample
08:18 - so a sample is a subset of the
08:19 - population so the question is
08:22 - how is it that we get a sample
08:26 - from the population how when when we
08:30 - collect a random sample from the
08:32 - population
08:33 - where every individual in the sample is
08:35 - picked
08:36 - with some with equal likelihood how will
08:39 - that sample behave
08:40 - how will statistics computed in that
08:43 - sample such
08:44 - such as for example the sample mean or
08:47 - even then like going beyond mean like
08:49 - let's say we track the smallest
08:51 - age how will statistics like that
08:54 - behave the behavior of those statistics
08:58 - is determined by probability
09:02 - so probability describes how
09:05 - random samples from a population behave
09:08 - and as statisticians we develop
09:11 - probability models
09:12 - for our samples and then use those
09:14 - probability models
09:16 - to describe uh parameters of the
09:18 - population
09:20 - so this would be statistics
09:26 - so probability and statistics are
09:28 - working in inverse directions
09:30 - where probability describes how if you
09:33 - knew the population how will the sample
09:35 - behave
09:36 - whereas statistics is interested in
09:39 - given a sample
09:40 - how can we infer the properties of the
09:42 - population like for example
09:44 - important population parameters
09:48 - so
09:51 - okay
09:54 - all right so uh let's let's continue on
09:57 - uh
09:59 - how we define a population largely
10:01 - depends on
10:02 - our problem for example in the examples
10:05 - that i gave before about the united
10:06 - states and my class
10:09 - we would consider a study involving
10:11 - those populations in numerative studies
10:14 - since in principle there is
10:17 - the population exists the population
10:20 - exists in physical
10:21 - and temporal space so
10:24 - that means that there's no like
10:28 - you can actually find in principle every
10:30 - single individual
10:32 - in the united states and you can find
10:34 - every single
10:35 - individual in my class and a census is
10:37 - in fact possible to conduct
10:39 - now compare that instead to analytic
10:43 - studies where the population may not
10:45 - exist so
10:46 - in my previous example where i was
10:48 - talking about petri dishes as being a
10:50 - potential
10:51 - sample a petri dish for
10:54 - some bacterial culture is probably going
10:56 - to appear in an analytic study
10:58 - a lot of those biological studies are
10:59 - going to be analytic studies because the
11:01 - population isn't necessarily considered
11:03 - existing
11:04 - if or in a simpler case if all i wanted
11:07 - to do was determine
11:09 - whether a single dice that i have was
11:11 - fair
11:12 - in principle you can't really say that
11:17 - all of the possible roles for this dice
11:19 - exist
11:20 - in some physical temporal space i can
11:23 - keep rolling this dice
11:24 - forever so uh
11:28 - you can't really consider an enumerative
11:30 - study so you're just gonna say the
11:31 - population is all possible die
11:33 - rolls for this dice and in the case of a
11:35 - medical study you might say
11:37 - that the population is all humans past
11:40 - present and future and we're studying
11:43 - the behavior
11:44 - of some drug in humans
11:47 - we're we're thinking of humans as some
11:49 - biological
11:51 - as a human species as a biological
11:53 - entity uh not necessarily
11:55 - the current people who are living right
11:57 - now we would probably want to include
11:58 - future people too
11:59 - um all right so
12:03 - uh statistics is it depends very
12:05 - crucially on how the data is collected
12:07 - and we're not actually going to talk
12:09 - very much in this class about how to
12:11 - collect
12:12 - data correctly i'm just going to leave
12:14 - you a few words
12:16 - in this class we're going to assume that
12:18 - data was collected
12:20 - via a simple random sample so
12:23 - if data was in fact collected via a
12:26 - simple random sample
12:27 - then ins there is a sense in which every
12:30 - individual
12:31 - in in the population is equally likely
12:34 - to have been chosen
12:35 - uh the metaphor that i like to think of
12:38 - is
12:38 - we have a hat and that hat has little
12:42 - slips of paper and each of those slips
12:44 - of paper will identify
12:47 - uh individuals in our population we pull
12:50 - one of the slips of paper at random with
12:52 - equal likelihood from the hat
12:54 - and that gives us an individual in the
12:56 - population that will be
12:57 - included in our sample compare that
13:00 - instead
13:01 - to stratified random sampling which is a
13:03 - more complicated procedure
13:05 - and the procedure that the census bureau
13:07 - is actually going to use
13:08 - in these annual american community
13:11 - survey
13:12 - studies where in this case you actually
13:15 - divide up your population according to
13:17 - some known strata
13:19 - a strata is something that you
13:22 - automatically know
13:23 - for individuals in your population for
13:25 - example what state they reside in
13:28 - so you divide up your population into
13:30 - strata
13:31 - and those strata don't necessarily have
13:33 - equal size
13:35 - but you're going to pick a random sample
13:38 - from each strata a simple random sample
13:41 - as i described before
13:44 - and then make inferences
13:48 - this procedure the stratification
13:51 - procedure
13:52 - can increase uh the power or the
13:55 - ability of your statistical procedure
13:59 - without having to collect as much data
14:02 - so it's a nice procedure to have in hand
14:04 - it's the
14:05 - procedure that is being used in more
14:07 - complicated surveys
14:09 - but we're not going to discuss it here
14:11 - the procedures that we discuss here are
14:13 - not appropriate
14:14 - for stratified sampling uh so it's of
14:17 - course
14:18 - very easy to sample badly
14:21 - uh so one instance of bad
14:24 - sampling is convenience sampling where
14:27 - you're basically selecting individuals
14:29 - from the population based on how easy it
14:30 - is for you to get the data
14:32 - in the case of this class if you were
14:35 - interested in how and what the grades of
14:37 - this class were
14:39 - and you decide to ask some of your
14:41 - friends what their grades are
14:43 - that's a convenience sample it is not a
14:45 - random sample
14:47 - because in the end the sample is going
14:50 - to resemble
14:51 - you in some way it's going to be more a
14:55 - reflection of you
14:56 - than it is of the class
14:59 - so you may end up if your friends are
15:02 - like better students and that's because
15:05 - better students like to
15:07 - co-mingle then that could be a problem
15:09 - because you're not going to get an
15:10 - accurate view
15:11 - or let's say we're studying politics
15:14 - and you decide that you want to
15:17 - determine people's um
15:18 - political party affiliation so you stand
15:20 - outside of the marriott library on
15:22 - campus
15:23 - and you start asking students uh what
15:25 - party do you support
15:27 - in elections uh in that case you are not
15:30 - gonna get anyone from st
15:32 - george utah uh almost sure well i mean
15:35 - you might get a couple people but for
15:36 - the most part st
15:37 - george utah will be extremely
15:38 - underrepresented in your sample
15:42 - and that's going to be a problem now you
15:44 - don't have an accurate representation
15:46 - of the state of utah if that if the
15:49 - state of utah were in fact
15:50 - your population of interest
15:54 - and even if the university of utah
15:55 - itself for your population of interest
15:57 - not every student is going to be hanging
15:59 - out around the library so that'll be a
16:00 - problem
16:01 - uh or in the case of a voluntary
16:04 - sampling where
16:05 - like the classic example is a
16:09 - a tv host goes and tells his viewers to
16:14 - participate in a survey is like who do
16:15 - you like the democrats or the
16:16 - republicans
16:18 - and it's like oh my gosh all my viewers
16:20 - like the same party as i do and they
16:22 - agree with me
16:23 - no shocker so that would be a case of
16:26 - a very unreliable study um
16:31 - so yeah that's that's another thing but
16:33 - we're not really going to talk too much
16:34 - about this uh
16:36 - this chapter this section i mean there's
16:38 - a lot that can be said about how to
16:40 - appropriately sample and
16:44 - actually this is one area where
16:48 - current events i would like to say some
16:49 - more about it like the coronavirus
16:52 - uh what are some of the statistical
16:53 - issues surrounding the coronavirus
16:55 - because there's a lot of statistical
16:56 - issues involving the coronavirus and our
16:59 - understanding
17:00 - of it and its effect on the population
17:02 - and i'm thinking i'm going
17:04 - to leave that to a separate video but
17:07 - for now that is the end of this section
17:09 - and uh i'll see you in section two on
17:13 - pictorial tabular methods
17:14 - in descriptive statistics so all right
17:16 - best of luck to you
17:24 - hey students alright so next section
17:28 - is on uh making graphs and making tables
17:31 - for descriptive statistics all right so
17:35 - first let's describe the idea of a
17:37 - distribution
17:38 - when we're talking about random
17:39 - variables there's a couple things that
17:42 - we should keep track of
17:43 - there is what values a random variable
17:47 - could take and there's also how
17:49 - frequently those values are taken
17:51 - those ideas are are
17:54 - captured in the distribution of the data
17:58 - set
18:00 - so um in uh in in this section we're
18:04 - going to see
18:05 - some uh basic ways to understand a
18:07 - distribution
18:08 - of an observed data set and in later
18:11 - sections we're going to
18:12 - talk about the idea of the distribution
18:14 - of a population
18:16 - and how it could possibly uh extend the
18:18 - idea of distribution
18:20 - and one basic way that we could
18:22 - understand a distribution is using a
18:24 - table
18:24 - and we can also use visualizations
18:26 - visualizations can actually
18:28 - be very helpful in understanding a
18:29 - distribution because there are things
18:31 - that our eyes are very good
18:33 - at picking out and when we make pictures
18:36 - of our data set we pick those things
18:38 - out like for example where does the data
18:40 - tend to cluster how
18:42 - spread out is the data are there
18:44 - outliers in the data set
18:46 - influential observations and our eyes
18:48 - are actually very good
18:49 - at noticing those features so
18:53 - creating visualizations of data says can
18:56 - lead to us learning a lot that actually
18:59 - really cannot be learned
19:00 - by numbers on their own in fact there
19:03 - are some
19:04 - uh examples for example there's this
19:06 - thing known as um
19:07 - amscom's quartet maybe i should uh maybe
19:10 - i should very quickly
19:12 - uh have a look at anscombe's quartet
19:15 - because we can do that
19:16 - um very quick
19:21 - okay so and oh let's see
19:25 - wk and scum is it
19:28 - b uh quartet
19:34 - this is close enough it'll probably
19:35 - figure it out
19:42 - is it ants comb
19:53 - and i can't type oh okay so it has a b
19:57 - in it all right so there's this picture
19:59 - right here
20:00 - and this is known as enscom's quartet
20:02 - this is for a bivariate data set
20:04 - uh which is not what we're talking about
20:06 - right now but
20:08 - uh when working with bivariate data sets
20:10 - you want to track
20:11 - uh for example the mean of x
20:14 - and the mean of y and also
20:17 - if you were to come up with a best
20:18 - fitting line a regression line between
20:20 - the two variables
20:21 - uh what would the parameters of that
20:23 - line be and what is the correlation
20:25 - between the variables
20:26 - and numerically every single one of
20:29 - these data sets are exactly the same
20:32 - even though our eyes can see they are
20:36 - not the same at all these are clearly
20:38 - not the same data set
20:39 - and they have very different properties
20:41 - so making visualizations can help us
20:43 - learn a lot in fact this is
20:45 - a subject in and of itself you can take
20:47 - a class
20:48 - from the computer science department on
20:50 - just visualization
20:51 - um but we're not going to be going into
20:54 - that much depth and visualization today
20:56 - um what we learned today is
20:59 - pretty much enough to get you by in life
21:02 - unless you're doing some really serious
21:04 - work
21:05 - so from this point on in this video
21:08 - and future videos i use
21:11 - this notation to describe a data set and
21:16 - this data set has sample size
21:18 - n so the sample size is the number of
21:20 - observations in a data set
21:22 - um the indexing here we have x1 x2 xn
21:26 - the indexing the numbers themselves
21:28 - don't actually matter they're just a way
21:30 - for us to differentiate observations
21:32 - these observations may have the same
21:34 - values or different values
21:36 - uh it so there's it it actually doesn't
21:39 - really say all that much about the data
21:40 - set
21:41 - in particular i don't want you to look
21:43 - at this and think that this data set is
21:45 - now
21:46 - ordered this data set is not assumed to
21:48 - be ordered
21:49 - if the data set ever becomes ordered i
21:51 - will let you know
21:52 - but right now it isn't um
21:55 - i also would like to draw attention to
21:57 - the fact that this is in lower case
21:59 - and it's it's not a
22:02 - it's not a perfect rule i've often
22:05 - violated it
22:07 - uh but it is a hint that lowercase
22:10 - things and this will probably be the
22:11 - case in the in the in the
22:14 - videos for this class um it is generally
22:17 - the case that lower case numbers are
22:19 - constant
22:20 - uh and a data set is constant a data set
22:22 - is something that you observe
22:23 - and because you observed it is now
22:25 - constant whereas
22:26 - capitals often refer to random things
22:29 - which is not what we're going to talk
22:30 - about today
22:32 - so uh the first visualization method
22:34 - that we're going to talk about
22:35 - is very basic it's called a stem and
22:38 - leaf plot
22:39 - so we create a stem leaf plot using the
22:41 - following steps
22:43 - the first thing we're going to do given
22:44 - a data set is we're going to select the
22:46 - number of
22:47 - leading digits to be the stem values
22:51 - so we have a number maybe that number
22:54 - is 123.45
22:59 - and we're going to select some number of
23:02 - digits maybe the first two digits
23:04 - and these will be the stem values the
23:06 - remaining step the remaining
23:08 - values uh let's use a different color
23:10 - for that uh
23:12 - where did red go the remaining values
23:14 - will be
23:15 - the leaf values which in principle you
23:18 - could
23:19 - put like the entire block 345 as a
23:22 - leaf value although a lot of people will
23:25 - just like round
23:26 - to just one digit uh because well you'll
23:29 - see for a second why someone might be
23:31 - inclined to do that
23:32 - um so after you make that selection
23:36 - you're going to draw
23:37 - a vertical line let's not use that color
23:40 - uh we're going to draw a vertical line
23:43 - and we're going to list
23:44 - the stem or possible stem values uh
23:47 - on the left-hand side of this line so we
23:49 - might have 12
23:51 - 11 10 and maybe 13
23:54 - and 14 and on the on the right hand side
23:58 - of this line
23:59 - we would record the leaf values let's
24:01 - assume for a second just for simplicity
24:03 - that we didn't observe this part
24:05 - and all we saw was the three
24:09 - we might list one two three and this
24:12 - would
24:12 - be read off as a hundred and twenty
24:15 - three
24:16 - because according to part four we're
24:18 - somewhere in this display we're going to
24:19 - indicate the units of the stem
24:21 - so we might say in this case tens uh to
24:24 - indicate that the
24:25 - stem is uh tens place and beyond
24:29 - uh which means that everything to the
24:30 - right will be in the ones place
24:33 - or maybe we're doing a more advanced
24:34 - plot where maybe we actually did write
24:36 - three four
24:37 - five so this would be one two three
24:39 - point four
24:40 - five uh but we're not going to do that
24:42 - because
24:43 - uh i actually think that would be it
24:47 - kind of defeats the purpose of a stem
24:48 - and leaf plot
24:49 - because what we would instead do is list
24:52 - the leaf values for each observation our
24:54 - data set so maybe we saw 101
24:56 - 101 1 1 2 3
24:59 - four so we have an observation a hundred
25:02 - and fourteen and another observation
25:04 - that's a hundred and thirteen and an
25:05 - observer uh
25:06 - another observation that's 112. uh or
25:09 - we'll also even throw in a nine so
25:10 - there's an observation that's 119.
25:13 - we've got 123 130 144
25:17 - um and that and now we have
25:21 - a display and let's just throw in like
25:24 - one more number uh like right here
25:28 - uh now we have a display that
25:31 - that um actually later on we're going to
25:34 - see a histogram and say
25:36 - oh this actually looks a lot like a
25:37 - histogram but we do have this
25:40 - display that's indicating that that's
25:43 - telling us a lot about this data set
25:44 - for one thing it actually tells us the
25:46 - actual values in the data set that's one
25:48 - advantage
25:49 - of a stem-and-leaf plot when you have a
25:51 - stem-and-leaf plot you get to see the
25:52 - actual data values
25:54 - and that's one thing that's nice another
25:56 - thing that's nice about them is that
25:57 - they're very quick to create
25:59 - like i imagine the stem and leaf plot
26:01 - being created in the field
26:03 - like you are out there you are watching
26:06 - uh i don't know birds and tracking how
26:08 - many birds you're seeing or
26:10 - characteristics of birds and you
26:12 - actually get to write down
26:14 - uh as as you are recording your data
26:16 - you're also visualizing it so you can
26:18 - learn
26:19 - uh things about it because you can see
26:20 - like for example this data set uh
26:24 - seems to cluster in this 110s area
26:28 - uh we might be able to guess that the
26:30 - mean is somewhere around here that this
26:33 - is like where the center of the data set
26:34 - is
26:35 - and we get some sense of its spread all
26:37 - right so um
26:39 - here is another data set for us to
26:41 - practice with
26:42 - uh this is a subset of mcdonald's
26:46 - data on hint on heightened finger
26:48 - lengths and
26:50 - this right here is actu this part right
26:52 - here is actually our code
26:53 - so we could go into r and type this part
26:56 - in
26:57 - and uh use r to do some of this analysis
27:02 - so um and i'll actually show you a
27:05 - little later
27:06 - how this can get turned into like what r
27:09 - would do with this part
27:10 - uh but um we're just gonna take this and
27:13 - treat it as our data set
27:15 - and construct a standard leaf plot by
27:17 - hand
27:18 - so what are we going to do uh we first
27:21 - need to decide
27:23 - um where we're going to make the cutoff
27:26 - we're
27:26 - not going to make it in the ones place
27:29 - because if we put it in the ones place
27:30 - then there's going to be just
27:32 - five for our stem value and what you'll
27:36 - end up with
27:37 - is a plot uh that looks like this
27:40 - and it will be just like a giant
27:42 - skyscraper and you won't
27:43 - you won't learn anything so we don't
27:46 - want that
27:47 - so the next place that is available to
27:49 - us is the what is the tenths
27:51 - place and the tens place seems pretty
27:53 - good so
27:55 - i'm now going to make the line for the
27:57 - stem
27:58 - and i'm going to write let's see we've
28:00 - got five
28:01 - three five five we have a five two
28:04 - oh we also have a five oh so we'll put
28:07 - five
28:07 - zero five one
28:11 - five two five
28:16 - three five four
28:19 - uh and we go all the way up to five nine
28:22 - so five
28:23 - five five six
28:26 - five seven five
28:30 - eight five nine
28:35 - okay there's there's some dead spots on
28:38 - this
28:38 - super super cheap computer i bought it
28:40 - for 200
28:42 - um at target across the street the
28:46 - which was the most convenient place to
28:47 - buy a laptop super cheap so
28:51 - you can't what you pay for and i'm
28:52 - especially learning that right now uh
28:54 - anyway um
28:57 - so uh we now have the stem values and
29:00 - now we can start recording our data set
29:01 - if you were to do this in a computer a
29:03 - computer would actually
29:04 - order the uh leaf values here they
29:08 - actually are ordered because nine comes
29:10 - after four which comes after three
29:12 - uh so a computer would actually order it
29:14 - i don't really see the point because the
29:16 - point
29:17 - is to understand the distribution as
29:21 - in in a visual way and ordering believes
29:24 - does not aid in that so we can go ahead
29:27 - and just start
29:28 - reading across in this data set to
29:32 - make our stem and leaf plot so we've got
29:35 - 5.55 so we'll put a 5 right here
29:37 - [Music]
29:38 - point three zero so we'll put zero there
29:42 - five six three five three
29:46 - zero five one three
29:50 - five zero five five
29:53 - three eight five nine
29:56 - six five two one
30:00 - and five three eight so in this case it
30:04 - actually turned out to be ordered and
30:05 - that's pure coincidence i wasn't
30:06 - planning on that
30:08 - but now we have a stem-and-leaf plot and
30:11 - if we scroll down
30:12 - and or if you're actually if you
30:14 - actually have these notes printed out oh
30:16 - one last thing we need to do is indicate
30:19 - what this line means
30:21 - and this line is the tenths place
30:31 - okay all right so if we were to continue
30:34 - on to the next page
30:36 - we would actually see some r co that
30:37 - would do this so this is the high data
30:39 - set from before
30:40 - this part right here the r function
30:44 - oh okay so this is the act
30:47 - the same height data set we had before
30:49 - the the r function stem creates stem and
30:52 - leaf plots
30:53 - and this is basically the same as we had
30:54 - before okay before i continue on i'm
30:57 - just going to double check to make sure
30:58 - i'm still streaming
31:00 - yeah we're still streaming okay all
31:02 - right um
31:05 - okay so next up for our visualizations
31:09 - we have a dot plot a dot plot
31:12 - represents each data point as a dot
31:15 - along a real number line
31:17 - uh so we can we would then be looking
31:20 - for clusters in dots
31:21 - to note patterns in our data set
31:25 - so in this example we're going to use
31:27 - the
31:28 - data from the previous example which has
31:30 - already been very nicely
31:32 - organized for us in a stem and leaf plot
31:35 - to create a dot plot let's see is there
31:38 - a nice
31:38 - easy way to zoom out oh very nice
31:43 - oh that's not what i wanted that's
31:44 - delete i need i'm new to this program
31:47 - uh and we're at the very bottom too
31:51 - that's not what i want either okay so
31:54 - all right so this this program is new to
31:57 - me my apologies
31:58 - um all right so all i want to do is zoom
32:03 - out a little bit okay that's good
32:06 - that's good um
32:09 - oh cool all right i just want to be able
32:13 - to see
32:14 - this uh nice stem and leaf plot that we
32:16 - created before
32:18 - when creating our dot plot
32:21 - so for our dot plot
32:25 - uh let's see we've got let's see
32:29 - so for the left point on the range we
32:32 - should pick
32:33 - 5.0 and
32:35 - uh for the right point of the range we
32:39 - should probably pick
32:39 - 6.0 because we get really close to 6.
32:43 - so in between we've got 5.5
32:47 - and then we can just mark off
32:50 - in in a fifths so
32:56 - two three four five okay that's that's
32:58 - that's close enough
33:00 - uh if you want perfection ask a computer
33:02 - to do it
33:03 - all right so um so we've got an
33:06 - observation 55.05 so that's about right
33:11 - there
33:13 - 5.13 is about there
33:18 - 5.21
33:20 - is about there and then we've got 5.30
33:24 - [Music]
33:25 - um that is about there and that's also
33:28 - got
33:29 - another one right with it uh there's
33:31 - another
33:32 - observation uh oh that's actually could
33:35 - be a little bit to the left but we could
33:37 - also call that 5.38
33:39 - so here's 5.30 for realsies this time
33:43 - we've got 5.55
33:47 - 5.63
33:49 - and then 5.96 so then we end up with the
33:53 - stop plot
33:54 - that's also giving us a sense of where
33:55 - the data tends to lie
33:57 - but this time along a real number line
33:59 - so
34:00 - and and i i'm sure you can guess so far
34:02 - that the methods that we have been
34:03 - working with
34:04 - uh our visualization methods are pretty
34:06 - much devoted exclusively
34:08 - to uh quantitative data we'll see
34:10 - categorical in a second
34:12 - i'm pretty much going to say draw bar
34:14 - plot and that's it so
34:16 - here is our code for uh what i just did
34:19 - the r function for making dot plots is
34:22 - a strip chart
34:26 - so strip chart is what makes these
34:28 - things and i'm just passing some
34:30 - additional parameters to strip chart
34:32 - to uh make the function to make the plot
34:35 - how i like like for example this
34:37 - parameter right here controls
34:39 - how it's like what exactly we're drawing
34:40 - here are we drawing filled in circles or
34:42 - empty circles and so on
34:44 - so this you can think of this as point
34:46 - character because you're choosing
34:48 - the character of the point and 19
34:50 - corresponds to a filled in circle
34:52 - uh we have a little bit of an offset and
34:56 - uh some other parameters that just help
34:58 - make the
34:59 - visualization look nice i would
35:01 - recommend go ahead going ahead firing up
35:04 - r playing around with these parameters
35:06 - seeing what happens
35:07 - uh stack for example uh means that when
35:10 - two points are about to collide with one
35:12 - another
35:12 - stack them on top of each other because
35:15 - it's not the only way to do things there
35:16 - are often very many ways to solve the
35:18 - same problem
35:20 - so all right uh continuing on
35:25 - so a quantitative variable we we need a
35:28 - further
35:29 - a refinement of quantitative variables
35:32 - and and we say that a quantitative
35:34 - variable is discrete
35:35 - if it's possible values are countable uh
35:38 - countable that could be for example one
35:41 - two three four five six
35:43 - or that could be one two three four five
35:45 - six seven eight nine blah blah blah up
35:46 - into the future
35:48 - uh it could be the integers so including
35:50 - negative numbers as well
35:52 - or including zero as well um
35:55 - and then so we would call any of those
35:57 - things countable
35:58 - there's also continuous variables
36:02 - where the possible values could come
36:04 - from a continuum
36:05 - so that includes 0 1 and anything in
36:08 - between so
36:10 - 0 one zero point five
36:13 - uh zero point zero one one zero one two
36:16 - five uh one divided by pi
36:19 - any one of those things are possible uh
36:22 - there's actually a nice little rule of
36:24 - thumb
36:24 - which is that uh discrete random
36:27 - variable
36:28 - discrete variables emerge when you count
36:31 - things like for example i count sheep so
36:34 - what am i going to get i'm going to get
36:35 - an integer in the end so if that's going
36:38 - to be the case
36:39 - then it's comfortable whereas continuous
36:41 - emerges from measurements
36:43 - so like with a ruler so you measure how
36:45 - long something is with a ruler and that
36:47 - will probably produce
36:48 - a continuous variable
36:51 - so continuing on uh some further notions
36:55 - when describing a distribution
36:57 - there is the frequency of a of a value
37:01 - and there's also the possible values it
37:03 - could take so frequency
37:05 - is how frequently we see
37:08 - a variable take a particular value and
37:11 - for discrete variables it is in fact
37:13 - possible and quite likely that
37:18 - we will see the same numbers repeatedly
37:21 - whereas if we're dealing with continuous
37:23 - you cannot necessarily assume that so
37:25 - the frequency for each value for a
37:28 - continuous variable would be
37:30 - rather uninteresting since you're
37:32 - probably going to see
37:33 - like going back to the data set we were
37:35 - just looking at
37:36 - admittedly uh for this uh height
37:39 - data set we did have some collisions
37:43 - but you can't really count on that you
37:44 - can imagine that
37:46 - if you were to increase the decimal
37:47 - position precision of the numbers that
37:49 - you're looking at
37:50 - you would um
37:54 - you would actually not have a collision
37:56 - you'd have two distinct numbers the only
37:58 - reason why
37:59 - we get 5.30 twice is because that's
38:02 - where we decided to round so
38:06 - in the case of continuous variables well
38:08 - with frequency
38:09 - well with discrete variables it's okay
38:11 - to just kind of consider the values in
38:13 - and of themselves and just say
38:14 - how many times did one show up how many
38:16 - times did two show up and so on
38:18 - for continuous variables we probably
38:21 - should do a procedure known as binning
38:24 - so binning is where we define a range
38:28 - in which a data point could possibly be
38:30 - and instead of tracking how frequently
38:32 - a certain value is taken we're going to
38:34 - track how frequently
38:36 - um a variable will fall into a bin
38:39 - right so you can imagine for instance
38:43 - that this is a this this is a continuous
38:46 - line between zero and one
38:48 - and we saw uh numbers
38:51 - uh in this region and they're just
38:53 - random numbers
38:55 - and we're not going to actually list out
38:57 - in a table
38:58 - how often we saw those individual
39:00 - numbers because most of the time the
39:01 - frequency for that number will be one
39:04 - instead what we'll do is we'll divide up
39:06 - this region and then count
39:08 - how many times uh numbers fell into
39:11 - those individual bins like for example
39:12 - we get we got twice here
39:14 - three times a year uh one two three four
39:17 - five
39:17 - six times here one two three
39:20 - four five six times here and then four
39:22 - times here and then we'd have like a
39:25 - more useful table
39:26 - um for understanding our distribution
39:29 - very closely related to the frequency is
39:31 - the relative frequency relative
39:32 - frequency
39:33 - is where you take the frequency and
39:35 - divide it by the sample size
39:36 - that's it so if you really want to see
39:40 - a formula for the relative frequency
39:44 - the relative frequency
39:50 - the relative frequency is equal to
39:54 - frequency
39:57 - divided by n
40:00 - a frequency distribution is a tabulation
40:03 - of the frequencies or the relative
40:05 - frequencies or
40:06 - in the case of continuous variables we'd
40:08 - probably also include the frequency of
40:10 - bins um something that i haven't really
40:12 - discussed so far
40:14 - is that actually how should we choose
40:16 - the bins
40:18 - like like like bidding is a choice
40:21 - i chose these bins but alternatively
40:25 - i could have chosen bins like this like
40:27 - here's one bin and here's another bin
40:29 - why didn't i do that so
40:32 - the actual bidding decision is it is in
40:35 - fact a decision
40:37 - you need to decide how many bins they
40:40 - are and what regions they're going to
40:41 - cover
40:42 - so in terms of the regions i think it's
40:45 - pretty safe to say
40:46 - the bin should be equal length if the
40:48 - bins are not equal length then what
40:50 - you're going to end up with
40:51 - is a difficult to understand chart or a
40:54 - difficult to understand table
40:55 - because people then have to pay much
40:57 - more attention to the bin length
41:00 - um and also when you're trying to
41:02 - visualize
41:03 - what's like this is leading up to
41:04 - histograms and we're going
41:06 - and the binge bin size is uh
41:10 - something you have to make when talking
41:11 - about histograms and when you
41:14 - um uh when you end up with bins of
41:17 - unequal length then the histogram
41:20 - like you there is ac there is a precise
41:22 - way to understand
41:23 - what's being visualized in the histogram
41:25 - but unfortunately
41:26 - people's eyes are not going to perceive
41:27 - it that way um
41:29 - and it's that you are making the
41:31 - histogram more difficult to understand
41:34 - so so just always make your bins equal
41:38 - length
41:39 - but the number of bins that you have and
41:41 - also what exact regions they cover
41:43 - because you can still for example take
41:46 - all of these bins shown here
41:50 - take all these bins and then shift them
41:52 - slightly to the left
41:55 - and then you have then you have a
41:56 - different visualization
41:59 - or you then have a very different
42:01 - description of your
42:02 - of your data set how exactly should you
42:06 - decide where the boundaries of your bins
42:08 - are
42:09 - and how many bins should you have these
42:12 - are all very important decisions that
42:14 - can have uh drastic implications for
42:17 - visualizations
42:18 - but i i really should move on to
42:21 - actually creating a frequency
42:23 - distribution which is a tabulation of
42:24 - frequencies or relative frequencies
42:26 - i should really move on before i start
42:29 - digging into that
42:30 - topic so let's make
42:34 - a frequency distribution for this
42:37 - soccer data set where we have a supposed
42:40 - statistically minded parent
42:41 - maybe that parent is you if you have a
42:43 - daughter or a son and they play soccer
42:46 - and you've and you've decided that
42:48 - you're going to
42:50 - uh track their little league soccer team
42:53 - scores during a regular season
42:55 - so here i have a data set i've and by
42:58 - the way what i've done here
43:00 - uh in terms of our code is creating an r
43:02 - vector
43:03 - so i have this list of values
43:07 - um in what's known as an r vector i'm
43:09 - not really going to talk about
43:10 - r in these videos i'm going to
43:14 - pretend and assume that
43:17 - you are watching my video series on how
43:20 - to use
43:20 - r uh but enough of that for now um
43:25 - i want to construct a frequency
43:26 - distribution for this data set so what
43:28 - are possible values
43:29 - in this data set uh well uh
43:33 - i should let's see uh we've got
43:36 - we've got a so on the left hand side of
43:40 - this line for this table
43:41 - so we're going to have um a value
43:46 - and we're going to have on the right
43:48 - hand side the frequency
43:52 - so what are possible values well we've
43:54 - got one and we've got nine and we've got
43:56 - numbers in between so we'll just go uh
43:59 - one two
44:00 - three four five
44:03 - six seven eight nine
44:07 - all right and then we need frequency so
44:09 - let's see how many times did 9 appear
44:12 - looks like 9 just appeared once all
44:14 - right so we got that covered
44:16 - uh six appeared uh
44:19 - so one two yeah that's it so six
44:23 - appeared twice
44:24 - uh five appeared three times
44:30 - and uh eight appeared twice
44:36 - uh and each each of the remaining
44:38 - numbers appears only once
44:40 - so one appeared once two appeared once
44:43 - uh three appeared once and four appeared
44:46 - once
44:47 - okay uh seven never appeared
44:52 - all right
44:56 - one second i catch up in my notes
44:59 - um so the sample size for this data set
45:02 - you can count
45:03 - there are 1 2 3 4 5 6 7
45:07 - 8 9 10 11 12 observations or
45:10 - alternatively what you could have done
45:12 - is um and in fact i'll go ahead and just
45:14 - kind of
45:15 - scroll down a little bit and down here
45:17 - i'm going to track
45:19 - the sum and yeah it adds up to 12.
45:22 - another thing i want to do is in this
45:25 - table
45:26 - track the relative frequency
45:31 - so we'll call this relative frequency
45:38 - and in the relative frequency i'm going
45:41 - to take the frequency and then divide by
45:42 - the sample size
45:44 - so 1 divided by 12. uh that
45:47 - is going to be a 0.05
45:55 - or is that an 8 no that's an 8. 0.08
45:59 - three
46:00 - and the three is a continuing uh digit
46:04 - all right so for the next one it's the
46:05 - same thing zero
46:07 - eight three uh and
46:10 - again for three zero point zero eight
46:14 - three okay and then four
46:17 - five uh no wait so let's see uh
46:20 - four also appeared that amount so zero
46:23 - point
46:24 - zero eight three uh
46:27 - four appeared three times so that's
46:30 - going to be
46:31 - zero point uh
46:34 - two five because three divided by
46:38 - uh 12 will be 0.25 3 is a quarter of 12.
46:43 - 6 will be 0.16 where the 6 is a
46:49 - continuing digit
46:51 - 0 0.16
46:55 - and 0.083
47:00 - okay and if you add these numbers up
47:02 - this is a way for you
47:04 - to check your sanity this should add up
47:08 - to one
47:11 - okay
47:16 - so moving on if you were interested in
47:19 - how to make
47:20 - a table like this in r the function
47:24 - for r in r for making such a table is
47:27 - well table so you would have given it
47:29 - the soccer data set and ask it to make a
47:31 - table
47:32 - and it will tabulate uh which
47:34 - observations
47:35 - were taken and how frequently they were
47:36 - taken notice that it did skip
47:38 - seven um which i don't know is that a
47:41 - feature you want
47:42 - i don't know uh i actually personally
47:45 - would
47:46 - rather not skip seven and you'll
47:47 - probably see why in a second
47:50 - um when working with continuous data as
47:52 - i was mentioning before there's this
47:54 - issue of
47:54 - binning uh deciding on the number of
47:57 - bins uh deciding um
47:59 - uh where the boundaries of the bins are
48:02 - and so forth
48:03 - but um so
48:08 - but uh once you've decided on the number
48:12 - of bins and there are some rules of
48:13 - thumb
48:14 - that you can use uh for how many bins
48:18 - that you should use
48:19 - like for example if the if the sample
48:21 - size is n you could perhaps choose
48:23 - the number of bins to be the square root
48:25 - of n actually there is some
48:26 - statistical arguments that the correct
48:29 - number of bins should be on the order of
48:31 - n to the power one fifth or the fifth
48:33 - root of n
48:34 - um there are some arguments for that but
48:38 - uh square root of n is also fine for now
48:41 - um
48:42 - at the end of the day whatever it is
48:44 - that r is going to use
48:47 - for its uh binning decisions is better
48:49 - than what you're going to do
48:51 - and it's going to have some uh nice
48:54 - theory backing it up
48:55 - so you probably should just trust r
48:57 - whatever r is doing
48:59 - is probably better than what than than
49:01 - the square root of n roll
49:02 - um so after you've decided on the number
49:06 - of bins
49:07 - you segment your number line so that you
49:10 - have that many equal length bins
49:13 - uh and uh depending on where in each
49:16 - data point falls assign it to a bin
49:18 - if it falls on a border between the bins
49:20 - assign it to the bin
49:21 - on the right so in other words bins are
49:25 - right inclusive that is a parameter some
49:27 - people prefer left inclusive bins does
49:30 - it really matter no it doesn't
49:31 - it doesn't really matter nobody really
49:33 - cares uh just just pick one
49:35 - right and be consistent either put it on
49:37 - the left band or the right bin just
49:38 - don't
49:39 - put it in both please and then construct
49:42 - a frequency distribution for the bims
49:44 - uh very quickly before i keep going okay
49:47 - we're still good
49:48 - all right um i i i'm very nervous about
49:52 - losing this video
49:53 - i'll just put it this way today i've
49:55 - recorded a number of videos that
49:56 - suddenly just went up into thin air
49:58 - because things went bad and i don't want
50:00 - that to happen again
50:02 - all right um so uh example four so using
50:05 - the data from example one
50:07 - uh you can scroll you can go back in
50:09 - time to see what that is
50:11 - this is the heights data set uh
50:13 - construct a frequency distribution
50:16 - and for what it's worth there were 10
50:18 - observations in that data set
50:21 - the function length when given a vector
50:24 - will tell you the number of elements in
50:26 - that vector
50:26 - in r so since the number of elements is
50:30 - the number of data points we want to use
50:31 - length to determine the sample size
50:35 - okay continuing on i have made some
50:39 - decisions
50:40 - i'm going to decide that the number of
50:43 - bins i'm going to use
50:44 - is going to be uh about the square root
50:48 - of 10. the square root of 10
50:52 - is between three and four
50:55 - so uh uh so if you wanted to use the
50:59 - square root of ten rule
51:00 - uh that would be um about uh
51:05 - we'll say for rounding up
51:08 - although apparently in my notes i
51:10 - decided that i was going to go with
51:11 - five and i don't want to change it so
51:15 - i'm gonna go with five why five why not
51:18 - five
51:19 - um it's it's it's
51:22 - it's whatever you want i i think the
51:24 - reason why i went with five
51:26 - uh i don't know i just did
51:30 - so um let's go ahead and
51:33 - zoom oops is that what i want
51:38 - yeah that's fine okay so
51:41 - uh i'm going to make a table so i
51:45 - on uh in this table i'm going to list
51:48 - the bins
51:49 - so i've got 5 to
51:52 - 5.2 and to indicate this is going to be
51:56 - right inclusive i'm going to put a
51:58 - little uh
52:00 - less than sign to say 5 2 less than 5.2
52:04 - not including 5.2 and then 5.2
52:08 - to uh 5.4
52:12 - uh 5.4 hold on
52:17 - okay five point
52:21 - four two five
52:25 - point six five
52:28 - yeah i need to kind of get away from
52:31 - there
52:32 - okay uh 5.6
52:36 - to 5.8
52:39 - and then 5.82
52:43 - uh less than six and we can just leave
52:46 - it at that
52:47 - all right and then we have the frequency
52:49 - for each of these bins
52:53 - so the frequency for these is going to
52:55 - be well there were two numbers between
52:57 - five and five point two
52:59 - uh five numbers between 5.2 and 5.4
53:02 - and then one number in each of the
53:04 - remaining bins
53:06 - so uh if we were to do the relative
53:12 - frequency
53:15 - we're going to take each of those
53:16 - frequencies and divide it by 10. in
53:17 - which case it's you're just going to
53:18 - move the decimal point over one place
53:20 - so 0.2 0.5 0.1.1.1
53:26 - okay all right once we have a frequency
53:30 - distribution
53:31 - such as this we can now construct what's
53:33 - known as a histogram
53:34 - which is a plot for visualizing the
53:36 - distribution of quantitative data
53:38 - so how do we construct a histogram first
53:41 - draw a number line
53:43 - and mark the location of the bins so for
53:46 - example we could do something like this
53:48 - and we're going to say that the bins are
53:49 - going to be about
53:51 - here um and then for just
53:54 - if you want to for discrete data you can
53:56 - center your bins on the corresponding
53:58 - values themselves
53:59 - um because you're not really doing any
54:02 - binning with the
54:03 - the discrete variables or you can
54:04 - imagine that your bins are exactly what
54:06 - they need to be
54:07 - uh to be touching each other and uh
54:10 - centered on the integer
54:12 - uh the corresponding integer and then
54:15 - for
54:15 - each of these classes or bins draw a bar
54:19 - extending from the number line so we
54:21 - have
54:22 - uh some y-axis that's tracking let's say
54:26 - the frequency
54:32 - so we have a y-axis that's checking the
54:33 - frequency we're going to draw
54:35 - a bar from the number line to the
54:39 - y value that corresponds to the
54:40 - frequency of that bin
54:43 - or the relative frequency if that is in
54:45 - fact what you're plotting on the y axis
54:47 - so it would end up with a plot maybe
54:49 - looking like this
54:52 - and that resulting plot is a histogram
54:59 - okay so uh going back to some of our
55:02 - examples
55:03 - let's draw a histogram for the data set
55:06 - in example three which was that soccer
55:08 - data set
55:13 - so for the soccer data set we had
55:15 - numbers between
55:17 - one through nine we'll go ahead and
55:18 - include zero as well
55:21 - for this data set so
55:24 - i'm just going to i'm going to start out
55:27 - by drawing the graph the
55:30 - x-axis corresponds to goals
55:34 - so goals in a game
55:38 - and the y-axis corresponds to the
55:40 - frequency
55:46 - so let's see the the frequencies never
55:48 - seem to go beyond three so we've got
55:51 - uh one two three
55:55 - okay and then possible integer values
55:58 - i'm going to go
55:59 - i'm going to go ahead and include zero
56:01 - because in principle this soccer team
56:03 - could go could score zero points so
56:06 - we'll go ahead and include zero
56:08 - but we'll also include at the very end
56:10 - we'll have
56:11 - nine and we've got one two
56:14 - three four five
56:17 - six seven eight nine okay
56:21 - so uh they there was one game where they
56:24 - scored one point
56:26 - uh one game where they scored two points
56:29 - one game where they scored three points
56:31 - uh and one game where they scored four
56:33 - points
56:34 - then they had three games where the team
56:37 - scored five points uh
56:40 - two games where they scored six points
56:44 - uh no games where they scored seven
56:46 - points
56:47 - uh two games where they scored eight
56:50 - points and
56:51 - one game where they scored nine points
56:56 - okay hmm new feature let's go ahead and
56:59 - see what happens
57:01 - uh when i no i don't think that works
57:04 - oh well um so there there be the
57:07 - histogram
57:08 - it's it's not a perfect picture but it's
57:10 - mine
57:11 - so if we wanted to down here is the r
57:15 - code
57:16 - for constructing a histogram so the r
57:19 - function is hist and
57:22 - uh i've given it some parameters to
57:25 - manually specify the breaks
57:27 - because i because otherwise
57:30 - um it would choose its own algorithm it
57:33 - would use its own procedures to come up
57:35 - with the breaks
57:36 - and i wanted to override that and this
57:38 - is one way to do it
57:39 - where i basically gave uh the
57:43 - um uh the function
57:46 - the uh break points so we have
57:50 - so the minimum of scott of soccer is
57:52 - going to be uh
57:54 - 1 so 1 minus 0.5 will be
57:58 - um why can't i do math it's 0.5
58:03 - um and then you have and then i went one
58:06 - above the maximum score
58:08 - so that would be this number right here
58:09 - so this right here corresponds to 0.5
58:12 - and this will be a 9.5 uh over
58:15 - at the right hand side and that's just
58:17 - telling it where i want those breaks
58:19 - and then it will infer that you have
58:23 - that everything in between is a bin uh
58:25 - but then you end up with a
58:26 - with a pot that is essentially what we
58:28 - came up with by hand
58:31 - okay uh next example for
58:34 - uh the data set in example one let's
58:37 - create a histogram
58:39 - okay so for that one i am going to use
58:42 - the relative frequencies instead
58:44 - which do you use at this point it
58:47 - doesn't really matter
58:48 - because the shape is going to be the
58:50 - same regardless of whether we divide by
58:52 - n or not
58:53 - and the interesting part when you're
58:55 - creating a histogram
58:56 - is looking at the shape of the resulting
58:58 - histogram so
58:59 - i actually really don't care although
59:02 - for what it's worth if you want to use
59:04 - this thing for more probabilistic
59:06 - inference
59:06 - you probably should pay more attention
59:08 - to whether you've got the frequency or
59:09 - the relative frequency
59:10 - um and if also the bins were not
59:15 - of um
59:18 - uh equal length then you would have to
59:22 - pay much more attention
59:23 - to the frequency versus relative
59:25 - frequency but also just take my advice
59:28 - and make your bins all equal length
59:30 - so uh we've got the relative frequency
59:34 - that is what we're going to draw this
59:35 - time
59:37 - and uh we've got the highest it will
59:40 - ever go
59:42 - is uh 0.5 in fact i don't think it ever
59:44 - reaches there
59:45 - so we've got right yeah it does it does
59:48 - reach
59:49 - 0.5 so and we'll just increment by
59:52 - 0.1 so 0.1.2 0.3.4.5
59:56 - so this is 0.1
60:00 - uh right here so possible bin values i
60:03 - said
60:04 - i decided that we were going to have
60:06 - five bins for some crazy reason i don't
60:08 - remember what it was
60:09 - uh and so let's see is that five
60:12 - it is now so we've got numbers between
60:15 - five
60:16 - and six and using the table that we came
60:18 - up with uh
60:19 - that time ago we're going to end up with
60:21 - a histogram
60:22 - uh looking something uh like like uh
60:26 - this so it goes up to here
60:30 - and then oh yeah so it goes to 0.2 and
60:33 - then
60:34 - 0.1 for these remaining bins
60:38 - and that's our histogram
60:44 - so if we were to continue along and look
60:46 - on the next page
60:47 - uh oh i think that's the reason why i
60:49 - chose 0.5 it's because
60:50 - it it's exactly what r thinks it should
60:53 - be and you should probably
60:54 - given the choice between what you think
60:56 - the number of fins should be
60:57 - and what are things it should be you
60:58 - should probably go with what r is good
61:00 - what r
61:00 - is doing um don't fully trust it
61:04 - but at this point you probably don't
61:06 - have the experience to
61:07 - have your own opinion so um
61:11 - yeah it came up with basically the same
61:14 - picture
61:15 - okay now we've been coming up with these
61:18 - nice plots and everything is great
61:20 - one second let's just uh satisfy my
61:23 - nervousness
61:24 - okay we're still good okay um
61:28 - i mean making these pictures is nice but
61:30 - why are we making these pictures well
61:32 - there are certain things that we are
61:34 - looking for when we're making
61:35 - visualizations like this
61:36 - for example is the data unimoda we're
61:39 - like
61:39 - one thing we're looking for is modality
61:41 - which is where does the
61:43 - data tend to cluster is the data
61:45 - unimodal
61:46 - where it only has one peak this would
61:48 - suggest it's clustering around
61:49 - one area or on the other hand is a
61:52 - bimodal or
61:53 - multimodal where you have multiple peaks
61:56 - so the unimodal case would be something
61:59 - that resembles this
62:01 - uh where so we'll call this unimodal
62:05 - bimodal would be a situation like this
62:08 - uh maybe think of it as a camel hump and
62:11 - multimodal
62:12 - uh is uh like all like you've got
62:16 - all sorts of different modes all sorts
62:18 - of different peaks
62:21 - so what would it mean if you had a unimo
62:24 - versus bimodal versus multimodal
62:27 - for starters the multimodal case the
62:30 - first thing
62:31 - that you should ask with multimodal is
62:34 - did i choose a bin that's too small
62:37 - because you can end up with situations
62:39 - if you choose your bin size
62:41 - to be too large when you're making a
62:44 - histogram
62:45 - uh if if at one extreme you can have a
62:48 - skyscraper where everything is in one
62:50 - bin
62:51 - and that is a chart that doesn't really
62:52 - tell you anything on the other hand you
62:54 - could have that
62:55 - basically a pancake where every single
62:57 - observation
62:58 - gets its own bin and that really doesn't
63:01 - tell you anything either that's
63:02 - basically a dot chart
63:03 - and you've kind of lost the point of the
63:05 - histogram
63:06 - so the first thing in the case of blue
63:09 - of this uh blue
63:10 - uh sort of histogram it's not really
63:13 - histogram because it's a smooth curve
63:15 - but
63:15 - whatever um in this in this case you
63:19 - should probably ask yourself whether
63:20 - you've
63:21 - chosen too few or too few bins
63:24 - uh it could happen that you have true
63:26 - genuinely multi-modal data
63:29 - but the number of modes should be
63:33 - should not be too many because modality
63:37 - and having more than one mode is
63:39 - indicative of there being more than one
63:42 - actual population in your data set so
63:44 - for example if
63:46 - i say that this is tracking height
63:49 - of people i could genuinely have
63:54 - a bimodal data set because actually
63:56 - there isn't
63:57 - one population in this data set of
63:59 - people there's actually two populations
64:02 - men and women because men and women will
64:03 - cluster around different
64:05 - average heights so that's that's that's
64:08 - that's features that you're looking for
64:10 - unimodal indicates that your data set it
64:12 - probably
64:12 - consists of one population um
64:16 - and another thing that we're looking for
64:19 - when looking at stuff like histograms
64:21 - and by the way
64:22 - a lot of this discussion also applies to
64:24 - the strip the
64:26 - stem leaf plot and the dot plot
64:29 - in particular the seven leaf plot
64:30 - because you can argue that the standard
64:32 - leaf plot is
64:33 - actually a histogram it's just a
64:35 - histogram with partic with a particular
64:37 - bin choice
64:38 - but there's nothing really different
64:40 - about it um
64:42 - but anyway is the data positively skewed
64:45 - or negatively skewed or symmetric
64:48 - so a positively skewed data set
64:51 - the way i like to think of it is well
64:53 - let's first draw it out
64:54 - we have the positively skewed data set
64:57 - and then we have
64:58 - the negatively skewed data set
65:02 - which looks more like this and then we
65:04 - have the symmetric data set
65:10 - let's do a little bit better than that
65:13 - okay that's a little bit more symmetric
65:16 - okay so if you're
65:17 - if you're at all bothered by the terms
65:19 - positively skewed negatively skewed
65:21 - symmetric and you're wondering
65:22 - how can i tell the difference between
65:24 - positively skewed and negatively skewed
65:26 - here's a little rule for you draw a
65:28 - dinosaur
65:33 - draw a dinosaur and then
65:36 - ask where is his tail pointing in the
65:39 - case of the black dinosaur
65:40 - it's pointing towards the positive end
65:42 - so it's positively skewed
65:44 - all right now let's talk about the green
65:46 - dinosaur well let's uh draw
65:48 - the green dinosaur give him some legs
65:51 - give him some stuff on his back because
65:53 - he's like a stegosaurus so he's got
65:55 - these plates on his back
65:56 - where is his tail pointing oh it's
65:58 - pointing in the negative direction
66:00 - so it's negatively skewed right and you
66:03 - don't draw
66:04 - you don't draw a dinosaur for symmetric
66:05 - because
66:07 - i know we're not we're just not going to
66:08 - do that we're already silly enough
66:10 - but um why does it matter whether a data
66:14 - set is positively skewed or negatively
66:15 - skewed
66:16 - um it matters when we're thinking about
66:18 - the relationship
66:19 - between uh important statistics such as
66:21 - the mean and the median
66:23 - so the mean and the he the median are
66:25 - going to behave a certain
66:26 - way and have certain relationships
66:28 - depending on whether the data is
66:29 - positively skewed or negatively skewed
66:31 - uh positively skewed data sets
66:33 - what that basically means is that
66:35 - outliers there are outliers in this data
66:37 - set
66:37 - and when observations tend to be large
66:39 - they tend to be very large
66:41 - um negatively skewed data sets are data
66:44 - sets where when an observation is small
66:46 - is small it tends to be very small and i
66:48 - can think of specific data sets
66:50 - that fall into these types of categories
66:52 - like for example
66:53 - income tends to be positively skewed
66:57 - so you have most people in a certain
67:00 - range of
67:01 - incomes and then you have a few people
67:03 - who make
67:04 - much much more than that right so
67:07 - most people are around here but when
67:09 - you're rich you tend to be very rich
67:11 - and on the other hand for negatively
67:14 - skewed data sets
67:15 - i do have something in mind i have test
67:18 - scores
67:19 - test scores for me like there's a lot of
67:22 - ways
67:24 - test scores could actually appear
67:25 - statistically but it seems like most
67:28 - students
67:29 - tend to fall within a certain range and
67:31 - the students who
67:32 - really didn't get it uh when they fail
67:35 - they fail hard so it tends to be
67:38 - negatively skewed
67:40 - and symmetric symmetric is kind of this
67:42 - ideal case
67:43 - where you're just as equally likely to
67:45 - be above or below i think
67:46 - heights could be possibly uh i haven't
67:49 - really looked at a height distribution a
67:50 - long time
67:51 - uh but i think that heights could
67:53 - probably be
67:55 - a symmetric uh where it's like you're
67:57 - just as likely to be
67:58 - uh really tall or really short um at
68:02 - some degree
68:04 - so you're also looking in these uh plots
68:06 - for outliers for example
68:07 - a histogram that has an outlier
68:11 - you might have a histogram that looks
68:13 - something like this and then you have a
68:15 - point that's way out here
68:17 - and you would say that this point right
68:19 - here is a candidate's being outlier
68:22 - and you're also interested in how spread
68:23 - out the data is
68:26 - and by spread uh when we're talking
68:28 - about spread we're talking about
68:31 - the less spread out black distribution
68:35 - as opposed to the more spread out
68:38 - green distribution which of these cases
68:41 - are you actually looking at
68:42 - or at the very least you're just
68:43 - interested in what the range of the data
68:45 - is
68:47 - okay so that's it for visualizing
68:51 - qualitative
68:52 - quantitative data now let's talk about
68:54 - qualitative or
68:55 - categorical data how do you visualize
68:58 - that we're only going to advocate
69:00 - one method here and that's a bar plot in
69:02 - fact in my visualization class
69:04 - i was told if you don't know what
69:06 - visualization method to use use a bar
69:07 - plot
69:08 - because bar plots are actually very good
69:10 - visualizations
69:11 - never ever for the love of god draw a
69:14 - pie chart
69:15 - i know i know especially in like public
69:18 - policy
69:19 - and economics people love their pie
69:21 - charts for the love of god do not create
69:23 - a pie chart
69:25 - do you have any idea how many memes of
69:27 - ugly and stupid pie charts there are
69:30 - please do not drop our chart okay um
69:32 - continuing on uh
69:34 - to construct a bar plot list
69:37 - each possible value of the variable and
69:39 - how frequently that value
69:40 - is taken uh a lot like what we were
69:43 - doing before
69:44 - it's just instead of having numbers on
69:45 - the left hand side
69:47 - for like these bins instead of that
69:49 - you're just going to have the categories
69:50 - that your dataset could be in
69:52 - uh and then you're going to draw a
69:54 - horizontal line
69:56 - so let's see a cartoon bar plot that
69:59 - we're making so we have like category a
70:01 - category b
70:02 - category c so draw a horizontal line it
70:06 - could be a vertical line it doesn't
70:07 - really matter
70:08 - and along the axis marks possible values
70:11 - of the variables
70:12 - and then draw a bar for each category
70:14 - extending to the categories
70:15 - observed frequency so we'd end up with
70:17 - something that looks like this um
70:21 - uh it is worth mentioning bar plots and
70:24 - histograms are two different things
70:27 - and the difference is this axis here
70:30 - this
70:30 - x-axis with a histogram
70:34 - that axis is a number line
70:37 - and there is a very specific order and
70:40 - if you were to plot a
70:41 - point on that line it would mean
70:44 - something
70:45 - whereas with a bar plot this doesn't
70:48 - mean anything
70:49 - you could rearrange it if you wanted to
70:52 - and the bar plot would say
70:53 - exactly the same thing even in the case
70:56 - of ordinal data you could space
70:58 - it out there's all sorts of
70:59 - transformations that you could do and
71:01 - the plot says the exact same thing
71:03 - that is not the case for histograms so
71:05 - bar plots and histograms
71:07 - despite looking somewhat similar are
71:09 - certainly not
71:10 - the same thing okay
71:14 - so uh for our next example
71:18 - uh there is a data set showing the
71:20 - frequency of the class of passengers
71:21 - aboard the titanic who survived
71:24 - her sinking uh i have the titanic data
71:27 - set in
71:27 - r uh contains this data set but they're
71:30 - actually tracking a lot of things
71:32 - they don't just track the class they
71:35 - track male and female they track age
71:37 - they track survival
71:39 - and so on here i have restricted
71:43 - to using the supply function which i'm
71:46 - not going to talk about right now
71:49 - i have restricted it to the case of
71:51 - survivors
71:52 - for individual for certain classes i
71:55 - want to create a bar plot for the
71:56 - frequency of each class's
71:58 - survival so um
72:01 - i've already got the frequency
72:02 - distribution r is already given it to be
72:04 - very nicely
72:05 - so i'm going to say we've got first
72:07 - class second class
72:09 - third crap class class and
72:12 - a crew okay and then we're going to
72:16 - extend up we're going to say
72:20 - up here let's say that this is a
72:25 - 220 and right here we've got
72:28 - 110. we're gonna just eyeball this
72:32 - uh so for first class there were about
72:35 - 203
72:37 - survivors so that's about here for crew
72:41 - there were 212 survivors so that's about
72:43 - here
72:44 - uh for second class there are about 118
72:47 - survivors that's about here
72:48 - for third class about 178 so our bar
72:51 - plot should look something like this
73:03 - okay and
73:06 - in fact we can look at what r does we
73:09 - give
73:10 - the bar plot function in r this bar uh
73:13 - this data set
73:14 - and it will in fact make a very nice bar
73:16 - plot for us
73:18 - okay there is actually a visualization
73:21 - method
73:21 - that i haven't really discussed here and
73:23 - you know i'm
73:24 - on i'm making a video everything's quite
73:28 - nice
73:28 - i really don't see why i shouldn't
73:32 - show you this if you're familiar with r
73:36 - so let's see first are we still
73:38 - streaming yes we are okay
73:39 - so um there is another type of
73:42 - visualization
73:43 - called a density plot you cannot make a
73:46 - density plot by hand
73:48 - so i'm going to go ahead and i'm going
73:49 - to make a density plot and does this
73:51 - command still work this this is a new
73:55 - installation oh good
73:56 - everything's working uh maybe we should
73:58 - go back
73:59 - to um
74:02 - well let's see what's a data set that we
74:05 - could work
74:06 - with um rivers is fine
74:10 - this is the length of some north
74:13 - american rivers
74:14 - and there is a plot called the density
74:16 - plot remember that i was drawing some
74:18 - smooth
74:18 - curves and histograms do not look like
74:21 - smooth curves
74:22 - what i kind of was drawing was a density
74:24 - plot so
74:25 - i could create such a plot by typing in
74:27 - plot
74:29 - density and then give it the name of the
74:32 - data set if it's stored in a vector so
74:34 - in this case
74:35 - rivers and this is the resulting density
74:38 - plot
74:39 - and it's actually plotting a smooth
74:40 - curve uh let's see
74:42 - let's what's uh one of the data sets
74:44 - that we were looking at before
74:46 - um page up page up
74:50 - uh what is it was this okay
74:56 - okay so we had this height data set so
74:59 - let's uh go ahead and combine those two
75:01 - things
75:03 - uh center on this okay so
75:06 - height uh will be
75:10 - a vector consisting of the numbers 5.55
75:14 - 5.30 uh
75:17 - 5.03
75:20 - 5.30
75:24 - 5.13
75:27 - 5.05 uh
75:30 - what was that number 5.36 no that's 5.38
75:35 - so we've got 5.38
75:41 - 5.96 and
75:44 - uh 5.21 and
75:47 - 5.38 okay so here's our data set
75:52 - and i'm just going to create a density
75:56 - plot
75:58 - for this data set
76:01 - um that's not it it's still rivers
76:04 - oh because i typed in reverse silly me
76:07 - um
76:08 - okay so what i want instead is height
76:14 - there that's better and it makes a
76:16 - smooth curve that
76:17 - kind of resembles actually it it yeah it
76:20 - certainly does resemble the histogram
76:22 - that we drew
76:22 - um except it's actually like in the
76:25 - histogram
76:26 - there is in fact when you look at that
76:27 - data set kind of a peak in this region
76:30 - um and that was completely masked by the
76:33 - histogram
76:34 - but the density plot was able to capture
76:36 - it interesting
76:38 - so but you have to make a plot like this
76:41 - an
76:42 - r you cannot make a plot like this by
76:44 - hand
76:45 - so just i'm just bringing it to your
76:47 - attention because these two these kind
76:49 - of plots do show up
76:52 - all right so uh that's it for section
76:55 - two of the book
76:56 - and uh i thank you for joining me and uh
76:59 - i will see
77:00 - and i hope that you watch the video for
77:02 - section three so have a nice day
77:12 - hey students all right so uh next
77:14 - section
77:15 - is on measures of location
77:18 - so up to this point we've talked about
77:20 - visual summaries and
77:22 - visual summaries are nice the thing is
77:24 - though we don't want to restrict
77:26 - ourselves
77:26 - just to visual summaries we would also
77:29 - like to be able to have numerical
77:31 - measures
77:32 - of data to understand
77:35 - distributions so we're going to start
77:39 - with measures of location measures of
77:42 - location tell us where
77:44 - a data set tends to be located along a
77:47 - number line
77:49 - so the first and most common measure you
77:51 - may have you probably have already seen
77:53 - a lot of these measures that we're going
77:54 - to talk about
77:55 - but the very first one we're going to
77:56 - talk about is the sample mean
77:59 - and for a data set consisting of
78:01 - observations x1
78:03 - to xn the sample mean is just is defined
78:07 - as x bar which
78:10 - equals 1 divided by
78:14 - n times the sum
78:18 - from i equals 1 to n
78:22 - x i which if you're not familiar with
78:25 - this notation what this means is we
78:26 - would take our data set
78:28 - add up everything in the data set and
78:30 - then divide
78:31 - the resulting sum by n
78:36 - now the there's the thing called the
78:38 - sample proportion and in fact
78:40 - relative frequencies are sample
78:42 - proportions they're counting
78:44 - the proportion of observations in a
78:46 - sample that take a certain value
78:49 - the sample proportion is also
78:52 - a measure of location it loosely is like
78:55 - the proportion of observations in the
78:57 - sample that have a certain
78:58 - characteristic
79:00 - so we divide the sample
79:03 - into successes and failures we like to
79:06 - use that vocabulary of success and
79:07 - failure
79:08 - a success and the sample proportion will
79:11 - count the number of successes
79:13 - so we'll have p hat
79:16 - equals and very loosely we're just going
79:20 - to say this is the
79:22 - number of whatever we consider to be a
79:26 - success
79:31 - and divide this by the sample size
79:34 - now this could also be written as
79:39 - x over n which could also
79:42 - where x is this uh number of
79:45 - successes and then we could say
79:48 - let's suppose that x i
79:53 - is equal to one
79:57 - if the ith observation counts
80:00 - as a success and zero otherwise
80:04 - what then does it mean to count the
80:06 - number of successes
80:07 - to count the number of success successes
80:10 - is to go through each
80:11 - observation and then add one to a
80:13 - running count
80:14 - if that observation counts as a success
80:16 - and otherwise do nothing which is the
80:18 - same as adding zero
80:20 - so we could then say that the number of
80:22 - successes
80:23 - is equal to the sum from i
80:26 - equals one
80:30 - to n of this new
80:33 - x i variable that is counting the number
80:36 - of successes effectively
80:37 - and saying whether an observation is a
80:40 - success
80:40 - so we should say that this sum
80:44 - is equal to the number of successes
80:47 - and then we take this sum and divide it
80:50 - by
80:50 - n
80:54 - which is also for what it's worth the
80:57 - same as taking the sum
80:59 - and multiplying it by 1 over n
81:04 - so notice what i just wrote down
81:07 - i just wrote down the sample mean again
81:11 - which means that the sample proportion
81:15 - is the same as the sample mean of a
81:17 - sample that consists of ones for
81:18 - successes and zeros
81:20 - otherwise so it
81:24 - recognizing this is actually very
81:26 - important because in probability theory
81:28 - the mean or more more directly
81:32 - sums of variables or sums of random
81:34 - variables
81:35 - are very important so recognizing
81:38 - something as a sample mean means that
81:41 - any
81:42 - theorems from probability theory that
81:44 - involve the sample mean
81:46 - can be applied to that variable
81:50 - so this is actually quite important to
81:52 - recognize
81:54 - but that means that after this point we
81:56 - really don't have to say much more about
81:58 - sample proportions because the sample
82:00 - mean
82:00 - whenever we're talking about the sample
82:02 - mean we're also talking about sample
82:03 - proportions
82:05 - so let's get started with an example
82:08 - what is the average number of points
82:10 - your daughter's soccer team scores here
82:13 - is
82:13 - as a reminder the data set this is
82:15 - actually proper r code right here just
82:18 - to write
82:18 - the just to write down the variable name
82:21 - soccer
82:22 - and what will happen is r will then
82:24 - print out
82:25 - that data set so uh but we're just going
82:28 - to take that for granted for now
82:30 - and uh compute first let's go ahead and
82:33 - compute the sum of these numbers
82:37 - so the sum from i equals 1 to n
82:40 - x i which is basically this right here
82:42 - just means take all the
82:44 - all the numbers in this data set and add
82:46 - them up
82:47 - so we've got 9 plus 6 is 15 plus 5 is 20
82:51 - plus 5 is 25 plus 5 is 30
82:54 - plus 6 is 36 plus 2 is 38
82:57 - plus 8 is 46 plus 3 is 49
83:01 - plus 4 is 53 plus 8 is 51.
83:06 - plus oh it looks like i'm all right so i
83:08 - actually have something written down i i
83:10 - have
83:11 - i i have 62. i think i might i might
83:13 - have uh
83:14 - missed something in that account but
83:15 - it's going to add up to 62. okay
83:18 - so just take my word for it uh so this
83:20 - adds up to 62
83:22 - uh n is equal to 12. so the sample size
83:25 - is equal to 12.
83:27 - and the sample mean then
83:30 - x bar will be 62
83:33 - divided by 12 which equals
83:36 - 31 over 6
83:40 - which is equal to 5.16 with a repeating
83:45 - six
83:47 - okay if we
83:50 - were to go to the next page
83:54 - in these notes we would see some r code
83:57 - that
83:57 - computes the sample mean for this data
83:59 - set the r
84:00 - function is mean so we ask
84:04 - so we say mean of the soccer data set
84:06 - and it will report to us the mean which
84:08 - is what we computed
84:10 - now let's suppose that uh
84:14 - r1 rn is the ordered version of this
84:16 - data set
84:17 - so x1 to xn uh is just
84:20 - so r1 to rn is x1 and xn but ordered
84:23 - remember from a previous video
84:25 - uh for section two on um
84:28 - that that for this notation uh x1 xn
84:32 - i do not necessarily imply
84:35 - any sort of ordering now i do for r1 to
84:39 - rn i'm going to
84:40 - imply an ordering uh
84:43 - the sample median is another measure
84:46 - for the location of the data set it is
84:50 - defined as the number that splits this
84:52 - data set
84:53 - in half so uh
84:56 - we can that is basically the definition
84:59 - um
85:00 - and from that we can come up with
85:01 - mathematical formula
85:03 - so we can say that x tilde that's the
85:06 - notation we will use for the sample
85:08 - median x tilde will be
85:10 - one of two possibilities first there is
85:13 - a case
85:14 - when there are an odd number of
85:16 - observations
85:19 - if there are an odd number of
85:21 - observations
85:25 - uh let's see let's zoom in so i can have
85:27 - a little bit more precise writing
85:30 - uh if there are odd number of
85:32 - observations
85:33 - after we order the data set the
85:35 - observation in the position
85:37 - n plus one divided by 2 will be the
85:40 - number
85:41 - that splits the data set in half
85:45 - so this will be what we're using if
85:48 - n is odd
85:51 - so as a so to think about this if we had
85:54 - 11 observations
85:56 - 11 plus 1 divided by 2 so that's 12
85:58 - divided by 2 that's equal to 6.
86:00 - the sixth observation after you order
86:02 - the data set
86:03 - will be the median okay uh
86:07 - now suppose that there are an even
86:09 - number of
86:10 - observations we could
86:13 - potentially choose our
86:16 - the r so the um
86:21 - observation in the n over tooth position
86:24 - so if uh the sample size were 12
86:27 - this would be the ordered observation
86:30 - the sixth ordered observation
86:33 - or we could potentially have the seventh
86:35 - ordered observation
86:37 - both of those are kind of dividing uh
86:40 - the center
86:41 - so what we'll do instead is we will take
86:44 - the midpoint between these two numbers
86:46 - which could potentially end up being the
86:47 - same number
86:48 - there's nothing that says that these two
86:50 - numbers are not the same but we're just
86:51 - going to average those two numbers take
86:53 - the midpoint in between them
86:55 - and admittedly if you had um
87:00 - a number line and you have some
87:04 - data over here and some data over here
87:07 - and you have these two
87:10 - uh observations as being potentially the
87:13 - median
87:14 - any number in between them could be
87:17 - defined as the median since i
87:19 - any of those numbers would divide the
87:22 - data set
87:22 - in half and in fact you may see
87:25 - alternative definitions
87:27 - of the median in r to take advantage of
87:29 - this fact um
87:32 - but for now it doesn't really matter if
87:35 - we had a lot of observations how
87:36 - exactly we define the median uh this is
87:40 - perfectly fine to just take the midpoint
87:41 - between
87:42 - uh the or the ordered observation
87:46 - in the position n over two and the order
87:48 - observation in the position n over two
87:50 - plus one
87:51 - so if we had uh twelve observations
87:54 - we would take the sixth and the seventh
87:56 - observations and average them to get the
87:58 - median
87:59 - okay let's see an example of computing
88:03 - the median find the median of the first
88:06 - 11 soccer games
88:08 - uh your daughter's team participated in
88:10 - i chose 11 just to have an even number
88:12 - to kind of
88:13 - no sorry odd number uh just
88:16 - for demonstration purposes um in this
88:19 - case
88:20 - uh i've our the the sort function
88:23 - in r uh this function
88:27 - will order your data set
88:30 - from smallest to largest uh given a
88:32 - vector it will order that vector
88:35 - from smallest to largest so
88:38 - we have sorted this data set we now have
88:40 - an order data set
88:42 - and i want to compute the median of that
88:45 - data set well first off
88:46 - there are n equals 11 observations
88:50 - in this data set that means that the
88:52 - observation in the middle will be
88:53 - m plus one over 2 which is 12 over 2
88:58 - which equals 6. therefore
89:01 - the median will be
89:04 - the sixth ordered observation
89:08 - which is let's see one two three four
89:12 - five six si five so it will be five
89:17 - that will be the medium
89:22 - if we were to look at some r code there
89:25 - is a function
89:25 - an r function called median and given
89:28 - that data set
89:29 - by the way i didn't mention this before
89:32 - this right here
89:33 - uh this square bracket stuff this is
89:36 - a subsetting notation uh it basically
89:40 - translates to
89:41 - get the observations uh get all
89:44 - observations between the first and the
89:46 - eleventh in this data set
89:48 - okay and remember that soccer itself is
89:51 - not ordered so we're just
89:52 - getting observations x1 through x11
89:56 - okay but i just feed that vector
90:00 - to the median function and it tells me
90:02 - that the median is 5.
90:04 - and by the way if you're wondering what
90:06 - this little one right here means
90:07 - uh that's just part of how r prints
90:10 - vectors
90:11 - if there were if this vector was quite
90:13 - long it would split over a number of
90:15 - lines
90:16 - and this would and this little one would
90:19 - just be tracking
90:20 - uh which observation you're looking at
90:23 - with each line so you wouldn't have
90:25 - so like if there was um uh 10
90:28 - down so if there was like more numbers
90:30 - after this
90:31 - and then we had in square brackets 10
90:33 - and saw the number six
90:34 - and numbers after that this would tell
90:36 - us that the sixth number and that
90:38 - that would tell us the tenth number and
90:40 - that back in that uh vector was six
90:42 - it's just something that's supposed to
90:44 - make reading
90:46 - uh what's in vectors um visually easier
90:50 - okay so uh i think i pressed something
90:55 - let's uh do that okay all right
90:58 - so continuing on and other measures of
91:01 - location are percentiles so we're going
91:04 - to say that the
91:05 - so this is the greek letter alpha and
91:07 - alpha is a number between 0 and 1 and
91:09 - we're going to include
91:10 - 0 and 1 as well the alpha times 100th
91:13 - percentile is the number such that
91:15 - roughly alpha times 100 percent
91:18 - of the data in the order data set lies
91:21 - to the left of that
91:22 - number uh so if we choose
91:26 - uh alpha equals 0.5 that would be uh
91:30 - the 50th percentile so roughly 50
91:33 - of the data set lies to the left of that
91:35 - number which means that 50
91:37 - lies to the right and what i just
91:40 - described
91:40 - is the median because the median is the
91:43 - observation that splits the data set in
91:44 - half
91:44 - which means that half of the data set is
91:46 - to the left or 50 and 50
91:48 - is to the right so um
91:51 - the median actually counts as a
91:54 - percentile percentiles are a
91:55 - generalization of the notion of a median
91:58 - in fact there are other percentiles that
92:00 - we care about such as quartiles
92:02 - quartiles divide the data set up into
92:04 - quarters
92:05 - so for the first quartile roughly 25
92:08 - percent
92:09 - of the data set rise to the left of that
92:11 - quartile and for the third quartile
92:13 - roughly 75 of the percent of the data
92:15 - set lies to the left of that quartile
92:17 - so um to visualize we would have
92:22 - um so we would have the first second and
92:25 - third
92:26 - quartiles and roughly 25 lies to the
92:29 - left
92:29 - of the first quartile and roughly 75
92:32 - percent
92:33 - of the of the data set lies to the left
92:36 - of the third quartile
92:37 - and the second quartile is the quartile
92:40 - where roughly 50 percent
92:42 - lies to the left of that four quartile
92:44 - in other words the median
92:45 - again um there's you can also say that
92:49 - there is a zeroth quartile
92:51 - which corresponds to alpha equals zero
92:53 - alpha equals zero
92:54 - means that there is nothing really to
92:56 - the left of this observation so
92:58 - that would correspond to the sample
93:00 - minimum whereas
93:01 - alpha equals one means that about 100
93:04 - percent of the data set lies to the left
93:05 - of that number
93:06 - that corresponds to the sample maximum
93:11 - there are actually a number of
93:13 - procedures
93:14 - for computing percentiles from data sets
93:19 - and i'm not going to repeat all of those
93:21 - procedures
93:23 - if anything i'm just going to list off
93:25 - the procedure that's easiest to do by
93:27 - hand because
93:28 - at the end of the day in the real world
93:30 - what you would do
93:31 - is ask r to get a percentile
93:35 - and r has its own algorithm for getting
93:38 - percentiles that's more complicated than
93:39 - what we're about to do
93:41 - and i really don't see the point in
93:44 - telling introductory students how to do
93:46 - that because you're it's it's more
93:48 - complicatedly intensive and
93:50 - like what's the point you get the if
93:51 - you're doing things by hand let's keep
93:53 - things simple
93:54 - um i should also probably mention um
93:57 - if you combine the zeroth first second
94:00 - third and fourth quartile fourth
94:01 - quartile is the maximum
94:03 - if you combine those together you end up
94:06 - with what's known
94:07 - as the five number summary of a data set
94:13 - so um here's a procedure for finding
94:16 - quartiles
94:19 - first find the median of the data set
94:23 - then split the data set into two data
94:26 - sets at the median
94:27 - and we're working with the order data
94:28 - set now so split it into do data sets
94:31 - at the medium if n is odd remove the
94:34 - data point that corresponds to the
94:36 - median
94:36 - and then to find the first quartile find
94:39 - the median of the lower data set
94:42 - and then to find the third quartile find
94:44 - the median of the
94:46 - upper data set so to visualize this
94:48 - procedure
94:49 - we have a data set find the median
94:52 - and then and this will split the data
94:55 - set into two
94:56 - then find the medians of the other two
94:58 - halves
95:00 - okay that will tell you what the first
95:03 - and third quartile
95:04 - are the minimum and the maximum are easy
95:06 - find the smallest and the largest
95:07 - numbers in the order data set
95:09 - okay so for our first example we're
95:12 - going to find the first and third
95:13 - quartiles
95:14 - of our of uh of this uh little girl's uh
95:18 - first 11 soccer games
95:21 - so uh let me get caught up in my notes
95:24 - for a second okay
95:28 - so to find these two quartiles
95:33 - how about we write down uh what that
95:36 - data set
95:36 - was just for our own purposes so we've
95:40 - got
95:40 - two three four uh
95:44 - five five five
95:47 - uh six six eight nine
95:51 - okay so the median was five and this
95:54 - is an odd number data set so we're going
95:56 - to
95:58 - delete that median
96:02 - six six oh oops there were two eights
96:06 - sorry about that so we're going to
96:08 - delete the median which is five
96:10 - and then we have split the data set in
96:12 - two
96:13 - so then we find the median of the first
96:15 - half there is an odd number of
96:17 - observations here
96:19 - so the median will be four thus the
96:21 - first quartile which i will call q1
96:24 - that will be four and the median
96:27 - of the upper data set that will be eight
96:30 - so that means that the third quartile
96:32 - will be eight and if we wanted to we can
96:35 - kind of fill out the five number summary
96:38 - saying the second quartile which is the
96:40 - median
96:42 - this that number is going to be five
96:47 - the zeroth quartile which is the minimum
96:53 - well that's going to be 2. 2 is the
96:56 - smallest number
96:57 - and the 4th quartile which is the
96:59 - maximum
97:03 - well the largest number in the data set
97:04 - is 9 so that will be 9.
97:08 - okay
97:11 - okay next example find the 10th and 90th
97:14 - percentiles of the height data i have
97:16 - listed the data
97:17 - for you below in order so the data set
97:21 - oh yeah this is actually what i was
97:22 - getting to um
97:24 - regarding those little numbers in square
97:26 - brackets this is the ninth observation
97:28 - right so that means that this number
97:30 - right here is the tenth observation in
97:32 - the data set
97:33 - so that means that this data set has
97:36 - n equals 10 observations
97:40 - uh so 10th percentile so roughly 10
97:44 - percent
97:44 - of the data set lies to the left of that
97:46 - number so
97:48 - uh 0.1 times 10
97:51 - is equal to one so that's about uh
97:54 - so about one number lies there so we're
97:56 - going to choose 5.05
98:00 - so uh 5.05
98:03 - will be the 10th percentile
98:06 - which we will call uh
98:09 - we'll call that q hat point uh
98:13 - 10 and uh for the 90th percentile
98:17 - so 0.9 times 10 that's going to be
98:21 - 9 so that's observation number 9. so 90
98:24 - of the data set lies including that
98:28 - number
98:28 - to the left of 5.63 so we'll say
98:32 - that 5.63 will be the 90th percentile
98:36 - which we'll call
98:37 - uh q q hat 90.
98:42 - okay and actually r
98:45 - has functions for computing
98:48 - uh quantiles another word for what we're
98:52 - talking about here are quantiles
98:54 - here i have asked r specifically to give
98:56 - me the 25th and 75th percentiles
98:59 - it is not using this procedure that i
99:01 - described it is using a different
99:03 - procedure
99:04 - for finding quantiles there's a number
99:06 - of different procedures for finding
99:07 - quantiles
99:08 - and percentiles and if your sample size
99:11 - is large nobody cares
99:12 - which one you use really um it really
99:15 - only matters at smaller sample sizes
99:17 - which procedure exactly you use
99:19 - and there are various different
99:20 - motivations um
99:22 - for different kinds of procedures
99:24 - there's various ways to solve the same
99:25 - problem
99:26 - and what i described is simple enough to
99:29 - do by hand
99:30 - so if you're going to do it by hand go
99:32 - ahead and use what i used this is
99:33 - probably
99:34 - uh whatever came up with this is
99:36 - probably much more complicated
99:37 - and if you're going to do if you need
99:39 - something more complicated then don't do
99:40 - it by hand that's hard to do it
99:42 - so you can read rs documentation to see
99:44 - what exactly is being done
99:46 - here it's using some sort of
99:47 - interpolation trick so
99:49 - here i've asked so the second parameter
99:52 - here this is a vector that contains the
99:54 - numbers 0.25 and 0.75 corresponding to
99:56 - the 25th and 75th percentiles
99:58 - or quantiles i'm not really sure what
100:00 - the difference between those two words
100:01 - is
100:02 - um here i ask for the 10th and 90th
100:05 - quantiles or percentiles
100:07 - and it gives me numbers these are all
100:09 - pretty close to what i came up with
100:10 - before
100:14 - okay uh next up uh let's kind of
100:17 - we we've come up with some measures of
100:20 - location
100:21 - and there's actually a number of
100:23 - different measures of location like for
100:25 - example
100:26 - i have seen a measure where you take the
100:29 - largest and the smallest observations
100:32 - so you could say that um
100:35 - so you could say that's r n m
100:38 - minus r one uh no plus r one
100:42 - so take the midpoint between the largest
100:45 - and the smallest observations that's
100:47 - actually another measure of location
100:48 - there's a number of different measures
100:50 - of location
100:51 - what i've shown here so far is fine but
100:54 - let's go ahead
100:55 - and start comparing these different
101:00 - methods for uh
101:04 - for measuring for describing the
101:06 - location
101:08 - of a data set so the sample mean x bar
101:12 - is known to be sensitive to outliers
101:15 - which
101:15 - means that outliers the data set have a
101:18 - profound effect
101:19 - on the sample mean so if you had
101:22 - for example a data set that consists of
101:24 - one one
101:25 - uh let's say one two three the average
101:28 - of that data set or the mean of that
101:30 - data set will be
101:31 - two and the median of that data set will
101:33 - also be two
101:34 - uh compare that to
101:38 - a data set that contains the numbers one
101:40 - to one thousand
101:42 - so uh the mean of that is going to be
101:46 - about 500 which is much larger than 2
101:50 - what it was before compare that to the
101:53 - median
101:54 - the median of that data set 1 to 1000
101:57 - is still 2 which means that the median
102:01 - is insensitive to outliers it basically
102:04 - doesn't care
102:04 - what they are all it cares about is the
102:07 - ordering of the observations
102:09 - so long as an outlet if you were to
102:10 - change the value of an outlier
102:12 - so long as it doesn't change the
102:14 - ordering of the data set
102:16 - the median will not change
102:19 - so as an example of this i'm going to
102:22 - compute
102:24 - the sample mean and the sample mean of
102:25 - the daughter's uh
102:28 - uh soccer games if i throw in a 12th
102:31 - soccer game and i'm going to consider a
102:34 - number of different scenarios
102:37 - where her 12th game was
102:40 - 1 point or one goal of four goals two
102:44 - goals and
102:44 - eighteen goals we're going to consider
102:47 - that
102:48 - and what i actually did here was um i
102:51 - created a vector that contains
102:55 - these uh that contains these so
102:59 - uh i'm not gonna talk too much more
103:01 - about this because
103:02 - i wanna focus on the math um
103:06 - so uh first off when i add
103:10 - up the 11 other games
103:17 - when i add those up i end up with a
103:19 - cumulative score
103:20 - of 61. so let's create a table
103:27 - for all of these potential outliers
103:30 - so in this table uh we're going to
103:32 - consider when the outlier
103:34 - when the 12th game is 1 when it's 4 when
103:38 - it's
103:38 - 2 and when it's 18
103:41 - and we're going to have a column for
103:45 - the sum of the observations we're going
103:48 - to have a column
103:49 - for the sample mean and a column for
103:52 - the sample median so in the case
103:56 - where the 12th game is one this will add
103:59 - up to
104:00 - 62. and by the way i'm using the word
104:02 - outlier for one but one in this case
104:03 - would not be an outlier
104:05 - same with 4 and 2 but 18 certainly would
104:07 - be considered an outlier
104:08 - so if her 12th game is 1 then it adds up
104:12 - to 62.
104:13 - if her 12th game is 4 it adds up to 65.
104:16 - if her 12th game is 2 the game's
104:20 - cumulative score is going to be not 62.
104:22 - uh 63
104:24 - and if it's 18 they all add up to 79.
104:29 - and then we're going to take these sums
104:31 - and divide them by 12.
104:32 - in the end we end up with uh
104:35 - in the first case we get a sample mean
104:37 - of 5.16
104:40 - uh in the second case we get a sample
104:42 - mean of 5.416
104:46 - uh repeating six uh in the third case we
104:50 - get
104:51 - 5.25 and in the fourth case
104:54 - we get 6.58 which is
104:57 - uh much different from what we had
105:00 - before
105:01 - uh
105:05 - hold on i think my pen might be okay my
105:07 - pen's back
105:08 - is it though might need to charge
105:11 - okay now the median in the first case
105:16 - uh so since uh
105:19 - so in the first case the median is still
105:22 - going to be five and in fact it's going
105:24 - to be
105:24 - five for the first three cases because
105:27 - the median
105:28 - was five and it didn't change the order
105:31 - of the data set if you go back and look
105:32 - at the original data set
105:33 - we're essentially in in these first
105:36 - three cases we shift all the numbers to
105:38 - uh the left which means that the
105:40 - median's gonna be
105:42 - uh the number to the left of what it was
105:44 - before which was
105:45 - five but if we chose or it's actually
105:49 - going to be the average of five and five
105:50 - which is still
105:51 - five but in the last case
105:54 - uh the median actually will be the
105:57 - average of five and six
105:59 - so it will be 5.5 so the median did
106:02 - change
106:02 - a little bit um in the last case but
106:05 - it's mostly because of
106:06 - where that outlier ended up it ended up
106:08 - on the right hand side of the data set
106:11 - or the right hand side of what the
106:12 - median was before
106:14 - and it could have been 273 and the
106:17 - median would be exactly the same
106:20 - but as we can see the median isn't
106:22 - changing
106:23 - really at all compare especially when
106:25 - you compare it to
106:27 - the mean and here is some r code
106:30 - that the idea of this code is i'm going
106:33 - through
106:33 - a loop adding this observation to
106:37 - a to a version of the data set and then
106:40 - uh computing the uh
106:44 - median and the mean of that data set the
106:46 - result will be an
106:47 - r matrix i take the transpose of that
106:50 - matrix because that's the version of the
106:52 - matrix that i prefer i'm giving the
106:54 - matrix some row names and column names
106:57 - do some rounding and this is a resulting
106:59 - matrix and it's pretty similar to what
107:01 - we had before
107:04 - in fact there is a relationship that we
107:06 - can say in general between the mean
107:09 - and the median depending on whether the
107:11 - data is negatively skewed
107:13 - positively skewed or symmetric if the
107:16 - data set
107:17 - is nic is a positively skewed
107:23 - so that would so the data set looks
107:25 - roughly like this
107:26 - then the me the median which is the
107:29 - point that devas
107:30 - divides the data set roughly in half
107:33 - will be less than the mean
107:39 - and that's because the mean is going to
107:40 - try to chase the outliers the outliers
107:42 - are going to be on the right hand side
107:44 - of the bulk of the data
107:46 - so in the case of
107:49 - negatively skewed data
107:53 - we're going to have the opposite
107:54 - relationship where the median
107:58 - since the outliers are going to be on
107:59 - the left hand side of the bulk of the
108:01 - data
108:02 - the mean is going to chase the smaller
108:03 - numbers so the
108:05 - me the median will tend to be greater
108:07 - than the mean
108:08 - and in the case of a symmetric data set
108:12 - the mean and the median should be
108:14 - approximately the same
108:16 - in real data i mean probabilistically
108:19 - when we talk about
108:20 - means and medians and probability uh
108:23 - they will be exactly the same when the
108:24 - date when the distribution of the data
108:26 - is symmetric but in real data that's
108:28 - never the case
108:29 - in real data they will just be close and
108:31 - what it means to be close
108:33 - is is like that's a judgment call
108:36 - right um so what would that some
108:38 - implications for that
108:40 - thinking back to some examples of
108:42 - positively skewed and negatively skewed
108:43 - data sets
108:44 - i said that a positively skewed data set
108:46 - is incomes
108:48 - this relationship means um
108:52 - this relationship means that the average
108:54 - income tends to be larger than the
108:56 - median income
108:57 - and economists generally prefer to use
108:59 - the median income
109:01 - for income distributions because it
109:03 - seems inappropriate
109:05 - to use the mean this kind of gets to the
109:08 - issue of
109:09 - i've given you these competing measures
109:13 - for means and for for me for measuring
109:16 - the location of a data set
109:18 - which one of these measures should you
109:20 - use
109:21 - i would say use the one that's
109:24 - appropriate
109:25 - which means um well here's one thing
109:27 - once you use the mean
109:29 - uh i would say you should use the mean
109:31 - when
109:33 - large observations are allowed to
109:35 - compensate for small ones
109:37 - let's say for example that you're
109:39 - gambling if you're gambling what you
109:41 - care about
109:42 - are your mean earnings and not your
109:44 - median earnings
109:45 - because it's okay for you to win nothing
109:50 - 99 times if you win a million
109:53 - or i guess it also depends on how how
109:56 - much this game is worth but let's say
109:57 - that
109:58 - you're playing a game that costs a
109:59 - dollar each time you play it's okay for
110:01 - you
110:02 - to win nothing 99 times if on the 100th
110:05 - time
110:06 - you win a million dollars that would be
110:08 - awesome for you
110:10 - whereas in that situation the median
110:12 - would be zero dollars
110:13 - and if you were judging by the median
110:15 - how well you were doing you would think
110:16 - you were actually doing poorly
110:19 - so if you're allowing large observations
110:22 - or small
110:23 - observations to to
110:27 - maybe uh replace or detract from
110:30 - the overall score then the mean is
110:33 - appropriate
110:34 - on the other hand the median in the case
110:37 - of
110:38 - uh social sciences we care more about
110:40 - what
110:41 - like what fifty percent of the obs of
110:44 - the population is experiencing
110:45 - and we don't necessarily want to allow
110:47 - uh like the
110:49 - very the the uh the um
110:52 - the great fortune of the wealthy to
110:56 - uh compensate for uh the great poverty
111:00 - of the very poor
111:01 - so in that situation the medium would be
111:03 - the preferred observation and of course
111:05 - if theoretically what you're trying to
111:07 - measure is the population median
111:09 - then you should use the sample median if
111:11 - you're trying to estimate the population
111:12 - mean
111:13 - you should use the sample mean and in
111:15 - which case you should not be using
111:17 - the opposite now there are some
111:19 - exceptions to this and we'll talk about
111:20 - this
111:21 - when we talk about probability and talk
111:23 - about um
111:24 - hypothesis testing there's notions such
111:26 - as most powerful tests like if
111:27 - your data set was symmetric and you knew
111:29 - it came from a normal distribution you
111:31 - should always use the mean even
111:32 - even if what you care about is the
111:33 - median uh but and the reason why that is
111:36 - is because for a normal distribution the
111:38 - two numbers are the same
111:39 - uh but um we're just going to leave that
111:42 - for now
111:43 - um
111:46 - so um and there so all of this was
111:50 - talking about uh sample results
111:52 - uh there are analogous population
111:54 - numbers too
111:55 - okay so um i just want to very quickly
112:00 - satisfy my nervousness okay we're still
112:02 - streaming okay
112:03 - um all right there is another number
112:08 - another measure for location called
112:12 - uh the trimmed mean so i'm going to comp
112:16 - i'm going to tell you about the trim
112:17 - bean and i'm even going to compute it
112:18 - for you but then i'm going to
112:20 - uh have some caveats about using it
112:23 - um you probably should not be using uh
112:26 - the trimmed mean
112:28 - and i'll explain that in a second but
112:30 - the idea of the trend mean
112:32 - is we have the stat we have the median
112:36 - and the median is not sensitive to
112:38 - outliers which is generally considered a
112:39 - blessing but it's not always a good
112:41 - thing because if
112:42 - it feels a little inappropriate to throw
112:44 - out so much data when you're computing
112:46 - the median
112:47 - because once you know the ordering of
112:50 - the data and you know
112:51 - which two which one or two numbers are
112:53 - in the middle then the other day
112:54 - the rest of the data doesn't matter and
112:56 - that feels a little extreme
112:58 - on the other hand you're a little
112:59 - bothered by the means
113:02 - sensitivity to outliers
113:05 - and furthermore on outliers you might
113:08 - think should we throw
113:09 - out outliers should we ignore outliers
113:12 - because that's actually kind of what the
113:14 - truth mean is suggesting that we should
113:15 - do
113:16 - with the trimmed mean what we're going
113:18 - to do is we're going to
113:21 - uh use only um uh
113:25 - we're going to throw out a certain
113:26 - percentage of the data like we're going
113:28 - to throw out 10 percent on the left-hand
113:30 - side and 10
113:31 - on the right-hand side of the data set
113:34 - so throw out
113:35 - the 10 10 percent of the smallest
113:37 - numbers
113:38 - and or the um
113:42 - so in this data set ten percent of the
113:44 - numbers that are the smallest numbers
113:45 - that the data set
113:46 - and ten percent of the numbers are the
113:47 - large larger numbers in the data set
113:49 - that would be the
113:50 - uh trend mean where alpha equals point
113:51 - one where you're trimming at ten 10 on
113:53 - each end
113:54 - um so the idea of the trend mean is
113:57 - throw out the outliers
113:59 - um so on this issue of whether you
114:03 - should throw out
114:04 - outliers you should actually think very
114:06 - carefully before you throw out outliers
114:08 - in general
114:10 - if you're competing a trimmed mean then
114:11 - that's kind of what you're doing
114:13 - but let's say you look at a data set and
114:15 - you see that
114:17 - your estimators are actually very
114:19 - influenced by
114:20 - a couple outliers and you're thinking
114:23 - maybe i should just throw those out i
114:25 - would first ask
114:26 - why are you throwing them out are you
114:28 - throwing it out
114:29 - because uh you think the number is
114:32 - erroneous because i
114:33 - in my own experience have seen numbers
114:36 - in data sets where it's like that's
114:38 - probably an error someone probably
114:40 - entered the wrong number so i'm going to
114:41 - throw it out
114:43 - um like when someone writes in
114:46 - the american community survey that
114:47 - someone's made a trillion dollars it's
114:49 - like that's probably not correct
114:51 - um you should just throw it out um if
114:53 - that's what's going on
114:54 - go ahead and throw it out because the
114:56 - reason why you're throwing it out is
114:57 - because of data contamination
115:00 - but if you're throwing it out just
115:02 - because
115:03 - it's causing bad behavior in your
115:07 - estimators
115:09 - that is probably inappropriate and you
115:12 - should
115:12 - instead try to model the outlier or just
115:16 - accept it
115:16 - rather than throw it out or think harder
115:20 - about why it is that you are using the
115:22 - mean rather than the median
115:25 - i would actually suggest that over
115:28 - throwing
115:29 - the outlier out but anyway um
115:32 - let's go ahead and compute uh the
115:35 - trimmed mean
115:36 - for the height data set so let's
115:40 - go ahead and rewrite that data set
115:43 - we've got uh i don't want blue
115:49 - okay so i've got for the height data set
115:53 - 5.05 you can go ahead and like
115:55 - skip ahead a little bit to skip me
115:57 - writing down numbers
115:59 - so 5.05 5.13
116:04 - 5.21 5.3
116:08 - uh 5.3 this
116:11 - splits the data set in half so the next
116:14 - number is
116:18 - 5.38
116:20 - and we've got 5.38
116:23 - again 5.55
116:30 - 5.63
116:32 - and 5.96
116:36 - okay so there are 10 numbers in this
116:40 - data set
116:41 - if we're going to trim so if we're going
116:43 - to say alpha
116:44 - equals 0.1 so we're going to trim 10 of
116:47 - the data off on each end
116:54 - so trim ten percent at each end
117:01 - that means that we're going to end up
117:02 - trimming
117:04 - uh 0.1 times
117:07 - 10 where 10 is the sample size
117:11 - which is 1. we're going to trim off the
117:14 - smallest and the largest number
117:16 - in this data set so that would be
117:21 - that would be 5.05 and 5.96
117:27 - and then take the average of the
117:28 - remaining data set so
117:30 - in that case x bar where we trim off
117:35 - uh 10 percent
117:39 - will be the average
117:43 - of the eight remaining numbers where
117:46 - we're sum
117:46 - from i equals two to nine uh
117:50 - the order data set
117:54 - um and that is going to end up being
117:58 - uh 42 so the sum is going to be 42.88
118:05 - uh and then we divide that by eight and
118:07 - the result
118:09 - will be uh 5.36
118:12 - so 5.36 feet because this data set is in
118:15 - feet we're we're talking about height
118:18 - okay and uh r can compute trimmed means
118:22 - in fact you can just use the mean
118:23 - function like we had before we're just
118:24 - going to pass it an additional parameter
118:27 - that tells the function to trim now i
118:30 - mentioned
118:31 - a little while back you actually
118:32 - probably should not
118:34 - be computing trimmed means and the
118:36 - reason why
118:37 - is because when we compute a median
118:40 - which i
118:40 - a a little side note i guess the median
118:43 - both the mean and the median count
118:45 - as particular trimmed means where the
118:48 - median
118:48 - is like the trend mean where you trim
118:50 - off 50 percent and
118:52 - the mean is where it is the trimmed beam
118:54 - where you turn off zero percent
118:56 - so the trend being kind of generalizes
118:58 - these other two statistics
119:00 - um but you probably should only use
119:02 - those other two statistics you should
119:03 - probably not use the trim mean
119:05 - i mean i guess if what you were doing is
119:07 - instead of taking off 10 on either end
119:09 - it's like
119:10 - always take off uh the two largest and
119:13 - two smallest numbers
119:14 - that could be appropriate um from
119:18 - a theoretical perspective
119:21 - uh so because basically as you increase
119:24 - the sample size the number of
119:26 - observations that you're trimming off
119:27 - becomes very small
119:29 - relative to the rest of the sample but
119:32 - trimming off ten percent
119:34 - at either end from a theoretical
119:36 - standing
119:37 - is a little odd and the reason why
119:40 - is that there is actually a population
119:42 - median
119:43 - that we are estimating when we are using
119:45 - a sample median
119:46 - and there is a population mean that we
119:49 - are estimating
119:50 - when we use a sample mean and both of
119:52 - those quantities are very well
119:54 - understood
119:55 - but when you're using a trend mean you
119:57 - are estimating neither of those things
119:59 - you're estimating some weird hybrid a
120:03 - monstrous monstrous population statistic
120:05 - that we don't necessarily understand
120:07 - you're
120:07 - estimating essentially the population
120:10 - version
120:10 - of a trimmed mean and it's questionable
120:13 - whether that's
120:14 - actually what you want it seems like the
120:17 - worst of both worlds in that case
120:19 - because no one can actually like why
120:20 - would we talk about
120:22 - the population except for the 10 largest
120:25 - and 10
120:26 - smallest numbers like that doesn't
120:28 - really make a whole lot of sense
120:30 - so for that reason
120:34 - unless you are actually i would actually
120:37 - more advocate for like a fixed trimming
120:39 - where you take off the two largest and
120:41 - two smallest observations
120:43 - um although at that point you probably
120:44 - should just use the mean
120:46 - or use the median i probably would not
120:48 - use the trimmed mean
120:51 - so okay so that's it for
120:54 - this section in the next section we will
120:56 - be discussing
120:57 - uh measures of variability so
121:00 - um i will cut it off here and have a
121:03 - good day
121:20 - okay so this section is about
121:24 - measures of variability so last
121:27 - section we discussed measures of
121:30 - location
121:31 - let's start by justifying why we need
121:34 - measures of variability
121:36 - consider these three data sets and i'm
121:38 - going to construct a dot plot
121:40 - for each of these data sets so i've got
121:44 - uh three lines for my three dot plots
121:46 - data set one data set two data set three
121:49 - uh and in the these dot plots i'm going
121:52 - to
121:53 - uh start uh with one and it's
121:56 - ending the twelve
121:59 - and uh in between i've got six
122:03 - so so let's see i'm going to have
122:07 - we're going to keep these all on the
122:10 - same scale
122:11 - so 1 12 6
122:15 - 1 12. uh
122:19 - six and uh we'll go one
122:23 - two three four
122:26 - five six seven eight nine
122:30 - ten 12 okay that's one two
122:33 - three four oops five six
122:36 - okay that's that's that's that's a
122:39 - little inexcusable we're gonna have to
122:40 - try a little harder on that one
122:41 - uh one two three
122:44 - four 5 6 7 8
122:48 - 9 10 11 12. okay
122:51 - 1 2 3 4 5
122:54 - 6 7 8 9
122:57 - 10 11 12. okay so i've now got these
123:01 - three number lines and let's start with
123:02 - data set one so we've got numbers
123:05 - at uh so four
123:09 - five six seven
123:12 - eight and then we've got one at
123:16 - uh two five six
123:20 - seven ten and then finally we have
123:24 - one three six
123:28 - 9 9
123:31 - and then 11. okay so
123:35 - look at these three dot plots now i want
123:38 - you to
123:40 - let's let's first start actually by uh
123:42 - computing the mean and the medium for
123:43 - each of these data sets
123:44 - so data set one four plus five plus six
123:46 - plus seven plus eight plus nine
123:48 - uh that is going to add up to 36
123:53 - the second data set well we subtract 2
123:55 - from 4 to get 2
123:56 - but then add 2 to 8 to get 10. so that
123:58 - second one is also going to add up to
124:00 - 36.
124:01 - and for the third one you kind of are
124:02 - going to do the same trick so they all
124:03 - add up to 36
124:05 - which then means that the sample mean is
124:07 - going to be
124:08 - 36 divided by there's six observations
124:11 - no actually there's
124:12 - five uh oh i'm sorry they don't add up
124:15 - to 36 they don't add up to 36.
124:19 - oh silly curtis silly curtis
124:22 - they added to 30
124:27 - so the sampling would be 30 divided by 5
124:30 - which equals 6
124:32 - which is also equal to the median
124:37 - because you look at them because these
124:39 - data sets are ordered they have five
124:40 - observations
124:41 - so the third row is going to correspond
124:43 - to the median so that means that the
124:45 - mean and the median for these data sets
124:47 - are all the same
124:48 - and yet let's suppose now that i were to
124:50 - ask tell you
124:51 - that this was the waiting time for the
124:52 - train uh which
124:54 - of these data sets would you prefer to
124:56 - be the observed waiting times for the
124:58 - train
124:59 - probably the first one at least if
125:02 - you're like me
125:03 - because for myself i actually did not
125:07 - really like inconsistent trains
125:10 - i mean it's kind of cool that this train
125:12 - will there might be
125:13 - a one-minute waiting time uh for this
125:16 - train
125:16 - but there also could be an 11 minute
125:18 - waiting time for this train
125:20 - and one way or the other i would just
125:22 - love it if trains always showed up
125:24 - exactly six minutes
125:26 - um between which okay admittedly around
125:29 - here they generally do do that
125:31 - but um you don't really like a lot of
125:34 - variability in the wait time for the
125:36 - train because that makes the train
125:37 - unreliable
125:39 - that said this aspect
125:42 - of the data set is not being captured by
125:46 - uh our measures of location the sample
125:48 - mean and the sample median
125:50 - unfortunately so and the reason why is
125:53 - because the attribute that we're talking
125:55 - about is an attribute that doesn't have
125:56 - anything to do with the location
125:58 - these data sets are located at
126:00 - essentially the same place
126:02 - they it has to do instead with spread
126:04 - and we now have a pictographic
126:06 - method for understanding spread we can
126:08 - see that these data sets have different
126:10 - spread
126:11 - but we would like to have some numerical
126:13 - measures
126:14 - so very quickly i'm just going to say
126:16 - that i would prefer one because it's
126:18 - more consistent
126:18 - or less spread
126:31 - okay what we need is a measure of
126:34 - variability to describe how spread out a
126:36 - data set
126:37 - is uh how could we possibly do that well
126:40 - we might
126:41 - start by examining deviations
126:45 - which where we look at x i and subtract
126:47 - out the sample mean
126:49 - and and if we were to add these up
126:51 - together this might give us a measure
126:53 - for um how spread out the data set is
126:57 - here's the thing though when we try that
127:00 - um
127:01 - we're going to sum up from
127:04 - i equals 1 to n
127:08 - uh no that should be a 1. so from i
127:12 - equals 1 to n
127:13 - x i minus x bar
127:18 - and this is a sum sums are linear
127:22 - which means that i can now break up this
127:24 - sum into two sums and say that this is
127:26 - going to be a sum
127:27 - from i equals one
127:30 - to n x i
127:34 - minus um the sum from i equals one
127:39 - to n x bar but
127:42 - here's the thing about that ladder sum
127:46 - see this is actually adding up a
127:50 - constant
127:51 - n times and you probably remember from
127:54 - second grade what it means to add up
127:56 - the same number and times you end up
127:58 - with multiplication
127:59 - so this number is going to actually end
128:01 - up being
128:03 - n times x bar
128:06 - okay and this number also can be
128:09 - interpreted as
128:10 - n times x bar because it is
128:13 - the sum of the observations divided by
128:16 - the sample size and then multiplied by
128:18 - the sample size again
128:20 - so we end up with
128:23 - n x bar minus
128:26 - n x bar and that equals zero
128:31 - so what that means is that this quantity
128:33 - is always
128:34 - equal to zero always equal to zero
128:42 - i think i found a dead spot on my screen
128:50 - okay so that always adds up to zero uh
128:53 - which means
128:54 - i mean the issue is that these
128:56 - deviations
128:58 - they always have the same sign
129:02 - well okay the they they all have um they
129:05 - all right what i just said was literally
129:07 - false um
129:08 - they don't always have the same sign in
129:11 - fact uh
129:12 - you have opposite signs you have some
129:14 - positive some negative deviations
129:16 - and it turns out that uh
129:19 - the positive and negative deviations
129:20 - cancel each other out
129:22 - so you end up with a zero um so that
129:25 - didn't quite work although there was an
129:26 - interesting idea there
129:28 - um looking at the distance between an
129:31 - observation and the sample mean
129:33 - and one might be tempted
129:37 - to try this replace the parentheses
129:41 - with absolute values so you end up
129:43 - adding up
129:44 - the absolute value of x i minus x bar
129:47 - and um
129:50 - that now you don't have that issue of
129:52 - negatives and positives canceling each
129:54 - other out because everything will be
129:56 - positive
129:56 - and you'll end up with a positive number
129:58 - and that makes sense
130:00 - um the thing though is
130:03 - this is actually more difficult for a
130:05 - mathematical perspective to work with
130:09 - and the reason why is because it's
130:10 - involving absolute values and
130:12 - absolute values are not differentiable
130:15 - absolute values if you remember from
130:17 - calculus 1
130:18 - they have a cusp of a sharp point
130:22 - and sharp points are not differentiable
130:25 - compare that instead
130:28 - to so this is like the absolute value of
130:30 - x compare that instead
130:32 - with the function x squared
130:35 - that should work i mean that that has
130:38 - the uh nice feature of being
130:40 - differentiable
130:41 - so what actually statisticians end up
130:43 - doing
130:44 - is they say we should add up the sum
130:48 - from i equals 1 to
130:51 - n x i
130:54 - minus x bar
130:57 - squared and this quantity is known
131:01 - as the sum of squared errors x i minus x
131:05 - bar
131:05 - a term that statisticians like to use
131:07 - for that is the error
131:09 - and um
131:12 - we're adding up the squared errors
131:15 - and there is in fact an interpretation
131:18 - for
131:19 - uh for the square part which is maybe
131:22 - you remember this is how i like to think
131:24 - of it
131:24 - i think that this formula kind of rhymes
131:27 - with x1 minus x2
131:31 - squared plus y1
131:35 - minus y2 do you remember that do you
131:38 - remember that from geometry class
131:41 - if you take the square root of this
131:42 - quantity you end up with the distance
131:45 - formula
131:46 - for euclidean distance for euclidean
131:48 - geometry
131:49 - and it actually kind of rhymes with
131:53 - that sum that i've drawn that i've uh
131:55 - shown
131:56 - uh up above so uh and in fact
132:00 - there is um a very deep
132:03 - connection between uh the sum of squared
132:07 - errors and euclidean geometry
132:09 - um but this quantity to me
132:12 - like it that seems like an appropriate
132:15 - way to think about distance
132:16 - and if we if we were to
132:20 - average this by by saying like this is
132:23 - one over n
132:24 - we would have an average squared
132:26 - distance
132:27 - now that's actually a good idea but um
132:30 - there's a better idea which is to divide
132:33 - instead of by
132:33 - n by n minus one now that
132:36 - might strike you as a little bit odd why
132:38 - is it that we're dividing by n minus one
132:41 - uh there's a few reasons for that some
132:43 - of which we'll talk about later in maybe
132:44 - chapter six
132:45 - but uh n minus 1 there's actually a term
132:48 - for this quantity and it's known in this
132:50 - in this context as the degrees of
132:52 - freedom
132:58 - and why are we dividing by the degrees
133:01 - of
133:02 - freedom rather than n
133:05 - i'm going to present to you a few
133:07 - arguments for why you'd want to do that
133:11 - for starters let's imagine that we had a
133:13 - data set of size
133:14 - 1. right so there's only one observation
133:17 - or data set
133:18 - what we're trying to measure right now
133:20 - is
133:21 - is a spread in the data set if we had a
133:24 - data set of
133:25 - size one is there really any way to
133:28 - estimate spread
133:29 - how can you determine the spread from a
133:31 - data set of one
133:33 - observation um that doesn't really make
133:36 - a whole lot of sense
133:38 - and it seems like something has
133:41 - fundamentally gone
133:42 - wrong in that situation
133:45 - and when you divide by n it's not going
133:48 - to reveal that something is wrong but
133:50 - when you divide by n minus 1 a sample
133:51 - size of 1 is explicitly forbidden
133:53 - because you cannot divide by 0. okay
133:56 - so that's one way to think that that's
133:59 - one way to think that maybe n
134:03 - minus one is more appropriate um
134:06 - and then uh uh secondly
134:09 - uh what we're actually doing with this
134:13 - number we actually call this
134:14 - we we've given this number a name as
134:16 - statisticians
134:18 - this is known as the variance the sample
134:24 - variance
134:28 - now maybe you recall from previous
134:31 - sections
134:32 - uh my saying that there is a sample
134:35 - meaning you can actually talk about a
134:36 - population mean
134:38 - and there's a sample median and you
134:39 - could talk about a population median
134:41 - and there is a sense in which the sample
134:43 - mean estimates the population mean
134:45 - and the media and the sample median
134:46 - estimates the population median
134:48 - so the sample variance should estimate
134:50 - the population variance
134:52 - there is in fact a population variance
134:55 - but here's the thing though about the
134:56 - population variance um
134:59 - our estimator if we were to divide by n
135:02 - would have a tendency to be too
135:04 - small i mean it would still be close to
135:07 - the population
135:08 - variance but you could be a little bit
135:10 - better
135:11 - by dividing by n minus 1 instead of n if
135:14 - you were to divide by n you'd actually
135:15 - be a little too small
135:17 - so we should divide by minus 1 instead
135:20 - there's a term
135:21 - called biasedness that we will discuss
135:24 - more in chapter six but
135:28 - long story short it turns out that when
135:30 - you divide by n minus one you have an
135:32 - unbiased estimator
135:33 - for the population variance whereas if
135:35 - you divide by n
135:36 - there is a very small bias now that said
135:40 - you can still divide by n and have a
135:42 - reasonable estimator
135:43 - it just will have that bias problem the
135:45 - bias gets really small as you increase
135:47 - the sample size but it is still there
135:50 - so why not just get rid of it um
135:54 - so these are some potential arguments
135:55 - for why you should be dividing
135:57 - by n minus one and later on in ch in
135:59 - that chapter we will actually
136:01 - uh i may actually show that if you
136:04 - compute the expected value and you
136:06 - divide by n minus one you get
136:08 - the sample variance but uh we're a long
136:10 - ways off from that
136:11 - so accept it that you pretty much have
136:14 - to divide by n minus one instead of and
136:15 - although it is still reasonable to think
136:18 - of this
136:20 - as um uh
136:23 - like an average squared distance oh yes
136:26 - another argument for why you should uh
136:28 - be dividing by n minus one
136:30 - uh when you have when you compute the
136:32 - variance
136:33 - there's something you have to do first
136:35 - you have to compute
136:37 - the sample mean you have to compute the
136:41 - sample mean first and there's a penalty
136:43 - that you have to pay for that
136:44 - the term degrees of freedom means
136:48 - that if you this is basically the number
136:51 - of
136:52 - observations in the data set that you
136:54 - are allowed to change
136:56 - um and where you can change those
137:00 - observations um freely
137:04 - and you could still end up with the same
137:06 - sample mean
137:07 - um it because it turns out for
137:10 - basically the reason that i showed up
137:12 - here
137:14 - uh this this this line of reasoning that
137:18 - um if you know n minus one of the
137:21 - observations and you know the sample
137:23 - mean
137:24 - then you know the nth observation that
137:26 - you didn't list out before
137:28 - so the sample mean contains information
137:33 - and the fact that you had to estimate a
137:34 - parameter
137:36 - before you could estimate the sample
137:38 - variance means that you need to divide
137:40 - that it means that there's in some sense
137:42 - a penalty
137:43 - to your sample size um so it's
137:46 - inappropriate to divide by n minus one
137:48 - you now need to do
137:49 - or to divide by n you now need to divide
137:52 - by n minus one
137:54 - all right now here's the thing about the
137:55 - sample variants that we don't like
137:57 - uh think about the units
138:00 - of these things let's say that we were
138:03 - talking about feet or going back to some
138:04 - examples or
138:05 - even for this uh soccer data set that
138:07 - i've seen in a few videos in the past
138:10 - where we're tracking the goals scored by
138:14 - a little league soccer team
138:17 - uh if we were talking about goals then
138:20 - this right here is a goal for a game so
138:22 - it's units or goals the sample mean
138:24 - is also in goals because you add up
138:26 - goals divided by something without units
138:28 - you end up with goals
138:30 - and you have goals minus goals so you
138:33 - still
138:33 - in that difference have goals but then
138:36 - you square
138:37 - and you end up with goals squared what
138:40 - the heck are skull
138:41 - are goals squared that's a unit that
138:44 - doesn't mean anything to us
138:47 - we don't like the fact that in the end
138:49 - the sample variance produces
138:51 - squared units we would rather have
138:54 - an s some measure of spread that is in
138:57 - the same units as the data set
138:59 - and there is such a measure called the
139:02 - sample standard deviation
139:04 - so the sample standard deviation is s
139:06 - which is equal to the
139:07 - square root of s squared so
139:10 - s squared is the sample variance s is
139:13 - the sampled standard deviation
139:17 - so we'll call that sd right
139:20 - i mean it's right there so uh the sample
139:24 - standard deviation
139:28 - so yes
139:32 - since you've taken the square root of
139:35 - the variance
139:37 - uh you now take the square root of gold
139:40 - squared
139:40 - and now end up the with the unit goals
139:44 - which is what you want so the sample
139:46 - standard deviation
139:48 - will be in the same units as the data
139:50 - set and we like that
139:51 - um furthermore it is still reasonable
139:55 - to think of the sample standard
139:57 - deviation as measuring
139:59 - the average distance of an observation
140:02 - from the mean or a typical distance
140:06 - all right so continue on uh there are in
140:09 - fact population analogs to these
140:11 - quantities
140:12 - uh such as the um
140:16 - such as uh the population variance and
140:19 - the population standard deviation
140:21 - and those will be discussions for a
140:24 - later chapter i believe that's chapter
140:26 - uh three so um now
140:31 - when you're computing the sample
140:32 - variance another way to write it
140:34 - if we write if we define s x x as the
140:38 - sum from y equals
140:39 - 1 to n of x i minus x bar squared we
140:41 - could say that the sample variance which
140:43 - is what you need to compute
140:44 - for the sample mean is equal to s x
140:47 - x divided by n minus 1. the thing though
140:50 - is
140:51 - a lot of people don't like to compute
140:53 - the deviations and then square them
140:56 - so compute the mean and then compute the
140:58 - deviations
140:59 - and by subtracting the mean from the
141:00 - observations and squaring them
141:02 - people don't seem to like to do that so
141:04 - when doing stuff by hand
141:05 - it's often easier to use this shortcut
141:08 - formula where you add up the
141:09 - observations
141:10 - squared and then subtract uh the mean
141:13 - squared
141:14 - multiplied with n and it is in fact
141:18 - possible to show
141:19 - and because this is the second time i'm
141:20 - recording this video i'm not going to
141:22 - show it because
141:23 - i'm tired um
141:26 - it is possible to show that these two
141:27 - quantities are the same and i would say
141:29 - i'm going to leave this as an exercise
141:30 - to you if you are curious if you're
141:33 - uh thinking you're probably going to
141:34 - take some more advanced stats classes
141:36 - why don't you take a second to show uh
141:39 - that
141:40 - these two quantities uh the
141:43 - uh some the sum of squared errors
141:46 - and the shortcut formula are the same
141:51 - um but i'm just going to leave it for
141:52 - now that these are in fact
141:54 - the same number so that gives
141:57 - that could help you potentially save
141:59 - some time when computing
142:01 - the sample variance by hand okay
142:05 - so uh let's start out let's now start
142:07 - looking at examples
142:08 - in example 14 we're going to compute the
142:10 - sample variance and sample standard
142:12 - deviation
142:13 - of the soccer game scores so here are
142:16 - the scores let's set again
142:17 - uh length is an r function
142:21 - uh length of so sock or in
142:24 - r is known as a vector and
142:27 - the length of the v of a vector will
142:30 - tell you how many objects are in that
142:32 - vector
142:33 - so um there's also an r function called
142:36 - summary
142:36 - which will give you uh some basic
142:39 - statistical summaries
142:41 - for a data set stored uh in a vector
142:44 - or actually summary is something we'll
142:47 - give you
142:48 - some basic statistical information about
142:50 - lots of things
142:51 - uh but that we're going to leave that
142:53 - for the r lab for now it's just giving
142:55 - us some basic
142:56 - statistics for an a vector and
143:01 - i'm now going to compute uh
143:04 - the sample variance
143:07 - of this uh data set
143:12 - okay so uh i like to
143:15 - create a table uh when computing this by
143:18 - hand if you don't want to watch me
143:20 - compute this by hand
143:21 - uh because it is kind of a tedious
143:24 - calculation
143:25 - if you don't want to watch it this is a
143:27 - part that you can skip over
143:29 - um all right anyway so um
143:32 - we have 12 observations in our data set
143:34 - so i'm going to start numbering off
143:37 - 1 2 3 4 5 6 just to track the
143:39 - observations
143:41 - 7 8 9 10
143:44 - 11 12. okay
143:48 - we have an observation and we have an
143:51 - observation
143:55 - uh hold on
143:58 - another dead spot
144:02 - we have an observation squared okay so
144:06 - uh observations in our data set
144:09 - we had uh nine six
144:13 - five five five
144:16 - uh six two
144:19 - eight uh
144:23 - three four eight one and then we're
144:28 - going to square each of these
144:29 - observations so we'll get 81
144:34 - 36 25
144:38 - three times
144:43 - uh 36
144:46 - for uh 64
144:50 - 9 16
144:54 - 64 and one
144:58 - okay um if you're out also
145:02 - like you kind of want to work on this by
145:04 - hand a little bit but you don't want to
145:06 - completely trivialize the problem by
145:07 - going to r
145:08 - and asking for the variance and standard
145:09 - deviation because it'll just give it to
145:11 - you
145:12 - uh maybe this would be something to work
145:14 - on in excel
145:16 - because i because what i'm basically
145:18 - doing is being
145:19 - a a human excel spreadsheet at the very
145:22 - bottom i'm going to sum up these two
145:24 - columns the first column sums up to 62
145:27 - and the second column consisting of
145:29 - squares sums up to 386.
145:34 - so now i want to compute the sum of
145:37 - squared
145:38 - errors and that's sxx and we have our
145:41 - shortcut formula for computing that
145:44 - that's going to be 386 which is the sum
145:47 - of the squares of the observations
145:49 - minus 12 which is the sample size times
145:53 - the mean
145:54 - all right we need to compute the mean so
145:56 - the mean
145:57 - is going to be the sum of the
145:58 - observations which is 62
146:00 - divided by 12. so in this
146:04 - parentheses i'm going to put 62 over 12
146:07 - and square it and you plug into
146:10 - calculator and the number that you get
146:12 - for the sum of squared errors is 197
146:15 - divided by 3
146:19 - which is as a decimal number uh 65.6
146:24 - where the six is repeating so the sample
146:27 - variance
146:28 - will be the sum of squared errors
146:31 - divided by
146:34 - as a reminder the sample size minus 1
146:36 - which is going to be
146:39 - 65.6
146:42 - divided by 12 minus 1 which is 11 which
146:45 - is equal to
146:46 - 5.99
146:50 - where the 9 6 itself is repeating now
146:54 - this is nice but the thing is the sample
146:56 - variance is in
146:57 - the units of the sample variance is gold
146:58 - squared we don't like that we want to
147:00 - compute the standard deviation too
147:02 - because that's a more interpretable
147:03 - number
147:04 - so the standard deviation is going to be
147:06 - the square root of the variance
147:08 - which is going to be about
147:11 - uh 2.443
147:15 - into three decimal places so you can
147:18 - think of this as
147:19 - your uh daughter's soccer team is
147:22 - varying
147:23 - around their average uh score of about
147:26 - five points
147:28 - but they're they're deviating from that
147:30 - by about two points so on average
147:32 - they'll be about two points away
147:35 - okay all right
147:38 - uh so in r the functions that are
147:41 - responsible
147:42 - for computing these quantities are var
147:44 - and sd r computes the variance and sd
147:46 - computes the standard deviation
147:48 - so var of the soccer data set is
147:50 - basically what i wrote down
147:52 - and the standard deviation it's
147:53 - basically the same thing too
147:55 - all right so um the sample mean so
147:58 - for the sample variance we actually have
148:00 - some nice properties that also translate
148:02 - into properties for uh the standard
148:05 - deviation actually before i continue on
148:07 - i'm going to double check because this
148:10 - scares me okay
148:13 - okay everything is good
148:16 - i'm scared okay um
148:20 - so uh some basic properties
148:24 - uh let's suppose for
148:27 - in this proposition uh let's suppose
148:30 - that
148:30 - we take our data set and then we shift
148:33 - everything by a constant
148:34 - to produce a new data set that's shifted
148:36 - by a constant
148:38 - it turns out that the set the uh
148:41 - sample variance for the new set data set
148:44 - will be the same as the sample variance
148:46 - for the old data set where you didn't
148:47 - shift
148:48 - uh that's a good thing what that means
148:51 - is basically this is in fact
148:52 - a measure of spread if it wasn't a
148:55 - measure of spread
148:57 - uh well basically if this was not the
148:59 - case
149:00 - if the sample variance changed
149:03 - uh by shifting the data then
149:07 - it doesn't seem really fair to
149:10 - uh call it a measure
149:13 - of of spread because it's also capturing
149:17 - location too
149:18 - but the fact that you don't have to uh
149:21 - the fact that it doesn't care about the
149:22 - location or in a way it doesn't actually
149:25 - care about the mean
149:26 - because and you can think of that as
149:27 - because it subtracts the means out the
149:29 - mean out for the data set
149:31 - uh the fact that it doesn't care means
149:32 - it is in fact a bona fide
149:34 - measure of spread and not measuring
149:36 - something else along with it
149:38 - the second proposition says that the
149:41 - variance
149:42 - uh if you were to rescale your data set
149:45 - by
149:46 - c uh the variance will scale by
149:50 - c squared and the standard deviation
149:53 - will scale by the absolute value of c
149:56 - so um the standard deviation is always
149:59 - going to be positive
150:01 - uh the variance will always be positive
150:03 - um
150:04 - so when you rescale the data set it's
150:06 - not going to
150:08 - the whether you multiply by a positive
150:11 - or a negative number doesn't actually
150:12 - matter
150:13 - and also it tells you that um if like
150:16 - you can think of this as saying
150:17 - something about
150:18 - uh unit conversions uh because remember
150:21 - that unit conversions
150:23 - uh generally and are multiplicative
150:25 - operations
150:27 - if you wanted to change the units of the
150:30 - standard deviation um
150:32 - you could do so by just
150:35 - multiplying the original standard
150:37 - deviation by whatever unit conversion
150:39 - formula you have
150:40 - and also this is basically telling us
150:41 - what i was saying before
150:43 - that the variance is in units squared
150:46 - but the standard deviation is in just
150:48 - the same units as the data set
150:52 - so these are good properties to be aware
150:54 - of
150:55 - um okay so
150:58 - the uh the sample variance and standard
151:01 - deviation
151:02 - uh these are one uh these are
151:05 - these are these are one class of uh
151:07 - estimator of spread they're not the only
151:09 - ones
151:10 - oh by the way you we did have this
151:12 - discussion
151:13 - when talking about measures of location
151:16 - about
151:17 - biasedness no no not biasedness um
151:21 - sensitivity to outliers
151:24 - it turns out that the sample mean
151:28 - and the sample standard the sample
151:29 - variance and sample standard deviation
151:31 - are also sensitive to outliers in fact
151:33 - they are more sensitive to outliers than
151:35 - the mean is
151:36 - so they care a great deal about outliers
151:38 - too um
151:40 - just throwing that out there and you can
151:42 - kind of tell by
151:43 - looking at those formulas since
151:46 - uh when you look at them they're
151:49 - basically
151:49 - means right they're averages or at least
151:52 - the variance
151:53 - looks like an average and the standard
151:55 - deviation is the square root of an
151:57 - average
151:58 - so uh if you end up having a very large
152:01 - error then that's going to make your
152:04 - variance very large
152:06 - so it's sensitive to outliers just
152:08 - mentioning that uh
152:10 - another measure for spread is known as
152:14 - the fourth spread or sometimes like in
152:16 - math 1070 we call the interquartile
152:18 - range
152:18 - it is the third quartile minus the first
152:21 - quartile
152:22 - and we're going to denote it in this
152:24 - class with fs
152:25 - this is another measure of dispersion so
152:27 - let's compute the fourth spread for the
152:29 - soccer game scores
152:30 - uh we already computed in uh
152:33 - in a previous uh video uh
152:37 - the third and first quartile for the
152:39 - soccer game
152:40 - for the soccer games so the fourth
152:41 - spread
152:45 - will be the third quartile minus the
152:48 - first quartile
152:49 - which actually turns out for this data
152:51 - set to be eight minus four
152:54 - which is equal to four
152:58 - okay so the fourth spread is
153:01 - in and of itself a measure of dispersion
153:04 - uh and one way statisticians might use
153:08 - the fourth spread is as a tool for
153:10 - outlier detection remember we care a
153:11 - great deal about outliers
153:13 - uh if there's an out we would like to
153:15 - have outlier detection tools
153:17 - because if there is an outlier in this
153:18 - data set we would like to investigate it
153:20 - further
153:21 - and decide how we should approach it and
153:23 - why the outlier is there
153:25 - outliers are very interesting aspects of
153:27 - data sets so we would like to be able to
153:29 - detect them
153:30 - so we might call an observation
153:33 - that is further than one and a half
153:35 - times the fourth spread from its nearest
153:37 - quartile a mild outlier
153:39 - so as an example uh let's suppose we
153:42 - have a data set our data set looks
153:43 - something like this here's kind of a
153:45 - a dot plot sketch of our data set and
153:48 - we've also got
153:49 - a couple observations over here so those
153:52 - two observations visually
153:54 - look like outliers what the for
153:57 - let's suppose that we have the
154:01 - uh oh
154:04 - i didn't realize that was the thing okay
154:06 - uh let's suppose
154:09 - that uh our first and third quartiles
154:12 - are here and here
154:14 - okay um if that is the case
154:18 - uh the fourth spread or the iqr
154:22 - is going to be the distance between
154:25 - uh those two quartiles so we'll call
154:27 - this q1
154:28 - q3 uh the distance between those will be
154:31 - the fourth spread
154:32 - so according to this rule how you detect
154:35 - an outlier is you take this quantity
154:37 - and then increase it by uh one so one
154:40 - and a half
154:42 - and go beyond and you're gonna
154:45 - like see compare an observation to its
154:48 - nearest quartile so these are going to
154:51 - be close to the third quartile because
154:52 - they're above the third quartile
154:54 - um and you um
154:58 - compare see if those observations
155:01 - are one and a half times the iqr away
155:05 - from their nearest quartile
155:07 - and if they are beyond that range then
155:09 - they are candidates to be
155:10 - outliers so these are now starting to
155:13 - look like at least
155:14 - mild outliers but in fact if we were to
155:18 - uh double that quantity so three times
155:22 - the fourth spread
155:23 - um so that would look like this
155:27 - turns out that they are beyond three
155:29 - times the fourth spread as well
155:31 - so now these are looking like extreme
155:33 - outliers
155:35 - we could also do the same thing on the
155:37 - left hand side of the data set
155:38 - look for outliers that are you know
155:41 - numbers that are really small there's
155:42 - nothing that says that outliers have to
155:44 - always be large numbers they can also be
155:46 - really small numbers too uh and there's
155:49 - no
155:49 - outliers on the left-hand side of this
155:51 - data set
155:53 - i should point out it's tempting for
155:55 - students to think that what i just gave
155:56 - you as a definition for outliers
155:59 - the word outlier is intentionally vague
156:01 - because there's many different ways you
156:03 - could define an
156:04 - outlier this is one such definition
156:08 - or one such a criterion for deciding if
156:11 - something is an outlier
156:12 - this criterion would not work if we were
156:16 - to go
156:16 - into two dimensions it is a
156:18 - one-dimensional
156:20 - one-dimensional approach not a
156:22 - two-dimensional approach
156:23 - and you can still have uh outliers
156:27 - in two dimensions or in bivariate data
156:30 - and
156:30 - uh um you can still have outliers there
156:34 - and they're going to behave
156:36 - like there's more possibilities the
156:37 - moment you've moved on to a plane
156:39 - as opposed to just a number line more
156:41 - ways for things to be outliers
156:44 - um so um
156:48 - i i'm i'm i'm hesitant to allow to just
156:52 - let students think that this is what an
156:54 - outlier is
156:55 - there's actually different ways to think
156:57 - about outliers we could come up with
156:58 - different definitions
157:00 - based off of our problem and how we want
157:02 - to approach it
157:03 - so so we could we could choose a
157:07 - procedure
157:08 - that is tailored to our problem to
157:10 - define outliers so that it's most useful
157:12 - to us
157:13 - so this is what we're using in this
157:14 - class but it is far from
157:16 - like what we'd always use okay and it's
157:20 - not really the definition of an outlier
157:21 - we would just say
157:22 - that an outlier is a point that that
157:25 - seems unusual to the other points that
157:28 - seems to be distant in some way
157:30 - from the other points or it doesn't seem
157:32 - to follow the same pattern as the rest
157:34 - of the data
157:35 - okay so moving on into example 16 use
157:38 - the fourth spread
157:39 - to detect outliers and soccer game
157:41 - scores what is the minimum score needed
157:43 - for a data point to be a mile outlier or
157:45 - an extreme outlier
157:47 - so the force spread as you may recall
157:49 - from above
157:50 - was four so 1.5
157:53 - and i don't want that green that color
157:56 - all right so 1.5
157:58 - times the fourth spread is going to be 4
158:01 - times 1.5 which is going to be 6
158:04 - and 3 times the fourth spread is going
158:07 - to be
158:08 - 12. so uh
158:11 - let's see what it would take for
158:12 - something to be considered at least a
158:14 - mild outlier
158:17 - to be mild you would have to possibly
158:20 - exceed the third quartile plus
158:24 - 1.5 times the fourth spread
158:27 - which is going to be eight plus six
158:31 - which is 14. there were no
158:34 - double digit scores in our soccer data
158:37 - set
158:38 - so that means that there were no
158:39 - outliers at least on the positive end
158:41 - and as for the negative end uh in order
158:44 - for something to be so small that it's
158:45 - an hour it would have to be less than q1
158:48 - minus uh 1.5 times the force spread
158:54 - which is going to be uh that's going to
158:57 - be 4 minus 6
158:59 - which is negative 2. well
159:02 - negative soccer scores are impossible so
159:04 - there's not going to be any mild
159:06 - outliers on the left hand side which
159:07 - means that there are no outliers in this
159:08 - data set
159:10 - now that's it let's let's go ahead and
159:11 - continue on just just for fun
159:13 - let's see what it would take for
159:14 - something to be an extreme outlier
159:20 - to be an extreme outlier you would have
159:22 - to exceed
159:23 - q3 plus 3fs
159:26 - which is 8 plus 12
159:31 - which is uh 20. so in other words the
159:34 - other team didn't show up
159:36 - to be on the left hand side you'd uh to
159:39 - be an outlier on the left-hand
159:40 - side you have to be less than q1 minus
159:42 - three fs
159:44 - which is um uh four minus 12
159:50 - uh which is negative which is negative
159:53 - eight
159:56 - and no way that's going to happen so
159:57 - there are no outliers
160:00 - in our data set
160:07 - okay so uh
160:13 - there's another visualization method
160:15 - that i would like to discuss
160:16 - that we weren't able to discuss in
160:18 - section two
160:19 - the reason why is because this is a box
160:21 - plot and box plots require
160:24 - uh the five number summary which is the
160:27 - minimum max so the five number summary
160:30 - is the minimum
160:31 - uh maximum median first and third
160:34 - quartiles
160:34 - so you need to have that in order to be
160:36 - able to compute a box plot
160:38 - and create a box plot that's why we
160:39 - didn't talk about before because we
160:41 - hadn't actually talked about those
160:42 - things
160:43 - uh so um so for
160:46 - a box plot we first compute those
160:49 - quantities
160:50 - uh on a number line which could
160:53 - this could be a vertical number line or
160:55 - it could be a horizontal number line if
160:57 - you want
160:58 - uh box plots oriented horizontally or
161:01 - oriented vertically
161:03 - either one is fine whatever whatever
161:06 - suits your needs do whatever you want
161:09 - okay but on a number line
161:13 - let's say something like this we're
161:14 - going to draw a box so we've got
161:17 - the first quartile and the third
161:20 - quartile
161:21 - we're going to draw a box whose ends are
161:25 - at those quartiles we're also going to
161:28 - draw a line
161:29 - in that box corresponding to the
161:31 - location of the median
161:34 - we will then draw what are known as
161:36 - whiskers that extend out
161:38 - to the maximum and to the minimum
161:44 - so the whiskers will stand out to extend
161:46 - out
161:47 - to the extrema of the data set
161:51 - um and that's that's a box plot
161:54 - now i should point out that r does
161:58 - not draw a box plus this way by default
162:01 - r does something different r will
162:03 - actually try to detect outliers and it
162:05 - will
162:06 - draw the whiskers out to the
162:07 - observations that are not
162:09 - outliers so the largest observation that
162:12 - is on outlier and the smallest
162:13 - observation that is not an
162:15 - outlier the outliers are treated
162:17 - differently
162:18 - they get their own points in the box
162:21 - plot
162:21 - kind of like with the dot plot
162:25 - uh that's more complicated to do and i'm
162:27 - not going to ask you to do that
162:28 - uh using just the five number summary
162:30 - and extending out to
162:32 - the maxima is fine for me and i feel
162:34 - like that if you were ever
162:35 - to draw a box plot by hand you should
162:37 - just keep things simple because you can
162:39 - it's not too hard to compute
162:41 - a five number summary if you especially
162:43 - if you had say
162:44 - um uh if you had like a stem-and-leaf
162:47 - plot
162:48 - but a box plot like competing
162:51 - the outliers is a bit much so
162:55 - a box plot on its own i mean it's okay
162:58 - but a box plot really shines
163:00 - when there are other box plots with it
163:02 - and when you have that you can now start
163:04 - drawing comparative box plots
163:06 - and when you have comparative box plots
163:08 - you can start to say things about the
163:10 - relationships
163:11 - amongst different groups that uh
163:15 - like if you want to compare different
163:16 - data sets you may have a data you may
163:18 - have two data sets for
163:19 - two similar but not the same populations
163:23 - uh like for example men and women men
163:25 - and women are both human
163:26 - uh but if you were to compare height you
163:29 - would probably want to differentiate
163:30 - between men and women
163:31 - so you could have a box plot for men's
163:34 - heights
163:37 - and you can have a box plot for women's
163:39 - heights
163:41 - and then you can make comparisons and
163:43 - you could compare in this case
163:45 - this looks like uh if if this were in
163:47 - fact talking about men's and women's
163:49 - heights
163:49 - this would be suggesting that men tend
163:51 - to be taller than women
163:53 - um so one thing that you could do when
163:56 - looking at a comparative box plot is
163:58 - compare look the location of the boxes
164:00 - you can also compare the spreads of the
164:02 - boxes so we for example
164:04 - if we saw one box plot that looks like
164:06 - this for one group and a box plot that
164:08 - looks like this for another group
164:10 - we might say that those two groups
164:12 - certainly have different spread
164:15 - okay so you can start making comparisons
164:17 - that are more easily made with with
164:20 - plots such as these and if you were to
164:21 - say
164:22 - try to overlap density plots or
164:24 - something
164:25 - um so comparative box plots are very
164:27 - nice because they allow you to
164:29 - at a glance compare two different groups
164:32 - and their distributions so let's go
164:35 - ahead
164:36 - and start creating some box plots
164:39 - uh in this example uh we're using a data
164:42 - set from
164:43 - r uh in this example we're studying
164:46 - we're studying the tooth growth of
164:48 - guinea pigs that were given a vitamin c
164:49 - supplement
164:50 - via orange juice at three different
164:52 - dosage levels
164:53 - uh here's a bunch of r code that takes
164:56 - the tooth growth data set
164:58 - and transforms it into a format that is
165:02 - um uh nice for this problem where
165:05 - uh it did not look like this before it
165:08 - go ahead and look at the tooth growth
165:09 - data
165:09 - it does not look like this at all for
165:11 - starters the
165:13 - tooth growth data set it also includes a
165:16 - group where the guinea pigs were given
165:18 - vitamin c
165:19 - like a vitamin c supplement directly
165:21 - rather than through via orange juice
165:24 - and we've completely excluded that group
165:26 - using this filter command
165:28 - these are known as pipes they are
165:31 - part of the dipler package um
165:35 - so we um filtered so that we were
165:39 - looking at only orange juice
165:40 - we selected the length and the dosage uh
165:44 - as the variables we were interested in
165:46 - and then did some other stuff
165:47 - so that the data came in a format that i
165:51 - liked which is where
165:54 - it's ordered from smallest to largest
165:57 - for each of these three groups and we
165:58 - have the half dosage group
166:00 - full dosage group and double dosage
166:02 - group
166:04 - okay so uh
166:07 - all right so and the data set is ordered
166:09 - which means it's going to be
166:10 - uh somewhat easy to compute a five
166:13 - number summary for each of these three
166:15 - groups
166:15 - so for example so for instance uh the
166:18 - last row is going to be the maximum for
166:20 - the three groups
166:20 - and the first row is going to be the
166:22 - minimum for the three groups
166:24 - as for the other quantities we also need
166:28 - the median
166:29 - so there are 10 observations for each
166:31 - group
166:33 - so the median is going to be the average
166:37 - of the fifth and the sixth rows so
166:40 - uh the medians after we compute those
166:43 - averages
166:46 - or or midpoints if you prefer the
166:48 - medians for
166:50 - the half dosage group uh its median
166:53 - is uh 12.25
166:56 - for the full dosage group it's going to
166:58 - be 23.45
167:01 - and for the double dosage group it's
167:04 - going to be
167:06 - 25.95
167:09 - okay so those are the medians
167:12 - now we need to compute the first
167:14 - quartile remember what we do is we split
167:17 - the data set in half and then look
167:18 - at the median for the smaller data set
167:21 - or the lower data set
167:23 - this will be the first quartiles and the
167:26 - median for
167:26 - the third quarter data set remember that
167:28 - both these data sets have five
167:30 - observations each
167:31 - after we do the splitting so the median
167:33 - for the upper data set that will be the
167:35 - third quartile
167:37 - okay so we now have everything we need
167:39 - to start constructing our box plots
167:43 - okay so i'm going to construct these
167:47 - this by hand i don't want that
167:51 - let's make it black isn't that a song
167:55 - okay oh yeah i want a painted black yeah
167:59 - that's right
168:00 - that is a song all right um i want to
168:03 - paint it black
168:04 - uh anyway uh so um
168:07 - i have the half dosage group the full
168:11 - dosage group
168:12 - and a the double dosage group
168:16 - i'm going to have my box plot end at
168:19 - 31 up here and we're gonna start down
168:22 - here
168:23 - at eight so we're gonna go eight uh
168:26 - nine ten eleven twelve 13.
168:31 - okay uh 14 15
168:34 - 16 17 18.
168:39 - and then we go 1920.
168:44 - uh 21 22 23 24
168:48 - 25
168:53 - uh 26 27 28
168:56 - 29 30 31 okay
169:00 - all right so we've got our scale uh so
169:03 - for our first
169:04 - group uh let's zoom out a little bit
169:11 - so for the first group the median
169:14 - of the minimum was 8.2 so
169:18 - minimum of 8.2 we'll put a dot right
169:20 - there
169:22 - uh then we go
169:26 - to the first quartile so that's going to
169:29 - be
169:31 - 9.7 that's about there
169:35 - um then we've got 12.25
169:40 - that's about there uh
169:43 - q3 is 16.5 that's about
169:47 - 13. that's about there
169:52 - and finally the maximum is 21.5
169:56 - so that is going to be about
170:00 - there okay
170:04 - so then uh we have a box
170:08 - the median and the whiskers
170:13 - all right so there's our first box plot
170:17 - all right so now for full dosage uh
170:20 - scrolling up a little bit
170:23 - so for full dosage the minimum is at
170:27 - uh 14.5 which is about there
170:32 - uh then we have a quartile at 20
170:35 - which is about there
170:38 - uh the median's at 23.445
170:42 - so that's about there uh
170:45 - q3 is at 25.8 so that's about
170:50 - there um and then
170:54 - uh the maximum is at 27.3
170:58 - so that's about there okay so draw the
171:02 - box
171:04 - the line for the median and draw out the
171:07 - whiskers
171:09 - okay and finally for the double dosage
171:12 - group the minimum is at 22.5 which is
171:15 - about there
171:16 - uh the first quartile is at 24.5 which
171:19 - is about there
171:20 - uh the median is at 25.95 which is about
171:24 - there
171:25 - uh the third quartile is at 27.3 which
171:28 - is about
171:29 - uh there and the maximum is at
171:33 - uh 30.9 which is about there so
171:36 - draw the box draw the line for the
171:39 - median
171:40 - extend out the whiskers and there we go
171:43 - all right
171:43 - and now we have a box plot a comparative
171:46 - box plot
171:47 - and what can we see we can see that in
171:50 - fact increasing the dosage does seem to
171:52 - increase the tooth growth length
171:54 - we also see that there's much more
171:56 - spread in the half and full dose than
171:57 - there is for the double dose which is an
172:00 - interesting fact
172:01 - uh all right so moving on uh there is um
172:04 - an r function called box plot
172:06 - that can construct these box plots for
172:08 - you this is largely in agreement with
172:10 - what we drew
172:11 - um and as a reminder
172:14 - r doesn't by default uh produce box
172:17 - plots in the way that i just described
172:19 - where it draws whiskers out
172:20 - to the minimum and the maximum it does
172:23 - something a little bit different
172:24 - where it will um uh draw it out to
172:28 - the largest and smallest observations
172:29 - that are not outliers and then draw the
172:31 - outliers as their own individual points
172:34 - okay uh there's actually one more
172:36 - visualization that's similar to a box
172:38 - plot that i would like to discuss
172:40 - um i didn't discuss it in the lecture
172:43 - notes because
172:44 - you can't really draw it by hand
172:48 - uh but i've got a computer in front of
172:50 - me okay very quickly
172:52 - okay everything seems to be fine okay um
172:56 - so you can't really draw it by hand uh
172:58 - because
173:00 - oh what was that
176:23 - hey students
176:26 - let's get started with the chapter on
176:29 - probability
176:30 - probability is the mathematical study of
176:32 - randomness and uncertain outcomes
176:35 - so the subject in fact may be about as
176:37 - old as calculus at some level
176:39 - humans have known about probability for
176:41 - a very long time
176:42 - it's just it wasn't until around the
176:45 - time of probability that we saw some of
176:46 - the first
176:47 - uh semi-rigorous treatments of
176:50 - probability
176:51 - and then probability really became a
176:54 - serious mathematical subject
176:56 - around uh the beginning of the 20th 20th
176:59 - century
177:00 - uh when a mathematician by the name of
177:03 - kolmogorov
177:04 - rooted probability theory in in the
177:08 - in some real analysis theory so he
177:10 - developed a set of axioms that made it a
177:12 - rigorous
177:13 - uh mathematical subject in its own right
177:15 - and here we are today
177:17 - and statistics relies very heavily on
177:20 - probability
177:21 - we've seen in the previous chapter
177:24 - quantities such as the mean and the
177:26 - median
177:27 - we saw all these sample statistics we
177:29 - discussed the ideas
177:30 - of a sample and a sample's relationship
177:33 - to
177:34 - a population but it's hard you can't
177:37 - really say much more than that and
177:39 - really can't have
177:41 - a rigorous discussion about different
177:45 - uh sample statistics without having
177:48 - a probability theory to back it up so
177:52 - we're gonna start with that right now
177:54 - we're gonna start with section one on
177:56 - sample spaces and events so we start out
177:58 - with the idea of an experiment
178:01 - and experiment is an activity
178:04 - or process with an uncertain outcome
178:07 - examples of experiments including
178:09 - flipping a coin or flipping a coin until
178:13 - the coin lands heads up or you could
178:16 - have
178:17 - rolling a die
178:20 - a six sided die
178:24 - or a rolling two six sided die
178:28 - or you could even have something a bit
178:30 - more abstract such as
178:32 - uh the time in the morning that you wake
178:35 - up that can also be understood
178:37 - via probability theory
178:40 - so when we have an experiment that we
178:43 - have described
178:44 - narratively in a sense so i say i'm
178:47 - going to flip a coin or i'm going to
178:49 - flip a coin until it lands heads up
178:51 - after we have an experiment we need to
178:52 - describe the sample space
178:55 - which we are going to denote in this
178:57 - class with the letter s
178:59 - although i should point out that at
179:01 - least in my experience
179:02 - omega the greek letter omega is
179:06 - more common um notation for the sample
179:10 - space
179:11 - um but this is fine uh s is fine
179:15 - um so this will be the sample space
179:19 - is the set of all possible outcomes
179:22 - of the experiment the sample space is
179:25 - defined
179:26 - by the person who's developing this
179:29 - probability model
179:31 - so it basically you say what the sample
179:33 - space is and you're going to pick a
179:35 - sample space that seems appropriate to
179:36 - the phenomena that you wish to describe
179:39 - a set is very loosely defined as a
179:41 - collection of
179:43 - of objects uh actually this definition
179:46 - of set
179:47 - is bad um because it's possible
179:50 - using just the idea of a collection of
179:53 - objects
179:54 - to construct impossible sets uh sets
179:57 - that are
179:57 - like it's impossible in the sense of
180:00 - being contradictory to itself
180:02 - so uh there's this uh area of
180:04 - mathematics called axiomatic set theory
180:07 - that actually develops a rigorous notion
180:09 - of sets that
180:10 - largely allows for sets that we'd like
180:12 - to think of but honestly
180:14 - uh for our purposes this is f this is
180:17 - definitely overkill
180:18 - uh just thinking of set as a collection
180:20 - of objects is uh
180:22 - fine for us events are subsets of the
180:25 - sample space
180:26 - defining possible outcomes of an
180:28 - experiment
180:31 - we automatically get an event
180:34 - called or a subset called the empty set
180:37 - or the null
180:38 - event uh which is noted with this
180:40 - notation
180:41 - this is a set with no members it can be
180:44 - thought of as an event
180:46 - of as the event where nothing happens
180:49 - and that is precisely how you should
180:51 - think about it i might uh
180:52 - create a separate video describing
180:57 - what precisely the empty set is and kind
180:59 - of try to dispel
181:00 - some inclinations of students to try to
181:03 - assign some
181:04 - deeper meaning to what to the empty set
181:06 - it's like no no no
181:08 - the empty set is a set with nothing in
181:10 - it and you really cannot call it
181:11 - anything else
181:12 - it's more uh a necessity of the
181:15 - mathematics than it is anything that you
181:16 - can
181:17 - honestly interpret so
181:20 - let's get started uh with an example
181:23 - we're going to define a sample space for
181:25 - the experiment of flipping a coin
181:27 - we're going to list all possible events
181:29 - for this
181:30 - experiment let me just get caught up in
181:33 - my notes
181:34 - uh that i have a side here and uh all
181:37 - right so
181:38 - i'm going to say uh that this sample
181:41 - space
181:42 - uh which i'm going to call s uh
181:46 - what are going to be the possible
181:48 - outcomes of flipping a coin
181:50 - well despite what might be physically
181:52 - possible like i actually have seen coins
181:55 - uh not not necessarily like mint coins
181:58 - uh but things very coin like that
182:00 - end up landing on their side but that is
182:02 - not going to be allowed here
182:03 - there's only two possible outcomes heads
182:07 - and tails and notice that
182:11 - notice the curly braces often sets what
182:14 - we are talking about
182:15 - is a set generally sets are going to be
182:18 - denoted with uh curly braces another
182:22 - important fact about sets is that the
182:25 - objects in sets only appear
182:28 - once generally if you were so like for
182:30 - example this set
182:32 - is the same as h
182:35 - h uh t so at at some level there's
182:38 - uniqueness in a set you get imposed
182:40 - uniqueness
182:40 - so if you list heads twice it's the same
182:43 - set
182:44 - okay um and additionally the ordering of
182:49 - how i write stuff down in a set does not
182:51 - matter
182:52 - so i could have written tails heads and
182:54 - it would have been the exact same
182:56 - set here so but yeah now we have the
183:00 - sample space and this is what it is by
183:02 - definition i decided this is the sample
183:04 - space for my experiment
183:06 - and i decided this because i believe
183:08 - that this sample space is going to be
183:10 - the appropriate sample space
183:12 - uh for my problem so i'm saying that
183:14 - there's two possible outcomes of this
183:16 - experiment you the coin either lands
183:17 - heads up or it lands tails up
183:20 - uh and i so next i'm going to list some
183:22 - possible events
183:24 - for this experiment so an event is a
183:27 - subset of the sample space
183:30 - okay so what is one possible subset
183:33 - uh well
183:37 - one possible subset is the sec that
183:40 - contains
183:41 - only h right so only heads so this is
183:45 - the event
183:47 - or the subset where when you flip the
183:49 - coin it lands
183:50 - heads up and similarly we have the event
183:54 - where it lands tails up and some authors
183:58 - like to call
184:00 - um sets like these sets with only one
184:04 - element
184:04 - simple events because they have uh only
184:07 - one outcome even komogorov in his book
184:09 - on probability theory uh denoted
184:13 - uh had the notion of simple events where
184:16 - it has only one
184:17 - outcome corresponding to something that
184:18 - you would actually observe
184:20 - um i i i personally don't really care
184:23 - for the distinction myself
184:25 - uh but students might like it uh
184:28 - we can also have the event heads or
184:31 - tails
184:32 - so what's a what is a possible outcome
184:34 - for this well we get get heads or tails
184:36 - basically um and notice right here that
184:40 - this
184:41 - is the same as the sample space so i
184:44 - could have said
184:45 - the sample space is an event
184:48 - and generally that's true because what
184:50 - does it mean
184:52 - for a set to be a
184:55 - subset of another set what does it mean
184:58 - for
184:59 - for something to be a subset it means
185:01 - that
185:02 - every element in a set
185:05 - is present in another set or
185:09 - equivalently there are no elements
185:12 - in the subset that are not present
185:16 - in let's call it the parent set right
185:19 - so equivalently you cannot find an
185:21 - element in the subset that isn't present
185:24 - in this uh containing set okay
185:28 - so by that definition the sample space
185:30 - is a subset of itself
185:32 - since every element in the sample space
185:35 - is also present in the sample space
185:38 - so it seems almost tautologically true
185:41 - and yet at the same time it matters it
185:44 - matters a great deal
185:45 - that one set that you automatically get
185:48 - when you de
185:49 - one event you automatically get we need
185:50 - to find a sample space is the sample
185:52 - space itself
185:54 - and there is one more event
185:57 - that we have the moment we define the
186:00 - sample space
186:01 - the empty set that is also
186:04 - a subset of the sample space now it
186:07 - seems really weird because you ask
186:09 - yourself
186:10 - how is it that a set with nothing in it
186:14 - another way
186:15 - to write the sample space is like this
186:17 - where you write two curly braces but
186:19 - with
186:19 - nothing in between them because the
186:21 - empty set has nothing in it
186:23 - okay um so you ask yourself
186:28 - how is it that the sample space every
186:30 - element of the sample space is also
186:32 - every element of the empty set is also
186:33 - in the sample space it has no elements
186:36 - well exactly because by this alternative
186:38 - way to think about
186:40 - what it takes to be a subset there is
186:42 - nothing in
186:43 - the empty set that isn't present in the
186:46 - sample space
186:47 - because there's nothing in the empty set
186:49 - therefore
186:50 - you automatically get that you
186:52 - automatically get
186:53 - that the empty set is a subset of the
186:56 - sample space
186:57 - and therefore the empty space is an
187:00 - event
187:02 - now i i i'm kind of implying here
187:05 - that what it takes for something to be
187:07 - an event
187:08 - is that this set
187:12 - needs to be a subset of the sample space
187:14 - so in other words what it takes to be an
187:15 - event is that you simply be a subs of
187:17 - the sample space
187:18 - technically that is not true but
187:22 - the reason why it's not true is going
187:24 - well beyond the scope of this class
187:27 - and uh you might see a little bit of it
187:30 - in probability theory
187:32 - and it would become much more important
187:34 - if you were to take
187:35 - graduate level probability theory
187:37 - measure theoretic probability theory
187:38 - technically it is not true that every
187:40 - subset
187:41 - um of the sample space is an event
187:44 - that said it's really hard to imagine a
187:48 - subset that is a one so basically if you
187:50 - imagine the subset and you didn't
187:51 - actually try
187:52 - to break the theory if you imagine a
187:55 - subset it's probably an event
187:57 - so uh it's for now
188:00 - it's probably fine although i'll
188:02 - probably add a little more
188:04 - rigor to the notion of what it takes to
188:05 - be an event um
188:07 - or do i do that only in like a class
188:10 - devoted or probability theory i'm not
188:11 - really sure if i talk about in this
188:12 - class
188:13 - um we'll see we'll see we'll have to see
188:16 - as we go through the notes
188:18 - all right uh so that's that um by the
188:21 - way i should
188:21 - probably uh mention something uh here
188:25 - we might give names to these events like
188:28 - we might call this first
188:29 - event eh we might call this
188:33 - second uh simple event e
188:36 - t uh to say that one sub
188:39 - one set is a subset of another we can
188:42 - use the notation say
188:44 - uh e h uh is a subset uh and i like to
188:48 - put a line underneath
188:49 - um so to say that it could possibly be
188:52 - the same as
188:53 - uh the sample space so we have this
188:56 - or ah i mean a lot of people also will
188:59 - just write it like this
189:00 - uh so we set have that the event e h is
189:04 - a subset of the sample space
189:06 - e t is a subset of the sample space
189:11 - um s is a subset of itself
189:15 - and the empty set is a subset of the
189:18 - sample space
189:22 - okay so uh
189:25 - continuing on next example define a
189:28 - sample space for the experiment of
189:29 - rolling
189:30 - a six-sided die list three events
189:33 - uh based on this sample space okay
189:36 - so i'm going to say because i think this
189:38 - is the best way to understand this
189:40 - problem
189:40 - uh that it's going to consist of
189:44 - outcomes uh one two three four five six
189:46 - but i'm not going to write the numbers
189:48 - one two three four five six and i have
189:50 - and i have reasons for not writing the
189:52 - numbers
189:52 - the reason is there is nothing in
189:54 - probability theory that requires that
189:56 - your sets contain numbers
189:58 - nothing says that it just says some
189:59 - object very loosely defined
190:02 - so instead of writing numbers i'm going
190:04 - to write
190:05 - little dice faces so i have a dice face
190:09 - that has one pip a dice face that has
190:12 - two pips
190:13 - a dice face that has three pips a dice
190:15 - face that has four pips
190:18 - a dice face that has five pips
190:22 - and finally a dice face that has six
190:25 - pips
190:31 - okay that that looks like six pips all
190:34 - right
190:35 - so that's my sample space and now i'm
190:37 - going to list three events based on this
190:38 - sample space
190:40 - so one event
190:44 - uh one event is the empty set i'm just
190:46 - gonna say it right now
190:47 - the empty set is this upset so here we
190:49 - go we get one we get one event for free
190:53 - i've promised myself i'm not gonna do
190:54 - that here that's my preferred notation
190:57 - i'm not sure if it's the notation used
190:58 - in the book and not everybody uses the
190:59 - same notation
191:00 - that's something that you need to get
191:01 - used to in mathematics you need to pay
191:03 - attention to what notation someone is
191:04 - using
191:05 - because despite the fact that the books
191:08 - and sometimes the instructors make it
191:10 - look like there's one set of notation
191:13 - uh for a subject that's just not true
191:16 - and that's and that's including
191:18 - probability theory and statistics it's
191:19 - just not true that there is one set of
191:21 - notation
191:22 - and you need to pay attention to what
191:24 - someone is actually using to
191:26 - mean their stuff um anyway um
191:31 - so the empty set is a subset of the
191:33 - sample space the sample space is a
191:35 - subset of the sample space
191:36 - both of these are uh valid events
191:40 - okay but they're almost trivial at this
191:42 - point because these are events that you
191:44 - automatically get
191:46 - so what's something that's a little bit
191:47 - more interesting uh well we could have
191:50 - the event where you roll a four
191:53 - uh this is one of those simple events
191:56 - that you may have heard of
191:57 - so four is an event and it's a subset of
191:59 - the sample space
192:01 - uh what's another one that we could have
192:03 - well we could
192:04 - have the event where uh
192:07 - you have an even number of pips
192:13 - that's a valid event uh because
192:16 - in fact even though i've written this
192:17 - down in english and often it's useful to
192:20 - write down
192:20 - sets in english sentences what this
192:23 - actually translates
192:24 - into is a set with three elements you
192:26 - have the
192:27 - set containing the dice with a with two
192:30 - pips
192:31 - on its face uh four pips on its face
192:34 - and six pips on its face
192:40 - these are all uh come on you come on i
192:43 - said six
192:44 - pips don't make me a fool you stupid
192:46 - laptop
192:52 - some people
192:55 - there we go so this is also
192:58 - a subset of the sample space this is
193:00 - also an event
193:02 - um okay
193:06 - so um there we go i've given you
193:09 - four events in fact uh so you got a
193:12 - little bit more than what you paid for
193:13 - anyway uh example three define a sample
193:17 - space
193:17 - describing the event the experiment of
193:19 - flipping a coin
193:20 - until it lands heads up list five events
193:23 - from this sample space ooh
193:25 - so what does this look like what does
193:28 - this look like
193:29 - well i'm going to say here's my sample
193:31 - space
193:32 - and uh well what's one possibility
193:36 - uh flip a coin until it lands heads up
193:39 - well it could land heads on the first
193:41 - flip so you flip the coin it lands heads
193:43 - and then you stop
193:45 - uh you could then fl you could another
193:47 - outcome is you flip the coin and it
193:49 - lands tails
193:50 - so you haven't gotten heavy yet so you
193:51 - need to flip it again and then it lands
193:52 - up
193:53 - heads the second time so tails heads is
193:56 - another possible
193:57 - possibility uh tails tails heads
194:01 - is a third possibility tails tails tails
194:04 - heads
194:05 - is yet another possibility and so on
194:09 - this set has an infinite number of
194:11 - elements because
194:12 - in principle if i have if i have an
194:16 - outcome where uh where you've got so
194:19 - many tails and the last one is ahead
194:21 - it's possible to also have an outcome
194:24 - where you
194:24 - where in order where before you got to
194:26 - that point you flipped the coin you got
194:28 - tails once
194:29 - so for every outcome you can find a next
194:31 - outcome
194:32 - in a sense so since there's always going
194:34 - to be an x outcome this set
194:35 - must have an infinite number of members
194:39 - now there is actually an interesting
194:41 - wrinkle that co that came up in
194:43 - one of my lectures on this is there an
194:45 - outcome
194:46 - in this sample space that corresponds to
194:49 - flipping the coin and it never comes up
194:51 - with heads
194:53 - the answer is no the answer is
194:57 - in this probability model it is
195:01 - impossible for the coin to land
195:04 - to never get heads because
195:08 - i never described an element in this
195:11 - sample space
195:13 - where the coin just where you just flip
195:15 - the coin forever because you never get
195:17 - heads it is explicitly forbidden
195:20 - in this probability model since you
195:22 - cannot find an
195:23 - outcome corresponding to it
195:26 - so therefore since there is no such
195:28 - outcome where the coin never
195:30 - where you never stop flipping the coin
195:32 - it is literally impossible
195:34 - in this probability model it's a very
195:36 - subtle point
195:38 - it seems like you should have that
195:40 - outcome in this but in fact you don't
195:42 - um it it's just and the reason why being
195:46 - i didn't define this sample space to
195:48 - allow for that possibility since there
195:50 - isn't a possibility that corresponds to
195:52 - it
195:52 - it simply doesn't exist right
195:56 - although it seems a little unfair
195:58 - because we can imagine a universe in
196:00 - which someone
196:01 - flips a coin and they never get tails
196:03 - and they never get heads forever
196:05 - it does seem like it's a possibility but
196:07 - it is explicitly forbidden in this
196:08 - probability model since it is not in the
196:10 - universe of possibilities
196:12 - um and by the way it is different from
196:14 - something being
196:16 - impossible and improbable improbable
196:19 - will probably mean
196:20 - uh when you define a probability model
196:22 - for this the probability of something
196:24 - happening is zero
196:25 - which seems realistic to say for
196:27 - flipping a coin
196:28 - until it where you flip a coin forever
196:32 - and uh you never get heads it seems like
196:35 - it's reasonable to say it's improbable
196:36 - but not impossible
196:37 - but right now it is literally impossible
196:40 - since there is no
196:41 - outcome that corresponds to that
196:45 - we would have to add a separate element
196:48 - which
196:48 - if we really wanted to we could
196:51 - add maybe the infinity element
196:55 - to our probability model to represent
196:57 - the
196:58 - outcome where you flip the coin forever
197:01 - um
197:01 - and by the way none of this says
197:03 - anything about what the probability of
197:04 - these
197:05 - events or these outcomes are i have no
197:08 - notion of probably at this point this is
197:10 - all set theory right so
197:15 - it seems like you would say that the
197:16 - probably that you flipped the coin
197:17 - forever is zero but i've i have said
197:20 - nothing about probability so far now
197:22 - that said that is a complication
197:23 - that we are going to leave out we are
197:25 - not going to consider it any further
197:28 - uh we are just going to stick with this
197:30 - probability model
197:31 - uh maybe i would revisit uh this notion
197:34 - of flipping the coin forever
197:36 - uh in a later lecture but that that's it
197:38 - for now
197:39 - uh let's list five events from this
197:41 - sample space
197:43 - well what's one event one event is the
197:46 - sam what is the empty set why didn't i
197:48 - listen that because i'm bored
197:49 - well no not because i'm bored because
197:51 - i'm lazy um another one
197:53 - is um let's say uh
197:56 - you've you can maybe flip the coin
197:59 - exactly three times
198:00 - that were that would correspond to tails
198:03 - tails heads
198:07 - okay this is a possible event um
198:10 - that seems almost like a triviality we
198:12 - could have
198:14 - the event where you flip the coin
198:18 - um at uh at most
198:22 - let's say three times because i don't
198:23 - want to write too much all right
198:25 - at most three times
198:29 - so at most three flips what would this
198:33 - correspond to like this is in words but
198:35 - in fact i can translate in
198:37 - that into a collection of outcomes
198:40 - well you could have flipped the coin
198:42 - only once that's at that's that's no
198:43 - more than three
198:44 - uh you could have flipped twice that's
198:47 - no more than three but if you flip twice
198:48 - and that means that the first flip was
198:50 - tails
198:51 - and uh if you flip three times exactly
198:54 - then you got two tails and a head
198:56 - so we could have tails tails heads and
198:58 - this would be the event
199:00 - uh what's another event we could say um
199:03 - [Music]
199:05 - well another possible event would be uh
199:08 - in words um at least
199:14 - uh three flips
199:22 - that seems reasonable what would that
199:24 - translate into
199:26 - that would be the event where you
199:29 - flip it three times because it's at
199:31 - least three flips
199:32 - so tails tails heads
199:36 - and tails tails tails heads has at least
199:39 - three flips since it has four flips
199:42 - and tails tails tails tails heads
199:46 - has five flips so that counts and in
199:49 - fact there's an infinite number of such
199:50 - flips
199:54 - okay uh and one final uh
199:57 - possibility is an even number of flips
200:05 - that's that's possible what would that
200:07 - look like if we were
200:08 - actually using the elements uh of the
200:11 - set to describe it that would be the
200:14 - event where you have tails heads that
200:17 - has an even number of flips since it has
200:18 - two
200:19 - tails tails tails heads has an even
200:22 - number of flips
200:23 - uh tails tails tails tails
200:26 - tails heads that also counts since it
200:29 - has six flips
200:30 - and so on this event also has an
200:33 - infinite number
200:34 - of outcomes so you can see here it's
200:36 - perfectly possible to talk about a
200:38 - probability model that has an infinite
200:39 - number of outcomes
200:41 - um in fact such models are quite common
200:44 - uh and in fact this particular model
200:47 - where i'm flipping a coin until i get
200:49 - heads as an as an instructor i really
200:51 - like this model because
200:53 - uh it's it's um it's not too difficult
200:56 - at least in my opinion to understand
200:58 - what is going on
200:59 - the idea of flipping a coin until you
201:01 - get heads it's a perfectly reasonable
201:03 - thing to think about
201:04 - and yet at the same time despite its
201:06 - apparent simplicity it
201:07 - is actually a quite rich probability
201:10 - model
201:10 - and makes a lot of points about
201:12 - probability theory
201:14 - so i'll probably be revisiting this one
201:15 - it like it's
201:18 - it's simple but it can very easily
201:23 - get out of hand in a way uh you you can
201:26 - start
201:26 - it can get quite complicated when you
201:29 - start
201:30 - analyzing it uh probabilistically and
201:32 - the mathematics themselves
201:33 - like uh a um like students at this level
201:37 - can't understand
201:38 - it but it's also starting to push their
201:40 - knowledge a little bit
201:42 - and it starts requiring some trickier uh
201:45 - calculations to do all right i'm just
201:46 - going to check something
201:49 - that's not what i wanted yeah we're
201:51 - still streaming okay
201:53 - all right uh example four define a
201:56 - sample space describing the experiment
201:58 - of rolling two
201:59 - six-sided die simultaneously list three
202:02 - events
202:03 - from this sample space uh all right two
202:06 - six sided die simultaneously what would
202:09 - that look like
202:11 - well it's tempting to say that this
202:13 - sample space consists of the numbers two
202:15 - through twelve but that's actually
202:16 - not what we should use the reason why is
202:20 - probably what you're thinking is i'm
202:21 - adding the two the pips on the two dice
202:23 - together
202:24 - but i never said that i never said that
202:26 - there was going to be
202:28 - um addition of two pips so
202:31 - we're not going to do that because you
202:33 - can define a number
202:34 - of uh potential outcomes like maybe
202:36 - instead
202:37 - of uh combining the two pipes together
202:39 - you're taking the larger of the two pips
202:42 - something like that um or the smaller of
202:45 - the two pips
202:46 - so we don't want to define our our
202:48 - sample space that way
202:50 - um what's another thing that we probably
202:53 - should do
202:54 - well when developing such a probability
202:57 - model it's generally better to imagine
203:00 - that you are actually rolling two
203:02 - distinct dice
203:03 - reason why is because when you think of
203:05 - it that way you end up with more
203:06 - appropriate mathematics
203:08 - so it's actually better to think of this
203:10 - sample space
203:12 - uh i keep doing that it's better to keep
203:15 - it's better think of this sample space
203:17 - as consisting of rolling a red dye and a
203:20 - blue dye
203:21 - so that's what we're going to do and
203:23 - just uh
203:24 - to just for my own sanity i try to draw
203:27 - things out as a table
203:28 - so as an example we have a blue dye
203:32 - or we'll call it the left eye and we
203:35 - have
203:35 - a red dye so we have an outcome where
203:39 - the blue dye comes up with a one and the
203:41 - red dye comes up with a two
203:43 - we could also have an outcome where the
203:44 - blue die comes up with a one
203:47 - and the red die comes up with a two
203:51 - i think i might have said the wrong
203:53 - thing a second ago but whatever
203:54 - and we can have an outcome where the
203:56 - blue die comes up with a one
203:58 - and the red die comes up with a three
204:03 - and we would continue on with this i'm
204:04 - not gonna i'm not going to list out
204:07 - everything because i got better ways to
204:08 - spend my day
204:10 - i'm going to say that in this first row
204:13 - uh the last element is where the blue
204:15 - die comes up with the one
204:16 - and the red die comes up uh with a six
204:24 - all right uh so for the next row
204:27 - in the next row we'll have the blue die
204:29 - comes up with a two
204:32 - and the red die comes up with a one
204:35 - uh so we'll just say that
204:39 - uh everything in the blue second row
204:42 - the blue die will be a two so
204:45 - we'll just start out uh listing some of
204:47 - those outcomes
204:49 - and then let's uh and in the third row
204:52 - the blue die
204:53 - will be uh three
205:03 - and we in our very last row we would
205:06 - have the blue die
205:07 - is a six
205:10 - so six six
205:20 - six
205:24 - and finally six all right so i probably
205:27 - should still write down my red die so
205:30 - i'm going to write down my red die
205:32 - uh so we got one and one
205:36 - two two
205:41 - uh two
205:44 - three
205:49 - excuse me i have somewhat of a cough
205:52 - three
205:53 - three and uh
205:57 - six
206:03 - ah you failure you failure of a computer
206:06 - so six
206:11 - uh six
206:17 - and finally six
206:22 - ah for goodness sakes like i said
206:26 - really cheap computer
206:30 - six all right there we go
206:33 - i'm satisfied uh we'll just
206:36 - kind of tidy stuff up put some ellipses
206:44 - um
206:46 - i like things to be rather organized
206:51 - and while we're at it we'll put some
206:52 - commas too although i at this point i'm
206:54 - not really sure
206:55 - if the commas matter all that much but
206:58 - this is a set
206:59 - this is containing a bunch of stuff this
207:01 - is what our set contains
207:02 - um so just uh so
207:06 - how many how many elements are in this
207:08 - set
207:09 - um well this set contains 36 elements
207:13 - since you have
207:14 - six possibilities for the blue dice and
207:15 - six possibility for the red dice so
207:17 - there's 36 things
207:19 - in this sample space which
207:22 - using it this way comes up with more
207:24 - appropriate probability models
207:26 - because this is would be a model where
207:28 - uh if you were thinking about adding up
207:30 - the dice
207:31 - uh a sum of two would be less likely
207:34 - than a sum of three because there's
207:36 - three ways
207:37 - to get the dice to add up to three but
207:39 - only no there's two ways to get the dice
207:41 - to add up to three but there's only one
207:42 - way for the dice to add up to two
207:44 - uh so and that's and that's more
207:47 - appropriate
207:48 - for our probability model
207:51 - and we can start listing some events
207:53 - from this sample space i'm going to list
207:55 - one event
207:56 - uh one of one of them from this sample
207:59 - space
208:00 - uh and it will be the empty set because
208:02 - i'm i'm i'm tired
208:04 - uh e2 another uh event would be the
208:07 - sample space itself so anything happens
208:09 - uh again i'm just really lazy right now
208:11 - all right now
208:12 - i actually have to start writing down
208:15 - some real events
208:16 - well i mean those were real events but
208:19 - uh
208:19 - something that's a bit more interesting
208:21 - um
208:22 - let's see e3 what can we do for e3 well
208:26 - we do have the outcome where we roll um
208:30 - six for the blue die
208:34 - and uh one for the red
208:38 - die why why why this event because why
208:40 - not this event it is an event
208:42 - uh what's a fourth event let's let's
208:44 - start to get a little bit more creative
208:46 - we'll say that the fourth event will be
208:49 - um an even number
208:53 - of total pips total suggesting that
208:56 - you're adding the pips together
208:58 - so
209:03 - all right so let's uh translate this
209:05 - into
209:06 - something a little bit more mathy so you
209:07 - can have let's see
209:09 - there's a whole bunch of outcomes for
209:11 - one where you could have uh
209:13 - uh let's say that the
209:17 - uh oops all right we could say that the
209:20 - blue dice is a one and what's
209:22 - one outcome we have the red dice also be
209:24 - one
209:25 - uh we could have the blue dice b1
209:29 - and the red dice b3 that has an even
209:32 - number of total pips we can just keep
209:34 - going on with this i'm not really sure
209:36 - off the top of my head i could compute
209:38 - it but i'm not really sure off the top
209:39 - of my head how many outcomes
209:41 - uh how many how many outcomes are
209:43 - actually in this uh sample space but it
209:45 - seems like it's going to be a number
209:47 - and i'm tired and i'm sure you don't
209:49 - want to watch me just list out stuff
209:51 - so uh we'll just end this with uh
209:54 - six and six so
209:59 - because at this point you probably get
210:00 - the idea
210:05 - plus i also don't honestly remember the
210:06 - last time anyone asked
210:08 - for this type of this type of event
210:11 - uh another of it so we could have
210:14 - for our fifth event um that
210:17 - the die add up to seven
210:27 - all right so what would be in this event
210:29 - well
210:30 - uh we could have that the blue dice is
210:33 - one
210:34 - and if the blue dice is one since they
210:36 - have to add up to seven that means that
210:37 - the red dice is going to be
210:38 - six
210:42 - uh what's another possible outcome well
210:44 - you could have that the blue dice is two
210:46 - and since the the
210:47 - oh no they don't add up to six they add
210:48 - up to seven yeah that's what i said
210:52 - so uh you could have that the blue dice
210:54 - is two in which case the red dice must
210:56 - be five
210:58 - uh you can have the blue dice be three
211:02 - in which case the red dice uh must be
211:05 - must be four and you keep going on like
211:09 - this
211:10 - uh until eventually you get to the very
211:14 - uh last element that if you were to
211:17 - continue writing down
211:19 - you would write down that element being
211:20 - when the blue dice is six
211:24 - and the red dice is one so
211:27 - how many elements are going to be in
211:29 - this uh event
211:30 - well it seems like for every blue die
211:32 - there's a corresponding
211:34 - and for every blue outcome between one
211:36 - and six there's a corresponding red
211:37 - outcome that would lead to the die
211:39 - setting of just
211:40 - adding up to seven so there must be six
211:43 - things
211:43 - in this event so yes
211:47 - um by the way for whatever
211:50 - for for what it's worth i'm not sure if
211:51 - i mentioned this later but i'll just
211:52 - mention it now uh
211:54 - we often use this notation uh like for
211:57 - example
211:58 - uh this we would put a set in between
212:01 - two what almost look like absolute value
212:02 - lines
212:03 - to mean the size of a set or more
212:06 - technically the cardinality of a set but
212:08 - for now
212:09 - when we're talking about um finite sets
212:12 - it's fine to talk about to say the size
212:15 - of a set
212:15 - meaning the number of elements in that
212:17 - set in which case
212:19 - the size of the sample space is 36 the
212:22 - size of the empty set the set with
212:25 - nothing in it since there's nothing in
212:26 - it its size is zero
212:28 - uh the size of um
212:31 - e5
212:34 - would be six there's six elements in e5
212:38 - and there's uh one element in e3
212:46 - okay continuing on uh our next example
212:51 - define a sample space describing the
212:53 - experiment
212:54 - of waking up in the morning at a
212:56 - particular time where the time you wake
212:58 - up at
212:59 - the thought of as a real number and
213:01 - that's a critical point
213:02 - is the outcome of interest list three
213:05 - events from this sample space
213:07 - um i'm going to say that the sample
213:10 - space
213:11 - so we can think of it in terms of hours
213:14 - in a day but we're going to allow hours
213:16 - to be
213:17 - decimal points so an hour and a half
213:19 - would be
213:20 - 1.5 so we would say that
213:24 - midnight corresponds to zero hours
213:28 - and we're just going to say that you
213:29 - cannot reach the midnight of the next
213:31 - day
213:32 - so maybe you've seen this notation
213:34 - before when regarding
213:35 - uh intervals of the real line where a
213:38 - square bracket
213:39 - means that that number on that side is
213:42 - being included in the set
213:44 - and an open bracket or a parentheses
213:47 - on a side means that that number is not
213:50 - being included
213:51 - in that event uh so some more notation
213:54 - we say we use uh
213:57 - what almost looks like an e or an
213:59 - epsilon to say that something is a
214:02 - member of a set
214:03 - so for example zero is an element
214:06 - of the sample space uh 12 is an element
214:10 - of the sample space
214:12 - um but 24
214:16 - uh well let's let's uh come back to 24
214:18 - in a second 50
214:19 - is certainly not an element of the
214:21 - sample space so we'll put a slash in
214:23 - that
214:24 - uh and 24 is not an element of the
214:27 - sample space
214:28 - either because i put an open what i call
214:30 - an open bracket or
214:31 - a parenthesis around the 24.
214:34 - uh so in this alternate so if i were
214:37 - thinking of 0
214:38 - 24 as a set this would be a set that
214:41 - doesn't include zero either
214:43 - alternatively uh just getting you more
214:45 - familiar with this notation
214:47 - uh we could describe a set that includes
214:50 - both of its endpoints
214:51 - 0 and 24 and by the way all of these are
214:54 - corresponding oh go away
214:57 - all of these by the way are
214:59 - corresponding to
215:00 - uh intervals of the real line so
215:04 - some some interval of the real line uh
215:07 - hopefully you're somewhat familiar with
215:08 - that notation
215:10 - uh yeah okay so
215:13 - uh continuing on this is our sample
215:14 - space let's uh erase all this stuff
215:16 - because this is someone evident aside
215:22 - um
215:25 - okay uh so uh three events from this
215:29 - sample space
215:30 - we could have an event where um
215:34 - uh we wake up at noon so that would
215:37 - probably
215:38 - correspond uh to the outcome where you
215:41 - wake up 12 hours from midnight
215:44 - um so this would be exactly 12 hours
215:48 - though
215:48 - exactly 12. not a second
215:52 - more not a millisecond more not a
215:54 - millisecond less and not a second less
215:56 - exactly
215:57 - 12 hours later which you think of
216:01 - that that matters it is exactly 12 and
216:04 - nothing
216:05 - it seems like like like our own
216:07 - intuition of the world is
216:09 - like if you wake up a millisecond after
216:11 - mid afternoon
216:12 - um you still woke up at noon i was like
216:15 - no that is not what i mean right here
216:18 - so it is a precise number and that
216:21 - precision
216:22 - should lead you to think well that's
216:25 - impossible it's
216:26 - it's or maybe not impossible but
216:29 - but really really really really hard to
216:32 - the point
216:32 - of being almost impossible and that is
216:35 - true
216:36 - when we develop for a probability model
216:39 - for this experiment we would probably
216:41 - assign a probability of zero
216:43 - to the event of waking up exactly 12
216:45 - hours from midnight which is kind of a
216:48 - strange idea but
216:49 - i'm going to talk about that later video
216:51 - just understand that
216:53 - just to understand what exactly i'm
216:55 - saying here uh let's uh come up with
216:57 - another event
216:59 - uh this event corresponds to waking up
217:01 - between
217:03 - let's say instead that you wake up
217:05 - sometime between 8 and 12 hours
217:07 - now my language is a little imprecise
217:11 - here
217:13 - so i need to specify do i mean
217:16 - uh including eight hours
217:19 - uh including exactly eight hours
217:22 - um or am i saying more than eight hours
217:25 - so eight hours in a millisecond is okay
217:27 - but eight hours
217:28 - on the nose is wrong uh so we'll just
217:31 - say
217:31 - that we're going to exclude those
217:33 - endpoints why because i said so
217:35 - um so
217:38 - this excuse me uh this uh this event
217:42 - uh would be an event where
217:46 - you do in fact uh where you're
217:49 - where you can wake up um eight hours in
217:51 - a millisecond eight hours in a second
217:53 - or 12 hours less a second but you cannot
217:56 - wake up at exactly eight hours or
217:58 - exactly 12 hours and
218:00 - have this event actually have occurred
218:02 - okay
218:03 - um let's say for a third event
218:07 - um we're going to wake up um let's say
218:12 - that
218:12 - that this is uh waking up after 3 a.m
218:15 - so or um
218:18 - let's see uh
218:22 - maybe no earlier than three event at 3am
218:24 - so no earlier than 3am
218:27 - uh that suggests that we should include
218:29 - 3am
218:30 - in this in this uh in this event
218:33 - so uh so we should include 3am or 3
218:37 - hours past midnight
218:38 - but all right so what should the end
218:40 - point on the other end be it seems like
218:42 - we can wake up any time
218:43 - after 3 a.m and that's fine so we're
218:46 - going to end at 24
218:48 - since that's the last um so
218:51 - since those points are the last ones in
218:53 - this sample space
218:58 - okay
219:02 - let me get caught up all right moving on
219:06 - uh events once we have some events
219:10 - we can start uh manipulating these
219:12 - events in ways
219:13 - that create new events uh
219:17 - so we can start giving some uh
219:19 - operations
219:20 - on our set theory so for example let's
219:23 - just say that a
219:24 - and b are events uh and events in this
219:27 - situation they're also synonymous with
219:29 - sets so anything you know about set
219:31 - theory
219:32 - uh you can import that here i am
219:35 - actually when i'm talking about events
219:36 - i'm really talking about sets there
219:38 - those two terms in this class are used
219:40 - almost interchangeably
219:42 - so the complement of a which in this
219:45 - class we're denoting with a
219:47 - oops uh in this class we're denoting
219:49 - with a prime but that's not the only
219:51 - notation there's a bar
219:53 - or a complement this is actually my
219:55 - preferred notation
219:56 - uh that's but anyway uh in this class
219:59 - we're going to use a prime
220:00 - this is the set of outcomes of s
220:04 - the sample space not in
220:08 - uh the event a
220:12 - uh so it's all outcomes that could
220:15 - possibly happens
220:16 - that are not in a which so
220:20 - and in fact we the the the
220:23 - english words that we use for this event
220:25 - is not
220:26 - a that's that's a perfectly way to
220:29 - describe it
220:30 - there's the union of two sets
220:33 - uh which we would say a union b but i
220:36 - also like to just say
220:37 - a or b since this corresponds to the
220:40 - logical notion of or
220:42 - where something in one event can happen
220:44 - or something in another
220:45 - event could happen and by the way when
220:47 - we're using or in this context we're
220:49 - using it in the logical sense where
220:51 - um we can have an outcome in is
220:54 - in a only would count as happening in a
220:57 - or b
220:58 - an outcome in b only would
221:02 - count as have having occurred in the
221:03 - event a or b and an
221:05 - outcome that is both in a and in b
221:09 - counts as happening in the event a or b
221:12 - which is a little bit different from how
221:13 - the word or is used in english because
221:15 - it's
221:16 - quite often the case in english that or
221:17 - means exclusively or
221:19 - exclusive or or you can say you can have
221:22 - her
221:22 - your cake or eat it and
221:26 - like in that so the phrase have your
221:28 - cake or eat it
221:29 - it suggests that you can either have
221:30 - your cake or you can eat it but you
221:32 - can't have both
221:33 - right if you there was actually a it was
221:36 - actually a really long time until that
221:38 - phrase made sense so uh this might
221:41 - actually be something for
221:42 - um people whose native language is not
221:44 - english
221:45 - uh what they mean by have your cake and
221:47 - eat it too
221:49 - um you can have your cake in your hand
221:51 - or you can eat your cake
221:53 - but if you eat your cake it is no longer
221:55 - in your hand so that's what it means all
221:56 - right so
221:57 - just in case i think that that might be
222:00 - something that
222:00 - for non-native speakers it's actually
222:03 - worth it to make that clarification but
222:04 - for a long time i
222:05 - also as someone who's spoken english all
222:07 - their life and was on the debate team
222:09 - and on the literary magazine
222:10 - did not get anyway um
222:14 - uh the intersection of two sets a
222:17 - and b or a intersect b so it's the set
222:21 - that
222:21 - contains only objects that appear in
222:23 - both a and b
222:25 - so in words we use a and b
222:29 - i'm going to go ahead and get started
222:30 - drawing some diagrams
222:32 - venn diagrams are a way to
222:36 - describe set theoretic relationships
222:38 - between
222:39 - uh different sets and venn diagrams you
222:42 - can construct venn diagrams for pretty
222:43 - much
222:44 - up to uh three events
222:47 - and venn diagrams make sense and very
222:49 - diagrams are very simple
222:51 - and the moment you try to go to four
222:52 - events event diagrams become impossible
222:55 - so we're just going to live in a world
222:56 - where there's only three events um
223:00 - so here's how you kind of draw a venn
223:01 - diagram at least for this class
223:04 - you draw circles and squares and stuff
223:06 - to denote sets
223:08 - often we draw a giant square
223:11 - and this square denotes the sample space
223:14 - so the square denotes
223:16 - the universe of possibilities and we
223:19 - denote
223:20 - a subset of this sample space otherwise
223:22 - known as an event
223:23 - with some other shape such as a circle
223:26 - or maybe
223:27 - i there are times where i will draw
223:30 - lines to try to divide it up but
223:31 - basically we're going to draw shapes and
223:33 - lines
223:34 - inside of this sample space that divide
223:37 - regions of the sample space from each
223:40 - other
223:41 - and those will denote subsets so often
223:43 - i'm going to draw
223:44 - circles to denote events so i'll often
223:48 - label one circle a
223:49 - and another circle b and
223:52 - how we draw these circles allows us to
223:55 - reason about
223:56 - relationships amongst events so here i
223:59 - have drawn
224:00 - a sample space with two events a and b
224:04 - and those events have some outcomes in
224:07 - in common
224:08 - since there is a region in which both
224:11 - set
224:11 - both of these circles intersect now
224:13 - there's also a situation where there are
224:16 - outcomes in a that are not in b so
224:19 - situations where a happens but b doesn't
224:21 - happen
224:21 - because you can kind of imagine if you
224:23 - really want a probability model
224:25 - that we're going to pick a random point
224:27 - from
224:28 - this set and we're going to ask where
224:30 - did this appear did this appear in
224:32 - in a did it appear in b did it appear in
224:35 - a or b
224:36 - um did it appear in a and b or did it
224:39 - not appear in either one
224:40 - um that would almost be a probability
224:42 - model so we can use
224:44 - diagrams such as this to start reasoning
224:47 - about
224:47 - uh set theoretic relationships um
224:51 - and in fact here's something to
224:55 - kind of think about well actually no i'm
224:58 - going to save that for the next section
225:00 - um but this is a perfect perfectly
225:02 - reasonable way
225:03 - to uh think about uh set theory at this
225:07 - level
225:08 - um okay so uh two
225:11 - so uh let's uh describe the situation
225:16 - uh a or b
225:20 - so a union b means an outcome that is
225:22 - either in a
225:23 - or in b or both so
225:26 - what i would do in a venn diagram is
225:29 - shade the region that corresponds to
225:31 - this event
225:32 - well maybe before i do that maybe before
225:34 - i go
225:35 - into that level of extra complication
225:38 - how about i first describe the event
225:41 - just a
225:42 - well visually using a venn diagram that
225:44 - would what i would do is i would shade
225:46 - the region a
225:47 - and nothing else and that would
225:50 - correspond
225:51 - to the event where a happens so i'm
225:53 - using
225:54 - shading up these circles uh to
225:57 - try and reason about what happens here
226:00 - uh i wonder if there's a
226:02 - let's use this highlighter tool i wonder
226:03 - what would happen if i use the
226:04 - highlighter
226:06 - uh it makes my computer lag a lot i'm
226:08 - not gonna use that
226:10 - um so um
226:13 - uh similarly we could describe another
226:16 - event where we just
226:18 - uh have b occur so we'll say the event b
226:22 - how would we shade this we're going to
226:24 - shade it like so just shade in the
226:25 - region that's enclosed by b
226:27 - and nothing else so this corresponds so
226:30 - this is the region that corresponds
226:32 - to the event b happening now let's go
226:34 - back to some of these other
226:36 - uh potential relationships uh such as
226:39 - a or b or a union b
226:42 - um hmm this seems like a fancy feature
226:46 - no no it doesn't work that way so i need
226:48 - to get rid of that
226:50 - red region
226:55 - all right so um
226:56 - [Music]
226:58 - here's a way to draw venn diagrams uh
227:01 - when you're trying to do
227:03 - um stuff so like
227:06 - let's take for example the union
227:08 - operation a
227:09 - union b to draw a union operation what i
227:13 - generally will do
227:14 - is i'll take a common color and i will
227:17 - shade
227:18 - in first uh a
227:21 - uh i don't want black i want blue
227:25 - i'll first shade in a the the set on the
227:27 - left side of the union relationship
227:30 - and then i will shade and be the element
227:33 - on the right hand side
227:34 - and a point that was shaded at all
227:37 - is a member of this set
227:42 - so um so
227:45 - this region that i've kind of enclosed
227:46 - in red
227:48 - uh corresponds that's that a or b since
227:50 - any point that got shaded by blue at all
227:53 - counts as being a member in this event
227:56 - okay uh let's draw another
228:00 - uh event let's uh illustrate for example
228:04 - a intersected with b so to draw
228:07 - intersection
228:08 - let's uh first draw our sets a and b
228:14 - to draw intersection you're going to
228:15 - subtract points uh you're going to
228:18 - subtract from uh
228:19 - uh or or actually what you would do
228:22 - is i like to think of it in terms of
228:24 - pieces of paper uh
228:26 - where you overlap two pace pieces of
228:28 - paper on top of each other and then
228:30 - uh cut the pieces of paper so that only
228:32 - the overlapping region is what's left
228:35 - so uh you could imagine here
228:38 - uh i draw uh something on a
228:42 - and i draw something on b and then
228:45 - i'm going to erase from the picture the
228:48 - region
228:49 - that what that was in b but not in a
228:54 - and i'm going to erase from the picture
228:56 - the region that is in a but not in b
229:03 - and what's left is going to be the
229:05 - region that corresponds to a
229:07 - intersected with b
229:11 - okay um so another
229:15 - important notion is disjointedness two
229:17 - sets are disjoint if they have no
229:19 - elements in common
229:20 - in that case a intersected with b is the
229:23 - empty set
229:24 - so to draw disjointedness this is what i
229:27 - would do
229:28 - here's my sample space i draw an event a
229:32 - and i draw an event b such that there is
229:34 - nothing in common between the two events
229:36 - all right i'm going to draw the
229:37 - intersection between these two events
229:39 - i'm done
229:40 - because there's nothing intersecting so
229:42 - the intersection between these two
229:44 - events
229:44 - is uh is the empty set
229:48 - to so this is one way if you really
229:52 - wanted to assign some sort of meaning to
229:56 - the empty set it would be
229:58 - that the empty set is a logical
230:01 - contradiction in a way since the empty
230:03 - set shows up
230:04 - and when you have events that have
230:06 - nothing in common
230:07 - which are almost logical contradictions
230:11 - uh another notion in probability theory
230:14 - no in a set theory is complementation
230:16 - how would i illustrate complementation
230:18 - so let's uh draw a sample space
230:21 - i have a i have b
230:25 - what is the so this is um
230:28 - a intersect with b uh equals the empty
230:31 - set that means disjoint
230:33 - uh what is a complement a complement is
230:37 - the region in the sample space that is
230:39 - an
230:39 - a that is in the sample space but it's
230:42 - not an a
230:43 - so that's going to correspond to shading
230:45 - everything that's
230:46 - outside of a
230:50 - so there are some parts of b that gets
230:52 - that get shaded but nothing that's in a
230:57 - okay so you shade everything except a
231:02 - uh some other important subset
231:04 - relationships
231:06 - um we have uh
231:10 - uh or so some other set relationships
231:13 - here is the relationship
231:14 - a is a subset of b
231:17 - what it means for one set to be a subset
231:20 - of another
231:21 - is that um all of the elements in a
231:24 - are also impres are also present in b or
231:27 - alternatively there is nothing that a
231:30 - that doesn't also exist in b
231:32 - so a subset relationship looks like so
231:34 - you have the set b
231:35 - and the set b contains the the set a so
231:38 - you would
231:39 - draw a as being in the interior of b
231:43 - another uh notion that i haven't really
231:46 - described above but let's
231:47 - go ahead and mention it is the notion of
231:50 - set subtraction
231:51 - we have a subtract the set b that
231:54 - corresponds to every element in a that
231:57 - is
231:57 - not in b so uh it
232:00 - this it is in fact possible to prove
232:03 - that
232:03 - a subtract b and you might actually be
232:06 - required to show this in your homework
232:08 - and the way you probably show it
232:09 - is by playing around with venn diagrams
232:12 - you can say this is a
232:16 - intersected with um the complement
232:19 - of b so a and
232:23 - not b so what does set subtraction look
232:27 - like
232:27 - as a venn diagram we have the set a
232:31 - we have the set b and we draw everything
232:34 - in a
232:35 - and we shade everything in a that is not
232:37 - in b
232:41 - so you kind of subtract out the set b
232:43 - it's as if
232:44 - you had these pieces of paper they put
232:46 - them you put uh
232:47 - the piece of paper the b piece of paper
232:49 - on top of the a piece of paper and then
232:51 - you cut out the part of the b
232:52 - of the a piece of paper that's in the b
232:54 - piece of paper
232:58 - excuse me okay
233:01 - uh so let's see an example use a venn
233:04 - diagram to illustrate
233:05 - uh a or b complement or a and b
233:09 - uh before i continue on let's just make
233:11 - sure i'm still streaming i'm still
233:12 - streaming
233:13 - um at some point i'll relax about that
233:17 - but
233:17 - today is not that day uh let's get
233:20 - started by just drawing uh
233:22 - the venn diagram oops i don't want red
233:25 - yet
233:30 - so here's my venn diagram for the sample
233:32 - space
233:33 - s i have a and i have
233:37 - b okay so i'm first going to
233:41 - illustrate uh the event a intersected
233:44 - with b
233:44 - i'm going to do so in blue so a
233:46 - intersective b that's the region that is
233:48 - in both a
233:49 - and b uh as for
233:52 - a or b complement you have the component
233:55 - you have the union of a or b which is
233:57 - kind of this uh
233:59 - figure eight looking region here
234:02 - and i want everything outside of that
234:05 - region because i want the complement of
234:07 - that region
234:08 - so i'm going to shade it everything that
234:10 - is outside
234:12 - of that region and this is what you end
234:16 - up with
234:23 - okay so that's one example next example
234:26 - consider three sets a b and c
234:29 - so how would i draw these in a venn
234:32 - diagram
234:33 - i would have my square denoting the
234:37 - universe of possibilities which is
234:38 - s and then i draw um
234:42 - three circles one for a one for b and
234:46 - one for c
234:47 - so what is the union of a b and c
234:50 - well it's going to be the set where uh
234:53 - if i shade a
234:55 - everything in a and then i shade
234:57 - everything that's in b
234:59 - and then i shade everything that's in c
235:02 - if the point ever got shaded
235:03 - it's going to be in that union
235:07 - all right next up we have a intersected
235:10 - with b
235:11 - intersected with c
235:14 - so let's draw our universe
235:18 - here's our sample space
235:22 - so we have a b and c
235:26 - so let's see uh let's take a point right
235:28 - here
235:29 - this point that i just drew is uh a
235:32 - point that is in b
235:33 - and it is in c but it is not an a so
235:35 - it's not going to be in the intersection
235:37 - uh this point right here is in b but
235:39 - it's in neither a or c
235:41 - but it it's also just not an a so it's
235:43 - not going to be in the intersection of
235:44 - all three
235:45 - you can just start reasoning about all
235:47 - of these points in a similar way
235:49 - and the conclusion that you're forced to
235:51 - reach is that this little sliver here
235:54 - is the only part that's going to be in
235:55 - all three sets so that corresponds to
235:57 - the intersection of all three
236:00 - all right uh next example this is a more
236:02 - complicated one
236:03 - we have a intersected with b or a
236:06 - intersective with c
236:07 - or b intersected with c what is that
236:09 - going to look like well let's get
236:11 - started
236:12 - i don't want red
236:17 - all right here's our sample space
236:20 - here is a here is b
236:25 - here is c all right let's start out by
236:29 - uh building this thing up with uh the
236:32 - intersection a and b
236:34 - right here is a and b here's a and c
236:38 - and here's b and c so this wind or
236:40 - pinwheel
236:41 - looking region uh is the only region
236:44 - that ever got shaded so that's going to
236:45 - correspond
236:46 - to the union of the three of the three
236:49 - intersections
236:50 - so this would in words this would be an
236:53 - event where at least
236:54 - two events happen this will correspond
236:58 - to
236:59 - all right uh so example eight describe
237:02 - the intersection complement and union of
237:04 - events described in examples one through
237:06 - five
237:11 - let me get caught up um
237:15 - so the point of this section is to go
237:20 - from these pictorial representation or
237:22 - not this section this example
237:24 - is to go from these pictorial
237:26 - representations
237:27 - of events to more algebraic
237:30 - representations
237:32 - so uh let's go with example one
237:39 - so for example one that was the example
237:43 - where we were flipping a coin we could
237:44 - get either heads or tails
237:46 - uh we could have uh the intersection of
237:49 - heads and tails
237:51 - uh of the two uh sets heads
237:54 - and tails these two sets have nothing in
237:57 - common
237:58 - they are therefore considered disjoint
238:00 - events
238:01 - so the intersection of these is the
238:02 - empty set since you would only
238:04 - restrict yourself to what is in common
238:06 - and they have nothing in common
238:08 - uh whereas the event
238:11 - heads or tails
238:16 - uh that event would correspond to uh the
238:20 - event heads with
238:21 - heads and tails uh which corresponds by
238:24 - the way to the sample space
238:28 - okay uh here's also some basic uh
238:31 - properties for you uh let's let's take
238:34 - an
238:34 - arbitrary uh set
238:38 - s no no no not s uh let's take an
238:41 - arbitrary event a
238:43 - a intersected with a sample space is
238:46 - equal to
238:47 - a since a is a subset of the sample
238:49 - space in fact
238:50 - in general if a is a subset
238:53 - of b then a intersected with b
238:58 - is going to be uh a
239:01 - and a union with b is going to be
239:05 - b and i'm just going to leave it at that
239:06 - i want you to think about why that's
239:08 - true
239:08 - uh go try and reason about it with a
239:11 - venn diagrams
239:12 - um so and in fact that's pretty much
239:15 - that pretty much says everything that i
239:16 - would want to say about uh the sample
239:19 - space
239:20 - and the empty set because uh since a is
239:22 - the subset of the sample space a
239:24 - intersected with s
239:25 - is going to be a and a unioned with
239:28 - s is going to be s likewise
239:32 - a intersect intersected with the empty
239:35 - set the empty set
239:36 - is a subset of a therefore the
239:40 - intersection of two is going to be
239:42 - the subset which is going to be the
239:44 - empty set
239:45 - on the other hand a union with the empty
239:48 - set
239:48 - is going to equal a since a contains the
239:51 - empty set
239:54 - all right uh continuing on with what i
239:56 - was saying before
239:58 - um the next example
240:03 - uh so example two uh so
240:06 - uh we have uh for example the sets
240:10 - um my notation in the notes is a little
240:13 - bit different from one i'm sure i wrote
240:15 - down
240:16 - earlier um so let's uh
240:20 - uh um
240:23 - i don't want to go all the way back
240:25 - there
240:28 - i'll just uh i'll just throw some stuff
240:31 - out
240:31 - uh we can have um for example uh
240:38 - so this was the one where we were
240:41 - rolling a die
240:42 - uh one we could say that the union of
240:45 - the set
240:47 - uh one with the die roll of one
240:50 - and the set with the die roll of uh
240:54 - so the intersection of these two sets
240:56 - this is also going to be the empty set
240:58 - since they have nothing in common
241:01 - but uh the intersection
241:05 - of the event with um
241:08 - no the union of these two sets
241:12 - is going to be
241:16 - uh is going to be um the
241:19 - event that contains both of these
241:21 - possible outcomes
241:27 - darn laptop
241:31 - so one and three
241:36 - and uh maybe less trivially um
241:41 - uh we could have
241:44 - uh we could have the um event
241:48 - uh let's say one three five
241:52 - so this would correspond to an odd
241:54 - number of pips
241:58 - and we're going to intersect that with
242:00 - the event where the number of pips
242:01 - doesn't exceed three
242:03 - um so that would be the event where you
242:06 - have
242:06 - one uh two
242:10 - and three oh
242:14 - okay so
242:19 - uh the intersection of these two events
242:22 - is going to be
242:23 - well let's see what outcomes do they
242:24 - have in common uh one amperes in both of
242:27 - them
242:28 - three appears in this set but it doesn't
242:30 - appear on the other one so let's see
242:31 - we've got
242:32 - one one appears in both
242:36 - uh three appears on only one
242:39 - no three actually appears in both of
242:40 - them so three appears in both of them
242:44 - five appears only and one and two only
242:46 - appears on one so whatever got
242:47 - underlined twice that is going to be in
242:50 - the intersection of the end
242:51 - of these two sets so we're going to end
242:55 - up with the
242:57 - set
243:00 - one and three
243:09 - now for a lot of these uh set
243:11 - relationships such as and and or you can
243:13 - kind of think
243:14 - logically using english like for example
243:17 - um
243:18 - uh maybe going to uh the fourth example
243:21 - where i had uh
243:24 - sets where um yeah i was rolling a two
243:28 - dice
243:29 - uh a red dye and a blue dye you could
243:32 - imagine a set
243:33 - where uh they summed to seven
243:37 - and the blue dice is at least three
243:40 - so maybe going to that so in the context
243:44 - of example four
243:49 - so in the context of oops
243:52 - in the context of example four
243:55 - so they sum to seven
244:03 - and they don't
244:07 - or no no no no uh the blue dice
244:15 - uh so blue dice is
244:21 - um uh let's say five
244:25 - so that would correspond to this to the
244:27 - event
244:29 - where uh you have um
244:33 - so let's see the blue dice is at least
244:35 - five so we could have
244:36 - the blue dice b5
244:41 - stupid pen so the blue dice could be
244:44 - five
244:46 - or the blue dice could be six and
244:49 - in these two situations we know what the
244:51 - red dice is going to be
244:53 - in the first the red dice is going to be
244:54 - two and in the second the red dice is
244:57 - going to be one
244:58 - so this would be uh the corresponding uh
245:01 - resulting event after we do that
245:02 - intersection
245:04 - okay um i'm going to leave it at that
245:07 - there's
245:08 - because there's a lot of examples so for
245:09 - the rest of these maybe
245:11 - uh try going through uh some of the set
245:14 - the events that i listed down and
245:17 - figuring out if you
245:18 - intersect or complement these events so
245:20 - intersection complementation
245:21 - union uh all these things try going
245:24 - through those sets and
245:26 - uh seeing what you get okay
245:29 - but i've given you some examples to
245:30 - start working off of for now
245:32 - all right so i'm gonna leave that for
245:34 - this section and uh
245:36 - i will see you in section two
245:39 - where we go beyond just uh talking about
245:42 - some basic set theory and actually
245:44 - start saying what it takes to build a
245:46 - probability model
245:47 - all right so i will see you later
246:15 - hey students all right so let's move on
246:19 - now to the next section
246:21 - uh on starting probability theory
246:24 - so uh in order for us to uh
246:28 - we would like to be able to assign
246:30 - numbers
246:31 - to events from uh sample spaces
246:35 - to describe how likely those events are
246:38 - and in order to do so we need to develop
246:41 - a notion of probability
246:42 - so we'll start by defining what a
246:44 - probability measure is
246:45 - so we have so we have what's known as a
246:48 - probability measure
246:49 - p and this is a function it's a function
246:53 - that takes
246:53 - events as inputs and returns numbers
246:56 - between
246:57 - zero and one and satisfies the following
247:00 - three
247:00 - axioms and axioms are things that are
247:02 - true basically because we say so they're
247:05 - they're almost similar to assumptions
247:10 - maybe you remember axioms from geometry
247:12 - but the idea is that
247:14 - we have some starting things that we
247:15 - simply say they are true because they
247:16 - are almost self-evident
247:18 - first we say that every probability must
247:21 - be at least zero
247:23 - uh you cannot have negative
247:24 - probabilities second
247:26 - we say that the probability of the
247:28 - sample space is equal to one so the
247:30 - probability that anything happens
247:32 - is equal to one finally uh this is the
247:35 - weirdest
247:36 - looking axiom if we have a sequence of
247:40 - disjoint events
247:42 - uh so in other words for any uh i that's
247:45 - equal to j the intersection of uh any
247:48 - two such events
247:49 - is uh equal to the empty set and
247:52 - furthermore
247:53 - this is for a potentially infinite
247:55 - sequence of disjoint events
247:57 - we say that the probability of the union
247:59 - of those events is equal to
248:01 - the sum of their individual
248:02 - probabilities this is very wordy
248:05 - this is very technical there is a much
248:08 - easier way
248:09 - to understand what this axiom is saying
248:11 - if we
248:12 - decide well okay there's usually only
248:15 - one situation
248:16 - where you're going to understand where
248:18 - you're actually going to use this
248:20 - axiom most of the time which is that if
248:22 - a intersected with b
248:24 - is equal to the empty set so if a and b
248:26 - are two disjoint events
248:28 - then the probability of a or b is equal
248:31 - to the probability of a
248:32 - plus the probability of b so
248:35 - the probability of uh two events that
248:38 - have nothing in common
248:40 - uh if the probability that either one
248:41 - happens will be the sum of their
248:43 - individual probabilities
248:46 - okay so these are true because we say so
248:50 - and from this we get uh
248:54 - pretty much everything else that we
248:55 - believe should be true about
248:57 - uh probability or about how
248:59 - probabilities
249:00 - work so uh for starters uh we have
249:05 - uh that the probability of the empty set
249:08 - is equal to zero
249:10 - so this is not something that we said
249:12 - this is not something that we said must
249:14 - be true
249:15 - this is simply something that
249:18 - um this is actually a consequence of the
249:21 - assumptions that we have made
249:23 - up to this point so this is kind of a
249:26 - weird
249:27 - so we're going to show that this is in
249:30 - fact
249:30 - true using just
249:34 - these uh three um
249:37 - just these three assumptions not just
249:39 - these three axioms
249:41 - so only these
249:44 - three things all right um
249:48 - now that said we're going to have to use
249:51 - the fact that probably the sample space
249:52 - is equal to one
249:53 - what we could say is this the sample
249:55 - space
249:58 - the sample space is equal to
250:01 - the sample space unioned
250:04 - with the empty set and in order to use
250:08 - axiom three in the way we have written
250:12 - axiom three down
250:14 - we're going to have to say that the
250:15 - empty set is equal to
250:18 - uh the union from i equals one
250:22 - uh to well i equals one to infinity
250:25 - the empty set so in other words we need
250:27 - to create an infinite union
250:29 - of empty sets and it's certainly true
250:31 - that if you take an infinite
250:33 - uh collection of empty sets and you
250:35 - union them all together there's still
250:36 - not going to be anything
250:38 - in that union so you're still going to
250:39 - have the empty set furthermore
250:41 - the intersection of the empty set with
250:43 - the empty set is also going to be the
250:45 - empty set so technically the empty set
250:47 - is disjoint with itself so that means
250:50 - that this collection
250:52 - of empty sets this giant uh repeating of
250:54 - the empty set
250:55 - technically satisfies the condition of
250:58 - the conditions of the third axiom
251:01 - so then and and also it's true that the
251:04 - set
251:04 - the sample space uh intersected with the
251:07 - empty set is going to be
251:09 - the empty set because the empty set is a
251:11 - subset of the sample space
251:13 - therefore we can now apply this
251:17 - uh this uh this uh
251:20 - third axiom to prove the prob uh
251:22 - proposition
251:23 - because we know that one is equal to the
251:26 - probability
251:27 - of the sample space and the probability
251:30 - of the sample space is equal to the
251:31 - probability
251:32 - of the sample spaced uh unioned
251:36 - with the empty set which is equal to
251:40 - uh the probability of the sample space
251:43 - unioned with this infinite collection
251:48 - of uh empties of uh empty sets
251:54 - and then we apply that third axiom to
251:56 - say
251:57 - that uh this is going to equal to the
252:00 - probability
252:01 - of the sample space uh plus
252:05 - uh this uh plus um the uh
252:09 - the sum from uh i equals one to infinity
252:13 - uh the probability of the empty set
252:17 - and what that forces us to then say is
252:20 - that since
252:21 - uh the probability of the sample space
252:22 - is equal to one we're forced then to say
252:25 - that the sum from i equals one
252:28 - to infinity of the probability of the
252:30 - empty set
252:32 - is equal to 0 but that's only going to
252:35 - be possible
252:37 - if the probability of the empty set
252:40 - is itself equal to 0.
252:44 - because since probabilities uh
252:47 - cannot um be uh
252:51 - negative and oh and even just because of
252:53 - the fact
252:54 - that uh we're adding up something an
252:57 - infinite number of times
252:58 - and that's and that's the exact same
252:59 - thing and it adds up to zero
253:01 - so that means that each one of those
253:03 - must be equal to zero
253:05 - therefore this uh axiom this uh
253:08 - proposition is proven
253:10 - this one this is actually rather weird
253:13 - it's
253:13 - it's so surprising like there is a way
253:16 - in which you
253:17 - you you kind of just want to say uh you
253:20 - have
253:21 - one equals to the probability of the
253:24 - sample space
253:25 - which is equal to uh the probability of
253:27 - the sample space
253:29 - plus the sample space unioned with the
253:31 - empty set and then it turns into a sum
253:33 - of probabilities
253:34 - and uh that means that probably the
253:37 - empty set is equal to zero
253:38 - but we have to in order to be
253:40 - mathematically rigorous
253:41 - use these axioms in the way they were
253:43 - written down
253:44 - so we would actually have written the
253:46 - axiom in a way in which it wasn't
253:47 - written and therefore wouldn't be able
253:49 - to
253:50 - directly apply it so we had to be really
253:52 - tricky but the good news
253:54 - is that we only had to be tricky really
253:56 - once
253:57 - with this uh proposition just with this
254:00 - one because now that we have
254:02 - this proposition the others will not
254:04 - nearly uh
254:05 - will not be nearly as uh strange for
254:08 - example proposition three
254:09 - like this one this proposition if we had
254:12 - this already
254:14 - then that proposition that we just
254:15 - proved would actually be rather easy
254:17 - because
254:18 - uh the complement of the sample space
254:21 - is equal to the empty set uh the
254:24 - probability of the
254:25 - the sample space uh is equal to one
254:28 - and since the uh uh the the empty set is
254:31 - the complement of the sample space we
254:32 - can say
254:33 - if this proposition 3 were in fact true
254:38 - the probability of the empty set is
254:40 - equal to
254:41 - 1 minus the probability of the sample
254:44 - space
254:45 - which is 1 minus 1 which equals 0.
254:49 - right if we had proposition three then
254:51 - it would actually be rather easy
254:53 - to prove that the probably the empty set
254:56 - is zero
254:56 - but the thing though is in order to be
254:59 - able to prove
255:00 - uh proposition three we're going to have
255:03 - to
255:04 - uh probably use uh some of those uh
255:07 - other propositions so
255:09 - unfortunately uh it's it's um
255:13 - it's not quite that it's we we
255:15 - technically need to
255:17 - take a very long circuitous route
255:20 - in our proof before we can actually
255:22 - invoke this one
255:24 - so what we're going to say is that the
255:27 - probability
255:28 - of the sample space we can say that the
255:30 - sample space can be divided
255:32 - into uh into the set a
255:36 - unioned with the complement of a
255:40 - and uh a union a complement
255:44 - um well okay a intersected with a
255:46 - complement
255:48 - um is equal to the empty set
255:52 - so since a intersected with uh its
255:55 - complement is
255:56 - is the empty set that means that these
255:57 - two sets are going to be disjoint
256:00 - which means that we can then invoke uh
256:03 - that um that a second that third axiom
256:07 - and say that the probability of the
256:09 - sample space is equal to
256:11 - the probability of a union a complement
256:16 - which is equal to by that that other
256:18 - axiom the probability of a
256:20 - plus the probability of a complement
256:24 - and uh by the first axiom this is all
256:26 - equal to one
256:28 - therefore after you do a little bit of
256:30 - algebra where you subtract
256:33 - uh you subtract both from both sides the
256:36 - probability of
256:37 - a
256:41 - uh after you do that algebra you can now
256:44 - say
256:45 - that the probability of a complement
256:48 - is equal to one minus the probability of
256:52 - a and we're done that proposition has
256:55 - been proven
256:57 - uh next proposition
257:00 - we have that the probability of a is
257:02 - less than or equal to 1
257:04 - for any event a let me get caught up in
257:07 - my
257:08 - notes over here
257:12 - all right uh so
257:15 - the probability of a complement
257:19 - since a complement is in fact an event
257:22 - by the first axiom uh we get to say that
257:25 - the probability of a complement
257:27 - is greater than or equal to zero
257:30 - which means that one minus the
257:33 - probability
257:34 - of a is going to be greater than or
257:36 - equal to zero
257:38 - and therefore it follows after oops
257:41 - after we add the probability
257:46 - of a to both sides
257:50 - after we do that uh we get to say
257:54 - that uh uh
257:57 - so since these two things end up
257:58 - cancelling out uh the probability
258:01 - of a is less than or equal to one
258:05 - so that proposition has been proven uh
258:09 - next one or is that it no that's not it
258:13 - uh the probability of a union b is equal
258:16 - to the probability of a plus the
258:17 - probability of b
258:18 - minus the probability of a intersected
258:19 - with b for any events
258:22 - a and b so
258:25 - uh this by the way is generalizing
258:28 - more or less that third axiom since that
258:31 - third axiom required disjoint events
258:34 - whereas here in this proposition we do
258:37 - not require
258:38 - disjoint events um we do not require
258:40 - just disjoint events and the penalty
258:42 - that we pay for that is that we have to
258:43 - subtract out
258:44 - the probability of the intersection
258:46 - here's the thing though uh this
258:48 - this makes perfect sense what's going on
258:50 - if we draw
258:52 - a venn diagram um
258:55 - it turns out that probability
258:59 - is part of this general class of
259:01 - mathematical objects
259:03 - or probability measures are part of this
259:05 - general class of mathematical objects
259:06 - known as measures
259:08 - and amongst that family of mathematical
259:10 - objects we include
259:12 - things such as measures of area or
259:14 - measures of length
259:15 - which means that it's actually rather
259:18 - appropriate
259:19 - and convenient to reason about
259:22 - probabilities the same way we reason
259:23 - about
259:25 - areas or lengths as we do in the real
259:27 - world
259:28 - so let's think instead about
259:31 - we have um a couple uh sets
259:35 - a and b we want to figure out what the
259:39 - area
259:39 - is in a and
259:42 - b like the that's enclosed within both
259:45 - of those we want to figure out that area
259:47 - um and how we're going to do so is uh
259:50 - we have a couple sheets we have an a
259:53 - sheet and a b
259:54 - sheet and we lay them down and we have
259:56 - some scissors
259:58 - so um we can measure the areas
260:02 - of these sheets so uh given that
260:05 - um we're able to measure the areas of
260:07 - these sheets how can we possibly compute
260:08 - the area that's enclosed
260:10 - in both a and b well what we would do
260:13 - is we would lay down the a sheet onto
260:17 - this diagram and we would have its area
260:20 - then we would lay down the b sheet on
260:23 - this diagram and we would have its area
260:25 - but the thing though is once we've done
260:27 - that we now have an
260:28 - overlap we have an overlapping area
260:31 - um so we've actually over counted
260:35 - the area um in a and b we the number is
260:38 - too large
260:40 - so what we need to do is subtract out uh
260:43 - the part the overlapping part of the
260:47 - a and b subtract out its area because
260:49 - otherwise we would have double counted
260:51 - it
260:52 - so we're going to uh subtract out that
260:55 - area um and uh or at least we'll like
261:00 - cut out
261:00 - a little slice of it um uh maybe just
261:04 - like the red slice part
261:06 - and leaving only the blue part left and
261:08 - and we're allowed to measure the area of
261:10 - the part that we cut
261:11 - out so after we uh subtract that
261:15 - the area that we removed we now have the
261:17 - area that's enclosed between
261:19 - uh that's closed within these two
261:21 - circles and
261:22 - that's an aerial way to understand
261:26 - why this formula here is in fact true
261:30 - because this is base because how i
261:31 - described it is
261:33 - basically what we're doing we have the
261:35 - area of the region a and the area of the
261:37 - region
261:38 - b and we subtract out once the area
261:40 - that's in between them since we
261:42 - accidentally well not accidentally since
261:45 - uh without doing so
261:47 - we would have double counted that part
261:50 - so we need to remove the double counting
261:54 - so how are now that's like a reasoning
261:58 - for what we're doing
261:59 - and now let's turn that into
262:03 - a series of mathematical statements and
262:05 - a proof
262:07 - here's a way to think about this region
262:10 - this venn diagram
262:11 - we can divide up in this venn diagram
262:17 - uh we can divide up uh this uh region so
262:20 - that we have
262:22 - uh the light blue region
262:25 - we have the green region
262:28 - and we have the red region
262:34 - we will call the blue region
262:37 - a and not b
262:42 - we will call the green region
262:45 - a and b and we will call the red region
262:50 - uh not a and b
262:54 - and it's clear that these three regions
262:57 - are um
263:00 - these three regions are disjoint you can
263:03 - see so visually but if you wanted
263:05 - a non-visual way to reason why that is
263:07 - the case let's suppose an element
263:10 - is let's suppose that we pick a point in
263:12 - that is in the set a
263:14 - and not b can that set be an a and b
263:17 - well it must not because in order b
263:19 - in the part a and b it must be b b end b
263:22 - uh but it is not in b since it's an a
263:25 - and
263:25 - not b so it cannot be in there and uh
263:28 - similarly for
263:30 - uh not a and b and for the same reason
263:32 - if you're in a and not b
263:34 - you cannot also be in not a and b since
263:36 - you're in a and therefore you're not in
263:38 - not
263:39 - a i know this is the verbiage is getting
263:41 - really complicated but
263:43 - that is that is really the logic for why
263:46 - these things
263:46 - must be destroyed or you can just look
263:48 - at the picture and be satisfied
263:50 - um for that reason we can say
263:54 - that the i mean you can also look at
263:56 - this picture and see that when you take
263:57 - the union of these three
263:59 - uh events uh you have in fact the region
264:02 - a or b
264:03 - and there's also no double counting here
264:05 - since you counted each little part at
264:07 - this each division once
264:09 - so now we can say i'm going to zoom in
264:12 - uh some more we can say that the
264:15 - probability
264:16 - of a or b
264:20 - is equal to invoking that third axiom
264:24 - the probability of a and
264:27 - not b plus
264:30 - the probability of a and
264:34 - b plus the probability
264:39 - of um uh
264:42 - not a and b
264:46 - and uh let's see let's
264:49 - see what how should i proceed
264:52 - next well i do notice for starters that
264:56 - this part right here is equal to the
264:58 - probability of a
265:01 - which is uh clear from the picture
265:04 - because if you add the probability of a
265:06 - and not b
265:06 - and the probability of a and b then
265:08 - you've basically counted the probability
265:10 - of a
265:11 - so this must be the probability of a uh
265:13 - but the thing is though
265:15 - uh we need to somehow account for
265:18 - uh i mean we end up in the end with a
265:20 - subtraction of the probability of a
265:22 - and b so how are we going to do that
265:25 - huh well if we look at the probability
265:27 - of b
265:28 - we could say as an aside that the
265:30 - probability of
265:31 - b is equal to the probability
265:34 - for basically uh the reasons that i just
265:37 - wrote here
265:39 - that this is the probability
265:42 - of um a
265:45 - and b plus the probability
265:50 - of uh uh not a
265:54 - and b and then if you do
265:58 - some algebra you can then say
266:01 - that the probability of not a
266:04 - and b is equal to the probability
266:08 - of b minus uh the probability
266:12 - of uh a and b
266:16 - okay oh well look at that we now
266:18 - basically have what we need because
266:20 - we've identified a part
266:21 - as the probability of a and we can also
266:24 - identify
266:25 - the latter part in this sum as the
266:27 - probability of b
266:29 - uh minus the probability of
266:32 - a and b which is exactly the statement
266:35 - that we wanted to prove
266:37 - so therefore we're done
266:40 - and we have in fact proven this
266:42 - statement and notice that this statement
266:45 - does in fact include that third axiom
266:47 - since
266:47 - if since that third axiom was about
266:50 - disjoint events
266:51 - so um if we have a disjoint event we've
266:56 - proven in uh in
266:59 - our the first proposition that we proved
267:01 - in this section that the probability
267:03 - of the empty set is equal to zero and
267:06 - the subtraction of the probability of a
267:09 - and b
267:10 - is going to give you um
267:13 - so so that's going to uh so the
267:16 - intersection of a and b
267:18 - if and since in this imaginary scenario
267:19 - they're uh disjoint that's going to be
267:22 - the probably the empty set which is
267:23 - equal to zero and thus you get
267:26 - uh that uh that other axiom
267:32 - all right so uh oh oops i am
267:35 - uh i am doing something that i don't
267:38 - want to do
267:39 - what i want to do is zoom out
267:46 - okay so the next proposition i'm not
267:49 - going to bother to prove
267:51 - just because it's a lot of work although
267:53 - i may give you an
267:54 - argument for why it's true so
267:56 - proposition 6 now we have
267:58 - three events a b and c and the
268:01 - probability of the union of those three
268:03 - events is going to be
268:04 - the sum of the probabilities of the
268:06 - events minus the probabilities of
268:08 - intersections of two
268:10 - plus the probability of the intersection
268:11 - of all three all right so
268:14 - i'm not going to prove this but i'm
268:15 - going to give you an argument for why
268:16 - this must be true
268:18 - so here we have our sample space um we
268:21 - have the set
268:23 - a we have the set b
268:26 - and we have the set c it's basically the
268:29 - same reasoning as we used before
268:32 - um let's uh uh let's
268:35 - see so uh so what does it mean to add
268:39 - the probability of a probably be
268:40 - probably c so if we
268:42 - add those three probabilities so we're
268:44 - gonna add the probability of a
268:47 - because what we're trying to do is
268:48 - figure out in the area that is enclosed
268:51 - in all three circles uh but those
268:54 - circles are overlapping with each other
268:55 - so we need to figure out how to handle
268:56 - the overlaps so we're gonna color in the
268:58 - probability of a that's what we're told
268:59 - to do first
269:00 - uh then we're gonna color in the region
269:03 - and close by b
269:04 - that's what we need to do second and
269:06 - then we're going to enclose
269:07 - the region enclosed then we're going to
269:09 - color in the region and close by c
269:13 - because that's what we've been told to
269:15 - do
269:16 - yeah you can almost read this as a set
269:18 - of instructions on how to calculate an
269:20 - uh how to calculate um
269:23 - uh um uh an area
269:27 - so we see that we have done some double
269:30 - counting
269:31 - we've double counted here we've double
269:33 - counted here and we double counted here
269:35 - all right so we need to remove those
269:39 - double counts
269:40 - uh let's start with the um let's uh
269:43 - let's start with the double counting
269:45 - here we're going to subtract
269:48 - out uh the probability of a and b which
269:52 - will i will under
269:53 - i will understand that as subtracting
269:54 - out a little green sliver
269:57 - so how does this i'm wondering if this
270:00 - is doing something oh
270:01 - no it just was being laggy
270:05 - all right so um
270:09 - so i have subtracted out the green area
270:11 - from that sliver
270:13 - uh leaving a blue left
270:17 - and some red red left
270:20 - okay uh and then i need to
270:24 - i'm gonna cut out the uh
270:27 - red area that is intersecting and that
270:30 - is at the intersection of a
270:31 - and c okay so i'm going to cut that
270:35 - out uh leaving only blue
270:38 - uh um leaving only blue there
270:43 - uh let's see so we're going to have
270:46 - uh right here
270:50 - so and then we need to uh
270:53 - cut out the area that is in the
270:55 - intersection of
270:56 - b uh and so we now need to cut out the
271:00 - uh
271:01 - region that's in uh both b and c
271:04 - so we're going to subtract out uh the
271:06 - red area but the thing is that doesn't
271:08 - that doesn't uh that isn't enough we
271:10 - cannot just take out the red arrow
271:12 - because we need to take out
271:13 - everything that's in here so we also
271:14 - have to take out what that little blue
271:16 - sliver that's left as well
271:18 - so uh we're going to be left with green
271:22 - in that in that spot uh but the thing is
271:25 - in that little sliver
271:29 - uh that that is in the intersection of
271:32 - all three
271:33 - we have now cut out oops uh we have now
271:36 - cut out too much and
271:38 - that area is not being accounted for at
271:41 - all there's nothing left in there
271:43 - so we need to add that area
271:47 - back in
271:50 - in order to be able to have an accurate
271:51 - computation of the area
271:54 - and there we go so after we add it back
271:57 - in uh we now have the area so everything
271:59 - has been colored
272:00 - uh exactly once and uh we're able to
272:04 - compute the area of
272:06 - how much area we've colored uh in a
272:08 - sense so we
272:09 - have what we need so not exactly a proof
272:11 - because we need to
272:12 - uh do some do some of that a tricky
272:14 - algebra business but
272:16 - uh we're not gonna bother with that this
272:18 - should give you an idea of why it's true
272:21 - okay uh so the next example that all of
272:24 - that stuff was uh
272:25 - uh rather theoretical uh let's let's um
272:29 - start seeing how uh probability theory
272:32 - is actually going to be used so for the
272:34 - most part of the remainder of the
272:36 - section i'm going to be going through a
272:37 - number
272:38 - of illustrative examples uh we do talk a
272:41 - little bit more
272:42 - about uh theory mostly about what
272:44 - probability means
272:46 - uh but uh for the most part we can just
272:48 - talk about
272:49 - um examples so example nine reconsider
272:53 - the experiment of flipping a coin
272:55 - and assume that the coin is equally
272:57 - likely to land
272:58 - with each face facing up assign
273:00 - probabilities to all
273:01 - outcomes in the sample space so recall
273:04 - that the sample space
273:06 - for this experiment consists of the
273:08 - outcome heads and the outcome tails
273:11 - and we're assuming that all of the
273:12 - outcomes in this sample space
273:14 - are equally likely so that means that
273:16 - the probability
273:18 - of uh getting heads uh and
273:21 - i'm writing this in terms of a simple
273:23 - event but honestly
273:25 - this writing it this way often gets
273:28 - rather tedious so
273:29 - i'm often going to admit uh to omit the
273:32 - curly braces that are usually used
273:34 - to denote sets and just write whatever
273:36 - is in the set
273:37 - so we have the probability of h uh this
273:40 - is going to equal the probability
273:42 - of t because all outcomes are equally
273:45 - likely
273:46 - so the thing though is the probability
273:48 - the sample space
273:51 - is equal to the probability
273:54 - of the set containing only h unioned
273:58 - with the set containing only t since
274:00 - this
274:00 - uh so since the union of those two sets
274:03 - is in fact equal to the sample space
274:05 - and furthermore those two sets have
274:06 - nothing in common one has h one has t
274:08 - and they don't share anything so that
274:10 - means that they are disjoint events
274:12 - and therefore we can write this
274:13 - probability as a sum
274:15 - as the probability of h
274:19 - plus the probability of t and both of
274:22 - these are the same so we're going to say
274:23 - that this is equal to p
274:25 - so this is equal to uh p plus
274:29 - p which is two p and furthermore we know
274:33 - from
274:34 - uh axiom two that the probability of the
274:36 - sample space
274:37 - is equal to one so this is equal to one
274:40 - and that implies after you do some
274:42 - division by two
274:44 - that p is equal to one half
274:49 - which makes perfect sense right if
274:51 - you're saying that heads is just as
274:52 - likely as tails then the probability of
274:54 - getting heads is one
274:55 - one half perfectly intuitive right well
274:58 - you know
274:58 - uh probability is intuitive up until the
275:01 - point it isn't
275:02 - and when it stops being intuitive it
275:04 - really stops being intuitive
275:06 - and it becomes suddenly very very
275:08 - strange so it's simultaneously intuitive
275:11 - and
275:11 - unintuitive in fact i've heard someone
275:13 - say that
275:14 - there are fewer subjects with as many
275:17 - paradoxes they're not literally
275:19 - paradoxes but they contradict
275:21 - how humans think of the world very few
275:24 - subjects have as many paradoxes as
275:26 - probability
275:27 - because probability can get really weird
275:29 - really fast we have not reached that
275:31 - point yet but we probably will
275:32 - eventually
275:33 - example 10 do the same as example 9 but
275:37 - when rolling a single dice that is
275:39 - um we are trying to
275:43 - so we we're trying to assign
275:45 - probabilities to outcomes in a sample
275:46 - space when we say that those outcomes
275:48 - are equally likely
275:49 - so the sample space in this case is
275:52 - going to consist of die rolls
275:54 - um so we have uh an outcome one
275:58 - a two uh three
276:04 - four
276:08 - five
276:12 - and six
276:19 - okay so those are our six outcomes and
276:22 - we say that each one of them is equally
276:24 - likely so that means that the
276:25 - probability
276:26 - of rolling a one is the same as
276:30 - is uh the same as the probability
276:33 - of rolling a two and that's going to be
276:37 - the same as
276:38 - dot dot dot as this and the same as
276:41 - uh the probability of rolling a six
276:51 - all right so uh it's a six so
276:54 - all right there we go six it's a it's an
276:57 - oddly painted die
276:58 - all right so all of those are going to
277:00 - be the same and we're just going to say
277:01 - for
277:02 - convenience that this is equal to p all
277:04 - right
277:05 - so then we say that the probability
277:08 - of the sample space is equal to the
277:12 - probability of
277:14 - the result one uh unioned with the
277:18 - result
277:20 - uh two union with
277:23 - dot dot dot unioned with uh
277:28 - the uh outcome uh containing
277:32 - uh six
277:37 - there we go
277:41 - because that's equal to the sample space
277:43 - you basically written the sample space
277:44 - as a union of each of its elements
277:46 - and furthermore each of these sets again
277:48 - are disjoint
277:50 - so uh we can then write them as the sum
277:53 - of probabilities so say that this is
277:55 - equal to the probability
277:56 - of rolling a 1 plus the probability
278:00 - of rolling a 2
278:04 - plus dot dot dot plus the probability
278:08 - of rolling a six
278:15 - and then we use the fact that we said
278:17 - from the beginning that all these
278:18 - probabilities are the same
278:20 - so this is p plus p plus p plus p plus p
278:23 - plus p so this is equal to six p but
278:26 - it's also equal to one
278:27 - because of axiom two that says that
278:29 - probably the sample space is equal to
278:30 - one
278:31 - therefore p must be equal to one over
278:34 - six
278:36 - and you can see where this is going in
278:39 - general if you have a sample space with
278:41 - a finite number of elements
278:43 - uh and you say each of those elements
278:45 - are equally likely
278:46 - then the probability of a single one of
278:49 - those elements
278:50 - in that sample space probably of drawing
278:52 - that element is going to be 1 divided by
278:54 - the size of the sample space or the
278:56 - number of elements number of unique
278:57 - elements in that sample space
279:00 - um so in fact what we're seeing here
279:03 - can be very easily generalized
279:07 - okay uh so example 11 we're now
279:11 - going to try and move away
279:14 - from equally likely outcomes and say
279:17 - that the dice from example 10 has been
279:19 - altered with weights
279:21 - now the probability of the dice rolling
279:22 - a 6 is twice as likely as rolling a 1
279:24 - while all the other sides have the same
279:26 - probability of appearing as before
279:29 - therefore we want to know what the new
279:31 - probability model is
279:33 - so we're going to say that the
279:34 - probability
279:36 - of rolling a six
279:41 - so one two three
279:44 - four five six
279:47 - so the probability of this of rolling a
279:49 - six is equal to
279:50 - two times the probability of rolling
279:54 - a one and we're just going to say that's
279:58 - equal to two
279:58 - times p or uh
280:02 - we'll just we'll try to keep things a
280:04 - little bit different we'll call it q
280:05 - this time
280:06 - all right so we need to figure out q
280:08 - because q gives us the probability of
280:10 - rolling a one
280:11 - and if we figure out the probability of
280:12 - rolling a one we then instantly know
280:14 - rolling a six
280:15 - now remember these are the only two
280:17 - faces that have been altered
280:19 - all the other dice faces have the exact
280:21 - same probability as before so the
280:23 - probability of rolling a two
280:24 - is equal to the probability of rolling a
280:26 - three which is the probability of
280:27 - rolling a four which is probably ruling
280:28 - a five and all
280:29 - all of those are equal to one over six
280:32 - so that means um that the probability
280:37 - of rolling a one uh plus the probability
280:41 - of rolling a two
280:44 - plus dot dot dot plus the probability
280:48 - of rolling a 5
280:52 - plus the probability of rolling a 6.
281:01 - come on cooperate you stupid screen
281:06 - i hate drawing the 6 because my screen
281:08 - is not very cooperative
281:10 - all right uh this is equal to one
281:14 - but uh what's different now is that all
281:17 - of these are equal to one over six
281:19 - so you end up adding one over six four
281:22 - times
281:22 - since it's two three four five there's
281:24 - four things there um
281:26 - the probability of rolling a six is
281:28 - equal to two q
281:30 - and the probability of rolling a one is
281:32 - equal to q
281:34 - okay so um collecting all of that
281:37 - information
281:39 - we now say um that we have a
281:44 - q plus 2q
281:48 - plus 4 over 6 which is 2 over 3
281:53 - is equal to 1 which implies
281:57 - after you do some algebra that 3q
282:00 - is equal to 1 3 which then means
282:04 - that q is equal to ah no is equal to
282:08 - one over nine not nine because that's
282:10 - not even a probability
282:12 - uh that's that's that's one thing to
282:13 - keep in mind if you get a probability
282:15 - that's above one you have made a mistake
282:17 - so nine is not possible the problem so
282:20 - that means that the probability of
282:21 - rolling a one
282:23 - so yeah let's let's uh write this down
282:25 - now the probability of rolling a one
282:27 - in this new probability model is equal
282:29 - to one-ninth
282:30 - and the probability of rolling a six in
282:32 - this new probability model
282:38 - is equal to two over nine
282:42 - and in fact this isn't this is
282:43 - consistent because
282:45 - one over nine plus two over nine is
282:47 - equal to three over nine which is one
282:49 - third
282:50 - there are the uh other dice faces when
282:53 - you add up their probabilities you have
282:55 - one over six one over six one over six
282:56 - one over six which is two-thirds
282:59 - and so you have one-third plus
283:00 - two-thirds which equals one
283:02 - so the probabilities still add up to one
283:05 - which means that we're fine
283:06 - this is a way for you to check uh
283:08 - whether this is one way for you to check
283:10 - that you're that when you do your
283:12 - probability calculations you have done
283:13 - so correctly
283:14 - make sure that all the probabilities add
283:16 - up to one if they don't add up to one
283:19 - you've made a mistake i remember once a
283:21 - student was
283:22 - working in a i had a student of mine and
283:25 - i gave her a quiz
283:26 - and she had to do some calculations with
283:28 - probabilities and
283:30 - she added up the probably sample space
283:32 - and it added up to
283:33 - something that was to some fraction it
283:35 - might have been
283:37 - uh maybe uh eight over nine or eight
283:40 - like 80 or 81 who knows
283:42 - but but i said this is very wrong and
283:45 - she's like well
283:46 - it needs to add up to one and she said
283:48 - well it's close to one
283:49 - and i said close to one is not one
283:53 - so if it doesn't add up to one it's just
283:56 - wrong
283:57 - it like close to one no caboose it's
284:00 - either one or it's not one
284:02 - if it doesn't add up to one it's wrong
284:04 - it's very wrong
284:06 - it always adds up to one so that's a way
284:08 - for you to check
284:09 - that you've done things correctly make
284:10 - sure that your probabilities add up to
284:13 - okay moving on
284:17 - example 12 reconsider the experiment of
284:19 - rolling two six sided die
284:21 - it is reasonable to assume that each
284:22 - outcome in s is equally likely this is
284:24 - the reason why
284:25 - instead of writing the numbers 2 through
284:28 - 12 or maybe just
284:30 - listing out without really caring about
284:32 - the ordering of the die
284:34 - or not really thinking about there being
284:35 - a red dye and a blue dye
284:37 - we we decided that we were actually
284:39 - going to assign an ordering to the dice
284:41 - it's so that we could basically work
284:42 - with problem
284:43 - example 12 and get a reasonable looking
284:46 - probability model
284:47 - because now if we assume that the two
284:49 - dice are distinct we can now say that
284:51 - everything in the sample space
284:53 - in that sample space with a red and blue
284:55 - die is equally likely
284:56 - and then get accurate probability
284:59 - calculations
285:00 - so uh getting back to this it is
285:03 - reasonable to assume that each outcome
285:05 - in the sample space s is equally likely
285:08 - what then
285:08 - is the probability of each outcome in s
285:12 - um well basically i already argued it uh
285:14 - to you before we'll say that omega
285:16 - is an element of s so this
285:20 - i'm saying this in general right um
285:23 - omega is the element is an element in in
285:26 - s
285:26 - and every element in s is equally likely
285:29 - i argued before without writing it down
285:31 - that the probability of drawing omega
285:34 - then
285:35 - will be 1 divided by the size of
285:39 - s so in this case for example 12
285:43 - where you have two six-sided dice um
285:46 - the size of the sample space was 36 so
285:49 - the probability of a sink
285:50 - of a of a particular outcome of dice is
285:53 - going to be 1 over 36.
285:55 - now we can use this uh this model
285:59 - uh to find the probability of an event e
286:03 - where so i'll i'll just go ahead and
286:05 - write down in this case
286:06 - that um the size of the sample space s
286:09 - is equal to 36
286:11 - so for this particular problem this is
286:13 - going to be 1 over 36
286:18 - but in fact what i just wrote down here
286:19 - true what i just wrote down here is
286:22 - pretty much drew in general
286:24 - like um i'm i
286:27 - i'm thinking in the context of uh
286:29 - rolling a couple die but
286:31 - actually this is true in general when
286:33 - you say
286:34 - that s which is a finite sample space it
286:36 - has a finite number of elements
286:38 - um when everything in that sample space
286:40 - is equally likely
286:42 - um then the probability of an individual
286:44 - individual element is going to be one
286:45 - over the samples
286:46 - over the size of the sample space and
286:48 - it's not too hard why go ahead
286:50 - uh show that the uh probably the sample
286:52 - space when you do this is equal to one
286:55 - uh so use this model to find the
286:57 - probability of an event e
286:59 - where first e is the event where at
287:01 - least one die
287:02 - is a six so let's actually write down
287:06 - um let's write down
287:10 - uh what's in e so that would be we have
287:13 - a red dye
287:14 - and a blue dye at least one dye is six
287:17 - so
287:18 - that includes when the red dye is
287:21 - one and the blue dye uh is
287:24 - six
287:28 - we have uh the case when
287:32 - the red die is two
287:35 - and the blue die is six
287:44 - we have and we can keep going on like
287:46 - this
287:47 - until we eventually reach the case where
287:51 - uh the the red die
287:54 - is a five
288:02 - the red die is five and the blue die
288:06 - is six
288:13 - yeah damn it you cooperate
288:17 - all right that's six um
288:20 - so uh that's one set of outcomes and
288:23 - notice that there's
288:24 - uh that i've just listed down five
288:27 - elements
288:29 - okay uh so next
288:32 - up we will now make the red dye six
288:35 - so we've got one two three
288:39 - four five six
288:42 - it never wants to do the last one never
288:45 - ever wants to do the last one
288:47 - all right so one two three four five six
288:50 - and the blue die
288:51 - is one and
288:55 - uh just just repeat just keep uh
288:58 - carrying on with this logic until we get
289:03 - where the red dice is six
289:08 - and the blue dice is five
289:14 - and finally we have one last outcome
289:17 - where both the red dye and the blue dye
289:21 - are six so one two
289:24 - three four five six and
289:28 - uh one two three four five
289:31 - six all right
289:34 - so
289:37 - uh there are
289:41 - um five elements where
289:45 - the uh um
289:48 - where the red die is fixed at six and
289:50 - then there's
289:51 - one extra element where
289:54 - uh both of them are six so that means
289:57 - that the size
289:58 - of this event is going to be um
290:02 - 11. which then means when we're
290:05 - computing the probability of this event
290:08 - we could add up the probability of each
290:10 - one of these outcomes
290:12 - and each one of these outcomes has an
290:13 - equal likelihood and
290:16 - they all have an equal probability all
290:17 - those probabilities are 36 so you're
290:19 - gonna add up 36
290:20 - and one over 36 11 times so this is
290:23 - going to add up to 11 over 36.
290:28 - and in fact it's once you have this uh
290:32 - this um assumption that
290:35 - all outcomes in this finite sample space
290:38 - are equally likely in general the
290:41 - probability of any event
290:42 - e is going to be the number of elements
290:45 - in e
290:46 - or the size of e divided by the size of
290:48 - the sample space
290:50 - so at this point all we need to do is
290:52 - decide how to determine how many
290:53 - elements are in our event
290:55 - in order to figure out that events
290:56 - probability all right so
290:58 - for let's let's use that now for uh
291:02 - this uh next problem where e is the set
291:06 - where is the event where the sum of the
291:07 - pips showing on the two die is uh
291:09 - five how many outcomes are there where
291:12 - the sum
291:14 - is going to be five let's start listing
291:16 - out
291:17 - uh possible things in this sample space
291:24 - so uh we've got i guess i've been
291:26 - writing the
291:27 - red dice first
291:33 - okay so
291:37 - okay so this is all right so i've been
291:40 - writing out the red dye first
291:41 - so let's suppose let's see the the two
291:44 - dice is five so can the red dice be one
291:45 - yeah sure why not
291:47 - uh the red dice can be one and if it's
291:49 - one well it must add up to five so that
291:51 - means that the blue dice must be four
291:54 - okay that's one possibility uh next
291:57 - possibility is when the
291:59 - blue dice is two and which is when the
292:01 - red dice is two sorry in which case the
292:03 - blue dice must be three
292:05 - uh we could have the case where the red
292:07 - dice is three
292:08 - in which case the blue dice must be two
292:12 - and finally we have the case
292:15 - where the red dice is four
292:20 - in which case the blue dice must be one
292:22 - and we can't go to five
292:24 - because it's not possible for the blue
292:26 - blue dice to
292:27 - roll a zero um so we're just gonna have
292:30 - to end it there
292:32 - uh we now have uh a sample
292:35 - an event and this event has four
292:37 - outcomes in it
292:39 - so that means that the size of the event
292:43 - is going to be a four in which case
292:47 - the probability of the event e
292:50 - is going to be uh 4
292:53 - over 36 which is going to be
292:57 - uh 1 over 9.
293:03 - okay next part uh
293:07 - e is the event where the maximum
293:10 - of the two numbers showing on the dice
293:12 - is greater than two
293:14 - okay now here's the thing though um
293:17 - there's actually a lot of outcomes in
293:18 - this event so
293:20 - we could attempt to try and figure out
293:23 - how many
293:24 - outcomes there are where
293:27 - the larger of the two numbers is greater
293:30 - than 2.
293:32 - well let's see we could even have like
293:33 - three one three two three three three
293:35 - four three or five three six
293:36 - all the all the cases where the red dice
293:38 - is three and that's already six
293:40 - such outcomes and that's only for three
293:42 - so
293:43 - uh this could actually be quite
293:46 - difficult to compute
293:48 - except for the fact that it's actually
293:50 - much easier to compute
293:51 - the comp the probability of the
293:53 - complement of this event
293:54 - if we were to look at the complement of
293:56 - the event e
293:58 - that's going to be the event where the
294:00 - maximum um
294:02 - of the two dice is
294:06 - um not greater than two
294:09 - it so it's at most two okay
294:16 - uh where what are what are combinations
294:19 - of dice facing
294:20 - faces where the maximum is at most two
294:23 - well we have one situation where
294:25 - uh we have um oh yeah i said that the
294:28 - red dice is first
294:30 - so we have the situation where the red
294:32 - dice is one
294:33 - and the blue dice is one
294:37 - all right in that case the maximum of
294:38 - the two dice is going to be one
294:40 - and that is uh not greater than two so
294:43 - this is in fact
294:44 - in our event uh then we have the outcome
294:48 - where the red dice is two and the blue
294:51 - dice is one
294:53 - in that case the maximum would be two
294:55 - and that doesn't exceed two
294:57 - so this is in our event we also have
295:01 - uh the case where the red dice is one
295:05 - and the blue dice is two
295:08 - let's see all right there we go so the
295:10 - blue dice is a two
295:13 - and we also have the outcome
295:16 - where the red dice is two and the blue
295:20 - dice is two
295:22 - and the maximum of those in this case
295:24 - would be two and again that's uh at most
295:26 - two
295:26 - and we have to stop there because then
295:28 - because the the next thing we would do
295:30 - is make one of the dice
295:31 - at least three and in which case the
295:34 - maximum would be
295:35 - at least three so it's not long going to
295:37 - be
295:38 - in this event so therefore we have an
295:41 - event with four outcomes in it and the
295:42 - probability
295:43 - of the complement of the event e
295:47 - is going to be the number of things in
295:49 - the complement of e
295:50 - uh which is four divided by 36 which is
295:53 - equal to
295:54 - one over nine all right we're cooking
295:56 - because now we can use one of those
295:58 - propositions that said that the
295:59 - probability of the complement of an
296:01 - event
296:01 - is one minus the probability of the
296:02 - event so that means that the probability
296:06 - of e itself which is what we actually
296:08 - want to compute a probability of
296:11 - that's going to be 1 minus the
296:13 - probability
296:14 - of e complement because e is the
296:17 - complement of
296:18 - e complement so and that's going to be
296:21 - 1 minus 1 over 9 which is
296:24 - 8 over 9. there we go
296:28 - we've we've solved it and that's much
296:31 - easier
296:32 - than if we tried to do it directly and
296:34 - this is one reason why you
296:35 - care about uh these uh complementation
296:37 - rules because sometimes
296:39 - it's easier work to work with the
296:40 - complement of an event rather than the
296:41 - event itself
296:43 - if we had tried to work with that event
296:44 - uh we would have ended up with a sample
296:46 - space with uh
296:47 - or an event with um
296:50 - 32 elements or or a size of 32 so we
296:54 - would have ended up having to count 32
296:56 - things
296:57 - i mean it's not like impossible to count
296:58 - 32 things but it's also a lot more work
297:01 - so so this is a very good trick to have
297:05 - and something that you should be looking
297:07 - out for when you're doing your own work
297:09 - you should be looking out for situations
297:11 - where the compliment is actually easier
297:12 - to work with
297:13 - than the actual event itself okay
297:17 - uh just real quick uh satisfy my
297:19 - nervousness
297:21 - all right we're still streaming all
297:23 - right so we shall continue
297:26 - example 13 reconsider the experiment of
297:29 - flipping a coin until heads is seen ooh
297:31 - this one this one's going to get rather
297:34 - involved
297:35 - what is one way to assign probabilities
297:37 - to all
297:38 - outcomes of this experiment so that we
297:41 - have a legal probability model
297:43 - justify your answer so
297:46 - how could we possibly do this because
297:50 - we no longer can say that each
297:54 - outcome in the sample space is equally
297:56 - likely because that only works
297:58 - when the sample space is finite but as i
298:01 - mentioned before
298:02 - this sample space is infinite um there's
298:05 - an infinite number
298:06 - of outcomes in the sample space since
298:08 - it's basically the integers in a way
298:10 - is you can map the sample space to the
298:12 - integers and kind of identify it with
298:13 - the integers
298:14 - or not the integers but the natural
298:16 - numbers
298:17 - and in fact i think that's what we
298:19 - should do we
298:21 - should uh try to view this sample space
298:25 - in terms of the natural numbers i'm
298:28 - going to want to zoom in for this one
298:31 - zooming in for me is a way to get to
298:34 - almost get more room
298:35 - on this piece of paper so
298:39 - we're going to say um
298:42 - we're going to define n of omega
298:46 - so omega is an element of this sample
298:50 - space
298:51 - so this is going to be one of those
298:53 - strings heads tails heads tails tails
298:55 - heads
298:56 - tails tails tails heads and so on so
298:59 - we'll say that um
299:02 - n of omega is
299:06 - going to equal the length
299:11 - of the string
299:17 - omega so for example
299:21 - uh n of the string t t
299:24 - h this is a string of length
299:27 - three
299:31 - it then follows that um using abusing
299:34 - notation a little bit because
299:36 - n is a function that doesn't take sets
299:39 - as inputs
299:40 - but let's suppose for a second that we
299:42 - were to put a set as the input this is
299:44 - actually pretty commonly done
299:45 - uh often when you have a function and
299:47 - you plug in
299:48 - uh its domain what you're talking what
299:51 - you're
299:52 - actually referring to is a set
299:53 - representing the range of that function
299:56 - so in this case uh the range of the
300:00 - function
300:01 - n is going to be the natural numbers
300:04 - it's going to be one
300:06 - two three four and
300:10 - so on these are all the possible
300:12 - outcomes it's going to be the natural
300:14 - numbers
300:15 - okay so um
300:19 - i'm going to suggest that what we should
300:22 - do
300:22 - for our probability assignment is say
300:25 - that the probability of an outcome omega
300:28 - is equal to uh
300:32 - one-half to the power
300:35 - of uh n of omega so in other words it's
300:39 - going to be one half to the power of the
300:40 - length of the string omega
300:43 - so for example in the in this earlier
300:45 - case where we had t
300:46 - t h the probability of that outcome
300:49 - tth would be uh one half to the power of
300:53 - the length of the string tth or one half
300:54 - to the power of three
300:55 - which is one over eight okay
301:01 - all right then so this is a suggestion
301:03 - for what the probability should be
301:06 - but now we need to make sure that this
301:08 - is a valid probability measure
301:10 - so what is what needs to be true in
301:12 - order for this to be the case
301:14 - first off is the probability of um
301:17 - are all probabilities greater than or
301:18 - equal to zero under this method yes
301:20 - that is certainly true because there's
301:22 - no way that this function will produce
301:23 - negative numbers
301:24 - uh secondly uh is the probability of the
301:27 - sample space
301:28 - equal to one uh well that's actually
301:31 - something that we're probably going to
301:32 - have to check
301:33 - and then there's that third axiom about
301:35 - um the probability of a
301:38 - excuse me uh about the probability of
301:40 - unions of uh
301:41 - disjoint events and i'm not going to
301:43 - check that one because that one's
301:45 - actually getting rather complicated
301:46 - that's getting even more theoretical a
301:49 - bit too theoretical for this class
301:51 - but i personally think it's perfectly
301:53 - appropriate to check
301:55 - that under this probability model the
301:57 - probability of the sample space is equal
301:58 - to one
302:00 - um and in fact in my classes
302:04 - this is actually something that i love
302:06 - to ask
302:07 - questions about on quizzes i love to ask
302:11 - students
302:12 - that the probability of the sample space
302:14 - is equal to one
302:15 - under some probability model i love
302:18 - asking that
302:20 - so if you are in my class you should
302:23 - expect a question like that
302:24 - you should expect me to ask you to show
302:28 - that the probability of the sample space
302:29 - is one under some
302:32 - potential probability measure or
302:34 - alternatively
302:35 - very very similarly i might ask you
302:39 - to i might give you a probably measure
302:41 - but it depends on some
302:42 - unknown constant and i might ask you
302:45 - to compute what the constant is that
302:48 - causes this
302:49 - the probability measure to be a valid
302:50 - probability measure that is the
302:52 - probability of the sample space would be
302:53 - one
302:54 - under that measure all right so so this
302:56 - is something to look out for if you're
302:58 - actually taking classes from me
302:59 - uh but let's uh get back to the issue at
303:02 - hand i need to show
303:03 - that the probability of the sample space
303:06 - under this probability model
303:08 - is in fact equal to one so the
303:10 - probability of the sample space here
303:12 - is going to be the pr is going to end up
303:14 - being the sum
303:18 - um i i can basically view the sample
303:21 - space
303:22 - as consisting of you of
303:25 - as being the union of all of these
303:28 - of events uh con where each of these
303:31 - events contains
303:32 - uh one of these uh strings h t t
303:36 - h t t t t h something like that uh and
303:39 - this
303:39 - is actually a situation where that third
303:42 - form of
303:43 - where the way i wrote down axiom three
303:46 - uh before you really actually need it
303:49 - the way
303:50 - i wrote it down originally in like the
303:52 - body of the lecture notes rather than
303:54 - that
303:54 - footnote because we actually do in fact
303:57 - here
303:58 - have an infinite collection of events so
304:01 - i'm going to write this is going to be
304:03 - the sum over all the
304:05 - all of the omega that's in the sample
304:07 - space of the probability
304:09 - of that omega right so remember what
304:13 - this is actually doing is summing up the
304:14 - probability of
304:15 - h the probability of th the probability
304:18 - of t
304:18 - t h the probability of t t t h and so on
304:22 - all right uh continuing on uh we can
304:25 - then say
304:26 - that this is uh equal to
304:30 - um the sum over all
304:33 - omega in the sample space
304:36 - uh we have our probability assignment
304:39 - this is one half
304:41 - to the power of uh n
304:44 - of omega and at this point i'm just
304:47 - going to say that n of omega
304:49 - is equal to the length that sample is
304:50 - equal to length that string
304:52 - i can simplify what i'm writing down
304:55 - here a little bit
304:56 - by basically writing down uh what the
304:59 - image
305:00 - of n is uh under
305:03 - uh uh over the set s so i can now write
305:07 - equivalently
305:09 - that this is going to be the sum when uh
305:12 - n equals one to infinity
305:16 - of uh one half
305:19 - to the power n and now you should
305:22 - recognize from i think they talk about
305:24 - this in math 1010 at the university of
305:26 - utah
305:27 - intermediate algebra that this is a
305:30 - geometric cell
305:31 - this is a geometric sum and there is a
305:34 - formula
305:35 - for finding uh the value
305:38 - of of a geometric sum so recall
305:42 - this formula from your previous classes
305:46 - you have a a number r such that
305:50 - uh the the magnitude of r does not
305:53 - exceed one
305:54 - then the sum from uh
305:58 - n equals uh we'll say 1
306:01 - to infinity of r to the power n
306:04 - is equal to r over 1 minus
306:08 - r remember that well we're going to use
306:11 - that
306:12 - here right now and we're going to say
306:14 - that this sum
306:15 - is equal to one-half over one minus
306:18 - one-half
306:19 - which is equal to one-half divided by
306:22 - one-half
306:23 - which is equal to one which is what we
306:26 - wanted to show
306:27 - we have now shown that the probability
306:30 - of the sample space
306:31 - under this probability measure is equal
306:33 - to one all right hopefully you have
306:35 - written down this formula if you don't
306:37 - remember it
306:39 - because i need to reclaim that space now
306:42 - that we have this
306:43 - we can now start answering some
306:44 - questions now that we know that this is
306:46 - valid probability model we can start
306:47 - using it
306:48 - uh so under this model what is the
306:50 - probability that the number of flips
306:52 - needed to see the first head uh exceeds
306:55 - four
306:56 - so what i'm asking for is the
306:57 - probability
306:59 - of um i'm gonna have to write this
307:03 - in a somewhat funny looking way go away
307:06 - stop
307:06 - stop bothering me i have to write this
307:08 - in a somewhat funny looking way
307:10 - i'm going to say that this is the
307:11 - probability of drawing an omega from the
307:14 - sample space
307:15 - uh or or drawing a sequence of flips
307:18 - such that um the length of that sequence
307:23 - is greater than four
307:26 - and this is another one of those
307:27 - situations where it's actually easier to
307:29 - work with the complement
307:30 - and say that this is equal to one minus
307:33 - the probability
307:34 - of drawing a string of flips
307:37 - such that the length of that string
307:41 - is less than or equal to four okay
307:44 - this is actually a finite set because we
307:47 - can actually list out the strings of
307:48 - flips
307:49 - in which uh
307:50 - [Music]
307:52 - in which the length of string is less
307:54 - than or equal to four we have heads
307:56 - we have tails heads we have tails tails
307:58 - heads and tails tails tails heads
308:01 - so this is one minus the probability no
308:03 - one minus
308:05 - uh the probability that you get heads on
308:07 - the first flip
308:08 - plus the probability that you get tails
308:11 - heads
308:12 - uh first tails then heads uh plus the
308:14 - probability
308:15 - they get tails and tails and heads and
308:18 - then you have
308:19 - the probability of three tails
308:24 - and a head
308:27 - and we can in fact figure out uh what
308:30 - each of those probabilities are
308:31 - the first probability is going to be one
308:34 - half
308:34 - the second is going to be one half to
308:37 - the power two or one fourth
308:38 - the third is going to be one half to the
308:40 - power of three or one eighth
308:42 - uh 1 8 and the last one is going to be
308:45 - one half to the power of 4
308:46 - or 1 16 and long story short
308:51 - uh this is going to be equal to
308:54 - 1 minus 15 over
308:57 - 16 which is equal to
309:01 - 1 over 16.
309:05 - which is kind of funny it's a little
309:08 - funny to think about that
309:09 - why is it that it is equal to 1 over 16.
309:12 - hmm
309:13 - well here's kind of another way you
309:15 - could think about it
309:17 - um well it'll make more sense when you
309:21 - get
309:21 - when we start talking about uh
309:24 - independence
309:25 - but when thinking about independence
309:28 - what you end up doing is
309:29 - like we know that if a coin is equally
309:32 - likely to get heads and tails
309:34 - then we know that uh the probability of
309:37 - getting
309:38 - heads is one half so it's so what would
309:41 - then be the probability
309:43 - of getting four tails in a row
309:47 - um because that we must get at least
309:50 - four tails in a row
309:52 - in order for us to have to have at least
309:55 - four
309:55 - flips in order for us to get ahead so
309:58 - what we're actually asking for is the
310:00 - probability of getting four tails in a
310:02 - row
310:02 - so what i notice is well let's see it's
310:05 - if we were to think about
310:06 - um getting two tails in a row it seems
310:09 - like the probability of getting two
310:10 - tails in a row
310:11 - is one fourth and three tails in a row
310:14 - is one eighth
310:15 - um and then four tails of a row that's
310:18 - one over sixteenth
310:19 - hmm so that's an alternative way to
310:21 - think about what's going on here
310:23 - uh but there's an algebraic way to get
310:26 - it
310:27 - to an algebraic way to get the answer
310:29 - all right uh
310:30 - the second part what is the probability
310:32 - the number of the number of flips until
310:34 - the experiment
310:35 - ends in other words the last flip will
310:37 - be heads is between
310:39 - 3 and 20. this one is a little bit more
310:43 - painful
310:45 - but actually maybe we could use that
310:48 - trick
310:51 - well let's see uh we what we could
310:54 - potentially do
310:56 - is say um
311:00 - that the probability of observing a
311:03 - sequence of flips
311:05 - such that the length of that sequence um
311:08 - is exactly less than three
311:13 - well that's going to be uh so that's
311:16 - going to be the sequences where the
311:18 - length of the sequence is one or two
311:20 - so that's going to be heads or tails
311:22 - heads and that's going to be one-half
311:23 - plus one-fourth
311:24 - which is uh three-fourths
311:27 - um now let's compute the probability
311:32 - of observing a sequence
311:36 - of flips where the length of that
311:38 - sequence
311:40 - is greater than 20.
311:47 - now this is not so far computing the
311:49 - probability
311:51 - of uh being between inclusively 3 and
311:54 - 20.
311:55 - but the reason why i'm computing these
311:57 - numbers is because i'm actually thinking
311:59 - i might want to use
312:01 - that complement trick again so i want to
312:03 - compute this probability
312:05 - where the length of the sequence is
312:08 - at least no is strictly greater than 20
312:12 - and i could actually use the arguments
312:14 - that i was using above to argue that
312:16 - this is going to be a sum
312:18 - from n equals uh 21
312:21 - to infinity of one-half
312:24 - to the power n all right it's basically
312:28 - the same arguments that i was using
312:29 - before
312:30 - uh to compute the size of the sample
312:32 - space because this is going to be
312:34 - uh computing so you have the sum of the
312:36 - probably when you have 21 flips and then
312:38 - probably when you have 22 flips and 23
312:41 - flips and so on
312:42 - and i could say
312:45 - [Music]
312:46 - what should i do next the thing is
312:48 - unfortunately i don't have a formula
312:51 - for when we're adding up starting at 21.
312:55 - i do have a formula when we add up
312:56 - starting at 1.
312:58 - so maybe what we could do is try to find
313:01 - a sneaky way
313:03 - to uh try adding up at 1 again
313:07 - what i could potentially do is say
313:11 - that n minus 20 plus 20
313:15 - i could say that this is equal to
313:18 - the sum
313:22 - n minus 20 is equal to
313:26 - uh one
313:30 - because that's equivalent to saying that
313:33 - n is equal to 21 right
313:36 - certainly and then i could say this is
313:39 - one half
313:40 - uh to the power uh n minus 20
313:44 - uh plus 20. hmm
313:46 - [Music]
313:48 - well i know i remember uh now
313:52 - that uh we can actually write this
313:55 - as uh we can write this as uh
314:00 - or the region the part enclosed in red
314:04 - as uh one-half to the power
314:07 - n minus 20 multiplied
314:10 - with one-half to the power 20. that's
314:13 - equivalent
314:14 - okay um so what can i
314:17 - use with that well i can now
314:20 - say that this is equal
314:23 - oops that this is equal to
314:27 - uh one half to the power oh i don't want
314:30 - red
314:31 - one half to the power 20 since that is
314:34 - effectively a constant that doesn't
314:35 - depend on
314:36 - n and i have a sum from
314:39 - n minus 20 equals one
314:43 - up to infinity of one-half
314:46 - to the power n minus 20. uh
314:50 - but what i could do is say well
314:53 - n minus 20 uh you know what i'm just
314:56 - going to call that i
314:57 - i'm just going to call n minus 20 i'm
314:59 - just going to say that's i so replace
315:02 - all the n minus 20's
315:04 - with i so i equals 1 to infinity
315:08 - 1 half to the power i oh i know how to
315:10 - compute this
315:11 - i know how to compute this this is going
315:14 - to be
315:15 - uh one half times uh
315:18 - one half divided by one minus one half
315:20 - so actually this part right here
315:22 - i actually in fact this right here is
315:24 - pretty much a probability model
315:26 - uh this is summing over the sample space
315:28 - so this
315:29 - this part this blue part is equal to one
315:34 - so that means that this probability is
315:37 - equal to um
315:41 - is uh equal to one half to the power 20.
315:46 - think about that uh once again
315:49 - going back to this probability uh
315:52 - the probability of getting a sample of
315:54 - getting a string of at least 20 means
315:55 - that you must have
315:56 - flipped at least 20 tails and i've
315:59 - competed effectively the probability of
316:01 - flipping of getting 20 tails in a row
316:04 - oh all right now uh i'm still not done
316:08 - though
316:08 - uh let's uh start collecting this
316:10 - information
316:12 - um i now know
316:16 - uh let's see that uh so i'm gonna
316:19 - start erasing all this because i need to
316:20 - start reclaiming some space
316:24 - uh
316:29 - all right so uh this is equal to
316:33 - um
316:36 - uh one half to the power 20.
316:40 - all right so we can now say that the
316:43 - probability
316:45 - that uh of uh observing a sequence of
316:48 - flips
316:49 - such that three is less than or equal to
316:53 - the length of that sequence
316:55 - which is less than or equal to 20. i
316:57 - could work
316:58 - instead with the complement of this and
317:00 - say this is one minus the probability
317:02 - of drawing an omega such that either
317:06 - n of omega is less than three
317:09 - or n of omega
317:12 - is greater than 20. and here's the thing
317:14 - though i've basically written down
317:17 - two disjoint sets one set where n of
317:19 - omega is less than three
317:21 - or one set that is great where n of
317:23 - omega is greater than 20.
317:25 - um so so either a sequence ha is less
317:28 - than three flips or more than twenty
317:29 - twenty flips
317:30 - but it's not both so this is actually
317:32 - the union of two disjoint events
317:34 - and therefore i can say this is 1 minus
317:37 - the probability
317:39 - of drawing an omega such that
317:42 - n of omega is less than 3
317:46 - plus the probability of
317:50 - drawing an omega such that uh
317:54 - the length of the string omega is
317:56 - greater than 20.
317:57 - and i have computed both of those
317:58 - probabilities so this is equal to
318:01 - uh 1 minus three-fourths
318:07 - uh plus one-half to the power
318:10 - uh 20 and i'm just going to leave it at
318:12 - that i'm not going to bother
318:14 - uh trying to simplify this because this
318:16 - is the correct answer
318:17 - so we're done at this point i'm not
318:19 - gonna go any further
318:20 - um i'm going to that said reclaim all
318:23 - this space
318:25 - and say um
318:28 - so hopefully you can rewind if you
318:31 - missed some of that
318:32 - and catch up on what you missed because
318:34 - i'm erasing this right now
318:36 - um and we're going to say
318:40 - that the probability of this event e
318:43 - is equal to 1 minus three fourths
318:47 - plus um uh
318:50 - one half to the power twenty
318:54 - all right uh next part what is oh
318:58 - yeah i don't want this this business all
319:00 - right uh what is the probability that an
319:02 - even number of flips is seen before the
319:05 - experiment ends
319:06 - hmm uh an even number of flips how could
319:09 - i possibly think about that
319:11 - uh so i want the probability
319:14 - of an e
319:18 - probability of an even number
319:22 - of uh flips
319:27 - and in short using some of the reasoning
319:29 - that i was using above
319:31 - i could say that this is the sum from
319:34 - of a one-half how would i represent an
319:37 - even number
319:38 - i could say that an even number is 2
319:41 - times a natural number that guarantees
319:43 - that the number the number is even so
319:45 - just take any natural number you want
319:47 - and multiply it by 2 and that's going to
319:49 - be an even number of fact that's going
319:50 - to cover all the even numbers
319:51 - and i'm going to sum this up from m
319:53 - equals 1
319:55 - to infinity
319:58 - now what i do next well what i could do
320:01 - is
320:01 - recognize that i can basically take the
320:04 - m out and say that this is one half
320:06 - squared
320:06 - raised to the power m so that this is
320:09 - equal to
320:10 - the sum uh when m equals 1
320:13 - to infinity of 1 4
320:17 - to the power m oh i know how to sum that
320:21 - that's going to be since this is a
320:23 - geometric sum
320:25 - 1 4 over 1 minus 1 4
320:29 - which is equal to 1 4 over
320:32 - 3 4 and those
320:36 - 1 4 parts cancel out so this ends up
320:38 - being
320:40 - one-third so the probability of an
320:43 - even number of flips is one-third
320:47 - which is kind of funny to think about if
320:48 - you think about it it seems like it like
320:50 - an even number of flips and an odd
320:52 - number of flips is equal
320:53 - equally likely but actually no they're
320:55 - not equally likely at all
320:56 - it's much more likely to have an odd
320:57 - number of flips since it's much more
320:59 - likely to get one
321:01 - one flip exactly
321:04 - okay um in fact getting one flip is
321:07 - twice as likely as getting two flips
321:09 - so by that reasoning it actually makes
321:11 - perfect sense
321:13 - okay uh moving on to the next part that
321:16 - was all a lot of work
321:18 - that was a lot of work okay uh
321:21 - example 14. in a small town
321:25 - uh 20 of the population is considered
321:28 - wealthy
321:28 - 30 of the population identifies as black
321:31 - and 5
321:32 - of the population is wealthy and black
321:34 - select a random individual from this
321:36 - population everyone equally likely to be
321:38 - selected but i don't think that actually
321:39 - matters to this problem so much because
321:41 - of how i've laid it out what is the
321:43 - probability that an individual
321:45 - is wealthy and not black
321:48 - so let's see let me catch up in my notes
321:53 - here's how i like to think about this
321:55 - one with problems like this where you
321:56 - have
321:57 - uh let's let's start out by saying that
321:59 - we've got uh
322:02 - we've got a we've we've got a sample
322:04 - space but i'm not going to think too
322:05 - hard about what the sample space is for
322:07 - this one
322:08 - i'm just going to say we have an event b
322:10 - which
322:11 - uh corresponds to an individual being
322:14 - black
322:16 - or uh racially identifying as black
322:19 - uh w will be the event that a person
322:23 - identifies as no not identifies uh
322:26 - probably this is probably like some sort
322:28 - of a census business so
322:30 - the census bureau has maybe may have
322:32 - some uh
322:33 - more technical definition of wealthy
322:35 - like they make over ninety thousand
322:37 - is making over ninety thousand rendering
322:39 - you wealthy well you're certainly well
322:41 - off but um anyway that stuff aside
322:44 - we'll just say w that denotes the event
322:47 - that you are wealthy
322:48 - by some uh criterion and uh the
322:51 - probability of b
322:53 - that an individual is a is a black
322:57 - uh will be 0.3 and the probability
323:00 - of w that this individual is wealthy is
323:03 - going to be
323:03 - 0.2 okay and that's just true because
323:07 - the problem said so
323:09 - so now what we want to well actually
323:11 - we're also given another thing that the
323:13 - probability of an individual being both
323:14 - wealthy
323:15 - and black uh is going to be
323:19 - uh 0.05
323:23 - oh by the way here's the thing to keep
323:25 - in mind
323:26 - um the probability of a
323:30 - and b is always going to be less than or
323:33 - equal to the probability of a
323:35 - or if you want you can replace the
323:36 - probability of a with probability of b
323:38 - it's just as true
323:39 - and that's because a and b is certainly
323:42 - a subset
323:43 - of a and if you think about
323:47 - um if you think about what about
323:51 - probabilities in terms of areas the area
323:53 - of a subset is certainly less than the
323:55 - less than or equal to the area of the
323:57 - thing it is it is uh
323:58 - nested within so you're always going to
324:01 - have a decrease in area it's not
324:02 - possible
324:03 - despite some human cognitive biases
324:07 - for the probability of a and b
324:10 - or something being true and something
324:12 - else also being true
324:14 - it is not possible for that to exceed
324:16 - the probability of the original thing is
324:17 - true
324:18 - i think people sometimes confuse uh
324:21 - intersection with conditional
324:23 - probability um that's probably what is
324:25 - going on
324:26 - when p when you start seeing those uh
324:28 - studies talk about stuff like that
324:29 - anyway uh we need to figure out uh what
324:32 - is the probability that this indiv
324:34 - that an individual selected from this uh
324:36 - from this uh village
324:38 - is wealthy and not black
324:42 - and here's how i like to solve problems
324:44 - like this and let's
324:46 - make it clearer all right
324:49 - here's how i like to solve problems like
324:51 - this i like to create a venn diagram
324:54 - and in that venn diagram i will start
324:58 - filling out regions so we'll have the
325:00 - wealthy circle and the black circle
325:03 - and we'll have this and we have the
325:05 - sample space
325:06 - uh and uh you start thinking about this
325:08 - like a puzzle
325:10 - like we know that the wealthy and black
325:12 - part is 0.05 and we're going to need to
325:14 - zoom in some more the wealthy and black
325:18 - part
325:19 - that is going to be 0.05
325:23 - okay and then we have the wealthy part
325:25 - which is 0.2 but we can't point
325:27 - put 0.2 here because that 0.2 is also
325:30 - including that 0.05
325:32 - so we need to put here is 0.15 so that
325:35 - when we add
325:36 - uh the this blue region and this red
325:40 - region those need to add up to
325:42 - 0.2 okay now for the black part
325:46 - um that needs to add up to 0.3 so that
325:49 - means that the part in
325:51 - that is not in the intersection is set
325:53 - to be 0.25
325:54 - and i didn't write that very clearly um
325:57 - that needs to be point
325:59 - uh 0.25
326:04 - okay which for what it's worth we now
326:06 - know the probability of w
326:08 - or b which would be 0.45
326:11 - which means that the probability of
326:12 - being neither wealthy nor black is going
326:15 - to be 0.55
326:19 - so that's something that i recommend
326:20 - when you encounter problems like these
326:22 - create a venn diagram and then fill out
326:24 - the diagram
326:25 - figure out the probability of every
326:27 - single little sliver
326:28 - in that venn diagram and then you can
326:30 - get any probability you want
326:33 - all right um and it will it will be
326:37 - like a genuine a generally useful
326:40 - chart for you not just for that
326:42 - individual problem because often these
326:44 - uh
326:45 - problems come in bunches
326:48 - all right so uh continuing on the
326:50 - probability of being wealthy and not
326:52 - black
326:53 - well actually that corresponds in our
326:56 - little chart up here to the blue region
326:59 - which is going to be 0.15
327:01 - so this is going to be 0.15
327:08 - okay what is the probability that the
327:10 - individual is neither wealthy nor black
327:12 - actually i already computed that i set
327:13 - it in words it's going to be 0.55
327:20 - all right uh next example a ball
327:23 - contains
327:24 - a bag contains balls and blocks thirty
327:27 - percent of the bag's contents are balls
327:29 - an object is either red or blue and
327:31 - forty percent of the objects are red
327:32 - an object is made of either wood or
327:34 - plastic and sixty five percent of the
327:35 - objects are wooden
327:37 - ten percent of the objects are wooden
327:38 - balls five percent of the objects are
327:39 - red balls and twenty percent of the
327:41 - objects are red and plastic
327:42 - two percent of the objects are red
327:45 - plastic
327:46 - uh uh there's a typo here uh
327:49 - we're going to need to change blocks
327:53 - to balls
327:57 - okay reach into the bag and pick out an
328:00 - object at random
328:02 - each object equally likely to be
328:03 - selected what is the probability that
328:06 - the object selected is a ball
328:08 - red or wooden and this is inclusive by
328:11 - the way
328:12 - well this is another one of those
328:13 - problems where what i suggest you do
328:15 - is you create a venn diagram to
328:18 - represent the situation
328:20 - and fill out all of the little parts of
328:22 - that venn diagram
328:24 - so what does that look like here um
328:28 - all right so i've got my venn diagram
328:31 - this is my sample space
328:34 - uh right so i've got giant circles but i
328:38 - should probably be a bit more precise
328:40 - about uh let's start before i start
328:43 - filling out this chart
328:44 - uh let's create some notation and fill
328:47 - out what we already know
328:48 - in mathematical notation so we've got
328:51 - balls
328:52 - so b will be the event that we grab a
328:55 - ball
328:57 - and we'll say so objects are either red
328:59 - or blue
329:00 - so we'll say r is the event that an
329:04 - object pulled out of here is red so
329:07 - b complement is going to be a block and
329:09 - our complement is going to be
329:11 - a blue object and
329:14 - we say that so we can either have
329:17 - wood or plastic so we'll have a w
329:21 - correspond uh to the event that you get
329:24 - a wooden object
329:28 - all right uh let's start collecting some
329:30 - more information
329:32 - uh we have that the probability of uh
329:34 - drawing a
329:36 - stop it uh we have the probability of
329:38 - drawing a ball so the probability of the
329:40 - event b
329:41 - is equal to 0.3
329:46 - the probability of getting a red object
329:49 - is a 0.4
329:53 - the probability of a wooden object
329:59 - probably of a wooden object is going to
330:01 - be 0.65
330:05 - uh the probability that an
330:08 - object is a wooden ball well that's w
330:11 - intersected with b
330:13 - since the object is both wooden and a
330:15 - ball that
330:16 - is going to be point one uh
330:19 - the probability of getting a red ball
330:23 - uh a red ball
330:27 - is going to be 0.05
330:31 - the probability of getting
330:34 - uh something that's red and plastic
330:37 - so that's r and w complement
330:43 - that's going to be 0.2
330:46 - and finally we have
330:51 - the probability of drawing
330:55 - a red
330:58 - uh plastic so w complement
331:03 - a block
331:07 - no no not a block it's no block it's a
331:08 - ball this is equal to 0.02
331:14 - okay so so so so
331:18 - uh we need to start filling out this
331:21 - chart we're going to have uh we're going
331:24 - to have red here
331:25 - we're going to have balls here and we're
331:27 - going to have
331:28 - wooden objects over here uh i always
331:31 - recommend
331:31 - starting out with the smallest region
331:34 - first
331:35 - so the greatest intersection start out
331:37 - with that and then fill outwards
331:39 - so in this case the smallest region is
331:42 - red plastic balls uh where is that
331:46 - on our venn diagram uh you are
331:50 - in r you're in b but you're not in w so
331:53 - that actually corresponds to the blue
331:55 - region
331:56 - okay so the probability of the blue
331:58 - region according to the problem is 0.02
332:02 - all right uh next up we could
332:05 - probably ask for uh what is the
332:08 - probability
332:09 - of being red and plastic uh red and
332:11 - plastic
332:13 - uh that corresponds to being
332:16 - um let's see so you're in the red
332:19 - region uh but you're not in the w
332:23 - region so that's going to correspond to
332:25 - this blue area
332:27 - and we know that the probability of
332:31 - being red and plastic is 0.2 but we've
332:34 - already got a 0.02
332:36 - so that means that that in the other
332:39 - part we're going to be left with 0.18
332:44 - so the probability of being a red
332:46 - plastic block
332:47 - is going to be 0.18 all right uh
332:51 - how about next uh we've got
332:54 - uh the probability of being
332:57 - a red ball so a problem the probability
333:01 - of being
333:03 - a red ball is going to be .05 what
333:06 - corresponds to
333:07 - red balls well this is the ball so we
333:10 - have the balls region and we have
333:11 - red stuff so that's going to be
333:14 - corresponding to the red region
333:16 - okay and that is 0.05 which means
333:20 - we've already got a 0.02 here it needs
333:21 - to add at 0.05 so that means that this
333:23 - little sliver in the middle
333:25 - is going to be 0.03 okay
333:28 - uh let's see we need next uh
333:32 - wooden balls so wooden balls corresponds
333:35 - to this green region
333:36 - uh the probability of being a wooden
333:38 - ball is point
333:39 - one so that we've already got a 0.03 so
333:43 - that means that
333:44 - we've got 0.07 in this other little
333:46 - sliver
333:49 - all right we're getting close to done uh
333:51 - what about
333:55 - uh let's see so we've got uh wooden
333:58 - balls
333:59 - red balls red plastic things
334:03 - uh what else have we got well we can
334:06 - we've got the probability of balls the
334:07 - probably balls is gonna be
334:08 - point uh 0.3 and
334:12 - that so balls correspond to this blue
334:14 - region
334:15 - and we've already got um
334:18 - 0.12 of the area accounted for
334:22 - the total area needs to be three point
334:24 - three so that means that
334:25 - for this uh remaining sliver uh we have
334:29 - point one eight for its area
334:32 - okay uh next
334:36 - up next up
334:40 - well we've got um the probability of red
334:43 - stuff
334:44 - and we know that the probability of red
334:45 - stuff is point four
334:47 - so point two so red stuff is going to be
334:51 - uh the region i've just encircled in red
334:54 - and
334:54 - so far 0.23 of that area is accounted
334:58 - for
334:59 - but point its total area is going to be
335:01 - 0.4 so that means that
335:03 - only so that means that when we're
335:06 - filling out
335:07 - uh this remaining blue sliver it must be
335:10 - 0.17
335:14 - okay and for wooden stuff we know that
335:16 - the total area in the wooden region
335:18 - it's going to be 0.65 and
335:22 - 0.27 of that area is already accounted
335:26 - for so that means that the remaining
335:29 - area is going to be 0.38
335:33 - all right and we are almost done we
335:35 - actually in fact know the probability of
335:37 - everything else
335:38 - if you were to add up all of those
335:39 - probabilities they would add up to 0.98
335:41 - so that means that the remaining area
335:43 - that is outside of this region is 0.02
335:46 - all right we have filled out that
335:48 - diagram and we are ready to answer some
335:50 - questions
335:51 - what is the probability that an object
335:52 - selected is a ball
335:54 - red or wooden so that's the probability
335:58 - of r or w
336:01 - or b and we have already figured that
336:04 - out
336:05 - that corresponds uh to the 0.02
336:10 - so that is equal to
336:15 - 0.02
336:19 - okay uh next up what is the probability
336:21 - that the object
336:22 - is a red wooden ball uh so the
336:25 - probability of being
336:28 - red
336:31 - of being red and wooden and a ball
336:35 - what is that going to be well we're
336:37 - going to go back to our diagram that we
336:39 - created
336:40 - we look for red wooden balls so
336:44 - you need to be in the red circle you
336:47 - need to be
336:49 - uh wooden so you need to be in the blue
336:50 - circle and you need to be a ball so you
336:52 - need to be in the green circle
336:54 - uh oh i guess that's a darker blue so
336:56 - that leaves us 0.03
337:02 - so that leaves us 0.03 for that for uh
337:05 - that area so this is going to be 0.03
337:12 - all right uh what is the probability
337:14 - let's go ahead and do some zooming out
337:16 - we no longer
337:17 - need such fine control when drawing uh
337:20 - what is the probability that an object
337:21 - is a blue plastic block
337:23 - well the probability of being a
337:26 - blue which is the complement of red
337:30 - uh not in a color sense not like
337:33 - when artists talk about complements
337:34 - that's a red is not a complement of blue
337:37 - but
337:37 - whatever in a probabilistic sense in our
337:40 - probability model
337:41 - uh so that's the reason that we've drawn
337:43 - here and it's actually a problem
337:45 - i'm gonna leave this to yourself but
337:47 - look up de morgan's laws
337:49 - uh learn about de morgan's laws uh for
337:52 - my students this is an assignment
337:54 - um i believe if i remember right but
337:57 - basically
337:57 - this is r union w
338:01 - union b complement
338:05 - which then means it's 1 minus the
338:08 - probability
338:09 - of r union w union b
338:13 - and that is a probability you've already
338:14 - figured out that's 1 minus 0.98
338:19 - which is equal to 0.02
338:24 - okay
338:30 - okay so that's it for the examples those
338:33 - are a lot of examples
338:34 - but i'm just gonna we're now wrapping up
338:38 - this section
338:38 - and i'm going to wrap up this section
338:41 - with a
338:42 - short discussion on the interpretation
338:45 - of
338:45 - probability now it turns out this is
338:48 - actually a rather large topic
338:50 - philosophers actually are debating uh
338:54 - what do probabilities actually mean
338:58 - [Music]
338:59 - and what is an appropriate definition
339:03 - of uh of uh of what a probability is
339:06 - because we have a mathematical notion
339:09 - of probability but that doesn't
339:10 - necessarily translate into a real-world
339:12 - notion of probability
339:13 - now i'm going to leave that discussion
339:15 - aside i might make an aside video
339:18 - about the interpretation of probability
339:21 - but in this class despite the
339:23 - limitations of this interpretation
339:26 - we're going to adopt the frequentest
339:27 - interpretation of probability
339:29 - which is interpreting probabilities as
339:31 - long run frequencies of events
339:33 - so in other words if we were to repeat
339:35 - an event an experiment many many many
339:36 - many times
339:37 - and each of those repetitions were
339:39 - independent of the past
339:41 - uh we would the
339:44 - sample proportion of times uh we would
339:47 - see that
339:48 - experiment occur would approach that
339:50 - probability
339:52 - so here's kind of and i here is a chart
339:56 - that kind of illustrates this idea
339:59 - we're going to have along the y-axis
340:04 - the y-axis ranges from 0 to 1
340:07 - and somewhere along here we have the
340:09 - probability of some event
340:11 - uh e uh
340:15 - stop being uncooperative tablet
340:20 - all right so we have the probability of
340:21 - an event e
340:23 - okay and uh along the x-axis well it's
340:27 - not really the x-axis this time it will
340:29 - be
340:29 - the n-axis and we're going to mark this
340:32 - axis
340:32 - off at integer values
340:37 - 1 2 3 4
340:41 - and so on and um
340:45 - uh we are going to track uh p
340:48 - hat n which is
340:51 - the sample proportion
340:58 - of times
341:01 - the event e occurs
341:05 - and you remember how to compute sample
341:07 - proportions uh but here's kind of the
341:09 - thing
341:10 - about what we're doing here uh we are
341:12 - remembering pat the past so you could
341:14 - imagine that we
341:15 - uh are flipping a coin and we're
341:17 - tracking how many times we see heads
341:20 - in this uh experiment and uh
341:24 - uh so the first time you flip it you're
341:26 - gonna have either one or zero heads
341:28 - and uh the second time you're going to
341:30 - keep the results of that first flip but
341:32 - then recompute your proportion for a
341:34 - sample size of two
341:35 - uh for the third time you then flip the
341:38 - coin a third time and you keep the
341:39 - previous two results
341:41 - uh and then recompute the proportion
341:43 - using those previous two results and
341:44 - also the third flip you just did
341:46 - uh and so on keep doing this in a
341:49 - sequence
341:50 - so uh what could possibly happen so i'm
341:53 - going to
341:54 - put a little dashed line at the
341:57 - probability of
341:58 - e according to the frequentist
342:00 - interpretation of probability
342:02 - uh the probability is basically this
342:03 - long-run frequency where
342:05 - if you're tracking p hat n you'll start
342:08 - out
342:08 - at um like maybe zero or one depending
342:12 - on whether the event happened on the
342:13 - first trial or not
342:14 - and then you might track that it happens
342:16 - on the second trial
342:18 - and the third trial and the fourth trial
342:20 - and so on and
342:21 - what will happen is this line
342:24 - will get very very close to the
342:26 - probability of e
342:28 - um and that's how frequencies think
342:31 - about probabilities as a long run
342:33 - proportion
342:33 - as a limiting proportion um
342:36 - now the there is an unfortunate thing
342:38 - about this uh interpretation which that
342:40 - actually
342:41 - in mathematical probability theory
342:44 - that is not an assumption that's a
342:46 - theorem so this is not something that we
342:48 - assume is true this is something we
342:50 - prove is true
342:51 - so why are we assuming that something is
342:53 - true that we then prove is true
342:55 - that that seems like a circular
342:57 - reasoning and it's unfortunate
342:59 - but i'm going to leave those
343:01 - philosophical issues aside
343:03 - uh because at some level my own personal
343:06 - belief is that
343:08 - all of those technical philosophical
343:09 - issues aside at some level it's almost
343:11 - a limitation of human language uh and
343:14 - probability theory despite
343:16 - our difficulty in coming up with a
343:17 - definition a proper definition for it
343:19 - uh is a very real phenomenon so whatever
343:23 - the definition is it matches
343:25 - this frequentist notion uh i've actually
343:27 - got some r code here that you can look
343:29 - at
343:30 - uh this functions r is able of capable
343:33 - of generating random numbers and set
343:35 - seed is what's known as sets what's
343:36 - known as a random seed
343:38 - and make sure that when you're
343:39 - generating random numbers they actually
343:41 - are going to come up with the exact same
343:42 - results uh
343:44 - the same time so if you run this code it
343:46 - will produce the exact same results
343:47 - uh at least in principle uh different
343:50 - versions of software may cause different
343:52 - results but
343:53 - uh whatever so i set the sample size for
343:56 - my experiments
343:58 - uh i conduct a number of flips
344:03 - i track uh i accumulatively track the
344:06 - number of heads in these flips
344:09 - and then i plot these uh sample
344:12 - proportions
344:13 - and also draw a line at the theoretical
344:16 - probability of that event
344:18 - so this is the case when you were to if
344:20 - you were to flip a coin
344:22 - flip 15 times and keep a running
344:23 - proportion of how many times you saw
344:25 - heads
344:26 - this is what it looks like um and you
344:28 - can kind of see it seems to be
344:29 - converging around 0.5
344:31 - if we repeat this experiment but this
344:33 - time when
344:35 - n is equal to 50 what we'll see is
344:38 - it kind of looks like it's getting even
344:40 - closer to something constant
344:42 - and then if we were to repeat this again
344:44 - but now uh
344:46 - the sample size is 500 it almost
344:49 - converges
344:51 - it almost converges to the truth now
344:54 - that said with probabilities it never
344:56 - means that
344:57 - you are guaranteed to see that many in
345:00 - any single sample you we did not
345:02 - say that uh which means that at the very
345:05 - end here you're not actually at one half
345:07 - you're pretty close to one half but
345:08 - you're not
345:09 - at one half but if you were to just keep
345:12 - going
345:12 - on like this and you were in fact
345:14 - possible for you to continue forever
345:16 - and god suddenly grants you the ability
345:19 - to live forever so long as you continue
345:21 - to flip this coin
345:23 - at the end of eternity you will have one
345:25 - half a flip uh
345:26 - half of your flips will be one half
345:28 - that's basically what we're saying
345:32 - all right so that's it for this video
345:34 - quite intense
345:35 - quite intense it's probably giving you
345:37 - some idea that probability can get
345:39 - rather out of hand um at least in terms
345:42 - of uh computational complexity
345:44 - uh but uh i've given you a number of
345:47 - useful examples and a number of useful
345:48 - techniques
345:49 - the thing though is those techniques
345:53 - like they're useful now and it's good
345:54 - for you to see them but you always kind
345:56 - of have to adapt to individual problems
345:59 - the next section is going back to the
346:01 - situation where
346:02 - you have a finite sample space we're
346:05 - going to talk about how you count
346:07 - elements from that sample space
346:09 - this part is both fun and frustrating
346:11 - because
346:12 - it's fun because we get to talk about
346:14 - things like poker problems and gambling
346:16 - and stuff like that
346:17 - but it's frustrating because counting is
346:19 - hard counting is really hard
346:21 - and also like i would love it to give
346:23 - you a magic formula
346:24 - that solves every and all counting
346:26 - problems and i will give you some
346:27 - formulas but those are kind of
346:29 - those are just tools that you kind of
346:31 - need to combine and the
346:32 - combination of those tools that's the
346:34 - hard part and i can't really give you a
346:37 - general principle for that
346:39 - every counting problem is kind of its
346:41 - own thing but enough of that we're
346:43 - we're calling it for now uh i will see
346:45 - you
346:46 - later in the future video on counting
346:54 - let's move on now to the section on
346:57 - counting techniques
346:58 - to start things off let's suppose that
347:02 - we have a burger shop that we're going
347:03 - to call
347:04 - and just off the top of our head bob's
347:06 - burgers
347:08 - and uh they're offering three types of
347:11 - bread
347:12 - we have white bread rye bread and
347:14 - sourdough
347:16 - a burger can come with or without cheese
347:18 - how many burgers
347:19 - are possible well let's see
347:23 - uh we have uh let's let's come up with
347:27 - some encoding for our possibilities
347:29 - uh we have white bread we have rye bread
347:34 - and we have a sourdough
347:37 - and a burger can come with or without
347:40 - cheese
347:42 - so we'll say we can have with cheese or
347:44 - we can have no
347:45 - cheese all right so given this encoding
347:49 - uh what are the possible burgers that we
347:51 - could have
347:53 - so we've got our possible burgers
347:57 - and we could have a burger with white
348:01 - bread and
348:02 - cheese a burger with rye bread and
348:05 - cheese and a burger with sourdough
348:06 - and cheese or a burger with white bread
348:09 - and
348:10 - no cheese a burger with rye bread and no
348:13 - cheese and a burger with sourdough and
348:15 - no cheat and
348:17 - no cheese so let's see how many burgers
348:20 - is that that's going to be
348:23 - six burgers
348:26 - uh so that's the way to do it by hand
348:29 - where you basically enumerate all the
348:31 - possibilities for
348:33 - uh how many burgers are going to be but
348:36 - the thing though is i mean that's only
348:39 - going to work for so long i mean
348:41 - you can you can enumerate stuff when
348:43 - it's possible to do and then literally
348:46 - count uh how many possibilities there
348:48 - are but there's also other ways to
348:50 - possibly represent what's going on here
348:52 - for example we could use what's known as
348:54 - a tree diagram
348:55 - with the t tree diagram we're going to
348:57 - create a tree
348:59 - that visualizes um
349:03 - the branching possibilities of our
349:05 - choices
349:06 - so for example at the first node we're
349:09 - going to choose the type of bread so we
349:10 - have white
349:11 - rye and sourdough and then
349:14 - at the second level of node we decide
349:16 - whether we're going to have
349:18 - cheese or no cheese so we have
349:22 - um for the upper branch
349:25 - cheese and for the lower branch no
349:27 - cheese and we're going to do this
349:29 - a few more times for the different types
349:31 - of bread that we could have chosen and
349:34 - we've got cheese no cheese
349:36 - cheese no cheese and then we're going to
349:40 - count how many no's there are on the end
349:43 - points
349:44 - and the number of nodes on the endpoints
349:46 - will correspond to the number
349:48 - of possible choices in this case there
349:50 - are six nodes
349:52 - so that means six possibilities
349:56 - and of course one thing that's nice
349:58 - about these types of tree diagrams
350:01 - is that we could allow for
350:05 - more flexibility in our possibilities
350:07 - such as
350:08 - perhaps for some reason bob has decided
350:11 - that he is not going to permit
350:13 - a sourdough bread without cheese in
350:16 - which case you just remove that
350:18 - possibility from
350:19 - from the from the tree diagram you
350:21 - remove that branch
350:22 - in which case there would now be five
350:24 - nodes in the branch
350:26 - so it's pretty general uh
350:30 - there's also something that we can use
350:32 - uh called the product rule
350:34 - uh proposition seven if there are n1
350:36 - possibilities for choice one and two
350:38 - possibilities for choice two
350:40 - all the way to nk choice of
350:42 - possibilities for choice k
350:43 - then the total number of possible
350:45 - combinations is going to be the product
350:47 - of the number of possible choices we can
350:49 - make at each point
350:50 - so uh let's use the product rule to
350:53 - answer this question
350:55 - according to the product rule
350:58 - we could have three possible choices for
351:02 - the first node
351:04 - and two possible choices for
351:07 - the second node and thus the total
351:10 - number of possibilities will be
351:12 - three times two which is equal to
351:15 - six all right so we've seen so far
351:18 - uh six ways to basically answer this
351:21 - question
351:22 - uh now let's uh move on some more
351:26 - uh the sandwich shop deluxe deli offers
351:29 - four bread
351:30 - options where we have white sourdough
351:32 - whole wheat
351:33 - and rye five meat options turkey ham
351:36 - beet
351:36 - beef chicken or no meat six cheese
351:40 - options cheddar white cheddar swiss
351:41 - american pepper jack and no cheese
351:43 - with or without lettuce with or without
351:45 - tomatoes whether without bacon with
351:46 - without mayonnaise and with without
351:47 - mustard
351:48 - how many sandwiches are possible let's
351:51 - see
351:51 - uh how many decisions do we have to make
351:54 - first we need to decide
351:56 - on our bread option so we'll say that
351:58 - that's uh decision one
352:00 - so we'll say that n1 there are uh
352:03 - four possible options so four
352:06 - then we can decide uh our meat options
352:10 - and there's uh five meat options uh
352:14 - we have six cheese options so n3 will be
352:18 - six uh there are
352:21 - and then for the remaining options it's
352:23 - all binary
352:25 - so we've got with or without lettuce so
352:27 - that means that n4
352:28 - equals two with or without tomatoes so
352:32 - n5 equals two with or without
352:35 - bacon so n6 equals 2
352:39 - with or without mayonnaise so and 7
352:42 - equals 2 and with or without
352:45 - mustard so n 8 equals
352:49 - two okay so based off this how many
352:53 - sandwiches are possible
352:54 - well according to the product rule we're
352:55 - just going to multiply all of those
352:57 - numbers together
352:58 - so we've got four times five times
353:02 - six times uh two five times so we'll say
353:06 - two to the fifth power
353:07 - so this when you multiply all this out
353:10 - you're going to end up with
353:11 - 3 840 possibilities
353:15 - for this sandwich shop
353:20 - okay so uh moving on here's
353:24 - here's the thing we are now uh
353:28 - very fully into the realm of
353:30 - combinatorics combinatorics
353:33 - is generally figuring out how many ways
353:35 - there are to do things
353:37 - and uh combi or how many combinations
353:41 - there are of things
353:43 - how large a finite size sets are
353:46 - when they're generated with certain
353:48 - rules
353:50 - and honestly combinatorics is pretty
353:52 - hard like for example i find
353:54 - combinatorics rather challenging
353:56 - um i remember once when i was in
354:00 - a probability class and i was in
354:04 - uh excuse me
354:07 - i was in office hours with a professor
354:10 - and
354:10 - a fellow student of mine just we were
354:13 - discussing
354:14 - combinatorics and uh my a fellow student
354:17 - of mine was just like is this what
354:18 - probability is
354:19 - or statistics is because if it is i i
354:22 - don't know if i want to do this and he
354:23 - was like no
354:24 - it's not uh you just kind of have to do
354:26 - this
354:27 - you have to learn this at some point but
354:30 - it's
354:30 - not what the bulk of probability is and
354:32 - i can understand why she said that
354:34 - because
354:35 - it can get pretty painful here's the
354:36 - thing about combinatorics
354:38 - uh what i'm about to do is give you some
354:42 - tools some counting tools for
354:45 - problem for counting type
354:48 - problems that are often reappearing
354:52 - but the thing is every single
354:54 - combinatorics problem
354:56 - is its own thing it's really hard to
355:00 - come up with general tools like what i'm
355:01 - coming up with here
355:03 - um there are certainly tools in the
355:04 - toolbox uh things that you kind of look
355:07 - out for
355:08 - when you're solving combinatorics type
355:10 - problems but you still need to think of
355:12 - it
355:12 - of each problem as its own thing
355:16 - and for this section i kind of have to
355:18 - come up with a number of examples and
355:20 - run through those examples to give you
355:22 - an idea of the thought process
355:24 - of combinatorics but it's really hard to
355:27 - teach that thought process
355:29 - uh without example since i can't just
355:31 - give you an
355:32 - algorithm that will solve every single
355:34 - covenantoric's problem
355:36 - every problem is its own beast and
355:39 - you're
355:40 - pretty much forced to think very
355:42 - carefully
355:43 - about uh the process by which
355:47 - a single combination has been formed uh
355:50 - also i should probably just mention uh
355:52 - if we have
355:53 - uh there's there's something that i
355:55 - didn't mention in these uh
355:57 - typed up notes but i call this the sum
356:00 - rule
356:04 - uh so uh if you're choosing
356:11 - uh from k
356:16 - uh k disjoint sets
356:25 - so sets each with
356:30 - uh each with um
356:34 - we'll say n sub i
356:38 - uh possibilities uh possible choices
356:50 - uh at one stage so you're gonna pick uh
356:53 - one of these sets and an item from that
356:56 - set
356:57 - uh then in this case
357:04 - uh there's going to be the sum from i
357:07 - equals 1 uh to k
357:11 - uh n sub i uh possible
357:14 - so possibilities
357:20 - so in other words if you need to choose
357:22 - an item from bin a
357:24 - or an item from bin b exclusively
357:27 - then the number of ways you can make
357:29 - that decision is going to be the sum of
357:31 - the items in those two bins
357:33 - not a particularly difficult concept but
357:35 - i'm going to just lay it out
357:37 - for you right now just so that you are
357:40 - aware of that and it's a
357:41 - it's a valid approach to solving
357:42 - cognitoric's problems
357:44 - and i would say that the the uh
357:47 - techniques that i'm about to show
357:49 - like these will these will get you
357:53 - through
357:54 - these are things that you look out for
357:55 - when solving combinatorics problems uh
357:58 - but uh they probably won't take you all
358:00 - the way
358:01 - all right so uh when we're solving some
358:05 - problems
358:06 - uh suppose that out of n possibilities
358:08 - we will be choosing k
358:10 - we have two essential questions to
358:12 - answer
358:13 - um are we choosing with or without
358:16 - replacement
358:17 - and does order matter depending on our
358:20 - answer to those questions we're going to
358:21 - have different solutions that are
358:22 - summarized below
358:24 - uh here uh if if the choice
358:27 - was where uh
358:30 - there's order and we're doing so with
358:32 - replacement then the number of possible
358:34 - outcomes is end of the power k
358:36 - uh ordered without replacement that's
358:39 - known as a permutation
358:41 - and here is a formula for a permutation
358:44 - uh n factorial by the way so this
358:47 - so uh n with an exclamation point means
358:50 - n factorial
358:51 - and that is n times n minus 1
358:56 - times n minus 2
359:01 - times dot dot dot times
359:04 - 3 times 2 times one
359:09 - that is n factorial and by convention
359:13 - zero factorial equals one
359:16 - huh that's kind of strange uh
359:19 - at least for me when i first saw that
359:22 - zero factorial equals one i found it
359:24 - rather surprising
359:25 - but actually it makes perfect sense for
359:27 - zero factorial to equal one
359:29 - and i might may explain why zero
359:31 - factorial is one in a separate video
359:34 - but we're just going to leave it at that
359:35 - so that is what n factorial is
359:38 - um and one way to think of what n
359:41 - factorial is
359:42 - it's it's the number of way to order
359:44 - number of ways
359:45 - to order n things so it's the number of
359:49 - permutations of
359:50 - n things um
359:54 - uh now in this case for uh ordered with
359:57 - without replacement uh we we all we're
360:00 - also calling that a permutation
360:02 - uh it's just a permutation of k things
360:04 - out of n
360:05 - so uh suppose that
360:09 - uh we are that we don't have replacement
360:13 - so remember what we're so think about
360:14 - what replacement means it means that
360:17 - uh so replacement means that if you
360:19 - choose
360:21 - uh an option you can choose it again
360:24 - so you're allowed to choose it again an
360:26 - example of
360:28 - of ordered with replacement is a
360:30 - sequence of heads and tails
360:32 - in coin flips where you may consider
360:36 - possibly you may not but if you're
360:39 - looking at a string there are
360:42 - different ways that heads and tails can
360:44 - manifest themselves in the string
360:46 - and furthermore if you get heads on the
360:49 - first flip
360:50 - you're still allowed to get hats on the
360:51 - second flip whereas if you don't have
360:54 - replacement you can get heads in the
360:55 - first flip but you can't get heads on
360:56 - the second flip
360:57 - so that would be without replacement in
361:00 - order matters
361:01 - that means uh you're that means that the
361:03 - order in which you see
361:04 - an item in a sequence matter so you're
361:06 - tracking that
361:08 - so headsets tails is different from
361:10 - head's tails heads
361:12 - whereas if order doesn't matter then
361:14 - headset's tails is essentially the same
361:15 - as heads tails heads
361:17 - because you don't care about the
361:18 - ordering of the items
361:20 - so that's what we mean by whether
361:21 - without replacement or with or without
361:23 - order
361:25 - okay so suppose that we're choosing
361:27 - items where order doesn't matter
361:29 - and uh we don't have replacement
361:33 - in that situation uh we have we're going
361:36 - to use the formula
361:37 - n choose k which is n factorial divided
361:39 - by k factorial
361:41 - uh and or the product of k factorial n
361:44 - minus k factorial
361:45 - i'm going to prove each one of these
361:47 - formulas
361:48 - in a second because i do believe that
361:50 - the proofs
361:52 - are enlightening on the combinatoric
361:54 - thought process
361:56 - and the combinatoric thought process is
361:57 - something where you just kind of have to
361:58 - get exposed to it a lot in order to be
362:00 - able to
362:01 - uh think that way yourself um finally we
362:05 - have the situation where
362:07 - uh you are you're choosing with
362:10 - replacement
362:10 - but order doesn't matter in which case
362:14 - you're going to have
362:15 - k plus n minus one choose n minus one
362:19 - okay so next up is uh
362:22 - the uh the proofs or the justifications
362:25 - for each of these formulas we're going
362:29 - and i'm going to uh prove this
362:33 - in um a sneaking fashion
362:37 - or a clockwise matter because
362:41 - often the formula after the formula in
362:43 - this
362:44 - clockwise order from before is going to
362:46 - be used in the next proof
362:48 - okay so uh let's get started
362:53 - uh we're going to start by
362:56 - showing by proving the formula and we in
362:59 - which case we have
363:00 - ordering and
363:04 - so and we also have replacement
363:12 - all right so ordered with replacement
363:14 - i'm going to zoom in because i'm going
363:15 - to need some uh
363:17 - i'm going to need more space all right
363:20 - so we want to come up with the formula
363:22 - um n to the power k well all right so we
363:25 - have k
363:25 - choices to make and since we have
363:28 - replacement
363:30 - we're going to use the product rule the
363:32 - product rule is kind of the underlying
363:35 - uh rule that we can use for all of these
363:37 - so starting out with the product rule
363:42 - we have k decisions to make i like to
363:45 - think of confidence of a
363:47 - combinatoric problems where you are
363:49 - describing any combinatoric problem
363:51 - how to construct an instance of
363:55 - an element of this set and
363:59 - when you are thinking about how to
364:01 - construct an element you construct a
364:03 - narrative for how to uh construct
364:06 - one of these elements and you're
364:08 - counting how many ways there are
364:09 - to make the decisions along the way and
364:12 - you're tracking how
364:13 - and you're tracking how many decisions
364:14 - you need to make and
364:17 - using the product rule the entire time
364:19 - so
364:20 - uh we can think of if we're doing
364:23 - ordering with replacement we might have
364:25 - for example
364:27 - we might have for example three spaces
364:29 - and we need to fill up those spaces with
364:31 - elements so
364:32 - we might have two possibilities for each
364:34 - space so
364:36 - at the first space we would have two
364:39 - possibilities and at the second space
364:40 - we'd have two possibilities
364:42 - and at the third space we have two
364:43 - possibilities and we care about the
364:45 - ordering
364:46 - and um
364:50 - we care about the ordering of what we
364:52 - put in in these spaces
364:54 - so the number of possibilities for
364:56 - filling up these three spaces would be 2
364:58 - to the power 3
365:00 - because we're going to multiply and we
365:02 - have to make a decision at each space
365:04 - and when you have to make a decision at
365:05 - each space
365:06 - you multiply the possible number of
365:08 - decisions that you needed to make
365:10 - so uh generalizing this idea
365:14 - uh we have uh we need to make a decision
365:17 - in the first slot the second thought the
365:20 - third slot all the way to the kth slot
365:22 - and for each of these
365:24 - and then and for each of these slots
365:29 - there were n possible things to choose
365:31 - one from because
365:33 - we're not taking we're not removing
365:35 - options
365:36 - as we go through our sequence of case
365:38 - slots
365:40 - so then by using the product rule
365:43 - according to the product rule we end up
365:44 - having to multiply
365:46 - um we have we end up having to multiply
365:51 - uh each of our possibilities
365:54 - or each of the possible options we can
365:56 - make at each slot
365:58 - but since all of those are the same
366:05 - since all of those are the same number
366:08 - you end up with multiplying n
366:12 - k times which is n to the power k
366:17 - so that gives us the first formula the
366:19 - next formula we need to come up with is
366:22 - ordering without replacement which gives
366:23 - us the number of permutations
366:26 - okay so we now next have
366:29 - oops that is definitely not what i
366:31 - wanted
366:32 - all right so next up we have ordered
366:37 - without replacement
366:47 - i'm going to go back for a second and
366:50 - i have in my head prototype problems
366:54 - for each of these four scenarios
366:57 - so ordering with replacement is like
367:00 - determining how many strings of heads
367:02 - and tails there are in a string of flips
367:05 - because the ordering in that situation
367:06 - would matter because your track
367:08 - because heads tails it's not the same as
367:10 - tails heads in a string
367:12 - and you have replacement because if you
367:14 - get heads on the first flip you can get
367:15 - heads again on the second flip
367:18 - um ordering without replacement to me
367:21 - my prototype problem for that is forming
367:23 - a list
367:25 - because the item that you put at the top
367:27 - of the list cannot be chosen for
367:28 - the sec for the next item in the list so
367:31 - you are removing items as you go through
367:33 - this list
367:34 - and maybe it's like a times top top 100
367:38 - people or 100 people for the year list
367:41 - where the ordering matters and there's
367:43 - certainly more people than 100
367:45 - so they might have this larger
367:48 - this larger master list of people they
367:51 - would consider to be candidates on their
367:53 - top 100 list
367:54 - so you're going to choose people to be
367:56 - in certain slots on this list and when
367:58 - you choose a person to be on the list
367:59 - you can't pick them again
368:01 - so that's kind of my prototype problem
368:04 - for
368:06 - ordered with without replacement for
368:09 - um not
368:12 - ordered and with replacement this is
368:14 - like uh poker problems
368:16 - because when you draw cards from
368:20 - a deck of cards you
368:23 - are allowed
368:26 - to reorder the cards in your hand so
368:29 - there is no ordering
368:31 - but once you draw a card you cannot draw
368:35 - it again
368:36 - so there isn't replacement and for the
368:39 - final situation my prototype problem
368:41 - here
368:42 - is choosing a dozen donuts when you have
368:45 - uh so many flavors of donuts because you
368:48 - are allowed to reorder the donuts in the
368:50 - box
368:51 - and in principle the donut shop has
368:54 - this almost infinite list for the
368:58 - number of or an almost infinite supply
369:01 - of donuts in this imaginary donut shop
369:03 - so there is replacement when you choose
369:06 - a flavor
369:07 - like when you choose a flavor you're
369:09 - allowed to choose as many donuts of that
369:11 - flavor
369:11 - as you want so a replacement is not an
369:14 - issue
369:16 - all right so you do have in fact
369:17 - replacement those are my prototype
369:19 - problems
369:20 - and those are good to keep in mind when
369:22 - we're going through and trying to think
369:24 - about
369:25 - what formulas work we should have and
369:27 - how we're going to
369:29 - prove these formulas and additionally uh
369:32 - it's kind of good to have these
369:33 - prototype problems in your mind when
369:35 - you're trying to solve common torque
369:37 - problems yourself
369:38 - all right so next up uh getting back to
369:40 - these proofs
369:41 - uh we need to show the formula for
369:43 - ordering without replacement
369:45 - uh so i had that formula for n factorial
369:49 - um so all right so
369:52 - how many ways are there to uh
369:55 - form a list of k things
369:59 - uh when you have n possibilities
370:02 - so we would need to pick the first item
370:04 - in the list
370:05 - and the number of ways that we could
370:07 - pick the first item in the list is
370:08 - n then we need to pick the the so there
370:11 - are
370:12 - n ways to
370:15 - pick the first item in the list
370:24 - then we need to pick the second item in
370:26 - the list
370:27 - so there are going to be n minus one
370:32 - ways
370:34 - to pick the second
370:42 - the reason why is that whatever we chose
370:44 - to be our
370:45 - our first element in the list cannot be
370:47 - chosen again to be the second element
370:50 - so we have n minus one ways to fill the
370:52 - second item
370:53 - then we have n minus two ways to pick
370:55 - the third
370:56 - the third item because now now in
370:58 - addition to not being able to pick
371:00 - what we chose for the first item in the
371:02 - list we also can't pick what we chose
371:04 - for the second item of the list so we're
371:06 - going to have n minus two way
371:07 - uh mis two choices now so mi it's two
371:10 - ways
371:11 - uh for the second item
371:17 - no no not the second the third
371:25 - and we just continue on with this uh
371:28 - with this process until we reach the kth
371:31 - slot
371:32 - uh and there will be uh for the
371:36 - kth item in the list there will be
371:40 - n minus k minus 1
371:43 - ways to pick that item which by the way
371:45 - is equal to n minus k
371:48 - uh plus one right so
371:51 - so n minus k plus one ways
371:55 - to pick
371:58 - the cave item
372:05 - so then we're going to use the product
372:10 - rule
372:15 - and by the product rule we multiply all
372:18 - of these numbers together so we're going
372:19 - to get
372:20 - so the number of possibilities is going
372:22 - to be n times n minus 1
372:24 - times dot dot times
372:27 - n minus k minus 1.
372:33 - and if we wanted to we could stop there
372:35 - this is in fact the formula but
372:37 - thing though is it's sometimes uh better
372:40 - to
372:42 - we we might prefer a more compact uh
372:46 - formula so we might say instead that
372:50 - um we have n n minus 1
372:54 - n minus 2 uh dot dot dot
372:58 - and then we have n minus k plus 1
373:03 - and then we could keep going multiplying
373:08 - and say we're going to multiply by n
373:09 - minus k
373:11 - and still you know still decreasing down
373:13 - but but we've now multiplied by n minus
373:15 - k
373:16 - and if we're going to multiply by n
373:17 - minus k we now
373:19 - need to divide by n minus k in order to
373:22 - keep things balanced
373:23 - and then we're going to multiply by n
373:24 - minus k minus 1.
373:27 - okay so we're going to need to divide by
373:28 - n minus k minus 1.
373:33 - and we're going to keep going with this
373:34 - process
373:36 - on both the top and the bottom until
373:38 - we're multi we multiply
373:40 - three two and one
373:47 - and what i've actually written down in
373:49 - doing this the top can be recognized
373:52 - uh so the top uh part of this uh
373:56 - fraction you you may recognize that as
373:59 - being uh i don't want that color
374:03 - you may recognize the top part as being
374:06 - n factorial
374:10 - and the bottom part of the fraction the
374:13 - denominator
374:14 - as being uh as being
374:18 - n minus k
374:22 - factorial
374:25 - hence uh producing the formula
374:31 - this will produce the formula uh n
374:34 - factorial
374:35 - over n minus k factorial
374:40 - which we may also just call p and k
374:49 - and that gives us our second formula
374:53 - okay uh next one very
374:57 - now what we now need to do is figure out
375:00 - how many ways there
375:01 - are to choose items that are not
375:04 - ordered and there is no replacement
375:08 - this gives us the formula that is often
375:10 - referred to in english as
375:11 - and choose k so
375:15 - the proof for this one is actually quite
375:17 - tricky where you start out by assuming
375:19 - that you know how to do it
375:21 - and then you get recover the formula in
375:23 - the end
375:24 - where you you pretend that you know the
375:26 - formula and after you pretend that you
375:28 - know the formula you
375:29 - then figure out something that you
375:31 - already know which in this case is the
375:33 - number of permutations of a list
375:36 - and once you have the number of
375:39 - permutations in the list you're able to
375:41 - recover
375:42 - uh the formula that you pretended that
375:43 - you knew but you actually actually
375:45 - didn't
375:46 - all right so that's that's rather
375:47 - convoluted let's get started
375:49 - with uh uh showing how we can get this
375:52 - formula so the proof again is kind of
375:54 - weird
375:54 - it's a really weird one but all of a
375:56 - sudden at the end
375:58 - we're going to have the result that we
376:00 - wanted so
376:01 - we have no order
376:06 - and we don't have replacement
376:17 - i keep touching that all right i keep
376:20 - touching it
376:22 - alright so not ordered without
376:24 - replacement so
376:25 - suppose
376:30 - uh n choose k
376:35 - is the number of ways
376:40 - to is a number of ways to do this
376:47 - so we know how so we know
376:51 - the number of ways to choose items when
376:53 - we don't care about order and we don't
376:54 - care about replacement and the number of
376:56 - ways to do so is n choose k
376:59 - okay bear with me i now
377:02 - want to know how many ways
377:10 - are there going to be to pick
377:17 - to pick
377:22 - uh k items
377:26 - out of n when replacement doesn't matter
377:41 - so as opposed to um doing so in order
377:46 - doesn't matter which we're assuming that
377:48 - we know
377:49 - uh that is what i want to do
377:54 - i want it what i want to do is calculate
378:02 - uh uh p and k or the number
378:05 - of of k length permutations of n objects
378:10 - okay so uh i want to compute that which
378:13 - by the way
378:14 - we have already computed that's that
378:17 - formula is actually already known to us
378:18 - it's up here
378:19 - but we're going to suggest that there is
378:21 - an alternative way
378:23 - to calculate this if somehow you knew
378:26 - the number of ways to pick k items out
378:30 - of
378:30 - n uh without without order
378:33 - so how would we do this how would we
378:36 - let's think about how we would construct
378:39 - a single permutation if what we had to
378:42 - do
378:43 - was pick items without order first
378:47 - so our first step
378:50 - if we were to attempt to form
378:53 - a single list of k items out of n
378:57 - when we can pick items without ordering
379:00 - them the first thing we would do is pick
379:02 - the number of
379:02 - items or pick what items will appear
379:06 - on the list without ordering them first
379:08 - so our first step
379:10 - in our process is to pick objects
379:19 - when order doesn't matter
379:24 - so you've decided basically what's going
379:26 - to appear on the list you just don't
379:27 - know
379:28 - in what slots it will appear
379:37 - and supposedly we know how to do so
379:41 - there's and choose k ways to do so so
379:54 - there are n choose k ways to pick the
379:56 - items that will appear on our list
379:57 - without
379:58 - ordering the items so first we pick
380:01 - a set of items that will appear on the
380:02 - list the next step then
380:04 - is to order those items
380:13 - so the second step is to order the k
380:20 - objects
380:23 - so how do we order the k objects well uh
380:26 - we picked the first
380:27 - so we pick uh one of the objects that we
380:30 - have selected
380:31 - to be the first item of the list uh
380:33 - there's k ways to do
380:34 - that then we pick another one of the
380:38 - objects that we haven't picked yet to be
380:39 - the second item on the list
380:40 - there's k minus ways to k minus one ways
380:43 - to do that keep doing so until you run
380:46 - out of items
380:47 - so there are uh
380:52 - k factorial ways to order the items
380:57 - oops oh darn it so there's k minus one
381:01 - ways to order the items going back to
381:04 - where i was
381:11 - or k factorial ways to order
381:22 - so now we're going to use the product
381:23 - rule and say
381:25 - that the number of ways to pick
381:28 - uh items to appear on our list is going
381:31 - to be the number of
381:32 - is going to be the number of decisions
381:33 - we have to make in step one which is the
381:35 - number of ways to pick the items to
381:36 - appear on the list
381:38 - uh there's n choose k ways to do that
381:41 - uh and then multiply that with the
381:43 - number of ways to order the items
381:45 - so there will be k factorial so that's
381:48 - the number of ways to form permutations
381:50 - but we also know that the number of ways
381:52 - to get permutations
381:53 - from what we did before is n factorial
381:56 - divided by
381:57 - n minus k factorial
382:02 - okay well what we actually were
382:04 - interested in this whole time
382:06 - was calculating this number that i just
382:08 - highlighted in red
382:10 - that's the number we actually want
382:13 - well how can we get that with
382:16 - division
382:21 - because now we have an algebraic
382:23 - relationship
382:24 - uh with which we can so that we can
382:27 - solve
382:28 - to get and choose k and it follows
382:32 - that n choose k is equal to
382:36 - n factorial divided by k factorial
382:41 - times n minus k factorial
382:48 - and we're done we computed what we
382:50 - actually
382:51 - wanted to compute
382:54 - so that was a little odd that was a
382:56 - little strange um
382:57 - here's some more ways to kind of justify
383:01 - uh this formula that we ended up with uh
383:04 - so the number of permutations is larger
383:08 - than the number of combinations
383:10 - because the because when you are
383:12 - sensitive to order
383:14 - you're going to end up with many more
383:15 - possibilities than if you're not
383:16 - sensitive to order
383:18 - so as a result you need to divide out
383:22 - uh by a factor that effectively removes
383:26 - uh all of the orderings that are that
383:28 - contain the same items that just in
383:30 - different
383:30 - and just in a different arrangement or
383:32 - to get the combinations
383:34 - and when it comes to computation like
383:36 - let's say for example three choose
383:38 - no uh five choose three um
383:41 - this formula for me at least when i was
383:44 - taking this class
383:45 - the way i think of it is i have five
383:47 - factorial on the bottom and on the
383:48 - bottom
383:49 - i'm going no i have five factorial in
383:50 - the top and on the bottom i'm going to
383:52 - have 3 factorial and whatever it takes
383:54 - to get
383:54 - 2 and the other number such that 3
383:58 - and the other number adds up to 5. so
383:59 - i'm going to have 5 factorial divided by
384:02 - 3 factorial times 2 factorial okay
384:05 - so that's a way so the formula is
384:07 - actually not that hard
384:08 - to remember because it's like okay uh
384:12 - 10 choose 7. that's going to be 10
384:14 - factorial divided by
384:15 - well we've got 7 factorial down there
384:17 - and we've also got something that adds
384:18 - up to ten
384:19 - okay three factorial so ten factorial
384:21 - divided by seven factorial times three
384:23 - factorial so the formula itself is not
384:25 - that hard to remember
384:26 - uh but you now have it
384:30 - okay uh continuing on
384:33 - so that was our third formula that i
384:37 - promised that we were going to prove
384:39 - uh so now we have one last formula
384:43 - to prove uh and that formula
384:47 - is let's see how much space we got
384:50 - uh hopefully we've got enough so the
384:53 - last formula we have
384:55 - uh not ordered
385:03 - and we have replacement so this is the
385:06 - donut shop situation
385:14 - okay so the donut shop uh situation
385:20 - uh here's like this this proof is again
385:23 - also rather tricky combinatorics often
385:26 - requires
385:28 - tricks honestly that that's that's one
385:31 - that's one reason why combinatorics is
385:32 - rather painful
385:34 - uh it feels like there are rather few
385:37 - unifying
385:38 - uh principles and combinatorics i know
385:40 - that there are some
385:42 - but it doesn't really seem that there's
385:44 - all that many
385:46 - so here's what we're going to do
385:49 - to um to solve this one
385:53 - uh what let's let's suppose that we're
385:56 - in a donut shop
385:57 - and we're going to choose say five
386:01 - donuts
386:02 - of three flavors uh how could we
386:06 - possibly do that
386:07 - uh we don't care about the ordering of
386:09 - the donuts because you put in the box
386:11 - and i mean no one's going to ask in what
386:13 - order the donuts
386:14 - were when you come home so
386:18 - what i like to do is imagine okay
386:19 - whatever we're going to do we're always
386:21 - going to arrange the donuts in the box
386:23 - right so so whatever arrangement they
386:25 - originally were we're going to put them
386:27 - into a fixed arrangement where we have
386:30 - one flavor first one flavor second and
386:32 - one flavor for
386:33 - flavor third so what we could end up
386:36 - doing to form
386:37 - a single box after we say that we're
386:39 - going to arrange to rearrange them at
386:41 - the very end
386:41 - is we're going to have
386:45 - uh we're going to uh put down all the
386:49 - donuts of a single flavor
386:51 - and we know that the donut that comes
386:53 - first
386:54 - is going to be of a certain flavor but
386:57 - we're also going to
386:59 - put dividers in our box to separate
387:02 - flavors
387:04 - so so the second flavor will come after
387:07 - the first
387:08 - divider and the third flavor will come
387:09 - after the second divider
387:12 - so and it is also possible to uh
387:15 - not have a donut of uh
387:18 - certain flavors so for example if we
387:20 - wanted a box of only the third flavor
387:22 - we put a divider in the first position
387:24 - of divider in the second position
387:27 - and then put donuts in all the remaining
387:29 - positions
387:30 - and with this encoding i have now
387:32 - encoded one of the one of the boxes of
387:34 - five donuts
387:35 - this is a box where you have only the
387:37 - third flavor and you can imagine what a
387:39 - box
387:40 - consisting of only the second flavor
387:42 - would look like so you'd have a divider
387:44 - at the beginning and a divider at the
387:45 - end
387:46 - and then in between you have your donuts
387:50 - so this is a box
387:51 - that has only donuts of the second
387:53 - flavor
387:56 - and maybe play around with this encoding
387:58 - of donut boxes
388:00 - but once we have this encoding it is now
388:03 - possible to calculate how many how many
388:05 - boxes we can achieve
388:07 - because what we can do is say how many
388:09 - ways are there
388:11 - to pick positions for dividers
388:14 - and positions for donuts
388:20 - how many ways are there to do that well
388:22 - we're going to have if
388:23 - in the if we have uh three flavors we're
388:26 - going to have two dividers
388:28 - and if we have five donuts we're gonna
388:30 - have positions four or five donuts
388:33 - so we're gonna end up with seven
388:35 - positions that we need to fill up
388:37 - uh we could just pick two of the seven
388:40 - positions
388:41 - to contain dividers and the remaining
388:44 - positions will contain
388:46 - donuts so let's see we could
388:49 - potentially pick numbers we could assign
388:52 - a number to each of the positions
388:54 - so we have positions one two three four
388:58 - five six seven and we pick
389:01 - numbers that represent positions that
389:04 - will
389:05 - contain dividers so in this case uh
389:08 - uh numbers four and six represent
389:10 - positions four and six
389:12 - so those will be uh so
389:15 - if we pick four and six which by the way
389:17 - is the same as picking six and four
389:19 - to contain dividers so in other words we
389:20 - don't care on the ordering of the
389:22 - numbers that we end up picking
389:24 - uh once we pick that we now know what
389:26 - our box of donuts is going to be
389:28 - because we've picked the positions for
389:29 - the dividers and therefore every other
389:31 - position will contain
389:32 - donuts okay
389:35 - all right then um so
389:38 - if that is the case how many boxes
389:42 - of donuts are there going to be
389:45 - um
389:53 - so in this case the number of
389:55 - possibilities
389:56 - which is n in this scenario is equal to
390:00 - three
390:01 - and out of that n we're going to be
390:03 - choosing
390:04 - uh k possibilities uh or we're going to
390:07 - be choosing k
390:08 - items or k donuts so for this problem k
390:10 - is equal to five
390:13 - okay so um
390:16 - how many ways were there to make this
390:18 - decision well it turns out
390:21 - uh there were um
390:24 - there were five plus three
390:28 - minus one choose three minus one ways
390:31 - to pick donuts in this fashion
390:35 - because we end up picking the or
390:39 - in other words this is uh uh this is
390:42 - seven
390:42 - choose two because we have seven slots
390:46 - and we pick two of those slots to
390:48 - contain dividers and the rest of the
390:49 - slots contain donuts hence we get the
390:52 - number of donuts
390:53 - all right so let's generalize this idea
390:55 - so if we have
390:56 - n possibilities we're going to have n
390:59 - minus 1 div
391:00 - n minus 1 plus k slots because we're
391:03 - going to have
391:04 - n minus 1 dividers and then also the
391:06 - case slots for the k
391:07 - things that we're going to end up
391:08 - picking so in general
391:10 - um uh so
391:13 - let's see uh
391:16 - so we're so we have um
391:23 - so we have uh
391:27 - n that no uh
391:31 - k plus n minus slots
391:35 - n minus one slots
391:38 - uh to fill
391:43 - with uh n minus one
391:47 - dividers
391:51 - which we call uh which we're going to
391:54 - call the
391:55 - the bar uh remaining slots
392:06 - so the remaining slots
392:09 - contain items
392:17 - uh so we need to pick uh so pick the
392:20 - positions
392:23 - let's see yeah so pick positions
392:29 - for dividers
392:37 - and order doesn't matter
392:48 - so since order doesn't matter
392:52 - there are
392:55 - going to be uh n plus
392:59 - uh no uh i like a different ordering uh
393:02 - there are going to be k plus n minus one
393:05 - choose
393:05 - n minus one ways to do so
393:13 - and we're done
393:17 - you end up with the formula
393:26 - and we're done so that was exhausting
393:29 - uh this is rather tricky
393:33 - types of mathematics that honestly it's
393:35 - it's frustrating
393:37 - because it often just requires knowing
393:39 - special tricks
393:41 - in order to solve a certain problem it
393:43 - just feels like all you're doing is
393:44 - coming up with
393:45 - a longer and longer list of tricks and
393:47 - it doesn't really feel like there's much
393:48 - of a unifying principle to them
393:50 - uh at least to me at least to me
393:52 - personally
393:53 - it's it seems also to me like there are
393:55 - some people out there
393:57 - some really smart mathematicians for
393:58 - which this type of thinking
394:00 - just for some reason just clicks and
394:03 - they and they
394:03 - are able to see an underlying principle
394:06 - i don't see it
394:07 - neces uh often uh but i often know
394:10 - enough
394:11 - combinatorics like this co this amount
394:12 - of combinatorics will get you very far
394:14 - in life
394:15 - uh in your statistics life that is uh
394:17 - you don't really need to know
394:19 - that much more than this
394:23 - all right uh so those were complicated
394:26 - proofs
394:27 - uh now we're gonna go through a series
394:30 - of examples to show
394:32 - uh how these techniques can be applied
394:35 - so uh suppose we're going to roll two
394:37 - six-sided dice and we assume
394:40 - each outcome is equally likely and
394:43 - we're going to say that the dice are
394:44 - different colors so uh
394:46 - if the red dice has a six and the blue
394:48 - dice has a one that's different from the
394:49 - red dice having a one a blue dice having
394:51 - a six
394:52 - those are two different pop those are
394:53 - two different outcomes
394:55 - how many possible outcomes are there and
394:58 - what about
394:59 - the situation when there's three six
395:00 - sided die uh
395:02 - so we're to first answer problem that
395:05 - i'm going to call
395:06 - one and the problem that i'm going to
395:08 - call two
395:09 - all right uh so the answer to one
395:12 - uh order matters and we're doing so with
395:15 - replacement because if we roll
395:17 - um a one for the red dice we're still
395:19 - allowed to roll a one for the blue dice
395:21 - so we end up with uh there are six
395:24 - possibilities and we're choosing two of
395:25 - them so we get 36.
395:29 - all right for the next option uh well
395:32 - it's just the same things now there's
395:33 - we're just choosing three instead of two
395:35 - so that's going to be
395:36 - 216. so this is a situation where order
395:40 - matters
395:42 - because the dices the dice are different
395:44 - colors
395:46 - and there is replacement
395:52 - because dice don't care what the other
395:53 - dice rolled
395:59 - all right uh next example a high school
396:02 - has 27 boys playing men's basketball
396:05 - in basketball there are five positions
396:06 - point guard shooting guard small forward
396:08 - power forward and center
396:10 - each assignment of player two position
396:12 - is unique
396:13 - how many teams can then be formed this
396:16 - is
396:17 - a this is a permutation type problem so
396:20 - here order matters because there's
396:21 - different positions
396:24 - and each of those positions are distinct
396:28 - but there is a replacement because a
396:30 - person playing point guard cannot also
396:32 - be center
396:33 - so position doesn't matter
396:37 - oh no position matters or order matters
396:41 - no sorry sorry no replacement no
396:43 - replacement i get it eventually
396:45 - all right so no replacement
396:55 - so that means we're going to be using
396:57 - that a second uh
396:59 - that second formula in the clock so
397:01 - we've got
397:02 - uh p uh there's a 27 possibilities we're
397:06 - going to order five of them
397:09 - so that's going to be 27 factorial
397:12 - divided by 27 minus 5
397:16 - factorial which is 27 factorial
397:22 - over 22 factorial which also can be
397:26 - written
397:26 - as uh maybe more simply into the point
397:29 - 27 times 26 times 25
397:35 - times 24.
397:38 - times 23 that's almost easier than
397:41 - remembering the formula just remembering
397:43 - that you
397:44 - decrement that many times and this
397:47 - multiplies
397:48 - out to uh 9 million six hundred and
397:52 - eighty seven
397:54 - thousand six hundred potential teams
397:59 - okay uh example nineteen when playing
398:02 - poker
398:03 - players draw five cards from a 52 card
398:05 - deck
398:06 - every card is distinct but the order of
398:09 - the draw
398:10 - does not matter you are allowed to
398:12 - reorder the cards in your hand
398:14 - how many hands are possible in this
398:16 - situation
398:17 - because you're allowed to reorder order
398:19 - doesn't matter
398:28 - and since order and also an additional
398:30 - ordering not mattering there is no
398:32 - replacement
398:33 - because if you draw an ace you can't if
398:35 - you draw an ace of spades you're not
398:37 - allowed to draw an ace of spades again
398:38 - so no replacement
398:45 - okay uh so let's see if that's the case
398:48 - then we're going to use that fourth
398:49 - third formula in the clock uh we have uh
398:52 - 52 possibilities for 52 cards we're
398:55 - going to choose five of them when we
398:56 - draw cards
398:57 - and that's going to be 52 factorial
399:00 - divided by 5 factorial
399:04 - times 47 factorial
399:08 - which is equal to 52
399:12 - times 51 times 50
399:16 - times 49 times 48
399:23 - divided by 5 times 4
399:27 - times 3 times two times one
399:31 - okay uh and we and we don't care about
399:33 - the one because it's times one that that
399:35 - doesn't really do anything
399:36 - um we can do some cancellation like for
399:39 - example the 50 and the five cancel down
399:41 - to a ten
399:43 - uh the 4 3 and 2
399:47 - those met multiply the 24 so that
399:50 - reduces with the 48 rendering it a 2
399:53 - so this is equal to 52 times
399:57 - 51 times
400:00 - 10 times 49
400:03 - times 2 which you then go to your
400:06 - calculator
400:07 - and it will give you 2 million uh
400:10 - 500 and thousand
400:14 - uh 960
400:17 - poker hands
400:22 - now r can do a lot of these calculations
400:25 - so
400:26 - for example 16 you can just say what is
400:30 - 6 to the power 2 and i'll tell you that
400:31 - the 36
400:32 - and 4 6 to the power 3 that's 216.
400:35 - r does have a factorial function
400:40 - now be careful with some of these
400:42 - functions because you might end up
400:45 - with integer overflow you might end up
400:47 - with
400:48 - numbers that are so large that uh the
400:51 - computer cannot handle them so i would
400:54 - not just blindly use these functions
400:56 - because it is possible
400:58 - for these numbers to explode very
401:00 - rapidly in which case
401:02 - your calculations will end up being
401:03 - wrong but we do have a factorial
401:05 - function and we do have a choose
401:07 - function
401:08 - so here's example 17 and here's example
401:10 - 18.
401:11 - um uh
401:14 - so or is that that number might be wrong
401:19 - yeah that that that numbering's wrong my
401:21 - apologies
401:23 - uh so i guess at some point when i wrote
401:25 - this
401:26 - r code uh i uh
401:30 - or when i wrote these notes i must have
401:31 - deleted an example a long time ago but i
401:33 - did not change the comments in the r
401:35 - code this is why i need to be very
401:36 - careful with comments
401:38 - comments expire eventually they turn bad
401:42 - all right and you know what's worse than
401:45 - a note then no comment a misleading
401:48 - comment
401:49 - that's even worse um all right so
401:52 - example 20 you want to choose a dozen
401:55 - donuts from a donut shop
401:56 - there are eight different kinds of
401:58 - donuts how many boxes of a dozen donuts
402:00 - are possible
402:01 - well okay so in this situation uh we are
402:04 - choosing
402:05 - 12 donuts and there are eight
402:08 - possibilities so using that uh
402:11 - fourth formula in the clock
402:14 - we have uh 12 plus
402:18 - 8 minus 1 choose
402:21 - 8 minus 1 possibilities which is going
402:24 - to be
402:26 - 19 choose 7 which is equal to 19
402:29 - factorial
402:31 - divided by 7 factorial
402:35 - times 12 factorial which is equal to
402:41 - 19 times 18
402:45 - times 17 times
402:48 - 16 times 15
402:51 - times 14
402:54 - times 13 divided by 7
402:57 - times 6 times five times
403:01 - four times three times
403:04 - two times one all right and uh let's do
403:08 - some
403:09 - cancellation to help make our life a
403:11 - little bit easier
403:12 - uh let's see the 7 and the 2
403:16 - are going to cancel with the 14
403:19 - we've got what else 4 and 16 will reduce
403:23 - the 16 down to 4
403:26 - the 5 and the 3 will cancel out the 15
403:29 - and the 6 cancels out with the 18
403:31 - reducing it to 3
403:33 - so in the end we're going to have this
403:36 - is 19
403:37 - times 17 times 13
403:41 - times three times four
403:44 - and then you go to your calculator and
403:45 - ask what that is and you'll get
403:48 - fifty thousand uh 388 potential boxes of
403:53 - donuts from this donut shop
403:57 - okay and here is some arco that will
404:01 - also
404:02 - compute that quantity
404:06 - okay uh so let's do some classic poker
404:10 - problems
404:10 - once people uh introduce combinatorics
404:14 - it's like the next thing you have to
404:15 - talk about are poker problems because
404:17 - poker problems are fun
404:18 - because poker is fun it's fun to talk
404:20 - about poker
404:21 - um so um
404:25 - now that said there is uh one potential
404:28 - was talking about poker problems is that
404:30 - unfortunately not everyone is familiar
404:32 - with the poker deck
404:34 - or the standard playing card deck as
404:36 - it's known in the english-speaking world
404:38 - which for what it's worth the standard
404:40 - deck is technically the french deck
404:43 - and different european countries have
404:45 - different traditional playing card
404:47 - decks so like for example my advisor leo
404:50 - horvath the
404:52 - traditional deck that's used in hungary
404:54 - is not the french deck
404:56 - so he doesn't like personally poker
404:58 - problems because there's something that
404:59 - like they talk about a deck and he's not
405:01 - very familiar with that deck and it just
405:03 - goes against his intuition whereas i
405:05 - myself
405:06 - i grew up with this deck i grew up in
405:08 - america
405:09 - so i'm very familiar with uh with what
405:12 - is inside
405:13 - of a playing card deck that said if
405:16 - you're like an international student or
405:17 - something like that
405:18 - here is a description of what is inside
405:23 - of a playing card of a standard playing
405:25 - card deck or
405:27 - a a french deck the deck that's used in
405:29 - the english speaking world
405:31 - and often used in these probability
405:33 - textbooks because
405:34 - most of these probably authors are most
405:37 - most of these
405:38 - most of the authors of these uh probably
405:39 - books they may be in america but they're
405:41 - certainly
405:42 - speaking english they're probably using
405:43 - uh this deck
405:45 - so and here's some additional notation
405:48 - for
405:49 - uh this is a poker notation to describe
405:52 - what goes inside
405:54 - uh what cards are inside of a deck all
405:56 - right so but basically you've got four
405:58 - sweets and 13 possible faces uh
406:02 - all right for a total of 52 possible
406:05 - cards you should know how to do that by
406:06 - now because
406:07 - uh how do you form a single card you
406:09 - first pick
406:10 - a suit there's four suits possible then
406:14 - pick a face value there's 13 faces so
406:17 - 13 times 4 will be 52. all right so
406:20 - you've learned something today uh
406:22 - example 21 a poker hand
406:24 - is four of a kind if four cards
406:27 - have the same face value how many four
406:29 - of a kind hands
406:30 - exist how are we going to solve this
406:33 - problem
406:34 - well uh the trick that we're going to
406:38 - use
406:38 - again with these poker with these not
406:40 - just poker problems but
406:41 - most combinatoric problems what i
406:44 - suggest that you do
406:45 - is come up with a narrative like i just
406:48 - did right now
406:48 - for figuring out how many cards there
406:50 - are in a deck come up with a narrative
406:53 - for forming a single combination
406:56 - and then once you have that narrative
406:58 - figure out how many choices there were
406:59 - to make
407:00 - at each at each junction and
407:04 - multiply them together with the pop with
407:05 - the uh power rule
407:07 - and you'll get what you need um so
407:10 - first thing we're going to do to form a
407:12 - four of a kind hand
407:15 - is we're going to decide what card
407:18 - face value will be the four of a kind
407:21 - card so first we're going to pick
407:24 - uh the four of a kind card
407:33 - so we're going to say for example
407:35 - they're going to be four kings
407:36 - in this hand or uh four twos or four
407:40 - tens something like that so we need to
407:42 - pick a face value
407:44 - and there are 13 face values to be the
407:47 - four of a kind
407:48 - part right so there are 13 ways
407:51 - to pick the face value then
407:55 - we need to pick the remaining card
407:57 - because a poker hand has five cards
407:59 - we have picked four of those cards if we
408:02 - decided
408:03 - that we were going to use the ace we
408:05 - were going to use ace
408:07 - for the four of a kind card then we've
408:09 - automatically got
408:10 - the ace of spades the ace of hearts the
408:13 - ace of diamonds and the ace of clubs
408:15 - that's four cards now we need to pick
408:18 - the remaining card if we picked ace then
408:22 - we cannot pick the ace
408:23 - face value again so
408:26 - uh we've uh and in fact in that 52 card
408:30 - deck we have taken out
408:32 - four of the cards and put them in our
408:34 - hand leaving 48 cards remaining
408:36 - in our deck so that means that we have
408:39 - 48
408:40 - cards to be the potential fifth card
408:43 - so pick the fifth card
408:52 - so 13 times 48 you multiply those two
408:55 - numbers together to get 624 poker hands
408:59 - with four of a kind
409:05 - uh next up a poker hand is considered
409:08 - full house if two cards have the same
409:11 - face value
409:12 - and three different cards have another
409:14 - common face value
409:16 - how many full house hands exist
409:19 - let me just list out for you an example
409:21 - of a full house hand
409:22 - you could have let's say the four
409:26 - of diamonds the four
409:29 - of spades
409:32 - so the four of spades and
409:36 - uh the four of uh hearts
409:41 - and uh so that's three cards with a
409:43 - common face value and then we need two
409:45 - cards with another
409:46 - face value we can't pick four for this
409:48 - so we're going to pick
409:50 - uh let's say king so we'll have the king
409:53 - of
409:53 - uh clubs
409:57 - and the king of uh diamonds
410:01 - so this would be a full house hand
410:04 - so let's see how can we form a full
410:07 - house hand in general
410:09 - well we're going to have two different
410:11 - face values
410:12 - in our full house hand but one face
410:16 - value will be the
410:17 - more numerous face value the three of a
410:19 - kind and the other face value will be
410:21 - the two of a kind
410:22 - so what i suggest we do is first
410:27 - pick the card that will be the three of
410:29 - a kind card
410:31 - so pick the card that will be the three
410:32 - of a kind card
410:35 - so pick the three of a kind
410:42 - face value
410:48 - and there's 13 face values for this
410:50 - choice
410:51 - so there's 13 ways to make that choice
410:54 - then we need to pick the suits because
410:58 - we picked for example four
411:00 - but there are four suits from which we
411:02 - can choose and we need to pick
411:03 - three of them so now we need to pick the
411:08 - suits
411:11 - and order doesn't matter in this
411:13 - situation we don't care about the
411:15 - ordering of the suits we just need to
411:16 - pick them
411:17 - so so we need to pick exactly three
411:20 - suits
411:21 - for all those three cards and since
411:23 - order doesn't matter for this choice
411:26 - we have four choose three ways to pick
411:30 - the suits and four choose three
411:33 - evaluates to four
411:37 - which is not surprising you know like
411:39 - alternatively and by the way this this
411:41 - thought process is quite useful
411:43 - instead of picking the three faces we're
411:45 - going to include we could have picked
411:46 - the one face
411:48 - no no no no not face i am so sorry
411:51 - i for the life of me i can't keep my
411:53 - word straight i'm always doing this
411:56 - uh so not face but suits uh instead of
411:58 - picking
411:59 - uh the three suits we're going to
412:02 - include
412:03 - we could pick the one suit we're going
412:05 - to exclude
412:07 - and there's four ways to pick the suit
412:09 - we're going to skip
412:11 - all right so there's an useful trick to
412:12 - keep in mind uh
412:14 - next up we need to pick the two of a
412:17 - kind
412:18 - card
412:24 - we need to pick the two of a kind face
412:26 - value
412:30 - so we can't pick what we chose before we
412:33 - like for example we couldn't choose 4
412:34 - again
412:35 - so we need to pick one of the 12
412:37 - remaining face values so there's 12 ways
412:39 - to make this choice
412:41 - and then after we pick the face of the
412:44 - two of a kind card we now need to pick
412:45 - the suits
412:47 - so we need to pick two
412:50 - suits and again
412:53 - uh we need to do so where we don't have
412:56 - replacement but order doesn't matter
412:58 - you can't have replacement because there
412:59 - are not two uh king of hearts in the
413:02 - deck there's only one king of heart
413:04 - so you pick so yeah you can't
413:07 - pick the same suit twice uh so um
413:11 - of the four suits we need to choose two
413:14 - of them to include
413:16 - so that's going to be 4 choose 2 4
413:18 - choose 2 evaluates to 6.
413:21 - so in the end this is going to equal
413:27 - 13 times 12
413:30 - times 6 times 4 which is equal to
413:34 - uh 3744
413:38 - uh full house hands
413:44 - all right uh and here's some r code
413:47 - that's computing those quantities
413:49 - uh notice by the way this is a useful
413:52 - trick to
413:52 - to know to be aware of uh when doing uh
413:56 - when when using r notice that i
413:59 - wrapped the entire expression in
414:02 - parentheses
414:04 - if i didn't put the parentheses there
414:06 - nothing would have printed but when i
414:07 - wrap an entire
414:08 - expression in parentheses uh when i'm
414:11 - doing some variable assignment
414:13 - the variable the value of the variable
414:15 - that i just assigned gets printed
414:17 - which is really nice so if you remove
414:20 - these parentheses
414:21 - these parentheses the 624 would not have
414:24 - been printed
414:25 - but since i put the parentheses there it
414:28 - in addition to
414:29 - doing the assignment done here it prints
414:31 - the value of the variable
414:35 - that's a that's a nice trick all right
414:38 - so
414:38 - example 23 a flush is
414:42 - a poker hand where all cards belong to
414:45 - the same suit
414:46 - how many flush hands exist including
414:49 - what's called a straight flush
414:51 - a straight flush is a flush
414:55 - where the cards are also where you can
414:58 - order the car so that they're in
414:59 - sequence
415:00 - uh poker actually has a number of
415:02 - sequences but for example
415:04 - a hand where all the cards are spades
415:07 - and
415:07 - the cards are five six seven eight nine
415:10 - that is a straight flush since they're
415:11 - also in or
415:12 - since you can order them right and the
415:15 - straight flush is considered
415:17 - a different kind of hand in poker that's
415:20 - a straight flush is in fact
415:22 - the best possible poker hand so
415:26 - we actually should be accounting for
415:27 - straight flushes because generally when
415:29 - people say flush they are not
415:30 - including straight flush but we're just
415:33 - going to
415:34 - include straight flushes for now um
415:40 - we're going to allow that possibility so
415:43 - uh
415:43 - for a flush hand we're going to
415:47 - so in a flush hand all the cards have
415:49 - the same suit
415:51 - there are four possible suits so the
415:53 - first thing we need to do
415:54 - is pick the suit
416:01 - and there are four ways to pick the suit
416:04 - after we pick the suit we need to pick
416:07 - the face values
416:17 - so there are 13 possible face values we
416:20 - need to pick
416:20 - five of them to be in our hand so uh
416:24 - once you pick one face value you cannot
416:25 - pick it again
416:27 - because there are no two king of spades
416:29 - for example
416:30 - so there are 13 face values and we're
416:33 - going to choose five of them we don't
416:34 - care about the ordering and we're going
416:35 - to do so without replacement
416:37 - so you multiply those two numbers
416:38 - together uh this is going to be
416:41 - four times uh 1287 that's what that
416:46 - uh 13 choose 5 evaluates to and this is
416:49 - going to equal
416:53 - 5148
416:56 - but this is also including the straight
416:58 - flush hands
417:00 - in the next problem i'm going to show
417:02 - you how we could potentially
417:04 - remove the straight flush hands because
417:07 - i'm actually going to ask that we
417:08 - that we do in fact remove them so a
417:11 - straight
417:11 - in poker is where cards can be arranged
417:15 - in sequence
417:16 - so for example we have five of uh five
417:19 - of spades six of clubs seven of clubs
417:22 - eight of hearts nine of hearts so this
417:24 - is a straight
417:25 - and the suit doesn't matter for a
417:28 - straight
417:29 - unless it's a straight flush if it's a
417:31 - straight flush the suit must be
417:33 - the same because it's also a flush
417:37 - so a straight flush is both a straight
417:39 - and a flush so it has a flush with all
417:41 - cards
417:42 - no it is uh so it is um
417:46 - uh i should change the that wording
417:49 - uh it is a straight
417:52 - it is a straight with all cars belong to
417:54 - the same suit
417:58 - and it is also the best possible poker
418:01 - hand
418:01 - uh hold on okay good
418:07 - all right uh how many straight flush
418:09 - hands exist we're going to compute that
418:11 - number now
418:12 - how many straight flushes exist
418:16 - so to get a straight flush so let's
418:19 - uh let's uh start some separations
418:23 - so first thing we're going to do is
418:24 - compute the straight flush
418:31 - all right so excuse me uh
418:34 - here by the way um is uh the
418:38 - uh the ordering of poker hands because
418:40 - poker
418:41 - poker face values have a ring uh have a
418:44 - ranking
418:45 - uh we have ace two three four
418:48 - five six seven eight
418:52 - nine and i'm gonna write x for ten
418:55 - the roman numeral x then we have
418:59 - uh uh jack
419:02 - queen uh king
419:06 - and also ace again because ace can be
419:09 - both low and high um and in fact the
419:13 - best
419:14 - literal possible poker hand is
419:17 - 10 jack queen king ice all of the same
419:20 - suit
419:21 - that is the best straight flush
419:24 - okay um
419:28 - so uh to have so to
419:31 - so for the flush part we we need to
419:33 - figure out how many ways there are for
419:35 - the flush part
419:36 - and how many ways there are to get the
419:37 - straight part so there are four ways to
419:39 - pick the flush part
419:40 - the part where you have the same suit so
419:42 - we're going to pick
419:43 - the suit for the flush part and then we
419:45 - have the straight part and the straight
419:47 - part
419:48 - for that what i would recommend you do
419:50 - is
419:51 - think about how many ways there are to
419:53 - pick the first card
419:56 - so pick the first card of the straight
420:06 - because if you know that the first card
420:09 - or the lowest card
420:10 - of the straight is ace since you there
420:14 - are five cards in hand that means that
420:15 - the other
420:16 - four cards are two three four five right
420:19 - so how many ways are there to pick the
420:21 - lowest card
420:22 - in the straight so let's see we got uh
420:26 - one two three four five six uh
420:30 - five six seven eight nine
420:33 - oh ten what do you know because the
420:36 - moment you get to ten
420:37 - the the uh the straight hand would be
420:40 - ten
420:40 - jack queen king ace and there is no
420:44 - straight that starts the jack because
420:46 - there is no
420:47 - you would not be able to get the fifth
420:49 - card since you can't circle around back
420:51 - to two
420:52 - so um so that means that there's ten
420:56 - ways to pick the lowest card
420:58 - in the straight so that means that the
421:00 - number of straight flush hands is going
421:02 - to be 4 times 10 which equals 40.
421:05 - all right and i asked also how many
421:09 - straights are possible
421:10 - but this time i'm not including straight
421:12 - flush because straight flush is
421:14 - considered different
421:19 - well uh the first thing we're going to
421:23 - do
421:23 - is pick the lowest card
421:31 - like we did above and then we're going
421:34 - to pick the suits
421:37 - so pick the suits of the cards
421:42 - so we decided there are 10 ways to pick
421:44 - the first card
421:47 - of to pick the first card of the
421:50 - straight
421:50 - and now to pick the suits well if i
421:54 - decide that the first card
421:55 - let's say the ace is going to be clubs
421:58 - and then i pick the next card which is
422:00 - going to be a 2
422:02 - i can still pick clubs again because
422:05 - i took out the the ace of clubs but i
422:07 - didn't take out the two of clubs so i
422:08 - can still pick clubs
422:10 - for that second card so and i can still
422:13 - pick clubs for the third card and so on
422:15 - so basically i do have replacement
422:17 - but order does matter because uh picking
422:20 - the first car there to be the ace of
422:22 - clubs and the second car to be
422:24 - the two of hearts is different from the
422:25 - first car being the ace of hearts and
422:27 - the second card
422:28 - being the two of clubs those are two
422:29 - different hands so order does matter
422:32 - and also um
422:36 - uh you're doing so with replacement
422:37 - there are four possibilities we need to
422:39 - pick
422:39 - five of them so this will be four of the
422:42 - power five
422:43 - the thing though is once i've done this
422:46 - i am including
422:48 - picking the first card to be hearts the
422:49 - second card would be hard the third card
422:51 - to be hard the fourth card would be
422:52 - hearts and the fifth card to be hearts
422:54 - that's a straight flush i don't want to
422:56 - include straight flushes
422:58 - so how am i gonna remove the straight
422:59 - flushes i'm gonna subtract them out
423:04 - so this is straight flushes
423:10 - so just remove them remove them from the
423:12 - calculations subtract them out
423:14 - and you no longer need to worry about
423:16 - them anymore
423:18 - this is basically remember that sum rule
423:20 - that i very briefly mentioned this is
423:22 - basically that sum rule
423:23 - right where you say the total number of
423:26 - like straight flush
423:28 - number of straights including flushes is
423:29 - going to be the number of straight flush
423:31 - and straight non-flush
423:33 - hands so you add those together which
423:34 - means you can do some algebra to get
423:36 - subtraction too
423:37 - anyway uh in the end you calculate this
423:39 - and you get
423:40 - 10 200 uh straight hands
423:44 - excluding the straight flush
423:54 - okay uh we are almost done with this
423:57 - section
423:58 - we've had a long discussion about
424:00 - counting
424:02 - and i haven't really said anything about
424:03 - how this test of what this has to do
424:05 - with probability
424:06 - well this entire section was devoted to
424:09 - the case that you may have thought was
424:10 - the easy case
424:12 - where you have a set
424:15 - where your sample space has finite size
424:18 - your sample space has finite size
424:20 - and you decide that every element in
424:22 - that sample space is equally likely and
424:24 - you may have thought this is the easy
424:26 - case
424:26 - because in that situation it's actually
424:28 - rather easy
424:29 - to compute probabilities you can assign
424:32 - a very natural probability measure
424:34 - the probability of any event a which is
424:37 - a subset of this sample space
424:38 - will be the number of elements in that
424:41 - set a
424:42 - or in that event a divided by the size
424:44 - of the sample space
424:48 - which is a very nice natural probability
424:50 - measure
424:51 - but here's the thing though you need to
424:53 - compute then
424:54 - using these counting techniques the size
424:57 - of the sample space and the size of your
424:59 - set a
425:00 - and that's actually tricky because now
425:02 - you need to use these uh
425:04 - counting techniques and i personally
425:06 - don't think the counting techniques are
425:07 - all that easy
425:08 - all right um so based off of this
425:12 - once we have this natural probability
425:13 - measure we can use counting techniques
425:15 - if we need to
425:16 - to compute probabilities so we're going
425:19 - to use this to compute the probability
425:21 - of the poker hands that we were
425:22 - considering above
425:23 - so the size of pos of the sample space
425:26 - the sample space consists of possible
425:28 - poker hands
425:29 - we say that each of those poker hands
425:31 - are equally likely we do not care about
425:33 - the ordering of poker hands
425:34 - so there's going to be 52 choose five
425:37 - such poker hands
425:39 - which is going to be uh two million five
425:42 - hundred ninety eight
425:44 - uh thousand uh 960 possible poker hands
425:48 - all right so i want to compute the
425:50 - probability of
425:52 - each of the poker hands that we saw
425:55 - in the previous examples so
425:58 - for example 21
426:02 - this was the four of a kind so the
426:05 - probability of a four of a kind hand
426:11 - is going to be the number of four of a
426:13 - kind hands which we computed to be
426:15 - to be 624 divided by the size of the
426:19 - sample space which is 2 million 198
426:22 - thousand
426:23 - 960 poker hands which is going to be
426:27 - approximately 0.0002
426:33 - uh for example 22
426:41 - the probability of getting a full house
426:49 - is going to be uh let's see
426:52 - so probability of full house is going to
426:55 - be 3744
427:00 - divided by two million five hundred
427:02 - ninety eight thousand
427:04 - nine hundred and sixty the number of
427:05 - full house hands divided by the total
427:06 - number of possible poker hands
427:08 - this is approximately zero point
427:13 - zero zero one four
427:16 - uh next up so
427:20 - uh for example 23.
427:27 - so for example 23 we're going to compute
427:29 - the probability of our
427:30 - so-called flush hand a flush that isn't
427:33 - actually of
427:34 - that includes straight flush so just
427:37 - the suit is the same uh how many
427:40 - so there were 5148
427:44 - such hands possible we're going to
427:46 - divide that by the size of the sample
427:47 - space which is that 2 million ish number
427:50 - um and that's going to be approximately
427:53 - 0.002
427:55 - and now for example 24
428:02 - the probability of a straight flush
428:12 - uh that's going to be there were 40
428:14 - straight flush hands
428:16 - divided by the size of the sample space
428:18 - this is going to be
428:19 - approximately 0.00002 really really
428:26 - really small
428:27 - and the probability of a straight
428:32 - so that's excluding the straight flush
428:36 - is going to be
428:39 - 10 200 divided by the size of the sample
428:43 - space
428:44 - which is approximately 0.004
428:50 - there's always kind of this question of
428:51 - which is more likely a flush or a
428:52 - straight
428:53 - turns out the straight is twice as
428:55 - likely as the flush
428:56 - which is which to me is a little
428:58 - surprising it feels it feels
429:00 - intuitively like the straight is harder
429:03 - but it's actually not
429:04 - um and that's one that's a common trait
429:07 - of probability
429:08 - it it defies what you feel like should
429:10 - be true
429:11 - all right so that's it for this section
429:15 - and uh in the next success section we're
429:18 - going to talk about
429:19 - a conditional probability so
429:23 - uh i will see you then
429:31 - since somewhere using the idea
429:34 - of independence you can almost find it
429:36 - anywhere if you look
429:38 - hard enough so independence is an
429:40 - extremely
429:41 - important idea in probability i've heard
429:44 - some people say
429:45 - that independence is what keeps
429:48 - probability as a mathematical theory
429:49 - from just
429:50 - being a subset of analysis so we're
429:53 - going to talk about
429:54 - uh independence if you come away from
429:56 - this lecture
429:58 - not or come away from this class maybe
429:59 - more appropriately
430:01 - not knowing what independence is then
430:04 - that's
430:05 - that's bad that's really bad you must
430:08 - understand independence because it is
430:10 - used everywhere it's one of the most
430:12 - important ideas in probability theory
430:15 - so two events a and b are said to be
430:19 - independent
430:20 - if the probability of a given b is the
430:22 - probability of a
430:24 - so in some sense the information about
430:26 - the event b
430:27 - gives no information about whether a has
430:28 - happened
430:30 - since uh the probability of a knowing
430:33 - that b happened does not change
430:36 - so first off i've said only that the
430:39 - probability of a given b is the
430:40 - probability of a
430:41 - uh what about the probability of b given
430:45 - a
430:45 - and before i proceed i should probably
430:47 - uh make a quick
430:48 - note saying i'm going to assume
430:52 - that a and b are events that have
430:54 - probabilities on their own that are not
430:56 - necessarily zero just because
430:59 - it's easier mathematically to assume
431:01 - that their probabilities are not zero
431:03 - and you kind of have to
431:05 - uh treat the case where they are events
431:08 - with
431:08 - probability zero as a little as a
431:12 - as a separate thing but honestly
431:15 - for for the most part you get the
431:17 - information that you need with
431:18 - just pretending that just ignoring the
431:21 - the chance that
431:22 - uh they have probability zero okay so
431:26 - uh all right then let's uh compute the
431:29 - probability of b
431:30 - given a uh it seems
431:33 - like what should be the case is that if
431:37 - a is independent of b
431:38 - then b should be independent of a as
431:41 - well
431:42 - and in fact that turns out to be the
431:44 - case because we compute the probability
431:46 - of b
431:47 - given a and that's going to be the
431:50 - probability
431:51 - of a and b divided by the probability
431:55 - of a and we then
431:58 - say that the numerator is equal to the
432:00 - probability of
432:01 - a given b uh
432:04 - times the probability of b
432:08 - uh divided by the probability of a by
432:10 - the way
432:11 - uh if you're not recognizing this part
432:13 - uh recalling back to the previous
432:15 - lecture this is
432:16 - a bayes theorem prototype
432:23 - that all right that's some bad bad
432:25 - handwriting up there i cannot
432:27 - i cannot let that go i know that in some
432:28 - of these videos my handwriting isn't
432:30 - great but i cannot let that go
432:35 - so this is a bayes theorem prototype in
432:38 - that you're
432:38 - pretty much one step away from getting
432:40 - base
432:42 - theorem all you would need to do at this
432:44 - point
432:46 - is apply uh the law of total probability
432:49 - to the denominator
432:50 - uh down here in order to get bayes
432:52 - theorem but anyway
432:54 - that aside all right that was a a
432:56 - distraction
432:57 - um we now say that this is the
432:59 - probability of a given b
433:01 - times the probability of b uh divided by
433:03 - the probability of a
433:04 - but we now know that the probability of
433:07 - a given b
433:08 - because a is independent of b that's
433:10 - going to be the probability of
433:11 - a so this is the probability of a times
433:14 - the probability of b
433:17 - uh divided by the probability of b you
433:19 - can a probability of a
433:20 - sorry and you can probably see why we're
433:23 - assuming that these events don't have
433:24 - zero probability because we could end up
433:26 - with division by zero
433:28 - so uh we're going to cancel those
433:31 - out because they appear in the numerator
433:33 - and denominator and we get the
433:35 - probability of
433:36 - b in the end okay
433:39 - all right so that shows what we wanted
433:41 - to want to show so
433:43 - um this implies that
433:47 - if a is uh if a is independent
433:50 - of b
433:54 - so if a is independent of b uh
433:57 - that up that's going to imply that b is
434:00 - independent of a
434:04 - and vice versa all right uh it seems
434:07 - that
434:08 - it would seem to be the case that if a
434:11 - is independent of b
434:12 - and so um heuristically that knowing
434:16 - whether b
434:16 - happened gives you no information about
434:18 - whether a happened it should also be the
434:20 - true
434:21 - for the complement of a if you if
434:24 - um so knowing that b happened should not
434:27 - tell you whether
434:28 - a didn't happen either so
434:31 - this does in fact turn out to be the
434:33 - case because we compute the probability
434:35 - of
434:35 - a complement given b and that's going to
434:37 - be the probability
434:38 - of a complement given b uh
434:42 - we know from uh the previous section
434:45 - this is 1 minus the probability
434:47 - of a given b and the probability of a
434:50 - given b
434:52 - is going to be the probability of a so
434:54 - this is 1 minus the probability of a
434:56 - which we know from section 2 is equal to
434:59 - the probability
435:00 - of a complement so
435:04 - that means that the complement of a is
435:06 - also independent of b
435:10 - so there is an immediate consequence
435:14 - of this definition which is that the
435:16 - probability
435:18 - of a intersected with b is equal to
435:22 - the probability of a times the
435:25 - probability
435:26 - of b you can see that because you could
435:30 - uh potentially say that uh
435:34 - like you would start out before saying
435:35 - the probability of a that this is the
435:36 - probability of a given b
435:39 - but since a is independent of b that's
435:41 - just going to be the probability of a
435:43 - so that means that the probability of
435:45 - intersections turns into the product of
435:47 - probabilities
435:48 - and not only is this a consequence of
435:52 - how i've defined
435:54 - the definition oh i misspelled
435:56 - definition
435:58 - not only is this a consequence of how
436:01 - i uh defined independence
436:04 - independence of events this is
436:08 - actually taken in later courses in
436:10 - higher level probability courses
436:13 - or courses devoted to probability as the
436:15 - definition
436:16 - of independence because the two are
436:19 - essentially equivalent and furthermore
436:24 - what i've highlighted in blue here it's
436:27 - used
436:27 - even more frequently and
436:33 - and also
436:36 - uh it it's uh you have this issue of
436:40 - zero probability events
436:41 - and we don't have that issue here
436:43 - because there's no
436:44 - division taking place okay uh
436:48 - students often want to
436:51 - like i've been encouraging students to
436:52 - think about uh
436:54 - probabilities and and and uh
436:57 - sets and events in terms of venn
437:00 - diagrams
437:02 - and uh is it possible to get a venn
437:07 - diagram
437:08 - uh graphically representing independence
437:11 - it is although it's kind of hard and
437:14 - honestly not
437:16 - that enlightening i'm going to show it
437:17 - to you anyway though
437:19 - first off this is not what independence
437:22 - is
437:23 - this is not a picture of independence we
437:26 - have
437:26 - a we have b and we have our sample space
437:28 - that is not independence
437:30 - this is disjointedness and two events
437:34 - that are disjoint
437:35 - and have nonzero probability are
437:38 - certainly not
437:39 - independent of each other because if you
437:41 - knew that one event
437:42 - one of those two events happened you
437:43 - know that the other one didn't
437:45 - so disjointedness is not the same thing
437:48 - in fact it's almost the opposite thing
437:50 - or implies non-independence
437:54 - so disjointness is not independence it's
437:56 - almost the opposite
437:58 - okay so uh i'm promising you to
438:01 - make a to show what independence kind of
438:03 - looks like
438:04 - as a venn diagram i can kind of torture
438:07 - a venn diagram
438:09 - so what i do is um i'm gonna need to
438:12 - zoom in a lot for this
438:16 - so to attempt
438:20 - to draw uh two sets that are independent
438:24 - of each other
438:25 - i uh draw the sample space as a square
438:30 - i divide it up into fourths
438:35 - and then i'm going to give the set
438:39 - a the upper left corner nothing
438:42 - special about that particular corner it
438:44 - just needs to be a corner
438:47 - and then i give the event b
438:51 - the middle quadrant it should be the
438:54 - same
438:54 - area roughly as
438:57 - a it's just taking up the middle area
439:01 - and
439:04 - the area inside of a
439:08 - and b relative to b is same is the same
439:11 - as a's
439:12 - area relative to the entire sample space
439:16 - so or or that is uh a
439:19 - is take a's area is a quarter of the
439:21 - sample space
439:22 - and a's area and b or a's intersection
439:25 - and b
439:26 - is a is a quarter of b
439:29 - so knowing that you fell into
439:32 - that so basically a corresponds to the
439:35 - probability of falling into the upper
439:36 - left hand corner
439:38 - quadrant and knowing that you are in
439:41 - this middle square does not really tell
439:43 - you whether you fell
439:44 - into that quadrant or not um that gives
439:47 - you basically
439:48 - no information on that so this is a
439:50 - sketch of what
439:51 - independent events might look like if
439:53 - you want to draw them as a venn diagram
439:55 - if this sketch does not make sense to
439:57 - you then don't worry about it just leave
439:59 - it alone
440:01 - because you know it's it's very much a
440:04 - tortured example
440:05 - you have to kind of work hard to get
440:06 - something that looks like this
440:08 - and i'm not super convinced that it's
440:10 - necessarily all that enlightening
440:12 - as to what independence means so if it
440:15 - doesn't make sense to you then just move
440:16 - on don't worry about it
440:18 - uh okay so the next example
440:21 - we're going to consider rolling a
440:22 - six-sided die and i'm going to show that
440:24 - the event a
440:25 - which is that the number uh of pips does
440:28 - not exceed
440:29 - four and the event b that the number of
440:31 - pips is even
440:32 - are independent events so let's uh go
440:35 - ahead and
440:36 - enumerate what falls into uh
440:39 - these two uh separate events we have
440:43 - the event a well not separate just
440:46 - different
440:47 - so the event a uh the number does not
440:51 - exceed
440:52 - four so that includes uh
440:55 - one pip two pips
440:58 - three pips and four pips
441:04 - all right uh the event b which is that
441:08 - you get an even number of pips is going
441:10 - to be
441:11 - two pips four pips
441:16 - uh come on i want the fourth pip
441:23 - i tried and six pips
441:31 - ugh not always cooperative all right
441:34 - um so let's compute the probability of a
441:38 - given b to show that two events are
441:40 - independent it is sufficient
441:41 - to just compute the probability of
441:45 - a given b and show that that equals the
441:47 - probability of a
441:49 - so the probability of a given b is going
441:50 - to be the probability of a
441:52 - and b divided by the probability of b
441:58 - all of these outcomes are equally likely
442:01 - so the intersection of the two
442:05 - sets is going to consist of two
442:08 - and four those are the two things the
442:11 - two sets
442:12 - have in common so this is going to be
442:16 - the dice with face two and the dice
442:20 - with face four
442:28 - why is it so uncooperative
442:32 - okay there we go uh probably two
442:35 - probably four
442:36 - and um divided by uh
442:40 - the probability of b okay and at this
442:44 - point we can basically just count how
442:45 - many elements are in these sets
442:46 - so uh for the intersection there's two
442:49 - outcomes in that set uh and uh for the
442:53 - probability of b
442:55 - there's three so that's going to be
442:56 - three over six so
442:58 - we end up with uh the six is cancelling
443:01 - out and the probability
443:03 - of a given b is equal to two-thirds
443:06 - as a result but the probability of a
443:10 - there are four outcomes in a uh divided
443:14 - by
443:15 - the size of the sample space which is
443:16 - six which is also two-thirds
443:19 - those two numbers are the same therefore
443:22 - these two events
443:23 - are independent of each other
443:27 - okay next up we suppose we have events
443:30 - a1 through a n
443:32 - these sets are said to be mutually
443:35 - independent if
443:36 - and i know that this definition is
443:38 - rather complicated
443:39 - for k less than or equal to n the
443:41 - probability of any
443:42 - subset of that collection of events will
443:45 - become the product of their individual
443:46 - probabilities
443:48 - so it is not sufficient to just
443:51 - check that for all of these events a1
443:53 - through a n
443:54 - uh the probability of the intersection
443:56 - is the probability of probabilities it
443:57 - has to be true
443:58 - for not just the entire collection of
444:01 - events
444:01 - but any subcollection of that collection
444:04 - of events
444:05 - it they all must turn into product of
444:08 - probabilities
444:09 - if this is not true then they are not
444:11 - said to be mutually independent of each
444:12 - other
444:13 - um and here's kind of the reason why um
444:16 - when we were saying mutually independent
444:18 - it what we really kind of want to say
444:20 - is that uh none of these events give
444:24 - any information about any of the other
444:25 - events
444:27 - and so that means it you would want to
444:31 - say that you take any two
444:33 - events in this collection of events
444:36 - and they will be independent of each
444:38 - other you want to be able to say that
444:41 - but it is not sufficient to just check
444:44 - that the entire intersection the
444:46 - probably the entire
444:47 - intersection turns into the product of
444:48 - the respective probabilities
444:50 - and here is an example that shows uh
444:53 - why that is not in fact true um
444:58 - so uh this is an example
445:02 - from uh from actually
445:05 - uh this paper that i've highlighted in
445:07 - red
445:09 - use the diagram below for finding
445:12 - probabilities compute the probability of
445:14 - a and b and c that actually is just
445:16 - going to be
445:17 - that tiny little sliver so probably of a
445:20 - and b and c
445:22 - is equal to 0.04
445:26 - and then we're going to find the product
445:30 - of the of the probabilities of the
445:31 - events a b excuse me
445:33 - and c uh so the probability of
445:36 - a so that's going to be the probability
445:40 - of the blue circle which is going to be
445:43 - 0.1 plus 0.06 plus 0.04 which is 0.2
445:48 - so this will be 0.2
445:51 - the probability of b
445:54 - is going to be the green circle
445:58 - so that's going to be 0.06 plus 0.04
446:01 - that's 0.1
446:02 - plus another 0.1 is 0.2 plus another 0.2
446:06 - is 0.4 so this will be
446:08 - 0.4 and
446:12 - finally we've got the probability
446:15 - of c which is going to be
446:19 - this blue circle here so we've got
446:23 - uh 0.16 plus 0.04 is 0.2 plus another
446:26 - 0.2 is 0.4 plus 0.1 will be 0.5
446:30 - so uh this probability the probability
446:34 - of c is equal to 0.5
446:38 - so uh the probability of a
446:41 - times the probability of b times the
446:45 - probability
446:45 - of c is going to be
446:49 - 0.2 times 0.5 which is 0.1
446:53 - times 0.4 which will be 0.04
446:56 - and those two numbers are equal to each
446:59 - other
447:00 - so it's tempting to say uh that
447:03 - these events are independent of each
447:05 - other but then
447:07 - i ask you to compute the probability of
447:10 - a uh
447:14 - intersected with b
447:18 - so the probability of a intersected with
447:20 - b corresponds to this blue region that
447:22 - i've highlighted
447:23 - which is going to be 0.1 so that equals
447:26 - 0.1
447:28 - but that does not equal
447:32 - the probability of a times the
447:35 - probability of b
447:38 - which is equal to 0.08 and you might
447:41 - and before you say well those numbers
447:43 - are close close doesn't mean a thing
447:45 - i don't care about close they need to be
447:47 - the same
447:48 - so uh yeah they are they are not
447:52 - independent events
447:54 - and as a result these events a b and c
447:57 - are uh not mutually independent
448:00 - could you say that there is some other
448:03 - notion of independence that you could
448:04 - apply
448:06 - i don't really know and i don't really
448:07 - care because i have seen
448:09 - notions maybe like pairwise independence
448:13 - as an alternative notion of independence
448:15 - and i've never
448:16 - seen it ever used in my own
448:20 - life or work it doesn't really seem like
448:23 - it leads to any sort of useful uh
448:26 - useful concepts so yeah
448:30 - that's that's that's that
448:36 - all right uh next example
448:44 - this example is supposed to motivate uh
448:47 - maybe you remember when i was uh talking
448:49 - about flipping a coin take it has this
448:51 - example supposed to motivate uh
448:54 - our assignment of probabilities in that
448:57 - situation
448:59 - so we're going to flip uh remove the
449:01 - eight part that
449:03 - i don't know why that was there i don't
449:05 - know why i have uh
449:06 - eight fair coins so flip fair coins
449:09 - until we get heads
449:20 - and furthermore each flip is independent
449:32 - okay so what is the probability of heads
449:34 - tails heads tail sales heads
449:36 - tails those stills heads uh what in
449:38 - general what would be the probability of
449:40 - a sequence of flips
449:41 - a sequence of n flips to have n minus
449:43 - one tails
449:44 - and a heads at the end so the
449:47 - probability
449:48 - of heads if this is a fair coin is going
449:50 - to be
449:51 - uh one half uh the probability of tails
449:55 - heads
449:57 - and this is by the way getting a little
449:59 - abusive with notation
450:01 - but the but the probability of tails
450:03 - heads is gonna be the probability of
450:05 - tails
450:06 - times the probability of heads since
450:09 - those two flips were independent of each
450:11 - other
450:18 - we are now on to chapter three
450:21 - on discrete random variables and
450:23 - probability distributions
450:25 - this chapter serves both as
450:28 - an introduction to
450:31 - random variables and also an
450:34 - introduction to discrete random
450:36 - variables in particular
450:37 - but many of the concepts that we see
450:40 - here
450:41 - are going to transform is are going to
450:43 - transfer over
450:45 - to the continuous context when we're
450:47 - dealing with continuous random variables
450:50 - so let's get started
450:54 - so in this section we're going to talk
450:57 - about
450:58 - some random variables a random variable
451:01 - which is sometimes abbreviated with the
451:02 - letters rb is a function
451:05 - taking values from the sample space s
451:07 - and associating numbers with them
451:09 - conventional notation for random
451:11 - variables uses capital letters
451:13 - from the end of the english alphabet
451:14 - with lowercase letters
451:16 - while lowercase letters are usually used
451:19 - to denote
451:20 - a non-random value or outcome so
451:23 - up to this point we have been using
451:25 - lowercase letters
451:26 - for uh data and when talking about
451:30 - random variables we're going to switch
451:31 - to uppercase
451:33 - variables or uppercase letters
451:36 - and the distinction does somewhat matter
451:39 - um
451:40 - honestly this is a rule that is very
451:41 - frequently broken although at the same
451:44 - while i say that in this class i'm not
451:47 - planning on breaking that rule all that
451:49 - much uh
451:51 - yeah i mean i i break it all the time in
451:53 - in my
451:54 - research work but this is a situation
451:57 - where
451:58 - it's probably going i'm probably going
452:01 - to stay rather true
452:02 - to it so there's a difference between a
452:05 - random variable whose outcome is unknown
452:08 - and a possible value that random
452:10 - variable could take
452:12 - so uh using the fact that random
452:15 - variables are actually functions like
452:17 - the term random variable is somewhat
452:19 - there's somewhat of a joke
452:21 - that random variables are neither random
452:22 - nor variables because they are
452:25 - uh because random variables are not
452:28 - really variables they're
452:29 - treated as functions and they're not
452:32 - random because if you know what outcome
452:34 - from the sample space you got you know
452:35 - the value of the random variable
452:37 - because the randomness comes not from
452:40 - the random variable x itself but rather
452:42 - from omega which is an
452:43 - outcome from the sample space so
452:47 - when uh so if omega is an outcome from
452:50 - the sample space the notation x of omega
452:52 - equals little x can be used to say
452:54 - the value of the random variable x when
452:56 - the when the outcome
452:57 - little omega occurs is little x
453:01 - so little omega that is a random outcome
453:05 - uh little x is uh non-random
453:09 - and x will equal little x when the
453:12 - random outcome omega occurs
453:21 - all right so the set of omega
453:25 - such that x of omega is equal to x is
453:28 - the event
453:29 - that an element of s is drawn that
453:31 - causes the random variable variable
453:33 - x to equal little x
453:38 - and the set of omega such that x of
453:40 - omega is
453:41 - in some other set a where that other set
453:44 - is often some such a subset of the real
453:47 - numbers
453:48 - is the event that an element of s is
453:50 - drawn that causes the random variable x
453:52 - to assume a value that is in the set
453:55 - a so technically when we want to talk
453:59 - about
454:00 - whether a random variable is in a it
454:03 - takes a value inside of a set or not
454:06 - this is the notation this is what we
454:09 - should be using
454:10 - for that notation we are asking for the
454:13 - probability
454:14 - that we draw an omega that causes x of
454:16 - omega to be in the set a
454:19 - but that is often rather tedious we
454:22 - would rather just say
454:23 - what is the probability that x is in a
454:26 - that's much simpler
454:29 - random variables are commonly classified
454:31 - as being either discrete or continuous
454:33 - discrete random variables or discrete
454:35 - real valued random variables take values
454:37 - in a finite or countably infinite or
454:39 - innumerable if you prefer
454:41 - a set with positive probability so
454:44 - examples of sets that random discrete
454:47 - random variables
454:48 - uh take their values in could be a
454:51 - finite set of numbers
454:53 - such as zero or one it could be
454:56 - the
455:00 - the integers it could be uh the natural
455:02 - numbers such as 0 1 2 3
455:04 - 4 5 6 and so on if it's innumerable
455:08 - then it could potentially be a discrete
455:10 - random variable
455:12 - if the set that it falls into is all any
455:15 - number between zero and one including
455:17 - fractions and including
455:19 - um uh rational numbers and irrational
455:21 - numbers any of those numbers then it's
455:23 - no longer discrete it's going to be
455:24 - considered continuous
455:26 - uh on the other hand continuous real
455:28 - value variables satisfy the following
455:31 - two properties
455:32 - first the random variables take values
455:34 - and intervals which are possibly
455:36 - infinite in length
455:37 - or disjoint unions of intervals of the
455:39 - real line with positive probability
455:42 - that's the first property that
455:45 - continuous random variable satisfied the
455:46 - second is that
455:48 - for any constant c and r the probability
455:50 - that that random variable is equal to c
455:52 - is equal to zero which is kind of a
455:54 - strange assumption you're saying
455:56 - that you know that this random variable
455:59 - this continuous random variable is going
456:01 - to equal
456:01 - some real number but you're saying the
456:03 - probability that it equals any real
456:04 - number is equal to zero
456:06 - well for starters that must be the case
456:10 - because uh this would be this is an
456:13 - infinite set so you need to have
456:15 - some way to assign probabilities
456:18 - and there's too many numbers in the real
456:20 - number system
456:21 - for uh numbers in general to have
456:23 - positive probability
456:25 - and secondly uh
456:28 - there's a way that my probability
456:31 - instructor put it
456:32 - he said if you
456:36 - like you know that someone's going to
456:38 - win the lottery
456:39 - you just know it's not going to be you
456:41 - so
456:43 - by that same logic you know that these
456:46 - random variables will fall inside of an
456:47 - interval you just don't know what number
456:49 - it will be you will never know what
456:50 - number it will be
456:51 - and uh it's highly highly highly
456:53 - unlikely that it will be
456:55 - that particular number you chose so
456:57 - unlikely
456:58 - that that probability is effectively
457:00 - zero uh perhaps the simplest non-trivial
457:03 - random variable
457:04 - is the bernoulli random variable if x is
457:07 - a bernoulli random variable then the
457:08 - probability that x equals one is equal
457:09 - to one minus probably that x equals zero
457:12 - which is p and we say uh
457:15 - that x follows a bernoulli distribution
457:18 - with parameter p
457:19 - that's that's what this we're saying so
457:23 - we have the random variable x and this
457:26 - notation means that it comes from
457:28 - some distribution with some parameter
457:31 - uh we're gonna leave that um alone
457:34 - that that uh verbiage about
457:37 - distributions alone for a minute that's
457:38 - going to be the subject
457:40 - of the next video
457:43 - uh so but basically we just say that x
457:45 - is a bernoulli random variable because
457:47 - it has these properties
457:49 - all right so the set s on which x of
457:51 - omega is defined could be
457:53 - really just about anything uh
457:56 - it's natural to think of bernoulli
457:59 - random variables
458:00 - as being equivalent to coin flips or
458:03 - possibly biased coin flips
458:05 - the thing though is you do not have to
458:09 - necessarily have coin flips for example
458:11 - you could have a probability space
458:13 - where there is a coin flip where if you
458:16 - draw
458:16 - heads this random variable will evaluate
458:19 - to the number one
458:21 - and if you draw tails or you flip and it
458:24 - lands tails up
458:25 - it will evaluate to zero
458:29 - that's one possibility to get a
458:32 - fair or balanced bernoulli random
458:35 - variable where the probability of
458:36 - getting one is 0.5
458:39 - alternatively you could roll a fair die
458:42 - track whether the result of the die was
458:45 - even or odd and in the event of an even
458:48 - number of pips
458:49 - this random variable evaluates to one
458:52 - and the event of an
458:53 - odd number of pips this random variable
458:54 - evaluates to zero
458:57 - either one of those situations could be
458:59 - the case
459:01 - and it is statistically impossible to
459:03 - differentiate
459:04 - between these two possible setups
459:08 - for of random variables and
459:11 - the and and sample spaces
459:15 - so in that case it's almost
459:18 - as if once you know the distribution of
459:21 - the random variable
459:22 - you can pretty much forget whatever the
459:26 - original sample space was and any of the
459:29 - properties of that sample space
459:31 - you can now work in some you can work as
459:35 - if this
459:36 - random variable were the identity
459:37 - function and its sample space
459:40 - is going to be the real numbers or
459:42 - something uh maybe the number zero
459:44 - one so uh in that sense
459:47 - you almost forget all of that stuff that
459:49 - we talked about before
459:51 - um i mean i okay i guess you don't
459:53 - forget about it but you no longer care
459:54 - about the specifics of the sample space
459:57 - and the elements that you're drawing
459:58 - from the sample space the specifics no
460:00 - longer matter
460:02 - so we we get to talk about these things
460:05 - in a very general way all right so for
460:08 - the first example which of the following
460:09 - random variables are likely to be
460:11 - continu
460:11 - considered discrete and which are likely
460:13 - to be considered considered continuous
460:15 - describe the space of outcomes the
460:17 - random variable takes with positive
460:19 - probability
460:20 - so for first case flip a coin record one
460:22 - for heads and zero for tails
460:24 - if this is a situation the sample space
460:28 - for experiment would probably be the
460:30 - sample space consisting of heads and
460:32 - tails
460:34 - and the random variable x when given the
460:38 - outcome
460:38 - h is equal to is equal to one
460:42 - and the random variable x when taken
460:45 - the value tails is going to equal
460:48 - zero so the space of outcomes which i
460:52 - will
460:53 - denote by x of s where you almost feed
460:57 - the entire sample space into this
460:58 - function
460:59 - x is going to be the set
461:02 - uh 0 to 1. by the way the term for
461:06 - uh this is this is the image
461:10 - of the sample space s under the under
461:13 - the random variable x or the function x
461:18 - all right uh this random variable since
461:22 - the space in which its outcomes falls is
461:25 - generally
461:26 - like that's a finite space there's only
461:27 - two numbers in it so this random
461:29 - variable would be considered discrete
461:37 - okay uh next example roll a die
461:41 - record the number of pips showing so the
461:44 - sample space in this situation would be
461:48 - uh die rolls so we got one
461:52 - uh two
461:55 - three
462:00 - four
462:03 - five
462:08 - and six all right
462:16 - so that's the sample space uh x
462:20 - of one would be equal to
462:24 - one x of
462:28 - uh two or two pips would be equal to two
462:33 - and you'd kind of keep going on like
462:35 - this for uh
462:36 - other possibilities so you could say x
462:38 - when you have
462:40 - six pips on your face
462:43 - would be equal to six
462:48 - so this is the reason why we didn't want
462:51 - to write down the numbers one through
462:52 - six
462:53 - when talking about die rolls it makes it
462:56 - easier then to talk about random
462:58 - variables as being a translation
463:00 - from the number of pips showing on a
463:02 - dice face
463:03 - to numbers actual numbers
463:07 - and we would like to be able to make
463:08 - that translation random variables do not
463:10 - need to be defined
463:12 - on numeric spaces it doesn't have it
463:14 - they basically say
463:15 - nothing about whatever's in that
463:18 - original space
463:19 - they don't care at all what's in that
463:21 - original space
463:22 - and once you have random variable you
463:23 - get to work in the real numbers and
463:26 - that's really nice
463:27 - so the image of the sample space under x
463:30 - is going to be the numbers 1 2
463:33 - 3 4 5 6.
463:40 - and that suggests since since this is a
463:43 - finite set
463:44 - that this is a discrete random variable
463:50 - all right next example roll a die record
463:54 - one for an even number of pips and
463:55 - negative one for uh
463:57 - for an odd number of pips okay
464:00 - so the sample space is the same as
464:02 - before so we're gonna copy copy that
464:04 - sample space down
464:05 - so x of one
464:09 - is equal to x of three
464:15 - which is equal to x of five
464:22 - in all of these cases x is going to
464:25 - come out as negative ones and to have an
464:27 - odd number of pips
464:29 - uh in the case of two uh
464:36 - uh that's so x evaluated for the dice
464:39 - with two pips
464:41 - is going to be the same as when there's
464:43 - four pips which is going to be the same
464:45 - as when there's uh six pips
464:53 - in all of these situations the random
464:56 - variable x comes out as
464:57 - one so the image of the sample space
465:01 - under this random variable will be
465:04 - negative one and one and again this is a
465:07 - discrete random variable
465:16 - okay uh the time in minutes needed to
465:20 - complete a race uh
465:23 - it seems appropriate to say here that
465:25 - the sample space is going to be
465:29 - some uh positive real number which we'll
465:31 - put with the
465:32 - r plus which will be which is equal to
465:36 - the set
465:36 - uh zero to infinity including zero
465:40 - and in this case so
465:43 - it seems like the the the random
465:46 - variable is going to be the identity
465:47 - function where it takes one of the
465:48 - numbers from the space and just spits
465:49 - out that exact same number
465:51 - so in this case x of
465:55 - uh omega is equal to omega which is in
466:00 - uh the which is in the uh
466:05 - posit or non-negative real numbers so
466:08 - that means that
466:09 - the image of the sample space
466:12 - under x is going to be the positive real
466:14 - numbers
466:15 - or the non-negative real numbers and
466:17 - this is a continuous space
466:20 - so this is going to be a continuous
466:22 - random variable
466:24 - and basically what we're saying is uh
466:27 - the time it takes to
466:28 - complete the race can be any number from
466:32 - 0 to infinity
466:33 - uh not just integers but also including
466:36 - fractions and algebraic numbers and
466:39 - transcendental numbers every single
466:41 - possible
466:42 - real number and since the real numbers
466:43 - are an uncountable space
466:45 - that means that this random variable is
466:47 - going to be continuous
466:49 - uh for similar reasons number five the
466:52 - length in centimeters of a hair plucked
466:53 - from a person's head
466:54 - uh we could say um that the sample space
466:59 - is going to be uh hairs
467:02 - no no no no not x sorry
467:05 - the sample space will consist of hairs
467:15 - and the a random variable x
467:18 - from that sample space which i'm going
467:21 - to draw
467:22 - a hair okay that's the thing
467:25 - all right uh so x when given a hair
467:28 - uh gives you uh a number
467:33 - so the length of the hair
467:41 - and so it suggests that the
467:44 - image of the sample space under x is
467:47 - going to be the positive real numbers
467:49 - or the non-negative real numbers again
467:52 - since hairs can be in principle of any
467:53 - length and it's an uncountable length
467:56 - so i mean idea all right it's it's going
467:59 - to be true
467:59 - that there's a finite number of hairs in
468:02 - the world and therefore a finite number
468:03 - of hair lengths
468:05 - so if we were being super super duper
468:09 - technical
468:09 - we would say actually this is a discrete
468:11 - random variable because there's only a
468:12 - finite number of possible hair lengths
468:14 - since there's only a finite number of
468:16 - pairs but that seems ridiculous that
468:17 - seems like a ridiculous model that seems
468:19 - like way too much complication
468:21 - continuous random variables are
468:22 - continuous because it's easier to work
468:24 - with continuous things than discrete
468:25 - things
468:26 - and you're probably going to agree with
468:27 - that when we start
468:29 - doing all the work for the discrete
468:31 - random variables and all the work for
468:32 - the continuous random variables and see
468:34 - oh it's actually not
468:35 - it's actually somewhat easier to work
468:37 - with continuous stuff
468:38 - so uh we're going to say that it's the
468:40 - real numbers in which case this is
468:42 - a continuous random variable
468:50 - okay uh next up roll two dice record the
468:54 - sum of the number of pep showing
468:56 - uh i'm not gonna bother writing out the
468:57 - sample space but i'm going to say that
468:59 - uh x could be like say if you gave x the
469:03 - numbers
469:03 - uh one and four it's going to add up the
469:07 - pips
469:08 - showing on those two dice and give you
469:10 - the result five
469:11 - so the image of x under the sample space
469:14 - is going to be the numbers
469:16 - from 2 to 12.
469:20 - and since this is a finite set this is
469:23 - going to be
469:28 - discrete okay next up
469:31 - flip a coin until h is seen and count
469:32 - the number of flips so the sample space
469:35 - consists of heads tails heads
469:39 - tails tails heads uh and so on
469:44 - so the si so the random variable x when
469:46 - given one of these uh strings let's say
469:48 - tails tails heads
469:50 - uh is going to evaluate to this
469:53 - to the length of the string which means
469:56 - that x when fed this sample space its
470:00 - image is going to be
470:01 - the natural numbers which are the
470:03 - numbers uh
470:05 - 1 2 3 and so on
470:08 - and this is a discrete space because the
470:12 - because there's an innumerable amount
470:15 - of numbers in this space and since it's
470:18 - innumerable
470:20 - that means it's going to be discrete so
470:22 - this is a discrete sample this is a
470:23 - discrete random variable
470:30 - by the way if you're not familiar with
470:31 - the word innumerable that means
470:33 - listable as then you can list it out and
470:35 - even though it will take you
470:37 - an infinite number of years to list out
470:41 - everything in this sample space or in
470:43 - this uh
470:44 - set there will you will hit everything
470:48 - in that set a in finite time
470:52 - so every possibility the same cannot be
470:55 - said for the real numbers by the way
470:57 - if even if given infinite number of
470:59 - years you will not hit every value
471:01 - if you started trying to list them out
471:04 - so
471:04 - that's actually a very deep result in
471:07 - set theory
471:08 - or i don't know about i don't know if
471:10 - deep is quite the right word for it but
471:11 - it's certainly an important result
471:13 - so uh we're just going to take it for
471:16 - granted here that that is the case
471:18 - example two consider an experiment of
471:20 - rolling two six-sided die
471:22 - define two random variables for this
471:24 - experiment are they continuous or
471:25 - discrete
471:26 - we can define multiple random variables
471:28 - for the same sample space
471:29 - and there are advantages to doing so
471:31 - because when doing so we can talk about
471:33 - notions such as correlation
471:35 - or study of the behavior different
471:36 - random variables in the same space
471:40 - consider different possibilities i'll
471:42 - talk about their joint distribution
471:44 - together
471:45 - stuff like that so as a reminder the
471:48 - sample space consists
471:50 - of dice faces so we could have for
471:52 - example one one
471:54 - uh one two and we would keep going on
471:58 - like this
471:59 - you've seen in previous videos how to do
472:01 - this until eventually we listed out
472:04 - 6 6.
472:10 - okay so this is our sample space what's
472:13 - one random variable we could define we
472:14 - could say that
472:15 - x the random variable x when given some
472:18 - outcome omega in the sample space
472:20 - is equal to the sum of the pips
472:28 - whereas if we gave uh no let's not call
472:31 - the other random variable x we'll call
472:32 - it y
472:34 - that y will be another random variable
472:35 - defined on this space and it will be the
472:37 - max
472:39 - of the pips
472:43 - so for example uh x
472:46 - of the dice face
472:49 - of the combination one and two
472:52 - will equal three because that's the sum
472:55 - whereas y
472:57 - of one and two
473:00 - will equal two because the maximum of
473:03 - the two
473:04 - of the number of pips is going to be two
473:07 - okay so that's it for this section this
473:09 - was a basic introduction
473:11 - into what
473:31 - one uh we are on to the next section in
473:33 - our chapter on
473:35 - uh discrete random variables
473:38 - and at this point you're probably
473:39 - thinking okay we got these things called
473:41 - discrete random variables
473:43 - uh all right what's the point why why do
473:46 - we have these things
473:47 - random variables what why do why does
473:49 - anyone care about this
473:50 - this seems like an extra complication
473:54 - on uh this uh on these probability
473:57 - spaces we we were still
473:59 - able to talk about probability we
474:00 - haven't really
474:02 - added really anything so far well
474:05 - random variables truly are and
474:09 - and an addition that makes things much
474:11 - better and allows you to say
474:13 - additional things about uh
474:16 - randomness uh once you have random
474:19 - variables you are now
474:20 - allowing for um
474:24 - concepts such as um
474:28 - uh you're allowing for concepts such as
474:32 - we have uh we we have a
474:36 - phenomena that is essentially uh
474:40 - like phenomena that is essentially the
474:42 - same the same except for a couple
474:44 - parameters
474:45 - like there's a few parameters that we
474:48 - need to figure out and once we know
474:50 - those parameters we basically know
474:52 - everything there is to know about this
474:54 - phenomena or
474:55 - there's concepts such as uh expectation
474:58 - or
474:58 - mean uh you in order to be able to talk
475:02 - about expected values you need to have
475:03 - random variables
475:04 - so once we've introduced random
475:07 - variables
475:09 - uh these things that take inc
475:12 - that take uh
475:15 - things in our sample space and turn it
475:17 - into numbers you now gain a lot of
475:19 - additional structure
475:21 - the first thing that we get that is an
475:23 - essential property
475:25 - of uh random variables is a probability
475:28 - distribution so a probability
475:29 - distribution
475:30 - for a random variable is a function that
475:33 - describes the probability that a random
475:35 - variable takes on certain values
475:37 - discrete random variables are determined
475:38 - completely by
475:40 - the probability mass function which is
475:42 - abbreviated
475:43 - pmf and the probably mass function
475:47 - for a random variable is p of x
475:52 - which is equal to the probability that
475:54 - that the random variable x
475:56 - capital x so remember that capital x is
475:58 - referring to a random variable
476:00 - this is a thing that hasn't taken a
476:01 - value at yet or we don't
476:04 - view it as being equal to anything per
476:07 - se
476:07 - at this moment uh but we're asking for
476:10 - the probability that this random
476:12 - variable
476:12 - when we actually evaluate it and get a
476:14 - number out of it is going
476:16 - to equal x
476:19 - i like to think of random variables as
476:21 - uh they will have a future value
476:24 - but they don't really have a value right
476:27 - now when we're asking for their
476:28 - probability and stuff
476:31 - so we're asking what could this thing be
476:33 - in the future
476:35 - so uh a coin flip
476:38 - is random before you make the flip
476:41 - if that makes sense after you've made
476:45 - the flip
476:45 - then it's no longer random because you
476:47 - get to see whether it was heads or tails
476:49 - which actually is um
476:53 - getting more again into the issue of
476:56 - what does probability actually mean like
476:58 - for example
476:59 - uh let's suppose that we flip a coin
477:03 - and it lands heads up and then we cover
477:06 - it up we don't get to look at it
477:07 - we never saw what the coin did we
477:09 - immediately the moment it lands
477:11 - on the ground uh covered up in a box
477:15 - in principle uh we would say that that
477:18 - under this uh frequentest notion of
477:20 - probability
477:22 - we should say that this random variable
477:24 - has a value and is no longer
477:26 - random we just don't know what it is
477:28 - whereas
477:29 - if you're adopting maybe the gambler's
477:31 - notion of random
477:32 - of randomness and probability you might
477:35 - still
477:36 - view it as being random where you can
477:39 - start placing a bet on whether it's
477:40 - heads or tails when you take the
477:42 - lid off of the uh when you when you take
477:45 - the box off of the of the coin and then
477:46 - actually observe it
477:48 - uh i'm not gonna talk anymore about that
477:49 - i've already recorded a half hour video
477:52 - about the interpretation of probability
477:53 - probability
477:54 - and you can watch that if you want to
477:56 - learn more uh but
477:59 - uh all right so i'm just saying this
478:02 - because i feel like
478:03 - students especially with this notation
478:06 - this notation especially when you're
478:08 - starting out can
478:10 - bother students and they're wondering
478:12 - what capital x is and what little x is
478:14 - and what's the difference between them
478:15 - and the difference is this is random and
478:16 - we don't actually know what it is
478:18 - whereas little x is something that is
478:21 - fixed
478:22 - and we know what little x is so um
478:24 - really when i'm writing this down little
478:26 - x is going to be substituted for a
478:28 - number like for example there's going to
478:29 - be p
478:29 - of 1 which is going to be the
478:31 - probability that capital x is equal to
478:34 - 1. like at some point we're going to
478:37 - make that substitution
478:38 - um so basically the little x right here
478:43 - is going to get substituted with a
478:44 - number but the
478:46 - capital x is never going to get
478:48 - substituted and that's always going to
478:50 - be viewed right now as being random
478:52 - and we don't really know what its value
478:54 - is going to be and we're just
478:56 - studying uh what its value could
478:58 - possibly end up being and how likely it
479:00 - will end up
479:01 - taking certain values uh sorry if i'm
479:04 - going on too long about this
479:06 - this is just something that i know that
479:08 - students at some level struggle with
479:10 - and i try over and over again to try and
479:13 - explain it
479:14 - and i'm never fully satisfied with my
479:16 - own explanation
479:18 - okay um the probably mass function one
479:22 - way to visualize
479:23 - a probably mass function is using a line
479:25 - graph
479:27 - where a line is placed on each point x
479:29 - of r that x takes a positive probability
479:31 - extends to the height
479:32 - representing p of x um okay so before i
479:35 - draw a visualization i'm going to say
479:38 - that we are totally allowed
479:40 - to say all right we've got inputs x and
479:43 - outcomes p of x
479:45 - where p of x is a function that gives us
479:47 - the probability this random variable
479:48 - will equal x
479:49 - and we can construct a table if we
479:50 - wanted like for example 0
479:53 - 1 2 3 4. we could construct a table
479:56 - of possible inputs to this function and
479:59 - for possible outputs we could say
480:02 - uh let's see uh what says something we
480:04 - could do
480:05 - uh we'll just say that all of these are
480:07 - equally likely so all of these are
480:08 - one-fifths so this function puts out
480:10 - one-fifth all the
480:10 - all the time well yeah
480:14 - should we always do that uh we might say
480:17 - this is two-fifths
480:19 - and this is one-tenth and this is
480:21 - one-tenth there
480:23 - i think that's okay does this still add
480:25 - up to one this must add up to one by the
480:27 - way
480:27 - actually that's the thing we're going to
480:28 - talk about in the future let's see two
480:30 - one-fifth plus one-fifth is two-fifths
480:32 - plus two-fifths is four-fifths
480:33 - plus two-tenths is another fifth so that
480:36 - does in fact add up to one
480:39 - which probably mass functions if you add
480:41 - them up
480:42 - if you add up all of their non-zero
480:44 - values
480:45 - then they must always add up to one
480:47 - always
480:48 - probably mass functions always add up to
480:50 - one if they don't add up to
480:52 - one then they're wrong they're not
480:54 - probably mass functions
480:56 - if you ever compute a probably mass
480:58 - function and it doesn't add up to one
481:00 - then it's not a probably mass function
481:01 - and i don't care if it's close
481:04 - it closes close as nothing because it's
481:06 - not one
481:07 - one is one all right um
481:11 - okay and and i suppose we're allowed to
481:13 - say all right let's suppose i threw in
481:15 - a fifth value then the probably mass
481:17 - function will be zero
481:18 - and presumably anything that's on this
481:20 - table if i didn't list it out
481:22 - then the s function is zero right so
481:25 - anything not written down is zero all
481:28 - right
481:28 - uh but then we can visualize a
481:30 - probability mass function using a line
481:32 - graph
481:34 - so we've got possible x values that this
481:37 - thing could take
481:38 - and we've got it's probably mass
481:39 - function uh it's it's probability at
481:41 - those points
481:42 - so we could have x equals zero one two
481:46 - three four uh and then for those let's
481:50 - let's see i've i've already got
481:52 - a lot of what i need so uh we'll do
481:55 - one tenth two tenths three tenths
481:59 - four tenths so zero is going to be two
482:02 - tenths so that's about here
482:04 - one is going to be two tenths again and
482:06 - then we got four tenths
482:08 - and then one tenth and one tenth this is
482:10 - a visualization
482:12 - of the probably mass function that i
482:14 - wrote on the right hand side
482:16 - of the page so i generally am like
482:20 - drawing a dot
482:21 - at the probability and then drawing a
482:22 - line up
482:24 - uh to the probability of the random
482:26 - variable equaling that value
482:28 - or something like that
482:31 - so yeah uh that's uh how you can
482:34 - visualize it
482:35 - there's also probably histograms which
482:38 - are very similar to line
482:39 - to the line graphs and very similar to
482:40 - the histograms that i was discussing
482:43 - uh several lecture videos ago where you
482:46 - could
482:47 - instead of having these uh lines
482:51 - uh draw i like these the the
482:54 - the table that i have is a lot like the
482:57 - the relative frequencies
482:58 - that i was discussing when discussing
483:00 - histograms so what you would do is draw
483:02 - something similar to the relative
483:03 - frequency of those uh
483:05 - of uh those um uh possible
483:08 - values so we got 0 1 2
483:12 - 3 4 and we draw
483:16 - a bar that is
483:20 - uh centered on that point so we got uh
483:24 - uh so we've got something going up to
483:26 - two tenths
483:28 - and two tenths again and then
483:31 - four-tenths and then two one-tenths
483:37 - yeah so there we go uh so uh there's
483:40 - that
483:42 - uh this is another way to visualize the
483:43 - problem distribution if you
483:45 - if you prefer it um there's really no
483:48 - difference
483:49 - um in fact i would say that edward tufte
483:51 - would probably say that they are the
483:52 - same graph essentially it's just
483:54 - uh the one up here it should be
483:56 - preferred because it uses less ink
483:58 - okay uh continuing on uh let's see some
484:01 - examples
484:02 - uh a fair coin is flipped we define a
484:05 - random variable x
484:07 - uh when h occurs x evaluates to one and
484:10 - when t
484:11 - occurs x evaluates to 0 find the problem
484:14 - as function of
484:14 - x which is p of x visualize p of x
484:18 - through with a line graph and you're
484:20 - thinking
484:21 - how do we know what the probabilities
484:23 - are
484:24 - well we know because i said this is a
484:27 - fair coin
484:28 - and since i know that it's a fair coin i
484:30 - know that the probability of heads is
484:31 - one and probably a tails of zero
484:33 - okay so uh i'm going to start out
484:36 - actually with that tabular form
484:37 - so we've got x and p of x
484:41 - so we've got so possible values that x
484:44 - that the random variable x could take
484:48 - are one and zero so we're going to put
484:50 - zero
484:51 - and one as potential values for this
484:53 - random variable
484:54 - uh this random variable will equal zero
484:57 - if the coin comes tails up
484:58 - and since this is a fair coin the
485:00 - probability of getting tails so
485:02 - actually maybe i should be more verbose
485:03 - about this and say that the probability
485:06 - that x equals zero is
485:10 - equal to the probability of drawing an m
485:13 - and omega from the sample space an
485:15 - outcome from the sample space
485:17 - such that x when evaluated at that
485:20 - outcome equals zero i'm being very
485:23 - verbose about this
485:24 - uh what are such outcomes that causes us
485:27 - to evaluate to zero well we know from
485:28 - the definition of the problem
485:30 - that such outcomes are only tails
485:33 - so this is the probability that you get
485:35 - tails and the probability of tails
485:36 - because there's a fair coin is one half
485:40 - so that means that um x is
485:43 - so at z at zero the probably mass
485:46 - function will be one half
485:47 - and at one the probability mass function
485:49 - will also be one half because
485:51 - well a uh this random variable
485:55 - um well for starters this random
485:57 - variable is going to be one when the
485:58 - coin lands heads up and the probability
486:00 - of heads is
486:01 - one half that's one way to think about
486:04 - it
486:04 - and another thing to think about it is
486:07 - there's only two
486:08 - numbers that this random variable could
486:10 - take with positive probability
486:12 - zero and one zero
486:15 - it's probably mass function is one half
486:16 - so at one it must be whatever it takes
486:19 - to cause the probably mass function to
486:21 - add up to one so one minus one half
486:24 - meter is going to be one half
486:25 - and thus the other value is going to be
486:28 - one half
486:29 - uh so uh we we visualize this with a
486:31 - line graph
486:33 - uh we'll go ahead and make this one half
486:35 - uh what we end up having for
486:37 - our visualization of the probability
486:39 - mass function is we have lines
486:41 - uh extending up to uh
486:44 - one half all right and that's our
486:47 - visualization for it
486:51 - okay uh this by the way is a complete
486:54 - description
486:56 - of the problems function if we are
486:58 - willing to say that anywhere isn't
487:00 - anywhere we don't list uh the probably
487:02 - mass function
487:03 - evaluates to zero because like like if
487:07 - that should make complete sense to you
487:09 - because let's say what is going to be p
487:12 - at one half well that is the probability
487:16 - that x equals one half
487:20 - which equals the probability of drawing
487:23 - an outcome from the sample space
487:25 - that causes the random variable to
487:28 - evaluate
487:29 - to one half okay so we know
487:32 - that there are two outcomes in the
487:34 - sample space
487:35 - which are heads and tails
487:38 - and x is a random variable and therefore
487:41 - it is a function so
487:42 - you know that functions uh when given
487:46 - one input give you only one output only
487:48 - one output will come out
487:50 - so what does that mean here uh well
487:54 - we know what this function will be at
487:57 - heads which is one outcome in the sample
487:58 - space
487:59 - we know how this function will be at
488:00 - tails which is the other outcome in the
488:02 - sample space
488:03 - so what outcome causes this random
488:06 - variable
488:07 - to equal one-half because neither of
488:09 - those caused the function to evaluate at
488:11 - one-half
488:12 - so that means that the probability of
488:16 - drawing an outcome from the
488:18 - sample space that causes this to
488:19 - evaluate to one-half
488:21 - is the probability of the empty set
488:24 - because the the set of all numbers
488:28 - or not numbers the set of all outcomes
488:30 - in our experiment that causes this
488:32 - random variable to evaluate to one half
488:35 - is the empty set because there is no
488:37 - such outcome
488:39 - so you end up computing the probability
488:40 - of the empty set the probability then of
488:42 - the empty set is zero
488:45 - so that would mean that anything that
488:46 - isn't listed here
488:48 - uh it's natural to say that the problem
488:50 - s function is zero
488:52 - okay okay okay
488:56 - uh moving along
489:00 - moving along there is an
489:03 - r package called discrete rv
489:07 - and this package allows for
489:11 - users to define random variables and
489:13 - work with them
489:15 - and i think this package is
489:16 - pedagogically useful
489:18 - but for serious work with these random
489:22 - variables i wouldn't recommend using it
489:24 - um i uh i've uh i actually just made a
489:28 - few minutes ago
489:30 - well it wasn't a few minutes ago it was
489:32 - more like a few hours ago i just made a
489:34 - few hours ago
489:35 - uh lecture videos for the lab for r uh
489:38 - are on my our introduction on
489:40 - introductory videos um
489:42 - functions for dealing with probability
489:44 - in r and dealing with a lot of random
489:46 - variables and classes of random
489:47 - variables families random variables and
489:49 - i never use this package
489:51 - because it's more for allowing students
489:54 - a laboratory to work with
489:56 - random variables in the notation that
489:59 - we're using
489:59 - in the lecture class or a notation very
490:01 - similar to it
490:02 - and not actually meant for serious work
490:05 - trying to compute
490:07 - trying to work with the cdfs and the
490:08 - pdfs and expectability and all that
490:10 - stuff
490:10 - of these random variables but it's kind
490:13 - of nice
490:14 - so for example in this situation i could
490:17 - define the random variable x
490:19 - and say that this is a random variable
490:21 - with the rv function
490:22 - its possible values are zero and one
490:26 - and the probability of getting those
490:28 - outcomes are each one half
490:30 - and it will print and make a nice uh
490:33 - output
490:34 - a printout for that basically
490:35 - summarizing what i just said
490:37 - and in addition to that when i tell r to
490:39 - plot this random variable
490:41 - it creates the plot of the probability
490:44 - mass function
490:46 - so that was all very nice uh next
490:48 - example
490:49 - let s be the sum of the number of pips
490:53 - rolled onto dice find p of s
490:56 - and plot it okay okay
491:00 - so let's uh come so let's uh form our
491:03 - table again
491:04 - so we've got s and we've got
491:07 - p of s
491:11 - okay so s are possible sums of the dice
491:15 - so what are some possible sums of the
491:17 - dice well
491:19 - uh the smallest it could be is two
491:20 - because that's what happens when you
491:21 - roll snake eyes or one and one
491:23 - so the poss the smallest possible sum is
491:26 - two and the largest one happens when you
491:28 - roll box cars which is
491:30 - both of them are six so that will be
491:32 - twelve so it's going to be everything in
491:33 - between so two
491:34 - three four five six
491:38 - 7 8 9
491:42 - 10 11 12. uh
491:45 - hold on hold on uh
491:49 - i didn't write that quite right 10
491:53 - 11 12. okay
491:57 - and now we need to figure out
492:00 - uh the probably mass function okay
492:04 - so we're saying we we could imagine that
492:06 - there is a sample space
492:08 - that's consisting of a
492:11 - dice of combinations of two dice like we
492:15 - got one one
492:16 - uh uh one
492:19 - [Music]
492:20 - two
492:23 - and two and going on
492:27 - and uh we got like six six we've already
492:30 - worked with this
492:31 - uh type of uh sample space in previous
492:34 - videos
492:36 - and i don't want to go into too much
492:37 - more detail into it because it can get
492:39 - kind of tedious so um
492:43 - we know that there are 36 outcomes
492:46 - in this uh sample space so
492:50 - and we're saying that everything is
492:51 - equally likely so therefore
492:53 - uh the probability of the event the dice
492:55 - add up to two is going to be the number
492:56 - of outcomes
492:58 - uh where the two dice add up to two
493:00 - divided by the size of the sample space
493:02 - so how many outcomes are there with the
493:04 - die set up to two well
493:06 - you can you can get uh snake eyes and
493:08 - that's it
493:09 - uh so there's only one outcome that
493:12 - corresponds to that and then we divide
493:13 - it by 36.
493:15 - uh how about three we could either roll
493:18 - one the left dice and two on the right
493:19 - dice or i guess let's uh
493:21 - uh re-adopt that uh blue dice red dice
493:26 - uh verbiage and we could say
493:29 - uh the blue dice rolls a one and the red
493:32 - dice rolls a two
493:34 - or we could have the blue dice roll a
493:36 - two and a red dice to rule a one
493:38 - and that's it otherwise it will not be
493:40 - it will not add up to three
493:42 - so that's two outcomes
493:45 - that correspond with this so we're gonna
493:47 - have 2 over 36
493:49 - and for 4 we could either have 1 and 3 2
493:52 - and 2 or 3 and 1 so that would be
493:54 - 3 over 36 and that's going to keep going
493:59 - so we would have 4 over 36 4 5
494:03 - 5 over 36 for 6 and
494:06 - 6 over 36 for 7. uh all right
494:09 - 12. there's only one outcome where you
494:12 - can get 12 and that's box cars 6 and 6.
494:14 - so this will be 1 over 36 for 11. you
494:17 - could get 5 and 6 or 6 and 5. so we'll
494:19 - have 2 over 36
494:20 - and you can see the pattern it's going
494:22 - to become 3 over 36 for 10
494:25 - uh 4 over 36 for nine
494:29 - and uh five over 36 for eight
494:32 - okay and now we're ready to create our
494:35 - little visualization
494:37 - so we got two three uh
494:40 - four five six
494:43 - seven eight nine ten
494:47 - eleven uh twelve
494:51 - uh that is an awful looking eleven i
494:54 - really did try harder
494:56 - but sometimes the screen doesn't want to
494:58 - cooperate
494:59 - all right and uh let's see
495:02 - for the y-axis we could go
495:05 - uh the highest you ever go is six over
495:07 - 36 so we could go one
495:08 - two three four five six
495:12 - there we go so six over 36 is at the top
495:16 - so for our probability so we'll go one
495:20 - two three
495:24 - four five
495:28 - six five
495:31 - four three
495:36 - two one
495:40 - okay and that's it that's our probably
495:42 - mass function
495:44 - so um using this discrete rv package
495:48 - uh we've got what we could do is create
495:52 - a random variable representing a single
495:54 - die
495:55 - so that's created here and then we could
495:58 - say all right
495:59 - s is the sum of independent and
496:01 - identically distributed copies
496:03 - of the random variable d that is getting
496:06 - into
496:07 - verbage that uh and terms and ideas that
496:12 - i haven't really talked about yet
496:13 - but basically you add up two independent
496:15 - dice which is kind of what's going on
496:17 - in this experiment and then i tell it to
496:20 - plot it
496:20 - and it makes a plot and that's a very
496:22 - good plot all right
496:24 - uh next up consider flipping a fair coin
496:27 - until heads is seen let n be the number
496:30 - of flips
496:30 - find a probably mass function describing
496:32 - the distribution of n and plot the first
496:34 - few values
496:36 - of the pmf and um we've actually talked
496:40 - about
496:41 - uh this type of random variable before
496:45 - a number of times in the previous
496:46 - chapter uh as i mentioned there
496:49 - it's one of my favorite random variables
496:52 - to refer to since
496:54 - on the one hand it's not like the setup
496:56 - is quite simple to understand you flip a
496:58 - coin until you get heads that doesn't
496:59 - that's not that doesn't take a great
497:01 - deal of imagination
497:02 - and yet you can still get a lot of
497:05 - richness
497:06 - out of it and the mathematics can get
497:09 - kind of involved
497:10 - so um
497:14 - okay so in this case what we ended up
497:16 - coming up with before
497:18 - and suggest what we should have now in
497:20 - fact maybe if you go back to that video
497:23 - and look at how it was uh showing that
497:25 - the probably the sample space under some
497:27 - probably measure
497:28 - um will in fact be one i actually kind
497:31 - of defined a prototype random variable
497:33 - n of omega uh that was uh
497:37 - measuring the length of the string and
497:39 - that was
497:40 - that's basically a random variable right
497:42 - there
497:43 - um i defined a random variable on that
497:45 - space so that i could compute
497:46 - show that the probability um of that
497:49 - space added up to one
497:51 - so yeah they're very useful things but
497:53 - basically we could just say that this is
497:55 - one half to the power
497:56 - n for when
498:00 - n is a natural number
498:04 - uh otherwise you would just assume that
498:06 - this thing is going to be zero
498:08 - so this is a natural probability mass
498:10 - function for this space
498:12 - we actually showed in that section that
498:14 - it adds up to one
498:16 - and uh yeah so okay so uh let's um
498:20 - uh plot this uh probably mass function
498:23 - we got
498:24 - possible values a half a fourth an
498:26 - eighth a sixteenth
498:28 - a thirty second so uh one
498:32 - two three four
498:36 - five so for one we go up to a half
498:41 - for two we go up to a fourth
498:44 - for three we go up to an eighth for four
498:47 - we go
498:47 - up to a sixteenth and for five we go up
498:50 - to thirty second
498:51 - and in principle this this graph goes on
498:54 - forever
498:55 - but we're just going to stop at 5.
499:09 - section is on expected values let's
499:12 - start by deciding
499:13 - defining what an expected value is the
499:16 - expected value of a discrete random
499:18 - variable
499:18 - uh which we are often dealing with e x
499:22 - is given below so the expected value
499:26 - of x is equal to
499:29 - the sum of x times the probability mass
499:34 - function of x wherever the problem mass
499:38 - function
499:39 - of x is non-zero
499:45 - this is just a more general way to write
499:47 - down what expected values are
499:50 - in principle most of the time the x over
499:53 - what you're summing
499:54 - are the integers so if you really wanted
499:56 - to most of the time you'll be okay
499:58 - thinking of expected values as this like
500:00 - um
500:02 - x equals let's say zero to infinity
500:05 - or another way we could do it is
500:08 - this is actually allowed uh x equals
500:11 - negative infinity to infinity
500:14 - um or if you know what that um
500:18 - your little x ranges from a to b you
500:20 - could say
500:21 - uh we could say
500:25 - x equals a to b there's all sorts of
500:28 - ways we could possibly rearrange this
500:30 - but really what matters is that you're
500:32 - summing up
500:33 - uh this expression x times the
500:36 - probability mass function
500:37 - of x wherever that probability mass
500:40 - function
500:42 - is non-zero so i'm just going to leave
500:45 - this as
500:46 - x such that probably mass function of x
500:48 - is greater than zero
500:50 - actually i'm just going to not even
500:51 - really say all that much over what
500:53 - x we're summing over i'm just going to
500:55 - say that you sum
500:57 - over x okay
501:00 - next uh expected value of x is viewed as
501:02 - the population mean
501:04 - which we're often denoting with the
501:05 - greek letter mu described in previous
501:07 - chapters
501:08 - we can always compute the expected
501:09 - values of functions of x oh no not
501:11 - always why did i say always like at the
501:13 - very
501:14 - end of this section i'm going to give
501:16 - you an example of an expected value that
501:17 - can't be computed
501:18 - but we can also compute i meant also
501:22 - also compute the expected value of
501:24 - functions of x
501:26 - functions of x that is um so that would
501:29 - be the expected value of
501:30 - h of x in a natural way by saying
501:34 - that the expected value of
501:37 - h of x is equal to
501:41 - the sum over x
501:45 - of h of x times the probability mass
501:49 - function
501:50 - at x and this formula should make some
501:53 - sense to you because you've actually
501:54 - seen it before
501:56 - or something very similar to it before
501:59 - remember when we remember when we were
502:00 - computing the sample mean
502:02 - the sample mean was 1 over n times the
502:04 - sum
502:05 - from i equals 1 to
502:09 - n x i which is also equal to the sum
502:13 - from i equals one to n
502:18 - uh x i times one over n
502:22 - and the one over n i mean if you add up
502:25 - one over n
502:26 - n times what number do you get you get
502:27 - one and also one over n
502:30 - is a number that's uh greater than zero
502:33 - so actually you could see this
502:36 - as a probably mass function
502:40 - for x i so you can imagine that you have
502:44 - your sample of observations
502:46 - and you're going to pull an observation
502:49 - at random with equal probability from
502:51 - your sample if it's pull from with equal
502:52 - probability
502:53 - then you're going to pull it with
502:54 - probability 1 over n so
502:57 - actually you've seen this expected value
503:00 - formula before it's just that this
503:02 - formula down here is a more general
503:04 - notion
503:05 - of of a sample mean or not really a
503:07 - sample mean but of mean
503:10 - so it allows for more situations it
503:11 - allows for
503:15 - infinite uh possibilities for x
503:18 - and so on the expected value is in some
503:20 - sense a best prediction
503:22 - for the value of x and uh this
503:25 - form uh no did i say sample mean i don't
503:28 - think i did
503:29 - expected value you can see this footnote
503:31 - for what sense in which it's the best
503:33 - prediction
503:34 - but it's hence the term expectation
503:37 - it's like if you had to guess what value
503:40 - this random variable is going to be
503:42 - uh you can use the expected value to do
503:44 - that and it will be
503:45 - uh correct in some sense so example
503:49 - 11 complete the expected value for some
503:51 - of the random variables that we've seen
503:52 - in previous sections
503:55 - so bernoulli random variables discrete
503:57 - uniform this
503:58 - discrete uniform random variable that
503:59 - represents a die roll
504:01 - and the geometric random variable with
504:04 - parameter p
504:06 - okay so let's get started
504:09 - um the first situation for a bernoulli
504:13 - random variable
504:15 - the expected value of x
504:20 - is equal to the sum
504:23 - of x p of x and
504:26 - the values of x for which p of x
504:30 - is possibly non-zero will be
504:33 - x equals zero to one so this will be
504:37 - um zero times p of zero
504:41 - which is one minus p
504:45 - this corresponds to the poly mass
504:47 - function at 0
504:49 - plus 1 times p
504:54 - where p is the value of the problem as
504:57 - function at one
504:58 - i guess this uh notation is actually a
505:00 - little unfortunate
505:01 - because i've got a couple different p's
505:03 - so maybe
505:04 - i should switch out the notation
505:08 - for the probably mass function in this
505:11 - example just switch it with f
505:15 - that could uh probably make things a bit
505:18 - more clear
505:19 - so this still nevertheless in any
505:21 - situation
505:23 - is the probably mass function for x
505:30 - okay and well let's see that just goes
505:33 - to zero
505:33 - and that's going to equal p so that
505:35 - means that the expected value of x is
505:36 - equal to p
505:38 - which is nice and actually
505:41 - rather insightful it's saying that
505:45 - the expected value of a bernoulli random
505:47 - variable with parameter p
505:48 - is equal to the probability that that
505:50 - random variable is equal to one
505:52 - and that's actually a very useful fact
505:55 - that's
505:56 - rather useful it allows us
505:59 - to relate bernoulli random variables
506:03 - probabilities for events and expected
506:05 - values
506:06 - it gives us a way to what we could do
506:09 - if we wanted to relate expected values
506:12 - to probability of an event
506:13 - is create a bernoulli random variable
506:15 - that's equal to one when that event
506:17 - occurs and zero
506:18 - otherwise and then the expected value of
506:20 - that random variable would be
506:22 - the probability of that event occurring
506:24 - but i'm just going to leave that
506:25 - issue for now let's work with the
506:29 - next example so the expected value of s
506:34 - is equal to the sum of
506:38 - let's say s p of s again we're talking
506:41 - about
506:42 - uh a probability mass function when
506:45 - we're talking about p
506:47 - uh down here and
506:50 - let's uh think about what are things
506:52 - that this random variable could take
506:53 - with positive probability
506:55 - we'll end up summing from s equals one
506:58 - to
506:58 - six so that's going to be
507:02 - uh let's see p of s is always
507:05 - 1 over 6. so this is equal to 1 over 6
507:09 - times the sum from s equals
507:12 - 1 to 6 of s and actually there's a
507:15 - formula for that
507:16 - hopefully you remember that from from
507:18 - your algebra classes
507:19 - in general you have the sum
507:24 - of uh s equals 1 to n
507:28 - of s this is a sum
507:31 - of an arithmetic series
507:34 - that's going to be
507:37 - n times n plus one
507:41 - divided by two okay
507:44 - which means that this sum is going to
507:47 - equal
507:48 - one-sixth times six
507:51 - times seven divided by two
507:55 - those two sixes cancel out so this will
507:58 - equal
507:59 - 7 over 2 or 3.5
508:06 - okay finally
508:09 - we have the expected value of what am i
508:12 - calling this geometric random variable
508:14 - i'm calling it
508:14 - n the expected value of
508:18 - n which is going to be the sum
508:23 - of n times p of n
508:27 - uh where n ranges from one to
508:30 - infinity all right this is gonna get
508:32 - much more complicated
508:34 - uh so because this is going to involve
508:36 - an infinite sum
508:37 - p of n is going to be
508:40 - we've got np 1 minus p to the power
508:44 - n minus 1. and we're uh summing from
508:47 - n equals 1 to infinity
508:50 - let's see the p here is a constant so we
508:54 - can bring that out
508:55 - we are not summing over p that means
508:58 - that we can say
508:59 - that this is equal to p
509:02 - times the sum from n equals
509:05 - 1 to infinity
509:09 - n times 1 minus p to the power n minus
509:12 - 1.
509:13 - you probably did not see a formula for
509:15 - this
509:17 - so what are we going to do
509:20 - we're going to get tricky we're going to
509:23 - get really tricky
509:25 - you've taken calculus presumably you've
509:27 - taken calculus 1 which includes
509:29 - differential calculus
509:31 - so you have seen this formula before
509:36 - the derivative with respect to x
509:39 - of x to the power n is equal to
509:43 - n x n minus one
509:46 - with some assumptions on n like for
509:48 - example that it's
509:49 - uh let's say at least one in
509:52 - which case that would hold
509:56 - in this situation if you're looking at
509:58 - one minus p
510:00 - as your thing as the thing you're
510:01 - differentiating
510:03 - hmm those two things look rather similar
510:07 - which means that actually
510:11 - we could be invoking some sort of
510:15 - differential
510:16 - or derivative in our
510:19 - sum and say instead
510:22 - that this is equal to p
510:26 - times the sum from n
510:30 - equals one to infinity
510:34 - the derivative with respect to p
510:38 - 1 minus p to the power n
510:42 - now you would notice that right now if
510:43 - you were to in fact take that derivative
510:46 - you would almost get what i wrote on the
510:49 - left
510:49 - over here that i've underlined in green
510:51 - you would almost get that except you'd
510:53 - be off by a sign
510:54 - as in you'd get negative something
510:56 - because you have to invoke
510:58 - the chain rule so what are we gonna do
511:01 - about that we're just gonna
511:02 - throw a negative out here and
511:06 - uh then it's certainly true
511:09 - although i'm going to mention that
511:12 - well okay it's actually true as written
511:15 - down there's
511:16 - no there's no caveats yet
511:20 - but there is going to be one in just a
511:22 - second because
511:23 - i'm going to say that this
511:28 - is equal to negative p
511:32 - times the derivative with respect to
511:35 - p the sum from n equals 1
511:40 - to infinity 1 minus p to the power
511:44 - n okay you can't just bring derivatives
511:47 - out of sums like that
511:50 - or you can but there
511:53 - are conditions like you can't just look
511:56 - at any
511:57 - sum in the world and see derivatives
511:59 - inside the sum and say
512:01 - okay i could just slip the derivative
512:03 - out it's not that simple
512:05 - there are details there are conditions
512:07 - under which you can do this
512:09 - those conditions are satisfied here i
512:11 - actually don't remember them off the top
512:13 - my head
512:13 - um i just know you can you can go
512:17 - look at some calculus book or some
512:19 - analysis book it's probably going to be
512:20 - an analysis book
512:22 - uh or wikipedia wikipedia has pretty
512:24 - good math articles
512:26 - and it would tell you when you can
512:29 - switch
512:31 - because that's what you're doing you're
512:33 - switching
512:34 - a sum and a derivative
512:37 - it will tell you when you're allowed to
512:38 - do that and i'm just
512:40 - completely sweeping that under the rug
512:41 - and i'm fine with that because
512:43 - i don't care i got other stuff to do all
512:45 - right um
512:46 - the thing though is we know how to
512:49 - compute this sum
512:52 - you know that this sum is going to
512:54 - become
512:57 - uh 1 minus p divided by p
513:03 - so that means that this will become
513:06 - negative p
513:08 - oops wrong color negative p
513:13 - and then we've got the derivative with
513:14 - respect to p
513:16 - of um 1 minus p
513:20 - divided by p
513:24 - which we should probably uh simplify
513:27 - somewhat
513:28 - and say that this is equal to negative
513:32 - p and the derivative with respect to p
513:35 - of
513:37 - let's see one over p minus
513:40 - one yeah that's right and the derivat so
513:44 - then we take that derivative and say
513:46 - we've got
513:46 - negative p and
513:50 - on the inside after we take the
513:51 - derivative we'll get negative one over p
513:52 - squared
513:56 - those two negatives turn into positives
513:58 - we cancel out one of ps and this is
514:00 - equal to one over p
514:04 - there we go this actually has a very
514:07 - nice
514:07 - intuitive um interpretation
514:11 - which is that let's say that you're
514:12 - flipping a coin until you get heads
514:14 - how many times do you what is the
514:16 - expected number of flip first ones tails
514:18 - second one's heads
514:20 - makes pretty good sense to me or let's
514:22 - say
514:23 - that you have a bias coin and
514:26 - you flip this coin until you get heads
514:28 - and the probably that you get heads
514:31 - is uh 0.1 how many flips do you need 10
514:34 - flips
514:36 - seems to make a pretty good sense at
514:38 - least to me
514:41 - it allows at least in my mind
514:45 - way for me to relate probability
514:48 - of an event happening with time
514:52 - and say that if that if you were to have
514:55 - an uh
514:56 - a sequence of independent re
514:58 - replications of this event
514:59 - this is about how long you have to wait
515:01 - until you see that event happen
515:03 - which is another way to think of or
515:05 - another way to reason about how rare
515:06 - that event would be
515:07 - like for example if there's like a 10
515:10 - chance of an earthquake every year how
515:12 - many years
515:12 - is it going to take for you to see an
515:14 - earthquake 10 years on average
515:16 - uh something like that that's so i
515:19 - really like
515:20 - uh that formula interpretations like
515:22 - that okay moving on
515:24 - this is the same body of lecture notes
515:25 - that we've seen before so we have loaded
515:28 - up
515:28 - the discrete rv library in r
515:31 - when you whenever you see the r
515:35 - sections and there is an
515:38 - there's a discrete rv function called e
515:41 - that is for computing expected values
515:44 - so x was a random variable that we
515:46 - defined at some point that corresponds
515:47 - to
515:48 - the bernoulli random variable that we
515:49 - were talking about s
515:51 - corresponds to actually the sum of two
515:54 - dice so in this case it we should be
515:55 - doing
515:56 - seven over two or something like that um
516:01 - uh if we were actually talking about the
516:03 - same s but this is a different
516:05 - s uh this is a uh sum of two dice
516:11 - two independent dice by the way uh
516:15 - not just any two dice although i don't
516:17 - know how you make
516:18 - two non-independent dice that would be
516:20 - really hard and is the what we're
516:22 - talking about and notice that the answer
516:23 - is approximate
516:25 - uh we know that in in for this end the p
516:28 - parameter was 2
516:30 - so the number that results should be 2
516:32 - but it's not 2.
516:33 - it's 1.9999999 so
516:36 - be aware of that it's giving us an
516:38 - approximate answer because it's only
516:39 - fine
516:40 - summing over a finite number of uh
516:43 - places but you get the idea you can tell
516:46 - that that is
516:47 - that it's essentially doing the right
516:48 - thing uh okay
516:50 - uh continuing on expectations are linear
516:54 - functions
516:55 - and being a linear function is an
516:58 - extremely
516:59 - important property um
517:02 - and it's for that reason that we can say
517:04 - expectations are integrals
517:06 - but that is a 60 40 idea right there
517:09 - instead
517:10 - we're going to talk about how the
517:12 - expected value of ax
517:13 - plus b is equal to a times the expected
517:15 - value of x plus b
517:17 - this is something that we can show uh
517:19 - watch the expected value
517:22 - of a x plus b
517:25 - uh is equal to this this by the way is
517:29 - uh
517:29 - we this is basically h of x where h
517:33 - of let's say s is equal to a
517:36 - s plus b
517:39 - so we can use some of those that uh
517:42 - expectation formula that we mentioned
517:44 - above uh towards the beginning of this
517:46 - lecture
517:47 - and say that this expectation
517:50 - is equal to the sum over
517:53 - x a
517:57 - x plus b times the probably mass
518:00 - function of
518:01 - x which is equal to the sum
518:05 - over x and then we'll factor all that
518:07 - stuff together we've got
518:08 - a x p of x
518:12 - plus b p of x
518:16 - and then we'll break up the sum and say
518:18 - that this is the sum
518:20 - over x a x p of x
518:24 - plus uh the sum over
518:28 - x scroll down
518:31 - scroll down uh the sum over x
518:35 - b p of x factor out the constants to say
518:39 - that this
518:40 - is a times the sum over
518:43 - x x p of x
518:47 - plus b times the sum over
518:50 - x p of x and
518:53 - we can recognize what some of those sums
518:56 - are
518:56 - for instance this sum is the expected
518:59 - value
519:00 - of x and this sum is the sum of the
519:03 - probability mass function
519:04 - um over everything where it's positive
519:07 - so this is going to sum to 1
519:09 - and hence you get uh the result
519:12 - uh a times the expected value
519:16 - of x plus b
519:21 - hence it's a linear the variance
519:25 - of a random variable is given by
519:28 - we'll call it var of x
519:33 - which is equal to the expected value
519:36 - of x minus mu
519:42 - where mu is just the expected value of x
519:47 - we just don't want to write that again
519:48 - in there because it's it feels confusing
519:51 - so we just put a mu in there but
519:53 - basically it's the
519:54 - mean squared distance of x from its uh
519:57 - from its expected value so
520:00 - this actually does correspond very
520:03 - closely
520:04 - to the sample variance as well if you
520:07 - could think of the sample variance as
520:08 - divided by one over n
520:10 - instead of one over n minus one uh you
520:12 - could say
520:13 - we could argue as we did for how the
520:16 - sample mean is very similar to
520:19 - the population mean in terms of expected
520:20 - values
520:22 - and say that this is a sample variance
520:24 - too so
520:26 - yeah they so this is uh something to
520:29 - notice
520:30 - the greek letter that's used to
520:33 - represent the sample variance
520:35 - is sigma squared and like
520:38 - with the sample standard deviation you
520:40 - can get the population standard
520:42 - deviation by taking the square root of
520:44 - the variance it's just not as common to
520:47 - do so
520:48 - um all right so uh there is actually a
520:51 - handy formula for computing the variance
520:53 - that is often easier than computing it
520:55 - directly
520:56 - and in this formula you may recognize
520:58 - this from when we were working with the
521:00 - uh the uh did i say sample variance a
521:03 - second ago
521:04 - uh i meant the variance or the
521:06 - population variance but this formula
521:08 - resembles the formula for the sum of
521:10 - squared sum of squared errors
521:13 - uh that we had that we saw in chapter
521:15 - one
521:16 - where you have the mean of x squared
521:19 - minus the mean of x
521:21 - squared
521:25 - where hopefully you can tell from my
521:27 - inflection
521:28 - what's being squared
521:33 - okay so the variance of x is thought of
521:36 - as the population variance
521:38 - and is denoted by var x which is sigma
521:39 - squared and the population standard
521:41 - deviation is sigma which is the square
521:42 - root of sigma squared
521:44 - sometimes i'll write though the standard
521:46 - deviation of x
521:47 - because it's it's sometimes nice to do
521:50 - uh
521:50 - all right so our next example compute
521:53 - the variance and standard deviation of
521:54 - the random variables
521:56 - listed in example 11.
521:59 - so if that's the case let's start out
522:02 - with
522:03 - x we already have the expected value of
522:08 - x which was p
522:13 - the expected value of s
522:17 - which is uh seven halves
522:21 - and the expected value of n
522:25 - which is equal to one over p so as a
522:27 - reminder of what we have already
522:29 - so now let's compute the expected value
522:32 - of
522:32 - x squared
522:36 - which is equal to the sum
522:39 - from little x equals 0 to 1
522:43 - um x squared uh f of x
522:47 - because that's what i'm calling the
522:48 - probably mass function which is equal to
522:51 - 0 squared times 1 minus p
522:54 - plus 1 squared times p
522:57 - which is equal to p again hence you get
523:00 - to say
523:01 - that the variance of x
523:05 - is going to be the expected value
523:08 - of x squared minus
523:12 - the mean of x
523:16 - squared which is equal to p squared
523:19 - minus p which we could be done right
523:21 - there but people often like to factor
523:23 - this
523:24 - into p times one minus p
523:31 - all right next example
523:35 - uh in the case of s so
523:38 - the expected value of s squared
523:43 - is going to be the sum from
523:46 - s equals 1 to 6
523:50 - of s square times probably mass function
523:53 - at
523:54 - s which is 1 over
523:57 - 6 times the sum from
524:00 - s equals 1 to 6 s squared
524:05 - this is something that we have actually
524:08 - all right you might not have seen this a
524:11 - formula for
524:12 - the sum of squares uh
524:15 - in your previous algebra classes maybe
524:18 - you saw that maybe you didn't
524:20 - but there is in fact a formula for that
524:24 - which i'm gonna have to look up okay so
524:27 - you have that the sum
524:30 - yeah let's use a different color for
524:32 - this
524:34 - the sum from s
524:38 - equals 1 to n
524:42 - s squared is equal to
524:45 - n times n plus one
524:48 - times two n plus one
524:52 - divided by six
525:00 - okay so using that here
525:03 - we get to say that this is equal to
525:06 - 1 6 times 6
525:10 - times 7 times
525:15 - 13
525:18 - divided by six
525:21 - so those cancel and that's pretty much
525:24 - all we can cancel
525:26 - so we get to say that this is equal to
525:30 - 91 divided by 6. therefore the variance
525:35 - of s is going to be
525:39 - the expected value of s squared
525:42 - minus the mean of s squared
525:50 - which is equal to
525:53 - 91 over 6 minus
525:57 - 49 over four
526:01 - what is that number uh
526:05 - uh 35 over 12.
526:15 - and you should have that the variance is
526:18 - always
526:19 - a non-negative number in fact the only
526:21 - time that the variance is ever equal to
526:23 - zero as if the random variable is
526:24 - degenerate that is if it's effectively a
526:27 - constant
526:28 - so it's unlikely that your variance is
526:30 - zero
526:32 - and uh and
526:35 - in the more general case it's
526:39 - impossible for your r for your variance
526:42 - to be negative
526:44 - so if you ever ended up with a negative
526:46 - variance then you've done something
526:47 - wrong
526:50 - all right for the final one and this one
526:54 - is where things get weird all right i'm
526:57 - i'm going to zoom in for this one
527:01 - because this one is where things get
527:03 - really tricky because
527:04 - we're now working with the geometric
527:06 - case and we need to compute the expected
527:08 - value
527:09 - of n squared okay and that is equal to
527:14 - uh the sum from n equals
527:18 - 1 to infinity n squared times the
527:21 - probability mass function
527:22 - at n which is equal to
527:25 - uh the sum from
527:29 - n equals 1 to infinity i'm going to go
527:31 - ahead and already do some simplification
527:32 - we get p
527:35 - n squared 1 minus p to the power
527:38 - n minus 1. now
527:42 - how on earth are we going to compute
527:44 - that
527:45 - well we're going to get we're going to
527:48 - get really tricky is what we're going to
527:50 - do
527:51 - so we're going to say that
527:54 - let's let's zoom in even more
527:58 - we're going to say and you're not all
528:00 - right this this is just so weird what's
528:02 - about to happen
528:05 - um according to my notes it's actually
528:08 - advantageous
528:09 - to keep the p inside uh
528:12 - so let's let's uh put the p
528:17 - back inside of this sum rather than
528:20 - factor it out
528:22 - uh so we got a sum from n equals one to
528:25 - infinity
528:28 - n squared p all right
528:33 - what's going to end up happening is
528:34 - we're going to end up um
528:38 - adding 1 no hold on
528:41 - subtracting 1 and then adding 1 again
528:44 - inside of that square
528:49 - and leave everything else the same
528:54 - and we're going to go do some
528:55 - calculations
528:57 - and at the very end of them
529:01 - the expected value go away
529:05 - the expected value of n squared is going
529:08 - to appear on both the left hand and the
529:10 - right hand side
529:11 - uh left hand and right hand side of an
529:14 - equal sign
529:16 - so after that happens the thing is
529:18 - though on the right hand side of that
529:20 - equal sign
529:21 - it's not going to be just the expected
529:22 - value of n squared it's going to be the
529:24 - expected value of n squared plus
529:26 - something times something
529:28 - and when you have a situation like that
529:30 - you're going to be able to
529:32 - solve for the expected value of n
529:34 - squared because you just have an
529:35 - algebraic relationship
529:38 - and you're just going to have to see it
529:40 - and and watch it happen
529:42 - in order to kind of understand it's just
529:44 - at the very end
529:45 - all of a sudden what you're going to
529:46 - need pops out
529:48 - this is one of those situations where
529:50 - it's a trick and
529:51 - you're going to see the trick and you
529:53 - might not understand the motivation for
529:55 - the trick
529:56 - but someone did that trick once and it
529:58 - seems to work
530:00 - all right so uh here we go
530:04 - this part right here is a perfect is a
530:07 - it's a square
530:08 - a perfect square so uh we can
530:12 - we can now write that part i i'm not
530:16 - going to write
530:16 - n equals 1 to infinity all the time
530:18 - that's going to get annoying
530:20 - so i'm just going to write a sum over n
530:22 - down here
530:23 - and say we've got n plus one no no no no
530:27 - not n plus one n minus one
530:31 - we've got n minus one squared
530:35 - plus two times
530:38 - n minus one plus one
530:42 - and then we've got p one minus p
530:46 - to the power n minus one
530:50 - yeah okay and then this is equal to
530:56 - this is equal to uh breaking up
531:00 - this part and breaking up the resulting
531:02 - sum we get the sum
531:05 - over n
531:08 - and we have n minus 1
531:11 - squared p 1 minus p
531:15 - power n minus 1.
531:19 - [Music]
531:20 - plus 2
531:24 - plus 2 times the sum over
531:27 - n uh n minus 1
531:31 - p 1 minus p to the power n minus 1
531:36 - plus the sum over n
531:39 - of p 1 minus p
531:43 - to the power n minus 1. okay now we can
531:46 - start
531:46 - uh recognizing some stuff uh the term
531:50 - on the very left hand side no not left
531:53 - hand right hand side
531:55 - this term this is equal to one because
531:57 - this is just the sum of the probably
531:58 - mass function
532:00 - this term is the expected value of
532:03 - n minus one
532:08 - okay and then we get to say that
532:12 - so far collecting our stuff um
532:14 - recognizing those substitutions is the
532:16 - sum
532:16 - over n uh n minus 1
532:20 - squared p 1 minus p to the power n minus
532:24 - 1
532:25 - plus 2 times the expected value
532:28 - of i'm going to write this as the
532:30 - expected value of
532:31 - n minus 1 because we have that linearity
532:35 - property that i proved
532:36 - a few mo a few minutes ago and then we
532:39 - have plus one
532:40 - all right and uh doing some even further
532:43 - simplification
532:44 - we're able to recognize that the
532:46 - expected value of n
532:47 - is equal to one over p so that means
532:50 - that this
532:50 - term uh that we're adding is going to be
532:54 - uh one over p minus one so we got
532:58 - plus 2 over p minus 2 plus 1 so this
533:01 - will be
533:02 - uh so that means that this is going to
533:04 - simplify
533:06 - into
533:09 - plus 2 over p minus 1.
533:15 - and then we're going to take
533:18 - this n minus 1 and say oh well that's a
533:20 - perfect square too so this will be n
533:22 - squared minus 2n
533:24 - plus 1. all right so
533:27 - we then get um oh wait actually we don't
533:31 - want to do that
533:32 - no we don't want to do that we do not
533:34 - want to do that
533:36 - uh we don't want to do that we want to
533:39 - do something even
533:40 - trickier what we're going to do instead
533:44 - all right let's write in again what i
533:46 - have what what i have been omitting this
533:47 - whole time
533:48 - that this is the sum from n equals 1 to
533:50 - infinity
533:52 - but we're going to re-index this
533:59 - we're going to re-index this and say
534:02 - well
534:03 - actually this is the same as saying uh
534:06 - n minus one equals zero to infinity
534:10 - all right and then replace
534:14 - all of those n minus ones
534:18 - with let's say j
534:22 - and say this is j equals 0 to infinity
534:24 - so we get j
534:26 - j squared and the thing is though we're
534:29 - uh
534:32 - the first term in this sum though is
534:34 - going to end up being zero because j
534:36 - because all right plug in j equals zero
534:38 - that means the first term is going to be
534:39 - zero because zero squared is zero
534:41 - zero times whatever is zero so that
534:44 - means that the first term
534:45 - is actually zero so we get to
534:48 - um replace j equals zero
534:51 - with j equals one because
534:55 - well when you start at zero you just add
534:58 - a zero term you're just adding zero to
535:00 - the sum so we get to start at one
535:02 - and this is now looking almost like
535:06 - almost like uh the expected value
535:09 - except for uh the power
535:13 - up here is wrong it should be j minus
535:16 - one
535:16 - to have the problem mass function but
535:17 - we've got j instead okay
535:20 - so we'll replace that with j minus one
535:22 - plus one
535:24 - okay and to account for the plus one
535:27 - that means that what we need to do
535:29 - is factor out a 1 minus p
535:33 - so all told we will have
535:39 - 1 minus p times the sum
535:43 - from j equals 1 to infinity
535:47 - j squared p
535:50 - 1 minus p to the power j minus 1
535:56 - plus 2 over p minus 1.
535:59 - and that sum
536:03 - is what we started with
536:06 - this term right here is the expected
536:08 - value
536:10 - of n squared
536:13 - oh look at that so we get to say
536:18 - that this is going to be
536:23 - uh 1 minus p times the expected value
536:28 - of n squared
536:31 - plus two over p
536:36 - minus one
536:39 - and as a reminder at the very beginning
536:42 - of this
536:43 - long statement of equalities is the
536:45 - expected value
536:48 - of n squared oh look at that
536:52 - we can do some algebra now for instance
536:56 - we can subtract over uh we can subtract
537:00 - from both sides
537:01 - the expected value of n squared
537:11 - and say that this is going to suggest
537:15 - that uh what is uh 1 minus p
537:18 - minus 1 uh that's going to be negative p
537:22 - so we've got negative p times the
537:25 - expected value
537:27 - of n squared uh we will subtract
537:32 - two over p and add one to both sides
537:36 - minus two over p plus one
537:40 - to get uh that this is
537:43 - equal to negative two over p plus one
537:49 - and then divide both sides
537:52 - by negative p
537:56 - and now we get to say that this is equal
537:59 - to
538:02 - that uh the expected value of n squared
538:09 - is equal to 2 over p squared
538:15 - minus 1 over p
538:18 - and there it is that by the way is the
538:20 - expected value of n squared
538:24 - i'm just writing it down again because
538:25 - it's on my screen and because it was put
538:27 - on a separate line
538:28 - that's what we need to compute
538:32 - so it then follows
538:36 - that the variance of n
538:41 - is equal to 2 over p squared
538:45 - minus 1 over p minus 1 over p squared
538:49 - which is equal to
538:50 - [Music]
538:52 - 1 over p squared minus 1 over p
538:56 - which is equal to p minus 1
538:59 - over p squared wait a minute that's
539:03 - hold it hold it hold it hold it hold it
539:06 - that's
539:07 - oh no not p minus one uh one minus p
539:10 - okay
539:11 - otherwise that would have been bad
539:12 - because i would have i would have just
539:13 - computed a negative variance so one
539:15 - minus p over p squared
539:16 - uh did i ask to
539:20 - uh did i did i did i say that we should
539:22 - compute the standard deviation too
539:26 - it does it's not it's not super hard to
539:28 - do you just take the square root
539:29 - but yeah i did ask for standard
539:31 - deviations so
539:34 - okay computing the standard deviations
539:35 - too like that's super easy
539:37 - you just take the square root of the
539:39 - variance so the standard deviation of
539:41 - x is equal to the square root of p times
539:44 - one minus p
539:45 - uh the expected the standard deviation
539:49 - of
539:49 - s
539:53 - is equal to uh the square root uh let's
539:56 - see
539:57 - uh one half times the square root
540:01 - of uh 35
540:04 - over three and for
540:08 - n uh let's see the variance no the
540:11 - standard
540:12 - deviation of
540:15 - n is going to be the square root of 1
540:18 - minus p
540:19 - divided by p
540:23 - that's an ugly looking p
540:28 - all right there we go and in fact
540:31 - there are functions for
540:34 - in this uh discrete rv library for
540:37 - computing variance and standard
540:38 - deviation
540:39 - they're going to be v oops
540:43 - v and sd respectively and you can see
540:45 - this function computing the variance and
540:47 - standard deviation remember that this is
540:48 - not the same as we were talking about
540:49 - before
540:51 - so we get to compute those things and
540:55 - we get pretty much what we had so
540:58 - all right uh proposition 10 the variance
541:02 - of ax plus b
541:03 - is equal to a squared times the variance
541:04 - of x and
541:06 - the standard deviation of ax plus b is a
541:09 - times the standard deviation of x or the
541:11 - absolute value of a times the standard
541:12 - deviation of x
541:14 - uh actually this would probably be
541:15 - better to write in terms of that sd
541:17 - notation
541:17 - to say that sd of a x
541:21 - plus b is equal to the absolute value of
541:24 - a
541:24 - times the standard deviation of
541:28 - x
541:31 - so uh let's go ahead let's see this uh
541:35 - let's let's go ahead and prove this so
541:36 - the variance
541:39 - of a x plus b
541:42 - so the variance of ax plus b is the
541:44 - expected value
541:47 - of a x plus b
541:51 - minus we should have the expected value
541:54 - of
541:55 - a x plus b here
541:59 - here's the thing though uh the b's
542:03 - are going to cancel because expectations
542:06 - are linear
542:08 - and the a's can that a can be factored
542:10 - out in front of that expectation
542:13 - so that means that a is going to be a
542:15 - common factor and therefore the a can be
542:17 - factored out
542:18 - completely if we just square it so we
542:20 - get to say that this is the expected
542:22 - value
542:23 - of a squared and then we have on the
542:27 - inside
542:27 - x minus the expected value of x
542:34 - which is equal to a squared expected
542:37 - value
542:38 - of x minus mu squared just remember that
542:42 - mu is the expected value of x
542:44 - and that's the variance so this is equal
542:45 - to a squared times the variance
542:48 - of x all right what do we then
542:52 - say for the standard deviation we say
542:54 - that the standard deviation
542:56 - of a x plus b is the square root
543:01 - is equal to the square root of the
543:02 - variance of
543:04 - a x plus b which is equal to
543:08 - after you do that uh algebra the
543:11 - absolute value of
543:12 - a times the standard deviation of x
543:14 - because you're just going to take
543:16 - the square root of what i've highlighted
543:18 - in blue just take the square root of
543:20 - that and you're good
543:21 - all right so there's that formula
543:25 - uh one final note
543:28 - there is nothing that says that
543:30 - expectations need to be finite or even
543:32 - exist
543:33 - there are random variables out there
543:35 - that do not have
543:37 - finite uh uh standard
543:40 - uh finite expectations and they may not
543:43 - even have like
543:45 - like an infinite expectation they might
543:48 - not have an expectation at all like
543:49 - there's just no way to define it like
543:51 - i guess technically an infinite
543:53 - expectation is considered undefined
543:56 - but it's like you can't but there's a
543:58 - sense in which it is defined like
544:00 - infinite just means arbitrarily large
544:02 - but even then
544:04 - even then you might not be able to say
544:07 - that it's even infinite
544:08 - it could be anything there are random
544:10 - variables out there that don't have
544:11 - expectations
544:15 - so uh let's actually see an example of
544:19 - this this one's a fun one
544:21 - uh this is what's known as the saint
544:24 - petersburg game
544:26 - consider a game where a fair coin is
544:28 - flipped until it lands heads up
544:30 - a player would earn a dollar if the game
544:33 - ends with one flip
544:34 - two dollars if it ends two flips four
544:36 - dollars if it ends with three flips
544:37 - eight dollars if it ends with four flips
544:38 - and so on
544:39 - so basically your winnings are doubling
544:41 - every time
544:42 - uh this game goes on the fair price of a
544:45 - game
544:46 - corresponds to the game's expected
544:48 - payout what then is the fair price to
544:50 - play this game
544:51 - and before i continue on i would like
544:53 - for you to think about
544:54 - how much you think this game is worth
544:56 - and how much you would be willing to pay
544:57 - for it
544:58 - how much would you be willing to play at
545:00 - pay to play this gambling game
545:02 - what do you think is a fair price you
545:05 - might be surprised
545:09 - so let's calculate it we're going to say
545:14 - that n is following
545:19 - a geometric distribution
545:22 - with parameter one-half because that's
545:24 - how you should
545:25 - we're flipping a coin until we get heads
545:28 - and
545:29 - this geometric random variable it's a
545:31 - fair coin is what will
545:33 - model such an experiment so then
545:37 - what would be our winnings it would be
545:39 - two to the power
545:41 - um n minus one
545:45 - because if you get one flip that would
545:47 - be
545:48 - you should get one dollar so
545:51 - n minus one so that'll be one minus one
545:53 - so to the power zero
545:55 - which is one uh if we get if it took two
545:57 - flips that's
545:58 - two minus one in the power so that'll be
546:00 - two minus one
546:02 - uh so the power will be one so we get to
546:04 - the one which is equal to two
546:05 - and if we have three flips that's going
546:07 - to be two squared
546:08 - so we'll get four so this is in fact
546:10 - corresponding to what we think it should
546:12 - all right then what we were computing is
546:14 - the expected value
546:16 - of two to the power and minus one
546:21 - which we can make our lives a little bit
546:23 - easier by saying that this is one half
546:25 - times the expected value of two to the
546:28 - power
546:29 - n and we know how to compute the
546:32 - expected value of 2 to the power n
546:34 - so we'll say this is one half and we
546:36 - have the sum
546:37 - from n equals 1 to infinity
546:41 - to the power n and then we write down
546:44 - the probability mass function
546:45 - for the geometric random variable which
546:48 - is 2 to the power negative n
546:50 - or which is the same as that's the same
546:52 - thing as 1 half to the power n
546:54 - so n minus n that's going to be one so
546:58 - this is one half
547:00 - times the sum from n equals one to
547:03 - infinity
547:05 - one which is equal to infinity
547:09 - this game has infinite value
547:12 - you should pay a dollar to play this
547:14 - game you should pay ten dollars to play
547:16 - this game
547:17 - you should pay a hundred dollars to play
547:19 - this game you should go to the bank
547:21 - and take out a loan for a billion
547:23 - dollars and play this game
547:27 - because this game has infinite expected
547:29 - value any finite price is a bargain
547:33 - and yet no one in their right mind would
547:36 - ever do that
547:38 - no one thinks that this game is really
547:40 - worth anything people think this is a
547:42 - terrible game
547:44 - and why that is is somewhat remarkable
547:48 - it gets to the point that once you've
547:51 - earned a million dollars
547:54 - another million dollars doesn't seem
547:55 - that great i mean it's pretty good
547:58 - to go from one million dollars to two
547:59 - billion dollars that's nice
548:03 - same thing with one trillion dollars and
548:04 - two trillion dollars
548:06 - like you're gaining a trillion dollars
548:08 - when you go from one trillion dollars to
548:09 - two trillion dollars but it doesn't
548:10 - really feel like it
548:12 - like you already got everything you want
548:14 - at one trillion dollars
548:16 - the other trillion dollars is just gravy
548:20 - so basically the point is with this game
548:23 - the way to rationalize the paradox of
548:26 - this game
548:27 - the fact that it's expected value is
548:28 - infinite but no one wants to pay that
548:32 - is that people are not actually thinking
548:35 - about
548:37 - the winnings in terms of literal money
548:39 - they're thinking about in terms of the
548:40 - utility they get from that money
548:42 - and people know that the net the next
548:45 - trillion dollars
548:46 - is not as good as the first trillion
548:47 - dollars
548:49 - so in that case this game actually
548:52 - doesn't look very good
548:53 - when once you actually account for
548:55 - decreasing utility
548:57 - from your winnings that's how you
548:59 - resolve the paradox you resolve it with
549:01 - economics
549:02 - that's it for this video uh we have been
549:05 - talking about
549:07 - general ideas and random variable theory
549:11 - let's call it random variable theory
549:12 - that seems like a good word
549:14 - uh probably mass functions accumulate
549:17 - distribution functions all that stuff
549:18 - these are general things that
549:20 - all discrete random variables have and
549:22 - expectations
549:24 - uh excluding the cases where they don't
549:27 - exist they
549:28 - they are they're generally around two
549:31 - so now
549:34 - we're going to start looking at specific
549:36 - examples of common families of random
549:38 - variables that probabilists care about
549:40 - the first one being the binomial
549:42 - probability distribution
549:44 - a lot of the ideas also that we talked
549:46 - about here
549:47 - actually transfer over to the continuous
549:50 - case when we're talking about continuous
549:51 - random variables
549:53 - uh there they have analogs that are
549:55 - pretty similar what you do is you
549:57 - replace probably mass functions with
549:59 - probably density functions
550:02 - and you replace sums with integrals
550:05 - so that's what that's how you go from
550:08 - the discrete to the continuous case
550:10 - and but everything else applies
550:12 - everything else is the same
550:14 - variances expectations uh
550:17 - cdfs probably mass functions become
550:19 - probably density functions which are
550:21 - pretty similar
550:22 - so these are all basic ideas that you're
550:24 - going to see over and over again when
550:25 - talking about probability
550:27 - and from this point on we're going to
550:29 - for the remainder of this chapter we're
550:31 - going to be looking
550:32 - at specific examples
550:36 - because the advantage of talking about a
550:38 - family of distributions
550:40 - is once you have a family distributions
550:42 - you get to talk about
550:44 - you get to talk about it once and then
550:45 - you get to generalize to
550:47 - lots of different cases and it's like
550:49 - the expected value if you're able to
550:51 - recognize a random variable as being a
550:54 - particular case of a binomial
550:56 - then there's an expected value formula
550:58 - available to you and you don't need to
551:00 - compute it by hand
551:01 - you shouldn't compute it by hand because
551:02 - it's going to be a pain you could just
551:04 - use that formula
551:06 - and it's really easy same thing with a
551:08 - lot of these other
551:09 - random variables hypergeometric negative
551:11 - binomial
551:12 - there they would be a pain for you to do
551:14 - over and over again but because we've
551:15 - identified a family of random variables
551:18 - with common characteristics someone
551:19 - computed a formula and now you get to
551:21 - use that formula
551:22 - and that's really nice that's really
551:24 - nice
551:25 - okay so i'll just uh leave it at that
551:29 - uh we will end this uh the study of this
551:32 - section
551:33 - and i will see you later i will see you
551:36 - when we start talking about binomial
551:38 - random variables
551:51 - for our next section we are now
551:54 - discussing
551:55 - the binomial probability distribution
551:58 - so a binomial experiment is an
552:00 - experiment that
552:02 - satisfies the following requirements the
552:04 - experiment consists of n
552:06 - bernoulli trials that end in either
552:08 - success which we will denote
552:10 - s or failure with which we will denote
552:13 - f the trials are independent and
552:16 - for each trial the probability of s is 1
552:20 - minus the probability of failure
552:22 - which is some number p between 0 and 1.
552:25 - in the case of p being 0 or 1 this
552:28 - random variable is degenerate
552:30 - or the resulting random variable i guess
552:32 - i haven't mentioned a random variable
552:33 - yet
552:34 - but that random variable would be
552:35 - degenerate because you'd either always
552:37 - get success or always get failure
552:39 - so we don't consider that situation uh
552:42 - so we can think of the outcome of an
552:44 - experience as
552:46 - a sequence of s and f such as
552:49 - s f s f which
552:52 - in that case the
552:56 - uh the duration of the experiment would
552:59 - be
553:00 - n equals five so the binomial random
553:03 - variable is the associated random
553:05 - variable with binomial experiments and
553:07 - what a binomial random variable does
553:09 - is it will count the number of successes
553:12 - in the experiment
553:14 - so x of omega is equal to the number of
553:17 - s in omega uh we should probably
553:20 - not be uh um
553:23 - i mean why did i write that as a set
553:26 - that doesn't make any sense it's not a
553:28 - set
553:28 - right it's just a number
553:32 - uh we will then say that x follows a
553:35 - binomial distribution
553:36 - with parameters n and p so for example
553:40 - given that sequence of s's and f's
553:44 - that we saw before the binomial random
553:46 - variable would evaluate to three
553:49 - so we denote the problem mass function
553:51 - of x with
553:52 - lowercase b although that's more
553:54 - notation for this class
553:56 - i don't really see notations that we use
553:58 - in this class elsewhere because
554:01 - people know what they're talking about
554:03 - when you're reading papers
554:04 - and stuff so they don't bother to come
554:07 - up with some special notation for it
554:10 - anyway
554:12 - here we have the uh probably mass
554:16 - function for binomial random variables
554:19 - this is zero for x that's not an integer
554:21 - from zero to n
554:22 - and for x between zero and n it can
554:26 - in fact be computed that the
554:30 - probability mass function for a binomial
554:32 - random variable
554:33 - with parameters n and p is equal to
554:38 - n choose x
554:41 - p to the power x
554:44 - times 1 minus p to the power
554:47 - n minus x
554:55 - so here's some further explanation of
554:59 - this formula
555:01 - in this situation there are x
555:04 - successes out of n trials
555:08 - okay the probability of each of those
555:10 - successes is p
555:11 - they are independent trials so you
555:13 - multiply
555:15 - p x times and you would multiply
555:19 - y minus p minus 6 times this is
555:21 - accounting for the probability of each
555:22 - of those failures
555:24 - that occurred and here's the thing
555:27 - that will get you the probability of
555:29 - let's say the sequence
555:32 - ssf
555:35 - that would get you the probability of
555:37 - getting that particular sequence
555:40 - but the thing is there's a number of
555:42 - sequences
555:43 - where you could have three successes
555:45 - until two failures
555:47 - for example you could have sss ff
555:51 - or the other way around like ff sss and
555:54 - so on
555:55 - so we need to pick the positions
555:58 - in which successes occur and failures
556:01 - occur
556:02 - or we'll just simply pick the position
556:04 - of successes
556:05 - and if we pick the position of successes
556:07 - we then know
556:08 - where all the failures occurred in the
556:10 - sample
556:11 - or in this string so we end up with n
556:15 - choose x
556:16 - meaning out of x position out of n
556:19 - positions
556:20 - choose the x positions where
556:23 - successes occur
556:30 - the cdf of this random variable x is
556:32 - given
556:33 - next the probability
556:37 - no i don't want black blue
556:44 - the probability that x is less than or
556:48 - equal to
556:48 - little x is equal to
556:52 - the cdf of
556:56 - the binomial random variable
557:00 - which is equal to the sum
557:03 - from i equals zero because
557:07 - you could potentially have zero
557:08 - successes in your sample
557:10 - so from i equals zero to x rounded down
557:15 - the probability mass function at i
557:19 - n p which is equal to
557:23 - the sum from i equals zero
557:28 - to x rounded down
557:32 - and choose i
557:36 - p to the power i 1 minus p
557:42 - to the power n minus i
557:46 - and it looks like all i did was write
557:48 - down sum over the probability mass
557:50 - function then that that is what i'm
557:51 - writing down
557:52 - i didn't simplify this any further
557:54 - there's not really a whole lot more
557:57 - that you can say with this formula
557:59 - there's no
558:00 - fun little algebraic simplifications
558:03 - that you get
558:04 - all you're just going to say is sum up
558:07 - over the probably mass function
558:09 - and for that reason you're
558:13 - with the exception of um
558:16 - uh cases like some strange
558:20 - n or p
558:23 - historically in this class i would have
558:25 - students use the tables that were
558:27 - provided in the back
558:28 - of the textbook to
558:32 - work with the probably mass function or
558:34 - no not the problem mass function
558:36 - the cumulative distribution function
558:39 - now seeing as i am teaching this class
558:42 - online
558:43 - at the moment i don't necessarily see
558:47 - why i shouldn't like i'm telling my
558:50 - students
558:52 - that they can use r for pretty much
558:55 - anything
558:56 - even on quizzes and even on tests so
559:00 - for that reason i'm just not going to
559:02 - bother
559:04 - with working with the textbook and using
559:06 - the tables in the back of the book
559:08 - in these videos instead i'm just going
559:12 - to use
559:12 - r to get the
559:16 - cdf for binomial random variables
559:18 - although there may be some situations
559:20 - where like we might have um the input x
559:25 - we might replace the input x with say
559:27 - one okay if it's one
559:29 - you don't necessarily have to use r
559:31 - maybe i'll tell you not to look up the
559:32 - number and not to use r
559:34 - and ask you to compute the cdf just
559:37 - because there's only two things you're
559:38 - going to end up
559:39 - having to compute only two things are
559:41 - going to get plugged in
559:42 - so but that's kind of where
559:46 - what we're working with right now i
559:48 - might recycle these videos
559:50 - in the future and if i do be aware
559:55 - all right so one thing that's nice
559:59 - in these uh upcoming sections is
560:02 - i'm not going to go through the trouble
560:05 - of computing
560:06 - expected values using that
560:09 - summation formula using like x times p
560:13 - of x the sum over all x with p of x is
560:15 - not zero
560:16 - i'm not gonna bother with that anymore
560:18 - i'm just going to tell you what the
560:19 - expected value for this random variable
560:20 - is
560:21 - it's np no it's just np
560:24 - it's np i was jumping ahead of my head
560:27 - to the variance
560:28 - the variance of this random variable x
560:30 - is equal to n
560:31 - times p times 1 minus p and
560:35 - the standard deviation of x
560:38 - is just the square root of the variance
560:44 - going to draw your attention to
560:45 - something one way you can view
560:48 - binomial random variables is as the sum
560:52 - of bernoulli random variables in fact
560:55 - you could probably play around oops i
560:58 - didn't want to race
561:01 - have a look at this probably mass
561:02 - function formula
561:05 - and show for me
561:09 - that if you choose an n equal to one
561:14 - the resulting probability mass function
561:16 - is the is the probably mass function of
561:18 - the bernoulli random variable
561:20 - that is in fact the case um so
561:23 - uh i so that's that's something to look
561:25 - into
561:27 - uh but
561:30 - okay i'm saying that binomial random
561:33 - variables
561:34 - are the sum of n bernoulli random
561:36 - variables the expected value of us
561:38 - oh hold on and independent bernoulli
561:41 - random variables
561:42 - that's critical
561:46 - if that's the case remember
561:49 - that the expected value of a bernoulli
561:51 - random variable was p
561:53 - and the expected value of a binomial
561:55 - random variable
561:56 - is n times p
562:00 - so you're saying in a sense that we add
562:03 - up p
562:04 - n times to get the expected value
562:07 - hmm intriguing and
562:10 - actually remember that the variance of a
562:12 - bernoulli random variable
562:14 - was p times 1 minus p well now we're
562:16 - adding up n of those and we get
562:18 - np1 minus p for the variance
562:23 - intriguing so
562:26 - that is something to notice and also
562:29 - these
562:29 - expected value well i don't know
562:31 - necessarily about the variance being
562:33 - something very easily interpreted but
562:34 - the expected value certainly is
562:36 - it's saying that if there's a
562:37 - probability of a success happening let's
562:40 - say that it's a
562:41 - let's say that the probably success is
562:43 - 0.1
562:44 - and you do this experiment 10 times then
562:46 - you expect to see one success in your
562:48 - sample
562:49 - or if you do this experiment 100 times
562:50 - you expect to see 10 successes in your
562:52 - in your sample so it's actually a rather
562:57 - easily interpreted quantity this uh n
562:59 - times p
563:00 - quantity okay
563:03 - and i mention here that select values
563:08 - of bx and p are given in table 8.1 of
563:10 - the textbook but
563:12 - in this video i'm just going to use r
563:17 - okay uh moving on you flip
563:20 - a fair coin ten times all right
563:24 - so we should start filling out with
563:26 - numbers you flip a fair coin ten times
563:29 - there's going to be a binomial random
563:31 - variable showing up
563:32 - so fair coin suggests that the p
563:35 - parameter of this binomial random
563:36 - variable
563:37 - is going to be one half presumably what
563:39 - we're doing is counting the number of
563:41 - heads
563:41 - and if we're counting the number of
563:43 - heads then the resulting
563:44 - random variable is binomial and the end
563:48 - parameter of that binomial random
563:49 - variable will be 10.
563:51 - all right what is the probability you
563:53 - see exactly four heads
563:54 - do so without using a table the
563:58 - probability
563:59 - that this random variable x which is
564:02 - following a binomial
564:04 - binomial distribution with parameters
564:07 - n is 10 and p is one half
564:12 - so the probability that x is equal to 4
564:18 - is going to be 10 choose 4
564:24 - one-half to the power 4 and
564:28 - one half to the power ten minus four
564:33 - okay you're probably noticing well okay
564:36 - we got one half to the power
564:37 - four one half to the power ten minus
564:39 - four so that's the same as
564:41 - 10 choose four one half
564:44 - to the power ten and that's just
564:47 - basically because
564:49 - one half is equal to one minus one half
564:51 - so maybe i should write one minus one
564:52 - half to be a little bit more clear
564:55 - uh like that that that's the reason why
564:58 - but if he had instead
564:59 - instead of one half we said the
565:01 - probability of getting heads is point
565:02 - one
565:04 - then this would be the then
565:07 - thinking about the problems function
565:09 - this way would have been more correct
565:12 - or would have been correct it's not more
565:14 - correct because the other one is
565:15 - incorrect
565:16 - all right so uh
565:20 - actually we're going to compute this
565:21 - thing by hand so we're
565:24 - going to say that 10 choose 4
565:28 - is 10 factorial divided by
565:32 - 4 factorial times 6 factorial
565:36 - and we have 1 over 2 to the power
565:40 - 10 that's one half raised to the power
565:42 - 10
565:43 - which is equal to 10 factorial
565:46 - divided by 4 factorial times 6 factorial
565:48 - we've got 10
565:50 - times nine times
565:53 - eight times seven
565:57 - over four times three
566:00 - times two times one and then this is all
566:04 - multiplied by one half to the power
566:05 - 10 and the 4
566:09 - and the 2 will cancel out with the 8 and
566:11 - the 3 will cancel out with a 9
566:14 - leaving us a 3 so that gets us
566:18 - uh 210
566:23 - over 1024 that's the tenth power of two
566:29 - uh which is equal to since there is a
566:31 - two in common
566:32 - uh 105 over 512
566:37 - which is approximately equal to 0.2
566:43 - okay so that's the answer to that one if
566:46 - x follows a binomial distribution with
566:48 - parameters 10 and 0.5 compute the
566:49 - probability that 4 is less than x which
566:51 - is less than or equal to 6.
566:54 - so we've actually got a couple ways we
566:56 - could do this
566:58 - let's do this without the table the
567:01 - probability
567:03 - that 4 is less than
567:07 - x which is less than or equal to 6
567:10 - is equal to
567:14 - the probably mass function at five
567:16 - because you don't include four
567:21 - plus the probably mass function at six
567:29 - which is equal to uh
567:33 - uh it's going to be 10 choose
567:36 - five and we know we're just going to end
567:38 - up with one half to the power 10
567:41 - in the end if in general if
567:45 - uh our parameter were not one half we
567:47 - should probably
567:49 - we should probably reason this way
567:54 - so we've got 10 choose five plus
567:58 - ten choose six i guess we switch to
568:01 - green
568:02 - uh one half to the power ten
568:06 - and that means what we need to compute
568:07 - now is
568:10 - ten choose five and ten two six we
568:12 - already know that one half to the power
568:13 - of the
568:14 - ten is uh uh one over a thousand twenty
568:17 - four
568:20 - so ten choose five that's going to be uh
568:23 - ten times nine times 8
568:27 - times 7 times 6
568:31 - divided by 5
568:34 - times 4 times 3
568:37 - times two times one
568:42 - and ten to six
568:46 - is equal to two hundred and ten
568:51 - and that's because 10 choose 6 is equal
568:54 - to 10 choose 4.
568:56 - i'm going to leave it up to you to
568:57 - figure that out i believe that was a
568:59 - problem in the exercise set but
569:02 - yeah that's the thing so now what we
569:04 - need to figure out is 10 choose 5.
569:06 - so we've got the 5 and the 2 canceling
569:09 - out with the 10 the 3 canceling out with
569:11 - the 9 reducing it to 3
569:13 - uh and the 4 canceling out with the 8
569:15 - reducing it to 2.
569:18 - so we've got in the numerator
569:21 - 3 times 2 times 7 times 6 and 3 times 2
569:24 - times 7 times 6
569:26 - is uh 252
569:30 - i believe yeah so it's going to be
569:34 - yeah that's 252. so this quantity
569:38 - evaluates to 252.
569:44 - so we will get for their
569:48 - our probability uh 252 plus 210
569:53 - which is uh
569:56 - which is going to be
569:59 - 462 divided by 1024.
570:04 - which that is
570:08 - that's around point uh five
570:13 - after you do some rounding uh we can
570:15 - also do some reducing of that fraction
570:17 - too
570:19 - now that said there was an alternative
570:21 - way we could have computed this quantity
570:23 - which was to say that this is equal to
570:27 - the cdf at six
570:33 - minus the cdf at
570:37 - 4.
570:41 - and then what's left what's left to do
570:44 - is get the cdf
570:46 - at six and four all right
570:51 - well let's get that
570:54 - i'm gonna have to boot up an r session
571:02 - all right so we've got p binom
571:05 - that's the function that is responsible
571:08 - for working with binomial random
571:09 - variables
571:10 - and we're going to give p bino what are
571:14 - we going to give it
571:16 - uh right so we're going to give it
571:19 - six our other parameters are size
571:22 - that's 10 and prob is equal to 0.5
571:28 - minus p by nom which is going
571:32 - at 4 size equals
571:35 - 10 prob equals 0.5
571:39 - yeah about 0.45 which rounds to about
571:42 - 0.5
571:43 - which for what it's worth i said it was
571:46 - approximately 0.5
571:47 - when we were doing stuff by hand and
571:50 - that was because we were rounding
571:54 - i knew i was rounding when i was uh when
571:56 - i was
571:57 - when i was saying that's about 0.5
572:00 - so
572:04 - uh well let's go ahead and write down
572:06 - that more exam
572:07 - exact answer say that this is
572:11 - approximately 0.45
572:17 - okay next example
572:21 - compute the probability that 2 is less
572:22 - than or equal to x which is less than or
572:24 - equal to four
572:25 - we could do this by summing up over the
572:27 - probability mass function but now i
572:28 - really don't want to do that
572:30 - i'm just i mean i've got better
572:33 - ways to spend my time so i'm going to
572:37 - instead say that this is equal to
572:41 - the cdf at
572:44 - 4
572:51 - minus the cdf at
572:54 - uh two minus one
572:59 - remember we have to do the two minus one
573:01 - because we need to include the two in
573:02 - our region
573:03 - and the only way to do that is if we do
573:05 - two minus one
573:07 - okay and the other parameters are ten
573:10 - and one half
573:12 - and and you guys know what two minus one
573:15 - is this is equal to one
573:16 - which is going to be well
573:23 - let's compute this so
573:26 - p binom at
573:30 - 1 minus p by nom
573:34 - or other way around actually so we got
573:36 - four
573:39 - all right so 0.366
573:44 - so approximately equal to 0.36
573:48 - six what is the probability that you see
573:53 - more than seven heads strictly more so
573:55 - this is the probability
573:57 - that x is greater than
574:01 - seven that is
574:04 - well the converse event or the uh
574:08 - um the complementary event to the x
574:11 - being greater than seven
574:13 - is x is less than or equal to seven
574:16 - so this is going to be one minus the
574:19 - probability
574:21 - that x is less than or equal to seven
574:26 - since remember the probability of a
574:28 - complement is 1 minus the probability of
574:30 - a
574:31 - the complement of the set x is smaller
574:33 - than 7 is x is less than or equal to 7.
574:37 - so then we get that formula uh so this
574:40 - is going to be
574:42 - uh oops
574:46 - this is equal to one minus a cdf
574:49 - at seven with parameters ten
574:52 - and one half
574:56 - and now we need to compute that and
574:58 - we're going to
574:59 - turn to r for that so this is 1
575:03 - minus the cdf
575:07 - at 7 size equals 10
575:11 - prob equals 0.5
575:14 - so the probability is about 0.05
575:17 - let's say 0.055
575:20 - so this is approximately equal to
575:28 - .055
575:29 - now i should probably mention something
575:31 - else about how the software was working
575:33 - we could have done
575:34 - instead p by nom
575:37 - 7 size equals 10
575:40 - prob equals 0.5
575:43 - and there's an additional parameter that
575:45 - all of these uh
575:47 - p functions have which is lower dot tail
575:51 - lower dot tail let's set that equal to
575:53 - false
575:56 - that got us the same thing uh
576:01 - the p so basically these p bottom
576:03 - functions by default they're giving you
576:05 - the cdf but they can also give you one
576:06 - minus the cdf if you
576:07 - set lower tail equals false
576:11 - if you were to ask the developers for
576:13 - these functions they would say
576:14 - that rather than doing one minus the
576:18 - the cdf or one minus p binom or whatever
576:22 - you should use the lower tail equals
576:24 - false parameter the reason being that
576:27 - you're going to get less numerical error
576:29 - if you're using the lower tail equals
576:30 - false parameter
576:33 - because numerical error is very much a
576:35 - thing like we care
576:36 - whenever whenever we're using software
576:38 - to compute numbers we care about
576:39 - numerical error
576:41 - and it turns out that setting lower
576:42 - table tail equals false
576:44 - that results in less numerical error uh
576:47 - i
576:47 - i don't really know why i'm guessing
576:49 - it's because they can do some more
576:50 - optimizations or some other fancy
576:52 - uh numerical tricks thing is
576:56 - as an instructor i like to make sure
576:59 - that people are thinking like
577:01 - this expression or this relationship
577:05 - i really want students to understand
577:07 - that and it's very easy to just
577:10 - lose that under the easiness of this
577:13 - of this um of this uh of this function
577:17 - in this notation
577:19 - so i don't know how frequently i'm going
577:20 - to do that um
577:22 - and also for for whatever it's worth
577:24 - sometimes when i'm
577:25 - writing my own uh functions for
577:28 - probability
577:29 - i don't always include that parameter
577:30 - myself just because i can't i don't have
577:33 - it i know i don't really know what the
577:34 - developers are doing
577:35 - to make sure that lower tail goes false
577:38 - gives you more accurate answers
577:41 - uh so also by the way if
577:44 - if you're watching this in the future
577:46 - hello future person
577:47 - if you're watching this in the future
577:49 - and you're using a table because
577:51 - maybe because i told you to uh you're
577:54 - using a table for these calculations
577:56 - you don't have access to the probably
577:58 - that x is greater than seven you only
577:59 - have
578:00 - access to the probability that x is less
578:01 - than or equal to seven
578:03 - so being aware of um
578:07 - how this stuff is working uh or at least
578:10 - being aware of this
578:11 - relationship that i've underlined in
578:12 - blue still matters a great deal
578:16 - all right so continuing
578:20 - on compute the expected value of
578:24 - x the variance of x and the standard
578:26 - deviation
578:28 - of x
578:32 - all right so the expected value
578:36 - of x
578:40 - let's see all right the expected value
578:43 - of
578:43 - x is n times p
578:47 - which is 10 times one half
578:53 - which is equal to five all right simple
578:55 - enough
578:56 - the variance of x
579:00 - is n times p times 1 minus p
579:07 - which is 5 times one half
579:12 - which is five halves or two point five
579:17 - the standard deviation of x is going to
579:20 - be the square root
579:22 - of five halves and i don't know what
579:24 - that is off the top of my head
579:26 - so we can just leave it like that that's
579:28 - fine
579:29 - all right uh our functions that are
579:32 - doing this stuff well i think we just
579:34 - did it so some of this is completely
579:35 - redundant
579:37 - um
579:40 - here i actually created a random
579:42 - variable
579:44 - using discrete rv which presumably is
579:47 - loaded up
579:48 - uh so i create a random variable x to
579:50 - represent the x that we were talking
579:51 - about before
579:52 - and computing's expected value variance
579:54 - and standard deviation
579:55 - these are basically the same as what i
579:57 - had before i also plotted its
579:59 - probability mass function this is what
580:00 - it's probably a mass function looks like
580:04 - okay next example your
580:07 - manufacturer of widgets sends
580:10 - batches of witches and giant bins your
580:13 - company will accept a shipment of widget
580:15 - of widgets if no more than seven percent
580:18 - of widgets are
580:19 - defective the procedure for deciding
580:22 - whether a shipment is defective is to
580:23 - choose
580:23 - four widgets from the batch at random
580:25 - without replacement
580:26 - if more than one widget is defective the
580:28 - batch is rejected
580:30 - what's the probability of rejecting the
580:31 - batch if seven percent of widgets are
580:32 - defective
580:33 - model the process using a binomial
580:35 - random variable
580:37 - so we have so we're going to assume that
580:41 - there are in fact seven percent of
580:42 - widgets and actually
580:44 - the argument being used in this problem
580:47 - uh this problem is kind of suggesting
580:49 - the possibility
580:51 - well actually the procedure being
580:54 - described in this problem is basically a
580:56 - hypothesis test
580:58 - and uh we're gonna talk more about
581:02 - hypothesis testing later
581:04 - in a later chapter uh but basically what
581:08 - you do
581:08 - when you're working on the mathematics
581:10 - of a hypothesis test
581:12 - for computing p-values and all that
581:13 - stuff you assume that the null
581:15 - hypothesis is true
581:17 - so the null hypothesis is that seven
581:19 - percent of the widgets are defective
581:21 - so um uh so in this case we assume that
581:25 - seven percent of widgets are defective
581:27 - in which case the distribution
581:29 - of the random variable x is going to be
581:32 - let's see how many widgets did they pull
581:34 - out for
581:36 - uh so n is equal to
581:40 - four and p is equal to 0.07
581:48 - okay so also there's another wrinkle
581:52 - here
581:54 - the bin of widgets has a finite number
581:58 - of bims
582:00 - uh no no no not not fighting overpins
582:02 - has a finite number of widgets
582:05 - but this binomial random variable
582:09 - is not supposed to work in that
582:10 - situation
582:13 - see if there's a finite number
582:16 - of successes in this possible
582:19 - in this finite population then an
582:23 - implication of that is that
582:26 - you don't have independent successes and
582:29 - failures
582:30 - because if you pull a success out of the
582:33 - population
582:34 - you cannot pull that success again and
582:36 - presumably in this example when a
582:38 - when this uh when you're checking the
582:41 - widgets
582:42 - uh you pull out a widget you check it
582:44 - but you don't put it back in the bin
582:46 - it's for you to draw again no one ever
582:48 - does that
582:50 - so actually we don't have independent
582:55 - bernoulli trials
582:58 - so we don't have a sum of independent
583:00 - bernoullis we don't have independent
583:02 - trials
583:03 - which means that technically this random
583:05 - variable should not be
583:06 - a binomial random variable actually the
583:09 - random variable
583:10 - that is more accurate for this context
583:14 - is what's known as a hypergeometric
583:16 - random variable which we'll talk about
583:18 - in a later section
583:19 - thing though is if the sample if the
583:21 - population is large enough
583:24 - relative to how many successes there are
583:27 - in the sample like if there's a million
583:29 - widgets and seven percent of those
583:31 - widgets
583:32 - are defective then that means that about
583:36 - 70 000 defective widgets exist in the
583:39 - sample
583:40 - or in the population my my apologies uh
583:43 - in which case it the the numbers are so
583:46 - large that basically you can treat this
583:48 - as
583:49 - a binomial experiment anyway
583:53 - because the difference between the
583:55 - binomial
583:57 - random variable and this
584:00 - more accurate random variable the
584:02 - hypergeometric random variable those
584:04 - differences are negligible so
584:07 - you so you you get to you get to cheat
584:10 - you get to use the
584:11 - simpler binomial random variable as
584:14 - opposed to the
584:15 - more complicated more complicated
584:17 - hypergeometric
584:18 - okay
584:21 - so let's carry on then
584:24 - they want to know what is the
584:26 - probability of rejecting the batch if
584:28 - some percent are defective
584:30 - uh if more than one widget is defective
584:32 - okay
584:33 - with this problem we actually need to
584:35 - translate out because this is a word
584:37 - problem and by the way in stats classes
584:39 - i
584:39 - absolutely love to ask word problems
584:42 - so many word problems because statistics
584:46 - is so
584:46 - applied that it just feels inappropriate
584:49 - to
584:50 - not be asking word problems um it's
584:53 - it's such an applied subject that you
584:55 - have to be asking them
584:56 - okay so what corresponds to rejecting
584:59 - the batch you reject
585:00 - the batch if there's more than one
585:03 - defective widget
585:05 - in your sample so that's it so more than
585:08 - one that means greater than one
585:10 - and our random variable for tracking the
585:12 - number of defective widgets that we
585:14 - found
585:14 - is x and i guess all right a student
585:17 - might find it wait a minute wait a
585:19 - minute you're calling it defective
585:20 - widget
585:20 - a success yes it's mostly for the
585:24 - language
585:25 - all right so this is the probability we
585:27 - want to compute we want to compute the
585:29 - probability that x is greater than one
585:31 - which is equal to one minus the
585:33 - probability
585:34 - that x is less than or equal to one and
585:37 - at this point you could say all right
585:38 - let's go to r
585:39 - and compute this and i'm gonna say no
585:41 - we're not going to do that
585:42 - we're going to instead compute this
585:44 - thing by hand
585:46 - and say that this is going to be the pmf
585:50 - at uh 0 for
585:53 - parameters 4 and 0.07 plus the
585:56 - probability mass function
585:58 - at 1.
586:01 - oh i'm sorry i'm sorry uh i need to say
586:06 - that this is equal to 1 minus
586:08 - parentheses
586:09 - all that stuff okay
586:15 - all right there we go that's correct
586:19 - and now we need to compute each of those
586:20 - probability mass functions
586:23 - okay so the probably mass function at
586:26 - zero
586:31 - is going to be we've got
586:34 - four choose zero and four choose zero
586:38 - is one there's only one way to choose
586:42 - uh none of the four things
586:45 - in your in your little group and that's
586:47 - to choose all the one
586:48 - all the other ones there's only one way
586:50 - to do it so four to zero is one
586:52 - uh next up we've got uh what what have
586:56 - we got
586:56 - um oh yeah uh so .07
587:01 - to the zeroth power and .93
587:05 - to the fourth power b
587:09 - one for .07
587:14 - is equal to four choose one which
587:17 - hopefully you know
587:18 - is four you can at least reason about i
587:20 - was like okay how many ways are there to
587:22 - pick one thing out of four
587:23 - we'll pick one of those four things all
587:25 - right there we go um
587:27 - so there we go uh so then we got .07
587:31 - to the first power .93
587:34 - to the third power and
587:37 - after this you go to a calculator to
587:39 - compute those numbers
587:41 - i think i actually computed them
587:43 - [Music]
587:44 - rather accurately in my notes
587:50 - so let's see what did
587:55 - i have
588:00 - okay so what i'm seeing here
588:06 - is that this is equal to
588:11 - so the so the top one is equal to point
588:13 - seven
588:14 - four
588:18 - that is an exact number i got
588:21 - a little carried away with the accuracy
588:23 - and the second one is point two two
588:26 - five two two
588:31 - and that means that we're going to have
588:35 - that this quantity here
588:39 - that ultimately is what we're trying to
588:40 - compute
588:42 - is equal to 1 minus
588:50 - 0.748052
588:52 - plus 0.22522
588:56 - which is equal to 1 minus 0.973272
589:04 - 3272 which is equal to
589:10 - 0.026 seven
589:12 - two eight
589:15 - all right
589:18 - uh next up oh yeah there it is
589:22 - excuse me
589:30 - my apologies i had to sneeze okay uh
589:33 - next example uh this one's fun
589:37 - i claim that i can make 80 of my free
589:39 - throw shots when playing basketball
589:41 - you plan to test me by having me shoot
589:43 - 20 baskets if i make fewer baskets in a
589:45 - specified amount
589:47 - you will call me a liar the threshold
589:49 - amount of baskets is chosen so that the
589:51 - probably make less than this amount
589:53 - given that i am in fact an 80 free throw
589:55 - shooter does not exceed five percent
589:57 - what is the threshold amount all right
590:00 - uh oh yeah additionally compute the mean
590:01 - and standard deviation of the number of
590:03 - shots i would make if my claim is true
590:04 - okay uh let's do this let's do the
590:06 - second part first because that's easier
590:08 - uh the first part is going to require a
590:10 - conversation
590:12 - so the expected value will say um
590:15 - s is following a binomial distribution
590:18 - uh with uh parameters i'm going to shoot
590:22 - 20 baskets yeah so n is 20 and
590:25 - p is 0.8
590:29 - because all right this is again a
590:32 - hypothesis testing type
590:33 - problem in which case you're assuming
590:36 - that i am in fact an 80 free throw
590:39 - shooter
590:41 - so the expected value of not x because i
590:44 - decided not to do x
590:45 - um of s is going to be
590:48 - 0.8 times 20
590:52 - which is equal to 16.
590:56 - so you expect me to make 16 of my
590:58 - baskets
590:59 - uh the did i ask for standard deviation
591:02 - yes i did so
591:03 - the variance of this random variable
591:08 - is going to be uh 20 times
591:11 - 0.8 times 0.2
591:16 - which is equal to 3.2
591:23 - and the standard deviation
591:27 - of s is equal to the square root of 3.2
591:32 - which i don't know off the top of my
591:33 - head and i i'm i'm just
591:35 - i'm just not gonna bother okay
591:39 - all right so that was the easy part now
591:42 - for the hard part
591:44 - uh i have asked
591:47 - that you pick a threshold amount
591:51 - of baskets so that the probability i
591:54 - make
591:54 - less than this amount given that i am in
591:56 - fact an 80 free throw shooter
591:58 - does not exceed five percent all right
592:02 - so we will call this threshold amount we
592:04 - will give it a name
592:06 - uh the threshold amount we will call
592:09 - this
592:09 - we will call this quantity k all right
592:13 - i'm asking that i i'm asking you to find
592:18 - a k
592:20 - such that the probability that s is
592:23 - let's see
592:23 - so if i make fewer baskets so s
592:26 - is less than k
592:31 - this number needs to be
592:35 - at most 5 so this needs to be
592:38 - at least 0.05
592:42 - well at most 0.05 and
592:45 - actually there's going to be a number of
592:47 - possible k's
592:49 - such that it's less than 0.05 but we're
592:50 - going to say that this is the largest
592:52 - possible k
592:54 - uh such that such that this probability
592:56 - is less than
592:58 - or equal to 0.05 uh k is a constant here
593:01 - we just don't know what it is
593:03 - uh s is a random variable uh let's go
593:06 - ahead and play around with this
593:08 - expression some more
593:10 - before continuing on this is saying that
593:13 - the probability
593:15 - that s is less than or equal to k minus
593:18 - 1
593:23 - uh less than or equal to k minus one
593:29 - that's going to be less than or equal to
593:30 - 0.05
593:33 - all right so we actually so by doing
593:35 - that i have
593:37 - the cdf on the left hand side of the
593:39 - inequality
593:41 - and 0.05 on the right hand side so we're
593:43 - go what we need to do now
593:44 - is basically a reverse lookup we're
593:48 - looking up
593:48 - a number such that the cdf
593:53 - is um less than or equal to 0.05
593:57 - the largest number possible such that
593:59 - the cdf is less or equal 0.05
594:02 - this is similar to the notion of
594:04 - quantile
594:06 - because a quo so you have a probability
594:08 - that a random variable
594:10 - is less than or equal to uh let's say
594:13 - 10 percent then the 10 quantile is that
594:16 - number
594:18 - so this is actually related to the
594:20 - notion of quantiles the unfortunate
594:21 - thing though is that what we're talking
594:22 - about
594:22 - are discrete random variables which
594:26 - adds this complication in that it's
594:28 - possible that the cdf in fact not just
594:30 - possible
594:31 - it's quite likely that the cdf never
594:34 - actually equals 0.05
594:36 - if it were in fact equal for the cdf to
594:38 - equal 0.05
594:39 - then we would say all right pick a k
594:42 - minus 1
594:42 - such that cdf is equal to 0.05 the only
594:46 - thing though is that's not actually
594:47 - likely to be the case
594:49 - um it's likely that the cdf never
594:52 - actually reaches 0.05
594:53 - in fact let's now go to r
594:58 - so let's look at so we got uh the cdf
595:04 - the cdf is
595:09 - so possible values for the standard
595:11 - variable are from 0 to 20.
595:13 - so we're going to ask for the cdf's
595:15 - values from
595:16 - for all numbers between 0 and 20. size
595:19 - is equal to 20
595:20 - prob is equal to 0.8
595:24 - ah that's it's not very helpful
595:27 - um all right i think that if we were
595:30 - looking at
595:31 - the textbook we would be
595:34 - rounding to uh three decimal places
595:38 - so uh so digits
595:42 - equals three
595:46 - all right yeah this is yeah we're gonna
595:48 - do this just because
595:51 - like what we have up here is scientific
595:52 - notation which may be more accurate but
595:56 - uh also it's hard to read so we're gonna
595:58 - round to three decimal places and work
596:00 - with this
596:01 - so this is the cdf uh and what we're
596:05 - looking for
596:06 - is uh we would go let's actually
596:09 - let's actually call give this vector
596:12 - a name so we'll call it cdf
596:16 - and we'll say names cdf
596:20 - will be 0 to 20.
596:23 - so now let's print out cdf okay
596:27 - all right so so so so so
596:30 - so um
596:31 - [Music]
596:34 - what we're looking for in this is a qua
596:37 - is a quantity
596:38 - where the cdf uh
596:42 - does so that so notice that the cdf is
596:44 - increasing as we
596:45 - increase the input to the cdf right
596:49 - our cdf is increasing but and we want to
596:52 - find the largest number
596:54 - that we can put into the cdf such that
596:56 - it doesn't exceed 0.05
596:58 - and that number is 12 because at 12 the
597:02 - cdf is going to be 0.032
597:04 - and at 13 the cdf is going to be 0.087
597:07 - so it crosses that threshold at 12
597:09 - suggesting
597:11 - the that k minus 1
597:16 - suggesting that k minus 1
597:20 - equals 12. and then we add 1 to both
597:24 - sides
597:25 - to suggest that k is equal to 13 that is
597:30 - if i score less than if i make less than
597:32 - 13 baskets you're going to call me a
597:34 - liar
597:35 - uh what is uh 13 divided by 20
597:38 - that is uh 0.6 so
597:41 - that's 0.65 so if i make uh 65
597:45 - or less than if i make less than 65 of
597:48 - my basket you're going to call me a liar
597:49 - the principle being
597:51 - the the logic being that that is such a
597:53 - rare amount
597:55 - it's so unlikely for you to score less
597:58 - than 60
597:59 - 65 of your baskets if you were in fact
598:01 - an eighty percent three
598:02 - free throw shooter that we're actually
598:05 - gonna say it's more likely that you're
598:06 - lying
598:07 - then you're actually telling the truth
598:10 - or at least it seems
598:12 - it seems unreasonable to continue to
598:14 - believe
598:15 - that you are still telling the truth so
598:18 - this is this so so that's how you would
598:20 - use like the cdf if we were
598:23 - if we if you were using the textbook you
598:24 - would use the cdf this way
598:26 - you would scan the cdf uh
598:29 - for some reason my mouse stopped working
598:31 - i don't know why this is a super cheap
598:33 - computer
598:34 - you would scan the cdf until eventually
598:38 - you crossed over that 0.05
598:40 - threshold and then take whatever this
598:43 - what
598:44 - take whatever number got the cdf2 uh
598:46 - just below that
598:47 - threshold so if i had switched this to
598:49 - uh say all right it needs to be less
598:51 - than or equal to 0.10
598:54 - then we would go to 13 we haven't
598:56 - crossed point 10 yet
598:57 - we would go to 14 but then we then we
598:59 - would cross so we would say all right
599:01 - the threshold amount is 13. uh let's say
599:04 - that i said
599:04 - instead 0.01 uh actually 11 is probably
599:08 - not what you would choose because
599:10 - you know that we're rounding here so you
599:12 - would go with 10.
599:14 - so because you know that actually well i
599:17 - don't know
599:18 - what what was a 11 so if we look back at
599:21 - the
599:22 - original vector that's a bit more
599:24 - accurate that doesn't have any rounding
599:26 - so one two three four five
599:29 - six seven eight
599:33 - nine ten eleven
599:40 - uh
599:42 - next one oh actually it's rounding up
599:45 - so you could choose 11. yeah 11 would be
599:48 - fine
599:49 - so yeah it doesn't always round down
599:53 - uh so that's what you would do you would
599:55 - just kind of reverse look
599:57 - from the uh the uh from the cdf
600:01 - especially if you're using the book or
600:04 - you could use q binom uh
600:07 - we'll put in 0.05 size equals 20
600:12 - prob equals 0.8 you could have used uh
600:16 - okay what exactly is cubinom doing
600:20 - uh it might be doing something else it
600:22 - might be saying okay you exceed it
600:24 - because so we actually might need to do
600:26 - q by
600:27 - minus one uh let's look at the
600:29 - documentation for q
600:30 - by nom
600:35 - oh i think you just yeah you would just
600:38 - have to
600:39 - recognize all right so uh it's going to
600:42 - give you some details
600:43 - somewhere okay so it says right here
600:47 - the quantile is defined as the smallest
600:49 - value x such that f of x is greater than
600:51 - or equal to p
600:52 - which is actually different from how i
600:55 - just defined it so r's definition of
600:56 - what a quantile is for discrete random
600:58 - variables
600:59 - is uh different than what i just said so
601:03 - being aware of that you would actually
601:05 - have to take
601:07 - uh whatever q binong gave you and then
601:09 - do
601:10 - minus one
601:13 - to get the right answer
601:16 - okay but all right
601:20 - or i mean i
601:23 - i don't know i think there's so many
601:26 - different ways
601:27 - to possibly think about it and say okay
601:29 - the q binom that
601:30 - that r actually has is uh
601:34 - effectively uh giving you this quantity
601:37 - right away
601:38 - so you don't have to work with the cdf
601:40 - you that's another way you could
601:42 - possibly think about it
601:45 - all right so uh by the way what we're
601:47 - basically saying is that if you ended up
601:49 - shooting a number of baskets
601:52 - such that you ended up in uh 11 12
601:55 - such that you ended up in this region
601:58 - it's so unlikely
601:59 - if you were in fact an 80 free throw
602:01 - sure to end up in this region
602:03 - i'm justified in saying you're a liar
602:06 - how unlikely is it well it's actually uh
602:09 - 3.2 percent
602:11 - uh it's so so so unlikely that we would
602:14 - just call you a liar
602:15 - because if you were in fact a three
602:17 - throw shooter you should probably be
602:18 - ending up in the other region
602:20 - and 80 free throw shooter my apologies
602:22 - i'm always confusing my words
602:24 - okay so that's it for uh the binomial
602:28 - random variable and
602:30 - in the next section we will be talking
602:31 - about the hypergeometric and negative
602:33 - binomial distributions
602:35 - all right so i will see you there
602:44 - uh we are now on to the last section
602:48 - of this chapter discussing the poisson
602:52 - process and the poisson probability
602:54 - distribution
602:55 - so x is said to follow a poisson poisson
602:59 - distribution with
603:00 - parameter mu or if
603:03 - the probability mass function of x is
603:06 - given by
603:07 - p of x parameterized by
603:10 - mu this is equal to
603:15 - e to the power negative mu mu to the
603:18 - power
603:19 - x over x factorial
603:23 - for uh x being
603:27 - a member of the set 0 1
603:30 - 2 and so on so in other words if if x is
603:34 - a whole number
603:34 - if x is a whole number then this is the
603:36 - probably mass function otherwise it's
603:37 - zero
603:39 - so the first question you may ask is
603:41 - this a valid pmf
603:42 - the answer is yes and why is that well
603:45 - let's sum
603:46 - the probably mass function from x uh
603:48 - sorry
603:49 - from x uh equals
603:53 - zero to infinity we got p
603:56 - of x parameterized by mu
604:00 - this is equal to the sum from x
604:04 - equals zero to infinity
604:08 - e negative mu mu
604:11 - to the x over x
604:14 - factorial the e to the negative mu part
604:18 - that is effectively a constant so that
604:19 - can be pulled out
604:21 - so we could say that this is e to the
604:22 - negative mu and then we have the sum
604:25 - from x equals zero to infinity
604:29 - mu to the x over x factorial
604:34 - and that part which i have highlighted
604:39 - in red that is e to the power mu
604:43 - y calculus 2. this is the
604:46 - if i remember right the taylor expansion
604:48 - of the function e to the power
604:50 - x so this is coming from calculus 2
604:54 - but yeah that evaluates to e to the mu
604:57 - hence we get e negative mu
605:02 - e to the power of mu and those are that
605:05 - equals one
605:07 - and furthermore this probably mass
605:09 - function is positive everywhere
605:11 - so we get this is a valid pmf
605:14 - so uh poison random variables if x is
605:17 - following a poisson
605:18 - distribution with prion or mu then the
605:19 - expected value of x is equal to mu
605:21 - and also the variance of x is equal to
605:23 - mu
605:25 - so that means that
605:28 - we are parameterizing poisson random
605:30 - variables by their mean
605:33 - so in uh
605:37 - in your book if you're using the uh
605:40 - the dvor book table 8.2 contains the cdf
605:44 - of select poisson random variables
605:46 - or select poisson distributions uh
605:50 - in r the functions that are responsible
605:53 - for
605:54 - handling points on random variables
605:55 - random variables sorry
605:57 - our deploys
606:10 - handling polymath function cdf
606:14 - quantiles and random and random variants
606:17 - so in other words creating
606:18 - random instances of poisson random
606:21 - variables
606:22 - and they will be parameterized by their
606:23 - mean
606:25 - so the poisson distribution describes uh
606:28 - random variables that follow the poisson
606:30 - process
606:31 - so here is the intuition of poisson
606:33 - random variables
606:35 - they are
606:39 - tracking how many times within some how
606:42 - often
606:43 - a quote-unquote rare event occurs within
606:46 - a finite span of time
606:49 - so and which isn't at all clear from
606:52 - looking at this probably mass function
606:54 - that would in fact be the interpretation
606:57 - uh this is more of a limiting result
606:59 - this
607:01 - footnote that i have right here actually
607:04 - gives a little bit more justification as
607:06 - to why this is the case
607:08 - but you can think of it as all right we
607:09 - have some rare event
607:11 - uh me like i have a favorite example
607:15 - that was
607:15 - used by my uh 3070 instructor maybe i'll
607:20 - just pull up his webpage for you uh like
607:23 - i
607:24 - he he's got some he's got some
607:26 - interesting stuff
607:28 - on his on his web page that even now
607:32 - i kind of look back to it and like i
607:34 - think about it and i just kind of
607:36 - i kind of want to update it i think i
607:39 - think it could be updated but there's
607:40 - some interesting stuff there genuinely
607:42 - so math.utah.edu
607:46 - tilde treyberg
607:51 - uh his name was uh andres trebergs
607:54 - a professor at the u not actually a
607:56 - probability
607:57 - uh professor that's not his uh area of
607:59 - expertise although he's taught
608:01 - math 70 a few times and math the
608:04 - no matt 3070 sorry and matt 3080 a few
608:07 - times and
608:08 - like he he's interested in statistics
608:10 - but he is a
608:12 - uh what what is that area of study of
608:16 - his
608:17 - i think it's like analytic geometry i
608:19 - think that's what he studies
608:21 - so i wonder what happens if i just go
608:23 - straight here we could probably find
608:26 - from here his uh 3070 page
608:32 - yes i think this is
608:35 - i think this is it
608:39 - math 3070 fall 2013.
608:43 - math 3070 fall 2012
608:46 - is the last that was the class that i
608:48 - took this was my
608:49 - web page this is where i went to find
608:52 - his stuff
608:53 - so yeah i i took matt 3070
608:56 - in this class i think i'm pretty sure
608:59 - that's true
609:00 - all right if i scroll down yeah the
609:03 - supplementary materials
609:06 - oh yeah so here's a simple r uh
609:10 - which by john farzani this is the
609:12 - uh
609:13 - lab textbook for the r lab
609:16 - and uh several example problems he's got
609:19 - this
609:20 - one let's find it
609:23 - let's see prussian no no press
609:28 - poise
609:30 - ah let's see
609:33 - is this it i think this is it
609:38 - i think this is it
609:42 - uh are there horse kicks
609:47 - are people getting kicked by horses
609:50 - that's what i want to know
609:52 - i think this is i think this is it um
609:59 - i don't think so
610:04 - i don't think this is the horse kick
610:05 - example but the horse kick example is
610:07 - fun
610:10 - let's see oh why i want to close that
610:12 - webpage
610:14 - uh
610:18 - if we try horse
610:21 - come on
610:26 - there it is there it is the horse kick
610:28 - example
610:30 - ah yeah i love this one
610:34 - yeah so so have a look at this but
610:35 - basically it turned out that in the
610:37 - prussian army
610:38 - um you can model the number of people
610:40 - who died from horse kicks
610:43 - with poison random variables that
610:46 - poisson random variables actually do a
610:48 - good job of modeling stuff like that
610:49 - but i also had as
610:53 - potential examples i'm not entirely sure
610:54 - how the how accurate this is
610:56 - but you could maybe like maybe think of
610:59 - uh
611:00 - number of calls that a call center is
611:02 - getting in a day or the number of points
611:04 - that scored by a team in a game
611:07 - uh poison random variables
611:11 - so and also as being a poisson process
611:15 - a poisson process uh
611:18 - is a bit more general than that so a
611:21 - poisson process
611:23 - is a stochastic process stochastic
611:26 - processes
611:27 - are not the subject of this class
611:31 - beyond this little description i think
611:34 - another thing that could be considered a
611:36 - poisson process
611:37 - or that could be modeled by a poisson
611:39 - random variable is let's say you have
611:40 - some radioactive matter
611:42 - and the number of uh
611:46 - is it atoms i'm not sure i'm not very
611:48 - good at physics but
611:50 - some some particles are leaving the
611:53 - radioactive mass and the amount of
611:54 - particles that are leaving the mass
611:56 - uh over some period of time can be
611:58 - understood as a poisson
612:01 - random variable and also as a poisson
612:03 - process
612:05 - so i have this r code that's meant to
612:09 - simulate a poisson process and with a
612:11 - poisson process
612:12 - the time at which a particle or
612:16 - the time at which uh the pro the process
612:18 - jumps is random
612:19 - so it will so there are going to be
612:22 - random times at which the process
612:24 - increases its value by one so
612:27 - uh it will jump so you'll wait for a
612:30 - period of time and then suddenly the
612:31 - process will jump and
612:32 - basically something has left and then
612:34 - you'll wait a little longer and
612:35 - something else has happened maybe you
612:36 - could imagine this as being
612:38 - uh points in a game maybe a basketball
612:40 - game although i
612:41 - you could probably see why it's a little
612:43 - bit inaccurate to view points in
612:45 - basketball
612:46 - game as a poisson process because
612:48 - there's
612:49 - no way that you could well i guess this
612:52 - is possible
612:53 - if maybe if maybe you said both teams
612:57 - uh if you're counting like total points
612:59 - scored by either team
613:00 - i don't know it seems unreasonable
613:02 - though but you could
613:04 - but basically at random times this
613:05 - process is going to jump
613:07 - uh by one and the value of the process
613:10 - is going to increase and basically
613:13 - any fixed time the value of the process
613:17 - at a fixed time
613:18 - uh so you fix it at two at two
613:22 - or something and the value of the
613:23 - process at that time
613:25 - can be modeled by a poisson random
613:27 - variable
613:29 - so yeah that's uh hopefully that gives
613:32 - you some idea of what they're what
613:33 - they're describing
613:35 - how often some event occurs over a fixed
613:38 - period of time
613:40 - which in principle that event could
613:43 - be unbounded there's an infinite number
613:46 - of pos
613:46 - it could happen like there's no upper
613:49 - bound on how many times this event could
613:51 - happen it can
613:51 - happen an infinite number of times
613:53 - there's well okay maybe not literally
613:55 - infinite but
613:56 - any large number of times it's highly
613:58 - unlikely that it will be very large but
614:00 - there is no upper bound
614:02 - right so you're not gonna restrict it
614:03 - like the number of points that you could
614:05 - score in a basketball game
614:07 - like there's no there's no limit on that
614:10 - no one will just end the game well i
614:12 - don't know maybe but
614:13 - that's that seems that seems rather
614:16 - academic but
614:17 - yeah that's so this is going to be a
614:19 - random variable
614:21 - that is
614:24 - defined
614:31 - hello everyone we are now on the next
614:34 - chapter
614:35 - on continuous random variables and
614:37 - probability distributions
614:39 - continuous probability models are
614:41 - another major class of probability
614:43 - models
614:44 - in the previous chapter we saw models
614:47 - for
614:47 - discrete random variables with discrete
614:50 - random variables
614:52 - there was either a finite number of
614:54 - possible numbers this random variable
614:56 - could take
614:56 - or it was countably infinite for example
614:59 - the whole numbers
615:01 - were were possible in this situation
615:05 - not only are the number of possibilities
615:08 - infinite
615:08 - they are uncountably infinite so
615:13 - this these uh probably models allow
615:16 - for any real number within some range it
615:20 - could be within a range a to b
615:22 - or z like a to infinity or zero to
615:25 - infinity
615:26 - or negative infinity to infinity so any
615:29 - real number
615:30 - any of those could be possibly taken by
615:33 - the
615:34 - random variable so
615:37 - as a consequence of this we're going to
615:40 - change some of the notions that we had
615:41 - with discrete random variables but
615:45 - not by much for example the probability
615:47 - mass function
615:49 - is going to be replaced with the
615:50 - probability density function which is
615:51 - what we're going to be talking about
615:52 - right now
615:54 - and the cdf is still defined
615:58 - the same way but it's going to be
616:01 - computed a little bit differently
616:02 - instead of it being
616:03 - involving a sum it's going to involve
616:07 - an integral and expectations are also
616:09 - going to involve integrals basically
616:11 - whenever you would add numbers with
616:13 - discrete random variables
616:15 - you integrate with continuous random
616:17 - variables
616:18 - so this is where your calculus knowledge
616:20 - is going to be
616:22 - tested one nice thing though
616:25 - about continuous random variables though
616:29 - is that when you're working with
616:31 - continuous random variables
616:32 - uh the probability that the that that
616:34 - random variable
616:36 - is equal to any particular number is
616:38 - always zero
616:40 - which is the reason why we have to have
616:42 - uh integration and density functions
616:45 - so the probability that x is less than x
616:47 - is the probability that x is less than
616:49 - or equal to x
616:50 - and admittedly it is a little strange
616:53 - that the probability that this random
616:54 - variable
616:54 - is equal to a particular number is zero
616:57 - it's
616:58 - it's somewhat strange because when you
617:01 - go to your random number generator and
617:03 - ask this thing to produce a number it
617:04 - gives you a number
617:05 - but the probability that you got that
617:06 - number was zero so
617:09 - events that are happening with probably
617:10 - zero are happening all the time whenever
617:13 - you're working with
617:14 - continuous random variables
617:17 - but the way ferrous rasulaga
617:21 - one of my probability instructors
617:24 - researcher at the university of utah
617:26 - very good mathematician uh one way he
617:29 - put it
617:30 - was uh
617:33 - the you know at some level that
617:36 - someone's going to win the lottery
617:38 - you just know it's not going to be you
617:41 - so you get the probably that you win the
617:42 - lottery is zero
617:44 - and the probably that anyone wins the
617:46 - lottery is not zero
617:48 - that's the way he put it so uh
617:51 - let's uh move on to discussing
617:53 - probability density functions
617:56 - so these are the analog to the
617:58 - probability mass function for discrete
617:59 - random variables
618:00 - the pdf is a non-net negative function
618:03 - which i'm calling f
618:04 - x such that for any two numbers a and b
618:08 - with a less than or equal to b the
618:09 - probability
618:11 - that a is less than or equal to x
618:14 - which is our random variable which is
618:16 - less than or equal to b
618:19 - is equal to the integral from
618:23 - a to b f of x
618:27 - dx
618:30 - naturally in order for f to be a valid
618:33 - pdf we must also have that the integral
618:36 - from no not a uh the integral from
618:40 - negative infinity
618:41 - to infinity of f of x
618:45 - dx well that's the probability
618:49 - that this random variable
618:52 - is between negative infinity and
618:54 - infinity
618:58 - and that's basically asking for what is
619:00 - the probability that this random
619:01 - variable
619:03 - is any real number we're basically
619:06 - asking what is the probability that x
619:08 - is a real number
619:12 - so naturally this must equal 1 because
619:15 - we know that
619:16 - x will be a random a real number
619:19 - and it will be finite
619:23 - so this is another relation we have to
619:24 - have now regarding this
619:26 - f itself might not be
619:31 - continuous everywhere and it's also
619:34 - possible that
619:35 - actually integrating from negative
619:37 - infinity to infinity is
619:40 - a bit much because actually this random
619:43 - variable is only
619:44 - positive on some on some
619:48 - interval of finite length and everywhere
619:51 - else it's zero
619:52 - so you're integrating for the most part
619:54 - zero and in fact we'll see
619:56 - one example of this
620:04 - one we are now discussing the
620:07 - ever famous maybe two famous sometimes
620:09 - infamous
620:10 - infamous not infamous that's not a word
620:13 - sometimes
620:14 - infamous normal distribution we
620:17 - say that a random variable x follows the
620:20 - normal distribution sometimes denoted
620:22 - like so
620:23 - x uh x follows a distribution n
620:27 - mu sigma so we say x follows a normal
620:30 - distribution with mean mu and standard
620:32 - deviation sigma
620:33 - if it has the pdf
620:37 - v of x parameterized by uh
620:42 - mu and sigma
620:45 - is equal to all right we should probably
620:48 - zoom in for this
620:50 - um it's a fairly
620:54 - a little complicated we have 1
620:57 - over the square root of 2
621:01 - pi sigma squared
621:06 - so this fraction multiplying with e
621:10 - to the power negative x
621:13 - minus mu squared divided by
621:17 - all in the power uh two
621:20 - sigma squared it's worth mentioning
621:24 - that often the normal distribution is
621:27 - parameterized not by its standard
621:29 - deviation but rather by
621:31 - uh its variance and you can kind of see
621:33 - why
621:34 - when you look when i've written this
621:36 - formula down yeah it's possible to take
621:38 - this sigma squared
621:40 - and pull it out in front of the square
621:42 - root
621:43 - uh but we can put it inside of the
621:46 - square root and then
621:47 - you have uh and then everything you've
621:51 - got sigma squares everywhere so you
621:52 - could just specify sigma squared
621:54 - directly
621:54 - and also it feels to statisticians to be
621:58 - or and probabilists my apologies to be
622:01 - more appropriate to parameterize by the
622:04 - variance rather than the standard
622:05 - deviation
622:06 - since it's often easier to work with the
622:07 - variance uh
622:09 - directly rather than the standard
622:10 - deviation
622:12 - and in addition to this
622:15 - you could say that parameterizing with
622:17 - the variance generalizes better when you
622:19 - start talking about
622:20 - multivariate versions of the normal
622:22 - distribution but this is fine for now
622:24 - like admittedly at at an
622:28 - introductory stats level it feels
622:31 - somewhat
622:31 - like since the standard deviation is the
622:33 - more uh
622:35 - natural measure of spread and the
622:36 - variance a little bit more alien
622:39 - you could argue that to these students
622:42 - it seems
622:44 - somewhat better to use the standard
622:45 - deviation rather than the variance but
622:47 - it's fine
622:48 - so this is the curve here is a sketch of
622:51 - the density curve
622:52 - for the normal distribution
622:56 - we have uh oops
622:59 - okay so here's kind of what it looks
623:02 - like
623:03 - uh we would have basically here is the
623:06 - mean
623:08 - here is the mean plus one standard
623:10 - deviation
623:12 - and here's the mean minus one standard
623:15 - deviation
623:17 - okay so we would have
623:20 - basically a curve that goes up there
623:25 - within one standard deviation is an
623:27 - inflection point
623:29 - so it will go from convex to concave
623:33 - at the inflection point and then
623:39 - it's going to be a symmetric curve
623:42 - and then go from concave back to convex
623:50 - all right and this is a
623:54 - simple sketch of what it may look like
623:57 - its peak occurs
624:02 - around the mean
624:06 - and we have our inflection points being
624:09 - within
624:10 - one standard deviation of the mean
624:15 - okay so that's a this is the
624:19 - whenever you hear the words the bell
624:20 - curve they are
624:22 - probably referring to the normal
624:24 - distribution
624:25 - be aware that the normal distribution is
624:28 - not
624:28 - the only bell-shaped curve that is used
624:32 - in probability and statistics
624:34 - there are other bell-shaped curves for
624:37 - it for instance there's the t
624:38 - distribution there's the koshi
624:40 - distribution
624:42 - i think i encountered a distribution
624:44 - recently
624:46 - that is meant to model uh
624:49 - when well okay it's not really a
624:51 - probability distribution though but
624:53 - it's kind of like one uh when someone
624:56 - gets the coronavir
624:57 - or no not cordovice when someone gets a
624:59 - virus
625:01 - during a pandemic type situation there's
625:04 - a curve for that that looks like the
625:05 - normal but isn't the normal
625:07 - i can't remember what it is though it's
625:08 - like the logistic curve i
625:10 - i don't know but yeah um
625:15 - uh this is usually when people are
625:17 - talking about the bell curve
625:19 - all in caps like capitalize and all that
625:22 - they're talking about the normal
625:23 - distribution
625:24 - here's an r plot of the density function
625:27 - of a standard normal curve
625:29 - and it's got that bell shape so
625:34 - the expected value of a random variable
625:38 - x following this distribution the
625:40 - variance and the standard deviation will
625:42 - be good given next
625:43 - uh these should not be shocking at all
625:46 - the expected value of x is equal to mu
625:50 - the variance of x is equal to
625:54 - sigma squared and the standard deviation
625:58 - of x is equal to sigma
626:01 - so normal random variables
626:04 - are specified by their mean and their
626:07 - variance
626:10 - okay you set the mean and the variance
626:12 - directly with normal random variables
626:15 - okay one property of the normal
626:17 - distribution is the 68 95 99.7 rule
626:21 - which i ain't going to sketch out for
626:23 - you this is basically a rule of thumb
626:25 - for
626:26 - how much of the distribution is within
626:29 - uh let's say uh within one standard
626:31 - deviation of the mean
626:33 - so we got mu plus sigma mu minus sigma
626:38 - within two standard deviations of the
626:40 - mean
626:42 - so mu plus two sigma mu minus two sigma
626:47 - and within three standard deviations of
626:50 - the mean
626:51 - so mu plus three sigma
626:55 - and uh mu minus
626:58 - three sigma okay
627:02 - so the peak of the curve happens at mu
627:06 - and we'll and let's see uh just kind of
627:09 - getting
627:10 - a sketch so uh the inflection points
627:14 - are going to happen with within uh
627:18 - one standard deviation
627:22 - all right so we could sketch out the
627:25 - curve
627:26 - to look something
627:30 - like this
627:40 - okay so
627:44 - uh what this rule says is
627:49 - let's uh start out within uh one
627:52 - standard deviation the area
627:54 - underneath the curve within
627:58 - one standard deviation
627:59 - [Music]
628:04 - is going to be 0.68
628:10 - so within one standard deviation
628:14 - so mu plus or minus sigma all right
628:17 - uh let's go to within two standard
628:20 - deviations
628:22 - okay so within
628:26 - two standard deviations so the area
628:29 - underneath the curve
628:32 - within two standard deviations
628:37 - so the area underneath the curve within
628:39 - two standard deviations
628:40 - is going to be 0.95
628:44 - so this will be mu
628:47 - plus or minus 2 sigma
628:51 - and then if we go to
628:55 - three standard deviations
628:59 - so let's uh have this going
629:02 - now let's have this going up and down
629:05 - horizontally
629:06 - is rough to draw
629:18 - okay
629:20 - on the other hand that's kind of a
629:21 - conflicting picture but you get the idea
629:24 - um within three standard deviations
629:28 - the area underneath the curve within
629:29 - three standard deviations is going to be
629:32 - 0.997
629:35 - so this will be mu
629:38 - plus or minus three sigma
629:43 - okay the unfortunate thing about
629:46 - that is i just drew over uh the text but
629:49 - i can still read
629:50 - so uh let z follow a
629:54 - normal distribution with mean zero and
629:56 - standard deviation one
629:58 - we then say that the random variable z
630:00 - follows the standard
630:02 - normal distribution and this
630:05 - distribution is useful
630:07 - since we can relate an arbitrary normal
630:09 - random variable to the standard normal
630:11 - distribution and vice versa
630:12 - and we can do so like so
630:16 - uh as a so as a reminder z is following
630:18 - a standard normal
630:22 - uh mu is an arbitrary normal random
630:24 - variable
630:25 - no no no not mu sorry mu is not a random
630:27 - variable
630:30 - so x is an arbitrary
630:34 - normal random variable so any mean and
630:36 - sigma
630:38 - in this case what you get
630:42 - is that if you take
630:46 - a normal random variable subtract out
630:48 - its mean
630:50 - and then divide by its standard
630:52 - deviation
630:53 - the distribution of the resulting
630:55 - variable will be equal to the
630:56 - distribution
630:58 - of the random variable z
631:01 - that is it's following a standard normal
631:03 - distribution
631:05 - which should make sense what this
631:08 - operation does
631:09 - is shift the mean to zero
631:22 - and then what you do is you
631:26 - uh scale by sigma or scale but let's say
631:29 - scale by one over sigma
631:34 - so you both uh take your curve
631:37 - along the number line oops
631:40 - i didn't want to erase stuff all right
631:47 - so what you're doing is you're taking
631:50 - your standard normal curve
631:54 - which is located at mu you are shifting
631:57 - it to the left
631:59 - uh by by mu and then
632:03 - you are compressing the curve
632:06 - so let's let's draw like a new curve
632:08 - that's oops
632:10 - let's draw a new curve that's centered
632:12 - at zero
632:14 - and then you compress your curve
632:18 - so it has a standard deviation of one
632:23 - all right that's what that operation is
632:25 - doing
632:26 - and similarly we could say
632:31 - that if we take a our standard normal
632:33 - random variable z
632:35 - scale it by sigma and then add mu
632:40 - the resulting random variable will be
632:41 - equal in distribution
632:47 - to the random variable x which is
632:48 - following a normal distribution with
632:50 - mean mu and standard dev
632:51 - standard deviation sigma all right what
632:54 - this is doing it's
632:55 - basically doing the opposite we
632:58 - are scaling by sigma
633:06 - and then we are and then we are shifting
633:09 - the mean to mu
633:18 - so to sketch out what this is doing
633:21 - we start out with our standard normal
633:23 - random variable which is centered at
633:25 - zero
633:26 - we then take that random variable
633:30 - shift its mean by mu
633:34 - so we end up with something over here
633:38 - and then we scale out its uh standard
633:42 - deviation
633:43 - so we get a curve with possibly a
633:46 - different
633:46 - standard deviation it doesn't have to
633:48 - get bigger it could get smaller too but
633:51 - you get the idea of scaling okay
633:55 - so uh let
633:58 - capital phi of little z be the
634:01 - probability that a standard normal
634:02 - random variable is less than or equal to
634:04 - z
634:04 - this is the cdf of the standard normal
634:06 - distribution
634:08 - then if x is the probability of an
634:09 - arbitrary normal distribution
634:11 - or normally just of an arbitrary
634:13 - normally distributed random variable
634:15 - we have excuse me
634:18 - we have the falling relationship between
634:20 - f and phi
634:22 - f of x is equal to
634:27 - phi of x minus mu
634:30 - over sigma where phi
634:33 - is the cdf of a standard normal
634:35 - distribution
634:37 - which means since we can relate
634:40 - any normal distributions cdf
634:43 - to the cdf of the standard normal random
634:46 - variable
634:47 - this means that we only need to worry
634:50 - about tabulating values
634:51 - for phi of z for the standard normal
634:55 - distribution
634:56 - in order to work with any normal
634:57 - distribution which is what's done
635:00 - in table 8.3 of dvor's book and often
635:03 - many of these statistics and probability
635:05 - books will have
635:07 - tables of the cdf of the standard
635:11 - random variable and i even remember
635:13 - buying
635:14 - a study card for math 3070 for my set my
635:18 - stats class
635:19 - and that table came with a
635:23 - no and that card came with a very small
635:27 - table for c for working with the cdf
635:31 - of a standard normal random variable and
635:33 - the reason why you can do
635:34 - why they're doing that is because you
635:36 - can get
635:38 - all of the information you need for any
635:39 - normal random variable from
635:41 - that table which is very nice because
635:44 - you can have just
635:45 - you can have any real number mean and
635:47 - any positive real number standard
635:49 - deviation
635:50 - and have an infinite number of normal
635:53 - random variables
635:54 - but you only need to print one table
635:56 - because once you have the table for the
635:58 - standard normal
636:00 - then everything works out great
636:03 - and additionally let's consider
636:06 - for a second the pdf of a standard or
636:10 - of a normal random variable uh let's
636:13 - let's consider actually first
636:15 - the pdf of a standard normal uh fee
636:19 - which will we will call just fee of x
636:21 - that's going to be
636:22 - uh 1 over the square root of 2 pi
636:26 - um actually let's let's call this v of z
636:32 - it's going to be 1 over the square root
636:33 - of 2 pi e negative
636:36 - z squared over 2.
636:40 - and this is kind of a mess so let's
636:42 - clean that up
636:47 - okay you
636:50 - you know by now that cdfs are computed
636:54 - via integration because all
636:58 - probabilities are computed v integration
636:59 - when you're working with
637:00 - normal random variables what then
637:04 - is the antiderivative
637:09 - of this
637:14 - the answer is basically what you see
637:17 - the thing is uh you there is no
637:20 - closed form or elementary solution
637:25 - for the anti-derivative of a normal
637:27 - random variable
637:29 - it doesn't exist it simply doesn't exist
637:32 - it's and i think in fact it's provable
637:35 - that you can't come up
637:37 - with an antiderivative
637:41 - for a normal random variable
637:44 - for a number of random variables pdf um
637:47 - in a in a in in any element in any
637:51 - elementary form
637:53 - so at the end of the day you're just
637:54 - kind of left with saying all right this
637:56 - is ranging from
637:57 - from uh negative infinity to
638:01 - uh i don't know x negative infinity
638:05 - to x you're just left with saying that
638:07 - this is equal to
638:08 - phi of x like that you're kind of left
638:12 - with this uh
638:13 - unsatisfying uh unsatisfying answer
638:17 - where you just say all right phi is this
638:19 - der is this integral
638:21 - and yet at the same time this is not a
638:25 - problem
638:26 - in fact nobody really cares nobody
638:28 - really needs
638:30 - to have an anti-derivative for
638:33 - what i've written down in black nobody
638:36 - really needs it because
638:38 - we have numerical routines that can
638:41 - compute these integrals
638:44 - mathematically we can still work with it
638:47 - there's nothing that says that we can't
638:48 - work with this we have the fundamental
638:50 - theorem of calculus that is able that
638:51 - tells us how to take derivatives of this
638:53 - thing
638:54 - so we can take derivatives we can still
638:55 - study its properties we can
638:57 - study how how quickly it grows and
638:59 - decays and
639:00 - and stuff like that there's really
639:04 - no reason we need a simpler expression
639:08 - for the antiderivative of a normal
639:11 - random variable
639:12 - so we just say this is true by d
639:16 - this is true by definition and go about
639:20 - our business
639:21 - because whenever we need to actually
639:23 - compute
639:25 - what the cdf is we have techniques for
639:27 - doing it
639:29 - we can use all these numerical routines
639:32 - uh numerical routines maybe some monte
639:35 - carlo simulation something like that
639:36 - there's all sorts of things that we can
639:38 - do
639:40 - to compute the cdf and you only need to
639:43 - do it once
639:44 - for the standard normal curve in fact i
639:47 - think
639:48 - r actually internally when when working
639:51 - with the cdf of a
639:53 - uh stan of um of normal random variables
639:57 - is work
639:57 - it's working with an internal table uh
640:01 - that computes probabilities
640:05 - so um admittedly
640:08 - i'm teaching this right now in an online
640:10 - format
640:12 - in a context where students are not
640:15 - going to ever enter a testing center
640:18 - so they don't need to use the table
640:21 - and if i were teaching this in a regular
640:23 - semester i would teach
640:24 - students to
640:28 - use the table and it just feels like
640:31 - right now that's silly because they have
640:33 - access to r
640:34 - and they're not going to want to use the
640:35 - table and i've all never really had a
640:37 - problem with
640:38 - teaching the table from a pedagogical
640:41 - perspective because i feel like using
640:42 - the table
640:44 - was good practice for
640:49 - working with the basic properties of the
640:52 - normal distribution
640:55 - and now i don't really see a reason to
640:59 - use it
641:00 - i mean there is still that pedagogical
641:02 - reason but it's just
641:04 - completely swamped
641:07 - by the convenience of having r around
641:12 - so we're going to lose that i'm actually
641:15 - rather sad that this time
641:17 - i'm not really going to use the table
641:19 - that i'm just going to
641:21 - compute probabilities using r but
641:23 - hopefully you can still
641:25 - get the message and understand some of
641:28 - the properties
641:30 - that would be learned by working with
641:32 - the table
641:33 - such as the symmetry of the normal
641:36 - distribution or
641:38 - working with one minus and stuff like
641:40 - that
641:41 - okay if i ever teach this class again in
641:45 - person
641:46 - uh maybe what a maybe i would and if i
641:50 - were to use these lecture videos again
641:51 - maybe
641:52 - i would create a separate video for uh
641:56 - working with the table
641:57 - but i don't think i will do that now all
641:59 - right
642:01 - so um anyway
642:05 - let's compute the following
642:08 - we're now working with a standard normal
642:10 - random variable so remember that this is
642:12 - a random variable
642:14 - uh with uh a mean
642:17 - i can do better than that
642:21 - hold on okay so this is a random
642:25 - variable
642:28 - good grief
642:31 - good grief come on just
642:36 - some people just don't want to press
642:39 - the undo button good grief
642:44 - ugh this screen how i hate it
642:48 - okay um
642:51 - okay so remember that we
642:55 - are working with the standard normal
642:56 - distribution so it's a distribution
642:58 - centered at zero standard deviation one
643:00 - i'll just tell you that standard
643:01 - deviation is one
643:02 - i want to compute the probability that z
643:04 - is less than equal to zero
643:06 - so this is what the normal distribution
643:09 - looks like this is the area that i want
643:10 - to compute
643:14 - what is that area well i know the area
643:16 - under the entire curve is one
643:18 - because it's a pdf and we know
643:22 - that we're shading the area to the left
643:25 - of zero and the zero is the point of
643:27 - symmetry
643:28 - so that means half the area so if you
643:30 - were to fold
643:31 - the curve over on itself around zero it
643:33 - would have equal area to the left and to
643:35 - the right of zero which means that this
643:36 - must be
643:37 - half of the area underneath the curve so
643:41 - that means that this is going to be
643:42 - equal to 0.5 because
643:45 - 0 is the median of a
643:48 - standard normal distribution all right
643:52 - uh next what is the probability that z
643:53 - is less than or equal to
643:56 - 1.23 okay
643:58 - so what where she so what we're
644:00 - computing here
644:02 - here's a standard normal curve here's
644:05 - 1.23 we are computing the area
644:09 - underneath the curve and to the left of
644:12 - 1.23
644:14 - okay and at this point
644:17 - i'm going to ask r what that area
644:20 - is so
644:28 - okay so i want
644:31 - a p norm by default p norm is working
644:35 - with a standard normal curve
644:37 - so we've got p norm of
644:46 - right uh 1.23 so that's going to be
644:50 - 0.8906 uh or we'll say 0.8907
644:53 - so this will be this is equal to
644:56 - 0.89
645:02 - okay uh let's see next up oh look at
645:06 - that some r code
645:07 - and uh it's basically confirming what we
645:09 - got via
645:10 - r so not shocking all right next up the
645:14 - probability that uh the center number
645:15 - and a variable is between negative 1.97
645:17 - and
645:18 - 2.1 so this will be
645:23 - we've got the standard normal curve
645:26 - okay here's zero
645:30 - here's uh 2.1 here's negative 1.97
645:37 - and we want the area in this region the
645:40 - area in between
645:42 - 2.1 and negative 1.97 so how are we
645:45 - going to do that
645:46 - well one thing we could do is say
645:49 - remembering
645:50 - that we are working with a cdf
645:54 - that this is going to be the area
645:59 - underneath the curve to the left of 2.1
646:05 - minus the area underneath the curve to
646:09 - the right of negative 1.97
646:14 - so you just subtract out the area from
646:16 - the left of negative 1.97
646:18 - from the air that's to the left of 2.1
646:21 - you can think of that as
646:22 - you have a piece of construction paper
646:24 - and this piece of construction
646:26 - paper can is the normal curve including
646:29 - the region
646:30 - underneath the normal curve that's a
646:31 - really long piece of construction paper
646:33 - since
646:34 - uh the normal curve extends from
646:36 - negative infinity to infinity
646:38 - right i never put any sort of bounds on
646:40 - this curve
646:42 - so i hope you notice that that this is a
646:44 - random variable that can take any number
646:46 - any real number between negative
646:48 - infinity and infinity
646:51 - um so we uh uh but you know we imagine
646:54 - that we have this uh maybe we clipped it
646:56 - off after a certain point
646:58 - and uh which is fine because after after
647:01 - a while the
647:02 - normal curve becomes minuscule so so
647:05 - minuscule
647:07 - because it it approaches zero very very
647:09 - quickly one way to interpret the 30
647:12 - 68 95 99.7 rule is saying that
647:16 - almost all of the curve is within three
647:18 - standard deviations and there's almost
647:19 - nothing outside of it
647:21 - so and that goes even more so for
647:24 - four standard deviations five standard
647:26 - deviations and so on there's almost
647:27 - nothing there
647:28 - um anyway we have we imagine that we
647:31 - have this piece of construction paper
647:34 - and we we clip off the area
647:37 - at 2.1 and have the area underneath the
647:40 - curve and to the left 2.1 and then we
647:42 - clip off
647:43 - the area underneath the curve to the
647:44 - right to the left of negative one point
647:46 - nine seven
647:46 - and what we end up with is the area that
647:48 - we want so we met or
647:50 - and if what we could end up doing is we
647:53 - start out by measuring the area
647:55 - underneath the curve uh the the area of
647:58 - our construction paper
647:59 - when we did our first clip when we
648:00 - clipped at 2.1
648:02 - and we measured that area and then we
648:05 - clip
648:06 - again at negative 1.97 and measure the
648:09 - area of the part that we clipped off
648:11 - and then subtract that from our earlier
648:13 - calculation
648:15 - to get the area that's remaining uh
648:19 - uh for our construction paper so this
648:22 - will be
648:23 - uh fee uh
648:26 - remember this remember that capital fee
648:28 - is a cdf of a standard normal random
648:30 - variable
648:31 - so fiat 2.1 minus
648:34 - fee at negative
648:40 - 1.97
648:43 - okay and then we need to compute this
648:48 - so p norm
648:51 - uh 2.1 minus p norm
648:57 - negative 1.97
649:00 - and this is what we get so we get 0.9577
649:04 - so we get oops so we get that this is
649:07 - equal to
649:14 - 0.9577
649:18 - okay uh next example the probability
649:21 - that z is greater than or equal to 1.8
649:24 - so this is the area underneath
649:27 - the normal curve that's
649:30 - to the right of 1.8
649:37 - okay and we say that this is going to be
649:43 - well we could say that this is uh the
649:44 - area underneath the entire curve
649:49 - so here i've shaded the whole thing
649:53 - minus the area underneath the curve
649:58 - uh to the left of 1.8
650:03 - going back to our construction paper
650:04 - analogy
650:06 - you have this piece of construction
650:07 - paper that has that's the the area
650:09 - underneath the entire curve
650:12 - uh and you clip off
650:16 - the uh the part at 1.8 and you're
650:20 - left with uh the part
650:23 - from negative infinity up to 1.8 so you
650:26 - lost the other part and you want to
650:27 - figure out the area of the other part
650:29 - well you knew the entire area was one so
650:30 - you measure the area of the part that's
650:32 - that you um the part to the left of
650:36 - uh to the left of 1.8 and subtract that
650:40 - from one
650:40 - to get uh to get the area that's
650:43 - underneath the curve and to the right of
650:45 - 1.8
650:46 - so this would be
650:51 - 1 minus the cdf
650:54 - of the standard normal at 1.8
650:58 - and we can go to r and compute this
651:03 - so 1 minus p norm
651:08 - at 1.8 and this is going to be 0.0359
651:14 - so this is 0.0359
651:20 - okay excellent uh the probability
651:23 - that it's greater than 5.2
651:26 - so this is going to be approximately
651:29 - zero
651:31 - but i'll go ahead and
651:35 - compute this in r and say
651:38 - one minus p norm
651:42 - uh what was the number we're plugging in
651:44 - 5.2 okay 5.2
651:47 - all right very very very small number
651:50 - uh not quite numerically zero because we
651:54 - can go to 16 decimal places
651:56 - but very very close
651:59 - um very small number
652:03 - and actually if you were using the table
652:06 - and if you look at most tables most
652:07 - tables don't go beyond
652:09 - four standard deviations they might even
652:11 - go beyond three
652:12 - or three and a half but you'll almost
652:15 - never see a
652:16 - table go beyond four standard deviations
652:18 - and then here we're asking for
652:20 - the area underneath the curve to the
652:22 - left of five standard deviations
652:25 - so uh if we were actually into the table
652:29 - which is
652:29 - uh the context in which these notes were
652:31 - written uh we would say
652:34 - what i would basically tell students is
652:35 - say this is approximately zero
652:37 - so don't even bother to look at the
652:40 - table just
652:40 - say this is approximately zero
652:44 - okay here's some arco that's doing all
652:47 - these calculations
652:48 - uh we also we also could have done some
652:50 - of that one minus stuff using the lower
652:52 - tail
652:52 - uh parameter let's see is that different
652:55 - from what we had
652:57 - no it's not different so that looks to
653:00 - be about the same
653:01 - um all right uh next example
653:05 - so iq scores are said to be normally
653:06 - distributed with mean 100 and standard
653:08 - deviation 15 like q be randomly selected
653:10 - be a randomly selected individual's iq
653:12 - score uh compute the probability that q
653:15 - is between 85 and 115.
653:18 - so in this case
653:25 - this was here's another thing um this is
653:28 - there's a lot of reasons why i really
653:30 - liked working with the table and one of
653:31 - the things that was great about working
653:32 - with the table was that it forced you to
653:35 - translate
653:37 - from a normal distribution to a standard
653:38 - normal distribution
653:40 - or any normal distribution to standard
653:42 - normal
653:44 - and thus i felt like it would force
653:47 - students to learn
653:48 - the relationship between any normal
653:50 - random variable and a standard normal
653:52 - random variable
653:53 - so what we could say here is that this
653:56 - is going to be
653:58 - the probability that 85
654:02 - minus the so q
654:06 - is going to be according to this problem
654:09 - a normal random variable with being 100
654:11 - and standard deviation 15. so this will
654:15 - be
654:15 - 85 minus 100 over
654:19 - 15 that's less than or equal to q
654:23 - minus 100 over 15
654:29 - which is less than or equal to 115
654:33 - minus 100 over 15.
654:38 - and this part right here is equal in
654:41 - distribution
654:43 - to a standard normal random variable z
654:51 - so we could say that this is equal to
654:54 - after you compute that lower and upper
654:56 - bound you'll find that this is
654:58 - the probability that negative one is
655:00 - less than or equal to z
655:03 - which is less than or equal to one which
655:06 - is going to be about 0.68
655:10 - because of the 68 95 99.7 roll
655:16 - so you really wouldn't even have to go
655:17 - the oh okay
655:19 - so 0.68 is very much an approximation
655:22 - it's not exactly true
655:24 - uh but it's kind of close it's i think
655:26 - it's true if you round to two decimal
655:28 - places
655:29 - so um well i'm actually not really sure
655:34 - uh but yeah you do have this um and this
655:37 - is basically forcing us to
655:40 - convert to the standard normal case
655:44 - um the unfortunate thing though is that
655:46 - now i could just do this you could do
655:48 - p norm um
655:51 - uh 115 uh
655:54 - mean equals 100
655:58 - and the other parameter is sd is equal
656:01 - to
656:02 - 15 minus p norm
656:08 - uh 85
656:12 - mean equals 100
656:16 - sd equals 15.
656:20 - and we get 0.6826 or
656:23 - 2 7. uh if we wanted to
656:26 - uh we could instead have written p norm
656:31 - 1 minus p norm
656:35 - negative 1 and they get the same number
656:39 - uh using basically that alternate form
656:42 - because you might not be converting you
656:45 - might not be
656:46 - converting to a standard random variable
656:47 - but r i'm pretty sure is
656:50 - so all right there's competing that
656:54 - uh this so the sad thing is that you can
656:56 - just do that and now i can't force you
656:59 - to uh use a table
657:02 - and uh and convert
657:06 - uh and uh convert to a standard normal
657:08 - random variable that's unfortunate
657:11 - okay uh so the probability that q is
657:13 - greater than 90.
657:15 - you know what i can't force to do it but
657:17 - i can still do it because i still have a
657:19 - point to make
657:20 - right so this is going to equal
657:23 - the probability that
657:26 - q minus 100
657:29 - over 15 is greater than
657:33 - 90 minus 100
657:37 - over 15
657:40 - which is equal to uh
657:45 - 1 minus the cdf of a standard normal
657:47 - curve a standard normal random variable
657:50 - at uh at a 90 minus 100 over 15
657:53 - this part becomes uh 10 over 15 or
657:57 - negative 10 over 15
658:01 - which is negative two-thirds
658:04 - so i would say uh this is going to be
658:08 - fee at i'll even round it i'll se
658:11 - i'll even convert it to a decimal number
658:13 - so negative
658:14 - zero 0.67
658:18 - which is approximate but this is
658:19 - supposed to be
658:21 - negative two-thirds
658:24 - and then i go to r and compute this
658:29 - and say this is one minus p norm
658:34 - uh negative two thirds
658:40 - which is point seven four seven five
658:44 - so this is approximately equal to 0.7475
658:51 - all right next up the international
658:54 - society for philosophical inquiry
658:56 - requires potential members to have an iq
658:58 - of at least 135
659:00 - in order to join the society this is one
659:01 - of those so-called genius societies
659:04 - based on this what proportion of the
659:07 - population
659:08 - is eligible for membership
659:11 - so this is the probability
659:14 - that an individual's iq is at least
659:19 - 135 which is
659:22 - equal to uh the probability
659:28 - that q minus 100
659:32 - over 15 is greater than or equal to
659:35 - 135 uh hold on
659:40 - uh let's move this
659:43 - so this is equal to
659:44 - [Music]
659:46 - probability that q minus 100
659:51 - over 15. is greater than or equal to
659:55 - 135 minus 100
659:59 - over 15 which is equal to
660:06 - uh the probability that a well okay this
660:09 - is going to be
660:11 - uh one minus
660:15 - the cdf of
660:19 - a normal variable or one minus fee at so
660:22 - 135
660:23 - minus 100 over 15 is 35 over 15
660:26 - uh which is seven thirds
660:32 - which is about 2.33
660:40 - okay so about 2.33 so
660:44 - 1 minus p norm
660:47 - 2.33 actually we'll just do seven
660:50 - divided by
660:51 - three so point zero zero nine eight
660:57 - so this is approximately point zero zero
661:00 - nine
661:00 - eight okay
661:05 - and here is some r code where i'm
661:08 - actually
661:09 - calling these mean functions sd function
661:13 - so mean sd parameters also using lower
661:15 - tail
661:17 - rather than doing one minus the cdf and
661:20 - so on
661:21 - all right uh so that was all
661:25 - trying to compute probabilities but
661:27 - sometimes we want to compute quantiles
661:29 - so we've got the notation z alpha which
661:32 - means that
661:33 - the cdf of at z alpha
661:36 - is equal to one minus alpha we can
661:39 - relate this
661:39 - back to general uh percentiles to find
661:42 - for arbitrary normal
661:44 - normally distributed random variables
661:46 - because these are the
661:47 - percentiles of um
661:52 - standard normal random variables okay
661:55 - so this is going to so we have in
661:58 - general
661:59 - a to p is going to be
662:04 - uh sigma z
662:07 - 1 minus p
662:11 - plus mu that's our percentile formula
662:16 - and z1 minus alpha can be found using
662:19 - table
662:19 - 8.3 using a reverse lookup but since you
662:22 - now have r
662:23 - there's really no point about discussing
662:25 - reverse lookups like this
662:28 - okay
662:34 - i mean i felt like they were good
662:35 - practice but
662:37 - we're now not going to be doing that
662:38 - anymore all right so uh
662:40 - what is z 0.5 that's the median of a
662:44 - standard normal
662:45 - z 0.5 is the area where the curve is
662:49 - split
662:49 - in two equal parts and that's going to
662:52 - be zero
662:55 - uh what is z 0.05
662:58 - well uh what we could do is say all
663:01 - right let's go to p
663:02 - norm no no no no we're not using p norm
663:04 - anymore we're using cute arm
663:07 - so q norm uh 0.05 so
663:11 - uh when you could put in 0.99
663:14 - no put put in 0.95 instead that would
663:17 - give us the same thing
663:19 - this is going to be 1 minus so this is 1
663:22 - minus .05
663:25 - or alternatively we could do q norm
663:29 - .05 lower dot tail
663:32 - equals false and that also works
663:36 - those all get us the same number so
663:39 - in the end though this is about 1.64 so
663:43 - this
663:43 - is so
663:46 - z 0.05
663:54 - is about 1.64
663:59 - okay what are the first and third
664:01 - quartiles of the standard normal
664:03 - distribution
664:04 - so what we're looking for is uh z
664:08 - so the first quartile will be z point
664:11 - seven
664:12 - five because remember we're doing up so
664:15 - the uh so here the uh
664:18 - subscript of the z is the upper tail
664:22 - area
664:23 - so this is the first quartile
664:27 - and the third quartile is 0.25
664:32 - this is q3
664:35 - all right so what are those going to be
664:38 - when we go to
664:39 - r and say q norm
664:46 - 0.75
664:48 - dot tail
664:51 - equals false so we get negative zero
664:55 - point six
664:55 - seven so negative
664:58 - zero point six
665:02 - seven and this one well actually i'm not
665:04 - even going to bother computer because i
665:06 - know what it is it's 0.67
665:08 - because we're working with a symmetric
665:11 - curve
665:12 - since we're working with a symmetric
665:14 - curve if we know one of those things
665:15 - that we know the other one
665:17 - because if the area underneath the curve
665:19 - to the left
665:20 - of z 7.75 is .25 the area underneath the
665:25 - curve to the right of point
665:27 - z 0.25 is also 0.25 so all we ever did
665:31 - was just flip over the flip over the y
665:35 - axis where x is equal to zero oh
665:38 - no let's not put a y there that's just
665:40 - confusing
665:42 - but my stupid undo button isn't working
665:45 - [Music]
665:46 - gosh why does stuff have to be so
665:52 - moody anyway um
665:56 - well since we're working with a
665:59 - symmetric distribution we actually have
666:01 - a property that
666:02 - i think gets written down uh later
666:06 - uh where did i write it down did i was i
666:08 - supposed to write it down
666:09 - up here where did i write it
666:14 - i know i wrote it down i know that i
666:17 - plan on talking about it at some point
666:20 - in these lecture notes
666:23 - oh i brought about on page 21 but
666:25 - basically i'll i'll i'll just cut to the
666:27 - chase and write it right now
666:28 - uh so z
666:32 - alpha is equal to negative z
666:35 - one minus alpha so basically
666:39 - uh you can just flip over the flip over
666:42 - the y
666:42 - uh the uh y axis so change the sign
666:46 - and work with one minus that area and
666:48 - you can get the same quantiles
666:50 - so but if it makes you feel better i'll
666:53 - go ahead and compute it
666:56 - and this is 0.25 yeah so what i did
667:00 - basically was exploit
667:01 - the symmetry of the standard normal
667:04 - distribution it's symmetry around zero
667:08 - our description of the random variable q
667:09 - from example 11.
667:11 - uh so using that answer the following
667:13 - questions mensa international requires
667:15 - individuals have an iq score
667:17 - that would place them in the top two
667:18 - percent of the population was the
667:20 - minimum iq score needed to be a member
667:22 - of mensa well that would be um
667:25 - [Music]
667:27 - so the standard deviation is 15.
667:30 - we've got z so top two percent so that's
667:33 - going to be 0.02
667:34 - the upper tail area is 0.02 plus 100
667:38 - and this is going to be let's see
667:43 - we've got 15
667:46 - times q norm 0.02
667:50 - lower dot tail equals
667:54 - false plus 100
667:58 - 130 or i guess he'd ran into 131
668:01 - uh so we'll say 131. uh
668:04 - after rounding so 131 you need to have
668:07 - an iq of 131
668:09 - in order to be a member of mensa uh
668:12 - there's an alternative way to do that
668:13 - though we could have instead
668:15 - we could have instead done q norm
668:19 - .02 mean equals 100
668:22 - sd equals 15 lower dot tail
668:26 - equals false that would have also worked
668:30 - okay uh the part of the population with
668:33 - the lowest five percent of iq scores is
668:35 - considered to be intellectually disabled
668:37 - what is the highest iq score needed to
668:39 - be in this group
668:42 - okay so uh that means that
668:46 - we are looking at uh z
668:50 - 0.95 or actually we're asking for eta
668:54 - of 0.05
668:58 - up here in this problem we were looking
669:00 - for
669:02 - eta of 0.98
669:06 - okay so
669:09 - this is going to be 15z 0.95 plus 100
669:15 - and then we go and compute that
669:18 - so in this case we got
669:21 - 0.95 now this is going to be 75.32 so
669:26 - we'll say
669:28 - about after rounding 75.
669:32 - so if you have a so if you have an iq
669:34 - score of 75 or lower you're considered
669:36 - intellectually disabled
669:40 - okay so
669:44 - right there's some r code that's doing
669:46 - the same thing so do the symmetry at the
669:48 - normal distribution we have the
669:49 - following useful identities for fee
669:52 - uh one second
669:58 - okay we have that the cdf
670:04 - at z is equal to
670:08 - one minus the cdf
670:12 - at negative z
670:16 - which what this is saying is
670:20 - uh if you were looking at
670:24 - so this is a standard normal
670:25 - distribution
670:27 - if you were looking at the area
670:30 - underneath the curve
670:31 - and to the left of z
670:36 - another way you could compute that
670:37 - quantity
670:39 - is look at the area underneath the curve
670:42 - and to the right of negative z
670:44 - which is what hap which is what you get
670:45 - when you flip over
670:47 - the uh y axis and then subtract that
670:50 - from 1
670:51 - to get the to get the red area
670:54 - underneath the curve so the so the area
670:56 - above negative z um
670:59 - is going to be equal to the area below z
671:02 - or the area to the right of negative z
671:04 - is equal to the area to the left of z
671:07 - because
671:07 - of the symmetry of the curve and
671:10 - equivalently
671:12 - well as a consequence of this we have z
671:16 - alpha is equal to negative z
671:19 - 1 minus alpha that's our immediate
671:22 - consequence
671:24 - all right so as mentioned before fee can
671:26 - be used to approximate the cdf of other
671:28 - random variables
671:30 - so it turns out that the normal
671:31 - distribution can
671:33 - be used to approximate the uh
671:36 - cdf of other random variables or
671:40 - approximately other random variables
671:41 - basically
671:44 - which of course mattered more
671:45 - historically
671:47 - when we didn't have uh
671:50 - when we had to basically physically
671:52 - print out tables for random variables
671:55 - but you still want to be able to get
671:56 - probabilities for
671:58 - binomials with large parameters large n
672:01 - or
672:02 - uh poisson random variables with large
672:05 - mu but in this situation
672:09 - um like in the in the world in which we
672:11 - currently live
672:12 - that's less of an issue because software
672:14 - doesn't ask you how big
672:16 - n is it just works so
672:19 - um all right so well i guess it does
672:21 - literally ask you
672:23 - but uh it's not like you put in the
672:25 - wrong number and it will just not work
672:28 - um unless of course of course you put in
672:30 - something that's
672:31 - really big to the point that software
672:32 - can handle it but
672:34 - that's highly unlikely uh anyway
672:38 - uh still the fact that certain random
672:41 - variables can be approximated by normal
672:42 - random variables
672:43 - is not only important
672:46 - it's getting to fundamental theorems of
672:50 - statistics and probability
672:51 - uh that will be discussed in chapter
672:53 - five
672:54 - so let's say for example let's work it
672:58 - let's work with the binomial random
672:59 - variable
673:00 - when n the the sample size is large
673:07 - the cdf of a binomial random variable
673:12 - at x with parameters n and p
673:15 - and here we're assuming that n is
673:17 - somewhat large
673:18 - uh you should probably say well okay so
673:21 - a rule of thumb is that n times p
673:22 - is greater than or equal to 10 and n
673:24 - times 1 minus p is greater than equal to
673:26 - 10
673:26 - that's one rule of thumb that they're
673:28 - using um
673:30 - we could probably say that if your p is
673:31 - between uh 0.1 and 0.9
673:34 - then and a sample size of 40
673:37 - is probably fine so this is going to be
673:40 - approximately equal to
673:43 - the cdf of a standard normal random
673:46 - variable evaluated at
673:49 - x minus the mean of the random variable
673:52 - which is np because that's the mean of a
673:54 - binomial
673:55 - divided by the standard deviation of a
673:57 - binomial which is n times p
673:59 - times 1 minus p and then
674:02 - in addition to this we do plus 0.5
674:07 - the plus point five is what's known as a
674:09 - continuity correction
674:11 - uh it accounts for the fact that we are
674:14 - using a continuous random variable
674:16 - to uh approximate a discrete random
674:20 - variable
674:22 - if you didn't do this then you often end
674:24 - up with some numerical inaccuracy with
674:26 - the
674:27 - approximation the approximation is still
674:29 - at some level true
674:31 - but it's just off and you get better
674:35 - you get better approximate computations
674:38 - for
674:38 - say the cdf when you include
674:42 - this uh continuity correction i think
674:45 - the justification
674:47 - for why you add plus 0.5 is you could
674:51 - imagine that
674:52 - you have this uh probably mass histogram
674:56 - and your
674:59 - and your uh and the uh
675:03 - or cdf of the normal curve would be
675:06 - going through it
675:08 - basically at the left endpoints so you'd
675:10 - have
675:11 - something that's looking like
675:14 - this and if you shift everything over
675:19 - to the right yeah i think it's to the
675:22 - right when you add do plus or no it's
675:24 - to the left but when you shift the curve
675:27 - oops
675:30 - the undo button is not working when you
675:32 - shift the curve
675:33 - over a little bit you get
675:37 - like a better pass through
675:40 - of these uh histograms so it's like
675:43 - re-centering so that
675:44 - it's centered evenly um
675:48 - on the uh on the uh probability
675:51 - histogram
675:52 - or this uh probably mass function
675:54 - understood as a histogram
675:55 - anyway uh let's let's work let's do an
675:58 - example
675:59 - a manufacturer will reject a batch of
676:02 - widgets if in a sample of 100 randomly
676:04 - selected widgets of the batch
676:06 - uh 15 or more are defective if 12 of the
676:08 - widgets in the batch are defective
676:10 - was the probability of rejecting the
676:11 - batch so the random variable in question
676:14 - is uh we'll call it s
676:18 - and it's following a binomial
676:20 - distribution
676:22 - uh the the parameter n is 100
676:26 - and the parameter p for the probability
676:29 - of getting a defective widget is 0.12
676:33 - okay so the approximating normal random
676:36 - variable
676:37 - follows a normal distribution with what
676:40 - is going to be in the mean well it's
676:41 - going to be the sample size times p
676:43 - so that's 12. and then we've got
676:47 - a standard deviation which is going to
676:51 - be
676:53 - the square root of a hundred
676:56 - times 0.12 times 0.88
677:01 - okay so the standard deviation is about
677:04 - 3.25
677:06 - so 3.25 this is the distribution of the
677:10 - approximating normal random variable so
677:13 - s's distribution is approximate so s is
677:17 - approximately equal in distribution to x
677:22 - so then when we want so okay reject the
677:24 - batch
677:25 - when is the batch rejected the batch is
677:27 - rejected when
677:29 - uh 15 or more widgets are defective so
677:33 - we're looking at the probability that s
677:36 - is greater than or equal to 15
677:41 - and this is going to be
677:44 - uh 1 minus the cdf
677:47 - of this random variable at um uh
677:53 - hold on uh yeah at 14
677:57 - uh and it's got parameters 100 and 0.2
678:01 - and point 12.
678:06 - okay and
678:09 - this is according to our normal
678:11 - approximation approximately equal to
678:16 - uh one minus phi
678:20 - and we've got 14
678:24 - minus 12 divided by
678:28 - 3.25 and then we add in
678:32 - the continuity correction 0.5
678:37 - and what is this going to be equal to
678:40 - well
678:40 - we've got 1 minus p norm
678:46 - so we've got uh
678:50 - so 14 minus 12
678:53 - plus 0.5 divided by
678:59 - 3.25 so
679:03 - 0.2209
679:10 - and just for reference we could have
679:13 - alternatively computed
679:18 - p by nom and
679:22 - we would have used
679:26 - 14
679:29 - size equals 100 prob
679:32 - equals 0.12 and we would have said
679:36 - lower dot tail equals
679:41 - false yeah so that's
679:45 - pretty close to what we uh got using the
679:48 - normal approximation
679:51 - okay all right so
679:55 - uh scrolling down
679:58 - oh did i
680:02 - oh something's different uh
680:07 - on the other hand is the r code wrong
680:21 - okay it looks like i might have made a
680:22 - mistake here
680:24 - so the probably that s is greater than
680:25 - or equal to 15 is probably that
680:28 - is one minus probably that s is less
680:29 - than or is strictly less than 15
680:32 - which is one minus the probability that
680:35 - s is less than or equal to
680:37 - 14. so i think that my
680:40 - r code in these in this
680:43 - yeah i think that i did not
680:47 - put in
680:50 - yeah or
680:54 - [Music]
680:57 - hmm curious
681:02 - i don't know i i'm thinking actually
681:04 - that this might be wrong
681:14 - on a second look okay
681:18 - but you get the point at the very least
681:21 - and by the way i would suggest using the
681:24 - normal pro
681:24 - approximation at the very last step so
681:28 - right when you're about to compute
681:29 - something and that you don't know how to
681:30 - compute so
681:31 - like for example um i guess
681:34 - i should probably write down what steps
681:37 - i've kind of been emitting i can say
681:39 - this is the probability that s is no
681:42 - this is 1 minus the probability that s
681:44 - is less than 15
681:46 - which is 1 minus the probability that
681:50 - s is less than or equal to 14
681:54 - so here i um
681:57 - basically was still treating s as if it
681:59 - were a discrete random variable
682:02 - you should still do that uh you should
682:05 - like if i was
682:06 - treating this as a continuous random
682:08 - variable i would not be caring so much
682:10 - about whether i was working with less
682:11 - than or less than or equal to
682:13 - but you should you should still treat
682:16 - your random variable that you're
682:18 - approximating with a normal random
682:19 - variable
682:20 - as if it's discrete up until the final
682:22 - point
682:23 - when you need to compute something like
682:25 - the cdf
682:27 - okay so that's what i recommend all
682:30 - right
682:30 - uh the approximation works for poisson
682:33 - random variables as well
682:34 - when uh this lambda parameter is large
682:38 - or i think it was i don't know why i
682:40 - wrote lambda here
682:41 - i think that might be because the book's
682:42 - using lambda i'm not really sure why
682:44 - because of what i remember is that in
682:46 - chapter 3 i was using mu
682:47 - to write down poisson random variables
682:50 - okay but whatever
682:51 - um so
682:55 - in this case the approximating
682:57 - distribution for a poisson random
682:59 - variable
683:00 - uh let's suppose that
683:05 - um x follows a poisson distribution
683:11 - uh with mean parameter
683:15 - mu then we could approximate it with
683:18 - y which is following a normal
683:20 - distribution with mean mu
683:22 - and standard deviation square square
683:24 - root of mu
683:25 - because the standard deviation of a
683:26 - poisson random variable is square root
683:28 - is the square root of mu okay
683:31 - uh so suppose that x follows a poisson
683:35 - distribution with parameter 100
683:36 - let's estimate the probability that x is
683:38 - less than or equal to 110
683:40 - so uh the probability
683:43 - that x is less than or equal to 110
683:47 - is approximately equal to fee at
683:52 - 110 minus 100
683:56 - plus 0.05 that's the continuity
683:58 - correction
684:00 - divided by uh the square root
684:03 - of 100 which is 10. so
684:08 - this is going to be this is going to be
684:11 - the cdf
684:15 - at 10.5 divided by
684:19 - 10 which
684:22 - is
684:25 - 10.5 minus 10. i don't know divided by
684:28 - 10.
684:31 - oh yeah that's a 1.05 so that's going to
684:33 - be
684:36 - 0.8531
684:43 - okay uh let's compare that
684:46 - to uh
684:49 - to what we would have had uh if we used
684:52 - the poisson distribution directly
684:54 - so 110 and uh
684:58 - lambda is equal to 100.
685:01 - oh very close very very close so a good
685:04 - approximation
685:06 - all right okay that's
685:10 - that concludes this section this is a
685:12 - very important section
685:13 - very very important because the normal
685:14 - distribution is a distribution that is
685:16 - appearing all over the place
685:19 - so and not just in this chapter but
685:22 - in later chapters too uh chapter five
685:26 - chapter seven chapter eight you're going
685:27 - to be using the normal distribution all
685:29 - the time
685:30 - it's going to be assumed that random
685:31 - variables are normally distributed so
685:34 - you need to get comfortable
685:35 - with this distribution all right so work
685:38 - on problems for this
685:39 - make sure you understand it if you don't
685:41 - understand it fix that
685:43 - and yeah it's it's your responsibility
685:45 - to fix it part of the way you fix it is
685:47 - by asking me what you don't understand
685:49 - right so um like
685:53 - part of how you fix not understanding
685:55 - something is getting help when you need
685:56 - it
685:56 - right from from whoever could possibly
685:58 - help you but you need to fix what you
686:01 - don't understand
686:02 - please please learn the normal
686:04 - distribution inside and out
686:06 - and you will be rewarded for it all
686:08 - right so that's it for
686:10 - uh this uh this section and i will see
686:13 - you
686:13 - in the next section when we talk about
686:15 - exponential random variables which
686:17 - we've already talked about quite a bit
686:19 - and also the gamma distribution which is
686:22 - an interesting distribution
686:23 - often shows up in applications and also
686:26 - in a more
686:27 - theoretical setting okay uh right so see
686:30 - you then

Cleaned transcript:

well it looks like it's time to get started so let's start with section one of chapter one discussing uh descriptive statistics and some basic terminology of statistics so all right let's uh get to work then so uh so in this chapter in this section we're first going to start out uh with some basic terminology uh let's start let's start out with the data data is what statistics is interested in we're going to say that data is basically a collection of facts we have generally a population or some group of interest for which we want to make some statements so if we were to collect data for the entire population we would have conducted a census uh let's let's have some examples there's the classic example of the united states because right now we're actually in a census the 2020 census being conducted by the census bureau and this is a massive undertaking by the u.s government mandated by the constitution to count every single individual currently living in the united states and with that get to get a count that is supposed to be a complete count not an estimate but a count this becomes the facts of how many people are currently living in the united states on a smaller scale there is this class and i presumably have the grades for this class so if i were interested in the grades of this class i could conduct a census by looking at the grades of every single student and with that i would have the entire data set the entire population where i've defined the population to be this class now that said you may not necessarily have access to the entire population for example in the case of the united states the census is an extremely expensive and complex undertaking that can only be done every 10 years and in the case of this class well i can conduct a census you yourself as a student may not be able to conduct a census so if you were interested in how your other fellow students were doing in the class you would be forced to collect a sample a sample is a subset of the population for which you managed to collect data so in the case of the class if you were to talk to some of your friends which may not be the greatest idea anyway but if you were to talk to some of your friends about uh how and ask them how they were doing in the class then you would have conducted this that you would have uh selected a sample uh whereas um uh in the united states we're regularly uh subjected to surveys where a subset of the american population is going to be examined and you're going to ask questions about them and in the case of the census bureau they have a lesser intensity project such as the american community survey which isn't aimed at getting the entire population i think every year they try to get one percent of the population which is big in and of itself all right so uh in a sample we have observations so maybe these are people but in principle they could be just about anything like they could be petri dishes they could be the price of a stock on a particular day something like that and uh with those observations we have variables so in the case of our little person over here who is an observation in our sample we may have tracked their age we'll say this person is 22 we may track their gender uh we'll say that this person uh identifies as male or maybe we are tracking i don't know their uh occupation and we'll just say that this person is a student all right so univariate data is a data where there's only one variable being tracked so in this case if all we did was track age then this would be a univariate data set but this is actually a multivariate data set because we're not only tracking age we're also tracking gender and occupation so that makes this uh data set multivariate presumably there's more than just this individual in our sample uh maybe there are some under other individuals too for whom we've collected some data in this case this would be a multivariate data set there is kind of a special case for the multivariate case which is the bivariate case and bivariate data is a data where there's two variables and when you have only two variables at least in this class we can start talking about ideas such as correlation how are two uh variables related to each other uh you can do this for general multivariate data too but that's kind of going uh beyond the scope of this course and even then you kind of start out with the bivariate case first so data itself the variables can come in different classes such as there can be categorical variables so an example of categorical would be gender and occupation these would be categorical variables because there are a finite number of categories that these could be in compare that to say age which we would consider quantitative because what's being tracked is a number presumably an unbounded number now you might think well what about say uh the roll of a dice if you're rolling a dice then there's only a finite number of numbers one through six this is pretty much a simplification more with categorical we're thinking with things where there may or may not be an ordering but we really don't want to view categorical data necessarily as numbers per se whereas quantitative are numbers right and there's also alternative formulations alternative breakdowns uh with maybe some more granularity on how you're going to break down data like for example you might say there's nominal data where it's just categories that have no relation amongst each other you might then progress to ordinal data where you do add a little bit more structure where one observation where one value could be considered greater than another maybe we could talk about a person's year in college in which case you would say that sophomore is less than in some sense junior which is less than a senior although you don't necessarily want to attach numbers to uh sophomore freshman sophomore junior senior and then you could have maybe interval where you're allowed to do addition and ratio where you're allowed to do division so uh but but yeah that's an alternative breakdown don't worry too much about it in modern statistics we depend very heavily on probability theory probability theory is a field of mathematics that describes the behavior of objects in the presence of uncertainty the mathematical study of probability we've known about probability as a species for a very long time at least since the greeks but it's only until around the time calculus was being developed and probability uh started to take a mathematical interest and then it was around the beginning of the 20th century where probability became its own proper uh field in mathematics so why do statisticians care about probability how's probability used in statistics well we have this relationship um we have a population and we have a sample so a sample is a subset of the population so the question is how is it that we get a sample from the population how when when we collect a random sample from the population where every individual in the sample is picked with some with equal likelihood how will that sample behave how will statistics computed in that sample such such as for example the sample mean or even then like going beyond mean like let's say we track the smallest age how will statistics like that behave the behavior of those statistics is determined by probability so probability describes how random samples from a population behave and as statisticians we develop probability models for our samples and then use those probability models to describe uh parameters of the population so this would be statistics so probability and statistics are working in inverse directions where probability describes how if you knew the population how will the sample behave whereas statistics is interested in given a sample how can we infer the properties of the population like for example important population parameters so okay all right so uh let's let's continue on uh how we define a population largely depends on our problem for example in the examples that i gave before about the united states and my class we would consider a study involving those populations in numerative studies since in principle there is the population exists the population exists in physical and temporal space so that means that there's no like you can actually find in principle every single individual in the united states and you can find every single individual in my class and a census is in fact possible to conduct now compare that instead to analytic studies where the population may not exist so in my previous example where i was talking about petri dishes as being a potential sample a petri dish for some bacterial culture is probably going to appear in an analytic study a lot of those biological studies are going to be analytic studies because the population isn't necessarily considered existing if or in a simpler case if all i wanted to do was determine whether a single dice that i have was fair in principle you can't really say that all of the possible roles for this dice exist in some physical temporal space i can keep rolling this dice forever so uh you can't really consider an enumerative study so you're just gonna say the population is all possible die rolls for this dice and in the case of a medical study you might say that the population is all humans past present and future and we're studying the behavior of some drug in humans we're we're thinking of humans as some biological as a human species as a biological entity uh not necessarily the current people who are living right now we would probably want to include future people too um all right so uh statistics is it depends very crucially on how the data is collected and we're not actually going to talk very much in this class about how to collect data correctly i'm just going to leave you a few words in this class we're going to assume that data was collected via a simple random sample so if data was in fact collected via a simple random sample then ins there is a sense in which every individual in in the population is equally likely to have been chosen uh the metaphor that i like to think of is we have a hat and that hat has little slips of paper and each of those slips of paper will identify uh individuals in our population we pull one of the slips of paper at random with equal likelihood from the hat and that gives us an individual in the population that will be included in our sample compare that instead to stratified random sampling which is a more complicated procedure and the procedure that the census bureau is actually going to use in these annual american community survey studies where in this case you actually divide up your population according to some known strata a strata is something that you automatically know for individuals in your population for example what state they reside in so you divide up your population into strata and those strata don't necessarily have equal size but you're going to pick a random sample from each strata a simple random sample as i described before and then make inferences this procedure the stratification procedure can increase uh the power or the ability of your statistical procedure without having to collect as much data so it's a nice procedure to have in hand it's the procedure that is being used in more complicated surveys but we're not going to discuss it here the procedures that we discuss here are not appropriate for stratified sampling uh so it's of course very easy to sample badly uh so one instance of bad sampling is convenience sampling where you're basically selecting individuals from the population based on how easy it is for you to get the data in the case of this class if you were interested in how and what the grades of this class were and you decide to ask some of your friends what their grades are that's a convenience sample it is not a random sample because in the end the sample is going to resemble you in some way it's going to be more a reflection of you than it is of the class so you may end up if your friends are like better students and that's because better students like to comingle then that could be a problem because you're not going to get an accurate view or let's say we're studying politics and you decide that you want to determine people's um political party affiliation so you stand outside of the marriott library on campus and you start asking students uh what party do you support in elections uh in that case you are not gonna get anyone from st george utah uh almost sure well i mean you might get a couple people but for the most part st george utah will be extremely underrepresented in your sample and that's going to be a problem now you don't have an accurate representation of the state of utah if that if the state of utah were in fact your population of interest and even if the university of utah itself for your population of interest not every student is going to be hanging out around the library so that'll be a problem uh or in the case of a voluntary sampling where like the classic example is a a tv host goes and tells his viewers to participate in a survey is like who do you like the democrats or the republicans and it's like oh my gosh all my viewers like the same party as i do and they agree with me no shocker so that would be a case of a very unreliable study um so yeah that's that's another thing but we're not really going to talk too much about this uh this chapter this section i mean there's a lot that can be said about how to appropriately sample and actually this is one area where current events i would like to say some more about it like the coronavirus uh what are some of the statistical issues surrounding the coronavirus because there's a lot of statistical issues involving the coronavirus and our understanding of it and its effect on the population and i'm thinking i'm going to leave that to a separate video but for now that is the end of this section and uh i'll see you in section two on pictorial tabular methods in descriptive statistics so all right best of luck to you hey students alright so next section is on uh making graphs and making tables for descriptive statistics all right so first let's describe the idea of a distribution when we're talking about random variables there's a couple things that we should keep track of there is what values a random variable could take and there's also how frequently those values are taken those ideas are are captured in the distribution of the data set so um in uh in in this section we're going to see some uh basic ways to understand a distribution of an observed data set and in later sections we're going to talk about the idea of the distribution of a population and how it could possibly uh extend the idea of distribution and one basic way that we could understand a distribution is using a table and we can also use visualizations visualizations can actually be very helpful in understanding a distribution because there are things that our eyes are very good at picking out and when we make pictures of our data set we pick those things out like for example where does the data tend to cluster how spread out is the data are there outliers in the data set influential observations and our eyes are actually very good at noticing those features so creating visualizations of data says can lead to us learning a lot that actually really cannot be learned by numbers on their own in fact there are some uh examples for example there's this thing known as um amscom's quartet maybe i should uh maybe i should very quickly uh have a look at anscombe's quartet because we can do that um very quick okay so and oh let's see wk and scum is it b uh quartet this is close enough it'll probably figure it out is it ants comb and i can't type oh okay so it has a b in it all right so there's this picture right here and this is known as enscom's quartet this is for a bivariate data set uh which is not what we're talking about right now but uh when working with bivariate data sets you want to track uh for example the mean of x and the mean of y and also if you were to come up with a best fitting line a regression line between the two variables uh what would the parameters of that line be and what is the correlation between the variables and numerically every single one of these data sets are exactly the same even though our eyes can see they are not the same at all these are clearly not the same data set and they have very different properties so making visualizations can help us learn a lot in fact this is a subject in and of itself you can take a class from the computer science department on just visualization um but we're not going to be going into that much depth and visualization today um what we learned today is pretty much enough to get you by in life unless you're doing some really serious work so from this point on in this video and future videos i use this notation to describe a data set and this data set has sample size n so the sample size is the number of observations in a data set um the indexing here we have x1 x2 xn the indexing the numbers themselves don't actually matter they're just a way for us to differentiate observations these observations may have the same values or different values uh it so there's it it actually doesn't really say all that much about the data set in particular i don't want you to look at this and think that this data set is now ordered this data set is not assumed to be ordered if the data set ever becomes ordered i will let you know but right now it isn't um i also would like to draw attention to the fact that this is in lower case and it's it's not a it's not a perfect rule i've often violated it uh but it is a hint that lowercase things and this will probably be the case in the in the in the videos for this class um it is generally the case that lower case numbers are constant uh and a data set is constant a data set is something that you observe and because you observed it is now constant whereas capitals often refer to random things which is not what we're going to talk about today so uh the first visualization method that we're going to talk about is very basic it's called a stem and leaf plot so we create a stem leaf plot using the following steps the first thing we're going to do given a data set is we're going to select the number of leading digits to be the stem values so we have a number maybe that number is 123.45 and we're going to select some number of digits maybe the first two digits and these will be the stem values the remaining step the remaining values uh let's use a different color for that uh where did red go the remaining values will be the leaf values which in principle you could put like the entire block 345 as a leaf value although a lot of people will just like round to just one digit uh because well you'll see for a second why someone might be inclined to do that um so after you make that selection you're going to draw a vertical line let's not use that color uh we're going to draw a vertical line and we're going to list the stem or possible stem values uh on the lefthand side of this line so we might have 12 11 10 and maybe 13 and 14 and on the on the right hand side of this line we would record the leaf values let's assume for a second just for simplicity that we didn't observe this part and all we saw was the three we might list one two three and this would be read off as a hundred and twenty three because according to part four we're somewhere in this display we're going to indicate the units of the stem so we might say in this case tens uh to indicate that the stem is uh tens place and beyond uh which means that everything to the right will be in the ones place or maybe we're doing a more advanced plot where maybe we actually did write three four five so this would be one two three point four five uh but we're not going to do that because uh i actually think that would be it kind of defeats the purpose of a stem and leaf plot because what we would instead do is list the leaf values for each observation our data set so maybe we saw 101 101 1 1 2 3 four so we have an observation a hundred and fourteen and another observation that's a hundred and thirteen and an observer uh another observation that's 112. uh or we'll also even throw in a nine so there's an observation that's 119. we've got 123 130 144 um and that and now we have a display and let's just throw in like one more number uh like right here uh now we have a display that that um actually later on we're going to see a histogram and say oh this actually looks a lot like a histogram but we do have this display that's indicating that that's telling us a lot about this data set for one thing it actually tells us the actual values in the data set that's one advantage of a stemandleaf plot when you have a stemandleaf plot you get to see the actual data values and that's one thing that's nice another thing that's nice about them is that they're very quick to create like i imagine the stem and leaf plot being created in the field like you are out there you are watching uh i don't know birds and tracking how many birds you're seeing or characteristics of birds and you actually get to write down uh as as you are recording your data you're also visualizing it so you can learn uh things about it because you can see like for example this data set uh seems to cluster in this 110s area uh we might be able to guess that the mean is somewhere around here that this is like where the center of the data set is and we get some sense of its spread all right so um here is another data set for us to practice with uh this is a subset of mcdonald's data on hint on heightened finger lengths and this right here is actu this part right here is actually our code so we could go into r and type this part in and uh use r to do some of this analysis so um and i'll actually show you a little later how this can get turned into like what r would do with this part uh but um we're just gonna take this and treat it as our data set and construct a standard leaf plot by hand so what are we going to do uh we first need to decide um where we're going to make the cutoff we're not going to make it in the ones place because if we put it in the ones place then there's going to be just five for our stem value and what you'll end up with is a plot uh that looks like this and it will be just like a giant skyscraper and you won't you won't learn anything so we don't want that so the next place that is available to us is the what is the tenths place and the tens place seems pretty good so i'm now going to make the line for the stem and i'm going to write let's see we've got five three five five we have a five two oh we also have a five oh so we'll put five zero five one five two five three five four uh and we go all the way up to five nine so five five five six five seven five eight five nine okay there's there's some dead spots on this super super cheap computer i bought it for 200 um at target across the street the which was the most convenient place to buy a laptop super cheap so you can't what you pay for and i'm especially learning that right now uh anyway um so uh we now have the stem values and now we can start recording our data set if you were to do this in a computer a computer would actually order the uh leaf values here they actually are ordered because nine comes after four which comes after three uh so a computer would actually order it i don't really see the point because the point is to understand the distribution as in in a visual way and ordering believes does not aid in that so we can go ahead and just start reading across in this data set to make our stem and leaf plot so we've got 5.55 so we'll put a 5 right here point three zero so we'll put zero there five six three five three zero five one three five zero five five three eight five nine six five two one and five three eight so in this case it actually turned out to be ordered and that's pure coincidence i wasn't planning on that but now we have a stemandleaf plot and if we scroll down and or if you're actually if you actually have these notes printed out oh one last thing we need to do is indicate what this line means and this line is the tenths place okay all right so if we were to continue on to the next page we would actually see some r co that would do this so this is the high data set from before this part right here the r function oh okay so this is the act the same height data set we had before the the r function stem creates stem and leaf plots and this is basically the same as we had before okay before i continue on i'm just going to double check to make sure i'm still streaming yeah we're still streaming okay all right um okay so next up for our visualizations we have a dot plot a dot plot represents each data point as a dot along a real number line uh so we can we would then be looking for clusters in dots to note patterns in our data set so in this example we're going to use the data from the previous example which has already been very nicely organized for us in a stem and leaf plot to create a dot plot let's see is there a nice easy way to zoom out oh very nice oh that's not what i wanted that's delete i need i'm new to this program uh and we're at the very bottom too that's not what i want either okay so all right so this this program is new to me my apologies um all right so all i want to do is zoom out a little bit okay that's good that's good um oh cool all right i just want to be able to see this uh nice stem and leaf plot that we created before when creating our dot plot so for our dot plot uh let's see we've got let's see so for the left point on the range we should pick 5.0 and uh for the right point of the range we should probably pick 6.0 because we get really close to 6. so in between we've got 5.5 and then we can just mark off in in a fifths so two three four five okay that's that's that's close enough uh if you want perfection ask a computer to do it all right so um so we've got an observation 55.05 so that's about right there 5.13 is about there 5.21 is about there and then we've got 5.30 um that is about there and that's also got another one right with it uh there's another observation uh oh that's actually could be a little bit to the left but we could also call that 5.38 so here's 5.30 for realsies this time we've got 5.55 5.63 and then 5.96 so then we end up with the stop plot that's also giving us a sense of where the data tends to lie but this time along a real number line so and and i i'm sure you can guess so far that the methods that we have been working with uh our visualization methods are pretty much devoted exclusively to uh quantitative data we'll see categorical in a second i'm pretty much going to say draw bar plot and that's it so here is our code for uh what i just did the r function for making dot plots is a strip chart so strip chart is what makes these things and i'm just passing some additional parameters to strip chart to uh make the function to make the plot how i like like for example this parameter right here controls how it's like what exactly we're drawing here are we drawing filled in circles or empty circles and so on so this you can think of this as point character because you're choosing the character of the point and 19 corresponds to a filled in circle uh we have a little bit of an offset and uh some other parameters that just help make the visualization look nice i would recommend go ahead going ahead firing up r playing around with these parameters seeing what happens uh stack for example uh means that when two points are about to collide with one another stack them on top of each other because it's not the only way to do things there are often very many ways to solve the same problem so all right uh continuing on so a quantitative variable we we need a further a refinement of quantitative variables and and we say that a quantitative variable is discrete if it's possible values are countable uh countable that could be for example one two three four five six or that could be one two three four five six seven eight nine blah blah blah up into the future uh it could be the integers so including negative numbers as well or including zero as well um and then so we would call any of those things countable there's also continuous variables where the possible values could come from a continuum so that includes 0 1 and anything in between so 0 one zero point five uh zero point zero one one zero one two five uh one divided by pi any one of those things are possible uh there's actually a nice little rule of thumb which is that uh discrete random variable discrete variables emerge when you count things like for example i count sheep so what am i going to get i'm going to get an integer in the end so if that's going to be the case then it's comfortable whereas continuous emerges from measurements so like with a ruler so you measure how long something is with a ruler and that will probably produce a continuous variable so continuing on uh some further notions when describing a distribution there is the frequency of a of a value and there's also the possible values it could take so frequency is how frequently we see a variable take a particular value and for discrete variables it is in fact possible and quite likely that we will see the same numbers repeatedly whereas if we're dealing with continuous you cannot necessarily assume that so the frequency for each value for a continuous variable would be rather uninteresting since you're probably going to see like going back to the data set we were just looking at admittedly uh for this uh height data set we did have some collisions but you can't really count on that you can imagine that if you were to increase the decimal position precision of the numbers that you're looking at you would um you would actually not have a collision you'd have two distinct numbers the only reason why we get 5.30 twice is because that's where we decided to round so in the case of continuous variables well with frequency well with discrete variables it's okay to just kind of consider the values in and of themselves and just say how many times did one show up how many times did two show up and so on for continuous variables we probably should do a procedure known as binning so binning is where we define a range in which a data point could possibly be and instead of tracking how frequently a certain value is taken we're going to track how frequently um a variable will fall into a bin right so you can imagine for instance that this is a this this is a continuous line between zero and one and we saw uh numbers uh in this region and they're just random numbers and we're not going to actually list out in a table how often we saw those individual numbers because most of the time the frequency for that number will be one instead what we'll do is we'll divide up this region and then count how many times uh numbers fell into those individual bins like for example we get we got twice here three times a year uh one two three four five six times here one two three four five six times here and then four times here and then we'd have like a more useful table um for understanding our distribution very closely related to the frequency is the relative frequency relative frequency is where you take the frequency and divide it by the sample size that's it so if you really want to see a formula for the relative frequency the relative frequency the relative frequency is equal to frequency divided by n a frequency distribution is a tabulation of the frequencies or the relative frequencies or in the case of continuous variables we'd probably also include the frequency of bins um something that i haven't really discussed so far is that actually how should we choose the bins like like like bidding is a choice i chose these bins but alternatively i could have chosen bins like this like here's one bin and here's another bin why didn't i do that so the actual bidding decision is it is in fact a decision you need to decide how many bins they are and what regions they're going to cover so in terms of the regions i think it's pretty safe to say the bin should be equal length if the bins are not equal length then what you're going to end up with is a difficult to understand chart or a difficult to understand table because people then have to pay much more attention to the bin length um and also when you're trying to visualize what's like this is leading up to histograms and we're going and the binge bin size is uh something you have to make when talking about histograms and when you um uh when you end up with bins of unequal length then the histogram like you there is ac there is a precise way to understand what's being visualized in the histogram but unfortunately people's eyes are not going to perceive it that way um and it's that you are making the histogram more difficult to understand so so just always make your bins equal length but the number of bins that you have and also what exact regions they cover because you can still for example take all of these bins shown here take all these bins and then shift them slightly to the left and then you have then you have a different visualization or you then have a very different description of your of your data set how exactly should you decide where the boundaries of your bins are and how many bins should you have these are all very important decisions that can have uh drastic implications for visualizations but i i really should move on to actually creating a frequency distribution which is a tabulation of frequencies or relative frequencies i should really move on before i start digging into that topic so let's make a frequency distribution for this soccer data set where we have a supposed statistically minded parent maybe that parent is you if you have a daughter or a son and they play soccer and you've and you've decided that you're going to uh track their little league soccer team scores during a regular season so here i have a data set i've and by the way what i've done here uh in terms of our code is creating an r vector so i have this list of values um in what's known as an r vector i'm not really going to talk about r in these videos i'm going to pretend and assume that you are watching my video series on how to use r uh but enough of that for now um i want to construct a frequency distribution for this data set so what are possible values in this data set uh well uh i should let's see uh we've got we've got a so on the left hand side of this line for this table so we're going to have um a value and we're going to have on the right hand side the frequency so what are possible values well we've got one and we've got nine and we've got numbers in between so we'll just go uh one two three four five six seven eight nine all right and then we need frequency so let's see how many times did 9 appear looks like 9 just appeared once all right so we got that covered uh six appeared uh so one two yeah that's it so six appeared twice uh five appeared three times and uh eight appeared twice uh and each each of the remaining numbers appears only once so one appeared once two appeared once uh three appeared once and four appeared once okay uh seven never appeared all right one second i catch up in my notes um so the sample size for this data set you can count there are 1 2 3 4 5 6 7 8 9 10 11 12 observations or alternatively what you could have done is um and in fact i'll go ahead and just kind of scroll down a little bit and down here i'm going to track the sum and yeah it adds up to 12. another thing i want to do is in this table track the relative frequency so we'll call this relative frequency and in the relative frequency i'm going to take the frequency and then divide by the sample size so 1 divided by 12. uh that is going to be a 0.05 or is that an 8 no that's an 8. 0.08 three and the three is a continuing uh digit all right so for the next one it's the same thing zero eight three uh and again for three zero point zero eight three okay and then four five uh no wait so let's see uh four also appeared that amount so zero point zero eight three uh four appeared three times so that's going to be zero point uh two five because three divided by uh 12 will be 0.25 3 is a quarter of 12. 6 will be 0.16 where the 6 is a continuing digit 0 0.16 and 0.083 okay and if you add these numbers up this is a way for you to check your sanity this should add up to one okay so moving on if you were interested in how to make a table like this in r the function for r in r for making such a table is well table so you would have given it the soccer data set and ask it to make a table and it will tabulate uh which observations were taken and how frequently they were taken notice that it did skip seven um which i don't know is that a feature you want i don't know uh i actually personally would rather not skip seven and you'll probably see why in a second um when working with continuous data as i was mentioning before there's this issue of binning uh deciding on the number of bins uh deciding um uh where the boundaries of the bins are and so forth but um so but uh once you've decided on the number of bins and there are some rules of thumb that you can use uh for how many bins that you should use like for example if the if the sample size is n you could perhaps choose the number of bins to be the square root of n actually there is some statistical arguments that the correct number of bins should be on the order of n to the power one fifth or the fifth root of n um there are some arguments for that but uh square root of n is also fine for now um at the end of the day whatever it is that r is going to use for its uh binning decisions is better than what you're going to do and it's going to have some uh nice theory backing it up so you probably should just trust r whatever r is doing is probably better than what than than the square root of n roll um so after you've decided on the number of bins you segment your number line so that you have that many equal length bins uh and uh depending on where in each data point falls assign it to a bin if it falls on a border between the bins assign it to the bin on the right so in other words bins are right inclusive that is a parameter some people prefer left inclusive bins does it really matter no it doesn't it doesn't really matter nobody really cares uh just just pick one right and be consistent either put it on the left band or the right bin just don't put it in both please and then construct a frequency distribution for the bims uh very quickly before i keep going okay we're still good all right um i i i'm very nervous about losing this video i'll just put it this way today i've recorded a number of videos that suddenly just went up into thin air because things went bad and i don't want that to happen again all right um so uh example four so using the data from example one uh you can scroll you can go back in time to see what that is this is the heights data set uh construct a frequency distribution and for what it's worth there were 10 observations in that data set the function length when given a vector will tell you the number of elements in that vector in r so since the number of elements is the number of data points we want to use length to determine the sample size okay continuing on i have made some decisions i'm going to decide that the number of bins i'm going to use is going to be uh about the square root of 10. the square root of 10 is between three and four so uh uh so if you wanted to use the square root of ten rule uh that would be um about uh we'll say for rounding up although apparently in my notes i decided that i was going to go with five and i don't want to change it so i'm gonna go with five why five why not five um it's it's it's it's whatever you want i i think the reason why i went with five uh i don't know i just did so um let's go ahead and zoom oops is that what i want yeah that's fine okay so uh i'm going to make a table so i on uh in this table i'm going to list the bins so i've got 5 to 5.2 and to indicate this is going to be right inclusive i'm going to put a little uh less than sign to say 5 2 less than 5.2 not including 5.2 and then 5.2 to uh 5.4 uh 5.4 hold on okay five point four two five point six five yeah i need to kind of get away from there okay uh 5.6 to 5.8 and then 5.82 uh less than six and we can just leave it at that all right and then we have the frequency for each of these bins so the frequency for these is going to be well there were two numbers between five and five point two uh five numbers between 5.2 and 5.4 and then one number in each of the remaining bins so uh if we were to do the relative frequency we're going to take each of those frequencies and divide it by 10. in which case it's you're just going to move the decimal point over one place so 0.2 0.5 0.1.1.1 okay all right once we have a frequency distribution such as this we can now construct what's known as a histogram which is a plot for visualizing the distribution of quantitative data so how do we construct a histogram first draw a number line and mark the location of the bins so for example we could do something like this and we're going to say that the bins are going to be about here um and then for just if you want to for discrete data you can center your bins on the corresponding values themselves um because you're not really doing any binning with the the discrete variables or you can imagine that your bins are exactly what they need to be uh to be touching each other and uh centered on the integer uh the corresponding integer and then for each of these classes or bins draw a bar extending from the number line so we have uh some yaxis that's tracking let's say the frequency so we have a yaxis that's checking the frequency we're going to draw a bar from the number line to the y value that corresponds to the frequency of that bin or the relative frequency if that is in fact what you're plotting on the y axis so it would end up with a plot maybe looking like this and that resulting plot is a histogram okay so uh going back to some of our examples let's draw a histogram for the data set in example three which was that soccer data set so for the soccer data set we had numbers between one through nine we'll go ahead and include zero as well for this data set so i'm just going to i'm going to start out by drawing the graph the xaxis corresponds to goals so goals in a game and the yaxis corresponds to the frequency so let's see the the frequencies never seem to go beyond three so we've got uh one two three okay and then possible integer values i'm going to go i'm going to go ahead and include zero because in principle this soccer team could go could score zero points so we'll go ahead and include zero but we'll also include at the very end we'll have nine and we've got one two three four five six seven eight nine okay so uh they there was one game where they scored one point uh one game where they scored two points one game where they scored three points uh and one game where they scored four points then they had three games where the team scored five points uh two games where they scored six points uh no games where they scored seven points uh two games where they scored eight points and one game where they scored nine points okay hmm new feature let's go ahead and see what happens uh when i no i don't think that works oh well um so there there be the histogram it's it's not a perfect picture but it's mine so if we wanted to down here is the r code for constructing a histogram so the r function is hist and uh i've given it some parameters to manually specify the breaks because i because otherwise um it would choose its own algorithm it would use its own procedures to come up with the breaks and i wanted to override that and this is one way to do it where i basically gave uh the um uh the function the uh break points so we have so the minimum of scott of soccer is going to be uh 1 so 1 minus 0.5 will be um why can't i do math it's 0.5 um and then you have and then i went one above the maximum score so that would be this number right here so this right here corresponds to 0.5 and this will be a 9.5 uh over at the right hand side and that's just telling it where i want those breaks and then it will infer that you have that everything in between is a bin uh but then you end up with a with a pot that is essentially what we came up with by hand okay uh next example for uh the data set in example one let's create a histogram okay so for that one i am going to use the relative frequencies instead which do you use at this point it doesn't really matter because the shape is going to be the same regardless of whether we divide by n or not and the interesting part when you're creating a histogram is looking at the shape of the resulting histogram so i actually really don't care although for what it's worth if you want to use this thing for more probabilistic inference you probably should pay more attention to whether you've got the frequency or the relative frequency um and if also the bins were not of um uh equal length then you would have to pay much more attention to the frequency versus relative frequency but also just take my advice and make your bins all equal length so uh we've got the relative frequency that is what we're going to draw this time and uh we've got the highest it will ever go is uh 0.5 in fact i don't think it ever reaches there so we've got right yeah it does it does reach 0.5 so and we'll just increment by 0.1 so 0.1.2 0.3.4.5 so this is 0.1 uh right here so possible bin values i said i decided that we were going to have five bins for some crazy reason i don't remember what it was uh and so let's see is that five it is now so we've got numbers between five and six and using the table that we came up with uh that time ago we're going to end up with a histogram uh looking something uh like like uh this so it goes up to here and then oh yeah so it goes to 0.2 and then 0.1 for these remaining bins and that's our histogram so if we were to continue along and look on the next page uh oh i think that's the reason why i chose 0.5 it's because it it's exactly what r thinks it should be and you should probably given the choice between what you think the number of fins should be and what are things it should be you should probably go with what r is good what r is doing um don't fully trust it but at this point you probably don't have the experience to have your own opinion so um yeah it came up with basically the same picture okay now we've been coming up with these nice plots and everything is great one second let's just uh satisfy my nervousness okay we're still good okay um i mean making these pictures is nice but why are we making these pictures well there are certain things that we are looking for when we're making visualizations like this for example is the data unimoda we're like one thing we're looking for is modality which is where does the data tend to cluster is the data unimodal where it only has one peak this would suggest it's clustering around one area or on the other hand is a bimodal or multimodal where you have multiple peaks so the unimodal case would be something that resembles this uh where so we'll call this unimodal bimodal would be a situation like this uh maybe think of it as a camel hump and multimodal uh is uh like all like you've got all sorts of different modes all sorts of different peaks so what would it mean if you had a unimo versus bimodal versus multimodal for starters the multimodal case the first thing that you should ask with multimodal is did i choose a bin that's too small because you can end up with situations if you choose your bin size to be too large when you're making a histogram uh if if at one extreme you can have a skyscraper where everything is in one bin and that is a chart that doesn't really tell you anything on the other hand you could have that basically a pancake where every single observation gets its own bin and that really doesn't tell you anything either that's basically a dot chart and you've kind of lost the point of the histogram so the first thing in the case of blue of this uh blue uh sort of histogram it's not really histogram because it's a smooth curve but whatever um in this in this case you should probably ask yourself whether you've chosen too few or too few bins uh it could happen that you have true genuinely multimodal data but the number of modes should be should not be too many because modality and having more than one mode is indicative of there being more than one actual population in your data set so for example if i say that this is tracking height of people i could genuinely have a bimodal data set because actually there isn't one population in this data set of people there's actually two populations men and women because men and women will cluster around different average heights so that's that's that's that's features that you're looking for unimodal indicates that your data set it probably consists of one population um and another thing that we're looking for when looking at stuff like histograms and by the way a lot of this discussion also applies to the strip the stem leaf plot and the dot plot in particular the seven leaf plot because you can argue that the standard leaf plot is actually a histogram it's just a histogram with partic with a particular bin choice but there's nothing really different about it um but anyway is the data positively skewed or negatively skewed or symmetric so a positively skewed data set the way i like to think of it is well let's first draw it out we have the positively skewed data set and then we have the negatively skewed data set which looks more like this and then we have the symmetric data set let's do a little bit better than that okay that's a little bit more symmetric okay so if you're if you're at all bothered by the terms positively skewed negatively skewed symmetric and you're wondering how can i tell the difference between positively skewed and negatively skewed here's a little rule for you draw a dinosaur draw a dinosaur and then ask where is his tail pointing in the case of the black dinosaur it's pointing towards the positive end so it's positively skewed all right now let's talk about the green dinosaur well let's uh draw the green dinosaur give him some legs give him some stuff on his back because he's like a stegosaurus so he's got these plates on his back where is his tail pointing oh it's pointing in the negative direction so it's negatively skewed right and you don't draw you don't draw a dinosaur for symmetric because i know we're not we're just not going to do that we're already silly enough but um why does it matter whether a data set is positively skewed or negatively skewed um it matters when we're thinking about the relationship between uh important statistics such as the mean and the median so the mean and the he the median are going to behave a certain way and have certain relationships depending on whether the data is positively skewed or negatively skewed uh positively skewed data sets what that basically means is that outliers there are outliers in this data set and when observations tend to be large they tend to be very large um negatively skewed data sets are data sets where when an observation is small is small it tends to be very small and i can think of specific data sets that fall into these types of categories like for example income tends to be positively skewed so you have most people in a certain range of incomes and then you have a few people who make much much more than that right so most people are around here but when you're rich you tend to be very rich and on the other hand for negatively skewed data sets i do have something in mind i have test scores test scores for me like there's a lot of ways test scores could actually appear statistically but it seems like most students tend to fall within a certain range and the students who really didn't get it uh when they fail they fail hard so it tends to be negatively skewed and symmetric symmetric is kind of this ideal case where you're just as equally likely to be above or below i think heights could be possibly uh i haven't really looked at a height distribution a long time uh but i think that heights could probably be a symmetric uh where it's like you're just as likely to be uh really tall or really short um at some degree so you're also looking in these uh plots for outliers for example a histogram that has an outlier you might have a histogram that looks something like this and then you have a point that's way out here and you would say that this point right here is a candidate's being outlier and you're also interested in how spread out the data is and by spread uh when we're talking about spread we're talking about the less spread out black distribution as opposed to the more spread out green distribution which of these cases are you actually looking at or at the very least you're just interested in what the range of the data is okay so that's it for visualizing qualitative quantitative data now let's talk about qualitative or categorical data how do you visualize that we're only going to advocate one method here and that's a bar plot in fact in my visualization class i was told if you don't know what visualization method to use use a bar plot because bar plots are actually very good visualizations never ever for the love of god draw a pie chart i know i know especially in like public policy and economics people love their pie charts for the love of god do not create a pie chart do you have any idea how many memes of ugly and stupid pie charts there are please do not drop our chart okay um continuing on uh to construct a bar plot list each possible value of the variable and how frequently that value is taken uh a lot like what we were doing before it's just instead of having numbers on the left hand side for like these bins instead of that you're just going to have the categories that your dataset could be in uh and then you're going to draw a horizontal line so let's see a cartoon bar plot that we're making so we have like category a category b category c so draw a horizontal line it could be a vertical line it doesn't really matter and along the axis marks possible values of the variables and then draw a bar for each category extending to the categories observed frequency so we'd end up with something that looks like this um uh it is worth mentioning bar plots and histograms are two different things and the difference is this axis here this xaxis with a histogram that axis is a number line and there is a very specific order and if you were to plot a point on that line it would mean something whereas with a bar plot this doesn't mean anything you could rearrange it if you wanted to and the bar plot would say exactly the same thing even in the case of ordinal data you could space it out there's all sorts of transformations that you could do and the plot says the exact same thing that is not the case for histograms so bar plots and histograms despite looking somewhat similar are certainly not the same thing okay so uh for our next example uh there is a data set showing the frequency of the class of passengers aboard the titanic who survived her sinking uh i have the titanic data set in r uh contains this data set but they're actually tracking a lot of things they don't just track the class they track male and female they track age they track survival and so on here i have restricted to using the supply function which i'm not going to talk about right now i have restricted it to the case of survivors for individual for certain classes i want to create a bar plot for the frequency of each class's survival so um i've already got the frequency distribution r is already given it to be very nicely so i'm going to say we've got first class second class third crap class class and a crew okay and then we're going to extend up we're going to say up here let's say that this is a 220 and right here we've got 110. we're gonna just eyeball this uh so for first class there were about 203 survivors so that's about here for crew there were 212 survivors so that's about here uh for second class there are about 118 survivors that's about here for third class about 178 so our bar plot should look something like this okay and in fact we can look at what r does we give the bar plot function in r this bar uh this data set and it will in fact make a very nice bar plot for us okay there is actually a visualization method that i haven't really discussed here and you know i'm on i'm making a video everything's quite nice i really don't see why i shouldn't show you this if you're familiar with r so let's see first are we still streaming yes we are okay so um there is another type of visualization called a density plot you cannot make a density plot by hand so i'm going to go ahead and i'm going to make a density plot and does this command still work this this is a new installation oh good everything's working uh maybe we should go back to um well let's see what's a data set that we could work with um rivers is fine this is the length of some north american rivers and there is a plot called the density plot remember that i was drawing some smooth curves and histograms do not look like smooth curves what i kind of was drawing was a density plot so i could create such a plot by typing in plot density and then give it the name of the data set if it's stored in a vector so in this case rivers and this is the resulting density plot and it's actually plotting a smooth curve uh let's see let's what's uh one of the data sets that we were looking at before um page up page up uh what is it was this okay okay so we had this height data set so let's uh go ahead and combine those two things uh center on this okay so height uh will be a vector consisting of the numbers 5.55 5.30 uh 5.03 5.30 5.13 5.05 uh what was that number 5.36 no that's 5.38 so we've got 5.38 5.96 and uh 5.21 and 5.38 okay so here's our data set and i'm just going to create a density plot for this data set um that's not it it's still rivers oh because i typed in reverse silly me um okay so what i want instead is height there that's better and it makes a smooth curve that kind of resembles actually it it yeah it certainly does resemble the histogram that we drew um except it's actually like in the histogram there is in fact when you look at that data set kind of a peak in this region um and that was completely masked by the histogram but the density plot was able to capture it interesting so but you have to make a plot like this an r you cannot make a plot like this by hand so just i'm just bringing it to your attention because these two these kind of plots do show up all right so uh that's it for section two of the book and uh i thank you for joining me and uh i will see and i hope that you watch the video for section three so have a nice day hey students all right so uh next section is on measures of location so up to this point we've talked about visual summaries and visual summaries are nice the thing is though we don't want to restrict ourselves just to visual summaries we would also like to be able to have numerical measures of data to understand distributions so we're going to start with measures of location measures of location tell us where a data set tends to be located along a number line so the first and most common measure you may have you probably have already seen a lot of these measures that we're going to talk about but the very first one we're going to talk about is the sample mean and for a data set consisting of observations x1 to xn the sample mean is just is defined as x bar which equals 1 divided by n times the sum from i equals 1 to n x i which if you're not familiar with this notation what this means is we would take our data set add up everything in the data set and then divide the resulting sum by n now the there's the thing called the sample proportion and in fact relative frequencies are sample proportions they're counting the proportion of observations in a sample that take a certain value the sample proportion is also a measure of location it loosely is like the proportion of observations in the sample that have a certain characteristic so we divide the sample into successes and failures we like to use that vocabulary of success and failure a success and the sample proportion will count the number of successes so we'll have p hat equals and very loosely we're just going to say this is the number of whatever we consider to be a success and divide this by the sample size now this could also be written as x over n which could also where x is this uh number of successes and then we could say let's suppose that x i is equal to one if the ith observation counts as a success and zero otherwise what then does it mean to count the number of successes to count the number of success successes is to go through each observation and then add one to a running count if that observation counts as a success and otherwise do nothing which is the same as adding zero so we could then say that the number of successes is equal to the sum from i equals one to n of this new x i variable that is counting the number of successes effectively and saying whether an observation is a success so we should say that this sum is equal to the number of successes and then we take this sum and divide it by n which is also for what it's worth the same as taking the sum and multiplying it by 1 over n so notice what i just wrote down i just wrote down the sample mean again which means that the sample proportion is the same as the sample mean of a sample that consists of ones for successes and zeros otherwise so it recognizing this is actually very important because in probability theory the mean or more more directly sums of variables or sums of random variables are very important so recognizing something as a sample mean means that any theorems from probability theory that involve the sample mean can be applied to that variable so this is actually quite important to recognize but that means that after this point we really don't have to say much more about sample proportions because the sample mean whenever we're talking about the sample mean we're also talking about sample proportions so let's get started with an example what is the average number of points your daughter's soccer team scores here is as a reminder the data set this is actually proper r code right here just to write the just to write down the variable name soccer and what will happen is r will then print out that data set so uh but we're just going to take that for granted for now and uh compute first let's go ahead and compute the sum of these numbers so the sum from i equals 1 to n x i which is basically this right here just means take all the all the numbers in this data set and add them up so we've got 9 plus 6 is 15 plus 5 is 20 plus 5 is 25 plus 5 is 30 plus 6 is 36 plus 2 is 38 plus 8 is 46 plus 3 is 49 plus 4 is 53 plus 8 is 51. plus oh it looks like i'm all right so i actually have something written down i i have i i have 62. i think i might i might have uh missed something in that account but it's going to add up to 62. okay so just take my word for it uh so this adds up to 62 uh n is equal to 12. so the sample size is equal to 12. and the sample mean then x bar will be 62 divided by 12 which equals 31 over 6 which is equal to 5.16 with a repeating six okay if we were to go to the next page in these notes we would see some r code that computes the sample mean for this data set the r function is mean so we ask so we say mean of the soccer data set and it will report to us the mean which is what we computed now let's suppose that uh r1 rn is the ordered version of this data set so x1 to xn uh is just so r1 to rn is x1 and xn but ordered remember from a previous video uh for section two on um that that for this notation uh x1 xn i do not necessarily imply any sort of ordering now i do for r1 to rn i'm going to imply an ordering uh the sample median is another measure for the location of the data set it is defined as the number that splits this data set in half so uh we can that is basically the definition um and from that we can come up with mathematical formula so we can say that x tilde that's the notation we will use for the sample median x tilde will be one of two possibilities first there is a case when there are an odd number of observations if there are an odd number of observations uh let's see let's zoom in so i can have a little bit more precise writing uh if there are odd number of observations after we order the data set the observation in the position n plus one divided by 2 will be the number that splits the data set in half so this will be what we're using if n is odd so as a so to think about this if we had 11 observations 11 plus 1 divided by 2 so that's 12 divided by 2 that's equal to 6. the sixth observation after you order the data set will be the median okay uh now suppose that there are an even number of observations we could potentially choose our the r so the um observation in the n over tooth position so if uh the sample size were 12 this would be the ordered observation the sixth ordered observation or we could potentially have the seventh ordered observation both of those are kind of dividing uh the center so what we'll do instead is we will take the midpoint between these two numbers which could potentially end up being the same number there's nothing that says that these two numbers are not the same but we're just going to average those two numbers take the midpoint in between them and admittedly if you had um a number line and you have some data over here and some data over here and you have these two uh observations as being potentially the median any number in between them could be defined as the median since i any of those numbers would divide the data set in half and in fact you may see alternative definitions of the median in r to take advantage of this fact um but for now it doesn't really matter if we had a lot of observations how exactly we define the median uh this is perfectly fine to just take the midpoint between uh the or the ordered observation in the position n over two and the order observation in the position n over two plus one so if we had uh twelve observations we would take the sixth and the seventh observations and average them to get the median okay let's see an example of computing the median find the median of the first 11 soccer games uh your daughter's team participated in i chose 11 just to have an even number to kind of no sorry odd number uh just for demonstration purposes um in this case uh i've our the the sort function in r uh this function will order your data set from smallest to largest uh given a vector it will order that vector from smallest to largest so we have sorted this data set we now have an order data set and i want to compute the median of that data set well first off there are n equals 11 observations in this data set that means that the observation in the middle will be m plus one over 2 which is 12 over 2 which equals 6. therefore the median will be the sixth ordered observation which is let's see one two three four five six si five so it will be five that will be the medium if we were to look at some r code there is a function an r function called median and given that data set by the way i didn't mention this before this right here uh this square bracket stuff this is a subsetting notation uh it basically translates to get the observations uh get all observations between the first and the eleventh in this data set okay and remember that soccer itself is not ordered so we're just getting observations x1 through x11 okay but i just feed that vector to the median function and it tells me that the median is 5. and by the way if you're wondering what this little one right here means uh that's just part of how r prints vectors if there were if this vector was quite long it would split over a number of lines and this would and this little one would just be tracking uh which observation you're looking at with each line so you wouldn't have so like if there was um uh 10 down so if there was like more numbers after this and then we had in square brackets 10 and saw the number six and numbers after that this would tell us that the sixth number and that that would tell us the tenth number and that back in that uh vector was six it's just something that's supposed to make reading uh what's in vectors um visually easier okay so uh i think i pressed something let's uh do that okay all right so continuing on and other measures of location are percentiles so we're going to say that the so this is the greek letter alpha and alpha is a number between 0 and 1 and we're going to include 0 and 1 as well the alpha times 100th percentile is the number such that roughly alpha times 100 percent of the data in the order data set lies to the left of that number uh so if we choose uh alpha equals 0.5 that would be uh the 50th percentile so roughly 50 of the data set lies to the left of that number which means that 50 lies to the right and what i just described is the median because the median is the observation that splits the data set in half which means that half of the data set is to the left or 50 and 50 is to the right so um the median actually counts as a percentile percentiles are a generalization of the notion of a median in fact there are other percentiles that we care about such as quartiles quartiles divide the data set up into quarters so for the first quartile roughly 25 percent of the data set rise to the left of that quartile and for the third quartile roughly 75 of the percent of the data set lies to the left of that quartile so um to visualize we would have um so we would have the first second and third quartiles and roughly 25 lies to the left of the first quartile and roughly 75 percent of the of the data set lies to the left of the third quartile and the second quartile is the quartile where roughly 50 percent lies to the left of that four quartile in other words the median again um there's you can also say that there is a zeroth quartile which corresponds to alpha equals zero alpha equals zero means that there is nothing really to the left of this observation so that would correspond to the sample minimum whereas alpha equals one means that about 100 percent of the data set lies to the left of that number that corresponds to the sample maximum there are actually a number of procedures for computing percentiles from data sets and i'm not going to repeat all of those procedures if anything i'm just going to list off the procedure that's easiest to do by hand because at the end of the day in the real world what you would do is ask r to get a percentile and r has its own algorithm for getting percentiles that's more complicated than what we're about to do and i really don't see the point in telling introductory students how to do that because you're it's it's more complicatedly intensive and like what's the point you get the if you're doing things by hand let's keep things simple um i should also probably mention um if you combine the zeroth first second third and fourth quartile fourth quartile is the maximum if you combine those together you end up with what's known as the five number summary of a data set so um here's a procedure for finding quartiles first find the median of the data set then split the data set into two data sets at the median and we're working with the order data set now so split it into do data sets at the medium if n is odd remove the data point that corresponds to the median and then to find the first quartile find the median of the lower data set and then to find the third quartile find the median of the upper data set so to visualize this procedure we have a data set find the median and then and this will split the data set into two then find the medians of the other two halves okay that will tell you what the first and third quartile are the minimum and the maximum are easy find the smallest and the largest numbers in the order data set okay so for our first example we're going to find the first and third quartiles of our of uh of this uh little girl's uh first 11 soccer games so uh let me get caught up in my notes for a second okay so to find these two quartiles how about we write down uh what that data set was just for our own purposes so we've got two three four uh five five five uh six six eight nine okay so the median was five and this is an odd number data set so we're going to delete that median six six oh oops there were two eights sorry about that so we're going to delete the median which is five and then we have split the data set in two so then we find the median of the first half there is an odd number of observations here so the median will be four thus the first quartile which i will call q1 that will be four and the median of the upper data set that will be eight so that means that the third quartile will be eight and if we wanted to we can kind of fill out the five number summary saying the second quartile which is the median this that number is going to be five the zeroth quartile which is the minimum well that's going to be 2. 2 is the smallest number and the 4th quartile which is the maximum well the largest number in the data set is 9 so that will be 9. okay okay next example find the 10th and 90th percentiles of the height data i have listed the data for you below in order so the data set oh yeah this is actually what i was getting to um regarding those little numbers in square brackets this is the ninth observation right so that means that this number right here is the tenth observation in the data set so that means that this data set has n equals 10 observations uh so 10th percentile so roughly 10 percent of the data set lies to the left of that number so uh 0.1 times 10 is equal to one so that's about uh so about one number lies there so we're going to choose 5.05 so uh 5.05 will be the 10th percentile which we will call uh we'll call that q hat point uh 10 and uh for the 90th percentile so 0.9 times 10 that's going to be 9 so that's observation number 9. so 90 of the data set lies including that number to the left of 5.63 so we'll say that 5.63 will be the 90th percentile which we'll call uh q q hat 90. okay and actually r has functions for computing uh quantiles another word for what we're talking about here are quantiles here i have asked r specifically to give me the 25th and 75th percentiles it is not using this procedure that i described it is using a different procedure for finding quantiles there's a number of different procedures for finding quantiles and percentiles and if your sample size is large nobody cares which one you use really um it really only matters at smaller sample sizes which procedure exactly you use and there are various different motivations um for different kinds of procedures there's various ways to solve the same problem and what i described is simple enough to do by hand so if you're going to do it by hand go ahead and use what i used this is probably uh whatever came up with this is probably much more complicated and if you're going to do if you need something more complicated then don't do it by hand that's hard to do it so you can read rs documentation to see what exactly is being done here it's using some sort of interpolation trick so here i've asked so the second parameter here this is a vector that contains the numbers 0.25 and 0.75 corresponding to the 25th and 75th percentiles or quantiles i'm not really sure what the difference between those two words is um here i ask for the 10th and 90th quantiles or percentiles and it gives me numbers these are all pretty close to what i came up with before okay uh next up uh let's kind of we we've come up with some measures of location and there's actually a number of different measures of location like for example i have seen a measure where you take the largest and the smallest observations so you could say that um so you could say that's r n m minus r one uh no plus r one so take the midpoint between the largest and the smallest observations that's actually another measure of location there's a number of different measures of location what i've shown here so far is fine but let's go ahead and start comparing these different methods for uh for measuring for describing the location of a data set so the sample mean x bar is known to be sensitive to outliers which means that outliers the data set have a profound effect on the sample mean so if you had for example a data set that consists of one one uh let's say one two three the average of that data set or the mean of that data set will be two and the median of that data set will also be two uh compare that to a data set that contains the numbers one to one thousand so uh the mean of that is going to be about 500 which is much larger than 2 what it was before compare that to the median the median of that data set 1 to 1000 is still 2 which means that the median is insensitive to outliers it basically doesn't care what they are all it cares about is the ordering of the observations so long as an outlet if you were to change the value of an outlier so long as it doesn't change the ordering of the data set the median will not change so as an example of this i'm going to compute the sample mean and the sample mean of the daughter's uh uh soccer games if i throw in a 12th soccer game and i'm going to consider a number of different scenarios where her 12th game was 1 point or one goal of four goals two goals and eighteen goals we're going to consider that and what i actually did here was um i created a vector that contains these uh that contains these so uh i'm not gonna talk too much more about this because i wanna focus on the math um so uh first off when i add up the 11 other games when i add those up i end up with a cumulative score of 61. so let's create a table for all of these potential outliers so in this table uh we're going to consider when the outlier when the 12th game is 1 when it's 4 when it's 2 and when it's 18 and we're going to have a column for the sum of the observations we're going to have a column for the sample mean and a column for the sample median so in the case where the 12th game is one this will add up to 62. and by the way i'm using the word outlier for one but one in this case would not be an outlier same with 4 and 2 but 18 certainly would be considered an outlier so if her 12th game is 1 then it adds up to 62. if her 12th game is 4 it adds up to 65. if her 12th game is 2 the game's cumulative score is going to be not 62. uh 63 and if it's 18 they all add up to 79. and then we're going to take these sums and divide them by 12. in the end we end up with uh in the first case we get a sample mean of 5.16 uh in the second case we get a sample mean of 5.416 uh repeating six uh in the third case we get 5.25 and in the fourth case we get 6.58 which is uh much different from what we had before uh hold on i think my pen might be okay my pen's back is it though might need to charge okay now the median in the first case uh so since uh so in the first case the median is still going to be five and in fact it's going to be five for the first three cases because the median was five and it didn't change the order of the data set if you go back and look at the original data set we're essentially in in these first three cases we shift all the numbers to uh the left which means that the median's gonna be uh the number to the left of what it was before which was five but if we chose or it's actually going to be the average of five and five which is still five but in the last case uh the median actually will be the average of five and six so it will be 5.5 so the median did change a little bit um in the last case but it's mostly because of where that outlier ended up it ended up on the right hand side of the data set or the right hand side of what the median was before and it could have been 273 and the median would be exactly the same but as we can see the median isn't changing really at all compare especially when you compare it to the mean and here is some r code that the idea of this code is i'm going through a loop adding this observation to a to a version of the data set and then uh computing the uh median and the mean of that data set the result will be an r matrix i take the transpose of that matrix because that's the version of the matrix that i prefer i'm giving the matrix some row names and column names do some rounding and this is a resulting matrix and it's pretty similar to what we had before in fact there is a relationship that we can say in general between the mean and the median depending on whether the data is negatively skewed positively skewed or symmetric if the data set is nic is a positively skewed so that would so the data set looks roughly like this then the me the median which is the point that devas divides the data set roughly in half will be less than the mean and that's because the mean is going to try to chase the outliers the outliers are going to be on the right hand side of the bulk of the data so in the case of negatively skewed data we're going to have the opposite relationship where the median since the outliers are going to be on the left hand side of the bulk of the data the mean is going to chase the smaller numbers so the me the median will tend to be greater than the mean and in the case of a symmetric data set the mean and the median should be approximately the same in real data i mean probabilistically when we talk about means and medians and probability uh they will be exactly the same when the date when the distribution of the data is symmetric but in real data that's never the case in real data they will just be close and what it means to be close is is like that's a judgment call right um so what would that some implications for that thinking back to some examples of positively skewed and negatively skewed data sets i said that a positively skewed data set is incomes this relationship means um this relationship means that the average income tends to be larger than the median income and economists generally prefer to use the median income for income distributions because it seems inappropriate to use the mean this kind of gets to the issue of i've given you these competing measures for means and for for me for measuring the location of a data set which one of these measures should you use i would say use the one that's appropriate which means um well here's one thing once you use the mean uh i would say you should use the mean when large observations are allowed to compensate for small ones let's say for example that you're gambling if you're gambling what you care about are your mean earnings and not your median earnings because it's okay for you to win nothing 99 times if you win a million or i guess it also depends on how how much this game is worth but let's say that you're playing a game that costs a dollar each time you play it's okay for you to win nothing 99 times if on the 100th time you win a million dollars that would be awesome for you whereas in that situation the median would be zero dollars and if you were judging by the median how well you were doing you would think you were actually doing poorly so if you're allowing large observations or small observations to to maybe uh replace or detract from the overall score then the mean is appropriate on the other hand the median in the case of uh social sciences we care more about what like what fifty percent of the obs of the population is experiencing and we don't necessarily want to allow uh like the very the the uh the um the great fortune of the wealthy to uh compensate for uh the great poverty of the very poor so in that situation the medium would be the preferred observation and of course if theoretically what you're trying to measure is the population median then you should use the sample median if you're trying to estimate the population mean you should use the sample mean and in which case you should not be using the opposite now there are some exceptions to this and we'll talk about this when we talk about probability and talk about um hypothesis testing there's notions such as most powerful tests like if your data set was symmetric and you knew it came from a normal distribution you should always use the mean even even if what you care about is the median uh but and the reason why that is is because for a normal distribution the two numbers are the same uh but um we're just going to leave that for now um so um and there so all of this was talking about uh sample results uh there are analogous population numbers too okay so um i just want to very quickly satisfy my nervousness okay we're still streaming okay um all right there is another number another measure for location called uh the trimmed mean so i'm going to comp i'm going to tell you about the trim bean and i'm even going to compute it for you but then i'm going to uh have some caveats about using it um you probably should not be using uh the trimmed mean and i'll explain that in a second but the idea of the trend mean is we have the stat we have the median and the median is not sensitive to outliers which is generally considered a blessing but it's not always a good thing because if it feels a little inappropriate to throw out so much data when you're computing the median because once you know the ordering of the data and you know which two which one or two numbers are in the middle then the other day the rest of the data doesn't matter and that feels a little extreme on the other hand you're a little bothered by the means sensitivity to outliers and furthermore on outliers you might think should we throw out outliers should we ignore outliers because that's actually kind of what the truth mean is suggesting that we should do with the trimmed mean what we're going to do is we're going to uh use only um uh we're going to throw out a certain percentage of the data like we're going to throw out 10 percent on the lefthand side and 10 on the righthand side of the data set so throw out the 10 10 percent of the smallest numbers and or the um so in this data set ten percent of the numbers that are the smallest numbers that the data set and ten percent of the numbers are the large larger numbers in the data set that would be the uh trend mean where alpha equals point one where you're trimming at ten 10 on each end um so the idea of the trend mean is throw out the outliers um so on this issue of whether you should throw out outliers you should actually think very carefully before you throw out outliers in general if you're competing a trimmed mean then that's kind of what you're doing but let's say you look at a data set and you see that your estimators are actually very influenced by a couple outliers and you're thinking maybe i should just throw those out i would first ask why are you throwing them out are you throwing it out because uh you think the number is erroneous because i in my own experience have seen numbers in data sets where it's like that's probably an error someone probably entered the wrong number so i'm going to throw it out um like when someone writes in the american community survey that someone's made a trillion dollars it's like that's probably not correct um you should just throw it out um if that's what's going on go ahead and throw it out because the reason why you're throwing it out is because of data contamination but if you're throwing it out just because it's causing bad behavior in your estimators that is probably inappropriate and you should instead try to model the outlier or just accept it rather than throw it out or think harder about why it is that you are using the mean rather than the median i would actually suggest that over throwing the outlier out but anyway um let's go ahead and compute uh the trimmed mean for the height data set so let's go ahead and rewrite that data set we've got uh i don't want blue okay so i've got for the height data set 5.05 you can go ahead and like skip ahead a little bit to skip me writing down numbers so 5.05 5.13 5.21 5.3 uh 5.3 this splits the data set in half so the next number is 5.38 and we've got 5.38 again 5.55 5.63 and 5.96 okay so there are 10 numbers in this data set if we're going to trim so if we're going to say alpha equals 0.1 so we're going to trim 10 of the data off on each end so trim ten percent at each end that means that we're going to end up trimming uh 0.1 times 10 where 10 is the sample size which is 1. we're going to trim off the smallest and the largest number in this data set so that would be that would be 5.05 and 5.96 and then take the average of the remaining data set so in that case x bar where we trim off uh 10 percent will be the average of the eight remaining numbers where we're sum from i equals two to nine uh the order data set um and that is going to end up being uh 42 so the sum is going to be 42.88 uh and then we divide that by eight and the result will be uh 5.36 so 5.36 feet because this data set is in feet we're we're talking about height okay and uh r can compute trimmed means in fact you can just use the mean function like we had before we're just going to pass it an additional parameter that tells the function to trim now i mentioned a little while back you actually probably should not be computing trimmed means and the reason why is because when we compute a median which i a a little side note i guess the median both the mean and the median count as particular trimmed means where the median is like the trend mean where you trim off 50 percent and the mean is where it is the trimmed beam where you turn off zero percent so the trend being kind of generalizes these other two statistics um but you probably should only use those other two statistics you should probably not use the trim mean i mean i guess if what you were doing is instead of taking off 10 on either end it's like always take off uh the two largest and two smallest numbers that could be appropriate um from a theoretical perspective uh so because basically as you increase the sample size the number of observations that you're trimming off becomes very small relative to the rest of the sample but trimming off ten percent at either end from a theoretical standing is a little odd and the reason why is that there is actually a population median that we are estimating when we are using a sample median and there is a population mean that we are estimating when we use a sample mean and both of those quantities are very well understood but when you're using a trend mean you are estimating neither of those things you're estimating some weird hybrid a monstrous monstrous population statistic that we don't necessarily understand you're estimating essentially the population version of a trimmed mean and it's questionable whether that's actually what you want it seems like the worst of both worlds in that case because no one can actually like why would we talk about the population except for the 10 largest and 10 smallest numbers like that doesn't really make a whole lot of sense so for that reason unless you are actually i would actually more advocate for like a fixed trimming where you take off the two largest and two smallest observations um although at that point you probably should just use the mean or use the median i probably would not use the trimmed mean so okay so that's it for this section in the next section we will be discussing uh measures of variability so um i will cut it off here and have a good day okay so this section is about measures of variability so last section we discussed measures of location let's start by justifying why we need measures of variability consider these three data sets and i'm going to construct a dot plot for each of these data sets so i've got uh three lines for my three dot plots data set one data set two data set three uh and in the these dot plots i'm going to uh start uh with one and it's ending the twelve and uh in between i've got six so so let's see i'm going to have we're going to keep these all on the same scale so 1 12 6 1 12. uh six and uh we'll go one two three four five six seven eight nine ten 12 okay that's one two three four oops five six okay that's that's that's that's a little inexcusable we're gonna have to try a little harder on that one uh one two three four 5 6 7 8 9 10 11 12. okay 1 2 3 4 5 6 7 8 9 10 11 12. okay so i've now got these three number lines and let's start with data set one so we've got numbers at uh so four five six seven eight and then we've got one at uh two five six seven ten and then finally we have one three six 9 9 and then 11. okay so look at these three dot plots now i want you to let's let's first start actually by uh computing the mean and the medium for each of these data sets so data set one four plus five plus six plus seven plus eight plus nine uh that is going to add up to 36 the second data set well we subtract 2 from 4 to get 2 but then add 2 to 8 to get 10. so that second one is also going to add up to 36. and for the third one you kind of are going to do the same trick so they all add up to 36 which then means that the sample mean is going to be 36 divided by there's six observations no actually there's five uh oh i'm sorry they don't add up to 36 they don't add up to 36. oh silly curtis silly curtis they added to 30 so the sampling would be 30 divided by 5 which equals 6 which is also equal to the median because you look at them because these data sets are ordered they have five observations so the third row is going to correspond to the median so that means that the mean and the median for these data sets are all the same and yet let's suppose now that i were to ask tell you that this was the waiting time for the train uh which of these data sets would you prefer to be the observed waiting times for the train probably the first one at least if you're like me because for myself i actually did not really like inconsistent trains i mean it's kind of cool that this train will there might be a oneminute waiting time uh for this train but there also could be an 11 minute waiting time for this train and one way or the other i would just love it if trains always showed up exactly six minutes um between which okay admittedly around here they generally do do that but um you don't really like a lot of variability in the wait time for the train because that makes the train unreliable that said this aspect of the data set is not being captured by uh our measures of location the sample mean and the sample median unfortunately so and the reason why is because the attribute that we're talking about is an attribute that doesn't have anything to do with the location these data sets are located at essentially the same place they it has to do instead with spread and we now have a pictographic method for understanding spread we can see that these data sets have different spread but we would like to have some numerical measures so very quickly i'm just going to say that i would prefer one because it's more consistent or less spread okay what we need is a measure of variability to describe how spread out a data set is uh how could we possibly do that well we might start by examining deviations which where we look at x i and subtract out the sample mean and and if we were to add these up together this might give us a measure for um how spread out the data set is here's the thing though when we try that um we're going to sum up from i equals 1 to n uh no that should be a 1. so from i equals 1 to n x i minus x bar and this is a sum sums are linear which means that i can now break up this sum into two sums and say that this is going to be a sum from i equals one to n x i minus um the sum from i equals one to n x bar but here's the thing about that ladder sum see this is actually adding up a constant n times and you probably remember from second grade what it means to add up the same number and times you end up with multiplication so this number is going to actually end up being n times x bar okay and this number also can be interpreted as n times x bar because it is the sum of the observations divided by the sample size and then multiplied by the sample size again so we end up with n x bar minus n x bar and that equals zero so what that means is that this quantity is always equal to zero always equal to zero i think i found a dead spot on my screen okay so that always adds up to zero uh which means i mean the issue is that these deviations they always have the same sign well okay the they they all have um they all right what i just said was literally false um they don't always have the same sign in fact uh you have opposite signs you have some positive some negative deviations and it turns out that uh the positive and negative deviations cancel each other out so you end up with a zero um so that didn't quite work although there was an interesting idea there um looking at the distance between an observation and the sample mean and one might be tempted to try this replace the parentheses with absolute values so you end up adding up the absolute value of x i minus x bar and um that now you don't have that issue of negatives and positives canceling each other out because everything will be positive and you'll end up with a positive number and that makes sense um the thing though is this is actually more difficult for a mathematical perspective to work with and the reason why is because it's involving absolute values and absolute values are not differentiable absolute values if you remember from calculus 1 they have a cusp of a sharp point and sharp points are not differentiable compare that instead to so this is like the absolute value of x compare that instead with the function x squared that should work i mean that that has the uh nice feature of being differentiable so what actually statisticians end up doing is they say we should add up the sum from i equals 1 to n x i minus x bar squared and this quantity is known as the sum of squared errors x i minus x bar a term that statisticians like to use for that is the error and um we're adding up the squared errors and there is in fact an interpretation for uh for the square part which is maybe you remember this is how i like to think of it i think that this formula kind of rhymes with x1 minus x2 squared plus y1 minus y2 do you remember that do you remember that from geometry class if you take the square root of this quantity you end up with the distance formula for euclidean distance for euclidean geometry and it actually kind of rhymes with that sum that i've drawn that i've uh shown uh up above so uh and in fact there is um a very deep connection between uh the sum of squared errors and euclidean geometry um but this quantity to me like it that seems like an appropriate way to think about distance and if we if we were to average this by by saying like this is one over n we would have an average squared distance now that's actually a good idea but um there's a better idea which is to divide instead of by n by n minus one now that might strike you as a little bit odd why is it that we're dividing by n minus one uh there's a few reasons for that some of which we'll talk about later in maybe chapter six but uh n minus 1 there's actually a term for this quantity and it's known in this in this context as the degrees of freedom and why are we dividing by the degrees of freedom rather than n i'm going to present to you a few arguments for why you'd want to do that for starters let's imagine that we had a data set of size 1. right so there's only one observation or data set what we're trying to measure right now is is a spread in the data set if we had a data set of size one is there really any way to estimate spread how can you determine the spread from a data set of one observation um that doesn't really make a whole lot of sense and it seems like something has fundamentally gone wrong in that situation and when you divide by n it's not going to reveal that something is wrong but when you divide by n minus 1 a sample size of 1 is explicitly forbidden because you cannot divide by 0. okay so that's one way to think that that's one way to think that maybe n minus one is more appropriate um and then uh uh secondly uh what we're actually doing with this number we actually call this we we've given this number a name as statisticians this is known as the variance the sample variance now maybe you recall from previous sections uh my saying that there is a sample meaning you can actually talk about a population mean and there's a sample median and you could talk about a population median and there is a sense in which the sample mean estimates the population mean and the media and the sample median estimates the population median so the sample variance should estimate the population variance there is in fact a population variance but here's the thing though about the population variance um our estimator if we were to divide by n would have a tendency to be too small i mean it would still be close to the population variance but you could be a little bit better by dividing by n minus 1 instead of n if you were to divide by n you'd actually be a little too small so we should divide by minus 1 instead there's a term called biasedness that we will discuss more in chapter six but long story short it turns out that when you divide by n minus one you have an unbiased estimator for the population variance whereas if you divide by n there is a very small bias now that said you can still divide by n and have a reasonable estimator it just will have that bias problem the bias gets really small as you increase the sample size but it is still there so why not just get rid of it um so these are some potential arguments for why you should be dividing by n minus one and later on in ch in that chapter we will actually uh i may actually show that if you compute the expected value and you divide by n minus one you get the sample variance but uh we're a long ways off from that so accept it that you pretty much have to divide by n minus one instead of and although it is still reasonable to think of this as um uh like an average squared distance oh yes another argument for why you should uh be dividing by n minus one uh when you have when you compute the variance there's something you have to do first you have to compute the sample mean you have to compute the sample mean first and there's a penalty that you have to pay for that the term degrees of freedom means that if you this is basically the number of observations in the data set that you are allowed to change um and where you can change those observations um freely and you could still end up with the same sample mean um it because it turns out for basically the reason that i showed up here uh this this this line of reasoning that um if you know n minus one of the observations and you know the sample mean then you know the nth observation that you didn't list out before so the sample mean contains information and the fact that you had to estimate a parameter before you could estimate the sample variance means that you need to divide that it means that there's in some sense a penalty to your sample size um so it's inappropriate to divide by n minus one you now need to do or to divide by n you now need to divide by n minus one all right now here's the thing about the sample variants that we don't like uh think about the units of these things let's say that we were talking about feet or going back to some examples or even for this uh soccer data set that i've seen in a few videos in the past where we're tracking the goals scored by a little league soccer team uh if we were talking about goals then this right here is a goal for a game so it's units or goals the sample mean is also in goals because you add up goals divided by something without units you end up with goals and you have goals minus goals so you still in that difference have goals but then you square and you end up with goals squared what the heck are skull are goals squared that's a unit that doesn't mean anything to us we don't like the fact that in the end the sample variance produces squared units we would rather have an s some measure of spread that is in the same units as the data set and there is such a measure called the sample standard deviation so the sample standard deviation is s which is equal to the square root of s squared so s squared is the sample variance s is the sampled standard deviation so we'll call that sd right i mean it's right there so uh the sample standard deviation so yes since you've taken the square root of the variance uh you now take the square root of gold squared and now end up the with the unit goals which is what you want so the sample standard deviation will be in the same units as the data set and we like that um furthermore it is still reasonable to think of the sample standard deviation as measuring the average distance of an observation from the mean or a typical distance all right so continue on uh there are in fact population analogs to these quantities uh such as the um such as uh the population variance and the population standard deviation and those will be discussions for a later chapter i believe that's chapter uh three so um now when you're computing the sample variance another way to write it if we write if we define s x x as the sum from y equals 1 to n of x i minus x bar squared we could say that the sample variance which is what you need to compute for the sample mean is equal to s x x divided by n minus 1. the thing though is a lot of people don't like to compute the deviations and then square them so compute the mean and then compute the deviations and by subtracting the mean from the observations and squaring them people don't seem to like to do that so when doing stuff by hand it's often easier to use this shortcut formula where you add up the observations squared and then subtract uh the mean squared multiplied with n and it is in fact possible to show and because this is the second time i'm recording this video i'm not going to show it because i'm tired um it is possible to show that these two quantities are the same and i would say i'm going to leave this as an exercise to you if you are curious if you're uh thinking you're probably going to take some more advanced stats classes why don't you take a second to show uh that these two quantities uh the uh some the sum of squared errors and the shortcut formula are the same um but i'm just going to leave it for now that these are in fact the same number so that gives that could help you potentially save some time when computing the sample variance by hand okay so uh let's start out let's now start looking at examples in example 14 we're going to compute the sample variance and sample standard deviation of the soccer game scores so here are the scores let's set again uh length is an r function uh length of so sock or in r is known as a vector and the length of the v of a vector will tell you how many objects are in that vector so um there's also an r function called summary which will give you uh some basic statistical summaries for a data set stored uh in a vector or actually summary is something we'll give you some basic statistical information about lots of things uh but that we're going to leave that for the r lab for now it's just giving us some basic statistics for an a vector and i'm now going to compute uh the sample variance of this uh data set okay so uh i like to create a table uh when computing this by hand if you don't want to watch me compute this by hand uh because it is kind of a tedious calculation if you don't want to watch it this is a part that you can skip over um all right anyway so um we have 12 observations in our data set so i'm going to start numbering off 1 2 3 4 5 6 just to track the observations 7 8 9 10 11 12. okay we have an observation and we have an observation uh hold on another dead spot we have an observation squared okay so uh observations in our data set we had uh nine six five five five uh six two eight uh three four eight one and then we're going to square each of these observations so we'll get 81 36 25 three times uh 36 for uh 64 9 16 64 and one okay um if you're out also like you kind of want to work on this by hand a little bit but you don't want to completely trivialize the problem by going to r and asking for the variance and standard deviation because it'll just give it to you uh maybe this would be something to work on in excel because i because what i'm basically doing is being a a human excel spreadsheet at the very bottom i'm going to sum up these two columns the first column sums up to 62 and the second column consisting of squares sums up to 386. so now i want to compute the sum of squared errors and that's sxx and we have our shortcut formula for computing that that's going to be 386 which is the sum of the squares of the observations minus 12 which is the sample size times the mean all right we need to compute the mean so the mean is going to be the sum of the observations which is 62 divided by 12. so in this parentheses i'm going to put 62 over 12 and square it and you plug into calculator and the number that you get for the sum of squared errors is 197 divided by 3 which is as a decimal number uh 65.6 where the six is repeating so the sample variance will be the sum of squared errors divided by as a reminder the sample size minus 1 which is going to be 65.6 divided by 12 minus 1 which is 11 which is equal to 5.99 where the 9 6 itself is repeating now this is nice but the thing is the sample variance is in the units of the sample variance is gold squared we don't like that we want to compute the standard deviation too because that's a more interpretable number so the standard deviation is going to be the square root of the variance which is going to be about uh 2.443 into three decimal places so you can think of this as your uh daughter's soccer team is varying around their average uh score of about five points but they're they're deviating from that by about two points so on average they'll be about two points away okay all right uh so in r the functions that are responsible for computing these quantities are var and sd r computes the variance and sd computes the standard deviation so var of the soccer data set is basically what i wrote down and the standard deviation it's basically the same thing too all right so um the sample mean so for the sample variance we actually have some nice properties that also translate into properties for uh the standard deviation actually before i continue on i'm going to double check because this scares me okay okay everything is good i'm scared okay um so uh some basic properties uh let's suppose for in this proposition uh let's suppose that we take our data set and then we shift everything by a constant to produce a new data set that's shifted by a constant it turns out that the set the uh sample variance for the new set data set will be the same as the sample variance for the old data set where you didn't shift uh that's a good thing what that means is basically this is in fact a measure of spread if it wasn't a measure of spread uh well basically if this was not the case if the sample variance changed uh by shifting the data then it doesn't seem really fair to uh call it a measure of of spread because it's also capturing location too but the fact that you don't have to uh the fact that it doesn't care about the location or in a way it doesn't actually care about the mean because and you can think of that as because it subtracts the means out the mean out for the data set uh the fact that it doesn't care means it is in fact a bona fide measure of spread and not measuring something else along with it the second proposition says that the variance uh if you were to rescale your data set by c uh the variance will scale by c squared and the standard deviation will scale by the absolute value of c so um the standard deviation is always going to be positive uh the variance will always be positive um so when you rescale the data set it's not going to the whether you multiply by a positive or a negative number doesn't actually matter and also it tells you that um if like you can think of this as saying something about uh unit conversions uh because remember that unit conversions uh generally and are multiplicative operations if you wanted to change the units of the standard deviation um you could do so by just multiplying the original standard deviation by whatever unit conversion formula you have and also this is basically telling us what i was saying before that the variance is in units squared but the standard deviation is in just the same units as the data set so these are good properties to be aware of um okay so the uh the sample variance and standard deviation uh these are one uh these are these are these are one class of uh estimator of spread they're not the only ones oh by the way you we did have this discussion when talking about measures of location about biasedness no no not biasedness um sensitivity to outliers it turns out that the sample mean and the sample standard the sample variance and sample standard deviation are also sensitive to outliers in fact they are more sensitive to outliers than the mean is so they care a great deal about outliers too um just throwing that out there and you can kind of tell by looking at those formulas since uh when you look at them they're basically means right they're averages or at least the variance looks like an average and the standard deviation is the square root of an average so uh if you end up having a very large error then that's going to make your variance very large so it's sensitive to outliers just mentioning that uh another measure for spread is known as the fourth spread or sometimes like in math 1070 we call the interquartile range it is the third quartile minus the first quartile and we're going to denote it in this class with fs this is another measure of dispersion so let's compute the fourth spread for the soccer game scores uh we already computed in uh in a previous uh video uh the third and first quartile for the soccer game for the soccer games so the fourth spread will be the third quartile minus the first quartile which actually turns out for this data set to be eight minus four which is equal to four okay so the fourth spread is in and of itself a measure of dispersion uh and one way statisticians might use the fourth spread is as a tool for outlier detection remember we care a great deal about outliers uh if there's an out we would like to have outlier detection tools because if there is an outlier in this data set we would like to investigate it further and decide how we should approach it and why the outlier is there outliers are very interesting aspects of data sets so we would like to be able to detect them so we might call an observation that is further than one and a half times the fourth spread from its nearest quartile a mild outlier so as an example uh let's suppose we have a data set our data set looks something like this here's kind of a a dot plot sketch of our data set and we've also got a couple observations over here so those two observations visually look like outliers what the for let's suppose that we have the uh oh i didn't realize that was the thing okay uh let's suppose that uh our first and third quartiles are here and here okay um if that is the case uh the fourth spread or the iqr is going to be the distance between uh those two quartiles so we'll call this q1 q3 uh the distance between those will be the fourth spread so according to this rule how you detect an outlier is you take this quantity and then increase it by uh one so one and a half and go beyond and you're gonna like see compare an observation to its nearest quartile so these are going to be close to the third quartile because they're above the third quartile um and you um compare see if those observations are one and a half times the iqr away from their nearest quartile and if they are beyond that range then they are candidates to be outliers so these are now starting to look like at least mild outliers but in fact if we were to uh double that quantity so three times the fourth spread um so that would look like this turns out that they are beyond three times the fourth spread as well so now these are looking like extreme outliers we could also do the same thing on the left hand side of the data set look for outliers that are you know numbers that are really small there's nothing that says that outliers have to always be large numbers they can also be really small numbers too uh and there's no outliers on the lefthand side of this data set i should point out it's tempting for students to think that what i just gave you as a definition for outliers the word outlier is intentionally vague because there's many different ways you could define an outlier this is one such definition or one such a criterion for deciding if something is an outlier this criterion would not work if we were to go into two dimensions it is a onedimensional onedimensional approach not a twodimensional approach and you can still have uh outliers in two dimensions or in bivariate data and uh um you can still have outliers there and they're going to behave like there's more possibilities the moment you've moved on to a plane as opposed to just a number line more ways for things to be outliers um so um i i'm i'm i'm hesitant to allow to just let students think that this is what an outlier is there's actually different ways to think about outliers we could come up with different definitions based off of our problem and how we want to approach it so so we could we could choose a procedure that is tailored to our problem to define outliers so that it's most useful to us so this is what we're using in this class but it is far from like what we'd always use okay and it's not really the definition of an outlier we would just say that an outlier is a point that that seems unusual to the other points that seems to be distant in some way from the other points or it doesn't seem to follow the same pattern as the rest of the data okay so moving on into example 16 use the fourth spread to detect outliers and soccer game scores what is the minimum score needed for a data point to be a mile outlier or an extreme outlier so the force spread as you may recall from above was four so 1.5 and i don't want that green that color all right so 1.5 times the fourth spread is going to be 4 times 1.5 which is going to be 6 and 3 times the fourth spread is going to be 12. so uh let's see what it would take for something to be considered at least a mild outlier to be mild you would have to possibly exceed the third quartile plus 1.5 times the fourth spread which is going to be eight plus six which is 14. there were no double digit scores in our soccer data set so that means that there were no outliers at least on the positive end and as for the negative end uh in order for something to be so small that it's an hour it would have to be less than q1 minus uh 1.5 times the force spread which is going to be uh that's going to be 4 minus 6 which is negative 2. well negative soccer scores are impossible so there's not going to be any mild outliers on the left hand side which means that there are no outliers in this data set now that's it let's let's go ahead and continue on just just for fun let's see what it would take for something to be an extreme outlier to be an extreme outlier you would have to exceed q3 plus 3fs which is 8 plus 12 which is uh 20. so in other words the other team didn't show up to be on the left hand side you'd uh to be an outlier on the lefthand side you have to be less than q1 minus three fs which is um uh four minus 12 uh which is negative which is negative eight and no way that's going to happen so there are no outliers in our data set okay so uh there's another visualization method that i would like to discuss that we weren't able to discuss in section two the reason why is because this is a box plot and box plots require uh the five number summary which is the minimum max so the five number summary is the minimum uh maximum median first and third quartiles so you need to have that in order to be able to compute a box plot and create a box plot that's why we didn't talk about before because we hadn't actually talked about those things uh so um so for a box plot we first compute those quantities uh on a number line which could this could be a vertical number line or it could be a horizontal number line if you want uh box plots oriented horizontally or oriented vertically either one is fine whatever whatever suits your needs do whatever you want okay but on a number line let's say something like this we're going to draw a box so we've got the first quartile and the third quartile we're going to draw a box whose ends are at those quartiles we're also going to draw a line in that box corresponding to the location of the median we will then draw what are known as whiskers that extend out to the maximum and to the minimum so the whiskers will stand out to extend out to the extrema of the data set um and that's that's a box plot now i should point out that r does not draw a box plus this way by default r does something different r will actually try to detect outliers and it will draw the whiskers out to the observations that are not outliers so the largest observation that is on outlier and the smallest observation that is not an outlier the outliers are treated differently they get their own points in the box plot kind of like with the dot plot uh that's more complicated to do and i'm not going to ask you to do that uh using just the five number summary and extending out to the maxima is fine for me and i feel like that if you were ever to draw a box plot by hand you should just keep things simple because you can it's not too hard to compute a five number summary if you especially if you had say um uh if you had like a stemandleaf plot but a box plot like competing the outliers is a bit much so a box plot on its own i mean it's okay but a box plot really shines when there are other box plots with it and when you have that you can now start drawing comparative box plots and when you have comparative box plots you can start to say things about the relationships amongst different groups that uh like if you want to compare different data sets you may have a data you may have two data sets for two similar but not the same populations uh like for example men and women men and women are both human uh but if you were to compare height you would probably want to differentiate between men and women so you could have a box plot for men's heights and you can have a box plot for women's heights and then you can make comparisons and you could compare in this case this looks like uh if if this were in fact talking about men's and women's heights this would be suggesting that men tend to be taller than women um so one thing that you could do when looking at a comparative box plot is compare look the location of the boxes you can also compare the spreads of the boxes so we for example if we saw one box plot that looks like this for one group and a box plot that looks like this for another group we might say that those two groups certainly have different spread okay so you can start making comparisons that are more easily made with with plots such as these and if you were to say try to overlap density plots or something um so comparative box plots are very nice because they allow you to at a glance compare two different groups and their distributions so let's go ahead and start creating some box plots uh in this example uh we're using a data set from r uh in this example we're studying we're studying the tooth growth of guinea pigs that were given a vitamin c supplement via orange juice at three different dosage levels uh here's a bunch of r code that takes the tooth growth data set and transforms it into a format that is um uh nice for this problem where uh it did not look like this before it go ahead and look at the tooth growth data it does not look like this at all for starters the tooth growth data set it also includes a group where the guinea pigs were given vitamin c like a vitamin c supplement directly rather than through via orange juice and we've completely excluded that group using this filter command these are known as pipes they are part of the dipler package um so we um filtered so that we were looking at only orange juice we selected the length and the dosage uh as the variables we were interested in and then did some other stuff so that the data came in a format that i liked which is where it's ordered from smallest to largest for each of these three groups and we have the half dosage group full dosage group and double dosage group okay so uh all right so and the data set is ordered which means it's going to be uh somewhat easy to compute a five number summary for each of these three groups so for example so for instance uh the last row is going to be the maximum for the three groups and the first row is going to be the minimum for the three groups as for the other quantities we also need the median so there are 10 observations for each group so the median is going to be the average of the fifth and the sixth rows so uh the medians after we compute those averages or or midpoints if you prefer the medians for the half dosage group uh its median is uh 12.25 for the full dosage group it's going to be 23.45 and for the double dosage group it's going to be 25.95 okay so those are the medians now we need to compute the first quartile remember what we do is we split the data set in half and then look at the median for the smaller data set or the lower data set this will be the first quartiles and the median for the third quarter data set remember that both these data sets have five observations each after we do the splitting so the median for the upper data set that will be the third quartile okay so we now have everything we need to start constructing our box plots okay so i'm going to construct these this by hand i don't want that let's make it black isn't that a song okay oh yeah i want a painted black yeah that's right that is a song all right um i want to paint it black uh anyway uh so um i have the half dosage group the full dosage group and a the double dosage group i'm going to have my box plot end at 31 up here and we're gonna start down here at eight so we're gonna go eight uh nine ten eleven twelve 13. okay uh 14 15 16 17 18. and then we go 1920. uh 21 22 23 24 25 uh 26 27 28 29 30 31 okay all right so we've got our scale uh so for our first group uh let's zoom out a little bit so for the first group the median of the minimum was 8.2 so minimum of 8.2 we'll put a dot right there uh then we go to the first quartile so that's going to be 9.7 that's about there um then we've got 12.25 that's about there uh q3 is 16.5 that's about 13. that's about there and finally the maximum is 21.5 so that is going to be about there okay so then uh we have a box the median and the whiskers all right so there's our first box plot all right so now for full dosage uh scrolling up a little bit so for full dosage the minimum is at uh 14.5 which is about there uh then we have a quartile at 20 which is about there uh the median's at 23.445 so that's about there uh q3 is at 25.8 so that's about there um and then uh the maximum is at 27.3 so that's about there okay so draw the box the line for the median and draw out the whiskers okay and finally for the double dosage group the minimum is at 22.5 which is about there uh the first quartile is at 24.5 which is about there uh the median is at 25.95 which is about there uh the third quartile is at 27.3 which is about uh there and the maximum is at uh 30.9 which is about there so draw the box draw the line for the median extend out the whiskers and there we go all right and now we have a box plot a comparative box plot and what can we see we can see that in fact increasing the dosage does seem to increase the tooth growth length we also see that there's much more spread in the half and full dose than there is for the double dose which is an interesting fact uh all right so moving on uh there is um an r function called box plot that can construct these box plots for you this is largely in agreement with what we drew um and as a reminder r doesn't by default uh produce box plots in the way that i just described where it draws whiskers out to the minimum and the maximum it does something a little bit different where it will um uh draw it out to the largest and smallest observations that are not outliers and then draw the outliers as their own individual points okay uh there's actually one more visualization that's similar to a box plot that i would like to discuss um i didn't discuss it in the lecture notes because you can't really draw it by hand uh but i've got a computer in front of me okay very quickly okay everything seems to be fine okay um so you can't really draw it by hand uh because oh what was that hey students let's get started with the chapter on probability probability is the mathematical study of randomness and uncertain outcomes so the subject in fact may be about as old as calculus at some level humans have known about probability for a very long time it's just it wasn't until around the time of probability that we saw some of the first uh semirigorous treatments of probability and then probability really became a serious mathematical subject around uh the beginning of the 20th 20th century uh when a mathematician by the name of kolmogorov rooted probability theory in in the in some real analysis theory so he developed a set of axioms that made it a rigorous uh mathematical subject in its own right and here we are today and statistics relies very heavily on probability we've seen in the previous chapter quantities such as the mean and the median we saw all these sample statistics we discussed the ideas of a sample and a sample's relationship to a population but it's hard you can't really say much more than that and really can't have a rigorous discussion about different uh sample statistics without having a probability theory to back it up so we're gonna start with that right now we're gonna start with section one on sample spaces and events so we start out with the idea of an experiment and experiment is an activity or process with an uncertain outcome examples of experiments including flipping a coin or flipping a coin until the coin lands heads up or you could have rolling a die a six sided die or a rolling two six sided die or you could even have something a bit more abstract such as uh the time in the morning that you wake up that can also be understood via probability theory so when we have an experiment that we have described narratively in a sense so i say i'm going to flip a coin or i'm going to flip a coin until it lands heads up after we have an experiment we need to describe the sample space which we are going to denote in this class with the letter s although i should point out that at least in my experience omega the greek letter omega is more common um notation for the sample space um but this is fine uh s is fine um so this will be the sample space is the set of all possible outcomes of the experiment the sample space is defined by the person who's developing this probability model so it basically you say what the sample space is and you're going to pick a sample space that seems appropriate to the phenomena that you wish to describe a set is very loosely defined as a collection of of objects uh actually this definition of set is bad um because it's possible using just the idea of a collection of objects to construct impossible sets uh sets that are like it's impossible in the sense of being contradictory to itself so uh there's this uh area of mathematics called axiomatic set theory that actually develops a rigorous notion of sets that largely allows for sets that we'd like to think of but honestly uh for our purposes this is f this is definitely overkill uh just thinking of set as a collection of objects is uh fine for us events are subsets of the sample space defining possible outcomes of an experiment we automatically get an event called or a subset called the empty set or the null event uh which is noted with this notation this is a set with no members it can be thought of as an event of as the event where nothing happens and that is precisely how you should think about it i might uh create a separate video describing what precisely the empty set is and kind of try to dispel some inclinations of students to try to assign some deeper meaning to what to the empty set it's like no no no the empty set is a set with nothing in it and you really cannot call it anything else it's more uh a necessity of the mathematics than it is anything that you can honestly interpret so let's get started uh with an example we're going to define a sample space for the experiment of flipping a coin we're going to list all possible events for this experiment let me just get caught up in my notes uh that i have a side here and uh all right so i'm going to say uh that this sample space uh which i'm going to call s uh what are going to be the possible outcomes of flipping a coin well despite what might be physically possible like i actually have seen coins uh not not necessarily like mint coins uh but things very coin like that end up landing on their side but that is not going to be allowed here there's only two possible outcomes heads and tails and notice that notice the curly braces often sets what we are talking about is a set generally sets are going to be denoted with uh curly braces another important fact about sets is that the objects in sets only appear once generally if you were so like for example this set is the same as h h uh t so at at some level there's uniqueness in a set you get imposed uniqueness so if you list heads twice it's the same set okay um and additionally the ordering of how i write stuff down in a set does not matter so i could have written tails heads and it would have been the exact same set here so but yeah now we have the sample space and this is what it is by definition i decided this is the sample space for my experiment and i decided this because i believe that this sample space is going to be the appropriate sample space uh for my problem so i'm saying that there's two possible outcomes of this experiment you the coin either lands heads up or it lands tails up uh and i so next i'm going to list some possible events for this experiment so an event is a subset of the sample space okay so what is one possible subset uh well one possible subset is the sec that contains only h right so only heads so this is the event or the subset where when you flip the coin it lands heads up and similarly we have the event where it lands tails up and some authors like to call um sets like these sets with only one element simple events because they have uh only one outcome even komogorov in his book on probability theory uh denoted uh had the notion of simple events where it has only one outcome corresponding to something that you would actually observe um i i i personally don't really care for the distinction myself uh but students might like it uh we can also have the event heads or tails so what's a what is a possible outcome for this well we get get heads or tails basically um and notice right here that this is the same as the sample space so i could have said the sample space is an event and generally that's true because what does it mean for a set to be a subset of another set what does it mean for for something to be a subset it means that every element in a set is present in another set or equivalently there are no elements in the subset that are not present in let's call it the parent set right so equivalently you cannot find an element in the subset that isn't present in this uh containing set okay so by that definition the sample space is a subset of itself since every element in the sample space is also present in the sample space so it seems almost tautologically true and yet at the same time it matters it matters a great deal that one set that you automatically get when you de one event you automatically get we need to find a sample space is the sample space itself and there is one more event that we have the moment we define the sample space the empty set that is also a subset of the sample space now it seems really weird because you ask yourself how is it that a set with nothing in it another way to write the sample space is like this where you write two curly braces but with nothing in between them because the empty set has nothing in it okay um so you ask yourself how is it that the sample space every element of the sample space is also every element of the empty set is also in the sample space it has no elements well exactly because by this alternative way to think about what it takes to be a subset there is nothing in the empty set that isn't present in the sample space because there's nothing in the empty set therefore you automatically get that you automatically get that the empty set is a subset of the sample space and therefore the empty space is an event now i i i'm kind of implying here that what it takes for something to be an event is that this set needs to be a subset of the sample space so in other words what it takes to be an event is that you simply be a subs of the sample space technically that is not true but the reason why it's not true is going well beyond the scope of this class and uh you might see a little bit of it in probability theory and it would become much more important if you were to take graduate level probability theory measure theoretic probability theory technically it is not true that every subset um of the sample space is an event that said it's really hard to imagine a subset that is a one so basically if you imagine the subset and you didn't actually try to break the theory if you imagine a subset it's probably an event so uh it's for now it's probably fine although i'll probably add a little more rigor to the notion of what it takes to be an event um or do i do that only in like a class devoted or probability theory i'm not really sure if i talk about in this class um we'll see we'll see we'll have to see as we go through the notes all right uh so that's that um by the way i should probably uh mention something uh here we might give names to these events like we might call this first event eh we might call this second uh simple event e t uh to say that one sub one set is a subset of another we can use the notation say uh e h uh is a subset uh and i like to put a line underneath um so to say that it could possibly be the same as uh the sample space so we have this or ah i mean a lot of people also will just write it like this uh so we set have that the event e h is a subset of the sample space e t is a subset of the sample space um s is a subset of itself and the empty set is a subset of the sample space okay so uh continuing on next example define a sample space for the experiment of rolling a sixsided die list three events uh based on this sample space okay so i'm going to say because i think this is the best way to understand this problem uh that it's going to consist of outcomes uh one two three four five six but i'm not going to write the numbers one two three four five six and i have and i have reasons for not writing the numbers the reason is there is nothing in probability theory that requires that your sets contain numbers nothing says that it just says some object very loosely defined so instead of writing numbers i'm going to write little dice faces so i have a dice face that has one pip a dice face that has two pips a dice face that has three pips a dice face that has four pips a dice face that has five pips and finally a dice face that has six pips okay that that looks like six pips all right so that's my sample space and now i'm going to list three events based on this sample space so one event uh one event is the empty set i'm just gonna say it right now the empty set is this upset so here we go we get one we get one event for free i've promised myself i'm not gonna do that here that's my preferred notation i'm not sure if it's the notation used in the book and not everybody uses the same notation that's something that you need to get used to in mathematics you need to pay attention to what notation someone is using because despite the fact that the books and sometimes the instructors make it look like there's one set of notation uh for a subject that's just not true and that's and that's including probability theory and statistics it's just not true that there is one set of notation and you need to pay attention to what someone is actually using to mean their stuff um anyway um so the empty set is a subset of the sample space the sample space is a subset of the sample space both of these are uh valid events okay but they're almost trivial at this point because these are events that you automatically get so what's something that's a little bit more interesting uh well we could have the event where you roll a four uh this is one of those simple events that you may have heard of so four is an event and it's a subset of the sample space uh what's another one that we could have well we could have the event where uh you have an even number of pips that's a valid event uh because in fact even though i've written this down in english and often it's useful to write down sets in english sentences what this actually translates into is a set with three elements you have the set containing the dice with a with two pips on its face uh four pips on its face and six pips on its face these are all uh come on you come on i said six pips don't make me a fool you stupid laptop some people there we go so this is also a subset of the sample space this is also an event um okay so um there we go i've given you four events in fact uh so you got a little bit more than what you paid for anyway uh example three define a sample space describing the event the experiment of flipping a coin until it lands heads up list five events from this sample space ooh so what does this look like what does this look like well i'm going to say here's my sample space and uh well what's one possibility uh flip a coin until it lands heads up well it could land heads on the first flip so you flip the coin it lands heads and then you stop uh you could then fl you could another outcome is you flip the coin and it lands tails so you haven't gotten heavy yet so you need to flip it again and then it lands up heads the second time so tails heads is another possible possibility uh tails tails heads is a third possibility tails tails tails heads is yet another possibility and so on this set has an infinite number of elements because in principle if i have if i have an outcome where uh where you've got so many tails and the last one is ahead it's possible to also have an outcome where you where in order where before you got to that point you flipped the coin you got tails once so for every outcome you can find a next outcome in a sense so since there's always going to be an x outcome this set must have an infinite number of members now there is actually an interesting wrinkle that co that came up in one of my lectures on this is there an outcome in this sample space that corresponds to flipping the coin and it never comes up with heads the answer is no the answer is in this probability model it is impossible for the coin to land to never get heads because i never described an element in this sample space where the coin just where you just flip the coin forever because you never get heads it is explicitly forbidden in this probability model since you cannot find an outcome corresponding to it so therefore since there is no such outcome where the coin never where you never stop flipping the coin it is literally impossible in this probability model it's a very subtle point it seems like you should have that outcome in this but in fact you don't um it it's just and the reason why being i didn't define this sample space to allow for that possibility since there isn't a possibility that corresponds to it it simply doesn't exist right although it seems a little unfair because we can imagine a universe in which someone flips a coin and they never get tails and they never get heads forever it does seem like it's a possibility but it is explicitly forbidden in this probability model since it is not in the universe of possibilities um and by the way it is different from something being impossible and improbable improbable will probably mean uh when you define a probability model for this the probability of something happening is zero which seems realistic to say for flipping a coin until it where you flip a coin forever and uh you never get heads it seems like it's reasonable to say it's improbable but not impossible but right now it is literally impossible since there is no outcome that corresponds to that we would have to add a separate element which if we really wanted to we could add maybe the infinity element to our probability model to represent the outcome where you flip the coin forever um and by the way none of this says anything about what the probability of these events or these outcomes are i have no notion of probably at this point this is all set theory right so it seems like you would say that the probably that you flipped the coin forever is zero but i've i have said nothing about probability so far now that said that is a complication that we are going to leave out we are not going to consider it any further uh we are just going to stick with this probability model uh maybe i would revisit uh this notion of flipping the coin forever uh in a later lecture but that that's it for now uh let's list five events from this sample space well what's one event one event is the sam what is the empty set why didn't i listen that because i'm bored well no not because i'm bored because i'm lazy um another one is um let's say uh you've you can maybe flip the coin exactly three times that were that would correspond to tails tails heads okay this is a possible event um that seems almost like a triviality we could have the event where you flip the coin um at uh at most let's say three times because i don't want to write too much all right at most three times so at most three flips what would this correspond to like this is in words but in fact i can translate in that into a collection of outcomes well you could have flipped the coin only once that's at that's that's no more than three uh you could have flipped twice that's no more than three but if you flip twice and that means that the first flip was tails and uh if you flip three times exactly then you got two tails and a head so we could have tails tails heads and this would be the event uh what's another event we could say um well another possible event would be uh in words um at least uh three flips that seems reasonable what would that translate into that would be the event where you flip it three times because it's at least three flips so tails tails heads and tails tails tails heads has at least three flips since it has four flips and tails tails tails tails heads has five flips so that counts and in fact there's an infinite number of such flips okay uh and one final uh possibility is an even number of flips that's that's possible what would that look like if we were actually using the elements uh of the set to describe it that would be the event where you have tails heads that has an even number of flips since it has two tails tails tails heads has an even number of flips uh tails tails tails tails tails heads that also counts since it has six flips and so on this event also has an infinite number of outcomes so you can see here it's perfectly possible to talk about a probability model that has an infinite number of outcomes um in fact such models are quite common uh and in fact this particular model where i'm flipping a coin until i get heads as an as an instructor i really like this model because uh it's it's um it's not too difficult at least in my opinion to understand what is going on the idea of flipping a coin until you get heads it's a perfectly reasonable thing to think about and yet at the same time despite its apparent simplicity it is actually a quite rich probability model and makes a lot of points about probability theory so i'll probably be revisiting this one it like it's it's simple but it can very easily get out of hand in a way uh you you can start it can get quite complicated when you start analyzing it uh probabilistically and the mathematics themselves like uh a um like students at this level can't understand it but it's also starting to push their knowledge a little bit and it starts requiring some trickier uh calculations to do all right i'm just going to check something that's not what i wanted yeah we're still streaming okay all right uh example four define a sample space describing the experiment of rolling two sixsided die simultaneously list three events from this sample space uh all right two six sided die simultaneously what would that look like well it's tempting to say that this sample space consists of the numbers two through twelve but that's actually not what we should use the reason why is probably what you're thinking is i'm adding the two the pips on the two dice together but i never said that i never said that there was going to be um addition of two pips so we're not going to do that because you can define a number of uh potential outcomes like maybe instead of uh combining the two pipes together you're taking the larger of the two pips something like that um or the smaller of the two pips so we don't want to define our our sample space that way um what's another thing that we probably should do well when developing such a probability model it's generally better to imagine that you are actually rolling two distinct dice reason why is because when you think of it that way you end up with more appropriate mathematics so it's actually better to think of this sample space uh i keep doing that it's better to keep it's better think of this sample space as consisting of rolling a red dye and a blue dye so that's what we're going to do and just uh to just for my own sanity i try to draw things out as a table so as an example we have a blue dye or we'll call it the left eye and we have a red dye so we have an outcome where the blue dye comes up with a one and the red dye comes up with a two we could also have an outcome where the blue die comes up with a one and the red die comes up with a two i think i might have said the wrong thing a second ago but whatever and we can have an outcome where the blue die comes up with a one and the red die comes up with a three and we would continue on with this i'm not gonna i'm not going to list out everything because i got better ways to spend my day i'm going to say that in this first row uh the last element is where the blue die comes up with the one and the red die comes up uh with a six all right uh so for the next row in the next row we'll have the blue die comes up with a two and the red die comes up with a one uh so we'll just say that uh everything in the blue second row the blue die will be a two so we'll just start out uh listing some of those outcomes and then let's uh and in the third row the blue die will be uh three and we in our very last row we would have the blue die is a six so six six six and finally six all right so i probably should still write down my red die so i'm going to write down my red die uh so we got one and one two two uh two three excuse me i have somewhat of a cough three three and uh six ah you failure you failure of a computer so six uh six and finally six ah for goodness sakes like i said really cheap computer six all right there we go i'm satisfied uh we'll just kind of tidy stuff up put some ellipses um i like things to be rather organized and while we're at it we'll put some commas too although i at this point i'm not really sure if the commas matter all that much but this is a set this is containing a bunch of stuff this is what our set contains um so just uh so how many how many elements are in this set um well this set contains 36 elements since you have six possibilities for the blue dice and six possibility for the red dice so there's 36 things in this sample space which using it this way comes up with more appropriate probability models because this is would be a model where uh if you were thinking about adding up the dice uh a sum of two would be less likely than a sum of three because there's three ways to get the dice to add up to three but only no there's two ways to get the dice to add up to three but there's only one way for the dice to add up to two uh so and that's and that's more appropriate for our probability model and we can start listing some events from this sample space i'm going to list one event uh one of one of them from this sample space uh and it will be the empty set because i'm i'm i'm tired uh e2 another uh event would be the sample space itself so anything happens uh again i'm just really lazy right now all right now i actually have to start writing down some real events well i mean those were real events but uh something that's a bit more interesting um let's see e3 what can we do for e3 well we do have the outcome where we roll um six for the blue die and uh one for the red die why why why this event because why not this event it is an event uh what's a fourth event let's let's start to get a little bit more creative we'll say that the fourth event will be um an even number of total pips total suggesting that you're adding the pips together so all right so let's uh translate this into something a little bit more mathy so you can have let's see there's a whole bunch of outcomes for one where you could have uh uh let's say that the uh oops all right we could say that the blue dice is a one and what's one outcome we have the red dice also be one uh we could have the blue dice b1 and the red dice b3 that has an even number of total pips we can just keep going on with this i'm not really sure off the top of my head i could compute it but i'm not really sure off the top of my head how many outcomes uh how many how many outcomes are actually in this uh sample space but it seems like it's going to be a number and i'm tired and i'm sure you don't want to watch me just list out stuff so uh we'll just end this with uh six and six so because at this point you probably get the idea plus i also don't honestly remember the last time anyone asked for this type of this type of event uh another of it so we could have for our fifth event um that the die add up to seven all right so what would be in this event well uh we could have that the blue dice is one and if the blue dice is one since they have to add up to seven that means that the red dice is going to be six uh what's another possible outcome well you could have that the blue dice is two and since the the oh no they don't add up to six they add up to seven yeah that's what i said so uh you could have that the blue dice is two in which case the red dice must be five uh you can have the blue dice be three in which case the red dice uh must be must be four and you keep going on like this uh until eventually you get to the very uh last element that if you were to continue writing down you would write down that element being when the blue dice is six and the red dice is one so how many elements are going to be in this uh event well it seems like for every blue die there's a corresponding and for every blue outcome between one and six there's a corresponding red outcome that would lead to the die setting of just adding up to seven so there must be six things in this event so yes um by the way for whatever for for what it's worth i'm not sure if i mentioned this later but i'll just mention it now uh we often use this notation uh like for example uh this we would put a set in between two what almost look like absolute value lines to mean the size of a set or more technically the cardinality of a set but for now when we're talking about um finite sets it's fine to talk about to say the size of a set meaning the number of elements in that set in which case the size of the sample space is 36 the size of the empty set the set with nothing in it since there's nothing in it its size is zero uh the size of um e5 would be six there's six elements in e5 and there's uh one element in e3 okay continuing on uh our next example define a sample space describing the experiment of waking up in the morning at a particular time where the time you wake up at the thought of as a real number and that's a critical point is the outcome of interest list three events from this sample space um i'm going to say that the sample space so we can think of it in terms of hours in a day but we're going to allow hours to be decimal points so an hour and a half would be 1.5 so we would say that midnight corresponds to zero hours and we're just going to say that you cannot reach the midnight of the next day so maybe you've seen this notation before when regarding uh intervals of the real line where a square bracket means that that number on that side is being included in the set and an open bracket or a parentheses on a side means that that number is not being included in that event uh so some more notation we say we use uh what almost looks like an e or an epsilon to say that something is a member of a set so for example zero is an element of the sample space uh 12 is an element of the sample space um but 24 uh well let's let's uh come back to 24 in a second 50 is certainly not an element of the sample space so we'll put a slash in that uh and 24 is not an element of the sample space either because i put an open what i call an open bracket or a parenthesis around the 24. uh so in this alternate so if i were thinking of 0 24 as a set this would be a set that doesn't include zero either alternatively uh just getting you more familiar with this notation uh we could describe a set that includes both of its endpoints 0 and 24 and by the way all of these are corresponding oh go away all of these by the way are corresponding to uh intervals of the real line so some some interval of the real line uh hopefully you're somewhat familiar with that notation uh yeah okay so uh continuing on this is our sample space let's uh erase all this stuff because this is someone evident aside um okay uh so uh three events from this sample space we could have an event where um uh we wake up at noon so that would probably correspond uh to the outcome where you wake up 12 hours from midnight um so this would be exactly 12 hours though exactly 12. not a second more not a millisecond more not a millisecond less and not a second less exactly 12 hours later which you think of that that matters it is exactly 12 and nothing it seems like like like our own intuition of the world is like if you wake up a millisecond after mid afternoon um you still woke up at noon i was like no that is not what i mean right here so it is a precise number and that precision should lead you to think well that's impossible it's it's or maybe not impossible but but really really really really hard to the point of being almost impossible and that is true when we develop for a probability model for this experiment we would probably assign a probability of zero to the event of waking up exactly 12 hours from midnight which is kind of a strange idea but i'm going to talk about that later video just understand that just to understand what exactly i'm saying here uh let's uh come up with another event uh this event corresponds to waking up between let's say instead that you wake up sometime between 8 and 12 hours now my language is a little imprecise here so i need to specify do i mean uh including eight hours uh including exactly eight hours um or am i saying more than eight hours so eight hours in a millisecond is okay but eight hours on the nose is wrong uh so we'll just say that we're going to exclude those endpoints why because i said so um so this excuse me uh this uh this event uh would be an event where you do in fact uh where you're where you can wake up um eight hours in a millisecond eight hours in a second or 12 hours less a second but you cannot wake up at exactly eight hours or exactly 12 hours and have this event actually have occurred okay um let's say for a third event um we're going to wake up um let's say that that this is uh waking up after 3 a.m so or um let's see uh maybe no earlier than three event at 3am so no earlier than 3am uh that suggests that we should include 3am in this in this uh in this event so uh so we should include 3am or 3 hours past midnight but all right so what should the end point on the other end be it seems like we can wake up any time after 3 a.m and that's fine so we're going to end at 24 since that's the last um so since those points are the last ones in this sample space okay let me get caught up all right moving on uh events once we have some events we can start uh manipulating these events in ways that create new events uh so we can start giving some uh operations on our set theory so for example let's just say that a and b are events uh and events in this situation they're also synonymous with sets so anything you know about set theory uh you can import that here i am actually when i'm talking about events i'm really talking about sets there those two terms in this class are used almost interchangeably so the complement of a which in this class we're denoting with a oops uh in this class we're denoting with a prime but that's not the only notation there's a bar or a complement this is actually my preferred notation uh that's but anyway uh in this class we're going to use a prime this is the set of outcomes of s the sample space not in uh the event a uh so it's all outcomes that could possibly happens that are not in a which so and in fact we the the the english words that we use for this event is not a that's that's a perfectly way to describe it there's the union of two sets uh which we would say a union b but i also like to just say a or b since this corresponds to the logical notion of or where something in one event can happen or something in another event could happen and by the way when we're using or in this context we're using it in the logical sense where um we can have an outcome in is in a only would count as happening in a or b an outcome in b only would count as have having occurred in the event a or b and an outcome that is both in a and in b counts as happening in the event a or b which is a little bit different from how the word or is used in english because it's quite often the case in english that or means exclusively or exclusive or or you can say you can have her your cake or eat it and like in that so the phrase have your cake or eat it it suggests that you can either have your cake or you can eat it but you can't have both right if you there was actually a it was actually a really long time until that phrase made sense so uh this might actually be something for um people whose native language is not english uh what they mean by have your cake and eat it too um you can have your cake in your hand or you can eat your cake but if you eat your cake it is no longer in your hand so that's what it means all right so just in case i think that that might be something that for nonnative speakers it's actually worth it to make that clarification but for a long time i also as someone who's spoken english all their life and was on the debate team and on the literary magazine did not get anyway um uh the intersection of two sets a and b or a intersect b so it's the set that contains only objects that appear in both a and b so in words we use a and b i'm going to go ahead and get started drawing some diagrams venn diagrams are a way to describe set theoretic relationships between uh different sets and venn diagrams you can construct venn diagrams for pretty much up to uh three events and venn diagrams make sense and very diagrams are very simple and the moment you try to go to four events event diagrams become impossible so we're just going to live in a world where there's only three events um so here's how you kind of draw a venn diagram at least for this class you draw circles and squares and stuff to denote sets often we draw a giant square and this square denotes the sample space so the square denotes the universe of possibilities and we denote a subset of this sample space otherwise known as an event with some other shape such as a circle or maybe i there are times where i will draw lines to try to divide it up but basically we're going to draw shapes and lines inside of this sample space that divide regions of the sample space from each other and those will denote subsets so often i'm going to draw circles to denote events so i'll often label one circle a and another circle b and how we draw these circles allows us to reason about relationships amongst events so here i have drawn a sample space with two events a and b and those events have some outcomes in in common since there is a region in which both set both of these circles intersect now there's also a situation where there are outcomes in a that are not in b so situations where a happens but b doesn't happen because you can kind of imagine if you really want a probability model that we're going to pick a random point from this set and we're going to ask where did this appear did this appear in in a did it appear in b did it appear in a or b um did it appear in a and b or did it not appear in either one um that would almost be a probability model so we can use diagrams such as this to start reasoning about uh set theoretic relationships um and in fact here's something to kind of think about well actually no i'm going to save that for the next section um but this is a perfect perfectly reasonable way to uh think about uh set theory at this level um okay so uh two so uh let's uh describe the situation uh a or b so a union b means an outcome that is either in a or in b or both so what i would do in a venn diagram is shade the region that corresponds to this event well maybe before i do that maybe before i go into that level of extra complication how about i first describe the event just a well visually using a venn diagram that would what i would do is i would shade the region a and nothing else and that would correspond to the event where a happens so i'm using shading up these circles uh to try and reason about what happens here uh i wonder if there's a let's use this highlighter tool i wonder what would happen if i use the highlighter uh it makes my computer lag a lot i'm not gonna use that um so um uh similarly we could describe another event where we just uh have b occur so we'll say the event b how would we shade this we're going to shade it like so just shade in the region that's enclosed by b and nothing else so this corresponds so this is the region that corresponds to the event b happening now let's go back to some of these other uh potential relationships uh such as a or b or a union b um hmm this seems like a fancy feature no no it doesn't work that way so i need to get rid of that red region all right so um here's a way to draw venn diagrams uh when you're trying to do um stuff so like let's take for example the union operation a union b to draw a union operation what i generally will do is i'll take a common color and i will shade in first uh a uh i don't want black i want blue i'll first shade in a the the set on the left side of the union relationship and then i will shade and be the element on the right hand side and a point that was shaded at all is a member of this set so um so this region that i've kind of enclosed in red uh corresponds that's that a or b since any point that got shaded by blue at all counts as being a member in this event okay uh let's draw another uh event let's uh illustrate for example a intersected with b so to draw intersection let's uh first draw our sets a and b to draw intersection you're going to subtract points uh you're going to subtract from uh uh or or actually what you would do is i like to think of it in terms of pieces of paper uh where you overlap two pace pieces of paper on top of each other and then uh cut the pieces of paper so that only the overlapping region is what's left so uh you could imagine here uh i draw uh something on a and i draw something on b and then i'm going to erase from the picture the region that what that was in b but not in a and i'm going to erase from the picture the region that is in a but not in b and what's left is going to be the region that corresponds to a intersected with b okay um so another important notion is disjointedness two sets are disjoint if they have no elements in common in that case a intersected with b is the empty set so to draw disjointedness this is what i would do here's my sample space i draw an event a and i draw an event b such that there is nothing in common between the two events all right i'm going to draw the intersection between these two events i'm done because there's nothing intersecting so the intersection between these two events is uh is the empty set to so this is one way if you really wanted to assign some sort of meaning to the empty set it would be that the empty set is a logical contradiction in a way since the empty set shows up and when you have events that have nothing in common which are almost logical contradictions uh another notion in probability theory no in a set theory is complementation how would i illustrate complementation so let's uh draw a sample space i have a i have b what is the so this is um a intersect with b uh equals the empty set that means disjoint uh what is a complement a complement is the region in the sample space that is an a that is in the sample space but it's not an a so that's going to correspond to shading everything that's outside of a so there are some parts of b that gets that get shaded but nothing that's in a okay so you shade everything except a uh some other important subset relationships um we have uh uh or so some other set relationships here is the relationship a is a subset of b what it means for one set to be a subset of another is that um all of the elements in a are also impres are also present in b or alternatively there is nothing that a that doesn't also exist in b so a subset relationship looks like so you have the set b and the set b contains the the set a so you would draw a as being in the interior of b another uh notion that i haven't really described above but let's go ahead and mention it is the notion of set subtraction we have a subtract the set b that corresponds to every element in a that is not in b so uh it this it is in fact possible to prove that a subtract b and you might actually be required to show this in your homework and the way you probably show it is by playing around with venn diagrams you can say this is a intersected with um the complement of b so a and not b so what does set subtraction look like as a venn diagram we have the set a we have the set b and we draw everything in a and we shade everything in a that is not in b so you kind of subtract out the set b it's as if you had these pieces of paper they put them you put uh the piece of paper the b piece of paper on top of the a piece of paper and then you cut out the part of the b of the a piece of paper that's in the b piece of paper excuse me okay uh so let's see an example use a venn diagram to illustrate uh a or b complement or a and b uh before i continue on let's just make sure i'm still streaming i'm still streaming um at some point i'll relax about that but today is not that day uh let's get started by just drawing uh the venn diagram oops i don't want red yet so here's my venn diagram for the sample space s i have a and i have b okay so i'm first going to illustrate uh the event a intersected with b i'm going to do so in blue so a intersective b that's the region that is in both a and b uh as for a or b complement you have the component you have the union of a or b which is kind of this uh figure eight looking region here and i want everything outside of that region because i want the complement of that region so i'm going to shade it everything that is outside of that region and this is what you end up with okay so that's one example next example consider three sets a b and c so how would i draw these in a venn diagram i would have my square denoting the universe of possibilities which is s and then i draw um three circles one for a one for b and one for c so what is the union of a b and c well it's going to be the set where uh if i shade a everything in a and then i shade everything that's in b and then i shade everything that's in c if the point ever got shaded it's going to be in that union all right next up we have a intersected with b intersected with c so let's draw our universe here's our sample space so we have a b and c so let's see uh let's take a point right here this point that i just drew is uh a point that is in b and it is in c but it is not an a so it's not going to be in the intersection uh this point right here is in b but it's in neither a or c but it it's also just not an a so it's not going to be in the intersection of all three you can just start reasoning about all of these points in a similar way and the conclusion that you're forced to reach is that this little sliver here is the only part that's going to be in all three sets so that corresponds to the intersection of all three all right uh next example this is a more complicated one we have a intersected with b or a intersective with c or b intersected with c what is that going to look like well let's get started i don't want red all right here's our sample space here is a here is b here is c all right let's start out by uh building this thing up with uh the intersection a and b right here is a and b here's a and c and here's b and c so this wind or pinwheel looking region uh is the only region that ever got shaded so that's going to correspond to the union of the three of the three intersections so this would in words this would be an event where at least two events happen this will correspond to all right uh so example eight describe the intersection complement and union of events described in examples one through five let me get caught up um so the point of this section is to go from these pictorial representation or not this section this example is to go from these pictorial representations of events to more algebraic representations so uh let's go with example one so for example one that was the example where we were flipping a coin we could get either heads or tails uh we could have uh the intersection of heads and tails uh of the two uh sets heads and tails these two sets have nothing in common they are therefore considered disjoint events so the intersection of these is the empty set since you would only restrict yourself to what is in common and they have nothing in common uh whereas the event heads or tails uh that event would correspond to uh the event heads with heads and tails uh which corresponds by the way to the sample space okay uh here's also some basic uh properties for you uh let's let's take an arbitrary uh set s no no no not s uh let's take an arbitrary event a a intersected with a sample space is equal to a since a is a subset of the sample space in fact in general if a is a subset of b then a intersected with b is going to be uh a and a union with b is going to be b and i'm just going to leave it at that i want you to think about why that's true uh go try and reason about it with a venn diagrams um so and in fact that's pretty much that pretty much says everything that i would want to say about uh the sample space and the empty set because uh since a is the subset of the sample space a intersected with s is going to be a and a unioned with s is going to be s likewise a intersect intersected with the empty set the empty set is a subset of a therefore the intersection of two is going to be the subset which is going to be the empty set on the other hand a union with the empty set is going to equal a since a contains the empty set all right uh continuing on with what i was saying before um the next example uh so example two uh so uh we have uh for example the sets um my notation in the notes is a little bit different from one i'm sure i wrote down earlier um so let's uh uh um i don't want to go all the way back there i'll just uh i'll just throw some stuff out uh we can have um for example uh so this was the one where we were rolling a die uh one we could say that the union of the set uh one with the die roll of one and the set with the die roll of uh so the intersection of these two sets this is also going to be the empty set since they have nothing in common but uh the intersection of the event with um no the union of these two sets is going to be uh is going to be um the event that contains both of these possible outcomes darn laptop so one and three and uh maybe less trivially um uh we could have uh we could have the um event uh let's say one three five so this would correspond to an odd number of pips and we're going to intersect that with the event where the number of pips doesn't exceed three um so that would be the event where you have one uh two and three oh okay so uh the intersection of these two events is going to be well let's see what outcomes do they have in common uh one amperes in both of them three appears in this set but it doesn't appear on the other one so let's see we've got one one appears in both uh three appears on only one no three actually appears in both of them so three appears in both of them five appears only and one and two only appears on one so whatever got underlined twice that is going to be in the intersection of the end of these two sets so we're going to end up with the set one and three now for a lot of these uh set relationships such as and and or you can kind of think logically using english like for example um uh maybe going to uh the fourth example where i had uh sets where um yeah i was rolling a two dice uh a red dye and a blue dye you could imagine a set where uh they summed to seven and the blue dice is at least three so maybe going to that so in the context of example four so in the context of oops in the context of example four so they sum to seven and they don't or no no no no uh the blue dice uh so blue dice is um uh let's say five so that would correspond to this to the event where uh you have um so let's see the blue dice is at least five so we could have the blue dice b5 stupid pen so the blue dice could be five or the blue dice could be six and in these two situations we know what the red dice is going to be in the first the red dice is going to be two and in the second the red dice is going to be one so this would be uh the corresponding uh resulting event after we do that intersection okay um i'm going to leave it at that there's because there's a lot of examples so for the rest of these maybe uh try going through uh some of the set the events that i listed down and figuring out if you intersect or complement these events so intersection complementation union uh all these things try going through those sets and uh seeing what you get okay but i've given you some examples to start working off of for now all right so i'm gonna leave that for this section and uh i will see you in section two where we go beyond just uh talking about some basic set theory and actually start saying what it takes to build a probability model all right so i will see you later hey students all right so let's move on now to the next section uh on starting probability theory so uh in order for us to uh we would like to be able to assign numbers to events from uh sample spaces to describe how likely those events are and in order to do so we need to develop a notion of probability so we'll start by defining what a probability measure is so we have so we have what's known as a probability measure p and this is a function it's a function that takes events as inputs and returns numbers between zero and one and satisfies the following three axioms and axioms are things that are true basically because we say so they're they're almost similar to assumptions maybe you remember axioms from geometry but the idea is that we have some starting things that we simply say they are true because they are almost selfevident first we say that every probability must be at least zero uh you cannot have negative probabilities second we say that the probability of the sample space is equal to one so the probability that anything happens is equal to one finally uh this is the weirdest looking axiom if we have a sequence of disjoint events uh so in other words for any uh i that's equal to j the intersection of uh any two such events is uh equal to the empty set and furthermore this is for a potentially infinite sequence of disjoint events we say that the probability of the union of those events is equal to the sum of their individual probabilities this is very wordy this is very technical there is a much easier way to understand what this axiom is saying if we decide well okay there's usually only one situation where you're going to understand where you're actually going to use this axiom most of the time which is that if a intersected with b is equal to the empty set so if a and b are two disjoint events then the probability of a or b is equal to the probability of a plus the probability of b so the probability of uh two events that have nothing in common uh if the probability that either one happens will be the sum of their individual probabilities okay so these are true because we say so and from this we get uh pretty much everything else that we believe should be true about uh probability or about how probabilities work so uh for starters uh we have uh that the probability of the empty set is equal to zero so this is not something that we said this is not something that we said must be true this is simply something that um this is actually a consequence of the assumptions that we have made up to this point so this is kind of a weird so we're going to show that this is in fact true using just these uh three um just these three assumptions not just these three axioms so only these three things all right um now that said we're going to have to use the fact that probably the sample space is equal to one what we could say is this the sample space the sample space is equal to the sample space unioned with the empty set and in order to use axiom three in the way we have written axiom three down we're going to have to say that the empty set is equal to uh the union from i equals one uh to well i equals one to infinity the empty set so in other words we need to create an infinite union of empty sets and it's certainly true that if you take an infinite uh collection of empty sets and you union them all together there's still not going to be anything in that union so you're still going to have the empty set furthermore the intersection of the empty set with the empty set is also going to be the empty set so technically the empty set is disjoint with itself so that means that this collection of empty sets this giant uh repeating of the empty set technically satisfies the condition of the conditions of the third axiom so then and and also it's true that the set the sample space uh intersected with the empty set is going to be the empty set because the empty set is a subset of the sample space therefore we can now apply this uh this uh this uh third axiom to prove the prob uh proposition because we know that one is equal to the probability of the sample space and the probability of the sample space is equal to the probability of the sample spaced uh unioned with the empty set which is equal to uh the probability of the sample space unioned with this infinite collection of uh empties of uh empty sets and then we apply that third axiom to say that uh this is going to equal to the probability of the sample space uh plus uh this uh plus um the uh the sum from uh i equals one to infinity uh the probability of the empty set and what that forces us to then say is that since uh the probability of the sample space is equal to one we're forced then to say that the sum from i equals one to infinity of the probability of the empty set is equal to 0 but that's only going to be possible if the probability of the empty set is itself equal to 0. because since probabilities uh cannot um be uh negative and oh and even just because of the fact that uh we're adding up something an infinite number of times and that's and that's the exact same thing and it adds up to zero so that means that each one of those must be equal to zero therefore this uh axiom this uh proposition is proven this one this is actually rather weird it's it's so surprising like there is a way in which you you you kind of just want to say uh you have one equals to the probability of the sample space which is equal to uh the probability of the sample space plus the sample space unioned with the empty set and then it turns into a sum of probabilities and uh that means that probably the empty set is equal to zero but we have to in order to be mathematically rigorous use these axioms in the way they were written down so we would actually have written the axiom in a way in which it wasn't written and therefore wouldn't be able to directly apply it so we had to be really tricky but the good news is that we only had to be tricky really once with this uh proposition just with this one because now that we have this proposition the others will not nearly uh will not be nearly as uh strange for example proposition three like this one this proposition if we had this already then that proposition that we just proved would actually be rather easy because uh the complement of the sample space is equal to the empty set uh the probability of the the sample space uh is equal to one and since the uh uh the the empty set is the complement of the sample space we can say if this proposition 3 were in fact true the probability of the empty set is equal to 1 minus the probability of the sample space which is 1 minus 1 which equals 0. right if we had proposition three then it would actually be rather easy to prove that the probably the empty set is zero but the thing though is in order to be able to prove uh proposition three we're going to have to uh probably use uh some of those uh other propositions so unfortunately uh it's it's um it's not quite that it's we we technically need to take a very long circuitous route in our proof before we can actually invoke this one so what we're going to say is that the probability of the sample space we can say that the sample space can be divided into uh into the set a unioned with the complement of a and uh a union a complement um well okay a intersected with a complement um is equal to the empty set so since a intersected with uh its complement is is the empty set that means that these two sets are going to be disjoint which means that we can then invoke uh that um that a second that third axiom and say that the probability of the sample space is equal to the probability of a union a complement which is equal to by that that other axiom the probability of a plus the probability of a complement and uh by the first axiom this is all equal to one therefore after you do a little bit of algebra where you subtract uh you subtract both from both sides the probability of a uh after you do that algebra you can now say that the probability of a complement is equal to one minus the probability of a and we're done that proposition has been proven uh next proposition we have that the probability of a is less than or equal to 1 for any event a let me get caught up in my notes over here all right uh so the probability of a complement since a complement is in fact an event by the first axiom uh we get to say that the probability of a complement is greater than or equal to zero which means that one minus the probability of a is going to be greater than or equal to zero and therefore it follows after oops after we add the probability of a to both sides after we do that uh we get to say that uh uh so since these two things end up cancelling out uh the probability of a is less than or equal to one so that proposition has been proven uh next one or is that it no that's not it uh the probability of a union b is equal to the probability of a plus the probability of b minus the probability of a intersected with b for any events a and b so uh this by the way is generalizing more or less that third axiom since that third axiom required disjoint events whereas here in this proposition we do not require disjoint events um we do not require just disjoint events and the penalty that we pay for that is that we have to subtract out the probability of the intersection here's the thing though uh this this makes perfect sense what's going on if we draw a venn diagram um it turns out that probability is part of this general class of mathematical objects or probability measures are part of this general class of mathematical objects known as measures and amongst that family of mathematical objects we include things such as measures of area or measures of length which means that it's actually rather appropriate and convenient to reason about probabilities the same way we reason about areas or lengths as we do in the real world so let's think instead about we have um a couple uh sets a and b we want to figure out what the area is in a and b like the that's enclosed within both of those we want to figure out that area um and how we're going to do so is uh we have a couple sheets we have an a sheet and a b sheet and we lay them down and we have some scissors so um we can measure the areas of these sheets so uh given that um we're able to measure the areas of these sheets how can we possibly compute the area that's enclosed in both a and b well what we would do is we would lay down the a sheet onto this diagram and we would have its area then we would lay down the b sheet on this diagram and we would have its area but the thing though is once we've done that we now have an overlap we have an overlapping area um so we've actually over counted the area um in a and b we the number is too large so what we need to do is subtract out uh the part the overlapping part of the a and b subtract out its area because otherwise we would have double counted it so we're going to uh subtract out that area um and uh or at least we'll like cut out a little slice of it um uh maybe just like the red slice part and leaving only the blue part left and and we're allowed to measure the area of the part that we cut out so after we uh subtract that the area that we removed we now have the area that's enclosed between uh that's closed within these two circles and that's an aerial way to understand why this formula here is in fact true because this is base because how i described it is basically what we're doing we have the area of the region a and the area of the region b and we subtract out once the area that's in between them since we accidentally well not accidentally since uh without doing so we would have double counted that part so we need to remove the double counting so how are now that's like a reasoning for what we're doing and now let's turn that into a series of mathematical statements and a proof here's a way to think about this region this venn diagram we can divide up in this venn diagram uh we can divide up uh this uh region so that we have uh the light blue region we have the green region and we have the red region we will call the blue region a and not b we will call the green region a and b and we will call the red region uh not a and b and it's clear that these three regions are um these three regions are disjoint you can see so visually but if you wanted a nonvisual way to reason why that is the case let's suppose an element is let's suppose that we pick a point in that is in the set a and not b can that set be an a and b well it must not because in order b in the part a and b it must be b b end b uh but it is not in b since it's an a and not b so it cannot be in there and uh similarly for uh not a and b and for the same reason if you're in a and not b you cannot also be in not a and b since you're in a and therefore you're not in not a i know this is the verbiage is getting really complicated but that is that is really the logic for why these things must be destroyed or you can just look at the picture and be satisfied um for that reason we can say that the i mean you can also look at this picture and see that when you take the union of these three uh events uh you have in fact the region a or b and there's also no double counting here since you counted each little part at this each division once so now we can say i'm going to zoom in uh some more we can say that the probability of a or b is equal to invoking that third axiom the probability of a and not b plus the probability of a and b plus the probability of um uh not a and b and uh let's see let's see what how should i proceed next well i do notice for starters that this part right here is equal to the probability of a which is uh clear from the picture because if you add the probability of a and not b and the probability of a and b then you've basically counted the probability of a so this must be the probability of a uh but the thing is though uh we need to somehow account for uh i mean we end up in the end with a subtraction of the probability of a and b so how are we going to do that huh well if we look at the probability of b we could say as an aside that the probability of b is equal to the probability for basically uh the reasons that i just wrote here that this is the probability of um a and b plus the probability of uh uh not a and b and then if you do some algebra you can then say that the probability of not a and b is equal to the probability of b minus uh the probability of uh a and b okay oh well look at that we now basically have what we need because we've identified a part as the probability of a and we can also identify the latter part in this sum as the probability of b uh minus the probability of a and b which is exactly the statement that we wanted to prove so therefore we're done and we have in fact proven this statement and notice that this statement does in fact include that third axiom since if since that third axiom was about disjoint events so um if we have a disjoint event we've proven in uh in our the first proposition that we proved in this section that the probability of the empty set is equal to zero and the subtraction of the probability of a and b is going to give you um so so that's going to uh so the intersection of a and b if and since in this imaginary scenario they're uh disjoint that's going to be the probably the empty set which is equal to zero and thus you get uh that uh that other axiom all right so uh oh oops i am uh i am doing something that i don't want to do what i want to do is zoom out okay so the next proposition i'm not going to bother to prove just because it's a lot of work although i may give you an argument for why it's true so proposition 6 now we have three events a b and c and the probability of the union of those three events is going to be the sum of the probabilities of the events minus the probabilities of intersections of two plus the probability of the intersection of all three all right so i'm not going to prove this but i'm going to give you an argument for why this must be true so here we have our sample space um we have the set a we have the set b and we have the set c it's basically the same reasoning as we used before um let's uh uh let's see so uh so what does it mean to add the probability of a probably be probably c so if we add those three probabilities so we're gonna add the probability of a because what we're trying to do is figure out in the area that is enclosed in all three circles uh but those circles are overlapping with each other so we need to figure out how to handle the overlaps so we're gonna color in the probability of a that's what we're told to do first uh then we're gonna color in the region and close by b that's what we need to do second and then we're going to enclose the region enclosed then we're going to color in the region and close by c because that's what we've been told to do yeah you can almost read this as a set of instructions on how to calculate an uh how to calculate um uh um uh an area so we see that we have done some double counting we've double counted here we've double counted here and we double counted here all right so we need to remove those double counts uh let's start with the um let's uh let's start with the double counting here we're going to subtract out uh the probability of a and b which will i will under i will understand that as subtracting out a little green sliver so how does this i'm wondering if this is doing something oh no it just was being laggy all right so um so i have subtracted out the green area from that sliver uh leaving a blue left and some red red left okay uh and then i need to i'm gonna cut out the uh red area that is intersecting and that is at the intersection of a and c okay so i'm going to cut that out uh leaving only blue uh um leaving only blue there uh let's see so we're going to have uh right here so and then we need to uh cut out the area that is in the intersection of b uh and so we now need to cut out the uh region that's in uh both b and c so we're going to subtract out uh the red area but the thing is that doesn't that doesn't uh that isn't enough we cannot just take out the red arrow because we need to take out everything that's in here so we also have to take out what that little blue sliver that's left as well so uh we're going to be left with green in that in that spot uh but the thing is in that little sliver uh that that is in the intersection of all three we have now cut out oops uh we have now cut out too much and that area is not being accounted for at all there's nothing left in there so we need to add that area back in in order to be able to have an accurate computation of the area and there we go so after we add it back in uh we now have the area so everything has been colored uh exactly once and uh we're able to compute the area of how much area we've colored uh in a sense so we have what we need so not exactly a proof because we need to uh do some do some of that a tricky algebra business but uh we're not gonna bother with that this should give you an idea of why it's true okay uh so the next example that all of that stuff was uh uh rather theoretical uh let's let's um start seeing how uh probability theory is actually going to be used so for the most part of the remainder of the section i'm going to be going through a number of illustrative examples uh we do talk a little bit more about uh theory mostly about what probability means uh but uh for the most part we can just talk about um examples so example nine reconsider the experiment of flipping a coin and assume that the coin is equally likely to land with each face facing up assign probabilities to all outcomes in the sample space so recall that the sample space for this experiment consists of the outcome heads and the outcome tails and we're assuming that all of the outcomes in this sample space are equally likely so that means that the probability of uh getting heads uh and i'm writing this in terms of a simple event but honestly this writing it this way often gets rather tedious so i'm often going to admit uh to omit the curly braces that are usually used to denote sets and just write whatever is in the set so we have the probability of h uh this is going to equal the probability of t because all outcomes are equally likely so the thing though is the probability the sample space is equal to the probability of the set containing only h unioned with the set containing only t since this uh so since the union of those two sets is in fact equal to the sample space and furthermore those two sets have nothing in common one has h one has t and they don't share anything so that means that they are disjoint events and therefore we can write this probability as a sum as the probability of h plus the probability of t and both of these are the same so we're going to say that this is equal to p so this is equal to uh p plus p which is two p and furthermore we know from uh axiom two that the probability of the sample space is equal to one so this is equal to one and that implies after you do some division by two that p is equal to one half which makes perfect sense right if you're saying that heads is just as likely as tails then the probability of getting heads is one one half perfectly intuitive right well you know uh probability is intuitive up until the point it isn't and when it stops being intuitive it really stops being intuitive and it becomes suddenly very very strange so it's simultaneously intuitive and unintuitive in fact i've heard someone say that there are fewer subjects with as many paradoxes they're not literally paradoxes but they contradict how humans think of the world very few subjects have as many paradoxes as probability because probability can get really weird really fast we have not reached that point yet but we probably will eventually example 10 do the same as example 9 but when rolling a single dice that is um we are trying to so we we're trying to assign probabilities to outcomes in a sample space when we say that those outcomes are equally likely so the sample space in this case is going to consist of die rolls um so we have uh an outcome one a two uh three four five and six okay so those are our six outcomes and we say that each one of them is equally likely so that means that the probability of rolling a one is the same as is uh the same as the probability of rolling a two and that's going to be the same as dot dot dot as this and the same as uh the probability of rolling a six all right so uh it's a six so all right there we go six it's a it's an oddly painted die all right so all of those are going to be the same and we're just going to say for convenience that this is equal to p all right so then we say that the probability of the sample space is equal to the probability of the result one uh unioned with the result uh two union with dot dot dot unioned with uh the uh outcome uh containing uh six there we go because that's equal to the sample space you basically written the sample space as a union of each of its elements and furthermore each of these sets again are disjoint so uh we can then write them as the sum of probabilities so say that this is equal to the probability of rolling a 1 plus the probability of rolling a 2 plus dot dot dot plus the probability of rolling a six and then we use the fact that we said from the beginning that all these probabilities are the same so this is p plus p plus p plus p plus p plus p so this is equal to six p but it's also equal to one because of axiom two that says that probably the sample space is equal to one therefore p must be equal to one over six and you can see where this is going in general if you have a sample space with a finite number of elements uh and you say each of those elements are equally likely then the probability of a single one of those elements in that sample space probably of drawing that element is going to be 1 divided by the size of the sample space or the number of elements number of unique elements in that sample space um so in fact what we're seeing here can be very easily generalized okay uh so example 11 we're now going to try and move away from equally likely outcomes and say that the dice from example 10 has been altered with weights now the probability of the dice rolling a 6 is twice as likely as rolling a 1 while all the other sides have the same probability of appearing as before therefore we want to know what the new probability model is so we're going to say that the probability of rolling a six so one two three four five six so the probability of this of rolling a six is equal to two times the probability of rolling a one and we're just going to say that's equal to two times p or uh we'll just we'll try to keep things a little bit different we'll call it q this time all right so we need to figure out q because q gives us the probability of rolling a one and if we figure out the probability of rolling a one we then instantly know rolling a six now remember these are the only two faces that have been altered all the other dice faces have the exact same probability as before so the probability of rolling a two is equal to the probability of rolling a three which is the probability of rolling a four which is probably ruling a five and all all of those are equal to one over six so that means um that the probability of rolling a one uh plus the probability of rolling a two plus dot dot dot plus the probability of rolling a 5 plus the probability of rolling a 6. come on cooperate you stupid screen i hate drawing the 6 because my screen is not very cooperative all right uh this is equal to one but uh what's different now is that all of these are equal to one over six so you end up adding one over six four times since it's two three four five there's four things there um the probability of rolling a six is equal to two q and the probability of rolling a one is equal to q okay so um collecting all of that information we now say um that we have a q plus 2q plus 4 over 6 which is 2 over 3 is equal to 1 which implies after you do some algebra that 3q is equal to 1 3 which then means that q is equal to ah no is equal to one over nine not nine because that's not even a probability uh that's that's that's one thing to keep in mind if you get a probability that's above one you have made a mistake so nine is not possible the problem so that means that the probability of rolling a one so yeah let's let's uh write this down now the probability of rolling a one in this new probability model is equal to oneninth and the probability of rolling a six in this new probability model is equal to two over nine and in fact this isn't this is consistent because one over nine plus two over nine is equal to three over nine which is one third there are the uh other dice faces when you add up their probabilities you have one over six one over six one over six one over six which is twothirds and so you have onethird plus twothirds which equals one so the probabilities still add up to one which means that we're fine this is a way for you to check uh whether this is one way for you to check that you're that when you do your probability calculations you have done so correctly make sure that all the probabilities add up to one if they don't add up to one you've made a mistake i remember once a student was working in a i had a student of mine and i gave her a quiz and she had to do some calculations with probabilities and she added up the probably sample space and it added up to something that was to some fraction it might have been uh maybe uh eight over nine or eight like 80 or 81 who knows but but i said this is very wrong and she's like well it needs to add up to one and she said well it's close to one and i said close to one is not one so if it doesn't add up to one it's just wrong it like close to one no caboose it's either one or it's not one if it doesn't add up to one it's wrong it's very wrong it always adds up to one so that's a way for you to check that you've done things correctly make sure that your probabilities add up to okay moving on example 12 reconsider the experiment of rolling two six sided die it is reasonable to assume that each outcome in s is equally likely this is the reason why instead of writing the numbers 2 through 12 or maybe just listing out without really caring about the ordering of the die or not really thinking about there being a red dye and a blue dye we we decided that we were actually going to assign an ordering to the dice it's so that we could basically work with problem example 12 and get a reasonable looking probability model because now if we assume that the two dice are distinct we can now say that everything in the sample space in that sample space with a red and blue die is equally likely and then get accurate probability calculations so uh getting back to this it is reasonable to assume that each outcome in the sample space s is equally likely what then is the probability of each outcome in s um well basically i already argued it uh to you before we'll say that omega is an element of s so this i'm saying this in general right um omega is the element is an element in in s and every element in s is equally likely i argued before without writing it down that the probability of drawing omega then will be 1 divided by the size of s so in this case for example 12 where you have two sixsided dice um the size of the sample space was 36 so the probability of a sink of a of a particular outcome of dice is going to be 1 over 36. now we can use this uh this model uh to find the probability of an event e where so i'll i'll just go ahead and write down in this case that um the size of the sample space s is equal to 36 so for this particular problem this is going to be 1 over 36 but in fact what i just wrote down here true what i just wrote down here is pretty much drew in general like um i'm i i'm thinking in the context of uh rolling a couple die but actually this is true in general when you say that s which is a finite sample space it has a finite number of elements um when everything in that sample space is equally likely um then the probability of an individual individual element is going to be one over the samples over the size of the sample space and it's not too hard why go ahead uh show that the uh probably the sample space when you do this is equal to one uh so use this model to find the probability of an event e where first e is the event where at least one die is a six so let's actually write down um let's write down uh what's in e so that would be we have a red dye and a blue dye at least one dye is six so that includes when the red dye is one and the blue dye uh is six we have uh the case when the red die is two and the blue die is six we have and we can keep going on like this until we eventually reach the case where uh the the red die is a five the red die is five and the blue die is six yeah damn it you cooperate all right that's six um so uh that's one set of outcomes and notice that there's uh that i've just listed down five elements okay uh so next up we will now make the red dye six so we've got one two three four five six it never wants to do the last one never ever wants to do the last one all right so one two three four five six and the blue die is one and uh just just repeat just keep uh carrying on with this logic until we get where the red dice is six and the blue dice is five and finally we have one last outcome where both the red dye and the blue dye are six so one two three four five six and uh one two three four five six all right so uh there are um five elements where the uh um where the red die is fixed at six and then there's one extra element where uh both of them are six so that means that the size of this event is going to be um 11. which then means when we're computing the probability of this event we could add up the probability of each one of these outcomes and each one of these outcomes has an equal likelihood and they all have an equal probability all those probabilities are 36 so you're gonna add up 36 and one over 36 11 times so this is going to add up to 11 over 36. and in fact it's once you have this uh this um assumption that all outcomes in this finite sample space are equally likely in general the probability of any event e is going to be the number of elements in e or the size of e divided by the size of the sample space so at this point all we need to do is decide how to determine how many elements are in our event in order to figure out that events probability all right so for let's let's use that now for uh this uh next problem where e is the set where is the event where the sum of the pips showing on the two die is uh five how many outcomes are there where the sum is going to be five let's start listing out uh possible things in this sample space so uh we've got i guess i've been writing the red dice first okay so okay so this is all right so i've been writing out the red dye first so let's suppose let's see the the two dice is five so can the red dice be one yeah sure why not uh the red dice can be one and if it's one well it must add up to five so that means that the blue dice must be four okay that's one possibility uh next possibility is when the blue dice is two and which is when the red dice is two sorry in which case the blue dice must be three uh we could have the case where the red dice is three in which case the blue dice must be two and finally we have the case where the red dice is four in which case the blue dice must be one and we can't go to five because it's not possible for the blue blue dice to roll a zero um so we're just gonna have to end it there uh we now have uh a sample an event and this event has four outcomes in it so that means that the size of the event is going to be a four in which case the probability of the event e is going to be uh 4 over 36 which is going to be uh 1 over 9. okay next part uh e is the event where the maximum of the two numbers showing on the dice is greater than two okay now here's the thing though um there's actually a lot of outcomes in this event so we could attempt to try and figure out how many outcomes there are where the larger of the two numbers is greater than 2. well let's see we could even have like three one three two three three three four three or five three six all the all the cases where the red dice is three and that's already six such outcomes and that's only for three so uh this could actually be quite difficult to compute except for the fact that it's actually much easier to compute the comp the probability of the complement of this event if we were to look at the complement of the event e that's going to be the event where the maximum um of the two dice is um not greater than two it so it's at most two okay uh where what are what are combinations of dice facing faces where the maximum is at most two well we have one situation where uh we have um oh yeah i said that the red dice is first so we have the situation where the red dice is one and the blue dice is one all right in that case the maximum of the two dice is going to be one and that is uh not greater than two so this is in fact in our event uh then we have the outcome where the red dice is two and the blue dice is one in that case the maximum would be two and that doesn't exceed two so this is in our event we also have uh the case where the red dice is one and the blue dice is two let's see all right there we go so the blue dice is a two and we also have the outcome where the red dice is two and the blue dice is two and the maximum of those in this case would be two and again that's uh at most two and we have to stop there because then because the the next thing we would do is make one of the dice at least three and in which case the maximum would be at least three so it's not long going to be in this event so therefore we have an event with four outcomes in it and the probability of the complement of the event e is going to be the number of things in the complement of e uh which is four divided by 36 which is equal to one over nine all right we're cooking because now we can use one of those propositions that said that the probability of the complement of an event is one minus the probability of the event so that means that the probability of e itself which is what we actually want to compute a probability of that's going to be 1 minus the probability of e complement because e is the complement of e complement so and that's going to be 1 minus 1 over 9 which is 8 over 9. there we go we've we've solved it and that's much easier than if we tried to do it directly and this is one reason why you care about uh these uh complementation rules because sometimes it's easier work to work with the complement of an event rather than the event itself if we had tried to work with that event uh we would have ended up with a sample space with uh or an event with um 32 elements or or a size of 32 so we would have ended up having to count 32 things i mean it's not like impossible to count 32 things but it's also a lot more work so so this is a very good trick to have and something that you should be looking out for when you're doing your own work you should be looking out for situations where the compliment is actually easier to work with than the actual event itself okay uh just real quick uh satisfy my nervousness all right we're still streaming all right so we shall continue example 13 reconsider the experiment of flipping a coin until heads is seen ooh this one this one's going to get rather involved what is one way to assign probabilities to all outcomes of this experiment so that we have a legal probability model justify your answer so how could we possibly do this because we no longer can say that each outcome in the sample space is equally likely because that only works when the sample space is finite but as i mentioned before this sample space is infinite um there's an infinite number of outcomes in the sample space since it's basically the integers in a way is you can map the sample space to the integers and kind of identify it with the integers or not the integers but the natural numbers and in fact i think that's what we should do we should uh try to view this sample space in terms of the natural numbers i'm going to want to zoom in for this one zooming in for me is a way to get to almost get more room on this piece of paper so we're going to say um we're going to define n of omega so omega is an element of this sample space so this is going to be one of those strings heads tails heads tails tails heads tails tails tails heads and so on so we'll say that um n of omega is going to equal the length of the string omega so for example uh n of the string t t h this is a string of length three it then follows that um using abusing notation a little bit because n is a function that doesn't take sets as inputs but let's suppose for a second that we were to put a set as the input this is actually pretty commonly done uh often when you have a function and you plug in uh its domain what you're talking what you're actually referring to is a set representing the range of that function so in this case uh the range of the function n is going to be the natural numbers it's going to be one two three four and so on these are all the possible outcomes it's going to be the natural numbers okay so um i'm going to suggest that what we should do for our probability assignment is say that the probability of an outcome omega is equal to uh onehalf to the power of uh n of omega so in other words it's going to be one half to the power of the length of the string omega so for example in the in this earlier case where we had t t h the probability of that outcome tth would be uh one half to the power of the length of the string tth or one half to the power of three which is one over eight okay all right then so this is a suggestion for what the probability should be but now we need to make sure that this is a valid probability measure so what is what needs to be true in order for this to be the case first off is the probability of um are all probabilities greater than or equal to zero under this method yes that is certainly true because there's no way that this function will produce negative numbers uh secondly uh is the probability of the sample space equal to one uh well that's actually something that we're probably going to have to check and then there's that third axiom about um the probability of a excuse me uh about the probability of unions of uh disjoint events and i'm not going to check that one because that one's actually getting rather complicated that's getting even more theoretical a bit too theoretical for this class but i personally think it's perfectly appropriate to check that under this probability model the probability of the sample space is equal to one um and in fact in my classes this is actually something that i love to ask questions about on quizzes i love to ask students that the probability of the sample space is equal to one under some probability model i love asking that so if you are in my class you should expect a question like that you should expect me to ask you to show that the probability of the sample space is one under some potential probability measure or alternatively very very similarly i might ask you to i might give you a probably measure but it depends on some unknown constant and i might ask you to compute what the constant is that causes this the probability measure to be a valid probability measure that is the probability of the sample space would be one under that measure all right so so this is something to look out for if you're actually taking classes from me uh but let's uh get back to the issue at hand i need to show that the probability of the sample space under this probability model is in fact equal to one so the probability of the sample space here is going to be the pr is going to end up being the sum um i i can basically view the sample space as consisting of you of as being the union of all of these of events uh con where each of these events contains uh one of these uh strings h t t h t t t t h something like that uh and this is actually a situation where that third form of where the way i wrote down axiom three uh before you really actually need it the way i wrote it down originally in like the body of the lecture notes rather than that footnote because we actually do in fact here have an infinite collection of events so i'm going to write this is going to be the sum over all the all of the omega that's in the sample space of the probability of that omega right so remember what this is actually doing is summing up the probability of h the probability of th the probability of t t h the probability of t t t h and so on all right uh continuing on uh we can then say that this is uh equal to um the sum over all omega in the sample space uh we have our probability assignment this is one half to the power of uh n of omega and at this point i'm just going to say that n of omega is equal to the length that sample is equal to length that string i can simplify what i'm writing down here a little bit by basically writing down uh what the image of n is uh under uh uh over the set s so i can now write equivalently that this is going to be the sum when uh n equals one to infinity of uh one half to the power n and now you should recognize from i think they talk about this in math 1010 at the university of utah intermediate algebra that this is a geometric cell this is a geometric sum and there is a formula for finding uh the value of of a geometric sum so recall this formula from your previous classes you have a a number r such that uh the the magnitude of r does not exceed one then the sum from uh n equals uh we'll say 1 to infinity of r to the power n is equal to r over 1 minus r remember that well we're going to use that here right now and we're going to say that this sum is equal to onehalf over one minus onehalf which is equal to onehalf divided by onehalf which is equal to one which is what we wanted to show we have now shown that the probability of the sample space under this probability measure is equal to one all right hopefully you have written down this formula if you don't remember it because i need to reclaim that space now that we have this we can now start answering some questions now that we know that this is valid probability model we can start using it uh so under this model what is the probability that the number of flips needed to see the first head uh exceeds four so what i'm asking for is the probability of um i'm gonna have to write this in a somewhat funny looking way go away stop stop bothering me i have to write this in a somewhat funny looking way i'm going to say that this is the probability of drawing an omega from the sample space uh or or drawing a sequence of flips such that um the length of that sequence is greater than four and this is another one of those situations where it's actually easier to work with the complement and say that this is equal to one minus the probability of drawing a string of flips such that the length of that string is less than or equal to four okay this is actually a finite set because we can actually list out the strings of flips in which uh in which the length of string is less than or equal to four we have heads we have tails heads we have tails tails heads and tails tails tails heads so this is one minus the probability no one minus uh the probability that you get heads on the first flip plus the probability that you get tails heads uh first tails then heads uh plus the probability they get tails and tails and heads and then you have the probability of three tails and a head and we can in fact figure out uh what each of those probabilities are the first probability is going to be one half the second is going to be one half to the power two or one fourth the third is going to be one half to the power of three or one eighth uh 1 8 and the last one is going to be one half to the power of 4 or 1 16 and long story short uh this is going to be equal to 1 minus 15 over 16 which is equal to 1 over 16. which is kind of funny it's a little funny to think about that why is it that it is equal to 1 over 16. hmm well here's kind of another way you could think about it um well it'll make more sense when you get when we start talking about uh independence but when thinking about independence what you end up doing is like we know that if a coin is equally likely to get heads and tails then we know that uh the probability of getting heads is one half so it's so what would then be the probability of getting four tails in a row um because that we must get at least four tails in a row in order for us to have to have at least four flips in order for us to get ahead so what we're actually asking for is the probability of getting four tails in a row so what i notice is well let's see it's if we were to think about um getting two tails in a row it seems like the probability of getting two tails in a row is one fourth and three tails in a row is one eighth um and then four tails of a row that's one over sixteenth hmm so that's an alternative way to think about what's going on here uh but there's an algebraic way to get it to an algebraic way to get the answer all right uh the second part what is the probability the number of the number of flips until the experiment ends in other words the last flip will be heads is between 3 and 20. this one is a little bit more painful but actually maybe we could use that trick well let's see uh we what we could potentially do is say um that the probability of observing a sequence of flips such that the length of that sequence um is exactly less than three well that's going to be uh so that's going to be the sequences where the length of the sequence is one or two so that's going to be heads or tails heads and that's going to be onehalf plus onefourth which is uh threefourths um now let's compute the probability of observing a sequence of flips where the length of that sequence is greater than 20. now this is not so far computing the probability of uh being between inclusively 3 and 20. but the reason why i'm computing these numbers is because i'm actually thinking i might want to use that complement trick again so i want to compute this probability where the length of the sequence is at least no is strictly greater than 20 and i could actually use the arguments that i was using above to argue that this is going to be a sum from n equals uh 21 to infinity of onehalf to the power n all right it's basically the same arguments that i was using before uh to compute the size of the sample space because this is going to be uh computing so you have the sum of the probably when you have 21 flips and then probably when you have 22 flips and 23 flips and so on and i could say what should i do next the thing is unfortunately i don't have a formula for when we're adding up starting at 21. i do have a formula when we add up starting at 1. so maybe what we could do is try to find a sneaky way to uh try adding up at 1 again what i could potentially do is say that n minus 20 plus 20 i could say that this is equal to the sum n minus 20 is equal to uh one because that's equivalent to saying that n is equal to 21 right certainly and then i could say this is one half uh to the power uh n minus 20 uh plus 20. hmm well i know i remember uh now that uh we can actually write this as uh we can write this as uh or the region the part enclosed in red as uh onehalf to the power n minus 20 multiplied with onehalf to the power 20. that's equivalent okay um so what can i use with that well i can now say that this is equal oops that this is equal to uh one half to the power oh i don't want red one half to the power 20 since that is effectively a constant that doesn't depend on n and i have a sum from n minus 20 equals one up to infinity of onehalf to the power n minus 20. uh but what i could do is say well n minus 20 uh you know what i'm just going to call that i i'm just going to call n minus 20 i'm just going to say that's i so replace all the n minus 20's with i so i equals 1 to infinity 1 half to the power i oh i know how to compute this i know how to compute this this is going to be uh one half times uh one half divided by one minus one half so actually this part right here i actually in fact this right here is pretty much a probability model uh this is summing over the sample space so this this part this blue part is equal to one so that means that this probability is equal to um is uh equal to one half to the power 20. think about that uh once again going back to this probability uh the probability of getting a sample of getting a string of at least 20 means that you must have flipped at least 20 tails and i've competed effectively the probability of flipping of getting 20 tails in a row oh all right now uh i'm still not done though uh let's uh start collecting this information um i now know uh let's see that uh so i'm gonna start erasing all this because i need to start reclaiming some space uh all right so uh this is equal to um uh one half to the power 20. all right so we can now say that the probability that uh of uh observing a sequence of flips such that three is less than or equal to the length of that sequence which is less than or equal to 20. i could work instead with the complement of this and say this is one minus the probability of drawing an omega such that either n of omega is less than three or n of omega is greater than 20. and here's the thing though i've basically written down two disjoint sets one set where n of omega is less than three or one set that is great where n of omega is greater than 20. um so so either a sequence ha is less than three flips or more than twenty twenty flips but it's not both so this is actually the union of two disjoint events and therefore i can say this is 1 minus the probability of drawing an omega such that n of omega is less than 3 plus the probability of drawing an omega such that uh the length of the string omega is greater than 20. and i have computed both of those probabilities so this is equal to uh 1 minus threefourths uh plus onehalf to the power uh 20 and i'm just going to leave it at that i'm not going to bother uh trying to simplify this because this is the correct answer so we're done at this point i'm not gonna go any further um i'm going to that said reclaim all this space and say um so hopefully you can rewind if you missed some of that and catch up on what you missed because i'm erasing this right now um and we're going to say that the probability of this event e is equal to 1 minus three fourths plus um uh one half to the power twenty all right uh next part what is oh yeah i don't want this this business all right uh what is the probability that an even number of flips is seen before the experiment ends hmm uh an even number of flips how could i possibly think about that uh so i want the probability of an e probability of an even number of uh flips and in short using some of the reasoning that i was using above i could say that this is the sum from of a onehalf how would i represent an even number i could say that an even number is 2 times a natural number that guarantees that the number the number is even so just take any natural number you want and multiply it by 2 and that's going to be an even number of fact that's going to cover all the even numbers and i'm going to sum this up from m equals 1 to infinity now what i do next well what i could do is recognize that i can basically take the m out and say that this is one half squared raised to the power m so that this is equal to the sum uh when m equals 1 to infinity of 1 4 to the power m oh i know how to sum that that's going to be since this is a geometric sum 1 4 over 1 minus 1 4 which is equal to 1 4 over 3 4 and those 1 4 parts cancel out so this ends up being onethird so the probability of an even number of flips is onethird which is kind of funny to think about if you think about it it seems like it like an even number of flips and an odd number of flips is equal equally likely but actually no they're not equally likely at all it's much more likely to have an odd number of flips since it's much more likely to get one one flip exactly okay um in fact getting one flip is twice as likely as getting two flips so by that reasoning it actually makes perfect sense okay uh moving on to the next part that was all a lot of work that was a lot of work okay uh example 14. in a small town uh 20 of the population is considered wealthy 30 of the population identifies as black and 5 of the population is wealthy and black select a random individual from this population everyone equally likely to be selected but i don't think that actually matters to this problem so much because of how i've laid it out what is the probability that an individual is wealthy and not black so let's see let me catch up in my notes here's how i like to think about this one with problems like this where you have uh let's let's start out by saying that we've got uh we've got a we've we've got a sample space but i'm not going to think too hard about what the sample space is for this one i'm just going to say we have an event b which uh corresponds to an individual being black or uh racially identifying as black uh w will be the event that a person identifies as no not identifies uh probably this is probably like some sort of a census business so the census bureau has maybe may have some uh more technical definition of wealthy like they make over ninety thousand is making over ninety thousand rendering you wealthy well you're certainly well off but um anyway that stuff aside we'll just say w that denotes the event that you are wealthy by some uh criterion and uh the probability of b that an individual is a is a black uh will be 0.3 and the probability of w that this individual is wealthy is going to be 0.2 okay and that's just true because the problem said so so now what we want to well actually we're also given another thing that the probability of an individual being both wealthy and black uh is going to be uh 0.05 oh by the way here's the thing to keep in mind um the probability of a and b is always going to be less than or equal to the probability of a or if you want you can replace the probability of a with probability of b it's just as true and that's because a and b is certainly a subset of a and if you think about um if you think about what about probabilities in terms of areas the area of a subset is certainly less than the less than or equal to the area of the thing it is it is uh nested within so you're always going to have a decrease in area it's not possible despite some human cognitive biases for the probability of a and b or something being true and something else also being true it is not possible for that to exceed the probability of the original thing is true i think people sometimes confuse uh intersection with conditional probability um that's probably what is going on when p when you start seeing those uh studies talk about stuff like that anyway uh we need to figure out uh what is the probability that this indiv that an individual selected from this uh from this uh village is wealthy and not black and here's how i like to solve problems like this and let's make it clearer all right here's how i like to solve problems like this i like to create a venn diagram and in that venn diagram i will start filling out regions so we'll have the wealthy circle and the black circle and we'll have this and we have the sample space uh and uh you start thinking about this like a puzzle like we know that the wealthy and black part is 0.05 and we're going to need to zoom in some more the wealthy and black part that is going to be 0.05 okay and then we have the wealthy part which is 0.2 but we can't point put 0.2 here because that 0.2 is also including that 0.05 so we need to put here is 0.15 so that when we add uh the this blue region and this red region those need to add up to 0.2 okay now for the black part um that needs to add up to 0.3 so that means that the part in that is not in the intersection is set to be 0.25 and i didn't write that very clearly um that needs to be point uh 0.25 okay which for what it's worth we now know the probability of w or b which would be 0.45 which means that the probability of being neither wealthy nor black is going to be 0.55 so that's something that i recommend when you encounter problems like these create a venn diagram and then fill out the diagram figure out the probability of every single little sliver in that venn diagram and then you can get any probability you want all right um and it will it will be like a genuine a generally useful chart for you not just for that individual problem because often these uh problems come in bunches all right so uh continuing on the probability of being wealthy and not black well actually that corresponds in our little chart up here to the blue region which is going to be 0.15 so this is going to be 0.15 okay what is the probability that the individual is neither wealthy nor black actually i already computed that i set it in words it's going to be 0.55 all right uh next example a ball contains a bag contains balls and blocks thirty percent of the bag's contents are balls an object is either red or blue and forty percent of the objects are red an object is made of either wood or plastic and sixty five percent of the objects are wooden ten percent of the objects are wooden balls five percent of the objects are red balls and twenty percent of the objects are red and plastic two percent of the objects are red plastic uh uh there's a typo here uh we're going to need to change blocks to balls okay reach into the bag and pick out an object at random each object equally likely to be selected what is the probability that the object selected is a ball red or wooden and this is inclusive by the way well this is another one of those problems where what i suggest you do is you create a venn diagram to represent the situation and fill out all of the little parts of that venn diagram so what does that look like here um all right so i've got my venn diagram this is my sample space uh right so i've got giant circles but i should probably be a bit more precise about uh let's start before i start filling out this chart uh let's create some notation and fill out what we already know in mathematical notation so we've got balls so b will be the event that we grab a ball and we'll say so objects are either red or blue so we'll say r is the event that an object pulled out of here is red so b complement is going to be a block and our complement is going to be a blue object and we say that so we can either have wood or plastic so we'll have a w correspond uh to the event that you get a wooden object all right uh let's start collecting some more information uh we have that the probability of uh drawing a stop it uh we have the probability of drawing a ball so the probability of the event b is equal to 0.3 the probability of getting a red object is a 0.4 the probability of a wooden object probably of a wooden object is going to be 0.65 uh the probability that an object is a wooden ball well that's w intersected with b since the object is both wooden and a ball that is going to be point one uh the probability of getting a red ball uh a red ball is going to be 0.05 the probability of getting uh something that's red and plastic so that's r and w complement that's going to be 0.2 and finally we have the probability of drawing a red uh plastic so w complement a block no no not a block it's no block it's a ball this is equal to 0.02 okay so so so so uh we need to start filling out this chart we're going to have uh we're going to have red here we're going to have balls here and we're going to have wooden objects over here uh i always recommend starting out with the smallest region first so the greatest intersection start out with that and then fill outwards so in this case the smallest region is red plastic balls uh where is that on our venn diagram uh you are in r you're in b but you're not in w so that actually corresponds to the blue region okay so the probability of the blue region according to the problem is 0.02 all right uh next up we could probably ask for uh what is the probability of being red and plastic uh red and plastic uh that corresponds to being um let's see so you're in the red region uh but you're not in the w region so that's going to correspond to this blue area and we know that the probability of being red and plastic is 0.2 but we've already got a 0.02 so that means that that in the other part we're going to be left with 0.18 so the probability of being a red plastic block is going to be 0.18 all right uh how about next uh we've got uh the probability of being a red ball so a problem the probability of being a red ball is going to be .05 what corresponds to red balls well this is the ball so we have the balls region and we have red stuff so that's going to be corresponding to the red region okay and that is 0.05 which means we've already got a 0.02 here it needs to add at 0.05 so that means that this little sliver in the middle is going to be 0.03 okay uh let's see we need next uh wooden balls so wooden balls corresponds to this green region uh the probability of being a wooden ball is point one so that we've already got a 0.03 so that means that we've got 0.07 in this other little sliver all right we're getting close to done uh what about uh let's see so we've got uh wooden balls red balls red plastic things uh what else have we got well we can we've got the probability of balls the probably balls is gonna be point uh 0.3 and that so balls correspond to this blue region and we've already got um 0.12 of the area accounted for the total area needs to be three point three so that means that for this uh remaining sliver uh we have point one eight for its area okay uh next up next up well we've got um the probability of red stuff and we know that the probability of red stuff is point four so point two so red stuff is going to be uh the region i've just encircled in red and so far 0.23 of that area is accounted for but point its total area is going to be 0.4 so that means that only so that means that when we're filling out uh this remaining blue sliver it must be 0.17 okay and for wooden stuff we know that the total area in the wooden region it's going to be 0.65 and 0.27 of that area is already accounted for so that means that the remaining area is going to be 0.38 all right and we are almost done we actually in fact know the probability of everything else if you were to add up all of those probabilities they would add up to 0.98 so that means that the remaining area that is outside of this region is 0.02 all right we have filled out that diagram and we are ready to answer some questions what is the probability that an object selected is a ball red or wooden so that's the probability of r or w or b and we have already figured that out that corresponds uh to the 0.02 so that is equal to 0.02 okay uh next up what is the probability that the object is a red wooden ball uh so the probability of being red of being red and wooden and a ball what is that going to be well we're going to go back to our diagram that we created we look for red wooden balls so you need to be in the red circle you need to be uh wooden so you need to be in the blue circle and you need to be a ball so you need to be in the green circle uh oh i guess that's a darker blue so that leaves us 0.03 so that leaves us 0.03 for that for uh that area so this is going to be 0.03 all right uh what is the probability let's go ahead and do some zooming out we no longer need such fine control when drawing uh what is the probability that an object is a blue plastic block well the probability of being a blue which is the complement of red uh not in a color sense not like when artists talk about complements that's a red is not a complement of blue but whatever in a probabilistic sense in our probability model uh so that's the reason that we've drawn here and it's actually a problem i'm gonna leave this to yourself but look up de morgan's laws uh learn about de morgan's laws uh for my students this is an assignment um i believe if i remember right but basically this is r union w union b complement which then means it's 1 minus the probability of r union w union b and that is a probability you've already figured out that's 1 minus 0.98 which is equal to 0.02 okay okay so that's it for the examples those are a lot of examples but i'm just gonna we're now wrapping up this section and i'm going to wrap up this section with a short discussion on the interpretation of probability now it turns out this is actually a rather large topic philosophers actually are debating uh what do probabilities actually mean and what is an appropriate definition of uh of uh of what a probability is because we have a mathematical notion of probability but that doesn't necessarily translate into a realworld notion of probability now i'm going to leave that discussion aside i might make an aside video about the interpretation of probability but in this class despite the limitations of this interpretation we're going to adopt the frequentest interpretation of probability which is interpreting probabilities as long run frequencies of events so in other words if we were to repeat an event an experiment many many many many times and each of those repetitions were independent of the past uh we would the sample proportion of times uh we would see that experiment occur would approach that probability so here's kind of and i here is a chart that kind of illustrates this idea we're going to have along the yaxis the yaxis ranges from 0 to 1 and somewhere along here we have the probability of some event uh e uh stop being uncooperative tablet all right so we have the probability of an event e okay and uh along the xaxis well it's not really the xaxis this time it will be the naxis and we're going to mark this axis off at integer values 1 2 3 4 and so on and um uh we are going to track uh p hat n which is the sample proportion of times the event e occurs and you remember how to compute sample proportions uh but here's kind of the thing about what we're doing here uh we are remembering pat the past so you could imagine that we uh are flipping a coin and we're tracking how many times we see heads in this uh experiment and uh uh so the first time you flip it you're gonna have either one or zero heads and uh the second time you're going to keep the results of that first flip but then recompute your proportion for a sample size of two uh for the third time you then flip the coin a third time and you keep the previous two results uh and then recompute the proportion using those previous two results and also the third flip you just did uh and so on keep doing this in a sequence so uh what could possibly happen so i'm going to put a little dashed line at the probability of e according to the frequentist interpretation of probability uh the probability is basically this longrun frequency where if you're tracking p hat n you'll start out at um like maybe zero or one depending on whether the event happened on the first trial or not and then you might track that it happens on the second trial and the third trial and the fourth trial and so on and what will happen is this line will get very very close to the probability of e um and that's how frequencies think about probabilities as a long run proportion as a limiting proportion um now the there is an unfortunate thing about this uh interpretation which that actually in mathematical probability theory that is not an assumption that's a theorem so this is not something that we assume is true this is something we prove is true so why are we assuming that something is true that we then prove is true that that seems like a circular reasoning and it's unfortunate but i'm going to leave those philosophical issues aside uh because at some level my own personal belief is that all of those technical philosophical issues aside at some level it's almost a limitation of human language uh and probability theory despite our difficulty in coming up with a definition a proper definition for it uh is a very real phenomenon so whatever the definition is it matches this frequentist notion uh i've actually got some r code here that you can look at uh this functions r is able of capable of generating random numbers and set seed is what's known as sets what's known as a random seed and make sure that when you're generating random numbers they actually are going to come up with the exact same results uh the same time so if you run this code it will produce the exact same results uh at least in principle uh different versions of software may cause different results but uh whatever so i set the sample size for my experiments uh i conduct a number of flips i track uh i accumulatively track the number of heads in these flips and then i plot these uh sample proportions and also draw a line at the theoretical probability of that event so this is the case when you were to if you were to flip a coin flip 15 times and keep a running proportion of how many times you saw heads this is what it looks like um and you can kind of see it seems to be converging around 0.5 if we repeat this experiment but this time when n is equal to 50 what we'll see is it kind of looks like it's getting even closer to something constant and then if we were to repeat this again but now uh the sample size is 500 it almost converges it almost converges to the truth now that said with probabilities it never means that you are guaranteed to see that many in any single sample you we did not say that uh which means that at the very end here you're not actually at one half you're pretty close to one half but you're not at one half but if you were to just keep going on like this and you were in fact possible for you to continue forever and god suddenly grants you the ability to live forever so long as you continue to flip this coin at the end of eternity you will have one half a flip uh half of your flips will be one half that's basically what we're saying all right so that's it for this video quite intense quite intense it's probably giving you some idea that probability can get rather out of hand um at least in terms of uh computational complexity uh but uh i've given you a number of useful examples and a number of useful techniques the thing though is those techniques like they're useful now and it's good for you to see them but you always kind of have to adapt to individual problems the next section is going back to the situation where you have a finite sample space we're going to talk about how you count elements from that sample space this part is both fun and frustrating because it's fun because we get to talk about things like poker problems and gambling and stuff like that but it's frustrating because counting is hard counting is really hard and also like i would love it to give you a magic formula that solves every and all counting problems and i will give you some formulas but those are kind of those are just tools that you kind of need to combine and the combination of those tools that's the hard part and i can't really give you a general principle for that every counting problem is kind of its own thing but enough of that we're we're calling it for now uh i will see you later in the future video on counting let's move on now to the section on counting techniques to start things off let's suppose that we have a burger shop that we're going to call and just off the top of our head bob's burgers and uh they're offering three types of bread we have white bread rye bread and sourdough a burger can come with or without cheese how many burgers are possible well let's see uh we have uh let's let's come up with some encoding for our possibilities uh we have white bread we have rye bread and we have a sourdough and a burger can come with or without cheese so we'll say we can have with cheese or we can have no cheese all right so given this encoding uh what are the possible burgers that we could have so we've got our possible burgers and we could have a burger with white bread and cheese a burger with rye bread and cheese and a burger with sourdough and cheese or a burger with white bread and no cheese a burger with rye bread and no cheese and a burger with sourdough and no cheat and no cheese so let's see how many burgers is that that's going to be six burgers uh so that's the way to do it by hand where you basically enumerate all the possibilities for uh how many burgers are going to be but the thing though is i mean that's only going to work for so long i mean you can you can enumerate stuff when it's possible to do and then literally count uh how many possibilities there are but there's also other ways to possibly represent what's going on here for example we could use what's known as a tree diagram with the t tree diagram we're going to create a tree that visualizes um the branching possibilities of our choices so for example at the first node we're going to choose the type of bread so we have white rye and sourdough and then at the second level of node we decide whether we're going to have cheese or no cheese so we have um for the upper branch cheese and for the lower branch no cheese and we're going to do this a few more times for the different types of bread that we could have chosen and we've got cheese no cheese cheese no cheese and then we're going to count how many no's there are on the end points and the number of nodes on the endpoints will correspond to the number of possible choices in this case there are six nodes so that means six possibilities and of course one thing that's nice about these types of tree diagrams is that we could allow for more flexibility in our possibilities such as perhaps for some reason bob has decided that he is not going to permit a sourdough bread without cheese in which case you just remove that possibility from from the from the tree diagram you remove that branch in which case there would now be five nodes in the branch so it's pretty general uh there's also something that we can use uh called the product rule uh proposition seven if there are n1 possibilities for choice one and two possibilities for choice two all the way to nk choice of possibilities for choice k then the total number of possible combinations is going to be the product of the number of possible choices we can make at each point so uh let's use the product rule to answer this question according to the product rule we could have three possible choices for the first node and two possible choices for the second node and thus the total number of possibilities will be three times two which is equal to six all right so we've seen so far uh six ways to basically answer this question uh now let's uh move on some more uh the sandwich shop deluxe deli offers four bread options where we have white sourdough whole wheat and rye five meat options turkey ham beet beef chicken or no meat six cheese options cheddar white cheddar swiss american pepper jack and no cheese with or without lettuce with or without tomatoes whether without bacon with without mayonnaise and with without mustard how many sandwiches are possible let's see uh how many decisions do we have to make first we need to decide on our bread option so we'll say that that's uh decision one so we'll say that n1 there are uh four possible options so four then we can decide uh our meat options and there's uh five meat options uh we have six cheese options so n3 will be six uh there are and then for the remaining options it's all binary so we've got with or without lettuce so that means that n4 equals two with or without tomatoes so n5 equals two with or without bacon so n6 equals 2 with or without mayonnaise so and 7 equals 2 and with or without mustard so n 8 equals two okay so based off this how many sandwiches are possible well according to the product rule we're just going to multiply all of those numbers together so we've got four times five times six times uh two five times so we'll say two to the fifth power so this when you multiply all this out you're going to end up with 3 840 possibilities for this sandwich shop okay so uh moving on here's here's the thing we are now uh very fully into the realm of combinatorics combinatorics is generally figuring out how many ways there are to do things and uh combi or how many combinations there are of things how large a finite size sets are when they're generated with certain rules and honestly combinatorics is pretty hard like for example i find combinatorics rather challenging um i remember once when i was in a probability class and i was in uh excuse me i was in office hours with a professor and a fellow student of mine just we were discussing combinatorics and uh my a fellow student of mine was just like is this what probability is or statistics is because if it is i i don't know if i want to do this and he was like no it's not uh you just kind of have to do this you have to learn this at some point but it's not what the bulk of probability is and i can understand why she said that because it can get pretty painful here's the thing about combinatorics uh what i'm about to do is give you some tools some counting tools for problem for counting type problems that are often reappearing but the thing is every single combinatorics problem is its own thing it's really hard to come up with general tools like what i'm coming up with here um there are certainly tools in the toolbox uh things that you kind of look out for when you're solving combinatorics type problems but you still need to think of it of each problem as its own thing and for this section i kind of have to come up with a number of examples and run through those examples to give you an idea of the thought process of combinatorics but it's really hard to teach that thought process uh without example since i can't just give you an algorithm that will solve every single covenantoric's problem every problem is its own beast and you're pretty much forced to think very carefully about uh the process by which a single combination has been formed uh also i should probably just mention uh if we have uh there's there's something that i didn't mention in these uh typed up notes but i call this the sum rule uh so uh if you're choosing uh from k uh k disjoint sets so sets each with uh each with um we'll say n sub i uh possibilities uh possible choices uh at one stage so you're gonna pick uh one of these sets and an item from that set uh then in this case uh there's going to be the sum from i equals 1 uh to k uh n sub i uh possible so possibilities so in other words if you need to choose an item from bin a or an item from bin b exclusively then the number of ways you can make that decision is going to be the sum of the items in those two bins not a particularly difficult concept but i'm going to just lay it out for you right now just so that you are aware of that and it's a it's a valid approach to solving cognitoric's problems and i would say that the the uh techniques that i'm about to show like these will these will get you through these are things that you look out for when solving combinatorics problems uh but uh they probably won't take you all the way all right so uh when we're solving some problems uh suppose that out of n possibilities we will be choosing k we have two essential questions to answer um are we choosing with or without replacement and does order matter depending on our answer to those questions we're going to have different solutions that are summarized below uh here uh if if the choice was where uh there's order and we're doing so with replacement then the number of possible outcomes is end of the power k uh ordered without replacement that's known as a permutation and here is a formula for a permutation uh n factorial by the way so this so uh n with an exclamation point means n factorial and that is n times n minus 1 times n minus 2 times dot dot dot times 3 times 2 times one that is n factorial and by convention zero factorial equals one huh that's kind of strange uh at least for me when i first saw that zero factorial equals one i found it rather surprising but actually it makes perfect sense for zero factorial to equal one and i might may explain why zero factorial is one in a separate video but we're just going to leave it at that so that is what n factorial is um and one way to think of what n factorial is it's it's the number of way to order number of ways to order n things so it's the number of permutations of n things um uh now in this case for uh ordered with without replacement uh we we all we're also calling that a permutation uh it's just a permutation of k things out of n so uh suppose that uh we are that we don't have replacement so remember what we're so think about what replacement means it means that uh so replacement means that if you choose uh an option you can choose it again so you're allowed to choose it again an example of of ordered with replacement is a sequence of heads and tails in coin flips where you may consider possibly you may not but if you're looking at a string there are different ways that heads and tails can manifest themselves in the string and furthermore if you get heads on the first flip you're still allowed to get hats on the second flip whereas if you don't have replacement you can get heads in the first flip but you can't get heads on the second flip so that would be without replacement in order matters that means uh you're that means that the order in which you see an item in a sequence matter so you're tracking that so headsets tails is different from head's tails heads whereas if order doesn't matter then headset's tails is essentially the same as heads tails heads because you don't care about the ordering of the items so that's what we mean by whether without replacement or with or without order okay so suppose that we're choosing items where order doesn't matter and uh we don't have replacement in that situation uh we have we're going to use the formula n choose k which is n factorial divided by k factorial uh and or the product of k factorial n minus k factorial i'm going to prove each one of these formulas in a second because i do believe that the proofs are enlightening on the combinatoric thought process and the combinatoric thought process is something where you just kind of have to get exposed to it a lot in order to be able to uh think that way yourself um finally we have the situation where uh you are you're choosing with replacement but order doesn't matter in which case you're going to have k plus n minus one choose n minus one okay so next up is uh the uh the proofs or the justifications for each of these formulas we're going and i'm going to uh prove this in um a sneaking fashion or a clockwise matter because often the formula after the formula in this clockwise order from before is going to be used in the next proof okay so uh let's get started uh we're going to start by showing by proving the formula and we in which case we have ordering and so and we also have replacement all right so ordered with replacement i'm going to zoom in because i'm going to need some uh i'm going to need more space all right so we want to come up with the formula um n to the power k well all right so we have k choices to make and since we have replacement we're going to use the product rule the product rule is kind of the underlying uh rule that we can use for all of these so starting out with the product rule we have k decisions to make i like to think of confidence of a combinatoric problems where you are describing any combinatoric problem how to construct an instance of an element of this set and when you are thinking about how to construct an element you construct a narrative for how to uh construct one of these elements and you're counting how many ways there are to make the decisions along the way and you're tracking how and you're tracking how many decisions you need to make and using the product rule the entire time so uh we can think of if we're doing ordering with replacement we might have for example we might have for example three spaces and we need to fill up those spaces with elements so we might have two possibilities for each space so at the first space we would have two possibilities and at the second space we'd have two possibilities and at the third space we have two possibilities and we care about the ordering and um we care about the ordering of what we put in in these spaces so the number of possibilities for filling up these three spaces would be 2 to the power 3 because we're going to multiply and we have to make a decision at each space and when you have to make a decision at each space you multiply the possible number of decisions that you needed to make so uh generalizing this idea uh we have uh we need to make a decision in the first slot the second thought the third slot all the way to the kth slot and for each of these and then and for each of these slots there were n possible things to choose one from because we're not taking we're not removing options as we go through our sequence of case slots so then by using the product rule according to the product rule we end up having to multiply um we have we end up having to multiply uh each of our possibilities or each of the possible options we can make at each slot but since all of those are the same since all of those are the same number you end up with multiplying n k times which is n to the power k so that gives us the first formula the next formula we need to come up with is ordering without replacement which gives us the number of permutations okay so we now next have oops that is definitely not what i wanted all right so next up we have ordered without replacement i'm going to go back for a second and i have in my head prototype problems for each of these four scenarios so ordering with replacement is like determining how many strings of heads and tails there are in a string of flips because the ordering in that situation would matter because your track because heads tails it's not the same as tails heads in a string and you have replacement because if you get heads on the first flip you can get heads again on the second flip um ordering without replacement to me my prototype problem for that is forming a list because the item that you put at the top of the list cannot be chosen for the sec for the next item in the list so you are removing items as you go through this list and maybe it's like a times top top 100 people or 100 people for the year list where the ordering matters and there's certainly more people than 100 so they might have this larger this larger master list of people they would consider to be candidates on their top 100 list so you're going to choose people to be in certain slots on this list and when you choose a person to be on the list you can't pick them again so that's kind of my prototype problem for ordered with without replacement for um not ordered and with replacement this is like uh poker problems because when you draw cards from a deck of cards you are allowed to reorder the cards in your hand so there is no ordering but once you draw a card you cannot draw it again so there isn't replacement and for the final situation my prototype problem here is choosing a dozen donuts when you have uh so many flavors of donuts because you are allowed to reorder the donuts in the box and in principle the donut shop has this almost infinite list for the number of or an almost infinite supply of donuts in this imaginary donut shop so there is replacement when you choose a flavor like when you choose a flavor you're allowed to choose as many donuts of that flavor as you want so a replacement is not an issue all right so you do have in fact replacement those are my prototype problems and those are good to keep in mind when we're going through and trying to think about what formulas work we should have and how we're going to prove these formulas and additionally uh it's kind of good to have these prototype problems in your mind when you're trying to solve common torque problems yourself all right so next up uh getting back to these proofs uh we need to show the formula for ordering without replacement uh so i had that formula for n factorial um so all right so how many ways are there to uh form a list of k things uh when you have n possibilities so we would need to pick the first item in the list and the number of ways that we could pick the first item in the list is n then we need to pick the the so there are n ways to pick the first item in the list then we need to pick the second item in the list so there are going to be n minus one ways to pick the second the reason why is that whatever we chose to be our our first element in the list cannot be chosen again to be the second element so we have n minus one ways to fill the second item then we have n minus two ways to pick the third the third item because now now in addition to not being able to pick what we chose for the first item in the list we also can't pick what we chose for the second item of the list so we're going to have n minus two way uh mis two choices now so mi it's two ways uh for the second item no no not the second the third and we just continue on with this uh with this process until we reach the kth slot uh and there will be uh for the kth item in the list there will be n minus k minus 1 ways to pick that item which by the way is equal to n minus k uh plus one right so so n minus k plus one ways to pick the cave item so then we're going to use the product rule and by the product rule we multiply all of these numbers together so we're going to get so the number of possibilities is going to be n times n minus 1 times dot dot times n minus k minus 1. and if we wanted to we could stop there this is in fact the formula but thing though is it's sometimes uh better to we we might prefer a more compact uh formula so we might say instead that um we have n n minus 1 n minus 2 uh dot dot dot and then we have n minus k plus 1 and then we could keep going multiplying and say we're going to multiply by n minus k and still you know still decreasing down but but we've now multiplied by n minus k and if we're going to multiply by n minus k we now need to divide by n minus k in order to keep things balanced and then we're going to multiply by n minus k minus 1. okay so we're going to need to divide by n minus k minus 1. and we're going to keep going with this process on both the top and the bottom until we're multi we multiply three two and one and what i've actually written down in doing this the top can be recognized uh so the top uh part of this uh fraction you you may recognize that as being uh i don't want that color you may recognize the top part as being n factorial and the bottom part of the fraction the denominator as being uh as being n minus k factorial hence uh producing the formula this will produce the formula uh n factorial over n minus k factorial which we may also just call p and k and that gives us our second formula okay uh next one very now what we now need to do is figure out how many ways there are to choose items that are not ordered and there is no replacement this gives us the formula that is often referred to in english as and choose k so the proof for this one is actually quite tricky where you start out by assuming that you know how to do it and then you get recover the formula in the end where you you pretend that you know the formula and after you pretend that you know the formula you then figure out something that you already know which in this case is the number of permutations of a list and once you have the number of permutations in the list you're able to recover uh the formula that you pretended that you knew but you actually actually didn't all right so that's that's rather convoluted let's get started with uh uh showing how we can get this formula so the proof again is kind of weird it's a really weird one but all of a sudden at the end we're going to have the result that we wanted so we have no order and we don't have replacement i keep touching that all right i keep touching it alright so not ordered without replacement so suppose uh n choose k is the number of ways to is a number of ways to do this so we know how so we know the number of ways to choose items when we don't care about order and we don't care about replacement and the number of ways to do so is n choose k okay bear with me i now want to know how many ways are there going to be to pick to pick uh k items out of n when replacement doesn't matter so as opposed to um doing so in order doesn't matter which we're assuming that we know uh that is what i want to do i want it what i want to do is calculate uh uh p and k or the number of of k length permutations of n objects okay so uh i want to compute that which by the way we have already computed that's that formula is actually already known to us it's up here but we're going to suggest that there is an alternative way to calculate this if somehow you knew the number of ways to pick k items out of n uh without without order so how would we do this how would we let's think about how we would construct a single permutation if what we had to do was pick items without order first so our first step if we were to attempt to form a single list of k items out of n when we can pick items without ordering them the first thing we would do is pick the number of items or pick what items will appear on the list without ordering them first so our first step in our process is to pick objects when order doesn't matter so you've decided basically what's going to appear on the list you just don't know in what slots it will appear and supposedly we know how to do so there's and choose k ways to do so so there are n choose k ways to pick the items that will appear on our list without ordering the items so first we pick a set of items that will appear on the list the next step then is to order those items so the second step is to order the k objects so how do we order the k objects well uh we picked the first so we pick uh one of the objects that we have selected to be the first item of the list uh there's k ways to do that then we pick another one of the objects that we haven't picked yet to be the second item on the list there's k minus ways to k minus one ways to do that keep doing so until you run out of items so there are uh k factorial ways to order the items oops oh darn it so there's k minus one ways to order the items going back to where i was or k factorial ways to order so now we're going to use the product rule and say that the number of ways to pick uh items to appear on our list is going to be the number of is going to be the number of decisions we have to make in step one which is the number of ways to pick the items to appear on the list uh there's n choose k ways to do that uh and then multiply that with the number of ways to order the items so there will be k factorial so that's the number of ways to form permutations but we also know that the number of ways to get permutations from what we did before is n factorial divided by n minus k factorial okay well what we actually were interested in this whole time was calculating this number that i just highlighted in red that's the number we actually want well how can we get that with division because now we have an algebraic relationship uh with which we can so that we can solve to get and choose k and it follows that n choose k is equal to n factorial divided by k factorial times n minus k factorial and we're done we computed what we actually wanted to compute so that was a little odd that was a little strange um here's some more ways to kind of justify uh this formula that we ended up with uh so the number of permutations is larger than the number of combinations because the because when you are sensitive to order you're going to end up with many more possibilities than if you're not sensitive to order so as a result you need to divide out uh by a factor that effectively removes uh all of the orderings that are that contain the same items that just in different and just in a different arrangement or to get the combinations and when it comes to computation like let's say for example three choose no uh five choose three um this formula for me at least when i was taking this class the way i think of it is i have five factorial on the bottom and on the bottom i'm going no i have five factorial in the top and on the bottom i'm going to have 3 factorial and whatever it takes to get 2 and the other number such that 3 and the other number adds up to 5. so i'm going to have 5 factorial divided by 3 factorial times 2 factorial okay so that's a way so the formula is actually not that hard to remember because it's like okay uh 10 choose 7. that's going to be 10 factorial divided by well we've got 7 factorial down there and we've also got something that adds up to ten okay three factorial so ten factorial divided by seven factorial times three factorial so the formula itself is not that hard to remember uh but you now have it okay uh continuing on so that was our third formula that i promised that we were going to prove uh so now we have one last formula to prove uh and that formula is let's see how much space we got uh hopefully we've got enough so the last formula we have uh not ordered and we have replacement so this is the donut shop situation okay so the donut shop uh situation uh here's like this this proof is again also rather tricky combinatorics often requires tricks honestly that that's that's one that's one reason why combinatorics is rather painful uh it feels like there are rather few unifying uh principles and combinatorics i know that there are some but it doesn't really seem that there's all that many so here's what we're going to do to um to solve this one uh what let's let's suppose that we're in a donut shop and we're going to choose say five donuts of three flavors uh how could we possibly do that uh we don't care about the ordering of the donuts because you put in the box and i mean no one's going to ask in what order the donuts were when you come home so what i like to do is imagine okay whatever we're going to do we're always going to arrange the donuts in the box right so so whatever arrangement they originally were we're going to put them into a fixed arrangement where we have one flavor first one flavor second and one flavor for flavor third so what we could end up doing to form a single box after we say that we're going to arrange to rearrange them at the very end is we're going to have uh we're going to uh put down all the donuts of a single flavor and we know that the donut that comes first is going to be of a certain flavor but we're also going to put dividers in our box to separate flavors so so the second flavor will come after the first divider and the third flavor will come after the second divider so and it is also possible to uh not have a donut of uh certain flavors so for example if we wanted a box of only the third flavor we put a divider in the first position of divider in the second position and then put donuts in all the remaining positions and with this encoding i have now encoded one of the one of the boxes of five donuts this is a box where you have only the third flavor and you can imagine what a box consisting of only the second flavor would look like so you'd have a divider at the beginning and a divider at the end and then in between you have your donuts so this is a box that has only donuts of the second flavor and maybe play around with this encoding of donut boxes but once we have this encoding it is now possible to calculate how many how many boxes we can achieve because what we can do is say how many ways are there to pick positions for dividers and positions for donuts how many ways are there to do that well we're going to have if in the if we have uh three flavors we're going to have two dividers and if we have five donuts we're gonna have positions four or five donuts so we're gonna end up with seven positions that we need to fill up uh we could just pick two of the seven positions to contain dividers and the remaining positions will contain donuts so let's see we could potentially pick numbers we could assign a number to each of the positions so we have positions one two three four five six seven and we pick numbers that represent positions that will contain dividers so in this case uh uh numbers four and six represent positions four and six so those will be uh so if we pick four and six which by the way is the same as picking six and four to contain dividers so in other words we don't care on the ordering of the numbers that we end up picking uh once we pick that we now know what our box of donuts is going to be because we've picked the positions for the dividers and therefore every other position will contain donuts okay all right then um so if that is the case how many boxes of donuts are there going to be um so in this case the number of possibilities which is n in this scenario is equal to three and out of that n we're going to be choosing uh k possibilities uh or we're going to be choosing k items or k donuts so for this problem k is equal to five okay so um how many ways were there to make this decision well it turns out uh there were um there were five plus three minus one choose three minus one ways to pick donuts in this fashion because we end up picking the or in other words this is uh uh this is seven choose two because we have seven slots and we pick two of those slots to contain dividers and the rest of the slots contain donuts hence we get the number of donuts all right so let's generalize this idea so if we have n possibilities we're going to have n minus 1 div n minus 1 plus k slots because we're going to have n minus 1 dividers and then also the case slots for the k things that we're going to end up picking so in general um uh so let's see uh so we're so we have um so we have uh n that no uh k plus n minus slots n minus one slots uh to fill with uh n minus one dividers which we call uh which we're going to call the the bar uh remaining slots so the remaining slots contain items uh so we need to pick uh so pick the positions let's see yeah so pick positions for dividers and order doesn't matter so since order doesn't matter there are going to be uh n plus uh no uh i like a different ordering uh there are going to be k plus n minus one choose n minus one ways to do so and we're done you end up with the formula and we're done so that was exhausting uh this is rather tricky types of mathematics that honestly it's it's frustrating because it often just requires knowing special tricks in order to solve a certain problem it just feels like all you're doing is coming up with a longer and longer list of tricks and it doesn't really feel like there's much of a unifying principle to them uh at least to me at least to me personally it's it seems also to me like there are some people out there some really smart mathematicians for which this type of thinking just for some reason just clicks and they and they are able to see an underlying principle i don't see it neces uh often uh but i often know enough combinatorics like this co this amount of combinatorics will get you very far in life uh in your statistics life that is uh you don't really need to know that much more than this all right uh so those were complicated proofs uh now we're gonna go through a series of examples to show uh how these techniques can be applied so uh suppose we're going to roll two sixsided dice and we assume each outcome is equally likely and we're going to say that the dice are different colors so uh if the red dice has a six and the blue dice has a one that's different from the red dice having a one a blue dice having a six those are two different pop those are two different outcomes how many possible outcomes are there and what about the situation when there's three six sided die uh so we're to first answer problem that i'm going to call one and the problem that i'm going to call two all right uh so the answer to one uh order matters and we're doing so with replacement because if we roll um a one for the red dice we're still allowed to roll a one for the blue dice so we end up with uh there are six possibilities and we're choosing two of them so we get 36. all right for the next option uh well it's just the same things now there's we're just choosing three instead of two so that's going to be 216. so this is a situation where order matters because the dices the dice are different colors and there is replacement because dice don't care what the other dice rolled all right uh next example a high school has 27 boys playing men's basketball in basketball there are five positions point guard shooting guard small forward power forward and center each assignment of player two position is unique how many teams can then be formed this is a this is a permutation type problem so here order matters because there's different positions and each of those positions are distinct but there is a replacement because a person playing point guard cannot also be center so position doesn't matter oh no position matters or order matters no sorry sorry no replacement no replacement i get it eventually all right so no replacement so that means we're going to be using that a second uh that second formula in the clock so we've got uh p uh there's a 27 possibilities we're going to order five of them so that's going to be 27 factorial divided by 27 minus 5 factorial which is 27 factorial over 22 factorial which also can be written as uh maybe more simply into the point 27 times 26 times 25 times 24. times 23 that's almost easier than remembering the formula just remembering that you decrement that many times and this multiplies out to uh 9 million six hundred and eighty seven thousand six hundred potential teams okay uh example nineteen when playing poker players draw five cards from a 52 card deck every card is distinct but the order of the draw does not matter you are allowed to reorder the cards in your hand how many hands are possible in this situation because you're allowed to reorder order doesn't matter and since order and also an additional ordering not mattering there is no replacement because if you draw an ace you can't if you draw an ace of spades you're not allowed to draw an ace of spades again so no replacement okay uh so let's see if that's the case then we're going to use that fourth third formula in the clock uh we have uh 52 possibilities for 52 cards we're going to choose five of them when we draw cards and that's going to be 52 factorial divided by 5 factorial times 47 factorial which is equal to 52 times 51 times 50 times 49 times 48 divided by 5 times 4 times 3 times two times one okay uh and we and we don't care about the one because it's times one that that doesn't really do anything um we can do some cancellation like for example the 50 and the five cancel down to a ten uh the 4 3 and 2 those met multiply the 24 so that reduces with the 48 rendering it a 2 so this is equal to 52 times 51 times 10 times 49 times 2 which you then go to your calculator and it will give you 2 million uh 500 and thousand uh 960 poker hands now r can do a lot of these calculations so for example 16 you can just say what is 6 to the power 2 and i'll tell you that the 36 and 4 6 to the power 3 that's 216. r does have a factorial function now be careful with some of these functions because you might end up with integer overflow you might end up with numbers that are so large that uh the computer cannot handle them so i would not just blindly use these functions because it is possible for these numbers to explode very rapidly in which case your calculations will end up being wrong but we do have a factorial function and we do have a choose function so here's example 17 and here's example 18. um uh so or is that that number might be wrong yeah that that that numbering's wrong my apologies uh so i guess at some point when i wrote this r code uh i uh or when i wrote these notes i must have deleted an example a long time ago but i did not change the comments in the r code this is why i need to be very careful with comments comments expire eventually they turn bad all right and you know what's worse than a note then no comment a misleading comment that's even worse um all right so example 20 you want to choose a dozen donuts from a donut shop there are eight different kinds of donuts how many boxes of a dozen donuts are possible well okay so in this situation uh we are choosing 12 donuts and there are eight possibilities so using that uh fourth formula in the clock we have uh 12 plus 8 minus 1 choose 8 minus 1 possibilities which is going to be 19 choose 7 which is equal to 19 factorial divided by 7 factorial times 12 factorial which is equal to 19 times 18 times 17 times 16 times 15 times 14 times 13 divided by 7 times 6 times five times four times three times two times one all right and uh let's do some cancellation to help make our life a little bit easier uh let's see the 7 and the 2 are going to cancel with the 14 we've got what else 4 and 16 will reduce the 16 down to 4 the 5 and the 3 will cancel out the 15 and the 6 cancels out with the 18 reducing it to 3 so in the end we're going to have this is 19 times 17 times 13 times three times four and then you go to your calculator and ask what that is and you'll get fifty thousand uh 388 potential boxes of donuts from this donut shop okay and here is some arco that will also compute that quantity okay uh so let's do some classic poker problems once people uh introduce combinatorics it's like the next thing you have to talk about are poker problems because poker problems are fun because poker is fun it's fun to talk about poker um so um now that said there is uh one potential was talking about poker problems is that unfortunately not everyone is familiar with the poker deck or the standard playing card deck as it's known in the englishspeaking world which for what it's worth the standard deck is technically the french deck and different european countries have different traditional playing card decks so like for example my advisor leo horvath the traditional deck that's used in hungary is not the french deck so he doesn't like personally poker problems because there's something that like they talk about a deck and he's not very familiar with that deck and it just goes against his intuition whereas i myself i grew up with this deck i grew up in america so i'm very familiar with uh with what is inside of a playing card deck that said if you're like an international student or something like that here is a description of what is inside of a playing card of a standard playing card deck or a a french deck the deck that's used in the english speaking world and often used in these probability textbooks because most of these probably authors are most most of these most of the authors of these uh probably books they may be in america but they're certainly speaking english they're probably using uh this deck so and here's some additional notation for uh this is a poker notation to describe what goes inside uh what cards are inside of a deck all right so but basically you've got four sweets and 13 possible faces uh all right for a total of 52 possible cards you should know how to do that by now because uh how do you form a single card you first pick a suit there's four suits possible then pick a face value there's 13 faces so 13 times 4 will be 52. all right so you've learned something today uh example 21 a poker hand is four of a kind if four cards have the same face value how many four of a kind hands exist how are we going to solve this problem well uh the trick that we're going to use again with these poker with these not just poker problems but most combinatoric problems what i suggest that you do is come up with a narrative like i just did right now for figuring out how many cards there are in a deck come up with a narrative for forming a single combination and then once you have that narrative figure out how many choices there were to make at each at each junction and multiply them together with the pop with the uh power rule and you'll get what you need um so first thing we're going to do to form a four of a kind hand is we're going to decide what card face value will be the four of a kind card so first we're going to pick uh the four of a kind card so we're going to say for example they're going to be four kings in this hand or uh four twos or four tens something like that so we need to pick a face value and there are 13 face values to be the four of a kind part right so there are 13 ways to pick the face value then we need to pick the remaining card because a poker hand has five cards we have picked four of those cards if we decided that we were going to use the ace we were going to use ace for the four of a kind card then we've automatically got the ace of spades the ace of hearts the ace of diamonds and the ace of clubs that's four cards now we need to pick the remaining card if we picked ace then we cannot pick the ace face value again so uh we've uh and in fact in that 52 card deck we have taken out four of the cards and put them in our hand leaving 48 cards remaining in our deck so that means that we have 48 cards to be the potential fifth card so pick the fifth card so 13 times 48 you multiply those two numbers together to get 624 poker hands with four of a kind uh next up a poker hand is considered full house if two cards have the same face value and three different cards have another common face value how many full house hands exist let me just list out for you an example of a full house hand you could have let's say the four of diamonds the four of spades so the four of spades and uh the four of uh hearts and uh so that's three cards with a common face value and then we need two cards with another face value we can't pick four for this so we're going to pick uh let's say king so we'll have the king of uh clubs and the king of uh diamonds so this would be a full house hand so let's see how can we form a full house hand in general well we're going to have two different face values in our full house hand but one face value will be the more numerous face value the three of a kind and the other face value will be the two of a kind so what i suggest we do is first pick the card that will be the three of a kind card so pick the card that will be the three of a kind card so pick the three of a kind face value and there's 13 face values for this choice so there's 13 ways to make that choice then we need to pick the suits because we picked for example four but there are four suits from which we can choose and we need to pick three of them so now we need to pick the suits and order doesn't matter in this situation we don't care about the ordering of the suits we just need to pick them so so we need to pick exactly three suits for all those three cards and since order doesn't matter for this choice we have four choose three ways to pick the suits and four choose three evaluates to four which is not surprising you know like alternatively and by the way this this thought process is quite useful instead of picking the three faces we're going to include we could have picked the one face no no no no not face i am so sorry i for the life of me i can't keep my word straight i'm always doing this uh so not face but suits uh instead of picking uh the three suits we're going to include we could pick the one suit we're going to exclude and there's four ways to pick the suit we're going to skip all right so there's an useful trick to keep in mind uh next up we need to pick the two of a kind card we need to pick the two of a kind face value so we can't pick what we chose before we like for example we couldn't choose 4 again so we need to pick one of the 12 remaining face values so there's 12 ways to make this choice and then after we pick the face of the two of a kind card we now need to pick the suits so we need to pick two suits and again uh we need to do so where we don't have replacement but order doesn't matter you can't have replacement because there are not two uh king of hearts in the deck there's only one king of heart so you pick so yeah you can't pick the same suit twice uh so um of the four suits we need to choose two of them to include so that's going to be 4 choose 2 4 choose 2 evaluates to 6. so in the end this is going to equal 13 times 12 times 6 times 4 which is equal to uh 3744 uh full house hands all right uh and here's some r code that's computing those quantities uh notice by the way this is a useful trick to to know to be aware of uh when doing uh when when using r notice that i wrapped the entire expression in parentheses if i didn't put the parentheses there nothing would have printed but when i wrap an entire expression in parentheses uh when i'm doing some variable assignment the variable the value of the variable that i just assigned gets printed which is really nice so if you remove these parentheses these parentheses the 624 would not have been printed but since i put the parentheses there it in addition to doing the assignment done here it prints the value of the variable that's a that's a nice trick all right so example 23 a flush is a poker hand where all cards belong to the same suit how many flush hands exist including what's called a straight flush a straight flush is a flush where the cards are also where you can order the car so that they're in sequence uh poker actually has a number of sequences but for example a hand where all the cards are spades and the cards are five six seven eight nine that is a straight flush since they're also in or since you can order them right and the straight flush is considered a different kind of hand in poker that's a straight flush is in fact the best possible poker hand so we actually should be accounting for straight flushes because generally when people say flush they are not including straight flush but we're just going to include straight flushes for now um we're going to allow that possibility so uh for a flush hand we're going to so in a flush hand all the cards have the same suit there are four possible suits so the first thing we need to do is pick the suit and there are four ways to pick the suit after we pick the suit we need to pick the face values so there are 13 possible face values we need to pick five of them to be in our hand so uh once you pick one face value you cannot pick it again because there are no two king of spades for example so there are 13 face values and we're going to choose five of them we don't care about the ordering and we're going to do so without replacement so you multiply those two numbers together uh this is going to be four times uh 1287 that's what that uh 13 choose 5 evaluates to and this is going to equal 5148 but this is also including the straight flush hands in the next problem i'm going to show you how we could potentially remove the straight flush hands because i'm actually going to ask that we that we do in fact remove them so a straight in poker is where cards can be arranged in sequence so for example we have five of uh five of spades six of clubs seven of clubs eight of hearts nine of hearts so this is a straight and the suit doesn't matter for a straight unless it's a straight flush if it's a straight flush the suit must be the same because it's also a flush so a straight flush is both a straight and a flush so it has a flush with all cards no it is uh so it is um uh i should change the that wording uh it is a straight it is a straight with all cars belong to the same suit and it is also the best possible poker hand uh hold on okay good all right uh how many straight flush hands exist we're going to compute that number now how many straight flushes exist so to get a straight flush so let's uh let's uh start some separations so first thing we're going to do is compute the straight flush all right so excuse me uh here by the way um is uh the uh the ordering of poker hands because poker poker face values have a ring uh have a ranking uh we have ace two three four five six seven eight nine and i'm gonna write x for ten the roman numeral x then we have uh uh jack queen uh king and also ace again because ace can be both low and high um and in fact the best literal possible poker hand is 10 jack queen king ice all of the same suit that is the best straight flush okay um so uh to have so to so for the flush part we we need to figure out how many ways there are for the flush part and how many ways there are to get the straight part so there are four ways to pick the flush part the part where you have the same suit so we're going to pick the suit for the flush part and then we have the straight part and the straight part for that what i would recommend you do is think about how many ways there are to pick the first card so pick the first card of the straight because if you know that the first card or the lowest card of the straight is ace since you there are five cards in hand that means that the other four cards are two three four five right so how many ways are there to pick the lowest card in the straight so let's see we got uh one two three four five six uh five six seven eight nine oh ten what do you know because the moment you get to ten the the uh the straight hand would be ten jack queen king ace and there is no straight that starts the jack because there is no you would not be able to get the fifth card since you can't circle around back to two so um so that means that there's ten ways to pick the lowest card in the straight so that means that the number of straight flush hands is going to be 4 times 10 which equals 40. all right and i asked also how many straights are possible but this time i'm not including straight flush because straight flush is considered different well uh the first thing we're going to do is pick the lowest card like we did above and then we're going to pick the suits so pick the suits of the cards so we decided there are 10 ways to pick the first card of to pick the first card of the straight and now to pick the suits well if i decide that the first card let's say the ace is going to be clubs and then i pick the next card which is going to be a 2 i can still pick clubs again because i took out the the ace of clubs but i didn't take out the two of clubs so i can still pick clubs for that second card so and i can still pick clubs for the third card and so on so basically i do have replacement but order does matter because uh picking the first car there to be the ace of clubs and the second car to be the two of hearts is different from the first car being the ace of hearts and the second card being the two of clubs those are two different hands so order does matter and also um uh you're doing so with replacement there are four possibilities we need to pick five of them so this will be four of the power five the thing though is once i've done this i am including picking the first card to be hearts the second card would be hard the third card to be hard the fourth card would be hearts and the fifth card to be hearts that's a straight flush i don't want to include straight flushes so how am i gonna remove the straight flushes i'm gonna subtract them out so this is straight flushes so just remove them remove them from the calculations subtract them out and you no longer need to worry about them anymore this is basically remember that sum rule that i very briefly mentioned this is basically that sum rule right where you say the total number of like straight flush number of straights including flushes is going to be the number of straight flush and straight nonflush hands so you add those together which means you can do some algebra to get subtraction too anyway uh in the end you calculate this and you get 10 200 uh straight hands excluding the straight flush okay uh we are almost done with this section we've had a long discussion about counting and i haven't really said anything about how this test of what this has to do with probability well this entire section was devoted to the case that you may have thought was the easy case where you have a set where your sample space has finite size your sample space has finite size and you decide that every element in that sample space is equally likely and you may have thought this is the easy case because in that situation it's actually rather easy to compute probabilities you can assign a very natural probability measure the probability of any event a which is a subset of this sample space will be the number of elements in that set a or in that event a divided by the size of the sample space which is a very nice natural probability measure but here's the thing though you need to compute then using these counting techniques the size of the sample space and the size of your set a and that's actually tricky because now you need to use these uh counting techniques and i personally don't think the counting techniques are all that easy all right um so based off of this once we have this natural probability measure we can use counting techniques if we need to to compute probabilities so we're going to use this to compute the probability of the poker hands that we were considering above so the size of pos of the sample space the sample space consists of possible poker hands we say that each of those poker hands are equally likely we do not care about the ordering of poker hands so there's going to be 52 choose five such poker hands which is going to be uh two million five hundred ninety eight uh thousand uh 960 possible poker hands all right so i want to compute the probability of each of the poker hands that we saw in the previous examples so for example 21 this was the four of a kind so the probability of a four of a kind hand is going to be the number of four of a kind hands which we computed to be to be 624 divided by the size of the sample space which is 2 million 198 thousand 960 poker hands which is going to be approximately 0.0002 uh for example 22 the probability of getting a full house is going to be uh let's see so probability of full house is going to be 3744 divided by two million five hundred ninety eight thousand nine hundred and sixty the number of full house hands divided by the total number of possible poker hands this is approximately zero point zero zero one four uh next up so uh for example 23. so for example 23 we're going to compute the probability of our socalled flush hand a flush that isn't actually of that includes straight flush so just the suit is the same uh how many so there were 5148 such hands possible we're going to divide that by the size of the sample space which is that 2 million ish number um and that's going to be approximately 0.002 and now for example 24 the probability of a straight flush uh that's going to be there were 40 straight flush hands divided by the size of the sample space this is going to be approximately 0.00002 really really really small and the probability of a straight so that's excluding the straight flush is going to be 10 200 divided by the size of the sample space which is approximately 0.004 there's always kind of this question of which is more likely a flush or a straight turns out the straight is twice as likely as the flush which is which to me is a little surprising it feels it feels intuitively like the straight is harder but it's actually not um and that's one that's a common trait of probability it it defies what you feel like should be true all right so that's it for this section and uh in the next success section we're going to talk about a conditional probability so uh i will see you then since somewhere using the idea of independence you can almost find it anywhere if you look hard enough so independence is an extremely important idea in probability i've heard some people say that independence is what keeps probability as a mathematical theory from just being a subset of analysis so we're going to talk about uh independence if you come away from this lecture not or come away from this class maybe more appropriately not knowing what independence is then that's that's bad that's really bad you must understand independence because it is used everywhere it's one of the most important ideas in probability theory so two events a and b are said to be independent if the probability of a given b is the probability of a so in some sense the information about the event b gives no information about whether a has happened since uh the probability of a knowing that b happened does not change so first off i've said only that the probability of a given b is the probability of a uh what about the probability of b given a and before i proceed i should probably uh make a quick note saying i'm going to assume that a and b are events that have probabilities on their own that are not necessarily zero just because it's easier mathematically to assume that their probabilities are not zero and you kind of have to uh treat the case where they are events with probability zero as a little as a as a separate thing but honestly for for the most part you get the information that you need with just pretending that just ignoring the the chance that uh they have probability zero okay so uh all right then let's uh compute the probability of b given a uh it seems like what should be the case is that if a is independent of b then b should be independent of a as well and in fact that turns out to be the case because we compute the probability of b given a and that's going to be the probability of a and b divided by the probability of a and we then say that the numerator is equal to the probability of a given b uh times the probability of b uh divided by the probability of a by the way uh if you're not recognizing this part uh recalling back to the previous lecture this is a bayes theorem prototype that all right that's some bad bad handwriting up there i cannot i cannot let that go i know that in some of these videos my handwriting isn't great but i cannot let that go so this is a bayes theorem prototype in that you're pretty much one step away from getting base theorem all you would need to do at this point is apply uh the law of total probability to the denominator uh down here in order to get bayes theorem but anyway that aside all right that was a a distraction um we now say that this is the probability of a given b times the probability of b uh divided by the probability of a but we now know that the probability of a given b because a is independent of b that's going to be the probability of a so this is the probability of a times the probability of b uh divided by the probability of b you can a probability of a sorry and you can probably see why we're assuming that these events don't have zero probability because we could end up with division by zero so uh we're going to cancel those out because they appear in the numerator and denominator and we get the probability of b in the end okay all right so that shows what we wanted to want to show so um this implies that if a is uh if a is independent of b so if a is independent of b uh that up that's going to imply that b is independent of a and vice versa all right uh it seems that it would seem to be the case that if a is independent of b and so um heuristically that knowing whether b happened gives you no information about whether a happened it should also be the true for the complement of a if you if um so knowing that b happened should not tell you whether a didn't happen either so this does in fact turn out to be the case because we compute the probability of a complement given b and that's going to be the probability of a complement given b uh we know from uh the previous section this is 1 minus the probability of a given b and the probability of a given b is going to be the probability of a so this is 1 minus the probability of a which we know from section 2 is equal to the probability of a complement so that means that the complement of a is also independent of b so there is an immediate consequence of this definition which is that the probability of a intersected with b is equal to the probability of a times the probability of b you can see that because you could uh potentially say that uh like you would start out before saying the probability of a that this is the probability of a given b but since a is independent of b that's just going to be the probability of a so that means that the probability of intersections turns into the product of probabilities and not only is this a consequence of how i've defined the definition oh i misspelled definition not only is this a consequence of how i uh defined independence independence of events this is actually taken in later courses in higher level probability courses or courses devoted to probability as the definition of independence because the two are essentially equivalent and furthermore what i've highlighted in blue here it's used even more frequently and and also uh it it's uh you have this issue of zero probability events and we don't have that issue here because there's no division taking place okay uh students often want to like i've been encouraging students to think about uh probabilities and and and uh sets and events in terms of venn diagrams and uh is it possible to get a venn diagram uh graphically representing independence it is although it's kind of hard and honestly not that enlightening i'm going to show it to you anyway though first off this is not what independence is this is not a picture of independence we have a we have b and we have our sample space that is not independence this is disjointedness and two events that are disjoint and have nonzero probability are certainly not independent of each other because if you knew that one event one of those two events happened you know that the other one didn't so disjointedness is not the same thing in fact it's almost the opposite thing or implies nonindependence so disjointness is not independence it's almost the opposite okay so uh i'm promising you to make a to show what independence kind of looks like as a venn diagram i can kind of torture a venn diagram so what i do is um i'm gonna need to zoom in a lot for this so to attempt to draw uh two sets that are independent of each other i uh draw the sample space as a square i divide it up into fourths and then i'm going to give the set a the upper left corner nothing special about that particular corner it just needs to be a corner and then i give the event b the middle quadrant it should be the same area roughly as a it's just taking up the middle area and the area inside of a and b relative to b is same is the same as a's area relative to the entire sample space so or or that is uh a is take a's area is a quarter of the sample space and a's area and b or a's intersection and b is a is a quarter of b so knowing that you fell into that so basically a corresponds to the probability of falling into the upper left hand corner quadrant and knowing that you are in this middle square does not really tell you whether you fell into that quadrant or not um that gives you basically no information on that so this is a sketch of what independent events might look like if you want to draw them as a venn diagram if this sketch does not make sense to you then don't worry about it just leave it alone because you know it's it's very much a tortured example you have to kind of work hard to get something that looks like this and i'm not super convinced that it's necessarily all that enlightening as to what independence means so if it doesn't make sense to you then just move on don't worry about it uh okay so the next example we're going to consider rolling a sixsided die and i'm going to show that the event a which is that the number uh of pips does not exceed four and the event b that the number of pips is even are independent events so let's uh go ahead and enumerate what falls into uh these two uh separate events we have the event a well not separate just different so the event a uh the number does not exceed four so that includes uh one pip two pips three pips and four pips all right uh the event b which is that you get an even number of pips is going to be two pips four pips uh come on i want the fourth pip i tried and six pips ugh not always cooperative all right um so let's compute the probability of a given b to show that two events are independent it is sufficient to just compute the probability of a given b and show that that equals the probability of a so the probability of a given b is going to be the probability of a and b divided by the probability of b all of these outcomes are equally likely so the intersection of the two sets is going to consist of two and four those are the two things the two sets have in common so this is going to be the dice with face two and the dice with face four why is it so uncooperative okay there we go uh probably two probably four and um divided by uh the probability of b okay and at this point we can basically just count how many elements are in these sets so uh for the intersection there's two outcomes in that set uh and uh for the probability of b there's three so that's going to be three over six so we end up with uh the six is cancelling out and the probability of a given b is equal to twothirds as a result but the probability of a there are four outcomes in a uh divided by the size of the sample space which is six which is also twothirds those two numbers are the same therefore these two events are independent of each other okay next up we suppose we have events a1 through a n these sets are said to be mutually independent if and i know that this definition is rather complicated for k less than or equal to n the probability of any subset of that collection of events will become the product of their individual probabilities so it is not sufficient to just check that for all of these events a1 through a n uh the probability of the intersection is the probability of probabilities it has to be true for not just the entire collection of events but any subcollection of that collection of events it they all must turn into product of probabilities if this is not true then they are not said to be mutually independent of each other um and here's kind of the reason why um when we were saying mutually independent it what we really kind of want to say is that uh none of these events give any information about any of the other events and so that means it you would want to say that you take any two events in this collection of events and they will be independent of each other you want to be able to say that but it is not sufficient to just check that the entire intersection the probably the entire intersection turns into the product of the respective probabilities and here is an example that shows uh why that is not in fact true um so uh this is an example from uh from actually uh this paper that i've highlighted in red use the diagram below for finding probabilities compute the probability of a and b and c that actually is just going to be that tiny little sliver so probably of a and b and c is equal to 0.04 and then we're going to find the product of the of the probabilities of the events a b excuse me and c uh so the probability of a so that's going to be the probability of the blue circle which is going to be 0.1 plus 0.06 plus 0.04 which is 0.2 so this will be 0.2 the probability of b is going to be the green circle so that's going to be 0.06 plus 0.04 that's 0.1 plus another 0.1 is 0.2 plus another 0.2 is 0.4 so this will be 0.4 and finally we've got the probability of c which is going to be this blue circle here so we've got uh 0.16 plus 0.04 is 0.2 plus another 0.2 is 0.4 plus 0.1 will be 0.5 so uh this probability the probability of c is equal to 0.5 so uh the probability of a times the probability of b times the probability of c is going to be 0.2 times 0.5 which is 0.1 times 0.4 which will be 0.04 and those two numbers are equal to each other so it's tempting to say uh that these events are independent of each other but then i ask you to compute the probability of a uh intersected with b so the probability of a intersected with b corresponds to this blue region that i've highlighted which is going to be 0.1 so that equals 0.1 but that does not equal the probability of a times the probability of b which is equal to 0.08 and you might and before you say well those numbers are close close doesn't mean a thing i don't care about close they need to be the same so uh yeah they are they are not independent events and as a result these events a b and c are uh not mutually independent could you say that there is some other notion of independence that you could apply i don't really know and i don't really care because i have seen notions maybe like pairwise independence as an alternative notion of independence and i've never seen it ever used in my own life or work it doesn't really seem like it leads to any sort of useful uh useful concepts so yeah that's that's that's that all right uh next example this example is supposed to motivate uh maybe you remember when i was uh talking about flipping a coin take it has this example supposed to motivate uh our assignment of probabilities in that situation so we're going to flip uh remove the eight part that i don't know why that was there i don't know why i have uh eight fair coins so flip fair coins until we get heads and furthermore each flip is independent okay so what is the probability of heads tails heads tail sales heads tails those stills heads uh what in general what would be the probability of a sequence of flips a sequence of n flips to have n minus one tails and a heads at the end so the probability of heads if this is a fair coin is going to be uh one half uh the probability of tails heads and this is by the way getting a little abusive with notation but the but the probability of tails heads is gonna be the probability of tails times the probability of heads since those two flips were independent of each other we are now on to chapter three on discrete random variables and probability distributions this chapter serves both as an introduction to random variables and also an introduction to discrete random variables in particular but many of the concepts that we see here are going to transform is are going to transfer over to the continuous context when we're dealing with continuous random variables so let's get started so in this section we're going to talk about some random variables a random variable which is sometimes abbreviated with the letters rb is a function taking values from the sample space s and associating numbers with them conventional notation for random variables uses capital letters from the end of the english alphabet with lowercase letters while lowercase letters are usually used to denote a nonrandom value or outcome so up to this point we have been using lowercase letters for uh data and when talking about random variables we're going to switch to uppercase variables or uppercase letters and the distinction does somewhat matter um honestly this is a rule that is very frequently broken although at the same while i say that in this class i'm not planning on breaking that rule all that much uh yeah i mean i i break it all the time in in my research work but this is a situation where it's probably going i'm probably going to stay rather true to it so there's a difference between a random variable whose outcome is unknown and a possible value that random variable could take so uh using the fact that random variables are actually functions like the term random variable is somewhat there's somewhat of a joke that random variables are neither random nor variables because they are uh because random variables are not really variables they're treated as functions and they're not random because if you know what outcome from the sample space you got you know the value of the random variable because the randomness comes not from the random variable x itself but rather from omega which is an outcome from the sample space so when uh so if omega is an outcome from the sample space the notation x of omega equals little x can be used to say the value of the random variable x when the when the outcome little omega occurs is little x so little omega that is a random outcome uh little x is uh nonrandom and x will equal little x when the random outcome omega occurs all right so the set of omega such that x of omega is equal to x is the event that an element of s is drawn that causes the random variable variable x to equal little x and the set of omega such that x of omega is in some other set a where that other set is often some such a subset of the real numbers is the event that an element of s is drawn that causes the random variable x to assume a value that is in the set a so technically when we want to talk about whether a random variable is in a it takes a value inside of a set or not this is the notation this is what we should be using for that notation we are asking for the probability that we draw an omega that causes x of omega to be in the set a but that is often rather tedious we would rather just say what is the probability that x is in a that's much simpler random variables are commonly classified as being either discrete or continuous discrete random variables or discrete real valued random variables take values in a finite or countably infinite or innumerable if you prefer a set with positive probability so examples of sets that random discrete random variables uh take their values in could be a finite set of numbers such as zero or one it could be the the integers it could be uh the natural numbers such as 0 1 2 3 4 5 6 and so on if it's innumerable then it could potentially be a discrete random variable if the set that it falls into is all any number between zero and one including fractions and including um uh rational numbers and irrational numbers any of those numbers then it's no longer discrete it's going to be considered continuous uh on the other hand continuous real value variables satisfy the following two properties first the random variables take values and intervals which are possibly infinite in length or disjoint unions of intervals of the real line with positive probability that's the first property that continuous random variable satisfied the second is that for any constant c and r the probability that that random variable is equal to c is equal to zero which is kind of a strange assumption you're saying that you know that this random variable this continuous random variable is going to equal some real number but you're saying the probability that it equals any real number is equal to zero well for starters that must be the case because uh this would be this is an infinite set so you need to have some way to assign probabilities and there's too many numbers in the real number system for uh numbers in general to have positive probability and secondly uh there's a way that my probability instructor put it he said if you like you know that someone's going to win the lottery you just know it's not going to be you so by that same logic you know that these random variables will fall inside of an interval you just don't know what number it will be you will never know what number it will be and uh it's highly highly highly unlikely that it will be that particular number you chose so unlikely that that probability is effectively zero uh perhaps the simplest nontrivial random variable is the bernoulli random variable if x is a bernoulli random variable then the probability that x equals one is equal to one minus probably that x equals zero which is p and we say uh that x follows a bernoulli distribution with parameter p that's that's what this we're saying so we have the random variable x and this notation means that it comes from some distribution with some parameter uh we're gonna leave that um alone that that uh verbiage about distributions alone for a minute that's going to be the subject of the next video uh so but basically we just say that x is a bernoulli random variable because it has these properties all right so the set s on which x of omega is defined could be really just about anything uh it's natural to think of bernoulli random variables as being equivalent to coin flips or possibly biased coin flips the thing though is you do not have to necessarily have coin flips for example you could have a probability space where there is a coin flip where if you draw heads this random variable will evaluate to the number one and if you draw tails or you flip and it lands tails up it will evaluate to zero that's one possibility to get a fair or balanced bernoulli random variable where the probability of getting one is 0.5 alternatively you could roll a fair die track whether the result of the die was even or odd and in the event of an even number of pips this random variable evaluates to one and the event of an odd number of pips this random variable evaluates to zero either one of those situations could be the case and it is statistically impossible to differentiate between these two possible setups for of random variables and the and and sample spaces so in that case it's almost as if once you know the distribution of the random variable you can pretty much forget whatever the original sample space was and any of the properties of that sample space you can now work in some you can work as if this random variable were the identity function and its sample space is going to be the real numbers or something uh maybe the number zero one so uh in that sense you almost forget all of that stuff that we talked about before um i mean i okay i guess you don't forget about it but you no longer care about the specifics of the sample space and the elements that you're drawing from the sample space the specifics no longer matter so we we get to talk about these things in a very general way all right so for the first example which of the following random variables are likely to be continu considered discrete and which are likely to be considered considered continuous describe the space of outcomes the random variable takes with positive probability so for first case flip a coin record one for heads and zero for tails if this is a situation the sample space for experiment would probably be the sample space consisting of heads and tails and the random variable x when given the outcome h is equal to is equal to one and the random variable x when taken the value tails is going to equal zero so the space of outcomes which i will denote by x of s where you almost feed the entire sample space into this function x is going to be the set uh 0 to 1. by the way the term for uh this is this is the image of the sample space s under the under the random variable x or the function x all right uh this random variable since the space in which its outcomes falls is generally like that's a finite space there's only two numbers in it so this random variable would be considered discrete okay uh next example roll a die record the number of pips showing so the sample space in this situation would be uh die rolls so we got one uh two three four five and six all right so that's the sample space uh x of one would be equal to one x of uh two or two pips would be equal to two and you'd kind of keep going on like this for uh other possibilities so you could say x when you have six pips on your face would be equal to six so this is the reason why we didn't want to write down the numbers one through six when talking about die rolls it makes it easier then to talk about random variables as being a translation from the number of pips showing on a dice face to numbers actual numbers and we would like to be able to make that translation random variables do not need to be defined on numeric spaces it doesn't have it they basically say nothing about whatever's in that original space they don't care at all what's in that original space and once you have random variable you get to work in the real numbers and that's really nice so the image of the sample space under x is going to be the numbers 1 2 3 4 5 6. and that suggests since since this is a finite set that this is a discrete random variable all right next example roll a die record one for an even number of pips and negative one for uh for an odd number of pips okay so the sample space is the same as before so we're gonna copy copy that sample space down so x of one is equal to x of three which is equal to x of five in all of these cases x is going to come out as negative ones and to have an odd number of pips uh in the case of two uh uh that's so x evaluated for the dice with two pips is going to be the same as when there's four pips which is going to be the same as when there's uh six pips in all of these situations the random variable x comes out as one so the image of the sample space under this random variable will be negative one and one and again this is a discrete random variable okay uh the time in minutes needed to complete a race uh it seems appropriate to say here that the sample space is going to be some uh positive real number which we'll put with the r plus which will be which is equal to the set uh zero to infinity including zero and in this case so it seems like the the the random variable is going to be the identity function where it takes one of the numbers from the space and just spits out that exact same number so in this case x of uh omega is equal to omega which is in uh the which is in the uh posit or nonnegative real numbers so that means that the image of the sample space under x is going to be the positive real numbers or the nonnegative real numbers and this is a continuous space so this is going to be a continuous random variable and basically what we're saying is uh the time it takes to complete the race can be any number from 0 to infinity uh not just integers but also including fractions and algebraic numbers and transcendental numbers every single possible real number and since the real numbers are an uncountable space that means that this random variable is going to be continuous uh for similar reasons number five the length in centimeters of a hair plucked from a person's head uh we could say um that the sample space is going to be uh hairs no no no no not x sorry the sample space will consist of hairs and the a random variable x from that sample space which i'm going to draw a hair okay that's the thing all right uh so x when given a hair uh gives you uh a number so the length of the hair and so it suggests that the image of the sample space under x is going to be the positive real numbers or the nonnegative real numbers again since hairs can be in principle of any length and it's an uncountable length so i mean idea all right it's it's going to be true that there's a finite number of hairs in the world and therefore a finite number of hair lengths so if we were being super super duper technical we would say actually this is a discrete random variable because there's only a finite number of possible hair lengths since there's only a finite number of pairs but that seems ridiculous that seems like a ridiculous model that seems like way too much complication continuous random variables are continuous because it's easier to work with continuous things than discrete things and you're probably going to agree with that when we start doing all the work for the discrete random variables and all the work for the continuous random variables and see oh it's actually not it's actually somewhat easier to work with continuous stuff so uh we're going to say that it's the real numbers in which case this is a continuous random variable okay uh next up roll two dice record the sum of the number of pep showing uh i'm not gonna bother writing out the sample space but i'm going to say that uh x could be like say if you gave x the numbers uh one and four it's going to add up the pips showing on those two dice and give you the result five so the image of x under the sample space is going to be the numbers from 2 to 12. and since this is a finite set this is going to be discrete okay next up flip a coin until h is seen and count the number of flips so the sample space consists of heads tails heads tails tails heads uh and so on so the si so the random variable x when given one of these uh strings let's say tails tails heads uh is going to evaluate to this to the length of the string which means that x when fed this sample space its image is going to be the natural numbers which are the numbers uh 1 2 3 and so on and this is a discrete space because the because there's an innumerable amount of numbers in this space and since it's innumerable that means it's going to be discrete so this is a discrete sample this is a discrete random variable by the way if you're not familiar with the word innumerable that means listable as then you can list it out and even though it will take you an infinite number of years to list out everything in this sample space or in this uh set there will you will hit everything in that set a in finite time so every possibility the same cannot be said for the real numbers by the way if even if given infinite number of years you will not hit every value if you started trying to list them out so that's actually a very deep result in set theory or i don't know about i don't know if deep is quite the right word for it but it's certainly an important result so uh we're just going to take it for granted here that that is the case example two consider an experiment of rolling two sixsided die define two random variables for this experiment are they continuous or discrete we can define multiple random variables for the same sample space and there are advantages to doing so because when doing so we can talk about notions such as correlation or study of the behavior different random variables in the same space consider different possibilities i'll talk about their joint distribution together stuff like that so as a reminder the sample space consists of dice faces so we could have for example one one uh one two and we would keep going on like this you've seen in previous videos how to do this until eventually we listed out 6 6. okay so this is our sample space what's one random variable we could define we could say that x the random variable x when given some outcome omega in the sample space is equal to the sum of the pips whereas if we gave uh no let's not call the other random variable x we'll call it y that y will be another random variable defined on this space and it will be the max of the pips so for example uh x of the dice face of the combination one and two will equal three because that's the sum whereas y of one and two will equal two because the maximum of the two of the number of pips is going to be two okay so that's it for this section this was a basic introduction into what one uh we are on to the next section in our chapter on uh discrete random variables and at this point you're probably thinking okay we got these things called discrete random variables uh all right what's the point why why do we have these things random variables what why do why does anyone care about this this seems like an extra complication on uh this uh on these probability spaces we we were still able to talk about probability we haven't really added really anything so far well random variables truly are and and an addition that makes things much better and allows you to say additional things about uh randomness uh once you have random variables you are now allowing for um concepts such as um uh you're allowing for concepts such as we have uh we we have a phenomena that is essentially uh like phenomena that is essentially the same the same except for a couple parameters like there's a few parameters that we need to figure out and once we know those parameters we basically know everything there is to know about this phenomena or there's concepts such as uh expectation or mean uh you in order to be able to talk about expected values you need to have random variables so once we've introduced random variables uh these things that take inc that take uh things in our sample space and turn it into numbers you now gain a lot of additional structure the first thing that we get that is an essential property of uh random variables is a probability distribution so a probability distribution for a random variable is a function that describes the probability that a random variable takes on certain values discrete random variables are determined completely by the probability mass function which is abbreviated pmf and the probably mass function for a random variable is p of x which is equal to the probability that that the random variable x capital x so remember that capital x is referring to a random variable this is a thing that hasn't taken a value at yet or we don't view it as being equal to anything per se at this moment uh but we're asking for the probability that this random variable when we actually evaluate it and get a number out of it is going to equal x i like to think of random variables as uh they will have a future value but they don't really have a value right now when we're asking for their probability and stuff so we're asking what could this thing be in the future so uh a coin flip is random before you make the flip if that makes sense after you've made the flip then it's no longer random because you get to see whether it was heads or tails which actually is um getting more again into the issue of what does probability actually mean like for example uh let's suppose that we flip a coin and it lands heads up and then we cover it up we don't get to look at it we never saw what the coin did we immediately the moment it lands on the ground uh covered up in a box in principle uh we would say that that under this uh frequentest notion of probability we should say that this random variable has a value and is no longer random we just don't know what it is whereas if you're adopting maybe the gambler's notion of random of randomness and probability you might still view it as being random where you can start placing a bet on whether it's heads or tails when you take the lid off of the uh when you when you take the box off of the of the coin and then actually observe it uh i'm not gonna talk anymore about that i've already recorded a half hour video about the interpretation of probability probability and you can watch that if you want to learn more uh but uh all right so i'm just saying this because i feel like students especially with this notation this notation especially when you're starting out can bother students and they're wondering what capital x is and what little x is and what's the difference between them and the difference is this is random and we don't actually know what it is whereas little x is something that is fixed and we know what little x is so um really when i'm writing this down little x is going to be substituted for a number like for example there's going to be p of 1 which is going to be the probability that capital x is equal to 1. like at some point we're going to make that substitution um so basically the little x right here is going to get substituted with a number but the capital x is never going to get substituted and that's always going to be viewed right now as being random and we don't really know what its value is going to be and we're just studying uh what its value could possibly end up being and how likely it will end up taking certain values uh sorry if i'm going on too long about this this is just something that i know that students at some level struggle with and i try over and over again to try and explain it and i'm never fully satisfied with my own explanation okay um the probably mass function one way to visualize a probably mass function is using a line graph where a line is placed on each point x of r that x takes a positive probability extends to the height representing p of x um okay so before i draw a visualization i'm going to say that we are totally allowed to say all right we've got inputs x and outcomes p of x where p of x is a function that gives us the probability this random variable will equal x and we can construct a table if we wanted like for example 0 1 2 3 4. we could construct a table of possible inputs to this function and for possible outputs we could say uh let's see uh what says something we could do uh we'll just say that all of these are equally likely so all of these are onefifths so this function puts out onefifth all the all the time well yeah should we always do that uh we might say this is twofifths and this is onetenth and this is onetenth there i think that's okay does this still add up to one this must add up to one by the way actually that's the thing we're going to talk about in the future let's see two onefifth plus onefifth is twofifths plus twofifths is fourfifths plus twotenths is another fifth so that does in fact add up to one which probably mass functions if you add them up if you add up all of their nonzero values then they must always add up to one always probably mass functions always add up to one if they don't add up to one then they're wrong they're not probably mass functions if you ever compute a probably mass function and it doesn't add up to one then it's not a probably mass function and i don't care if it's close it closes close as nothing because it's not one one is one all right um okay and and i suppose we're allowed to say all right let's suppose i threw in a fifth value then the probably mass function will be zero and presumably anything that's on this table if i didn't list it out then the s function is zero right so anything not written down is zero all right uh but then we can visualize a probability mass function using a line graph so we've got possible x values that this thing could take and we've got it's probably mass function uh it's it's probability at those points so we could have x equals zero one two three four uh and then for those let's let's see i've i've already got a lot of what i need so uh we'll do one tenth two tenths three tenths four tenths so zero is going to be two tenths so that's about here one is going to be two tenths again and then we got four tenths and then one tenth and one tenth this is a visualization of the probably mass function that i wrote on the right hand side of the page so i generally am like drawing a dot at the probability and then drawing a line up uh to the probability of the random variable equaling that value or something like that so yeah uh that's uh how you can visualize it there's also probably histograms which are very similar to line to the line graphs and very similar to the histograms that i was discussing uh several lecture videos ago where you could instead of having these uh lines uh draw i like these the the the table that i have is a lot like the the relative frequencies that i was discussing when discussing histograms so what you would do is draw something similar to the relative frequency of those uh of uh those um uh possible values so we got 0 1 2 3 4 and we draw a bar that is uh centered on that point so we got uh uh so we've got something going up to two tenths and two tenths again and then fourtenths and then two onetenths yeah so there we go uh so uh there's that uh this is another way to visualize the problem distribution if you if you prefer it um there's really no difference um in fact i would say that edward tufte would probably say that they are the same graph essentially it's just uh the one up here it should be preferred because it uses less ink okay uh continuing on uh let's see some examples uh a fair coin is flipped we define a random variable x uh when h occurs x evaluates to one and when t occurs x evaluates to 0 find the problem as function of x which is p of x visualize p of x through with a line graph and you're thinking how do we know what the probabilities are well we know because i said this is a fair coin and since i know that it's a fair coin i know that the probability of heads is one and probably a tails of zero okay so uh i'm going to start out actually with that tabular form so we've got x and p of x so we've got so possible values that x that the random variable x could take are one and zero so we're going to put zero and one as potential values for this random variable uh this random variable will equal zero if the coin comes tails up and since this is a fair coin the probability of getting tails so actually maybe i should be more verbose about this and say that the probability that x equals zero is equal to the probability of drawing an m and omega from the sample space an outcome from the sample space such that x when evaluated at that outcome equals zero i'm being very verbose about this uh what are such outcomes that causes us to evaluate to zero well we know from the definition of the problem that such outcomes are only tails so this is the probability that you get tails and the probability of tails because there's a fair coin is one half so that means that um x is so at z at zero the probably mass function will be one half and at one the probability mass function will also be one half because well a uh this random variable um well for starters this random variable is going to be one when the coin lands heads up and the probability of heads is one half that's one way to think about it and another thing to think about it is there's only two numbers that this random variable could take with positive probability zero and one zero it's probably mass function is one half so at one it must be whatever it takes to cause the probably mass function to add up to one so one minus one half meter is going to be one half and thus the other value is going to be one half uh so uh we we visualize this with a line graph uh we'll go ahead and make this one half uh what we end up having for our visualization of the probability mass function is we have lines uh extending up to uh one half all right and that's our visualization for it okay uh this by the way is a complete description of the problems function if we are willing to say that anywhere isn't anywhere we don't list uh the probably mass function evaluates to zero because like like if that should make complete sense to you because let's say what is going to be p at one half well that is the probability that x equals one half which equals the probability of drawing an outcome from the sample space that causes the random variable to evaluate to one half okay so we know that there are two outcomes in the sample space which are heads and tails and x is a random variable and therefore it is a function so you know that functions uh when given one input give you only one output only one output will come out so what does that mean here uh well we know what this function will be at heads which is one outcome in the sample space we know how this function will be at tails which is the other outcome in the sample space so what outcome causes this random variable to equal onehalf because neither of those caused the function to evaluate at onehalf so that means that the probability of drawing an outcome from the sample space that causes this to evaluate to onehalf is the probability of the empty set because the the set of all numbers or not numbers the set of all outcomes in our experiment that causes this random variable to evaluate to one half is the empty set because there is no such outcome so you end up computing the probability of the empty set the probability then of the empty set is zero so that would mean that anything that isn't listed here uh it's natural to say that the problem s function is zero okay okay okay uh moving along moving along there is an r package called discrete rv and this package allows for users to define random variables and work with them and i think this package is pedagogically useful but for serious work with these random variables i wouldn't recommend using it um i uh i've uh i actually just made a few minutes ago well it wasn't a few minutes ago it was more like a few hours ago i just made a few hours ago uh lecture videos for the lab for r uh are on my our introduction on introductory videos um functions for dealing with probability in r and dealing with a lot of random variables and classes of random variables families random variables and i never use this package because it's more for allowing students a laboratory to work with random variables in the notation that we're using in the lecture class or a notation very similar to it and not actually meant for serious work trying to compute trying to work with the cdfs and the pdfs and expectability and all that stuff of these random variables but it's kind of nice so for example in this situation i could define the random variable x and say that this is a random variable with the rv function its possible values are zero and one and the probability of getting those outcomes are each one half and it will print and make a nice uh output a printout for that basically summarizing what i just said and in addition to that when i tell r to plot this random variable it creates the plot of the probability mass function so that was all very nice uh next example let s be the sum of the number of pips rolled onto dice find p of s and plot it okay okay so let's uh come so let's uh form our table again so we've got s and we've got p of s okay so s are possible sums of the dice so what are some possible sums of the dice well uh the smallest it could be is two because that's what happens when you roll snake eyes or one and one so the poss the smallest possible sum is two and the largest one happens when you roll box cars which is both of them are six so that will be twelve so it's going to be everything in between so two three four five six 7 8 9 10 11 12. uh hold on hold on uh i didn't write that quite right 10 11 12. okay and now we need to figure out uh the probably mass function okay so we're saying we we could imagine that there is a sample space that's consisting of a dice of combinations of two dice like we got one one uh uh one two and two and going on and uh we got like six six we've already worked with this uh type of uh sample space in previous videos and i don't want to go into too much more detail into it because it can get kind of tedious so um we know that there are 36 outcomes in this uh sample space so and we're saying that everything is equally likely so therefore uh the probability of the event the dice add up to two is going to be the number of outcomes uh where the two dice add up to two divided by the size of the sample space so how many outcomes are there with the die set up to two well you can you can get uh snake eyes and that's it uh so there's only one outcome that corresponds to that and then we divide it by 36. uh how about three we could either roll one the left dice and two on the right dice or i guess let's uh uh readopt that uh blue dice red dice uh verbiage and we could say uh the blue dice rolls a one and the red dice rolls a two or we could have the blue dice roll a two and a red dice to rule a one and that's it otherwise it will not be it will not add up to three so that's two outcomes that correspond with this so we're gonna have 2 over 36 and for 4 we could either have 1 and 3 2 and 2 or 3 and 1 so that would be 3 over 36 and that's going to keep going so we would have 4 over 36 4 5 5 over 36 for 6 and 6 over 36 for 7. uh all right 12. there's only one outcome where you can get 12 and that's box cars 6 and 6. so this will be 1 over 36 for 11. you could get 5 and 6 or 6 and 5. so we'll have 2 over 36 and you can see the pattern it's going to become 3 over 36 for 10 uh 4 over 36 for nine and uh five over 36 for eight okay and now we're ready to create our little visualization so we got two three uh four five six seven eight nine ten eleven uh twelve uh that is an awful looking eleven i really did try harder but sometimes the screen doesn't want to cooperate all right and uh let's see for the yaxis we could go uh the highest you ever go is six over 36 so we could go one two three four five six there we go so six over 36 is at the top so for our probability so we'll go one two three four five six five four three two one okay and that's it that's our probably mass function so um using this discrete rv package uh we've got what we could do is create a random variable representing a single die so that's created here and then we could say all right s is the sum of independent and identically distributed copies of the random variable d that is getting into verbage that uh and terms and ideas that i haven't really talked about yet but basically you add up two independent dice which is kind of what's going on in this experiment and then i tell it to plot it and it makes a plot and that's a very good plot all right uh next up consider flipping a fair coin until heads is seen let n be the number of flips find a probably mass function describing the distribution of n and plot the first few values of the pmf and um we've actually talked about uh this type of random variable before a number of times in the previous chapter uh as i mentioned there it's one of my favorite random variables to refer to since on the one hand it's not like the setup is quite simple to understand you flip a coin until you get heads that doesn't that's not that doesn't take a great deal of imagination and yet you can still get a lot of richness out of it and the mathematics can get kind of involved so um okay so in this case what we ended up coming up with before and suggest what we should have now in fact maybe if you go back to that video and look at how it was uh showing that the probably the sample space under some probably measure um will in fact be one i actually kind of defined a prototype random variable n of omega uh that was uh measuring the length of the string and that was that's basically a random variable right there um i defined a random variable on that space so that i could compute show that the probability um of that space added up to one so yeah they're very useful things but basically we could just say that this is one half to the power n for when n is a natural number uh otherwise you would just assume that this thing is going to be zero so this is a natural probability mass function for this space we actually showed in that section that it adds up to one and uh yeah so okay so uh let's um uh plot this uh probably mass function we got possible values a half a fourth an eighth a sixteenth a thirty second so uh one two three four five so for one we go up to a half for two we go up to a fourth for three we go up to an eighth for four we go up to a sixteenth and for five we go up to thirty second and in principle this this graph goes on forever but we're just going to stop at 5. section is on expected values let's start by deciding defining what an expected value is the expected value of a discrete random variable uh which we are often dealing with e x is given below so the expected value of x is equal to the sum of x times the probability mass function of x wherever the problem mass function of x is nonzero this is just a more general way to write down what expected values are in principle most of the time the x over what you're summing are the integers so if you really wanted to most of the time you'll be okay thinking of expected values as this like um x equals let's say zero to infinity or another way we could do it is this is actually allowed uh x equals negative infinity to infinity um or if you know what that um your little x ranges from a to b you could say uh we could say x equals a to b there's all sorts of ways we could possibly rearrange this but really what matters is that you're summing up uh this expression x times the probability mass function of x wherever that probability mass function is nonzero so i'm just going to leave this as x such that probably mass function of x is greater than zero actually i'm just going to not even really say all that much over what x we're summing over i'm just going to say that you sum over x okay next uh expected value of x is viewed as the population mean which we're often denoting with the greek letter mu described in previous chapters we can always compute the expected values of functions of x oh no not always why did i say always like at the very end of this section i'm going to give you an example of an expected value that can't be computed but we can also compute i meant also also compute the expected value of functions of x functions of x that is um so that would be the expected value of h of x in a natural way by saying that the expected value of h of x is equal to the sum over x of h of x times the probability mass function at x and this formula should make some sense to you because you've actually seen it before or something very similar to it before remember when we remember when we were computing the sample mean the sample mean was 1 over n times the sum from i equals 1 to n x i which is also equal to the sum from i equals one to n uh x i times one over n and the one over n i mean if you add up one over n n times what number do you get you get one and also one over n is a number that's uh greater than zero so actually you could see this as a probably mass function for x i so you can imagine that you have your sample of observations and you're going to pull an observation at random with equal probability from your sample if it's pull from with equal probability then you're going to pull it with probability 1 over n so actually you've seen this expected value formula before it's just that this formula down here is a more general notion of of a sample mean or not really a sample mean but of mean so it allows for more situations it allows for infinite uh possibilities for x and so on the expected value is in some sense a best prediction for the value of x and uh this form uh no did i say sample mean i don't think i did expected value you can see this footnote for what sense in which it's the best prediction but it's hence the term expectation it's like if you had to guess what value this random variable is going to be uh you can use the expected value to do that and it will be uh correct in some sense so example 11 complete the expected value for some of the random variables that we've seen in previous sections so bernoulli random variables discrete uniform this discrete uniform random variable that represents a die roll and the geometric random variable with parameter p okay so let's get started um the first situation for a bernoulli random variable the expected value of x is equal to the sum of x p of x and the values of x for which p of x is possibly nonzero will be x equals zero to one so this will be um zero times p of zero which is one minus p this corresponds to the poly mass function at 0 plus 1 times p where p is the value of the problem as function at one i guess this uh notation is actually a little unfortunate because i've got a couple different p's so maybe i should switch out the notation for the probably mass function in this example just switch it with f that could uh probably make things a bit more clear so this still nevertheless in any situation is the probably mass function for x okay and well let's see that just goes to zero and that's going to equal p so that means that the expected value of x is equal to p which is nice and actually rather insightful it's saying that the expected value of a bernoulli random variable with parameter p is equal to the probability that that random variable is equal to one and that's actually a very useful fact that's rather useful it allows us to relate bernoulli random variables probabilities for events and expected values it gives us a way to what we could do if we wanted to relate expected values to probability of an event is create a bernoulli random variable that's equal to one when that event occurs and zero otherwise and then the expected value of that random variable would be the probability of that event occurring but i'm just going to leave that issue for now let's work with the next example so the expected value of s is equal to the sum of let's say s p of s again we're talking about uh a probability mass function when we're talking about p uh down here and let's uh think about what are things that this random variable could take with positive probability we'll end up summing from s equals one to six so that's going to be uh let's see p of s is always 1 over 6. so this is equal to 1 over 6 times the sum from s equals 1 to 6 of s and actually there's a formula for that hopefully you remember that from from your algebra classes in general you have the sum of uh s equals 1 to n of s this is a sum of an arithmetic series that's going to be n times n plus one divided by two okay which means that this sum is going to equal onesixth times six times seven divided by two those two sixes cancel out so this will equal 7 over 2 or 3.5 okay finally we have the expected value of what am i calling this geometric random variable i'm calling it n the expected value of n which is going to be the sum of n times p of n uh where n ranges from one to infinity all right this is gonna get much more complicated uh so because this is going to involve an infinite sum p of n is going to be we've got np 1 minus p to the power n minus 1. and we're uh summing from n equals 1 to infinity let's see the p here is a constant so we can bring that out we are not summing over p that means that we can say that this is equal to p times the sum from n equals 1 to infinity n times 1 minus p to the power n minus 1. you probably did not see a formula for this so what are we going to do we're going to get tricky we're going to get really tricky you've taken calculus presumably you've taken calculus 1 which includes differential calculus so you have seen this formula before the derivative with respect to x of x to the power n is equal to n x n minus one with some assumptions on n like for example that it's uh let's say at least one in which case that would hold in this situation if you're looking at one minus p as your thing as the thing you're differentiating hmm those two things look rather similar which means that actually we could be invoking some sort of differential or derivative in our sum and say instead that this is equal to p times the sum from n equals one to infinity the derivative with respect to p 1 minus p to the power n now you would notice that right now if you were to in fact take that derivative you would almost get what i wrote on the left over here that i've underlined in green you would almost get that except you'd be off by a sign as in you'd get negative something because you have to invoke the chain rule so what are we gonna do about that we're just gonna throw a negative out here and uh then it's certainly true although i'm going to mention that well okay it's actually true as written down there's no there's no caveats yet but there is going to be one in just a second because i'm going to say that this is equal to negative p times the derivative with respect to p the sum from n equals 1 to infinity 1 minus p to the power n okay you can't just bring derivatives out of sums like that or you can but there are conditions like you can't just look at any sum in the world and see derivatives inside the sum and say okay i could just slip the derivative out it's not that simple there are details there are conditions under which you can do this those conditions are satisfied here i actually don't remember them off the top my head um i just know you can you can go look at some calculus book or some analysis book it's probably going to be an analysis book uh or wikipedia wikipedia has pretty good math articles and it would tell you when you can switch because that's what you're doing you're switching a sum and a derivative it will tell you when you're allowed to do that and i'm just completely sweeping that under the rug and i'm fine with that because i don't care i got other stuff to do all right um the thing though is we know how to compute this sum you know that this sum is going to become uh 1 minus p divided by p so that means that this will become negative p oops wrong color negative p and then we've got the derivative with respect to p of um 1 minus p divided by p which we should probably uh simplify somewhat and say that this is equal to negative p and the derivative with respect to p of let's see one over p minus one yeah that's right and the derivat so then we take that derivative and say we've got negative p and on the inside after we take the derivative we'll get negative one over p squared those two negatives turn into positives we cancel out one of ps and this is equal to one over p there we go this actually has a very nice intuitive um interpretation which is that let's say that you're flipping a coin until you get heads how many times do you what is the expected number of flip first ones tails second one's heads makes pretty good sense to me or let's say that you have a bias coin and you flip this coin until you get heads and the probably that you get heads is uh 0.1 how many flips do you need 10 flips seems to make a pretty good sense at least to me it allows at least in my mind way for me to relate probability of an event happening with time and say that if that if you were to have an uh a sequence of independent re replications of this event this is about how long you have to wait until you see that event happen which is another way to think of or another way to reason about how rare that event would be like for example if there's like a 10 chance of an earthquake every year how many years is it going to take for you to see an earthquake 10 years on average uh something like that that's so i really like uh that formula interpretations like that okay moving on this is the same body of lecture notes that we've seen before so we have loaded up the discrete rv library in r when you whenever you see the r sections and there is an there's a discrete rv function called e that is for computing expected values so x was a random variable that we defined at some point that corresponds to the bernoulli random variable that we were talking about s corresponds to actually the sum of two dice so in this case it we should be doing seven over two or something like that um uh if we were actually talking about the same s but this is a different s uh this is a uh sum of two dice two independent dice by the way uh not just any two dice although i don't know how you make two nonindependent dice that would be really hard and is the what we're talking about and notice that the answer is approximate uh we know that in in for this end the p parameter was 2 so the number that results should be 2 but it's not 2. it's 1.9999999 so be aware of that it's giving us an approximate answer because it's only fine summing over a finite number of uh places but you get the idea you can tell that that is that it's essentially doing the right thing uh okay uh continuing on expectations are linear functions and being a linear function is an extremely important property um and it's for that reason that we can say expectations are integrals but that is a 60 40 idea right there instead we're going to talk about how the expected value of ax plus b is equal to a times the expected value of x plus b this is something that we can show uh watch the expected value of a x plus b uh is equal to this this by the way is uh we this is basically h of x where h of let's say s is equal to a s plus b so we can use some of those that uh expectation formula that we mentioned above uh towards the beginning of this lecture and say that this expectation is equal to the sum over x a x plus b times the probably mass function of x which is equal to the sum over x and then we'll factor all that stuff together we've got a x p of x plus b p of x and then we'll break up the sum and say that this is the sum over x a x p of x plus uh the sum over x scroll down scroll down uh the sum over x b p of x factor out the constants to say that this is a times the sum over x x p of x plus b times the sum over x p of x and we can recognize what some of those sums are for instance this sum is the expected value of x and this sum is the sum of the probability mass function um over everything where it's positive so this is going to sum to 1 and hence you get uh the result uh a times the expected value of x plus b hence it's a linear the variance of a random variable is given by we'll call it var of x which is equal to the expected value of x minus mu where mu is just the expected value of x we just don't want to write that again in there because it's it feels confusing so we just put a mu in there but basically it's the mean squared distance of x from its uh from its expected value so this actually does correspond very closely to the sample variance as well if you could think of the sample variance as divided by one over n instead of one over n minus one uh you could say we could argue as we did for how the sample mean is very similar to the population mean in terms of expected values and say that this is a sample variance too so yeah they so this is uh something to notice the greek letter that's used to represent the sample variance is sigma squared and like with the sample standard deviation you can get the population standard deviation by taking the square root of the variance it's just not as common to do so um all right so uh there is actually a handy formula for computing the variance that is often easier than computing it directly and in this formula you may recognize this from when we were working with the uh the uh did i say sample variance a second ago uh i meant the variance or the population variance but this formula resembles the formula for the sum of squared sum of squared errors uh that we had that we saw in chapter one where you have the mean of x squared minus the mean of x squared where hopefully you can tell from my inflection what's being squared okay so the variance of x is thought of as the population variance and is denoted by var x which is sigma squared and the population standard deviation is sigma which is the square root of sigma squared sometimes i'll write though the standard deviation of x because it's it's sometimes nice to do uh all right so our next example compute the variance and standard deviation of the random variables listed in example 11. so if that's the case let's start out with x we already have the expected value of x which was p the expected value of s which is uh seven halves and the expected value of n which is equal to one over p so as a reminder of what we have already so now let's compute the expected value of x squared which is equal to the sum from little x equals 0 to 1 um x squared uh f of x because that's what i'm calling the probably mass function which is equal to 0 squared times 1 minus p plus 1 squared times p which is equal to p again hence you get to say that the variance of x is going to be the expected value of x squared minus the mean of x squared which is equal to p squared minus p which we could be done right there but people often like to factor this into p times one minus p all right next example uh in the case of s so the expected value of s squared is going to be the sum from s equals 1 to 6 of s square times probably mass function at s which is 1 over 6 times the sum from s equals 1 to 6 s squared this is something that we have actually all right you might not have seen this a formula for the sum of squares uh in your previous algebra classes maybe you saw that maybe you didn't but there is in fact a formula for that which i'm gonna have to look up okay so you have that the sum yeah let's use a different color for this the sum from s equals 1 to n s squared is equal to n times n plus one times two n plus one divided by six okay so using that here we get to say that this is equal to 1 6 times 6 times 7 times 13 divided by six so those cancel and that's pretty much all we can cancel so we get to say that this is equal to 91 divided by 6. therefore the variance of s is going to be the expected value of s squared minus the mean of s squared which is equal to 91 over 6 minus 49 over four what is that number uh uh 35 over 12. and you should have that the variance is always a nonnegative number in fact the only time that the variance is ever equal to zero as if the random variable is degenerate that is if it's effectively a constant so it's unlikely that your variance is zero and uh and in the more general case it's impossible for your r for your variance to be negative so if you ever ended up with a negative variance then you've done something wrong all right for the final one and this one is where things get weird all right i'm i'm going to zoom in for this one because this one is where things get really tricky because we're now working with the geometric case and we need to compute the expected value of n squared okay and that is equal to uh the sum from n equals 1 to infinity n squared times the probability mass function at n which is equal to uh the sum from n equals 1 to infinity i'm going to go ahead and already do some simplification we get p n squared 1 minus p to the power n minus 1. now how on earth are we going to compute that well we're going to get we're going to get really tricky is what we're going to do so we're going to say that let's let's zoom in even more we're going to say and you're not all right this this is just so weird what's about to happen um according to my notes it's actually advantageous to keep the p inside uh so let's let's uh put the p back inside of this sum rather than factor it out uh so we got a sum from n equals one to infinity n squared p all right what's going to end up happening is we're going to end up um adding 1 no hold on subtracting 1 and then adding 1 again inside of that square and leave everything else the same and we're going to go do some calculations and at the very end of them the expected value go away the expected value of n squared is going to appear on both the left hand and the right hand side uh left hand and right hand side of an equal sign so after that happens the thing is though on the right hand side of that equal sign it's not going to be just the expected value of n squared it's going to be the expected value of n squared plus something times something and when you have a situation like that you're going to be able to solve for the expected value of n squared because you just have an algebraic relationship and you're just going to have to see it and and watch it happen in order to kind of understand it's just at the very end all of a sudden what you're going to need pops out this is one of those situations where it's a trick and you're going to see the trick and you might not understand the motivation for the trick but someone did that trick once and it seems to work all right so uh here we go this part right here is a perfect is a it's a square a perfect square so uh we can we can now write that part i i'm not going to write n equals 1 to infinity all the time that's going to get annoying so i'm just going to write a sum over n down here and say we've got n plus one no no no no not n plus one n minus one we've got n minus one squared plus two times n minus one plus one and then we've got p one minus p to the power n minus one yeah okay and then this is equal to this is equal to uh breaking up this part and breaking up the resulting sum we get the sum over n and we have n minus 1 squared p 1 minus p power n minus 1. plus 2 plus 2 times the sum over n uh n minus 1 p 1 minus p to the power n minus 1 plus the sum over n of p 1 minus p to the power n minus 1. okay now we can start uh recognizing some stuff uh the term on the very left hand side no not left hand right hand side this term this is equal to one because this is just the sum of the probably mass function this term is the expected value of n minus one okay and then we get to say that so far collecting our stuff um recognizing those substitutions is the sum over n uh n minus 1 squared p 1 minus p to the power n minus 1 plus 2 times the expected value of i'm going to write this as the expected value of n minus 1 because we have that linearity property that i proved a few mo a few minutes ago and then we have plus one all right and uh doing some even further simplification we're able to recognize that the expected value of n is equal to one over p so that means that this term uh that we're adding is going to be uh one over p minus one so we got plus 2 over p minus 2 plus 1 so this will be uh so that means that this is going to simplify into plus 2 over p minus 1. and then we're going to take this n minus 1 and say oh well that's a perfect square too so this will be n squared minus 2n plus 1. all right so we then get um oh wait actually we don't want to do that no we don't want to do that we do not want to do that uh we don't want to do that we want to do something even trickier what we're going to do instead all right let's write in again what i have what what i have been omitting this whole time that this is the sum from n equals 1 to infinity but we're going to reindex this we're going to reindex this and say well actually this is the same as saying uh n minus one equals zero to infinity all right and then replace all of those n minus ones with let's say j and say this is j equals 0 to infinity so we get j j squared and the thing is though we're uh the first term in this sum though is going to end up being zero because j because all right plug in j equals zero that means the first term is going to be zero because zero squared is zero zero times whatever is zero so that means that the first term is actually zero so we get to um replace j equals zero with j equals one because well when you start at zero you just add a zero term you're just adding zero to the sum so we get to start at one and this is now looking almost like almost like uh the expected value except for uh the power up here is wrong it should be j minus one to have the problem mass function but we've got j instead okay so we'll replace that with j minus one plus one okay and to account for the plus one that means that what we need to do is factor out a 1 minus p so all told we will have 1 minus p times the sum from j equals 1 to infinity j squared p 1 minus p to the power j minus 1 plus 2 over p minus 1. and that sum is what we started with this term right here is the expected value of n squared oh look at that so we get to say that this is going to be uh 1 minus p times the expected value of n squared plus two over p minus one and as a reminder at the very beginning of this long statement of equalities is the expected value of n squared oh look at that we can do some algebra now for instance we can subtract over uh we can subtract from both sides the expected value of n squared and say that this is going to suggest that uh what is uh 1 minus p minus 1 uh that's going to be negative p so we've got negative p times the expected value of n squared uh we will subtract two over p and add one to both sides minus two over p plus one to get uh that this is equal to negative two over p plus one and then divide both sides by negative p and now we get to say that this is equal to that uh the expected value of n squared is equal to 2 over p squared minus 1 over p and there it is that by the way is the expected value of n squared i'm just writing it down again because it's on my screen and because it was put on a separate line that's what we need to compute so it then follows that the variance of n is equal to 2 over p squared minus 1 over p minus 1 over p squared which is equal to 1 over p squared minus 1 over p which is equal to p minus 1 over p squared wait a minute that's hold it hold it hold it hold it hold it that's oh no not p minus one uh one minus p okay otherwise that would have been bad because i would have i would have just computed a negative variance so one minus p over p squared uh did i ask to uh did i did i did i say that we should compute the standard deviation too it does it's not it's not super hard to do you just take the square root but yeah i did ask for standard deviations so okay computing the standard deviations too like that's super easy you just take the square root of the variance so the standard deviation of x is equal to the square root of p times one minus p uh the expected the standard deviation of s is equal to uh the square root uh let's see uh one half times the square root of uh 35 over three and for n uh let's see the variance no the standard deviation of n is going to be the square root of 1 minus p divided by p that's an ugly looking p all right there we go and in fact there are functions for in this uh discrete rv library for computing variance and standard deviation they're going to be v oops v and sd respectively and you can see this function computing the variance and standard deviation remember that this is not the same as we were talking about before so we get to compute those things and we get pretty much what we had so all right uh proposition 10 the variance of ax plus b is equal to a squared times the variance of x and the standard deviation of ax plus b is a times the standard deviation of x or the absolute value of a times the standard deviation of x uh actually this would probably be better to write in terms of that sd notation to say that sd of a x plus b is equal to the absolute value of a times the standard deviation of x so uh let's go ahead let's see this uh let's let's go ahead and prove this so the variance of a x plus b so the variance of ax plus b is the expected value of a x plus b minus we should have the expected value of a x plus b here here's the thing though uh the b's are going to cancel because expectations are linear and the a's can that a can be factored out in front of that expectation so that means that a is going to be a common factor and therefore the a can be factored out completely if we just square it so we get to say that this is the expected value of a squared and then we have on the inside x minus the expected value of x which is equal to a squared expected value of x minus mu squared just remember that mu is the expected value of x and that's the variance so this is equal to a squared times the variance of x all right what do we then say for the standard deviation we say that the standard deviation of a x plus b is the square root is equal to the square root of the variance of a x plus b which is equal to after you do that uh algebra the absolute value of a times the standard deviation of x because you're just going to take the square root of what i've highlighted in blue just take the square root of that and you're good all right so there's that formula uh one final note there is nothing that says that expectations need to be finite or even exist there are random variables out there that do not have finite uh uh standard uh finite expectations and they may not even have like like an infinite expectation they might not have an expectation at all like there's just no way to define it like i guess technically an infinite expectation is considered undefined but it's like you can't but there's a sense in which it is defined like infinite just means arbitrarily large but even then even then you might not be able to say that it's even infinite it could be anything there are random variables out there that don't have expectations so uh let's actually see an example of this this one's a fun one uh this is what's known as the saint petersburg game consider a game where a fair coin is flipped until it lands heads up a player would earn a dollar if the game ends with one flip two dollars if it ends two flips four dollars if it ends with three flips eight dollars if it ends with four flips and so on so basically your winnings are doubling every time uh this game goes on the fair price of a game corresponds to the game's expected payout what then is the fair price to play this game and before i continue on i would like for you to think about how much you think this game is worth and how much you would be willing to pay for it how much would you be willing to play at pay to play this gambling game what do you think is a fair price you might be surprised so let's calculate it we're going to say that n is following a geometric distribution with parameter onehalf because that's how you should we're flipping a coin until we get heads and this geometric random variable it's a fair coin is what will model such an experiment so then what would be our winnings it would be two to the power um n minus one because if you get one flip that would be you should get one dollar so n minus one so that'll be one minus one so to the power zero which is one uh if we get if it took two flips that's two minus one in the power so that'll be two minus one uh so the power will be one so we get to the one which is equal to two and if we have three flips that's going to be two squared so we'll get four so this is in fact corresponding to what we think it should all right then what we were computing is the expected value of two to the power and minus one which we can make our lives a little bit easier by saying that this is one half times the expected value of two to the power n and we know how to compute the expected value of 2 to the power n so we'll say this is one half and we have the sum from n equals 1 to infinity to the power n and then we write down the probability mass function for the geometric random variable which is 2 to the power negative n or which is the same as that's the same thing as 1 half to the power n so n minus n that's going to be one so this is one half times the sum from n equals one to infinity one which is equal to infinity this game has infinite value you should pay a dollar to play this game you should pay ten dollars to play this game you should pay a hundred dollars to play this game you should go to the bank and take out a loan for a billion dollars and play this game because this game has infinite expected value any finite price is a bargain and yet no one in their right mind would ever do that no one thinks that this game is really worth anything people think this is a terrible game and why that is is somewhat remarkable it gets to the point that once you've earned a million dollars another million dollars doesn't seem that great i mean it's pretty good to go from one million dollars to two billion dollars that's nice same thing with one trillion dollars and two trillion dollars like you're gaining a trillion dollars when you go from one trillion dollars to two trillion dollars but it doesn't really feel like it like you already got everything you want at one trillion dollars the other trillion dollars is just gravy so basically the point is with this game the way to rationalize the paradox of this game the fact that it's expected value is infinite but no one wants to pay that is that people are not actually thinking about the winnings in terms of literal money they're thinking about in terms of the utility they get from that money and people know that the net the next trillion dollars is not as good as the first trillion dollars so in that case this game actually doesn't look very good when once you actually account for decreasing utility from your winnings that's how you resolve the paradox you resolve it with economics that's it for this video uh we have been talking about general ideas and random variable theory let's call it random variable theory that seems like a good word uh probably mass functions accumulate distribution functions all that stuff these are general things that all discrete random variables have and expectations uh excluding the cases where they don't exist they they are they're generally around two so now we're going to start looking at specific examples of common families of random variables that probabilists care about the first one being the binomial probability distribution a lot of the ideas also that we talked about here actually transfer over to the continuous case when we're talking about continuous random variables uh there they have analogs that are pretty similar what you do is you replace probably mass functions with probably density functions and you replace sums with integrals so that's what that's how you go from the discrete to the continuous case and but everything else applies everything else is the same variances expectations uh cdfs probably mass functions become probably density functions which are pretty similar so these are all basic ideas that you're going to see over and over again when talking about probability and from this point on we're going to for the remainder of this chapter we're going to be looking at specific examples because the advantage of talking about a family of distributions is once you have a family distributions you get to talk about you get to talk about it once and then you get to generalize to lots of different cases and it's like the expected value if you're able to recognize a random variable as being a particular case of a binomial then there's an expected value formula available to you and you don't need to compute it by hand you shouldn't compute it by hand because it's going to be a pain you could just use that formula and it's really easy same thing with a lot of these other random variables hypergeometric negative binomial there they would be a pain for you to do over and over again but because we've identified a family of random variables with common characteristics someone computed a formula and now you get to use that formula and that's really nice that's really nice okay so i'll just uh leave it at that uh we will end this uh the study of this section and i will see you later i will see you when we start talking about binomial random variables for our next section we are now discussing the binomial probability distribution so a binomial experiment is an experiment that satisfies the following requirements the experiment consists of n bernoulli trials that end in either success which we will denote s or failure with which we will denote f the trials are independent and for each trial the probability of s is 1 minus the probability of failure which is some number p between 0 and 1. in the case of p being 0 or 1 this random variable is degenerate or the resulting random variable i guess i haven't mentioned a random variable yet but that random variable would be degenerate because you'd either always get success or always get failure so we don't consider that situation uh so we can think of the outcome of an experience as a sequence of s and f such as s f s f which in that case the uh the duration of the experiment would be n equals five so the binomial random variable is the associated random variable with binomial experiments and what a binomial random variable does is it will count the number of successes in the experiment so x of omega is equal to the number of s in omega uh we should probably not be uh um i mean why did i write that as a set that doesn't make any sense it's not a set right it's just a number uh we will then say that x follows a binomial distribution with parameters n and p so for example given that sequence of s's and f's that we saw before the binomial random variable would evaluate to three so we denote the problem mass function of x with lowercase b although that's more notation for this class i don't really see notations that we use in this class elsewhere because people know what they're talking about when you're reading papers and stuff so they don't bother to come up with some special notation for it anyway here we have the uh probably mass function for binomial random variables this is zero for x that's not an integer from zero to n and for x between zero and n it can in fact be computed that the probability mass function for a binomial random variable with parameters n and p is equal to n choose x p to the power x times 1 minus p to the power n minus x so here's some further explanation of this formula in this situation there are x successes out of n trials okay the probability of each of those successes is p they are independent trials so you multiply p x times and you would multiply y minus p minus 6 times this is accounting for the probability of each of those failures that occurred and here's the thing that will get you the probability of let's say the sequence ssf that would get you the probability of getting that particular sequence but the thing is there's a number of sequences where you could have three successes until two failures for example you could have sss ff or the other way around like ff sss and so on so we need to pick the positions in which successes occur and failures occur or we'll just simply pick the position of successes and if we pick the position of successes we then know where all the failures occurred in the sample or in this string so we end up with n choose x meaning out of x position out of n positions choose the x positions where successes occur the cdf of this random variable x is given next the probability no i don't want black blue the probability that x is less than or equal to little x is equal to the cdf of the binomial random variable which is equal to the sum from i equals zero because you could potentially have zero successes in your sample so from i equals zero to x rounded down the probability mass function at i n p which is equal to the sum from i equals zero to x rounded down and choose i p to the power i 1 minus p to the power n minus i and it looks like all i did was write down sum over the probability mass function then that that is what i'm writing down i didn't simplify this any further there's not really a whole lot more that you can say with this formula there's no fun little algebraic simplifications that you get all you're just going to say is sum up over the probably mass function and for that reason you're with the exception of um uh cases like some strange n or p historically in this class i would have students use the tables that were provided in the back of the textbook to work with the probably mass function or no not the problem mass function the cumulative distribution function now seeing as i am teaching this class online at the moment i don't necessarily see why i shouldn't like i'm telling my students that they can use r for pretty much anything even on quizzes and even on tests so for that reason i'm just not going to bother with working with the textbook and using the tables in the back of the book in these videos instead i'm just going to use r to get the cdf for binomial random variables although there may be some situations where like we might have um the input x we might replace the input x with say one okay if it's one you don't necessarily have to use r maybe i'll tell you not to look up the number and not to use r and ask you to compute the cdf just because there's only two things you're going to end up having to compute only two things are going to get plugged in so but that's kind of where what we're working with right now i might recycle these videos in the future and if i do be aware all right so one thing that's nice in these uh upcoming sections is i'm not going to go through the trouble of computing expected values using that summation formula using like x times p of x the sum over all x with p of x is not zero i'm not gonna bother with that anymore i'm just going to tell you what the expected value for this random variable is it's np no it's just np it's np i was jumping ahead of my head to the variance the variance of this random variable x is equal to n times p times 1 minus p and the standard deviation of x is just the square root of the variance going to draw your attention to something one way you can view binomial random variables is as the sum of bernoulli random variables in fact you could probably play around oops i didn't want to race have a look at this probably mass function formula and show for me that if you choose an n equal to one the resulting probability mass function is the is the probably mass function of the bernoulli random variable that is in fact the case um so uh i so that's that's something to look into uh but okay i'm saying that binomial random variables are the sum of n bernoulli random variables the expected value of us oh hold on and independent bernoulli random variables that's critical if that's the case remember that the expected value of a bernoulli random variable was p and the expected value of a binomial random variable is n times p so you're saying in a sense that we add up p n times to get the expected value hmm intriguing and actually remember that the variance of a bernoulli random variable was p times 1 minus p well now we're adding up n of those and we get np1 minus p for the variance intriguing so that is something to notice and also these expected value well i don't know necessarily about the variance being something very easily interpreted but the expected value certainly is it's saying that if there's a probability of a success happening let's say that it's a let's say that the probably success is 0.1 and you do this experiment 10 times then you expect to see one success in your sample or if you do this experiment 100 times you expect to see 10 successes in your in your sample so it's actually a rather easily interpreted quantity this uh n times p quantity okay and i mention here that select values of bx and p are given in table 8.1 of the textbook but in this video i'm just going to use r okay uh moving on you flip a fair coin ten times all right so we should start filling out with numbers you flip a fair coin ten times there's going to be a binomial random variable showing up so fair coin suggests that the p parameter of this binomial random variable is going to be one half presumably what we're doing is counting the number of heads and if we're counting the number of heads then the resulting random variable is binomial and the end parameter of that binomial random variable will be 10. all right what is the probability you see exactly four heads do so without using a table the probability that this random variable x which is following a binomial binomial distribution with parameters n is 10 and p is one half so the probability that x is equal to 4 is going to be 10 choose 4 onehalf to the power 4 and one half to the power ten minus four okay you're probably noticing well okay we got one half to the power four one half to the power ten minus four so that's the same as 10 choose four one half to the power ten and that's just basically because one half is equal to one minus one half so maybe i should write one minus one half to be a little bit more clear uh like that that that's the reason why but if he had instead instead of one half we said the probability of getting heads is point one then this would be the then thinking about the problems function this way would have been more correct or would have been correct it's not more correct because the other one is incorrect all right so uh actually we're going to compute this thing by hand so we're going to say that 10 choose 4 is 10 factorial divided by 4 factorial times 6 factorial and we have 1 over 2 to the power 10 that's one half raised to the power 10 which is equal to 10 factorial divided by 4 factorial times 6 factorial we've got 10 times nine times eight times seven over four times three times two times one and then this is all multiplied by one half to the power 10 and the 4 and the 2 will cancel out with the 8 and the 3 will cancel out with a 9 leaving us a 3 so that gets us uh 210 over 1024 that's the tenth power of two uh which is equal to since there is a two in common uh 105 over 512 which is approximately equal to 0.2 okay so that's the answer to that one if x follows a binomial distribution with parameters 10 and 0.5 compute the probability that 4 is less than x which is less than or equal to 6. so we've actually got a couple ways we could do this let's do this without the table the probability that 4 is less than x which is less than or equal to 6 is equal to the probably mass function at five because you don't include four plus the probably mass function at six which is equal to uh uh it's going to be 10 choose five and we know we're just going to end up with one half to the power 10 in the end if in general if uh our parameter were not one half we should probably we should probably reason this way so we've got 10 choose five plus ten choose six i guess we switch to green uh one half to the power ten and that means what we need to compute now is ten choose five and ten two six we already know that one half to the power of the ten is uh uh one over a thousand twenty four so ten choose five that's going to be uh ten times nine times 8 times 7 times 6 divided by 5 times 4 times 3 times two times one and ten to six is equal to two hundred and ten and that's because 10 choose 6 is equal to 10 choose 4. i'm going to leave it up to you to figure that out i believe that was a problem in the exercise set but yeah that's the thing so now what we need to figure out is 10 choose 5. so we've got the 5 and the 2 canceling out with the 10 the 3 canceling out with the 9 reducing it to 3 uh and the 4 canceling out with the 8 reducing it to 2. so we've got in the numerator 3 times 2 times 7 times 6 and 3 times 2 times 7 times 6 is uh 252 i believe yeah so it's going to be yeah that's 252. so this quantity evaluates to 252. so we will get for their our probability uh 252 plus 210 which is uh which is going to be 462 divided by 1024. which that is that's around point uh five after you do some rounding uh we can also do some reducing of that fraction too now that said there was an alternative way we could have computed this quantity which was to say that this is equal to the cdf at six minus the cdf at 4. and then what's left what's left to do is get the cdf at six and four all right well let's get that i'm gonna have to boot up an r session all right so we've got p binom that's the function that is responsible for working with binomial random variables and we're going to give p bino what are we going to give it uh right so we're going to give it six our other parameters are size that's 10 and prob is equal to 0.5 minus p by nom which is going at 4 size equals 10 prob equals 0.5 yeah about 0.45 which rounds to about 0.5 which for what it's worth i said it was approximately 0.5 when we were doing stuff by hand and that was because we were rounding i knew i was rounding when i was uh when i was when i was saying that's about 0.5 so uh well let's go ahead and write down that more exam exact answer say that this is approximately 0.45 okay next example compute the probability that 2 is less than or equal to x which is less than or equal to four we could do this by summing up over the probability mass function but now i really don't want to do that i'm just i mean i've got better ways to spend my time so i'm going to instead say that this is equal to the cdf at 4 minus the cdf at uh two minus one remember we have to do the two minus one because we need to include the two in our region and the only way to do that is if we do two minus one okay and the other parameters are ten and one half and and you guys know what two minus one is this is equal to one which is going to be well let's compute this so p binom at 1 minus p by nom or other way around actually so we got four all right so 0.366 so approximately equal to 0.36 six what is the probability that you see more than seven heads strictly more so this is the probability that x is greater than seven that is well the converse event or the uh um the complementary event to the x being greater than seven is x is less than or equal to seven so this is going to be one minus the probability that x is less than or equal to seven since remember the probability of a complement is 1 minus the probability of a the complement of the set x is smaller than 7 is x is less than or equal to 7. so then we get that formula uh so this is going to be uh oops this is equal to one minus a cdf at seven with parameters ten and one half and now we need to compute that and we're going to turn to r for that so this is 1 minus the cdf at 7 size equals 10 prob equals 0.5 so the probability is about 0.05 let's say 0.055 so this is approximately equal to .055 now i should probably mention something else about how the software was working we could have done instead p by nom 7 size equals 10 prob equals 0.5 and there's an additional parameter that all of these uh p functions have which is lower dot tail lower dot tail let's set that equal to false that got us the same thing uh the p so basically these p bottom functions by default they're giving you the cdf but they can also give you one minus the cdf if you set lower tail equals false if you were to ask the developers for these functions they would say that rather than doing one minus the the cdf or one minus p binom or whatever you should use the lower tail equals false parameter the reason being that you're going to get less numerical error if you're using the lower tail equals false parameter because numerical error is very much a thing like we care whenever whenever we're using software to compute numbers we care about numerical error and it turns out that setting lower table tail equals false that results in less numerical error uh i i don't really know why i'm guessing it's because they can do some more optimizations or some other fancy uh numerical tricks thing is as an instructor i like to make sure that people are thinking like this expression or this relationship i really want students to understand that and it's very easy to just lose that under the easiness of this of this um of this uh of this function in this notation so i don't know how frequently i'm going to do that um and also for for whatever it's worth sometimes when i'm writing my own uh functions for probability i don't always include that parameter myself just because i can't i don't have it i know i don't really know what the developers are doing to make sure that lower tail goes false gives you more accurate answers uh so also by the way if if you're watching this in the future hello future person if you're watching this in the future and you're using a table because maybe because i told you to uh you're using a table for these calculations you don't have access to the probably that x is greater than seven you only have access to the probability that x is less than or equal to seven so being aware of um how this stuff is working uh or at least being aware of this relationship that i've underlined in blue still matters a great deal all right so continuing on compute the expected value of x the variance of x and the standard deviation of x all right so the expected value of x let's see all right the expected value of x is n times p which is 10 times one half which is equal to five all right simple enough the variance of x is n times p times 1 minus p which is 5 times one half which is five halves or two point five the standard deviation of x is going to be the square root of five halves and i don't know what that is off the top of my head so we can just leave it like that that's fine all right uh our functions that are doing this stuff well i think we just did it so some of this is completely redundant um here i actually created a random variable using discrete rv which presumably is loaded up uh so i create a random variable x to represent the x that we were talking about before and computing's expected value variance and standard deviation these are basically the same as what i had before i also plotted its probability mass function this is what it's probably a mass function looks like okay next example your manufacturer of widgets sends batches of witches and giant bins your company will accept a shipment of widget of widgets if no more than seven percent of widgets are defective the procedure for deciding whether a shipment is defective is to choose four widgets from the batch at random without replacement if more than one widget is defective the batch is rejected what's the probability of rejecting the batch if seven percent of widgets are defective model the process using a binomial random variable so we have so we're going to assume that there are in fact seven percent of widgets and actually the argument being used in this problem uh this problem is kind of suggesting the possibility well actually the procedure being described in this problem is basically a hypothesis test and uh we're gonna talk more about hypothesis testing later in a later chapter uh but basically what you do when you're working on the mathematics of a hypothesis test for computing pvalues and all that stuff you assume that the null hypothesis is true so the null hypothesis is that seven percent of the widgets are defective so um uh so in this case we assume that seven percent of widgets are defective in which case the distribution of the random variable x is going to be let's see how many widgets did they pull out for uh so n is equal to four and p is equal to 0.07 okay so also there's another wrinkle here the bin of widgets has a finite number of bims uh no no no not not fighting overpins has a finite number of widgets but this binomial random variable is not supposed to work in that situation see if there's a finite number of successes in this possible in this finite population then an implication of that is that you don't have independent successes and failures because if you pull a success out of the population you cannot pull that success again and presumably in this example when a when this uh when you're checking the widgets uh you pull out a widget you check it but you don't put it back in the bin it's for you to draw again no one ever does that so actually we don't have independent bernoulli trials so we don't have a sum of independent bernoullis we don't have independent trials which means that technically this random variable should not be a binomial random variable actually the random variable that is more accurate for this context is what's known as a hypergeometric random variable which we'll talk about in a later section thing though is if the sample if the population is large enough relative to how many successes there are in the sample like if there's a million widgets and seven percent of those widgets are defective then that means that about 70 000 defective widgets exist in the sample or in the population my my apologies uh in which case it the the numbers are so large that basically you can treat this as a binomial experiment anyway because the difference between the binomial random variable and this more accurate random variable the hypergeometric random variable those differences are negligible so you so you you get to you get to cheat you get to use the simpler binomial random variable as opposed to the more complicated more complicated hypergeometric okay so let's carry on then they want to know what is the probability of rejecting the batch if some percent are defective uh if more than one widget is defective okay with this problem we actually need to translate out because this is a word problem and by the way in stats classes i absolutely love to ask word problems so many word problems because statistics is so applied that it just feels inappropriate to not be asking word problems um it's it's such an applied subject that you have to be asking them okay so what corresponds to rejecting the batch you reject the batch if there's more than one defective widget in your sample so that's it so more than one that means greater than one and our random variable for tracking the number of defective widgets that we found is x and i guess all right a student might find it wait a minute wait a minute you're calling it defective widget a success yes it's mostly for the language all right so this is the probability we want to compute we want to compute the probability that x is greater than one which is equal to one minus the probability that x is less than or equal to one and at this point you could say all right let's go to r and compute this and i'm gonna say no we're not going to do that we're going to instead compute this thing by hand and say that this is going to be the pmf at uh 0 for parameters 4 and 0.07 plus the probability mass function at 1. oh i'm sorry i'm sorry uh i need to say that this is equal to 1 minus parentheses all that stuff okay all right there we go that's correct and now we need to compute each of those probability mass functions okay so the probably mass function at zero is going to be we've got four choose zero and four choose zero is one there's only one way to choose uh none of the four things in your in your little group and that's to choose all the one all the other ones there's only one way to do it so four to zero is one uh next up we've got uh what what have we got um oh yeah uh so .07 to the zeroth power and .93 to the fourth power b one for .07 is equal to four choose one which hopefully you know is four you can at least reason about i was like okay how many ways are there to pick one thing out of four we'll pick one of those four things all right there we go um so there we go uh so then we got .07 to the first power .93 to the third power and after this you go to a calculator to compute those numbers i think i actually computed them rather accurately in my notes so let's see what did i have okay so what i'm seeing here is that this is equal to so the so the top one is equal to point seven four that is an exact number i got a little carried away with the accuracy and the second one is point two two five two two and that means that we're going to have that this quantity here that ultimately is what we're trying to compute is equal to 1 minus 0.748052 plus 0.22522 which is equal to 1 minus 0.973272 3272 which is equal to 0.026 seven two eight all right uh next up oh yeah there it is excuse me my apologies i had to sneeze okay uh next example uh this one's fun i claim that i can make 80 of my free throw shots when playing basketball you plan to test me by having me shoot 20 baskets if i make fewer baskets in a specified amount you will call me a liar the threshold amount of baskets is chosen so that the probably make less than this amount given that i am in fact an 80 free throw shooter does not exceed five percent what is the threshold amount all right uh oh yeah additionally compute the mean and standard deviation of the number of shots i would make if my claim is true okay uh let's do this let's do the second part first because that's easier uh the first part is going to require a conversation so the expected value will say um s is following a binomial distribution uh with uh parameters i'm going to shoot 20 baskets yeah so n is 20 and p is 0.8 because all right this is again a hypothesis testing type problem in which case you're assuming that i am in fact an 80 free throw shooter so the expected value of not x because i decided not to do x um of s is going to be 0.8 times 20 which is equal to 16. so you expect me to make 16 of my baskets uh the did i ask for standard deviation yes i did so the variance of this random variable is going to be uh 20 times 0.8 times 0.2 which is equal to 3.2 and the standard deviation of s is equal to the square root of 3.2 which i don't know off the top of my head and i i'm i'm just i'm just not gonna bother okay all right so that was the easy part now for the hard part uh i have asked that you pick a threshold amount of baskets so that the probability i make less than this amount given that i am in fact an 80 free throw shooter does not exceed five percent all right so we will call this threshold amount we will give it a name uh the threshold amount we will call this we will call this quantity k all right i'm asking that i i'm asking you to find a k such that the probability that s is let's see so if i make fewer baskets so s is less than k this number needs to be at most 5 so this needs to be at least 0.05 well at most 0.05 and actually there's going to be a number of possible k's such that it's less than 0.05 but we're going to say that this is the largest possible k uh such that such that this probability is less than or equal to 0.05 uh k is a constant here we just don't know what it is uh s is a random variable uh let's go ahead and play around with this expression some more before continuing on this is saying that the probability that s is less than or equal to k minus 1 uh less than or equal to k minus one that's going to be less than or equal to 0.05 all right so we actually so by doing that i have the cdf on the left hand side of the inequality and 0.05 on the right hand side so we're go what we need to do now is basically a reverse lookup we're looking up a number such that the cdf is um less than or equal to 0.05 the largest number possible such that the cdf is less or equal 0.05 this is similar to the notion of quantile because a quo so you have a probability that a random variable is less than or equal to uh let's say 10 percent then the 10 quantile is that number so this is actually related to the notion of quantiles the unfortunate thing though is that what we're talking about are discrete random variables which adds this complication in that it's possible that the cdf in fact not just possible it's quite likely that the cdf never actually equals 0.05 if it were in fact equal for the cdf to equal 0.05 then we would say all right pick a k minus 1 such that cdf is equal to 0.05 the only thing though is that's not actually likely to be the case um it's likely that the cdf never actually reaches 0.05 in fact let's now go to r so let's look at so we got uh the cdf the cdf is so possible values for the standard variable are from 0 to 20. so we're going to ask for the cdf's values from for all numbers between 0 and 20. size is equal to 20 prob is equal to 0.8 ah that's it's not very helpful um all right i think that if we were looking at the textbook we would be rounding to uh three decimal places so uh so digits equals three all right yeah this is yeah we're gonna do this just because like what we have up here is scientific notation which may be more accurate but uh also it's hard to read so we're gonna round to three decimal places and work with this so this is the cdf uh and what we're looking for is uh we would go let's actually let's actually call give this vector a name so we'll call it cdf and we'll say names cdf will be 0 to 20. so now let's print out cdf okay all right so so so so so so um what we're looking for in this is a qua is a quantity where the cdf uh does so that so notice that the cdf is increasing as we increase the input to the cdf right our cdf is increasing but and we want to find the largest number that we can put into the cdf such that it doesn't exceed 0.05 and that number is 12 because at 12 the cdf is going to be 0.032 and at 13 the cdf is going to be 0.087 so it crosses that threshold at 12 suggesting the that k minus 1 suggesting that k minus 1 equals 12. and then we add 1 to both sides to suggest that k is equal to 13 that is if i score less than if i make less than 13 baskets you're going to call me a liar uh what is uh 13 divided by 20 that is uh 0.6 so that's 0.65 so if i make uh 65 or less than if i make less than 65 of my basket you're going to call me a liar the principle being the the logic being that that is such a rare amount it's so unlikely for you to score less than 60 65 of your baskets if you were in fact an eighty percent three free throw shooter that we're actually gonna say it's more likely that you're lying then you're actually telling the truth or at least it seems it seems unreasonable to continue to believe that you are still telling the truth so this is this so so that's how you would use like the cdf if we were if we if you were using the textbook you would use the cdf this way you would scan the cdf uh for some reason my mouse stopped working i don't know why this is a super cheap computer you would scan the cdf until eventually you crossed over that 0.05 threshold and then take whatever this what take whatever number got the cdf2 uh just below that threshold so if i had switched this to uh say all right it needs to be less than or equal to 0.10 then we would go to 13 we haven't crossed point 10 yet we would go to 14 but then we then we would cross so we would say all right the threshold amount is 13. uh let's say that i said instead 0.01 uh actually 11 is probably not what you would choose because you know that we're rounding here so you would go with 10. so because you know that actually well i don't know what what was a 11 so if we look back at the original vector that's a bit more accurate that doesn't have any rounding so one two three four five six seven eight nine ten eleven uh next one oh actually it's rounding up so you could choose 11. yeah 11 would be fine so yeah it doesn't always round down uh so that's what you would do you would just kind of reverse look from the uh the uh from the cdf especially if you're using the book or you could use q binom uh we'll put in 0.05 size equals 20 prob equals 0.8 you could have used uh okay what exactly is cubinom doing uh it might be doing something else it might be saying okay you exceed it because so we actually might need to do q by minus one uh let's look at the documentation for q by nom oh i think you just yeah you would just have to recognize all right so uh it's going to give you some details somewhere okay so it says right here the quantile is defined as the smallest value x such that f of x is greater than or equal to p which is actually different from how i just defined it so r's definition of what a quantile is for discrete random variables is uh different than what i just said so being aware of that you would actually have to take uh whatever q binong gave you and then do minus one to get the right answer okay but all right or i mean i i don't know i think there's so many different ways to possibly think about it and say okay the q binom that that r actually has is uh effectively uh giving you this quantity right away so you don't have to work with the cdf you that's another way you could possibly think about it all right so uh by the way what we're basically saying is that if you ended up shooting a number of baskets such that you ended up in uh 11 12 such that you ended up in this region it's so unlikely if you were in fact an 80 free throw sure to end up in this region i'm justified in saying you're a liar how unlikely is it well it's actually uh 3.2 percent uh it's so so so unlikely that we would just call you a liar because if you were in fact a three throw shooter you should probably be ending up in the other region and 80 free throw shooter my apologies i'm always confusing my words okay so that's it for uh the binomial random variable and in the next section we will be talking about the hypergeometric and negative binomial distributions all right so i will see you there uh we are now on to the last section of this chapter discussing the poisson process and the poisson probability distribution so x is said to follow a poisson poisson distribution with parameter mu or if the probability mass function of x is given by p of x parameterized by mu this is equal to e to the power negative mu mu to the power x over x factorial for uh x being a member of the set 0 1 2 and so on so in other words if if x is a whole number if x is a whole number then this is the probably mass function otherwise it's zero so the first question you may ask is this a valid pmf the answer is yes and why is that well let's sum the probably mass function from x uh sorry from x uh equals zero to infinity we got p of x parameterized by mu this is equal to the sum from x equals zero to infinity e negative mu mu to the x over x factorial the e to the negative mu part that is effectively a constant so that can be pulled out so we could say that this is e to the negative mu and then we have the sum from x equals zero to infinity mu to the x over x factorial and that part which i have highlighted in red that is e to the power mu y calculus 2. this is the if i remember right the taylor expansion of the function e to the power x so this is coming from calculus 2 but yeah that evaluates to e to the mu hence we get e negative mu e to the power of mu and those are that equals one and furthermore this probably mass function is positive everywhere so we get this is a valid pmf so uh poison random variables if x is following a poisson distribution with prion or mu then the expected value of x is equal to mu and also the variance of x is equal to mu so that means that we are parameterizing poisson random variables by their mean so in uh in your book if you're using the uh the dvor book table 8.2 contains the cdf of select poisson random variables or select poisson distributions uh in r the functions that are responsible for handling points on random variables random variables sorry our deploys handling polymath function cdf quantiles and random and random variants so in other words creating random instances of poisson random variables and they will be parameterized by their mean so the poisson distribution describes uh random variables that follow the poisson process so here is the intuition of poisson random variables they are tracking how many times within some how often a quoteunquote rare event occurs within a finite span of time so and which isn't at all clear from looking at this probably mass function that would in fact be the interpretation uh this is more of a limiting result this footnote that i have right here actually gives a little bit more justification as to why this is the case but you can think of it as all right we have some rare event uh me like i have a favorite example that was used by my uh 3070 instructor maybe i'll just pull up his webpage for you uh like i he he's got some he's got some interesting stuff on his on his web page that even now i kind of look back to it and like i think about it and i just kind of i kind of want to update it i think i think it could be updated but there's some interesting stuff there genuinely so math.utah.edu tilde treyberg uh his name was uh andres trebergs a professor at the u not actually a probability uh professor that's not his uh area of expertise although he's taught math 70 a few times and math the no matt 3070 sorry and matt 3080 a few times and like he he's interested in statistics but he is a uh what what is that area of study of his i think it's like analytic geometry i think that's what he studies so i wonder what happens if i just go straight here we could probably find from here his uh 3070 page yes i think this is i think this is it math 3070 fall 2013. math 3070 fall 2012 is the last that was the class that i took this was my web page this is where i went to find his stuff so yeah i i took matt 3070 in this class i think i'm pretty sure that's true all right if i scroll down yeah the supplementary materials oh yeah so here's a simple r uh which by john farzani this is the uh lab textbook for the r lab and uh several example problems he's got this one let's find it let's see prussian no no press poise ah let's see is this it i think this is it i think this is it uh are there horse kicks are people getting kicked by horses that's what i want to know i think this is i think this is it um i don't think so i don't think this is the horse kick example but the horse kick example is fun let's see oh why i want to close that webpage uh if we try horse come on there it is there it is the horse kick example ah yeah i love this one yeah so so have a look at this but basically it turned out that in the prussian army um you can model the number of people who died from horse kicks with poison random variables that poisson random variables actually do a good job of modeling stuff like that but i also had as potential examples i'm not entirely sure how the how accurate this is but you could maybe like maybe think of uh number of calls that a call center is getting in a day or the number of points that scored by a team in a game uh poison random variables so and also as being a poisson process a poisson process uh is a bit more general than that so a poisson process is a stochastic process stochastic processes are not the subject of this class beyond this little description i think another thing that could be considered a poisson process or that could be modeled by a poisson random variable is let's say you have some radioactive matter and the number of uh is it atoms i'm not sure i'm not very good at physics but some some particles are leaving the radioactive mass and the amount of particles that are leaving the mass uh over some period of time can be understood as a poisson random variable and also as a poisson process so i have this r code that's meant to simulate a poisson process and with a poisson process the time at which a particle or the time at which uh the pro the process jumps is random so it will so there are going to be random times at which the process increases its value by one so uh it will jump so you'll wait for a period of time and then suddenly the process will jump and basically something has left and then you'll wait a little longer and something else has happened maybe you could imagine this as being uh points in a game maybe a basketball game although i you could probably see why it's a little bit inaccurate to view points in basketball game as a poisson process because there's no way that you could well i guess this is possible if maybe if maybe you said both teams uh if you're counting like total points scored by either team i don't know it seems unreasonable though but you could but basically at random times this process is going to jump uh by one and the value of the process is going to increase and basically any fixed time the value of the process at a fixed time uh so you fix it at two at two or something and the value of the process at that time can be modeled by a poisson random variable so yeah that's uh hopefully that gives you some idea of what they're what they're describing how often some event occurs over a fixed period of time which in principle that event could be unbounded there's an infinite number of pos it could happen like there's no upper bound on how many times this event could happen it can happen an infinite number of times there's well okay maybe not literally infinite but any large number of times it's highly unlikely that it will be very large but there is no upper bound right so you're not gonna restrict it like the number of points that you could score in a basketball game like there's no there's no limit on that no one will just end the game well i don't know maybe but that's that seems that seems rather academic but yeah that's so this is going to be a random variable that is defined hello everyone we are now on the next chapter on continuous random variables and probability distributions continuous probability models are another major class of probability models in the previous chapter we saw models for discrete random variables with discrete random variables there was either a finite number of possible numbers this random variable could take or it was countably infinite for example the whole numbers were were possible in this situation not only are the number of possibilities infinite they are uncountably infinite so this these uh probably models allow for any real number within some range it could be within a range a to b or z like a to infinity or zero to infinity or negative infinity to infinity so any real number any of those could be possibly taken by the random variable so as a consequence of this we're going to change some of the notions that we had with discrete random variables but not by much for example the probability mass function is going to be replaced with the probability density function which is what we're going to be talking about right now and the cdf is still defined the same way but it's going to be computed a little bit differently instead of it being involving a sum it's going to involve an integral and expectations are also going to involve integrals basically whenever you would add numbers with discrete random variables you integrate with continuous random variables so this is where your calculus knowledge is going to be tested one nice thing though about continuous random variables though is that when you're working with continuous random variables uh the probability that the that that random variable is equal to any particular number is always zero which is the reason why we have to have uh integration and density functions so the probability that x is less than x is the probability that x is less than or equal to x and admittedly it is a little strange that the probability that this random variable is equal to a particular number is zero it's it's somewhat strange because when you go to your random number generator and ask this thing to produce a number it gives you a number but the probability that you got that number was zero so events that are happening with probably zero are happening all the time whenever you're working with continuous random variables but the way ferrous rasulaga one of my probability instructors researcher at the university of utah very good mathematician uh one way he put it was uh the you know at some level that someone's going to win the lottery you just know it's not going to be you so you get the probably that you win the lottery is zero and the probably that anyone wins the lottery is not zero that's the way he put it so uh let's uh move on to discussing probability density functions so these are the analog to the probability mass function for discrete random variables the pdf is a nonnet negative function which i'm calling f x such that for any two numbers a and b with a less than or equal to b the probability that a is less than or equal to x which is our random variable which is less than or equal to b is equal to the integral from a to b f of x dx naturally in order for f to be a valid pdf we must also have that the integral from no not a uh the integral from negative infinity to infinity of f of x dx well that's the probability that this random variable is between negative infinity and infinity and that's basically asking for what is the probability that this random variable is any real number we're basically asking what is the probability that x is a real number so naturally this must equal 1 because we know that x will be a random a real number and it will be finite so this is another relation we have to have now regarding this f itself might not be continuous everywhere and it's also possible that actually integrating from negative infinity to infinity is a bit much because actually this random variable is only positive on some on some interval of finite length and everywhere else it's zero so you're integrating for the most part zero and in fact we'll see one example of this one we are now discussing the ever famous maybe two famous sometimes infamous infamous not infamous that's not a word sometimes infamous normal distribution we say that a random variable x follows the normal distribution sometimes denoted like so x uh x follows a distribution n mu sigma so we say x follows a normal distribution with mean mu and standard deviation sigma if it has the pdf v of x parameterized by uh mu and sigma is equal to all right we should probably zoom in for this um it's a fairly a little complicated we have 1 over the square root of 2 pi sigma squared so this fraction multiplying with e to the power negative x minus mu squared divided by all in the power uh two sigma squared it's worth mentioning that often the normal distribution is parameterized not by its standard deviation but rather by uh its variance and you can kind of see why when you look when i've written this formula down yeah it's possible to take this sigma squared and pull it out in front of the square root uh but we can put it inside of the square root and then you have uh and then everything you've got sigma squares everywhere so you could just specify sigma squared directly and also it feels to statisticians to be or and probabilists my apologies to be more appropriate to parameterize by the variance rather than the standard deviation since it's often easier to work with the variance uh directly rather than the standard deviation and in addition to this you could say that parameterizing with the variance generalizes better when you start talking about multivariate versions of the normal distribution but this is fine for now like admittedly at at an introductory stats level it feels somewhat like since the standard deviation is the more uh natural measure of spread and the variance a little bit more alien you could argue that to these students it seems somewhat better to use the standard deviation rather than the variance but it's fine so this is the curve here is a sketch of the density curve for the normal distribution we have uh oops okay so here's kind of what it looks like uh we would have basically here is the mean here is the mean plus one standard deviation and here's the mean minus one standard deviation okay so we would have basically a curve that goes up there within one standard deviation is an inflection point so it will go from convex to concave at the inflection point and then it's going to be a symmetric curve and then go from concave back to convex all right and this is a simple sketch of what it may look like its peak occurs around the mean and we have our inflection points being within one standard deviation of the mean okay so that's a this is the whenever you hear the words the bell curve they are probably referring to the normal distribution be aware that the normal distribution is not the only bellshaped curve that is used in probability and statistics there are other bellshaped curves for it for instance there's the t distribution there's the koshi distribution i think i encountered a distribution recently that is meant to model uh when well okay it's not really a probability distribution though but it's kind of like one uh when someone gets the coronavir or no not cordovice when someone gets a virus during a pandemic type situation there's a curve for that that looks like the normal but isn't the normal i can't remember what it is though it's like the logistic curve i i don't know but yeah um uh this is usually when people are talking about the bell curve all in caps like capitalize and all that they're talking about the normal distribution here's an r plot of the density function of a standard normal curve and it's got that bell shape so the expected value of a random variable x following this distribution the variance and the standard deviation will be good given next uh these should not be shocking at all the expected value of x is equal to mu the variance of x is equal to sigma squared and the standard deviation of x is equal to sigma so normal random variables are specified by their mean and their variance okay you set the mean and the variance directly with normal random variables okay one property of the normal distribution is the 68 95 99.7 rule which i ain't going to sketch out for you this is basically a rule of thumb for how much of the distribution is within uh let's say uh within one standard deviation of the mean so we got mu plus sigma mu minus sigma within two standard deviations of the mean so mu plus two sigma mu minus two sigma and within three standard deviations of the mean so mu plus three sigma and uh mu minus three sigma okay so the peak of the curve happens at mu and we'll and let's see uh just kind of getting a sketch so uh the inflection points are going to happen with within uh one standard deviation all right so we could sketch out the curve to look something like this okay so uh what this rule says is let's uh start out within uh one standard deviation the area underneath the curve within one standard deviation is going to be 0.68 so within one standard deviation so mu plus or minus sigma all right uh let's go to within two standard deviations okay so within two standard deviations so the area underneath the curve within two standard deviations so the area underneath the curve within two standard deviations is going to be 0.95 so this will be mu plus or minus 2 sigma and then if we go to three standard deviations so let's uh have this going now let's have this going up and down horizontally is rough to draw okay on the other hand that's kind of a conflicting picture but you get the idea um within three standard deviations the area underneath the curve within three standard deviations is going to be 0.997 so this will be mu plus or minus three sigma okay the unfortunate thing about that is i just drew over uh the text but i can still read so uh let z follow a normal distribution with mean zero and standard deviation one we then say that the random variable z follows the standard normal distribution and this distribution is useful since we can relate an arbitrary normal random variable to the standard normal distribution and vice versa and we can do so like so uh as a so as a reminder z is following a standard normal uh mu is an arbitrary normal random variable no no no not mu sorry mu is not a random variable so x is an arbitrary normal random variable so any mean and sigma in this case what you get is that if you take a normal random variable subtract out its mean and then divide by its standard deviation the distribution of the resulting variable will be equal to the distribution of the random variable z that is it's following a standard normal distribution which should make sense what this operation does is shift the mean to zero and then what you do is you uh scale by sigma or scale but let's say scale by one over sigma so you both uh take your curve along the number line oops i didn't want to erase stuff all right so what you're doing is you're taking your standard normal curve which is located at mu you are shifting it to the left uh by by mu and then you are compressing the curve so let's let's draw like a new curve that's oops let's draw a new curve that's centered at zero and then you compress your curve so it has a standard deviation of one all right that's what that operation is doing and similarly we could say that if we take a our standard normal random variable z scale it by sigma and then add mu the resulting random variable will be equal in distribution to the random variable x which is following a normal distribution with mean mu and standard dev standard deviation sigma all right what this is doing it's basically doing the opposite we are scaling by sigma and then we are and then we are shifting the mean to mu so to sketch out what this is doing we start out with our standard normal random variable which is centered at zero we then take that random variable shift its mean by mu so we end up with something over here and then we scale out its uh standard deviation so we get a curve with possibly a different standard deviation it doesn't have to get bigger it could get smaller too but you get the idea of scaling okay so uh let capital phi of little z be the probability that a standard normal random variable is less than or equal to z this is the cdf of the standard normal distribution then if x is the probability of an arbitrary normal distribution or normally just of an arbitrary normally distributed random variable we have excuse me we have the falling relationship between f and phi f of x is equal to phi of x minus mu over sigma where phi is the cdf of a standard normal distribution which means since we can relate any normal distributions cdf to the cdf of the standard normal random variable this means that we only need to worry about tabulating values for phi of z for the standard normal distribution in order to work with any normal distribution which is what's done in table 8.3 of dvor's book and often many of these statistics and probability books will have tables of the cdf of the standard random variable and i even remember buying a study card for math 3070 for my set my stats class and that table came with a no and that card came with a very small table for c for working with the cdf of a standard normal random variable and the reason why you can do why they're doing that is because you can get all of the information you need for any normal random variable from that table which is very nice because you can have just you can have any real number mean and any positive real number standard deviation and have an infinite number of normal random variables but you only need to print one table because once you have the table for the standard normal then everything works out great and additionally let's consider for a second the pdf of a standard or of a normal random variable uh let's let's consider actually first the pdf of a standard normal uh fee which will we will call just fee of x that's going to be uh 1 over the square root of 2 pi um actually let's let's call this v of z it's going to be 1 over the square root of 2 pi e negative z squared over 2. and this is kind of a mess so let's clean that up okay you you know by now that cdfs are computed via integration because all probabilities are computed v integration when you're working with normal random variables what then is the antiderivative of this the answer is basically what you see the thing is uh you there is no closed form or elementary solution for the antiderivative of a normal random variable it doesn't exist it simply doesn't exist it's and i think in fact it's provable that you can't come up with an antiderivative for a normal random variable for a number of random variables pdf um in a in a in in any element in any elementary form so at the end of the day you're just kind of left with saying all right this is ranging from from uh negative infinity to uh i don't know x negative infinity to x you're just left with saying that this is equal to phi of x like that you're kind of left with this uh unsatisfying uh unsatisfying answer where you just say all right phi is this der is this integral and yet at the same time this is not a problem in fact nobody really cares nobody really needs to have an antiderivative for what i've written down in black nobody really needs it because we have numerical routines that can compute these integrals mathematically we can still work with it there's nothing that says that we can't work with this we have the fundamental theorem of calculus that is able that tells us how to take derivatives of this thing so we can take derivatives we can still study its properties we can study how how quickly it grows and decays and and stuff like that there's really no reason we need a simpler expression for the antiderivative of a normal random variable so we just say this is true by d this is true by definition and go about our business because whenever we need to actually compute what the cdf is we have techniques for doing it we can use all these numerical routines uh numerical routines maybe some monte carlo simulation something like that there's all sorts of things that we can do to compute the cdf and you only need to do it once for the standard normal curve in fact i think r actually internally when when working with the cdf of a uh stan of um of normal random variables is work it's working with an internal table uh that computes probabilities so um admittedly i'm teaching this right now in an online format in a context where students are not going to ever enter a testing center so they don't need to use the table and if i were teaching this in a regular semester i would teach students to use the table and it just feels like right now that's silly because they have access to r and they're not going to want to use the table and i've all never really had a problem with teaching the table from a pedagogical perspective because i feel like using the table was good practice for working with the basic properties of the normal distribution and now i don't really see a reason to use it i mean there is still that pedagogical reason but it's just completely swamped by the convenience of having r around so we're going to lose that i'm actually rather sad that this time i'm not really going to use the table that i'm just going to compute probabilities using r but hopefully you can still get the message and understand some of the properties that would be learned by working with the table such as the symmetry of the normal distribution or working with one minus and stuff like that okay if i ever teach this class again in person uh maybe what a maybe i would and if i were to use these lecture videos again maybe i would create a separate video for uh working with the table but i don't think i will do that now all right so um anyway let's compute the following we're now working with a standard normal random variable so remember that this is a random variable uh with uh a mean i can do better than that hold on okay so this is a random variable good grief good grief come on just some people just don't want to press the undo button good grief ugh this screen how i hate it okay um okay so remember that we are working with the standard normal distribution so it's a distribution centered at zero standard deviation one i'll just tell you that standard deviation is one i want to compute the probability that z is less than equal to zero so this is what the normal distribution looks like this is the area that i want to compute what is that area well i know the area under the entire curve is one because it's a pdf and we know that we're shading the area to the left of zero and the zero is the point of symmetry so that means half the area so if you were to fold the curve over on itself around zero it would have equal area to the left and to the right of zero which means that this must be half of the area underneath the curve so that means that this is going to be equal to 0.5 because 0 is the median of a standard normal distribution all right uh next what is the probability that z is less than or equal to 1.23 okay so what where she so what we're computing here here's a standard normal curve here's 1.23 we are computing the area underneath the curve and to the left of 1.23 okay and at this point i'm going to ask r what that area is so okay so i want a p norm by default p norm is working with a standard normal curve so we've got p norm of right uh 1.23 so that's going to be 0.8906 uh or we'll say 0.8907 so this will be this is equal to 0.89 okay uh let's see next up oh look at that some r code and uh it's basically confirming what we got via r so not shocking all right next up the probability that uh the center number and a variable is between negative 1.97 and 2.1 so this will be we've got the standard normal curve okay here's zero here's uh 2.1 here's negative 1.97 and we want the area in this region the area in between 2.1 and negative 1.97 so how are we going to do that well one thing we could do is say remembering that we are working with a cdf that this is going to be the area underneath the curve to the left of 2.1 minus the area underneath the curve to the right of negative 1.97 so you just subtract out the area from the left of negative 1.97 from the air that's to the left of 2.1 you can think of that as you have a piece of construction paper and this piece of construction paper can is the normal curve including the region underneath the normal curve that's a really long piece of construction paper since uh the normal curve extends from negative infinity to infinity right i never put any sort of bounds on this curve so i hope you notice that that this is a random variable that can take any number any real number between negative infinity and infinity um so we uh uh but you know we imagine that we have this uh maybe we clipped it off after a certain point and uh which is fine because after after a while the normal curve becomes minuscule so so minuscule because it it approaches zero very very quickly one way to interpret the 30 68 95 99.7 rule is saying that almost all of the curve is within three standard deviations and there's almost nothing outside of it so and that goes even more so for four standard deviations five standard deviations and so on there's almost nothing there um anyway we have we imagine that we have this piece of construction paper and we we clip off the area at 2.1 and have the area underneath the curve and to the left 2.1 and then we clip off the area underneath the curve to the right to the left of negative one point nine seven and what we end up with is the area that we want so we met or and if what we could end up doing is we start out by measuring the area underneath the curve uh the the area of our construction paper when we did our first clip when we clipped at 2.1 and we measured that area and then we clip again at negative 1.97 and measure the area of the part that we clipped off and then subtract that from our earlier calculation to get the area that's remaining uh uh for our construction paper so this will be uh fee uh remember this remember that capital fee is a cdf of a standard normal random variable so fiat 2.1 minus fee at negative 1.97 okay and then we need to compute this so p norm uh 2.1 minus p norm negative 1.97 and this is what we get so we get 0.9577 so we get oops so we get that this is equal to 0.9577 okay uh next example the probability that z is greater than or equal to 1.8 so this is the area underneath the normal curve that's to the right of 1.8 okay and we say that this is going to be well we could say that this is uh the area underneath the entire curve so here i've shaded the whole thing minus the area underneath the curve uh to the left of 1.8 going back to our construction paper analogy you have this piece of construction paper that has that's the the area underneath the entire curve uh and you clip off the uh the part at 1.8 and you're left with uh the part from negative infinity up to 1.8 so you lost the other part and you want to figure out the area of the other part well you knew the entire area was one so you measure the area of the part that's that you um the part to the left of uh to the left of 1.8 and subtract that from one to get uh to get the area that's underneath the curve and to the right of 1.8 so this would be 1 minus the cdf of the standard normal at 1.8 and we can go to r and compute this so 1 minus p norm at 1.8 and this is going to be 0.0359 so this is 0.0359 okay excellent uh the probability that it's greater than 5.2 so this is going to be approximately zero but i'll go ahead and compute this in r and say one minus p norm uh what was the number we're plugging in 5.2 okay 5.2 all right very very very small number uh not quite numerically zero because we can go to 16 decimal places but very very close um very small number and actually if you were using the table and if you look at most tables most tables don't go beyond four standard deviations they might even go beyond three or three and a half but you'll almost never see a table go beyond four standard deviations and then here we're asking for the area underneath the curve to the left of five standard deviations so uh if we were actually into the table which is uh the context in which these notes were written uh we would say what i would basically tell students is say this is approximately zero so don't even bother to look at the table just say this is approximately zero okay here's some arco that's doing all these calculations uh we also we also could have done some of that one minus stuff using the lower tail uh parameter let's see is that different from what we had no it's not different so that looks to be about the same um all right uh next example so iq scores are said to be normally distributed with mean 100 and standard deviation 15 like q be randomly selected be a randomly selected individual's iq score uh compute the probability that q is between 85 and 115. so in this case this was here's another thing um this is there's a lot of reasons why i really liked working with the table and one of the things that was great about working with the table was that it forced you to translate from a normal distribution to a standard normal distribution or any normal distribution to standard normal and thus i felt like it would force students to learn the relationship between any normal random variable and a standard normal random variable so what we could say here is that this is going to be the probability that 85 minus the so q is going to be according to this problem a normal random variable with being 100 and standard deviation 15. so this will be 85 minus 100 over 15 that's less than or equal to q minus 100 over 15 which is less than or equal to 115 minus 100 over 15. and this part right here is equal in distribution to a standard normal random variable z so we could say that this is equal to after you compute that lower and upper bound you'll find that this is the probability that negative one is less than or equal to z which is less than or equal to one which is going to be about 0.68 because of the 68 95 99.7 roll so you really wouldn't even have to go the oh okay so 0.68 is very much an approximation it's not exactly true uh but it's kind of close it's i think it's true if you round to two decimal places so um well i'm actually not really sure uh but yeah you do have this um and this is basically forcing us to convert to the standard normal case um the unfortunate thing though is that now i could just do this you could do p norm um uh 115 uh mean equals 100 and the other parameter is sd is equal to 15 minus p norm uh 85 mean equals 100 sd equals 15. and we get 0.6826 or 2 7. uh if we wanted to uh we could instead have written p norm 1 minus p norm negative 1 and they get the same number uh using basically that alternate form because you might not be converting you might not be converting to a standard random variable but r i'm pretty sure is so all right there's competing that uh this so the sad thing is that you can just do that and now i can't force you to uh use a table and uh and convert uh and uh convert to a standard normal random variable that's unfortunate okay uh so the probability that q is greater than 90. you know what i can't force to do it but i can still do it because i still have a point to make right so this is going to equal the probability that q minus 100 over 15 is greater than 90 minus 100 over 15 which is equal to uh 1 minus the cdf of a standard normal curve a standard normal random variable at uh at a 90 minus 100 over 15 this part becomes uh 10 over 15 or negative 10 over 15 which is negative twothirds so i would say uh this is going to be fee at i'll even round it i'll se i'll even convert it to a decimal number so negative zero 0.67 which is approximate but this is supposed to be negative twothirds and then i go to r and compute this and say this is one minus p norm uh negative two thirds which is point seven four seven five so this is approximately equal to 0.7475 all right next up the international society for philosophical inquiry requires potential members to have an iq of at least 135 in order to join the society this is one of those socalled genius societies based on this what proportion of the population is eligible for membership so this is the probability that an individual's iq is at least 135 which is equal to uh the probability that q minus 100 over 15 is greater than or equal to 135 uh hold on uh let's move this so this is equal to probability that q minus 100 over 15. is greater than or equal to 135 minus 100 over 15 which is equal to uh the probability that a well okay this is going to be uh one minus the cdf of a normal variable or one minus fee at so 135 minus 100 over 15 is 35 over 15 uh which is seven thirds which is about 2.33 okay so about 2.33 so 1 minus p norm 2.33 actually we'll just do seven divided by three so point zero zero nine eight so this is approximately point zero zero nine eight okay and here is some r code where i'm actually calling these mean functions sd function so mean sd parameters also using lower tail rather than doing one minus the cdf and so on all right uh so that was all trying to compute probabilities but sometimes we want to compute quantiles so we've got the notation z alpha which means that the cdf of at z alpha is equal to one minus alpha we can relate this back to general uh percentiles to find for arbitrary normal normally distributed random variables because these are the percentiles of um standard normal random variables okay so this is going to so we have in general a to p is going to be uh sigma z 1 minus p plus mu that's our percentile formula and z1 minus alpha can be found using table 8.3 using a reverse lookup but since you now have r there's really no point about discussing reverse lookups like this okay i mean i felt like they were good practice but we're now not going to be doing that anymore all right so uh what is z 0.5 that's the median of a standard normal z 0.5 is the area where the curve is split in two equal parts and that's going to be zero uh what is z 0.05 well uh what we could do is say all right let's go to p norm no no no no we're not using p norm anymore we're using cute arm so q norm uh 0.05 so uh when you could put in 0.99 no put put in 0.95 instead that would give us the same thing this is going to be 1 minus so this is 1 minus .05 or alternatively we could do q norm .05 lower dot tail equals false and that also works those all get us the same number so in the end though this is about 1.64 so this is so z 0.05 is about 1.64 okay what are the first and third quartiles of the standard normal distribution so what we're looking for is uh z so the first quartile will be z point seven five because remember we're doing up so the uh so here the uh subscript of the z is the upper tail area so this is the first quartile and the third quartile is 0.25 this is q3 all right so what are those going to be when we go to r and say q norm 0.75 dot tail equals false so we get negative zero point six seven so negative zero point six seven and this one well actually i'm not even going to bother computer because i know what it is it's 0.67 because we're working with a symmetric curve since we're working with a symmetric curve if we know one of those things that we know the other one because if the area underneath the curve to the left of z 7.75 is .25 the area underneath the curve to the right of point z 0.25 is also 0.25 so all we ever did was just flip over the flip over the y axis where x is equal to zero oh no let's not put a y there that's just confusing but my stupid undo button isn't working gosh why does stuff have to be so moody anyway um well since we're working with a symmetric distribution we actually have a property that i think gets written down uh later uh where did i write it down did i was i supposed to write it down up here where did i write it i know i wrote it down i know that i plan on talking about it at some point in these lecture notes oh i brought about on page 21 but basically i'll i'll i'll just cut to the chase and write it right now uh so z alpha is equal to negative z one minus alpha so basically uh you can just flip over the flip over the y uh the uh y axis so change the sign and work with one minus that area and you can get the same quantiles so but if it makes you feel better i'll go ahead and compute it and this is 0.25 yeah so what i did basically was exploit the symmetry of the standard normal distribution it's symmetry around zero our description of the random variable q from example 11. uh so using that answer the following questions mensa international requires individuals have an iq score that would place them in the top two percent of the population was the minimum iq score needed to be a member of mensa well that would be um so the standard deviation is 15. we've got z so top two percent so that's going to be 0.02 the upper tail area is 0.02 plus 100 and this is going to be let's see we've got 15 times q norm 0.02 lower dot tail equals false plus 100 130 or i guess he'd ran into 131 uh so we'll say 131. uh after rounding so 131 you need to have an iq of 131 in order to be a member of mensa uh there's an alternative way to do that though we could have instead we could have instead done q norm .02 mean equals 100 sd equals 15 lower dot tail equals false that would have also worked okay uh the part of the population with the lowest five percent of iq scores is considered to be intellectually disabled what is the highest iq score needed to be in this group okay so uh that means that we are looking at uh z 0.95 or actually we're asking for eta of 0.05 up here in this problem we were looking for eta of 0.98 okay so this is going to be 15z 0.95 plus 100 and then we go and compute that so in this case we got 0.95 now this is going to be 75.32 so we'll say about after rounding 75. so if you have a so if you have an iq score of 75 or lower you're considered intellectually disabled okay so right there's some r code that's doing the same thing so do the symmetry at the normal distribution we have the following useful identities for fee uh one second okay we have that the cdf at z is equal to one minus the cdf at negative z which what this is saying is uh if you were looking at so this is a standard normal distribution if you were looking at the area underneath the curve and to the left of z another way you could compute that quantity is look at the area underneath the curve and to the right of negative z which is what hap which is what you get when you flip over the uh y axis and then subtract that from 1 to get the to get the red area underneath the curve so the so the area above negative z um is going to be equal to the area below z or the area to the right of negative z is equal to the area to the left of z because of the symmetry of the curve and equivalently well as a consequence of this we have z alpha is equal to negative z 1 minus alpha that's our immediate consequence all right so as mentioned before fee can be used to approximate the cdf of other random variables so it turns out that the normal distribution can be used to approximate the uh cdf of other random variables or approximately other random variables basically which of course mattered more historically when we didn't have uh when we had to basically physically print out tables for random variables but you still want to be able to get probabilities for binomials with large parameters large n or uh poisson random variables with large mu but in this situation um like in the in the world in which we currently live that's less of an issue because software doesn't ask you how big n is it just works so um all right so well i guess it does literally ask you but uh it's not like you put in the wrong number and it will just not work um unless of course of course you put in something that's really big to the point that software can handle it but that's highly unlikely uh anyway uh still the fact that certain random variables can be approximated by normal random variables is not only important it's getting to fundamental theorems of statistics and probability uh that will be discussed in chapter five so let's say for example let's work it let's work with the binomial random variable when n the the sample size is large the cdf of a binomial random variable at x with parameters n and p and here we're assuming that n is somewhat large uh you should probably say well okay so a rule of thumb is that n times p is greater than or equal to 10 and n times 1 minus p is greater than equal to 10 that's one rule of thumb that they're using um we could probably say that if your p is between uh 0.1 and 0.9 then and a sample size of 40 is probably fine so this is going to be approximately equal to the cdf of a standard normal random variable evaluated at x minus the mean of the random variable which is np because that's the mean of a binomial divided by the standard deviation of a binomial which is n times p times 1 minus p and then in addition to this we do plus 0.5 the plus point five is what's known as a continuity correction uh it accounts for the fact that we are using a continuous random variable to uh approximate a discrete random variable if you didn't do this then you often end up with some numerical inaccuracy with the approximation the approximation is still at some level true but it's just off and you get better you get better approximate computations for say the cdf when you include this uh continuity correction i think the justification for why you add plus 0.5 is you could imagine that you have this uh probably mass histogram and your and your uh and the uh or cdf of the normal curve would be going through it basically at the left endpoints so you'd have something that's looking like this and if you shift everything over to the right yeah i think it's to the right when you add do plus or no it's to the left but when you shift the curve oops the undo button is not working when you shift the curve over a little bit you get like a better pass through of these uh histograms so it's like recentering so that it's centered evenly um on the uh on the uh probability histogram or this uh probably mass function understood as a histogram anyway uh let's let's work let's do an example a manufacturer will reject a batch of widgets if in a sample of 100 randomly selected widgets of the batch uh 15 or more are defective if 12 of the widgets in the batch are defective was the probability of rejecting the batch so the random variable in question is uh we'll call it s and it's following a binomial distribution uh the the parameter n is 100 and the parameter p for the probability of getting a defective widget is 0.12 okay so the approximating normal random variable follows a normal distribution with what is going to be in the mean well it's going to be the sample size times p so that's 12. and then we've got a standard deviation which is going to be the square root of a hundred times 0.12 times 0.88 okay so the standard deviation is about 3.25 so 3.25 this is the distribution of the approximating normal random variable so s's distribution is approximate so s is approximately equal in distribution to x so then when we want so okay reject the batch when is the batch rejected the batch is rejected when uh 15 or more widgets are defective so we're looking at the probability that s is greater than or equal to 15 and this is going to be uh 1 minus the cdf of this random variable at um uh hold on uh yeah at 14 uh and it's got parameters 100 and 0.2 and point 12. okay and this is according to our normal approximation approximately equal to uh one minus phi and we've got 14 minus 12 divided by 3.25 and then we add in the continuity correction 0.5 and what is this going to be equal to well we've got 1 minus p norm so we've got uh so 14 minus 12 plus 0.5 divided by 3.25 so 0.2209 and just for reference we could have alternatively computed p by nom and we would have used 14 size equals 100 prob equals 0.12 and we would have said lower dot tail equals false yeah so that's pretty close to what we uh got using the normal approximation okay all right so uh scrolling down oh did i oh something's different uh on the other hand is the r code wrong okay it looks like i might have made a mistake here so the probably that s is greater than or equal to 15 is probably that is one minus probably that s is less than or is strictly less than 15 which is one minus the probability that s is less than or equal to 14. so i think that my r code in these in this yeah i think that i did not put in yeah or hmm curious i don't know i i'm thinking actually that this might be wrong on a second look okay but you get the point at the very least and by the way i would suggest using the normal pro approximation at the very last step so right when you're about to compute something and that you don't know how to compute so like for example um i guess i should probably write down what steps i've kind of been emitting i can say this is the probability that s is no this is 1 minus the probability that s is less than 15 which is 1 minus the probability that s is less than or equal to 14 so here i um basically was still treating s as if it were a discrete random variable you should still do that uh you should like if i was treating this as a continuous random variable i would not be caring so much about whether i was working with less than or less than or equal to but you should you should still treat your random variable that you're approximating with a normal random variable as if it's discrete up until the final point when you need to compute something like the cdf okay so that's what i recommend all right uh the approximation works for poisson random variables as well when uh this lambda parameter is large or i think it was i don't know why i wrote lambda here i think that might be because the book's using lambda i'm not really sure why because of what i remember is that in chapter 3 i was using mu to write down poisson random variables okay but whatever um so in this case the approximating distribution for a poisson random variable uh let's suppose that um x follows a poisson distribution uh with mean parameter mu then we could approximate it with y which is following a normal distribution with mean mu and standard deviation square square root of mu because the standard deviation of a poisson random variable is square root is the square root of mu okay uh so suppose that x follows a poisson distribution with parameter 100 let's estimate the probability that x is less than or equal to 110 so uh the probability that x is less than or equal to 110 is approximately equal to fee at 110 minus 100 plus 0.05 that's the continuity correction divided by uh the square root of 100 which is 10. so this is going to be this is going to be the cdf at 10.5 divided by 10 which is 10.5 minus 10. i don't know divided by 10. oh yeah that's a 1.05 so that's going to be 0.8531 okay uh let's compare that to uh to what we would have had uh if we used the poisson distribution directly so 110 and uh lambda is equal to 100. oh very close very very close so a good approximation all right okay that's that concludes this section this is a very important section very very important because the normal distribution is a distribution that is appearing all over the place so and not just in this chapter but in later chapters too uh chapter five chapter seven chapter eight you're going to be using the normal distribution all the time it's going to be assumed that random variables are normally distributed so you need to get comfortable with this distribution all right so work on problems for this make sure you understand it if you don't understand it fix that and yeah it's it's your responsibility to fix it part of the way you fix it is by asking me what you don't understand right so um like part of how you fix not understanding something is getting help when you need it right from from whoever could possibly help you but you need to fix what you don't understand please please learn the normal distribution inside and out and you will be rewarded for it all right so that's it for uh this uh this section and i will see you in the next section when we talk about exponential random variables which we've already talked about quite a bit and also the gamma distribution which is an interesting distribution often shows up in applications and also in a more theoretical setting okay uh right so see you then
