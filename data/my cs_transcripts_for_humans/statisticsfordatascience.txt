With timestamps:

00:08 - welcome to our first video of our
00:10 - statistics series this video is going to
00:12 - really introduce the idea of statistics
00:15 - and answer the question
00:17 - what
00:18 - is
00:19 - statistics
00:26 - and as we start studying statistics
00:28 - there is some key vocabulary
00:30 - that we need to be very comfortable with
00:32 - in order to
00:34 - study statistics accurately and
00:37 - communicate our results accurately
00:44 - so the first word we need to know in
00:45 - statistics is what we call the
00:47 - population
00:52 - the population is basically everything
00:59 - or everyone we usually are talking about
01:01 - people but not necessarily
01:05 - being studied
01:13 - maybe we're investigating college
01:16 - students the population would be all
01:18 - college students
01:21 - number two the second word we need to
01:23 - know is what we call a parameter
01:28 - and a parameter is some characteristic
01:36 - of
01:36 - the
01:37 - population
01:45 - if we're talking about all college
01:47 - students the parameter might be the
01:48 - average gpa that be a parameter of the
01:52 - population when we're talking about
01:54 - parameters just as a note for now we
01:57 - will usually use greek letters
02:02 - to represent parameters you'll see
02:04 - things like the greek letter mu
02:06 - or the greek letter sigma something in
02:08 - greek generally means we're talking
02:10 - about a parameter of the entire
02:13 - population
02:15 - this is different than if i was talking
02:17 - about a sample
02:20 - a sample is a portion
02:25 - of the larger population
02:36 - so if i went out and said we're studying
02:38 - college students and their average gpa
02:40 - the sample might be i took a sample of
02:43 - 200 college students that would be my
02:46 - sample
02:48 - and then with a sample we have these
02:49 - things called statistics
02:54 - hence the name of our course a statistic
02:57 - is a characteristic
03:03 - of the
03:05 - sample
03:09 - so this might be the average gpa of the
03:13 - 200 college students i interviewed
03:17 - and to differentiate samples and
03:19 - populations statistics and parameters
03:23 - with statistics we will use english
03:25 - letters
03:32 - these english letters would be something
03:34 - like what we'll call
03:36 - x bar
03:37 - or just the letter s
03:46 - so as we're looking at statistics of
03:48 - samples which hopefully will estimate
03:51 - the parameter of a population we're also
03:54 - interested in what are called
03:58 - variables and these are not the
04:01 - variables of algebra a variable is any
04:04 - characteristic
04:10 - of interest
04:14 - gathered
04:19 - from each item
04:24 - in the sample
04:32 - another way to think about the variable
04:34 - is it's really when i'm conducting a
04:36 - survey it's really the question
04:41 - that is being asked
04:43 - so for example with our college students
04:46 - we'd be asking them what is your gpa our
04:48 - variable is the gpa and then our last
04:52 - word for this part will be what we'll
04:55 - call the data
04:58 - or the actual
05:01 - values
05:05 - of the variables
05:08 - so a data for our example would be
05:10 - something like 2.38 a 3.47
05:14 - a 4.0 that is the data
05:18 - so let's see if we can use this key
05:20 - vocabulary in an example
05:26 - see if we can identify
05:30 - the key vocabulary
05:44 - you want to know
05:50 - the average cost
05:57 - of statistics textbooks
06:13 - so you survey
06:18 - 25 textbooks
06:26 - and we're going to find and identify
06:28 - these key terms these same six key terms
06:31 - the population
06:36 - the parameter
06:41 - the sample
06:45 - the statistic
06:52 - the variable
06:57 - and the data
07:02 - first the population the population is
07:04 - everything that we're possibly
07:06 - interested in so we're talking about
07:08 - statistics textbooks the population
07:10 - would be all
07:12 - statistics
07:16 - textbooks
07:22 - and the parameter that we're interested
07:23 - in studying about those statistics
07:25 - textbook is the average cost
07:28 - the average cost
07:31 - but not just the average cost but the
07:33 - average cost
07:34 - of
07:35 - all
07:36 - statistics
07:41 - textbooks
07:46 - notice how i tie it back to the
07:47 - population because that's what the
07:49 - parameter describes
07:52 - that can be contrasted with our sample
07:55 - slightly different now our sample is the
07:57 - smaller group the subset we're looking
07:59 - at this is the 25
08:02 - textbooks
08:07 - and then the statistic has to describe
08:10 - that sample it's our it's our
08:13 - characteristic of interest for the
08:15 - sample it is the average cost
08:21 - of
08:22 - and then tie it back to the sample the
08:24 - 25 textbooks
08:34 - now the variable that's the information
08:36 - i'm gathering the variables when i look
08:38 - at each textbook what am i recording or
08:40 - the question what am i asking with the
08:42 - variable i'm asking the cost
08:46 - of a
08:48 - statistics
08:52 - textbook
08:56 - and the data is the answer to that
08:58 - question or the actual values of the
09:01 - variables so it's the actual cost
09:09 - of
09:10 - the textbooks
09:14 - an example might be you find an
09:16 - expensive one for 235 dollars that is a
09:19 - piece of data to the variable and answer
09:22 - to
09:23 - the question
09:28 - all right now that we have some
09:29 - vocabulary let's move on to an example
09:32 - of actually summarizing data which is
09:34 - what statistics is all about
09:38 - with statistics we're often interested
09:41 - in what is called the frequency
09:45 - of an event or thing
09:47 - and the frequency is just how often a
09:50 - value occurs
10:06 - and often we'll organize these
10:08 - frequencies in what we call a
10:11 - frequency
10:13 - table
10:18 - and to set up a frequency table we have
10:20 - a little bit more vocabulary
10:27 - first is what we're going to call the
10:30 - relative
10:33 - frequency
10:37 - and the relative frequency is just the
10:40 - proportion
10:45 - of times
10:48 - a value
10:51 - occurs
10:56 - in other words it's the decimal
10:59 - equivalent of the frequency
11:02 - divided by the total
11:08 - and often with frequency we're
11:09 - interested in what's called the
11:11 - cumulative
11:15 - relative
11:19 - frequency
11:24 - and that is the sum
11:27 - of all
11:29 - previous
11:32 - entries
11:39 - so with that vocabulary the way we're
11:41 - going to set up the actual frequency
11:43 - table is frequency tables we'll
11:45 - generally have four columns
11:48 - one column for the data value
11:53 - a column for the frequency which i'll
11:56 - denote with just f
11:58 - a column for the relative frequency
12:01 - which i can denote with rf
12:04 - and then a column for the cumulative
12:06 - relative frequency
12:08 - crf and so then we fill in our data
12:11 - values maybe one two and so on
12:16 - actually let's just do one and two
12:19 - and then we'll do the frequency maybe
12:20 - the first the one appears three times
12:22 - and the two appears seven times so out
12:25 - of 10 the relative frequency is 3 out of
12:27 - 10 or 0.3
12:30 - and for 2 the relative frequency is 7
12:32 - out of 10 or 0.7
12:35 - and the cumulative relative frequency
12:37 - will start to add all the previous
12:38 - values so 0.3 plus 0.7 is 1.0
12:43 - for the last column
12:46 - interesting to note is the last entry
12:53 - should sum
12:55 - to
12:56 - 1.
12:58 - now maybe if you have a round off error
12:59 - it might be 1.00001
13:02 - or 0.99999999
13:04 - that's okay but generally speaking we
13:06 - hope that cumulative relative frequency
13:09 - should sum to
13:11 - 1.
13:13 - that's all to show us how to set up the
13:15 - table let's actually make a frequency
13:18 - table
13:20 - let's do an example here
13:28 - let's say a baker
13:34 - keeps track
13:40 - of how many
13:42 - [Music]
13:44 - free donut holes
13:55 - his customers eat
14:04 - 25
14:06 - eat one donut hole
14:09 - 15
14:11 - eat two donut holes
14:14 - seven
14:16 - eat three donut holes
14:20 - and three
14:22 - eat four donut holes
14:28 - let's make a frequency table
14:32 - we have the values
14:36 - of one
14:38 - two
14:39 - three
14:40 - and four
14:42 - are the number of donut holes that were
14:43 - eaten
14:45 - for the frequencies
14:47 - we know 25 eat one donut hole so that's
14:51 - our frequency
14:52 - 15 is the frequency for two donut holes
14:55 - seven for three
14:57 - and 3
14:59 - eat 4.
15:02 - now if we want the relative frequency
15:05 - what we have to do is divide the
15:06 - frequency by the total so we need to
15:08 - know what the total is and so if we add
15:10 - these up
15:12 - we get a total of 50 customers
15:16 - so when we divide
15:18 - 25 by 50 we get a relative frequency of
15:22 - 0.5
15:25 - when we do 15 divided by 50 we get a
15:28 - relative frequency of 0.3
15:31 - 7 divided by 50 we get a relative
15:34 - frequency of 0.1 4
15:37 - and 3 divided by 50 we get a relative
15:39 - frequency of 0.06
15:44 - and the fractions aren't needed so much
15:46 - as the actual decimal answers in our
15:48 - table
15:50 - and then once we have our relative
15:51 - frequency we can find the cumulative
15:53 - relative frequency by adding all the
15:56 - values before that
15:58 - so for 1 we only have the 0.5
16:01 - but then for 2 we're going to add 0.3
16:06 - so we add point three to get
16:10 - point eight
16:12 - and then we add the next value add point
16:14 - fourteen to get point ninety-six
16:18 - and then we add .06 to get 1.00
16:26 - and that fills in our
16:28 - frequency
16:30 - table
16:33 - now that we have our frequency table we
16:35 - can answer some questions about this
16:38 - baker
16:39 - we could answer questions like what
16:41 - percent
16:44 - 8
16:45 - between
16:48 - 2 and three donuts
16:52 - two and three donuts have a relative
16:53 - frequency of 0.3
16:55 - and .4
16:58 - so when we add those together
17:02 - 0.3 plus 0.14
17:05 - the percent is 0.44 or as a percent 44
17:10 - percent
17:16 - how about what percent
17:23 - eight
17:24 - more than
17:30 - three well i'll highlight in pink more
17:33 - than three just means four so that must
17:36 - be this last entry of .06 and so we can
17:40 - say
17:41 - .06 or
17:44 - six
17:45 - percent
17:50 - finally what percent
17:56 - ate
17:58 - at most three
18:06 - well i'll mark them in blue here at most
18:08 - three as everybody else
18:15 - i could add those all together but what
18:17 - you might notice is that everybody else
18:20 - excludes the six percent so it might be
18:22 - easier to say we've got a hundred
18:24 - percent as everybody
18:26 - exclude the six percent that are more
18:29 - than three
18:31 - and that leaves us with 94 percent
18:35 - eight at most three donut holes
18:42 - so the big thing we're doing today is we
18:44 - are looking at statistics vocabulary and
18:47 - organizing data in frequency tables and
18:50 - interpreting that information
18:52 - take a look at the homework assignment
18:53 - that goes with this section and in class
18:56 - we will investigate these frequency
18:58 - tables a little further
19:06 - statistics is all about the
19:08 - interpretation of data so we're going to
19:10 - take a look today at the question
19:15 - how
19:17 - is statistical
19:23 - data
19:25 - collected
19:31 - and first we have to really understand
19:32 - what we mean when we talk about data
19:35 - data can be measured on several
19:37 - different levels so let's take a look at
19:40 - the levels of
19:42 - measurement
19:48 - levels of measurement start with the
19:49 - most simple type of data and grow to the
19:52 - most complex type of data it's important
19:54 - we know what we're working with so we
19:56 - make sure we can do the correct
19:58 - mathematical operations of it and make
20:00 - sense of our conclusions
20:03 - the most foundational level of
20:04 - measurement is what is called nominal
20:08 - which basically just means we put things
20:10 - into categories
20:17 - you might say there are no numbers
20:22 - so for some examples of nominal
20:25 - categories you could look at color
20:28 - or maybe a yes no survey
20:30 - do you support this political issue or
20:33 - maybe some type of label
20:38 - or gender
20:41 - those are nominal categories now
20:44 - slightly more involved than a nominal
20:46 - category is something that we can
20:48 - actually put in order and say this one
20:50 - is more than that one which is more than
20:52 - that one we call this ordinal data
20:59 - and that's data that can be put in order
21:08 - however with that order there is no
21:10 - clear
21:12 - space
21:14 - between
21:15 - the data values
21:17 - in other words we can say a comes before
21:19 - b but we don't really care how much
21:21 - before b the space between a and b might
21:24 - be different than the space between b
21:26 - and c
21:28 - an example of ordinal data might be
21:31 - finishing place in a race
21:38 - there's a clear first second and third
21:39 - but the space between first second and
21:41 - third is not necessarily well defined
21:44 - that's ordinal or we might say the top
21:47 - five cooks in america
21:50 - cook one two three four and five that's
21:53 - ordinal we can put them in order but we
21:54 - don't necessarily know how much better
21:57 - each one is than the other
22:01 - now when we do clearly define the space
22:03 - between values we have what we call
22:05 - interval data
22:09 - interval data has a space between the
22:12 - numbers with meaning
22:22 - the space has meaning
22:27 - however there still is no
22:30 - zero
22:31 - point
22:34 - the space between uh the two numbers is
22:37 - well defined like in the example of
22:38 - temperature
22:48 - 30 degrees is 10 more than 20 degrees
22:51 - just like 90 degrees is 10 more than 80
22:54 - degrees that space has meaning and while
22:57 - temperature does have a zero zero
22:59 - degrees doesn't mean the absent of
23:01 - temperature or no temperature it's just
23:03 - a point along the number line that has
23:05 - the same spacing as all the other
23:07 - temperatures
23:09 - now if we want to have a clearly defined
23:11 - zero that's when we have ratio data
23:15 - and that's where we have an absolute
23:19 - zero
23:21 - that means
23:23 - literally nothing
23:26 - or none
23:29 - an example of ratio data might be the
23:32 - length of a phone call or a test score
23:38 - or the number of children
23:46 - if we said we got a zero on a test or we
23:48 - have zero children that actually means
23:50 - nothing the lowest
23:52 - amount possible
23:55 - now these different levels of
23:57 - measurement can show up in different
23:59 - types of data let's scroll up
24:11 - it's very important we understand the
24:14 - two different types of data and the
24:16 - second one can actually be broken up
24:17 - into two subgroups the first type of
24:20 - data is what is called qualitative
24:26 - qualitative data which really focuses
24:28 - again on
24:30 - categories no numbers we're describing
24:33 - the qualities of something
24:37 - so
24:38 - for example
24:40 - we might be talking about the type of
24:42 - car
24:45 - or maybe some type of ethnic
24:49 - group
24:51 - those are the qualities of the data
24:53 - that's qualitative data
24:56 - the second type's the one we work with
24:58 - the most in statistics and that is what
25:01 - is called quantitative
25:06 - data
25:07 - and quantitative data is when we're
25:09 - looking at quantities
25:11 - quantitative data is either measured
25:18 - or counted
25:22 - really what we're talking about is
25:24 - numbers
25:26 - how many how much how far how long
25:34 - some examples of these
25:37 - might be the number of students
25:44 - or the distance to school
25:51 - quantitative data can even be divided up
25:54 - further quantitative data can be either
25:57 - discrete
26:01 - or leave a space continuous
26:08 - discrete data is data that is countable
26:14 - one two three four five when we count
26:17 - things we say we have discrete data
26:20 - generally we don't get decimals with
26:22 - discrete data examples might include
26:26 - the number of shoes
26:31 - a person owns
26:34 - you're not going to end up with decimal
26:35 - shoes you're not going to have 1.4 shoes
26:38 - discrete date is countable you count the
26:40 - number of shoes there are
26:43 - that's contrasted with continuous data
26:45 - which is measured
26:48 - which means every decimal and every
26:50 - decimal in between those decimals
26:53 - is possible
26:56 - measured data
26:57 - example might be the length of a phone
27:00 - call
27:07 - or maybe somebody's height
27:11 - that's continuous data it's measured
27:14 - always we can have decimals you can have
27:16 - half an inch you can have half of a half
27:18 - of an inch or a quarter you can have an
27:19 - eighth of an inch you can do all the
27:20 - decimals in between that is continuous
27:24 - data
27:27 - so that's the different levels of
27:29 - measurement we can do with different
27:31 - types of data but we haven't answered
27:33 - the question of how do we collect our
27:35 - data which is what we said we wanted to
27:37 - do at the beginning so let's take a look
27:40 - at actual
27:41 - sampling
27:45 - of
27:46 - data how do we collect it when we
27:48 - collect data in a sample we want it to
27:50 - be representative of the entire
27:52 - population
27:53 - in order for it to be representative of
27:55 - the entire op population we need the
27:58 - data to be
28:00 - random
28:02 - to avoid
28:05 - any bias
28:09 - in other words we want all options
28:14 - to be equally
28:16 - likely
28:20 - to be included in our sample
28:26 - so if random is best let's take a look
28:28 - at a few random sampling methods
28:32 - the first random sampling method is what
28:34 - we call
28:35 - simple
28:38 - random
28:41 - sampling
28:47 - simple random sampling is random
28:50 - selection
28:56 - methods
29:00 - such
29:02 - as
29:04 - random numbers
29:11 - or drawing out of a hat
29:26 - is the idea of i assign everybody a
29:28 - number and i pick a bunch of random
29:30 - numbers and those people are included in
29:32 - my survey
29:34 - an example is if i want to pick students
29:36 - we could assign students a number
29:47 - and pick
29:49 - random
29:52 - numbers to be included in the study
30:01 - a second type of random sampling
30:04 - is what is called stratified
30:09 - random sampling
30:16 - and this is when we divide
30:20 - the population
30:25 - into groups
30:29 - each group is called a strata
30:33 - and
30:34 - then select
30:38 - a proportionate
30:45 - number
30:50 - of each group
30:57 - polling is often done this way the idea
31:00 - is if we randomly survey 100 people or
31:04 - 200 people
31:06 - we don't want those 200 people to be all
31:08 - of the same political party otherwise we
31:10 - would get a biased sample so we
31:12 - guarantee it by saying if my state
31:17 - is 40 percent democrat
31:21 - 35 percent republican
31:26 - and 25
31:28 - independent
31:32 - then
31:36 - using a
31:38 - random
31:40 - method
31:43 - we will select
31:47 - i said we wanted 200 people so maybe we
31:49 - double the percentages
31:51 - maybe we'd select 80 democrats
31:54 - 70 republicans
31:57 - and 50
31:58 - independents to be included in my sample
32:02 - i have a proportionate representative of
32:04 - each group that matches the proportions
32:06 - of the state and so my sample still is
32:10 - random but i don't get the biased of
32:12 - only interviewing one party
32:18 - another type of random sampling also
32:20 - involves groups but it's called cluster
32:26 - random sampling
32:31 - again we're going to divide into groups
32:46 - but this time instead of selecting a
32:47 - proportionate number of each group we
32:50 - are going to
32:52 - randomly
32:55 - select
32:58 - entire
33:01 - groups
33:04 - so everybody in some groups are included
33:06 - and everybody in other groups are
33:08 - excluded
33:10 - an example of doing this
33:12 - might be a football stadium
33:22 - and it has different sections in the
33:24 - football stadium so we're instead of
33:25 - interviewing everybody in the stadium or
33:27 - random people through the stadium it's
33:29 - easier just to hit one section at a time
33:32 - so in the football stadium sections
33:36 - e and g
33:39 - are randomly selected
33:47 - and all fans
33:51 - from those
33:54 - sections
33:57 - are included
34:03 - cluster takes all people in each
34:06 - randomly chosen section while stratified
34:08 - takes a proportionate group out of each
34:11 - group
34:17 - the last random method we're going to
34:19 - use is called systematic
34:25 - random sampling
34:32 - the idea behind systematic random
34:34 - sampling is we start
34:39 - with a random
34:43 - item
34:45 - or person
34:51 - and choose
34:55 - every nth
34:57 - person
35:00 - after this
35:04 - that means like every fifth person or
35:06 - every 12th person or every 30th person
35:09 - an example of this might be
35:13 - if i pick a random phone book person
35:16 - phone number
35:19 - a random number in the phone book
35:28 - and then i choose
35:32 - every 50th person
35:40 - after this
35:44 - until i circle back
35:48 - to the beginning
35:53 - and go all the way through and back to
35:55 - the beginning and back to where i was
35:57 - taking every 50th person
36:02 - those are our four random sampling
36:04 - methods that you should be able to
36:05 - identify for this course there is a
36:07 - non-random method that is used quite a
36:10 - bit and so we should at least
36:11 - acknowledge
36:15 - the
36:16 - non-random method
36:21 - called convenience sampling
36:30 - which basically says
36:32 - use
36:34 - the results
36:39 - that are readily
36:43 - available
36:51 - as an example
36:54 - i would say i need to collect 50 data
36:56 - values for a survey so i'm just going to
36:58 - interview people nearby me
37:05 - or maybe people within driving distance
37:13 - maybe the friends on my facebook friend
37:15 - list something convenient and easy to
37:17 - get a hold of it's not really random
37:20 - and the problem with not being random is
37:22 - there are several drawbacks
37:26 - to not being random
37:29 - i could have bias results
37:33 - if i'm interviewing phone preference
37:35 - outside of the apple store i'm going to
37:38 - get more iphones
37:40 - that convenience sampling is going to
37:42 - work against me
37:44 - it may not be representative
37:54 - of the population
38:03 - if i only interview outside of a school
38:07 - because it's convenient near my house
38:09 - i'm going to get a lot more parents of
38:11 - young children than i will the general
38:13 - population
38:17 - and then the drawback of that is the
38:20 - results
38:24 - may not
38:26 - be useful
38:30 - outside of the sample
38:37 - i could conclude that
38:39 - such a percent of people prefer a
38:41 - certain type of music but when i'm with
38:43 - a different sample or a different
38:44 - population those results may not be
38:47 - useful so those are some drawbacks to
38:49 - convenience sampling we got to watch out
38:51 - for convenience sampling though it is
38:53 - used more often than it should be
38:55 - take a look at the homework assignment
38:57 - that practices with random sampling and
39:00 - also
39:01 - some of these different levels of
39:02 - measurement and types of data in class
39:04 - we're going to do more work with random
39:06 - sampling get really comfortable with the
39:08 - different types and i'll look forward to
39:10 - seeing you then
39:16 - now that we know the basic vocabulary of
39:19 - statistics and know how to collect data
39:21 - we're actually ready to start displaying
39:23 - some data our question for today is how
39:28 - do we
39:30 - display
39:33 - data
39:35 - visually
39:40 - and we're going to look at two main ways
39:42 - to display data the first way is going
39:44 - to be with what's called a histogram
39:49 - a histogram shows us frequencies
39:55 - over intervals
39:59 - and a histogram can really give us an
40:00 - idea of the shape of
40:03 - our data let's do a quick example here
40:07 - it's going to be easier to talk about a
40:08 - histogram with an example of a histogram
40:11 - so here's
40:14 - a scale two three four
40:17 - up the vertical axis the y-axis is
40:20 - always labeled frequency
40:25 - and then the x-axis is going to be some
40:28 - type of label of what we're actually
40:30 - looking at maybe we're looking at the
40:32 - number of tvs in a home
40:34 - and i'm going to actually start at
40:35 - negative 0.5
40:37 - i'm going to come over here to 1.5
40:40 - 3.5
40:41 - [Music]
40:43 - 5.5
40:44 - [Music]
40:45 - 7.5
40:47 - 9.5
40:49 - and we'll end at 11.5
40:54 - first box we're going to make one tall
40:57 - the next one we're going to go up to
40:59 - three
41:01 - the next one we're going to start at 2
41:05 - the next bar will start at one
41:08 - then we'll skip a space and put a bar at
41:11 - one
41:12 - i'm also going to give this a title
41:14 - let's title this the number of tvs
41:20 - and this picture is an example of what
41:23 - you would expect a histogram to look
41:26 - like
41:27 - a couple things that i want to note the
41:30 - bars on a histogram touch
41:35 - they come down
41:36 - [Music]
41:39 - on the interval numbers
41:46 - not in between but right on those
41:48 - numbers i labeled
41:50 - and they show
41:52 - frequency
41:56 - in a range
41:59 - in other words if i color the second bar
42:02 - green here what that second bar means is
42:04 - that there are three values or three
42:07 - people who reported having between 1.5
42:10 - and 3.5 tvs
42:14 - between 1.5 and 3.5 are the numbers 2
42:17 - and 3. so the numbers 2 and 3 all went
42:20 - into that bar
42:22 - now it's impossible to have 1.5 or 3.5
42:25 - tvs and that's what brings up the second
42:28 - point here point b if possible
42:32 - and it's usually easiest with discrete
42:34 - data
42:40 - never
42:43 - have
42:44 - a bar
42:47 - come down
42:51 - on a
42:52 - value
42:54 - in other words if i'm talking about the
42:56 - number of tvs i don't want this to come
42:58 - down on four tvs if a bar came down on
43:02 - four tvs
43:04 - i wouldn't know if that four goes in the
43:07 - left bar or the right bar
43:10 - so instead i make sure my bars come down
43:12 - staggered from actual data values in
43:14 - this case by .5
43:18 - and you also notice i took care to give
43:21 - a title
43:23 - to my graph
43:25 - and labels for the x and y axes
43:30 - title and labels are important
43:34 - so now that we kind of know what a
43:36 - histogram is and what it looks like
43:37 - showing frequencies over a range let's
43:40 - look at how to make
43:43 - a histogram
43:46 - first we need to decide
43:50 - on the number
43:53 - of bars
43:57 - once we decide the number of bars we'll
43:59 - use this nice little formula where we
44:01 - take the high number
44:04 - minus the low number
44:07 - and then divide by the number of bars
44:12 - and then whatever that number is we will
44:14 - round up
44:19 - and that will give us the bar
44:23 - width
44:24 - how wide each bar
44:26 - should be
44:29 - now it's important to note we always
44:30 - round up if it's 3.1 we'll still round
44:33 - up and the bars will be four
44:36 - in fact if it's three point zero we
44:39 - would still round up to have four bar
44:42 - bars of width four we always round up to
44:45 - the next number after this division
44:49 - next we need to decide
44:53 - on the starting value
44:58 - what i usually recommend we do to decide
45:00 - on the starting value is do 0.5
45:05 - before the lowest
45:08 - number
45:10 - in my tvs example 0 is the lowest number
45:13 - of tvs so i went 0.5 before that so i'm
45:16 - staggered and the bars won't come down
45:18 - on that
45:22 - then it's helpful to make a frequency
45:24 - table
45:30 - like we saw in section one
45:34 - with ranges
45:38 - with the ranges that we found in parts a
45:40 - and b here
45:43 - and we can use that frequency table then
45:45 - to build
45:47 - the histogram
45:54 - so if that's the method let's see if we
45:56 - can go ahead and actually do that with
45:59 - an example
46:10 - the number of miles
46:18 - 20 students
46:23 - commute
46:25 - to work
46:31 - is below
46:34 - we are going to make
46:38 - a histogram
46:44 - with five bars
46:47 - to represent the data
46:56 - and our data here is going to be 4
46:59 - 6
47:00 - 6
47:01 - 7
47:03 - 11
47:04 - 13
47:05 - 18 18
47:07 - 18
47:09 - 21
47:11 - 24
47:12 - 26 27
47:15 - 35
47:16 - 36
47:18 - 36
47:19 - 42
47:21 - 43
47:23 - 45
47:25 - and 49
47:31 - so for our bar width
47:36 - we need to take the high minus the low
47:39 - divided by the number of bars
47:42 - so the high is 49 and the lowest four
47:45 - 49 minus four
47:47 - divided by the five bars
47:50 - is nine
47:53 - now it's exactly nine so i have to round
47:57 - that up
48:00 - to ten
48:02 - is my width
48:05 - always round up to the next whole number
48:08 - otherwise your last values won't be
48:09 - included in the last bar
48:15 - now we can decide on our start values or
48:18 - our ranges
48:21 - so for x values
48:23 - to start with our graph we're going to
48:25 - start half a unit before the low value
48:28 - of 4. so half before 4 is 3.5
48:32 - and we're going to go up 10 to 13.5
48:38 - the next range is going to start at that
48:39 - 13.5
48:41 - and go up 10
48:43 - to 23.5
48:47 - then we have
48:49 - 23.5 to 33.5
48:53 - and i'm running out of space so i'm
48:54 - going to scroll up a bit
48:58 - 33.5 to 43.5
49:02 - and
49:03 - 43.5
49:04 - to
49:05 - 53.5
49:12 - those are our ranges
49:14 - now we just need to find the frequency
49:16 - inside each range
49:19 - 3.5 to 13.5
49:22 - there's one two three four five six
49:24 - numbers
49:26 - 13.5 to 23.5
49:30 - we only have four numbers
49:34 - then you see 23.5 to 33.5
49:39 - there's three numbers
49:43 - 33.5 to 43.5
49:47 - there's five numbers
49:50 - and 43.5 to 53.5
49:54 - we see two numbers
50:00 - now that we have that frequency table
50:03 - we're ready to build our histogram
50:08 - starting at 3.5 my next tick mark is 10
50:11 - later
50:12 - at 13.5
50:15 - then 23.5
50:18 - 33.5
50:20 - 43.5
50:23 - and 53.5
50:31 - our frequencies go up to 6 1 2 3 4 5 6.
50:37 - the first bar is 6 tall
50:42 - the second bar touching it is four tall
50:47 - the next bar touching it is three tall
50:51 - the next bar touching it is five tall
50:56 - and the next bar touching it is too tall
51:03 - and now my bars show the frequency
51:05 - within each of those ranges
51:08 - now i still need a title
51:10 - we're talking about miles
51:13 - driven to work
51:17 - it's a good title
51:19 - our x-axis is in miles
51:22 - and then the y-axis
51:25 - going from one to six always shows my
51:27 - frequency
51:32 - and there's my histogram
51:38 - now with histograms it's important not
51:40 - just to be able to draw the histogram
51:43 - but we also need to be able to describe
51:45 - the shape of the histogram
51:47 - that we end up with
51:50 - so a couple notes here on
51:52 - some vocabulary you can use to describe
51:57 - the shape
51:59 - of a histogram
52:02 - and now with real world data it's never
52:04 - perfect histograms are never perfectly
52:06 - any of these shapes but they tend to be
52:08 - close to one of these shapes not always
52:11 - but
52:12 - generally
52:13 - a common shape we'll see is what's
52:14 - called the uniform shape
52:18 - where the bars are about the same
52:25 - so just a quick sketch here you might
52:28 - see bars that
52:29 - they're not quite exactly the same but
52:32 - they're pretty darn close
52:35 - we would say that histogram is uniform
52:38 - they're all about the same height on all
52:40 - the ranges
52:44 - another word we should know is what's
52:45 - called a normal
52:47 - shape
52:49 - the normal shape is taller in the middle
52:55 - and shorter on the edges
53:02 - what a normal shape looks like is it
53:05 - generally starts short and gets taller
53:07 - until the middle
53:09 - and then after the middle it starts
53:10 - getting shorter again
53:14 - it's also called the bell shaped curve
53:16 - where it goes up and then back down
53:20 - the opposite of the normal shape is what
53:22 - we would call the v-shape
53:26 - and that is shorter in the middle
53:33 - and then taller on the edges
53:43 - so that's when we have a tall edge that
53:45 - gets shorter and then it comes back up
53:48 - and gets taller on the outside and you
53:50 - can almost see that v-shape
53:52 - right on top
53:55 - of those bars
54:00 - in addition to the shape we can talk
54:02 - about its symmetry we say a graph is
54:05 - symmetrical
54:09 - if it's basically the same
54:11 - on both sides
54:20 - so if we go tall and then shorter
54:22 - and then shorter it's going to be the
54:24 - same thing on both sides you notice
54:26 - that's the same as the v shape it's also
54:28 - symmetrical the same on both sides and
54:32 - we can combine these different
54:33 - descriptive words together to come up
54:35 - with a detailed description of a
54:38 - histogram
54:40 - then we have this idea of skewedness
54:43 - skewed means it's not symmetrical and we
54:46 - describe the unsymmetrical part where
54:49 - the extra stuff is skewed right
54:53 - means it's not symmetrical because we
54:55 - have extra stuff for lack of a better
54:58 - term
55:00 - on the right
55:06 - that's where we're trying to be
55:07 - symmetrical right
55:09 - but then on the right side there's all
55:11 - this extra stuff and it kind of goes
55:13 - down a lot slower
55:16 - that extra stuff means it's skewed right
55:20 - and you might expect the last term then
55:23 - is skewed
55:25 - left
55:27 - where the extra
55:29 - stuff
55:31 - is on the left
55:36 - and that's where
55:38 - we've got all these extra little short
55:39 - bars
55:40 - on the left before it starts growing and
55:43 - giving us our what would be symmetrical
55:45 - shape
55:50 - so that's histograms
55:52 - we can draw them to show the shape of
55:54 - the data we can describe them as uniform
55:56 - normal v shapes skewed symmetrical
56:01 - really can help us visualize what our
56:03 - data set looks like
56:06 - a second thing we can do though to
56:08 - describe our data visually
56:10 - is to draw what's called a box plot
56:19 - and a box plot
56:22 - shows
56:24 - the spread of data
56:28 - with what is called the five
56:31 - number
56:33 - summary
56:39 - five number summary is made up of five
56:42 - pieces
56:44 - a b
56:45 - c
56:46 - d
56:46 - and e
56:48 - the two easiest to find are the minimum
56:53 - and maximum
57:02 - and then right in the middle of them is
57:04 - what is called we'll put it on c
57:06 - the median
57:10 - or the middle
57:15 - when the data
57:17 - is in
57:18 - order
57:22 - so the median cuts the middle of the
57:25 - data and then we've got the top half and
57:27 - the bottom half
57:30 - in the bottom half we're going to find
57:32 - what's called q1 or the first quartile
57:41 - which is the middle
57:45 - of the lower half
57:50 - and similarly q3
57:53 - is called the third
57:55 - quartile
57:58 - which is the middle
58:02 - of the upper half
58:06 - so the quartiles and the medium really
58:08 - divide it into quarters now there's a
58:11 - little caveat with the quartiles in the
58:13 - median we said they're the middle values
58:15 - but if there are two
58:20 - middle values
58:26 - what we'll do is we will add them
58:28 - together
58:29 - and divide
58:31 - by two
58:33 - giving us the middle of the middle
58:39 - once we find that five number summary
58:42 - we'll usually represent it visually
58:45 - with what's called the box plot
58:51 - the box plot splits the data
58:59 - into quarters
59:06 - and to do that we put q1
59:09 - and q3
59:10 - as the edge
59:13 - of the box
59:18 - and then we draw whiskers
59:23 - out
59:25 - to the min
59:26 - and max values
59:31 - and finally we use a dotted line
59:36 - for the median
59:40 - and so what we end up with is there's
59:41 - some number line down the bottom
59:45 - and then floating above that number line
59:48 - is the box showing where the quartiles
59:50 - are q1 and q3
59:54 - whiskers
59:55 - out to the minimum and maximum values
60:01 - and then a dotted line
60:04 - for the median
60:07 - and each part of the box represents a
60:11 - quarter or 25
60:14 - of the data values
60:17 - the box then is the middle 50
60:20 - the whiskers are the outside 50 percent
60:22 - and we can make some visual
60:25 - conclusions about how our data is spread
60:28 - out
60:30 - let's see if we can make a box plot
60:35 - going back to our example
60:40 - with the commute time
60:46 - that list of numbers again for us was 4
60:49 - 6 6
60:51 - 7
60:53 - 11
60:54 - 13 18 18
60:58 - 18
60:59 - 21
61:01 - 24
61:02 - 26
61:04 - 27
61:06 - 35
61:07 - 36
61:09 - 36
61:11 - 42
61:12 - 43
61:14 - 45
61:16 - and 49
61:19 - now if this data was not in order it
61:22 - would be essential as a first step to
61:24 - put the data in order
61:26 - fortunately ours already is in order we
61:28 - know there are 20 data values
61:31 - so 20 cut in half is 10. we're going to
61:34 - have 10 below and 10 above so 1 two
61:37 - three four five six seven eight nine ten
61:40 - cutting in half
61:42 - ten below and ten above sticks us right
61:44 - between twenty one and twenty four
61:48 - because it's right in the middle we add
61:50 - those numbers together and divide by 2
61:53 - to get our median value
61:56 - which is 22.5
62:01 - then we can go after our quartiles
62:03 - our quartiles are in the middle of the
62:05 - bottom half and the top half
62:07 - well the bottom half has 10 values
62:10 - so we're going to split 5 and 5
62:13 - which sticks us right between 11 and 13.
62:17 - again because there's not one value in
62:19 - the middle we'll add those together and
62:20 - divide by 2.
62:22 - 11 plus 13 divided by 2 gives us our q1
62:26 - equal to
62:27 - 12.
62:31 - for the upper quartile again we're going
62:33 - to have five data values on each side
62:36 - sticks us right between 36 and 36
62:40 - when we add those together and divide by
62:43 - two
62:43 - the third quartile
62:46 - is 36.
62:50 - also include our minimum value of four
62:54 - and our maximum value of forty-nine
62:58 - and we're ready to make our box plot
63:01 - showing the spread of our data
63:05 - we should make every attempt to make
63:07 - this box plot 2 scale so if i count by
63:10 - 6's starting at 3 we'll have 3
63:13 - 9
63:14 - 15
63:16 - 21
63:18 - 27
63:20 - 33
63:22 - 39
63:24 - 45
63:25 - and 51 notice those are about the same
63:28 - size apart from each other
63:32 - my box is made from the quartiles at 12
63:37 - and 36
63:41 - connect our
63:42 - box
63:44 - the median is a dotted line at 22.5
63:48 - the whiskers go to the minimum of four
63:52 - and the maximum
63:53 - of 49
63:55 - and we have our box plot
63:59 - of course any graph needs a title so we
64:02 - can title this commute time
64:06 - and label the x-axis maybe time in
64:09 - minutes
64:15 - and we have our box plot
64:17 - now normally the box plot would all be
64:18 - one color but i did color coding to show
64:20 - where all the pieces came from in this
64:22 - example
64:25 - and just like we can describe the shape
64:28 - of our histograms
64:30 - we can also describe the shape
64:38 - of our box plots
64:41 - and there's basically three ideas here
64:43 - one is the idea of being spread out
64:48 - where we have a wide range
64:52 - of values
64:56 - this would be a big box plot
65:00 - covers a large range of values
65:05 - that means the data is not really close
65:07 - to each other not close to the median
65:08 - not
65:09 - not really close to anything it's all
65:11 - spread out
65:13 - the opposite of being spread out is to
65:15 - be clustered
65:19 - together
65:23 - where we have a small
65:25 - range
65:27 - of values
65:30 - and this is going to give us a really
65:31 - tiny box plot
65:34 - everything's really close to each other
65:40 - and often it's beneficial to split our
65:42 - description
65:44 - into
65:44 - the
65:45 - entire shape
65:54 - and
65:55 - the middle
65:57 - 50 percent
65:59 - or the box
66:04 - for example i could have a box plot that
66:07 - looks like this
66:13 - and we could say
66:16 - overall the data
66:20 - is spread out
66:26 - but the middle
66:30 - 50 percent
66:32 - is clustered
66:36 - together
66:39 - because the box is small compared to the
66:41 - rest of the data
66:44 - so that's what we're looking at today
66:46 - histograms and box plots practice making
66:49 - them practice interpreting them
66:51 - describing them take a look at some of
66:53 - them on the homework we'll work with
66:54 - these a little bit more in class and we
66:57 - will see you
66:58 - then
67:05 - in our previous section we took a look
67:07 - at how we could summarize data visually
67:09 - but quite often it's going to be useful
67:11 - to summarize data
67:13 - with numbers so that's going to be our
67:15 - question how do we
67:21 - summarize
67:25 - data
67:27 - numerically
67:33 - and there's two things that we'll try
67:35 - and summarize numerically the first one
67:37 - we're going to plant on for a minute
67:39 - here are the measures
67:43 - of
67:44 - center
67:45 - where is the middle of the data
67:48 - and depending on our context the measure
67:50 - of center will either be the mean
67:53 - the median
67:55 - or the mode
67:59 - let's start with the mean
68:01 - or what people typically call
68:03 - the average
68:07 - now if we're talking about a population
68:09 - mean
68:12 - we will always use a greek letter
68:14 - and that's going to be the greek letter
68:15 - mu
68:16 - but if we're talking about a sample mean
68:20 - we will use an english letter which we
68:23 - will notate with x bar
68:28 - and the formula
68:31 - for calculating the mean i'm going to
68:32 - use x bar but it works also for mu
68:36 - is equal to this symbol which we call
68:39 - sigma
68:41 - x
68:42 - over n
68:43 - and that symbol
68:46 - that funny looking thing means
68:49 - the sum
68:51 - what this means is sum up all the x's or
68:53 - sum up all the values and divide by n
68:57 - which is the
68:58 - sample size
69:02 - or population size if we're in the
69:04 - population context so for example
69:10 - if i had the numbers 1 3 3
69:14 - 4
69:15 - 4
69:16 - 4
69:17 - 5
69:17 - 5
69:19 - and i wanted to find the mean of this
69:23 - sample
69:25 - the numerator says sum up all the x
69:27 - values or do 1 plus 3 plus 3 plus 4 plus
69:31 - 4 plus 4 plus 5 plus 5
69:34 - and divide by the sample size and if i
69:37 - count here i see we've got a sample size
69:40 - of 8.
69:43 - so adding those all up we get 29 over 8
69:46 - which gives us a mean of
69:49 - 3.625 or the average is 3.625
69:55 - most students are familiar with that
69:57 - mean formula but one tweak we can do to
69:59 - it and we quite often have in statistics
70:03 - it's not two let's call this c because
70:05 - we're still under mean
70:08 - is if we have
70:12 - frequencies
70:17 - so we've already seen one formula for
70:19 - the mean
70:20 - the sum of the x's divided by the n
70:23 - but if we have frequencies given to us
70:25 - where we know how often each number is
70:28 - used the formula is going to tweak
70:30 - slightly x bar is going to be equal to
70:32 - the sum of the x's
70:34 - times the frequency
70:36 - divided by the sample size
70:39 - so if we have frequencies we should know
70:41 - this formula
70:44 - let's do the exact same example we just
70:46 - did but this time we're going to
70:48 - summarize those numbers
70:50 - by their frequencies
70:52 - so first i'll list out the numbers
70:56 - the numbers that showed up were 1
70:59 - 3
71:00 - 4
71:01 - and 5
71:02 - then i'm going to have another column
71:04 - that shows the frequency
71:06 - i had a single one there were two threes
71:09 - three fours
71:10 - and two fives
71:14 - what the sum says we need to do is first
71:17 - multiply the x's
71:20 - times the frequencies
71:23 - so when we multiply one times one is one
71:26 - three times two is six
71:28 - four times three is twelve and five
71:30 - times two is ten
71:33 - and then what we wanna do is sum this x
71:36 - times frequency table
71:39 - so 1 plus 6 plus 12 plus 10 is 29 that
71:44 - represents the sum of the x's times the
71:47 - frequencies
71:49 - and then we just have to divide by the
71:50 - sample size
71:52 - well the frequencies tell us how many
71:54 - things there are
71:55 - so my sample size is 1 plus 2 plus 3
71:58 - plus 2
72:00 - is equal to
72:02 - 8.
72:04 - so for my mean again we're doing 29
72:06 - divided by 8 to get the exact same
72:08 - number of 3.625
72:12 - but sometimes it's a lot quicker and
72:14 - easier
72:15 - if we have those
72:16 - frequencies given to us
72:21 - that's the first measure of center
72:23 - that measure of center of the average
72:25 - says if everybody was split up equally
72:27 - they'd have this many
72:29 - in common but quite often the problem
72:32 - with the mean is one large value or one
72:35 - small value can throw off the mean
72:37 - significantly
72:39 - which is why we might be interested more
72:42 - in the median
72:44 - or the middle number
72:48 - when they're first put in order
72:52 - this way one large or one small number
72:54 - won't have a dramatic impact on this
72:56 - measure of center
72:58 - so for example
73:04 - using our same data the one three three
73:09 - four four four
73:11 - five five the median is going to be the
73:14 - value right in the middle
73:16 - well if there's eight values the middle
73:18 - puts you between the four and the four
73:21 - and we know from our previous section
73:23 - when we made box plots we would add
73:25 - those and divide by two to get our
73:27 - median value
73:30 - which turns out here
73:33 - to be four
73:36 - and the big advantage of the median is
73:38 - that one extreme
73:42 - value
73:46 - does not
73:49 - impact
73:51 - the median
73:54 - as significantly
74:00 - as the mean
74:02 - it's a little more stable
74:07 - now the third measure of center
74:12 - and this is often used in categories or
74:15 - nominal data is what is called the mode
74:20 - the mode is the value that occurs the
74:22 - most
74:24 - often
74:25 - and again it's usually best
74:29 - for categories
74:33 - if we're talking about the color of cars
74:35 - in the parking lot we're not going to
74:36 - have an average of a blue point green
74:38 - car that doesn't make any sense
74:40 - but what we can do is say the most
74:41 - common frequent colored car is blue or
74:45 - gray or whatever that most frequent one
74:47 - is
74:49 - we can still look at mode in terms of
74:50 - numbers
74:52 - we seem to be using this example data
74:56 - of one three three
74:59 - four four four five so let's keep using
75:03 - it
75:04 - and what we see
75:06 - is the number four
75:08 - appears three times it is the one that
75:10 - occurs the most often so we will say
75:13 - that the mode is equal
75:16 - to
75:17 - four
75:22 - those are our measures of center the
75:24 - mean median and mode
75:27 - but the problem with just measuring the
75:29 - center
75:31 - is it only tells us where the middle
75:33 - value is it doesn't tell us kind of how
75:36 - all the rest of the data is behaving
75:39 - around the center
75:41 - is the data really spread out is it
75:43 - clustered close to the center what's
75:45 - happening with the rest of the data
75:48 - and that's why we also need some type of
75:50 - measure
75:53 - of spread
75:57 - it tells us more than just the middle it
75:59 - tells us how
76:01 - spread out
76:09 - the
76:11 - the other values
76:15 - are
76:17 - not just where the middle is but how's
76:19 - everybody else behaving around the
76:21 - middle and the reason this is important
76:24 - is we can look at data such as these
76:27 - three data sets i'm going to put up here
76:29 - and all of them
76:31 - all of the following
76:37 - have the same
76:40 - mean
76:42 - and median
76:47 - the first data set's going to be 1 1
76:50 - 1
76:51 - 5 9 9
76:53 - 9.
76:56 - both of that data set has a mean of 5 it
76:58 - also has a median of 5.
77:01 - but if i do another data set of 1
77:03 - 2 4
77:04 - 5
77:06 - 6
77:07 - 8
77:07 - 9
77:09 - that data set has a mean of 5 and a
77:12 - median of 5.
77:14 - and if i do a data set of five five five
77:18 - five five five five
77:21 - that data set has a mean of five and a
77:24 - median of five there's no difference
77:26 - between these three data sets if i just
77:28 - look at the center
77:30 - but the numbers are spread out quite
77:32 - differently the first data set the blue
77:34 - one are spread out very far
77:37 - the green ones kind of spread out evenly
77:39 - and the red one has absolutely no spread
77:42 - in it at all
77:44 - this is why we need a measure of spread
77:49 - and the most basic measure of spread is
77:51 - what we call
77:53 - the range
77:55 - the range tells us how much space there
77:57 - is between the largest and the smallest
77:59 - number
78:00 - we take the large number and subtract
78:03 - the small number to see how far apart
78:05 - those extreme
78:07 - values are
78:09 - so for example
78:12 - with our data we've been playing with
78:14 - today of one three three four four four
78:19 - five five
78:22 - the range would be the big
78:24 - minus the small
78:26 - 5 minus 1 equals 4.
78:29 - and so there is a space of 4 between
78:33 - all of these values
78:36 - now there's a problem with the range
78:37 - though
78:41 - the problem with the range is one
78:44 - extreme
78:47 - value
78:50 - could
78:51 - greatly
78:54 - impact
79:02 - for example if there was also a 27 on
79:05 - this data set 27-1 which we sound like
79:08 - 26 there's a large range between the
79:10 - numbers when most of them are actually
79:12 - clustered quite closely together
79:15 - so this is why we need a different
79:18 - better measure of center
79:21 - and one
79:22 - measure of center that might be better
79:25 - is what we call the inter
79:28 - quartile range
79:33 - it's often abbreviated as iqr for inter
79:37 - quartile range
79:40 - and the interquartile range you could
79:42 - think about as the range
79:46 - of the middle
79:48 - 50 percent
79:51 - what we'll do is we'll take the q3 and
79:53 - subtract the q1 value
79:58 - subtract the quartiles and we see the
80:00 - range of the middle 50 percent or how
80:01 - spread out the middle 50 percent is and
80:04 - then we're no longer going to be
80:06 - impacted by an extreme outlier that's
80:08 - way too big or way too small for the
80:10 - rest of the data
80:12 - so for example
80:15 - with our data of 1 3 3 4 4 4 5 5.
80:23 - we already said the median was between 4
80:26 - and 4.
80:28 - the first quartile then
80:30 - q1 is between three and three
80:34 - which is just three
80:36 - and the third quartile q3
80:39 - is between four and five which is four
80:42 - point five
80:45 - this means our interquartile range
80:48 - is 4.5 minus 3
80:52 - or 1.5
80:55 - and that might be a little bit of a
80:57 - better
80:58 - measure of the spread because it's
81:01 - only looking at the middle 50 percent
81:03 - the extreme outliers are not going to
81:06 - impact the interquartile range
81:09 - however we still have a problem
81:13 - and that problem is this interquartile
81:15 - range formula
81:17 - only considers
81:20 - two values
81:23 - the q1 and the q3
81:25 - it would be nice if we had some measure
81:28 - of spread
81:30 - that considered all the values and how
81:33 - spread out those are
81:37 - and this gives rise to the most
81:39 - important measure of spread the one
81:42 - we'll use a lot in this class
81:44 - called the standard
81:47 - deviation
81:51 - and the standard deviation
81:54 - attempts to measure what we call the
81:57 - average
81:58 - [Music]
81:59 - distance
82:02 - a point is
82:04 - from
82:06 - the mean
82:08 - how spread out is the data considering
82:12 - all the data values on average how far
82:14 - are they from the mean
82:16 - now with a population
82:20 - we will use a greek letter for the
82:21 - standard deviation and that's the greek
82:24 - letter sigma
82:26 - and with the sample
82:29 - we will use the english letter s
82:31 - to represent the sample
82:39 - the formula
82:43 - for the standard deviation
82:46 - kind of builds on this idea that we want
82:48 - the average distance from the mean
82:51 - so if i took any point
82:53 - and subtracted the mean that would give
82:56 - me the distance it is from the mean
83:00 - the problem is is some of these will be
83:01 - positive and some of these will be
83:03 - negative so if i add them up it actually
83:05 - adds up to 0.
83:07 - so to avoid the positive negative
83:08 - problem what we'll do is we'll square
83:11 - each of the values
83:13 - before we take the sum and add them all
83:15 - up
83:17 - then we'll divide by the sample size
83:20 - which
83:21 - turns out
83:23 - with standard deviation and when we
83:25 - derive the formula it's not exactly the
83:27 - sample size we divide by
83:29 - but we'll divide by n minus 1.
83:32 - and the reasons for the 1 are beyond the
83:33 - scope of this course so you just have to
83:35 - trust me to take an average
83:37 - distance from the mean with the standard
83:39 - deviation we're going to divide by n
83:40 - minus 1.
83:43 - the problem that we still have though is
83:45 - we squared everything so it's not really
83:47 - a true average so to undo the square
83:50 - we'll take the square root at the end of
83:52 - our formula
83:54 - and we will say s is equal to that
83:57 - square root
83:59 - and that is going to be an important
84:01 - formula for us
84:03 - in this course
84:06 - now i do have one little caveat
84:09 - turns out that the formula for a
84:11 - population standard deviation
84:14 - is
84:14 - slightly different
84:20 - than
84:22 - s
84:25 - the formula
84:28 - is we're not going to worry too much
84:31 - about that different formula for the
84:32 - population because generally we always
84:35 - have a sample we're always going to be
84:36 - taking sample standard deviations which
84:39 - is this formula we've looked at here
84:42 - so let's do an example
84:48 - and let's start with a smaller example
84:49 - and then we'll move to the bigger
84:51 - example that we've been seeing
84:52 - throughout this video we'll start with
84:54 - the example 11
84:56 - 13
84:57 - 14
84:58 - and 14.
85:00 - and i'll give you a hint
85:02 - if you go through and calculate the mean
85:03 - of these values the mean is going to be
85:06 - equal to 13.
85:12 - what we'll do to get started is we'll
85:14 - list our values
85:15 - 11
85:16 - 13
85:17 - 14
85:19 - 14.
85:21 - and then we're going to have a column
85:23 - for every step
85:25 - along the way
85:26 - in this formula
85:29 - the first step says take those x values
85:32 - and subtract the mean
85:34 - subtract 13.
85:36 - so 11 minus
85:39 - i can even put a little thirteen here
85:41 - we're subtracting thirteen eleven minus
85:42 - thirteen is negative two thirteen minus
85:45 - thirteen is zero fourteen minus thirteen
85:48 - is one and fourteen minus thirteen is
85:50 - one
85:53 - then the formula says to square r values
85:57 - so the x minus x bar each of those
85:59 - values needs to be squared
86:02 - negative 2 squared is 4 0 squared is 1
86:06 - 1 squared is 1
86:08 - and 1 squared is 1.
86:14 - finally the formula says to take the sum
86:18 - so the sum of x minus x bar squared
86:22 - is equal to 4 plus 1 plus 1
86:25 - which is 6.
86:29 - now i'll jump to the standard deviation
86:31 - formula which says the square root of
86:34 - the sum which is six
86:37 - divided by one less than the sample size
86:40 - which is three
86:41 - [Music]
86:43 - six divided by three is two and the
86:44 - square root is one point
86:46 - forty
86:47 - one
86:50 - [Music]
86:51 - now similar to our formula with means if
86:55 - the data is given to us
86:57 - with frequencies rather than individual
87:00 - data numbers
87:02 - we need to do a slight adjustment to our
87:06 - formula
87:09 - so
87:10 - if we
87:12 - have
87:13 - frequencies
87:18 - the formula will slightly adjust to s
87:21 - equals the square root of the sum of x
87:24 - minus x bar
87:26 - squared but before we take the sum we
87:29 - have to multiply
87:31 - by the frequency
87:33 - and then divide by n minus 1.
87:39 - so let's take a look at an example
87:42 - where we do it with frequencies
87:49 - and let's use that data set that we've
87:51 - been using where we know the x values
87:53 - were 1
87:54 - 3
87:55 - 4 and 5.
87:58 - and the frequencies of that were one
88:02 - two
88:03 - the number four appeared three times and
88:05 - the number five appeared twice
88:09 - now we already know
88:13 - the x bar the mean is 3.625
88:17 - [Music]
88:19 - we would have had to find that first if
88:20 - we didn't know that but since we do now
88:23 - we'll make another column for x minus x
88:25 - bar
88:28 - taking the x the number 1 minus 3.625
88:32 - is negative 2.625
88:36 - 3 minus 3.625 is negative 0.625
88:43 - 4 minus
88:44 - 3.625 is 0.375
88:49 - and 5 minus
88:51 - 3.625 is 1.375
88:59 - next the formula says we need to square
89:02 - that x minus x bar
89:05 - we're going to square each of these
89:07 - numbers so 2.625
89:09 - squared and i'm going to round to two
89:11 - decimal places
89:12 - is 6.89
89:16 - 0.625 squared is .39
89:20 - [Music]
89:24 - 0.375 squared is 0.14
89:29 - and
89:30 - 1.375 squared is 1.89
89:38 - now if we didn't have frequencies we
89:40 - would just add this column up
89:42 - but because we have frequencies we need
89:44 - to take this column the x minus x bar
89:46 - squared and multiply by those
89:49 - frequencies
89:52 - so the 6.89 needs to be multiplied by 1
89:54 - to get 6.89
89:58 - the point 39 needs to be multiplied by
90:00 - its frequency of 2 to get 0.78
90:04 - 0.14 times 3
90:06 - is .42
90:09 - and 1.89 times 2
90:12 - is 3.78
90:16 - and that's what we want to sum we want
90:18 - to get the sum of x minus x bar squared
90:22 - times the frequency
90:25 - and when we add up that column you
90:27 - should end up with 11.87
90:37 - now we plug into our formula for s
90:40 - s is the square root of that sum we just
90:42 - found is 11.87
90:46 - divided by one less than the sample size
90:49 - you could add the frequencies together
90:52 - to find out the sample size is eight or
90:54 - you might remember that because we've
90:55 - been working with this for quite a while
90:58 - one less than the sample size is seven
91:01 - so the square root of 11.87 divided by 7
91:05 - is 1.30
91:08 - so on average these points are about
91:11 - 1.30 units away from the mean of 3.625
91:16 - gives us an idea of the middle
91:18 - and how spread out the data actually is
91:26 - now the standard deviation is actually
91:29 - quite nice
91:31 - because it gives us a way to compare
91:33 - data
91:34 - based on how many
91:37 - standard
91:41 - deviations
91:46 - we are from the mean
91:49 - standard deviations measure
91:53 - distance
91:56 - from the
91:58 - mean
92:03 - and in statistics we will use a very
92:05 - important variable to represent the
92:08 - number of standard deviations we are
92:10 - from the mean that variable is always
92:13 - going to be z
92:15 - z is the number
92:18 - of standard
92:22 - deviations
92:26 - from
92:28 - the mean
92:32 - and we actually have two formulas that
92:34 - use z they're both really the same
92:36 - formula the idea is z is the number of
92:39 - standard deviations from the mean so
92:41 - we'll take the value we're working with
92:44 - we want to know how many standard
92:45 - deviations x is from the mean
92:48 - well first we need to know the distance
92:49 - to the mean
92:51 - so we'll subtract the mean or x bar and
92:54 - then we'll divide by s are the number of
92:56 - standard deviations that is
92:59 - that's our main formula
93:02 - for z
93:05 - now sometimes we have the opposite
93:06 - information and we want to know
93:08 - what is three standard deviations from
93:10 - the mean we know z we want to be three
93:12 - standard deviations from the mean what
93:15 - value is that and so we can solve this
93:17 - equation for x
93:20 - and when we do we get x is equal to the
93:23 - mean
93:25 - plus the number of standard deviations
93:28 - times
93:29 - the standard deviation
93:33 - and so this formula
93:35 - it's really the same formula it's just
93:37 - been solved for x
93:39 - will tell us the number of standard
93:41 - deviations
93:44 - is what value
93:48 - let's do some examples
93:51 - let's say for our data
93:54 - one three three four four four five five
94:01 - we wanna know how
94:05 - many
94:07 - standard
94:10 - deviations
94:14 - from the mean
94:18 - is the median
94:30 - well we've already found all these
94:31 - important values we found the median
94:34 - is equal to 4.
94:37 - also earlier we found the mean
94:41 - was equal to 3.625
94:45 - and also earlier we found the standard
94:47 - deviation
94:50 - is equal to 1.3
94:55 - so if we want to find out how many
94:57 - standard deviations the median is from
94:59 - the mean the medians are x
95:02 - the means the x bar
95:03 - and the standard deviation is s
95:06 - so z the number of standard deviations
95:09 - four is
95:10 - we subtract the mean of 3.625
95:14 - and divide by the standard deviation of
95:16 - 1.3
95:18 - and we find the median
95:21 - is 0.288
95:24 - standard deviations
95:27 - from the main
95:40 - or we could ask a similar question for
95:42 - the same data
95:49 - what
95:50 - value
95:53 - is 2
95:55 - standard
95:58 - deviations
96:02 - below
96:05 - the mean
96:11 - we already know the number of standard
96:12 - deviations
96:14 - so 2 that's actually going to be our z
96:17 - the number of standard deviations below
96:19 - the mean
96:21 - and because we want to be below the mean
96:24 - we will use a negative number to make it
96:27 - below the mean
96:30 - so we're looking for the x this time
96:31 - what is that value of interest
96:34 - so x is equal to the mean x bar of 3.625
96:40 - minus 2 because we have a negative 2 two
96:43 - standard deviations below the mean
96:46 - times the standard deviation
96:49 - of 1.3
96:51 - and that gives us 1.025
96:58 - now it turns out that we can say 95 of
97:02 - our data generally falls within two
97:06 - standard deviations of the mean
97:08 - if it's more than two standard
97:10 - deviations from the mean
97:12 - we say those values are unusual or
97:15 - extreme values
97:17 - we call those extreme values
97:21 - outliers
97:27 - and these outliers are values
97:30 - far removed
97:35 - from the rest of the data
97:45 - for example if i have the numbers 1 3 3
97:50 - 5 87
97:54 - 87 is far removed from the rest of those
97:56 - values it's an outlier and the outlier
97:58 - can either be large or small
98:02 - and we actually have two methods for
98:04 - calculating outliers and they generate
98:08 - very similar results so neither one is
98:10 - necessarily better than the other but i
98:12 - want to show you both of them
98:15 - the first one is called the
98:17 - interquartile
98:18 - range method
98:22 - and the idea behind this is we cannot be
98:26 - more
98:30 - 150
98:31 - or 1.5 times
98:38 - the iqr
98:42 - from the edge
98:45 - of the box in the box and whisker plot
98:51 - the idea is anything below
98:56 - the first quartile minus 1.5
98:59 - times the iqr
99:03 - or anything above
99:07 - the third quartile
99:09 - plus 1.5 times the iqr
99:14 - anything that's above or below those
99:16 - numbers
99:17 - becomes
99:18 - an outlier
99:26 - so for example
99:31 - let's say we've got the numbers 2 3 5 6
99:35 - 7 and 14.
99:42 - now the first chord the medians right in
99:44 - the middle
99:47 - but what we're interested in with the
99:48 - inner quartile range is the middle below
99:51 - the median
99:52 - the first quartile is three
99:56 - and the third quartile
99:58 - is seven
100:01 - so the interquartile range is seven
100:04 - minus three
100:05 - four four
100:10 - this means an outlier is anything below
100:14 - the first quartile
100:16 - 3
100:18 - minus 1.5
100:20 - times the interquartile range of four
100:25 - that gives us three minus six or
100:26 - negative three anything below negative
100:29 - three would be an outlier
100:31 - which there's nothing in this data set
100:33 - below negative three
100:36 - but also anything above the third
100:38 - quartile 7
100:40 - plus 1.5 times the interquartile range
100:45 - 7 plus 6 is 13 anything above 13 becomes
100:50 - an outlier
100:51 - and you notice 14 is right on the edge
100:54 - there
100:55 - and so we will say
100:58 - based on that 14
101:01 - is
101:02 - an out liar
101:07 - it lies outside of the majority of our
101:09 - data
101:14 - now that's the interquartile range
101:16 - method i said there's a second method
101:18 - it's based on the standard deviation
101:32 - the standard deviation method says that
101:34 - anything that is more
101:38 - than two
101:39 - standard
101:41 - deviations
101:46 - from the mean
101:51 - is
101:52 - an
101:52 - outlier
101:57 - which brings us back to that example
101:59 - that led into this discussion
102:04 - so
102:05 - actually let's first define this clearly
102:08 - let's do
102:10 - anything
102:11 - below
102:15 - x bar the mean
102:17 - minus two standard deviations or
102:19 - anything above
102:22 - x bar the mean plus two standard
102:24 - deviations
102:25 - is considered an outlier
102:30 - so for example
102:35 - we had our data of one three
102:38 - three
102:39 - four four
102:41 - four 4
102:41 - 5
102:42 - 5.
102:45 - and we found out already that the mean
102:47 - of this data
102:49 - is 3.625
102:53 - and we also found out the standard
102:54 - deviation of this data is 1.3
103:01 - so an outlier would be anything below
103:05 - the mean 3.625
103:09 - minus 2 standard deviations
103:17 - 3.625 minus 2 times 1.3
103:22 - is 1.025
103:29 - and we'll recognize there that we do
103:31 - have a value below
103:33 - 1.125 or 1.025 it's just a little below
103:37 - but it is below
103:38 - here 1 is an outlier
103:43 - we also have to check above
103:46 - so we'll do the 3.625
103:49 - plus 2 standard deviations or 2 times
103:52 - 1.3
103:54 - which gives us
103:57 - 6.225
104:03 - nothing above 6.225 so the only outlier
104:06 - we have here is the number one
104:11 - so we've covered quite a bit in this
104:12 - video we talked about measures of center
104:15 - the mean median and mode to estimate
104:17 - where the middle of the data is
104:19 - we talked about measures of spread to
104:20 - see how spread out we are around the
104:23 - mean or the center using the range the
104:25 - interquartile range and the most
104:27 - important ones the standard deviation
104:30 - and then we also looked at some uses of
104:31 - the standard deviation to see its
104:33 - distance from the mean
104:35 - finding out liars or identifying
104:37 - outliers so lots to take a look at on
104:39 - the assignment take a look and practice
104:41 - a few we will discuss them more in class
104:44 - we'll look forward to seeing you then
104:52 - our second unit is going to focus on
104:55 - probability and how to calculate various
104:57 - probabilities in different contexts
105:00 - so we're going to start out with basic
105:02 - probabilities and so that question we're
105:04 - going to answer is how
105:09 - do we calculate
105:11 - [Music]
105:17 - basic
105:19 - probabilities
105:26 - and as always we need to start off with
105:28 - some vocabulary
105:31 - to make sure we understand what we're
105:33 - talking about
105:35 - the first vocabulary word we need to
105:37 - know for probabilities
105:39 - is what is called the sample
105:42 - space
105:46 - the sample space is a list of all
105:50 - possible
105:52 - outcomes
105:56 - so for example if i was to flip a coin
106:02 - the sample space would be all possible
106:04 - outcomes i could get a heads
106:07 - or i could get a tails
106:11 - which begs the question
106:13 - what is an outcome so let's define an
106:15 - outcome really well
106:19 - an outcome is just the result
106:23 - of an experiment
106:30 - so an experiment might be flipping a
106:32 - coin
106:36 - an outcome would be just maybe heads
106:39 - or just tails and hence the sample space
106:42 - is all of the outcomes heads and tails
106:49 - when we're looking at the number of
106:51 - outcomes that occur in a sample space
106:52 - what we're really interested in finding
106:54 - is some type of probability
107:01 - or the chance
107:05 - that an event
107:11 - will
107:12 - occur
107:17 - and
107:18 - what we really end up with is a scale
107:24 - from zero to one
107:29 - where zero
107:30 - means the event certainly will not
107:34 - happen
107:37 - not going to happen
107:44 - and then the number one the maximum
107:46 - probability is it is certain
107:50 - to happen
107:56 - and then you could get any decimal in
107:58 - between
107:59 - so if i end up with like 0.5 that would
108:03 - be right in the middle so it's equally
108:07 - likely
108:15 - to happen
108:18 - or not happen
108:23 - so if the probability is .001 it's
108:26 - probably not going to happen but it
108:28 - could happen
108:29 - if the probability is 0.95 it probably
108:32 - will happen because it's closer to 1 but
108:34 - it might not happen
108:36 - the scale from 0 to 1 is our probability
108:41 - and there's two types of probabilities
108:44 - that we're going to be looking at the
108:46 - first is called the theoretical
108:48 - probability
108:57 - which basically says what we expect to
109:00 - happen or what should happen
109:09 - and the way we calculate a theoretical
109:12 - probability is we say that the
109:14 - probability of some event e
109:17 - is equal to the number of outcomes
109:24 - that we are looking for
109:26 - divided by the entire sample space
109:32 - or how many things occur could occur
109:36 - that's our basic probability formula
109:40 - so for example if i flip a coin
109:45 - and i want heads
109:50 - we would say
109:53 - the probability i get a heads
109:55 - is
109:56 - the number of outcomes there's only one
109:58 - outcome on a coin
110:00 - out of the sample space there are two
110:03 - possible outcomes on a coin heads or
110:05 - tails and then i would convert that to a
110:07 - decimal in this course we'll always use
110:09 - decimals for probabilities to get a
110:11 - probability of 0.05 of getting a heads
110:17 - that's theoretical probability what we
110:19 - expect to happen
110:23 - the other type of probability is called
110:25 - empirical probability
110:36 - and that is the chance of something
110:38 - happening based on our observations of
110:41 - some experiment
110:43 - what happened
110:49 - in
110:50 - an experiment
110:56 - so this is the example of maybe i flip a
110:59 - coin
111:02 - 500 times
111:07 - and i end up getting
111:10 - 257 heads
111:12 - because you know in actual practice the
111:14 - probabilities aren't perfect i'm going
111:16 - to get a few more heads or a few more
111:18 - tails it's not going to be exactly even
111:21 - and so in this case the probability of a
111:23 - heads as an empirical probability or an
111:25 - observed probability is 257 out of 500
111:30 - which is 0.514
111:36 - there's usually going to be a slight
111:38 - difference between the empirical
111:40 - probability and the theoretical
111:42 - probability
111:44 - but that difference can be made smaller
111:47 - using what's called the law
111:50 - of large numbers
111:54 - which basically says that the more
111:56 - trials i do
111:58 - the more trials done
112:02 - the closer
112:07 - the empirical
112:10 - probability
112:15 - is to
112:17 - the
112:18 - theoretical
112:22 - probability
112:27 - if i were to do a thousand trials this
112:29 - would be closer to 50 percent if i would
112:31 - do a million trials flipping a coin
112:34 - would get closer to 50 percent
112:36 - more trials the closer they're going to
112:39 - be
112:42 - now that's really basic probability
112:45 - but we do have some specific probability
112:47 - formulas
112:56 - to help us calculate some more involved
112:59 - situations and these three formulas
113:01 - we're going to look at are very closely
113:02 - related they really come as a group
113:05 - there's no one you should learn before
113:06 - the other because they're all so closely
113:08 - related so we'll do our best to define
113:10 - them one at a time when really they all
113:12 - come as a group
113:14 - the first is what we're going to call
113:15 - the conditional
113:18 - probability
113:23 - in a conditional probability we write it
113:26 - as the probability of b given a
113:30 - or with a vertical line between b and a
113:32 - and what that means is that is the
113:35 - probability
113:40 - of b
113:43 - given
113:44 - a
113:48 - has
113:49 - already
113:52 - occurred
113:55 - that's a conditional probability where
113:57 - we have some information and that's
113:59 - going to change the probability of b
114:03 - the formula for a conditional
114:05 - probability
114:06 - is the probability of b
114:08 - given a
114:10 - is equal to the probability that both
114:12 - occur
114:13 - a and b
114:16 - divided by the probability of the given
114:18 - information
114:20 - in this case the probability of a
114:23 - that conditional probability formula
114:25 - will be very important to us
114:29 - we'll do an example here in a minute
114:32 - let's go on to the second type of
114:34 - probability we need to know
114:37 - and that is the and
114:39 - probability
114:41 - the probability of a
114:43 - and b
114:46 - is the probability
114:52 - that both
114:54 - occur
114:57 - at the same
114:59 - time
115:00 - or together
115:07 - and the formula for the probability of a
115:10 - and b
115:12 - comes from the conditional probability
115:14 - if we multiply both sides of the
115:16 - conditional probability formula by the
115:18 - denominator the probability of a
115:20 - we end up with the probability of a
115:23 - times the probability of b
115:25 - given a has already occurred
115:30 - and that is the
115:32 - conditional
115:34 - i'm sorry that is the and probability
115:37 - formula
115:41 - the third formula we need to know is the
115:43 - or formula the probability of a
115:46 - or b occurring
115:49 - and that is the probability
115:55 - of a occurring
115:57 - or b
115:58 - occurring or both occurring
116:02 - one or the other or both
116:06 - and the formula for an or the
116:09 - probability of a or b
116:12 - is we're going to add the probabilities
116:15 - together
116:16 - the probability of a
116:18 - plus the probability of b
116:21 - the problem is this counts the and or
116:24 - the overlap twice it counts it in the
116:26 - probability of a and it counts it in the
116:28 - probability of b so we have to subtract
116:30 - off the overlap or subtract off the
116:32 - probability of a and b
116:35 - so it's not double counted it's only
116:37 - counted
116:38 - once
116:41 - and that gives us our formula
116:45 - for the or
116:49 - those are our three probability formulas
116:51 - the conditional probability is the
116:53 - probability of both divided by the given
116:55 - information
116:57 - the and probability with and we multiply
117:00 - the probabilities together given the
117:02 - first ones already occurred
117:03 - and with an or or probabilities we add
117:06 - them together
117:08 - subtracting off the overlap
117:10 - so let's do an example number four
117:16 - let's say we have three
117:19 - blue
117:20 - cards
117:23 - numbered
117:26 - one
117:27 - two and three
117:30 - and we also have
117:33 - let's write three out as a number since
117:35 - it's starting a sentence three blue
117:37 - cards numbered one two and three and i
117:39 - have two
117:41 - yellow cards
117:46 - numbered
117:49 - one
117:50 - two
117:57 - a we're going to find the probability
118:06 - actually let's just write as a
118:07 - probability statement we're going to
118:08 - find the probability
118:10 - that i get a blue card
118:16 - given the card is
118:19 - even
118:25 - with the conditional probability since i
118:27 - know the card is even we're not dealing
118:29 - with all five cards anymore we're just
118:31 - dealing with the even cards so
118:34 - we find the probability of both blue and
118:38 - even
118:40 - from the blue cards there is one that is
118:42 - blue and even so there's one of them
118:47 - out of the five cards
118:49 - divided by the probability of the given
118:52 - information
118:53 - the given information is that it's even
118:55 - there are
118:56 - two even numbers
118:59 - out of five
119:02 - now what's nice is generally those
119:03 - denominators will divide out and we're
119:06 - just left with one half or 0.5
119:10 - so that means if i know i've got an even
119:12 - card the probability is 50 percent that
119:15 - it's going to be even
119:20 - that's conditional probabilities
119:24 - let's find the probability that i get a
119:26 - blue card
119:28 - and
119:29 - an even card
119:33 - now this is looking for the probability
119:35 - that both occurred at the same time blue
119:39 - and even
119:41 - of the blue cards only one of them
119:45 - is blue and even
119:47 - out of the total cards now we're looking
119:49 - at the total sample space of five and so
119:52 - the probability that's blue and even is
119:56 - 0.2 there's only a 20 probability that's
119:59 - both blue and even
120:04 - what about the probability that it's
120:06 - blue
120:07 - or
120:08 - even
120:15 - for blue or even
120:19 - now we're looking for how many are blue
120:24 - the first option
120:25 - there are three blue cards
120:28 - out of five
120:29 - plus
120:30 - how many are even
120:32 - there are two even cards out of five but
120:35 - then we need to subtract off the overlap
120:38 - the ones that are blue and even
120:41 - blue and even we know there's only one
120:43 - out of five that's both blue and even
120:46 - so three plus two minus one is four
120:49 - fifths which means we have a probability
120:51 - of point
120:53 - eight
120:54 - that it is blue
120:56 - or
120:57 - even
120:59 - i want to do one more example that kind
121:01 - of illustrates
121:04 - the and formula maybe a little bit
121:06 - better
121:07 - and that is finding the probability that
121:10 - if i draw two cards
121:15 - without with replacement actually maybe
121:18 - i should write this out
121:20 - probability
121:24 - i draw
121:26 - two blue cards
121:32 - without replacement
121:38 - what i'm really saying is what's the
121:40 - probability the first one is blue
121:43 - and what's the probability the second
121:45 - one is blue
121:49 - well for the first one to be blue
121:51 - the probability of the first event there
121:53 - are three
121:55 - out of five
121:56 - that are blue
121:59 - then we multiply by the second event the
122:02 - probability that i get a blue
122:04 - given the first one was already blue
122:07 - so now one of the blues is gone maybe
122:09 - the two is gone
122:11 - now there's only two blues left
122:14 - out of there's only four cards left
122:19 - and when i multiply that across we end
122:22 - up with point
122:23 - three
122:25 - is the probability we get two blues
122:27 - without replacement
122:29 - thirty percent
122:34 - so that's what we're looking for with
122:35 - that and formula the second part the and
122:38 - we adjust the probability to assume the
122:41 - first one
122:42 - already occurred
122:48 - those are our basic probability formulas
122:50 - but there are two vocabulary concepts
122:53 - that are related to those that i want to
122:54 - make sure
122:55 - we are familiar with
122:58 - so the first one of those two is what
123:00 - are called independent
123:05 - events
123:08 - when two events are independent what
123:11 - that means is one occurring
123:16 - does not
123:19 - change
123:22 - the probability
123:27 - of
123:29 - the other
123:31 - occurring
123:36 - the opposite of this would be dependent
123:37 - events and if we think about the blue
123:39 - cards that we drew without replacement
123:41 - the second probability was two fourths
123:44 - that second probability had changed from
123:47 - the first probability because one
123:50 - occurring changes the other one's chance
123:53 - of occurring because there's fewer cards
123:55 - left there's fewer blues
124:00 - now the way we show things are
124:01 - independent we can show them
124:05 - we can show this
124:09 - in one of
124:11 - three ways
124:14 - and it doesn't matter which one of these
124:16 - ways we use
124:17 - so we'll just pick the one that's most
124:19 - convenient for our context
124:22 - the first is we can show that the
124:23 - probability of a
124:25 - given that b has occurred
124:29 - if b is not going to affect its
124:30 - probability it should still be the same
124:32 - as just a occurring by itself because b
124:36 - has no impact on it
124:40 - the opposite is also true we could say
124:42 - the probability of b given a
124:46 - is going to be just equal to the
124:47 - probability of b because a occurring has
124:50 - no impact on it
124:55 - or the third method we can look at is
124:57 - the probability of a and b
125:01 - the probability of both of them
125:02 - occurring is equal to just the product
125:05 - of the individual probabilities
125:08 - because the given part doesn't change
125:13 - so for example let's say in a class
125:19 - twenty percent of students
125:25 - are left-handed
125:32 - five percent of students
125:38 - are earning
125:41 - an a
125:43 - in the class
125:45 - good job to those five percent but only
125:49 - one percent
125:52 - of students
125:57 - are left-handed
126:01 - and earning an a
126:09 - are these events independent is there is
126:12 - there a relationship between
126:13 - left-handedness and earning an a
126:16 - well we'd have to look what's the
126:17 - probability that they're left-handed
126:20 - we're told the probability they're
126:21 - left-handed is point twenty or twenty
126:23 - percent
126:25 - the probability that they're earning an
126:27 - a
126:29 - of all students five percent are earning
126:31 - an a so the probability of earning an a
126:33 - is .05
126:37 - we're also told the probability of the
126:39 - students who are left-handed
126:41 - and earning an a both of them together
126:45 - is .01
126:51 - well we can use either one of the three
126:54 - formulas for showing things are
126:56 - independent it's probably going to be
126:57 - easiest in this context to use the third
127:00 - because we have all of those pieces
127:03 - so the probability of a and b the
127:05 - probability of left
127:08 - and
127:11 - receiving an a
127:14 - should be equal to the probability of
127:15 - being left-handed times the probability
127:17 - of receiving an a if they are in fact
127:20 - independent
127:22 - well the probability of a and l is 0.01
127:25 - the probability of left-handed is 0.2
127:28 - the probability of an a is 0.05
127:32 - and sure enough
127:33 - we get those are equal to each other
127:37 - because they're equal
127:39 - we'll say therefore
127:40 - they are
127:43 - independent
127:46 - events
127:51 - if it wasn't equal we would say the
127:53 - opposite or that they're dependent
127:55 - events
127:57 - so that's the first concept the idea of
127:59 - independent versus dependent the second
128:02 - concept that i want to wrap up with
128:04 - today
128:05 - is the idea
128:11 - of mutually
128:14 - exclusive
128:21 - two events are mutually exclusive that
128:23 - means both
128:25 - cannot
128:27 - occur
128:30 - at the same time
128:38 - essentially what we're saying is the
128:40 - probability of a
128:42 - and b
128:44 - is equal to zero
128:49 - an example of this would be if i were to
128:51 - roll a die a standard six-sided die
128:56 - and we're going to let
128:57 - o
128:58 - [Music]
129:01 - actually use the letter d because o is a
129:03 - bad letter for math
129:05 - d is going to represent an odd
129:10 - less
129:11 - than
129:12 - 4.
129:15 - b
129:16 - is going to represent a number bigger
129:22 - than 3.
129:26 - so what you see is d the odds less than
129:28 - 4 are the numbers 1
129:30 - and 3. that's kind of the sample space
129:33 - of d
129:35 - and b
129:36 - the event b is everything bigger than 3
129:39 - which is 4
129:40 - 5 and 6.
129:45 - these two have nothing in common you
129:48 - can't both be an odd less than four
129:51 - and a number bigger than three
129:54 - because we can't have both together we
129:55 - say they are mutually
129:58 - exclusive
130:01 - in other words the probability that we
130:04 - have an odd less than four
130:06 - and a number bigger than three
130:09 - is equal to zero
130:11 - that never happens
130:14 - so little vocabularies we wrap up with
130:16 - mutually exclusive both can't occur at
130:18 - the same time
130:20 - independent one occurring does not
130:22 - affect the other occurring
130:24 - but the big thing that we're looking at
130:25 - today are these probability formulas
130:28 - conditionals ands and ors take a look at
130:31 - the homework assignment to practice a
130:33 - few of these we will try a few more in
130:35 - class and answer any questions you might
130:37 - have
130:38 - then
130:45 - now that we're comfortable working with
130:47 - basic probabilities we're going to look
130:48 - at different ways we can organize our
130:51 - probabilities
130:52 - and information in today's video we're
130:54 - going to look at the question
130:56 - how
130:59 - do we
131:01 - organize
131:05 - probability information
131:14 - in a table
131:20 - specifically we're going to be in the
131:22 - context of what is called a contingency
131:28 - table
131:31 - which is basically just a table
131:34 - that lists
131:38 - results
131:42 - in relation
131:49 - to two variables
131:58 - these tables and this information will
132:00 - make
132:01 - calculating
132:06 - probabilities
132:12 - easier
132:14 - and what makes it easier is quite often
132:16 - we will add
132:19 - a column
132:22 - and row
132:27 - for totals
132:33 - so for example
132:37 - let's say we've done a survey
132:40 - and we're comparing whether or not
132:43 - people have speeding tickets in the last
132:45 - year
132:49 - or no speeding tickets in the last year
132:54 - and we're going to break this up
132:58 - into three groups
133:00 - the first group are going to be our
133:02 - younger drivers the under 21 drivers
133:07 - and then we're going to also look at the
133:09 - 21 to 25 year old drivers
133:13 - and then we'll also look at the over
133:16 - 25 drivers
133:20 - and the survey is conducted and there's
133:22 - 82 under 21s with the ticket
133:25 - 17 without a ticket in the past year
133:28 - for the 21 to 25s there were 39 with a
133:31 - speeding ticket and 27 without
133:34 - and for the over 25 there's 18 with a
133:37 - speeding ticket and 61 without
133:42 - now with this contingency table it's
133:44 - going to be helpful that we're going to
133:46 - add an extra row and an extra column
133:50 - if it's not there already
133:53 - that's going to give us the totals
133:56 - and these totals are going to make
133:59 - calculating individual probability
134:01 - questions much more efficient
134:04 - so if we total the under 21 we see we
134:06 - have 99 surveyed
134:08 - the 21 to 25 total that we get 66
134:12 - surveyed
134:13 - totally over 25 we get 79 surveyed
134:19 - working across the rows 82 plus 39 plus
134:22 - 18 there's 139 people surveyed who got a
134:26 - speeding ticket in the last year
134:28 - the no tickets 17 plus 27 plus 61 is 105
134:34 - and for the totals 99 plus 66 plus 79
134:38 - gives us 244 people total in the survey
134:42 - and a good way to check that that total
134:44 - is correct is if we add the other
134:45 - combination 139 plus 105 that should
134:48 - also equal the 244 which it does
134:53 - and so that what we have there as that
134:55 - example is a contingency
134:58 - table
135:00 - now we're ready to find some
135:01 - probabilities
135:03 - off this contingency table
135:06 - for example if i want to know the
135:07 - probability that someone is 21 to 25
135:11 - i can see very quickly on my contingency
135:13 - table that there are 66 people in the 21
135:17 - to 25 range
135:22 - out of a total
135:23 - of 244 people
135:26 - and so when i divide 66 by 244 we can
135:29 - quickly get our probability of 0.2705
135:37 - we could also do maybe the probability
135:41 - that someone has no tickets
135:47 - very similar i'd say well no tickets the
135:49 - total there is 105
135:53 - out of the grand total which is 244
135:58 - and when we divide 105 by 244 we get
136:01 - 0.4300
136:05 - for our probability
136:11 - we can also do ands and we can do ors
136:15 - we can find let's combine these together
136:16 - the probability that someone's 21
136:18 - through 25
136:20 - and has no tickets
136:26 - well the 21 to 25
136:28 - and have no tickets are when both of
136:30 - those occur together
136:32 - at the same time that's where they
136:34 - overlap here in the middle we have 27
136:37 - people who are of no tickets and they're
136:40 - 21 to 25
136:42 - out of the total of the whole group is
136:44 - still 244.
136:47 - and so when i divide 27 by 244 we get
136:50 - 0.1107
136:56 - and we can change that to an or
137:00 - we can find the probability
137:02 - that someone's 21 to 25
137:05 - or has no tickets
137:07 - and if you remember the or formula says
137:09 - we have to add the individual pieces and
137:12 - then subtract where they overlap
137:15 - so 21 to 25 there's 66 of them
137:20 - plus the no tickets there's 105 of them
137:25 - but we have to subtract where they
137:27 - overlap because these 27
137:30 - where they overlap
137:32 - have been counted twice in both the
137:34 - column and the row so when we subtract
137:36 - off the 27
137:38 - out of the 244
137:41 - when we do that math on our calculator
137:45 - we get
137:46 - 0.5902
137:48 - about a 59 percent probability they're
137:50 - one of those
137:51 - two
137:56 - we can even do given probabilities
138:00 - let's do the probability
138:02 - that we're in that 21 to 25 range let's
138:05 - get rid of these circles we don't need
138:09 - given we know the person has no tickets
138:18 - well with a given probability we are
138:21 - looking for
138:22 - both of them or the overlap
138:24 - divided by the given information
138:28 - so where they overlap 21 to 25 and no
138:31 - tickets they overlap with 27 but we're
138:34 - going to divide by the given information
138:37 - this time it's not the 244 because we've
138:39 - shrunk our sample size now we're just
138:42 - interested in those that have no tickets
138:44 - we're only interested in that 105.
138:48 - and so with the given information
138:50 - shrinking the sample size now the
138:52 - probability is 0.2571
138:59 - we can switch that
139:01 - and see how that probability compares
139:03 - the probability they have no tickets
139:07 - given they're between 21 and 25 years
139:09 - old
139:14 - you might pause the video and see if you
139:16 - can figure this one out on your own
139:20 - with a given probability we need to find
139:23 - where they overlap
139:24 - divided by the probability of the given
139:27 - information
139:28 - they overlap again no tickets in 21 to
139:31 - 25 with these 27 individuals
139:35 - however now our sample space the given
139:38 - information is just the 21 to 25 years
139:41 - old and that's the 66.
139:44 - so we'll do 27 divided by the 66. to get
139:48 - our probability
139:50 - of 0.4091
139:56 - and you can see how we move through each
139:58 - of these probabilities at a much greater
140:00 - accelerated pace when we have the
140:02 - contingency table to organize our data
140:06 - for us
140:09 - that's the nice thing about the
140:11 - contingency table
140:14 - one more thing i want to look at though
140:15 - is we have this vocabulary word from our
140:17 - previous video of independence
140:21 - so i want to know are being 21 to 25
140:26 - and having no tickets
140:32 - independent
140:36 - does that mean being 21 to 25 has no
140:39 - impact on whether or not you had a
140:41 - ticket in the past year
140:44 - well we talked about there being three
140:46 - different formulas we could use in order
140:48 - to show this
140:50 - one of those three formulas says that a
140:54 - given probability
140:56 - should not change
140:58 - the probability
141:00 - if they are in fact independent
141:02 - in other words the probability they're
141:04 - being 21 to 25
141:07 - given they have no tickets
141:12 - should be the same as the probability of
141:14 - just being 21 to 25
141:17 - if they're independent because the
141:18 - tickets shouldn't impact that at all
141:24 - well we just found both of these pieces
141:26 - the probability of being 21 to 25 given
141:28 - we have no tickets is actually here in
141:30 - number five that was 0.2571
141:36 - and the probability of being 21 to 25 we
141:38 - found in part one
141:40 - that's 0.2705
141:45 - and we see that these guys are different
141:50 - therefore the probability has changed
141:53 - once we have given information and
141:55 - shrunk down the sample size
141:57 - that means these two variables are
142:00 - actually
142:01 - dependent on each other
142:07 - so all we're looking at today is
142:09 - organizing our probability information
142:11 - in a contingency table and taking a look
142:14 - at how that helps facilitate calculating
142:17 - the actual individual probabilities it
142:19 - also gives us an opportunity to practice
142:21 - more with and or
142:23 - and the given probabilities so take a
142:26 - look at these on the homework assignment
142:28 - come to class ready to discuss them and
142:30 - work with these contingency tables a
142:33 - little bit more
142:39 - today we're going to continue our work
142:41 - with representing probabilities visually
142:44 - using what are called tree diagrams and
142:46 - we'll also take a look at bayes theorem
142:48 - the question though that we're
142:49 - attempting to answer is how do we
142:52 - visually organize
142:54 - conditional probabilities
142:58 - how do we visually
143:02 - organize
143:05 - conditional
143:09 - probabilities
143:17 - and really the best way
143:19 - to organize a conditional probability is
143:21 - using what is called a tree
143:24 - diagram
143:26 - and a tree diagram basically has a
143:29 - branch
143:31 - for each outcome
143:38 - and then
143:40 - each branch produces a conditional
143:42 - outcome from that outcome from that
143:44 - outcome and then to find final
143:46 - probabilities
143:48 - we can
143:50 - multiply
143:53 - down
143:54 - the branches
143:59 - for final
144:02 - probabilities
144:09 - for example let's say we have an urn
144:17 - with five blue
144:21 - blue
144:23 - and
144:25 - three
144:26 - green
144:29 - marbles
144:34 - and you will draw
144:40 - out two marbles
144:45 - without replacement
144:55 - we are being asked to make a tree
145:00 - diagram
145:04 - to model
145:06 - the possible
145:10 - outcomes
145:13 - and
145:15 - probabilities
145:24 - so what we need to do is we're going to
145:26 - draw a branch on this tree for every
145:28 - decision point
145:30 - so the first decision point is the very
145:32 - first draw
145:35 - so this represents the first draw
145:38 - and off that first draw we could end up
145:40 - with a blue marble
145:42 - or we could end up with a green marble
145:46 - now you can see there's a total of eight
145:48 - marbles five plus three so the
145:50 - probability of getting a blue marble on
145:52 - the first draw is five out of eight
145:55 - and the probability of getting a green
145:57 - marble on the first draw
145:59 - is three out of eight
146:02 - but we're not done there because we're
146:03 - still going to draw another marble
146:06 - and that marble could be either blue
146:10 - or green
146:11 - and it doesn't matter which color we got
146:13 - first we're going to have the same
146:15 - possible outcomes it could be either
146:17 - blue
146:18 - or
146:19 - green on this
146:21 - second draw
146:25 - and so you can kind of see that if you
146:27 - follow a
146:28 - draw down if we want a green first and a
146:31 - blue second we'd go down the green line
146:33 - and the blue line and that represents
146:36 - green first and blue
146:38 - second
146:40 - or if i wanted a blue then a green we
146:42 - would go down the blue line first and
146:44 - then the green line second and we'd end
146:47 - up with the blue green combination
146:51 - but first let's fill in the individual
146:52 - probabilities because on the second draw
146:56 - things have changed
146:58 - if we go down the left side where the
147:00 - blue was drawn first
147:03 - and if we want a blue on the second draw
147:05 - there's no longer five blues left to
147:06 - pick from there are only four blues left
147:09 - to pick from
147:11 - and there are only seven marbles left so
147:13 - we only have a four and seven chance of
147:15 - getting a blue marble on the second draw
147:18 - given the first draw was blue
147:21 - similarly with the green option if we
147:24 - want blue then green
147:26 - tracking that down there are still three
147:28 - green marbles left but now the total is
147:31 - only seven marbles because one has been
147:33 - drawn out a blue one has been drawn out
147:38 - similarly on the right side of the tree
147:40 - diagram if green is drawn first and then
147:43 - i want a blue on the next draw there are
147:46 - five blues to pick from
147:48 - out of the seven marbles that are left
147:54 - but with the green there are only two
147:56 - marbles that are green left
147:58 - out of the seven marbles that are left
148:04 - now to get my final probabilities for
148:06 - each of these possible outcomes we just
148:08 - have to multiply down the chain
148:11 - so if i want a blue blue outcome
148:14 - what i'll do is i'll multiply the 5 8
148:18 - times the 4 7
148:22 - and when i multiply 5 8
148:24 - times 4 7
148:26 - we get a probability of 0.3571
148:33 - now if i want blue then green
148:37 - we multiply down those probabilities to
148:39 - get the blue
148:41 - green possibility
148:44 - 5 8 times 3 7
148:47 - is 0.2679
148:54 - going down the next path we go green
148:57 - then blue
149:00 - so if we want a green marble
149:03 - then a blue marble
149:06 - that probability is 3 8
149:08 - times 5 7
149:10 - which turns out to also be 0.26
149:13 - 7
149:15 - 9. i think the other one was 2 6 7 9
149:17 - also i wrote it down wrong
149:22 - the last track is going down the green
149:25 - and then the green
149:27 - so if we want two green marbles
149:31 - green then green
149:33 - we'll multiply the 8
149:36 - times the 2 7 to get the 0.1071
149:45 - and now we see all the different
149:47 - possible probabilities for drawing two
149:50 - marbles out of this urn
149:53 - without replacement
149:57 - and we can use that table to find things
149:59 - like the probability that both
150:03 - are the same
150:05 - color
150:08 - well both the same color would be a blue
150:11 - blue
150:12 - or a green green and then we subtract
150:14 - the overlap but there's certainly no
150:16 - overlap with blue blue and green green
150:18 - so we'll just add those together
150:22 - point and i should do this in red
150:26 - 0.3571
150:27 - plus 0.1071
150:30 - will give me the probability that i get
150:32 - two are the same color
150:34 - of point four six
150:35 - four
150:37 - two
150:42 - one more example what's the probability
150:44 - that i get at least
150:52 - one blue
150:57 - now i could do this a couple ways one
151:00 - way is i could say these first three
151:02 - options each have at least one blue
151:07 - but that's a lot more work than saying
151:09 - let's look at the complement
151:12 - and let's subtract off the only one
151:14 - that's left from the absolute
151:16 - probability of one
151:18 - we know all the probabilities are one
151:21 - so we subtract out the probability that
151:24 - doesn't satisfy
151:26 - what we want if we subtract off the
151:28 - 0.071
151:30 - we'll end up with our probability of at
151:32 - least one blue of 0.8929
151:42 - and that's how using a table can greatly
151:45 - facilitate our conditional
151:48 - probabilities and their various
151:50 - calculations
151:53 - but one very important application
151:57 - of using these tables
152:00 - is to facilitate what is called
152:03 - bayes theorem
152:08 - and bayes theorem is a way to find a
152:12 - conditional probability like the
152:14 - probability of b given a
152:19 - using known values
152:25 - of the probability of b
152:27 - oops sorry the probability of a
152:29 - given b
152:33 - in other words the given values
152:35 - are backwards from the given values that
152:37 - we want
152:39 - now bayes theorem officially says that
152:42 - the probability of b given a
152:46 - is equal to
152:47 - the probability of both
152:49 - the probability of a and b
152:53 - divided by the probability of the given
152:55 - information or the probability of a
152:58 - but we might not know the exact
152:59 - probability of a
153:01 - but we do know the probability of a
153:04 - given b
153:06 - and we can add to that the probability
153:09 - of a
153:10 - given
153:11 - it's not b
153:14 - in other words we combine all the a
153:15 - probabilities together to build that
153:18 - denominator
153:20 - and that last part that not b it may be
153:23 - several
153:25 - options
153:27 - so we might need to add three four five
153:30 - six things
153:31 - that are the not these but we have to
153:32 - basically add all the combinations of
153:35 - the probabilities of a
153:36 - given the other stuff
153:40 - this formula is not worth memorizing
153:43 - because it is always easier
153:45 - to do this on a tree
153:47 - the tree makes this easy
153:52 - and i'm going to show you what i mean
153:55 - with a couple
153:57 - examples
154:00 - let's say a marketing company
154:08 - predicts
154:12 - 90 percent
154:15 - of new products
154:21 - are profitable
154:29 - however
154:32 - the marketing firm isn't perfect
154:36 - only 70
154:40 - of those products
154:44 - predicted
154:48 - to be profitable
154:54 - actually
154:57 - are
155:03 - in addition
155:09 - those predicted to be not profitable
155:27 - 20 percent
155:30 - actually are profitable
155:39 - so let's say we've got some product and
155:41 - it turns out that it was profitable
155:44 - that's given to us
155:46 - given a product
155:50 - was profitable
155:56 - what
155:58 - is the probability
156:05 - it was
156:09 - predicted
156:13 - to be
156:14 - profitable
156:22 - so was profitable
156:24 - what's the probability it actually
156:27 - was
156:28 - profitable
156:32 - let's make a tree to model this
156:35 - first the marketing firm looks at it and
156:38 - they say yes
156:39 - it will be profitable
156:42 - or not
156:45 - we are told the company predicts ninety
156:47 - percent of their products will be
156:49 - profitable
156:51 - therefore the complement 10 percent must
156:54 - not be profitable
157:02 - and that's actually not just profitable
157:04 - but that's a prediction of profitable
157:07 - and a prediction of not being profitable
157:10 - because then the next branch we're going
157:11 - to look at it actually is profitable
157:15 - or it's not profitable were they right
157:17 - or wrong
157:18 - it actually is profitable
157:20 - or it's not profitable
157:24 - we are told seventy percent of those
157:26 - that are predicted to be profitable
157:29 - actually are
157:30 - so if i go down that they're predicted
157:31 - to be profitable they actually are
157:34 - 70 are
157:36 - and the rest 30
157:39 - are not
157:41 - of those that were predicted to be not
157:44 - profitable
157:45 - going down the nut line
157:47 - 20 percent actually are so 20
157:50 - is on the right side and the rest 80
157:53 - percent
157:54 - are not
158:02 - going down the lines multiplying point
158:04 - nine times point seven the first branch
158:06 - is a point sixty three percent
158:07 - probability
158:09 - point nine times point three there's a
158:11 - point twenty seven percent probability
158:14 - next branch point one times point two is
158:16 - point o two
158:18 - and point one times point eight is point
158:20 - o eight
158:23 - so for my actual probability statement
158:26 - it's asking
158:28 - what is the probability it was predicted
158:31 - to be profitable
158:35 - the probability that we predicted
158:39 - it was
158:40 - profitable given
158:43 - given it actually was profitable given
158:46 - it actually is profitable
158:50 - well we know conditional probabilities
158:53 - are the probability of both divided by
158:55 - the probability of the given information
158:58 - so the probability of both where it's
159:01 - predicted to be profitable and it
159:03 - actually is is this far left branch here
159:08 - that's 0.63
159:10 - that is going to be our numerator the
159:13 - 0.63
159:16 - the denominator then is going to be the
159:18 - probability of the given information
159:21 - the given information is that it is
159:24 - profitable the is occurs in two
159:27 - locations
159:29 - the is is the left side
159:33 - of both branches the is profitable
159:37 - both of those together
159:39 - make up my sample space
159:41 - so i take the given information and i
159:44 - add those pieces together 0.63 plus 0.02
159:49 - and that gives me 0.63 at a point 65
159:53 - which gives me a probability that it was
159:56 - predicted to be profitable
159:58 - given it actually was
160:00 - of 0.9692
160:07 - and that is how bayes theorem helps us
160:09 - reverse the given probabilities in our
160:13 - table
160:17 - let's do one
160:19 - more of these bayes theorems with a
160:21 - little bit more
160:22 - detail in it
160:27 - let's say bob drives to work
160:34 - forty percent of the time
160:40 - he takes the bus
160:46 - fifty percent of the time
160:53 - he walks
160:57 - ten percent of the time
161:02 - so that's 100 of the time how he gets to
161:04 - work
161:06 - the probability
161:12 - he will be on time
161:20 - he is 90 if he drives
161:27 - 60
161:28 - if he takes the bus
161:43 - and 30 percent
161:45 - if he walks
161:52 - if bob arrives late
161:54 - that's our given information bob has
161:56 - arrived late
162:00 - what is
162:03 - the probability
162:09 - he took the bus
162:22 - well let's make our tree diagram
162:25 - to summarize what happens
162:28 - he actually has three things that can
162:30 - happen initially
162:32 - he can
162:33 - drive
162:35 - he can take the bus
162:39 - or he can walk
162:41 - in fact we're given probabilities that
162:43 - he drives forty percent of the time
162:45 - takes the bus fifty percent of the time
162:47 - and walks ten percent of the time
162:51 - once he does that he will be either late
162:55 - or on time
162:58 - doesn't matter which method he takes he
162:59 - will be either late
163:01 - or on time
163:03 - he will be either late or on time
163:11 - we're told about the on time half
163:13 - if he walks
163:16 - or night if he drives sorry if he drives
163:19 - going down the drive he will be on time
163:21 - ninety percent of the time or point nine
163:24 - which means he's late the rest of the
163:25 - time
163:26 - ten percent
163:29 - the bus we're told he is on time with
163:31 - the bus
163:32 - sixty percent of the time
163:34 - which means he's late the rest of the
163:36 - time
163:37 - forty percent
163:40 - but if he walks he's on time thirty
163:43 - percent of the time
163:45 - and he's late the rest or seventy
163:47 - percent of the time
163:51 - multiplying down the chain then
163:54 - driving in light is .04
163:57 - driving in on time is point 36
164:00 - bus and light is .20
164:03 - bus and on time is 0.30
164:07 - walking in late is 0.07
164:10 - walking and on time is 0.03
164:17 - we are asked to find the probability
164:19 - that he took the bus
164:22 - given he arrived late
164:27 - i like to start off by marking all of
164:30 - the given information
164:32 - so given he arrived late
164:34 - late was our left branch late late
164:38 - late
164:40 - what we're looking for is the
164:42 - probability that he took the bus
164:44 - and was late so what we're looking for
164:47 - is the 0.20
164:51 - now we're ready to build our probability
164:55 - the probability of both
164:57 - bus and late is that point 20.
165:02 - but divided by the probability of the
165:05 - given information
165:07 - the given information is that it was
165:09 - late
165:09 - we marked that as a 0.04
165:12 - plus a point 20
165:14 - plus a 0.07
165:16 - so we have point 20
165:19 - out of point 31
165:21 - which when we divide those you get .6452
165:28 - almost a 65 chance that bob took the bus
165:32 - given
165:33 - he's arriving late
165:37 - that's bayes theorem and that's how we
165:38 - can use tree diagrams to help us with
165:40 - conditional probabilities so take a look
165:43 - at a few of these on the homework we
165:45 - will discuss these more in class and
165:47 - i'll look forward to working with you
165:49 - then
165:57 - now that we're comfortable working with
165:58 - and finding probabilities we're ready to
166:00 - talk about discrete probability
166:03 - distributions and then later we'll talk
166:04 - about continuous probability
166:07 - distributions
166:08 - first i want to make sure we and we know
166:10 - what question we're trying to answer
166:13 - the question for today is what are
166:18 - discrete
166:22 - probability
166:27 - distributions
166:33 - and to set this up we're going to be
166:34 - working with this idea of finding what's
166:37 - called the probability
166:41 - distribution
166:47 - function
166:50 - often this is just abbreviated as pdf
166:53 - the probability distribution function
166:56 - and these probability distribution
166:58 - functions have two key
167:01 - characteristics
167:09 - the first is
167:11 - for this pdf each
167:13 - individual
167:15 - probability
167:19 - is between
167:24 - 0
167:25 - and 1.
167:29 - in addition if we took all the
167:31 - probabilities
167:36 - and added them together
167:38 - they would sum
167:40 - to
167:41 - one
167:44 - now often we'll organize these
167:46 - probability distribution functions by
167:49 - what we call
167:50 - random variables
167:56 - and a random variable just describes
168:01 - the outcomes
168:05 - of an experiment experiment
168:19 - now a discrete
168:24 - probability
168:29 - distribution
168:33 - is used with countable
168:38 - or discrete
168:41 - outcomes
168:46 - for example
168:52 - if we let the random variable x
168:55 - represent the number
168:59 - of movies
169:03 - watched
169:06 - last week
169:10 - that's a discreet random variable
169:12 - because you only can watch a certain
169:13 - number of movies we can't do decimals
169:15 - you either watched it or you didn't
169:18 - so because this is a countable result
169:20 - it's a discrete result and we can
169:22 - collect discrete data
169:24 - a survey is conducted
169:34 - and it is found
169:42 - that forty percent of the respondents
169:48 - watched two movies last week
169:53 - 50
169:57 - watched one movie
170:00 - and the rest
170:04 - watched
170:06 - no movies
170:13 - we can summarize
170:20 - the pdf
170:21 - or the probability distribution function
170:24 - in a table
170:29 - and in this table all we have to do is
170:31 - list the possible results
170:34 - i suppose countable wrong forgot the t
170:36 - in countable sorry about that
170:38 - all we have to do is list the possible
170:40 - results
170:41 - and their associated probabilities
170:43 - so x is the random variable people
170:45 - watched either 0 1 or 2
170:48 - videos and then we'll make a column
170:51 - representing the probability that x
170:54 - occurred
170:55 - we're told that forty percent watched
170:57 - two movies so the probability is point
170:59 - four
171:01 - fifty percent watched one movie
171:03 - the probability is 0.5
171:06 - we're not told what percent watched no
171:08 - movies but if we add what we have
171:10 - together we see we've covered 90 percent
171:12 - of the respondents so there's only 10
171:14 - percent left to make it equal 100
171:17 - percent and that must be the zero
171:20 - and this becomes our probably
171:22 - distribution table
171:26 - now what's nice about being able to
171:28 - organize all the countable possibilities
171:30 - is it allows us to consider
171:34 - the expected value
171:39 - which is also called the mean
171:43 - and the standard deviation
171:52 - of the probability distribution function
171:55 - or of the pdf
171:59 - and the formulas for finding these
172:01 - expected values and standard deviations
172:03 - are very similar to the formulas we had
172:05 - for means and standard deviations from
172:08 - chapter one
172:11 - but using the probabilities instead of
172:13 - the frequencies
172:16 - first the expected value
172:22 - the expected value is the long
172:25 - term
172:27 - average
172:29 - or mean
172:33 - of the pdf in other words if you ran
172:35 - lots and lots of experiments and took
172:37 - the average of the results what would
172:39 - that expected value or that average
172:41 - result b
172:43 - and the formula for the expected value
172:46 - or the mean
172:47 - is equal to the sum
172:49 - of all the x's
172:51 - times their individual probabilities
172:55 - this is a good formula
172:57 - to
172:58 - know
173:00 - so for our example
173:01 - where we had our number of movies being
173:05 - 0 1 and 2
173:07 - and their individual probabilities being
173:09 - 0.1
173:11 - 0.5 and 0.4
173:14 - we can find the average number of movies
173:16 - watched in this survey
173:19 - by making an extra column for x times p
173:22 - of x
173:23 - multiplying the x value times its
173:25 - probability
173:27 - zero times point one is zero
173:30 - one times point five is point five and
173:32 - two times point four is point eight
173:36 - and if i add those together
173:38 - that will give me the sum of the x's
173:40 - times the p of x's
173:42 - which is 1.3
173:46 - this tells me that the average student
173:51 - or survey respondent
174:00 - watched
174:02 - 1.3 videos
174:05 - last week
174:12 - the expected value if i took a whole
174:15 - bunch of students over and over again i
174:17 - would expect the average to be
174:19 - 1.3
174:23 - and again similar to how we found the
174:25 - standard deviation with frequencies we
174:27 - can find the standard
174:31 - deviation
174:34 - or the spread
174:38 - of our random variables
174:45 - using the formula that's sigma the
174:48 - standard deviation
174:50 - is equal to the square root
174:53 - of the sum
174:56 - of the difference between the mean and
174:58 - the value squared
175:00 - times the probability
175:02 - of the values
175:06 - again this would be a good formula to
175:08 - know how to use and basically like we
175:10 - did before with finding standard
175:12 - deviation we're going to make an
175:14 - extended table
175:17 - so again our values for the movies were
175:19 - 0 1 and 2.
175:22 - their individual probabilities were 0.1
175:26 - 0.5 and 0.4
175:30 - and we're going to start to build this
175:31 - formula by adding columns
175:35 - the first part of the formula wants the
175:37 - difference between the mean and the
175:38 - value
175:40 - so our mean remember we just found out
175:43 - that the mean was equal to 1.3 we did
175:46 - that in a previous section
175:49 - so 1.0 minus 1.3 is negative 1.3
175:55 - 1 minus point three is negative point
175:58 - three
176:00 - and two minus one point three is
176:03 - positive point seven
176:06 - but then our formula says we take that
176:08 - difference
176:10 - and we square it make them all positive
176:13 - 1.3 squared is 1.69
176:18 - 0.3 squared is 0.09
176:21 - and 0.7 squared is point
176:23 - 49.
176:27 - but then the formula says we need to
176:29 - take that square of the difference and
176:31 - multiply it
176:33 - by the individual probabilities
176:36 - so we'll take that green column times
176:38 - the second black column one point
176:41 - six nine times point one is point one
176:45 - six nine
176:47 - point five times .09 is point o
176:51 - four five
176:54 - and 0.4 times 0.49
176:57 - is 0.196
177:03 - and our formula says
177:06 - let's find that sum our formula wants
177:08 - the sum of the difference squared times
177:11 - the probability
177:13 - which is equal to
177:15 - 0.169 plus .045
177:18 - plus 0.196 is point 41
177:26 - and we're told the standard deviation is
177:28 - the square root
177:30 - of that value the square root of 0.41
177:33 - which is equal to 0.6
177:41 - so now we know our probability
177:43 - distribution
177:45 - function
177:47 - that table that we made has an average
177:49 - expected value of 1.3 and a standard
177:52 - deviation of 0.6403
177:56 - so that's what we're looking at today is
177:58 - we're taking a look at how to work with
178:00 - these probability distribution functions
178:02 - specifically the discrete ones and then
178:05 - in our next few videos we'll look at
178:06 - some specific types of discrete
178:09 - probability distributions but for now
178:11 - take a look at practicing a few of these
178:13 - on the homework we'll discuss them
178:15 - further in class
178:17 - good luck
178:24 - this video is going to take a look at a
178:26 - special type of discrete distribution
178:29 - called the binomial distribution and the
178:31 - binomial distribution helps us calculate
178:35 - what
178:37 - is the probability
178:45 - of x successes
178:50 - out of
178:52 - in
178:54 - trials
178:56 - in other words i'm going to conduct a
178:57 - survey and maybe i want to see how many
178:59 - people support a particular political
179:01 - candidate i want to know what's the
179:03 - probability that
179:05 - 100 out of 200 of them or 50 percent
179:09 - support my candidate
179:11 - that is what we call a binomial
179:17 - distribution
179:23 - and a binomial distribution has the
179:26 - following characteristics
179:32 - first there is always a fixed
179:36 - number
179:39 - of trials
179:42 - i'm going to interview a certain number
179:43 - of people or i'm going to run a test a
179:45 - certain number of times and when i run
179:47 - that experiment or that survey there are
179:49 - really only two options
179:56 - the two options are success
180:00 - and we usually use x to represent the
180:03 - number of successes
180:05 - or
180:08 - failure and it's important to note as we
180:11 - define success and failure in our
180:12 - experiment there is no
180:15 - moral or good bad judgment to the word
180:19 - success and quite
180:21 - often
180:22 - success is a bad thing if i'm in quality
180:25 - control i might say a success is a
180:28 - defective part and we're looking at how
180:30 - many successes there are in that case
180:32 - success is a bad thing so there's no
180:34 - moral or ethical standard for success
180:37 - success is just
180:39 - what we are looking
180:40 - for
180:41 - so try to avoid pinning positive and
180:44 - negative emotions to that word
180:47 - with those two options of success and
180:49 - failure we'll often talk about p which
180:52 - is equal to the probability
180:58 - of a success
181:01 - and if x is the number of successes
181:05 - and we'll use n for the number of trials
181:08 - x divided by n would be the probability
181:11 - of that success
181:15 - then we also have this letter q
181:18 - which is the probability
181:23 - of failure
181:26 - the opposite and since there's only two
181:29 - options success and failure we can
181:31 - quickly calculate q
181:34 - to be equal to 1 minus the probability
181:37 - of a success it's the complement
181:43 - so the distribution itself
181:51 - the distribution for the binomial is
181:53 - when we're going to say that x
181:56 - tilde or little squiggly line
182:00 - b for binomial
182:02 - and then in parentheses we'll do n the
182:04 - number of trials and p the probability
182:07 - of success and we'll use this notation
182:10 - to represent how the x is distributed
182:14 - it's distributed as a binomial within
182:16 - trials and a p probability of success
182:22 - and if we are in the context of the
182:23 - binomial we have some shortcuts to help
182:26 - us calculate things like the expected
182:31 - value
182:33 - the expected value or the mean of the
182:36 - distribution is simply going to be the
182:38 - number of trials times the probability
182:41 - of success
182:43 - and that seems to make sense if i have
182:45 - 30 trials and a 10 percent chance of
182:47 - success i would expect to be successful
182:50 - 10 percent of 30 or 3 times
182:54 - we can also use a shortcut formula for
182:57 - the standard deviation
183:02 - of the binomial
183:04 - and the standard deviation or sigma is
183:07 - equal to the square root of n p
183:10 - q
183:12 - now we could go through the formulas for
183:15 - mean and standard deviation like we did
183:18 - with just the generic discrete
183:19 - probability distributions it just gives
183:21 - us the same answer so we might as well
183:23 - use this nice little shortcut
183:30 - and actually before we move on let's go
183:33 - ahead and highlight these three pieces
183:35 - because these three pieces are
183:38 - foundational to
183:40 - doing our binomial distribution
183:46 - let's look at using
183:48 - the binomial distribution
183:56 - and you notice i never gave the formula
183:58 - for how we actually calculate binomial
184:00 - probabilities that's because we're going
184:02 - to cheat and we're going to use the
184:04 - calculator to do all the work for us
184:12 - in the calculator the steps that you're
184:14 - going to push on the ti 83 or 84
184:16 - calculator is you'll hit the second
184:19 - button
184:23 - and then you're going to hit the
184:26 - what's called the distribution button
184:30 - which is above the button that actually
184:32 - says vars
184:36 - so when you hit the second it gives you
184:37 - the command above the button the
184:39 - distribution function is what we want to
184:41 - use and we're going to use the binomial
184:43 - distribution
184:45 - so then we will scroll down and there's
184:48 - two options for the binomial
184:51 - the first option is called the binomial
184:56 - p
184:57 - d
184:57 - f
184:58 - and it opens a parenthesis
185:01 - that one gives us the probability
185:07 - of exactly
185:11 - x
185:12 - successes
185:16 - the one below it we'll also use is
185:19 - called the binomial
185:22 - cdf
185:24 - and the c in there stands for a
185:26 - cumulative distribution function that
185:29 - tells us the probability
185:34 - of up to
185:37 - and including
185:41 - x successes
185:45 - in other words with the cdf if i'm
185:47 - interested in the number three if i want
185:49 - three successes the pdf will tell me
185:52 - exactly three successes
185:55 - the cdf will tell me what's the total
185:57 - probability of three two one or zero
186:01 - successes all the probabilities up to
186:04 - and including that number
186:07 - now after the parentheses we do have to
186:09 - enter in the key information some
186:11 - calculators have some software that
186:13 - it'll prompt you for the information but
186:15 - if it doesn't prompt you you just need
186:18 - to know that the format
186:21 - for both of these is exactly the same
186:24 - we will use the binomial
186:28 - and i'm going to do a start because it
186:29 - could be the pdf or the c
186:31 - df both of them are the same
186:36 - then the first number you enter will be
186:38 - the number of trials
186:41 - comma
186:42 - the next will be the probability as a
186:44 - decimal
186:46 - comma
186:47 - then you'll do x or the number of
186:49 - successes
186:50 - and close the parentheses
186:55 - and that's how we can use the binomial
186:57 - distribution on the calculator and it's
186:59 - probably easiest to see
187:02 - with an example
187:05 - and according to the website
187:07 - citydata.com
187:09 - in moses lake
187:16 - seven point nine percent of workers
187:22 - carpool
187:25 - to work
187:32 - you're going to go out and you're going
187:33 - to conduct
187:38 - a sample
187:43 - of 41 workers
187:52 - first thing we want to do is identify
187:55 - what is the distribution
188:02 - for this situation
188:05 - the distribution of our x or our random
188:07 - variable
188:09 - is going to be randomly distributed as a
188:11 - binomial so we'll do a b because we're
188:14 - looking at the number of successes out
188:15 - of these 41 trials
188:19 - the first number is the number of trials
188:21 - 41 trials the second number is the
188:23 - probability of success which is 0.079
188:27 - we do need to change that percentage
188:30 - into a decimal
188:34 - so of our 41 workers
188:37 - how many
188:40 - would you
188:42 - expect
188:46 - to
188:46 - carpool
188:48 - to work
188:53 - well when we're looking at how many
188:55 - would we expect we're talking about the
188:57 - expected value
188:59 - or the
189:00 - mean the mean we said is equal to the
189:03 - number times the probability or the 41
189:06 - people you surveyed times the
189:08 - probability of 0.079
189:11 - that gives us an expected value of 3.23
189:17 - workers
189:20 - so maybe you'd round that down to three
189:22 - workers you'd expect about three maybe
189:24 - four out of your 41 workers to carpool
189:28 - to work
189:34 - let's scroll up we'll come back to the
189:36 - calculator strokes in a minute
189:40 - what is the standard deviation
189:53 - of our population
189:56 - well we have our formula for the
189:57 - standard deviation it's the square root
189:59 - of npq
190:02 - so we'll take the square root of n the
190:04 - number of trials is 41.
190:07 - p the probability is 0.079
190:11 - times q
190:12 - well q is the probability of a failure
190:15 - or the probability of someone not
190:17 - carpooling to work
190:19 - well if 7.9 percent carpool to work we
190:22 - can do 1 minus 0.079
190:25 - to get q is equal to 0.921
190:31 - so q is 0.921
190:36 - and when we multiply and take the square
190:38 - root we'll get a standard deviation
190:41 - of 1.727
190:53 - but we still haven't calculated any
190:54 - probabilities so let's do two or three
190:57 - of these
190:59 - let's say what is the probability
191:10 - exactly
191:12 - 3
191:13 - of 41
191:15 - carpool
191:17 - to work
191:23 - in other words what's the probability
191:24 - that our x our number of successes is
191:27 - exactly equal
191:29 - to three
191:33 - well because i'm looking for a specific
191:36 - exact number
191:38 - that is going to be the pdf on our
191:40 - calculator so on our calculator we're
191:42 - going to do the binomial
191:44 - pdf
191:47 - and then we do the number of trials
191:50 - the probability of .079
191:53 - comma the number of successes we want
191:56 - which is three
191:59 - let's go to the calculator and do that
192:03 - on my calculator we said the way we got
192:05 - the binomial pdf is we hit the second
192:08 - button
192:09 - and then above vars you see the word
192:11 - distribution
192:14 - now the binomial pdf is near the bottom
192:17 - so you can scroll down a bunch or if you
192:18 - scroll up once it'll take you to the
192:20 - bottom and you see options a and b are
192:23 - the binomial pdf and the binomial cdf
192:27 - i'll hit enter on the pdf
192:30 - now mine gives me the prompts so i just
192:33 - have to enter in the number of trials 41
192:37 - p the probability of .079
192:42 - the x value the number of successes i
192:44 - want which is 3
192:46 - and then i can select paste and what
192:48 - that'll do is it'll automatically type
192:50 - in the 41 comma 0.079 comma 3 for me
192:55 - if you don't have those prompts you can
192:57 - just type those numbers in with commas
192:59 - in between them the commas right above
193:01 - the number seven
193:02 - and when i hit enter it's going to give
193:05 - me the probability that i get exactly
193:07 - three successes the probability is point
193:10 - two three zero
193:12 - four and we will always round
193:15 - probabilities to four decimal places
193:18 - point two three
193:19 - zero
193:21 - four
193:29 - what is the probability
193:38 - that in my survey
193:41 - less
193:43 - than the expected value
193:49 - carpool to work
193:57 - in other words we want the probability
193:59 - that x is less than the expected value
194:01 - which we found in part b was 3.293
194:07 - well this is a discrete distribution you
194:09 - can't have .293 people saying yes they
194:13 - commute to work so what we're really
194:14 - saying is the probability that x is less
194:16 - than or equal to
194:18 - the number three
194:20 - and it's important to identify what it's
194:22 - equal to less than or equal to not just
194:25 - less than because in the cdf when we do
194:28 - the cumulative distribution we need to
194:30 - know what number to start at
194:33 - so
194:34 - now we're going to do the binomial
194:39 - cdf cumulative distribution which is
194:42 - going to add from 0 all the way up to 3.
194:46 - starting with 41 is the sample size
194:49 - 0.079
194:51 - is the probability and we're going all
194:53 - the way up to
194:55 - 3.
194:58 - we'll hit second distribution
195:02 - to the bottom but this time selecting
195:04 - the binomial cdf
195:08 - i have 41 trials
195:10 - 0.079 is my probability
195:13 - and my x value is 3
195:16 - and when i hit paste it's going to put
195:18 - those numbers in for me again if you
195:20 - don't have the prompts just put those
195:22 - numbers in separated by a comma and when
195:24 - i hit enter it's going to tell me the
195:26 - sum total of all the probabilities of 0
195:29 - 1 2 or 3 there is a
195:32 - 59.17 or a 0.5917
195:39 - probability that if i ask 41 people i
195:43 - will get three or fewer
195:48 - commuters
195:49 - sorry carpoolers
195:51 - as they commute to
195:53 - work
195:57 - let's do one last example as we wrap
196:00 - this video up we want to know what is
196:03 - the probability
196:12 - more than four carpool
196:22 - so now i'm asking for the probability
196:24 - that x is greater than four
196:30 - the problem is the cdf counts down
196:34 - so this would be a lot of work to do
196:36 - to find them individually the
196:38 - probability of 5 plus the probability of
196:40 - 6 plus the probability of 7 all the way
196:42 - up to the probability of 41 that is a
196:44 - lot of work instead what we're going to
196:47 - do is we're going to use the complement
196:50 - we're going to find out the probability
196:52 - that we're not talking about
196:54 - and subtract that probability from one
197:00 - one minus the probability that x is now
197:03 - less than or equal to
197:05 - and we have to decide what number we're
197:07 - less than or equal to to count
197:09 - everything that's not included with the
197:11 - greater than four
197:14 - the blue greater than four does not
197:16 - include the number four
197:19 - so in the complement we do want to
197:22 - include the number four
197:25 - if our probability statement had
197:26 - equality if it said or equal to four
197:29 - then we would have to do the complement
197:31 - which is the opposite and to not include
197:33 - four and so we'd start at three
197:35 - so you really have to be careful to
197:36 - decide what you're going to include
197:41 - so now on my calculator we're going to
197:42 - do 1 minus we want less than or equal to
197:45 - 4 that's the binomial
197:49 - cdf cumulative distribution function so
197:52 - it goes up 2 with the
197:55 - 41.079 probability we're going to go up
197:58 - to and include the number four
198:01 - one
198:02 - minus
198:04 - second distribution
198:08 - up to the binomial cdf
198:12 - we still have 41 trials we still have
198:14 - 0.079 but now we want the x value to be
198:16 - 4.
198:18 - paste
198:20 - and when i hit enter
198:22 - 1 minus the binomial cdf gives me a
198:25 - probability of 0.2205
198:31 - so there's a 22 percent chance that we
198:34 - would have more than 4 out of 41
198:38 - workers carpooling to work
198:42 - that's the binomial distribution the
198:44 - calculations we'll have the calculator
198:46 - do for us but we do need to know how to
198:48 - set them up how to find the mean and the
198:50 - standard deviation and interpret what
198:52 - pieces are talking about what parts of
198:55 - the binomial so take a look at a few on
198:57 - your assignment come to class ready to
198:59 - discuss it further and we'll continue to
199:01 - investigate the binomial distribution
199:12 - a second type of distribution that we
199:14 - can work with is called the poisson
199:16 - distribution it's named after a
199:18 - mathematician named poisson
199:20 - but the question that the poisson
199:22 - distribution attempts to ask is very
199:25 - similar to the binomial the binomial one
199:27 - to know the probability of x successes
199:30 - in n trials poisson also wants to know
199:34 - what is
199:35 - the probability
199:41 - of x successes
199:46 - but instead of in a certain number of
199:47 - trials it wants to know in a certain
199:54 - amount
199:56 - of time
200:00 - in other words we'll say
200:02 - some event happens on average three
200:04 - times an hour what's the probability
200:06 - this hour it's going to happen
200:09 - five times
200:12 - and to answer that question we use the
200:14 - poisson
200:16 - distribution
200:22 - and some characteristics
200:29 - of the poisson distribution is that we
200:31 - are interested
200:36 - in some number of successes
200:44 - in a fixed
200:48 - interval
200:50 - usually those intervals are a certain
200:52 - amount of time a day a week a month a
200:54 - year but it can be any fixed interval
200:56 - like an editor might be interested in
200:59 - the number of errors an author makes per
201:03 - page and so a page is the fixed interval
201:06 - that functions as the time interval but
201:09 - usually we're talking in the context of
201:11 - time
201:14 - and we also need to know
201:17 - or we will have
201:19 - the
201:20 - average
201:23 - number of successes
201:30 - in that interval
201:37 - so the editor knows that an author makes
201:39 - on average three errors per page
201:43 - the distribution itself
201:48 - we will notate the distribution very
201:50 - similarly to how we did the binomial
201:53 - we'll use the x to say that's our random
201:55 - variable
201:56 - the tilde to tell us it's distributed
201:58 - but this time because it's a poisson
202:00 - distribution we'll use p to represent
202:03 - poisson
202:05 - and the only variable we need for the
202:06 - poisson distribution is the mean or the
202:09 - average number of successes in the
202:12 - interval
202:15 - as you might expect the expected value
202:17 - is the mean that's given to us that's no
202:20 - surprise
202:21 - but the standard deviation turns out to
202:24 - just be the square root of that average
202:28 - and so these formulas can help us
202:29 - shortcut that process
202:31 - as we attempt to actually use
202:36 - the poisson
202:39 - distribution
202:43 - and just like we did with the binomial
202:46 - distribution when we're using the
202:48 - poisson distribution we will actually
202:50 - have the calculator
202:56 - find the probabilities for us we just
202:58 - need to know how to use the calculator
203:00 - to get the information we want
203:02 - very similar to the binomial we'll hit
203:04 - the second button
203:07 - and then you're going to hit the vars
203:10 - button
203:11 - because above the vars we remember it
203:13 - says distribution
203:19 - and then we need to select the correct
203:20 - distribution as there are several in the
203:23 - calculator
203:24 - again near the bottom are going to be
203:26 - the two we'll use one is called poisson
203:30 - pdf
203:33 - and just like the binomial that gives us
203:36 - the probability
203:41 - of exactly
203:44 - x successes
203:49 - and then the other one
203:52 - is the poisson
203:53 - cdf
203:56 - opening a parentheses parenthesis that's
203:58 - the one that gives us the probability
204:04 - of up to
204:07 - and including
204:11 - x
204:13 - successes
204:21 - and again similar to the binomial the
204:23 - format we will use if you don't have the
204:26 - prompts on your calculator
204:28 - will be the poisson
204:31 - either c or p they both work the same df
204:35 - and then the first number you'll enter
204:37 - in is actually the average or the mean
204:40 - over that time interval the second
204:42 - number is the x value that you're
204:45 - interested
204:46 - in
204:47 - finding
204:49 - so let's see if we can use the poisson
204:51 - distribution
204:55 - in
204:57 - an example
205:03 - let's say a certain hospital
205:10 - baby unit
205:16 - has an average
205:22 - of four births
205:26 - per week
205:32 - and we're going to randomly choose a
205:34 - week
205:35 - a week is randomly chosen
205:46 - first things first let's describe
205:49 - the distribution
205:59 - the distribution is a poisson because
206:01 - we're talking about a time
206:04 - period an interval a span of time one
206:07 - week and in that one week there is an
206:09 - average of four births
206:11 - in that week
206:14 - that is the distribution
206:17 - now we can find the expected value
206:23 - and standard deviation
206:25 - [Music]
206:29 - of the number of births the hospital has
206:32 - per week
206:34 - well the expected value that's just mu
206:37 - or the average which is given to us
206:39 - the expected value is 4.
206:43 - the standard deviation or sigma is the
206:45 - square root of the average and the
206:47 - square root of four is two
206:50 - so average of four births with a
206:53 - standard deviation of two births is what
206:55 - we would expect to happen on any given
206:58 - week in the baby unit of the hospital
207:03 - so let's find some probabilities and see
207:05 - how likely various things are to occur
207:09 - i'm always interested in how often the
207:11 - average occurs so what is the
207:13 - probability
207:20 - exactly
207:24 - four babies
207:27 - are born
207:29 - this week
207:35 - what we're asking is what is the
207:36 - probability that our random variable x
207:39 - is exactly equal
207:41 - to 4.
207:44 - now because i want my probability to be
207:46 - exactly equal to 4 what i'm interested
207:48 - in finding is the pdf for the poisson
207:52 - exactly equal to 4 in that time interval
207:56 - so we will do the poisson
208:00 - pdf probability distribution function
208:04 - the first number is the average of 4 the
208:06 - second number says we want exactly 4 to
208:09 - occur
208:10 - so let's see what the calculator says
208:12 - when we pull it up
208:16 - very similar to the binomial we'll hit
208:18 - second and distribute distributions
208:22 - scroll down to the bottom and below the
208:24 - binomial you'll see the poisson
208:26 - distributions
208:28 - the poisson's
208:29 - pdf
208:32 - and the calculator if it gives you props
208:34 - will ask for lambda that's the greek
208:36 - letter lambda is the average so for
208:39 - lambda
208:40 - the average we say is four the x value
208:43 - we're interested in is four
208:46 - and when we paste you'll see it puts it
208:48 - in the calculator for me if you don't
208:50 - have the prompts again you can just type
208:52 - in four comma four and close the
208:53 - parentheses the comma's above the seven
208:56 - and we see the probability that we get
208:58 - exactly four even though that's the
209:00 - average the probability that actually
209:02 - occurs is only about 19 and a half
209:04 - percent point one nine five
209:07 - four when we round
209:16 - so that might beg the question
209:20 - what is the probability
209:31 - of fewer
209:35 - than four births
209:39 - this week
209:44 - the average of four
209:46 - only has a less than twenty percent
209:48 - probability what's the probability that
209:50 - x is fewer or smaller than four
209:55 - well the calculator can only do exactly
209:57 - equal to a number and down so first
210:00 - let's identify that we're really looking
210:01 - for the probability that x is less than
210:03 - or equal to
210:05 - not four because four is not included
210:07 - but less than or equal to three
210:17 - because we want to be less than or equal
210:18 - to three counting all the possibilities
210:20 - up to three zero plus one plus two plus
210:23 - three
210:24 - that's going to be our cdf or our
210:26 - cumulative distribution that adds up all
210:29 - the values up to that point the poisson
210:33 - cdf
210:35 - which has an average of 4 but we want to
210:38 - know up to the number
210:40 - 3.
210:44 - so we go to our calculator and hit
210:46 - second
210:47 - distribution
210:49 - down to the bottom you see poisson cdf
210:54 - the mean is four
210:56 - we want to go up to three
210:58 - and when we paste
211:00 - types the numbers in or you can type
211:02 - them in manually if you don't have the
211:03 - prompts
211:05 - the probability that we get less than
211:06 - four is point four three three
211:10 - five
211:14 - so there's a 43 percent chance we're
211:16 - actually less than the average
211:19 - number of births
211:22 - [Music]
211:28 - let's say the hospital can only handle
211:30 - about one baby a day one birth a day
211:33 - it's a small hospital a small
211:35 - baby unit so we want to know
211:38 - what is the probability
211:48 - seven or more
211:52 - births
211:55 - occur
211:57 - this week
212:03 - what is the probability that x is
212:05 - greater than or equal to because it
212:07 - includes seven it says seven or more
212:10 - bursts this week
212:12 - now the thing about the poisson
212:14 - distribution is there's not actually any
212:16 - maximum
212:18 - in theory there could be one thousand
212:20 - births this week at this hospital
212:22 - it could go up that high now if the
212:24 - average is four the probability is
212:25 - basically zero but it could potentially
212:27 - happen
212:29 - so when we're doing greater than seven
212:30 - this is actually impossible to calculate
212:32 - straightforward because we'd have to do
212:34 - seven plus 8 plus 9 plus 10 plus 11 and
212:39 - we have to go all the way to infinity
212:41 - we can't do that
212:44 - so
212:46 - just like we did with the binomial when
212:47 - we want greater than we're going to
212:49 - actually work with the complement we're
212:51 - going to do 1 minus the probability that
212:54 - x is less than or equal to
212:56 - the opposite set of numbers
212:59 - now if we're going from 7 up this time
213:02 - it includes the number seven so when we
213:05 - count down we don't want to include
213:07 - seven so we're going to start at six
213:09 - counting
213:10 - down
213:12 - so in the calculator we'll type in one
213:14 - minus the poisson
213:16 - cdf cumulative it's going to add those
213:19 - all up
213:21 - the average is 4
213:24 - and we want to go up to 6 and then we'll
213:26 - subtract
213:28 - all of that off of 1 to get the
213:30 - complement
213:32 - so we'll type in 1 minus
213:35 - second distribution
213:38 - up to poisson cdf
213:41 - the average is still 4 but this time
213:43 - we're going up to 6.
213:47 - and when i paste it in and we do 1 minus
213:50 - that value we get a probability of
213:52 - 0.1107
213:57 - 0.1107 and so if they're not ready for
214:00 - uh seven or more births this week that's
214:03 - going to happen 11 percent of the time
214:05 - they might need to expand their baby
214:08 - unit in the hospital
214:11 - so that's the poisson distribution very
214:13 - very similar to the binomial the
214:15 - difference is now we're talking about
214:17 - the number of successes in a fixed
214:20 - interval usually a time interval
214:23 - so take a look at the homework
214:24 - assignment to practice a few of these
214:26 - and then when you come to class we'll
214:27 - continue working with poisson
214:29 - and that distribution
214:38 - we've spent several days discussing how
214:40 - to find probabilities of discrete
214:42 - distributions where the results are
214:44 - countable
214:45 - but probabilities with continuous
214:47 - distributions become a
214:49 - little bit more
214:51 - interesting
214:52 - that's going to be the question we look
214:54 - at is
214:55 - how
214:57 - do we
214:59 - find
215:01 - probabilities
215:07 - of continuous
215:11 - distributions
215:19 - because if it's continuous we've got to
215:21 - consider every possibility between 1 and
215:24 - 2 are an infinite number of decimals and
215:26 - we can't add the individual
215:28 - probabilities so instead we're going to
215:30 - steal a concept from calculus that says
215:33 - that the probability
215:39 - is simply the same thing as the area
215:44 - under
215:45 - a curve
215:50 - a really simple example of what i mean
215:52 - by this to help us visualize is
215:55 - probability distribution functions or
215:57 - density functions can be expressed as an
215:59 - equation maybe f of x equals one half
216:04 - of x that is a probability distribution
216:08 - function
216:13 - probability
216:25 - with results that are possible from
216:28 - 0 to 2.
216:34 - here's what i mean by that if we were to
216:36 - graph f of x equals one half of x
216:43 - we would know that has a y intercept of
216:45 - zero and a slope of rise 1 run 2
216:49 - and so this line
216:52 - going from
216:53 - 0 to 2 represents the probability
216:56 - density distribution function or the
216:58 - density function
216:59 - and the probabilities are just going to
217:02 - be the area
217:04 - underneath that line
217:07 - remember that if we take the the
217:09 - probability of all of our possibilities
217:11 - they have to add up to 1.
217:14 - well if i were to calculate the area
217:17 - of this triangle as it turns out to be
217:21 - we know areas for triangle is one half
217:23 - times the base times the height
217:26 - well the base is two wide
217:30 - and the height is one high
217:33 - so one half times two times one
217:36 - equals 1.00
217:39 - the total area of this triangle is 1.0
217:43 - just like the total probability of all
217:45 - the possible outcomes is 1.00
217:50 - so now we could find
217:53 - the probability that maybe our random
217:56 - variable x is less than one
217:59 - what we're really looking for
218:02 - is ones here in the middle
218:05 - if i were to shade the area less than
218:07 - one on this triangle
218:10 - we end up with this smaller triangle on
218:12 - the left side here
218:15 - well the probability that it's less than
218:17 - one is the area under the curve that is
218:20 - less than one which is still a triangle
218:24 - so the
218:25 - is the triangle area one half times the
218:27 - base well the base is
218:29 - one long
218:32 - times the height and if i were to draw
218:34 - this to scale you'd see the height was
218:36 - exactly 0.05
218:41 - and so one half times 1 times .05 we'd
218:44 - end up with an area of 0.25
218:49 - meaning there's a 25 percent probability
218:51 - that i'm between 0 and 1
218:54 - on this triangle
218:57 - now
218:59 - this
219:00 - f of x equals one half x probability
219:02 - density function i made it up it's fake
219:05 - it doesn't model anything
219:07 - but it does show us kind of how this
219:09 - idea works
219:11 - that probability is the area under the
219:15 - curve
219:16 - the curve can be any
219:19 - shape
219:23 - as long
219:25 - as the total
219:27 - area
219:29 - under it
219:35 - is one
219:37 - because that's the total probability
219:40 - that is equal to one
219:43 - so rather than dealing with a fake
219:45 - triangle let's deal with a real
219:46 - probability density function
219:50 - one that we do use is called the uniform
219:55 - distribution
220:01 - and the uniform distribution is a
220:04 - distribution where all outcomes
220:10 - are equally
220:13 - likely
220:16 - i could get any number with basically
220:20 - equal probability
220:23 - the distribution itself has a special
220:25 - notation just like the poisson and the
220:27 - binomial did
220:32 - when we have a uniform distribution we
220:35 - will say x
220:36 - tilde where x is
220:39 - uniformly distributed u
220:42 - between a and b
220:46 - where a is the low number
220:51 - and b
220:52 - is the high number
220:57 - so if we're going anywhere from 5 to 10
221:00 - with equal probability the numbers a and
221:02 - b would be 5
221:04 - and 10.
221:08 - now the curve
221:10 - that we want to be underneath
221:13 - for a uniform is f of x equals the
221:16 - reciprocal
221:17 - of the difference or one over b minus a
221:22 - and when we do that what we'll end up
221:24 - with
221:26 - is a rectangle
221:29 - that goes from a to b
221:31 - where they're all
221:32 - equally likely
221:35 - to occur
221:37 - and that height
221:38 - on that line is how likely it's to occur
221:41 - that one over b minus a
221:46 - if i were to find the area
221:48 - of this rectangle
221:50 - because it's a rectangle
221:52 - area is base times height
221:55 - so the area is the base the distance
221:58 - between the two
222:00 - we have to subtract them is b minus a
222:04 - and the height
222:05 - that's the green height that we just
222:07 - calculated is one over b minus a
222:11 - and those b minus a's divide out so the
222:13 - total area
222:15 - is 1.0
222:18 - so the curve the height of the rectangle
222:21 - for a uniform distribution is the
222:23 - reciprocal
222:24 - of the difference 1 over b minus a
222:29 - now just like the discrete distributions
222:31 - we can find the mean or the expected
222:33 - value
222:34 - and the formula for that with the
222:36 - uniform is just a plus b divided by 2
222:39 - or the average of the extremes that's
222:41 - going to stick us right in the middle
222:44 - to find the mean
222:48 - turns out the standard deviation
222:51 - is equal to the square root of b minus a
222:55 - squared divided by 12.
222:58 - and it's always divided by 12 regardless
223:00 - of the numbers just works out that way
223:04 - so let's try an example of this and see
223:08 - if we can see this uniform distribution
223:11 - work out
223:21 - for example
223:25 - a plumber
223:29 - estimates
223:35 - that service calls
223:42 - are uniformly distributed
223:57 - between half an hour or 0.5 hours
224:02 - and eight hours
224:09 - first let's describe the distribution
224:18 - we said our random variable x was going
224:20 - to be distributed uniformly
224:24 - the smallest number possible is 0.5
224:27 - the largest number possible is eight so
224:29 - our distribution is x tilde u
224:32 - point five
224:34 - eight
224:38 - we can easily then calculate the mean
224:44 - or expected value and standard deviation
224:51 - using the formulas from the uniform
224:54 - distribution
224:56 - the mean is the average 0.5 plus 8
225:00 - divided by 2
225:02 - which is 8.5 divided by 2 which is
225:07 - 4.25 hours
225:10 - so this plumber's average service call
225:12 - is about four and a quarter hours four
225:14 - hours fifteen minutes
225:17 - the standard deviation on those service
225:19 - calls is the square root of b minus a
225:22 - eight minus point five
225:24 - squared
225:25 - divided by 12.
225:32 - when we put that into our calculator we
225:34 - should end up with
225:36 - a standard deviation
225:38 - of 2.165
225:45 - hours
225:49 - so now that we know his average call
225:51 - and or service call and and standard
225:54 - deviation let's actually calculate
225:56 - some probabilities and what you'll find
225:59 - with calculating probabilities on
226:02 - continuous distributions
226:04 - it's always easier
226:07 - to draw a picture
226:09 - of the situation
226:12 - so if we want to find the probability
226:18 - a call
226:23 - takes less
226:27 - than three hours
226:33 - we're going to draw a picture
226:35 - of the probability
226:37 - it's a uniform distribution so we know
226:39 - it's a rectangle
226:40 - from a low of 0.5 all the way up to 8.
226:45 - the height
226:47 - is that f of x equals one over b minus a
226:52 - or one over eight minus point five
226:57 - one over seven point five
227:00 - and we can leave that decimal in there
227:02 - that's okay
227:05 - now it's asking for us to find the
227:07 - probability that we're actually less
227:09 - than three
227:11 - remember probability is area so if i
227:13 - mark on my graph approximately where
227:16 - three is and we want to be less than 3
227:19 - it's going to be the area of this
227:21 - rectangle
227:22 - off to the left
227:25 - so the probability that x is less than 3
227:30 - is the area of the rectangle base times
227:32 - height
227:34 - the base is the distance from three to
227:36 - point five
227:38 - or three minus point five
227:42 - and the height is what we just found out
227:44 - the f of x
227:45 - the 1 over
227:47 - 7.5
227:51 - and when i put this into my calculator 3
227:53 - minus 0.5 in parentheses
227:56 - times 1 over 7.5 we end up with point
227:59 - three three three
228:01 - three
228:03 - or about a one-third probability that
228:05 - the service call takes less
228:07 - than three hours
228:14 - let's try another example
228:16 - let's find the probability
228:22 - a call
228:26 - takes between
228:30 - two
228:31 - and four
228:34 - hours
228:38 - again probabilities are always easier
228:41 - if you draw a picture
228:43 - so here's our uniform probability it
228:45 - doesn't have to be to scale but it does
228:47 - show the lowest point five
228:49 - the highest point eight
228:51 - the height is still one over seven point
228:53 - five
228:54 - but now i want to be between
228:58 - two and four hours
229:00 - so between two
229:01 - and four hours we're going to shade that
229:04 - area
229:05 - in between
229:08 - we're looking for the probability that
229:10 - two is less than x which is less than
229:13 - four
229:14 - which is just this rectangle
229:17 - the rectangle is base times height
229:21 - the base is the space from two to 4
229:24 - or 4 minus 2
229:27 - times the height which is 1 over
229:29 - 7.5
229:34 - putting that in my calculator 4 minus 2
229:37 - is 2 divided by 7.5
229:40 - we get an area of
229:42 - 0.2667
229:45 - when we round
229:48 - so there's about a 26 and two-thirds
229:51 - percent probability
229:53 - that a call will take between
229:56 - two and four hours
230:01 - we can even find conditional
230:03 - probabilities in much the same way let's
230:06 - find the probability
230:11 - a call
230:14 - takes more than five hours
230:23 - given
230:25 - it was less
230:30 - than seven hours
230:40 - again we're going to draw a picture
230:44 - going from 0.5 to 8.
230:50 - with a height of 1 over 7.5
230:56 - but now i want to be more than 5
231:01 - given it was less than seven
231:05 - so we don't really care about this right
231:06 - side we just want to be less than seven
231:10 - and we want to know what's the
231:11 - probability that i'm
231:14 - more than five
231:16 - given that i'm less than 7.
231:19 - the probability that x is more than 5
231:23 - given x is
231:25 - less
231:27 - than 7.
231:31 - well with a given probability we know we
231:34 - look at the probability of both
231:37 - divided by the probability of the given
231:39 - information
231:41 - so the probability of both would be
231:43 - between 5 and 7.
231:45 - so between 5 and 7 has a base of 7 minus
231:49 - 5
231:51 - times a height of 1 over 7.5
231:55 - that's the both and then we divide by
231:57 - the probability of the given information
232:02 - the given information is that we're less
232:04 - than seven
232:05 - so that probability's got a base going
232:07 - all the way down
232:09 - of seven minus 0.5
232:12 - times a height of 1 over 7.5
232:17 - which is kind of nice as often occurs
232:19 - with given probabilities part of it will
232:21 - divide out and so we're left with 7
232:24 - minus five is two
232:27 - over seven minus point five is six point
232:29 - five
232:31 - and two divided by six point five is
232:34 - point three zero
232:36 - seven
232:38 - seven
232:39 - so there's just over a thirty percent
232:41 - probability the call took more than five
232:44 - hours
232:45 - given we knew it was less
232:47 - than seven hours
232:54 - another concept that we haven't spent
232:56 - much time with is the idea of what's
232:58 - called a percentile
233:02 - a percentile
233:06 - is the
233:10 - is the value
233:18 - where a certain percent
233:24 - is below
233:27 - the value
233:31 - you often see this with standardized
233:32 - test scores if you took a test and you
233:34 - scored in the 80th percentile that means
233:37 - your score was better than 80 percent of
233:41 - the participants
233:44 - so we could find for our example
233:47 - uh
233:48 - let's find
233:50 - the 80th
233:52 - percentile
233:55 - or what value has 80 percent below it
234:00 - let's draw a picture
234:03 - same picture it's a uniform distribution
234:05 - so it's just a big rectangle
234:07 - the height is still 1 over 7.5 and we're
234:10 - going from 0.5 to 8.
234:14 - but we want to know what value let's
234:16 - call it k for now
234:19 - will give us an area
234:23 - that's point eighty
234:25 - below it eighty percent is below it what
234:28 - value gives us that
234:33 - well to get there we're going to use our
234:35 - area formula the fact that we know that
234:38 - area of a rectangle is base
234:41 - times height
234:43 - the difference is this time
234:45 - we know the area
234:47 - we know the area
234:49 - is .80
234:52 - the base is the distance from k
234:55 - down to 0.5
234:58 - so we'll have k minus 0.5 is the base
235:02 - and the height we know is 1 over
235:05 - 7.5
235:09 - all we really need to do now is solve
235:11 - this equation
235:12 - 4k and that solution will be
235:16 - our 80th percentile the value that has
235:19 - 80 percent of service calls below it
235:23 - first we can get rid of the fraction by
235:25 - multiplying both sides of the equation
235:27 - by 7.5
235:29 - that's going to give us 6 equals k minus
235:32 - 0.5
235:34 - add 0.5 to both sides and k
235:38 - is equal
235:40 - to 6.5
235:44 - this means that 80
235:46 - of service calls
235:51 - are less
235:55 - then 6.5 hours
236:00 - that's what it means to be the 80th
236:02 - percentile
236:06 - now as we wrap up there's one more
236:08 - example i want to show you it's kind of
236:09 - a special case
236:11 - and it seems counter intuitive
236:14 - we want to find
236:16 - what is
236:18 - the probability
236:24 - a call
236:28 - takes exactly
236:31 - five hours
236:36 - and the key word here is exactly
236:39 - because that means something very
236:41 - specific in probability it doesn't mean
236:43 - between five hours and five hours zero
236:46 - minutes and one second it means at the
236:48 - exact moment of five hours
236:53 - let's draw a picture to represent the
236:55 - exact moment of five hours
236:58 - going from point five to eight
237:01 - and a height of one over seven point
237:03 - five
237:04 - the exact moment of five hours
237:07 - occurs somewhere in the middle
237:12 - well the probability that x equals
237:15 - exactly five
237:18 - is going to be whatever the base is
237:20 - times the height
237:22 - the problem is the base of that
237:24 - rectangle since it's just a line right
237:27 - at 5 hours with no width
237:29 - the base goes from 5 to 5
237:32 - with a height of one over seven point
237:34 - five
237:36 - but five minus five is zero
237:39 - and anything times zero is
237:42 - zero
237:44 - the probability that anything happens at
237:47 - exactly a specific
237:49 - moment
237:50 - in a continuous distribution
237:53 - is always equal to zero nothing ever
237:56 - happens at an exact moment that always
237:58 - happens over a span of time it could be
238:02 - between five hours and five hours in one
238:05 - second
238:06 - but that's a span of time at exactly a
238:09 - specific number that will never occur
238:12 - in a continuous distribution
238:17 - seems like a paradox
238:19 - but it's the way it works out
238:21 - we're focusing though today on finding
238:23 - probabilities off the uniform
238:25 - distribution and this idea with
238:27 - continuous distributions that
238:29 - probability is area under the curve so
238:33 - take a look at the assignment to
238:34 - practice a few of these and we will see
238:36 - you in class to continue working with
238:38 - the uniform distribution
238:47 - now that we've gotten familiar with
238:48 - continuous probability distributions
238:50 - we're going to move on to take a look at
238:52 - the most important continuous
238:54 - distribution
238:56 - in all of statistics and that is the
238:58 - normal distribution so the question
239:01 - today
239:02 - is simply what is
239:06 - the normal
239:09 - distribution
239:19 - [Music]
239:20 - the normal distribution
239:30 - is a probability density function
239:32 - that's got this beautiful
239:35 - equation
239:37 - of 1 over the standard deviation
239:40 - times the square root of 2 pi
239:44 - times e to the exponent of negative
239:47 - one-half
239:48 - times x minus the mean
239:51 - divided by the standard deviation
239:53 - squared
239:56 - now what's nice about the normal
239:57 - distribution is you do not need to know
239:59 - that formula
240:01 - instead we're going to cheat and use a
240:03 - table to help us actually calculate what
240:05 - that formula is going to be equal to at
240:07 - various points
240:09 - but what you should know about the
240:10 - normal distribution is the shape of the
240:13 - normal distribution
240:16 - just like the uniform distribution is a
240:18 - clear shape of the rectangle the normal
240:20 - distribution has a clear
240:22 - bell shape
240:26 - to the curve
240:28 - the normal distribution if this is my
240:30 - x-axis
240:32 - is a bell-shaped curve
240:35 - where the mean of the distribution falls
240:38 - right in the middle
240:44 - and we describe the distribution
240:51 - as x
240:52 - tilde in
240:54 - for normal
240:56 - and then we'll just state the mean
240:58 - and the standard deviation
241:00 - for the two
241:02 - arguments of the function
241:06 - now the normal distribution has slightly
241:08 - different shapes based on the standard
241:10 - deviation the smaller the standard
241:12 - deviation the taller and skinnier it is
241:14 - the larger the standard deviation the
241:16 - shorter and fatter it is
241:18 - but there's one special distribution
241:22 - which we call the standard
241:26 - normal
241:28 - distribution
241:31 - and when we're dealing with the standard
241:33 - normal we won't use x we'll use z so we
241:36 - know we're talking about the standard
241:38 - normal distribution which always has a
241:41 - mean of 0
241:43 - and a standard deviation of one
241:47 - and what's nice about the standard
241:49 - normal distribution it has
241:52 - a table
241:56 - to
241:58 - help us find
242:02 - areas
242:07 - this is what the table looks like
242:12 - the table gives us values
242:14 - for
242:15 - z
242:18 - going down the first column you see the
242:20 - first two digits maybe 1.2
242:23 - and then if i wanted 1.23
242:26 - i'd go to the 3 on the next and where
242:29 - those 2 overlapped
242:32 - at the 0.303907
242:34 - i can get my
242:35 - area under the curve based on those z
242:39 - values and we'll look more at using the
242:41 - table here in just a minute
242:43 - but what we need to know for now is
242:45 - there's a table to help us find
242:47 - areas
242:49 - and what we need to do quite often
242:52 - is we change
242:57 - between
243:00 - a regular normal curve
243:04 - which uses x values
243:08 - and the standard normal curve
243:12 - which uses z values
243:16 - so that we can use the table in order to
243:19 - find the
243:21 - areas
243:26 - the way we make that change is we use
243:28 - one of two equations we either use z
243:32 - equals the x value minus the mean
243:35 - divided by the standard deviation
243:41 - or if we solve for x in that same
243:44 - equation we end up with the mean
243:46 - plus the z value
243:48 - times the standard deviation
243:55 - and with these two definitions or these
243:57 - two formulas it's important that we keep
243:59 - track of what's an x and what's a z
244:02 - x
244:02 - has meaning in context
244:13 - x might be the height of the average
244:15 - person and so x we're looking at a 64
244:19 - inch person
244:21 - z does not have context or meaning
244:24 - z is simply
244:27 - the number
244:30 - of standard deviations
244:37 - we are from the mean
244:44 - and once we have a z value
244:46 - then we're able to go to the standard
244:49 - normal
244:51 - to find areas off the table
244:55 - the table
244:57 - gives
244:58 - the area
245:02 - between
245:04 - a z value
245:08 - and the mean
245:12 - and of course the mean is zero
245:17 - in other words if i've got this
245:19 - standard normal curve here
245:23 - the mean is always in the middle of zero
245:27 - off to the side we've got a z value
245:30 - the table
245:32 - gives the area
245:36 - between
245:39 - that z value and the mean of zero
245:44 - so if we want to find
245:48 - probabilities
245:53 - [Music]
245:54 - let's scroll up a bit and give us a
245:56 - little bit more room
245:57 - if we want to find probabilities off
245:59 - this table we'll have to decide what
246:01 - pieces we're interested in
246:04 - first thing that we'll use is the fact
246:06 - that the curve
246:10 - is symmetric
246:15 - in other words the z table does not have
246:17 - any negative values on it
246:20 - fortunately the negative values behave
246:22 - like the positive values because the
246:23 - curve is symmetric
246:28 - each half
246:30 - the left and right side
246:34 - has an area
246:38 - of 0.5
246:44 - and we use this one less often but the
246:46 - total area
246:51 - is
246:52 - 1.0
246:54 - because it's a probability
246:57 - so let's see if we can figure out how to
247:00 - use this information
247:02 - and use our table in order to calculate
247:04 - probabilities under the standard normal
247:08 - curve
247:11 - let's do some examples
247:20 - for this example if you look up
247:22 - according to google
247:24 - the average
247:27 - act score
247:32 - is 20.8
247:38 - with a standard deviation
247:47 - of 4.8
247:52 - let's do some examples off of this
247:54 - information
247:56 - first let's describe the distribution
248:04 - for the distribution our variable x
248:08 - is normally distributed
248:11 - the mean
248:13 - is 20.8
248:15 - and the standard deviation
248:19 - is 4.8
248:23 - let's find out the probability
248:29 - a student
248:34 - scores
248:37 - higher
248:41 - than 30.
248:47 - now with all of these probability
248:48 - problems it will always be easier to
248:52 - draw a picture first
248:56 - so we'll draw a picture of our normal
248:57 - curve our little bell-shaped curve
249:02 - we'll put the mean right in the middle
249:04 - the mean is an x value right now
249:07 - so i'm going to label my first row we're
249:09 - going to put x values on it and then in
249:11 - the second row we'll label what those
249:12 - equivalent z values are when we change
249:15 - them from x's disease
249:16 - that way we don't get them mixed up
249:19 - so the mean for our x value is 20.8
249:24 - we want a student to score higher than
249:26 - 30 30 is off to the right
249:31 - and we want the area that's higher than
249:34 - 30 because the area
249:36 - is
249:37 - the probability
249:41 - well let's change those x values into
249:43 - z's we know the mean changes to zero
249:48 - but the 30 we need to use our formula
249:50 - for z
249:52 - z says take the x value subtract the
249:55 - mean and divide by the standard
249:57 - deviation
249:59 - we have an x value of 30
250:02 - minus 20.8
250:05 - divided by the standard deviation of 4.8
250:09 - we end up with a z value of 1.92
250:13 - so 30 changes into a z value of 1.92
250:18 - we will always round our z values to two
250:21 - decimal digits because that's going to
250:24 - match our standard normal table
250:28 - so on our table we need to find 1.92
250:33 - let's scroll down a bit to help us see
250:35 - it a little better
250:37 - 1.9
250:39 - going down the column and then we want
250:42 - 1.92
250:44 - and so when we combine those
250:48 - we end up with them overlapping
250:51 - at .4726
250:56 - that .4726
251:01 - is the
251:04 - area between my z value
251:08 - and the mean
251:10 - it's the white area kind of in the
251:12 - center
251:14 - we don't want the white area in the
251:15 - center we want the tail off to the right
251:18 - of it
251:20 - and this is where we use what we know
251:22 - about the normal distribution
251:25 - we know the entire right side
251:28 - is 0.5
251:31 - so the probability that some student
251:33 - scores higher than 30
251:36 - is going to be equal to the 0.5
251:39 - minus that white area that's been cut
251:41 - out of 0.4726
251:46 - which leaves us with .0274
251:51 - there's just shy of a three percent
251:53 - chance that a random student will have
251:55 - scored higher
251:58 - than 30.
252:01 - let's try a few more of these so we can
252:03 - get really good at finding probabilities
252:05 - on this important
252:07 - normal distribution
252:10 - round number three
252:13 - let's find the probability
252:19 - a student
252:22 - scores
252:24 - less
252:27 - than 25.
252:36 - again we'll draw a picture
252:41 - the mean right in the middle of 20.8
252:44 - that's an x value
252:46 - we want to be less than 25 which is
252:49 - somewhere over here to the right
252:51 - and we want to be less than so we want
252:53 - the smaller part or the left side
252:57 - all of that area
253:02 - in order to do that we need to calculate
253:04 - our z values
253:06 - we already know the mean will have a z
253:08 - value of zero
253:10 - what we don't know is the z value of the
253:12 - 25
253:14 - so we'll subtract the mean of 20.8
253:18 - we'll divide by the standard deviation
253:20 - of 4.8
253:23 - and we get point 88
253:27 - so now we have a z value of point 88
253:30 - that corresponds with the x value of 25
253:35 - 0.88 is what we're going to look up in
253:38 - our table
253:40 - in the table we're looking for 0.88
253:43 - so 0.88
253:48 - when we go down and across we find out
253:51 - the probability there is 0.3106
253:57 - 0.3106
254:01 - and that again is the area
254:04 - 0.3106
254:06 - the area between the mean
254:08 - and that z value of 0.88
254:13 - but this time we also want
254:16 - not just that little bit but the whole
254:17 - area to the side
254:20 - that left side we know is 0.5
254:23 - so when we want the probability that x
254:26 - is less than 25
254:28 - this time we need an extra 0.5
254:31 - added to the 0.3106
254:35 - that we just found to get 0.8106
254:41 - or just over 81 percent of students
254:43 - score less than a 25
254:47 - on the
254:48 - act so you can see how the picture helps
254:51 - in the difference between example two
254:53 - and example three
254:55 - example two we had to subtract from
254:56 - point five to get the area that we
254:58 - wanted
254:59 - example three we had to add to point
255:01 - five to get the area we wanted
255:05 - let's try another example to keep
255:06 - working on how the picture is going to
255:08 - help us calculate
255:10 - our probability
255:13 - let's find the probability
255:19 - a student
255:23 - scores
255:26 - between
255:30 - 15
255:33 - and 23
255:35 - on their a ct
255:41 - so we draw a picture
255:44 - the mean in the middle has an x value of
255:47 - 20.8
255:49 - we want to be between 15 and 23.
255:52 - 15 is off to the left and 23 is off to
255:56 - the right
255:58 - we want the area between these two
256:01 - numbers
256:06 - so now when we convert to a z value we
256:08 - don't have to just change the mean
256:10 - to zero and one other value we have two
256:13 - other values that we need to convert to
256:16 - z values
256:18 - so let's do that
256:21 - for the 15 we take 15 minus the mean of
256:24 - 20.8 divided by the standard deviation
256:26 - of 4.8
256:29 - that's going to give us a negative
256:30 - number negative 1.21
256:34 - but that's okay because it's to the left
256:36 - of zero it makes sense that a number to
256:38 - the left of zero on the number line
256:40 - is negative that's what a negative z
256:43 - value is it just means we're to the left
256:45 - or smaller than the mean
256:48 - when we do the 23 we should get a
256:50 - positive number 23 divided by 0.8
256:53 - divided by 4.8
256:56 - and it's going to be positive because
256:57 - it's bigger than the mean
256:59 - this turns out to be 0.46 a z value of
257:02 - 0.46
257:06 - we're going to look up both of these
257:08 - values
257:09 - in the normal table
257:13 - first looking up the 1.21 it's negative
257:16 - but that's okay
257:18 - because the curve is symmetrical
257:20 - so we'll just look up the positive
257:22 - version and it's going to have the same
257:24 - area on the other side 1.21
257:28 - 1.21
257:31 - so when we do that we end up with this
257:34 - center area
257:36 - of 0.3869
257:43 - 0.3869 is the area between the mean and
257:46 - that z value
257:48 - 0.3869
257:53 - we still need to look up the z value of
257:54 - point forty six
257:58 - so i'll do it in blue here
257:59 - point forty
258:01 - six
258:03 - when i go across we end up with
258:06 - an area of point seventeen
258:09 - seventy
258:10 - two
258:14 - so the area there is point seventeen
258:18 - seventy-two
258:21 - we want the area between those two
258:23 - numbers which includes both halves
258:28 - so when i want to find the probability
258:30 - that 15 is less than our score which is
258:32 - less than 23
258:34 - we need to combine both those pieces
258:36 - together
258:38 - 0.3869
258:40 - plus 0.1772
258:43 - will give us a total area of 0.564
258:48 - one
258:50 - there's just over a fifty six percent
258:52 - chance that a student will score
258:54 - between
258:55 - fifteen
258:57 - and twenty three
259:00 - so sometimes you see we have to add
259:02 - pieces together
259:04 - but that's not always the case either
259:07 - let's look at this example
259:08 - let's find the probability
259:13 - a student
259:18 - just one student
259:22 - scores
259:26 - between
259:27 - 18
259:29 - and 20.
259:35 - now if we draw this picture
259:41 - with our mean in the center of 20.8
259:44 - that's an x value
259:46 - but now you notice 15 18 and 20 18 and
259:50 - 20 are both smaller
259:53 - than our mean
259:56 - and we want the area between them
260:01 - of course to get that area we have to
260:02 - change them to a z value
260:05 - the mean has a z value of zero but we
260:07 - have to work
260:08 - to get the other two points
260:11 - so first for the 18 18 minus the mean of
260:14 - 20.8 divided by the standard deviation
260:17 - of 4.8
260:20 - that's going to be equal to negative
260:22 - point 58
260:23 - so the z value there is negative point
260:26 - 58.
260:29 - for the second one
260:31 - the 20
260:33 - z is equal to 20 minus 20.8 divided by
260:36 - 4.8
260:38 - is equal to negative point seventeen
260:43 - negative point seventeen
260:45 - and we want the area
260:47 - between those
260:51 - going to our normal distribution
260:55 - our first value was negative point five
260:57 - eight point five
260:59 - eight so we'll go over and down
261:02 - and we find our first area is point two
261:05 - one
261:06 - nine
261:07 - zero point two one
261:09 - nine
261:10 - zero
261:12 - so this first one point two one
261:16 - nine zero
261:18 - now it's important to note that's not
261:19 - just the shaded region that goes all the
261:22 - way to the main
261:28 - it doesn't stop at the 20. the 2190 goes
261:30 - all the way to the main does not stop at
261:33 - the 20.
261:34 - we still need to find the other piece
261:38 - which is the negative point seventeen
261:41 - doing this one in blue the negative
261:43 - point one
261:45 - seven
261:47 - going down and across we find an area of
261:50 - 0.675
261:56 - so that's area of 0.6750675
262:03 - sorry forgot the 0.0675
262:08 - and now we're ready to answer the
262:09 - question what is the probability that 18
262:12 - is less than our score which is less
262:14 - than 20.
262:18 - well the 20.90 goes all the way to the
262:21 - mean
262:22 - but we don't want the white space
262:25 - the 0.675 that goes on the right side of
262:28 - the main
262:30 - so we're going to cut out that white
262:31 - space we just need to subtract
262:34 - we have 0.2190
262:37 - minus 0.0675
262:41 - that's going to be 0.1515
262:46 - just over a 15 probability a student
262:49 - will score
262:50 - between 18
262:52 - and 20.
262:58 - let's do one last example
263:01 - but let's make this one a little
263:03 - different
263:05 - this time we're going to find
263:09 - the third
263:11 - quartile
263:15 - of act scores
263:21 - remember the third quartile that's the
263:24 - value that's over 75 percent what we're
263:27 - really asking to find is the 75th
263:30 - percentile
263:37 - this is different than we were doing
263:38 - before we're not saying what's the
263:40 - probability of this number we have the
263:43 - probability
263:45 - we have the probability
263:51 - 0.75
263:54 - we're looking for the x and the z values
263:56 - that give us 0.75
264:02 - so let's draw our picture
264:05 - we've got our mean of 20.8 that's an x
264:09 - when it's a z the mean is zero
264:12 - and we're looking for some value out
264:14 - here we'll call it k
264:21 - some value out there where the area
264:23 - below is a total of 75 percent
264:29 - well the table is only going to give us
264:33 - the space between k
264:35 - and the mean
264:37 - and actually let's label k down below
264:39 - we're going to make k a z value
264:42 - we'll use that k to find the x value
264:47 - we know the left side of the curve is 50
264:50 - or 0.5 so the right side of the curve
264:53 - must be the remaining 0.25 of an area
264:58 - but we know the area of 0.25
265:02 - when we're given the area do not go down
265:05 - the z values z's are not areas z's are a
265:09 - scale of the number of standard
265:10 - deviations we are from the mean
265:13 - we need to look inside the body of the
265:16 - table for the area that we're given
265:20 - we want 0.25
265:23 - and if we kind of scan through our
265:25 - numbers
265:26 - you'll see 0.25 an area of 0.25 is right
265:30 - in between
265:31 - these two numbers between 24 86 and 25
265:35 - 17.
265:37 - now if it was closer to one i'd go with
265:38 - the one that's closer but it's really
265:41 - right in the middle
265:43 - so we'll call right in the middle 0.6
265:47 - right in between 7 8 so we're going to
265:49 - call that 0.675
265:52 - 0.675
265:56 - so that k value
265:58 - is 0.675
266:04 - how do we find the x value then
266:09 - we have that other formula we haven't
266:10 - used yet that says
266:12 - x
266:14 - is equal to the mean
266:16 - plus the number of standard deviations
266:19 - times the standard deviation
266:24 - and that z
266:26 - is that z value we just found of 0.675
266:30 - so if we plug in what we know
266:33 - the score x that's the 75th percentile
266:36 - or the third quartile is the mean of
266:38 - 20.8
266:40 - plus
266:41 - 0.675
266:43 - times the standard deviation of 4.8
266:48 - the mean is 24 i'm sorry not the mean
266:51 - but the x value
266:52 - 24.04
266:57 - the third quartile of act scores is
266:59 - about 24.
267:01 - that means
267:02 - a score of 24 is better than about 75
267:06 - percent of all of the scores
267:09 - on the act
267:12 - the normal distribution is truly the
267:15 - most important distribution you know how
267:17 - to use in all of probability you need to
267:20 - know how to use the table
267:21 - how to find the left side the right side
267:24 - what's in the tail what's in the middle
267:26 - how do you find percentiles how do you
267:28 - use the table backwards you need to be
267:30 - very comfortable and familiar with the
267:32 - normal distribution and how it works
267:35 - as we move forward in our study of
267:37 - probability so take a look at the
267:39 - homework assignment practice a few of
267:42 - these important problems in class we're
267:44 - going to keep working with the normal
267:45 - distribution and i will look forward to
267:48 - seeing you then
267:56 - now that we've gotten really comfortable
267:58 - at working with the
268:00 - normal distribution and finding
268:01 - probabilities we're ready to actually
268:03 - get into working with
268:05 - samples
268:07 - now the heart of statistics is
268:09 - collecting a sample to make an estimate
268:11 - or a conclusion about a population
268:14 - so first we need to know how do we find
268:17 - probabilities
268:29 - with sample means
268:37 - the big difference here is in the
268:39 - previous chapter in chapter 2 we were
268:41 - finding the probability for one
268:45 - individual value
268:47 - now we're going to take several values
268:49 - find the mean and look at the
268:51 - probability of the mean of several
268:55 - values and this is what gives rise to
268:58 - what is called the central limit theorem
269:08 - and the central limit theorem basically
269:10 - in words says that the mean of a sample
269:21 - should be
269:26 - close
269:30 - to the mean
269:34 - of a population
269:40 - and not only that it should have a
269:42 - smaller
269:47 - standard deviation
269:55 - the idea is that if we have several
269:58 - values averaged together the extreme
270:00 - values are going to be averaged out and
270:02 - pulled back in towards the center which
270:04 - makes the standard deviation smaller
270:08 - in fact we can go one step further and
270:10 - say that the larger
270:13 - the sample
270:17 - the closer
270:23 - to the mean
270:27 - we become
270:28 - and the smaller
270:33 - the standard deviation is
270:42 - and that makes sense if i interview
270:44 - nearly everybody i will be probably
270:46 - pretty close to the actual mean
270:48 - i'm not going to be off by much which is
270:50 - why the standard deviation is going to
270:52 - be so small
270:54 - and as we're working with samples this
270:57 - idea of the standard deviation or the
270:59 - smaller standard deviation we call the
271:02 - standard
271:06 - error
271:09 - the standard error will use the symbol
271:11 - of either sigma sub x bar
271:13 - or you'll often see s sub e for standard
271:17 - error
271:19 - and that's the standard deviation
271:30 - of the sample means
271:37 - and the way we calculate the standard
271:39 - error or the standard deviation of the
271:41 - sample means
271:43 - is we will take the standard deviation
271:45 - of the entire population
271:48 - and we'll divide by the square root of
271:50 - the sample size
271:53 - or if it's a sample we'll say s for the
271:55 - sample
271:56 - standard deviation
271:58 - divided by the square root of the sample
272:00 - size
272:03 - and that's a key
272:05 - equation that we're going to use quite a
272:07 - bit today
272:10 - using this new standard error we can
272:12 - replace the standard deviation in our
272:14 - distribution
272:19 - of
272:21 - the mean
272:24 - when we're talking about means we're
272:26 - going to say that means are normally
272:28 - distributed
272:29 - with the same mean as the whole
272:31 - population
272:34 - but then we will use the standard error
272:36 - or s divided by the square root of n
272:39 - to represent our new
272:41 - standard deviation
272:47 - then we can go forward and calculate
272:49 - z-scores
272:52 - and also
272:54 - probabilities in much the same way we
272:56 - did before
272:58 - now the z-score is equal to
273:00 - the mean of our sample
273:03 - minus the overall mean
273:05 - divided by the standard
273:08 - error
273:10 - and that's going to be the key new thing
273:12 - the central limit theorem gives us is
273:15 - that new standard error as we calculate
273:18 - our z values because we have a sample
273:22 - not just one
273:24 - value
273:26 - so let's look at an example where a
273:28 - sample is done
273:33 - a cell phone company
273:41 - finds
273:43 - that those
273:47 - who go over
273:51 - their data limit
273:56 - go over
274:00 - by an average
274:05 - of 2.2
274:07 - gigabytes
274:10 - with a standard
274:14 - deviation
274:20 - of 0.4 gigabytes
274:29 - you conduct
274:32 - a survey
274:36 - of 80 customers
274:43 - first thing we want to know is we want
274:45 - to know what's the probability
274:50 - the average
274:55 - overage
275:00 - is above
275:04 - 2.3
275:06 - gigabytes
275:11 - or what's the probability that x is
275:14 - greater than 2.3 actually x bar is
275:16 - greater than 2.3 that the average is
275:19 - more than 2.3
275:22 - well the first thing we're going to need
275:24 - to do here is we're going to figure out
275:27 - what is the distribution of the mean
275:31 - the mean should has the same mean as the
275:34 - population
275:36 - 2.2 gigabytes
275:38 - but the standard deviation is
275:40 - smaller
275:42 - we take the 0.4 gigabytes and we have to
275:44 - divide by the square root of the sample
275:47 - size
275:48 - divide by the square root of 80.
275:51 - so we have 2.2 comma 0.4 divided by the
275:55 - square root of 80 is about point zero
275:58 - four
275:59 - five
276:02 - that
276:03 - .045
276:05 - that is our new standard
276:08 - error
276:11 - so when we calculate our z-score
276:14 - we remember that z is x
276:17 - bar minus mu divided by the standard
276:20 - error
276:24 - our x bar we want to be greater than 2.3
276:28 - so we'll take 2.3
276:31 - we'll subtract the average of the
276:33 - population
276:36 - divided by our standard error
276:39 - because we have a sample not an
276:40 - individual of 0.045
276:46 - and when we divide we get 2.22
276:56 - so if we think about our normal
276:57 - distribution
277:01 - the mean of the populations at 2.2
277:08 - we these are x values
277:10 - we want to be at 2.3 or bigger
277:18 - so we standardized into z values
277:23 - and the z value actually turned out to
277:24 - be 2.22
277:28 - so that's what we're going to look at in
277:30 - our standard normal table
277:34 - in our standard normal table
277:36 - we've got 2.2
277:38 - and another 2.
277:42 - so we see the probability there is
277:45 - 0.4868
277:49 - but remember that is always the area
277:53 - between the z value
277:57 - and the mean
278:01 - we want the area
278:04 - in the tail
278:08 - so the probability that the mean is
278:10 - greater than 2.3
278:14 - we know the entire right side is 0.5
278:18 - subtract off the middle of 0.4868
278:23 - and we get .0132
278:29 - there's about a one and a third percent
278:31 - probability
278:33 - that if i interview 80 customers i'll
278:36 - get a mean bigger
278:38 - than 2.3
278:42 - and that's the idea of the central limit
278:44 - theorem we're shrinking the standard
278:46 - deviation
278:48 - by dividing it by the square root of the
278:50 - sample size
278:53 - whenever we have a sample we need to
278:55 - divide by the square root of the sample
278:58 - size
279:01 - let's keep with this example one more i
279:03 - also want to see if we can find
279:06 - q1
279:08 - or the 25th percentile
279:17 - so same problem with the cell phones
279:21 - where we've got these x values the mean
279:23 - x value is 2.2 gigs
279:26 - we want to find
279:29 - the x value where 25 percent
279:36 - or 0.25
279:38 - is in that first tail
279:44 - well the table is going to give us the
279:45 - other half because the table always goes
279:47 - between our percentile or our z value
279:50 - and the mean
279:52 - so the table is going to be 0.5 minus a
279:54 - quarter or 0.5 minus 0.25 which is also
279:58 - 0.25
280:00 - so we're going to look up the z that
280:02 - corresponds with an area
280:06 - of 0.25
280:09 - notice we're talking about an area we do
280:11 - not know the z value
280:14 - so when we go to the table
280:18 - we want we're looking for an area
280:22 - of
280:22 - 0.25
280:24 - we're looking inside the body of the
280:26 - table
280:27 - and .25 happens somewhere in the middle
280:31 - here
280:32 - between 0.67
280:36 - and 0.68
280:39 - so we're going to call that .675
280:47 - the z value is 0.675
280:52 - but notice it's to the left
280:55 - of 0 to the left of the mean so it
280:57 - actually has to be a negative
281:00 - .675 because it's to the left of zero
281:03 - it's smaller than zero
281:06 - we still need to convert that z value
281:08 - which has no context into a
281:11 - x value that does have context
281:14 - and very similar to how we did it back
281:16 - with the normal distribution
281:18 - our x bar is going to be equal to the
281:21 - mean
281:23 - plus the z value
281:25 - times the standard deviation
281:28 - which in this case is the standard error
281:32 - so x bar is equal to our mean of 2.2
281:37 - minus a 0.675 because it's negative
281:42 - times the standard error which we
281:44 - calculated the standard error to be
281:49 - .045
281:56 - 0.045
282:00 - and 2.2 minus 0.675 times .045
282:06 - gives us a mean
282:08 - of 2.17
282:11 - putting units on it
282:13 - gigabytes
282:17 - the 25th percentile
282:20 - of means of sample of size 80
282:24 - is going to be 2.17 gigabytes
282:28 - now it's time for you to take a look at
282:29 - the homework assignment to try and do
282:32 - some
282:34 - problems using the central limit theorem
282:36 - where we have this new standard error
282:39 - the new standard error is triggered
282:40 - because we have a sample
282:42 - not just one individual data value
282:45 - see if you can work with a few
282:47 - and in class we will discuss them
282:49 - further and practice this whole central
282:51 - limit theorem a little bit more
283:00 - now that we've talked about this idea
283:02 - that samples adjust the standard
283:04 - deviation to become what the standard
283:06 - error is we're ready to do some
283:08 - inferential statistics
283:10 - inferential statistics take a look at
283:12 - the idea of how can the sample
283:15 - help us make an estimate or a conclusion
283:19 - about the entire population
283:22 - and as you might guess a sample and the
283:25 - population will have similar but
283:28 - different
283:29 - statistics or parameters and so the
283:32 - question we're going to start this
283:33 - discussion off with is how close
283:39 - is a sample
283:42 - proportion
283:48 - to
283:49 - a population
283:54 - proportion
283:58 - in other words if i take a sample of a
284:00 - hundred people and ask what pers what
284:03 - number of them or what percent of them
284:08 - eat breakfast in the morning
284:11 - i'll end up with a percentage of people
284:13 - in my sample who eat breakfast in the
284:15 - morning that's a sample statistic
284:17 - and i can use that to try and estimate
284:20 - what percent of the entire population or
284:22 - all people eat breakfast that is the
284:25 - population parameter
284:28 - however the two numbers aren't going to
284:29 - be exactly the same there will be some
284:32 - type of error involved in that
284:35 - proportion so that's what we're going to
284:37 - look at today how what is that error and
284:39 - how can we calculate it
284:41 - first i want to make sure we really
284:43 - understand this concept of proportion
284:49 - when we're talking about proportions
284:51 - what we're really saying is that the
284:53 - underlying
284:57 - distribution
285:02 - is
285:04 - binomial
285:07 - and if you remember from our probability
285:09 - unit binomial takes a look at x
285:12 - successes
285:16 - out of n
285:17 - trials
285:19 - that's what we're looking at so if we
285:20 - were asking about the breakfast example
285:22 - we're looking at how many people eat
285:24 - breakfast the successes out of how many
285:26 - people total we're interviewing
285:29 - and then from that
285:31 - success and trial concept we can
285:33 - calculate the population proportion
285:37 - or i'm sorry the sample proportion
285:39 - which we represent as p hat with the
285:41 - little
285:43 - triangle over the p
285:46 - we calculate that by taking the
285:47 - successes divided by the trials to get
285:50 - some type of percentage proportion or
285:53 - decimal
285:55 - and it turns out that
285:57 - proportions
285:58 - are
286:00 - normally distributed
286:02 - with a mean that's equal to the
286:04 - proportion
286:08 - and a standard error that's the square
286:10 - root of the proportion
286:12 - times the probability of failure
286:15 - divided by the sample size
286:19 - and again that q hat
286:23 - q is failure the opposite of successes
286:25 - so it's one minus the proportions
286:29 - this underlying distribution will allow
286:32 - us
286:33 - to estimate where the actual population
286:37 - parameter
286:38 - lies
286:40 - to do that we will find
286:43 - what's called a confidence
286:46 - interval
286:51 - a confidence interval is an interval
286:58 - maybe between 20 and 30 percent that's
287:00 - an interval
287:02 - based on the sample
287:10 - statistic
287:19 - where
287:20 - the population
287:26 - perimeter
287:33 - is likely
287:36 - to be located
287:41 - so it's going to be this range of
287:42 - numbers where we are quite confident the
287:46 - actual population parameter lies
287:51 - and the idea behind it
287:54 - is that our sample statistic
288:06 - is likely
288:09 - not perfect it's likely off by some
288:12 - error
288:18 - and so what we'll do to calculate this
288:20 - interval is we'll say okay let's take
288:22 - the sample proportion and we'll subtract
288:25 - the error
288:27 - and then we'll take the sample
288:29 - proportion and add the error
288:32 - and the
288:33 - population parameter is likely somewhere
288:37 - between
288:38 - those two numbers
288:40 - so we take our sample statistic and add
288:43 - and subtract the error and that should
288:45 - give us a range of numbers where the
288:48 - population proportion actually lies
288:52 - but how big is that error
288:55 - well we don't really know
288:58 - what we have to do is we have to say
289:00 - we're going to be comfortable with some
289:02 - level of confidence or some level of air
289:06 - that's allowed to occur
289:09 - and if we're okay with being wrong five
289:12 - percent of the time
289:13 - we'll make what's called a ninety-five
289:16 - percent confidence interval
289:18 - if we're okay being wrong ten percent of
289:21 - the time we'll make what's called a 90
289:24 - confidence interval so the confidence
289:26 - kind of tells us how often were
289:28 - we want to be correct and accepts a
289:31 - certain amount of error because we can
289:32 - never be 100 confident
289:34 - unless we interview everybody
289:37 - so we've got this idea of a confidence
289:39 - level
289:44 - and that's going to be the probability
289:51 - the interval
289:56 - contains
290:02 - the population
290:06 - parameter
290:13 - will have
290:14 - some type of confidence level let's say
290:16 - for example
290:20 - i want to have a 95 percent confidence
290:22 - level
290:27 - that means i want to be right 95 percent
290:30 - of the time but i could be wrong
290:32 - we have this alpha
290:35 - which is a greek letter the greek letter
290:37 - alpha
290:39 - is the
290:41 - probability
290:46 - we are wrong
290:54 - and so if we want to be 95 confident
290:57 - alpha is going to be
290:59 - 1.0
291:01 - minus the 0.95 alpha is going to be 0.05
291:06 - a 5 probability that we are
291:09 - wrong
291:11 - visually on the normal curve what that
291:13 - means
291:15 - is if my sample
291:17 - proportion comes in
291:19 - in the middle of the normal curve
291:22 - we're going to put
291:25 - a range
291:27 - the proportion minus the air
291:30 - and the proportion plus the error
291:33 - and we want to be
291:35 - somewhere in the middle we're claiming
291:37 - the population proportion is somewhere
291:40 - in that range so in that range that's my
291:44 - confidence level the 95 percent
291:48 - which means out in the tails is where i
291:50 - could be wrong if it actually falls out
291:52 - there
291:53 - well if there's five percent in the
291:55 - tails
291:56 - and there's two tails
291:59 - we could have 2.5 percent splitting it
292:02 - in half
292:05 - in each tail
292:09 - our goal is going to be to figure out
292:11 - what that error amount is that we have
292:13 - to add and subtract
292:16 - in order to get 2.5 percent
292:19 - in
292:20 - each tail
292:22 - that's what we're doing
292:28 - so with proportions
292:32 - to calculate the error
292:35 - we have this nice formula that the error
292:38 - is equal to
292:40 - what we'll call z sub alpha over 2
292:44 - times the square root of p hat
292:48 - q hat
292:50 - over n
292:53 - this is a formula that we should be very
292:56 - comfortable
292:58 - using
293:02 - p hat q hat and n we should be familiar
293:04 - with because those all come from our
293:06 - sample we've already talked about those
293:08 - before
293:09 - this z sub alpha over 2 value
293:16 - that is the z value
293:19 - that gives
293:22 - the correct
293:25 - area
293:27 - in
293:28 - each
293:29 - tail
293:31 - so for this example up above where i
293:33 - wanted a 95 percent confidence interval
293:36 - that would be the z value that gave me
293:38 - 2.5 percent in each tail
293:41 - and we can look that up in the table
293:43 - backwards
293:46 - or we can consider some
293:49 - common
293:52 - z sub alpha over two values
293:57 - because really most confidence intervals
293:59 - come in one of three types
294:02 - we have confidence levels of either 90
294:05 - percent
294:06 - 95 percent
294:08 - or 99
294:12 - and the z sub alpha over two that goes
294:14 - with each of them with the ninety
294:17 - percent to get ten percent in the tails
294:19 - five percent in each tail
294:21 - we'll use one point six four five
294:25 - for the ninety five percent confidence
294:27 - interval like the example up above we'll
294:29 - use 1.960
294:33 - and for a 99 confidence interval it
294:36 - turns out the z sub alpha over 2
294:38 - is 2.576
294:46 - you do not need to memorize these
294:48 - numbers but you should have them handy
294:51 - as you're doing your assignments and
294:53 - this and the practices and labs for our
294:56 - class
295:00 - okay i think we're in need of an example
295:02 - so that we can see this work out so we
295:04 - can see how we find out how big the
295:07 - error is
295:08 - between our sample statistic and
295:11 - population proportion
295:14 - and once we know the error how do we
295:16 - find a confidence interval that contains
295:19 - or likely contains the actual
295:22 - population
295:24 - parameter do an example
295:30 - a survey is done
295:37 - and
295:39 - 95
295:43 - out of 174
295:46 - voters
295:49 - support
295:52 - a particular
295:55 - candidate
296:00 - for senate
296:05 - the first thing we're going to do is we
296:07 - are going to construct
296:12 - a 90 percent confidence interval
296:21 - for the true proportion
296:27 - of voters
296:31 - who support
296:33 - the candidate
296:40 - can this candidate be 90 confident that
296:44 - that she or he has a majority of the
296:47 - voter support
296:53 - well first we need to know what is the
296:55 - proportion p hat that we're dealing with
296:58 - we have to calculate this
297:00 - the proportion is our number of
297:02 - successes out of the number of trials
297:06 - 95 out of 174 voters support the
297:11 - candidate
297:12 - that comes out to a proportion of
297:16 - 0.546 or 54 percent of the voters in the
297:20 - survey support this candidate
297:24 - looks pretty good
297:27 - q hat
297:28 - the probability of failure is always 1
297:31 - minus the proportion so in this case 1
297:34 - minus 0.546
297:37 - which comes out to be 0.454
297:47 - we also need a z sub alpha over 2
297:51 - or in this case
297:52 - alpha is the probability of failure
297:56 - point 10 over 2
297:58 - which gives us z of 0.5 because we want
298:01 - 10 in the tails
298:03 - oops not 0.5.05
298:06 - 5
298:07 - in each tail and so we would need to
298:09 - find the z value
298:11 - that puts 5
298:13 - area in each tail
298:15 - we can look that up on our big z table
298:19 - or
298:20 - that is one of the common ones
298:23 - that we have
298:24 - from our chart up above so we will use z
298:28 - sub 0.05
298:30 - is equal to 1.645
298:38 - those are the three pieces that we will
298:40 - need
298:42 - in order to build
298:44 - our confidence interval
298:49 - the error between our sample proportion
298:53 - and the population
298:54 - proportion is equal to the z sub alpha
298:58 - over two
298:59 - times the square root of p hat q hat
299:02 - over n
299:05 - z sub alpha over two is one point six
299:07 - four five
299:10 - times the square root of p hat which is
299:12 - 0.546
299:15 - times q at which is 0.454
299:20 - divided by n the sample size of 174
299:26 - and when you do this on your calculator
299:29 - for some reason it's really common
299:30 - students forget to multiply by the 1.645
299:34 - they just do the square root
299:36 - make sure you do the whole thing
299:38 - and you should end up with an error of
299:40 - point zero
299:41 - six
299:43 - let's round it to point zero six two
299:48 - [Music]
299:53 - this is how much
299:54 - my sample
299:56 - might be off at 90 confident we're 90
300:00 - confident our sample might be off by
300:03 - about six percent
300:06 - so
300:07 - we will take the sample proportion
300:10 - and subtract the error
300:12 - to get the lower bound of the worst case
300:14 - scenario for this candidate
300:16 - and we'll do our proportion
300:18 - plus the air to get our upper bound
300:21 - to get our best case scenario for this
300:23 - candidate
300:25 - our proportion was 0.546
300:29 - minus 0.062 the air
300:32 - and 0.546
300:34 - plus 0.062
300:36 - the air
300:39 - and when we subtract
300:41 - we end up with 0.484
300:46 - and when we add we end up with 0.608
300:54 - and this range
300:56 - is where
300:57 - the population parameter or the
301:00 - population proportion
301:02 - likely lies
301:04 - in between
301:07 - let's make a better way of saying that
301:09 - though
301:11 - let's look at how we
301:13 - interpret
301:16 - a confidence
301:19 - interval
301:25 - interpreting a confidence interval is
301:27 - almost as important as how we calculate
301:29 - it because our statistics don't mean
301:32 - anything
301:34 - unless we can put it in context
301:37 - of the situation
301:39 - so what we will say to interpret a
301:42 - confidence interval and this becomes a
301:44 - nice script for interpreting any
301:46 - confidence interval we do in this class
301:48 - is we will say we estimate
301:54 - with
301:56 - some percentage
301:58 - that would be your confidence level
301:59 - whatever your confidence level is
302:02 - with some percentage
302:06 - confidence
302:10 - that the true
302:13 - let's actually say that
302:17 - the true
302:20 - population
302:25 - whatever we're talking about
302:28 - we're going to put the parameter
302:32 - in
302:33 - context
302:35 - so we're talking about the population
302:37 - mean the population proportion the
302:39 - population standard deviation whatever
302:41 - we're talking about but then we'll put
302:43 - it in context of the situation we're
302:45 - describing
302:48 - is between
302:51 - blank
302:53 - and blank
302:56 - and those would be as you might expect
302:58 - the low number
303:00 - and the high number
303:07 - so for our proportion
303:12 - we're going to interpret
303:17 - the confidence interval
303:24 - from the example
303:29 - above
303:32 - and you can still see it at the top of
303:33 - your screen there in purple the
303:35 - confidence interval is 0.484 0.608 but
303:39 - what that means in context
303:42 - is that we can estimate
303:45 - notice how i follow the script here we
303:47 - estimate with
303:50 - and this one was a 90 percent confidence
303:53 - interval so 90 percent confidence
304:00 - that the true
304:03 - population
304:07 - and now i'm going to describe the
304:09 - parameter in context
304:11 - we're doing proportions here what
304:13 - proportions support this candidate for
304:15 - senate
304:16 - but the true population proportion
304:24 - who support
304:28 - the candidate
304:32 - for senate
304:37 - notice that puts it completely in
304:38 - context so we know what the problem was
304:41 - discussing
304:43 - is between
304:48 - and the low number let's go ahead and
304:49 - make it a percent
304:51 - 48.4 percent
304:54 - and 60.8
304:58 - percent and that's how we will interpret
305:01 - that confidence interval
305:04 - we're 90 percent confident the true
305:06 - population proportion who support the
305:08 - candidate for senate is between 48.4
305:11 - and 60.8 percent so it's not a guarantee
305:14 - this candidate is going to get a
305:15 - majority you might say it's pretty
305:17 - likely but we don't know where in
305:18 - between these numbers the actual
305:20 - proportions going to lie
305:22 - we just know it's going to be between
305:24 - those numbers
305:26 - or at least we're 90 percent confident
305:28 - it's between those numbers
305:31 - so you should be able to today build a
305:34 - confidence interval using the formulas
305:37 - for proportions that we talked about
305:38 - today
305:39 - and then just as important you should be
305:41 - able to interpret that confidence
305:43 - interval using the script we've provided
305:45 - here
305:46 - so you can go ahead and take a look at a
305:47 - couple of those and try them we'll look
305:49 - forward to discussing confidence
305:51 - intervals more in class and continuing
305:53 - to work with them in inferential
305:55 - statistics we'll see you in class
306:04 - a significant part of statistics is
306:07 - testing a claim to see if we can really
306:10 - believe it's true
306:12 - so that's going to be our question for
306:14 - the day is how
306:18 - do we test
306:22 - a claim
306:24 - and today we're going to specifically
306:26 - focus on a claim
306:28 - for
306:28 - a proportion
306:36 - the process we're going to talk about
306:38 - though does work for all sorts of claims
306:42 - but specifically today we're going to
306:43 - stay in the context of a proportion the
306:46 - way we test claims in statistics is what
306:49 - is called
306:51 - hypothesis
306:54 - testing
306:57 - and the idea behind hypothesis testing
307:00 - is it's this clear process we can do to
307:02 - test if a claim is true
307:04 - what we'll do is we'll first set up
307:07 - two
307:08 - hypotheses
307:14 - and they're going to be contradictory
307:16 - hypothesis either the first one or the
307:18 - second one is true the first one we'll
307:20 - call h sub zero
307:23 - that is what we will call the null
307:26 - hypothesis
307:29 - and it will always include it will
307:33 - always
307:34 - use
307:36 - equals
307:38 - some variable equals something
307:41 - and another thing about the null
307:42 - hypothesis is we will always assume
307:45 - [Music]
307:46 - the null hypothesis
307:48 - is true
307:51 - until proven otherwise
307:55 - if the null hypothesis is not true the
307:57 - other hypothesis is h sub a which we
308:01 - call the alternate
308:05 - hypothesis
308:08 - and this is often what we're trying to
308:10 - show to disprove a claim and this will
308:14 - either use
308:17 - greater than less than or not equal to
308:22 - kind of the alternative choice if it's
308:24 - not equal to a number it must be
308:25 - different
308:26 - than it
308:29 - and then once we've set up those two
308:30 - hypotheses
308:32 - we will run
308:35 - a
308:36 - sample
308:37 - or an experiment
308:40 - and then based on that experiment
308:43 - we will calculate
308:47 - the probability
308:53 - the null
308:56 - hypothesis
309:01 - hypothesis
309:03 - is true
309:06 - based
309:08 - on our
309:10 - sample
309:15 - this
309:16 - calculation that the null hypothesis is
309:18 - true based on our sample is what we will
309:21 - call the p
309:23 - value
309:24 - it's going to be very important to us
309:26 - the p-value
309:28 - what is the probability the null
309:30 - hypothesis is true
309:32 - based on our sample
309:35 - once we know that probability we will
309:38 - compare it
309:42 - to
309:45 - the all-important alpha
309:50 - alpha
309:52 - actually no period we'll just say or
309:57 - the smallest
310:01 - probability
310:05 - that we will still
310:09 - believe
310:12 - the null
310:13 - hypothesis
310:18 - is true
310:22 - so if our probability our p-value is
310:25 - smaller than alpha
310:28 - that is too small of a probability to
310:30 - still believe the null hypothesis
310:33 - and so we will have to reject the null
310:35 - hypothesis in favor of the alternative
310:37 - because our p-value was too small it was
310:40 - smaller than alpha the smallest
310:42 - probability we still believe the null
310:43 - hypothesis
310:45 - is true
310:47 - or the p-value might be bigger than
310:49 - alpha there is a greater probability
310:52 - that it is actual the null hypothesis is
310:54 - true and then we won't reject the null
310:56 - hypothesis
310:58 - so step five
310:59 - is simply
311:01 - either two maybe a little more space
311:07 - we will either
311:11 - reject
311:14 - the null hypothesis
311:21 - and we do that if the p-value
311:25 - is
311:26 - less than
311:28 - alpha
311:29 - because the probability was too small to
311:31 - still believe the null hypothesis
311:35 - or we will
311:38 - fail
311:40 - to reject
311:44 - the null hypothesis
311:49 - and that's the case where the p-value
311:53 - is greater than alpha or it's just too
311:56 - big of a probability to believe that the
311:58 - null hypothesis is false
312:04 - a great example to kind of show how
312:06 - hypothesis testing works is to consider
312:11 - a
312:12 - trial in the united states we assume in
312:16 - a trial
312:17 - that a person is innocent until proven
312:20 - guilty and it's actually a perfect
312:21 - statistical hypothesis test
312:24 - let's say
312:26 - person
312:27 - a is accused
312:32 - of a crime
312:37 - the null hypothesis that everyone
312:40 - assumes is true until proven otherwise
312:44 - is that person a
312:46 - is innocent
312:51 - the alternative hypothesis what we try
312:53 - and prove or find enough evidence is
312:55 - that the person is guilty
313:03 - and what we always do is we assume
313:07 - innocent
313:12 - until proven
313:15 - that proof
313:17 - is the p-value
313:20 - what is the probability that they're
313:22 - innocent
313:25 - given there's all this proof that they
313:28 - are guilty
313:30 - and not just proof that they're guilty
313:32 - because you never know for sure they're
313:33 - guilty we just prove they're guilty
313:35 - beyond
313:38 - a reasonable
313:42 - doubt
313:44 - and that reasonable doubt
313:47 - that is the alpha if you go beyond alpha
313:50 - if p value gets smaller than alpha the
313:53 - proof of innocent is so small
313:56 - we can no longer assume they're innocent
314:00 - and there's actually two conclusions
314:02 - that we can make
314:05 - if
314:06 - proven guilty
314:12 - if the p-value is so small that they're
314:14 - innocent the probability of their
314:15 - innocent is so small it's beyond a
314:17 - reasonable doubt we reject the null
314:20 - hypothesis
314:26 - and conclude
314:30 - the defendant is guilty
314:36 - if not at least not beyond a reasonable
314:39 - doubt
314:41 - we will fail
314:43 - to reject
314:46 - the null hypothesis
314:52 - and conclude and this is where it gets
314:54 - interesting and it's very important in
314:56 - statistics we don't conclude they're
314:58 - innocent
315:01 - we conclude that they are not
315:04 - guilty
315:07 - the conclusion focuses on the
315:08 - alternative hypothesis
315:12 - what's key there is that we
315:16 - never
315:20 - conclude
315:24 - the null hypothesis h sub 0
315:28 - is
315:29 - true
315:31 - we just failed to conclude the
315:33 - alternative
315:34 - we didn't say they were innocent we just
315:36 - said there is not enough evidence to say
315:38 - they're guilty
315:41 - and that's an important
315:43 - conclusion
315:44 - that applies to statistical conclusions
315:46 - as well we will never conclude the null
315:48 - hypothesis is true we will always
315:51 - conclude that the alternative hypothesis
315:53 - could not be proven or could be proven
316:00 - speaking of conclusions
316:04 - let's talk about how we want to make our
316:06 - conclusions
316:10 - similar to how we interpreted a
316:12 - confidence interval for a proportion
316:14 - when we make a conclusion it's really
316:16 - important we make that conclusion in
316:18 - context
316:24 - but we also are going to focus
316:28 - on
316:29 - the alternative
316:33 - hypothesis
316:37 - the h sub
316:38 - a
316:40 - and so a nice script we can follow is we
316:42 - will say there
316:45 - is
316:47 - or there is not depending on the context
316:50 - we will say not
316:52 - if we fail to reject
316:57 - because we did not get the alternative
316:59 - hypothesis like we wanted
317:01 - so there there is or there is not
317:03 - sufficient evidence
317:10 - to conclude
317:15 - whatever we can conclude
317:18 - the conclusion though is always going to
317:20 - be the alternative
317:22 - or the alternate
317:25 - hypothesis
317:30 - in context
317:33 - to the problem
317:39 - let's do two examples where we can
317:40 - really see what this
317:43 - hypothesis testing thing
317:47 - looks like
317:52 - example
317:59 - first for doing a
318:01 - hypothesis test specifically with
318:03 - proportions
318:04 - everything we've done so far actually
318:06 - applies to all hypothesis testing but
318:08 - specifically with proportions there's a
318:10 - few formulas we need for proportions
318:22 - first off
318:23 - we know that proportions are normally
318:26 - distributed
318:28 - with the proportion acting as the mean
318:32 - and the square root of p q over n acting
318:35 - as the standard error
318:37 - but as you calculate these values
318:40 - different than a confidence interval
318:42 - because a confidence interval focused on
318:45 - the sample and what we could learn from
318:46 - the sample we used p and q from the
318:49 - sample
318:50 - here we're focusing on a null and an
318:52 - alternate hypothesis so we're going to
318:54 - focus on
318:56 - the claim
318:57 - that the null hypothesis is true
319:01 - use
319:02 - the null hypothesis values
319:11 - and then to calculate our p value or our
319:14 - probabilities
319:16 - we will have z is equal to p hat
319:20 - the sample proportion
319:22 - minus p the hypothesized proportion
319:26 - divided by the standard error and
319:29 - remember the standard error
319:31 - is the square root of pq
319:34 - over n
319:40 - so let's try this let's say a phone
319:43 - company
319:48 - claims
319:51 - that 43
319:54 - of smartphone
319:58 - users
320:01 - have
320:02 - an iphone
320:09 - but you doubt this claim
320:18 - so you conduct a survey
320:28 - of 83
320:31 - smartphone users
320:41 - 44 of them
320:46 - use an iphone
320:54 - what
320:56 - can you conclude
321:02 - if alpha
321:06 - equals 0.05
321:12 - in other words we're going to believe
321:13 - the claim of 43
321:16 - until the probability dips below 5
321:18 - percent
321:21 - that that claim is actually
321:23 - true
321:28 - well let's set this up
321:32 - our null hypothesis has to be that our
321:34 - proportion equals something and that's
321:37 - the claim that the proportion equals
321:39 - point
321:40 - 43
321:43 - for the alternative hypothesis we can
321:45 - either say the proportion is greater
321:47 - than less than or not equal to
321:50 - there's no direction given in your doubt
321:54 - when you doubt the 43 is accurate you're
321:56 - not saying that it's greater or less
321:58 - than you just doubt that it's accurate
322:00 - so this is going to actually be not
322:01 - equal to 0.43
322:04 - and when it's not equal to 0.43 we have
322:07 - what's called a 2
322:10 - tailed
322:12 - test
322:13 - and what that means is we could reject
322:16 - the null hypothesis if the proportion is
322:18 - bigger
322:19 - or if the proportion is smaller either
322:22 - direction
322:24 - maybe it'd be easier to see if we drew a
322:25 - picture and we're going to annotate this
322:27 - picture as we go on
322:29 - here's the normal curve for the
322:31 - proportion
322:32 - the claim is that the mean
322:35 - the proportion is 0.43
322:39 - but we doubt that's true
322:42 - we think it's either going to be lower
322:44 - somewhere in the red tail on the left
322:47 - or higher
322:48 - somewhere in the red tail on the right
322:51 - we don't know which side we just doubt
322:53 - it it's in both tails left and right
323:01 - the distribution then
323:05 - of the proportion
323:07 - just to review we know that the
323:09 - proportion of our sample will be
323:12 - normally distributed
323:15 - and again we're going to use the null
323:16 - hypothesis here
323:18 - around the claim of 0.43
323:21 - with a standard error of 0.43
323:24 - that's our p
323:26 - times q
323:27 - 1 minus 0.43 is 0.57
323:31 - over my sample size
323:34 - and here we did a survey of 83 people
323:38 - so what that really means is our
323:39 - proportion
323:41 - is normally distributed with a mean of
323:43 - 0.43
323:45 - and a standard error of 0.0543
323:56 - so
323:59 - what is our sample proportion
324:04 - well our sample proportion is going to
324:06 - be x divided by n or the 44
324:10 - out of 83 people who use the iphones
324:14 - and that's going to be
324:16 - point 53.
324:20 - run out of colors so we'll go back to
324:22 - blue
324:27 - actually one thing to put in since we
324:29 - know our proportions point 53 on my
324:32 - picture off to the right i'm going to
324:33 - put .53
324:35 - that is the x value where the red
324:38 - shading starts
324:39 - we don't know the value on the left
324:41 - if our proportion had ended up being
324:43 - less we would have put the number on the
324:45 - left but because our proportion was more
324:47 - we put it on the right
324:50 - now we're ready to calculate our z value
324:53 - and z is our sample proportion minus the
324:56 - hypothesized proportion
325:00 - divided by the standard
325:02 - error so point 53
325:06 - the sample
325:08 - minus the hypothesized point 43
325:11 - divided by the standard error of 0.0543
325:16 - the z value there is 1.84
325:22 - so if we have x's on top we'll stick z's
325:25 - down below
325:26 - i should have left a little more space
325:28 - when it's a z we assume the mean is zero
325:31 - and our value on the table is 1.84
325:38 - so let's go to the table and see
325:40 - what area
325:43 - goes with a z value
325:45 - of 1.84
325:47 - on the table we're looking at one
325:51 - point eight
325:53 - four
325:56 - so if i scroll over
325:58 - and if i draw my line straight we see
326:00 - 1.84 corresponds to an area of 0.4671
326:07 - but remember with that area of 0.4671
326:13 - that is the area in white point four six
326:17 - seven one
326:20 - we are interested in the area in the
326:22 - tail
326:25 - to get the tail we have to subtract from
326:26 - point 0.5 that'll give us 0.0329
326:32 - but what's important to know is because
326:34 - this is a two-tailed test we have to
326:36 - consider the other tail as well
326:38 - is .0329
326:41 - as well it's symmetrical which is nice
326:46 - so when we want to calculate
326:48 - the p-value
326:54 - the p-value is the total shaded area
326:57 - because this is a two-tailed test we
326:58 - have to add them together the .0329
327:02 - plus .0329
327:06 - that gives us a p-value of 0.01
327:16 - and remember that p value
327:18 - is the probability our null hypothesis
327:22 - is true
327:24 - based on our survey
327:27 - in other words based on our survey
327:40 - there is a
327:42 - change it to a percentage
327:44 - 6.58 percent chance
327:51 - the proportion
327:55 - of iphone users
328:01 - is actually
328:04 - 43 percent
328:06 - what the null hypothesis claims
328:10 - now at first glance you might say that's
328:12 - not a very large percent but remember we
328:15 - did say up at the top here that alpha is
328:17 - 0.05 and that means we will continue to
328:21 - believe the null hypothesis is true
328:25 - as long as the probability does not dip
328:28 - below 5
328:33 - so
328:40 - if the p-value is six percent
328:44 - that's not below the five percent
328:45 - threshold we say that still is not quite
328:48 - enough evidence to kick out the null
328:50 - hypothesis
328:52 - so our decision
328:55 - is we will fail to reject
329:00 - the null
329:01 - hypothesis
329:06 - because there's just not quite enough
329:07 - evidence it was close but not quite
329:09 - enough evidence to
329:11 - reject the null hypothesis the reason
329:14 - for that really clearly stated
329:16 - is the p-value
329:19 - the probability the null hypothesis is
329:21 - true
329:22 - is greater than that alpha that minimum
329:25 - threshold
329:27 - putting the numbers in there the p-value
329:29 - is 0.0658
329:32 - that is greater than the alpha of 0.05
329:37 - so we failed to reject and we're ready
329:39 - to make our conclusion
329:46 - following our script then we will say
329:48 - that there
329:49 - is
329:50 - not
329:51 - because we failed to reject we'll say
329:53 - not
329:54 - sufficient
329:57 - evidence
330:00 - to conclude
330:04 - and the conclusion must be in context
330:10 - of the alternative hypothesis so we're
330:14 - going to state the alternative
330:15 - hypothesis that the proportion's not 43
330:18 - percent
330:20 - of course we must put it in context so
330:22 - to conclude the proportion
330:29 - of iphone users
330:34 - is
330:35 - different
330:38 - than 43
330:49 - now one thing you might notice is
330:50 - there's a couple of p's going on in here
330:53 - in problems like these
330:55 - and it's very important we keep them all
330:57 - straight we have a p value
331:00 - that's just the probability the null
331:02 - hypothesis is true
331:05 - we have a p
331:07 - in the null hypothesis
331:09 - that is the claim for the population
331:12 - proportion
331:14 - and we also have a p hat
331:17 - that is the sample
331:19 - proportion
331:22 - be very careful not to get the three p's
331:24 - mixed up quite often we'll see students
331:27 - compare the wrong p to alpha
331:31 - and they'll make the wrong conclusion as
331:32 - a result make sure you compare the p
331:34 - value to alpha
331:36 - to make a conclusion about your p
331:39 - based on your p-hat
331:42 - sounds confusing but practice a few to
331:44 - make sure you get them straight
331:46 - just to kind of make this interesting
331:48 - and this isn't always required but it
331:50 - often is with a hypothesis test is let's
331:54 - make
331:56 - a 95
331:58 - confidence
332:01 - interval
332:04 - for the true proportion
332:10 - based on our sample so based on our
332:12 - sample p hat
332:14 - our sample we said
332:16 - was 53 percent
332:20 - which means q hat
332:22 - the opposite of that is going to be one
332:24 - minus that or 47 percent 0.47
332:29 - and if we're doing a 95 percent
332:30 - confidence interval we should know the z
332:32 - sub alpha over 2
332:35 - or z sub 0.05 over 2
332:39 - or z sub 0.025
332:43 - is equal to 1.96
332:48 - and we found out in our previous lesson
332:50 - that the error is equal to
332:52 - that z
332:54 - value of 1.96
332:56 - times the square root of pq over n
333:00 - but with the confidence interval notice
333:02 - i will use the sample data
333:06 - this is different than the hypothesis
333:07 - test where we use the hypothesis
333:10 - with the confidence interval we use the
333:12 - sample data of 0.53
333:15 - times 0.47
333:17 - divided by the sample size
333:20 - which was 83
333:23 - and that will give me an error of 0.107
333:31 - so my confidence interval then is the
333:33 - proportion .53 minus the error
333:37 - of 0.107
333:40 - and the sample proportion of 0.53
333:43 - plus the error of 0.107
333:47 - giving me a confidence interval for the
333:49 - true population proportion
333:52 - to be between 0.423
333:55 - and 0.637
334:00 - or in context we're saying that we are
334:05 - 95 percent confident
334:11 - the true
334:13 - population
334:16 - proportion
334:21 - of iphone users
334:25 - always put it in context
334:28 - is between
334:31 - 42.3 percent
334:34 - and
334:35 - 63.7 percent
334:40 - and what you notice is with that
334:42 - confidence interval our null hypothesis
334:44 - said
334:46 - what we were assuming to be true the
334:48 - null hypothesis said the proportion was
334:50 - equal to 0.43
334:52 - notice that 43
334:54 - is within that confidence interval
334:57 - that's why we cannot reject it because
335:00 - it still is a valid possibility for the
335:02 - true
335:03 - population proportion
335:05 - of iphone users
335:11 - i want to do one more example i know
335:13 - this video is running a little bit
335:14 - longer than normal
335:16 - but it's really important that we're
335:18 - comfortable with
335:20 - these hypothesis testing
335:23 - so
335:24 - it has been claimed
335:34 - that
335:36 - 58.4 percent
335:39 - of web users
335:43 - prefer
335:46 - chrome
335:52 - however you believe
335:56 - the number is lower
336:05 - so you're going to test it
336:07 - you sample
336:13 - 152 web users
336:22 - and 74 of them
336:26 - use
336:27 - chrome
336:34 - with alpha equal to 0.01 this time we
336:37 - want to be very confident we're going to
336:38 - go all the way down to one percent error
336:42 - what can you conclude
336:55 - well like before let's start with our
336:57 - hypotheses
337:00 - the null hypothesis
337:01 - is that the proportion equals what they
337:03 - claim it equals
337:06 - the claim is that it equals 0.584
337:11 - the alternative hypothesis
337:14 - is based on what you're trying to show
337:16 - and this time we're going to try and
337:18 - show that the actual number is
337:21 - lower
337:22 - that the proportion is less than 0.58
337:26 - which means this time we really have a
337:29 - one-tailed test or better said a
337:31 - left-tailed
337:35 - test
337:37 - meaning we're going to reject the null
337:40 - hypothesis
337:42 - if we end up far out into the left tail
337:46 - drawing a picture the hypothesized mean
337:49 - is at .584
337:52 - we're going to reject if it's less than
337:55 - significantly to the left
337:58 - or in that left tail
338:04 - so we have our distribution
338:16 - we know that the proportion is normally
338:18 - distributed
338:21 - at 0.584
338:25 - with the standard error
338:28 - of
338:28 - using the null hypothesis 0.584
338:33 - times q which is 0.419
338:38 - oops 416. sorry
338:42 - divided by the sample size of 152
338:49 - which means it's normally distributed at
338:51 - 0.584
338:55 - comma
338:58 - 0.0400 when we round
339:01 - if that's the distribution then we're
339:03 - going to compare it to the proportion or
339:05 - the p-hat that we get from our sample
339:08 - our sample said 74
339:11 - out of 152
339:13 - use chrome
339:14 - 74 out of 152 is 0.488
339:18 - that's the value off to the left it's
339:20 - less than 0.487
339:23 - where the shading starts
339:26 - is that far enough away that we can make
339:28 - a conclusion that it's actually less
339:30 - than
339:32 - well to do that we will go to our z
339:34 - value
339:36 - which is equal to our sample proportion
339:38 - of 0.487
339:42 - minus the hypothesized proportion of
339:44 - 0.584
339:47 - divided by the standard error of 0.0400
339:53 - our z value is negative 2.43
339:59 - so we've got our x's on our picture
340:01 - putting those all into z's the mean is 0
340:05 - and the z value of negative
340:08 - 2.43 is where the shading starts
340:13 - let's go to our z table
340:20 - and i'm going to scroll down a bit to
340:21 - see 2.43
340:26 - so 2.43
340:34 - looks like this time we're going to have
340:35 - a z value of 0.49
340:39 - i'm sorry an area of 0.49
340:45 - that's the area in between of 0.4925
340:51 - we need the area in the tail
340:54 - which is just 0.5 minus that or 0.0075
341:02 - this time we don't need to worry about
341:04 - the other side because it is a
341:05 - one-tailed test so my p-value
341:08 - is just that shaded area the 0.0075
341:14 - which means
341:16 - based on our survey
341:24 - there is
341:26 - a 0.75
341:29 - chance
341:33 - the null hypothesis is true
341:36 - or the proportion
341:43 - of chrome users
341:46 - or people who prefer chrome probably
341:48 - would have been a better way to say that
341:51 - is
341:54 - 58.4 percent
341:57 - that is really really small p-value in
342:00 - fact what's really important
342:02 - is when we compare that p-value to our
342:04 - alpha of 0.01
342:09 - it is actually less than that alpha of
342:12 - 0.01 which means that's too much
342:14 - evidence to the contrary
342:16 - so we will make a decision
342:24 - to reject
342:28 - the null hypothesis
342:33 - and the reason for that decision
342:37 - is because the p-value
342:40 - the probability it's true is less than
342:42 - the alpha
342:44 - or specifically
342:46 - 0.0700
342:51 - is less than
342:52 - the 0.01
342:55 - minimum threshold
342:58 - so our conclusion
343:03 - if we reject the null hypothesis
343:06 - we will say that there is
343:09 - sufficient evidence
343:17 - to conclude
343:21 - and then we will state the alternative
343:23 - hypothesis
343:25 - in context
343:27 - focusing on being less than the 0.8.584
343:34 - to conclude
343:37 - the proportion
343:42 - of web users
343:48 - who prefer
343:50 - chrome
343:53 - is less
343:58 - then
344:00 - 58.4
344:03 - percent
344:08 - one last little thing as we wrap up our
344:11 - conversation on hypothesis testing
344:14 - is
344:15 - when all is said and done we make a
344:19 - conclusion based on a p-value
344:22 - what's the probability that it's the
344:24 - null hypothesis is true do we reject it
344:27 - do we fail to reject it
344:29 - but when all is said and done at the end
344:30 - of the day in statistics we could be
344:33 - wrong
344:36 - and we really have no way of knowing
344:38 - if we're right or wrong we can be fairly
344:41 - confident 95 or 99 confidence but we
344:44 - could be wrong and there's two types of
344:46 - errors and statistics that we always try
344:49 - and minimize
344:50 - we call them type 1 and type 2 errors
344:54 - a type 1 error
344:57 - is when we reject
345:01 - the null hypothesis
345:04 - when it is true we should not have
345:08 - rejected it but we did
345:12 - the probability of that happening is
345:14 - actually the alpha that we're using in
345:16 - the problem
345:19 - the other type of error
345:20 - is what's called a type ii error
345:25 - and that's where we fail to reject
345:30 - the null hypothesis
345:34 - when we should have
345:36 - when it is
345:38 - false
345:41 - and the probability is not as evident
345:42 - for that we call that probability beta
345:45 - it comes up in more advanced statistics
345:47 - classes we're not going to spend a lot
345:48 - of time on that but you should be aware
345:51 - at this point
345:52 - of what a type 1 n type 2 error is
345:57 - in context
345:58 - so we did two examples today the first
346:01 - example was about iphones
346:06 - and we
346:08 - failed to
346:09 - reject the null hypothesis
346:13 - we could have committed a type
346:15 - 2
346:17 - air
346:20 - where we conclude
346:24 - not reject
346:26 - or fail to reject
346:31 - when we should have
346:37 - or to put it in context and this is
346:40 - probably a better way to say it
346:44 - or
346:44 - we
346:46 - believe
346:51 - the proportion
346:55 - of iphone users
347:01 - is 43 percent because we failed to
347:04 - reject it
347:05 - but we should have
347:07 - when it is not
347:11 - it is not actually 43 we should have
347:13 - rejected
347:15 - that's a type 2 error
347:18 - the second example
347:24 - because we did reject could have been
347:28 - a type 1 error
347:32 - where we conclude
347:35 - reject
347:39 - when we
347:42 - should not have
347:48 - or to put that in context
347:51 - we currently after running that sample
347:53 - we believe
347:55 - the proportion
348:01 - who prefer chrome
348:10 - is less than
348:15 - 58.4
348:19 - because that's what we concluded
348:22 - but it's not
348:26 - that would be a type 1 error where it's
348:29 - actually either equal to or possibly
348:31 - greater than 58.4 percent and we made
348:34 - the wrong conclusion
348:38 - those errors hopefully don't happen
348:40 - often to us but there's always a chance
348:42 - that they could happen because with
348:44 - hypothesis testing we're never sure of
348:46 - anything we could be wrong
348:48 - and that's what the type 1 and type 2
348:50 - errors tell us is what does it mean
348:53 - if we're wrong
348:56 - so we covered a lot of stuff in this
348:58 - video today
348:59 - we introduced the concept of what a
349:01 - hypothesis test is and how hypothesis
349:03 - test works and then we did several
349:05 - hypothesis test examples in the context
349:08 - of proportions
349:10 - and then really briefly at the end we
349:11 - talked about we could be wrong the type
349:13 - 1 and type 2 error so take a look at the
349:16 - assignment if you want to try a few of
349:17 - these a little bit of a longer video but
349:19 - some of the next few are going to be
349:21 - much shorter to make up for it so
349:23 - we'll see you in class
349:33 - now that we've taken a look at how
349:34 - hypothesis testing works specifically in
349:36 - the context of one proportion we can
349:38 - extend it to hypothesis testing in many
349:41 - different situations and one of those
349:43 - situations involves having two
349:46 - proportions or two groups and trying to
349:48 - decide are the two groups the same
349:50 - or are the two groups different so our
349:52 - question for the day is how
349:55 - do we test
349:58 - a claim
350:02 - about two
350:04 - proportions
350:10 - we're going to have two groups and we
350:11 - want to know are these two groups the
350:13 - same is one group bigger is one group
350:15 - smaller
350:17 - what can we conclude
350:19 - about these two proportions
350:21 - and the idea of the hypothesis test is
350:23 - identical to the idea that we did
350:26 - with one proportion the only difference
350:28 - is we have a few different equations
350:33 - needed
350:36 - to find the test statistic
350:42 - first thing we need to know is what
350:43 - we're going to call the pooled
350:46 - proportion
350:50 - the pooled proportion is what the
350:53 - proportion would be if the groups
350:55 - weren't separate so if we weren't
350:57 - comparing men and women but we were just
350:58 - looking at people how many successes are
351:01 - there out of the total group
351:04 - the pooled proportion we represent with
351:06 - p sub c
351:07 - the pooled proportion is equal to the
351:09 - sum of the successes from the first
351:11 - group and the sum of the successes from
351:14 - the second group
351:16 - divided by
351:18 - the number in the first group
351:20 - plus the number in the second group
351:23 - and as a tip if you do this on your
351:25 - calculator you will probably need to put
351:27 - parentheses around the numerator and
351:28 - denominator to make sure that division
351:30 - happens in the correct order because a
351:32 - proportion should always be between 0
351:35 - and 1.
351:36 - if your proportion's not between 0 and 1
351:38 - check the order of operations
351:41 - so this is our first important formula
351:44 - finding the pooled proportion
351:47 - in order to test the two proportions
351:50 - if there was no
351:51 - separation of the groups
351:54 - the second formula you need to know is
351:56 - the distribution
352:02 - for two proportions when we compare two
352:05 - proportions we don't just compare them
352:07 - what we actually do is we take the first
352:09 - proportion of our sample and subtract
352:12 - the second proportion of the sample
352:13 - we're actually looking at the
352:14 - distribution
352:17 - of the difference between the
352:18 - proportions
352:20 - and they are normally distributed as you
352:22 - might expect and if they are the same
352:25 - we should have a difference of zero
352:30 - the standard error formula is a little
352:32 - bit more involved but not difficult it's
352:34 - the pooled proportion
352:37 - times one minus the pooled proportion
352:39 - which might be the pooled q
352:43 - times one over the first sample size
352:46 - plus one over the second sample size
352:53 - and so that is the second key thing we
352:55 - need specifically because it is going to
352:58 - help us find the standard error that big
353:00 - square root is the standard error that
353:03 - we need to calculate the test statistic
353:12 - with the normal distribution the test
353:14 - statistic is z
353:16 - and the test statistic here is going to
353:17 - be the difference in the proportions
353:20 - p hat a minus p hat b
353:23 - divided by
353:25 - the standard
353:26 - error
353:30 - so that's the third new equation we need
353:33 - other than that everything is identical
353:36 - to what we've seen
353:38 - before so we should be able to jump
353:40 - right in
353:42 - to an example
353:45 - where we will compare
353:47 - two
353:48 - proportions
353:53 - let's say a restaurant
353:58 - wants to know
354:04 - if teens
354:08 - are more
354:09 - likely
354:13 - to order dessert
354:19 - then adults
354:25 - knowing this information can help them
354:26 - plan their future marketing campaign so
354:29 - they contact
354:36 - a sample
354:42 - 84 adults
354:50 - 33 order dessert
354:58 - they contact
355:00 - a sample
355:06 - of 91 teens
355:11 - 46 order dessert
355:20 - can they conclude
355:26 - teens
355:28 - are more likely
355:33 - to order dessert
355:38 - if our alpha level
355:40 - equals
355:42 - point
355:44 - 10.
355:48 - notice this example is different than
355:49 - when we had one single mean when we had
355:52 - one single mean there was a global claim
355:54 - that said the proportion is equal to
355:56 - such and such a percent
355:59 - here we don't have any such global claim
356:01 - here we're doing a sample of two
356:03 - separate groups
356:05 - and then we're going through and
356:07 - comparing are these two separate groups
356:09 - different or are they the same
356:13 - so setting up our null hypothesis the
356:15 - null hypothesis
356:17 - is always going to have equality so the
356:18 - null hypothesis is that the proportion
356:20 - of the teens
356:22 - who order dessert is equal to the
356:24 - proportion of adults who order dessert
356:28 - notice i use a subscript on each of the
356:30 - p so i know which one is which t for
356:32 - teens and a for adults
356:35 - that way when i set up my alternative
356:36 - hypothesis
356:38 - we want to know are the teens more
356:41 - likely to order dessert they want to
356:43 - know if the proportion of teens is
356:45 - greater than the proportion of the
356:48 - adults what we have actually is a right
356:52 - tailed
356:54 - test because we are going to reject
356:57 - in the right tail
357:00 - drawing our little picture
357:03 - the mean the assumed difference the
357:06 - hypothesized difference is that there is
357:08 - zero difference between them
357:14 - the order of my hypothesis tells me the
357:17 - order of the subtraction we always
357:18 - subtract left to right so what we're
357:20 - really doing is the proportion of teens
357:22 - minus the proportion of the adults
357:25 - we're claiming the teens are bigger than
357:28 - the adults so we're taking a big number
357:30 - minus a small number it should give us a
357:32 - positive number
357:34 - and something in that right
357:36 - tail that's we're going to attempt to
357:40 - find
357:43 - let's uh calculate some of the pieces
357:45 - that we're going to need in order to
357:47 - solve this
357:48 - first the proportion of teens
357:51 - there are 46 out of 91 teens that order
357:54 - dessert
357:56 - that's going to be 0.505
358:01 - the proportion of adults who order
358:02 - dessert
358:03 - that's going to be 33
358:06 - out of 84
358:08 - or 0.393
358:13 - and then for our pooled proportion
358:16 - if there was only one group not two
358:18 - separate groups and we just interviewed
358:19 - people
358:20 - the number of successes would have been
358:22 - 46 plus 33
358:25 - over the number of trials would be 91
358:27 - plus 84
358:30 - making sure i use parentheses so i don't
358:31 - get in trouble with order of operations
358:34 - the pooled proportion is 0.451
358:43 - we can use that information let's not
358:45 - scroll too far keep that
358:48 - hypothesis on there we can use that
358:50 - information to find the distribution
358:57 - we know the difference in the proportion
358:59 - between the teens
359:02 - and the adults
359:04 - is normally distributed
359:07 - with a mean of 0
359:09 - and a standard error equal to that big
359:11 - square root
359:13 - the pooled proportion of 0.451
359:17 - times 1 minus the 0.451
359:20 - times 1 over the first sample size of 91
359:24 - plus 1 over the second sample size of 84
359:29 - or normally distributed with a mean of 0
359:33 - and a standard error
359:35 - if you put that in your calculator of
359:36 - 0.0752
359:45 - now that we know the standard error in
359:47 - the distribution
359:51 - we can calculate
359:54 - the z value
359:56 - the test statistic
359:58 - let's label it test statistic
360:07 - the difference in the proportions we're
360:08 - going to do the same order as the
360:10 - hypothesis so we have to do the teens
360:12 - minus the adults 0.505
360:15 - minus
360:16 - 0.393
360:18 - over the standard error of 0.0752
360:24 - now just so i can label it on my picture
360:26 - let's do the subtraction in the
360:27 - numerator 0.505 minus 0.393 the actual
360:31 - difference is 0.112
360:35 - divided by the standard error of 0.0752
360:39 - so on my picture
360:40 - the actual difference is 0.112
360:44 - and when we divide by 0.0752
360:48 - we get a z value of 1.49
360:52 - so if we've got x's in the top rows
360:55 - these in the bottom row
360:57 - z's also have a mean of zero but now the
361:00 - z value
361:01 - is 1.49
361:05 - that 1.49
361:07 - is what we want to look up in the table
361:10 - to find
361:11 - our p-value
361:15 - looking at our table then
361:18 - we have 1.49 so 1.49
361:25 - that's going to give us an area of
361:27 - 0.4319
361:32 - so our area
361:34 - of 0.4319
361:37 - but the p-value
361:40 - is the area in the tail so we subtract
361:43 - from 0.5 to get 0.0681
361:52 - so our p-value
361:55 - is .0681
362:02 - which means
362:04 - the probability
362:06 - or given our sample i should say
362:08 - given
362:10 - our sample
362:14 - the probability
362:25 - the proportion
362:31 - of teens
362:33 - and adults
362:37 - who order dessert
362:44 - is the same
362:49 - is 6.81 percent
362:52 - probability they both order dessert at
362:54 - the same rate is 6.81 percent that's
362:57 - what the p value means
363:00 - we said we wanted our alpha to be point
363:03 - 10 which means we will believe the null
363:05 - hypothesis we will believe the
363:07 - proportions are equal as long as the
363:09 - probability is bigger than ten percent
363:12 - we got a probability of six percent
363:15 - so our decision
363:21 - is to reject
363:23 - the null
363:24 - hypothesis
363:28 - and the reason for that decision
363:31 - is our
363:32 - p-value
363:34 - is less than our alpha
363:36 - there's a six point eighty-one percent
363:38 - chance or uh let's not do it as a
363:41 - percent a point zero six eight one
363:44 - p-value which is less than the alpha of
363:46 - point ten so we reject the null
363:49 - hypothesis
363:54 - and we're ready to make our big
363:56 - conclusion
364:00 - the script for the conclusion remains
364:02 - exactly the same because we rejected the
364:04 - null hypothesis got the alternative we
364:06 - were looking for we will say there is
364:10 - sufficient
364:13 - evidence
364:17 - to conclude
364:21 - and then we will go back and state the
364:22 - alternate hypothesis
364:25 - in context that teens are greater than
364:28 - adults
364:30 - the proportion
364:35 - of teens
364:39 - who order dessert
364:44 - is greater
364:48 - than the proportion
364:53 - of adults
364:56 - who order dessert
365:03 - the conclusion is in context it focuses
365:06 - on the alternative hypothesis
365:10 - you should feel like what we just did
365:12 - was very very similar to
365:14 - the test hypothesis testing for a single
365:17 - mean
365:18 - the only difference that we had to do
365:20 - was we had to find this pooled
365:22 - proportion and a new standard error to
365:24 - calculate our test statistic but the
365:27 - process of the hypothesis test remains
365:30 - the same regardless of what we're
365:32 - testing
365:33 - so take a look at practicing some of
365:35 - these we're going to do some of this in
365:37 - class we'll look forward to seeing you
365:40 - then
365:47 - just as we have done inferential
365:48 - statistics with proportions we can do
365:51 - many of the same things with means in
365:53 - fact we more often are working with
365:55 - means than proportions so let's answer
365:58 - the question first how do we
366:03 - find
366:05 - a confidence
366:09 - interval
366:12 - for a mean
366:20 - very similar in idea to find a
366:22 - confidence interval for a proportion but
366:24 - there's one key difference
366:26 - with the means is normally when we do a
366:29 - sample and we have a mean of the sample
366:32 - we do not know
366:34 - the standard deviation
366:36 - of the population
366:38 - we only know the standard deviation
366:40 - of the sample
366:42 - which means the normal distribution will
366:45 - be a little bit too
366:47 - tight
366:49 - or a little bit too small
366:51 - to calculate a reliable confidence
366:53 - interval because we're only estimating
366:56 - the standard deviation of the population
366:58 - with the standard deviation of the
367:00 - sample
367:02 - so if we have no standard deviation
367:12 - of the population
367:18 - we can no longer use the normal
367:20 - distribution we need a different
367:22 - distribution and the distribution we
367:25 - will use is called the student's
367:29 - t
367:30 - distribution
367:34 - or often you'll hear it just called the
367:36 - t distribution
367:38 - the student's t distribution is very
367:40 - similar
367:41 - to
367:42 - the normal
367:44 - distribution
367:47 - but it
367:49 - allows
367:52 - for greater
367:55 - flexibility
368:04 - as
368:07 - we will use
368:12 - the sample
368:16 - standard deviation
368:24 - to estimate
368:29 - the population
368:34 - standard deviation
368:43 - and that's never perfect it's probably
368:45 - close but it's never perfect and that's
368:47 - why we need that extra flexibility that
368:50 - the student t distribution gives us
368:52 - is it allows us to still make a
368:54 - confidence interval with a little bit
368:56 - extra flexibility
368:59 - and it turns out that and it makes sense
369:02 - as well the larger
369:06 - the sample size
369:13 - the less
369:16 - flexibility
369:20 - is needed
369:24 - and that makes sense as if we interview
369:27 - more and more people getting closer and
369:29 - closer to the population
369:31 - our estimate for the standard deviation
369:33 - is probably going to be more and more
369:35 - accurate
369:36 - and the more accurate we are the less
369:38 - flexibility we need
369:40 - it turns out that we have to adjust the
369:42 - student t distribution then
369:44 - based on the sample size and we call
369:47 - that estimation the degrees
369:52 - of
369:53 - freedom
369:57 - the degrees of freedom is often
369:58 - abbreviated df for degrees of freedom
370:01 - and it's very easily calculated as n
370:05 - minus 1.
370:08 - the degrees of freedom is n minus 1 one
370:11 - less than the sample size
370:18 - and it turns out that if
370:21 - the sample size
370:24 - is greater than 30
370:28 - the student t
370:33 - is
370:34 - almost
370:36 - identical
370:40 - to the normal distribution
370:43 - and so when we have a sample size bigger
370:45 - than 30 we end up using normal values
370:47 - because they're so close together
370:51 - but if the num sample size is less than
370:54 - or equal to 30
370:57 - then we will use the student
371:02 - t table
371:05 - for finding critical
371:09 - values
371:19 - this table shows us the amount of area
371:22 - that we're going to get in a single tail
371:24 - of the t distribution you'll notice the
371:27 - shape looks very similar but our degrees
371:30 - of freedom
371:31 - are going to determine the critical
371:33 - values that we need to calculate
371:37 - so what we'll do is we'll first find out
371:38 - the degrees of freedom of our problem
371:41 - maybe we've got 11 degrees of freedom
371:44 - then we'll figure out how much area we
371:46 - want in one tail maybe we want one
371:49 - percent in one tail
371:51 - the table would then give us the
371:53 - critical value that we can use to
371:56 - calculate a confidence interval
372:00 - one more thing you'll notice is the very
372:01 - last row
372:03 - of the table
372:05 - is labeled z
372:07 - because that's when we pass a sample
372:09 - size of 30 or 30 degrees of freedom
372:12 - and at that point the t distribution
372:15 - starts to look like the normal
372:17 - distribution and you should recognize
372:19 - several of the numbers in this row as
372:21 - the critical values we used with
372:23 - proportions
372:25 - those are the z values that give us the
372:27 - area we want in each tail
372:30 - so once we're past 30 degrees of freedom
372:32 - we'll just use those normal values
372:41 - well now that we're kind of familiar
372:43 - with this idea of the student t
372:44 - distribution that we have to use if we
372:46 - don't know the population standard
372:47 - deviation
372:49 - let's talk about how we can use that to
372:51 - make a confidence interval
372:57 - for means
373:02 - first off the distribution
373:08 - for the mean if we don't know the
373:10 - standard deviation we'll just say is t
373:13 - with the subscript that is the degrees
373:16 - of freedom
373:17 - and remember the degrees of freedom is
373:19 - equal to one less
373:21 - than the sample size
373:24 - so that's the distribution we're working
373:26 - with
373:27 - the t distribution
373:31 - so we need to calculate an error
373:34 - that exists between the population
373:37 - and the sample mean
373:39 - that error
373:43 - actually we'll do a colon that error is
373:46 - equal to
373:47 - our t sub alpha over 2
373:50 - very similar to our z sub alpha over 2
373:52 - but this time we'll use the t table and
373:54 - the correct number of degrees of freedom
373:58 - times the standard deviation of the
374:00 - sample
374:01 - divided by the square root of the sample
374:03 - size
374:06 - and once we know the error we can find
374:08 - the confidence
374:10 - interval
374:14 - and very similar to proportions with the
374:16 - confidence interval we will subtract and
374:18 - add the error to our statistic
374:21 - so we'll take our x bar and we'll
374:22 - subtract the error to get our low value
374:26 - and our x bar and we'll add the error
374:28 - to get our high value
374:31 - and that's the confidence interval
374:34 - using the error we just calculated
374:37 - these three pieces will work together
374:40 - to get us
374:41 - our confidence interval
374:45 - so let's try an example
374:53 - you are interested
375:00 - in the average cost
375:07 - of a smartphone
375:15 - so you take a sample
375:23 - of 16 smartphones
375:33 - and find
375:36 - a mean cost
375:43 - of 531 dollars
375:48 - with a standard
375:54 - deviation of eighty-three dollars
376:02 - we are going to number one construct
376:07 - a ninety percent
376:10 - confidence interval
376:23 - a couple things we need to know to
376:24 - conduct this 90 confidence interval
376:27 - first our alpha
376:29 - the amount of area in both tails if it's
376:32 - a 90 confidence interval is going to be
376:34 - 0.10
376:36 - so the alpha over 2
376:38 - looking at just one tail is half of that
376:40 - or 0.05
376:46 - for our t distribution we need to know
376:48 - the number of degrees of freedom
376:50 - the degrees of freedom
376:52 - is always one less than the sample size
376:55 - so we've got 16 smartphones minus one we
376:58 - have 15 degrees of freedom
377:04 - and so now we're ready to calculate our
377:06 - t value that is 0.05 in the tail
377:11 - and 15
377:12 - degrees
377:14 - of freedom
377:18 - going down our degrees of freedom on the
377:20 - table we want 15 degrees of freedom and
377:22 - we want 0.05 area
377:25 - in that tail
377:28 - so we go down and across and we find
377:31 - a t
377:33 - value
377:34 - of 1.753
377:40 - 1.753
377:45 - now we're ready to calculate the error
377:52 - the error is that t value 1.753
377:58 - times the standard deviation of my
378:00 - sample size which was 83 dollars
378:04 - divided by the square root of my sample
378:07 - size
378:09 - which is 16.
378:11 - and putting that on my calculator we get
378:13 - an error of 36 dollars and 37 cents
378:23 - so if that's the maximum error between
378:26 - my sample mean and the population mean
378:29 - we just have to subtract and add it to
378:32 - my sample mean the 531 dollars minus the
378:36 - error of 36.37
378:38 - and the 531 dollars plus the error of
378:41 - 36.37
378:44 - gives me a 90 confidence interval
378:48 - of 494.63
378:52 - up to 567.37
379:03 - and very similar to how we constructed a
379:06 - confidence interval with proportions and
379:08 - then interpreted it
379:10 - we will also interpret
379:15 - the confidence interval
379:17 - for the means
379:19 - following almost the exact same script
379:22 - we estimate
379:26 - with 90 percent confidence
379:33 - the true
379:38 - population and then state the parameter
379:41 - in context
379:44 - mean
379:46 - smartphone cost
379:52 - is between
379:57 - 494 dollars and 63 cents
380:02 - and 567
380:05 - and 37 cents
380:08 - so as we can see constructing a
380:10 - confidence interval with a mean is very
380:13 - similar to how we constructed a
380:14 - confidence interval
380:16 - with a proportion
380:18 - we've got a different distribution
380:20 - a slightly different formula for the
380:22 - error but the exact same idea so you
380:24 - should be able to try a few of these and
380:26 - we'll talk about confidence intervals a
380:28 - little bit more in class
380:37 - now that we've gotten comfortable with
380:38 - working with the t distribution and
380:40 - making a confidence interval for a mean
380:43 - we're ready to do a hypothesis test
380:46 - for a mean
380:47 - so the question we're going to answer is
380:50 - how
380:53 - do we
380:54 - conduct
380:59 - a hypothesis test
381:08 - for
381:08 - a
381:09 - mean
381:14 - and the process of a hypothesis test is
381:17 - always the same whether we're talking
381:19 - about one proportion or two proportions
381:22 - or in this case one mean the only
381:24 - difference is we have a few different
381:26 - formulas
381:32 - for the mean
381:34 - to help us calculate that important test
381:36 - statistic that will help create the
381:39 - p-value
381:41 - first off for the mean the distribution
381:48 - because we don't know the population
381:50 - standard deviation the distribution of
381:52 - the mean
381:53 - is a t distribution with a subscript for
381:56 - the degrees of freedom
381:58 - one less than the sample size
382:02 - the way the standard error is calculated
382:09 - the standard error
382:10 - is equal to the standard deviation of
382:13 - the sample
382:14 - divided by the square root of n
382:18 - and then we'll use that standard error
382:20 - to calculate our test statistic
382:28 - and our test statistic is going to be t
382:30 - equal to
382:32 - the difference between the mean
382:34 - of the sample and the hypothesized mean
382:37 - divided by the standard error
382:40 - so these are the three pieces that will
382:44 - help us
382:45 - find
382:46 - the p-value to conduct our hypothesis
382:49 - test
382:51 - now because the t-distribution
382:55 - has a slightly different number based on
382:57 - the degrees of freedom we need a
382:59 - slightly better way than just looking up
383:01 - a value on a table to calculate
383:04 - the p-value in our t's
383:12 - the way we're going to do that is we're
383:14 - going to use our calculator
383:23 - first we have to set up the test
383:25 - and the way we set up the test is you're
383:27 - going to hit the stat button
383:31 - and then you will scroll
383:34 - over
383:36 - to
383:37 - tests
383:41 - then you will scroll
383:44 - down
383:46 - to the t
383:48 - test
383:52 - once you set it up you have to enter
383:57 - the stats
383:59 - from your study
384:01 - now the calculator has an option of
384:03 - entering the data or entering the stats
384:04 - we're going to enter the stats so
384:07 - if needed
384:11 - select
384:18 - stats so the calculator knows you're
384:21 - going to actually enter in the stats
384:24 - and the stats you're going to enter in
384:26 - first it'll ask for mu sub 0.
384:29 - that is the null hypothesis
384:37 - it's also going to ask you for x bar
384:40 - which is the sample mean
384:47 - it's also going to ask you for what they
384:48 - call sx
384:50 - which is the sample
384:53 - standard deviation
385:02 - then it'll ask you for n
385:05 - which is
385:07 - the sample size we know that one
385:13 - and finally it'll ask for mu
385:16 - which is the symbol
385:22 - in the alternate
385:26 - hypothesis
385:31 - whether that's less than greater than or
385:33 - a not equal to
385:37 - i'll show you how this process looks on
385:38 - the calculator but it really helps to
385:40 - have an example so let's
385:43 - do that let's build an example
385:50 - it is claimed
385:58 - the average
386:01 - page
386:04 - in a novel
386:08 - has
386:10 - 275 words
386:15 - per page
386:20 - to test this claim
386:28 - you sample
386:33 - 24
386:34 - pages
386:37 - of a novel
386:44 - and you find the average
386:47 - page
386:51 - has 260 words
386:59 - with a standard deviation
387:11 - of 34 words
387:19 - do you believe
387:24 - the claim
387:27 - is true
387:30 - if alpha
387:31 - equals 0.05
387:43 - well we start off every hypothesis test
387:45 - with the null hypothesis
387:48 - here we're talking about a population
387:49 - mean
387:51 - and the claim is that the mean is 275
387:55 - words per page
388:00 - for the alternative hypothesis we're not
388:02 - really saying it's less than or greater
388:03 - than you just want to know is the claim
388:06 - true so what we say is mu is not equal
388:09 - to 275.
388:13 - and because we have that not equal we're
388:15 - really dealing with a two
388:17 - tailed
388:18 - test
388:24 - where we've got our normal distribution
388:26 - the hypothesized mean of 275 is in the
388:29 - middle
388:30 - but it actually turned out to be 260
388:33 - which is less than it
388:36 - but because this is a two-tailed test
388:39 - we're going to shade
388:41 - both
388:42 - sides
388:46 - those are our x values we're going to
388:48 - calculate
388:51 - t values
388:53 - off of the distribution
389:00 - the mean is distributed as a t
389:02 - distribution because we don't know the
389:04 - standard deviation
389:06 - of the population
389:08 - but we do know the degrees of freedom is
389:11 - one less than the sample size so the
389:14 - degrees of freedom is 23.
389:19 - let's go to our calculator then to
389:21 - calculate the t value
389:23 - and the p-value
389:26 - again that keystroke that we're going to
389:28 - do is first you're going to hit the stat
389:30 - button which is right next to the arrow
389:34 - then we'll scroll over to test
389:38 - then we'll scroll down to the t-test
389:43 - we are going to input the actual
389:45 - statistics not the individual data
389:47 - values so we'll highlight statistics
389:51 - mu sub 0 is the null hypothesis at 275
389:56 - x bar is the sample mean our sample was
389:59 - 260.
390:02 - sx is the standard deviation of the
390:04 - sample which was 34.
390:08 - and in the sample size was 24.
390:13 - we're going to make sure we highlight
390:15 - the alternate hypothesis symbol not
390:18 - equal to
390:20 - and scroll down to calculate
390:25 - when we do you'll see the calculator
390:27 - gives us several things but what we're
390:28 - interested in most is t and p
390:31 - t the test statistic is negative 2.16
390:34 - and p the probability the null
390:36 - hypothesis is true
390:38 - given our sample is 0.0413
390:43 - so let's record that our test statistic
390:51 - is t equals negative 2.16
390:56 - we can add that to our picture
390:58 - to the left of zero
391:00 - and the p value
391:04 - is point zero
391:06 - four one three
391:14 - and what that p value means then
391:17 - that's the probability the null
391:19 - hypothesis is true so based on our
391:21 - sample
391:27 - the probability
391:33 - the average
391:37 - page
391:40 - in a novel
391:47 - having
391:49 - 275 words
391:52 - that probability is
391:55 - 4.13 percent
391:59 - and remember we said alpha was five
392:02 - percent
392:03 - five percent is the minimum probability
392:06 - where we will still believe the null
392:08 - hypothesis is true
392:11 - four percent is less than that so we can
392:13 - no longer believe
392:15 - the null hypothesis is true
392:21 - so we will make a decision
392:25 - to reject
392:27 - the null
392:28 - hypothesis
392:32 - and the reason for that decision
392:36 - is that the p-value
392:39 - is less than the alpha
392:42 - the p-value is 0.0413
392:44 - which is less than the alpha of 0.05
392:48 - that is too little probability there is
392:50 - overwhelming evidence to reject the null
392:53 - hypothesis
392:58 - and so
393:00 - we make our conclusion
393:05 - following our script we say that there
393:08 - is
393:09 - sufficient evidence
393:22 - to conclude
393:28 - and then we state the alternative
393:29 - hypothesis in context the alternative
393:31 - hypothesis was just not equal to or
393:34 - different than
393:35 - so there is sufficient evidence to
393:37 - conclude
393:40 - the
393:41 - average
393:45 - number of words
393:51 - per
393:52 - page
393:54 - in a novel
393:57 - is not
393:58 - or maybe we should say is different
394:06 - and then 475 words
394:16 - let's actually take this one step
394:18 - further and build a confidence interval
394:20 - for where we believe the actual
394:23 - mean number of words lies let's build a
394:26 - confidence interval
394:32 - and let's just do a 95 percent
394:34 - confidence interval
394:41 - now we know the distribution that we're
394:44 - dealing with
394:46 - we know the degrees of freedom off of
394:48 - that are one less than the sample size
394:51 - the degrees of freedom
394:55 - we said was 23 because the sample size
394:59 - is 24 pages so our degrees of freedom
395:03 - is 23.
395:06 - alpha
395:08 - the percent of chance that we're going
395:09 - to be wrong is 0.05 the opposite of the
395:12 - 95 percent
395:14 - so alpha over 2 is 0.025
395:17 - so we're looking for a t sub 0.025
395:23 - that has 23 degrees of freedom
395:26 - in our table
395:29 - looking at our table then we want 23
395:31 - degrees of freedom
395:34 - we want 0.025 and a tail
395:38 - and so that tells us that the critical
395:41 - value we're using this time is 2.069
395:49 - so t sub 0.025 is 2.069
395:55 - we're ready to calculate the error that
395:57 - might exist between our sample mean
396:00 - and the actual population mean
396:03 - remember the error is t sub 0.025
396:08 - times the standard deviation
396:11 - divided by the square root of the sample
396:13 - size
396:15 - so 2.069
396:19 - times the standard deviation
396:22 - we said the standard deviation was 34
396:24 - words
396:27 - divided by the square root of the sample
396:29 - size which was 24
396:32 - we end up with an error
396:34 - of about
396:35 - 14.4 words
396:42 - so for our confidence interval we will
396:44 - subtract and add that error to the mean
396:48 - we got
396:50 - in our sample
396:51 - our sample mean was 260.
396:53 - we'll subtract the 14.4 to get a low
396:56 - number
396:58 - we'll add the 14.4 to get a high number
397:01 - and so our confidence interval
397:03 - is 245.6
397:07 - through 274
397:12 - and you notice the hypothesized mean of
397:14 - 275 is outside of that confidence
397:17 - interval which is related to why we
397:19 - ended up rejecting
397:21 - the null hypothesis
397:25 - let's go ahead and interpret it
397:31 - we can estimate
397:38 - with 95 percent confidence
397:46 - the
397:48 - population
397:51 - and we're talking about a mean
397:54 - and put it in context
397:56 - number of words
398:01 - per page
398:06 - in a novel
398:10 - is between
398:16 - 245.6 words
398:20 - and 274.4
398:23 - words
398:28 - and now we have our confidence
398:32 - interval so hypothesis testing with a
398:35 - mean
398:36 - it should feel very similar to
398:37 - hypothesis testing with a proportion
398:39 - because the process of a hypothesis test
398:42 - is identical regardless of what we're
398:44 - studying
398:45 - the means are nice because we can use
398:47 - the calculator to make things a little
398:48 - bit shorter and quicker for us but the
398:51 - philosophy behind the hypothesis test is
398:53 - still exactly the same so try and take a
398:55 - look at a few of those
398:57 - we'll look forward to trying a few of
398:58 - these in class and answer any questions
399:00 - that you might have we'll see you then
399:09 - the next type of hypothesis test we're
399:11 - going to turn our attention to is a
399:12 - hypothesis test for two means as we
399:16 - attempt to answer the question how
399:20 - do we compare
399:23 - two
399:25 - means
399:30 - and just as this is the case with all
399:32 - the other different hypothesis tests the
399:34 - process is exactly the same
399:36 - the only difference is we have some
399:38 - formulas to help us set up our test
399:41 - statistic
399:45 - first the distribution
399:50 - for comparing two means when we're
399:52 - comparing two means we want to know if
399:53 - two separate groups have the same
399:56 - average is there a difference between
399:58 - the two groups or is one group higher or
400:00 - lower than the other group and when
400:02 - we're interested in comparing them what
400:04 - we're really comparing is the difference
400:07 - or the mean of the first group
400:10 - minus the mean of the second group
400:13 - and because we don't know those
400:15 - population standard deviations it's
400:17 - going to be a t distribution with a
400:20 - subscript that represents the degrees of
400:22 - freedom
400:25 - now the degrees of freedom
400:28 - is
400:29 - an ugly
400:31 - formula
400:34 - if you really want to know what it is
400:36 - you can look it up in your book it is in
400:38 - the section in your book
400:40 - preceding
400:41 - the practice assignments that we're
400:43 - doing for today
400:44 - it's ugly we are going to cheat and we
400:47 - will use a calculator
400:58 - we'll also use a calculator to find the
401:00 - t statistic but just so that we have it
401:02 - the standard error for two means is the
401:05 - square root of the first standard
401:07 - deviation squared
401:09 - divided by the first sample size
401:12 - plus the second standard deviation
401:14 - squared
401:15 - divided by the second sample size
401:18 - and then we use that standard error to
401:20 - calculate our test statistic t
401:24 - which is the difference in the means a
401:26 - and b
401:28 - divided by the standard error
401:32 - but as was the with the case with the t
401:35 - distribution with one mean
401:37 - it will be also the case with the t
401:39 - distribution with two means
401:42 - that we will use the calculator
401:46 - to do the hard calculations for us
401:50 - the setup is identical
401:54 - first you will hit the stat button
401:59 - then you will scroll over to tests
402:07 - and then you will scroll
402:10 - down
402:11 - but this time you're going to scroll
402:13 - down to select the two
402:16 - sample t-test
402:25 - once you're in the two-sample t-test you
402:27 - will enter
402:30 - all the stats we have
402:32 - from our problem
402:34 - and again if the calculator is expecting
402:36 - you to enter the data we're not going to
402:38 - enter the data so you might need to
402:40 - highlight
402:44 - stats if needed
402:51 - when you get in there the first thing
402:52 - it's going to ask you is for x bar 1
402:56 - that is the first
402:58 - sample mean
403:05 - then it will ask you for sx1
403:08 - that is the first
403:12 - sample standard deviation
403:22 - and it will ask you for n1
403:25 - which is the first
403:28 - sample
403:30 - size
403:32 - so we enter in all the information about
403:34 - the first group
403:35 - that we're going to compare
403:37 - to the second group
403:42 - as you might expect you'll see x bar 2 s
403:44 - x 2 and n 2
403:48 - is
403:50 - for
403:51 - the second
403:54 - sample that we will compare it with
404:02 - then it will give us a mu
404:05 - which is the
404:07 - alternate
404:10 - hypothesis
404:15 - symbol
404:18 - we need to tell the calculator is this a
404:20 - one-tailed test or a two-tailed test in
404:22 - which direction it is
404:25 - and finally the last thing the
404:26 - calculator will ask us for is if we have
404:30 - pooled data and for our purposes the
404:33 - answer is always going to be
404:36 - no
404:37 - we are not pulling the data
404:41 - and i'll show you what this looks like
404:43 - on the calculator again but again it's
404:45 - going to be easiest to see it
404:48 - if we have an example
404:51 - to work with
404:55 - so for our example you want to know
405:04 - if there is a difference
405:13 - in gpa
405:18 - of online students
405:25 - and face-to-face students
405:36 - so to determine this you survey
405:43 - 32
405:45 - online students
405:52 - who have an average gpa
406:03 - of 3.45
406:12 - with a standard deviation
406:22 - of 0.7
406:27 - you also interview
406:33 - 41
406:35 - face-to-face students
406:48 - who have an average
406:52 - gpa
406:57 - of 3.67
407:02 - with a standard deviation
407:12 - of 0.4
407:17 - if alpha equals 0.10
407:23 - can you conclude
407:28 - the groups
407:32 - are
407:33 - different
407:43 - we're comparing the average between the
407:46 - two groups not just a claim and testing
407:48 - against the claimed gpa we just want to
407:50 - know is there a difference between these
407:52 - two groups
407:53 - we have
407:54 - two means that we're comparing
408:00 - well the mean of the online students is
408:03 - hypothesized then to be the same as the
408:06 - mean of the face-to-face students
408:09 - again i'm using subscripts to make it
408:11 - really clear which group i'm talking
408:13 - about the null hypothesis
408:15 - no difference they are equal
408:19 - the alternative hypothesis is going to
408:21 - be either one is greater or they're not
408:23 - equal to each other
408:25 - this example didn't give me any inkling
408:27 - of a direction they didn't say face to
408:29 - face or higher or gpa
408:31 - or online students have a lower gpa or
408:34 - anything like that
408:35 - so we are just looking to see if the
408:38 - online students are not equal to the
408:40 - face-to-face students
408:44 - which means we have a two
408:46 - tailed test
408:52 - in other words we have our t
408:53 - distribution with our hypothesized
408:55 - difference
408:56 - we hypothesize that the difference
408:58 - between the gpas is zero
409:01 - and will reject
409:03 - on either tail whether it's higher or
409:06 - lower
409:07 - we will reject on either tail
409:11 - well the actual difference
409:16 - [Music]
409:18 - let's add that to our picture the actual
409:20 - difference in gpas and we have to
409:22 - subtract in the same order of the
409:24 - hypothesis so the online gpa has to come
409:27 - first
409:28 - the online gpa was 3.45
409:33 - and we subtract the face-to-face gpa of
409:35 - 3.67
409:38 - and we end up with .22
409:41 - negative 0.22
409:44 - so negative point 22
409:47 - is the x value we're looking for
409:51 - we need to figure out
409:54 - what the t values are
409:59 - well our distribution
410:06 - the difference between online students
410:09 - and face-to-face students
410:12 - is a t-distribution
410:14 - but we don't really know the degrees of
410:15 - freedom because the it's got that ugly
410:18 - formula so we're going to go to our
410:20 - calculator
410:26 - again on the calculator the way we get
410:29 - to the test is we'll go to stat it's
410:31 - right next to the arrows
410:33 - we'll scroll over to test
410:35 - and we'll scroll down to the two sample
410:38 - t-test
410:42 - make sure stats is highlighted because
410:44 - that's what we have
410:46 - our first group the online students we
410:49 - said the online students have an average
410:51 - gpa of 3.45
410:54 - with a standard deviation of 0.7
410:58 - and we said that there are 32 of them
411:03 - the second group the face-to-face
411:06 - students have an average gpa of 3.66
411:11 - and a standard deviation of 0.4
411:15 - and there were 41 of them
411:21 - we select an alternate hypothesis of not
411:23 - equals
411:25 - pooled is going to be no for our
411:27 - purposes
411:29 - and when we hit calculate
411:32 - you'll see the calculator gives us the
411:34 - three key pieces of information we need
411:37 - the degrees of freedom to finish out the
411:40 - distribution notice it's an ugly decimal
411:42 - that's very common with two samples
411:45 - and a t value and a p value
411:49 - let's copy that information over so the
411:51 - distribution had
411:53 - 46.5 degrees of freedom
411:59 - the t value
412:02 - that came off the calculator was
412:04 - negative 1.59
412:06 - so negative 1.59 to the left of 0
412:11 - and a p-value
412:14 - equal to 0.1193
412:20 - what does that p-value of 0.1193 mean
412:25 - well remember the p-value
412:27 - is the probability the null hypothesis
412:30 - is true
412:31 - given our survey
412:33 - so based on our survey
412:41 - the probability
412:47 - the average
412:50 - gpa
412:55 - is the same
412:59 - for online
413:02 - and face to face students
413:08 - is
413:08 - 11.93 percent
413:15 - and we compare that p-value to the alpha
413:17 - which is the smallest probability where
413:20 - we would still believe the null
413:22 - hypothesis is true
413:24 - we have a greater probability so we're
413:27 - going to go ahead and say
413:31 - our decision
413:35 - because the probability the p-value is
413:38 - bigger than alpha we will fail
413:41 - to
413:42 - reject
413:44 - and the reason for that
413:48 - is the p-value
413:50 - is greater than alpha
413:52 - there's more evidence for the null
413:54 - hypothesis
413:56 - the p-value is 0.11 0.1193 alpha's only
413:59 - 0.10
414:01 - and so for our conclusion
414:06 - in context because we failed to reject
414:09 - we say there is
414:12 - not
414:13 - sufficient
414:17 - evidence
414:20 - to conclude
414:25 - and then we will state the alternate
414:26 - hypothesis in context that there is a
414:29 - difference between the means
414:33 - there is
414:35 - a difference
414:42 - between
414:45 - the mean
414:47 - gpa
414:50 - of online
414:53 - and face to face students
415:01 - again we should start to feel really
415:04 - familiar and comfortable with this
415:05 - process of going through a hypothesis
415:08 - test
415:11 - it's exactly the same regardless of if
415:13 - we're testing one or two means or
415:15 - proportions
415:17 - the process is exactly the same the only
415:19 - tweak that's different each time is
415:21 - actually calculating the distribution
415:22 - and the test statistic
415:25 - but we should be very good at the
415:26 - process of setting up and conducting the
415:28 - hypothesis test by now
415:30 - so you can take a look at those on the
415:32 - assignment we'll work out with them a
415:34 - little bit more in class and we'll look
415:36 - forward to seeing you then
415:44 - another hypothesis test we can do is
415:47 - hypothesis testing which with what are
415:49 - called
415:50 - matched pairs
415:52 - matched pairs are where we have before
415:54 - and after data and we are looking to see
415:56 - are things the same or was there some
415:58 - type of improvement or maybe we'll pair
416:00 - together couples or and see if there's a
416:03 - difference between you know the
416:04 - husband's score and the wife score or
416:06 - maybe we'll pair together twins to see
416:08 - if there's any difference between the
416:10 - twins
416:11 - or maybe we'll compare your left hand to
416:13 - your right hand but the data is paired
416:15 - together and we're looking to see if
416:17 - there's any type of difference that is
416:20 - matched pairs and so the question we're
416:23 - going to ask is how
416:25 - do
416:26 - we
416:27 - hypothesize
416:34 - test
416:40 - for improvement
416:46 - or maybe be better to say a difference
416:52 - has there been any change
416:56 - and this is that idea of
416:58 - matched pairs all the data comes in
417:02 - pairs and the pairs are matched together
417:06 - usually we have that before or after
417:07 - score
417:09 - and the way matched pairs work is we're
417:11 - going to do
417:12 - a t-test
417:17 - for one
417:19 - mean
417:21 - just like we did a t-test for one mean
417:23 - before the only difference is we will
417:25 - first
417:26 - find
417:28 - the difference
417:33 - for each pair
417:37 - and then we'll use those differences as
417:40 - our one variable to figure out whether
417:43 - or not there is a positive negative or
417:45 - no difference between the before after
417:48 - data
417:51 - so with that in mind we've got some
417:52 - equations that we need to know to run
417:54 - this test the distribution
417:59 - it's very similar to the t test
418:00 - distribution because it is a t test the
418:02 - only difference is we're going to put a
418:04 - little subscript of d on the x bar to
418:06 - represent the average distance
418:09 - is a t distribution with a subscript
418:12 - representing the degrees of freedom
418:14 - where the degrees of freedom is simply
418:16 - the sample size minus one
418:20 - and then we can calculate the standard
418:22 - error
418:26 - and the standard error is simply the
418:28 - standard deviation of the differences
418:31 - divided by the square root of the sample
418:34 - size
418:35 - and we can use that standard error to
418:37 - find our test
418:39 - statistic
418:44 - which very similar to the test statistic
418:46 - for a single mean is t is equal to the
418:49 - difference between
418:52 - the
418:53 - average difference
418:55 - and the hypothesized difference
418:58 - divided by the standard error
419:01 - and the hypothesized difference is
419:03 - usually
419:05 - zero we usually assume there's no
419:07 - difference between before and after
419:09 - scores and we're looking to see is there
419:11 - a difference or is it positive or
419:13 - greater than zero or is it negative or
419:15 - less than zero
419:18 - now like before with the t-test though
419:20 - we're going to save all that work with
419:22 - using our calculator to do a lot of
419:25 - manual crunching of the data for us
419:31 - so first thing we're going to have to do
419:33 - on our calculator is we're going to have
419:35 - to tell the calculator all of our data
419:38 - so here's how you enter
419:40 - the data
419:43 - into your calculator
419:45 - you're going to start by hitting the
419:46 - stat button
419:50 - and then you will select
419:53 - edit
419:55 - to edit a list and the list
419:59 - the edit feature will already be
420:01 - highlighted when you hit stats so you
420:02 - just really have to hit enter
420:04 - and then in l1 you're going to enter
420:08 - the before data
420:11 - or the first set of data
420:15 - and then in l2
420:18 - you will enter
420:20 - the after data
420:23 - or the second set of data
420:28 - and then for l3 what you're going to do
420:30 - is you're going to scroll up and
420:31 - highlight
420:34 - l3
420:37 - and do
420:38 - l2
420:40 - minus l1 and the calculator will
420:43 - automatically
420:46 - subtract and find all the differences
420:48 - and fill list three with the differences
420:52 - now the way we get that is you'll hit
420:54 - the second button
420:57 - and you'll hit the number two which will
420:59 - give you l2
421:02 - minus
421:05 - the second button
421:07 - and then you'll hit the one which will
421:09 - give you
421:11 - the l1
421:14 - and now in l3 you will have a list of
421:16 - all of the differences between the
421:20 - before and after
421:21 - a positive difference means it got
421:23 - bigger
421:24 - a negative difference will mean it got
421:26 - smaller
421:30 - then you're ready to actually run the
421:32 - t-test
421:39 - so you can hit stat
421:45 - scroll over
421:48 - to
421:49 - tests
421:52 - and then scroll down
421:54 - [Music]
421:57 - to
421:58 - t-test
422:00 - and even though there's two sets of data
422:02 - before and after we're actually just
422:04 - working with the differences so it's
422:06 - just one t-test don't do the two-sample
422:08 - t-test it's a single sample t-test that
422:11 - single sample
422:13 - is the difference between the before and
422:15 - after checking to see if the numbers
422:17 - went up
422:19 - or down
422:20 - [Applause]
422:24 - so when you're running the t-test you
422:25 - have to enter in some information
422:28 - first thing you want to do
422:30 - is you want to highlight
422:35 - data
422:36 - because you're going you have entered
422:38 - the data into the calculator we don't
422:40 - have the summary statistics we actually
422:42 - enter the data this time
422:44 - and that is different than our last
422:46 - t-test
422:48 - then you'll see mu sub zero
422:50 - that is the hypothesized difference
423:02 - which is usually zero
423:04 - we usually hypothesize that there's no
423:06 - difference or that the difference equals
423:08 - zero
423:11 - then it'll ask for the list
423:14 - the list you want to be l3 where you
423:17 - have all those differences entered and
423:19 - the way we select l3 is you hit the
423:21 - second
423:23 - and then the number three will give that
423:25 - third
423:27 - list
423:30 - and finally we'll enter in mu which is
423:34 - the alternate
423:37 - hypothesis
423:41 - symbol
423:44 - are we looking for it to be smaller
423:45 - after the treatment bigger after the
423:47 - treatment or just not equal to zero
423:50 - after the treatment
423:53 - and as before it's this is a lot easier
423:55 - to see with an example so let's do an
423:58 - example where we check these matched
424:00 - pairs for some type of improvement
424:12 - a football coach
424:20 - wants to know
424:27 - if a strength class
424:34 - can help
424:36 - improve
424:40 - his players
424:46 - bench press weight
424:57 - the before and after
425:02 - data
425:04 - is below
425:09 - so we're going to have
425:12 - players
425:15 - we'll just mark the players with letters
425:18 - to protect their identity a b
425:21 - c and d
425:26 - and we'll have weights
425:28 - for before the class
425:31 - and after the class
425:35 - so before the class a bench 205
425:38 - after a bench 295.
425:41 - that looks pretty good
425:43 - before the class b benched 241
425:47 - after b benched 252.
425:50 - not as dramatic but still an increase
425:53 - c benched 338 before the class and 3 30
425:58 - after the class
425:59 - c went down a bit
426:01 - and d benched 368 and afterwards benched
426:05 - 360.
426:11 - so the question is
426:14 - if alpha equals 0.05
426:18 - can the coach
426:22 - conclude
426:24 - the class
426:27 - was
426:28 - helpful
426:40 - we're going to run a hypothesis test of
426:42 - matched pairs to see if there's a
426:44 - significant difference
426:47 - first the null hypothesis
426:50 - is that the average distance difference
426:55 - is equal to zero
426:57 - there's no difference
426:59 - the alternative hypothesis is that the
427:02 - average difference is going to be
427:04 - positive or greater than zero because
427:06 - the coach wants it to be helpful he
427:08 - wants the difference to be positive he
427:10 - wants it to be higher after
427:13 - than before
427:16 - which means we have a one tail test or
427:18 - better said a right tailed test
427:26 - if we were to draw a picture of this
427:31 - the hypothesized difference is zero
427:35 - somewhere over the right we hope to see
427:37 - improvement and we hope that right tail
427:39 - shows there was enough improvement
427:41 - out of the class
427:48 - well our distribution
428:00 - is that the average difference
428:03 - is a t distribution
428:06 - and the degrees of freedom is one less
428:07 - than the sample size there are four
428:09 - players one less than that is three
428:12 - it's very small degrees of freedom it's
428:14 - going to take quite a difference in
428:16 - order to
428:17 - say there's a difference
428:19 - so from here let's go to our calculator
428:21 - and see if it can help us
428:24 - find
428:26 - the differences
428:28 - to enter information into the calculator
428:30 - we're going to hit the stat button right
428:32 - next to the arrows
428:33 - edit is already highlighted
428:37 - if i already had numbers in my list
428:40 - let's say there were numbers already in
428:41 - here that i didn't want
428:43 - if you scroll up and highlight the list
428:46 - and hit the clear button
428:49 - and then enter it'll clear out the list
428:51 - so there's nothing left in the list
428:55 - the first list is for the before data so
428:58 - we'll enter in our data 205
429:00 - 241
429:02 - 338
429:04 - and 368.
429:09 - the second list is our after data
429:12 - make sure they're entered in the same
429:13 - order 295
429:15 - 252
429:17 - and
429:18 - 330
429:20 - and 360.
429:28 - then list three is where we're going to
429:30 - put our differences
429:32 - so if we scroll up to highlight list
429:34 - three
429:36 - and then we're going to say take list
429:38 - two and subtract list one
429:41 - hit second
429:44 - and then the number two
429:46 - that'll grab list two
429:48 - minus second
429:51 - and then the number one and now i see
429:54 - list three is equal to two minus one
429:57 - when i hit enter
429:59 - i will see all the differences
430:01 - and that's the data we're going to use
430:04 - in our single mean t-test
430:09 - to run the t-test we'll hit stat
430:12 - scroll over to test
430:15 - scroll down to the t-test
430:21 - and this time we've actually entered the
430:23 - data into the calculator so we'll scroll
430:25 - over and highlight the data enter
430:30 - the hypothesized mean
430:32 - is zero
430:36 - for the list our data is in list three
430:39 - that's where the differences are so
430:40 - we'll hit second
430:42 - and the number three to give us list
430:44 - three
430:46 - leave the frequency alone
430:48 - for the alternate hypothesis
430:51 - we want to show that
430:53 - the mean is greater than zero so we need
430:56 - to select the greater than alternate
430:58 - hypothesis
431:02 - and then we're ready to actually
431:03 - calculate
431:07 - when we do that we get our t value of
431:09 - 0.91 and a p-value
431:12 - of 0.2149
431:14 - we're also told that the average
431:16 - difference is 21.25
431:19 - so we can add that to our picture the
431:21 - average difference is 21.25
431:26 - but when we convert that to a t value
431:29 - the t value was only 0.91
431:33 - so the test statistic
431:38 - is that t equals 0.91 which gives us a
431:42 - p-value
431:45 - an area for that tail of 0.2149
431:49 - [Music]
431:54 - what that p-value means is given our
431:57 - sample
432:03 - the probability
432:08 - the strength class
432:17 - made no difference
432:27 - in bench press weight
432:31 - because that's the null hypothesis
432:35 - is
432:36 - 21.49 percent
432:40 - there's a 21 and a half percent chance
432:42 - that the class had no effect on bench
432:45 - press weight
432:47 - and if we look at our alpha of 0.05 we
432:49 - see that is much bigger than the 0.05
432:51 - alpha alpha is the minimum probability
432:54 - where we still believe the null
432:55 - hypothesis is true
432:58 - we are well beyond that probability much
433:00 - higher so our decision
433:09 - is to fail
433:11 - to reject
433:14 - the null hypothesis
433:22 - and the reason for that decision
433:25 - is the p-value
433:27 - the probability that the null hypothesis
433:29 - is true
433:30 - is greater than alpha which is our
433:32 - decision break
433:34 - with numbers the 0.2149 is greater than
433:38 - the 0.05
433:41 - and so for our final conclusion
433:47 - we will state that there
433:49 - is
433:51 - not
433:52 - sufficient evidence
433:58 - and then we state the alternative
433:59 - hypothesis
434:01 - in context
434:03 - to conclude
434:08 - the strength class
434:14 - increased
434:20 - bench
434:22 - press
434:24 - weights
434:30 - and so what you see is with the matched
434:32 - pair
434:33 - hypothesis test it really is exactly the
434:36 - same as a single mean hypothesis test
434:39 - with the t distribution the only
434:41 - difference is here we will focus
434:43 - exclusively on the differences so first
434:45 - we have to calculate that after minus
434:48 - before relationship to see how much
434:50 - things have improved or decreased
434:53 - after the treatment
434:55 - so that's how we do a hypothesis test
434:57 - for matched pairs take a look at a few
434:59 - of these on the assignment and we will
435:01 - see you in class to continue to work on
435:03 - matched pairs a little more
435:12 - quite often we read a claim that a
435:15 - certain population is distributed in
435:17 - certain way maybe we say that 20 percent
435:19 - fall in one category 40 fall in another
435:22 - category and 30 percent fall in another
435:24 - category
435:26 - the question we're going to take a look
435:27 - at today is how good that fit
435:30 - actually is to data we might collect so
435:34 - the official question here
435:36 - how do we
435:41 - test
435:44 - if data
435:47 - fits
435:49 - a claimed
435:52 - distribution
436:00 - and as we do this hypothesis test we
436:02 - have to run into a new distribution that
436:06 - tests how well data fits a claimed
436:09 - distribution and the new distribution is
436:12 - what we call the chi
436:14 - squared distribution
436:18 - and we use the greek letter chi with a
436:21 - little squared on it
436:22 - looks like an x with tails on the ends
436:26 - and this chi-squared distribution a few
436:29 - characteristics of it it is a
436:32 - non-symmetrical distribution
436:38 - and it is skewed
436:43 - right
436:49 - so unlike the normal and the t
436:50 - distribution which is perfectly
436:52 - symmetrical the chi-squared is skewed
436:54 - right in fact the shape itself varies
436:57 - based on the degrees of freedom there is
437:00 - a different
437:03 - shape
437:08 - based on
437:10 - the degrees
437:13 - of freedom
437:17 - and another unique thing about the
437:19 - chi-squared is that it is always
437:24 - greater
437:27 - than zero
437:32 - with the t-distribution and the normal
437:34 - distribution we found it's symmetrical
437:36 - around zero we had positive values and
437:39 - negative values representing if we were
437:40 - left or right of the mean the
437:42 - chi-squared doesn't do that the
437:44 - chi-squared actually starts at zero and
437:47 - depending on the degrees of freedom if
437:49 - there's only one degree of freedom
437:53 - one degree of freedom the graph
437:55 - looks something like this
437:58 - but if we increase the degrees of
438:00 - freedom to maybe three it's going to
438:02 - look something like this
438:07 - with a little hump skewed to the right
438:10 - and the more we increase the degrees of
438:12 - freedom that hump is going to move
438:15 - slightly over
438:17 - so this red line might represent
438:20 - 10
438:20 - degrees of freedom and so you see the
438:22 - shape varies quite dramatically based on
438:26 - the number of degrees of freedom we have
438:31 - so let's look at how we can use the
438:34 - chi-squared distribution to test a
438:37 - claimed distribution
438:40 - what we're going to do is the goodness
438:42 - of fit test
438:49 - or we've got some claim that a certain
438:51 - percent
438:52 - fall in various categories is that claim
438:55 - accurate does it does the data fit that
438:59 - distribution
439:00 - well is it a good distribution
439:04 - the test statistic we will use
439:12 - is chi squared
439:15 - is equal to the sum
439:18 - of the observed frequency
439:21 - minus the expected frequency squared
439:24 - divided by the expected frequency
439:28 - so we've got some new variables o is the
439:30 - observed
439:34 - frequency
439:38 - and e is the expected
439:43 - frequency
439:48 - and this chi-squared value we're going
439:50 - to calculate by hand it's not too bad
439:53 - but it is one we should know how to
439:56 - do
439:58 - now
439:59 - with chi squared we have to know the
440:00 - degrees of freedom
440:02 - the degrees of freedom with the goodness
440:04 - of fit is one less
440:09 - than the number of categories
440:22 - and then for our null hypothesis and
440:24 - alternative hypothesis
440:27 - with the goodness of fit test we usually
440:29 - state it in a sentence rather than
440:32 - symbolically like we did with the means
440:34 - in the t distribution or the normal
440:36 - distribution
440:38 - so in words the null hypothesis is that
440:41 - the data
440:44 - fits
440:48 - the distribution
440:56 - the alternate hypothesis is that it's
440:58 - not equal to the distribution or the
441:00 - data
441:02 - does not
441:05 - fit
441:07 - the distribution
441:17 - and what's interesting about the chi
441:19 - squared because it starts at zero and is
441:20 - skewed right that alternate hypothesis
441:23 - is always going to be a right
441:27 - tailed test
441:30 - in fact with chi squared almost always
441:32 - we're working with a right-tailed test
441:34 - there's only one context which we'll
441:36 - talk about in another video where we
441:38 - could have a two-tailed test or a
441:39 - left-tailed test but in general chi
441:42 - squared is a right tailed test
441:47 - now we can use our calculator to help us
441:49 - find
441:50 - the area that's in that right tail so
441:53 - let's take a look at how the calculator
441:56 - can do that
442:05 - what we'll do on the calculator is we
442:07 - will find
442:09 - the area
442:13 - from the test statistic
442:21 - all the way out to infinity how much
442:25 - area is in that right tail
442:27 - the problem is our calculators can't do
442:30 - infinity
442:32 - so we're going to put a number in that's
442:33 - pretty darn close to infinity we're
442:35 - going to use 10 to the 99th power to
442:38 - represent infinity because there's going
442:40 - to be very little area that's past 10 to
442:42 - the 99th power that's a 1 with 99 zeros
442:45 - after it a 100 digit number that's
442:48 - pretty darn close to infinity
442:53 - so the calculator keystrokes we want to
442:54 - do is first you'll hit the second button
442:59 - then you'll select the distribution
443:02 - function
443:03 - which is really the vars button and then
443:06 - above it with the second feature you're
443:08 - getting distribution
443:10 - then you can scroll down
443:15 - to the
443:17 - chi-squared cdf
443:25 - once you've selected the chi-squared cdf
443:28 - what you'll do is you'll enter the test
443:30 - statistic
443:36 - to represent the minimum value we're
443:39 - finding the area of
443:41 - then you'll do a comma
443:45 - then you will do
443:47 - the infinity which is 10
443:50 - raised to the 99th power
443:55 - then you will do a comma
443:59 - and then you'll type in the degrees of
444:02 - freedom
444:07 - we'll see this process work out when we
444:09 - do our example
444:12 - now
444:22 - let's say a researcher wants to verify a
444:25 - claim about her community
444:49 - she wants to verify the claim about her
444:51 - community
444:53 - that 40 percent
444:57 - of the residents
445:02 - speak spanish in the home
445:14 - 10 percent
445:16 - speak russian
445:23 - 45 percent
445:28 - speak english
445:34 - and five percent
445:38 - speak other languages
445:47 - so the researcher does a survey
445:51 - in a survey
445:56 - of 200 community members
446:07 - 71 speak spanish
446:16 - 23
446:18 - speak russian
446:24 - 102
446:26 - speak english
446:34 - and for
446:38 - speak another language
446:49 - if alpha equals 0.05
446:53 - can the researcher
446:59 - conclude
447:04 - the claimed distribution
447:13 - is accurate
447:30 - we're going to go through the same
447:32 - process of a hypothesis test that we've
447:34 - gone through before we've just got a
447:36 - different test statistic but
447:37 - everything's exactly the same from there
447:40 - our null hypothesis we said is that the
447:43 - language
447:46 - put it in context but that the claim is
447:49 - accurate that the language spoken
447:55 - in the home
447:59 - matches
448:01 - the distribution
448:06 - of the claim
448:15 - the alternative hypothesis is that it
448:16 - does not match the claim that the
448:19 - language
448:20 - spoken in the home
448:29 - does not
448:32 - match
448:36 - the claim
448:38 - or the distribution of the claim
448:42 - i should say distribution because that's
448:44 - important
448:52 - and we said with the chi-squared we are
448:55 - almost always dealing with a
448:56 - right-tailed test
448:59 - we end up more in the right tail the
449:01 - more different we are from the
449:02 - distribution
449:05 - the distribution itself
449:11 - is a chi-squared distribution
449:14 - and we'll do a little subscript for the
449:16 - number of degrees of freedom
449:18 - we've got spanish russian english and
449:22 - other four languages
449:26 - the degrees of freedom is always one
449:28 - less than the number of categories so we
449:29 - have three degrees of freedom
449:34 - and now we're ready to calculate the
449:36 - test statistic
449:43 - and this is where we're going to do our
449:45 - real work
449:49 - we've got
449:51 - spanish
449:53 - russian
449:55 - english
449:57 - and other
450:02 - the claimed proportion
450:07 - can't get it all on one page here but uh
450:10 - we've got 40 percent with spanish
450:13 - 10 with russian so spanish is 0.4
450:16 - russians 0.1
450:18 - english 45 0.45 others 5
450:23 - 0.05
450:30 - the expected value
450:37 - we've done a survey of 200 community
450:40 - members
450:43 - so of those 200 we should expect 40 to
450:47 - speak spanish so we can calculate forty
450:49 - percent or point four times two hundred
450:52 - and that gives us we would expect eighty
450:55 - people in our survey
450:57 - to speak
450:58 - spanish
451:00 - for the russian we'd expect it to be ten
451:02 - percent ten percent of the two hundred
451:05 - ten point ten times two hundred is
451:08 - twenty
451:11 - for the english point four five times
451:13 - two hundred is ninety
451:16 - and for the other .05 times 200 is 10.
451:21 - so we use those percentages
451:23 - and the total sample size to calculate
451:26 - what we expect to happen
451:31 - but something different happened in our
451:33 - observed probabilities what we observed
451:36 - is 71 spoke spanish this is from our
451:38 - survey
451:40 - we observed 23 russians
451:43 - we observed 102 english and we observed
451:47 - four other languages
451:53 - so now what we can do is we can use that
451:59 - chi-squared statistic
452:01 - is the observed minus the expected
452:04 - squared divided by the expected
452:09 - similar to how we did the standard
452:10 - deviation by hand back in chapter one
452:13 - we're going to just make a different
452:14 - column so that we can sum
452:17 - those observed minus expected squares
452:19 - over expected
452:21 - so first observed minus expected
452:25 - observed minus expected the observed
452:28 - comes first so 71 minus 80 is negative
452:30 - 9.
452:32 - 23 minus 20 is negative 3
452:35 - 102 minus 90 is 12.
452:42 - oops i'm sorry 23 minus 20 is a positive
452:45 - 3
452:47 - and 4 minus 10 is negative 6.
452:54 - then we square those values the observed
452:57 - minus expected
452:58 - squared
453:00 - 9 squared is 81. 3 squared is 9. 12
453:04 - squared is 144 and negative 6 squared is
453:08 - 36.
453:13 - now we take the observed minus the
453:15 - expected squared and we divide
453:19 - by the expected value
453:21 - the expected value column
453:25 - is that second column here
453:27 - so 81 divided by 80 is 1.0125
453:35 - 9 divided by 20 is 0.45
453:42 - 144 divided by 90 is 1.6
453:49 - and 36 divided by 10
453:52 - is 3.6
453:58 - now we're ready to actually find that
454:00 - sum
454:03 - of the observed minus expected squared
454:05 - divided by the expected by adding up
454:08 - that last column 1.0125
454:11 - plus 0.45 plus 1.6 plus 3.6 gives us
454:15 - 6.6625
454:19 - that is our chi
454:21 - squared test statistic
454:28 - now that we have our test statistic we
454:31 - are ready to calculate a p-value
454:36 - and we're going to do that p-value by
454:38 - doing the chi-squared cdf
454:41 - we're going to go from a low value of
454:43 - 6.6625
454:47 - to a high value of infinity which we use
454:49 - 10 to the 99th power for infinity
454:52 - and we said we had three degrees
454:55 - of freedom
454:59 - so on our calculator to get the
455:01 - chi-squared distribution we're going to
455:03 - hit second
455:04 - and then the vars button which gives us
455:06 - distribution
455:09 - and we'll scroll down we want the
455:11 - chi-squared
455:12 - cdf it has to be the cdf make sure you
455:15 - don't do the pdf we're never going to
455:17 - use that one
455:19 - the cdf
455:22 - the lower limit is 6.6625
455:27 - the upper limit is infinity which will
455:29 - use 10 to the 99th power
455:32 - and the degrees of freedom is 3.
455:35 - and when i hit paste you see it types
455:37 - those numbers in for me
455:40 - if you don't have the newest model of
455:42 - the ti 84 you might just have to enter
455:44 - in these numbers with commas separating
455:45 - them
455:46 - get the same thing though we end up with
455:49 - .0835
455:52 - .0835
455:56 - and what that p value tells us is the
455:58 - probability that null hypothesis is true
456:01 - given our data
456:04 - based on the survey
456:09 - there is
456:11 - 8.35
456:14 - chance
456:20 - the proportion
456:24 - of
456:26 - people
456:29 - who speak
456:32 - various languages
456:37 - matches
456:41 - the claimed
456:43 - distribution
456:50 - eight percent chance that it matches the
456:51 - claim distribution but we said that
456:54 - alpha's 0.05 in other words we will
456:57 - believe the claim distribution all the
456:59 - way down to 5
457:01 - we only have an 8 percent or we still
457:04 - have an 8 so we're still going to
457:06 - believe that null hypothesis our
457:09 - decision
457:11 - is to fail to reject
457:15 - the null hypothesis
457:21 - and the reason for that decision
457:24 - is because our p-value is bigger than
457:26 - alpha there's enough evidence to still
457:29 - believe the null hypothesis
457:31 - specifically the p-value was 0.0835
457:34 - which is bigger than alpha which was
457:36 - 0.05
457:41 - so we can make a conclusion
457:44 - in context focusing on the alternative
457:46 - hypothesis we say that there is not
457:52 - sufficient evidence
458:00 - to conclude
458:06 - the distribution
458:12 - of languages
458:15 - spoken in the home
458:23 - is different
458:26 - than the claimed distribution
458:40 - not enough evidence to say it's
458:41 - different so we'll have to be
458:44 - stuck believing that distribution is
458:46 - true
458:47 - that's the goodness of fit test the
458:49 - hypothesis test process still exactly
458:51 - identical we just have a new
458:53 - distribution of chi squared we have to
458:56 - do a little bit of arithmetic to
458:57 - calculate that chi-squared value but
458:59 - it's not too bad
459:00 - so go ahead and take a look at a few if
459:02 - you want to practice some we'll talk
459:04 - about it more in class we will see you
459:07 - then
459:15 - another use for the chi-squared
459:17 - distribution is testing to see if two
459:20 - variables are dependent or independent
459:24 - of each other so the question for today
459:27 - is how do we test
459:35 - if
459:36 - two
459:38 - variables
459:41 - are dependent
459:45 - or independent
459:57 - and to do this we will test
460:00 - for
460:01 - independence
460:07 - what we'll do to test for independence
460:09 - is we will collect
460:13 - frequency data
460:19 - and organize
460:24 - in rows
460:26 - and columns
460:31 - for example we might
460:34 - compare
460:35 - gender
460:36 - to
460:38 - we'll just do male female
460:40 - to whether or not you played sports in
460:44 - high school
460:45 - yes or no
460:47 - and this will generate a table where
460:49 - we'll enter in
460:53 - how many yeses how many no's for each
460:55 - gender we'll probably also have a column
460:58 - for totals
461:01 - and a row for totals
461:07 - and then off this contingency table we
461:09 - will see if playing sports is dependent
461:12 - or independent on
461:14 - gender
461:17 - once we have our frequency information
461:19 - we also need to calculate
461:23 - the expected frequencies
461:30 - and often i'll make a second table to do
461:33 - this
461:37 - of expected frequencies and the way we
461:40 - calculate the expected frequency
461:45 - is we will take the row total
461:49 - times the column total
461:53 - divided by the total
461:57 - of the entire survey
462:01 - and we'll have to do that for every
462:02 - single cell so first row first column
462:05 - first row second column
462:07 - second column second row
462:09 - find all the entries for their expected
462:12 - frequencies and then we can calculate
462:15 - the test statistic from here
462:20 - often i'll organize this in a third
462:22 - table
462:27 - because our chi squared is going to be
462:29 - equal to
462:30 - the observed value
462:32 - minus the expected value squared
462:35 - divided by the expected values and then
462:37 - we take the sum
462:39 - of all of those
462:43 - so we've got two equations that are
462:45 - going to be helpful to test for
462:47 - independence
462:49 - should know how to calculate the
462:50 - expected values and then from that
462:53 - calculated
462:54 - the chi-squared
462:59 - now as we're testing for independence we
463:01 - need to know the degrees of freedom
463:04 - the degrees of freedom is equal to a
463:06 - product with independence it's the
463:09 - product of the number of rows minus one
463:14 - times the number of columns
463:18 - minus one
463:22 - also with independence
463:24 - similar to when we did goodness of fit
463:27 - the null hypothesis and the alternative
463:28 - hypothesis are generally in words not in
463:32 - symbols so the null hypothesis is
463:34 - generally going to be that the variables
463:36 - are independent
463:40 - and then the alternate hypothesis is
463:42 - that they're not independent or that
463:44 - they are dependent
463:50 - and then similar to
463:52 - the goodness of fit test that dependence
463:54 - is always a right
463:56 - tailed
463:58 - test
464:03 - so let's try an example and see if we
464:06 - can test
464:07 - for
464:08 - independence
464:15 - an example
464:20 - a restaurant
464:29 - wants to know
464:35 - if breakfast preference
464:44 - is dependent
464:49 - on
464:50 - gender
464:55 - so the following data is collected
465:08 - and they're going to compare male to
465:11 - female
465:13 - and some are going to prefer french
465:16 - toast
465:19 - some will prefer pancakes
465:23 - some prefer waffles
465:27 - and some prefer omelets
465:33 - and then we'll have a total
465:35 - row
465:38 - and a total column
465:52 - we find 47 males prefer french toast 35
465:57 - prefer pancakes 28 prefer waffles and 53
466:01 - prefer omelets
466:04 - if we add all of those together we'll
466:06 - get a total of 163 males were surveyed
466:12 - for the females 65 prefer french toast
466:15 - 59 prefer pancakes 55 prefer waffles 60
466:19 - prefer omelets we interviewed a total of
466:22 - 239 females
466:25 - and if we total the individual breakfast
466:27 - choices we've got 112 who prefer french
466:30 - toast 94 who prefer pancakes 83 who
466:34 - prefer waffles and 113 who prefer
466:37 - omelets there's a total of 402
466:40 - individuals in this survey
466:45 - the question is if alpha equals 0.05
466:51 - can
466:52 - the restaurant
466:57 - conclude
467:00 - that breakfast preference
467:08 - is dependent
467:13 - on gender
467:28 - well let's set up
467:31 - our hypothesis test
467:37 - first
467:38 - we have our null hypothesis
467:42 - which states that
467:44 - breakfast preference
467:51 - is independent
467:56 - of gender
468:01 - breakfast presents preference does not
468:03 - change based on if you're male or female
468:05 - the alternative hypothesis is that there
468:07 - is some type of dependence that
468:09 - breakfast preference
468:17 - is dependent
468:22 - of gender or on gender
468:31 - as usual with the chi squared we have a
468:32 - right-tailed test
468:38 - and our degrees of freedom
468:42 - to help us calculate the actual
468:44 - distribution
468:46 - is going to be the number of rows we've
468:48 - got two rows don't count the total one
468:50 - two rows
468:52 - minus one
468:54 - times the column don't count the total
468:56 - again one two three
468:58 - four columns
469:00 - minus one
469:02 - which gives us one times three or three
469:05 - degrees of freedom
469:07 - our distribution
469:16 - for our chi-squared statistic is a
469:18 - chi-squared with three
469:20 - degrees of freedom
469:27 - then let's see if we can calculate our
469:30 - test statistic
469:42 - to calculate the test statistic we first
469:45 - need to
469:47 - make another table of our expected
469:50 - values so for our expected values
469:54 - we'll do the same table
469:56 - male and female
470:02 - and for french toast
470:06 - give me a little more space here
470:13 - the way we calculate the expected value
470:16 - for our males with french toast
470:20 - is we will take the row of males
470:23 - and the column of french toast those
470:26 - totals
470:28 - so the row of males had a total of 163.
470:32 - the column had a total of 112. that's
470:35 - how we get the french toast male cell
470:38 - for expected
470:39 - 163 times 112
470:43 - and then we'll divide by the total
470:47 - so we have
470:52 - 163
470:54 - times the 112
470:57 - divided by the total number of people
471:00 - which was 402
471:02 - the expected value for french toast is
471:05 - 45.4
471:18 - now we'll go over to pancakes
471:21 - pancakes is in the second column first
471:24 - row
471:25 - so in this case what we'll see is we
471:27 - want the pancakes for the males that's
471:30 - in the first row
471:32 - second column so those are the numbers
471:34 - we're going to multiply 163 times 94 and
471:38 - divide by the 402
471:40 - the row times the column
471:43 - so we have
471:45 - 163 times 94
471:49 - divided by the total of 402
471:52 - we end up with 38.11
471:55 - is the expected value for pancakes
472:03 - next is waffles
472:11 - with the waffles
472:14 - we want the third column in the first
472:16 - row so we're going to multiply 163 times
472:19 - 82
472:21 - and divide by the total
472:27 - so we have 163
472:31 - times the 80
472:33 - 3
472:34 - is it 83
472:42 - divided by the total of
472:44 - 402 and that's going to give us 33.65
472:50 - for our expected value
472:54 - finally we've got the omelets
472:58 - similarly with the omelets the total for
473:00 - the omelet row for the males was 163.
473:05 - the total number of omelets was 113
473:11 - and then we divide by the 402
473:15 - to get 45.82
473:20 - we'll do the same thing with the females
473:22 - this time the female row total was 239
473:25 - so we're going to do
473:26 - 239
473:28 - times the french toast total of 112
473:31 - divided by the 402
473:33 - the expected females who prefer french
473:35 - toast should be 66.59
473:40 - with the pancakes
473:43 - 239
473:47 - times the column total of 94
473:50 - divided by the total total of 402
473:53 - 55.89
473:59 - with the waffles the road total of 239
474:03 - times the column total of 83
474:06 - divided by the total total of 402
474:09 - the expected value is 49.35
474:15 - and finally with the omelets the row
474:18 - total
474:19 - is 239
474:21 - the column total 113 divided by the
474:24 - total total of 402
474:26 - the expected number of omelets for
474:28 - females is 67.18
474:35 - so we've got the second table that
474:37 - demonstrates for us all of the expected
474:40 - values based on the row totals and the
474:43 - column totals
474:47 - next i'll make another table
474:50 - that does the observed minus the
474:52 - expected
474:54 - squared
474:55 - divided by the expected
475:04 - for our males females
475:07 - first with the french toast
475:10 - our first cell and i've got to do a lot
475:12 - of scrolling here hopefully you can see
475:13 - it all on one screen
475:15 - 47 is what we observed
475:20 - 45
475:22 - actually go ahead and highlight it to
475:23 - emphasize what i'm looking at here
475:32 - highlighted in yellow here our first
475:34 - cell had an observed value of 47
475:38 - an expected value
475:40 - of 45.41
475:43 - so plugging that into our formula
475:50 - the observed value of 47
475:54 - minus the expected value of 45.41
475:58 - squared
475:59 - divided by the expected value of 45.41
476:04 - that equals .06
476:12 - then we'll do the pancake preference
476:17 - you'll see pancakes the observed value
476:20 - is 35.
476:23 - the expected value is 38.11
476:27 - so we plug it into our formula
476:31 - 35
476:34 - minus 38.11
476:36 - squared
476:38 - divided by the expected value of 38.11
476:42 - that gives us point twenty five
476:49 - waffles
476:53 - same thing
476:55 - we'll come up here
476:57 - we observed 28 waffles
477:00 - we expected 33.65
477:05 - so we plug that into our formula
477:08 - 28 minus 33.6
477:13 - squared
477:15 - divided by the expected value of 33.65
477:20 - we get 0.95
477:26 - and we'll keep going with all of our
477:28 - remaining cells for the omelets we
477:30 - observed 53
477:32 - we expected
477:34 - 45.82
477:37 - squared
477:39 - divided by the expected 45.82
477:43 - that equals 1.13
477:48 - doing the female row
477:50 - the observed french toast was 65.
477:54 - minus the expected 66.59
477:58 - squared divided by the expected 66.59
478:03 - equals 0.04
478:06 - with the pancakes the observed value of
478:09 - 59 minus
478:11 - 55.89
478:13 - squared
478:14 - divided by the 55.89
478:18 - that equals 0.17
478:22 - for the waffles the observed value was
478:25 - 55 minus the expected value of 49.35
478:30 - squared
478:31 - divided by 49.35
478:34 - equals 0.65
478:37 - and finally the observed value for the
478:40 - omelets was 60
478:42 - minus 67.18
478:46 - squared
478:47 - divided by 67.18
478:50 - that's going to give us point
478:52 - 77
478:55 - so we're going to get really good at
478:57 - that observed minus expected squared
478:59 - formula
479:04 - the reason we need to do that is our
479:06 - chi-squared test statistic
479:10 - is going to equal the sum of those
479:12 - observed minus expected squared divided
479:15 - by the expecteds we're going to actually
479:18 - add
479:19 - all eight of these numbers
479:22 - that we just found
479:24 - together
479:28 - and when we add all those numbers
479:30 - together you should get 4.02
479:36 - that is our test statistic
479:42 - so it's a little cumbersome and tedious
479:44 - to calculate because you've got to go
479:46 - through cell by cell
479:48 - first we calculate the expected values
479:51 - by taking the row total times the column
479:54 - total divided by the total total
479:57 - then you need to find that observed
479:59 - minus expected squared divided by
480:01 - expected plugging those values in
480:04 - calculating them out and then finally we
480:06 - add them together to get our final
480:09 - chi-squared value
480:12 - and now we're ready to go on with our
480:14 - hypothesis test
480:16 - everything should flow pretty quick from
480:19 - here
480:20 - we need to calculate a p-value or the
480:23 - probability the null hypothesis is true
480:26 - to do that we'll do the chi-squared on
480:28 - the calculator cdf
480:31 - the minimum value of 4.02
480:34 - all the way to a maximum value of
480:36 - infinity 10 to the 99 and we said there
480:39 - are three degrees of freedom
480:44 - so let's do that we'll hit second
480:52 - the distribution or vars button
480:55 - we'll scroll down to select chi squared
480:58 - cdf
481:02 - the lower value we want is 4.02
481:05 - the upper value is infinity
481:08 - 3 degrees of freedom
481:11 - again if you don't have the newer
481:13 - software you just have to separate them
481:15 - by commas
481:17 - and when we hit enter we find a
481:19 - probability of 0.2593
481:23 - 0.2
481:28 - which means there is a 25.93
481:31 - actually let's say based on our survey
481:34 - based on our
481:36 - survey
481:41 - there is a 25.93
481:45 - percent chance
481:51 - that breakfast preference
481:59 - is independent
482:04 - of gender
482:10 - and we said we would believe the null
482:13 - hypothesis
482:14 - that they are actually independent as
482:17 - long as that probability does not go
482:20 - below alpha of five percent
482:23 - 25 percent is well above that
482:26 - and so we will make a decision
482:31 - to fail
482:33 - to reject
482:35 - the null hypothesis
482:41 - and the reason for that decision
482:44 - is the p-value
482:47 - is greater than alpha or
482:49 - 0.2593 that's too much evidence
482:52 - overwhelming past the 0.05
482:56 - we must still believe the null
482:58 - hypothesis is
483:00 - true so we make a conclusion
483:11 - our conclusion is that there is not
483:14 - always in context in the alternative
483:16 - hypothesis
483:18 - there is not
483:19 - sufficient
483:22 - evidence
483:26 - to conclude
483:30 - breakfast preference
483:38 - is dependent
483:43 - on
483:44 - gender
483:51 - and that is how we do a chi-squared test
483:54 - for independence it takes a little bit
483:56 - of time to calculate the test statistic
483:58 - running through those calculations but
484:00 - it's not too difficult it just takes the
484:01 - time to run it out
484:05 - so go ahead and take a look at trying a
484:07 - few of these
484:08 - off the homework we'll do a few of these
484:10 - more in class as we look at it a bit
484:12 - further we'll see you then
484:20 - today we're going to take a look at
484:22 - another use of the chi-squared
484:24 - distribution and that is testing a claim
484:26 - about a single
484:28 - variance our question is going to be
484:32 - how do we test
484:39 - a claim
484:42 - about a variance
484:51 - and when we're testing a variance
484:56 - a single variance
485:02 - we have a test
485:04 - statistic
485:08 - which is a chi-squared
485:11 - statistic
485:13 - the formula with the variance for chi
485:15 - squared is the sample size minus 1
485:19 - times the sample
485:20 - variance divided by the sample
485:25 - i'm sorry divided by the population
485:27 - variance
485:29 - and we need to be very careful or be
485:31 - very aware
485:36 - of what we have
485:41 - in the problem
485:43 - we use sigma
485:46 - and s to represent the standard
485:49 - deviation
485:55 - but when those pieces are squared and we
485:58 - either have sigma squared
486:00 - and s squared
486:02 - those represent what we call the
486:04 - variance
486:06 - and so the formula says s squared and
486:08 - sigma squared those values represent the
486:10 - variance if it's already been squared
486:14 - if it hasn't been squared we would have
486:16 - sigma and s the standard deviations
486:18 - which need to be squared in order to
486:21 - find the test statistic so we need to be
486:24 - very careful do we have s or s squared
486:27 - do we have sigma or sigma squared and
486:29 - not get those backwards
486:34 - also with chi squared we need to know
486:35 - the number of degrees of freedom we have
486:37 - the degrees of freedom are always one
486:39 - less than the sample size with the chi
486:41 - squared
486:43 - and something that's unique about
486:45 - testing a variance
486:47 - is that it can be
486:51 - either
486:53 - left
486:55 - right
486:57 - or two
486:59 - tailed
487:01 - test
487:02 - and this is very unique because normally
487:05 - with the chi-squared we're dealing with
487:06 - a right-tailed test but in the context
487:09 - of the variance it's the only time we're
487:11 - allowed to have either a left tail test
487:14 - or a two-tailed test which doesn't
487:16 - happen as often
487:18 - so the best way to really attack this is
487:20 - to run through an example
487:29 - so let's say a customer wants to know
487:40 - how the cost of a list of school
487:42 - supplies
487:54 - varies from store to store
488:09 - a teacher claims
488:12 - the standard deviation
488:21 - is only
488:23 - dollars
488:27 - so to test this the
488:36 - customer surveys
488:41 - 43 stores
488:47 - and finds
488:50 - a mean
488:54 - of 84 dollars
488:58 - and a standard deviation
489:07 - of twelve dollars
489:13 - test
489:15 - whoops test
489:19 - if the standard deviation
489:28 - is less
489:32 - than the teacher's claim
489:39 - of 15
489:42 - if alpha equals 0.05
489:50 - so we've got a claim about the standard
489:51 - deviation the claim is the standard
489:54 - deviation is 15
489:56 - but notice that's the standard deviation
489:59 - not the variance
490:01 - the variance is the standard deviation
490:05 - squared so when we set up our null
490:07 - hypothesis
490:09 - our null hypothesis will state that the
490:11 - variance
490:13 - or sigma squared
490:15 - is equal to the standard deviation
490:17 - squared
490:18 - or 15 squared which is 225.
490:25 - notice again we had to square the
490:27 - standard deviation to get the variance
490:30 - the alternate hypothesis we believe that
490:34 - the variance is actually less than
490:36 - 15 squared or less than 225.
490:42 - because we're interested in less than we
490:44 - actually have a left-tailed test
490:49 - so let's draw a little picture of our
490:51 - left-tailed test
490:53 - chi-squared is skewed right
490:57 - and we're interested in being in the
491:01 - left tail
491:06 - now in the survey
491:08 - the standard deviation ended up being 12
491:11 - compared to the actual mean
491:14 - of 15.
491:17 - is that enough
491:18 - to
491:20 - reject the null hypothesis
491:24 - well first we need to know the degrees
491:25 - of freedom
491:27 - the sample size minus one
491:30 - 43 minus one we've got 42 degrees of
491:32 - freedom
491:34 - and then we'll also calculate the test
491:36 - statistic
491:43 - the test statistic is chi squared
491:47 - is equal to n minus 1
491:49 - 42 minus 1
491:52 - times s squared s
491:55 - i'll go ahead and actually write the
491:57 - formula here one more time so we can see
491:59 - it n minus 1 times s squared divided by
492:02 - sigma squared s
492:04 - is the standard deviation of the sample
492:10 - so for my sample
492:14 - the customer did a survey and found a
492:16 - standard deviation of 12. that is my
492:19 - sample standard deviation 12
492:22 - squared to get the variance
492:25 - divided by sigma
492:26 - the claimed distribution of the
492:29 - population
492:31 - the claim standard deviation is 15.
492:36 - so we'll divide by 15
492:38 - squared
492:41 - and when i do this on my calculator
492:45 - oops sorry the sample size was 43. 43
492:48 - minus 1 times 12 squared divided by 15
492:51 - squared we get a test statistic of 26.88
493:00 - now that we have a test statistic we're
493:02 - ready to find the p-value
493:04 - or the probability my null hypothesis is
493:07 - true the probability the standard
493:08 - deviation is actually 15.
493:11 - it's a chi-squared cdf
493:15 - normally with chi squared we go from
493:17 - smallest to largest
493:20 - here the smallest value on the
493:22 - chi-squared is going to be 0
493:25 - to the largest value of 26.88
493:30 - comma
493:31 - our degrees of freedom
493:34 - we said was 42.
493:39 - and let's see what the calculator gives
493:40 - us
493:43 - for that value
493:46 - to get the chi-squared distribution
493:48 - we'll hit second vars so we get the
493:50 - distribution
493:52 - we'll scroll down to chi-squared cdf
493:58 - going from 0
494:00 - to 26.88
494:04 - our degrees of freedom are 42
494:07 - and we'll hit paste
494:08 - if you have the older version of the
494:10 - software you just enter those numbers in
494:12 - separated by commas like you see on the
494:13 - screen here
494:15 - and when we hit enter we find a
494:16 - probability of 0.0337
494:21 - 0.0337
494:26 - that p-value tells us that based on our
494:29 - sample
494:35 - the probability
494:40 - that the null hypothesis is true that
494:42 - the standard deviation
494:50 - of the cost
494:55 - of school supplies
495:01 - being
495:02 - 15
495:05 - is
495:07 - 3.37
495:09 - percent
495:11 - there's a three percent chance that that
495:13 - null hypothesis
495:15 - is true
495:19 - well if that's the case we're ready to
495:21 - make a decision
495:26 - the decision point is always compared to
495:28 - the alpha
495:30 - and we said alpha was going to be 0.05
495:33 - five percent probability we'll still
495:35 - believe the null hypothesis we only have
495:37 - a three percent probability so we can no
495:39 - longer believe the null hypothesis so we
495:42 - will reject
495:44 - the null hypothesis
495:49 - and the reason for that decision
495:52 - is that the p-value is less than the
495:55 - alpha value or that
495:57 - .0337 is less than the 0.05
496:03 - and so our conclusion
496:07 - which focuses on the alternative
496:08 - hypothesis in context
496:11 - we can say that there is
496:14 - sufficient evidence
496:20 - to conclude
496:25 - the standard deviation
496:33 - of school supplies
496:40 - cost
496:42 - is less
496:46 - than 15
496:52 - and that's all there is to testing a
496:54 - single
496:55 - variance
496:57 - it's actually the easiest chi-squared
496:59 - problem to solve because it's very
497:01 - straightforward we don't have to find
497:02 - all those expected values that we do in
497:04 - other chi-squared tests
497:07 - so we test a single variance with our
497:08 - new chi-squared test statistic
497:11 - and then we run it through the same
497:12 - process of a hypothesis test that we've
497:14 - been seeing for several weeks now we
497:17 - should be very good at setting up these
497:19 - hypothesis tests so you can go and take
497:20 - a look at a few of these on your
497:22 - homework assignment we'll look at this
497:23 - more in detail in class and we will see
497:26 - you then
497:33 - now that we've taken a look at how a
497:35 - hypothesis test can be conducted for a
497:37 - claim about a single variance we can
497:39 - extend this to the next level and look
497:41 - at a hypothesis test on a claim of two
497:44 - variances
497:45 - and so the question we're going to
497:47 - answer is how
497:52 - do we
497:54 - compare
497:58 - two
497:59 - variances
498:04 - and similar to one variance we have to
498:05 - be careful if we're talking about
498:06 - standard deviation or variance because
498:08 - the standard deviation is the square
498:10 - root of the variance
498:11 - but different than the single variance
498:14 - is we need to introduce a new
498:16 - distribution that models a comparison of
498:19 - two variances and this new distribution
498:23 - is what we will call the f
498:25 - distribution
498:30 - the f distribution because this
498:31 - distribution is used as a fraction when
498:34 - comparing two variances
498:38 - similar to the chi-squared distribution
498:40 - the f-distribution is not symmetrical
498:47 - in fact it is also skewed
498:51 - right
498:54 - also similar to the chi squared is its
498:57 - shape is different
499:01 - based
499:04 - i should say different shape
499:09 - based
499:11 - on
499:12 - degrees of freedom
499:16 - but what's really unique about the f
499:18 - distribution
499:20 - is it's a fraction of two variances in
499:24 - other words we're going to have
499:27 - a ratio or let's just go ahead and call
499:29 - it a fraction
499:33 - or ratio
499:38 - with two
499:40 - sets
499:42 - of degrees of freedom
499:46 - we say the numerator
499:53 - has degrees of freedom for the numerator
499:56 - is equal to the first sample size minus
499:59 - one
500:01 - and the denominator
500:07 - is going to be the degrees of freedom of
500:09 - the denominator
500:11 - is equal to the second sample size minus
500:13 - one
500:17 - and what's interesting is as the degrees
500:20 - of freedom get larger
500:23 - for both the denominator and the
500:25 - numerator
500:29 - the curve
500:32 - becomes
500:35 - more
500:36 - normal
500:42 - one last thing to talk about the f
500:44 - distribution is similar to the chi
500:46 - squared the f is always positive
500:51 - or always greater
500:55 - than
500:56 - i guess it could be equal to
501:01 - zero
501:07 - so that's kind of a brief introduction
501:09 - of the f distribution we're going to use
501:10 - our calculator to do most of the
501:12 - calculations with the f distribution
501:14 - what we're interested in is can we set
501:16 - up and carry out a hypothesis test on
501:19 - two
501:20 - variances
501:28 - the test statistic
501:33 - for two variances
501:36 - is simply the fraction
501:39 - the first variance
501:41 - divided by the second variance
501:45 - or the standard deviation squared
501:47 - divided by the standard deviation
501:49 - squared
501:52 - if both variances are equal if we have
501:55 - equal variances
501:59 - that tells us that f is going to equal
502:01 - to one we're dividing everything by
502:08 - itself if we have different variances
502:22 - f is closer
502:25 - to zero
502:27 - if the second variance is larger
502:30 - or infinity
502:31 - if the first variance is larger
502:37 - we'll use our calculator actually to do
502:38 - most of the work for the f statistic
502:44 - and so just really briefly how to do
502:46 - that first you're going to hit the stat
502:49 - button
502:52 - and then you can scroll
502:55 - over
502:56 - to
502:57 - test
503:01 - then you can scroll
503:04 - down
503:08 - to
503:10 - two
503:11 - samp
503:13 - f
503:14 - test
503:19 - and that's where we can access the
503:20 - two-sample f-test for the comparing of
503:22 - our variances
503:27 - then to actually enter in our data you
503:28 - want to make sure that stats is
503:30 - highlighted
503:37 - and then you can enter
503:40 - the standard deviation
503:49 - notice i did not say the variance
503:54 - even though we're comparing variances
503:56 - with the f test the calculator wants us
503:58 - to enter in the standard deviation which
504:00 - is the square root of the variance so we
504:03 - can enter the standard deviation
504:05 - and our other data
504:15 - other than that the hypothesis test is
504:17 - going to work exactly like all the other
504:19 - hypothesis tests we've seen before
504:21 - so let's try an example and see if we
504:24 - can
504:27 - do a test of two variances
504:34 - quality control
504:40 - is interested
504:47 - in the variance
504:52 - of two machines
504:58 - making widgets
505:07 - the first make 32 widgets
505:20 - with
505:21 - a variance
505:25 - in the radius
505:28 - apparently there's a circle on these
505:30 - widgets
505:34 - of
505:35 - 4.1 millimeters
505:41 - the second
505:47 - makes 37
505:52 - widgets with a variance
505:58 - in the radius
506:06 - of 3.7 millimeters
506:14 - at the alpha equals 0.05 level
506:20 - can quality control
506:28 - conclude
506:34 - the first machine
506:39 - has a higher
506:42 - variance
506:58 - we'll start by setting up our null and
507:00 - alternative hypothesis like always
507:04 - the null hypothesis always has equity so
507:07 - we're assuming with our null hypothesis
507:09 - that the variance of the first machine
507:12 - is equal to the variance of the second
507:15 - machine
507:17 - the alternative hypothesis is that the
507:20 - variance of the first machine
507:22 - is greater because we want the first
507:24 - machine to be higher
507:27 - than the variance of the second machine
507:34 - for our degrees of freedom
507:37 - the degrees of freedom of the numerator
507:39 - we always do our division in order as
507:42 - listed in the hypothesis test so we're
507:44 - going to do the variance
507:48 - oops
507:49 - the variance of the first machine
507:53 - divided by the variance of the second
507:55 - machine
507:56 - same order as they're in the hypothesis
507:58 - test so the first machine is our
508:00 - numerator the first machine made 32
508:03 - widgets meaning the degrees of freedom
508:05 - of the numerator are 31.
508:09 - the degrees of freedom in the
508:10 - denominator the denominator being the
508:13 - second machine which made 37 widgets one
508:16 - less would be 36 for the degrees of
508:19 - freedom
508:20 - so our distribution
508:25 - is that we have an f statistic that is
508:28 - distributed as an f with 31 and 36
508:31 - degrees
508:33 - of freedom
508:36 - and we can calculate our test statistic
508:40 - by just dividing those variances notice
508:43 - we're given the variances the variance
508:46 - is 4.1 and the variance is 3.7 they've
508:50 - already been squared so the first
508:52 - variance is 4.1
508:54 - divided by the second variance of 3.7
508:58 - that gives us 1.1081
509:04 - so if we were to draw a picture of this
509:06 - situation the f distribution skewed
509:08 - right
509:10 - we have a test statistic right at 1.0181
509:17 - we want to be greater
509:20 - so we go for the right
509:22 - tail how much area is in that right tail
509:28 - this is where we're going to go to our
509:29 - calculator
509:30 - to find our p-value
509:34 - first we're going to hit stat
509:37 - scroll over to test
509:41 - and then we're going to scroll down for
509:42 - the two sample f test
509:49 - oh there it is
509:51 - two sample f test
509:55 - i'm going to enter in the statistics
509:59 - and it wants my first standard deviation
510:02 - not the variance the calculator is
510:05 - asking for the standard deviation which
510:07 - is the square root of the variance
510:09 - fortunately i can just type in the
510:11 - square root with the second and then the
510:14 - square root key is above the square
510:16 - diagonal from the seven
510:19 - and my first variance was 4.1
510:22 - so if the variance is 4.1 the standard
510:24 - deviation is the square root of 4.1 and
510:27 - when i hit enter it's going to calculate
510:29 - that value for me
510:33 - the first sample size had 32 widgets in
510:35 - it
510:37 - the second machine had a variance of 3.7
510:40 - so we'll take the square root of 3.7
510:44 - to get the standard deviation of 1.92
510:49 - and the second machine made 37 widgets
510:56 - for our alternate hypothesis we said
510:58 - that the first is greater than the
510:59 - second so we'll select greater than
511:04 - and go down and hit calculate
511:08 - notice it gives us the exact same f
511:10 - statistic that we found 1.1081
511:14 - but what we're really interested in is
511:16 - it gives us a p-value of 0.309
511:24 - 303809
511:26 - [Music]
511:31 - so let me scroll a bit give me a little
511:33 - more space to work
511:35 - we have a p-value
511:39 - of 0.3809
511:42 - remember the p-values the probability
511:44 - the null hypothesis is true so we will
511:48 - say the probability
511:52 - or based on our sample
512:01 - the probability
512:06 - both machines
512:13 - have the same
512:16 - variance
512:18 - that's the null hypothesis that they're
512:20 - equal
512:23 - is
512:24 - 38.09 percent
512:31 - and the alpha
512:33 - tells us the probability required to
512:35 - disprove
512:37 - that
512:38 - null hypothesis we have to drop below
512:40 - five percent we're well above the five
512:43 - percent so we're ready to make a
512:45 - decision
512:47 - which is to fail
512:50 - to reject
512:52 - the null hypothesis
512:58 - and the reason for that decision
513:00 - is that the p-value is greater than
513:02 - alpha there's too much evidence and
513:04 - support of the null hypothesis
513:06 - specifically with numbers 0.3809 is
513:09 - greater than 0.05
513:12 - and so for our final conclusion
513:17 - which is always written in the context
513:19 - of the alternative hypothesis
513:22 - i believe the alternative on there
513:25 - is that there is
513:28 - insufficient or there is not sufficient
513:34 - evidence
513:38 - to conclude
513:42 - the first machine
513:49 - has a higher
513:52 - variance
513:56 - than the second machine
514:08 - and that is how we can compare
514:11 - two
514:12 - variances so we have a new distribution
514:14 - the f distribution which is a fraction
514:17 - of the variances
514:18 - but the idea is still the same as our
514:20 - hypothesis test we've been doing we've
514:21 - got our null and alternative hypothesis
514:24 - the test statistic gives us an area a p
514:26 - value that we compare to alpha and make
514:29 - a conclusion whether or not we have
514:30 - sufficient evidence
514:32 - to believe the alternative hypothesis
514:35 - take a look at a few of these on your
514:37 - own if you'd like we'll talk about uh
514:39 - comparing two variances more in class
514:41 - and i'll look forward to seeing you
514:44 - then
514:51 - quite often in statistics we're
514:52 - concerned with comparing the mean from
514:55 - more than just two groups in the past we
514:58 - compared two groups with the t-test
515:01 - but here when we compare more than two
515:03 - groups we need a different statistical
515:06 - test and that's what we're going to look
515:07 - at today how do we compare
515:17 - the means
515:21 - of more
515:23 - than two groups
515:27 - this actually also works with two groups
515:29 - but the t-test is easier the t-test
515:32 - turns out to be a special case of this
515:34 - thing that is called the anova
515:40 - but the t-test is easier for two groups
515:42 - so we do the t-test
515:43 - but when there's more than two groups we
515:45 - use this test called the anova
515:48 - anova is actually an acronym it stands
515:50 - for analysis
515:53 - of
515:55 - variance
516:00 - and the idea behind the anova is we
516:03 - compare the variance between the groups
516:07 - to the variance within the groups and
516:09 - when we divide them we end up with an f
516:11 - statistic and so we can compare using
516:14 - the f ratio just like we did with two
516:16 - variances
516:18 - so a couple differences with the anova
516:20 - the hypothesis test
516:26 - for the hypothesis the null hypothesis
516:28 - is always the same that the first mean
516:31 - is equal to the second mean which is
516:34 - equal to the third mean which is equal
516:36 - to all the other means until you get to
516:38 - the very last mean basically all the
516:41 - means are the same and the alternative
516:44 - hypothesis is that at least
516:49 - one mean
516:52 - is
516:54 - different
517:01 - we don't know which mean but just one
517:03 - mean
517:04 - is different than the rest possibly all
517:06 - three are different from each other and
517:09 - we use
517:11 - the f test
517:15 - to compare
517:20 - which hypothesis we'll end up going with
517:24 - what we're doing is we are looking
517:27 - for a
517:32 - difference but not
517:35 - where
517:38 - the difference is
517:42 - turns out that once we decide that there
517:44 - actually is a difference between one of
517:46 - the means we have to do some follow-up
517:49 - statistical tests which are beyond the
517:50 - scope of this course to identify
517:53 - specifically where that difference is we
517:55 - might have an idea where it is but to
517:57 - get the exact difference
517:59 - we need follow-up tests that we're not
518:01 - going to cover in this course so for
518:03 - today all we're looking at is
518:05 - are they all the same or is there a
518:07 - difference somewhere
518:10 - we're not going to spend our time in
518:12 - this course with the complex
518:13 - calculations of the anova we're just
518:15 - going to look at actually carrying it
518:17 - out and having our calculator
518:21 - do the complex calculations how we're
518:24 - going to do this
518:26 - is we're going to put
518:27 - each group
518:32 - in its own list
518:38 - and the way we do that is we'll hit the
518:39 - stat button
518:42 - and then we'll select edit
518:45 - to edit our list
518:47 - and in list one we'll put the first
518:49 - group data list two we'll put the second
518:50 - group data list three the third group
518:52 - data until we get all of our groups
518:55 - and once all of our groups are actually
518:57 - listed in there we will run the test
519:02 - and we'll do that with the stat button
519:06 - we will scroll over to
519:09 - tests
519:13 - and then we will scroll down
519:16 - to
519:18 - the
519:19 - anova
519:25 - now the anova will not give us prompts
519:27 - on what information to enter in like a
519:29 - lot of the other statistical tests did
519:31 - so what we need to do is we're going to
519:33 - enter
519:35 - the lists
519:38 - separated
519:42 - by
519:43 - commas
519:46 - and the way we get the list is we'll hit
519:48 - second
519:50 - and then select the list number
519:57 - and we'll see how that all works out
519:58 - with our example let's go ahead and move
520:02 - to our example and see if we can compare
520:05 - this time this context
520:07 - we're going to compare three groups
520:11 - and see if they have the same mean
520:13 - or different means
520:16 - a university
520:22 - is comparing
520:31 - traditional
520:34 - students transfer students
520:43 - and non-traditional students
520:55 - by comparing
521:01 - gpa
521:06 - in the junior year
521:12 - here are their results
521:13 - the traditional students
521:15 - had gpas of 3.2
521:19 - 3.4 3.7
521:22 - 4.0
521:26 - the transfer students
521:28 - had gpas of 3.1
521:31 - 2.7
521:33 - 2.9
521:35 - 3.2 and 4.0
521:39 - and the non-traditional students
521:43 - had gpas of 3.4
521:46 - 2.2
521:48 - 2.7 and 2.8
521:54 - we want to know if all three groups can
521:56 - be considered to have the same gpa or
521:58 - different gpas
522:00 - if
522:01 - alpha equals point 10
522:07 - can the university
522:11 - conclude
522:14 - there is a difference
522:21 - in the groups
522:29 - let's scroll up and give ourselves a
522:30 - little room and start our hypothesis
522:32 - test
522:35 - the null hypothesis is that the mean of
522:38 - the traditional students
522:40 - is equal to the mean of the transfer
522:43 - students which is equal to the mean of
522:45 - the non-traditional students that they
522:48 - all have the same
522:50 - mean
522:52 - the alternate hypothesis is that at
522:54 - least possibly more
522:57 - one group
523:00 - have
523:02 - a
523:02 - different
523:07 - gpa
523:12 - let's go ahead and run this test on our
523:15 - calculator and see what hap
523:19 - first thing we need to do on the
523:20 - calculator is enter our data into stats
523:23 - and we'll select edit
523:25 - if there's extra stuff in this list you
523:27 - can highlight and hit clear enter and
523:29 - that'll delete the list or clear the
523:31 - list clear enter
523:34 - clear enter
523:36 - and so in list one i'm going to put the
523:38 - gpas 3.2 3.4 3.7
523:43 - and 4.0
523:46 - list 2 is my second group 3.1 2.7
523:51 - 2.9
523:53 - 3.2
523:54 - and 4.0
523:58 - the non-traditional group 3.4
524:01 - 2.2
524:02 - 2.7
524:04 - and 2.8
524:09 - now let's go ahead and run the anova
524:11 - we'll hit stat
524:13 - and this time going over to test
524:16 - and all the way down to the bottom or
524:19 - hitting up gets us straight to the
524:20 - bottom you'll see anova
524:25 - for the anova we enter in our list we
524:27 - have the three list so second one for
524:29 - the first list
524:31 - comma
524:32 - second two for the second list
524:35 - comma second three
524:38 - and we keep going based on how many
524:39 - lists we have we only had three lists
524:43 - and when we hit enter we get all sorts
524:45 - of information
524:49 - we've got our f statistic of 3.07
524:54 - we've got our p value of .09
524:58 - under factor
525:01 - you'll see we've got degrees of freedom
525:03 - equals two
525:04 - and under air we've got degrees of
525:06 - freedom equals ten the other numbers
525:08 - we're not going to concern ourselves
525:10 - with today
525:13 - but those degrees of freedom represent
525:16 - the numerator and the denominator so the
525:19 - factor is the numerator 2
525:21 - the error is the denominator 10. they
525:24 - are in order which is nice so when we
525:27 - say our distribution we'll say f is
525:30 - distributed as an f statistic with 2 and
525:33 - 10 degrees of freedom
525:35 - and we just found out that f equals 3.07
525:39 - and more specifically
525:41 - our p-value
525:47 - that's the important one
525:49 - was .0912
525:53 - but also go ahead and draw a picture of
525:55 - what's happening
525:57 - there's my f distribution
526:00 - my test statistic at 3.07
526:03 - and we shade that tail which we now know
526:06 - has an area of 0.0912
526:13 - now speaking of the p-value what that
526:15 - p-value means
526:18 - is based on our sample
526:26 - the probability
526:31 - all three
526:35 - groups of students
526:43 - have the same
526:47 - mean
526:48 - gpa
526:50 - in the junior year
526:54 - is nine point twelve percent we've got a
526:57 - nine percent chance that all three
526:59 - groups have the exact same probability
527:03 - the p value is the probability the null
527:05 - hypothesis is true
527:07 - which means we are ready to make a
527:09 - decision
527:11 - the decision is based on the alpha we
527:13 - said alpha's point 10 we're going to
527:15 - believe the null hypothesis is true
527:18 - until there's less than 10 percent
527:20 - chance it actually is true
527:22 - we we have a 9 chance that it's true so
527:25 - that passes our threshold so we will
527:28 - reject the null hypothesis
527:38 - and the reason for that decision
527:42 - is that the p-value is less than the
527:44 - alpha or the 0.0912
527:47 - is less than the point 10 we said was
527:49 - our decision breakpoint
527:53 - so our conclusion
527:56 - in context of the alternative hypothesis
527:59 - is that there is sufficient evidence
528:02 - because we rejected
528:09 - to conclude
528:15 - the mean gpa
528:21 - of traditional
528:25 - transfer
528:28 - and non-traditional
528:35 - students
528:38 - in the junior year
528:43 - is not the same
528:50 - again i'll notice this test doesn't tell
528:52 - us which group is different from the
528:53 - rest or possibly all three groups are
528:55 - different from each other it just tells
528:57 - us that they're not all the same
529:00 - so that's the anova it's another use of
529:03 - the f distribution there's actually
529:04 - several types of anova this is the most
529:06 - basic that we're going to look at in
529:07 - this course and you can see more anovas
529:09 - and more advanced statistics courses
529:12 - but for now we're comparing do multiple
529:14 - groups have the same mean we plug it
529:16 - into our calculator we get an f
529:17 - statistic and a p value and we should be
529:19 - able to conclude
529:21 - do they all have the same mean or is
529:23 - there a difference
529:24 - somewhere in the group
529:27 - try a few of these on the homework we'll
529:28 - talk about it more in class and we will
529:30 - see you then
529:39 - this video is going to look at the
529:40 - important concept a find a relationship
529:42 - between variables with what we call
529:44 - correlation
529:45 - and regression so that's where our
529:48 - question stems from our question is how
529:50 - do we test
529:57 - for a relationship
530:03 - between
530:07 - variables
530:16 - and the first thing we're going to start
530:17 - with is just getting a visual of how the
530:20 - two variables are related using what is
530:22 - called a scatter plot
530:28 - a scatter plot is basically just a graph
530:30 - of all of our data so it's best
530:33 - illustrated with an example
530:39 - let's say a researcher
530:44 - collects a sample
530:55 - the number
530:58 - of pages
531:03 - a person reads
531:10 - based on their age
531:19 - so we've got
531:21 - let's call the first column their age
531:26 - and the second column the pages that
531:29 - they read
531:31 - so we've got a 14 year old who read 40
531:34 - pages
531:36 - there's a 21 year old who read 45 pages
531:39 - they asked a 33 year old who read 92
531:42 - pages
531:44 - they asked a 45 year old who read
531:47 - pages and they asked a 63 year old who
531:50 - read 171 pages
531:55 - the idea of a scatter plot is if we call
531:57 - the first column the independent
531:59 - variable x
532:00 - and the thing that we think changes
532:02 - based on the independent variable or the
532:04 - dependent variable y
532:06 - we should be able to graph these
532:09 - to get a visual of how they relate to
532:12 - each other
532:13 - let's go up by tens on the x-axis 10 20
532:18 - 30
532:19 - 40
532:20 - 50
532:22 - 60
532:23 - 70 years
532:25 - and on the y-axis we'll go up by 30s
532:28 - 30
532:29 - 60
532:30 - 90
532:32 - 120
532:34 - 150
532:36 - 180.
532:40 - and so we'll make a point for each one
532:42 - of these and this is going to be what
532:43 - becomes our scatter plot so at 14 years
532:47 - old
532:47 - we'll go up to 40 pages which is right
532:51 - about where that blue dot is
532:54 - then we've got a 21 year old who goes 45
532:57 - pages so maybe a little bit higher
533:01 - then there's a 33 year old who's going
533:03 - to go up to 92 pages
533:06 - a 45 year old who will go up to 167
533:11 - pages
533:13 - and a 63 year old
533:16 - who will go up to 171
533:19 - pages
533:21 - and so we kind of can see a relationship
533:24 - here
533:25 - but before we get into that
533:27 - let's make sure our scatter plot is
533:29 - complete because a good scatter plot
533:31 - will have titles and labels so the
533:33 - bottom represents the age
533:36 - going up represents the pages and we
533:38 - might say pages read by age for the
533:42 - title
533:45 - and what we can see is the dots aren't
533:48 - exactly in a straight line but they do
533:49 - kind of trend upwards it seems that as
533:52 - age goes up the number of pages goes up
533:56 - and so we've got this relationship
533:59 - starting to establish visually
534:02 - now our calculators can also make these
534:04 - scatter plots so i'm going to show you
534:06 - how to make this exact same scatter plot
534:09 - on
534:10 - the
534:11 - calculator so first i'll write out the
534:13 - instructions then we'll go ahead and do
534:16 - it
534:18 - first we have to enter the data
534:22 - and the way we enter the data is you'll
534:24 - start by hitting the stat button
534:27 - and then you'll select edit
534:32 - and then you can put x
534:35 - in list one
534:38 - and y
534:40 - in list two
534:44 - once you've entered the data you're
534:45 - ready to make the graph
534:48 - and the way we make the graph is first
534:50 - you'll have to hit the second button
534:53 - and then you'll have to hit the y equals
534:55 - button
534:57 - because above y equals the second
534:59 - feature is stat plot it's for graphing
535:02 - statistics
535:04 - and you're going to go into the first
535:06 - stat plot and make sure it is on
535:10 - make sure you select the scatter plot
535:12 - which is going to be the dots
535:17 - and then you want to select
535:19 - what list you want l1
535:22 - and l2
535:25 - once you're done with
535:27 - setting up the stat plot you'll hit the
535:29 - zoom button
535:33 - and you'll select zoom
535:35 - stat which will center you
535:38 - right on the statistics
535:43 - so let's look at doing that on our
535:45 - calculator
535:47 - first we have to enter in our data
535:50 - to do that we'll start by hitting the
535:52 - stat button
535:54 - selecting edit
535:57 - and there's already stuff in these lists
535:59 - to clear it out i'll highlight the list
536:01 - name and hit clear enter and that clears
536:03 - out the list
536:07 - clear enter
536:09 - and then i'll put in l1 my x's my x's
536:12 - were 14
536:14 - 21
536:16 - 33
536:17 - 45 and 63
536:21 - and then l2 i'll put my y's 40
536:24 - 45
536:26 - 92
536:27 - 167
536:29 - and 171.
536:33 - now that i've entered in the data to set
536:35 - up the graph we'll hit second
536:38 - and the stat plot button
536:39 - which is the y equals button
536:42 - option number one we're going to make
536:43 - our scatter plot
536:46 - first you want to make sure you've
536:47 - turned it on by selecting on
536:51 - there's lots of graphs you want to make
536:52 - sure the dotted one which stands for the
536:54 - scatter plot is selected
536:57 - and then l1 and l2 are my list
537:01 - we're ready to see it
537:03 - we'll hit the zoom button
537:06 - and near the bottom
537:10 - maybe not the bottom bottom
537:12 - there it is number nine on my calculator
537:15 - is zoom stat
537:17 - what that does is that centers my graph
537:20 - around my statistics and you see we end
537:22 - up with much the same graph we had
537:24 - before the two dots next to each other a
537:26 - dot a high dot and another dot over to
537:28 - the side
537:31 - and so now we've got our scatter plot
537:34 - on our calculator
537:37 - but sometimes we're interested in more
537:38 - than just the dots what we might be
537:40 - interested in is can we draw a line that
537:43 - kind of models through the center of the
537:45 - data a best fit
537:48 - close to the dots
537:51 - that is what we call our regression
537:54 - equation
537:56 - [Music]
537:58 - so let's take a look at the regression
538:02 - equation
538:06 - also known as the line of best fit
538:14 - also known as the least
538:18 - squares line
538:22 - basically a line that models those dots
538:24 - as close as possible
538:29 - well since it's a line we know the
538:31 - equation of the line
538:33 - is going to be y equals
538:35 - some y-intercept
538:38 - plus the slope times x
538:41 - and in linear regression we call that y
538:43 - equals a plus bx a little different than
538:46 - algebra where you probably learn y
538:47 - equals mx plus b
538:49 - similar setup but now we use a for the
538:51 - y-intercept and b for the slope
538:55 - the equations for a and b are quite
538:57 - complex so we're going to have the
538:59 - calculator do our work for us
539:04 - to calculate
539:05 - the equation and the way the calculator
539:07 - can do that is we actually run what's
539:10 - called the linear regression t test
539:14 - so we'll hit stat
539:18 - we'll scroll over to tests
539:21 - and we'll scroll down to the lynn
539:24 - rag
539:25 - t
539:27 - test
539:33 - we can also graph it on our scatter plot
539:36 - graph by hitting the y equals button
539:40 - and then typing in the equation
539:47 - so let's do that
539:49 - for our age
539:52 - versus pages example
539:59 - so pulling back up my calculator
540:03 - we're going to hit the stat button
540:07 - scroll over to test
540:11 - and near the bottom you will see len reg
540:16 - t
540:17 - test make sure you do lin reg t test not
540:19 - any of the other len reg stuff lin reg t
540:22 - test
540:24 - and hit enter
540:28 - l1 and l2 are x and y
540:31 - uh we're going to always select the not
540:33 - equals to button for our len reg t test
540:38 - and then if we hit calculate
540:41 - it gives us a lot of information we're
540:44 - going to come back to some of this
540:45 - information in just a minute but
540:48 - specifically what we're interested in
540:49 - right now if i scroll down we see a is
540:52 - negative 5.523
540:55 - and b is 3.083
540:59 - i'm going to go ahead and round those to
541:00 - two decimal digits so a is equal to
541:03 - negative 5.52
541:06 - and b is equal to 3.08
541:12 - which means if i put that into my
541:13 - equation y equals a which is negative
541:16 - 5.52
541:19 - plus b
541:20 - 3.08
541:22 - times x
541:25 - this equation
541:28 - models as close as possible
541:31 - my
541:33 - scatter plot
541:36 - if i hit y equals i can type that in
541:38 - negative 5.52
541:41 - plus
541:42 - 3.08
541:43 - x
541:45 - x buttons right next to the stat button
541:47 - and when now when i hit the graph button
541:51 - what i'll see is i have a line that goes
541:53 - right through the middle of my dots
541:55 - seems to model that quite well we have a
541:59 - line of best fit
542:04 - the nice part about the line of best fit
542:07 - is i can use it to estimate
542:10 - values i don't have
542:13 - let's estimate
542:16 - the number of pages read
542:22 - by
542:22 - a
542:24 - 30 year old
542:32 - if we go back to the original data we
542:34 - don't have 30 in here
542:39 - but age is my x value so if i plug in
542:42 - the age for x we should be able to get a
542:45 - good estimate
542:47 - for what that 30 year old is reading
542:50 - so y equals negative 5.52
542:54 - plus
542:55 - 3.08
542:57 - x
542:58 - x is the age my 30 year old
543:03 - and when i put that into my calculator
543:06 - i end up with 86.88
543:12 - pages
543:13 - so round that baby to 87 pages we would
543:17 - expect this 30 year old to read
543:20 - [Music]
543:23 - one important thing to note
543:25 - about using the regression equation to
543:27 - estimate points
543:29 - is it only works
543:31 - within the domain of the problem
543:34 - it only works
543:40 - between
543:42 - the high
543:44 - and low values
543:48 - we can't estimate values outside of that
543:52 - range
543:53 - so for example this would be bad
543:59 - could we figure out a three-year-old
544:05 - well mathematically it would make sense
544:07 - if three is the age we'll just plug 3 in
544:11 - for the x and we get negative
544:14 - 5.52 plus
544:16 - 3.08 times our 3 year old and that's
544:20 - going to equal to 3.72
544:23 - so do we conclude that a three-year-old
544:25 - is reading 3.72 pages
544:29 - probably not there's not a lot of
544:31 - three-year-olds that can read anything
544:34 - maybe their name
544:36 - the problem is is the three-year-old is
544:38 - outside of our data our data had a low
544:41 - of 14 and a high of 63.
544:44 - we're only going to estimate values
544:47 - between those numbers if we go outside
544:49 - of the data the model can very easily
544:52 - break down and so we want to be careful
544:55 - not to take the model further
544:57 - than it's designed to go
545:06 - all right i want to look at one more
545:08 - thing with regression and correlation
545:11 - and that's what we call the correlation
545:16 - coefficient
545:21 - and this is where the hypothesis test
545:22 - comes in though we won't do it nearly as
545:24 - formally as we have in other contexts
545:27 - the idea of the correlation coefficient
545:30 - is i have this red line on my graph
545:33 - that
545:34 - goes kind of through the blue dots but
545:36 - not perfect well how good
545:39 - of a model
545:40 - is that line of best fit is it close
545:43 - is it far off
545:45 - what can we know from that line
545:49 - this is what the correlation coefficient
545:52 - measures and we have a special variable
545:55 - r that we use for the collate
545:57 - correlation coefficient and it tells us
546:00 - two things about the graph
546:03 - it tells us the strength
546:07 - and direction
546:14 - first
546:15 - r is between
546:19 - negative one
546:21 - and positive one
546:25 - we say if r equals zero
546:28 - that tells us that there is no
546:32 - relation
546:35 - no relation between the x and the y the
546:37 - dots are completely random
546:42 - and our line of best fit just has to go
546:44 - straight through the middle but that
546:46 - doesn't even model it well there's no
546:48 - relationship everything's random
546:53 - if r equals positive one what that means
546:57 - is we have a perfect
547:01 - positive
547:04 - relation
547:07 - positive means we're going uphill
547:10 - so now we're going to have these dots
547:15 - going uphill
547:16 - and the line goes right through all the
547:19 - dots uphill
547:21 - perfectly
547:23 - that would make r equal to 1.
547:27 - similarly r equals to negative one means
547:30 - we have a perfect
547:33 - negative
547:35 - relation
547:42 - or we're going downhill
547:46 - so now there's a bunch of dots going
547:48 - downhill
547:50 - and the line goes right through the
547:52 - middle of all the dots going downhill
547:57 - now there's no such thing as perfect
547:59 - data so r very rarely is 0 1 or negative
548:02 - 1. usually it's somewhere in between r
548:04 - might be negative 0.78 or r might be
548:07 - positive 0.23 and the closer it is to 0
548:10 - the less relationship we have and the
548:12 - closer to 1 the more likely we have a
548:15 - relationship and the way we determine if
548:17 - there's a relationship is we do a t-test
548:23 - it's called the lynn
548:24 - reg t-test
548:28 - linear regression t-test
548:33 - which
548:34 - gives a p-value
548:41 - that tells
548:44 - if the relationship
548:49 - between the two variables
548:51 - is significant
548:57 - a p-value less than alpha of 0.05
549:02 - will mean that we have a significant
549:04 - relationship
549:13 - but r works hand in hand with this
549:16 - p-value because r
549:18 - tells
549:20 - the strength
549:25 - of the relationship
549:33 - r could be positive or negative but what
549:35 - we'll say is if it's between 0 and
549:38 - 0.199999
549:42 - the strength is considered to be very
549:46 - weak
549:47 - there might be a relationship because
549:49 - the p value tells us it's significant
549:51 - but it's very weak below 0.199999
549:56 - from 0.2 to 0.399999
550:01 - we say it's just a weak relationship
550:04 - 0.4 to 0.59999
550:09 - is a moderate
550:11 - relationship
550:13 - but we like to see r bigger than 0.6 0.6
550:17 - to 0.7999
550:19 - means we have a strong relationship
550:23 - and on occasion we end up between 0.8
550:26 - and 1.0
550:28 - which would be a very
550:30 - strong
550:33 - relationship
550:40 - in addition to r telling us the strength
550:43 - of the relationship
550:45 - and p telling us whether or not the
550:47 - relationship is significant
550:50 - there's a third variable that we look at
550:53 - in analyzing correlation and regression
550:56 - and that is r squared
550:58 - which we get by squaring the r value
551:02 - r square tells the amount
551:07 - of
551:08 - variance
551:10 - actually not variance we'll say
551:11 - variation
551:20 - in the dependent variable
551:28 - that is explained
551:36 - by the independent variable
551:47 - how much of the changes in that
551:49 - dependent variable are explained by
551:51 - changes in the independent variable
551:53 - versus other factors
551:57 - this whole concept of the correlation
551:58 - coefficient is probably best seen by
552:01 - going back to our example so let's go
552:03 - back to our age versus pages example
552:15 - we can run the lynn
552:18 - rag
552:19 - t-test
552:22 - on our calculator to find these
552:24 - important values the t-value the r the
552:26 - p-value and the r-squared
552:30 - so going back to our calculator
552:32 - let's run it one more time i'll hit stat
552:35 - over to test
552:37 - and i'm going to do the len reg t test
552:41 - my numbers are already in there
552:44 - so when i hit calculate
552:47 - we see our test statistic the t is 5.09
552:52 - we see that gives us a p value of 0.0146
552:57 - and if i scroll down we will see an r
553:01 - value
553:04 - r is 0.9466
553:09 - and r squared is 0.896
553:14 - let me transfer all those values over
553:15 - here and we'll talk about what they mean
553:17 - so we had a t value of 5.09 when we
553:20 - round it
553:22 - which resulted in a p value
553:27 - of 0.00
553:35 - also there was an r our correlation
553:37 - coefficient of 0.947
553:41 - and r squared it says was 0.896
553:51 - with linear regression
553:54 - the null hypothesis
553:56 - is always that the relationship which we
553:59 - represent with the greek letter rho
554:01 - looks like a p
554:03 - equals zero that means there's no
554:05 - relationship
554:10 - the alternative
554:12 - is that rho
554:14 - is not equal to zero or that there is a
554:18 - relationship
554:23 - and we can see in our case
554:26 - we're going to make a decision
554:31 - because the p-value is less than 0.05 to
554:35 - reject
554:37 - the null
554:40 - and make a conclusion
554:45 - that there is
554:48 - significant evidence
554:56 - to conclude
555:03 - the alternative hypothesis which says
555:05 - there is a relationship
555:09 - there is
555:11 - a relationship
555:18 - between
555:20 - age
555:22 - and pages red
555:27 - but we can expand on that conclusion a
555:29 - bit and say
555:32 - because
555:35 - r is equal to 0.947
555:39 - which if we scroll up in our notes
555:42 - we see that puts us in the very strong
555:44 - category between 0.8 and 1.0
555:48 - we can say
555:55 - the relationship
556:00 - is
556:01 - very
556:02 - strong
556:06 - we can even go one step further and say
556:08 - because
556:15 - r squared equals 0.896
556:19 - we can claim
556:24 - that
556:25 - 89.6 percent
556:28 - of the variation
556:32 - in the dependent variable or in pages
556:35 - red
556:38 - is explained
556:43 - by the independent variable or by age
556:52 - and so you see we don't end up with the
556:53 - traditional hypothesis test going
556:55 - through all the same exact steps we have
556:57 - before but the pieces are still there
556:59 - that we decided based on our p-value
557:02 - that there is a significant relationship
557:05 - based on our r we can determine how
557:07 - strong the relationship is and based on
557:10 - our r squared we can say what percent of
557:12 - the variation is explained
557:15 - by the independent variable
557:19 - so that's what we're looking at today
557:21 - determining is there a relationship
557:23 - if there is a relationship what that
557:25 - relationship is based on the equation y
557:28 - equals a plus bx
557:30 - and also drawing the scatter plot so we
557:32 - can get a visual to see
557:34 - that relationship
557:36 - we'll look forward to seeing you in
557:37 - class so we can look at these scatter
557:39 - plots linear regression and correlation
557:42 - a bit further

Cleaned transcript:

welcome to our first video of our statistics series this video is going to really introduce the idea of statistics and answer the question what is statistics and as we start studying statistics there is some key vocabulary that we need to be very comfortable with in order to study statistics accurately and communicate our results accurately so the first word we need to know in statistics is what we call the population the population is basically everything or everyone we usually are talking about people but not necessarily being studied maybe we're investigating college students the population would be all college students number two the second word we need to know is what we call a parameter and a parameter is some characteristic of the population if we're talking about all college students the parameter might be the average gpa that be a parameter of the population when we're talking about parameters just as a note for now we will usually use greek letters to represent parameters you'll see things like the greek letter mu or the greek letter sigma something in greek generally means we're talking about a parameter of the entire population this is different than if i was talking about a sample a sample is a portion of the larger population so if i went out and said we're studying college students and their average gpa the sample might be i took a sample of 200 college students that would be my sample and then with a sample we have these things called statistics hence the name of our course a statistic is a characteristic of the sample so this might be the average gpa of the 200 college students i interviewed and to differentiate samples and populations statistics and parameters with statistics we will use english letters these english letters would be something like what we'll call x bar or just the letter s so as we're looking at statistics of samples which hopefully will estimate the parameter of a population we're also interested in what are called variables and these are not the variables of algebra a variable is any characteristic of interest gathered from each item in the sample another way to think about the variable is it's really when i'm conducting a survey it's really the question that is being asked so for example with our college students we'd be asking them what is your gpa our variable is the gpa and then our last word for this part will be what we'll call the data or the actual values of the variables so a data for our example would be something like 2.38 a 3.47 a 4.0 that is the data so let's see if we can use this key vocabulary in an example see if we can identify the key vocabulary you want to know the average cost of statistics textbooks so you survey 25 textbooks and we're going to find and identify these key terms these same six key terms the population the parameter the sample the statistic the variable and the data first the population the population is everything that we're possibly interested in so we're talking about statistics textbooks the population would be all statistics textbooks and the parameter that we're interested in studying about those statistics textbook is the average cost the average cost but not just the average cost but the average cost of all statistics textbooks notice how i tie it back to the population because that's what the parameter describes that can be contrasted with our sample slightly different now our sample is the smaller group the subset we're looking at this is the 25 textbooks and then the statistic has to describe that sample it's our it's our characteristic of interest for the sample it is the average cost of and then tie it back to the sample the 25 textbooks now the variable that's the information i'm gathering the variables when i look at each textbook what am i recording or the question what am i asking with the variable i'm asking the cost of a statistics textbook and the data is the answer to that question or the actual values of the variables so it's the actual cost of the textbooks an example might be you find an expensive one for 235 dollars that is a piece of data to the variable and answer to the question all right now that we have some vocabulary let's move on to an example of actually summarizing data which is what statistics is all about with statistics we're often interested in what is called the frequency of an event or thing and the frequency is just how often a value occurs and often we'll organize these frequencies in what we call a frequency table and to set up a frequency table we have a little bit more vocabulary first is what we're going to call the relative frequency and the relative frequency is just the proportion of times a value occurs in other words it's the decimal equivalent of the frequency divided by the total and often with frequency we're interested in what's called the cumulative relative frequency and that is the sum of all previous entries so with that vocabulary the way we're going to set up the actual frequency table is frequency tables we'll generally have four columns one column for the data value a column for the frequency which i'll denote with just f a column for the relative frequency which i can denote with rf and then a column for the cumulative relative frequency crf and so then we fill in our data values maybe one two and so on actually let's just do one and two and then we'll do the frequency maybe the first the one appears three times and the two appears seven times so out of 10 the relative frequency is 3 out of 10 or 0.3 and for 2 the relative frequency is 7 out of 10 or 0.7 and the cumulative relative frequency will start to add all the previous values so 0.3 plus 0.7 is 1.0 for the last column interesting to note is the last entry should sum to 1. now maybe if you have a round off error it might be 1.00001 or 0.99999999 that's okay but generally speaking we hope that cumulative relative frequency should sum to 1. that's all to show us how to set up the table let's actually make a frequency table let's do an example here let's say a baker keeps track of how many free donut holes his customers eat 25 eat one donut hole 15 eat two donut holes seven eat three donut holes and three eat four donut holes let's make a frequency table we have the values of one two three and four are the number of donut holes that were eaten for the frequencies we know 25 eat one donut hole so that's our frequency 15 is the frequency for two donut holes seven for three and 3 eat 4. now if we want the relative frequency what we have to do is divide the frequency by the total so we need to know what the total is and so if we add these up we get a total of 50 customers so when we divide 25 by 50 we get a relative frequency of 0.5 when we do 15 divided by 50 we get a relative frequency of 0.3 7 divided by 50 we get a relative frequency of 0.1 4 and 3 divided by 50 we get a relative frequency of 0.06 and the fractions aren't needed so much as the actual decimal answers in our table and then once we have our relative frequency we can find the cumulative relative frequency by adding all the values before that so for 1 we only have the 0.5 but then for 2 we're going to add 0.3 so we add point three to get point eight and then we add the next value add point fourteen to get point ninetysix and then we add .06 to get 1.00 and that fills in our frequency table now that we have our frequency table we can answer some questions about this baker we could answer questions like what percent 8 between 2 and three donuts two and three donuts have a relative frequency of 0.3 and .4 so when we add those together 0.3 plus 0.14 the percent is 0.44 or as a percent 44 percent how about what percent eight more than three well i'll highlight in pink more than three just means four so that must be this last entry of .06 and so we can say .06 or six percent finally what percent ate at most three well i'll mark them in blue here at most three as everybody else i could add those all together but what you might notice is that everybody else excludes the six percent so it might be easier to say we've got a hundred percent as everybody exclude the six percent that are more than three and that leaves us with 94 percent eight at most three donut holes so the big thing we're doing today is we are looking at statistics vocabulary and organizing data in frequency tables and interpreting that information take a look at the homework assignment that goes with this section and in class we will investigate these frequency tables a little further statistics is all about the interpretation of data so we're going to take a look today at the question how is statistical data collected and first we have to really understand what we mean when we talk about data data can be measured on several different levels so let's take a look at the levels of measurement levels of measurement start with the most simple type of data and grow to the most complex type of data it's important we know what we're working with so we make sure we can do the correct mathematical operations of it and make sense of our conclusions the most foundational level of measurement is what is called nominal which basically just means we put things into categories you might say there are no numbers so for some examples of nominal categories you could look at color or maybe a yes no survey do you support this political issue or maybe some type of label or gender those are nominal categories now slightly more involved than a nominal category is something that we can actually put in order and say this one is more than that one which is more than that one we call this ordinal data and that's data that can be put in order however with that order there is no clear space between the data values in other words we can say a comes before b but we don't really care how much before b the space between a and b might be different than the space between b and c an example of ordinal data might be finishing place in a race there's a clear first second and third but the space between first second and third is not necessarily well defined that's ordinal or we might say the top five cooks in america cook one two three four and five that's ordinal we can put them in order but we don't necessarily know how much better each one is than the other now when we do clearly define the space between values we have what we call interval data interval data has a space between the numbers with meaning the space has meaning however there still is no zero point the space between uh the two numbers is well defined like in the example of temperature 30 degrees is 10 more than 20 degrees just like 90 degrees is 10 more than 80 degrees that space has meaning and while temperature does have a zero zero degrees doesn't mean the absent of temperature or no temperature it's just a point along the number line that has the same spacing as all the other temperatures now if we want to have a clearly defined zero that's when we have ratio data and that's where we have an absolute zero that means literally nothing or none an example of ratio data might be the length of a phone call or a test score or the number of children if we said we got a zero on a test or we have zero children that actually means nothing the lowest amount possible now these different levels of measurement can show up in different types of data let's scroll up it's very important we understand the two different types of data and the second one can actually be broken up into two subgroups the first type of data is what is called qualitative qualitative data which really focuses again on categories no numbers we're describing the qualities of something so for example we might be talking about the type of car or maybe some type of ethnic group those are the qualities of the data that's qualitative data the second type's the one we work with the most in statistics and that is what is called quantitative data and quantitative data is when we're looking at quantities quantitative data is either measured or counted really what we're talking about is numbers how many how much how far how long some examples of these might be the number of students or the distance to school quantitative data can even be divided up further quantitative data can be either discrete or leave a space continuous discrete data is data that is countable one two three four five when we count things we say we have discrete data generally we don't get decimals with discrete data examples might include the number of shoes a person owns you're not going to end up with decimal shoes you're not going to have 1.4 shoes discrete date is countable you count the number of shoes there are that's contrasted with continuous data which is measured which means every decimal and every decimal in between those decimals is possible measured data example might be the length of a phone call or maybe somebody's height that's continuous data it's measured always we can have decimals you can have half an inch you can have half of a half of an inch or a quarter you can have an eighth of an inch you can do all the decimals in between that is continuous data so that's the different levels of measurement we can do with different types of data but we haven't answered the question of how do we collect our data which is what we said we wanted to do at the beginning so let's take a look at actual sampling of data how do we collect it when we collect data in a sample we want it to be representative of the entire population in order for it to be representative of the entire op population we need the data to be random to avoid any bias in other words we want all options to be equally likely to be included in our sample so if random is best let's take a look at a few random sampling methods the first random sampling method is what we call simple random sampling simple random sampling is random selection methods such as random numbers or drawing out of a hat is the idea of i assign everybody a number and i pick a bunch of random numbers and those people are included in my survey an example is if i want to pick students we could assign students a number and pick random numbers to be included in the study a second type of random sampling is what is called stratified random sampling and this is when we divide the population into groups each group is called a strata and then select a proportionate number of each group polling is often done this way the idea is if we randomly survey 100 people or 200 people we don't want those 200 people to be all of the same political party otherwise we would get a biased sample so we guarantee it by saying if my state is 40 percent democrat 35 percent republican and 25 independent then using a random method we will select i said we wanted 200 people so maybe we double the percentages maybe we'd select 80 democrats 70 republicans and 50 independents to be included in my sample i have a proportionate representative of each group that matches the proportions of the state and so my sample still is random but i don't get the biased of only interviewing one party another type of random sampling also involves groups but it's called cluster random sampling again we're going to divide into groups but this time instead of selecting a proportionate number of each group we are going to randomly select entire groups so everybody in some groups are included and everybody in other groups are excluded an example of doing this might be a football stadium and it has different sections in the football stadium so we're instead of interviewing everybody in the stadium or random people through the stadium it's easier just to hit one section at a time so in the football stadium sections e and g are randomly selected and all fans from those sections are included cluster takes all people in each randomly chosen section while stratified takes a proportionate group out of each group the last random method we're going to use is called systematic random sampling the idea behind systematic random sampling is we start with a random item or person and choose every nth person after this that means like every fifth person or every 12th person or every 30th person an example of this might be if i pick a random phone book person phone number a random number in the phone book and then i choose every 50th person after this until i circle back to the beginning and go all the way through and back to the beginning and back to where i was taking every 50th person those are our four random sampling methods that you should be able to identify for this course there is a nonrandom method that is used quite a bit and so we should at least acknowledge the nonrandom method called convenience sampling which basically says use the results that are readily available as an example i would say i need to collect 50 data values for a survey so i'm just going to interview people nearby me or maybe people within driving distance maybe the friends on my facebook friend list something convenient and easy to get a hold of it's not really random and the problem with not being random is there are several drawbacks to not being random i could have bias results if i'm interviewing phone preference outside of the apple store i'm going to get more iphones that convenience sampling is going to work against me it may not be representative of the population if i only interview outside of a school because it's convenient near my house i'm going to get a lot more parents of young children than i will the general population and then the drawback of that is the results may not be useful outside of the sample i could conclude that such a percent of people prefer a certain type of music but when i'm with a different sample or a different population those results may not be useful so those are some drawbacks to convenience sampling we got to watch out for convenience sampling though it is used more often than it should be take a look at the homework assignment that practices with random sampling and also some of these different levels of measurement and types of data in class we're going to do more work with random sampling get really comfortable with the different types and i'll look forward to seeing you then now that we know the basic vocabulary of statistics and know how to collect data we're actually ready to start displaying some data our question for today is how do we display data visually and we're going to look at two main ways to display data the first way is going to be with what's called a histogram a histogram shows us frequencies over intervals and a histogram can really give us an idea of the shape of our data let's do a quick example here it's going to be easier to talk about a histogram with an example of a histogram so here's a scale two three four up the vertical axis the yaxis is always labeled frequency and then the xaxis is going to be some type of label of what we're actually looking at maybe we're looking at the number of tvs in a home and i'm going to actually start at negative 0.5 i'm going to come over here to 1.5 3.5 5.5 7.5 9.5 and we'll end at 11.5 first box we're going to make one tall the next one we're going to go up to three the next one we're going to start at 2 the next bar will start at one then we'll skip a space and put a bar at one i'm also going to give this a title let's title this the number of tvs and this picture is an example of what you would expect a histogram to look like a couple things that i want to note the bars on a histogram touch they come down on the interval numbers not in between but right on those numbers i labeled and they show frequency in a range in other words if i color the second bar green here what that second bar means is that there are three values or three people who reported having between 1.5 and 3.5 tvs between 1.5 and 3.5 are the numbers 2 and 3. so the numbers 2 and 3 all went into that bar now it's impossible to have 1.5 or 3.5 tvs and that's what brings up the second point here point b if possible and it's usually easiest with discrete data never have a bar come down on a value in other words if i'm talking about the number of tvs i don't want this to come down on four tvs if a bar came down on four tvs i wouldn't know if that four goes in the left bar or the right bar so instead i make sure my bars come down staggered from actual data values in this case by .5 and you also notice i took care to give a title to my graph and labels for the x and y axes title and labels are important so now that we kind of know what a histogram is and what it looks like showing frequencies over a range let's look at how to make a histogram first we need to decide on the number of bars once we decide the number of bars we'll use this nice little formula where we take the high number minus the low number and then divide by the number of bars and then whatever that number is we will round up and that will give us the bar width how wide each bar should be now it's important to note we always round up if it's 3.1 we'll still round up and the bars will be four in fact if it's three point zero we would still round up to have four bar bars of width four we always round up to the next number after this division next we need to decide on the starting value what i usually recommend we do to decide on the starting value is do 0.5 before the lowest number in my tvs example 0 is the lowest number of tvs so i went 0.5 before that so i'm staggered and the bars won't come down on that then it's helpful to make a frequency table like we saw in section one with ranges with the ranges that we found in parts a and b here and we can use that frequency table then to build the histogram so if that's the method let's see if we can go ahead and actually do that with an example the number of miles 20 students commute to work is below we are going to make a histogram with five bars to represent the data and our data here is going to be 4 6 6 7 11 13 18 18 18 21 24 26 27 35 36 36 42 43 45 and 49 so for our bar width we need to take the high minus the low divided by the number of bars so the high is 49 and the lowest four 49 minus four divided by the five bars is nine now it's exactly nine so i have to round that up to ten is my width always round up to the next whole number otherwise your last values won't be included in the last bar now we can decide on our start values or our ranges so for x values to start with our graph we're going to start half a unit before the low value of 4. so half before 4 is 3.5 and we're going to go up 10 to 13.5 the next range is going to start at that 13.5 and go up 10 to 23.5 then we have 23.5 to 33.5 and i'm running out of space so i'm going to scroll up a bit 33.5 to 43.5 and 43.5 to 53.5 those are our ranges now we just need to find the frequency inside each range 3.5 to 13.5 there's one two three four five six numbers 13.5 to 23.5 we only have four numbers then you see 23.5 to 33.5 there's three numbers 33.5 to 43.5 there's five numbers and 43.5 to 53.5 we see two numbers now that we have that frequency table we're ready to build our histogram starting at 3.5 my next tick mark is 10 later at 13.5 then 23.5 33.5 43.5 and 53.5 our frequencies go up to 6 1 2 3 4 5 6. the first bar is 6 tall the second bar touching it is four tall the next bar touching it is three tall the next bar touching it is five tall and the next bar touching it is too tall and now my bars show the frequency within each of those ranges now i still need a title we're talking about miles driven to work it's a good title our xaxis is in miles and then the yaxis going from one to six always shows my frequency and there's my histogram now with histograms it's important not just to be able to draw the histogram but we also need to be able to describe the shape of the histogram that we end up with so a couple notes here on some vocabulary you can use to describe the shape of a histogram and now with real world data it's never perfect histograms are never perfectly any of these shapes but they tend to be close to one of these shapes not always but generally a common shape we'll see is what's called the uniform shape where the bars are about the same so just a quick sketch here you might see bars that they're not quite exactly the same but they're pretty darn close we would say that histogram is uniform they're all about the same height on all the ranges another word we should know is what's called a normal shape the normal shape is taller in the middle and shorter on the edges what a normal shape looks like is it generally starts short and gets taller until the middle and then after the middle it starts getting shorter again it's also called the bell shaped curve where it goes up and then back down the opposite of the normal shape is what we would call the vshape and that is shorter in the middle and then taller on the edges so that's when we have a tall edge that gets shorter and then it comes back up and gets taller on the outside and you can almost see that vshape right on top of those bars in addition to the shape we can talk about its symmetry we say a graph is symmetrical if it's basically the same on both sides so if we go tall and then shorter and then shorter it's going to be the same thing on both sides you notice that's the same as the v shape it's also symmetrical the same on both sides and we can combine these different descriptive words together to come up with a detailed description of a histogram then we have this idea of skewedness skewed means it's not symmetrical and we describe the unsymmetrical part where the extra stuff is skewed right means it's not symmetrical because we have extra stuff for lack of a better term on the right that's where we're trying to be symmetrical right but then on the right side there's all this extra stuff and it kind of goes down a lot slower that extra stuff means it's skewed right and you might expect the last term then is skewed left where the extra stuff is on the left and that's where we've got all these extra little short bars on the left before it starts growing and giving us our what would be symmetrical shape so that's histograms we can draw them to show the shape of the data we can describe them as uniform normal v shapes skewed symmetrical really can help us visualize what our data set looks like a second thing we can do though to describe our data visually is to draw what's called a box plot and a box plot shows the spread of data with what is called the five number summary five number summary is made up of five pieces a b c d and e the two easiest to find are the minimum and maximum and then right in the middle of them is what is called we'll put it on c the median or the middle when the data is in order so the median cuts the middle of the data and then we've got the top half and the bottom half in the bottom half we're going to find what's called q1 or the first quartile which is the middle of the lower half and similarly q3 is called the third quartile which is the middle of the upper half so the quartiles and the medium really divide it into quarters now there's a little caveat with the quartiles in the median we said they're the middle values but if there are two middle values what we'll do is we will add them together and divide by two giving us the middle of the middle once we find that five number summary we'll usually represent it visually with what's called the box plot the box plot splits the data into quarters and to do that we put q1 and q3 as the edge of the box and then we draw whiskers out to the min and max values and finally we use a dotted line for the median and so what we end up with is there's some number line down the bottom and then floating above that number line is the box showing where the quartiles are q1 and q3 whiskers out to the minimum and maximum values and then a dotted line for the median and each part of the box represents a quarter or 25 of the data values the box then is the middle 50 the whiskers are the outside 50 percent and we can make some visual conclusions about how our data is spread out let's see if we can make a box plot going back to our example with the commute time that list of numbers again for us was 4 6 6 7 11 13 18 18 18 21 24 26 27 35 36 36 42 43 45 and 49 now if this data was not in order it would be essential as a first step to put the data in order fortunately ours already is in order we know there are 20 data values so 20 cut in half is 10. we're going to have 10 below and 10 above so 1 two three four five six seven eight nine ten cutting in half ten below and ten above sticks us right between twenty one and twenty four because it's right in the middle we add those numbers together and divide by 2 to get our median value which is 22.5 then we can go after our quartiles our quartiles are in the middle of the bottom half and the top half well the bottom half has 10 values so we're going to split 5 and 5 which sticks us right between 11 and 13. again because there's not one value in the middle we'll add those together and divide by 2. 11 plus 13 divided by 2 gives us our q1 equal to 12. for the upper quartile again we're going to have five data values on each side sticks us right between 36 and 36 when we add those together and divide by two the third quartile is 36. also include our minimum value of four and our maximum value of fortynine and we're ready to make our box plot showing the spread of our data we should make every attempt to make this box plot 2 scale so if i count by 6's starting at 3 we'll have 3 9 15 21 27 33 39 45 and 51 notice those are about the same size apart from each other my box is made from the quartiles at 12 and 36 connect our box the median is a dotted line at 22.5 the whiskers go to the minimum of four and the maximum of 49 and we have our box plot of course any graph needs a title so we can title this commute time and label the xaxis maybe time in minutes and we have our box plot now normally the box plot would all be one color but i did color coding to show where all the pieces came from in this example and just like we can describe the shape of our histograms we can also describe the shape of our box plots and there's basically three ideas here one is the idea of being spread out where we have a wide range of values this would be a big box plot covers a large range of values that means the data is not really close to each other not close to the median not not really close to anything it's all spread out the opposite of being spread out is to be clustered together where we have a small range of values and this is going to give us a really tiny box plot everything's really close to each other and often it's beneficial to split our description into the entire shape and the middle 50 percent or the box for example i could have a box plot that looks like this and we could say overall the data is spread out but the middle 50 percent is clustered together because the box is small compared to the rest of the data so that's what we're looking at today histograms and box plots practice making them practice interpreting them describing them take a look at some of them on the homework we'll work with these a little bit more in class and we will see you then in our previous section we took a look at how we could summarize data visually but quite often it's going to be useful to summarize data with numbers so that's going to be our question how do we summarize data numerically and there's two things that we'll try and summarize numerically the first one we're going to plant on for a minute here are the measures of center where is the middle of the data and depending on our context the measure of center will either be the mean the median or the mode let's start with the mean or what people typically call the average now if we're talking about a population mean we will always use a greek letter and that's going to be the greek letter mu but if we're talking about a sample mean we will use an english letter which we will notate with x bar and the formula for calculating the mean i'm going to use x bar but it works also for mu is equal to this symbol which we call sigma x over n and that symbol that funny looking thing means the sum what this means is sum up all the x's or sum up all the values and divide by n which is the sample size or population size if we're in the population context so for example if i had the numbers 1 3 3 4 4 4 5 5 and i wanted to find the mean of this sample the numerator says sum up all the x values or do 1 plus 3 plus 3 plus 4 plus 4 plus 4 plus 5 plus 5 and divide by the sample size and if i count here i see we've got a sample size of 8. so adding those all up we get 29 over 8 which gives us a mean of 3.625 or the average is 3.625 most students are familiar with that mean formula but one tweak we can do to it and we quite often have in statistics it's not two let's call this c because we're still under mean is if we have frequencies so we've already seen one formula for the mean the sum of the x's divided by the n but if we have frequencies given to us where we know how often each number is used the formula is going to tweak slightly x bar is going to be equal to the sum of the x's times the frequency divided by the sample size so if we have frequencies we should know this formula let's do the exact same example we just did but this time we're going to summarize those numbers by their frequencies so first i'll list out the numbers the numbers that showed up were 1 3 4 and 5 then i'm going to have another column that shows the frequency i had a single one there were two threes three fours and two fives what the sum says we need to do is first multiply the x's times the frequencies so when we multiply one times one is one three times two is six four times three is twelve and five times two is ten and then what we wanna do is sum this x times frequency table so 1 plus 6 plus 12 plus 10 is 29 that represents the sum of the x's times the frequencies and then we just have to divide by the sample size well the frequencies tell us how many things there are so my sample size is 1 plus 2 plus 3 plus 2 is equal to 8. so for my mean again we're doing 29 divided by 8 to get the exact same number of 3.625 but sometimes it's a lot quicker and easier if we have those frequencies given to us that's the first measure of center that measure of center of the average says if everybody was split up equally they'd have this many in common but quite often the problem with the mean is one large value or one small value can throw off the mean significantly which is why we might be interested more in the median or the middle number when they're first put in order this way one large or one small number won't have a dramatic impact on this measure of center so for example using our same data the one three three four four four five five the median is going to be the value right in the middle well if there's eight values the middle puts you between the four and the four and we know from our previous section when we made box plots we would add those and divide by two to get our median value which turns out here to be four and the big advantage of the median is that one extreme value does not impact the median as significantly as the mean it's a little more stable now the third measure of center and this is often used in categories or nominal data is what is called the mode the mode is the value that occurs the most often and again it's usually best for categories if we're talking about the color of cars in the parking lot we're not going to have an average of a blue point green car that doesn't make any sense but what we can do is say the most common frequent colored car is blue or gray or whatever that most frequent one is we can still look at mode in terms of numbers we seem to be using this example data of one three three four four four five so let's keep using it and what we see is the number four appears three times it is the one that occurs the most often so we will say that the mode is equal to four those are our measures of center the mean median and mode but the problem with just measuring the center is it only tells us where the middle value is it doesn't tell us kind of how all the rest of the data is behaving around the center is the data really spread out is it clustered close to the center what's happening with the rest of the data and that's why we also need some type of measure of spread it tells us more than just the middle it tells us how spread out the the other values are not just where the middle is but how's everybody else behaving around the middle and the reason this is important is we can look at data such as these three data sets i'm going to put up here and all of them all of the following have the same mean and median the first data set's going to be 1 1 1 5 9 9 9. both of that data set has a mean of 5 it also has a median of 5. but if i do another data set of 1 2 4 5 6 8 9 that data set has a mean of 5 and a median of 5. and if i do a data set of five five five five five five five that data set has a mean of five and a median of five there's no difference between these three data sets if i just look at the center but the numbers are spread out quite differently the first data set the blue one are spread out very far the green ones kind of spread out evenly and the red one has absolutely no spread in it at all this is why we need a measure of spread and the most basic measure of spread is what we call the range the range tells us how much space there is between the largest and the smallest number we take the large number and subtract the small number to see how far apart those extreme values are so for example with our data we've been playing with today of one three three four four four five five the range would be the big minus the small 5 minus 1 equals 4. and so there is a space of 4 between all of these values now there's a problem with the range though the problem with the range is one extreme value could greatly impact for example if there was also a 27 on this data set 271 which we sound like 26 there's a large range between the numbers when most of them are actually clustered quite closely together so this is why we need a different better measure of center and one measure of center that might be better is what we call the inter quartile range it's often abbreviated as iqr for inter quartile range and the interquartile range you could think about as the range of the middle 50 percent what we'll do is we'll take the q3 and subtract the q1 value subtract the quartiles and we see the range of the middle 50 percent or how spread out the middle 50 percent is and then we're no longer going to be impacted by an extreme outlier that's way too big or way too small for the rest of the data so for example with our data of 1 3 3 4 4 4 5 5. we already said the median was between 4 and 4. the first quartile then q1 is between three and three which is just three and the third quartile q3 is between four and five which is four point five this means our interquartile range is 4.5 minus 3 or 1.5 and that might be a little bit of a better measure of the spread because it's only looking at the middle 50 percent the extreme outliers are not going to impact the interquartile range however we still have a problem and that problem is this interquartile range formula only considers two values the q1 and the q3 it would be nice if we had some measure of spread that considered all the values and how spread out those are and this gives rise to the most important measure of spread the one we'll use a lot in this class called the standard deviation and the standard deviation attempts to measure what we call the average distance a point is from the mean how spread out is the data considering all the data values on average how far are they from the mean now with a population we will use a greek letter for the standard deviation and that's the greek letter sigma and with the sample we will use the english letter s to represent the sample the formula for the standard deviation kind of builds on this idea that we want the average distance from the mean so if i took any point and subtracted the mean that would give me the distance it is from the mean the problem is is some of these will be positive and some of these will be negative so if i add them up it actually adds up to 0. so to avoid the positive negative problem what we'll do is we'll square each of the values before we take the sum and add them all up then we'll divide by the sample size which turns out with standard deviation and when we derive the formula it's not exactly the sample size we divide by but we'll divide by n minus 1. and the reasons for the 1 are beyond the scope of this course so you just have to trust me to take an average distance from the mean with the standard deviation we're going to divide by n minus 1. the problem that we still have though is we squared everything so it's not really a true average so to undo the square we'll take the square root at the end of our formula and we will say s is equal to that square root and that is going to be an important formula for us in this course now i do have one little caveat turns out that the formula for a population standard deviation is slightly different than s the formula is we're not going to worry too much about that different formula for the population because generally we always have a sample we're always going to be taking sample standard deviations which is this formula we've looked at here so let's do an example and let's start with a smaller example and then we'll move to the bigger example that we've been seeing throughout this video we'll start with the example 11 13 14 and 14. and i'll give you a hint if you go through and calculate the mean of these values the mean is going to be equal to 13. what we'll do to get started is we'll list our values 11 13 14 14. and then we're going to have a column for every step along the way in this formula the first step says take those x values and subtract the mean subtract 13. so 11 minus i can even put a little thirteen here we're subtracting thirteen eleven minus thirteen is negative two thirteen minus thirteen is zero fourteen minus thirteen is one and fourteen minus thirteen is one then the formula says to square r values so the x minus x bar each of those values needs to be squared negative 2 squared is 4 0 squared is 1 1 squared is 1 and 1 squared is 1. finally the formula says to take the sum so the sum of x minus x bar squared is equal to 4 plus 1 plus 1 which is 6. now i'll jump to the standard deviation formula which says the square root of the sum which is six divided by one less than the sample size which is three six divided by three is two and the square root is one point forty one now similar to our formula with means if the data is given to us with frequencies rather than individual data numbers we need to do a slight adjustment to our formula so if we have frequencies the formula will slightly adjust to s equals the square root of the sum of x minus x bar squared but before we take the sum we have to multiply by the frequency and then divide by n minus 1. so let's take a look at an example where we do it with frequencies and let's use that data set that we've been using where we know the x values were 1 3 4 and 5. and the frequencies of that were one two the number four appeared three times and the number five appeared twice now we already know the x bar the mean is 3.625 we would have had to find that first if we didn't know that but since we do now we'll make another column for x minus x bar taking the x the number 1 minus 3.625 is negative 2.625 3 minus 3.625 is negative 0.625 4 minus 3.625 is 0.375 and 5 minus 3.625 is 1.375 next the formula says we need to square that x minus x bar we're going to square each of these numbers so 2.625 squared and i'm going to round to two decimal places is 6.89 0.625 squared is .39 0.375 squared is 0.14 and 1.375 squared is 1.89 now if we didn't have frequencies we would just add this column up but because we have frequencies we need to take this column the x minus x bar squared and multiply by those frequencies so the 6.89 needs to be multiplied by 1 to get 6.89 the point 39 needs to be multiplied by its frequency of 2 to get 0.78 0.14 times 3 is .42 and 1.89 times 2 is 3.78 and that's what we want to sum we want to get the sum of x minus x bar squared times the frequency and when we add up that column you should end up with 11.87 now we plug into our formula for s s is the square root of that sum we just found is 11.87 divided by one less than the sample size you could add the frequencies together to find out the sample size is eight or you might remember that because we've been working with this for quite a while one less than the sample size is seven so the square root of 11.87 divided by 7 is 1.30 so on average these points are about 1.30 units away from the mean of 3.625 gives us an idea of the middle and how spread out the data actually is now the standard deviation is actually quite nice because it gives us a way to compare data based on how many standard deviations we are from the mean standard deviations measure distance from the mean and in statistics we will use a very important variable to represent the number of standard deviations we are from the mean that variable is always going to be z z is the number of standard deviations from the mean and we actually have two formulas that use z they're both really the same formula the idea is z is the number of standard deviations from the mean so we'll take the value we're working with we want to know how many standard deviations x is from the mean well first we need to know the distance to the mean so we'll subtract the mean or x bar and then we'll divide by s are the number of standard deviations that is that's our main formula for z now sometimes we have the opposite information and we want to know what is three standard deviations from the mean we know z we want to be three standard deviations from the mean what value is that and so we can solve this equation for x and when we do we get x is equal to the mean plus the number of standard deviations times the standard deviation and so this formula it's really the same formula it's just been solved for x will tell us the number of standard deviations is what value let's do some examples let's say for our data one three three four four four five five we wanna know how many standard deviations from the mean is the median well we've already found all these important values we found the median is equal to 4. also earlier we found the mean was equal to 3.625 and also earlier we found the standard deviation is equal to 1.3 so if we want to find out how many standard deviations the median is from the mean the medians are x the means the x bar and the standard deviation is s so z the number of standard deviations four is we subtract the mean of 3.625 and divide by the standard deviation of 1.3 and we find the median is 0.288 standard deviations from the main or we could ask a similar question for the same data what value is 2 standard deviations below the mean we already know the number of standard deviations so 2 that's actually going to be our z the number of standard deviations below the mean and because we want to be below the mean we will use a negative number to make it below the mean so we're looking for the x this time what is that value of interest so x is equal to the mean x bar of 3.625 minus 2 because we have a negative 2 two standard deviations below the mean times the standard deviation of 1.3 and that gives us 1.025 now it turns out that we can say 95 of our data generally falls within two standard deviations of the mean if it's more than two standard deviations from the mean we say those values are unusual or extreme values we call those extreme values outliers and these outliers are values far removed from the rest of the data for example if i have the numbers 1 3 3 5 87 87 is far removed from the rest of those values it's an outlier and the outlier can either be large or small and we actually have two methods for calculating outliers and they generate very similar results so neither one is necessarily better than the other but i want to show you both of them the first one is called the interquartile range method and the idea behind this is we cannot be more 150 or 1.5 times the iqr from the edge of the box in the box and whisker plot the idea is anything below the first quartile minus 1.5 times the iqr or anything above the third quartile plus 1.5 times the iqr anything that's above or below those numbers becomes an outlier so for example let's say we've got the numbers 2 3 5 6 7 and 14. now the first chord the medians right in the middle but what we're interested in with the inner quartile range is the middle below the median the first quartile is three and the third quartile is seven so the interquartile range is seven minus three four four this means an outlier is anything below the first quartile 3 minus 1.5 times the interquartile range of four that gives us three minus six or negative three anything below negative three would be an outlier which there's nothing in this data set below negative three but also anything above the third quartile 7 plus 1.5 times the interquartile range 7 plus 6 is 13 anything above 13 becomes an outlier and you notice 14 is right on the edge there and so we will say based on that 14 is an out liar it lies outside of the majority of our data now that's the interquartile range method i said there's a second method it's based on the standard deviation the standard deviation method says that anything that is more than two standard deviations from the mean is an outlier which brings us back to that example that led into this discussion so actually let's first define this clearly let's do anything below x bar the mean minus two standard deviations or anything above x bar the mean plus two standard deviations is considered an outlier so for example we had our data of one three three four four four 4 5 5. and we found out already that the mean of this data is 3.625 and we also found out the standard deviation of this data is 1.3 so an outlier would be anything below the mean 3.625 minus 2 standard deviations 3.625 minus 2 times 1.3 is 1.025 and we'll recognize there that we do have a value below 1.125 or 1.025 it's just a little below but it is below here 1 is an outlier we also have to check above so we'll do the 3.625 plus 2 standard deviations or 2 times 1.3 which gives us 6.225 nothing above 6.225 so the only outlier we have here is the number one so we've covered quite a bit in this video we talked about measures of center the mean median and mode to estimate where the middle of the data is we talked about measures of spread to see how spread out we are around the mean or the center using the range the interquartile range and the most important ones the standard deviation and then we also looked at some uses of the standard deviation to see its distance from the mean finding out liars or identifying outliers so lots to take a look at on the assignment take a look and practice a few we will discuss them more in class we'll look forward to seeing you then our second unit is going to focus on probability and how to calculate various probabilities in different contexts so we're going to start out with basic probabilities and so that question we're going to answer is how do we calculate basic probabilities and as always we need to start off with some vocabulary to make sure we understand what we're talking about the first vocabulary word we need to know for probabilities is what is called the sample space the sample space is a list of all possible outcomes so for example if i was to flip a coin the sample space would be all possible outcomes i could get a heads or i could get a tails which begs the question what is an outcome so let's define an outcome really well an outcome is just the result of an experiment so an experiment might be flipping a coin an outcome would be just maybe heads or just tails and hence the sample space is all of the outcomes heads and tails when we're looking at the number of outcomes that occur in a sample space what we're really interested in finding is some type of probability or the chance that an event will occur and what we really end up with is a scale from zero to one where zero means the event certainly will not happen not going to happen and then the number one the maximum probability is it is certain to happen and then you could get any decimal in between so if i end up with like 0.5 that would be right in the middle so it's equally likely to happen or not happen so if the probability is .001 it's probably not going to happen but it could happen if the probability is 0.95 it probably will happen because it's closer to 1 but it might not happen the scale from 0 to 1 is our probability and there's two types of probabilities that we're going to be looking at the first is called the theoretical probability which basically says what we expect to happen or what should happen and the way we calculate a theoretical probability is we say that the probability of some event e is equal to the number of outcomes that we are looking for divided by the entire sample space or how many things occur could occur that's our basic probability formula so for example if i flip a coin and i want heads we would say the probability i get a heads is the number of outcomes there's only one outcome on a coin out of the sample space there are two possible outcomes on a coin heads or tails and then i would convert that to a decimal in this course we'll always use decimals for probabilities to get a probability of 0.05 of getting a heads that's theoretical probability what we expect to happen the other type of probability is called empirical probability and that is the chance of something happening based on our observations of some experiment what happened in an experiment so this is the example of maybe i flip a coin 500 times and i end up getting 257 heads because you know in actual practice the probabilities aren't perfect i'm going to get a few more heads or a few more tails it's not going to be exactly even and so in this case the probability of a heads as an empirical probability or an observed probability is 257 out of 500 which is 0.514 there's usually going to be a slight difference between the empirical probability and the theoretical probability but that difference can be made smaller using what's called the law of large numbers which basically says that the more trials i do the more trials done the closer the empirical probability is to the theoretical probability if i were to do a thousand trials this would be closer to 50 percent if i would do a million trials flipping a coin would get closer to 50 percent more trials the closer they're going to be now that's really basic probability but we do have some specific probability formulas to help us calculate some more involved situations and these three formulas we're going to look at are very closely related they really come as a group there's no one you should learn before the other because they're all so closely related so we'll do our best to define them one at a time when really they all come as a group the first is what we're going to call the conditional probability in a conditional probability we write it as the probability of b given a or with a vertical line between b and a and what that means is that is the probability of b given a has already occurred that's a conditional probability where we have some information and that's going to change the probability of b the formula for a conditional probability is the probability of b given a is equal to the probability that both occur a and b divided by the probability of the given information in this case the probability of a that conditional probability formula will be very important to us we'll do an example here in a minute let's go on to the second type of probability we need to know and that is the and probability the probability of a and b is the probability that both occur at the same time or together and the formula for the probability of a and b comes from the conditional probability if we multiply both sides of the conditional probability formula by the denominator the probability of a we end up with the probability of a times the probability of b given a has already occurred and that is the conditional i'm sorry that is the and probability formula the third formula we need to know is the or formula the probability of a or b occurring and that is the probability of a occurring or b occurring or both occurring one or the other or both and the formula for an or the probability of a or b is we're going to add the probabilities together the probability of a plus the probability of b the problem is this counts the and or the overlap twice it counts it in the probability of a and it counts it in the probability of b so we have to subtract off the overlap or subtract off the probability of a and b so it's not double counted it's only counted once and that gives us our formula for the or those are our three probability formulas the conditional probability is the probability of both divided by the given information the and probability with and we multiply the probabilities together given the first ones already occurred and with an or or probabilities we add them together subtracting off the overlap so let's do an example number four let's say we have three blue cards numbered one two and three and we also have let's write three out as a number since it's starting a sentence three blue cards numbered one two and three and i have two yellow cards numbered one two a we're going to find the probability actually let's just write as a probability statement we're going to find the probability that i get a blue card given the card is even with the conditional probability since i know the card is even we're not dealing with all five cards anymore we're just dealing with the even cards so we find the probability of both blue and even from the blue cards there is one that is blue and even so there's one of them out of the five cards divided by the probability of the given information the given information is that it's even there are two even numbers out of five now what's nice is generally those denominators will divide out and we're just left with one half or 0.5 so that means if i know i've got an even card the probability is 50 percent that it's going to be even that's conditional probabilities let's find the probability that i get a blue card and an even card now this is looking for the probability that both occurred at the same time blue and even of the blue cards only one of them is blue and even out of the total cards now we're looking at the total sample space of five and so the probability that's blue and even is 0.2 there's only a 20 probability that's both blue and even what about the probability that it's blue or even for blue or even now we're looking for how many are blue the first option there are three blue cards out of five plus how many are even there are two even cards out of five but then we need to subtract off the overlap the ones that are blue and even blue and even we know there's only one out of five that's both blue and even so three plus two minus one is four fifths which means we have a probability of point eight that it is blue or even i want to do one more example that kind of illustrates the and formula maybe a little bit better and that is finding the probability that if i draw two cards without with replacement actually maybe i should write this out probability i draw two blue cards without replacement what i'm really saying is what's the probability the first one is blue and what's the probability the second one is blue well for the first one to be blue the probability of the first event there are three out of five that are blue then we multiply by the second event the probability that i get a blue given the first one was already blue so now one of the blues is gone maybe the two is gone now there's only two blues left out of there's only four cards left and when i multiply that across we end up with point three is the probability we get two blues without replacement thirty percent so that's what we're looking for with that and formula the second part the and we adjust the probability to assume the first one already occurred those are our basic probability formulas but there are two vocabulary concepts that are related to those that i want to make sure we are familiar with so the first one of those two is what are called independent events when two events are independent what that means is one occurring does not change the probability of the other occurring the opposite of this would be dependent events and if we think about the blue cards that we drew without replacement the second probability was two fourths that second probability had changed from the first probability because one occurring changes the other one's chance of occurring because there's fewer cards left there's fewer blues now the way we show things are independent we can show them we can show this in one of three ways and it doesn't matter which one of these ways we use so we'll just pick the one that's most convenient for our context the first is we can show that the probability of a given that b has occurred if b is not going to affect its probability it should still be the same as just a occurring by itself because b has no impact on it the opposite is also true we could say the probability of b given a is going to be just equal to the probability of b because a occurring has no impact on it or the third method we can look at is the probability of a and b the probability of both of them occurring is equal to just the product of the individual probabilities because the given part doesn't change so for example let's say in a class twenty percent of students are lefthanded five percent of students are earning an a in the class good job to those five percent but only one percent of students are lefthanded and earning an a are these events independent is there is there a relationship between lefthandedness and earning an a well we'd have to look what's the probability that they're lefthanded we're told the probability they're lefthanded is point twenty or twenty percent the probability that they're earning an a of all students five percent are earning an a so the probability of earning an a is .05 we're also told the probability of the students who are lefthanded and earning an a both of them together is .01 well we can use either one of the three formulas for showing things are independent it's probably going to be easiest in this context to use the third because we have all of those pieces so the probability of a and b the probability of left and receiving an a should be equal to the probability of being lefthanded times the probability of receiving an a if they are in fact independent well the probability of a and l is 0.01 the probability of lefthanded is 0.2 the probability of an a is 0.05 and sure enough we get those are equal to each other because they're equal we'll say therefore they are independent events if it wasn't equal we would say the opposite or that they're dependent events so that's the first concept the idea of independent versus dependent the second concept that i want to wrap up with today is the idea of mutually exclusive two events are mutually exclusive that means both cannot occur at the same time essentially what we're saying is the probability of a and b is equal to zero an example of this would be if i were to roll a die a standard sixsided die and we're going to let o actually use the letter d because o is a bad letter for math d is going to represent an odd less than 4. b is going to represent a number bigger than 3. so what you see is d the odds less than 4 are the numbers 1 and 3. that's kind of the sample space of d and b the event b is everything bigger than 3 which is 4 5 and 6. these two have nothing in common you can't both be an odd less than four and a number bigger than three because we can't have both together we say they are mutually exclusive in other words the probability that we have an odd less than four and a number bigger than three is equal to zero that never happens so little vocabularies we wrap up with mutually exclusive both can't occur at the same time independent one occurring does not affect the other occurring but the big thing that we're looking at today are these probability formulas conditionals ands and ors take a look at the homework assignment to practice a few of these we will try a few more in class and answer any questions you might have then now that we're comfortable working with basic probabilities we're going to look at different ways we can organize our probabilities and information in today's video we're going to look at the question how do we organize probability information in a table specifically we're going to be in the context of what is called a contingency table which is basically just a table that lists results in relation to two variables these tables and this information will make calculating probabilities easier and what makes it easier is quite often we will add a column and row for totals so for example let's say we've done a survey and we're comparing whether or not people have speeding tickets in the last year or no speeding tickets in the last year and we're going to break this up into three groups the first group are going to be our younger drivers the under 21 drivers and then we're going to also look at the 21 to 25 year old drivers and then we'll also look at the over 25 drivers and the survey is conducted and there's 82 under 21s with the ticket 17 without a ticket in the past year for the 21 to 25s there were 39 with a speeding ticket and 27 without and for the over 25 there's 18 with a speeding ticket and 61 without now with this contingency table it's going to be helpful that we're going to add an extra row and an extra column if it's not there already that's going to give us the totals and these totals are going to make calculating individual probability questions much more efficient so if we total the under 21 we see we have 99 surveyed the 21 to 25 total that we get 66 surveyed totally over 25 we get 79 surveyed working across the rows 82 plus 39 plus 18 there's 139 people surveyed who got a speeding ticket in the last year the no tickets 17 plus 27 plus 61 is 105 and for the totals 99 plus 66 plus 79 gives us 244 people total in the survey and a good way to check that that total is correct is if we add the other combination 139 plus 105 that should also equal the 244 which it does and so that what we have there as that example is a contingency table now we're ready to find some probabilities off this contingency table for example if i want to know the probability that someone is 21 to 25 i can see very quickly on my contingency table that there are 66 people in the 21 to 25 range out of a total of 244 people and so when i divide 66 by 244 we can quickly get our probability of 0.2705 we could also do maybe the probability that someone has no tickets very similar i'd say well no tickets the total there is 105 out of the grand total which is 244 and when we divide 105 by 244 we get 0.4300 for our probability we can also do ands and we can do ors we can find let's combine these together the probability that someone's 21 through 25 and has no tickets well the 21 to 25 and have no tickets are when both of those occur together at the same time that's where they overlap here in the middle we have 27 people who are of no tickets and they're 21 to 25 out of the total of the whole group is still 244. and so when i divide 27 by 244 we get 0.1107 and we can change that to an or we can find the probability that someone's 21 to 25 or has no tickets and if you remember the or formula says we have to add the individual pieces and then subtract where they overlap so 21 to 25 there's 66 of them plus the no tickets there's 105 of them but we have to subtract where they overlap because these 27 where they overlap have been counted twice in both the column and the row so when we subtract off the 27 out of the 244 when we do that math on our calculator we get 0.5902 about a 59 percent probability they're one of those two we can even do given probabilities let's do the probability that we're in that 21 to 25 range let's get rid of these circles we don't need given we know the person has no tickets well with a given probability we are looking for both of them or the overlap divided by the given information so where they overlap 21 to 25 and no tickets they overlap with 27 but we're going to divide by the given information this time it's not the 244 because we've shrunk our sample size now we're just interested in those that have no tickets we're only interested in that 105. and so with the given information shrinking the sample size now the probability is 0.2571 we can switch that and see how that probability compares the probability they have no tickets given they're between 21 and 25 years old you might pause the video and see if you can figure this one out on your own with a given probability we need to find where they overlap divided by the probability of the given information they overlap again no tickets in 21 to 25 with these 27 individuals however now our sample space the given information is just the 21 to 25 years old and that's the 66. so we'll do 27 divided by the 66. to get our probability of 0.4091 and you can see how we move through each of these probabilities at a much greater accelerated pace when we have the contingency table to organize our data for us that's the nice thing about the contingency table one more thing i want to look at though is we have this vocabulary word from our previous video of independence so i want to know are being 21 to 25 and having no tickets independent does that mean being 21 to 25 has no impact on whether or not you had a ticket in the past year well we talked about there being three different formulas we could use in order to show this one of those three formulas says that a given probability should not change the probability if they are in fact independent in other words the probability they're being 21 to 25 given they have no tickets should be the same as the probability of just being 21 to 25 if they're independent because the tickets shouldn't impact that at all well we just found both of these pieces the probability of being 21 to 25 given we have no tickets is actually here in number five that was 0.2571 and the probability of being 21 to 25 we found in part one that's 0.2705 and we see that these guys are different therefore the probability has changed once we have given information and shrunk down the sample size that means these two variables are actually dependent on each other so all we're looking at today is organizing our probability information in a contingency table and taking a look at how that helps facilitate calculating the actual individual probabilities it also gives us an opportunity to practice more with and or and the given probabilities so take a look at these on the homework assignment come to class ready to discuss them and work with these contingency tables a little bit more today we're going to continue our work with representing probabilities visually using what are called tree diagrams and we'll also take a look at bayes theorem the question though that we're attempting to answer is how do we visually organize conditional probabilities how do we visually organize conditional probabilities and really the best way to organize a conditional probability is using what is called a tree diagram and a tree diagram basically has a branch for each outcome and then each branch produces a conditional outcome from that outcome from that outcome and then to find final probabilities we can multiply down the branches for final probabilities for example let's say we have an urn with five blue blue and three green marbles and you will draw out two marbles without replacement we are being asked to make a tree diagram to model the possible outcomes and probabilities so what we need to do is we're going to draw a branch on this tree for every decision point so the first decision point is the very first draw so this represents the first draw and off that first draw we could end up with a blue marble or we could end up with a green marble now you can see there's a total of eight marbles five plus three so the probability of getting a blue marble on the first draw is five out of eight and the probability of getting a green marble on the first draw is three out of eight but we're not done there because we're still going to draw another marble and that marble could be either blue or green and it doesn't matter which color we got first we're going to have the same possible outcomes it could be either blue or green on this second draw and so you can kind of see that if you follow a draw down if we want a green first and a blue second we'd go down the green line and the blue line and that represents green first and blue second or if i wanted a blue then a green we would go down the blue line first and then the green line second and we'd end up with the blue green combination but first let's fill in the individual probabilities because on the second draw things have changed if we go down the left side where the blue was drawn first and if we want a blue on the second draw there's no longer five blues left to pick from there are only four blues left to pick from and there are only seven marbles left so we only have a four and seven chance of getting a blue marble on the second draw given the first draw was blue similarly with the green option if we want blue then green tracking that down there are still three green marbles left but now the total is only seven marbles because one has been drawn out a blue one has been drawn out similarly on the right side of the tree diagram if green is drawn first and then i want a blue on the next draw there are five blues to pick from out of the seven marbles that are left but with the green there are only two marbles that are green left out of the seven marbles that are left now to get my final probabilities for each of these possible outcomes we just have to multiply down the chain so if i want a blue blue outcome what i'll do is i'll multiply the 5 8 times the 4 7 and when i multiply 5 8 times 4 7 we get a probability of 0.3571 now if i want blue then green we multiply down those probabilities to get the blue green possibility 5 8 times 3 7 is 0.2679 going down the next path we go green then blue so if we want a green marble then a blue marble that probability is 3 8 times 5 7 which turns out to also be 0.26 7 9. i think the other one was 2 6 7 9 also i wrote it down wrong the last track is going down the green and then the green so if we want two green marbles green then green we'll multiply the 8 times the 2 7 to get the 0.1071 and now we see all the different possible probabilities for drawing two marbles out of this urn without replacement and we can use that table to find things like the probability that both are the same color well both the same color would be a blue blue or a green green and then we subtract the overlap but there's certainly no overlap with blue blue and green green so we'll just add those together point and i should do this in red 0.3571 plus 0.1071 will give me the probability that i get two are the same color of point four six four two one more example what's the probability that i get at least one blue now i could do this a couple ways one way is i could say these first three options each have at least one blue but that's a lot more work than saying let's look at the complement and let's subtract off the only one that's left from the absolute probability of one we know all the probabilities are one so we subtract out the probability that doesn't satisfy what we want if we subtract off the 0.071 we'll end up with our probability of at least one blue of 0.8929 and that's how using a table can greatly facilitate our conditional probabilities and their various calculations but one very important application of using these tables is to facilitate what is called bayes theorem and bayes theorem is a way to find a conditional probability like the probability of b given a using known values of the probability of b oops sorry the probability of a given b in other words the given values are backwards from the given values that we want now bayes theorem officially says that the probability of b given a is equal to the probability of both the probability of a and b divided by the probability of the given information or the probability of a but we might not know the exact probability of a but we do know the probability of a given b and we can add to that the probability of a given it's not b in other words we combine all the a probabilities together to build that denominator and that last part that not b it may be several options so we might need to add three four five six things that are the not these but we have to basically add all the combinations of the probabilities of a given the other stuff this formula is not worth memorizing because it is always easier to do this on a tree the tree makes this easy and i'm going to show you what i mean with a couple examples let's say a marketing company predicts 90 percent of new products are profitable however the marketing firm isn't perfect only 70 of those products predicted to be profitable actually are in addition those predicted to be not profitable 20 percent actually are profitable so let's say we've got some product and it turns out that it was profitable that's given to us given a product was profitable what is the probability it was predicted to be profitable so was profitable what's the probability it actually was profitable let's make a tree to model this first the marketing firm looks at it and they say yes it will be profitable or not we are told the company predicts ninety percent of their products will be profitable therefore the complement 10 percent must not be profitable and that's actually not just profitable but that's a prediction of profitable and a prediction of not being profitable because then the next branch we're going to look at it actually is profitable or it's not profitable were they right or wrong it actually is profitable or it's not profitable we are told seventy percent of those that are predicted to be profitable actually are so if i go down that they're predicted to be profitable they actually are 70 are and the rest 30 are not of those that were predicted to be not profitable going down the nut line 20 percent actually are so 20 is on the right side and the rest 80 percent are not going down the lines multiplying point nine times point seven the first branch is a point sixty three percent probability point nine times point three there's a point twenty seven percent probability next branch point one times point two is point o two and point one times point eight is point o eight so for my actual probability statement it's asking what is the probability it was predicted to be profitable the probability that we predicted it was profitable given given it actually was profitable given it actually is profitable well we know conditional probabilities are the probability of both divided by the probability of the given information so the probability of both where it's predicted to be profitable and it actually is is this far left branch here that's 0.63 that is going to be our numerator the 0.63 the denominator then is going to be the probability of the given information the given information is that it is profitable the is occurs in two locations the is is the left side of both branches the is profitable both of those together make up my sample space so i take the given information and i add those pieces together 0.63 plus 0.02 and that gives me 0.63 at a point 65 which gives me a probability that it was predicted to be profitable given it actually was of 0.9692 and that is how bayes theorem helps us reverse the given probabilities in our table let's do one more of these bayes theorems with a little bit more detail in it let's say bob drives to work forty percent of the time he takes the bus fifty percent of the time he walks ten percent of the time so that's 100 of the time how he gets to work the probability he will be on time he is 90 if he drives 60 if he takes the bus and 30 percent if he walks if bob arrives late that's our given information bob has arrived late what is the probability he took the bus well let's make our tree diagram to summarize what happens he actually has three things that can happen initially he can drive he can take the bus or he can walk in fact we're given probabilities that he drives forty percent of the time takes the bus fifty percent of the time and walks ten percent of the time once he does that he will be either late or on time doesn't matter which method he takes he will be either late or on time he will be either late or on time we're told about the on time half if he walks or night if he drives sorry if he drives going down the drive he will be on time ninety percent of the time or point nine which means he's late the rest of the time ten percent the bus we're told he is on time with the bus sixty percent of the time which means he's late the rest of the time forty percent but if he walks he's on time thirty percent of the time and he's late the rest or seventy percent of the time multiplying down the chain then driving in light is .04 driving in on time is point 36 bus and light is .20 bus and on time is 0.30 walking in late is 0.07 walking and on time is 0.03 we are asked to find the probability that he took the bus given he arrived late i like to start off by marking all of the given information so given he arrived late late was our left branch late late late what we're looking for is the probability that he took the bus and was late so what we're looking for is the 0.20 now we're ready to build our probability the probability of both bus and late is that point 20. but divided by the probability of the given information the given information is that it was late we marked that as a 0.04 plus a point 20 plus a 0.07 so we have point 20 out of point 31 which when we divide those you get .6452 almost a 65 chance that bob took the bus given he's arriving late that's bayes theorem and that's how we can use tree diagrams to help us with conditional probabilities so take a look at a few of these on the homework we will discuss these more in class and i'll look forward to working with you then now that we're comfortable working with and finding probabilities we're ready to talk about discrete probability distributions and then later we'll talk about continuous probability distributions first i want to make sure we and we know what question we're trying to answer the question for today is what are discrete probability distributions and to set this up we're going to be working with this idea of finding what's called the probability distribution function often this is just abbreviated as pdf the probability distribution function and these probability distribution functions have two key characteristics the first is for this pdf each individual probability is between 0 and 1. in addition if we took all the probabilities and added them together they would sum to one now often we'll organize these probability distribution functions by what we call random variables and a random variable just describes the outcomes of an experiment experiment now a discrete probability distribution is used with countable or discrete outcomes for example if we let the random variable x represent the number of movies watched last week that's a discreet random variable because you only can watch a certain number of movies we can't do decimals you either watched it or you didn't so because this is a countable result it's a discrete result and we can collect discrete data a survey is conducted and it is found that forty percent of the respondents watched two movies last week 50 watched one movie and the rest watched no movies we can summarize the pdf or the probability distribution function in a table and in this table all we have to do is list the possible results i suppose countable wrong forgot the t in countable sorry about that all we have to do is list the possible results and their associated probabilities so x is the random variable people watched either 0 1 or 2 videos and then we'll make a column representing the probability that x occurred we're told that forty percent watched two movies so the probability is point four fifty percent watched one movie the probability is 0.5 we're not told what percent watched no movies but if we add what we have together we see we've covered 90 percent of the respondents so there's only 10 percent left to make it equal 100 percent and that must be the zero and this becomes our probably distribution table now what's nice about being able to organize all the countable possibilities is it allows us to consider the expected value which is also called the mean and the standard deviation of the probability distribution function or of the pdf and the formulas for finding these expected values and standard deviations are very similar to the formulas we had for means and standard deviations from chapter one but using the probabilities instead of the frequencies first the expected value the expected value is the long term average or mean of the pdf in other words if you ran lots and lots of experiments and took the average of the results what would that expected value or that average result b and the formula for the expected value or the mean is equal to the sum of all the x's times their individual probabilities this is a good formula to know so for our example where we had our number of movies being 0 1 and 2 and their individual probabilities being 0.1 0.5 and 0.4 we can find the average number of movies watched in this survey by making an extra column for x times p of x multiplying the x value times its probability zero times point one is zero one times point five is point five and two times point four is point eight and if i add those together that will give me the sum of the x's times the p of x's which is 1.3 this tells me that the average student or survey respondent watched 1.3 videos last week the expected value if i took a whole bunch of students over and over again i would expect the average to be 1.3 and again similar to how we found the standard deviation with frequencies we can find the standard deviation or the spread of our random variables using the formula that's sigma the standard deviation is equal to the square root of the sum of the difference between the mean and the value squared times the probability of the values again this would be a good formula to know how to use and basically like we did before with finding standard deviation we're going to make an extended table so again our values for the movies were 0 1 and 2. their individual probabilities were 0.1 0.5 and 0.4 and we're going to start to build this formula by adding columns the first part of the formula wants the difference between the mean and the value so our mean remember we just found out that the mean was equal to 1.3 we did that in a previous section so 1.0 minus 1.3 is negative 1.3 1 minus point three is negative point three and two minus one point three is positive point seven but then our formula says we take that difference and we square it make them all positive 1.3 squared is 1.69 0.3 squared is 0.09 and 0.7 squared is point 49. but then the formula says we need to take that square of the difference and multiply it by the individual probabilities so we'll take that green column times the second black column one point six nine times point one is point one six nine point five times .09 is point o four five and 0.4 times 0.49 is 0.196 and our formula says let's find that sum our formula wants the sum of the difference squared times the probability which is equal to 0.169 plus .045 plus 0.196 is point 41 and we're told the standard deviation is the square root of that value the square root of 0.41 which is equal to 0.6 so now we know our probability distribution function that table that we made has an average expected value of 1.3 and a standard deviation of 0.6403 so that's what we're looking at today is we're taking a look at how to work with these probability distribution functions specifically the discrete ones and then in our next few videos we'll look at some specific types of discrete probability distributions but for now take a look at practicing a few of these on the homework we'll discuss them further in class good luck this video is going to take a look at a special type of discrete distribution called the binomial distribution and the binomial distribution helps us calculate what is the probability of x successes out of in trials in other words i'm going to conduct a survey and maybe i want to see how many people support a particular political candidate i want to know what's the probability that 100 out of 200 of them or 50 percent support my candidate that is what we call a binomial distribution and a binomial distribution has the following characteristics first there is always a fixed number of trials i'm going to interview a certain number of people or i'm going to run a test a certain number of times and when i run that experiment or that survey there are really only two options the two options are success and we usually use x to represent the number of successes or failure and it's important to note as we define success and failure in our experiment there is no moral or good bad judgment to the word success and quite often success is a bad thing if i'm in quality control i might say a success is a defective part and we're looking at how many successes there are in that case success is a bad thing so there's no moral or ethical standard for success success is just what we are looking for so try to avoid pinning positive and negative emotions to that word with those two options of success and failure we'll often talk about p which is equal to the probability of a success and if x is the number of successes and we'll use n for the number of trials x divided by n would be the probability of that success then we also have this letter q which is the probability of failure the opposite and since there's only two options success and failure we can quickly calculate q to be equal to 1 minus the probability of a success it's the complement so the distribution itself the distribution for the binomial is when we're going to say that x tilde or little squiggly line b for binomial and then in parentheses we'll do n the number of trials and p the probability of success and we'll use this notation to represent how the x is distributed it's distributed as a binomial within trials and a p probability of success and if we are in the context of the binomial we have some shortcuts to help us calculate things like the expected value the expected value or the mean of the distribution is simply going to be the number of trials times the probability of success and that seems to make sense if i have 30 trials and a 10 percent chance of success i would expect to be successful 10 percent of 30 or 3 times we can also use a shortcut formula for the standard deviation of the binomial and the standard deviation or sigma is equal to the square root of n p q now we could go through the formulas for mean and standard deviation like we did with just the generic discrete probability distributions it just gives us the same answer so we might as well use this nice little shortcut and actually before we move on let's go ahead and highlight these three pieces because these three pieces are foundational to doing our binomial distribution let's look at using the binomial distribution and you notice i never gave the formula for how we actually calculate binomial probabilities that's because we're going to cheat and we're going to use the calculator to do all the work for us in the calculator the steps that you're going to push on the ti 83 or 84 calculator is you'll hit the second button and then you're going to hit the what's called the distribution button which is above the button that actually says vars so when you hit the second it gives you the command above the button the distribution function is what we want to use and we're going to use the binomial distribution so then we will scroll down and there's two options for the binomial the first option is called the binomial p d f and it opens a parenthesis that one gives us the probability of exactly x successes the one below it we'll also use is called the binomial cdf and the c in there stands for a cumulative distribution function that tells us the probability of up to and including x successes in other words with the cdf if i'm interested in the number three if i want three successes the pdf will tell me exactly three successes the cdf will tell me what's the total probability of three two one or zero successes all the probabilities up to and including that number now after the parentheses we do have to enter in the key information some calculators have some software that it'll prompt you for the information but if it doesn't prompt you you just need to know that the format for both of these is exactly the same we will use the binomial and i'm going to do a start because it could be the pdf or the c df both of them are the same then the first number you enter will be the number of trials comma the next will be the probability as a decimal comma then you'll do x or the number of successes and close the parentheses and that's how we can use the binomial distribution on the calculator and it's probably easiest to see with an example and according to the website citydata.com in moses lake seven point nine percent of workers carpool to work you're going to go out and you're going to conduct a sample of 41 workers first thing we want to do is identify what is the distribution for this situation the distribution of our x or our random variable is going to be randomly distributed as a binomial so we'll do a b because we're looking at the number of successes out of these 41 trials the first number is the number of trials 41 trials the second number is the probability of success which is 0.079 we do need to change that percentage into a decimal so of our 41 workers how many would you expect to carpool to work well when we're looking at how many would we expect we're talking about the expected value or the mean the mean we said is equal to the number times the probability or the 41 people you surveyed times the probability of 0.079 that gives us an expected value of 3.23 workers so maybe you'd round that down to three workers you'd expect about three maybe four out of your 41 workers to carpool to work let's scroll up we'll come back to the calculator strokes in a minute what is the standard deviation of our population well we have our formula for the standard deviation it's the square root of npq so we'll take the square root of n the number of trials is 41. p the probability is 0.079 times q well q is the probability of a failure or the probability of someone not carpooling to work well if 7.9 percent carpool to work we can do 1 minus 0.079 to get q is equal to 0.921 so q is 0.921 and when we multiply and take the square root we'll get a standard deviation of 1.727 but we still haven't calculated any probabilities so let's do two or three of these let's say what is the probability exactly 3 of 41 carpool to work in other words what's the probability that our x our number of successes is exactly equal to three well because i'm looking for a specific exact number that is going to be the pdf on our calculator so on our calculator we're going to do the binomial pdf and then we do the number of trials the probability of .079 comma the number of successes we want which is three let's go to the calculator and do that on my calculator we said the way we got the binomial pdf is we hit the second button and then above vars you see the word distribution now the binomial pdf is near the bottom so you can scroll down a bunch or if you scroll up once it'll take you to the bottom and you see options a and b are the binomial pdf and the binomial cdf i'll hit enter on the pdf now mine gives me the prompts so i just have to enter in the number of trials 41 p the probability of .079 the x value the number of successes i want which is 3 and then i can select paste and what that'll do is it'll automatically type in the 41 comma 0.079 comma 3 for me if you don't have those prompts you can just type those numbers in with commas in between them the commas right above the number seven and when i hit enter it's going to give me the probability that i get exactly three successes the probability is point two three zero four and we will always round probabilities to four decimal places point two three zero four what is the probability that in my survey less than the expected value carpool to work in other words we want the probability that x is less than the expected value which we found in part b was 3.293 well this is a discrete distribution you can't have .293 people saying yes they commute to work so what we're really saying is the probability that x is less than or equal to the number three and it's important to identify what it's equal to less than or equal to not just less than because in the cdf when we do the cumulative distribution we need to know what number to start at so now we're going to do the binomial cdf cumulative distribution which is going to add from 0 all the way up to 3. starting with 41 is the sample size 0.079 is the probability and we're going all the way up to 3. we'll hit second distribution to the bottom but this time selecting the binomial cdf i have 41 trials 0.079 is my probability and my x value is 3 and when i hit paste it's going to put those numbers in for me again if you don't have the prompts just put those numbers in separated by a comma and when i hit enter it's going to tell me the sum total of all the probabilities of 0 1 2 or 3 there is a 59.17 or a 0.5917 probability that if i ask 41 people i will get three or fewer commuters sorry carpoolers as they commute to work let's do one last example as we wrap this video up we want to know what is the probability more than four carpool so now i'm asking for the probability that x is greater than four the problem is the cdf counts down so this would be a lot of work to do to find them individually the probability of 5 plus the probability of 6 plus the probability of 7 all the way up to the probability of 41 that is a lot of work instead what we're going to do is we're going to use the complement we're going to find out the probability that we're not talking about and subtract that probability from one one minus the probability that x is now less than or equal to and we have to decide what number we're less than or equal to to count everything that's not included with the greater than four the blue greater than four does not include the number four so in the complement we do want to include the number four if our probability statement had equality if it said or equal to four then we would have to do the complement which is the opposite and to not include four and so we'd start at three so you really have to be careful to decide what you're going to include so now on my calculator we're going to do 1 minus we want less than or equal to 4 that's the binomial cdf cumulative distribution function so it goes up 2 with the 41.079 probability we're going to go up to and include the number four one minus second distribution up to the binomial cdf we still have 41 trials we still have 0.079 but now we want the x value to be 4. paste and when i hit enter 1 minus the binomial cdf gives me a probability of 0.2205 so there's a 22 percent chance that we would have more than 4 out of 41 workers carpooling to work that's the binomial distribution the calculations we'll have the calculator do for us but we do need to know how to set them up how to find the mean and the standard deviation and interpret what pieces are talking about what parts of the binomial so take a look at a few on your assignment come to class ready to discuss it further and we'll continue to investigate the binomial distribution a second type of distribution that we can work with is called the poisson distribution it's named after a mathematician named poisson but the question that the poisson distribution attempts to ask is very similar to the binomial the binomial one to know the probability of x successes in n trials poisson also wants to know what is the probability of x successes but instead of in a certain number of trials it wants to know in a certain amount of time in other words we'll say some event happens on average three times an hour what's the probability this hour it's going to happen five times and to answer that question we use the poisson distribution and some characteristics of the poisson distribution is that we are interested in some number of successes in a fixed interval usually those intervals are a certain amount of time a day a week a month a year but it can be any fixed interval like an editor might be interested in the number of errors an author makes per page and so a page is the fixed interval that functions as the time interval but usually we're talking in the context of time and we also need to know or we will have the average number of successes in that interval so the editor knows that an author makes on average three errors per page the distribution itself we will notate the distribution very similarly to how we did the binomial we'll use the x to say that's our random variable the tilde to tell us it's distributed but this time because it's a poisson distribution we'll use p to represent poisson and the only variable we need for the poisson distribution is the mean or the average number of successes in the interval as you might expect the expected value is the mean that's given to us that's no surprise but the standard deviation turns out to just be the square root of that average and so these formulas can help us shortcut that process as we attempt to actually use the poisson distribution and just like we did with the binomial distribution when we're using the poisson distribution we will actually have the calculator find the probabilities for us we just need to know how to use the calculator to get the information we want very similar to the binomial we'll hit the second button and then you're going to hit the vars button because above the vars we remember it says distribution and then we need to select the correct distribution as there are several in the calculator again near the bottom are going to be the two we'll use one is called poisson pdf and just like the binomial that gives us the probability of exactly x successes and then the other one is the poisson cdf opening a parentheses parenthesis that's the one that gives us the probability of up to and including x successes and again similar to the binomial the format we will use if you don't have the prompts on your calculator will be the poisson either c or p they both work the same df and then the first number you'll enter in is actually the average or the mean over that time interval the second number is the x value that you're interested in finding so let's see if we can use the poisson distribution in an example let's say a certain hospital baby unit has an average of four births per week and we're going to randomly choose a week a week is randomly chosen first things first let's describe the distribution the distribution is a poisson because we're talking about a time period an interval a span of time one week and in that one week there is an average of four births in that week that is the distribution now we can find the expected value and standard deviation of the number of births the hospital has per week well the expected value that's just mu or the average which is given to us the expected value is 4. the standard deviation or sigma is the square root of the average and the square root of four is two so average of four births with a standard deviation of two births is what we would expect to happen on any given week in the baby unit of the hospital so let's find some probabilities and see how likely various things are to occur i'm always interested in how often the average occurs so what is the probability exactly four babies are born this week what we're asking is what is the probability that our random variable x is exactly equal to 4. now because i want my probability to be exactly equal to 4 what i'm interested in finding is the pdf for the poisson exactly equal to 4 in that time interval so we will do the poisson pdf probability distribution function the first number is the average of 4 the second number says we want exactly 4 to occur so let's see what the calculator says when we pull it up very similar to the binomial we'll hit second and distribute distributions scroll down to the bottom and below the binomial you'll see the poisson distributions the poisson's pdf and the calculator if it gives you props will ask for lambda that's the greek letter lambda is the average so for lambda the average we say is four the x value we're interested in is four and when we paste you'll see it puts it in the calculator for me if you don't have the prompts again you can just type in four comma four and close the parentheses the comma's above the seven and we see the probability that we get exactly four even though that's the average the probability that actually occurs is only about 19 and a half percent point one nine five four when we round so that might beg the question what is the probability of fewer than four births this week the average of four only has a less than twenty percent probability what's the probability that x is fewer or smaller than four well the calculator can only do exactly equal to a number and down so first let's identify that we're really looking for the probability that x is less than or equal to not four because four is not included but less than or equal to three because we want to be less than or equal to three counting all the possibilities up to three zero plus one plus two plus three that's going to be our cdf or our cumulative distribution that adds up all the values up to that point the poisson cdf which has an average of 4 but we want to know up to the number 3. so we go to our calculator and hit second distribution down to the bottom you see poisson cdf the mean is four we want to go up to three and when we paste types the numbers in or you can type them in manually if you don't have the prompts the probability that we get less than four is point four three three five so there's a 43 percent chance we're actually less than the average number of births let's say the hospital can only handle about one baby a day one birth a day it's a small hospital a small baby unit so we want to know what is the probability seven or more births occur this week what is the probability that x is greater than or equal to because it includes seven it says seven or more bursts this week now the thing about the poisson distribution is there's not actually any maximum in theory there could be one thousand births this week at this hospital it could go up that high now if the average is four the probability is basically zero but it could potentially happen so when we're doing greater than seven this is actually impossible to calculate straightforward because we'd have to do seven plus 8 plus 9 plus 10 plus 11 and we have to go all the way to infinity we can't do that so just like we did with the binomial when we want greater than we're going to actually work with the complement we're going to do 1 minus the probability that x is less than or equal to the opposite set of numbers now if we're going from 7 up this time it includes the number seven so when we count down we don't want to include seven so we're going to start at six counting down so in the calculator we'll type in one minus the poisson cdf cumulative it's going to add those all up the average is 4 and we want to go up to 6 and then we'll subtract all of that off of 1 to get the complement so we'll type in 1 minus second distribution up to poisson cdf the average is still 4 but this time we're going up to 6. and when i paste it in and we do 1 minus that value we get a probability of 0.1107 0.1107 and so if they're not ready for uh seven or more births this week that's going to happen 11 percent of the time they might need to expand their baby unit in the hospital so that's the poisson distribution very very similar to the binomial the difference is now we're talking about the number of successes in a fixed interval usually a time interval so take a look at the homework assignment to practice a few of these and then when you come to class we'll continue working with poisson and that distribution we've spent several days discussing how to find probabilities of discrete distributions where the results are countable but probabilities with continuous distributions become a little bit more interesting that's going to be the question we look at is how do we find probabilities of continuous distributions because if it's continuous we've got to consider every possibility between 1 and 2 are an infinite number of decimals and we can't add the individual probabilities so instead we're going to steal a concept from calculus that says that the probability is simply the same thing as the area under a curve a really simple example of what i mean by this to help us visualize is probability distribution functions or density functions can be expressed as an equation maybe f of x equals one half of x that is a probability distribution function probability with results that are possible from 0 to 2. here's what i mean by that if we were to graph f of x equals one half of x we would know that has a y intercept of zero and a slope of rise 1 run 2 and so this line going from 0 to 2 represents the probability density distribution function or the density function and the probabilities are just going to be the area underneath that line remember that if we take the the probability of all of our possibilities they have to add up to 1. well if i were to calculate the area of this triangle as it turns out to be we know areas for triangle is one half times the base times the height well the base is two wide and the height is one high so one half times two times one equals 1.00 the total area of this triangle is 1.0 just like the total probability of all the possible outcomes is 1.00 so now we could find the probability that maybe our random variable x is less than one what we're really looking for is ones here in the middle if i were to shade the area less than one on this triangle we end up with this smaller triangle on the left side here well the probability that it's less than one is the area under the curve that is less than one which is still a triangle so the is the triangle area one half times the base well the base is one long times the height and if i were to draw this to scale you'd see the height was exactly 0.05 and so one half times 1 times .05 we'd end up with an area of 0.25 meaning there's a 25 percent probability that i'm between 0 and 1 on this triangle now this f of x equals one half x probability density function i made it up it's fake it doesn't model anything but it does show us kind of how this idea works that probability is the area under the curve the curve can be any shape as long as the total area under it is one because that's the total probability that is equal to one so rather than dealing with a fake triangle let's deal with a real probability density function one that we do use is called the uniform distribution and the uniform distribution is a distribution where all outcomes are equally likely i could get any number with basically equal probability the distribution itself has a special notation just like the poisson and the binomial did when we have a uniform distribution we will say x tilde where x is uniformly distributed u between a and b where a is the low number and b is the high number so if we're going anywhere from 5 to 10 with equal probability the numbers a and b would be 5 and 10. now the curve that we want to be underneath for a uniform is f of x equals the reciprocal of the difference or one over b minus a and when we do that what we'll end up with is a rectangle that goes from a to b where they're all equally likely to occur and that height on that line is how likely it's to occur that one over b minus a if i were to find the area of this rectangle because it's a rectangle area is base times height so the area is the base the distance between the two we have to subtract them is b minus a and the height that's the green height that we just calculated is one over b minus a and those b minus a's divide out so the total area is 1.0 so the curve the height of the rectangle for a uniform distribution is the reciprocal of the difference 1 over b minus a now just like the discrete distributions we can find the mean or the expected value and the formula for that with the uniform is just a plus b divided by 2 or the average of the extremes that's going to stick us right in the middle to find the mean turns out the standard deviation is equal to the square root of b minus a squared divided by 12. and it's always divided by 12 regardless of the numbers just works out that way so let's try an example of this and see if we can see this uniform distribution work out for example a plumber estimates that service calls are uniformly distributed between half an hour or 0.5 hours and eight hours first let's describe the distribution we said our random variable x was going to be distributed uniformly the smallest number possible is 0.5 the largest number possible is eight so our distribution is x tilde u point five eight we can easily then calculate the mean or expected value and standard deviation using the formulas from the uniform distribution the mean is the average 0.5 plus 8 divided by 2 which is 8.5 divided by 2 which is 4.25 hours so this plumber's average service call is about four and a quarter hours four hours fifteen minutes the standard deviation on those service calls is the square root of b minus a eight minus point five squared divided by 12. when we put that into our calculator we should end up with a standard deviation of 2.165 hours so now that we know his average call and or service call and and standard deviation let's actually calculate some probabilities and what you'll find with calculating probabilities on continuous distributions it's always easier to draw a picture of the situation so if we want to find the probability a call takes less than three hours we're going to draw a picture of the probability it's a uniform distribution so we know it's a rectangle from a low of 0.5 all the way up to 8. the height is that f of x equals one over b minus a or one over eight minus point five one over seven point five and we can leave that decimal in there that's okay now it's asking for us to find the probability that we're actually less than three remember probability is area so if i mark on my graph approximately where three is and we want to be less than 3 it's going to be the area of this rectangle off to the left so the probability that x is less than 3 is the area of the rectangle base times height the base is the distance from three to point five or three minus point five and the height is what we just found out the f of x the 1 over 7.5 and when i put this into my calculator 3 minus 0.5 in parentheses times 1 over 7.5 we end up with point three three three three or about a onethird probability that the service call takes less than three hours let's try another example let's find the probability a call takes between two and four hours again probabilities are always easier if you draw a picture so here's our uniform probability it doesn't have to be to scale but it does show the lowest point five the highest point eight the height is still one over seven point five but now i want to be between two and four hours so between two and four hours we're going to shade that area in between we're looking for the probability that two is less than x which is less than four which is just this rectangle the rectangle is base times height the base is the space from two to 4 or 4 minus 2 times the height which is 1 over 7.5 putting that in my calculator 4 minus 2 is 2 divided by 7.5 we get an area of 0.2667 when we round so there's about a 26 and twothirds percent probability that a call will take between two and four hours we can even find conditional probabilities in much the same way let's find the probability a call takes more than five hours given it was less than seven hours again we're going to draw a picture going from 0.5 to 8. with a height of 1 over 7.5 but now i want to be more than 5 given it was less than seven so we don't really care about this right side we just want to be less than seven and we want to know what's the probability that i'm more than five given that i'm less than 7. the probability that x is more than 5 given x is less than 7. well with a given probability we know we look at the probability of both divided by the probability of the given information so the probability of both would be between 5 and 7. so between 5 and 7 has a base of 7 minus 5 times a height of 1 over 7.5 that's the both and then we divide by the probability of the given information the given information is that we're less than seven so that probability's got a base going all the way down of seven minus 0.5 times a height of 1 over 7.5 which is kind of nice as often occurs with given probabilities part of it will divide out and so we're left with 7 minus five is two over seven minus point five is six point five and two divided by six point five is point three zero seven seven so there's just over a thirty percent probability the call took more than five hours given we knew it was less than seven hours another concept that we haven't spent much time with is the idea of what's called a percentile a percentile is the is the value where a certain percent is below the value you often see this with standardized test scores if you took a test and you scored in the 80th percentile that means your score was better than 80 percent of the participants so we could find for our example uh let's find the 80th percentile or what value has 80 percent below it let's draw a picture same picture it's a uniform distribution so it's just a big rectangle the height is still 1 over 7.5 and we're going from 0.5 to 8. but we want to know what value let's call it k for now will give us an area that's point eighty below it eighty percent is below it what value gives us that well to get there we're going to use our area formula the fact that we know that area of a rectangle is base times height the difference is this time we know the area we know the area is .80 the base is the distance from k down to 0.5 so we'll have k minus 0.5 is the base and the height we know is 1 over 7.5 all we really need to do now is solve this equation 4k and that solution will be our 80th percentile the value that has 80 percent of service calls below it first we can get rid of the fraction by multiplying both sides of the equation by 7.5 that's going to give us 6 equals k minus 0.5 add 0.5 to both sides and k is equal to 6.5 this means that 80 of service calls are less then 6.5 hours that's what it means to be the 80th percentile now as we wrap up there's one more example i want to show you it's kind of a special case and it seems counter intuitive we want to find what is the probability a call takes exactly five hours and the key word here is exactly because that means something very specific in probability it doesn't mean between five hours and five hours zero minutes and one second it means at the exact moment of five hours let's draw a picture to represent the exact moment of five hours going from point five to eight and a height of one over seven point five the exact moment of five hours occurs somewhere in the middle well the probability that x equals exactly five is going to be whatever the base is times the height the problem is the base of that rectangle since it's just a line right at 5 hours with no width the base goes from 5 to 5 with a height of one over seven point five but five minus five is zero and anything times zero is zero the probability that anything happens at exactly a specific moment in a continuous distribution is always equal to zero nothing ever happens at an exact moment that always happens over a span of time it could be between five hours and five hours in one second but that's a span of time at exactly a specific number that will never occur in a continuous distribution seems like a paradox but it's the way it works out we're focusing though today on finding probabilities off the uniform distribution and this idea with continuous distributions that probability is area under the curve so take a look at the assignment to practice a few of these and we will see you in class to continue working with the uniform distribution now that we've gotten familiar with continuous probability distributions we're going to move on to take a look at the most important continuous distribution in all of statistics and that is the normal distribution so the question today is simply what is the normal distribution the normal distribution is a probability density function that's got this beautiful equation of 1 over the standard deviation times the square root of 2 pi times e to the exponent of negative onehalf times x minus the mean divided by the standard deviation squared now what's nice about the normal distribution is you do not need to know that formula instead we're going to cheat and use a table to help us actually calculate what that formula is going to be equal to at various points but what you should know about the normal distribution is the shape of the normal distribution just like the uniform distribution is a clear shape of the rectangle the normal distribution has a clear bell shape to the curve the normal distribution if this is my xaxis is a bellshaped curve where the mean of the distribution falls right in the middle and we describe the distribution as x tilde in for normal and then we'll just state the mean and the standard deviation for the two arguments of the function now the normal distribution has slightly different shapes based on the standard deviation the smaller the standard deviation the taller and skinnier it is the larger the standard deviation the shorter and fatter it is but there's one special distribution which we call the standard normal distribution and when we're dealing with the standard normal we won't use x we'll use z so we know we're talking about the standard normal distribution which always has a mean of 0 and a standard deviation of one and what's nice about the standard normal distribution it has a table to help us find areas this is what the table looks like the table gives us values for z going down the first column you see the first two digits maybe 1.2 and then if i wanted 1.23 i'd go to the 3 on the next and where those 2 overlapped at the 0.303907 i can get my area under the curve based on those z values and we'll look more at using the table here in just a minute but what we need to know for now is there's a table to help us find areas and what we need to do quite often is we change between a regular normal curve which uses x values and the standard normal curve which uses z values so that we can use the table in order to find the areas the way we make that change is we use one of two equations we either use z equals the x value minus the mean divided by the standard deviation or if we solve for x in that same equation we end up with the mean plus the z value times the standard deviation and with these two definitions or these two formulas it's important that we keep track of what's an x and what's a z x has meaning in context x might be the height of the average person and so x we're looking at a 64 inch person z does not have context or meaning z is simply the number of standard deviations we are from the mean and once we have a z value then we're able to go to the standard normal to find areas off the table the table gives the area between a z value and the mean and of course the mean is zero in other words if i've got this standard normal curve here the mean is always in the middle of zero off to the side we've got a z value the table gives the area between that z value and the mean of zero so if we want to find probabilities let's scroll up a bit and give us a little bit more room if we want to find probabilities off this table we'll have to decide what pieces we're interested in first thing that we'll use is the fact that the curve is symmetric in other words the z table does not have any negative values on it fortunately the negative values behave like the positive values because the curve is symmetric each half the left and right side has an area of 0.5 and we use this one less often but the total area is 1.0 because it's a probability so let's see if we can figure out how to use this information and use our table in order to calculate probabilities under the standard normal curve let's do some examples for this example if you look up according to google the average act score is 20.8 with a standard deviation of 4.8 let's do some examples off of this information first let's describe the distribution for the distribution our variable x is normally distributed the mean is 20.8 and the standard deviation is 4.8 let's find out the probability a student scores higher than 30. now with all of these probability problems it will always be easier to draw a picture first so we'll draw a picture of our normal curve our little bellshaped curve we'll put the mean right in the middle the mean is an x value right now so i'm going to label my first row we're going to put x values on it and then in the second row we'll label what those equivalent z values are when we change them from x's disease that way we don't get them mixed up so the mean for our x value is 20.8 we want a student to score higher than 30 30 is off to the right and we want the area that's higher than 30 because the area is the probability well let's change those x values into z's we know the mean changes to zero but the 30 we need to use our formula for z z says take the x value subtract the mean and divide by the standard deviation we have an x value of 30 minus 20.8 divided by the standard deviation of 4.8 we end up with a z value of 1.92 so 30 changes into a z value of 1.92 we will always round our z values to two decimal digits because that's going to match our standard normal table so on our table we need to find 1.92 let's scroll down a bit to help us see it a little better 1.9 going down the column and then we want 1.92 and so when we combine those we end up with them overlapping at .4726 that .4726 is the area between my z value and the mean it's the white area kind of in the center we don't want the white area in the center we want the tail off to the right of it and this is where we use what we know about the normal distribution we know the entire right side is 0.5 so the probability that some student scores higher than 30 is going to be equal to the 0.5 minus that white area that's been cut out of 0.4726 which leaves us with .0274 there's just shy of a three percent chance that a random student will have scored higher than 30. let's try a few more of these so we can get really good at finding probabilities on this important normal distribution round number three let's find the probability a student scores less than 25. again we'll draw a picture the mean right in the middle of 20.8 that's an x value we want to be less than 25 which is somewhere over here to the right and we want to be less than so we want the smaller part or the left side all of that area in order to do that we need to calculate our z values we already know the mean will have a z value of zero what we don't know is the z value of the 25 so we'll subtract the mean of 20.8 we'll divide by the standard deviation of 4.8 and we get point 88 so now we have a z value of point 88 that corresponds with the x value of 25 0.88 is what we're going to look up in our table in the table we're looking for 0.88 so 0.88 when we go down and across we find out the probability there is 0.3106 0.3106 and that again is the area 0.3106 the area between the mean and that z value of 0.88 but this time we also want not just that little bit but the whole area to the side that left side we know is 0.5 so when we want the probability that x is less than 25 this time we need an extra 0.5 added to the 0.3106 that we just found to get 0.8106 or just over 81 percent of students score less than a 25 on the act so you can see how the picture helps in the difference between example two and example three example two we had to subtract from point five to get the area that we wanted example three we had to add to point five to get the area we wanted let's try another example to keep working on how the picture is going to help us calculate our probability let's find the probability a student scores between 15 and 23 on their a ct so we draw a picture the mean in the middle has an x value of 20.8 we want to be between 15 and 23. 15 is off to the left and 23 is off to the right we want the area between these two numbers so now when we convert to a z value we don't have to just change the mean to zero and one other value we have two other values that we need to convert to z values so let's do that for the 15 we take 15 minus the mean of 20.8 divided by the standard deviation of 4.8 that's going to give us a negative number negative 1.21 but that's okay because it's to the left of zero it makes sense that a number to the left of zero on the number line is negative that's what a negative z value is it just means we're to the left or smaller than the mean when we do the 23 we should get a positive number 23 divided by 0.8 divided by 4.8 and it's going to be positive because it's bigger than the mean this turns out to be 0.46 a z value of 0.46 we're going to look up both of these values in the normal table first looking up the 1.21 it's negative but that's okay because the curve is symmetrical so we'll just look up the positive version and it's going to have the same area on the other side 1.21 1.21 so when we do that we end up with this center area of 0.3869 0.3869 is the area between the mean and that z value 0.3869 we still need to look up the z value of point forty six so i'll do it in blue here point forty six when i go across we end up with an area of point seventeen seventy two so the area there is point seventeen seventytwo we want the area between those two numbers which includes both halves so when i want to find the probability that 15 is less than our score which is less than 23 we need to combine both those pieces together 0.3869 plus 0.1772 will give us a total area of 0.564 one there's just over a fifty six percent chance that a student will score between fifteen and twenty three so sometimes you see we have to add pieces together but that's not always the case either let's look at this example let's find the probability a student just one student scores between 18 and 20. now if we draw this picture with our mean in the center of 20.8 that's an x value but now you notice 15 18 and 20 18 and 20 are both smaller than our mean and we want the area between them of course to get that area we have to change them to a z value the mean has a z value of zero but we have to work to get the other two points so first for the 18 18 minus the mean of 20.8 divided by the standard deviation of 4.8 that's going to be equal to negative point 58 so the z value there is negative point 58. for the second one the 20 z is equal to 20 minus 20.8 divided by 4.8 is equal to negative point seventeen negative point seventeen and we want the area between those going to our normal distribution our first value was negative point five eight point five eight so we'll go over and down and we find our first area is point two one nine zero point two one nine zero so this first one point two one nine zero now it's important to note that's not just the shaded region that goes all the way to the main it doesn't stop at the 20. the 2190 goes all the way to the main does not stop at the 20. we still need to find the other piece which is the negative point seventeen doing this one in blue the negative point one seven going down and across we find an area of 0.675 so that's area of 0.6750675 sorry forgot the 0.0675 and now we're ready to answer the question what is the probability that 18 is less than our score which is less than 20. well the 20.90 goes all the way to the mean but we don't want the white space the 0.675 that goes on the right side of the main so we're going to cut out that white space we just need to subtract we have 0.2190 minus 0.0675 that's going to be 0.1515 just over a 15 probability a student will score between 18 and 20. let's do one last example but let's make this one a little different this time we're going to find the third quartile of act scores remember the third quartile that's the value that's over 75 percent what we're really asking to find is the 75th percentile this is different than we were doing before we're not saying what's the probability of this number we have the probability we have the probability 0.75 we're looking for the x and the z values that give us 0.75 so let's draw our picture we've got our mean of 20.8 that's an x when it's a z the mean is zero and we're looking for some value out here we'll call it k some value out there where the area below is a total of 75 percent well the table is only going to give us the space between k and the mean and actually let's label k down below we're going to make k a z value we'll use that k to find the x value we know the left side of the curve is 50 or 0.5 so the right side of the curve must be the remaining 0.25 of an area but we know the area of 0.25 when we're given the area do not go down the z values z's are not areas z's are a scale of the number of standard deviations we are from the mean we need to look inside the body of the table for the area that we're given we want 0.25 and if we kind of scan through our numbers you'll see 0.25 an area of 0.25 is right in between these two numbers between 24 86 and 25 17. now if it was closer to one i'd go with the one that's closer but it's really right in the middle so we'll call right in the middle 0.6 right in between 7 8 so we're going to call that 0.675 0.675 so that k value is 0.675 how do we find the x value then we have that other formula we haven't used yet that says x is equal to the mean plus the number of standard deviations times the standard deviation and that z is that z value we just found of 0.675 so if we plug in what we know the score x that's the 75th percentile or the third quartile is the mean of 20.8 plus 0.675 times the standard deviation of 4.8 the mean is 24 i'm sorry not the mean but the x value 24.04 the third quartile of act scores is about 24. that means a score of 24 is better than about 75 percent of all of the scores on the act the normal distribution is truly the most important distribution you know how to use in all of probability you need to know how to use the table how to find the left side the right side what's in the tail what's in the middle how do you find percentiles how do you use the table backwards you need to be very comfortable and familiar with the normal distribution and how it works as we move forward in our study of probability so take a look at the homework assignment practice a few of these important problems in class we're going to keep working with the normal distribution and i will look forward to seeing you then now that we've gotten really comfortable at working with the normal distribution and finding probabilities we're ready to actually get into working with samples now the heart of statistics is collecting a sample to make an estimate or a conclusion about a population so first we need to know how do we find probabilities with sample means the big difference here is in the previous chapter in chapter 2 we were finding the probability for one individual value now we're going to take several values find the mean and look at the probability of the mean of several values and this is what gives rise to what is called the central limit theorem and the central limit theorem basically in words says that the mean of a sample should be close to the mean of a population and not only that it should have a smaller standard deviation the idea is that if we have several values averaged together the extreme values are going to be averaged out and pulled back in towards the center which makes the standard deviation smaller in fact we can go one step further and say that the larger the sample the closer to the mean we become and the smaller the standard deviation is and that makes sense if i interview nearly everybody i will be probably pretty close to the actual mean i'm not going to be off by much which is why the standard deviation is going to be so small and as we're working with samples this idea of the standard deviation or the smaller standard deviation we call the standard error the standard error will use the symbol of either sigma sub x bar or you'll often see s sub e for standard error and that's the standard deviation of the sample means and the way we calculate the standard error or the standard deviation of the sample means is we will take the standard deviation of the entire population and we'll divide by the square root of the sample size or if it's a sample we'll say s for the sample standard deviation divided by the square root of the sample size and that's a key equation that we're going to use quite a bit today using this new standard error we can replace the standard deviation in our distribution of the mean when we're talking about means we're going to say that means are normally distributed with the same mean as the whole population but then we will use the standard error or s divided by the square root of n to represent our new standard deviation then we can go forward and calculate zscores and also probabilities in much the same way we did before now the zscore is equal to the mean of our sample minus the overall mean divided by the standard error and that's going to be the key new thing the central limit theorem gives us is that new standard error as we calculate our z values because we have a sample not just one value so let's look at an example where a sample is done a cell phone company finds that those who go over their data limit go over by an average of 2.2 gigabytes with a standard deviation of 0.4 gigabytes you conduct a survey of 80 customers first thing we want to know is we want to know what's the probability the average overage is above 2.3 gigabytes or what's the probability that x is greater than 2.3 actually x bar is greater than 2.3 that the average is more than 2.3 well the first thing we're going to need to do here is we're going to figure out what is the distribution of the mean the mean should has the same mean as the population 2.2 gigabytes but the standard deviation is smaller we take the 0.4 gigabytes and we have to divide by the square root of the sample size divide by the square root of 80. so we have 2.2 comma 0.4 divided by the square root of 80 is about point zero four five that .045 that is our new standard error so when we calculate our zscore we remember that z is x bar minus mu divided by the standard error our x bar we want to be greater than 2.3 so we'll take 2.3 we'll subtract the average of the population divided by our standard error because we have a sample not an individual of 0.045 and when we divide we get 2.22 so if we think about our normal distribution the mean of the populations at 2.2 we these are x values we want to be at 2.3 or bigger so we standardized into z values and the z value actually turned out to be 2.22 so that's what we're going to look at in our standard normal table in our standard normal table we've got 2.2 and another 2. so we see the probability there is 0.4868 but remember that is always the area between the z value and the mean we want the area in the tail so the probability that the mean is greater than 2.3 we know the entire right side is 0.5 subtract off the middle of 0.4868 and we get .0132 there's about a one and a third percent probability that if i interview 80 customers i'll get a mean bigger than 2.3 and that's the idea of the central limit theorem we're shrinking the standard deviation by dividing it by the square root of the sample size whenever we have a sample we need to divide by the square root of the sample size let's keep with this example one more i also want to see if we can find q1 or the 25th percentile so same problem with the cell phones where we've got these x values the mean x value is 2.2 gigs we want to find the x value where 25 percent or 0.25 is in that first tail well the table is going to give us the other half because the table always goes between our percentile or our z value and the mean so the table is going to be 0.5 minus a quarter or 0.5 minus 0.25 which is also 0.25 so we're going to look up the z that corresponds with an area of 0.25 notice we're talking about an area we do not know the z value so when we go to the table we want we're looking for an area of 0.25 we're looking inside the body of the table and .25 happens somewhere in the middle here between 0.67 and 0.68 so we're going to call that .675 the z value is 0.675 but notice it's to the left of 0 to the left of the mean so it actually has to be a negative .675 because it's to the left of zero it's smaller than zero we still need to convert that z value which has no context into a x value that does have context and very similar to how we did it back with the normal distribution our x bar is going to be equal to the mean plus the z value times the standard deviation which in this case is the standard error so x bar is equal to our mean of 2.2 minus a 0.675 because it's negative times the standard error which we calculated the standard error to be .045 0.045 and 2.2 minus 0.675 times .045 gives us a mean of 2.17 putting units on it gigabytes the 25th percentile of means of sample of size 80 is going to be 2.17 gigabytes now it's time for you to take a look at the homework assignment to try and do some problems using the central limit theorem where we have this new standard error the new standard error is triggered because we have a sample not just one individual data value see if you can work with a few and in class we will discuss them further and practice this whole central limit theorem a little bit more now that we've talked about this idea that samples adjust the standard deviation to become what the standard error is we're ready to do some inferential statistics inferential statistics take a look at the idea of how can the sample help us make an estimate or a conclusion about the entire population and as you might guess a sample and the population will have similar but different statistics or parameters and so the question we're going to start this discussion off with is how close is a sample proportion to a population proportion in other words if i take a sample of a hundred people and ask what pers what number of them or what percent of them eat breakfast in the morning i'll end up with a percentage of people in my sample who eat breakfast in the morning that's a sample statistic and i can use that to try and estimate what percent of the entire population or all people eat breakfast that is the population parameter however the two numbers aren't going to be exactly the same there will be some type of error involved in that proportion so that's what we're going to look at today how what is that error and how can we calculate it first i want to make sure we really understand this concept of proportion when we're talking about proportions what we're really saying is that the underlying distribution is binomial and if you remember from our probability unit binomial takes a look at x successes out of n trials that's what we're looking at so if we were asking about the breakfast example we're looking at how many people eat breakfast the successes out of how many people total we're interviewing and then from that success and trial concept we can calculate the population proportion or i'm sorry the sample proportion which we represent as p hat with the little triangle over the p we calculate that by taking the successes divided by the trials to get some type of percentage proportion or decimal and it turns out that proportions are normally distributed with a mean that's equal to the proportion and a standard error that's the square root of the proportion times the probability of failure divided by the sample size and again that q hat q is failure the opposite of successes so it's one minus the proportions this underlying distribution will allow us to estimate where the actual population parameter lies to do that we will find what's called a confidence interval a confidence interval is an interval maybe between 20 and 30 percent that's an interval based on the sample statistic where the population perimeter is likely to be located so it's going to be this range of numbers where we are quite confident the actual population parameter lies and the idea behind it is that our sample statistic is likely not perfect it's likely off by some error and so what we'll do to calculate this interval is we'll say okay let's take the sample proportion and we'll subtract the error and then we'll take the sample proportion and add the error and the population parameter is likely somewhere between those two numbers so we take our sample statistic and add and subtract the error and that should give us a range of numbers where the population proportion actually lies but how big is that error well we don't really know what we have to do is we have to say we're going to be comfortable with some level of confidence or some level of air that's allowed to occur and if we're okay with being wrong five percent of the time we'll make what's called a ninetyfive percent confidence interval if we're okay being wrong ten percent of the time we'll make what's called a 90 confidence interval so the confidence kind of tells us how often were we want to be correct and accepts a certain amount of error because we can never be 100 confident unless we interview everybody so we've got this idea of a confidence level and that's going to be the probability the interval contains the population parameter will have some type of confidence level let's say for example i want to have a 95 percent confidence level that means i want to be right 95 percent of the time but i could be wrong we have this alpha which is a greek letter the greek letter alpha is the probability we are wrong and so if we want to be 95 confident alpha is going to be 1.0 minus the 0.95 alpha is going to be 0.05 a 5 probability that we are wrong visually on the normal curve what that means is if my sample proportion comes in in the middle of the normal curve we're going to put a range the proportion minus the air and the proportion plus the error and we want to be somewhere in the middle we're claiming the population proportion is somewhere in that range so in that range that's my confidence level the 95 percent which means out in the tails is where i could be wrong if it actually falls out there well if there's five percent in the tails and there's two tails we could have 2.5 percent splitting it in half in each tail our goal is going to be to figure out what that error amount is that we have to add and subtract in order to get 2.5 percent in each tail that's what we're doing so with proportions to calculate the error we have this nice formula that the error is equal to what we'll call z sub alpha over 2 times the square root of p hat q hat over n this is a formula that we should be very comfortable using p hat q hat and n we should be familiar with because those all come from our sample we've already talked about those before this z sub alpha over 2 value that is the z value that gives the correct area in each tail so for this example up above where i wanted a 95 percent confidence interval that would be the z value that gave me 2.5 percent in each tail and we can look that up in the table backwards or we can consider some common z sub alpha over two values because really most confidence intervals come in one of three types we have confidence levels of either 90 percent 95 percent or 99 and the z sub alpha over two that goes with each of them with the ninety percent to get ten percent in the tails five percent in each tail we'll use one point six four five for the ninety five percent confidence interval like the example up above we'll use 1.960 and for a 99 confidence interval it turns out the z sub alpha over 2 is 2.576 you do not need to memorize these numbers but you should have them handy as you're doing your assignments and this and the practices and labs for our class okay i think we're in need of an example so that we can see this work out so we can see how we find out how big the error is between our sample statistic and population proportion and once we know the error how do we find a confidence interval that contains or likely contains the actual population parameter do an example a survey is done and 95 out of 174 voters support a particular candidate for senate the first thing we're going to do is we are going to construct a 90 percent confidence interval for the true proportion of voters who support the candidate can this candidate be 90 confident that that she or he has a majority of the voter support well first we need to know what is the proportion p hat that we're dealing with we have to calculate this the proportion is our number of successes out of the number of trials 95 out of 174 voters support the candidate that comes out to a proportion of 0.546 or 54 percent of the voters in the survey support this candidate looks pretty good q hat the probability of failure is always 1 minus the proportion so in this case 1 minus 0.546 which comes out to be 0.454 we also need a z sub alpha over 2 or in this case alpha is the probability of failure point 10 over 2 which gives us z of 0.5 because we want 10 in the tails oops not 0.5.05 5 in each tail and so we would need to find the z value that puts 5 area in each tail we can look that up on our big z table or that is one of the common ones that we have from our chart up above so we will use z sub 0.05 is equal to 1.645 those are the three pieces that we will need in order to build our confidence interval the error between our sample proportion and the population proportion is equal to the z sub alpha over two times the square root of p hat q hat over n z sub alpha over two is one point six four five times the square root of p hat which is 0.546 times q at which is 0.454 divided by n the sample size of 174 and when you do this on your calculator for some reason it's really common students forget to multiply by the 1.645 they just do the square root make sure you do the whole thing and you should end up with an error of point zero six let's round it to point zero six two this is how much my sample might be off at 90 confident we're 90 confident our sample might be off by about six percent so we will take the sample proportion and subtract the error to get the lower bound of the worst case scenario for this candidate and we'll do our proportion plus the air to get our upper bound to get our best case scenario for this candidate our proportion was 0.546 minus 0.062 the air and 0.546 plus 0.062 the air and when we subtract we end up with 0.484 and when we add we end up with 0.608 and this range is where the population parameter or the population proportion likely lies in between let's make a better way of saying that though let's look at how we interpret a confidence interval interpreting a confidence interval is almost as important as how we calculate it because our statistics don't mean anything unless we can put it in context of the situation so what we will say to interpret a confidence interval and this becomes a nice script for interpreting any confidence interval we do in this class is we will say we estimate with some percentage that would be your confidence level whatever your confidence level is with some percentage confidence that the true let's actually say that the true population whatever we're talking about we're going to put the parameter in context so we're talking about the population mean the population proportion the population standard deviation whatever we're talking about but then we'll put it in context of the situation we're describing is between blank and blank and those would be as you might expect the low number and the high number so for our proportion we're going to interpret the confidence interval from the example above and you can still see it at the top of your screen there in purple the confidence interval is 0.484 0.608 but what that means in context is that we can estimate notice how i follow the script here we estimate with and this one was a 90 percent confidence interval so 90 percent confidence that the true population and now i'm going to describe the parameter in context we're doing proportions here what proportions support this candidate for senate but the true population proportion who support the candidate for senate notice that puts it completely in context so we know what the problem was discussing is between and the low number let's go ahead and make it a percent 48.4 percent and 60.8 percent and that's how we will interpret that confidence interval we're 90 percent confident the true population proportion who support the candidate for senate is between 48.4 and 60.8 percent so it's not a guarantee this candidate is going to get a majority you might say it's pretty likely but we don't know where in between these numbers the actual proportions going to lie we just know it's going to be between those numbers or at least we're 90 percent confident it's between those numbers so you should be able to today build a confidence interval using the formulas for proportions that we talked about today and then just as important you should be able to interpret that confidence interval using the script we've provided here so you can go ahead and take a look at a couple of those and try them we'll look forward to discussing confidence intervals more in class and continuing to work with them in inferential statistics we'll see you in class a significant part of statistics is testing a claim to see if we can really believe it's true so that's going to be our question for the day is how do we test a claim and today we're going to specifically focus on a claim for a proportion the process we're going to talk about though does work for all sorts of claims but specifically today we're going to stay in the context of a proportion the way we test claims in statistics is what is called hypothesis testing and the idea behind hypothesis testing is it's this clear process we can do to test if a claim is true what we'll do is we'll first set up two hypotheses and they're going to be contradictory hypothesis either the first one or the second one is true the first one we'll call h sub zero that is what we will call the null hypothesis and it will always include it will always use equals some variable equals something and another thing about the null hypothesis is we will always assume the null hypothesis is true until proven otherwise if the null hypothesis is not true the other hypothesis is h sub a which we call the alternate hypothesis and this is often what we're trying to show to disprove a claim and this will either use greater than less than or not equal to kind of the alternative choice if it's not equal to a number it must be different than it and then once we've set up those two hypotheses we will run a sample or an experiment and then based on that experiment we will calculate the probability the null hypothesis hypothesis is true based on our sample this calculation that the null hypothesis is true based on our sample is what we will call the p value it's going to be very important to us the pvalue what is the probability the null hypothesis is true based on our sample once we know that probability we will compare it to the allimportant alpha alpha actually no period we'll just say or the smallest probability that we will still believe the null hypothesis is true so if our probability our pvalue is smaller than alpha that is too small of a probability to still believe the null hypothesis and so we will have to reject the null hypothesis in favor of the alternative because our pvalue was too small it was smaller than alpha the smallest probability we still believe the null hypothesis is true or the pvalue might be bigger than alpha there is a greater probability that it is actual the null hypothesis is true and then we won't reject the null hypothesis so step five is simply either two maybe a little more space we will either reject the null hypothesis and we do that if the pvalue is less than alpha because the probability was too small to still believe the null hypothesis or we will fail to reject the null hypothesis and that's the case where the pvalue is greater than alpha or it's just too big of a probability to believe that the null hypothesis is false a great example to kind of show how hypothesis testing works is to consider a trial in the united states we assume in a trial that a person is innocent until proven guilty and it's actually a perfect statistical hypothesis test let's say person a is accused of a crime the null hypothesis that everyone assumes is true until proven otherwise is that person a is innocent the alternative hypothesis what we try and prove or find enough evidence is that the person is guilty and what we always do is we assume innocent until proven that proof is the pvalue what is the probability that they're innocent given there's all this proof that they are guilty and not just proof that they're guilty because you never know for sure they're guilty we just prove they're guilty beyond a reasonable doubt and that reasonable doubt that is the alpha if you go beyond alpha if p value gets smaller than alpha the proof of innocent is so small we can no longer assume they're innocent and there's actually two conclusions that we can make if proven guilty if the pvalue is so small that they're innocent the probability of their innocent is so small it's beyond a reasonable doubt we reject the null hypothesis and conclude the defendant is guilty if not at least not beyond a reasonable doubt we will fail to reject the null hypothesis and conclude and this is where it gets interesting and it's very important in statistics we don't conclude they're innocent we conclude that they are not guilty the conclusion focuses on the alternative hypothesis what's key there is that we never conclude the null hypothesis h sub 0 is true we just failed to conclude the alternative we didn't say they were innocent we just said there is not enough evidence to say they're guilty and that's an important conclusion that applies to statistical conclusions as well we will never conclude the null hypothesis is true we will always conclude that the alternative hypothesis could not be proven or could be proven speaking of conclusions let's talk about how we want to make our conclusions similar to how we interpreted a confidence interval for a proportion when we make a conclusion it's really important we make that conclusion in context but we also are going to focus on the alternative hypothesis the h sub a and so a nice script we can follow is we will say there is or there is not depending on the context we will say not if we fail to reject because we did not get the alternative hypothesis like we wanted so there there is or there is not sufficient evidence to conclude whatever we can conclude the conclusion though is always going to be the alternative or the alternate hypothesis in context to the problem let's do two examples where we can really see what this hypothesis testing thing looks like example first for doing a hypothesis test specifically with proportions everything we've done so far actually applies to all hypothesis testing but specifically with proportions there's a few formulas we need for proportions first off we know that proportions are normally distributed with the proportion acting as the mean and the square root of p q over n acting as the standard error but as you calculate these values different than a confidence interval because a confidence interval focused on the sample and what we could learn from the sample we used p and q from the sample here we're focusing on a null and an alternate hypothesis so we're going to focus on the claim that the null hypothesis is true use the null hypothesis values and then to calculate our p value or our probabilities we will have z is equal to p hat the sample proportion minus p the hypothesized proportion divided by the standard error and remember the standard error is the square root of pq over n so let's try this let's say a phone company claims that 43 of smartphone users have an iphone but you doubt this claim so you conduct a survey of 83 smartphone users 44 of them use an iphone what can you conclude if alpha equals 0.05 in other words we're going to believe the claim of 43 until the probability dips below 5 percent that that claim is actually true well let's set this up our null hypothesis has to be that our proportion equals something and that's the claim that the proportion equals point 43 for the alternative hypothesis we can either say the proportion is greater than less than or not equal to there's no direction given in your doubt when you doubt the 43 is accurate you're not saying that it's greater or less than you just doubt that it's accurate so this is going to actually be not equal to 0.43 and when it's not equal to 0.43 we have what's called a 2 tailed test and what that means is we could reject the null hypothesis if the proportion is bigger or if the proportion is smaller either direction maybe it'd be easier to see if we drew a picture and we're going to annotate this picture as we go on here's the normal curve for the proportion the claim is that the mean the proportion is 0.43 but we doubt that's true we think it's either going to be lower somewhere in the red tail on the left or higher somewhere in the red tail on the right we don't know which side we just doubt it it's in both tails left and right the distribution then of the proportion just to review we know that the proportion of our sample will be normally distributed and again we're going to use the null hypothesis here around the claim of 0.43 with a standard error of 0.43 that's our p times q 1 minus 0.43 is 0.57 over my sample size and here we did a survey of 83 people so what that really means is our proportion is normally distributed with a mean of 0.43 and a standard error of 0.0543 so what is our sample proportion well our sample proportion is going to be x divided by n or the 44 out of 83 people who use the iphones and that's going to be point 53. run out of colors so we'll go back to blue actually one thing to put in since we know our proportions point 53 on my picture off to the right i'm going to put .53 that is the x value where the red shading starts we don't know the value on the left if our proportion had ended up being less we would have put the number on the left but because our proportion was more we put it on the right now we're ready to calculate our z value and z is our sample proportion minus the hypothesized proportion divided by the standard error so point 53 the sample minus the hypothesized point 43 divided by the standard error of 0.0543 the z value there is 1.84 so if we have x's on top we'll stick z's down below i should have left a little more space when it's a z we assume the mean is zero and our value on the table is 1.84 so let's go to the table and see what area goes with a z value of 1.84 on the table we're looking at one point eight four so if i scroll over and if i draw my line straight we see 1.84 corresponds to an area of 0.4671 but remember with that area of 0.4671 that is the area in white point four six seven one we are interested in the area in the tail to get the tail we have to subtract from point 0.5 that'll give us 0.0329 but what's important to know is because this is a twotailed test we have to consider the other tail as well is .0329 as well it's symmetrical which is nice so when we want to calculate the pvalue the pvalue is the total shaded area because this is a twotailed test we have to add them together the .0329 plus .0329 that gives us a pvalue of 0.01 and remember that p value is the probability our null hypothesis is true based on our survey in other words based on our survey there is a change it to a percentage 6.58 percent chance the proportion of iphone users is actually 43 percent what the null hypothesis claims now at first glance you might say that's not a very large percent but remember we did say up at the top here that alpha is 0.05 and that means we will continue to believe the null hypothesis is true as long as the probability does not dip below 5 so if the pvalue is six percent that's not below the five percent threshold we say that still is not quite enough evidence to kick out the null hypothesis so our decision is we will fail to reject the null hypothesis because there's just not quite enough evidence it was close but not quite enough evidence to reject the null hypothesis the reason for that really clearly stated is the pvalue the probability the null hypothesis is true is greater than that alpha that minimum threshold putting the numbers in there the pvalue is 0.0658 that is greater than the alpha of 0.05 so we failed to reject and we're ready to make our conclusion following our script then we will say that there is not because we failed to reject we'll say not sufficient evidence to conclude and the conclusion must be in context of the alternative hypothesis so we're going to state the alternative hypothesis that the proportion's not 43 percent of course we must put it in context so to conclude the proportion of iphone users is different than 43 now one thing you might notice is there's a couple of p's going on in here in problems like these and it's very important we keep them all straight we have a p value that's just the probability the null hypothesis is true we have a p in the null hypothesis that is the claim for the population proportion and we also have a p hat that is the sample proportion be very careful not to get the three p's mixed up quite often we'll see students compare the wrong p to alpha and they'll make the wrong conclusion as a result make sure you compare the p value to alpha to make a conclusion about your p based on your phat sounds confusing but practice a few to make sure you get them straight just to kind of make this interesting and this isn't always required but it often is with a hypothesis test is let's make a 95 confidence interval for the true proportion based on our sample so based on our sample p hat our sample we said was 53 percent which means q hat the opposite of that is going to be one minus that or 47 percent 0.47 and if we're doing a 95 percent confidence interval we should know the z sub alpha over 2 or z sub 0.05 over 2 or z sub 0.025 is equal to 1.96 and we found out in our previous lesson that the error is equal to that z value of 1.96 times the square root of pq over n but with the confidence interval notice i will use the sample data this is different than the hypothesis test where we use the hypothesis with the confidence interval we use the sample data of 0.53 times 0.47 divided by the sample size which was 83 and that will give me an error of 0.107 so my confidence interval then is the proportion .53 minus the error of 0.107 and the sample proportion of 0.53 plus the error of 0.107 giving me a confidence interval for the true population proportion to be between 0.423 and 0.637 or in context we're saying that we are 95 percent confident the true population proportion of iphone users always put it in context is between 42.3 percent and 63.7 percent and what you notice is with that confidence interval our null hypothesis said what we were assuming to be true the null hypothesis said the proportion was equal to 0.43 notice that 43 is within that confidence interval that's why we cannot reject it because it still is a valid possibility for the true population proportion of iphone users i want to do one more example i know this video is running a little bit longer than normal but it's really important that we're comfortable with these hypothesis testing so it has been claimed that 58.4 percent of web users prefer chrome however you believe the number is lower so you're going to test it you sample 152 web users and 74 of them use chrome with alpha equal to 0.01 this time we want to be very confident we're going to go all the way down to one percent error what can you conclude well like before let's start with our hypotheses the null hypothesis is that the proportion equals what they claim it equals the claim is that it equals 0.584 the alternative hypothesis is based on what you're trying to show and this time we're going to try and show that the actual number is lower that the proportion is less than 0.58 which means this time we really have a onetailed test or better said a lefttailed test meaning we're going to reject the null hypothesis if we end up far out into the left tail drawing a picture the hypothesized mean is at .584 we're going to reject if it's less than significantly to the left or in that left tail so we have our distribution we know that the proportion is normally distributed at 0.584 with the standard error of using the null hypothesis 0.584 times q which is 0.419 oops 416. sorry divided by the sample size of 152 which means it's normally distributed at 0.584 comma 0.0400 when we round if that's the distribution then we're going to compare it to the proportion or the phat that we get from our sample our sample said 74 out of 152 use chrome 74 out of 152 is 0.488 that's the value off to the left it's less than 0.487 where the shading starts is that far enough away that we can make a conclusion that it's actually less than well to do that we will go to our z value which is equal to our sample proportion of 0.487 minus the hypothesized proportion of 0.584 divided by the standard error of 0.0400 our z value is negative 2.43 so we've got our x's on our picture putting those all into z's the mean is 0 and the z value of negative 2.43 is where the shading starts let's go to our z table and i'm going to scroll down a bit to see 2.43 so 2.43 looks like this time we're going to have a z value of 0.49 i'm sorry an area of 0.49 that's the area in between of 0.4925 we need the area in the tail which is just 0.5 minus that or 0.0075 this time we don't need to worry about the other side because it is a onetailed test so my pvalue is just that shaded area the 0.0075 which means based on our survey there is a 0.75 chance the null hypothesis is true or the proportion of chrome users or people who prefer chrome probably would have been a better way to say that is 58.4 percent that is really really small pvalue in fact what's really important is when we compare that pvalue to our alpha of 0.01 it is actually less than that alpha of 0.01 which means that's too much evidence to the contrary so we will make a decision to reject the null hypothesis and the reason for that decision is because the pvalue the probability it's true is less than the alpha or specifically 0.0700 is less than the 0.01 minimum threshold so our conclusion if we reject the null hypothesis we will say that there is sufficient evidence to conclude and then we will state the alternative hypothesis in context focusing on being less than the 0.8.584 to conclude the proportion of web users who prefer chrome is less then 58.4 percent one last little thing as we wrap up our conversation on hypothesis testing is when all is said and done we make a conclusion based on a pvalue what's the probability that it's the null hypothesis is true do we reject it do we fail to reject it but when all is said and done at the end of the day in statistics we could be wrong and we really have no way of knowing if we're right or wrong we can be fairly confident 95 or 99 confidence but we could be wrong and there's two types of errors and statistics that we always try and minimize we call them type 1 and type 2 errors a type 1 error is when we reject the null hypothesis when it is true we should not have rejected it but we did the probability of that happening is actually the alpha that we're using in the problem the other type of error is what's called a type ii error and that's where we fail to reject the null hypothesis when we should have when it is false and the probability is not as evident for that we call that probability beta it comes up in more advanced statistics classes we're not going to spend a lot of time on that but you should be aware at this point of what a type 1 n type 2 error is in context so we did two examples today the first example was about iphones and we failed to reject the null hypothesis we could have committed a type 2 air where we conclude not reject or fail to reject when we should have or to put it in context and this is probably a better way to say it or we believe the proportion of iphone users is 43 percent because we failed to reject it but we should have when it is not it is not actually 43 we should have rejected that's a type 2 error the second example because we did reject could have been a type 1 error where we conclude reject when we should not have or to put that in context we currently after running that sample we believe the proportion who prefer chrome is less than 58.4 because that's what we concluded but it's not that would be a type 1 error where it's actually either equal to or possibly greater than 58.4 percent and we made the wrong conclusion those errors hopefully don't happen often to us but there's always a chance that they could happen because with hypothesis testing we're never sure of anything we could be wrong and that's what the type 1 and type 2 errors tell us is what does it mean if we're wrong so we covered a lot of stuff in this video today we introduced the concept of what a hypothesis test is and how hypothesis test works and then we did several hypothesis test examples in the context of proportions and then really briefly at the end we talked about we could be wrong the type 1 and type 2 error so take a look at the assignment if you want to try a few of these a little bit of a longer video but some of the next few are going to be much shorter to make up for it so we'll see you in class now that we've taken a look at how hypothesis testing works specifically in the context of one proportion we can extend it to hypothesis testing in many different situations and one of those situations involves having two proportions or two groups and trying to decide are the two groups the same or are the two groups different so our question for the day is how do we test a claim about two proportions we're going to have two groups and we want to know are these two groups the same is one group bigger is one group smaller what can we conclude about these two proportions and the idea of the hypothesis test is identical to the idea that we did with one proportion the only difference is we have a few different equations needed to find the test statistic first thing we need to know is what we're going to call the pooled proportion the pooled proportion is what the proportion would be if the groups weren't separate so if we weren't comparing men and women but we were just looking at people how many successes are there out of the total group the pooled proportion we represent with p sub c the pooled proportion is equal to the sum of the successes from the first group and the sum of the successes from the second group divided by the number in the first group plus the number in the second group and as a tip if you do this on your calculator you will probably need to put parentheses around the numerator and denominator to make sure that division happens in the correct order because a proportion should always be between 0 and 1. if your proportion's not between 0 and 1 check the order of operations so this is our first important formula finding the pooled proportion in order to test the two proportions if there was no separation of the groups the second formula you need to know is the distribution for two proportions when we compare two proportions we don't just compare them what we actually do is we take the first proportion of our sample and subtract the second proportion of the sample we're actually looking at the distribution of the difference between the proportions and they are normally distributed as you might expect and if they are the same we should have a difference of zero the standard error formula is a little bit more involved but not difficult it's the pooled proportion times one minus the pooled proportion which might be the pooled q times one over the first sample size plus one over the second sample size and so that is the second key thing we need specifically because it is going to help us find the standard error that big square root is the standard error that we need to calculate the test statistic with the normal distribution the test statistic is z and the test statistic here is going to be the difference in the proportions p hat a minus p hat b divided by the standard error so that's the third new equation we need other than that everything is identical to what we've seen before so we should be able to jump right in to an example where we will compare two proportions let's say a restaurant wants to know if teens are more likely to order dessert then adults knowing this information can help them plan their future marketing campaign so they contact a sample 84 adults 33 order dessert they contact a sample of 91 teens 46 order dessert can they conclude teens are more likely to order dessert if our alpha level equals point 10. notice this example is different than when we had one single mean when we had one single mean there was a global claim that said the proportion is equal to such and such a percent here we don't have any such global claim here we're doing a sample of two separate groups and then we're going through and comparing are these two separate groups different or are they the same so setting up our null hypothesis the null hypothesis is always going to have equality so the null hypothesis is that the proportion of the teens who order dessert is equal to the proportion of adults who order dessert notice i use a subscript on each of the p so i know which one is which t for teens and a for adults that way when i set up my alternative hypothesis we want to know are the teens more likely to order dessert they want to know if the proportion of teens is greater than the proportion of the adults what we have actually is a right tailed test because we are going to reject in the right tail drawing our little picture the mean the assumed difference the hypothesized difference is that there is zero difference between them the order of my hypothesis tells me the order of the subtraction we always subtract left to right so what we're really doing is the proportion of teens minus the proportion of the adults we're claiming the teens are bigger than the adults so we're taking a big number minus a small number it should give us a positive number and something in that right tail that's we're going to attempt to find let's uh calculate some of the pieces that we're going to need in order to solve this first the proportion of teens there are 46 out of 91 teens that order dessert that's going to be 0.505 the proportion of adults who order dessert that's going to be 33 out of 84 or 0.393 and then for our pooled proportion if there was only one group not two separate groups and we just interviewed people the number of successes would have been 46 plus 33 over the number of trials would be 91 plus 84 making sure i use parentheses so i don't get in trouble with order of operations the pooled proportion is 0.451 we can use that information let's not scroll too far keep that hypothesis on there we can use that information to find the distribution we know the difference in the proportion between the teens and the adults is normally distributed with a mean of 0 and a standard error equal to that big square root the pooled proportion of 0.451 times 1 minus the 0.451 times 1 over the first sample size of 91 plus 1 over the second sample size of 84 or normally distributed with a mean of 0 and a standard error if you put that in your calculator of 0.0752 now that we know the standard error in the distribution we can calculate the z value the test statistic let's label it test statistic the difference in the proportions we're going to do the same order as the hypothesis so we have to do the teens minus the adults 0.505 minus 0.393 over the standard error of 0.0752 now just so i can label it on my picture let's do the subtraction in the numerator 0.505 minus 0.393 the actual difference is 0.112 divided by the standard error of 0.0752 so on my picture the actual difference is 0.112 and when we divide by 0.0752 we get a z value of 1.49 so if we've got x's in the top rows these in the bottom row z's also have a mean of zero but now the z value is 1.49 that 1.49 is what we want to look up in the table to find our pvalue looking at our table then we have 1.49 so 1.49 that's going to give us an area of 0.4319 so our area of 0.4319 but the pvalue is the area in the tail so we subtract from 0.5 to get 0.0681 so our pvalue is .0681 which means the probability or given our sample i should say given our sample the probability the proportion of teens and adults who order dessert is the same is 6.81 percent probability they both order dessert at the same rate is 6.81 percent that's what the p value means we said we wanted our alpha to be point 10 which means we will believe the null hypothesis we will believe the proportions are equal as long as the probability is bigger than ten percent we got a probability of six percent so our decision is to reject the null hypothesis and the reason for that decision is our pvalue is less than our alpha there's a six point eightyone percent chance or uh let's not do it as a percent a point zero six eight one pvalue which is less than the alpha of point ten so we reject the null hypothesis and we're ready to make our big conclusion the script for the conclusion remains exactly the same because we rejected the null hypothesis got the alternative we were looking for we will say there is sufficient evidence to conclude and then we will go back and state the alternate hypothesis in context that teens are greater than adults the proportion of teens who order dessert is greater than the proportion of adults who order dessert the conclusion is in context it focuses on the alternative hypothesis you should feel like what we just did was very very similar to the test hypothesis testing for a single mean the only difference that we had to do was we had to find this pooled proportion and a new standard error to calculate our test statistic but the process of the hypothesis test remains the same regardless of what we're testing so take a look at practicing some of these we're going to do some of this in class we'll look forward to seeing you then just as we have done inferential statistics with proportions we can do many of the same things with means in fact we more often are working with means than proportions so let's answer the question first how do we find a confidence interval for a mean very similar in idea to find a confidence interval for a proportion but there's one key difference with the means is normally when we do a sample and we have a mean of the sample we do not know the standard deviation of the population we only know the standard deviation of the sample which means the normal distribution will be a little bit too tight or a little bit too small to calculate a reliable confidence interval because we're only estimating the standard deviation of the population with the standard deviation of the sample so if we have no standard deviation of the population we can no longer use the normal distribution we need a different distribution and the distribution we will use is called the student's t distribution or often you'll hear it just called the t distribution the student's t distribution is very similar to the normal distribution but it allows for greater flexibility as we will use the sample standard deviation to estimate the population standard deviation and that's never perfect it's probably close but it's never perfect and that's why we need that extra flexibility that the student t distribution gives us is it allows us to still make a confidence interval with a little bit extra flexibility and it turns out that and it makes sense as well the larger the sample size the less flexibility is needed and that makes sense as if we interview more and more people getting closer and closer to the population our estimate for the standard deviation is probably going to be more and more accurate and the more accurate we are the less flexibility we need it turns out that we have to adjust the student t distribution then based on the sample size and we call that estimation the degrees of freedom the degrees of freedom is often abbreviated df for degrees of freedom and it's very easily calculated as n minus 1. the degrees of freedom is n minus 1 one less than the sample size and it turns out that if the sample size is greater than 30 the student t is almost identical to the normal distribution and so when we have a sample size bigger than 30 we end up using normal values because they're so close together but if the num sample size is less than or equal to 30 then we will use the student t table for finding critical values this table shows us the amount of area that we're going to get in a single tail of the t distribution you'll notice the shape looks very similar but our degrees of freedom are going to determine the critical values that we need to calculate so what we'll do is we'll first find out the degrees of freedom of our problem maybe we've got 11 degrees of freedom then we'll figure out how much area we want in one tail maybe we want one percent in one tail the table would then give us the critical value that we can use to calculate a confidence interval one more thing you'll notice is the very last row of the table is labeled z because that's when we pass a sample size of 30 or 30 degrees of freedom and at that point the t distribution starts to look like the normal distribution and you should recognize several of the numbers in this row as the critical values we used with proportions those are the z values that give us the area we want in each tail so once we're past 30 degrees of freedom we'll just use those normal values well now that we're kind of familiar with this idea of the student t distribution that we have to use if we don't know the population standard deviation let's talk about how we can use that to make a confidence interval for means first off the distribution for the mean if we don't know the standard deviation we'll just say is t with the subscript that is the degrees of freedom and remember the degrees of freedom is equal to one less than the sample size so that's the distribution we're working with the t distribution so we need to calculate an error that exists between the population and the sample mean that error actually we'll do a colon that error is equal to our t sub alpha over 2 very similar to our z sub alpha over 2 but this time we'll use the t table and the correct number of degrees of freedom times the standard deviation of the sample divided by the square root of the sample size and once we know the error we can find the confidence interval and very similar to proportions with the confidence interval we will subtract and add the error to our statistic so we'll take our x bar and we'll subtract the error to get our low value and our x bar and we'll add the error to get our high value and that's the confidence interval using the error we just calculated these three pieces will work together to get us our confidence interval so let's try an example you are interested in the average cost of a smartphone so you take a sample of 16 smartphones and find a mean cost of 531 dollars with a standard deviation of eightythree dollars we are going to number one construct a ninety percent confidence interval a couple things we need to know to conduct this 90 confidence interval first our alpha the amount of area in both tails if it's a 90 confidence interval is going to be 0.10 so the alpha over 2 looking at just one tail is half of that or 0.05 for our t distribution we need to know the number of degrees of freedom the degrees of freedom is always one less than the sample size so we've got 16 smartphones minus one we have 15 degrees of freedom and so now we're ready to calculate our t value that is 0.05 in the tail and 15 degrees of freedom going down our degrees of freedom on the table we want 15 degrees of freedom and we want 0.05 area in that tail so we go down and across and we find a t value of 1.753 1.753 now we're ready to calculate the error the error is that t value 1.753 times the standard deviation of my sample size which was 83 dollars divided by the square root of my sample size which is 16. and putting that on my calculator we get an error of 36 dollars and 37 cents so if that's the maximum error between my sample mean and the population mean we just have to subtract and add it to my sample mean the 531 dollars minus the error of 36.37 and the 531 dollars plus the error of 36.37 gives me a 90 confidence interval of 494.63 up to 567.37 and very similar to how we constructed a confidence interval with proportions and then interpreted it we will also interpret the confidence interval for the means following almost the exact same script we estimate with 90 percent confidence the true population and then state the parameter in context mean smartphone cost is between 494 dollars and 63 cents and 567 and 37 cents so as we can see constructing a confidence interval with a mean is very similar to how we constructed a confidence interval with a proportion we've got a different distribution a slightly different formula for the error but the exact same idea so you should be able to try a few of these and we'll talk about confidence intervals a little bit more in class now that we've gotten comfortable with working with the t distribution and making a confidence interval for a mean we're ready to do a hypothesis test for a mean so the question we're going to answer is how do we conduct a hypothesis test for a mean and the process of a hypothesis test is always the same whether we're talking about one proportion or two proportions or in this case one mean the only difference is we have a few different formulas for the mean to help us calculate that important test statistic that will help create the pvalue first off for the mean the distribution because we don't know the population standard deviation the distribution of the mean is a t distribution with a subscript for the degrees of freedom one less than the sample size the way the standard error is calculated the standard error is equal to the standard deviation of the sample divided by the square root of n and then we'll use that standard error to calculate our test statistic and our test statistic is going to be t equal to the difference between the mean of the sample and the hypothesized mean divided by the standard error so these are the three pieces that will help us find the pvalue to conduct our hypothesis test now because the tdistribution has a slightly different number based on the degrees of freedom we need a slightly better way than just looking up a value on a table to calculate the pvalue in our t's the way we're going to do that is we're going to use our calculator first we have to set up the test and the way we set up the test is you're going to hit the stat button and then you will scroll over to tests then you will scroll down to the t test once you set it up you have to enter the stats from your study now the calculator has an option of entering the data or entering the stats we're going to enter the stats so if needed select stats so the calculator knows you're going to actually enter in the stats and the stats you're going to enter in first it'll ask for mu sub 0. that is the null hypothesis it's also going to ask you for x bar which is the sample mean it's also going to ask you for what they call sx which is the sample standard deviation then it'll ask you for n which is the sample size we know that one and finally it'll ask for mu which is the symbol in the alternate hypothesis whether that's less than greater than or a not equal to i'll show you how this process looks on the calculator but it really helps to have an example so let's do that let's build an example it is claimed the average page in a novel has 275 words per page to test this claim you sample 24 pages of a novel and you find the average page has 260 words with a standard deviation of 34 words do you believe the claim is true if alpha equals 0.05 well we start off every hypothesis test with the null hypothesis here we're talking about a population mean and the claim is that the mean is 275 words per page for the alternative hypothesis we're not really saying it's less than or greater than you just want to know is the claim true so what we say is mu is not equal to 275. and because we have that not equal we're really dealing with a two tailed test where we've got our normal distribution the hypothesized mean of 275 is in the middle but it actually turned out to be 260 which is less than it but because this is a twotailed test we're going to shade both sides those are our x values we're going to calculate t values off of the distribution the mean is distributed as a t distribution because we don't know the standard deviation of the population but we do know the degrees of freedom is one less than the sample size so the degrees of freedom is 23. let's go to our calculator then to calculate the t value and the pvalue again that keystroke that we're going to do is first you're going to hit the stat button which is right next to the arrow then we'll scroll over to test then we'll scroll down to the ttest we are going to input the actual statistics not the individual data values so we'll highlight statistics mu sub 0 is the null hypothesis at 275 x bar is the sample mean our sample was 260. sx is the standard deviation of the sample which was 34. and in the sample size was 24. we're going to make sure we highlight the alternate hypothesis symbol not equal to and scroll down to calculate when we do you'll see the calculator gives us several things but what we're interested in most is t and p t the test statistic is negative 2.16 and p the probability the null hypothesis is true given our sample is 0.0413 so let's record that our test statistic is t equals negative 2.16 we can add that to our picture to the left of zero and the p value is point zero four one three and what that p value means then that's the probability the null hypothesis is true so based on our sample the probability the average page in a novel having 275 words that probability is 4.13 percent and remember we said alpha was five percent five percent is the minimum probability where we will still believe the null hypothesis is true four percent is less than that so we can no longer believe the null hypothesis is true so we will make a decision to reject the null hypothesis and the reason for that decision is that the pvalue is less than the alpha the pvalue is 0.0413 which is less than the alpha of 0.05 that is too little probability there is overwhelming evidence to reject the null hypothesis and so we make our conclusion following our script we say that there is sufficient evidence to conclude and then we state the alternative hypothesis in context the alternative hypothesis was just not equal to or different than so there is sufficient evidence to conclude the average number of words per page in a novel is not or maybe we should say is different and then 475 words let's actually take this one step further and build a confidence interval for where we believe the actual mean number of words lies let's build a confidence interval and let's just do a 95 percent confidence interval now we know the distribution that we're dealing with we know the degrees of freedom off of that are one less than the sample size the degrees of freedom we said was 23 because the sample size is 24 pages so our degrees of freedom is 23. alpha the percent of chance that we're going to be wrong is 0.05 the opposite of the 95 percent so alpha over 2 is 0.025 so we're looking for a t sub 0.025 that has 23 degrees of freedom in our table looking at our table then we want 23 degrees of freedom we want 0.025 and a tail and so that tells us that the critical value we're using this time is 2.069 so t sub 0.025 is 2.069 we're ready to calculate the error that might exist between our sample mean and the actual population mean remember the error is t sub 0.025 times the standard deviation divided by the square root of the sample size so 2.069 times the standard deviation we said the standard deviation was 34 words divided by the square root of the sample size which was 24 we end up with an error of about 14.4 words so for our confidence interval we will subtract and add that error to the mean we got in our sample our sample mean was 260. we'll subtract the 14.4 to get a low number we'll add the 14.4 to get a high number and so our confidence interval is 245.6 through 274 and you notice the hypothesized mean of 275 is outside of that confidence interval which is related to why we ended up rejecting the null hypothesis let's go ahead and interpret it we can estimate with 95 percent confidence the population and we're talking about a mean and put it in context number of words per page in a novel is between 245.6 words and 274.4 words and now we have our confidence interval so hypothesis testing with a mean it should feel very similar to hypothesis testing with a proportion because the process of a hypothesis test is identical regardless of what we're studying the means are nice because we can use the calculator to make things a little bit shorter and quicker for us but the philosophy behind the hypothesis test is still exactly the same so try and take a look at a few of those we'll look forward to trying a few of these in class and answer any questions that you might have we'll see you then the next type of hypothesis test we're going to turn our attention to is a hypothesis test for two means as we attempt to answer the question how do we compare two means and just as this is the case with all the other different hypothesis tests the process is exactly the same the only difference is we have some formulas to help us set up our test statistic first the distribution for comparing two means when we're comparing two means we want to know if two separate groups have the same average is there a difference between the two groups or is one group higher or lower than the other group and when we're interested in comparing them what we're really comparing is the difference or the mean of the first group minus the mean of the second group and because we don't know those population standard deviations it's going to be a t distribution with a subscript that represents the degrees of freedom now the degrees of freedom is an ugly formula if you really want to know what it is you can look it up in your book it is in the section in your book preceding the practice assignments that we're doing for today it's ugly we are going to cheat and we will use a calculator we'll also use a calculator to find the t statistic but just so that we have it the standard error for two means is the square root of the first standard deviation squared divided by the first sample size plus the second standard deviation squared divided by the second sample size and then we use that standard error to calculate our test statistic t which is the difference in the means a and b divided by the standard error but as was the with the case with the t distribution with one mean it will be also the case with the t distribution with two means that we will use the calculator to do the hard calculations for us the setup is identical first you will hit the stat button then you will scroll over to tests and then you will scroll down but this time you're going to scroll down to select the two sample ttest once you're in the twosample ttest you will enter all the stats we have from our problem and again if the calculator is expecting you to enter the data we're not going to enter the data so you might need to highlight stats if needed when you get in there the first thing it's going to ask you is for x bar 1 that is the first sample mean then it will ask you for sx1 that is the first sample standard deviation and it will ask you for n1 which is the first sample size so we enter in all the information about the first group that we're going to compare to the second group as you might expect you'll see x bar 2 s x 2 and n 2 is for the second sample that we will compare it with then it will give us a mu which is the alternate hypothesis symbol we need to tell the calculator is this a onetailed test or a twotailed test in which direction it is and finally the last thing the calculator will ask us for is if we have pooled data and for our purposes the answer is always going to be no we are not pulling the data and i'll show you what this looks like on the calculator again but again it's going to be easiest to see it if we have an example to work with so for our example you want to know if there is a difference in gpa of online students and facetoface students so to determine this you survey 32 online students who have an average gpa of 3.45 with a standard deviation of 0.7 you also interview 41 facetoface students who have an average gpa of 3.67 with a standard deviation of 0.4 if alpha equals 0.10 can you conclude the groups are different we're comparing the average between the two groups not just a claim and testing against the claimed gpa we just want to know is there a difference between these two groups we have two means that we're comparing well the mean of the online students is hypothesized then to be the same as the mean of the facetoface students again i'm using subscripts to make it really clear which group i'm talking about the null hypothesis no difference they are equal the alternative hypothesis is going to be either one is greater or they're not equal to each other this example didn't give me any inkling of a direction they didn't say face to face or higher or gpa or online students have a lower gpa or anything like that so we are just looking to see if the online students are not equal to the facetoface students which means we have a two tailed test in other words we have our t distribution with our hypothesized difference we hypothesize that the difference between the gpas is zero and will reject on either tail whether it's higher or lower we will reject on either tail well the actual difference let's add that to our picture the actual difference in gpas and we have to subtract in the same order of the hypothesis so the online gpa has to come first the online gpa was 3.45 and we subtract the facetoface gpa of 3.67 and we end up with .22 negative 0.22 so negative point 22 is the x value we're looking for we need to figure out what the t values are well our distribution the difference between online students and facetoface students is a tdistribution but we don't really know the degrees of freedom because the it's got that ugly formula so we're going to go to our calculator again on the calculator the way we get to the test is we'll go to stat it's right next to the arrows we'll scroll over to test and we'll scroll down to the two sample ttest make sure stats is highlighted because that's what we have our first group the online students we said the online students have an average gpa of 3.45 with a standard deviation of 0.7 and we said that there are 32 of them the second group the facetoface students have an average gpa of 3.66 and a standard deviation of 0.4 and there were 41 of them we select an alternate hypothesis of not equals pooled is going to be no for our purposes and when we hit calculate you'll see the calculator gives us the three key pieces of information we need the degrees of freedom to finish out the distribution notice it's an ugly decimal that's very common with two samples and a t value and a p value let's copy that information over so the distribution had 46.5 degrees of freedom the t value that came off the calculator was negative 1.59 so negative 1.59 to the left of 0 and a pvalue equal to 0.1193 what does that pvalue of 0.1193 mean well remember the pvalue is the probability the null hypothesis is true given our survey so based on our survey the probability the average gpa is the same for online and face to face students is 11.93 percent and we compare that pvalue to the alpha which is the smallest probability where we would still believe the null hypothesis is true we have a greater probability so we're going to go ahead and say our decision because the probability the pvalue is bigger than alpha we will fail to reject and the reason for that is the pvalue is greater than alpha there's more evidence for the null hypothesis the pvalue is 0.11 0.1193 alpha's only 0.10 and so for our conclusion in context because we failed to reject we say there is not sufficient evidence to conclude and then we will state the alternate hypothesis in context that there is a difference between the means there is a difference between the mean gpa of online and face to face students again we should start to feel really familiar and comfortable with this process of going through a hypothesis test it's exactly the same regardless of if we're testing one or two means or proportions the process is exactly the same the only tweak that's different each time is actually calculating the distribution and the test statistic but we should be very good at the process of setting up and conducting the hypothesis test by now so you can take a look at those on the assignment we'll work out with them a little bit more in class and we'll look forward to seeing you then another hypothesis test we can do is hypothesis testing which with what are called matched pairs matched pairs are where we have before and after data and we are looking to see are things the same or was there some type of improvement or maybe we'll pair together couples or and see if there's a difference between you know the husband's score and the wife score or maybe we'll pair together twins to see if there's any difference between the twins or maybe we'll compare your left hand to your right hand but the data is paired together and we're looking to see if there's any type of difference that is matched pairs and so the question we're going to ask is how do we hypothesize test for improvement or maybe be better to say a difference has there been any change and this is that idea of matched pairs all the data comes in pairs and the pairs are matched together usually we have that before or after score and the way matched pairs work is we're going to do a ttest for one mean just like we did a ttest for one mean before the only difference is we will first find the difference for each pair and then we'll use those differences as our one variable to figure out whether or not there is a positive negative or no difference between the before after data so with that in mind we've got some equations that we need to know to run this test the distribution it's very similar to the t test distribution because it is a t test the only difference is we're going to put a little subscript of d on the x bar to represent the average distance is a t distribution with a subscript representing the degrees of freedom where the degrees of freedom is simply the sample size minus one and then we can calculate the standard error and the standard error is simply the standard deviation of the differences divided by the square root of the sample size and we can use that standard error to find our test statistic which very similar to the test statistic for a single mean is t is equal to the difference between the average difference and the hypothesized difference divided by the standard error and the hypothesized difference is usually zero we usually assume there's no difference between before and after scores and we're looking to see is there a difference or is it positive or greater than zero or is it negative or less than zero now like before with the ttest though we're going to save all that work with using our calculator to do a lot of manual crunching of the data for us so first thing we're going to have to do on our calculator is we're going to have to tell the calculator all of our data so here's how you enter the data into your calculator you're going to start by hitting the stat button and then you will select edit to edit a list and the list the edit feature will already be highlighted when you hit stats so you just really have to hit enter and then in l1 you're going to enter the before data or the first set of data and then in l2 you will enter the after data or the second set of data and then for l3 what you're going to do is you're going to scroll up and highlight l3 and do l2 minus l1 and the calculator will automatically subtract and find all the differences and fill list three with the differences now the way we get that is you'll hit the second button and you'll hit the number two which will give you l2 minus the second button and then you'll hit the one which will give you the l1 and now in l3 you will have a list of all of the differences between the before and after a positive difference means it got bigger a negative difference will mean it got smaller then you're ready to actually run the ttest so you can hit stat scroll over to tests and then scroll down to ttest and even though there's two sets of data before and after we're actually just working with the differences so it's just one ttest don't do the twosample ttest it's a single sample ttest that single sample is the difference between the before and after checking to see if the numbers went up or down so when you're running the ttest you have to enter in some information first thing you want to do is you want to highlight data because you're going you have entered the data into the calculator we don't have the summary statistics we actually enter the data this time and that is different than our last ttest then you'll see mu sub zero that is the hypothesized difference which is usually zero we usually hypothesize that there's no difference or that the difference equals zero then it'll ask for the list the list you want to be l3 where you have all those differences entered and the way we select l3 is you hit the second and then the number three will give that third list and finally we'll enter in mu which is the alternate hypothesis symbol are we looking for it to be smaller after the treatment bigger after the treatment or just not equal to zero after the treatment and as before it's this is a lot easier to see with an example so let's do an example where we check these matched pairs for some type of improvement a football coach wants to know if a strength class can help improve his players bench press weight the before and after data is below so we're going to have players we'll just mark the players with letters to protect their identity a b c and d and we'll have weights for before the class and after the class so before the class a bench 205 after a bench 295. that looks pretty good before the class b benched 241 after b benched 252. not as dramatic but still an increase c benched 338 before the class and 3 30 after the class c went down a bit and d benched 368 and afterwards benched 360. so the question is if alpha equals 0.05 can the coach conclude the class was helpful we're going to run a hypothesis test of matched pairs to see if there's a significant difference first the null hypothesis is that the average distance difference is equal to zero there's no difference the alternative hypothesis is that the average difference is going to be positive or greater than zero because the coach wants it to be helpful he wants the difference to be positive he wants it to be higher after than before which means we have a one tail test or better said a right tailed test if we were to draw a picture of this the hypothesized difference is zero somewhere over the right we hope to see improvement and we hope that right tail shows there was enough improvement out of the class well our distribution is that the average difference is a t distribution and the degrees of freedom is one less than the sample size there are four players one less than that is three it's very small degrees of freedom it's going to take quite a difference in order to say there's a difference so from here let's go to our calculator and see if it can help us find the differences to enter information into the calculator we're going to hit the stat button right next to the arrows edit is already highlighted if i already had numbers in my list let's say there were numbers already in here that i didn't want if you scroll up and highlight the list and hit the clear button and then enter it'll clear out the list so there's nothing left in the list the first list is for the before data so we'll enter in our data 205 241 338 and 368. the second list is our after data make sure they're entered in the same order 295 252 and 330 and 360. then list three is where we're going to put our differences so if we scroll up to highlight list three and then we're going to say take list two and subtract list one hit second and then the number two that'll grab list two minus second and then the number one and now i see list three is equal to two minus one when i hit enter i will see all the differences and that's the data we're going to use in our single mean ttest to run the ttest we'll hit stat scroll over to test scroll down to the ttest and this time we've actually entered the data into the calculator so we'll scroll over and highlight the data enter the hypothesized mean is zero for the list our data is in list three that's where the differences are so we'll hit second and the number three to give us list three leave the frequency alone for the alternate hypothesis we want to show that the mean is greater than zero so we need to select the greater than alternate hypothesis and then we're ready to actually calculate when we do that we get our t value of 0.91 and a pvalue of 0.2149 we're also told that the average difference is 21.25 so we can add that to our picture the average difference is 21.25 but when we convert that to a t value the t value was only 0.91 so the test statistic is that t equals 0.91 which gives us a pvalue an area for that tail of 0.2149 what that pvalue means is given our sample the probability the strength class made no difference in bench press weight because that's the null hypothesis is 21.49 percent there's a 21 and a half percent chance that the class had no effect on bench press weight and if we look at our alpha of 0.05 we see that is much bigger than the 0.05 alpha alpha is the minimum probability where we still believe the null hypothesis is true we are well beyond that probability much higher so our decision is to fail to reject the null hypothesis and the reason for that decision is the pvalue the probability that the null hypothesis is true is greater than alpha which is our decision break with numbers the 0.2149 is greater than the 0.05 and so for our final conclusion we will state that there is not sufficient evidence and then we state the alternative hypothesis in context to conclude the strength class increased bench press weights and so what you see is with the matched pair hypothesis test it really is exactly the same as a single mean hypothesis test with the t distribution the only difference is here we will focus exclusively on the differences so first we have to calculate that after minus before relationship to see how much things have improved or decreased after the treatment so that's how we do a hypothesis test for matched pairs take a look at a few of these on the assignment and we will see you in class to continue to work on matched pairs a little more quite often we read a claim that a certain population is distributed in certain way maybe we say that 20 percent fall in one category 40 fall in another category and 30 percent fall in another category the question we're going to take a look at today is how good that fit actually is to data we might collect so the official question here how do we test if data fits a claimed distribution and as we do this hypothesis test we have to run into a new distribution that tests how well data fits a claimed distribution and the new distribution is what we call the chi squared distribution and we use the greek letter chi with a little squared on it looks like an x with tails on the ends and this chisquared distribution a few characteristics of it it is a nonsymmetrical distribution and it is skewed right so unlike the normal and the t distribution which is perfectly symmetrical the chisquared is skewed right in fact the shape itself varies based on the degrees of freedom there is a different shape based on the degrees of freedom and another unique thing about the chisquared is that it is always greater than zero with the tdistribution and the normal distribution we found it's symmetrical around zero we had positive values and negative values representing if we were left or right of the mean the chisquared doesn't do that the chisquared actually starts at zero and depending on the degrees of freedom if there's only one degree of freedom one degree of freedom the graph looks something like this but if we increase the degrees of freedom to maybe three it's going to look something like this with a little hump skewed to the right and the more we increase the degrees of freedom that hump is going to move slightly over so this red line might represent 10 degrees of freedom and so you see the shape varies quite dramatically based on the number of degrees of freedom we have so let's look at how we can use the chisquared distribution to test a claimed distribution what we're going to do is the goodness of fit test or we've got some claim that a certain percent fall in various categories is that claim accurate does it does the data fit that distribution well is it a good distribution the test statistic we will use is chi squared is equal to the sum of the observed frequency minus the expected frequency squared divided by the expected frequency so we've got some new variables o is the observed frequency and e is the expected frequency and this chisquared value we're going to calculate by hand it's not too bad but it is one we should know how to do now with chi squared we have to know the degrees of freedom the degrees of freedom with the goodness of fit is one less than the number of categories and then for our null hypothesis and alternative hypothesis with the goodness of fit test we usually state it in a sentence rather than symbolically like we did with the means in the t distribution or the normal distribution so in words the null hypothesis is that the data fits the distribution the alternate hypothesis is that it's not equal to the distribution or the data does not fit the distribution and what's interesting about the chi squared because it starts at zero and is skewed right that alternate hypothesis is always going to be a right tailed test in fact with chi squared almost always we're working with a righttailed test there's only one context which we'll talk about in another video where we could have a twotailed test or a lefttailed test but in general chi squared is a right tailed test now we can use our calculator to help us find the area that's in that right tail so let's take a look at how the calculator can do that what we'll do on the calculator is we will find the area from the test statistic all the way out to infinity how much area is in that right tail the problem is our calculators can't do infinity so we're going to put a number in that's pretty darn close to infinity we're going to use 10 to the 99th power to represent infinity because there's going to be very little area that's past 10 to the 99th power that's a 1 with 99 zeros after it a 100 digit number that's pretty darn close to infinity so the calculator keystrokes we want to do is first you'll hit the second button then you'll select the distribution function which is really the vars button and then above it with the second feature you're getting distribution then you can scroll down to the chisquared cdf once you've selected the chisquared cdf what you'll do is you'll enter the test statistic to represent the minimum value we're finding the area of then you'll do a comma then you will do the infinity which is 10 raised to the 99th power then you will do a comma and then you'll type in the degrees of freedom we'll see this process work out when we do our example now let's say a researcher wants to verify a claim about her community she wants to verify the claim about her community that 40 percent of the residents speak spanish in the home 10 percent speak russian 45 percent speak english and five percent speak other languages so the researcher does a survey in a survey of 200 community members 71 speak spanish 23 speak russian 102 speak english and for speak another language if alpha equals 0.05 can the researcher conclude the claimed distribution is accurate we're going to go through the same process of a hypothesis test that we've gone through before we've just got a different test statistic but everything's exactly the same from there our null hypothesis we said is that the language put it in context but that the claim is accurate that the language spoken in the home matches the distribution of the claim the alternative hypothesis is that it does not match the claim that the language spoken in the home does not match the claim or the distribution of the claim i should say distribution because that's important and we said with the chisquared we are almost always dealing with a righttailed test we end up more in the right tail the more different we are from the distribution the distribution itself is a chisquared distribution and we'll do a little subscript for the number of degrees of freedom we've got spanish russian english and other four languages the degrees of freedom is always one less than the number of categories so we have three degrees of freedom and now we're ready to calculate the test statistic and this is where we're going to do our real work we've got spanish russian english and other the claimed proportion can't get it all on one page here but uh we've got 40 percent with spanish 10 with russian so spanish is 0.4 russians 0.1 english 45 0.45 others 5 0.05 the expected value we've done a survey of 200 community members so of those 200 we should expect 40 to speak spanish so we can calculate forty percent or point four times two hundred and that gives us we would expect eighty people in our survey to speak spanish for the russian we'd expect it to be ten percent ten percent of the two hundred ten point ten times two hundred is twenty for the english point four five times two hundred is ninety and for the other .05 times 200 is 10. so we use those percentages and the total sample size to calculate what we expect to happen but something different happened in our observed probabilities what we observed is 71 spoke spanish this is from our survey we observed 23 russians we observed 102 english and we observed four other languages so now what we can do is we can use that chisquared statistic is the observed minus the expected squared divided by the expected similar to how we did the standard deviation by hand back in chapter one we're going to just make a different column so that we can sum those observed minus expected squares over expected so first observed minus expected observed minus expected the observed comes first so 71 minus 80 is negative 9. 23 minus 20 is negative 3 102 minus 90 is 12. oops i'm sorry 23 minus 20 is a positive 3 and 4 minus 10 is negative 6. then we square those values the observed minus expected squared 9 squared is 81. 3 squared is 9. 12 squared is 144 and negative 6 squared is 36. now we take the observed minus the expected squared and we divide by the expected value the expected value column is that second column here so 81 divided by 80 is 1.0125 9 divided by 20 is 0.45 144 divided by 90 is 1.6 and 36 divided by 10 is 3.6 now we're ready to actually find that sum of the observed minus expected squared divided by the expected by adding up that last column 1.0125 plus 0.45 plus 1.6 plus 3.6 gives us 6.6625 that is our chi squared test statistic now that we have our test statistic we are ready to calculate a pvalue and we're going to do that pvalue by doing the chisquared cdf we're going to go from a low value of 6.6625 to a high value of infinity which we use 10 to the 99th power for infinity and we said we had three degrees of freedom so on our calculator to get the chisquared distribution we're going to hit second and then the vars button which gives us distribution and we'll scroll down we want the chisquared cdf it has to be the cdf make sure you don't do the pdf we're never going to use that one the cdf the lower limit is 6.6625 the upper limit is infinity which will use 10 to the 99th power and the degrees of freedom is 3. and when i hit paste you see it types those numbers in for me if you don't have the newest model of the ti 84 you might just have to enter in these numbers with commas separating them get the same thing though we end up with .0835 .0835 and what that p value tells us is the probability that null hypothesis is true given our data based on the survey there is 8.35 chance the proportion of people who speak various languages matches the claimed distribution eight percent chance that it matches the claim distribution but we said that alpha's 0.05 in other words we will believe the claim distribution all the way down to 5 we only have an 8 percent or we still have an 8 so we're still going to believe that null hypothesis our decision is to fail to reject the null hypothesis and the reason for that decision is because our pvalue is bigger than alpha there's enough evidence to still believe the null hypothesis specifically the pvalue was 0.0835 which is bigger than alpha which was 0.05 so we can make a conclusion in context focusing on the alternative hypothesis we say that there is not sufficient evidence to conclude the distribution of languages spoken in the home is different than the claimed distribution not enough evidence to say it's different so we'll have to be stuck believing that distribution is true that's the goodness of fit test the hypothesis test process still exactly identical we just have a new distribution of chi squared we have to do a little bit of arithmetic to calculate that chisquared value but it's not too bad so go ahead and take a look at a few if you want to practice some we'll talk about it more in class we will see you then another use for the chisquared distribution is testing to see if two variables are dependent or independent of each other so the question for today is how do we test if two variables are dependent or independent and to do this we will test for independence what we'll do to test for independence is we will collect frequency data and organize in rows and columns for example we might compare gender to we'll just do male female to whether or not you played sports in high school yes or no and this will generate a table where we'll enter in how many yeses how many no's for each gender we'll probably also have a column for totals and a row for totals and then off this contingency table we will see if playing sports is dependent or independent on gender once we have our frequency information we also need to calculate the expected frequencies and often i'll make a second table to do this of expected frequencies and the way we calculate the expected frequency is we will take the row total times the column total divided by the total of the entire survey and we'll have to do that for every single cell so first row first column first row second column second column second row find all the entries for their expected frequencies and then we can calculate the test statistic from here often i'll organize this in a third table because our chi squared is going to be equal to the observed value minus the expected value squared divided by the expected values and then we take the sum of all of those so we've got two equations that are going to be helpful to test for independence should know how to calculate the expected values and then from that calculated the chisquared now as we're testing for independence we need to know the degrees of freedom the degrees of freedom is equal to a product with independence it's the product of the number of rows minus one times the number of columns minus one also with independence similar to when we did goodness of fit the null hypothesis and the alternative hypothesis are generally in words not in symbols so the null hypothesis is generally going to be that the variables are independent and then the alternate hypothesis is that they're not independent or that they are dependent and then similar to the goodness of fit test that dependence is always a right tailed test so let's try an example and see if we can test for independence an example a restaurant wants to know if breakfast preference is dependent on gender so the following data is collected and they're going to compare male to female and some are going to prefer french toast some will prefer pancakes some prefer waffles and some prefer omelets and then we'll have a total row and a total column we find 47 males prefer french toast 35 prefer pancakes 28 prefer waffles and 53 prefer omelets if we add all of those together we'll get a total of 163 males were surveyed for the females 65 prefer french toast 59 prefer pancakes 55 prefer waffles 60 prefer omelets we interviewed a total of 239 females and if we total the individual breakfast choices we've got 112 who prefer french toast 94 who prefer pancakes 83 who prefer waffles and 113 who prefer omelets there's a total of 402 individuals in this survey the question is if alpha equals 0.05 can the restaurant conclude that breakfast preference is dependent on gender well let's set up our hypothesis test first we have our null hypothesis which states that breakfast preference is independent of gender breakfast presents preference does not change based on if you're male or female the alternative hypothesis is that there is some type of dependence that breakfast preference is dependent of gender or on gender as usual with the chi squared we have a righttailed test and our degrees of freedom to help us calculate the actual distribution is going to be the number of rows we've got two rows don't count the total one two rows minus one times the column don't count the total again one two three four columns minus one which gives us one times three or three degrees of freedom our distribution for our chisquared statistic is a chisquared with three degrees of freedom then let's see if we can calculate our test statistic to calculate the test statistic we first need to make another table of our expected values so for our expected values we'll do the same table male and female and for french toast give me a little more space here the way we calculate the expected value for our males with french toast is we will take the row of males and the column of french toast those totals so the row of males had a total of 163. the column had a total of 112. that's how we get the french toast male cell for expected 163 times 112 and then we'll divide by the total so we have 163 times the 112 divided by the total number of people which was 402 the expected value for french toast is 45.4 now we'll go over to pancakes pancakes is in the second column first row so in this case what we'll see is we want the pancakes for the males that's in the first row second column so those are the numbers we're going to multiply 163 times 94 and divide by the 402 the row times the column so we have 163 times 94 divided by the total of 402 we end up with 38.11 is the expected value for pancakes next is waffles with the waffles we want the third column in the first row so we're going to multiply 163 times 82 and divide by the total so we have 163 times the 80 3 is it 83 divided by the total of 402 and that's going to give us 33.65 for our expected value finally we've got the omelets similarly with the omelets the total for the omelet row for the males was 163. the total number of omelets was 113 and then we divide by the 402 to get 45.82 we'll do the same thing with the females this time the female row total was 239 so we're going to do 239 times the french toast total of 112 divided by the 402 the expected females who prefer french toast should be 66.59 with the pancakes 239 times the column total of 94 divided by the total total of 402 55.89 with the waffles the road total of 239 times the column total of 83 divided by the total total of 402 the expected value is 49.35 and finally with the omelets the row total is 239 the column total 113 divided by the total total of 402 the expected number of omelets for females is 67.18 so we've got the second table that demonstrates for us all of the expected values based on the row totals and the column totals next i'll make another table that does the observed minus the expected squared divided by the expected for our males females first with the french toast our first cell and i've got to do a lot of scrolling here hopefully you can see it all on one screen 47 is what we observed 45 actually go ahead and highlight it to emphasize what i'm looking at here highlighted in yellow here our first cell had an observed value of 47 an expected value of 45.41 so plugging that into our formula the observed value of 47 minus the expected value of 45.41 squared divided by the expected value of 45.41 that equals .06 then we'll do the pancake preference you'll see pancakes the observed value is 35. the expected value is 38.11 so we plug it into our formula 35 minus 38.11 squared divided by the expected value of 38.11 that gives us point twenty five waffles same thing we'll come up here we observed 28 waffles we expected 33.65 so we plug that into our formula 28 minus 33.6 squared divided by the expected value of 33.65 we get 0.95 and we'll keep going with all of our remaining cells for the omelets we observed 53 we expected 45.82 squared divided by the expected 45.82 that equals 1.13 doing the female row the observed french toast was 65. minus the expected 66.59 squared divided by the expected 66.59 equals 0.04 with the pancakes the observed value of 59 minus 55.89 squared divided by the 55.89 that equals 0.17 for the waffles the observed value was 55 minus the expected value of 49.35 squared divided by 49.35 equals 0.65 and finally the observed value for the omelets was 60 minus 67.18 squared divided by 67.18 that's going to give us point 77 so we're going to get really good at that observed minus expected squared formula the reason we need to do that is our chisquared test statistic is going to equal the sum of those observed minus expected squared divided by the expecteds we're going to actually add all eight of these numbers that we just found together and when we add all those numbers together you should get 4.02 that is our test statistic so it's a little cumbersome and tedious to calculate because you've got to go through cell by cell first we calculate the expected values by taking the row total times the column total divided by the total total then you need to find that observed minus expected squared divided by expected plugging those values in calculating them out and then finally we add them together to get our final chisquared value and now we're ready to go on with our hypothesis test everything should flow pretty quick from here we need to calculate a pvalue or the probability the null hypothesis is true to do that we'll do the chisquared on the calculator cdf the minimum value of 4.02 all the way to a maximum value of infinity 10 to the 99 and we said there are three degrees of freedom so let's do that we'll hit second the distribution or vars button we'll scroll down to select chi squared cdf the lower value we want is 4.02 the upper value is infinity 3 degrees of freedom again if you don't have the newer software you just have to separate them by commas and when we hit enter we find a probability of 0.2593 0.2 which means there is a 25.93 actually let's say based on our survey based on our survey there is a 25.93 percent chance that breakfast preference is independent of gender and we said we would believe the null hypothesis that they are actually independent as long as that probability does not go below alpha of five percent 25 percent is well above that and so we will make a decision to fail to reject the null hypothesis and the reason for that decision is the pvalue is greater than alpha or 0.2593 that's too much evidence overwhelming past the 0.05 we must still believe the null hypothesis is true so we make a conclusion our conclusion is that there is not always in context in the alternative hypothesis there is not sufficient evidence to conclude breakfast preference is dependent on gender and that is how we do a chisquared test for independence it takes a little bit of time to calculate the test statistic running through those calculations but it's not too difficult it just takes the time to run it out so go ahead and take a look at trying a few of these off the homework we'll do a few of these more in class as we look at it a bit further we'll see you then today we're going to take a look at another use of the chisquared distribution and that is testing a claim about a single variance our question is going to be how do we test a claim about a variance and when we're testing a variance a single variance we have a test statistic which is a chisquared statistic the formula with the variance for chi squared is the sample size minus 1 times the sample variance divided by the sample i'm sorry divided by the population variance and we need to be very careful or be very aware of what we have in the problem we use sigma and s to represent the standard deviation but when those pieces are squared and we either have sigma squared and s squared those represent what we call the variance and so the formula says s squared and sigma squared those values represent the variance if it's already been squared if it hasn't been squared we would have sigma and s the standard deviations which need to be squared in order to find the test statistic so we need to be very careful do we have s or s squared do we have sigma or sigma squared and not get those backwards also with chi squared we need to know the number of degrees of freedom we have the degrees of freedom are always one less than the sample size with the chi squared and something that's unique about testing a variance is that it can be either left right or two tailed test and this is very unique because normally with the chisquared we're dealing with a righttailed test but in the context of the variance it's the only time we're allowed to have either a left tail test or a twotailed test which doesn't happen as often so the best way to really attack this is to run through an example so let's say a customer wants to know how the cost of a list of school supplies varies from store to store a teacher claims the standard deviation is only dollars so to test this the customer surveys 43 stores and finds a mean of 84 dollars and a standard deviation of twelve dollars test whoops test if the standard deviation is less than the teacher's claim of 15 if alpha equals 0.05 so we've got a claim about the standard deviation the claim is the standard deviation is 15 but notice that's the standard deviation not the variance the variance is the standard deviation squared so when we set up our null hypothesis our null hypothesis will state that the variance or sigma squared is equal to the standard deviation squared or 15 squared which is 225. notice again we had to square the standard deviation to get the variance the alternate hypothesis we believe that the variance is actually less than 15 squared or less than 225. because we're interested in less than we actually have a lefttailed test so let's draw a little picture of our lefttailed test chisquared is skewed right and we're interested in being in the left tail now in the survey the standard deviation ended up being 12 compared to the actual mean of 15. is that enough to reject the null hypothesis well first we need to know the degrees of freedom the sample size minus one 43 minus one we've got 42 degrees of freedom and then we'll also calculate the test statistic the test statistic is chi squared is equal to n minus 1 42 minus 1 times s squared s i'll go ahead and actually write the formula here one more time so we can see it n minus 1 times s squared divided by sigma squared s is the standard deviation of the sample so for my sample the customer did a survey and found a standard deviation of 12. that is my sample standard deviation 12 squared to get the variance divided by sigma the claimed distribution of the population the claim standard deviation is 15. so we'll divide by 15 squared and when i do this on my calculator oops sorry the sample size was 43. 43 minus 1 times 12 squared divided by 15 squared we get a test statistic of 26.88 now that we have a test statistic we're ready to find the pvalue or the probability my null hypothesis is true the probability the standard deviation is actually 15. it's a chisquared cdf normally with chi squared we go from smallest to largest here the smallest value on the chisquared is going to be 0 to the largest value of 26.88 comma our degrees of freedom we said was 42. and let's see what the calculator gives us for that value to get the chisquared distribution we'll hit second vars so we get the distribution we'll scroll down to chisquared cdf going from 0 to 26.88 our degrees of freedom are 42 and we'll hit paste if you have the older version of the software you just enter those numbers in separated by commas like you see on the screen here and when we hit enter we find a probability of 0.0337 0.0337 that pvalue tells us that based on our sample the probability that the null hypothesis is true that the standard deviation of the cost of school supplies being 15 is 3.37 percent there's a three percent chance that that null hypothesis is true well if that's the case we're ready to make a decision the decision point is always compared to the alpha and we said alpha was going to be 0.05 five percent probability we'll still believe the null hypothesis we only have a three percent probability so we can no longer believe the null hypothesis so we will reject the null hypothesis and the reason for that decision is that the pvalue is less than the alpha value or that .0337 is less than the 0.05 and so our conclusion which focuses on the alternative hypothesis in context we can say that there is sufficient evidence to conclude the standard deviation of school supplies cost is less than 15 and that's all there is to testing a single variance it's actually the easiest chisquared problem to solve because it's very straightforward we don't have to find all those expected values that we do in other chisquared tests so we test a single variance with our new chisquared test statistic and then we run it through the same process of a hypothesis test that we've been seeing for several weeks now we should be very good at setting up these hypothesis tests so you can go and take a look at a few of these on your homework assignment we'll look at this more in detail in class and we will see you then now that we've taken a look at how a hypothesis test can be conducted for a claim about a single variance we can extend this to the next level and look at a hypothesis test on a claim of two variances and so the question we're going to answer is how do we compare two variances and similar to one variance we have to be careful if we're talking about standard deviation or variance because the standard deviation is the square root of the variance but different than the single variance is we need to introduce a new distribution that models a comparison of two variances and this new distribution is what we will call the f distribution the f distribution because this distribution is used as a fraction when comparing two variances similar to the chisquared distribution the fdistribution is not symmetrical in fact it is also skewed right also similar to the chi squared is its shape is different based i should say different shape based on degrees of freedom but what's really unique about the f distribution is it's a fraction of two variances in other words we're going to have a ratio or let's just go ahead and call it a fraction or ratio with two sets of degrees of freedom we say the numerator has degrees of freedom for the numerator is equal to the first sample size minus one and the denominator is going to be the degrees of freedom of the denominator is equal to the second sample size minus one and what's interesting is as the degrees of freedom get larger for both the denominator and the numerator the curve becomes more normal one last thing to talk about the f distribution is similar to the chi squared the f is always positive or always greater than i guess it could be equal to zero so that's kind of a brief introduction of the f distribution we're going to use our calculator to do most of the calculations with the f distribution what we're interested in is can we set up and carry out a hypothesis test on two variances the test statistic for two variances is simply the fraction the first variance divided by the second variance or the standard deviation squared divided by the standard deviation squared if both variances are equal if we have equal variances that tells us that f is going to equal to one we're dividing everything by itself if we have different variances f is closer to zero if the second variance is larger or infinity if the first variance is larger we'll use our calculator actually to do most of the work for the f statistic and so just really briefly how to do that first you're going to hit the stat button and then you can scroll over to test then you can scroll down to two samp f test and that's where we can access the twosample ftest for the comparing of our variances then to actually enter in our data you want to make sure that stats is highlighted and then you can enter the standard deviation notice i did not say the variance even though we're comparing variances with the f test the calculator wants us to enter in the standard deviation which is the square root of the variance so we can enter the standard deviation and our other data other than that the hypothesis test is going to work exactly like all the other hypothesis tests we've seen before so let's try an example and see if we can do a test of two variances quality control is interested in the variance of two machines making widgets the first make 32 widgets with a variance in the radius apparently there's a circle on these widgets of 4.1 millimeters the second makes 37 widgets with a variance in the radius of 3.7 millimeters at the alpha equals 0.05 level can quality control conclude the first machine has a higher variance we'll start by setting up our null and alternative hypothesis like always the null hypothesis always has equity so we're assuming with our null hypothesis that the variance of the first machine is equal to the variance of the second machine the alternative hypothesis is that the variance of the first machine is greater because we want the first machine to be higher than the variance of the second machine for our degrees of freedom the degrees of freedom of the numerator we always do our division in order as listed in the hypothesis test so we're going to do the variance oops the variance of the first machine divided by the variance of the second machine same order as they're in the hypothesis test so the first machine is our numerator the first machine made 32 widgets meaning the degrees of freedom of the numerator are 31. the degrees of freedom in the denominator the denominator being the second machine which made 37 widgets one less would be 36 for the degrees of freedom so our distribution is that we have an f statistic that is distributed as an f with 31 and 36 degrees of freedom and we can calculate our test statistic by just dividing those variances notice we're given the variances the variance is 4.1 and the variance is 3.7 they've already been squared so the first variance is 4.1 divided by the second variance of 3.7 that gives us 1.1081 so if we were to draw a picture of this situation the f distribution skewed right we have a test statistic right at 1.0181 we want to be greater so we go for the right tail how much area is in that right tail this is where we're going to go to our calculator to find our pvalue first we're going to hit stat scroll over to test and then we're going to scroll down for the two sample f test oh there it is two sample f test i'm going to enter in the statistics and it wants my first standard deviation not the variance the calculator is asking for the standard deviation which is the square root of the variance fortunately i can just type in the square root with the second and then the square root key is above the square diagonal from the seven and my first variance was 4.1 so if the variance is 4.1 the standard deviation is the square root of 4.1 and when i hit enter it's going to calculate that value for me the first sample size had 32 widgets in it the second machine had a variance of 3.7 so we'll take the square root of 3.7 to get the standard deviation of 1.92 and the second machine made 37 widgets for our alternate hypothesis we said that the first is greater than the second so we'll select greater than and go down and hit calculate notice it gives us the exact same f statistic that we found 1.1081 but what we're really interested in is it gives us a pvalue of 0.309 303809 so let me scroll a bit give me a little more space to work we have a pvalue of 0.3809 remember the pvalues the probability the null hypothesis is true so we will say the probability or based on our sample the probability both machines have the same variance that's the null hypothesis that they're equal is 38.09 percent and the alpha tells us the probability required to disprove that null hypothesis we have to drop below five percent we're well above the five percent so we're ready to make a decision which is to fail to reject the null hypothesis and the reason for that decision is that the pvalue is greater than alpha there's too much evidence and support of the null hypothesis specifically with numbers 0.3809 is greater than 0.05 and so for our final conclusion which is always written in the context of the alternative hypothesis i believe the alternative on there is that there is insufficient or there is not sufficient evidence to conclude the first machine has a higher variance than the second machine and that is how we can compare two variances so we have a new distribution the f distribution which is a fraction of the variances but the idea is still the same as our hypothesis test we've been doing we've got our null and alternative hypothesis the test statistic gives us an area a p value that we compare to alpha and make a conclusion whether or not we have sufficient evidence to believe the alternative hypothesis take a look at a few of these on your own if you'd like we'll talk about uh comparing two variances more in class and i'll look forward to seeing you then quite often in statistics we're concerned with comparing the mean from more than just two groups in the past we compared two groups with the ttest but here when we compare more than two groups we need a different statistical test and that's what we're going to look at today how do we compare the means of more than two groups this actually also works with two groups but the ttest is easier the ttest turns out to be a special case of this thing that is called the anova but the ttest is easier for two groups so we do the ttest but when there's more than two groups we use this test called the anova anova is actually an acronym it stands for analysis of variance and the idea behind the anova is we compare the variance between the groups to the variance within the groups and when we divide them we end up with an f statistic and so we can compare using the f ratio just like we did with two variances so a couple differences with the anova the hypothesis test for the hypothesis the null hypothesis is always the same that the first mean is equal to the second mean which is equal to the third mean which is equal to all the other means until you get to the very last mean basically all the means are the same and the alternative hypothesis is that at least one mean is different we don't know which mean but just one mean is different than the rest possibly all three are different from each other and we use the f test to compare which hypothesis we'll end up going with what we're doing is we are looking for a difference but not where the difference is turns out that once we decide that there actually is a difference between one of the means we have to do some followup statistical tests which are beyond the scope of this course to identify specifically where that difference is we might have an idea where it is but to get the exact difference we need followup tests that we're not going to cover in this course so for today all we're looking at is are they all the same or is there a difference somewhere we're not going to spend our time in this course with the complex calculations of the anova we're just going to look at actually carrying it out and having our calculator do the complex calculations how we're going to do this is we're going to put each group in its own list and the way we do that is we'll hit the stat button and then we'll select edit to edit our list and in list one we'll put the first group data list two we'll put the second group data list three the third group data until we get all of our groups and once all of our groups are actually listed in there we will run the test and we'll do that with the stat button we will scroll over to tests and then we will scroll down to the anova now the anova will not give us prompts on what information to enter in like a lot of the other statistical tests did so what we need to do is we're going to enter the lists separated by commas and the way we get the list is we'll hit second and then select the list number and we'll see how that all works out with our example let's go ahead and move to our example and see if we can compare this time this context we're going to compare three groups and see if they have the same mean or different means a university is comparing traditional students transfer students and nontraditional students by comparing gpa in the junior year here are their results the traditional students had gpas of 3.2 3.4 3.7 4.0 the transfer students had gpas of 3.1 2.7 2.9 3.2 and 4.0 and the nontraditional students had gpas of 3.4 2.2 2.7 and 2.8 we want to know if all three groups can be considered to have the same gpa or different gpas if alpha equals point 10 can the university conclude there is a difference in the groups let's scroll up and give ourselves a little room and start our hypothesis test the null hypothesis is that the mean of the traditional students is equal to the mean of the transfer students which is equal to the mean of the nontraditional students that they all have the same mean the alternate hypothesis is that at least possibly more one group have a different gpa let's go ahead and run this test on our calculator and see what hap first thing we need to do on the calculator is enter our data into stats and we'll select edit if there's extra stuff in this list you can highlight and hit clear enter and that'll delete the list or clear the list clear enter clear enter and so in list one i'm going to put the gpas 3.2 3.4 3.7 and 4.0 list 2 is my second group 3.1 2.7 2.9 3.2 and 4.0 the nontraditional group 3.4 2.2 2.7 and 2.8 now let's go ahead and run the anova we'll hit stat and this time going over to test and all the way down to the bottom or hitting up gets us straight to the bottom you'll see anova for the anova we enter in our list we have the three list so second one for the first list comma second two for the second list comma second three and we keep going based on how many lists we have we only had three lists and when we hit enter we get all sorts of information we've got our f statistic of 3.07 we've got our p value of .09 under factor you'll see we've got degrees of freedom equals two and under air we've got degrees of freedom equals ten the other numbers we're not going to concern ourselves with today but those degrees of freedom represent the numerator and the denominator so the factor is the numerator 2 the error is the denominator 10. they are in order which is nice so when we say our distribution we'll say f is distributed as an f statistic with 2 and 10 degrees of freedom and we just found out that f equals 3.07 and more specifically our pvalue that's the important one was .0912 but also go ahead and draw a picture of what's happening there's my f distribution my test statistic at 3.07 and we shade that tail which we now know has an area of 0.0912 now speaking of the pvalue what that pvalue means is based on our sample the probability all three groups of students have the same mean gpa in the junior year is nine point twelve percent we've got a nine percent chance that all three groups have the exact same probability the p value is the probability the null hypothesis is true which means we are ready to make a decision the decision is based on the alpha we said alpha's point 10 we're going to believe the null hypothesis is true until there's less than 10 percent chance it actually is true we we have a 9 chance that it's true so that passes our threshold so we will reject the null hypothesis and the reason for that decision is that the pvalue is less than the alpha or the 0.0912 is less than the point 10 we said was our decision breakpoint so our conclusion in context of the alternative hypothesis is that there is sufficient evidence because we rejected to conclude the mean gpa of traditional transfer and nontraditional students in the junior year is not the same again i'll notice this test doesn't tell us which group is different from the rest or possibly all three groups are different from each other it just tells us that they're not all the same so that's the anova it's another use of the f distribution there's actually several types of anova this is the most basic that we're going to look at in this course and you can see more anovas and more advanced statistics courses but for now we're comparing do multiple groups have the same mean we plug it into our calculator we get an f statistic and a p value and we should be able to conclude do they all have the same mean or is there a difference somewhere in the group try a few of these on the homework we'll talk about it more in class and we will see you then this video is going to look at the important concept a find a relationship between variables with what we call correlation and regression so that's where our question stems from our question is how do we test for a relationship between variables and the first thing we're going to start with is just getting a visual of how the two variables are related using what is called a scatter plot a scatter plot is basically just a graph of all of our data so it's best illustrated with an example let's say a researcher collects a sample the number of pages a person reads based on their age so we've got let's call the first column their age and the second column the pages that they read so we've got a 14 year old who read 40 pages there's a 21 year old who read 45 pages they asked a 33 year old who read 92 pages they asked a 45 year old who read pages and they asked a 63 year old who read 171 pages the idea of a scatter plot is if we call the first column the independent variable x and the thing that we think changes based on the independent variable or the dependent variable y we should be able to graph these to get a visual of how they relate to each other let's go up by tens on the xaxis 10 20 30 40 50 60 70 years and on the yaxis we'll go up by 30s 30 60 90 120 150 180. and so we'll make a point for each one of these and this is going to be what becomes our scatter plot so at 14 years old we'll go up to 40 pages which is right about where that blue dot is then we've got a 21 year old who goes 45 pages so maybe a little bit higher then there's a 33 year old who's going to go up to 92 pages a 45 year old who will go up to 167 pages and a 63 year old who will go up to 171 pages and so we kind of can see a relationship here but before we get into that let's make sure our scatter plot is complete because a good scatter plot will have titles and labels so the bottom represents the age going up represents the pages and we might say pages read by age for the title and what we can see is the dots aren't exactly in a straight line but they do kind of trend upwards it seems that as age goes up the number of pages goes up and so we've got this relationship starting to establish visually now our calculators can also make these scatter plots so i'm going to show you how to make this exact same scatter plot on the calculator so first i'll write out the instructions then we'll go ahead and do it first we have to enter the data and the way we enter the data is you'll start by hitting the stat button and then you'll select edit and then you can put x in list one and y in list two once you've entered the data you're ready to make the graph and the way we make the graph is first you'll have to hit the second button and then you'll have to hit the y equals button because above y equals the second feature is stat plot it's for graphing statistics and you're going to go into the first stat plot and make sure it is on make sure you select the scatter plot which is going to be the dots and then you want to select what list you want l1 and l2 once you're done with setting up the stat plot you'll hit the zoom button and you'll select zoom stat which will center you right on the statistics so let's look at doing that on our calculator first we have to enter in our data to do that we'll start by hitting the stat button selecting edit and there's already stuff in these lists to clear it out i'll highlight the list name and hit clear enter and that clears out the list clear enter and then i'll put in l1 my x's my x's were 14 21 33 45 and 63 and then l2 i'll put my y's 40 45 92 167 and 171. now that i've entered in the data to set up the graph we'll hit second and the stat plot button which is the y equals button option number one we're going to make our scatter plot first you want to make sure you've turned it on by selecting on there's lots of graphs you want to make sure the dotted one which stands for the scatter plot is selected and then l1 and l2 are my list we're ready to see it we'll hit the zoom button and near the bottom maybe not the bottom bottom there it is number nine on my calculator is zoom stat what that does is that centers my graph around my statistics and you see we end up with much the same graph we had before the two dots next to each other a dot a high dot and another dot over to the side and so now we've got our scatter plot on our calculator but sometimes we're interested in more than just the dots what we might be interested in is can we draw a line that kind of models through the center of the data a best fit close to the dots that is what we call our regression equation so let's take a look at the regression equation also known as the line of best fit also known as the least squares line basically a line that models those dots as close as possible well since it's a line we know the equation of the line is going to be y equals some yintercept plus the slope times x and in linear regression we call that y equals a plus bx a little different than algebra where you probably learn y equals mx plus b similar setup but now we use a for the yintercept and b for the slope the equations for a and b are quite complex so we're going to have the calculator do our work for us to calculate the equation and the way the calculator can do that is we actually run what's called the linear regression t test so we'll hit stat we'll scroll over to tests and we'll scroll down to the lynn rag t test we can also graph it on our scatter plot graph by hitting the y equals button and then typing in the equation so let's do that for our age versus pages example so pulling back up my calculator we're going to hit the stat button scroll over to test and near the bottom you will see len reg t test make sure you do lin reg t test not any of the other len reg stuff lin reg t test and hit enter l1 and l2 are x and y uh we're going to always select the not equals to button for our len reg t test and then if we hit calculate it gives us a lot of information we're going to come back to some of this information in just a minute but specifically what we're interested in right now if i scroll down we see a is negative 5.523 and b is 3.083 i'm going to go ahead and round those to two decimal digits so a is equal to negative 5.52 and b is equal to 3.08 which means if i put that into my equation y equals a which is negative 5.52 plus b 3.08 times x this equation models as close as possible my scatter plot if i hit y equals i can type that in negative 5.52 plus 3.08 x x buttons right next to the stat button and when now when i hit the graph button what i'll see is i have a line that goes right through the middle of my dots seems to model that quite well we have a line of best fit the nice part about the line of best fit is i can use it to estimate values i don't have let's estimate the number of pages read by a 30 year old if we go back to the original data we don't have 30 in here but age is my x value so if i plug in the age for x we should be able to get a good estimate for what that 30 year old is reading so y equals negative 5.52 plus 3.08 x x is the age my 30 year old and when i put that into my calculator i end up with 86.88 pages so round that baby to 87 pages we would expect this 30 year old to read one important thing to note about using the regression equation to estimate points is it only works within the domain of the problem it only works between the high and low values we can't estimate values outside of that range so for example this would be bad could we figure out a threeyearold well mathematically it would make sense if three is the age we'll just plug 3 in for the x and we get negative 5.52 plus 3.08 times our 3 year old and that's going to equal to 3.72 so do we conclude that a threeyearold is reading 3.72 pages probably not there's not a lot of threeyearolds that can read anything maybe their name the problem is is the threeyearold is outside of our data our data had a low of 14 and a high of 63. we're only going to estimate values between those numbers if we go outside of the data the model can very easily break down and so we want to be careful not to take the model further than it's designed to go all right i want to look at one more thing with regression and correlation and that's what we call the correlation coefficient and this is where the hypothesis test comes in though we won't do it nearly as formally as we have in other contexts the idea of the correlation coefficient is i have this red line on my graph that goes kind of through the blue dots but not perfect well how good of a model is that line of best fit is it close is it far off what can we know from that line this is what the correlation coefficient measures and we have a special variable r that we use for the collate correlation coefficient and it tells us two things about the graph it tells us the strength and direction first r is between negative one and positive one we say if r equals zero that tells us that there is no relation no relation between the x and the y the dots are completely random and our line of best fit just has to go straight through the middle but that doesn't even model it well there's no relationship everything's random if r equals positive one what that means is we have a perfect positive relation positive means we're going uphill so now we're going to have these dots going uphill and the line goes right through all the dots uphill perfectly that would make r equal to 1. similarly r equals to negative one means we have a perfect negative relation or we're going downhill so now there's a bunch of dots going downhill and the line goes right through the middle of all the dots going downhill now there's no such thing as perfect data so r very rarely is 0 1 or negative 1. usually it's somewhere in between r might be negative 0.78 or r might be positive 0.23 and the closer it is to 0 the less relationship we have and the closer to 1 the more likely we have a relationship and the way we determine if there's a relationship is we do a ttest it's called the lynn reg ttest linear regression ttest which gives a pvalue that tells if the relationship between the two variables is significant a pvalue less than alpha of 0.05 will mean that we have a significant relationship but r works hand in hand with this pvalue because r tells the strength of the relationship r could be positive or negative but what we'll say is if it's between 0 and 0.199999 the strength is considered to be very weak there might be a relationship because the p value tells us it's significant but it's very weak below 0.199999 from 0.2 to 0.399999 we say it's just a weak relationship 0.4 to 0.59999 is a moderate relationship but we like to see r bigger than 0.6 0.6 to 0.7999 means we have a strong relationship and on occasion we end up between 0.8 and 1.0 which would be a very strong relationship in addition to r telling us the strength of the relationship and p telling us whether or not the relationship is significant there's a third variable that we look at in analyzing correlation and regression and that is r squared which we get by squaring the r value r square tells the amount of variance actually not variance we'll say variation in the dependent variable that is explained by the independent variable how much of the changes in that dependent variable are explained by changes in the independent variable versus other factors this whole concept of the correlation coefficient is probably best seen by going back to our example so let's go back to our age versus pages example we can run the lynn rag ttest on our calculator to find these important values the tvalue the r the pvalue and the rsquared so going back to our calculator let's run it one more time i'll hit stat over to test and i'm going to do the len reg t test my numbers are already in there so when i hit calculate we see our test statistic the t is 5.09 we see that gives us a p value of 0.0146 and if i scroll down we will see an r value r is 0.9466 and r squared is 0.896 let me transfer all those values over here and we'll talk about what they mean so we had a t value of 5.09 when we round it which resulted in a p value of 0.00 also there was an r our correlation coefficient of 0.947 and r squared it says was 0.896 with linear regression the null hypothesis is always that the relationship which we represent with the greek letter rho looks like a p equals zero that means there's no relationship the alternative is that rho is not equal to zero or that there is a relationship and we can see in our case we're going to make a decision because the pvalue is less than 0.05 to reject the null and make a conclusion that there is significant evidence to conclude the alternative hypothesis which says there is a relationship there is a relationship between age and pages red but we can expand on that conclusion a bit and say because r is equal to 0.947 which if we scroll up in our notes we see that puts us in the very strong category between 0.8 and 1.0 we can say the relationship is very strong we can even go one step further and say because r squared equals 0.896 we can claim that 89.6 percent of the variation in the dependent variable or in pages red is explained by the independent variable or by age and so you see we don't end up with the traditional hypothesis test going through all the same exact steps we have before but the pieces are still there that we decided based on our pvalue that there is a significant relationship based on our r we can determine how strong the relationship is and based on our r squared we can say what percent of the variation is explained by the independent variable so that's what we're looking at today determining is there a relationship if there is a relationship what that relationship is based on the equation y equals a plus bx and also drawing the scatter plot so we can get a visual to see that relationship we'll look forward to seeing you in class so we can look at these scatter plots linear regression and correlation a bit further
