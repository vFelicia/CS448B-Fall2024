With timestamps:

00:00 - [Music]
00:03 - this video will cover introductory
00:06 - information about business analytics
00:07 - including a definition of business
00:09 - analytics the wisdom hierarchy data
00:13 - sources and business analytics terms
00:16 - applications history and uses business
00:21 - analytics is the process of transforming
00:23 - data into insights that support improve
00:26 - and/or automate business decisions the
00:29 - data can be of many types and from a
00:31 - variety of sources and there are many
00:33 - techniques and software packages that
00:35 - can be used for an analysis a simple way
00:38 - to think of business analytics is that
00:39 - it's the tools and processes used to
00:42 - find value in data to transform the raw
00:44 - data into information that can be acted
00:46 - upon a common method of understanding
00:49 - the relationship between data
00:51 - information knowledge and wisdom is by
00:53 - using a pyramid this graphic is called
00:55 - the di kW pyramid or the wisdom
00:58 - hierarchy and it illustrates that data
01:00 - is the foundation upon which decisions
01:02 - can be made information is defined in
01:04 - terms of data knowledge is defined in
01:06 - terms of information and wisdom is
01:08 - defined in terms of knowledge data are
01:11 - numbers or text without any context
01:13 - information provides meaning from data
01:15 - often combining multiple data points to
01:18 - produce a tangible idea knowledge
01:20 - provides context from the information
01:22 - making it directly applicable to a
01:24 - situation wisdom applies the knowledge
01:26 - to make a decision the original data has
01:29 - become useful enabling an action to be
01:31 - taken so where does data come from data
01:34 - can come from a variety of sources both
01:36 - internal and external internal data is
01:38 - collected by businesses and stored
01:40 - within their own servers this data can
01:42 - be generated in a number of ways either
01:44 - by physical objects such as sensors or
01:46 - barcodes or by using computer software
01:48 - such as websites while collecting and
01:51 - storing proprietary data sources is
01:53 - still very common there are now many
01:55 - external data sources that businesses
01:57 - can use public domain data sources such
01:59 - as government surveys or social media
02:01 - posts can be accessed and there are also
02:03 - many services that offer paid data
02:05 - sources like stock market or weather
02:07 - data these external data sources can be
02:10 - combined with a company's proprietary
02:11 - data to build a more complete
02:13 - picture of reality the amount of data
02:15 - that is being generated and collected is
02:17 - growing exponentially this growth is
02:19 - occurring due to the similar exponential
02:21 - trend in computing power along with a
02:23 - decrease in costs for digital storage
02:25 - because so much new data is being
02:27 - created and captured every year there's
02:29 - a corresponding growth of demand for
02:31 - business analysts there are many terms
02:34 - that are synonymous or semi synonymous
02:36 - to business analytics in the past few
02:39 - years
02:39 - data science has become the most common
02:41 - term to be used to describe this field
02:43 - other terms are often used
02:45 - interchangeably such as business
02:47 - intelligence big data data mining
02:50 - knowledge discovery and machine learning
02:52 - while there are many discussions about
02:54 - which term to use in which scenario all
02:56 - of the terms refer to the overall
02:58 - concept of using data to make better
03:00 - decisions or better products business
03:03 - analytics is an interdisciplinary
03:04 - subject based heavily on math and
03:06 - statistics it uses computer science
03:09 - principles and algorithms the math and
03:12 - computer science concepts are applied to
03:14 - a specific subject
03:17 - Business Analytics has applications in
03:19 - every field data on all our clicks on
03:23 - the Internet are used by retailers to
03:24 - figure out our preferences and provide
03:26 - targeted advertising analyzing data on
03:29 - past marketing campaigns provides a
03:31 - useful base to determine who to target
03:33 - for the next round supply chains in
03:36 - almost all industries have become far
03:38 - more efficient due to analytics
03:40 - organizations use historical data to
03:42 - figure out optimal numbers for staffing
03:44 - and hiring companies use demand data to
03:47 - determine optimal prices and
03:49 - professional sports teams use analytics
03:52 - such as when they look at player
03:53 - performance to determine who would be
03:55 - the best draft pick no matter which
03:57 - industry you work in there's room for
03:59 - analytics to add value although the
04:02 - popularity of analytics has recently
04:03 - peaked the field is nowhere as new as
04:06 - certain companies or people make it out
04:07 - to be since the computer was invented it
04:10 - was being used to process data to solve
04:12 - problems from decoding messages in World
04:14 - War 2 to generating weather forecasts in
04:17 - 1950 to modeling credit risk in 1958
04:21 - of course these tasks involved enormous
04:23 - computational costs and only
04:25 - organizations with the most resources
04:27 - could attempt them toward the end of the
04:29 - 20th century as computing power became
04:32 - more affordable more organizations began
04:34 - collecting and storing data the types of
04:37 - analytical projects transitioned from
04:38 - being historical in nature to real-time
04:40 - in 1992 the first real-time credit card
04:44 - fraud system was introduced then the
04:46 - first analytically centric companies
04:48 - emerged companies such as Google used
04:51 - data to build their core product while
04:54 - other companies such as Amazon use
04:56 - analytical techniques to earn market
04:57 - share from competitors the rapid
05:00 - ascension of these tech companies has
05:01 - led to an arms race where all businesses
05:04 - have become committed to analytics
05:06 - businesses use analytics to gain an edge
05:08 - on their competitors and increase
05:10 - profits the three main areas in which
05:12 - they do this are competition to increase
05:14 - revenue from products or services sold
05:17 - efficiency to reduce the costs of
05:19 - resources or internal processes and
05:21 - customer satisfaction to improve the
05:24 - customer experience and encourage
05:25 - customer loyalty here's a case study as
05:28 - an example loyalty cards are
05:31 - used by grocery stores to uniquely
05:32 - identify their customers by requiring a
05:35 - loyalty card to obtain special discounts
05:37 - in the store the grocer can isolate
05:38 - habits of each customer and then provide
05:41 - customers with customized promotions to
05:43 - increase spending
05:44 - when a customer stops frequenting the
05:46 - store the grocer can mail coupons with
05:48 - aggressive offers the layout of a
05:50 - grocery store is constantly being
05:52 - changed to maximize customer spending
05:54 - this is why the milk section is always
05:56 - on the opposite side of the produce
05:57 - section so customers will have to
05:59 - traverse past every aisle to get to the
06:01 - two most commonly bought items each
06:04 - shelf is also analyzed to find the ideal
06:06 - arrangement more expensive items are
06:08 - typically placed at or around eye level
06:10 - while the cheaper products will be on
06:12 - the top or bottom shelves optimizing
06:15 - prices is another analytical technique
06:17 - used to maximize customer spending many
06:20 - grocery stores will have what are called
06:21 - loss leaders products that are very
06:23 - cheap to draw our customers into the
06:25 - store where they will inevitably spend
06:26 - more on other overpriced items grocers
06:29 - will also find the ideal times and
06:31 - prices to markdown expiring products
06:33 - preventing the product from being thrown
06:35 - away at a complete loss this concludes
06:38 - our introductory video about business
06:40 - analytics today we covered a definition
06:42 - of business analytics the wisdom
06:44 - hierarchy data sources and business
06:48 - analytics terms applications history and
06:51 - uses
06:54 - [Music]
07:13 - this video will cover introductory
07:16 - information about business analytics
07:18 - including the role of the business
07:20 - analyst what makes a business analyst
07:22 - successful and an overview of business
07:25 - analytics tools project outcomes and the
07:28 - analytical process there are three main
07:32 - reasons business analytics as an
07:33 - enticing career choice the first is that
07:36 - there's a high demand for business
07:37 - analysts and the relatively low supply
07:39 - of skilled workers means that salaries
07:41 - are higher in this field
07:42 - another reason is for the challenge of
07:44 - solving interesting problems
07:46 - analysts are typically people who are
07:48 - interested in solving complex puzzles
07:50 - finally those who are curious about how
07:52 - things work can use their skills in
07:54 - analyzing data to uncover previously
07:56 - unknown truths a business analyst can
07:59 - take many roles depending on the data
08:00 - and the type of project the most common
08:03 - roles are that of an interpreter in
08:04 - which the analyst uses descriptive
08:06 - analytics to tell the story of what
08:08 - happened an oracle and which analysts
08:11 - use predictive analytics to predict
08:13 - future events and a console in which the
08:15 - analyst uses prescriptive analytics to
08:18 - provide advice on the best course of
08:19 - action an analyst becomes successful due
08:22 - to a combination of hard and soft skills
08:24 - the hard skills are more tangible and
08:27 - refer to what the analyst can do with
08:29 - what tools while the soft skills are
08:31 - less flashy on a resume but equally or
08:33 - even more important than the hard skills
08:34 - analytical tools can be separated into
08:38 - two categories software that requires
08:40 - coding and software in a graphical user
08:42 - interface or GUI that is based on
08:45 - point-and-click interaction the main
08:47 - benefit of writing code is that it
08:48 - allows for more flexibility there are
08:50 - more features and it allows for more
08:52 - possibilities the drawback of coding is
08:54 - the extended learning curve within the
08:56 - last decade there have been many new GUI
08:58 - programs that make analytics easier to
09:00 - implement without the need for writing
09:01 - code programs such as tableau Alteryx
09:04 - and rapid miner have started gaining
09:06 - market share going along with older
09:08 - tools such as SAS Enterprise guide but
09:10 - none of these tools have yet to replace
09:12 - the overwhelming popularity of code
09:14 - based software such as SAS r Python or
09:17 - SQL there are many different goals that
09:19 - an analytics project can strive for
09:21 - typically these goals fit into one of
09:23 - two categories the first is providing
09:25 - information about a business
09:26 - such as reports and dashboards for
09:28 - business stakeholders reports or
09:30 - presentations provide one-time insights
09:32 - to explain events that have occurred and
09:34 - predict future events and dashboards are
09:36 - used by stakeholders for ongoing
09:38 - monitoring of key aspects of the
09:40 - business the second category is the
09:42 - production of analytical products in
09:44 - these types of projects the business's
09:46 - data becomes the input for a complex
09:47 - process that automatically produces in
09:49 - action this can take the form of
09:51 - features that offer a better experience
09:53 - for consumers for instance Amazon has an
09:55 - automated algorithm that determines
09:57 - products you might like to buy
09:58 - analytical products can also be built to
10:00 - make internal business processes more
10:02 - efficient an example of this is how
10:04 - credit card companies test every
10:06 - transaction for the probability of fraud
10:07 - an analytical project should start with
10:10 - the goals well-defined
10:11 - very rarely our project started to
10:13 - simply explore the data or find hidden
10:15 - truths collecting an inventory of all
10:17 - the relevant data sources is also an
10:19 - important step in the beginning of a
10:21 - project finally every analysis should
10:23 - begin with an effort to better
10:24 - understand the data this should be done
10:26 - before any analytical techniques are
10:28 - used simply observe the data files and
10:30 - write down a list of observations
10:32 - questions and any other ideas you have
10:34 - this process called creating disfluency
10:36 - enhances the data dictionary and helps
10:39 - the analyst internalize elements of the
10:40 - data cognitive disability nabel's
10:44 - students who take lecture notes by
10:45 - writing to retain more of the material
10:47 - than students who type notes even though
10:49 - those who type can take notes more
10:50 - efficiently sometimes the more work we
10:53 - have to do to process the information
10:54 - the better we can understand the
10:56 - information using this principle at the
10:58 - beginning of a project before using any
11:00 - advanced analytical techniques enhances
11:02 - the analysts capability to understand
11:04 - the data after the initial stages of a
11:06 - project there are a few more steps in
11:08 - the analytical process the majority of
11:10 - time is spent exploring and preparing
11:12 - data and the exploration stage the
11:14 - analyst learns more about the variables
11:16 - including distributions and frequency of
11:18 - values and also identifies variables
11:20 - with missing or null values in the
11:22 - preparation stage the data becomes more
11:24 - useful as variables are transformed and
11:26 - anomalous values are rectified this is
11:28 - referred to as data cleaning another
11:30 - common task during this stage is to join
11:32 - data into as few sources as possible
11:34 - perhaps one of the most valuable
11:36 - techniques to use during the preparation
11:38 - stage is called feature engineering
11:40 - in which the analysts transform certain
11:42 - variables into new variables containing
11:43 - slightly different data this allows
11:45 - hidden aspects of variables to be
11:47 - analyzed for example a data set with a
11:49 - date variable can have a new variable
11:51 - added to determine whether that date is
11:53 - on a weekday or the weekend and this new
11:55 - variable could add important information
11:57 - that was not readily available in the
11:58 - original data source this is one of the
12:00 - areas of the analytical process where
12:02 - creativity is needed the last two steps
12:04 - in the analytical process are to build
12:06 - models and then to put these models into
12:08 - production a model is a type of
12:10 - mathematical equation that describes
12:11 - relationships among variables in a
12:13 - dataset often for the purpose of
12:15 - predicting an outcome by putting a model
12:17 - in production an automated decision can
12:19 - be made when new data is observed in
12:21 - some cases models are not the goal of a
12:23 - project rather the goal is to analyze
12:25 - data and communicate the findings in an
12:27 - analytical report presentation or
12:28 - dashboard visualization of the data is
12:31 - also a key component throughout the
12:32 - entire analytical process as the analyst
12:34 - attempts to learn more about the data
12:36 - this concludes our introductory video
12:38 - about business analytics today we
12:40 - covered the role of the business analyst
12:42 - what makes a business analyst successful
12:45 - and an overview of tools project
12:47 - outcomes and the analytical process
12:52 - [Music]
13:10 - this video will cover some of the
13:12 - foundations of Business Analytics
13:13 - selecting filtering and sorting
13:16 - there are many synonymous terms to
13:18 - describe the aspects of a typical data
13:20 - file which can be referred to as a table
13:22 - spreadsheet data set or data source
13:24 - along the horizontal axis are the
13:26 - variables which can also be called
13:28 - fields attributes or columns along the
13:31 - vertical axis are the observations also
13:33 - referred to as records tuples or rows
13:35 - fields that are sparsely populated
13:37 - meaning that a high percentage of
13:39 - records is missing should not be
13:40 - selected
13:44 - fields with redundant values meaning
13:46 - that a high percentage of records have
13:48 - the same value should also not be
13:49 - selected techniques to identify these
13:51 - fields vary for now a visual inspection
13:54 - of the first few records can be used
13:56 - from this table we can remove the state
13:58 - column because all of its values are the
13:59 - same we can remove the religion column
14:02 - because it is sparsely populated our
14:03 - table now depicts only the variables of
14:05 - interest once the variables of interest
14:07 - have been selected they can be renamed
14:09 - if needed and assigned the proper data
14:10 - type there are two common variable types
14:12 - numeric and categorical numeric
14:14 - variables include discrete variables
14:16 - which can include whole numbers used for
14:18 - counting and IDs that represent a unique
14:20 - entity and continuous variables which
14:22 - are numbers with decimals used for
14:24 - measuring categorical variables include
14:25 - text which is any combination of letters
14:27 - numbers and symbols and strings or
14:29 - characters and boolean or binary
14:31 - variables which contain only one of two
14:33 - possible values variables that can be
14:35 - treated as either numeric or categorical
14:37 - include dates such as date time date or
14:40 - time and spatial objects which show
14:42 - location such as latitude and longitude
14:45 - here are the variable types for our
14:46 - table person ID is a discrete numeric
14:49 - variable gender and city are string
14:51 - variables weight is a fixed decimal
14:53 - variable date of birth is a date and
14:55 - student is boolean software packages can
14:57 - automatically assign fields with data
14:59 - types and sometimes these can be
15:01 - assigned incorrectly examination of
15:03 - these data types is necessary to ensure
15:04 - proper utilization further downstream
15:06 - sometimes data formats require advanced
15:09 - data transformation and extraction
15:11 - techniques for example dates can have
15:13 - varying formats that can be interpreted
15:14 - as strings and parts of these strings
15:16 - need to be separated before the software
15:18 - can identify as a date field for now
15:20 - we'll assume that you can change a
15:21 - fields datatype without these advanced
15:23 - techniques the final aspect of the
15:25 - Select step is to assign the proper size
15:27 - for each variable many software packages
15:29 - can do this automatically but it's still
15:31 - a good idea to review the sizes because
15:32 - the size of each field has a direct
15:34 - impact on the amount of storage required
15:36 - for a data set specifying these can save
15:38 - system resources and processing time the
15:40 - common issue is for one record that
15:41 - contains many more characters than the
15:43 - typical record to cause the size to be
15:45 - much larger than need be a visual
15:46 - inspection of the table can highlight
15:48 - these instances but beware that changing
15:50 - a variable size can truncate or cut off
15:52 - some of the values while selecting
15:54 - limits a data set size by omitting
15:56 - certain columns a filter limits
15:58 - sighs by omitting certain rows a filter
16:00 - is also known as a condition subset or
16:03 - in SQL a where clause and it's commonly
16:05 - used for investigative purposes it's
16:07 - important to be aware of the information
16:09 - contained within the row of a data table
16:11 - also commonly referred to as a record or
16:13 - observation a row defines the level of
16:15 - detail that is contained in the data set
16:17 - for this example each row represents one
16:19 - person an ID fields such as person ID in
16:21 - which no rows contain the same ID can be
16:24 - used to determine that the tables level
16:25 - of detail is a person if there are
16:27 - multiple rows for the same ID we know
16:29 - that the rows reflect data from the same
16:31 - person in this example the level of
16:32 - detail is more granular it shows a
16:34 - record of one person's weight by day
16:36 - finding and understanding the level of
16:37 - detail for a table is necessary before
16:40 - analyzing the data after determining the
16:42 - tables level of detail you should check
16:43 - for duplicate observations these are
16:45 - rows of data in which all values are
16:47 - exactly the same as another row in some
16:49 - cases duplicate observations may be
16:51 - legitimate but usually these are
16:53 - erroneous and need to be removed here's
16:55 - our table with the erroneous observation
16:56 - removed this table is filtered to show
16:58 - only records of people who live in
17:00 - Raleigh let's try another filter here's
17:02 - our original data set again this table
17:04 - is filtered to show only records of
17:06 - people who weigh more than 180 pounds
17:08 - filters are different variables can be
17:11 - applied together combining above
17:12 - examples we can filter the original
17:14 - table for people who live in Raleigh and
17:16 - weigh more than 180 this results in only
17:18 - one observation the next step is sorting
17:20 - when we sort we rearrange a table by
17:22 - ordering the rows according to the
17:24 - values of one or more fields and either
17:25 - ascending or descending order here's our
17:27 - original data set it's sorted by date of
17:29 - birth in ascending order here we've
17:31 - sorted by city in ascending order and
17:33 - then by weight in descending order this
17:35 - concludes our video on selecting
17:37 - filtering and sorting
17:41 - [Music]
18:02 - this video will cover two types of
18:05 - formulas same row and multi row a
18:08 - commonly used method of data preparation
18:10 - is using existing fields to create new
18:13 - variables
18:13 - examples include adding subtracting
18:16 - multiplying dividing or applying another
18:19 - mathematical function to numeric field
18:21 - extracting and transforming substrings
18:23 - from a text field extracting truncating
18:26 - and parsing date parts from a date field
18:28 - conditional statements using if-then and
18:31 - binning to create new variables and
18:33 - comparing values of two different fields
18:36 - to create a boolean variable formulas
18:39 - can be used for adding subtracting
18:40 - multiplying or dividing two numeric
18:43 - fields in this table we apply a formula
18:46 - to create the price column which is
18:48 - calculated by dividing units sold by
18:50 - total amount formulas can also be used
18:54 - when applying a mathematical function to
18:56 - a numeric field examples of mathematical
18:58 - functions are average floor finding the
19:01 - smallest value ceiling finding the
19:03 - largest value square square root
19:05 - absolute value trigonometric functions
19:08 - and logarithmic functions in this table
19:11 - we will create an observation average
19:13 - column using the average formula which
19:15 - averaged values from the observation 1
19:17 - and observation two columns formulas can
19:21 - also be used to extract and transform
19:23 - strings from a text field there are a
19:25 - few common functions for transforming
19:27 - text and they are typically named
19:28 - differently depending on the software
19:30 - package being used the most common of
19:32 - these are upper lower use to change the
19:34 - field to all upper or lowercase
19:36 - characters concatenate which combines
19:38 - two or more strings into one field
19:40 - substring used to extract a portion of a
19:43 - string trim which can be used to remove
19:45 - certain characters usually spaces from a
19:47 - field index used to find the location of
19:50 - a certain string within a field and
19:52 - length which finds how many characters
19:53 - are in a field these are the basic
19:55 - string functions that almost every
19:57 - software package will include and can be
19:59 - combined to complete almost any string
20:01 - transformation for the following example
20:03 - we'll use two functions to transform a
20:05 - field that holds a city and state into
20:07 - two fields to extract city and state
20:10 - from the airport location field we would
20:11 - need to extract all the text before the
20:13 - comma as the city
20:14 - field and everything after the comma and
20:16 - space as the state field to do this we
20:19 - need two functions index and substring
20:21 - the index function will help us identify
20:23 - the location of the comma in the text
20:25 - field counting from the left including
20:27 - spaces this is done because the comma is
20:29 - not always in the same location then we
20:31 - use the substring function to extract
20:33 - everything before the comma as the
20:35 - formula for the city field and extract
20:37 - everything after the comma and space for
20:39 - the state field to get the city and
20:41 - state columns back into the format of
20:43 - the original field we would use a
20:44 - concatenate function we can also use
20:46 - formulas for extracting truncating and
20:48 - parsing date parts from a date field a
20:51 - date or date/time field carries more
20:53 - information than a typical variable the
20:55 - time of day the day of week the day of
20:57 - month the month of year the year number
20:59 - and the difference in the date from
21:01 - other dates such as today extracting
21:03 - this and other information out of a date
21:05 - field is a transforming technique that
21:07 - can add to the value of a data set
21:09 - during analysis and reporting the most
21:11 - heavily used date functions are today
21:13 - used to get today's date date part used
21:16 - to get a part of a date for example
21:17 - getting the month of date will return
21:19 - the numeric value of the month date
21:21 - truncate which will return a date value
21:23 - at the beginning of a period specified
21:24 - for example truncating a date at the
21:27 - month level will return the first day of
21:28 - the month for the date date ad which
21:31 - will add or subtract a certain number of
21:33 - periods to a date and date difference
21:35 - used to calculate the number of periods
21:37 - between two dates this table contains
21:39 - the major holidays for 2016 for each
21:42 - date value we can extract other features
21:44 - we can add a column with a numeric value
21:46 - to describe the day of the week
21:48 - we can subtract today's date from the
21:50 - date column to find the number of days
21:52 - remaining until the holiday occurs with
21:54 - formulas we can also compare values of
21:56 - two different fields to create a boolean
21:58 - variable a boolean variable is typically
22:00 - used to capture true/false values to
22:03 - create a boolean variable a logical
22:05 - statement or condition can be used in a
22:07 - formula this logical statement can
22:09 - compare two values of the same type
22:11 - numeric values strings and dates this
22:14 - table depicts the population of States
22:16 - we can use a formula to create a column
22:19 - that displays whether the state is on
22:20 - the East Coast we can also create a
22:22 - column to display whether the state has
22:24 - a high population formulas can also be
22:26 - used with conditional statements that
22:28 - if then and when binning to create new
22:30 - variables to create a variable with
22:32 - multiple possible outcomes conditional
22:35 - statements are needed these usually take
22:37 - the form of if condition1 is met then
22:39 - outcome 1 else if condition 2 is met
22:43 - then outcome 2 else outcome 3 while the
22:47 - conditions within these statements are
22:49 - the same as boolean formulas these
22:51 - formulas offer more flexibility as the
22:53 - analyst can assign any value when the
22:55 - condition is met and can use as many
22:57 - conditions and outcomes as needed while
22:59 - there are many applications of
23:01 - conditional statements one of the most
23:02 - common is tube in or tile numeric
23:04 - variables a process which transforms the
23:07 - continuous numeric variables into
23:09 - categorical variables in this table we
23:11 - compute sales volume with if-then logic
23:13 - if sales is greater than 40000 then high
23:17 - else if sales is greater than 20,000
23:20 - then medium else low this formula begins
23:23 - with testing the first condition if
23:25 - sales is greater than 40,000 and if it
23:28 - is true the first outcome high will be
23:30 - assigned the second condition else if
23:33 - less than 20,000 is evaluated only if
23:36 - the first condition is false if the
23:38 - second condition is true the second
23:40 - outcome medium will be assigned if
23:42 - neither the first nor second conditions
23:45 - are true then the final outcome low will
23:47 - be assigned
23:50 - sometimes data from a separate row or
23:52 - rows needs to be used when creating
23:54 - variables for this a multi row formula
23:56 - is needed a running total is the most
23:58 - basic multirow formula it adds the value
24:01 - of field of a current row to the value
24:03 - of a previous row this can be done
24:05 - across an entire data set for a
24:07 - particular variable or it can be grouped
24:09 - by certain dimensions a lag value looks
24:12 - at the data in preceding rows while lead
24:14 - values look at data and subsequent rows
24:17 - and is the opposite of lag window
24:19 - functions provide the ability to perform
24:21 - calculations like sum average and rank
24:24 - across sets of rows that are related to
24:26 - the current query row this is equivalent
24:28 - to aggregating the data set across one
24:30 - or more dimensions then joining the
24:32 - resulting data set back to the original
24:34 - this should be used carefully as values
24:36 - will be repeated for all levels of the
24:38 - dimension an example of this is in the
24:40 - transportation industry where a row
24:42 - typically represents one leg of a trip
24:44 - after sorting the data set properly the
24:46 - lag function can be used to combine rows
24:48 - so that the data describing the round
24:50 - trip is displayed total price is a
24:52 - running total of ticket price grouped by
24:54 - passenger ID
24:55 - notice how upon reaching the first row
24:57 - for a passenger ID the sum starts over
24:59 - and total price equals ticket price
25:01 - total flight time is a running total of
25:04 - flight time grouped by passenger ID lag
25:06 - of destination uses the lag function
25:08 - which doesn't consider any variables
25:10 - other than destination this means it's
25:12 - dependent on the sort order of the data
25:14 - set sum of ticket price is a window
25:16 - function which is the sum of the ticket
25:18 - price partitioned or grouped by
25:19 - passenger ID this concludes our video on
25:23 - single and multi row formulas
25:27 - [Music]
25:45 - this video will cover unions and joins
25:49 - union is also referred to as appending
25:52 - concatenating or combining two tables
25:54 - vertically or simply adding rows the
25:57 - table should have the same variable
25:59 - names types and sizes variables that are
26:01 - in only one table will receive null or
26:04 - missing values for the rows from the
26:05 - tables that do not contain them in this
26:08 - union are two tables one with three rows
26:10 - and one with one row will combine to
26:12 - create one larger table which has 4 rows
26:14 - notice the null values in the storage
26:17 - column storage was not contained in both
26:19 - original tables so the rows that do not
26:21 - contain data for storage will have null
26:23 - or missing values
26:24 - joining is also referred to as merging
26:27 - very rarely as data collected and stored
26:29 - in a format that's ideal for analysis
26:31 - typically data is stored across multiple
26:33 - tables in a relational database schema
26:35 - like you see here schemas are designed
26:38 - to optimize for storage so values that
26:40 - repeat often and take up more storage
26:42 - such as names are reduced to
26:44 - representative IDs that take up much
26:46 - less storage in a schema the tables that
26:48 - contain identifying information are
26:50 - called dimension tables and can
26:51 - sometimes be referred to as look-up
26:53 - tables our diagram has three so for
26:55 - example the product dimension table
26:57 - contains the name manufacturer and type
27:00 - of product each dimension table has a
27:02 - primary key that is used to identify a
27:04 - unique record the fact table is a record
27:06 - of events that happen for a combination
27:08 - of dimensions it's comprised of two
27:10 - things foreign keys and measures foreign
27:12 - keys map to primary keys of dimension
27:14 - tables these are the fields that will be
27:16 - used to join the tables together for
27:18 - example in this diagram the foreign keys
27:20 - are person ID store ID and Product ID
27:23 - which each connect to their own
27:24 - dimension table measures can be any type
27:27 - of variable we've already discussed in
27:29 - this diagram we have two measures units
27:31 - sold and total amount to join these
27:33 - tables together we will use an inner
27:34 - join this will return only rows that are
27:37 - contained in both tables when a fact
27:39 - table has no lore missing values for a
27:41 - dimension using an inner join will
27:43 - remove the entire row since the row may
27:45 - contain useful data and other fields
27:47 - it's better to use a left outer join to
27:49 - keep all values from the left table in
27:51 - this case the fact table and the values
27:53 - from the dimension table that match on
27:55 - the key while there are other types of
27:56 - joins as shown here in
27:58 - and left outer joins are the most
27:59 - commonly used first we will join
28:01 - transaction fact with person dimension
28:06 - next we will join the results from the
28:08 - previous join with store dimension
28:12 - finally we will join our results from
28:14 - the previous join with product dimension
28:19 - notice how each step creates a
28:21 - progressively larger table this
28:24 - concludes our video on unions and joins
28:29 - [Music]
28:43 - this video will cover a definition of
28:46 - aggregation as well as examples of
28:48 - aggregation
28:50 - aggregation is also referred to as a
28:53 - PivotTable group by statement or
28:55 - summarize aggregation transforms data
28:58 - into lower dimensions using summing
29:00 - averaging and counting the benefits of
29:03 - this are to answer basic questions of
29:05 - data sets with many different dimensions
29:07 - the most basic aggregation can be done
29:09 - on the level of detail for the table in
29:11 - this case each rows a transaction and
29:14 - the table as a whole represents sales
29:16 - for phones for quarter 1 of 2015 the
29:19 - answers can be calculated by applying a
29:21 - formula to a single field using all the
29:23 - rows in the table how many units were
29:26 - sold 9 how much total revenue 3500 $90
29:38 - how many transactions six
29:43 - how many distinct products were sold
29:47 - three
29:53 - how many customers
29:57 - three
30:02 - how many stores had sales
30:09 - for
30:13 - what was the average price
30:19 - 398 dollars and 89 cents what was the
30:22 - average number of units sold per
30:24 - transaction
30:26 - one point five what was the average
30:30 - amount spent per transaction
30:35 - 598 dollars and 33 cents
30:40 - what was the average amount spent per
30:43 - customer
30:47 - 1196 dollars and 67 cents
30:52 - pay special attention to how the average
30:54 - function works it uses the sum of the
30:57 - field divided by the number of rows so
30:59 - it can't be used in averages such as
31:01 - this where the denominator is not the
31:03 - number of rows the next questions are
31:05 - related to aggregating one dimension at
31:07 - a time and can be calculated in one step
31:09 - for each of the questions the original
31:12 - table is grouped by the dimension of
31:13 - interest either the person ID store ID
31:17 - product ID or date for each value of the
31:21 - dimension chosen calculations are
31:23 - performed using the measures the number
31:25 - of rows and the number of distinct IDs a
31:28 - new data table is the result of these
31:30 - calculations
31:38 - you
31:41 - notice that when grouping by a dimension
31:43 - the dimension is sorted in the output
31:45 - for this example we'll aggregate at the
31:47 - person level of detail to answer the
31:49 - following questions how many units to
31:52 - each person by how much money did each
31:54 - person spend for each person ID we need
31:57 - to calculate the sum of units sold and
31:59 - the sum of the total amount
32:01 - starting with person ID 1 we see that
32:04 - there were 1 plus 1 equals 2 units sold
32:08 - and 365 plus 425 equals seven hundred
32:13 - and ninety dollars spent these values
32:15 - are populated in the resulting table
32:17 - similar calculations are done for every
32:20 - person ID in the original table
32:30 - you
32:33 - aggregating at the store level of detail
32:36 - can provide basic information about
32:37 - store performance to find the number of
32:40 - transactions each store had each
32:42 - occurrence of a store ID needs to be
32:44 - counted store ID 101 has two rows in the
32:47 - original table indicating that there
32:48 - were two total transactions each store
32:51 - ID has its number of rows counted and
32:53 - the final result is the num transactions
32:55 - column in the newly created table
32:58 - aggregating at the product level of
33:00 - detail can answer basic questions about
33:01 - each product such as what was the
33:04 - average price of each product how many
33:06 - distinct customers bought each product
33:09 - starting with product ID one zero zero
33:11 - one average price is calculated as the
33:14 - sum of the total amount
33:15 - 16:25 divided by the sum of units sold
33:18 - for the final value of 406 twenty five
33:22 - is populated into the resulting table
33:24 - again be aware that using an average
33:26 - function in this case would not give us
33:28 - the expected result distinct customers
33:31 - is calculated by counting the distinct
33:33 - number of person IDs for Product ID one
33:35 - zero zero one there are two person ID 1
33:38 - and person ID 2 the calculations are
33:41 - then completed for every product ID
33:49 - the final example uses the month and
33:51 - year components of the date field to
33:53 - calculate all possible levels of
33:54 - aggregation which were shown on the
33:56 - previous slides how many units were sold
33:59 - each month how much revenue each month
34:01 - how many transactions each month what
34:04 - was the average price how many distinct
34:07 - products were sold each month
34:16 - you
34:37 - the final type of questions that can be
34:39 - asked involve combinations of multiple
34:41 - dimensions one of the most common
34:43 - combinations is to aggregate by a date
34:45 - dimension along with another dimension
34:47 - for this calculation each combination of
34:49 - the dimensions chosen for grouping are
34:51 - performed in the transaction table there
34:54 - are four such combinations transactions
34:57 - for person ID one are only in March of
34:59 - 2015
35:03 - there's only one transaction for person
35:05 - ID to occurring in February 2015
35:10 - and there are three transactions for
35:12 - person ID for two occurring in January
35:15 - and one occurring in February of 2015
35:19 - within each of these combinations each
35:22 - of the calculations are performed here
35:24 - will show you the first row as an
35:26 - example
35:33 - you
35:41 - note that the more unique dimensions a
35:44 - data table contains the more the number
35:46 - of possible levels of aggregations
35:48 - increases in this example table with
35:51 - four unique dimensions there are
35:53 - fourteen different possible levels of
35:54 - aggregation each different level of
35:56 - aggregation can offer a unique insight
35:58 - but it's best to start with two or fewer
36:01 - levels example questions that can be
36:03 - answered using the example transaction
36:05 - data set include which customers tend to
36:08 - buy which products which stores two
36:10 - customers tend to frequent what products
36:12 - sell well in what stores and during what
36:14 - months do they sell more on average
36:17 - here's one final example using multiple
36:20 - concepts that have been covered using
36:22 - the transaction fact table we want to
36:23 - answer the question which stores had
36:25 - products that were on sale answering
36:27 - this question is a multi-step process
36:28 - the first step is to find the average
36:31 - price for each product for this we
36:33 - aggregate by product ID and for each
36:35 - Product ID we calculate the average
36:37 - price for example product one zero zero
36:39 - one sold for four hundred and twenty
36:41 - five dollars and four hundred dollars
36:43 - the average of these two values is
36:45 - calculated and then entered into the
36:47 - resulting table values for subsequent
36:49 - IDs are calculated in a similar manner
36:59 - you
37:04 - we also need to aggregate at the store
37:06 - and product level this removes the
37:08 - customer and date level information
37:09 - giving us the totals for each store and
37:11 - product combination due to the
37:15 - limitations of this example the
37:16 - aggregation does not result in fewer
37:18 - rows as it normally would the resulting
37:21 - table is however sorted by store ID and
37:24 - Product ID the next step is to join the
37:27 - product aggregate table to the store
37:29 - product aggregate table using product ID
37:31 - as the common field between the two
37:33 - tables this results in a table with a
37:36 - similar structure as the store product
37:38 - aggregate table with the only difference
37:40 - being the new column average product
37:43 - price we filter the resulting table to
37:46 - only include rows where price is less
37:48 - than average product price there are
37:50 - only two rows where this is true finally
37:53 - we join this table with the store
37:54 - dimension and product dimension tables
37:56 - adding store name and product name to
37:59 - the table is necessary to fully answer
38:01 - the original question the resulting
38:03 - table makes it clear that the g2 went on
38:06 - sale at Best Buy and the s6 went on sale
38:08 - at h/h Greg this example is a simplified
38:12 - an illustration of how data preparation
38:13 - techniques are often used together to
38:16 - answer typical questions of the data
38:17 - this concludes our video on aggregation
38:20 - today we covered a definition of
38:23 - aggregation and examples of aggregation
38:27 - [Music]
38:43 - this video will cover cross tabs and
38:46 - transposing cross tabs are also called
38:49 - pivot tables they are used to compare
38:52 - one measure across two or more
38:53 - dimensions one dimension will have its
38:56 - values transformed into columns and the
38:58 - other dimensions will be aggregated the
39:01 - visual result will be a table with fewer
39:03 - rows but more columns for example this
39:05 - table shows sales by region and month
39:08 - the data is stored in a form similar to
39:10 - how it would be captured with one row
39:12 - for every combination of region and
39:14 - month a crosstab function is performed
39:16 - with region defined as the grouping
39:18 - field month defined as the header field
39:20 - and sales defined as the data field
39:22 - there can be multiple grouping fields
39:24 - but only one header field and one data
39:26 - field are allowed cross tabs are an
39:28 - effective way to summarize and present
39:30 - data as trends within the data are
39:31 - easier to identify and data
39:33 - visualization color is added to build a
39:36 - heatmap cross tabs are also used in
39:38 - statistics to build contingency tables
39:41 - transposing can be thought of as the
39:43 - opposite of cross tabs it transforms the
39:45 - data from a wide format into a narrow
39:47 - format typically transposing is required
39:50 - after receiving financial data from a
39:52 - spreadsheet as columns can be used to
39:54 - capture dates locations or categories
39:56 - once the columns known as data fields
39:58 - are transposed into one variable the
40:00 - data is ready for a more sophisticated
40:02 - analysis any fields that are not to be
40:04 - transposed are called key fields for
40:07 - this example we can use the result of
40:09 - the crosstab function from the previous
40:10 - section region is the only key field
40:13 - while January February and March are the
40:15 - data fields to be transposed transposing
40:18 - results in the original data set notice
40:21 - how the column names are transformed
40:22 - into values for the newly created month
40:24 - column
40:26 - this concludes our video on cross tabs
40:28 - and transposing
40:31 - [Music]
40:47 - this video will cover contingency tables
40:50 - first we will discuss the definition of
40:53 - a contingency table and then the steps
40:55 - for creating one finally we will discuss
40:57 - chi-square distributions in statistics a
41:01 - contingency table is a type of table in
41:04 - a matrix format that displays the
41:05 - multivariate frequency distribution of
41:08 - variables contingency tables are heavily
41:10 - used in Survey Research Business
41:13 - Intelligence engineering and scientific
41:16 - research they provide a basic picture of
41:19 - the inter relation between two or more
41:21 - variables and can help find interactions
41:23 - a contingency table is also referred to
41:26 - as a two-way frequency table here's an
41:28 - example given this table can you
41:31 - calculate the following metrics the
41:33 - number of males who are right-handed the
41:36 - percent of males who are left-handed
41:38 - whether more males are left-handed than
41:40 - females or the percent of left-handed
41:43 - people who are females in a table such
41:46 - as this notice that there are no
41:48 - numerical values the person ID is a
41:50 - numerical identifier but the numbers are
41:52 - arbitrary so there are no obvious
41:54 - calculations to perform however because
41:57 - each row represents one person we can
41:59 - count the number of rows along each
42:01 - dimension gender and dominant hand to
42:03 - analyze how the dimensions are
42:05 - distributed to create a contingency
42:07 - table the first step is to aggregate the
42:09 - original data set along two dimensions
42:11 - gender and handedness and count the
42:13 - number of rows for each combination of
42:15 - values since there are two possible
42:18 - values for gender male and female and
42:20 - two possible values for dominant hand
42:22 - left and right there are four total
42:24 - possible combinations male and right
42:27 - male and left female and right and
42:29 - female and left for each of these
42:32 - combinations we count the number of rows
42:34 - that contain both values at this point
42:37 - we can extract information regarding the
42:39 - quantity of each combination allowing us
42:42 - to answer basic questions such as how
42:44 - many females are left-handed and are
42:46 - there more males or females who are
42:48 - left-handed next we perform a crosstab
42:50 - on the result moving the dominant hand
42:52 - to the horizontal axis this will make
42:55 - interpretation of the results much
42:57 - easier and allow us to easily calculate
42:59 - totals
43:00 - each dimension there are other questions
43:01 - however that are harder to answer and
43:04 - these deal with the proportions of each
43:05 - combination what proportion are percent
43:08 - of males are left-handed and are a
43:10 - greater proportion of males left-handed
43:12 - than the proportion of females for these
43:15 - questions the proportion of gender we
43:17 - need to divide the values in the cells
43:20 - by the totals along the gender axis
43:22 - which in this case is the vertical axis
43:24 - the resulting table called a row
43:26 - conditional frequency table will have
43:28 - 100% values in each cell in the total
43:31 - column the cells for each row will
43:34 - account for all the people within the
43:35 - category male and female of the gender
43:37 - dimension the table can be easily read
43:41 - eighty-three percent of males are
43:42 - right-handed and only 8 percent of
43:45 - females are left-handed but what if we
43:49 - want to know what percent of left-handed
43:51 - people are female this is a different
43:53 - type of question one that can be
43:54 - answered by dividing the original values
43:56 - of the cells by the totals along the
43:59 - dominant hand or horizontal axis of the
44:01 - contingency table these are called
44:03 - column relative frequencies now the
44:06 - results can be interpreted in two
44:08 - sentences by focusing on the columns in
44:10 - the contingency table 49% of
44:12 - right-handed people are males 31% of
44:15 - left-handed people are females etc but
44:19 - what if we want to answer questions
44:20 - about how prevalent each separate
44:22 - combination is among all people in this
44:24 - data set for instance what percent of
44:26 - the people are left-handed males for
44:29 - that we divide the original values by
44:31 - the total number of rows in the data set
44:33 - for our example this makes for an easy
44:35 - calculation since there were 100 rows
44:38 - with this table we can answer many basic
44:41 - questions about the data set including
44:42 - what percent of the people in the data
44:44 - set are right-handed what percent of the
44:47 - people in the data set are female and
44:48 - what percent of the people on the data
44:50 - set are left-handed females and so forth
44:54 - note that the language used what percent
44:57 - of people in the data set is different
44:58 - from the easier to say what percent of
45:00 - people this is because the data set
45:03 - being used is a sample of the population
45:05 - and to infer the trends about the
45:07 - population will need to use a
45:09 - statistical technique for a contingency
45:12 - table a chi
45:13 - square distribution can be used to make
45:15 - such an inference to do so however
45:17 - assumes that the sample you are using
45:19 - was acquired from the population
45:21 - randomly
45:23 - the chi-square distribution compares the
45:25 - actual values to the expected values to
45:28 - determine if the actual numbers that
45:30 - were observed and recorded in the data
45:31 - set are due to chance or if there's a
45:34 - difference between the two variables
45:36 - that cannot be explained by chance in
45:37 - this example we want to determine if the
45:40 - observed difference in the proportion of
45:42 - females who are left-handed is really
45:44 - smaller than the proportion of males who
45:46 - are left-handed or if that observation
45:48 - could be due to chance in other words we
45:51 - want to know if the dominant hand is
45:53 - dependent on gender to calculate the
45:55 - expected values for each cell multiply
45:57 - the relative horizontal and vertical
45:59 - dimension totals and divide by the
46:01 - number of total observations in this
46:04 - example to calculate the expected value
46:06 - for right-handed males multiply 52 the
46:09 - total number of males by 87 the total
46:12 - number of right-handed people and divide
46:15 - by 100 the total number of people
46:17 - similar calculations are done for each
46:19 - cell once the expected values have been
46:23 - computed the chi-square test can be run
46:25 - in Excel the chi-square test function
46:28 - uses the actual table and expected table
46:31 - as inputs to calculate the p-value this
46:34 - p-value is the probability that these
46:36 - results did not occur due to chance a
46:39 - common way to evaluate the p-value is to
46:42 - compare it to 0.05 if the p-value is
46:46 - less than 0.05 we say there is an
46:49 - association between the two variables
46:51 - that is statistically significant in
46:53 - this example our p-value is 0.18 which
46:57 - is greater than 0.05 so we conclude that
47:00 - the dominant hand variable is
47:02 - independent of gender if there is an
47:05 - association between two variables
47:06 - completing the calculation for the
47:08 - chi-square test statistic can be used to
47:11 - find which values contribute the most to
47:14 - the Association this can be calculated
47:16 - using this formula for each cell
47:20 - subtract the expected value from the
47:22 - observed and square the result then
47:25 - divide the answer by the expected value
47:27 - each of these results is summed
47:29 - indicated by the Greek letter Sigma this
47:32 - value is compared to the chi-square
47:34 - distribution
47:36 - within each cell the calculation
47:37 - describes how far the actual value is
47:40 - from the expected value in this example
47:42 - the calculations for left-handed people
47:44 - are much greater than those for
47:46 - right-handed people these cells have the
47:49 - greatest impact on the potential
47:50 - association between the two variables
47:53 - summing those values results in
47:55 - chi-square test statistic of 1.78 using
47:59 - this value in a chi-square distribution
48:01 - along with the degrees of freedom based
48:03 - on the number of values for each
48:05 - variable and a significance level such
48:07 - as 0.05 is how the p-value is obtained
48:11 - this concludes our video on contingency
48:14 - tables today we defined contingency
48:16 - tables and then we discussed the steps
48:18 - for creating one and finally we covered
48:21 - chi-square distributions
48:24 - [Music]
48:42 - this video will first define
48:45 - distribution then we'll cover measures
48:47 - of distribution the mean median outliers
48:52 - mode minimum and maximum values and
48:56 - quantiles the most common method of
48:59 - analyzing a numeric variable is by
49:01 - exploring how the values are distributed
49:03 - the distribution of a numeric variable
49:05 - shows all the possible values and how
49:07 - often they occur a distribution provides
49:10 - methods in which many records of data
49:12 - can be summarized to provide basic
49:14 - information about the variable these
49:16 - methods can either be numerical measures
49:18 - or visualizations we'll explore the most
49:20 - popular methods using this data set of
49:22 - 10 rows of bank teller salary data
49:24 - because the sample data set is so small
49:27 - we can make some quick observations
49:28 - about the salary variable the lowest
49:31 - salary is twenty eight thousand six
49:33 - hundred and sixty-five dollars this is
49:34 - referred to as the minimum value the
49:36 - highest salary is forty four thousand
49:39 - and twenty dollars the maximum value
49:41 - finally many of the salaries are in the
49:43 - low $30,000 range we'll begin by
49:46 - defining and calculating the salary
49:48 - variables measures of distribution mean
49:51 - median outliers mode minimum and maximum
49:56 - values in quantiles to find the mean or
49:59 - average add up all the numbers and
50:01 - divide by the number of rows 340 one
50:04 - thousand eight hundred and sixty dollars
50:06 - divided by 10 equals a mean of thirty
50:08 - four thousand 186 dollars to find the
50:12 - median sort the numbers and find the
50:14 - middle value if there are an odd number
50:16 - of rows there is one middle value if
50:18 - there are an even number of rows there
50:20 - are two middle values and the median
50:22 - will be the average of these two values
50:23 - here we sort the table by salary then
50:26 - find the average of the two middle
50:27 - values by adding thirty three thousand
50:30 - nine hundred eighty dollars to thirty
50:31 - four thousand eight hundred fifty
50:33 - dollars and dividing by 2 to get thirty
50:35 - four thousand one hundred and forty-five
50:37 - dollars values that fall outside of the
50:40 - normal range of the rest of the
50:41 - observations are called outliers in our
50:44 - example the value of forty four thousand
50:46 - twenty dollars is an outlier from the
50:47 - rest of the values
50:49 - the mode is the most commonly occurring
50:51 - value in our sample data set no values
50:53 - occur more than once
50:54 - so there is no mode as we've already
50:56 - noticed the minimum and maximum values
50:58 - are twenty thousand six hundred and
51:00 - sixty five and forty four thousand and
51:03 - twenty dollars a quantile is a set of
51:05 - values that divide a frequency
51:07 - distribution in two equal groups each
51:08 - containing the same fraction of the
51:10 - total population to find quantiles
51:12 - divide the distribution into groups of
51:14 - equal size with each group containing
51:16 - about the same number of rows the most
51:18 - simple of quantiles has already been
51:19 - calculated the median divides the
51:21 - records into two groups of five the
51:24 - following quantiles are most often used
51:26 - tersh aisles three groups quartiles four
51:29 - groups quintiles five groups deciles ten
51:32 - groups and percentiles 100 groups let's
51:35 - look at quartiles for our example
51:38 - the median is commonly referred to as q2
51:41 - since it is the second quartile q1 and
51:44 - q3 are also used often to calculate
51:47 - quartile 1 we can look at the middle
51:48 - value of the first 5 records sorted in
51:50 - order which is 30 $1,300 and then look
51:54 - at the middle value of the last 5
51:56 - records for quartile 3 which is 30
51:59 - $5,100 these values along with our
52:01 - median are our quartile values this
52:04 - concludes our video on measures of
52:06 - distribution today we define
52:08 - distribution and then we discuss the
52:10 - measures of distribution including mean
52:12 - median outliers mode minimum and maximum
52:17 - values and quantiles
52:21 - [Music]
52:34 - this video will cover variation
52:36 - including a definition of variation and
52:39 - the measures of variation range inter
52:42 - quartile range variance standard
52:45 - deviation and standardization as well as
52:48 - testing differences between means
52:50 - variation refers to how spread out the
52:53 - values are for a variable interpreting
52:55 - variation that is explaining a variables
52:57 - variation in reference to other
52:59 - variables is a foundational task in
53:01 - Business Analytics variables that have
53:04 - values that are spread out have higher
53:06 - variation while variables with values
53:08 - very close to the mean have lower
53:09 - variation the following measures help us
53:12 - understand how the values for a variable
53:14 - are spread out or on the mean range
53:17 - interquartile range
53:19 - variance standard deviation and
53:22 - standardization using an example of
53:25 - salaries by gender a question arises do
53:28 - men make more than women certainly some
53:30 - men make more than women but is this
53:32 - true for the group as a whole using the
53:34 - principles of variation will help us to
53:36 - answer this question the range is the
53:39 - largest number minus the smallest number
53:41 - to find the range we first sort from
53:43 - smallest to largest then we subtract the
53:46 - smallest value from the largest value
53:53 - to find the interquartile range first
53:56 - separate the data into quartiles and
53:58 - then subtract q1 from q3 to find the
54:01 - variance we first calculate the mean or
54:04 - average
54:10 - then we subtract the mean from each
54:12 - value and square the result by squaring
54:15 - the differences we remove the
54:16 - possibility of negative values
54:18 - cancelling out the positive values next
54:20 - find the average of the squared
54:22 - differences
54:28 - variance is not a very useful measure
54:30 - the value we got for variance is much
54:32 - different from the range of salaries
54:33 - that are variable captures to get it
54:36 - back into the correct scale we take the
54:37 - square root of the variance this results
54:39 - in the standard deviation this value is
54:42 - used relative to the mean to determine
54:44 - which values are within the normal
54:45 - variation of the salary variable
54:47 - subtract the standard deviation from the
54:50 - mean to find the lower threshold add the
54:52 - standard deviation to the mean to find
54:54 - the upper threshold using the standard
54:57 - deviation gives us a way to determine
54:59 - which values are within the normal
55:01 - variation and which values are either
55:02 - less than normal or greater than normal
55:04 - in other words it gives us the ability
55:06 - to identify outliers more easily in our
55:09 - example we can identify that the
55:11 - smallest two salaries twenty-eight
55:13 - thousand six hundred and sixty-five
55:14 - dollars and twenty-nine thousand five
55:16 - hundred dollars and the largest salary
55:19 - forty-four thousand twenty dollars are
55:21 - outside one standard deviation from the
55:23 - mean
55:25 - mean and standard deviation are measures
55:28 - that describe the distribution of a set
55:30 - of numbers but what if we want to
55:31 - describe the numbers within the set to
55:33 - compare two numbers within an entirely
55:35 - different set we can do this by
55:37 - standardizing the values in statistics
55:39 - this measure is called the z-score to
55:42 - standardize a value subtract the mean
55:43 - and divide by the standard deviation for
55:46 - example in the first row of our data set
55:48 - since we've already calculated this
55:50 - salary - mean column all we need to do
55:52 - is divide by 4103
55:54 - to get the result of negative 1.1 for
55:57 - this value can be interpreted as the
56:00 - value of $29,500 is 1.1 for standard
56:04 - deviations lower than the mean looking
56:07 - at the rest of the values we see that
56:09 - the largest salary forty four thousand
56:10 - and twenty dollars is actually very far
56:12 - away from the mean this must be a high
56:14 - performing bank teller we can use this
56:16 - standardized value to compare the high
56:18 - performing bank teller to the high
56:20 - performer of another profession
56:22 - let's return to the question post at the
56:24 - beginning do men make more than women to
56:27 - answer this question we can use a
56:28 - statistical test called a t-test this
56:31 - test requires that we make three
56:33 - assumptions about our data which for now
56:34 - we will assume to be true the two
56:37 - populations have the same variance the
56:39 - populations are normally distributed
56:41 - each value is sampled independently from
56:44 - each other value by considering the
56:47 - values of salary separately we can see
56:49 - that females have the lowest value and
56:51 - the highest value so the variation must
56:52 - be higher calculating the mean and
56:55 - standard deviation for each group we see
56:57 - that the mean is almost the same yet the
56:59 - standard deviation for females is more
57:01 - than twice that for males using these
57:03 - values we can calculate the t-statistic
57:05 - and corresponding p-value but in Excel
57:08 - we only need the original values in
57:10 - different columns using the t-test
57:13 - function we get a p-value of 0.44 since
57:16 - this value is much higher than point
57:18 - zero five we conclude that the two
57:20 - distributions are not statistically
57:21 - significant this is mainly due to the
57:24 - fact that our sample size for this
57:25 - example is very small and also because
57:28 - the means are almost equivalent
57:30 - this concludes our video on measures of
57:32 - variation today we defined variation and
57:35 - then discussed range enter quartile
57:37 - range variance standard deviation
57:40 - standardization and testing differences
57:42 - between the means
57:44 - [Music]
57:56 - this video will cover distribution
57:59 - visualizations including buckets
58:02 - histograms and an introduction to area
58:05 - line graphs we begin with a data set
58:08 - that contains gender dominant hand and
58:10 - salary we've calculated a number of
58:12 - different measures including the mean
58:14 - median minimum maximum range
58:18 - interquartile range variance standard
58:21 - deviation and a list of z-scores next
58:24 - we'll build a histogram which is the
58:26 - most common way to visualize a numeric
58:27 - distribution to build a distribution
58:29 - will group the data into buckets or bins
58:31 - of equal size this will effectively
58:33 - transform the variable into a
58:35 - categorical variable allowing us to
58:37 - count the occurrences of each bucket
58:39 - next we determine the size of each
58:41 - bucket first we locate our minimum and
58:43 - maximum values
58:46 - then we subtract the minimum from the
58:48 - maximum and divide by the number of
58:50 - buckets this gives us a value of one
58:52 - thousand five hundred and thirty six
58:53 - dollars next we need to figure out the
58:56 - starting value for each bucket starting
58:58 - with the lowest salary we add the size
59:00 - of the bucket to determine the starting
59:01 - value of the second bucket we do this
59:03 - until we obtain the starting values for
59:05 - all ten buckets
59:07 - we'll add a new column to the original
59:09 - table to assign each person into a
59:11 - salary bucket summarize the new variable
59:14 - starting value of salary bucket counting
59:17 - how many occurrences of each bucket are
59:18 - observed note that although this
59:20 - variable is numeric by summarizing it
59:22 - we're treating it as a categorical
59:23 - variable finally we'll make a simple bar
59:26 - chart to visualize this table this bar
59:29 - chart is known as a histogram and it's
59:30 - one of the most common methods of
59:32 - visualizing a numeric distribution
59:33 - although many software packages can
59:35 - produce a histogram very quickly as
59:37 - we've seen creating one involves quite a
59:39 - few steps if we change the visualization
59:42 - to an area line graph and change the
59:44 - vertical axis to the percent of records
59:46 - by dividing the count by the total
59:48 - number of records the result is similar
59:50 - to what is known as a probability
59:51 - density function or PDF this is commonly
59:55 - used in statistics to estimate the
59:56 - probability of a new value for now we
59:59 - just need to know that the shaded area
60:00 - under the curve adds up to 1 or 100% and
60:03 - that the lines are typically much smooth
60:05 - and we see in this example which is
60:07 - because most PDFs visualize more than 10
60:09 - data points and use more than 10 buckets
60:12 - this concludes our video on distribution
60:14 - visualizations today we covered buckets
60:17 - histograms as well as a brief
60:19 - introduction to the use of area line
60:21 - graphs
60:23 - [Music]
60:36 - this video will cover normal
60:38 - distributions continuous distributions
60:42 - density functions cumulative
60:44 - distribution functions and the 6895 99.7
60:49 - rule the single most important
60:51 - distribution in statistics is the normal
60:53 - distribution it's a continuous
60:55 - distribution and it's the basis of the
60:57 - familiar symmetric bell-shaped curve the
60:59 - mean of the normal distribution is in
61:01 - the center the standard deviations are
61:03 - marked at equal distances from the mean
61:05 - any particular normal distribution is
61:08 - specified by its mean and standard
61:09 - deviation by changing the mean the
61:11 - normal curve shifts to the left or right
61:19 - by changing how spread out the standard
61:21 - deviations are the curve also changes
61:23 - standard deviations can be spread out
61:25 - wider or closer together
61:28 - therefore there are really many normal
61:30 - distributions not just a single one the
61:32 - normal distribution is a two parameter
61:34 - family where the two parameters are the
61:36 - mean and standard deviation here's a
61:38 - tool you can play with online that
61:40 - illustrates a normal distribution in
61:42 - real life it looks like a triangular
61:43 - shaped pegboard into which balls are
61:45 - dropped when there's an equal
61:47 - probability that the balls will drop
61:48 - either left or right their final
61:49 - placement forms a normal distribution
61:51 - however when the probability of the
61:53 - balls dropping left or right is unequal
61:55 - which is something you can experiment
61:56 - with using this tool the distribution
61:58 - changes the formulas for mean and
62:01 - standard deviation are very complex but
62:03 - you will not have to compute them
62:04 - because the software will with
62:06 - continuous variables there is a
62:08 - continuum of possible values such as all
62:10 - values between 0 and 100 or all values
62:13 - greater than 0 instead of assigning
62:15 - probabilities to each individual value
62:17 - in the continuum the total probability
62:19 - of one is spread over the continuum thus
62:21 - the shaded area within the bell curve
62:23 - will always have an area of 1 the key to
62:26 - the spreading is called a density
62:27 - function which acts like a histogram the
62:30 - higher the value of the density function
62:31 - the more likely this region of the
62:33 - continuum is a density function usually
62:37 - denoted by FX specifies the probability
62:40 - distribution of a continuous random
62:41 - variable X the higher FX is the more
62:44 - likely X is probabilities are found from
62:47 - a density function as areas under the
62:49 - curve so for example the shaded portion
62:51 - under the spell curve represents the
62:53 - probability of X being between 65 and 75
62:56 - the cumulative distribution function or
62:59 - CDF is the probability that the variable
63:01 - takes a value less than or equal to X
63:03 - it's the total area under the normal
63:05 - curve up to X the beauty of the normal
63:08 - curve is that no matter what its mean
63:09 - and standard deviation are the area
63:11 - between the mean minus 1 standard
63:13 - deviation and the mean plus 1 standard
63:15 - deviation is always about 68% the area
63:18 - between the mean minus 2 standard
63:20 - deviations and the mean plus two
63:22 - standard deviations is always about 95%
63:26 - the area between the mean minus three
63:28 - standard deviations and the mean plus
63:30 - three standard deviations is always
63:31 - about ninety-nine point seven percent
63:33 - that means almost all values fall within
63:35 - three standard deviations on either side
63:37 - of the mean that's true for all normal
63:39 - curves no matter their shape but how
63:41 - good is this rule for real data let's go
63:44 - ahead and check out an example here's
63:45 - our data the mean of the weight of one
63:47 - hundred and twenty women runners in a
63:49 - sample is one twenty seven point eight
63:50 - pounds the standard deviation is fifteen
63:53 - point five here's what our distribution
63:55 - would look like let's look a little more
63:56 - closely at that distribution 68% of our
63:59 - 120 runners is about eighty three
64:02 - runners according to the 68 95 99 point
64:05 - seven rule those runners should fall
64:07 - fall within one standard deviation of
64:08 - the mean weight of one hundred and
64:10 - twenty seven point eight pounds that is
64:13 - eighty-three of our runners should fall
64:14 - between one hundred and twelve point
64:16 - three and one forty three point three
64:17 - pounds when we check our data we see
64:19 - that seventy-nine runners fall within
64:21 - one standard deviation of the mean
64:23 - furthermore ninety-five percent of our
64:25 - group or about 114 runners should fall
64:28 - within two standard deviations of the
64:29 - mean or between ninety six point eight
64:31 - and one hundred and fifty eight point
64:33 - eight pounds the data shows that 115
64:35 - runners fall within two standard
64:37 - deviations of the mean
64:39 - finally according to the rule
64:41 - ninety-nine point seven percent of our
64:43 - runners or one hundred nineteen point
64:45 - six runners should fall within three
64:46 - standard deviations of the mean or
64:48 - within a range of eighty one point three
64:50 - pounds to one hundred and seventy four
64:52 - point three pounds according to our data
64:54 - all 120 runners fall within this range
64:56 - so it seems as if the rule is pretty
64:58 - accurate in this case this concludes our
65:01 - video on normal distributions continuous
65:04 - distributions density functions and
65:06 - cumulative distribution functions as
65:08 - well as the 6895 99.7 rule
65:13 - [Music]
65:24 - this video will cover kurtosis including
65:27 - a definition as well as positive and
65:29 - negative kurtosis and asymmetrical
65:31 - distributions including those of
65:33 - positive and negative skew kurtosis is
65:36 - the measure that describes the size of
65:38 - the tails in a distribution a
65:39 - distribution with positive kurtosis
65:41 - contains fewer values in the tails than
65:44 - a normal distribution a distribution
65:46 - with more values in the tails has
65:48 - negative kurtosis the normal
65:50 - distribution is a type of symmetrical
65:51 - distribution in which the mean is equal
65:53 - to the median and there is an equal
65:55 - probability of a value falling on either
65:57 - side of the mean although normal
65:59 - distributions and other types of
66:01 - symmetrical distributions are very
66:02 - common there are often distributions
66:04 - that are asymmetrical we call these
66:06 - types of distributions skewed for a
66:08 - negatively skewed distribution the left
66:10 - tail is longer and the mean is less than
66:12 - the median this is due to the occurrence
66:14 - of outliers at the lower end of the
66:15 - distribution away from the most
66:17 - frequently occurring values a good
66:19 - example of this is the height of NBA
66:21 - players since taller basketball players
66:23 - have an advantage the majority of NBA
66:25 - players are very tall for a positively
66:27 - skewed distribution the right tail is
66:29 - longer as there are outliers with larger
66:31 - numbers these outliers cause the mean to
66:33 - be greater than the median an example of
66:35 - a positively skewed distribution is in
66:37 - the salary of baseball players there are
66:39 - a few star players who make much more
66:41 - than the majority of players these high
66:43 - salaries are outliers that make the mean
66:45 - of the distribution increase for
66:47 - distributions that are skewed both
66:49 - positively and negatively the meeting is
66:51 - a better representation of central
66:52 - tendency than the mean as the outliers
66:54 - will impact the calculation of the mean
66:56 - the median is not affected by outliers
66:58 - and typically is a much better
67:00 - approximation for the middle of a
67:01 - distribution this concludes our video on
67:04 - kurtosis today we defined kurtosis and
67:06 - discussed positive and negative kurtosis
67:08 - we also covered asymmetrical
67:10 - distributions including those of
67:12 - positive and negative skew
67:15 - [Music]
67:31 - this video will cover sampling basics
67:33 - including populations and inferences
67:35 - selecting a sample random sampling
67:39 - stratified sampling and cluster sampling
67:42 - a population is a set of all members
67:44 - about which is study intends to make
67:46 - inferences here's a population of people
67:49 - we'd like to study their television
67:50 - watching behavior and infer how many
67:52 - watch a particular shows so we can
67:53 - decide whether to purchase advertising
67:55 - spots during this period of time but our
67:57 - population is much too large to study
67:59 - effectively to study their television
68:01 - viewing habits we'll have to survey them
68:03 - and it's not feasible to survey every
68:05 - single individual
68:06 - so we'll take a sample of the population
68:07 - to study the sample will represent the
68:09 - population as a whole and so will it
68:11 - survey results if we choose our sample
68:13 - correctly we will focus on selecting a
68:15 - sample using probability a probability
68:17 - sample is chosen from a population using
68:20 - a random mechanism there are two types
68:22 - of probability sampling stratified and
68:24 - cluster a random sample is only random
68:27 - if each individual has the same chance
68:28 - of being chosen from the population so
68:31 - back to our television viewing research
68:32 - we want to figure out how many
68:33 - television viewers there might be during
68:35 - a particular so let's say there are
68:37 - 30,000 viewers in our population each
68:39 - viewer is known as a unit in order to
68:43 - select a sample n of viewers from this
68:45 - population of 30,000 we could choose to
68:47 - use a simple random sample this means
68:49 - that there is an equal probability that
68:51 - each viewer could be selected for
68:53 - inclusion in the sample if our desired
68:55 - sample size was 200.final echt 200
68:58 - viewers randomly and then we could send
68:59 - each of those viewers a questionnaire in
69:01 - the mail about their viewing habits but
69:03 - suppose various subpopulations within
69:05 - the total population can be identified
69:07 - these populations are called strata
69:09 - instead of taking a random sample from
69:11 - the entire population we might get
69:13 - better information by selecting a simple
69:15 - random sample from each stratum
69:16 - separately this is called stratified
69:18 - sampling examples of subpopulations and
69:21 - television viewers might include age or
69:23 - gender there are several advantages to
69:25 - stratified sampling one obvious
69:27 - advantage is that separate estimates can
69:29 - be obtained within each stratum which
69:31 - would not be obtained with a single
69:32 - random sample from the entire population
69:34 - for example let's say we're looking at
69:36 - our television viewers by age group we
69:38 - have three strata 18 to 24 25 to 39 and
69:42 - 40 plus we find that their peak
69:44 - times vary based on age thus we can make
69:47 - better decisions about which product to
69:48 - advertise during which time period a
69:50 - more important advantage of stratified
69:52 - sampling is that the accuracy of the
69:54 - resulting population estimates can be
69:55 - increased by using appropriately defined
69:58 - strata in cluster sampling the
70:00 - population is separated into clusters
70:02 - such as regions of the country and then
70:04 - a random sample of the clusters is
70:05 - selected the primary advantage of
70:07 - cluster sampling is sampling convenience
70:09 - and possibly lower cost selecting a
70:11 - cluster sample is straightforward the
70:13 - key is to define the sampling units as
70:15 - the clusters such as the regions of the
70:17 - continental US shown here this concludes
70:19 - our video on sampling basics today we
70:21 - covered populations and inferences
70:23 - selecting a sample random sampling
70:25 - stratified sampling and cluster sampling
70:30 - [Music]
70:43 - this video will cover bivariate data
70:47 - scatter plots and null values measures
70:51 - of central tendency variability and
70:53 - spread summarize a single variable by
70:55 - providing important information about
70:56 - its distribution often more than one
70:58 - variable is collected on each individual
71:00 - for example in large health studies of
71:02 - populations it's common to obtain
71:04 - variables such as age sex height weight
71:07 - blood pressure and total cholesterol in
71:09 - each individual economic studies may be
71:11 - interested in among other things
71:13 - personal income and years of education
71:15 - as a third example most university
71:17 - admissions committees ask for an
71:19 - applicant's high school grade point
71:20 - average in standardized admission test
71:22 - scores like the SAT bivariate data
71:24 - consists of two quantitative variables
71:26 - for each individual in contrast with
71:28 - univariate or single variable data our
71:31 - first interest is in summarizing such
71:33 - data in a way that's analogous to
71:35 - summarizing univariate data by way of
71:37 - illustration let's consider something
71:39 - with which we're all familiar age let's
71:41 - begin by asking if people tend to marry
71:43 - other people of about the same age our
71:45 - experience tells us yes but how good is
71:47 - the correspondence one way to address
71:49 - the question is to look at pairs of Ages
71:51 - for a sample of married couples table
71:54 - one shows the ages of 10 married couples
71:56 - going across the columns we see that yes
71:58 - husbands and wives tend to be of about
72:00 - the same age with men having a tendency
72:02 - to be slightly older than their wives
72:04 - this is no big surprise but at least the
72:06 - data bear out our experiences which is
72:08 - not always the case the pairs of Ages in
72:10 - table 1 are from a data set consisting
72:12 - of two hundred and eighty two pairs of
72:14 - spousal ages too many to make sense of
72:17 - from a table what we need is a way to
72:18 - summarize the two hundred and eighty two
72:20 - pairs of ages we know that each variable
72:23 - can be summarized by a histogram which
72:25 - is a graphical representation of a
72:26 - distribution a histogram partitions the
72:28 - variable on the x axis into various
72:31 - contiguous class intervals of usually
72:33 - equal widths the heights of the bars
72:35 - represent the class frequencies here we
72:37 - can see that each distribution is fairly
72:39 - skewed with a long right tail we can
72:42 - also summarize the variables with a mean
72:44 - and standard deviation
72:46 - from table 1 we can see that not all
72:48 - husband's are older than their wives and
72:50 - it's important to see that this fact is
72:51 - lost when we separate the variables that
72:54 - is even though we provide summary
72:55 - statistics on each variable the pairing
72:57 - within the couple is lost by separating
72:59 - the variables we cannot say for example
73:02 - based on means alone what percentage of
73:04 - couples has younger husbands and wives
73:06 - we have to count across the pair's to
73:08 - find this out only by maintaining the
73:10 - pairing can meaningful answers be found
73:12 - about the couples another example of
73:15 - information not available from the
73:16 - separate descriptions of husbands and
73:18 - wives ages is the mean age of husbands
73:21 - with wives of a certain age
73:22 - for instance what is the average age of
73:24 - husbands with 45 year old wives finally
73:27 - we don't know the relationship between
73:29 - the husband's age and the wife's age we
73:32 - can learn much more by displaying the
73:33 - bivariate data in a graphical form that
73:36 - maintains the pairing figure to shows a
73:38 - scatter plot of the paired ages the
73:40 - x-axis represents the age of the husband
73:42 - and the y-axis the age of the wife there
73:45 - are two important characteristics of the
73:46 - data revealed by figure two first it's
73:49 - clear that there's a strong relationship
73:50 - between the husband's age and the wife's
73:52 - age the older the husband the older the
73:55 - wife when one variable Y increases with
73:58 - the second variable X we say that X and
74:00 - y have a positive association conversely
74:03 - when Y decreases as x increases we say
74:06 - that they have a negative association
74:08 - second the points cluster along a
74:11 - straight line when this occurs the
74:12 - relationship is called a linear
74:14 - relationship figure 3 shows a
74:16 - scatterplot of arm strength and grip
74:18 - strength from 149 individuals working in
74:21 - physically demanding jobs including
74:23 - electricians construction maintenance
74:25 - workers and auto mechanics not
74:27 - surprisingly the stronger someone's grip
74:28 - the stronger their arm tends to be there
74:30 - is therefore a positive association
74:32 - between these variables
74:33 - although the points cluster along a line
74:36 - they're not clustered quite as closely
74:37 - as they are for the scatterplot of
74:39 - spousal age a common problem when
74:41 - working with real-world data is the
74:43 - presence of missing or null values
74:45 - within a data set there are three
74:46 - strategies to deal with the issue the
74:48 - first one is to omit the rows if the
74:51 - variable is very important to the
74:52 - analysis and there are not many
74:54 - observations with missing values it can
74:56 - be acceptable to filter or delete those
74:58 - rows
74:59 - the second is to treat missing as a
75:01 - separate category if the variable is
75:03 - categorical this is easy if the variable
75:06 - is numeric then the variable will need
75:07 - to be binned and a category created for
75:09 - the missing rows the third is to impute
75:12 - a value using distribution measures such
75:15 - as the mean or the median or other
75:16 - variables if values of other fields have
75:19 - differing distributions for the variable
75:21 - with missing values we can calculate
75:23 - separate distribution measures using
75:25 - these categories this concludes our
75:27 - video on bivariate data scatter plots
75:30 - and null values
75:34 - [Music]
75:47 - this video will cover uncertainty
75:50 - entropy and analyzing data the result of
75:54 - data analysis is information information
75:57 - resolves uncertainty the uncertainty of
76:00 - an event is measured by its probability
76:02 - of occurrence the more uncertain an
76:04 - event the more information is required
76:06 - to resolve the uncertainty of that event
76:09 - entropy refers to the fact that you
76:11 - cannot stir things apart it's a measure
76:13 - of information content and
76:15 - unpredictability here's a concrete
76:18 - example of entropy if you have cold
76:20 - water and hot water and mix them
76:22 - together you will have warm water you
76:25 - can't separate the cold and the hot
76:26 - after they're mixed this is what is
76:28 - meant by you cannot stir things apart to
76:30 - get an informal intuitive understanding
76:32 - of the connection between these terms
76:33 - consider the example of a pole on some
76:35 - political issue the outcome of the pole
76:38 - is relatively unpredictable and actually
76:40 - performing the pole and learning the
76:41 - results gives some new information these
76:43 - are just different ways of saying that
76:45 - the entropy of the poles results is
76:47 - large now let's say a second poll is
76:49 - performed shortly after the first poll
76:51 - since the result of the first poll is
76:53 - already known the outcome of the second
76:55 - poll can be predicted well and the
76:57 - results should not contain much new
76:58 - information in this case the entropy of
77:00 - the second poll result is small relative
77:02 - to the first now consider the example of
77:06 - a coin toss when the coin is fair that
77:08 - is when the probability of heads is the
77:10 - same as the probability of tails then
77:12 - the entropy of the coin toss is as high
77:14 - as it can be that's because there's no
77:16 - way to predict the outcome of the coin
77:18 - toss ahead of time such a coin toss has
77:20 - one bit of entropy since there are two
77:22 - possible outcomes that occur with equal
77:24 - probability and learning the actual
77:26 - outcome contains one bit of information
77:27 - on the contrary a coin toss with a coin
77:31 - that has two heads and no tails has zero
77:33 - entropy since the coin will always come
77:35 - up heads and the outcome can be
77:37 - predicted perfectly in this graph
77:39 - entropy is maximized when the
77:41 - probability is 50%
77:43 - when the probability is zero or 100%
77:46 - there is zero entropy by adding
77:48 - information we can reduce entropy and
77:50 - gain certainty how does entropy apply to
77:52 - analyzing data we can use the principles
77:55 - of entropy to decide what results are
77:57 - important and should be included in a
77:59 - report and what results are trivial and
78:01 - should not be included if there is no
78:04 - uncertainty for a variable then there's
78:05 - no information if we obtained a new
78:08 - observation we would already know the
78:09 - value of that variable typically
78:11 - variables with only one value are
78:13 - excluded from an analysis at the very
78:15 - beginning on the other hand if a
78:17 - variable has a maximum uncertainty
78:19 - because it contains values that are
78:21 - equally likely it will be difficult to
78:23 - guess the value for a new observation
78:24 - these variables are not excluded from
78:27 - analysis applying this concept to data
78:29 - analysis our goal is to reduce entropy
78:31 - by explaining outcomes using other
78:34 - variables we are reducing uncertainty
78:36 - and these results should be the focus of
78:37 - a report this concludes our video today
78:40 - we covered uncertainty entropy and
78:42 - analyzing data
78:45 - [Music]
78:58 - this video will cover the parts of an
79:01 - analytical report including the
79:03 - introduction data analysis and results
79:07 - in conclusion the introduction should
79:10 - provide a concise summary of the project
79:12 - including the problem faced the type of
79:15 - data gathered and the highlights of the
79:18 - solution the data section should go into
79:20 - detail about the data used to complete
79:22 - the project variables and other
79:24 - technical terms should be defined well
79:26 - an example value should be listed and
79:28 - interpreted also this section should
79:31 - mention any abnormalities in the data
79:32 - such as missing values and discuss the
79:34 - steps that were taken to clean and
79:36 - prepare the data for analysis in the
79:38 - analysis section the report should cover
79:40 - the thought process behind the analysis
79:42 - including any output and data
79:44 - visualizations that are pertinent
79:46 - methods that are used should be
79:47 - introduced along with a brief
79:49 - description for the reasons they were
79:50 - used and possibly including references
79:52 - to external sources for further study
79:55 - care should be taken to not include
79:57 - every possible analysis as this can
79:59 - provide information overload to the
80:01 - audience insignificant or less
80:03 - significant findings can be briefly
80:05 - summarized leaving the majority of
80:07 - content in this section to focus on the
80:09 - most important findings the results in
80:11 - conclusion section should summarize the
80:13 - results of the analysis and if
80:15 - applicable provide specific
80:16 - recommendations on a course of action
80:18 - because the analysis section covered
80:20 - most of the information gleaned from the
80:22 - data the results in conclusion should
80:24 - mostly just apply that information
80:25 - towards a goal or a further course of
80:27 - study combined the introduction data
80:29 - analysis and results in conclusion
80:31 - sections of an analytical report
80:33 - delivered succinctly and clearly by an
80:35 - effective business analyst can provide
80:37 - useful and pertinent information to
80:39 - drive business improvement this
80:41 - concludes our video on writing
80:42 - analytical reports today we covered the
80:44 - parts of a report including the
80:46 - introduction data analysis and results
80:49 - in conclusion
80:52 - [Music]
81:02 - this video will cover automation
81:05 - including a brief introduction to
81:06 - automation macros and stored procedures
81:10 - a report is not the only results of a
81:13 - business analytics project typically
81:15 - automation will often be a separate goal
81:17 - especially when a data source is often
81:19 - updated with new records the main
81:22 - benefit of automation is that it frees
81:23 - up the analyst time to work on different
81:25 - problems this is very valuable from a
81:27 - business perspective as analysts that
81:29 - are skilled in automating tasks can save
81:31 - the company money in terms of labor
81:33 - expenses and overtime can uncover more
81:35 - and more patterns in the data leading to
81:37 - greater profitability there are two main
81:39 - types of automation used in business
81:41 - analytics macros and stored procedures
81:43 - macros are also referred to as functions
81:46 - the purpose of macros are to easily
81:48 - replicate certain steps without having
81:50 - to write out those steps individually
81:51 - macros can make an analysis quicker and
81:53 - more concise for instance with a certain
81:56 - data set you may want to filter sort and
81:58 - then take the average of a field if this
82:00 - set of steps will be used multiple times
82:02 - or for different data sets you may want
82:04 - to transform these steps into a macro to
82:06 - save time
82:09 - the parameters of a macro are the input
82:12 - this can be datasets fields or values
82:14 - the parameters are a component that will
82:17 - change with each call to a macro after
82:19 - the steps of the macro are complete the
82:21 - output will be returned and like the
82:23 - parameters can also be of different
82:24 - types
82:25 - macros are the foundation of many
82:27 - analytics and software packages as many
82:29 - of the complex algorithms used in data
82:31 - science are mostly just the building of
82:33 - certain simpler functions by utilizing
82:36 - macros that have already been built an
82:37 - analyst can become more efficient by
82:39 - building customized macros and analysts
82:41 - can easily pass on their efforts to
82:42 - other analysts most companies with a
82:45 - data science team will build out a
82:47 - repository of customized macros called a
82:49 - code base it makes it easier to manage
82:51 - certain tasks such as committing changes
82:53 - in version control these concepts are
82:55 - taken from computer science best
82:57 - practices of developing software another
83:00 - concept that comes from computer science
83:01 - is the use of object-oriented
83:03 - programming this principle guides the
83:05 - development of macros and is based on
83:07 - the idea that the components of code
83:09 - should be compartmentalized the main
83:11 - benefit of this technique is that it
83:12 - makes large projects easier to develop
83:14 - and maintain instead of an analyst
83:16 - having to replicate an analysis manually
83:18 - every time new data is captured a stored
83:21 - procedure will execute the stored
83:23 - procedure is a type of algorithm that
83:25 - runs according to a schedule and
83:26 - executes a series of steps that the
83:28 - analysts sets up in advance
83:30 - usually these steps will be in the form
83:33 - of code but new software programs allow
83:35 - analysts to build stored procedures
83:37 - without having to write code once code
83:40 - has been production alized only
83:41 - monitoring the execution of the stored
83:43 - procedure is necessary when the
83:45 - procedure completes with an error the
83:47 - analyst will need to troubleshoot the
83:48 - code the most common reason for errors
83:50 - to arise are due to unforeseen data
83:52 - values that is variables will contain
83:54 - value types that were not present in the
83:56 - original data set for example a numeric
83:58 - field will contain an alphabetic
84:00 - character the best practice is to test
84:02 - for these values within the stored
84:04 - procedure and transform them or remove
84:06 - records altogether and add a warning
84:08 - message this will prevent the stored
84:10 - procedure from failing altogether logs
84:12 - are output from stored procedures to
84:14 - describe the processes that ran
84:16 - information that logs produce can vary
84:18 - but usually include the amount of time
84:20 - each process took the number of Records
84:22 - were input and any error or warning
84:24 - messages that were triggered analysts
84:26 - should add code to stored procedures to
84:28 - make logs more descriptive and thus
84:30 - easier to troubleshoot in problems arise
84:32 - depending on the tools being used and
84:34 - the amount of data being processed
84:35 - production Eliza an algorithm can be
84:38 - either a simple task for one analyst or
84:40 - a multi-year project involving many
84:42 - analysts project managers database
84:44 - administrators documentation writers and
84:46 - quality assurance specialists in these
84:50 - cases management methodologies such as
84:52 - agile software development are used to
84:54 - coordinate team members and progress
84:56 - through the project lifecycle this
84:58 - concludes our video on automation today
85:00 - we covered a brief introduction to
85:02 - automation macros and stored procedures
85:06 - [Music]
85:16 - this video on regression will cover
85:19 - simple linear regression analysis
85:22 - regression line fitting observed in
85:25 - predicted values the least-squares
85:28 - coefficient estimation goodness of fit
85:31 - explained an unexplained variation root
85:34 - mean square error and the coefficient of
85:36 - determination significance testing and
85:39 - regression assumptions regression
85:43 - analysis is used to predict the value of
85:45 - one variable the dependent variable Y on
85:47 - the basis of other variables the
85:49 - independent variable X in other words if
85:52 - you know something about X you can use
85:54 - it to predict something about why we
85:57 - provide the independent variable X and
85:59 - we observe the dependent variable Y the
86:02 - linear regression equation is shown here
86:05 - the variable X is considered the
86:07 - independent or predictor variable the
86:10 - variable Y is the dependent or outcome
86:12 - variable we have data on both x and y we
86:16 - use this information to estimate the
86:17 - value of the intercept beta 0 and the
86:20 - slope beta 1 that relate to x and y
86:24 - since the linear relationship is not
86:26 - exact we include an error term in the
86:29 - model Epsilon why is this useful because
86:32 - once we have estimates of beta0 and
86:34 - beta1 from our regression we can use
86:36 - this for any value of x to predict what
86:39 - the value of y would be so if we have
86:41 - data on NBA players weight and height we
86:43 - can estimate how much a typical NBA
86:45 - player weighs based on his height then
86:47 - if someone wants to join the NBA and we
86:49 - know what is height is we can estimate
86:51 - what weight he should be to make it to
86:53 - the NBA in linear regression analysis
86:56 - our goal is to estimate a pattern in
86:57 - this case a line the best fits the data
87:00 - the best fit for our data will go
87:02 - through the core of our data and
87:03 - minimize error the linear relationship
87:06 - in algebra is when a line is represented
87:08 - by its slope and intercept but instead
87:10 - of an exact relationship regression
87:12 - analysis estimates the line from data
87:14 - since the line does not fit data points
87:17 - precisely there is an error term AI
87:19 - measuring the deviation of actual Y from
87:22 - estimated y using the least squares koi
87:25 - fish
87:25 - estimation we can obtain the line that
87:27 - best fits the data by minimizing the sum
87:29 - of squared errors
87:35 - the total variance in Y is divided into
87:37 - two parts
87:38 - that which can be explained by X using
87:40 - regression and that which cannot no line
87:43 - is perfect there's always some error in
87:44 - the estimation unless there's a
87:46 - comprehensive dependency between the
87:48 - predictor and response there's always
87:50 - some part of the response Y that can't
87:52 - be explained by the predictor X using
87:55 - the mean value of y as our reference
87:57 - point we can decompose the total error
87:59 - in measurement between the part that is
88:01 - explained by the regression line and the
88:03 - part that remains unexplained
88:08 - here's a look at the decomposition the
88:12 - sum of squares regression or SSR is the
88:15 - explained variation attributable to the
88:17 - linear relationship between X and y the
88:20 - sum of squares error or SSE measures the
88:23 - variation attributable to factors other
88:25 - than the linear relationship between x
88:27 - and y the SST is the total sum of
88:31 - squares the SSR and SSE together make up
88:34 - the SST
88:41 - so given that total variation SST is the
88:44 - sum of explained and unexplained
88:46 - variation
88:48 - we can divide through by the total sum
88:51 - of squares SST
88:52 - to get the ratios equal to one the ratio
88:55 - of error sum of squares / total sum of
88:57 - squares plus the ratio of regression sum
89:00 - of squares / total sum of squares equals
89:02 - one
89:02 - either of these two ratios can be used
89:05 - to measure our model fit error sum of
89:07 - squares show is called the mean square
89:09 - error we want to choose models with the
89:12 - lowest mean square error regression sum
89:14 - of square ratio is called R squared or
89:16 - the coefficient of determination we
89:19 - choose a model with the highest R
89:20 - squared because R squared + mean square
89:23 - error equals 1 it has to be true that R
89:26 - square our coefficient of determination
89:27 - has to lie between 0 & 1
89:32 - likewise we can look at the coefficients
89:34 - of the intercept and slope to see if
89:36 - they're significantly different from
89:37 - zero this is done by examining the T
89:39 - statistic and the corresponding p-values
89:41 - p-value is less than 0.05 imply that the
89:45 - coefficient is significantly different
89:47 - from zero some key assumptions before
89:49 - you apply regression techniques first
89:52 - the variables have to have a linear
89:53 - relationship in this example the
89:55 - relationship between x and y is not
89:57 - linear so we cannot fit a linear
89:58 - regression line the variables also have
90:02 - to be approximately normally distributed
90:03 - in this example Y is not normally
90:05 - distributed at each value of x so it
90:08 - doesn't make sense to fit a linear
90:09 - regression line
90:12 - additionally the variance of y at each
90:14 - value of x should be the same or in
90:16 - other words we should have a homogeneity
90:17 - of variances the fourth and final
90:21 - assumption is that the observations are
90:23 - independent here one trendline is not
90:25 - sufficient in other words if sales today
90:27 - depend on sales yesterday then linear
90:30 - regression models will not work this
90:33 - concludes our video on regression
90:34 - today we covered simple linear
90:37 - regression analysis regression line
90:40 - fitting observed and predicted value
90:50 - you
91:10 - [Music]
91:15 - this lesson covers the t-distribution
91:17 - and how it compares to the normal
91:19 - distribution as well as a brief look at
91:21 - the student's t-distribution for large
91:24 - samples the normal distribution applies
91:26 - for small samples the standard deviation
91:29 - is measured in precisely and the data
91:31 - followed the T distribution a T
91:33 - distribution will approach a normal
91:35 - distribution for a larger n greater than
91:37 - or equal to 100 but it has fatter tails
91:40 - for a smaller n less than 100 the T
91:44 - distribution is very similar to the
91:45 - standard normal distribution it also has
91:47 - a bell curve but the standard deviations
91:49 - are computed from the sample data
91:51 - instead of the population suppose a
91:54 - simple random sample of size n is drawn
91:56 - from a population whose distribution can
91:58 - be approximated by a normal Mu Sigma
92:00 - model when the standard deviation is
92:03 - known then the sampling model for the
92:04 - mean X is distributed as a normal
92:07 - distribution with mean x-bar and
92:09 - standard deviation Sigma divided by the
92:11 - square root of n when the standard
92:13 - deviation is estimated from the sample
92:15 - standard deviation s the sampling model
92:17 - follows a T distribution with degrees of
92:19 - freedom and minus 1 this is the one
92:22 - sample T statistic in this figure both
92:27 - distributions have 0 means but the
92:29 - variances are a bit different the T
92:31 - distribution has a lower peak and fatter
92:33 - tails
92:35 - this concludes our video on t
92:37 - distributions today we covered the T
92:39 - distribution and showed how it is very
92:40 - similar to the normal distribution we
92:42 - also briefly showed the students T
92:44 - distribution
92:46 - [Music]
93:02 - in this video we will cover logistic
93:05 - regression including the need for
93:07 - logistic regression the logistic
93:09 - regression model and odds ratios and
93:11 - prediction in many instances when you're
93:15 - testing hypotheses and making
93:17 - predictions you will have dichotomous
93:18 - outcomes for example in a game you can
93:21 - either win or lose on a website a user
93:24 - either clicks or does not click in an
93:27 - election a person votes for a candidate
93:29 - or does not when the outcome variable is
93:32 - categorical such as our game example it
93:35 - does not follow a normal distribution
93:36 - the outcome variable is a probability
93:39 - measured between 0 & 1 the estimates you
93:42 - make should be numbers in that range a
93:44 - linear model cannot be applied we need a
93:46 - nonlinear function there are many
93:48 - nonlinear models we can choose from to
93:50 - fit our data some nonlinear functions
93:53 - are shown here the last one shown is the
93:55 - logistic function that will fit the data
93:57 - the best because it has an upper and
93:59 - lower bound here's a logistic regression
94:03 - model in orange versus a linear
94:04 - regression model in blue isn't it a
94:07 - better fit for the data to best suit our
94:09 - data we want a model that predicts
94:11 - probabilities between 0 and 1 so it will
94:13 - be s-shaped there are lots of s-shaped
94:16 - curves but the logistic regression model
94:18 - is what we'll use in this instance the
94:20 - logistic function is a nonlinear
94:22 - function of independent variables
94:24 - however we can convert this nonlinear
94:26 - function into a linear relationship
94:27 - using the log of the odds ratio note
94:31 - that instead of modeling just zeros and
94:32 - ones we're modeling the probability of
94:34 - an event occurring with a logistic
94:37 - regression model instead of winning or
94:39 - losing we build a model for log odds of
94:41 - winning or losing it's a natural
94:43 - logarithm of the odds of the outcome P
94:45 - stands for the probability of the
94:47 - outcome while 1 minus P stands for the
94:49 - probability of not getting an outcome
94:51 - however having log of P over 1 minus P
94:54 - on the y axis is not very helpful we
94:57 - have to compute the actual odds to do
94:59 - that we have to use the exponential
95:01 - functions let's look at a very simple
95:03 - example of a log it function does
95:05 - alcohol drinking predict political party
95:08 - political party is the outcome variable
95:10 - and it is binary therefore we need a
95:13 - logistic regression a typical log in
95:15 - equation contains
95:16 - the log of the odds ratio is the outcome
95:18 - which is a linear function of the
95:19 - predictors X the log it model to measure
95:23 - the impact of drinking on voter choice
95:25 - is going to be set up as follows the log
95:28 - of the odds ratio will be measured from
95:30 - data on X where X here is the number of
95:32 - drinks per week it's really important to
95:35 - understand that negative 1 point 4 is
95:37 - measuring the log of the odds ratio in
95:39 - other words it is the log of the
95:40 - probability of being Republican divided
95:43 - by the probability of not being a
95:44 - Republican to get the actual odds ratio
95:47 - you have to compute the exponent which
95:49 - is equal to 0.25 since the odds are less
95:53 - than 1 it tells us that the more you
95:55 - drink the lower your odds of being
95:56 - Republican all these calculations can be
95:59 - done automatically in SAS but it's
96:01 - important to understand the math behind
96:03 - what SAS is doing the same model can be
96:06 - extended to more than one variable we
96:08 - just add more predictors to the equation
96:11 - the coefficient beta measures the impact
96:14 - of x on the log of the odds ratio for
96:16 - example in linear regression if y equals
96:19 - 2 plus 3x a one unit increase in X will
96:22 - increase Y by three units in a logistic
96:25 - regression log P over 1 minus P minus 2
96:29 - plus 3x shows us that if x increases by
96:32 - one unit then the log odds of P y equals
96:35 - 1 increases by three units the impact on
96:39 - the odds ratio is represented by E
96:41 - exponent beta we can also compute
96:44 - probabilities directly to compute odds
96:46 - we have to use the exponent if we don't
96:48 - want to look at odds but the actual
96:50 - probabilities we apply the entire
96:52 - logistic function formula as shown in
96:54 - this probability function these are the
96:58 - odds ratios and log of odds ratios for
97:00 - various probabilities an important point
97:03 - to understand is how the odds ratios are
97:05 - tied to the probabilities note the
97:07 - mathematical equivalencies a 50%
97:10 - probability or probability of 0.5 is the
97:13 - same as 1 to 1 odds the log of the odds
97:16 - ratio at that point is equal to 0 as
97:19 - probability increases odds ratio
97:22 - increases from 0 to infinity while the
97:24 - log of the odds ratio can become any
97:26 - value
97:28 - this concludes our video on logistic
97:30 - regression today we covered the need for
97:32 - logistic regression logistic regression
97:35 - model and odds ratios and prediction
97:39 - [Music]
97:56 - this video will cover two types of
97:58 - statistical error type 1 or alpha and
98:02 - type 2 or beta all statistics derived
98:05 - from samples are subject to error a type
98:08 - 1 error rejects the null hypothesis when
98:10 - it is actually true a type 2 error
98:12 - accepts the null hypothesis when it is
98:15 - not true remember that a different
98:17 - sample can give a completely different
98:18 - result a sample mean is likely to fall
98:21 - in the confidence interval only 95% of
98:23 - the time so the inferences drawn from
98:25 - the sample may be wrong
98:27 - let's talk in a little more detail about
98:29 - the type 1 error the type 1 error occurs
98:31 - when a researcher thinks he or she has
98:33 - found a significant result but really
98:35 - that result is due to chance it's
98:37 - similar to a false positive on a drug
98:39 - test the type 1 error or the mistake of
98:42 - rejecting the true null hypothesis will
98:44 - happen with a frequency of alpha thus if
98:47 - alpha our critical value is 0.05 then a
98:50 - type 1 error will occur 5% of the time
98:53 - on the other hand a type 2 error occurs
98:55 - when results seem insignificant but in
98:58 - fact there was something significant
98:59 - going on type 2 errors are like a false
99:02 - negative on a drug test they occur when
99:04 - the alternative hypothesis is true but
99:06 - there's not enough evidence in the
99:08 - sample to reject the null hypothesis
99:10 - this type of error is traditionally
99:12 - considered less important than a type 1
99:14 - error but it can lead to serious
99:15 - consequences in real situations the
99:19 - power of a test is 1 minus the
99:21 - probability of a type 2 error it is the
99:24 - probability of rejecting the null
99:25 - hypothesis when the alternative
99:27 - hypothesis is true in these competing
99:29 - sampling distributions alpha is set to
99:32 - point zero 5 the bottom curve assumes H
99:34 - a is true the top curve assumes that the
99:37 - null hypothesis H naught is true its
99:40 - right tail shows that we will reject H
99:42 - naught when a sample mean exceeds one
99:44 - eighty nine point six the probability of
99:47 - getting a value greater than one eighty
99:49 - nine point six on the bottom curve is
99:51 - 0.5 one six zero corresponding to the
99:54 - power of the test here's a table that
99:56 - summarizes the types of errors here's an
100:00 - example using a fire alarm if a fire
100:02 - alarm is silent and there is no fire our
100:04 - null hypothesis that it is working is
100:07 - correct but what if the assumption
100:09 - wrong then we've accepted the null
100:11 - hypothesis but we actually have a fire
100:13 - that's our type 1 error the opposite
100:16 - case may also happen if the alarm goes
100:18 - off and there's actually a fire there's
100:20 - no error but if there's no fire and the
100:23 - alarm goes off it's a false alarm that's
100:25 - the type 2 error here it is the less
100:28 - serious problem this concludes our video
100:31 - on statistical error today we discuss
100:34 - type 1 or alpha error and type 2 or beta
100:37 - error
100:39 - [Music]
100:52 - this video will cover hypothesis testing
100:55 - which is also called significance
100:57 - testing and occurs when we test a claim
100:59 - about a population parameter using
101:01 - sample evidence that confirms or rejects
101:04 - the claim there are four steps in the
101:06 - hypothesis testing process all of which
101:08 - will be covered in this video here's a
101:10 - summary of the four steps in hypothesis
101:12 - testing after this we'll discuss each
101:14 - step in detail the first step is stating
101:16 - the null and alternative hypotheses we
101:19 - have to establish what we are testing to
101:21 - be true once we do that we have to
101:23 - decide how close to true our sample
101:25 - statistic has to be for us to accept the
101:27 - truth for example we might want our
101:29 - estimate to be accurate with a 5% margin
101:31 - of error this is called locating the
101:33 - critical region once we know that we
101:35 - have to compute the test statistic the Z
101:38 - value or the T value finally based on
101:40 - our results we draw conclusions from the
101:42 - study the first step in the procedure is
101:45 - to convert the research question to a
101:46 - statement of the hypotheses null and
101:48 - alternative forms our study will be to
101:51 - collect and seek evidence against the
101:53 - null hypothesis as a way of deductively
101:55 - bolstering the alternative hypothesis
101:57 - the null hypothesis abbreviated h naught
102:00 - is a statement of no difference in other
102:03 - words the null hypothesis argues that
102:04 - there is no significant difference
102:06 - between our specified populations and
102:08 - that any observed difference is due to
102:10 - sampling or experimental error the
102:12 - alternative hypothesis or H a is the
102:15 - opposite of the null hypothesis it
102:17 - provides a statement of difference in
102:19 - our study we will seek evidence against
102:21 - the claim of H naught as a way of
102:23 - proving h a here's an example of setting
102:26 - up the null and alternative hypotheses
102:28 - in the late 1970s the weight of US men
102:31 - between 20 and 29 years of age had a log
102:33 - normal distribution with a mean of 170
102:36 - pounds and a standard deviation of 40
102:38 - pounds to illustrate the hypothesis
102:40 - testing procedure we asked if body
102:42 - weight in this group has changed since
102:44 - 1970 this is called our research
102:46 - question and it can be answered in one
102:48 - of two ways under the null hypothesis
102:50 - there is no difference in the mean body
102:52 - weight between then and now in which
102:53 - case Mew would still equal 170 pounds
102:56 - under the alternative hypothesis we
102:59 - assert that the mean weight has changed
103:01 - EMU's not equal to 170 pounds this is
103:04 - called a two-sided test
103:05 - the most common form of hypothesis
103:07 - testing we can also do a one-sided test
103:10 - in which we ask if weight has increased
103:12 - over time so the alternative hypothesis
103:14 - would be mu is greater than 170 pounds
103:17 - in step 2 we will locate the critical
103:20 - region once we've established the
103:22 - research question we have to define the
103:24 - level of accuracy with which we want to
103:26 - measure our test statistic any estimate
103:29 - from a sample will not be exactly the
103:31 - same as the population parameter so we
103:33 - have to decide what we think is likely
103:35 - versus unlikely this is called locating
103:38 - the critical region the critical region
103:40 - consists of outcomes that are very
103:41 - unlikely to occur if the null hypothesis
103:44 - is true or in other words the sample
103:46 - means that are almost impossible to
103:48 - obtain when we're estimating population
103:50 - parameters using a sample we have to
103:52 - determine the cutoff values these cutoff
103:55 - values are called alpha if we decide
103:57 - that we want to measure the mean with a
103:59 - 90 percent precision level then the
104:01 - shaded area on the left and right will
104:03 - be larger if we want to measure with a 1
104:05 - percent precision then the area will be
104:07 - smaller and the range will be larger
104:09 - these are the locations of the critical
104:12 - region boundaries for three different
104:13 - levels of significance alpha equals 0.05
104:17 - alpha equals 0.01 and alpha equals 0.01
104:22 - note that boundaries get wider as the
104:24 - critical value Falls in most cases
104:27 - researchers choose an alpha of 0.05 or
104:30 - point zero one our rejection region
104:32 - should have a probability of alpha if
104:34 - the null hypothesis is true but some
104:37 - bigger probability if the alternative
104:38 - hypothesis is true so if the mean lies
104:41 - inside the cutoff value for alpha then
104:43 - the null hypothesis is true otherwise we
104:45 - fail to accept the null hypothesis the
104:48 - result is significant beyond the alpha
104:50 - level for example if alpha is 0.05 our
104:53 - result is significant if it's less than
104:55 - point zero five once we decide whether
104:57 - we want to measure accuracy at the 10
104:59 - percent 5% or 1% level we can compute
105:03 - the test statistic here we will use the
105:05 - z-score which is a ratio comparing the
105:07 - obtained difference between the sample
105:09 - mean and the hypothesized population
105:11 - mean this is an example of a one sample
105:14 - test of a mean when the standard
105:16 - deviation Sigma is known in our male way
105:19 - example we're going to use the Z
105:20 - statistic because we know the population
105:22 - mean and the population standard
105:24 - deviation to compute the Z statistic we
105:27 - simply insert values derived from our
105:29 - sample into the formula if in one sample
105:32 - we found that the sample mean was 173
105:34 - then the Z statistic would be 0.6 0 this
105:37 - value on the x-axis under a standard
105:40 - normal curve let's say we found the
105:42 - sample mean to be 185 putting these
105:44 - values into the Z stat formula we find
105:46 - the z stat is 3.0 this is much higher at
105:49 - the tail end of the x-axis on a normal
105:52 - distribution the final step is drawing
105:54 - conclusions
105:55 - once we've computed the Z value of our
105:57 - test statistic we have to look at the
105:59 - corresponding probability values to find
106:01 - out if it's reasonably close to the
106:02 - population mean a large value shows that
106:05 - the obtained mean difference is large
106:07 - and in the critical region the
106:09 - difference is significant which means we
106:11 - have to reject the null hypothesis that
106:12 - the weights have not changed over time
106:14 - if the mean difference is relatively
106:16 - small then the test statistic will have
106:19 - a low value in this case we conclude
106:21 - that the evidence from the sample is not
106:23 - sufficient and the decision is to fail
106:25 - to reject the null hypothesis the
106:27 - p-value is the area under the normal
106:28 - curve in the tails beyond the z stat it
106:31 - answers the question what is the
106:33 - probability of the observed test
106:35 - statistic or one more extreme when H
106:37 - naught is true to convert Z statistics
106:39 - to p-value we will use software in one
106:42 - sample with a sample mean of 173 the z
106:45 - statistic was 0.6 0 if we had this
106:48 - sample we would fail to reject the null
106:49 - hypothesis that the mean weights have
106:51 - increased over time likewise if we
106:53 - computed the p-values for Z equals 3.0
106:57 - we would get point zero zero 1 which
107:00 - means we have to reject the null
107:01 - hypothesis that the mean weight has
107:03 - remained the same over time note that
107:06 - when we're looking at weight change
107:07 - instead of weight increase all we have
107:09 - to do is multiply the one-sided p-value
107:11 - by 2 to do a two-tailed test since we
107:15 - will be using p-values in all our
107:17 - subsequent analysis it's worth
107:19 - emphasizing what that means p-values ask
107:22 - the question what is the probability of
107:24 - the observed test statistic when H
107:27 - naught is true remember the smaller the
107:29 - p-value the more likely that your null
107:31 - hypothesis
107:32 - this is not true this graphic depicts
107:34 - the significance of p-values at less
107:36 - than one percent between one and five
107:38 - percent between five and ten percent
107:41 - and greater than ten percent these are
107:43 - common significance levels five percent
107:46 - is the most common cutoff however a note
107:48 - that is unwise to draw firm borders for
107:51 - significance as an example a p-value of
107:54 - 0.27 would not be significant against H
107:57 - naught a p-value of 0.01 on the other
107:59 - hand would be highly significant against
108:01 - H naught this concludes our video on
108:04 - hypothesis testing also called
108:06 - significance testing which occurs when
108:08 - we test a claim about a population
108:10 - parameter using evidence that confirms
108:12 - or rejects that claim today we covered
108:15 - the four steps in hypothesis testing
108:16 - state the null and alternative
108:18 - hypotheses locate the critical region
108:21 - compute the test statistic and draw
108:24 - conclusions
108:26 - [Music]
108:44 - this presentation will cover correlation
108:47 - including a definition of correlation a
108:50 - discussion of the need for correlation
108:53 - details on computing correlation
108:55 - including variance covariance and the
108:59 - correlation coefficient strength of
109:01 - Association linear and curvilinear
109:04 - relationships
109:06 - properties of correlation R squared the
109:09 - coefficient of determination and a
109:12 - discussion of correlation versus
109:14 - causation correlation is one of the most
109:17 - common and useful statistics it's a
109:19 - measure of Association a single number
109:22 - that describes the degree of
109:23 - relationship between two variables we
109:26 - can examine correlations between two
109:27 - variables heuristic ly by looking at a
109:29 - scatter chart in this chart our
109:32 - observations are very tightly centered
109:33 - around the line in this case we would
109:35 - say that the relationship between x and
109:37 - y is more correlated we call this a
109:40 - strong correlation by contrast if the
109:43 - observations are scattered further out
109:44 - we might say the relationship between x
109:47 - and y is less correlated or that there
109:49 - is a weak correlation here are some
109:51 - examples of questions that ask about
109:53 - correlation is there any association
109:56 - between hours of study and grades
109:58 - is there any association between the
110:00 - number of churches in a city and the
110:02 - murder rate when the weather gets hot
110:04 - what happens to sweater sales what is
110:08 - the strength of association between them
110:09 - what about the sale of ice-cream versus
110:12 - temperature what is the strength of
110:14 - association between them furthermore how
110:17 - do we quantify the association while we
110:19 - can guess the relationship there's a
110:21 - better way to do this using statistical
110:23 - measures the measure we use is the
110:25 - Pearson correlation coefficient to
110:28 - compute correlation we'll need
110:29 - information on standard deviation and
110:31 - covariance we know that the variance is
110:34 - the dispersion within a variable X or Y
110:36 - or the squared average deviation from
110:38 - the mean as shown here the covariance is
110:41 - the dispersion of X multiplied by the
110:43 - dispersion in Y it is calculated as the
110:46 - average of the product of deviations in
110:48 - individual means using the information
110:50 - on variance and covariance we can
110:52 - compute the correlation coefficient as
110:54 - the covariance of x and y divided by the
110:57 - state
110:57 - deviation of X multiplied by the
110:59 - standard deviation in Y this measure of
111:02 - correlation ranges from negative one to
111:04 - positive one a higher number is a
111:07 - stronger correlation and the lower
111:09 - number is a weaker correlation
111:11 - correlation coefficient are measures the
111:13 - strength of linear Association it
111:16 - measures the extent to which two
111:17 - variables are proportional to each other
111:19 - it's unit free so for example a measure
111:22 - of correlation between player height
111:24 - measured in inches and player weight
111:26 - measured in pounds will be meaningful
111:28 - even if they're measured in different
111:29 - units here are some examples no linear
111:33 - Association negligible negative
111:35 - Association weak positive Association
111:38 - moderate negative Association very
111:40 - strong positive Association very strong
111:43 - negative association in these scatter
111:46 - plots what is happening to Y as X is
111:48 - increasing an important point to
111:50 - remember is that correlation is a
111:52 - measure of linear Association if the
111:55 - relationship is curvilinear using the
111:57 - correlation measure is not appropriate
111:59 - if X changes and Y stays the same then
112:02 - the correlation is zero since the
112:05 - correlation measure is a measure of
112:06 - linear Association we cannot use
112:09 - correlations on categorical data it's
112:12 - related to sample size and it's also
112:13 - very sensitive to outliers the
112:16 - correlation measure R measures the
112:17 - strength of linear Association squaring
112:20 - the correlation coefficient gives us R
112:22 - squared which is the coefficient of
112:24 - determination it is the proportion of
112:27 - common variation in two variables this
112:29 - measures the strength or the magnitude
112:31 - of the relationship while we cannot use
112:33 - percentage to interpret R we can do so
112:36 - for R squared for example if R squared
112:39 - equals 67% then we can say that 67% of
112:43 - variation in X is related to variation
112:46 - in Y correlation does not imply
112:48 - causation it's easy to see that in this
112:50 - chart the Internet Explorer market share
112:52 - correlates with the murder rate in the
112:54 - US but that doesn't mean that one caused
112:56 - the other causal relationships are
112:58 - determined based on facts and business
113:01 - models we cannot determine causality
113:03 - from data correlation is a mathematical
113:06 - formula you will get a number no matter
113:08 - what data you feed first you need to
113:10 - establish a logic
113:11 - relation and then find the correlation
113:13 - variables may be correlated if they have
113:16 - a causal relationship for example water
113:18 - causes plants to grow correlation can
113:21 - also occur when one variable is both the
113:23 - cause and the effect
113:25 - for example coffee consumption can cause
113:27 - nervousness but it's possible that
113:29 - nervous people also drink more coffee
113:32 - correlation can also be high because
113:34 - both variables move together due to a
113:36 - missing third variable for example this
113:39 - comparison of deaths due to drowning and
113:41 - soft drink consumption during summer
113:43 - both variables are related to heat and
113:45 - humidity a third variable not shown here
113:47 - emitting such variables can be dangerous
113:51 - here's a look at some additional
113:52 - measures of correlation using scatter
113:54 - charts
113:57 - this concludes our video on correlation
114:00 - today we discuss the definition of
114:02 - correlation the need for correlation
114:05 - details on computing correlation
114:07 - including variance covariance and the
114:10 - correlation coefficient strength of
114:12 - Association linear and curva linear
114:16 - relationships properties of correlation
114:18 - R squared the coefficient of
114:21 - determination and correlation versus
114:23 - causation
114:25 - [Music]
114:43 - this video will cover binomial
114:45 - distributions which are a type of
114:47 - discrete distribution we will first
114:49 - compare discrete and continuous
114:50 - distributions of a single random
114:52 - variable and then we'll look at the
114:54 - binomial distribution specifically there
114:57 - are two types of random variables
114:59 - discrete and continuous a discrete
115:02 - random variable has only a finite number
115:04 - of possible values whereas a continuous
115:06 - random variable has a continuum of
115:08 - possible values usually a discrete
115:11 - distribution results from account
115:13 - whereas a continuous distribution
115:14 - results from a measurement the
115:17 - distinction between counts and
115:18 - measurements is not always clear-cut a
115:20 - probability distribution is simply a
115:22 - mapping of all distinct events for a
115:23 - variable and their probability of
115:25 - occurrence such as the distribution of a
115:27 - coin flip experiment the form of the
115:29 - distribution depends on whether the
115:31 - variables are discrete or continuous
115:33 - here are some examples of discrete
115:35 - variables outcomes of dice rolls whether
115:38 - a customer likes or dislikes a product
115:40 - or the number of hits on a website some
115:43 - examples of continuous variables include
115:45 - the weekly change in the Dow Jones
115:47 - industrial average daily temperature or
115:50 - the time between machine failures to
115:53 - specify the probability distribution of
115:55 - event X we need to specify all of its
115:57 - possible values and their probabilities
115:59 - we assume that there are K possible
116:01 - values and write out our list of
116:03 - possible values like this a typical
116:05 - value is denoted like this and the
116:09 - probability of a typical value is
116:10 - denoted like this next we will discuss
116:13 - distributions of both discrete and
116:15 - continuous variables for each type of
116:17 - variable distributions can be
116:18 - characterized by three measures mean
116:20 - variance and standard deviation these
116:23 - are formulas for working with discrete
116:25 - distributions well we won't be computing
116:27 - these measures by hand you do need to be
116:29 - aware of the formulas the mean also
116:32 - called the expected value is calculated
116:34 - with this formula the mean is a weighted
116:36 - sum of all possible values weighted by
116:38 - their probabilities mean is denoted by
116:40 - the Greek letter mu the variance has a
116:43 - weighted sum of the squared deviations
116:45 - of the possible values from the mean
116:46 - where the weights are again the
116:48 - probabilities the standard deviation is
116:51 - simply the square root of the variance
116:52 - standard deviation is denoted by the
116:55 - Greek letter Sigma
116:57 - these are the formulas for working with
116:59 - continuous distributions
117:02 - a probability distribution visually
117:05 - summarizes the probabilities associated
117:07 - with all possible events for a variable
117:09 - we will focus on three probability
117:11 - distributions that are commonly used in
117:13 - explaining real-world events binomial
117:16 - and exponential distributions are used
117:18 - with discrete data while normal
117:20 - distributions are used with continuous
117:21 - data a binomial distribution is a
117:24 - discrete distribution that represents
117:26 - the number of successes in n independent
117:28 - trials each of which has the probability
117:30 - of success P each trial has a binary
117:34 - outcome for example a coin toss yields
117:36 - either heads or tails the probability of
117:41 - either observation heads or tails is the
117:43 - same each time we toss the coin these
117:46 - outcomes are generally called success
117:47 - and failure the probability of success
117:49 - is P and the probability of failure is 1
117:52 - minus P the distribution Maps the
117:55 - outcome of all the trials each trial has
117:57 - to be independent and the probability of
117:59 - success has to be the same for each
118:00 - trial this is the probability mass
118:03 - function formula for a binomial
118:04 - distribution if we toss a coin 100 times
118:07 - what is the probability that we will get
118:09 - 40 heads what is the probability of
118:11 - getting 90 heads that probability can be
118:14 - computed by applying this formula we
118:16 - have only two possible outcomes 1 0 or
118:19 - success/failure in n independent trials
118:21 - this formula depicts the probability of
118:24 - exactly X successes n is the number of
118:28 - trials x is the number of successes out
118:30 - of n trials P is the probability of
118:33 - success and 1 minus P is the probability
118:36 - of failure all probability distributions
118:39 - are characterized by an expected value
118:41 - and variance if we toss a coin 100 times
118:43 - what would be the average number of
118:45 - heads we would get what about the
118:47 - variance these are computed using these
118:49 - formulas you'll often see the
118:51 - assumptions of normal distribution being
118:53 - applied to discrete outcomes this is
118:55 - because the binomial distribution
118:56 - approximates to a normal distribution
118:58 - for large samples so for large enough
119:01 - samples we can calculate probabilities
119:02 - using normal probability rules this
119:06 - concludes our video on binomial
119:08 - distributions today we covered discrete
119:10 - and continuous distributions of a single
119:12 - random variable and the binomial
119:14 - distribution
119:17 - [Music]
119:35 - this video will cover normal
119:37 - distributions the probability density
119:40 - function cumulative distribution
119:43 - functions the 6895 99.7 rule and
119:48 - standardizing z values the single most
119:51 - important distribution in statistics is
119:53 - the normal distribution it is a
119:55 - continuous distribution and it's the
119:57 - basis of the familiar symmetric
119:59 - bell-shaped curve the mean of the normal
120:01 - distribution is in the center the
120:04 - standard deviations are marked at equal
120:05 - distances from the mean
120:07 - any particular normal distribution is
120:09 - specified by its mean and standard
120:10 - deviation by changing the mean the
120:13 - normal curve shifts to the left or the
120:14 - right by changing how spread out the
120:17 - standard deviations are the curve also
120:19 - changes standard deviations can be
120:21 - spread out wider or closer together
120:24 - therefore there are really many normal
120:27 - distributions not just a single one the
120:29 - normal distribution is a two parameter
120:30 - family where the two parameters are the
120:33 - mean and the standard deviation here's a
120:35 - tool you can play with online that
120:37 - illustrates a normal distribution in
120:39 - real life it looks like a triangular
120:41 - shaped pegboard into which balls are
120:42 - dropped when there's an equal
120:44 - probability that the balls will drop
120:46 - either left or right their final
120:48 - placement forms a normal distribution
120:50 - however when the probability of the
120:52 - balls dropping left or right is unequal
120:54 - which is something you can experiment
120:55 - with using this tool the distribution
120:58 - changes
121:02 - the formulas for mean and standard
121:04 - deviation are very complex but you will
121:06 - not have to compute them because the
121:08 - software will with continuous variables
121:11 - there is a continuum of possible values
121:12 - such as all values between 0 and 100 or
121:16 - all values greater than zero instead of
121:18 - assigning probabilities each individual
121:20 - value in the continuum the total
121:22 - probability of one is spread over this
121:24 - continuum thus the shaded area within
121:27 - the bell curve will always have an area
121:29 - of one the key to this spreading is
121:31 - called a density function which acts
121:33 - like a histogram the higher the value of
121:36 - the density function the more likely
121:37 - this region of the continuum is a
121:40 - density function usually denoted by FX
121:43 - specifies the probability distribution
121:45 - of a continuous random variable X the
121:48 - higher FX is the more likely X is
121:51 - probabilities are found from a density
121:54 - function as areas under the curve so for
121:57 - example the shaded portion under this
121:59 - bell curve represents the probability of
122:00 - X being between 65 and 75 the cumulative
122:04 - distribution function or CDF is the
122:07 - probability that the variable takes a
122:08 - value less than or equal to X it is the
122:11 - total area under the normal curve up to
122:13 - X here's an example
122:17 - the beauty of the normal curve is that
122:19 - no matter what its mean and standard
122:20 - deviation are the area between the mean
122:22 - minus one standard deviation and the
122:24 - mean plus one standard deviation is
122:26 - always about 68% the area between the
122:29 - mean minus two standard deviations and
122:31 - the mean plus two standard deviations is
122:33 - always about 95% and the area between
122:36 - the mean minus three standard deviations
122:37 - and the mean plus three standard
122:40 - deviations is always about ninety-nine
122:42 - point seven percent that means almost
122:44 - all values fall within three standard
122:45 - deviations on either side of the mean
122:47 - this is true for all normal curves no
122:50 - matter their shape but how good is this
122:52 - rule for real data let's go ahead and
122:54 - check out an example here's our data the
122:56 - mean of the weight of one hundred and
122:58 - twenty women runners in the sample is
123:00 - one hundred and twenty seven point eight
123:02 - pounds the standard deviation is fifteen
123:04 - point five here's what our distribution
123:06 - would look like let's look a little more
123:08 - closely at that distribution 68% of our
123:11 - 120 runners is about 83 runners
123:14 - according to the 68 95 99.7 rule those
123:18 - runners should all fall within one
123:20 - standard deviation of the mean weight of
123:22 - one hundred and twenty seven point eight
123:24 - that is eighty-three of our runners
123:25 - should fall between one hundred and
123:27 - twelve point three and one hundred and
123:29 - forty three point three pounds when we
123:31 - check our data we see that seventy-nine
123:33 - runners fall within one standard
123:34 - deviation of the mean
123:35 - furthermore ninety-five percent of our
123:37 - group or about 114 runners should fall
123:40 - within two standard deviations of the
123:41 - mean or between ninety six point eight
123:44 - and one hundred and fifty eight point
123:45 - eight pounds the data shows that 115
123:48 - runners fall within two standard
123:49 - deviations of the mean finally according
123:52 - to the rule ninety-nine point seven
123:53 - percent of our runners or one hundred
123:55 - and nineteen point six runners should
123:57 - fall within three standard deviations of
123:59 - the mean or within a range of eighty one
124:01 - point three pounds to one hundred and
124:03 - seventy four point three pounds
124:05 - according to our data all 120 runners
124:08 - fall within this range so it seems as if
124:10 - the rule is pretty accurate in this case
124:12 - there are indefinitely many normal
124:14 - distributions one for each pair of
124:16 - standard deviation and mean one
124:18 - particular combination of standard
124:19 - deviation and mean deserves special
124:21 - attention and that is the standard
124:23 - normal distribution all normal
124:25 - distributions can be converted into the
124:27 - standard normal curve by subtracting the
124:29 - mean
124:30 - dividing by the standard deviation but
124:32 - all of the integrals for the standard
124:33 - normal distribution have been calculated
124:35 - and put into a table for us and we also
124:37 - have software to help us out so we never
124:39 - have to integrate the long way this
124:41 - diagram illustrates the conversion of X
124:43 - values into Z values when we convert a
124:45 - normal distribution to a standard normal
124:47 - distribution
124:49 - here's a practice problem if birth
124:51 - weights in a population are normally
124:53 - distributed with a mean of 100 and 9
124:55 - ounces and a standard deviation of 13
124:57 - ounces
124:58 - what is the chance of obtaining a birth
125:00 - weight of 141 ounces or heavier when
125:03 - sampling birth records at random here's
125:05 - how we solve this problem we subtract
125:07 - 109 our mean from 141 and then divide by
125:11 - our standard deviation 13 so Z equals
125:14 - two point four six then we will use the
125:17 - normdist function in excel to discover
125:19 - that our value for Z two point four six
125:21 - equals zero point nine nine three the
125:24 - chance of a baby being born heavier
125:26 - corresponds to the right tail of the
125:28 - distribution so the probability that we
125:30 - will get a value for Z that's greater
125:32 - than or equal to two point four six can
125:35 - be discovered by subtracting our value
125:36 - for Z point nine nine three from the
125:39 - total area of the standard distribution
125:40 - one
125:43 - this concludes our video on normal
125:46 - distributions continuous distributions
125:48 - density functions cumulative
125:50 - distribution functions the 6895 99.7
125:54 - rule and standardizing Z values
125:58 - [Music]
126:12 - this video will cover populations and
126:15 - inferences sampling error and the
126:17 - central limit theorem a population is
126:20 - the set of all members about witches
126:22 - study intends to make inferences here's
126:24 - a population of people
126:25 - we'd like to study there television
126:27 - watching behavior to determine how many
126:28 - watch a particular show so we can decide
126:30 - whether to purchase advertising spots
126:32 - during this period of time but our
126:34 - population is much too large for a
126:36 - feasible study to study their television
126:38 - viewing habits we will have to survey
126:39 - them and it's not feasible to survey
126:41 - every single individual so instead we'll
126:43 - take a sample of the population to study
126:45 - choosing a representative sample we can
126:47 - make some inferences about the
126:48 - population behavior but it's unlikely
126:50 - that one sample can provide accurate
126:52 - measures of behavior for the entire
126:53 - population an estimate of the population
126:55 - parameter or the proportion watching a
126:57 - television show is likely to be
126:59 - different for different samples of the
127:01 - same size and is likely to be different
127:02 - from the population parameter this is
127:04 - called sampling error the sampling error
127:06 - is unknown but we can estimate the
127:08 - extent of this error by applying the
127:09 - central limit theorem the central limit
127:11 - theorem tells us that what we know about
127:13 - our sample can tell us about the larger
127:14 - population the sample came from for any
127:16 - results that are generated from samples
127:18 - we get a range of estimates of a
127:19 - population parameter which includes mean
127:21 - and standard deviation from each sample
127:23 - in our example it would be an estimate
127:25 - of the proportion watching a particular
127:27 - TV show these estimates have their own
127:28 - distribution and the central limit
127:30 - theorem tells us that the distribution
127:32 - looks like a bell curve the central
127:33 - limit theorem makes predicting outcomes
127:34 - a lot easier if the sample size is large
127:37 - enough then the sampling distribution of
127:39 - the mean is approximately normally
127:40 - distributed regardless of the
127:42 - distribution of the population if all
127:43 - possible random samples each of size n
127:45 - are taken from any population with the
127:48 - mean mu and a standard deviation Sigma
127:50 - the sampling distribution of the sample
127:52 - means or averages will have a mean have
127:54 - a standard deviation and be
127:55 - approximately normally distributed
127:57 - regardless of the shape of the parent
127:59 - population
127:59 - remember that normality improves with a
128:01 - larger n and it all comes back to Z note
128:04 - the symbols here the mean of the sample
128:06 - means is noted as mu of x bar the
128:08 - standard deviation of the sample means
128:10 - is written as Sigma of X bar and is also
128:12 - called the standard error of the sample
128:14 - mean that concludes our video today we
128:16 - covered populations and inferences
128:18 - sampling error and the central limit
128:20 - theorem
128:20 - [Music]
128:31 - in this video we will first define
128:33 - probability then we will cover the rule
128:36 - of complements the addition rule
128:38 - probabilistic independence conditional
128:41 - probability and the Bayes theorem a
128:43 - probability is a number between 0 and 1
128:45 - that measures the likelihood that some
128:47 - event will occur for a random variable
128:48 - an event with probability 0 cannot occur
128:51 - whereas an event with probability 1 is
128:54 - certain to occur an event with
128:55 - probability greater than 0 and less than
128:57 - 1 involves uncertainty here are some
129:00 - examples the odds of winning a lottery
129:02 - the likelihood of a particular candidate
129:04 - winning an election or the chance of
129:06 - rolling a 4 on a fair die in the case of
129:09 - the die there are 6 sides so the odds of
129:12 - rolling of four are one out of six the
129:14 - complementary rule in probability is
129:16 - simply the probability of an event not
129:18 - occurring if a is any event the
129:20 - probability of a is P of a the
129:23 - complement of a is the event that a does
129:26 - not occur the probability of the
129:28 - complement of a is shown by this
129:29 - equation one minus the probability of
129:31 - the event occurring in our dice example
129:33 - the probability of getting a four was
129:35 - one in six so the probability of not
129:37 - getting a four is one minus one and six
129:40 - which equals five and six the addition
129:43 - rule of probability involves the
129:44 - probability that at least one of the
129:46 - events will occur events are exhaustive
129:48 - if they exhaust all possibilities one of
129:50 - the events must occur for example when
129:52 - we roll a 6-sided die we will always end
129:54 - up with a number between 1 & 6 we say
129:57 - that events are mutually exclusive if at
129:59 - most one of them can occur for example
130:00 - you can't roll a 3 & a 6 on one die at
130:03 - the same time if you have two mutually
130:05 - exclusive events like our 3 & 6 then the
130:07 - probability of either one occurring is
130:09 - the sum of the two separate
130:10 - probabilities if two events are
130:13 - independent or their outcomes aren't
130:15 - affected by each other then the
130:16 - probability of both a and B occurring is
130:18 - simply the product of the two
130:20 - probabilities in the case of our die the
130:22 - probability of getting a six on the
130:23 - first roll and getting a three on the
130:25 - second roll is one in six times one and
130:28 - six which equals a 1 in 36 chance
130:30 - sometimes the probability of one event
130:32 - will affect another these are called
130:34 - dependent events and their probabilities
130:36 - are called conditional this is the
130:38 - formula for conditional probability the
130:40 - conditional probability of a conditional
130:42 - that B has already occurred
130:44 - is equal to the joint probability of
130:46 - both of the events occurring together
130:47 - divided by the probability of B
130:49 - occurring without regard to whether a
130:51 - has occurred or not
130:53 - the bayes theorem allows us to estimate
130:55 - posterior probabilities once we obtain
130:58 - new data with it we can measure the
130:59 - likelihood of event H occurring once we
131:02 - obtain particular pieces of evidence
131:04 - from data D the parts of the theorem
131:06 - include the independent probability of H
131:08 - or prior probability the independent
131:11 - probability of D the conditional
131:13 - probability of D given H or likelihood
131:15 - and conditional probability of H given D
131:18 - or posterior probability this concludes
131:21 - our video on basic probability today we
131:24 - defined probability and covered the rule
131:26 - of complements the addition rule
131:27 - probabilistic independence conditional
131:30 - probability and the Bayes theorem
131:34 - [Music]
131:47 - in this video we will cover variable
131:50 - roles including explanatory and outcome
131:52 - variables and variable classification
131:54 - including qualitative variables nominal
131:57 - ordinal and binary and quantitative
131:59 - variables discrete continuous interval
132:02 - and ratio any analytics project first
132:05 - begins with a question what is the
132:07 - problem you're trying to solve to
132:09 - address that question we need to collect
132:10 - data the next step in the process is to
132:13 - understand the data collected and only
132:15 - then can we move to further steps of
132:17 - data cleaning data analysis and solving
132:19 - the problem a key step in understanding
132:21 - the information collected is to identify
132:23 - all the variables in the data set we
132:25 - need to know what variable types we have
132:28 - in order to make them amenable to
132:29 - further analysis variables have two
132:32 - possible roles
132:33 - the first is explanatory explanatory
132:36 - variables are also called features or
132:38 - independent variables these are
132:40 - variables that are used as inputs to
132:42 - explain the variation in the outcome
132:44 - variable
132:44 - the second role a variable can take on
132:46 - is outcome an outcome variable is also
132:49 - known as a target or dependent variable
132:51 - these are variables that measure the
132:53 - output or impact that's being studied
132:55 - most studies have many independent
132:57 - variables and one dependent variable for
132:59 - example a person's weight could be a
133:01 - function of age gender and calories
133:03 - consumed fuel efficiency is a function
133:05 - of features such as car size weight and
133:08 - number of cylinders restaurant ratings
133:10 - are a function of food quality ambiance
133:12 - and service variables can be qualitative
133:15 - or quantitative qualitative data can be
133:17 - nominal ordinal or binary quantitative
133:21 - data can be discrete or continuous with
133:23 - either an interval or ratio level of
133:25 - measurement we'll start by discussing
133:27 - how to determine whether a variable is
133:29 - qualitative or quantitative the best way
133:31 - to decide whether a variable is
133:32 - qualitative or quantitative is to use
133:34 - the subtraction test if two experimental
133:37 - units such as people have different
133:39 - values for a particular measure then you
133:41 - should subtract the two values and ask
133:42 - yourself about the meaning of the
133:44 - difference for example when hair color
133:46 - is coded as 1 equals blonde 2 equals red
133:49 - 3 equals brown and 4 equals black the
133:52 - difference between the variables has no
133:53 - meaning so it fails the subtraction test
133:56 - which means hair color is a categorical
133:58 - or qualitative variable however if the
134:00 - difference
134:00 - is meaningful then it is a quantitative
134:02 - variable for example age in years the
134:05 - differences between these numbers have a
134:07 - meaning so the variable is quantitative
134:08 - we will now discuss qualitative
134:11 - variables in detail categorical
134:13 - variables are those that have only a few
134:15 - possible values
134:16 - thus assigning each value to a
134:17 - particular group or category for example
134:20 - oceans are categorical variable nominal
134:23 - and ordinal variables are often called
134:25 - labels a nominal variable has levels
134:28 - with arbitrary names for example car
134:30 - colors ordinal variables have a logical
134:33 - order for example exam grades a
134:35 - dichotomous or binary variable is a
134:38 - categorical variable that has only two
134:40 - levels or categories often the answer to
134:42 - a yes or no question but a variable
134:44 - doesn't have to be a yes/no variable to
134:46 - be binary it just has to have only two
134:48 - categories such as gender we will now
134:50 - discuss quantitative variables in detail
134:52 - quantitative variables are those for
134:54 - which the recorded numbers encode
134:55 - magnitude information based on a true
134:57 - quantitative scale they can be discrete
134:59 - or continuous a discrete variable has
135:01 - only whole number counts a continuous
135:04 - variable can take on any value on the
135:05 - number scale to determine whether a
135:07 - variable is discrete or continuous use
135:09 - the midway test if for every pair of
135:12 - values of a quantitative variable the
135:14 - value midway between them is a
135:16 - meaningful value then the variable is
135:18 - continuous otherwise it's discrete for
135:21 - example age is continuous because the
135:23 - difference between ages 20 and 30 is
135:25 - meaningful
135:27 - an example of a discrete variable is the
135:30 - number of children in a family you can
135:32 - see here that 2.5 does not make sense
135:34 - the interval level of measurement ranks
135:36 - data it can be either discrete or
135:38 - continuous with interval variables
135:40 - precise differences between units of
135:42 - measure exist but there's no meaningful
135:44 - 0 for example take IQ scores make sense
135:47 - to talk about someone having an IQ 50
135:49 - points higher than another person but an
135:51 - IQ of 0 has no meaning ratio variables
135:54 - are interval variables but with the
135:56 - added condition that 0 of the
135:57 - measurement indicates that there is none
135:59 - of that variable true ratios exist when
136:01 - the same variable is measured on two
136:03 - different members of the population for
136:05 - example consider the weight of an
136:06 - individual it makes sense to say that a
136:08 - hundred and fifty pound adult weighs
136:10 - twice as much as a 75 pound child
136:12 - however it doesn't make sense to say
136:14 - that 70 degrees Fahrenheit is twice as
136:16 - hot as 35 degrees Fahrenheit so
136:18 - temperature is not a ratio variable this
136:21 - concludes our video on variables in this
136:23 - video we covered variable roles
136:24 - including explanatory and outcome
136:27 - variables and we also covered variable
136:29 - classification including qualitative
136:31 - variables nominal ordinal and binary and
136:34 - quantitative variables discrete
136:36 - continuous interval and ratio
136:40 - [Music]
136:50 - this video will cover basic information
136:53 - about coding coding systems and types of
136:56 - variables in coding including binary
136:58 - ordinal nominal and continuous coding is
137:02 - the process of translating the
137:03 - information gathered from questionnaires
137:05 - and other investigations into something
137:07 - that can be analyzed usually using a
137:09 - computer program coding involves
137:11 - assigning a value to the information
137:13 - given in a questionnaire and often that
137:15 - value is given a label coding can make
137:17 - the data more consistent for example if
137:19 - you ask the question what gender you
137:21 - might end up with the answers male
137:22 - female M F etc coding will avoid such
137:26 - inconsistencies a common coding system
137:29 - for binary variables is the following
137:30 - zero equals no and one equals yes where
137:34 - the number is the value assigned and the
137:36 - yes or no is the label of that value
137:38 - some like to use a system of ones and
137:40 - twos where one equals no and two equals
137:43 - yes this brings out an important point
137:45 - in coding when you assign a value to a
137:47 - piece of information you must also make
137:48 - it clear what the value means in the
137:51 - first example one equals yes but in the
137:52 - second example one equals no either way
137:55 - is fine as long as it's clear how the
137:56 - data are coded you can make it clear by
137:58 - creating a data dictionary as a separate
138:00 - file to accompany the data set a binary
138:03 - variable is any variable that is coded
138:05 - to have two levels like this example in
138:07 - SAS data representing gender coded as MF
138:11 - would be converted into a binary
138:12 - variable here's an example
138:15 - if we're asking about the number of
138:17 - years of education a person has with a
138:19 - value of 1 for each year of education
138:22 - that would mean anyone with more than 12
138:24 - years of education has been to college
138:25 - and anyone with less than 12 years of
138:28 - education has not been to college we can
138:30 - recode into a binary yes/no variable by
138:33 - saying that if education is greater than
138:35 - 12 that implies that college equals one
138:38 - otherwise college equals zero this type
138:41 - of coding is useful in descriptive and
138:43 - predictive analytics
138:45 - the coding process is similar with other
138:48 - categorical variables for the variable
138:50 - education we might code as follows zero
138:53 - equals did not graduate from high school
138:54 - one equals high school graduate two
138:57 - equals some college or post high school
138:58 - education and three equals college
139:01 - graduate note that for this ordinal
139:03 - categorical variable we need to be
139:05 - consistent with the numbering because
139:07 - the value of the code assigned has
139:08 - significance the higher the code the
139:10 - more educated the respondent is in SAS
139:13 - we would convert years of education to
139:15 - education categories like this
139:21 - here's an example of what not to do zero
139:23 - equals some college or post high school
139:25 - education one equals high school
139:27 - graduate two equals college graduate and
139:30 - three equals did not graduate from high
139:32 - school can you tell what's wrong with
139:33 - this example the data we're trying to
139:36 - code has an inherent order but the
139:38 - coding in this example does not follow
139:39 - that order here's the correct way to do
139:41 - it
139:45 - for nominal categorical variables
139:47 - however the order makes no difference
139:48 - here's an example for the variable
139:51 - reside 1 equals northeast 2 equals South
139:55 - 3 equals northwest 4 equals Midwest and
139:58 - 5 equals southwest it doesn't matter
140:01 - what order we use for these categories
140:02 - Midwest can be coded as 4 2 or 5 because
140:06 - there's not an ordered value associated
140:07 - with each response continuous variables
140:10 - are usually left in the same format as
140:12 - they are in the original data set
140:13 - however be careful about missing values
140:15 - in Muscoda data you may also need to
140:17 - code responses from fill in the blank
140:19 - and open-ended questions with an
140:21 - open-ended question such as why did you
140:23 - choose not to see a doctor about this
140:25 - illness respondents will all answer
140:27 - differently also you may give response
140:30 - choices for a particular question but
140:31 - offer an other specify option as well
140:34 - where respondents can write whatever
140:35 - response they choose these types of
140:38 - open-ended questions can be a lot of
140:39 - work to analyze one way to analyze the
140:42 - information is to group together
140:43 - responses with similar themes for the
140:45 - question why did you choose not to see a
140:47 - doctor about this illness responses such
140:50 - as didn't feel sick enough to see a
140:51 - doctor
140:52 - symptoms stopped and the illness didn't
140:54 - last very long could all be grouped
140:56 - together as the illness was not severe
140:58 - you will also need to code don't know
141:01 - responses typically don't know is coded
141:03 - as 9 that concludes our video on coding
141:06 - with variables today we covered some
141:08 - basic information about coding coding
141:10 - systems and types of variables in coding
141:12 - including binary ordinal nominal and
141:16 - continuous
141:19 - [Music]
141:36 - this video will cover using graphs to
141:39 - understand data we will cover three
141:41 - types of graphs bar charts box plots and
141:44 - histograms it's important to know which
141:47 - graph to use if the variable is
141:50 - categorical look at it using a bar chart
141:52 - if it is continuous you should examine
141:54 - it using either a box and whisker plot
141:56 - or a histogram a bar chart translates
142:00 - the data from frequency tables into a
142:02 - pictorial representation it depicts
142:04 - categorical variables and shows
142:06 - frequency or proportion in each category
142:08 - this bar chart looks at the frequency
142:11 - distribution of patients with pulmonary
142:12 - embolism which occurs when one or more
142:15 - arteries and the lungs gets blocked by a
142:17 - blood clot notice that this is a binary
142:18 - variable with only two possible
142:20 - responses yes and no yes is coded as one
142:24 - and no is coded as zero the frequency
142:26 - distribution of a binary variable shows
142:29 - the number of patients in each group
142:30 - it's much easier to extract information
142:32 - from a bar chart than from a table this
142:35 - chart depicts shock index which is the
142:36 - ratio of heart rate to blood pressure
142:38 - and should lie between 0.5 and 0.8 the
142:41 - higher it is the greater the risk shock
142:43 - index is an ordinal categorical variable
142:46 - the vertical bars here represent the
142:48 - number of patients in each category the
142:50 - shape of a distribution describes how
142:52 - the data are distributed measures of
142:54 - shape include symmetric and skewed the
142:56 - first thing we're looking for in any
142:57 - continuous variable is the shape of
142:59 - distribution what are the boundaries of
143:01 - the data points and how are they
143:03 - clustered if a few small values are
143:05 - mixed in with a majority of values being
143:07 - much higher the data will have a left or
143:09 - negative skew likewise if we have some
143:11 - large values mixed in with a majority of
143:13 - small values the distribution will have
143:15 - a right skew or we say it is positively
143:17 - skewed if the distribution is balanced
143:19 - it is symmetric we can also observe
143:21 - skewness by inspecting the values for
143:24 - instance consider the numeric sequence
143:26 - 49 50 51 whose values are evenly
143:29 - distributed around a central value of 50
143:32 - this produces a symmetric shape we can
143:34 - transform the sequence into a negatively
143:36 - skewed distribution by adding a value
143:38 - far below the mean 40 49 50 51 this
143:42 - produces a left skew similarly we can
143:45 - make this sequence positively skewed by
143:46 - adding a value far above the mean 49
143:49 - 50:51 60 this produces a right skew a
143:53 - box-and-whisker plot provides an easy
143:55 - way to examine the entire distribution
143:57 - of a variable and it's also very useful
143:59 - when we want to examine relationships
144:01 - between two variables where one is
144:02 - categorical and another is continuous
144:04 - let's look at the Box first the bottom
144:07 - of the box represents the 25th
144:09 - percentile while the top of the box
144:11 - represents the 75th percentile the line
144:13 - in the middle represents the median the
144:15 - bigger the box the greater the spread of
144:17 - the data the whiskers in the box plot do
144:19 - not necessarily represent the minimum
144:21 - and maximum values they show the minimum
144:23 - and maximum only if these values are
144:25 - less than one and a half times the
144:27 - interquartile range if the values are
144:29 - bigger than that the whiskers represent
144:31 - one point five times the interquartile
144:33 - range or IQR values outside that range
144:36 - represented as dots in the example here
144:38 - note that there are many dots above the
144:40 - top whisker this is a quick and easy way
144:42 - to check for outliers we can look at the
144:45 - same shock index data with a histogram
144:46 - the dots in the box plot showed us that
144:49 - there were several large values greater
144:51 - than one point five times the
144:52 - interquartile range the same is
144:54 - represented in this histogram with the
144:56 - right skew there's no single rule of
144:58 - thumb for choosing bin sizes the bin
145:00 - sizes you choose will depend on the
145:02 - research question you're asking here
145:04 - using 100 bins shows too much detail and
145:06 - it's not useful
145:07 - likewise too few bins tells us little
145:09 - about the underlying shape of the
145:11 - distribution this example uses two bins
145:13 - and provides too little detail note that
145:15 - the box plot we looked at earlier shows
145:17 - the same positive or right skew that we
145:19 - observe in a histogram for shock index
145:21 - the median inside the box plot also
145:23 - provides information on the skewness of
145:25 - the distribution if the median is at the
145:27 - center of the box the distribution is
145:29 - symmetric if the data have a left skew
145:31 - then the median will be pulled to the
145:33 - right inside the box if the data have a
145:35 - right skew
145:35 - then the median will be pulled to the
145:37 - left in the box this concludes our video
145:38 - on using graphs to understand data today
145:41 - we covered three types of graphs bar
145:42 - charts box plots and histograms
145:47 - [Music]
146:00 - this video provides a quick review of
146:03 - the measures of central tendency
146:04 - including mean median and mode variation
146:07 - and dispersion range for tiles and
146:09 - interquartile range sample variance
146:11 - standard deviation and the normal curve
146:13 - as well as the 6895 99.7 rule before we
146:18 - begin here's a quick review of symbols
146:20 - we'll use in this video
146:25 - we will also use the abbreviation IQR
146:28 - for interquartile range the mean is the
146:31 - average or balancing point to find the
146:32 - mean find the sum of all the values
146:34 - divided by the sample size here's a
146:37 - simple example of calculating the mean
146:38 - of the age of several participants in a
146:40 - study the Sigma is the sum and the x-bar
146:43 - is the sample mean
146:43 - after adding the values together and
146:45 - dividing by the number of values 8 we
146:47 - arrived at our mean 23 point 2 5 we can
146:49 - construct means of binary variables the
146:52 - mean of a binary variable represents the
146:54 - percentage of one's the mean is affected
146:56 - by extreme values which is why we often
146:58 - look at means in conjunction with
147:00 - medians to understand how the data are
147:02 - distributed in this example the mean of
147:04 - the values 1 2 3 4 and 5 is calculated
147:08 - by adding the values together to make 15
147:10 - then dividing the values by 5 the mean
147:12 - of this group is 3 however if the values
147:16 - are 0 1 2 3 4 and 10 the mean shifts to
147:20 - 4 the median is the middle value of the
147:23 - data in this example we have 7 different
147:25 - ages to find the mean we first order
147:28 - them from smallest to largest and then
147:29 - locate the value in the center
147:32 - however if we have an even number of
147:34 - observations median is computed as the
147:36 - average of the two middle values
147:41 - the median is not impacted by outliers
147:44 - here our median of the five values is
147:46 - three if we add the value ten to the set
147:50 - of values our median is still three
147:54 - the mode is the value that occurs most
147:56 - frequently it is only useful when we
147:58 - have some values clustering together in
148:00 - this example the mode is 9 there may be
148:03 - no mode or there may be several modes
148:08 - there is no single measure of center
148:10 - that is best if the data are normally
148:12 - distributed then mean is used
148:14 - however if data are not normally
148:16 - distributed the median is a better
148:17 - measure often we use both to understand
148:20 - the underlying structure of the
148:21 - distribution
148:22 - there are several measures to examine
148:24 - the spread of the data they include
148:25 - range percentiles interquartile range
148:28 - and variance or standard deviation their
148:30 - range is the difference between the
148:31 - largest and the smallest value this
148:34 - histogram shows a minimum value of 15
148:36 - and a maximum value of 94 the ranges 94
148:40 - minus 15 equals 79 another measure of
148:43 - spread is the value of each quartile we
148:45 - take the total number of data points we
148:47 - have and divide them into four parts
148:48 - the value corresponding to the end point
148:51 - of each part is the quartile value the
148:53 - interquartile range is the difference
148:54 - between the value at the third quartile
148:56 - minus the value at the first quartile
148:58 - the first quartile q1 is the value for
149:01 - which 25% of the observations are
149:03 - smaller and 75% are larger the second
149:07 - quartile q2 is the same as the median
149:09 - 50% are smaller and 50% are larger only
149:13 - 25% of the observations are greater than
149:15 - the third quartile let's take the age
149:18 - values 1535 49 65 and 94 the first
149:23 - quartile is at 35 this means that 25% of
149:26 - the participants are below age 35
149:30 - like why is 25% are above 65 years old
149:33 - the interquartile range is 65 minus 35
149:36 - equals 30 years sample variance is
149:39 - calculated as the average of squared
149:41 - deviations of values from the mean as
149:43 - shown here we square the differences
149:44 - from the mean to provide equal weight to
149:46 - observations below the mean versus those
149:48 - above the mean because we square the
149:50 - difference values that are further away
149:51 - from the mean get higher weight than
149:53 - those close to the mean standard
149:55 - deviation is the most commonly used
149:56 - measure of variation it shows the
149:58 - variation around the mean and has the
150:00 - same units as the original data it is
150:02 - calculated by finding the square root of
150:03 - the variance here's an example of the
150:06 - standard deviation using age data note
150:08 - that sample standard deviation is
150:10 - represented by the symbol s X bar
150:12 - represents the sample mean the standard
150:15 - deviation is an extremely useful measure
150:17 - it tells us how close or far apart data
150:19 - are from the mean the higher the
150:21 - standard deviation the greater the
150:23 - spread of the data here in red is an
150:25 - example of a moderate standard deviation
150:27 - you can see that the data is spread
150:29 - pretty evenly the purple shows a low
150:31 - standard deviation in which the data is
150:33 - concentrated near the middle the blue
150:35 - example shows a high standard deviation
150:37 - where the data is concentrated on the
150:39 - outside these formulas are important to
150:42 - know well while software can compute
150:44 - these for you it's important to know how
150:46 - it's done using simple numbers whenever
150:48 - you work with data you'll have variables
150:50 - that have a center and spread a very
150:52 - useful rule to know is that no matter
150:54 - what the shape of the distribution 75%
150:57 - of values will lie within two standard
150:58 - deviations of the mean while 89 percent
151:01 - will lie three standard deviations from
151:03 - the mean so if someone gives you just
151:05 - these two pieces of information you can
151:07 - make some predictions on where a new
151:09 - data point will lie however what's even
151:11 - better in statistics is knowing that for
151:13 - large samples data are distributed
151:15 - symmetrically and follow the bell curve
151:17 - the 6895 99.7 rule states that 68% of
151:23 - the area of a normal curve lies within
151:25 - one standard deviation of the mean 95%
151:28 - of the area lies within two standard
151:29 - deviations of the mean and ninety-nine
151:32 - point seven percent of the area lies
151:33 - within three standard deviations of the
151:35 - mean
151:35 - this rule works for all normal curves no
151:39 - matter their shape that concludes our
151:41 - video on measures of central tendency
151:43 - including mean median and mode variation
151:46 - and dispersion range quartiles and
151:49 - interquartile range
151:50 - sample variance standard deviation the
151:53 - normal curve and the 6895 99.7 rule
151:59 - [Music]
152:07 - you

Cleaned transcript:

this video will cover introductory information about business analytics including a definition of business analytics the wisdom hierarchy data sources and business analytics terms applications history and uses business analytics is the process of transforming data into insights that support improve and/or automate business decisions the data can be of many types and from a variety of sources and there are many techniques and software packages that can be used for an analysis a simple way to think of business analytics is that it's the tools and processes used to find value in data to transform the raw data into information that can be acted upon a common method of understanding the relationship between data information knowledge and wisdom is by using a pyramid this graphic is called the di kW pyramid or the wisdom hierarchy and it illustrates that data is the foundation upon which decisions can be made information is defined in terms of data knowledge is defined in terms of information and wisdom is defined in terms of knowledge data are numbers or text without any context information provides meaning from data often combining multiple data points to produce a tangible idea knowledge provides context from the information making it directly applicable to a situation wisdom applies the knowledge to make a decision the original data has become useful enabling an action to be taken so where does data come from data can come from a variety of sources both internal and external internal data is collected by businesses and stored within their own servers this data can be generated in a number of ways either by physical objects such as sensors or barcodes or by using computer software such as websites while collecting and storing proprietary data sources is still very common there are now many external data sources that businesses can use public domain data sources such as government surveys or social media posts can be accessed and there are also many services that offer paid data sources like stock market or weather data these external data sources can be combined with a company's proprietary data to build a more complete picture of reality the amount of data that is being generated and collected is growing exponentially this growth is occurring due to the similar exponential trend in computing power along with a decrease in costs for digital storage because so much new data is being created and captured every year there's a corresponding growth of demand for business analysts there are many terms that are synonymous or semi synonymous to business analytics in the past few years data science has become the most common term to be used to describe this field other terms are often used interchangeably such as business intelligence big data data mining knowledge discovery and machine learning while there are many discussions about which term to use in which scenario all of the terms refer to the overall concept of using data to make better decisions or better products business analytics is an interdisciplinary subject based heavily on math and statistics it uses computer science principles and algorithms the math and computer science concepts are applied to a specific subject Business Analytics has applications in every field data on all our clicks on the Internet are used by retailers to figure out our preferences and provide targeted advertising analyzing data on past marketing campaigns provides a useful base to determine who to target for the next round supply chains in almost all industries have become far more efficient due to analytics organizations use historical data to figure out optimal numbers for staffing and hiring companies use demand data to determine optimal prices and professional sports teams use analytics such as when they look at player performance to determine who would be the best draft pick no matter which industry you work in there's room for analytics to add value although the popularity of analytics has recently peaked the field is nowhere as new as certain companies or people make it out to be since the computer was invented it was being used to process data to solve problems from decoding messages in World War 2 to generating weather forecasts in 1950 to modeling credit risk in 1958 of course these tasks involved enormous computational costs and only organizations with the most resources could attempt them toward the end of the 20th century as computing power became more affordable more organizations began collecting and storing data the types of analytical projects transitioned from being historical in nature to realtime in 1992 the first realtime credit card fraud system was introduced then the first analytically centric companies emerged companies such as Google used data to build their core product while other companies such as Amazon use analytical techniques to earn market share from competitors the rapid ascension of these tech companies has led to an arms race where all businesses have become committed to analytics businesses use analytics to gain an edge on their competitors and increase profits the three main areas in which they do this are competition to increase revenue from products or services sold efficiency to reduce the costs of resources or internal processes and customer satisfaction to improve the customer experience and encourage customer loyalty here's a case study as an example loyalty cards are used by grocery stores to uniquely identify their customers by requiring a loyalty card to obtain special discounts in the store the grocer can isolate habits of each customer and then provide customers with customized promotions to increase spending when a customer stops frequenting the store the grocer can mail coupons with aggressive offers the layout of a grocery store is constantly being changed to maximize customer spending this is why the milk section is always on the opposite side of the produce section so customers will have to traverse past every aisle to get to the two most commonly bought items each shelf is also analyzed to find the ideal arrangement more expensive items are typically placed at or around eye level while the cheaper products will be on the top or bottom shelves optimizing prices is another analytical technique used to maximize customer spending many grocery stores will have what are called loss leaders products that are very cheap to draw our customers into the store where they will inevitably spend more on other overpriced items grocers will also find the ideal times and prices to markdown expiring products preventing the product from being thrown away at a complete loss this concludes our introductory video about business analytics today we covered a definition of business analytics the wisdom hierarchy data sources and business analytics terms applications history and uses this video will cover introductory information about business analytics including the role of the business analyst what makes a business analyst successful and an overview of business analytics tools project outcomes and the analytical process there are three main reasons business analytics as an enticing career choice the first is that there's a high demand for business analysts and the relatively low supply of skilled workers means that salaries are higher in this field another reason is for the challenge of solving interesting problems analysts are typically people who are interested in solving complex puzzles finally those who are curious about how things work can use their skills in analyzing data to uncover previously unknown truths a business analyst can take many roles depending on the data and the type of project the most common roles are that of an interpreter in which the analyst uses descriptive analytics to tell the story of what happened an oracle and which analysts use predictive analytics to predict future events and a console in which the analyst uses prescriptive analytics to provide advice on the best course of action an analyst becomes successful due to a combination of hard and soft skills the hard skills are more tangible and refer to what the analyst can do with what tools while the soft skills are less flashy on a resume but equally or even more important than the hard skills analytical tools can be separated into two categories software that requires coding and software in a graphical user interface or GUI that is based on pointandclick interaction the main benefit of writing code is that it allows for more flexibility there are more features and it allows for more possibilities the drawback of coding is the extended learning curve within the last decade there have been many new GUI programs that make analytics easier to implement without the need for writing code programs such as tableau Alteryx and rapid miner have started gaining market share going along with older tools such as SAS Enterprise guide but none of these tools have yet to replace the overwhelming popularity of code based software such as SAS r Python or SQL there are many different goals that an analytics project can strive for typically these goals fit into one of two categories the first is providing information about a business such as reports and dashboards for business stakeholders reports or presentations provide onetime insights to explain events that have occurred and predict future events and dashboards are used by stakeholders for ongoing monitoring of key aspects of the business the second category is the production of analytical products in these types of projects the business's data becomes the input for a complex process that automatically produces in action this can take the form of features that offer a better experience for consumers for instance Amazon has an automated algorithm that determines products you might like to buy analytical products can also be built to make internal business processes more efficient an example of this is how credit card companies test every transaction for the probability of fraud an analytical project should start with the goals welldefined very rarely our project started to simply explore the data or find hidden truths collecting an inventory of all the relevant data sources is also an important step in the beginning of a project finally every analysis should begin with an effort to better understand the data this should be done before any analytical techniques are used simply observe the data files and write down a list of observations questions and any other ideas you have this process called creating disfluency enhances the data dictionary and helps the analyst internalize elements of the data cognitive disability nabel's students who take lecture notes by writing to retain more of the material than students who type notes even though those who type can take notes more efficiently sometimes the more work we have to do to process the information the better we can understand the information using this principle at the beginning of a project before using any advanced analytical techniques enhances the analysts capability to understand the data after the initial stages of a project there are a few more steps in the analytical process the majority of time is spent exploring and preparing data and the exploration stage the analyst learns more about the variables including distributions and frequency of values and also identifies variables with missing or null values in the preparation stage the data becomes more useful as variables are transformed and anomalous values are rectified this is referred to as data cleaning another common task during this stage is to join data into as few sources as possible perhaps one of the most valuable techniques to use during the preparation stage is called feature engineering in which the analysts transform certain variables into new variables containing slightly different data this allows hidden aspects of variables to be analyzed for example a data set with a date variable can have a new variable added to determine whether that date is on a weekday or the weekend and this new variable could add important information that was not readily available in the original data source this is one of the areas of the analytical process where creativity is needed the last two steps in the analytical process are to build models and then to put these models into production a model is a type of mathematical equation that describes relationships among variables in a dataset often for the purpose of predicting an outcome by putting a model in production an automated decision can be made when new data is observed in some cases models are not the goal of a project rather the goal is to analyze data and communicate the findings in an analytical report presentation or dashboard visualization of the data is also a key component throughout the entire analytical process as the analyst attempts to learn more about the data this concludes our introductory video about business analytics today we covered the role of the business analyst what makes a business analyst successful and an overview of tools project outcomes and the analytical process this video will cover some of the foundations of Business Analytics selecting filtering and sorting there are many synonymous terms to describe the aspects of a typical data file which can be referred to as a table spreadsheet data set or data source along the horizontal axis are the variables which can also be called fields attributes or columns along the vertical axis are the observations also referred to as records tuples or rows fields that are sparsely populated meaning that a high percentage of records is missing should not be selected fields with redundant values meaning that a high percentage of records have the same value should also not be selected techniques to identify these fields vary for now a visual inspection of the first few records can be used from this table we can remove the state column because all of its values are the same we can remove the religion column because it is sparsely populated our table now depicts only the variables of interest once the variables of interest have been selected they can be renamed if needed and assigned the proper data type there are two common variable types numeric and categorical numeric variables include discrete variables which can include whole numbers used for counting and IDs that represent a unique entity and continuous variables which are numbers with decimals used for measuring categorical variables include text which is any combination of letters numbers and symbols and strings or characters and boolean or binary variables which contain only one of two possible values variables that can be treated as either numeric or categorical include dates such as date time date or time and spatial objects which show location such as latitude and longitude here are the variable types for our table person ID is a discrete numeric variable gender and city are string variables weight is a fixed decimal variable date of birth is a date and student is boolean software packages can automatically assign fields with data types and sometimes these can be assigned incorrectly examination of these data types is necessary to ensure proper utilization further downstream sometimes data formats require advanced data transformation and extraction techniques for example dates can have varying formats that can be interpreted as strings and parts of these strings need to be separated before the software can identify as a date field for now we'll assume that you can change a fields datatype without these advanced techniques the final aspect of the Select step is to assign the proper size for each variable many software packages can do this automatically but it's still a good idea to review the sizes because the size of each field has a direct impact on the amount of storage required for a data set specifying these can save system resources and processing time the common issue is for one record that contains many more characters than the typical record to cause the size to be much larger than need be a visual inspection of the table can highlight these instances but beware that changing a variable size can truncate or cut off some of the values while selecting limits a data set size by omitting certain columns a filter limits sighs by omitting certain rows a filter is also known as a condition subset or in SQL a where clause and it's commonly used for investigative purposes it's important to be aware of the information contained within the row of a data table also commonly referred to as a record or observation a row defines the level of detail that is contained in the data set for this example each row represents one person an ID fields such as person ID in which no rows contain the same ID can be used to determine that the tables level of detail is a person if there are multiple rows for the same ID we know that the rows reflect data from the same person in this example the level of detail is more granular it shows a record of one person's weight by day finding and understanding the level of detail for a table is necessary before analyzing the data after determining the tables level of detail you should check for duplicate observations these are rows of data in which all values are exactly the same as another row in some cases duplicate observations may be legitimate but usually these are erroneous and need to be removed here's our table with the erroneous observation removed this table is filtered to show only records of people who live in Raleigh let's try another filter here's our original data set again this table is filtered to show only records of people who weigh more than 180 pounds filters are different variables can be applied together combining above examples we can filter the original table for people who live in Raleigh and weigh more than 180 this results in only one observation the next step is sorting when we sort we rearrange a table by ordering the rows according to the values of one or more fields and either ascending or descending order here's our original data set it's sorted by date of birth in ascending order here we've sorted by city in ascending order and then by weight in descending order this concludes our video on selecting filtering and sorting this video will cover two types of formulas same row and multi row a commonly used method of data preparation is using existing fields to create new variables examples include adding subtracting multiplying dividing or applying another mathematical function to numeric field extracting and transforming substrings from a text field extracting truncating and parsing date parts from a date field conditional statements using ifthen and binning to create new variables and comparing values of two different fields to create a boolean variable formulas can be used for adding subtracting multiplying or dividing two numeric fields in this table we apply a formula to create the price column which is calculated by dividing units sold by total amount formulas can also be used when applying a mathematical function to a numeric field examples of mathematical functions are average floor finding the smallest value ceiling finding the largest value square square root absolute value trigonometric functions and logarithmic functions in this table we will create an observation average column using the average formula which averaged values from the observation 1 and observation two columns formulas can also be used to extract and transform strings from a text field there are a few common functions for transforming text and they are typically named differently depending on the software package being used the most common of these are upper lower use to change the field to all upper or lowercase characters concatenate which combines two or more strings into one field substring used to extract a portion of a string trim which can be used to remove certain characters usually spaces from a field index used to find the location of a certain string within a field and length which finds how many characters are in a field these are the basic string functions that almost every software package will include and can be combined to complete almost any string transformation for the following example we'll use two functions to transform a field that holds a city and state into two fields to extract city and state from the airport location field we would need to extract all the text before the comma as the city field and everything after the comma and space as the state field to do this we need two functions index and substring the index function will help us identify the location of the comma in the text field counting from the left including spaces this is done because the comma is not always in the same location then we use the substring function to extract everything before the comma as the formula for the city field and extract everything after the comma and space for the state field to get the city and state columns back into the format of the original field we would use a concatenate function we can also use formulas for extracting truncating and parsing date parts from a date field a date or date/time field carries more information than a typical variable the time of day the day of week the day of month the month of year the year number and the difference in the date from other dates such as today extracting this and other information out of a date field is a transforming technique that can add to the value of a data set during analysis and reporting the most heavily used date functions are today used to get today's date date part used to get a part of a date for example getting the month of date will return the numeric value of the month date truncate which will return a date value at the beginning of a period specified for example truncating a date at the month level will return the first day of the month for the date date ad which will add or subtract a certain number of periods to a date and date difference used to calculate the number of periods between two dates this table contains the major holidays for 2016 for each date value we can extract other features we can add a column with a numeric value to describe the day of the week we can subtract today's date from the date column to find the number of days remaining until the holiday occurs with formulas we can also compare values of two different fields to create a boolean variable a boolean variable is typically used to capture true/false values to create a boolean variable a logical statement or condition can be used in a formula this logical statement can compare two values of the same type numeric values strings and dates this table depicts the population of States we can use a formula to create a column that displays whether the state is on the East Coast we can also create a column to display whether the state has a high population formulas can also be used with conditional statements that if then and when binning to create new variables to create a variable with multiple possible outcomes conditional statements are needed these usually take the form of if condition1 is met then outcome 1 else if condition 2 is met then outcome 2 else outcome 3 while the conditions within these statements are the same as boolean formulas these formulas offer more flexibility as the analyst can assign any value when the condition is met and can use as many conditions and outcomes as needed while there are many applications of conditional statements one of the most common is tube in or tile numeric variables a process which transforms the continuous numeric variables into categorical variables in this table we compute sales volume with ifthen logic if sales is greater than 40000 then high else if sales is greater than 20,000 then medium else low this formula begins with testing the first condition if sales is greater than 40,000 and if it is true the first outcome high will be assigned the second condition else if less than 20,000 is evaluated only if the first condition is false if the second condition is true the second outcome medium will be assigned if neither the first nor second conditions are true then the final outcome low will be assigned sometimes data from a separate row or rows needs to be used when creating variables for this a multi row formula is needed a running total is the most basic multirow formula it adds the value of field of a current row to the value of a previous row this can be done across an entire data set for a particular variable or it can be grouped by certain dimensions a lag value looks at the data in preceding rows while lead values look at data and subsequent rows and is the opposite of lag window functions provide the ability to perform calculations like sum average and rank across sets of rows that are related to the current query row this is equivalent to aggregating the data set across one or more dimensions then joining the resulting data set back to the original this should be used carefully as values will be repeated for all levels of the dimension an example of this is in the transportation industry where a row typically represents one leg of a trip after sorting the data set properly the lag function can be used to combine rows so that the data describing the round trip is displayed total price is a running total of ticket price grouped by passenger ID notice how upon reaching the first row for a passenger ID the sum starts over and total price equals ticket price total flight time is a running total of flight time grouped by passenger ID lag of destination uses the lag function which doesn't consider any variables other than destination this means it's dependent on the sort order of the data set sum of ticket price is a window function which is the sum of the ticket price partitioned or grouped by passenger ID this concludes our video on single and multi row formulas this video will cover unions and joins union is also referred to as appending concatenating or combining two tables vertically or simply adding rows the table should have the same variable names types and sizes variables that are in only one table will receive null or missing values for the rows from the tables that do not contain them in this union are two tables one with three rows and one with one row will combine to create one larger table which has 4 rows notice the null values in the storage column storage was not contained in both original tables so the rows that do not contain data for storage will have null or missing values joining is also referred to as merging very rarely as data collected and stored in a format that's ideal for analysis typically data is stored across multiple tables in a relational database schema like you see here schemas are designed to optimize for storage so values that repeat often and take up more storage such as names are reduced to representative IDs that take up much less storage in a schema the tables that contain identifying information are called dimension tables and can sometimes be referred to as lookup tables our diagram has three so for example the product dimension table contains the name manufacturer and type of product each dimension table has a primary key that is used to identify a unique record the fact table is a record of events that happen for a combination of dimensions it's comprised of two things foreign keys and measures foreign keys map to primary keys of dimension tables these are the fields that will be used to join the tables together for example in this diagram the foreign keys are person ID store ID and Product ID which each connect to their own dimension table measures can be any type of variable we've already discussed in this diagram we have two measures units sold and total amount to join these tables together we will use an inner join this will return only rows that are contained in both tables when a fact table has no lore missing values for a dimension using an inner join will remove the entire row since the row may contain useful data and other fields it's better to use a left outer join to keep all values from the left table in this case the fact table and the values from the dimension table that match on the key while there are other types of joins as shown here in and left outer joins are the most commonly used first we will join transaction fact with person dimension next we will join the results from the previous join with store dimension finally we will join our results from the previous join with product dimension notice how each step creates a progressively larger table this concludes our video on unions and joins this video will cover a definition of aggregation as well as examples of aggregation aggregation is also referred to as a PivotTable group by statement or summarize aggregation transforms data into lower dimensions using summing averaging and counting the benefits of this are to answer basic questions of data sets with many different dimensions the most basic aggregation can be done on the level of detail for the table in this case each rows a transaction and the table as a whole represents sales for phones for quarter 1 of 2015 the answers can be calculated by applying a formula to a single field using all the rows in the table how many units were sold 9 how much total revenue 3500 $90 how many transactions six how many distinct products were sold three how many customers three how many stores had sales for what was the average price 398 dollars and 89 cents what was the average number of units sold per transaction one point five what was the average amount spent per transaction 598 dollars and 33 cents what was the average amount spent per customer 1196 dollars and 67 cents pay special attention to how the average function works it uses the sum of the field divided by the number of rows so it can't be used in averages such as this where the denominator is not the number of rows the next questions are related to aggregating one dimension at a time and can be calculated in one step for each of the questions the original table is grouped by the dimension of interest either the person ID store ID product ID or date for each value of the dimension chosen calculations are performed using the measures the number of rows and the number of distinct IDs a new data table is the result of these calculations you notice that when grouping by a dimension the dimension is sorted in the output for this example we'll aggregate at the person level of detail to answer the following questions how many units to each person by how much money did each person spend for each person ID we need to calculate the sum of units sold and the sum of the total amount starting with person ID 1 we see that there were 1 plus 1 equals 2 units sold and 365 plus 425 equals seven hundred and ninety dollars spent these values are populated in the resulting table similar calculations are done for every person ID in the original table you aggregating at the store level of detail can provide basic information about store performance to find the number of transactions each store had each occurrence of a store ID needs to be counted store ID 101 has two rows in the original table indicating that there were two total transactions each store ID has its number of rows counted and the final result is the num transactions column in the newly created table aggregating at the product level of detail can answer basic questions about each product such as what was the average price of each product how many distinct customers bought each product starting with product ID one zero zero one average price is calculated as the sum of the total amount 1625 divided by the sum of units sold for the final value of 406 twenty five is populated into the resulting table again be aware that using an average function in this case would not give us the expected result distinct customers is calculated by counting the distinct number of person IDs for Product ID one zero zero one there are two person ID 1 and person ID 2 the calculations are then completed for every product ID the final example uses the month and year components of the date field to calculate all possible levels of aggregation which were shown on the previous slides how many units were sold each month how much revenue each month how many transactions each month what was the average price how many distinct products were sold each month you the final type of questions that can be asked involve combinations of multiple dimensions one of the most common combinations is to aggregate by a date dimension along with another dimension for this calculation each combination of the dimensions chosen for grouping are performed in the transaction table there are four such combinations transactions for person ID one are only in March of 2015 there's only one transaction for person ID to occurring in February 2015 and there are three transactions for person ID for two occurring in January and one occurring in February of 2015 within each of these combinations each of the calculations are performed here will show you the first row as an example you note that the more unique dimensions a data table contains the more the number of possible levels of aggregations increases in this example table with four unique dimensions there are fourteen different possible levels of aggregation each different level of aggregation can offer a unique insight but it's best to start with two or fewer levels example questions that can be answered using the example transaction data set include which customers tend to buy which products which stores two customers tend to frequent what products sell well in what stores and during what months do they sell more on average here's one final example using multiple concepts that have been covered using the transaction fact table we want to answer the question which stores had products that were on sale answering this question is a multistep process the first step is to find the average price for each product for this we aggregate by product ID and for each Product ID we calculate the average price for example product one zero zero one sold for four hundred and twenty five dollars and four hundred dollars the average of these two values is calculated and then entered into the resulting table values for subsequent IDs are calculated in a similar manner you we also need to aggregate at the store and product level this removes the customer and date level information giving us the totals for each store and product combination due to the limitations of this example the aggregation does not result in fewer rows as it normally would the resulting table is however sorted by store ID and Product ID the next step is to join the product aggregate table to the store product aggregate table using product ID as the common field between the two tables this results in a table with a similar structure as the store product aggregate table with the only difference being the new column average product price we filter the resulting table to only include rows where price is less than average product price there are only two rows where this is true finally we join this table with the store dimension and product dimension tables adding store name and product name to the table is necessary to fully answer the original question the resulting table makes it clear that the g2 went on sale at Best Buy and the s6 went on sale at h/h Greg this example is a simplified an illustration of how data preparation techniques are often used together to answer typical questions of the data this concludes our video on aggregation today we covered a definition of aggregation and examples of aggregation this video will cover cross tabs and transposing cross tabs are also called pivot tables they are used to compare one measure across two or more dimensions one dimension will have its values transformed into columns and the other dimensions will be aggregated the visual result will be a table with fewer rows but more columns for example this table shows sales by region and month the data is stored in a form similar to how it would be captured with one row for every combination of region and month a crosstab function is performed with region defined as the grouping field month defined as the header field and sales defined as the data field there can be multiple grouping fields but only one header field and one data field are allowed cross tabs are an effective way to summarize and present data as trends within the data are easier to identify and data visualization color is added to build a heatmap cross tabs are also used in statistics to build contingency tables transposing can be thought of as the opposite of cross tabs it transforms the data from a wide format into a narrow format typically transposing is required after receiving financial data from a spreadsheet as columns can be used to capture dates locations or categories once the columns known as data fields are transposed into one variable the data is ready for a more sophisticated analysis any fields that are not to be transposed are called key fields for this example we can use the result of the crosstab function from the previous section region is the only key field while January February and March are the data fields to be transposed transposing results in the original data set notice how the column names are transformed into values for the newly created month column this concludes our video on cross tabs and transposing this video will cover contingency tables first we will discuss the definition of a contingency table and then the steps for creating one finally we will discuss chisquare distributions in statistics a contingency table is a type of table in a matrix format that displays the multivariate frequency distribution of variables contingency tables are heavily used in Survey Research Business Intelligence engineering and scientific research they provide a basic picture of the inter relation between two or more variables and can help find interactions a contingency table is also referred to as a twoway frequency table here's an example given this table can you calculate the following metrics the number of males who are righthanded the percent of males who are lefthanded whether more males are lefthanded than females or the percent of lefthanded people who are females in a table such as this notice that there are no numerical values the person ID is a numerical identifier but the numbers are arbitrary so there are no obvious calculations to perform however because each row represents one person we can count the number of rows along each dimension gender and dominant hand to analyze how the dimensions are distributed to create a contingency table the first step is to aggregate the original data set along two dimensions gender and handedness and count the number of rows for each combination of values since there are two possible values for gender male and female and two possible values for dominant hand left and right there are four total possible combinations male and right male and left female and right and female and left for each of these combinations we count the number of rows that contain both values at this point we can extract information regarding the quantity of each combination allowing us to answer basic questions such as how many females are lefthanded and are there more males or females who are lefthanded next we perform a crosstab on the result moving the dominant hand to the horizontal axis this will make interpretation of the results much easier and allow us to easily calculate totals each dimension there are other questions however that are harder to answer and these deal with the proportions of each combination what proportion are percent of males are lefthanded and are a greater proportion of males lefthanded than the proportion of females for these questions the proportion of gender we need to divide the values in the cells by the totals along the gender axis which in this case is the vertical axis the resulting table called a row conditional frequency table will have 100% values in each cell in the total column the cells for each row will account for all the people within the category male and female of the gender dimension the table can be easily read eightythree percent of males are righthanded and only 8 percent of females are lefthanded but what if we want to know what percent of lefthanded people are female this is a different type of question one that can be answered by dividing the original values of the cells by the totals along the dominant hand or horizontal axis of the contingency table these are called column relative frequencies now the results can be interpreted in two sentences by focusing on the columns in the contingency table 49% of righthanded people are males 31% of lefthanded people are females etc but what if we want to answer questions about how prevalent each separate combination is among all people in this data set for instance what percent of the people are lefthanded males for that we divide the original values by the total number of rows in the data set for our example this makes for an easy calculation since there were 100 rows with this table we can answer many basic questions about the data set including what percent of the people in the data set are righthanded what percent of the people in the data set are female and what percent of the people on the data set are lefthanded females and so forth note that the language used what percent of people in the data set is different from the easier to say what percent of people this is because the data set being used is a sample of the population and to infer the trends about the population will need to use a statistical technique for a contingency table a chi square distribution can be used to make such an inference to do so however assumes that the sample you are using was acquired from the population randomly the chisquare distribution compares the actual values to the expected values to determine if the actual numbers that were observed and recorded in the data set are due to chance or if there's a difference between the two variables that cannot be explained by chance in this example we want to determine if the observed difference in the proportion of females who are lefthanded is really smaller than the proportion of males who are lefthanded or if that observation could be due to chance in other words we want to know if the dominant hand is dependent on gender to calculate the expected values for each cell multiply the relative horizontal and vertical dimension totals and divide by the number of total observations in this example to calculate the expected value for righthanded males multiply 52 the total number of males by 87 the total number of righthanded people and divide by 100 the total number of people similar calculations are done for each cell once the expected values have been computed the chisquare test can be run in Excel the chisquare test function uses the actual table and expected table as inputs to calculate the pvalue this pvalue is the probability that these results did not occur due to chance a common way to evaluate the pvalue is to compare it to 0.05 if the pvalue is less than 0.05 we say there is an association between the two variables that is statistically significant in this example our pvalue is 0.18 which is greater than 0.05 so we conclude that the dominant hand variable is independent of gender if there is an association between two variables completing the calculation for the chisquare test statistic can be used to find which values contribute the most to the Association this can be calculated using this formula for each cell subtract the expected value from the observed and square the result then divide the answer by the expected value each of these results is summed indicated by the Greek letter Sigma this value is compared to the chisquare distribution within each cell the calculation describes how far the actual value is from the expected value in this example the calculations for lefthanded people are much greater than those for righthanded people these cells have the greatest impact on the potential association between the two variables summing those values results in chisquare test statistic of 1.78 using this value in a chisquare distribution along with the degrees of freedom based on the number of values for each variable and a significance level such as 0.05 is how the pvalue is obtained this concludes our video on contingency tables today we defined contingency tables and then we discussed the steps for creating one and finally we covered chisquare distributions this video will first define distribution then we'll cover measures of distribution the mean median outliers mode minimum and maximum values and quantiles the most common method of analyzing a numeric variable is by exploring how the values are distributed the distribution of a numeric variable shows all the possible values and how often they occur a distribution provides methods in which many records of data can be summarized to provide basic information about the variable these methods can either be numerical measures or visualizations we'll explore the most popular methods using this data set of 10 rows of bank teller salary data because the sample data set is so small we can make some quick observations about the salary variable the lowest salary is twenty eight thousand six hundred and sixtyfive dollars this is referred to as the minimum value the highest salary is forty four thousand and twenty dollars the maximum value finally many of the salaries are in the low $30,000 range we'll begin by defining and calculating the salary variables measures of distribution mean median outliers mode minimum and maximum values in quantiles to find the mean or average add up all the numbers and divide by the number of rows 340 one thousand eight hundred and sixty dollars divided by 10 equals a mean of thirty four thousand 186 dollars to find the median sort the numbers and find the middle value if there are an odd number of rows there is one middle value if there are an even number of rows there are two middle values and the median will be the average of these two values here we sort the table by salary then find the average of the two middle values by adding thirty three thousand nine hundred eighty dollars to thirty four thousand eight hundred fifty dollars and dividing by 2 to get thirty four thousand one hundred and fortyfive dollars values that fall outside of the normal range of the rest of the observations are called outliers in our example the value of forty four thousand twenty dollars is an outlier from the rest of the values the mode is the most commonly occurring value in our sample data set no values occur more than once so there is no mode as we've already noticed the minimum and maximum values are twenty thousand six hundred and sixty five and forty four thousand and twenty dollars a quantile is a set of values that divide a frequency distribution in two equal groups each containing the same fraction of the total population to find quantiles divide the distribution into groups of equal size with each group containing about the same number of rows the most simple of quantiles has already been calculated the median divides the records into two groups of five the following quantiles are most often used tersh aisles three groups quartiles four groups quintiles five groups deciles ten groups and percentiles 100 groups let's look at quartiles for our example the median is commonly referred to as q2 since it is the second quartile q1 and q3 are also used often to calculate quartile 1 we can look at the middle value of the first 5 records sorted in order which is 30 $1,300 and then look at the middle value of the last 5 records for quartile 3 which is 30 $5,100 these values along with our median are our quartile values this concludes our video on measures of distribution today we define distribution and then we discuss the measures of distribution including mean median outliers mode minimum and maximum values and quantiles this video will cover variation including a definition of variation and the measures of variation range inter quartile range variance standard deviation and standardization as well as testing differences between means variation refers to how spread out the values are for a variable interpreting variation that is explaining a variables variation in reference to other variables is a foundational task in Business Analytics variables that have values that are spread out have higher variation while variables with values very close to the mean have lower variation the following measures help us understand how the values for a variable are spread out or on the mean range interquartile range variance standard deviation and standardization using an example of salaries by gender a question arises do men make more than women certainly some men make more than women but is this true for the group as a whole using the principles of variation will help us to answer this question the range is the largest number minus the smallest number to find the range we first sort from smallest to largest then we subtract the smallest value from the largest value to find the interquartile range first separate the data into quartiles and then subtract q1 from q3 to find the variance we first calculate the mean or average then we subtract the mean from each value and square the result by squaring the differences we remove the possibility of negative values cancelling out the positive values next find the average of the squared differences variance is not a very useful measure the value we got for variance is much different from the range of salaries that are variable captures to get it back into the correct scale we take the square root of the variance this results in the standard deviation this value is used relative to the mean to determine which values are within the normal variation of the salary variable subtract the standard deviation from the mean to find the lower threshold add the standard deviation to the mean to find the upper threshold using the standard deviation gives us a way to determine which values are within the normal variation and which values are either less than normal or greater than normal in other words it gives us the ability to identify outliers more easily in our example we can identify that the smallest two salaries twentyeight thousand six hundred and sixtyfive dollars and twentynine thousand five hundred dollars and the largest salary fortyfour thousand twenty dollars are outside one standard deviation from the mean mean and standard deviation are measures that describe the distribution of a set of numbers but what if we want to describe the numbers within the set to compare two numbers within an entirely different set we can do this by standardizing the values in statistics this measure is called the zscore to standardize a value subtract the mean and divide by the standard deviation for example in the first row of our data set since we've already calculated this salary mean column all we need to do is divide by 4103 to get the result of negative 1.1 for this value can be interpreted as the value of $29,500 is 1.1 for standard deviations lower than the mean looking at the rest of the values we see that the largest salary forty four thousand and twenty dollars is actually very far away from the mean this must be a high performing bank teller we can use this standardized value to compare the high performing bank teller to the high performer of another profession let's return to the question post at the beginning do men make more than women to answer this question we can use a statistical test called a ttest this test requires that we make three assumptions about our data which for now we will assume to be true the two populations have the same variance the populations are normally distributed each value is sampled independently from each other value by considering the values of salary separately we can see that females have the lowest value and the highest value so the variation must be higher calculating the mean and standard deviation for each group we see that the mean is almost the same yet the standard deviation for females is more than twice that for males using these values we can calculate the tstatistic and corresponding pvalue but in Excel we only need the original values in different columns using the ttest function we get a pvalue of 0.44 since this value is much higher than point zero five we conclude that the two distributions are not statistically significant this is mainly due to the fact that our sample size for this example is very small and also because the means are almost equivalent this concludes our video on measures of variation today we defined variation and then discussed range enter quartile range variance standard deviation standardization and testing differences between the means this video will cover distribution visualizations including buckets histograms and an introduction to area line graphs we begin with a data set that contains gender dominant hand and salary we've calculated a number of different measures including the mean median minimum maximum range interquartile range variance standard deviation and a list of zscores next we'll build a histogram which is the most common way to visualize a numeric distribution to build a distribution will group the data into buckets or bins of equal size this will effectively transform the variable into a categorical variable allowing us to count the occurrences of each bucket next we determine the size of each bucket first we locate our minimum and maximum values then we subtract the minimum from the maximum and divide by the number of buckets this gives us a value of one thousand five hundred and thirty six dollars next we need to figure out the starting value for each bucket starting with the lowest salary we add the size of the bucket to determine the starting value of the second bucket we do this until we obtain the starting values for all ten buckets we'll add a new column to the original table to assign each person into a salary bucket summarize the new variable starting value of salary bucket counting how many occurrences of each bucket are observed note that although this variable is numeric by summarizing it we're treating it as a categorical variable finally we'll make a simple bar chart to visualize this table this bar chart is known as a histogram and it's one of the most common methods of visualizing a numeric distribution although many software packages can produce a histogram very quickly as we've seen creating one involves quite a few steps if we change the visualization to an area line graph and change the vertical axis to the percent of records by dividing the count by the total number of records the result is similar to what is known as a probability density function or PDF this is commonly used in statistics to estimate the probability of a new value for now we just need to know that the shaded area under the curve adds up to 1 or 100% and that the lines are typically much smooth and we see in this example which is because most PDFs visualize more than 10 data points and use more than 10 buckets this concludes our video on distribution visualizations today we covered buckets histograms as well as a brief introduction to the use of area line graphs this video will cover normal distributions continuous distributions density functions cumulative distribution functions and the 6895 99.7 rule the single most important distribution in statistics is the normal distribution it's a continuous distribution and it's the basis of the familiar symmetric bellshaped curve the mean of the normal distribution is in the center the standard deviations are marked at equal distances from the mean any particular normal distribution is specified by its mean and standard deviation by changing the mean the normal curve shifts to the left or right by changing how spread out the standard deviations are the curve also changes standard deviations can be spread out wider or closer together therefore there are really many normal distributions not just a single one the normal distribution is a two parameter family where the two parameters are the mean and standard deviation here's a tool you can play with online that illustrates a normal distribution in real life it looks like a triangular shaped pegboard into which balls are dropped when there's an equal probability that the balls will drop either left or right their final placement forms a normal distribution however when the probability of the balls dropping left or right is unequal which is something you can experiment with using this tool the distribution changes the formulas for mean and standard deviation are very complex but you will not have to compute them because the software will with continuous variables there is a continuum of possible values such as all values between 0 and 100 or all values greater than 0 instead of assigning probabilities to each individual value in the continuum the total probability of one is spread over the continuum thus the shaded area within the bell curve will always have an area of 1 the key to the spreading is called a density function which acts like a histogram the higher the value of the density function the more likely this region of the continuum is a density function usually denoted by FX specifies the probability distribution of a continuous random variable X the higher FX is the more likely X is probabilities are found from a density function as areas under the curve so for example the shaded portion under the spell curve represents the probability of X being between 65 and 75 the cumulative distribution function or CDF is the probability that the variable takes a value less than or equal to X it's the total area under the normal curve up to X the beauty of the normal curve is that no matter what its mean and standard deviation are the area between the mean minus 1 standard deviation and the mean plus 1 standard deviation is always about 68% the area between the mean minus 2 standard deviations and the mean plus two standard deviations is always about 95% the area between the mean minus three standard deviations and the mean plus three standard deviations is always about ninetynine point seven percent that means almost all values fall within three standard deviations on either side of the mean that's true for all normal curves no matter their shape but how good is this rule for real data let's go ahead and check out an example here's our data the mean of the weight of one hundred and twenty women runners in a sample is one twenty seven point eight pounds the standard deviation is fifteen point five here's what our distribution would look like let's look a little more closely at that distribution 68% of our 120 runners is about eighty three runners according to the 68 95 99 point seven rule those runners should fall fall within one standard deviation of the mean weight of one hundred and twenty seven point eight pounds that is eightythree of our runners should fall between one hundred and twelve point three and one forty three point three pounds when we check our data we see that seventynine runners fall within one standard deviation of the mean furthermore ninetyfive percent of our group or about 114 runners should fall within two standard deviations of the mean or between ninety six point eight and one hundred and fifty eight point eight pounds the data shows that 115 runners fall within two standard deviations of the mean finally according to the rule ninetynine point seven percent of our runners or one hundred nineteen point six runners should fall within three standard deviations of the mean or within a range of eighty one point three pounds to one hundred and seventy four point three pounds according to our data all 120 runners fall within this range so it seems as if the rule is pretty accurate in this case this concludes our video on normal distributions continuous distributions density functions and cumulative distribution functions as well as the 6895 99.7 rule this video will cover kurtosis including a definition as well as positive and negative kurtosis and asymmetrical distributions including those of positive and negative skew kurtosis is the measure that describes the size of the tails in a distribution a distribution with positive kurtosis contains fewer values in the tails than a normal distribution a distribution with more values in the tails has negative kurtosis the normal distribution is a type of symmetrical distribution in which the mean is equal to the median and there is an equal probability of a value falling on either side of the mean although normal distributions and other types of symmetrical distributions are very common there are often distributions that are asymmetrical we call these types of distributions skewed for a negatively skewed distribution the left tail is longer and the mean is less than the median this is due to the occurrence of outliers at the lower end of the distribution away from the most frequently occurring values a good example of this is the height of NBA players since taller basketball players have an advantage the majority of NBA players are very tall for a positively skewed distribution the right tail is longer as there are outliers with larger numbers these outliers cause the mean to be greater than the median an example of a positively skewed distribution is in the salary of baseball players there are a few star players who make much more than the majority of players these high salaries are outliers that make the mean of the distribution increase for distributions that are skewed both positively and negatively the meeting is a better representation of central tendency than the mean as the outliers will impact the calculation of the mean the median is not affected by outliers and typically is a much better approximation for the middle of a distribution this concludes our video on kurtosis today we defined kurtosis and discussed positive and negative kurtosis we also covered asymmetrical distributions including those of positive and negative skew this video will cover sampling basics including populations and inferences selecting a sample random sampling stratified sampling and cluster sampling a population is a set of all members about which is study intends to make inferences here's a population of people we'd like to study their television watching behavior and infer how many watch a particular shows so we can decide whether to purchase advertising spots during this period of time but our population is much too large to study effectively to study their television viewing habits we'll have to survey them and it's not feasible to survey every single individual so we'll take a sample of the population to study the sample will represent the population as a whole and so will it survey results if we choose our sample correctly we will focus on selecting a sample using probability a probability sample is chosen from a population using a random mechanism there are two types of probability sampling stratified and cluster a random sample is only random if each individual has the same chance of being chosen from the population so back to our television viewing research we want to figure out how many television viewers there might be during a particular so let's say there are 30,000 viewers in our population each viewer is known as a unit in order to select a sample n of viewers from this population of 30,000 we could choose to use a simple random sample this means that there is an equal probability that each viewer could be selected for inclusion in the sample if our desired sample size was 200.final echt 200 viewers randomly and then we could send each of those viewers a questionnaire in the mail about their viewing habits but suppose various subpopulations within the total population can be identified these populations are called strata instead of taking a random sample from the entire population we might get better information by selecting a simple random sample from each stratum separately this is called stratified sampling examples of subpopulations and television viewers might include age or gender there are several advantages to stratified sampling one obvious advantage is that separate estimates can be obtained within each stratum which would not be obtained with a single random sample from the entire population for example let's say we're looking at our television viewers by age group we have three strata 18 to 24 25 to 39 and 40 plus we find that their peak times vary based on age thus we can make better decisions about which product to advertise during which time period a more important advantage of stratified sampling is that the accuracy of the resulting population estimates can be increased by using appropriately defined strata in cluster sampling the population is separated into clusters such as regions of the country and then a random sample of the clusters is selected the primary advantage of cluster sampling is sampling convenience and possibly lower cost selecting a cluster sample is straightforward the key is to define the sampling units as the clusters such as the regions of the continental US shown here this concludes our video on sampling basics today we covered populations and inferences selecting a sample random sampling stratified sampling and cluster sampling this video will cover bivariate data scatter plots and null values measures of central tendency variability and spread summarize a single variable by providing important information about its distribution often more than one variable is collected on each individual for example in large health studies of populations it's common to obtain variables such as age sex height weight blood pressure and total cholesterol in each individual economic studies may be interested in among other things personal income and years of education as a third example most university admissions committees ask for an applicant's high school grade point average in standardized admission test scores like the SAT bivariate data consists of two quantitative variables for each individual in contrast with univariate or single variable data our first interest is in summarizing such data in a way that's analogous to summarizing univariate data by way of illustration let's consider something with which we're all familiar age let's begin by asking if people tend to marry other people of about the same age our experience tells us yes but how good is the correspondence one way to address the question is to look at pairs of Ages for a sample of married couples table one shows the ages of 10 married couples going across the columns we see that yes husbands and wives tend to be of about the same age with men having a tendency to be slightly older than their wives this is no big surprise but at least the data bear out our experiences which is not always the case the pairs of Ages in table 1 are from a data set consisting of two hundred and eighty two pairs of spousal ages too many to make sense of from a table what we need is a way to summarize the two hundred and eighty two pairs of ages we know that each variable can be summarized by a histogram which is a graphical representation of a distribution a histogram partitions the variable on the x axis into various contiguous class intervals of usually equal widths the heights of the bars represent the class frequencies here we can see that each distribution is fairly skewed with a long right tail we can also summarize the variables with a mean and standard deviation from table 1 we can see that not all husband's are older than their wives and it's important to see that this fact is lost when we separate the variables that is even though we provide summary statistics on each variable the pairing within the couple is lost by separating the variables we cannot say for example based on means alone what percentage of couples has younger husbands and wives we have to count across the pair's to find this out only by maintaining the pairing can meaningful answers be found about the couples another example of information not available from the separate descriptions of husbands and wives ages is the mean age of husbands with wives of a certain age for instance what is the average age of husbands with 45 year old wives finally we don't know the relationship between the husband's age and the wife's age we can learn much more by displaying the bivariate data in a graphical form that maintains the pairing figure to shows a scatter plot of the paired ages the xaxis represents the age of the husband and the yaxis the age of the wife there are two important characteristics of the data revealed by figure two first it's clear that there's a strong relationship between the husband's age and the wife's age the older the husband the older the wife when one variable Y increases with the second variable X we say that X and y have a positive association conversely when Y decreases as x increases we say that they have a negative association second the points cluster along a straight line when this occurs the relationship is called a linear relationship figure 3 shows a scatterplot of arm strength and grip strength from 149 individuals working in physically demanding jobs including electricians construction maintenance workers and auto mechanics not surprisingly the stronger someone's grip the stronger their arm tends to be there is therefore a positive association between these variables although the points cluster along a line they're not clustered quite as closely as they are for the scatterplot of spousal age a common problem when working with realworld data is the presence of missing or null values within a data set there are three strategies to deal with the issue the first one is to omit the rows if the variable is very important to the analysis and there are not many observations with missing values it can be acceptable to filter or delete those rows the second is to treat missing as a separate category if the variable is categorical this is easy if the variable is numeric then the variable will need to be binned and a category created for the missing rows the third is to impute a value using distribution measures such as the mean or the median or other variables if values of other fields have differing distributions for the variable with missing values we can calculate separate distribution measures using these categories this concludes our video on bivariate data scatter plots and null values this video will cover uncertainty entropy and analyzing data the result of data analysis is information information resolves uncertainty the uncertainty of an event is measured by its probability of occurrence the more uncertain an event the more information is required to resolve the uncertainty of that event entropy refers to the fact that you cannot stir things apart it's a measure of information content and unpredictability here's a concrete example of entropy if you have cold water and hot water and mix them together you will have warm water you can't separate the cold and the hot after they're mixed this is what is meant by you cannot stir things apart to get an informal intuitive understanding of the connection between these terms consider the example of a pole on some political issue the outcome of the pole is relatively unpredictable and actually performing the pole and learning the results gives some new information these are just different ways of saying that the entropy of the poles results is large now let's say a second poll is performed shortly after the first poll since the result of the first poll is already known the outcome of the second poll can be predicted well and the results should not contain much new information in this case the entropy of the second poll result is small relative to the first now consider the example of a coin toss when the coin is fair that is when the probability of heads is the same as the probability of tails then the entropy of the coin toss is as high as it can be that's because there's no way to predict the outcome of the coin toss ahead of time such a coin toss has one bit of entropy since there are two possible outcomes that occur with equal probability and learning the actual outcome contains one bit of information on the contrary a coin toss with a coin that has two heads and no tails has zero entropy since the coin will always come up heads and the outcome can be predicted perfectly in this graph entropy is maximized when the probability is 50% when the probability is zero or 100% there is zero entropy by adding information we can reduce entropy and gain certainty how does entropy apply to analyzing data we can use the principles of entropy to decide what results are important and should be included in a report and what results are trivial and should not be included if there is no uncertainty for a variable then there's no information if we obtained a new observation we would already know the value of that variable typically variables with only one value are excluded from an analysis at the very beginning on the other hand if a variable has a maximum uncertainty because it contains values that are equally likely it will be difficult to guess the value for a new observation these variables are not excluded from analysis applying this concept to data analysis our goal is to reduce entropy by explaining outcomes using other variables we are reducing uncertainty and these results should be the focus of a report this concludes our video today we covered uncertainty entropy and analyzing data this video will cover the parts of an analytical report including the introduction data analysis and results in conclusion the introduction should provide a concise summary of the project including the problem faced the type of data gathered and the highlights of the solution the data section should go into detail about the data used to complete the project variables and other technical terms should be defined well an example value should be listed and interpreted also this section should mention any abnormalities in the data such as missing values and discuss the steps that were taken to clean and prepare the data for analysis in the analysis section the report should cover the thought process behind the analysis including any output and data visualizations that are pertinent methods that are used should be introduced along with a brief description for the reasons they were used and possibly including references to external sources for further study care should be taken to not include every possible analysis as this can provide information overload to the audience insignificant or less significant findings can be briefly summarized leaving the majority of content in this section to focus on the most important findings the results in conclusion section should summarize the results of the analysis and if applicable provide specific recommendations on a course of action because the analysis section covered most of the information gleaned from the data the results in conclusion should mostly just apply that information towards a goal or a further course of study combined the introduction data analysis and results in conclusion sections of an analytical report delivered succinctly and clearly by an effective business analyst can provide useful and pertinent information to drive business improvement this concludes our video on writing analytical reports today we covered the parts of a report including the introduction data analysis and results in conclusion this video will cover automation including a brief introduction to automation macros and stored procedures a report is not the only results of a business analytics project typically automation will often be a separate goal especially when a data source is often updated with new records the main benefit of automation is that it frees up the analyst time to work on different problems this is very valuable from a business perspective as analysts that are skilled in automating tasks can save the company money in terms of labor expenses and overtime can uncover more and more patterns in the data leading to greater profitability there are two main types of automation used in business analytics macros and stored procedures macros are also referred to as functions the purpose of macros are to easily replicate certain steps without having to write out those steps individually macros can make an analysis quicker and more concise for instance with a certain data set you may want to filter sort and then take the average of a field if this set of steps will be used multiple times or for different data sets you may want to transform these steps into a macro to save time the parameters of a macro are the input this can be datasets fields or values the parameters are a component that will change with each call to a macro after the steps of the macro are complete the output will be returned and like the parameters can also be of different types macros are the foundation of many analytics and software packages as many of the complex algorithms used in data science are mostly just the building of certain simpler functions by utilizing macros that have already been built an analyst can become more efficient by building customized macros and analysts can easily pass on their efforts to other analysts most companies with a data science team will build out a repository of customized macros called a code base it makes it easier to manage certain tasks such as committing changes in version control these concepts are taken from computer science best practices of developing software another concept that comes from computer science is the use of objectoriented programming this principle guides the development of macros and is based on the idea that the components of code should be compartmentalized the main benefit of this technique is that it makes large projects easier to develop and maintain instead of an analyst having to replicate an analysis manually every time new data is captured a stored procedure will execute the stored procedure is a type of algorithm that runs according to a schedule and executes a series of steps that the analysts sets up in advance usually these steps will be in the form of code but new software programs allow analysts to build stored procedures without having to write code once code has been production alized only monitoring the execution of the stored procedure is necessary when the procedure completes with an error the analyst will need to troubleshoot the code the most common reason for errors to arise are due to unforeseen data values that is variables will contain value types that were not present in the original data set for example a numeric field will contain an alphabetic character the best practice is to test for these values within the stored procedure and transform them or remove records altogether and add a warning message this will prevent the stored procedure from failing altogether logs are output from stored procedures to describe the processes that ran information that logs produce can vary but usually include the amount of time each process took the number of Records were input and any error or warning messages that were triggered analysts should add code to stored procedures to make logs more descriptive and thus easier to troubleshoot in problems arise depending on the tools being used and the amount of data being processed production Eliza an algorithm can be either a simple task for one analyst or a multiyear project involving many analysts project managers database administrators documentation writers and quality assurance specialists in these cases management methodologies such as agile software development are used to coordinate team members and progress through the project lifecycle this concludes our video on automation today we covered a brief introduction to automation macros and stored procedures this video on regression will cover simple linear regression analysis regression line fitting observed in predicted values the leastsquares coefficient estimation goodness of fit explained an unexplained variation root mean square error and the coefficient of determination significance testing and regression assumptions regression analysis is used to predict the value of one variable the dependent variable Y on the basis of other variables the independent variable X in other words if you know something about X you can use it to predict something about why we provide the independent variable X and we observe the dependent variable Y the linear regression equation is shown here the variable X is considered the independent or predictor variable the variable Y is the dependent or outcome variable we have data on both x and y we use this information to estimate the value of the intercept beta 0 and the slope beta 1 that relate to x and y since the linear relationship is not exact we include an error term in the model Epsilon why is this useful because once we have estimates of beta0 and beta1 from our regression we can use this for any value of x to predict what the value of y would be so if we have data on NBA players weight and height we can estimate how much a typical NBA player weighs based on his height then if someone wants to join the NBA and we know what is height is we can estimate what weight he should be to make it to the NBA in linear regression analysis our goal is to estimate a pattern in this case a line the best fits the data the best fit for our data will go through the core of our data and minimize error the linear relationship in algebra is when a line is represented by its slope and intercept but instead of an exact relationship regression analysis estimates the line from data since the line does not fit data points precisely there is an error term AI measuring the deviation of actual Y from estimated y using the least squares koi fish estimation we can obtain the line that best fits the data by minimizing the sum of squared errors the total variance in Y is divided into two parts that which can be explained by X using regression and that which cannot no line is perfect there's always some error in the estimation unless there's a comprehensive dependency between the predictor and response there's always some part of the response Y that can't be explained by the predictor X using the mean value of y as our reference point we can decompose the total error in measurement between the part that is explained by the regression line and the part that remains unexplained here's a look at the decomposition the sum of squares regression or SSR is the explained variation attributable to the linear relationship between X and y the sum of squares error or SSE measures the variation attributable to factors other than the linear relationship between x and y the SST is the total sum of squares the SSR and SSE together make up the SST so given that total variation SST is the sum of explained and unexplained variation we can divide through by the total sum of squares SST to get the ratios equal to one the ratio of error sum of squares / total sum of squares plus the ratio of regression sum of squares / total sum of squares equals one either of these two ratios can be used to measure our model fit error sum of squares show is called the mean square error we want to choose models with the lowest mean square error regression sum of square ratio is called R squared or the coefficient of determination we choose a model with the highest R squared because R squared + mean square error equals 1 it has to be true that R square our coefficient of determination has to lie between 0 & 1 likewise we can look at the coefficients of the intercept and slope to see if they're significantly different from zero this is done by examining the T statistic and the corresponding pvalues pvalue is less than 0.05 imply that the coefficient is significantly different from zero some key assumptions before you apply regression techniques first the variables have to have a linear relationship in this example the relationship between x and y is not linear so we cannot fit a linear regression line the variables also have to be approximately normally distributed in this example Y is not normally distributed at each value of x so it doesn't make sense to fit a linear regression line additionally the variance of y at each value of x should be the same or in other words we should have a homogeneity of variances the fourth and final assumption is that the observations are independent here one trendline is not sufficient in other words if sales today depend on sales yesterday then linear regression models will not work this concludes our video on regression today we covered simple linear regression analysis regression line fitting observed and predicted value you this lesson covers the tdistribution and how it compares to the normal distribution as well as a brief look at the student's tdistribution for large samples the normal distribution applies for small samples the standard deviation is measured in precisely and the data followed the T distribution a T distribution will approach a normal distribution for a larger n greater than or equal to 100 but it has fatter tails for a smaller n less than 100 the T distribution is very similar to the standard normal distribution it also has a bell curve but the standard deviations are computed from the sample data instead of the population suppose a simple random sample of size n is drawn from a population whose distribution can be approximated by a normal Mu Sigma model when the standard deviation is known then the sampling model for the mean X is distributed as a normal distribution with mean xbar and standard deviation Sigma divided by the square root of n when the standard deviation is estimated from the sample standard deviation s the sampling model follows a T distribution with degrees of freedom and minus 1 this is the one sample T statistic in this figure both distributions have 0 means but the variances are a bit different the T distribution has a lower peak and fatter tails this concludes our video on t distributions today we covered the T distribution and showed how it is very similar to the normal distribution we also briefly showed the students T distribution in this video we will cover logistic regression including the need for logistic regression the logistic regression model and odds ratios and prediction in many instances when you're testing hypotheses and making predictions you will have dichotomous outcomes for example in a game you can either win or lose on a website a user either clicks or does not click in an election a person votes for a candidate or does not when the outcome variable is categorical such as our game example it does not follow a normal distribution the outcome variable is a probability measured between 0 & 1 the estimates you make should be numbers in that range a linear model cannot be applied we need a nonlinear function there are many nonlinear models we can choose from to fit our data some nonlinear functions are shown here the last one shown is the logistic function that will fit the data the best because it has an upper and lower bound here's a logistic regression model in orange versus a linear regression model in blue isn't it a better fit for the data to best suit our data we want a model that predicts probabilities between 0 and 1 so it will be sshaped there are lots of sshaped curves but the logistic regression model is what we'll use in this instance the logistic function is a nonlinear function of independent variables however we can convert this nonlinear function into a linear relationship using the log of the odds ratio note that instead of modeling just zeros and ones we're modeling the probability of an event occurring with a logistic regression model instead of winning or losing we build a model for log odds of winning or losing it's a natural logarithm of the odds of the outcome P stands for the probability of the outcome while 1 minus P stands for the probability of not getting an outcome however having log of P over 1 minus P on the y axis is not very helpful we have to compute the actual odds to do that we have to use the exponential functions let's look at a very simple example of a log it function does alcohol drinking predict political party political party is the outcome variable and it is binary therefore we need a logistic regression a typical log in equation contains the log of the odds ratio is the outcome which is a linear function of the predictors X the log it model to measure the impact of drinking on voter choice is going to be set up as follows the log of the odds ratio will be measured from data on X where X here is the number of drinks per week it's really important to understand that negative 1 point 4 is measuring the log of the odds ratio in other words it is the log of the probability of being Republican divided by the probability of not being a Republican to get the actual odds ratio you have to compute the exponent which is equal to 0.25 since the odds are less than 1 it tells us that the more you drink the lower your odds of being Republican all these calculations can be done automatically in SAS but it's important to understand the math behind what SAS is doing the same model can be extended to more than one variable we just add more predictors to the equation the coefficient beta measures the impact of x on the log of the odds ratio for example in linear regression if y equals 2 plus 3x a one unit increase in X will increase Y by three units in a logistic regression log P over 1 minus P minus 2 plus 3x shows us that if x increases by one unit then the log odds of P y equals 1 increases by three units the impact on the odds ratio is represented by E exponent beta we can also compute probabilities directly to compute odds we have to use the exponent if we don't want to look at odds but the actual probabilities we apply the entire logistic function formula as shown in this probability function these are the odds ratios and log of odds ratios for various probabilities an important point to understand is how the odds ratios are tied to the probabilities note the mathematical equivalencies a 50% probability or probability of 0.5 is the same as 1 to 1 odds the log of the odds ratio at that point is equal to 0 as probability increases odds ratio increases from 0 to infinity while the log of the odds ratio can become any value this concludes our video on logistic regression today we covered the need for logistic regression logistic regression model and odds ratios and prediction this video will cover two types of statistical error type 1 or alpha and type 2 or beta all statistics derived from samples are subject to error a type 1 error rejects the null hypothesis when it is actually true a type 2 error accepts the null hypothesis when it is not true remember that a different sample can give a completely different result a sample mean is likely to fall in the confidence interval only 95% of the time so the inferences drawn from the sample may be wrong let's talk in a little more detail about the type 1 error the type 1 error occurs when a researcher thinks he or she has found a significant result but really that result is due to chance it's similar to a false positive on a drug test the type 1 error or the mistake of rejecting the true null hypothesis will happen with a frequency of alpha thus if alpha our critical value is 0.05 then a type 1 error will occur 5% of the time on the other hand a type 2 error occurs when results seem insignificant but in fact there was something significant going on type 2 errors are like a false negative on a drug test they occur when the alternative hypothesis is true but there's not enough evidence in the sample to reject the null hypothesis this type of error is traditionally considered less important than a type 1 error but it can lead to serious consequences in real situations the power of a test is 1 minus the probability of a type 2 error it is the probability of rejecting the null hypothesis when the alternative hypothesis is true in these competing sampling distributions alpha is set to point zero 5 the bottom curve assumes H a is true the top curve assumes that the null hypothesis H naught is true its right tail shows that we will reject H naught when a sample mean exceeds one eighty nine point six the probability of getting a value greater than one eighty nine point six on the bottom curve is 0.5 one six zero corresponding to the power of the test here's a table that summarizes the types of errors here's an example using a fire alarm if a fire alarm is silent and there is no fire our null hypothesis that it is working is correct but what if the assumption wrong then we've accepted the null hypothesis but we actually have a fire that's our type 1 error the opposite case may also happen if the alarm goes off and there's actually a fire there's no error but if there's no fire and the alarm goes off it's a false alarm that's the type 2 error here it is the less serious problem this concludes our video on statistical error today we discuss type 1 or alpha error and type 2 or beta error this video will cover hypothesis testing which is also called significance testing and occurs when we test a claim about a population parameter using sample evidence that confirms or rejects the claim there are four steps in the hypothesis testing process all of which will be covered in this video here's a summary of the four steps in hypothesis testing after this we'll discuss each step in detail the first step is stating the null and alternative hypotheses we have to establish what we are testing to be true once we do that we have to decide how close to true our sample statistic has to be for us to accept the truth for example we might want our estimate to be accurate with a 5% margin of error this is called locating the critical region once we know that we have to compute the test statistic the Z value or the T value finally based on our results we draw conclusions from the study the first step in the procedure is to convert the research question to a statement of the hypotheses null and alternative forms our study will be to collect and seek evidence against the null hypothesis as a way of deductively bolstering the alternative hypothesis the null hypothesis abbreviated h naught is a statement of no difference in other words the null hypothesis argues that there is no significant difference between our specified populations and that any observed difference is due to sampling or experimental error the alternative hypothesis or H a is the opposite of the null hypothesis it provides a statement of difference in our study we will seek evidence against the claim of H naught as a way of proving h a here's an example of setting up the null and alternative hypotheses in the late 1970s the weight of US men between 20 and 29 years of age had a log normal distribution with a mean of 170 pounds and a standard deviation of 40 pounds to illustrate the hypothesis testing procedure we asked if body weight in this group has changed since 1970 this is called our research question and it can be answered in one of two ways under the null hypothesis there is no difference in the mean body weight between then and now in which case Mew would still equal 170 pounds under the alternative hypothesis we assert that the mean weight has changed EMU's not equal to 170 pounds this is called a twosided test the most common form of hypothesis testing we can also do a onesided test in which we ask if weight has increased over time so the alternative hypothesis would be mu is greater than 170 pounds in step 2 we will locate the critical region once we've established the research question we have to define the level of accuracy with which we want to measure our test statistic any estimate from a sample will not be exactly the same as the population parameter so we have to decide what we think is likely versus unlikely this is called locating the critical region the critical region consists of outcomes that are very unlikely to occur if the null hypothesis is true or in other words the sample means that are almost impossible to obtain when we're estimating population parameters using a sample we have to determine the cutoff values these cutoff values are called alpha if we decide that we want to measure the mean with a 90 percent precision level then the shaded area on the left and right will be larger if we want to measure with a 1 percent precision then the area will be smaller and the range will be larger these are the locations of the critical region boundaries for three different levels of significance alpha equals 0.05 alpha equals 0.01 and alpha equals 0.01 note that boundaries get wider as the critical value Falls in most cases researchers choose an alpha of 0.05 or point zero one our rejection region should have a probability of alpha if the null hypothesis is true but some bigger probability if the alternative hypothesis is true so if the mean lies inside the cutoff value for alpha then the null hypothesis is true otherwise we fail to accept the null hypothesis the result is significant beyond the alpha level for example if alpha is 0.05 our result is significant if it's less than point zero five once we decide whether we want to measure accuracy at the 10 percent 5% or 1% level we can compute the test statistic here we will use the zscore which is a ratio comparing the obtained difference between the sample mean and the hypothesized population mean this is an example of a one sample test of a mean when the standard deviation Sigma is known in our male way example we're going to use the Z statistic because we know the population mean and the population standard deviation to compute the Z statistic we simply insert values derived from our sample into the formula if in one sample we found that the sample mean was 173 then the Z statistic would be 0.6 0 this value on the xaxis under a standard normal curve let's say we found the sample mean to be 185 putting these values into the Z stat formula we find the z stat is 3.0 this is much higher at the tail end of the xaxis on a normal distribution the final step is drawing conclusions once we've computed the Z value of our test statistic we have to look at the corresponding probability values to find out if it's reasonably close to the population mean a large value shows that the obtained mean difference is large and in the critical region the difference is significant which means we have to reject the null hypothesis that the weights have not changed over time if the mean difference is relatively small then the test statistic will have a low value in this case we conclude that the evidence from the sample is not sufficient and the decision is to fail to reject the null hypothesis the pvalue is the area under the normal curve in the tails beyond the z stat it answers the question what is the probability of the observed test statistic or one more extreme when H naught is true to convert Z statistics to pvalue we will use software in one sample with a sample mean of 173 the z statistic was 0.6 0 if we had this sample we would fail to reject the null hypothesis that the mean weights have increased over time likewise if we computed the pvalues for Z equals 3.0 we would get point zero zero 1 which means we have to reject the null hypothesis that the mean weight has remained the same over time note that when we're looking at weight change instead of weight increase all we have to do is multiply the onesided pvalue by 2 to do a twotailed test since we will be using pvalues in all our subsequent analysis it's worth emphasizing what that means pvalues ask the question what is the probability of the observed test statistic when H naught is true remember the smaller the pvalue the more likely that your null hypothesis this is not true this graphic depicts the significance of pvalues at less than one percent between one and five percent between five and ten percent and greater than ten percent these are common significance levels five percent is the most common cutoff however a note that is unwise to draw firm borders for significance as an example a pvalue of 0.27 would not be significant against H naught a pvalue of 0.01 on the other hand would be highly significant against H naught this concludes our video on hypothesis testing also called significance testing which occurs when we test a claim about a population parameter using evidence that confirms or rejects that claim today we covered the four steps in hypothesis testing state the null and alternative hypotheses locate the critical region compute the test statistic and draw conclusions this presentation will cover correlation including a definition of correlation a discussion of the need for correlation details on computing correlation including variance covariance and the correlation coefficient strength of Association linear and curvilinear relationships properties of correlation R squared the coefficient of determination and a discussion of correlation versus causation correlation is one of the most common and useful statistics it's a measure of Association a single number that describes the degree of relationship between two variables we can examine correlations between two variables heuristic ly by looking at a scatter chart in this chart our observations are very tightly centered around the line in this case we would say that the relationship between x and y is more correlated we call this a strong correlation by contrast if the observations are scattered further out we might say the relationship between x and y is less correlated or that there is a weak correlation here are some examples of questions that ask about correlation is there any association between hours of study and grades is there any association between the number of churches in a city and the murder rate when the weather gets hot what happens to sweater sales what is the strength of association between them what about the sale of icecream versus temperature what is the strength of association between them furthermore how do we quantify the association while we can guess the relationship there's a better way to do this using statistical measures the measure we use is the Pearson correlation coefficient to compute correlation we'll need information on standard deviation and covariance we know that the variance is the dispersion within a variable X or Y or the squared average deviation from the mean as shown here the covariance is the dispersion of X multiplied by the dispersion in Y it is calculated as the average of the product of deviations in individual means using the information on variance and covariance we can compute the correlation coefficient as the covariance of x and y divided by the state deviation of X multiplied by the standard deviation in Y this measure of correlation ranges from negative one to positive one a higher number is a stronger correlation and the lower number is a weaker correlation correlation coefficient are measures the strength of linear Association it measures the extent to which two variables are proportional to each other it's unit free so for example a measure of correlation between player height measured in inches and player weight measured in pounds will be meaningful even if they're measured in different units here are some examples no linear Association negligible negative Association weak positive Association moderate negative Association very strong positive Association very strong negative association in these scatter plots what is happening to Y as X is increasing an important point to remember is that correlation is a measure of linear Association if the relationship is curvilinear using the correlation measure is not appropriate if X changes and Y stays the same then the correlation is zero since the correlation measure is a measure of linear Association we cannot use correlations on categorical data it's related to sample size and it's also very sensitive to outliers the correlation measure R measures the strength of linear Association squaring the correlation coefficient gives us R squared which is the coefficient of determination it is the proportion of common variation in two variables this measures the strength or the magnitude of the relationship while we cannot use percentage to interpret R we can do so for R squared for example if R squared equals 67% then we can say that 67% of variation in X is related to variation in Y correlation does not imply causation it's easy to see that in this chart the Internet Explorer market share correlates with the murder rate in the US but that doesn't mean that one caused the other causal relationships are determined based on facts and business models we cannot determine causality from data correlation is a mathematical formula you will get a number no matter what data you feed first you need to establish a logic relation and then find the correlation variables may be correlated if they have a causal relationship for example water causes plants to grow correlation can also occur when one variable is both the cause and the effect for example coffee consumption can cause nervousness but it's possible that nervous people also drink more coffee correlation can also be high because both variables move together due to a missing third variable for example this comparison of deaths due to drowning and soft drink consumption during summer both variables are related to heat and humidity a third variable not shown here emitting such variables can be dangerous here's a look at some additional measures of correlation using scatter charts this concludes our video on correlation today we discuss the definition of correlation the need for correlation details on computing correlation including variance covariance and the correlation coefficient strength of Association linear and curva linear relationships properties of correlation R squared the coefficient of determination and correlation versus causation this video will cover binomial distributions which are a type of discrete distribution we will first compare discrete and continuous distributions of a single random variable and then we'll look at the binomial distribution specifically there are two types of random variables discrete and continuous a discrete random variable has only a finite number of possible values whereas a continuous random variable has a continuum of possible values usually a discrete distribution results from account whereas a continuous distribution results from a measurement the distinction between counts and measurements is not always clearcut a probability distribution is simply a mapping of all distinct events for a variable and their probability of occurrence such as the distribution of a coin flip experiment the form of the distribution depends on whether the variables are discrete or continuous here are some examples of discrete variables outcomes of dice rolls whether a customer likes or dislikes a product or the number of hits on a website some examples of continuous variables include the weekly change in the Dow Jones industrial average daily temperature or the time between machine failures to specify the probability distribution of event X we need to specify all of its possible values and their probabilities we assume that there are K possible values and write out our list of possible values like this a typical value is denoted like this and the probability of a typical value is denoted like this next we will discuss distributions of both discrete and continuous variables for each type of variable distributions can be characterized by three measures mean variance and standard deviation these are formulas for working with discrete distributions well we won't be computing these measures by hand you do need to be aware of the formulas the mean also called the expected value is calculated with this formula the mean is a weighted sum of all possible values weighted by their probabilities mean is denoted by the Greek letter mu the variance has a weighted sum of the squared deviations of the possible values from the mean where the weights are again the probabilities the standard deviation is simply the square root of the variance standard deviation is denoted by the Greek letter Sigma these are the formulas for working with continuous distributions a probability distribution visually summarizes the probabilities associated with all possible events for a variable we will focus on three probability distributions that are commonly used in explaining realworld events binomial and exponential distributions are used with discrete data while normal distributions are used with continuous data a binomial distribution is a discrete distribution that represents the number of successes in n independent trials each of which has the probability of success P each trial has a binary outcome for example a coin toss yields either heads or tails the probability of either observation heads or tails is the same each time we toss the coin these outcomes are generally called success and failure the probability of success is P and the probability of failure is 1 minus P the distribution Maps the outcome of all the trials each trial has to be independent and the probability of success has to be the same for each trial this is the probability mass function formula for a binomial distribution if we toss a coin 100 times what is the probability that we will get 40 heads what is the probability of getting 90 heads that probability can be computed by applying this formula we have only two possible outcomes 1 0 or success/failure in n independent trials this formula depicts the probability of exactly X successes n is the number of trials x is the number of successes out of n trials P is the probability of success and 1 minus P is the probability of failure all probability distributions are characterized by an expected value and variance if we toss a coin 100 times what would be the average number of heads we would get what about the variance these are computed using these formulas you'll often see the assumptions of normal distribution being applied to discrete outcomes this is because the binomial distribution approximates to a normal distribution for large samples so for large enough samples we can calculate probabilities using normal probability rules this concludes our video on binomial distributions today we covered discrete and continuous distributions of a single random variable and the binomial distribution this video will cover normal distributions the probability density function cumulative distribution functions the 6895 99.7 rule and standardizing z values the single most important distribution in statistics is the normal distribution it is a continuous distribution and it's the basis of the familiar symmetric bellshaped curve the mean of the normal distribution is in the center the standard deviations are marked at equal distances from the mean any particular normal distribution is specified by its mean and standard deviation by changing the mean the normal curve shifts to the left or the right by changing how spread out the standard deviations are the curve also changes standard deviations can be spread out wider or closer together therefore there are really many normal distributions not just a single one the normal distribution is a two parameter family where the two parameters are the mean and the standard deviation here's a tool you can play with online that illustrates a normal distribution in real life it looks like a triangular shaped pegboard into which balls are dropped when there's an equal probability that the balls will drop either left or right their final placement forms a normal distribution however when the probability of the balls dropping left or right is unequal which is something you can experiment with using this tool the distribution changes the formulas for mean and standard deviation are very complex but you will not have to compute them because the software will with continuous variables there is a continuum of possible values such as all values between 0 and 100 or all values greater than zero instead of assigning probabilities each individual value in the continuum the total probability of one is spread over this continuum thus the shaded area within the bell curve will always have an area of one the key to this spreading is called a density function which acts like a histogram the higher the value of the density function the more likely this region of the continuum is a density function usually denoted by FX specifies the probability distribution of a continuous random variable X the higher FX is the more likely X is probabilities are found from a density function as areas under the curve so for example the shaded portion under this bell curve represents the probability of X being between 65 and 75 the cumulative distribution function or CDF is the probability that the variable takes a value less than or equal to X it is the total area under the normal curve up to X here's an example the beauty of the normal curve is that no matter what its mean and standard deviation are the area between the mean minus one standard deviation and the mean plus one standard deviation is always about 68% the area between the mean minus two standard deviations and the mean plus two standard deviations is always about 95% and the area between the mean minus three standard deviations and the mean plus three standard deviations is always about ninetynine point seven percent that means almost all values fall within three standard deviations on either side of the mean this is true for all normal curves no matter their shape but how good is this rule for real data let's go ahead and check out an example here's our data the mean of the weight of one hundred and twenty women runners in the sample is one hundred and twenty seven point eight pounds the standard deviation is fifteen point five here's what our distribution would look like let's look a little more closely at that distribution 68% of our 120 runners is about 83 runners according to the 68 95 99.7 rule those runners should all fall within one standard deviation of the mean weight of one hundred and twenty seven point eight that is eightythree of our runners should fall between one hundred and twelve point three and one hundred and forty three point three pounds when we check our data we see that seventynine runners fall within one standard deviation of the mean furthermore ninetyfive percent of our group or about 114 runners should fall within two standard deviations of the mean or between ninety six point eight and one hundred and fifty eight point eight pounds the data shows that 115 runners fall within two standard deviations of the mean finally according to the rule ninetynine point seven percent of our runners or one hundred and nineteen point six runners should fall within three standard deviations of the mean or within a range of eighty one point three pounds to one hundred and seventy four point three pounds according to our data all 120 runners fall within this range so it seems as if the rule is pretty accurate in this case there are indefinitely many normal distributions one for each pair of standard deviation and mean one particular combination of standard deviation and mean deserves special attention and that is the standard normal distribution all normal distributions can be converted into the standard normal curve by subtracting the mean dividing by the standard deviation but all of the integrals for the standard normal distribution have been calculated and put into a table for us and we also have software to help us out so we never have to integrate the long way this diagram illustrates the conversion of X values into Z values when we convert a normal distribution to a standard normal distribution here's a practice problem if birth weights in a population are normally distributed with a mean of 100 and 9 ounces and a standard deviation of 13 ounces what is the chance of obtaining a birth weight of 141 ounces or heavier when sampling birth records at random here's how we solve this problem we subtract 109 our mean from 141 and then divide by our standard deviation 13 so Z equals two point four six then we will use the normdist function in excel to discover that our value for Z two point four six equals zero point nine nine three the chance of a baby being born heavier corresponds to the right tail of the distribution so the probability that we will get a value for Z that's greater than or equal to two point four six can be discovered by subtracting our value for Z point nine nine three from the total area of the standard distribution one this concludes our video on normal distributions continuous distributions density functions cumulative distribution functions the 6895 99.7 rule and standardizing Z values this video will cover populations and inferences sampling error and the central limit theorem a population is the set of all members about witches study intends to make inferences here's a population of people we'd like to study there television watching behavior to determine how many watch a particular show so we can decide whether to purchase advertising spots during this period of time but our population is much too large for a feasible study to study their television viewing habits we will have to survey them and it's not feasible to survey every single individual so instead we'll take a sample of the population to study choosing a representative sample we can make some inferences about the population behavior but it's unlikely that one sample can provide accurate measures of behavior for the entire population an estimate of the population parameter or the proportion watching a television show is likely to be different for different samples of the same size and is likely to be different from the population parameter this is called sampling error the sampling error is unknown but we can estimate the extent of this error by applying the central limit theorem the central limit theorem tells us that what we know about our sample can tell us about the larger population the sample came from for any results that are generated from samples we get a range of estimates of a population parameter which includes mean and standard deviation from each sample in our example it would be an estimate of the proportion watching a particular TV show these estimates have their own distribution and the central limit theorem tells us that the distribution looks like a bell curve the central limit theorem makes predicting outcomes a lot easier if the sample size is large enough then the sampling distribution of the mean is approximately normally distributed regardless of the distribution of the population if all possible random samples each of size n are taken from any population with the mean mu and a standard deviation Sigma the sampling distribution of the sample means or averages will have a mean have a standard deviation and be approximately normally distributed regardless of the shape of the parent population remember that normality improves with a larger n and it all comes back to Z note the symbols here the mean of the sample means is noted as mu of x bar the standard deviation of the sample means is written as Sigma of X bar and is also called the standard error of the sample mean that concludes our video today we covered populations and inferences sampling error and the central limit theorem in this video we will first define probability then we will cover the rule of complements the addition rule probabilistic independence conditional probability and the Bayes theorem a probability is a number between 0 and 1 that measures the likelihood that some event will occur for a random variable an event with probability 0 cannot occur whereas an event with probability 1 is certain to occur an event with probability greater than 0 and less than 1 involves uncertainty here are some examples the odds of winning a lottery the likelihood of a particular candidate winning an election or the chance of rolling a 4 on a fair die in the case of the die there are 6 sides so the odds of rolling of four are one out of six the complementary rule in probability is simply the probability of an event not occurring if a is any event the probability of a is P of a the complement of a is the event that a does not occur the probability of the complement of a is shown by this equation one minus the probability of the event occurring in our dice example the probability of getting a four was one in six so the probability of not getting a four is one minus one and six which equals five and six the addition rule of probability involves the probability that at least one of the events will occur events are exhaustive if they exhaust all possibilities one of the events must occur for example when we roll a 6sided die we will always end up with a number between 1 & 6 we say that events are mutually exclusive if at most one of them can occur for example you can't roll a 3 & a 6 on one die at the same time if you have two mutually exclusive events like our 3 & 6 then the probability of either one occurring is the sum of the two separate probabilities if two events are independent or their outcomes aren't affected by each other then the probability of both a and B occurring is simply the product of the two probabilities in the case of our die the probability of getting a six on the first roll and getting a three on the second roll is one in six times one and six which equals a 1 in 36 chance sometimes the probability of one event will affect another these are called dependent events and their probabilities are called conditional this is the formula for conditional probability the conditional probability of a conditional that B has already occurred is equal to the joint probability of both of the events occurring together divided by the probability of B occurring without regard to whether a has occurred or not the bayes theorem allows us to estimate posterior probabilities once we obtain new data with it we can measure the likelihood of event H occurring once we obtain particular pieces of evidence from data D the parts of the theorem include the independent probability of H or prior probability the independent probability of D the conditional probability of D given H or likelihood and conditional probability of H given D or posterior probability this concludes our video on basic probability today we defined probability and covered the rule of complements the addition rule probabilistic independence conditional probability and the Bayes theorem in this video we will cover variable roles including explanatory and outcome variables and variable classification including qualitative variables nominal ordinal and binary and quantitative variables discrete continuous interval and ratio any analytics project first begins with a question what is the problem you're trying to solve to address that question we need to collect data the next step in the process is to understand the data collected and only then can we move to further steps of data cleaning data analysis and solving the problem a key step in understanding the information collected is to identify all the variables in the data set we need to know what variable types we have in order to make them amenable to further analysis variables have two possible roles the first is explanatory explanatory variables are also called features or independent variables these are variables that are used as inputs to explain the variation in the outcome variable the second role a variable can take on is outcome an outcome variable is also known as a target or dependent variable these are variables that measure the output or impact that's being studied most studies have many independent variables and one dependent variable for example a person's weight could be a function of age gender and calories consumed fuel efficiency is a function of features such as car size weight and number of cylinders restaurant ratings are a function of food quality ambiance and service variables can be qualitative or quantitative qualitative data can be nominal ordinal or binary quantitative data can be discrete or continuous with either an interval or ratio level of measurement we'll start by discussing how to determine whether a variable is qualitative or quantitative the best way to decide whether a variable is qualitative or quantitative is to use the subtraction test if two experimental units such as people have different values for a particular measure then you should subtract the two values and ask yourself about the meaning of the difference for example when hair color is coded as 1 equals blonde 2 equals red 3 equals brown and 4 equals black the difference between the variables has no meaning so it fails the subtraction test which means hair color is a categorical or qualitative variable however if the difference is meaningful then it is a quantitative variable for example age in years the differences between these numbers have a meaning so the variable is quantitative we will now discuss qualitative variables in detail categorical variables are those that have only a few possible values thus assigning each value to a particular group or category for example oceans are categorical variable nominal and ordinal variables are often called labels a nominal variable has levels with arbitrary names for example car colors ordinal variables have a logical order for example exam grades a dichotomous or binary variable is a categorical variable that has only two levels or categories often the answer to a yes or no question but a variable doesn't have to be a yes/no variable to be binary it just has to have only two categories such as gender we will now discuss quantitative variables in detail quantitative variables are those for which the recorded numbers encode magnitude information based on a true quantitative scale they can be discrete or continuous a discrete variable has only whole number counts a continuous variable can take on any value on the number scale to determine whether a variable is discrete or continuous use the midway test if for every pair of values of a quantitative variable the value midway between them is a meaningful value then the variable is continuous otherwise it's discrete for example age is continuous because the difference between ages 20 and 30 is meaningful an example of a discrete variable is the number of children in a family you can see here that 2.5 does not make sense the interval level of measurement ranks data it can be either discrete or continuous with interval variables precise differences between units of measure exist but there's no meaningful 0 for example take IQ scores make sense to talk about someone having an IQ 50 points higher than another person but an IQ of 0 has no meaning ratio variables are interval variables but with the added condition that 0 of the measurement indicates that there is none of that variable true ratios exist when the same variable is measured on two different members of the population for example consider the weight of an individual it makes sense to say that a hundred and fifty pound adult weighs twice as much as a 75 pound child however it doesn't make sense to say that 70 degrees Fahrenheit is twice as hot as 35 degrees Fahrenheit so temperature is not a ratio variable this concludes our video on variables in this video we covered variable roles including explanatory and outcome variables and we also covered variable classification including qualitative variables nominal ordinal and binary and quantitative variables discrete continuous interval and ratio this video will cover basic information about coding coding systems and types of variables in coding including binary ordinal nominal and continuous coding is the process of translating the information gathered from questionnaires and other investigations into something that can be analyzed usually using a computer program coding involves assigning a value to the information given in a questionnaire and often that value is given a label coding can make the data more consistent for example if you ask the question what gender you might end up with the answers male female M F etc coding will avoid such inconsistencies a common coding system for binary variables is the following zero equals no and one equals yes where the number is the value assigned and the yes or no is the label of that value some like to use a system of ones and twos where one equals no and two equals yes this brings out an important point in coding when you assign a value to a piece of information you must also make it clear what the value means in the first example one equals yes but in the second example one equals no either way is fine as long as it's clear how the data are coded you can make it clear by creating a data dictionary as a separate file to accompany the data set a binary variable is any variable that is coded to have two levels like this example in SAS data representing gender coded as MF would be converted into a binary variable here's an example if we're asking about the number of years of education a person has with a value of 1 for each year of education that would mean anyone with more than 12 years of education has been to college and anyone with less than 12 years of education has not been to college we can recode into a binary yes/no variable by saying that if education is greater than 12 that implies that college equals one otherwise college equals zero this type of coding is useful in descriptive and predictive analytics the coding process is similar with other categorical variables for the variable education we might code as follows zero equals did not graduate from high school one equals high school graduate two equals some college or post high school education and three equals college graduate note that for this ordinal categorical variable we need to be consistent with the numbering because the value of the code assigned has significance the higher the code the more educated the respondent is in SAS we would convert years of education to education categories like this here's an example of what not to do zero equals some college or post high school education one equals high school graduate two equals college graduate and three equals did not graduate from high school can you tell what's wrong with this example the data we're trying to code has an inherent order but the coding in this example does not follow that order here's the correct way to do it for nominal categorical variables however the order makes no difference here's an example for the variable reside 1 equals northeast 2 equals South 3 equals northwest 4 equals Midwest and 5 equals southwest it doesn't matter what order we use for these categories Midwest can be coded as 4 2 or 5 because there's not an ordered value associated with each response continuous variables are usually left in the same format as they are in the original data set however be careful about missing values in Muscoda data you may also need to code responses from fill in the blank and openended questions with an openended question such as why did you choose not to see a doctor about this illness respondents will all answer differently also you may give response choices for a particular question but offer an other specify option as well where respondents can write whatever response they choose these types of openended questions can be a lot of work to analyze one way to analyze the information is to group together responses with similar themes for the question why did you choose not to see a doctor about this illness responses such as didn't feel sick enough to see a doctor symptoms stopped and the illness didn't last very long could all be grouped together as the illness was not severe you will also need to code don't know responses typically don't know is coded as 9 that concludes our video on coding with variables today we covered some basic information about coding coding systems and types of variables in coding including binary ordinal nominal and continuous this video will cover using graphs to understand data we will cover three types of graphs bar charts box plots and histograms it's important to know which graph to use if the variable is categorical look at it using a bar chart if it is continuous you should examine it using either a box and whisker plot or a histogram a bar chart translates the data from frequency tables into a pictorial representation it depicts categorical variables and shows frequency or proportion in each category this bar chart looks at the frequency distribution of patients with pulmonary embolism which occurs when one or more arteries and the lungs gets blocked by a blood clot notice that this is a binary variable with only two possible responses yes and no yes is coded as one and no is coded as zero the frequency distribution of a binary variable shows the number of patients in each group it's much easier to extract information from a bar chart than from a table this chart depicts shock index which is the ratio of heart rate to blood pressure and should lie between 0.5 and 0.8 the higher it is the greater the risk shock index is an ordinal categorical variable the vertical bars here represent the number of patients in each category the shape of a distribution describes how the data are distributed measures of shape include symmetric and skewed the first thing we're looking for in any continuous variable is the shape of distribution what are the boundaries of the data points and how are they clustered if a few small values are mixed in with a majority of values being much higher the data will have a left or negative skew likewise if we have some large values mixed in with a majority of small values the distribution will have a right skew or we say it is positively skewed if the distribution is balanced it is symmetric we can also observe skewness by inspecting the values for instance consider the numeric sequence 49 50 51 whose values are evenly distributed around a central value of 50 this produces a symmetric shape we can transform the sequence into a negatively skewed distribution by adding a value far below the mean 40 49 50 51 this produces a left skew similarly we can make this sequence positively skewed by adding a value far above the mean 49 5051 60 this produces a right skew a boxandwhisker plot provides an easy way to examine the entire distribution of a variable and it's also very useful when we want to examine relationships between two variables where one is categorical and another is continuous let's look at the Box first the bottom of the box represents the 25th percentile while the top of the box represents the 75th percentile the line in the middle represents the median the bigger the box the greater the spread of the data the whiskers in the box plot do not necessarily represent the minimum and maximum values they show the minimum and maximum only if these values are less than one and a half times the interquartile range if the values are bigger than that the whiskers represent one point five times the interquartile range or IQR values outside that range represented as dots in the example here note that there are many dots above the top whisker this is a quick and easy way to check for outliers we can look at the same shock index data with a histogram the dots in the box plot showed us that there were several large values greater than one point five times the interquartile range the same is represented in this histogram with the right skew there's no single rule of thumb for choosing bin sizes the bin sizes you choose will depend on the research question you're asking here using 100 bins shows too much detail and it's not useful likewise too few bins tells us little about the underlying shape of the distribution this example uses two bins and provides too little detail note that the box plot we looked at earlier shows the same positive or right skew that we observe in a histogram for shock index the median inside the box plot also provides information on the skewness of the distribution if the median is at the center of the box the distribution is symmetric if the data have a left skew then the median will be pulled to the right inside the box if the data have a right skew then the median will be pulled to the left in the box this concludes our video on using graphs to understand data today we covered three types of graphs bar charts box plots and histograms this video provides a quick review of the measures of central tendency including mean median and mode variation and dispersion range for tiles and interquartile range sample variance standard deviation and the normal curve as well as the 6895 99.7 rule before we begin here's a quick review of symbols we'll use in this video we will also use the abbreviation IQR for interquartile range the mean is the average or balancing point to find the mean find the sum of all the values divided by the sample size here's a simple example of calculating the mean of the age of several participants in a study the Sigma is the sum and the xbar is the sample mean after adding the values together and dividing by the number of values 8 we arrived at our mean 23 point 2 5 we can construct means of binary variables the mean of a binary variable represents the percentage of one's the mean is affected by extreme values which is why we often look at means in conjunction with medians to understand how the data are distributed in this example the mean of the values 1 2 3 4 and 5 is calculated by adding the values together to make 15 then dividing the values by 5 the mean of this group is 3 however if the values are 0 1 2 3 4 and 10 the mean shifts to 4 the median is the middle value of the data in this example we have 7 different ages to find the mean we first order them from smallest to largest and then locate the value in the center however if we have an even number of observations median is computed as the average of the two middle values the median is not impacted by outliers here our median of the five values is three if we add the value ten to the set of values our median is still three the mode is the value that occurs most frequently it is only useful when we have some values clustering together in this example the mode is 9 there may be no mode or there may be several modes there is no single measure of center that is best if the data are normally distributed then mean is used however if data are not normally distributed the median is a better measure often we use both to understand the underlying structure of the distribution there are several measures to examine the spread of the data they include range percentiles interquartile range and variance or standard deviation their range is the difference between the largest and the smallest value this histogram shows a minimum value of 15 and a maximum value of 94 the ranges 94 minus 15 equals 79 another measure of spread is the value of each quartile we take the total number of data points we have and divide them into four parts the value corresponding to the end point of each part is the quartile value the interquartile range is the difference between the value at the third quartile minus the value at the first quartile the first quartile q1 is the value for which 25% of the observations are smaller and 75% are larger the second quartile q2 is the same as the median 50% are smaller and 50% are larger only 25% of the observations are greater than the third quartile let's take the age values 1535 49 65 and 94 the first quartile is at 35 this means that 25% of the participants are below age 35 like why is 25% are above 65 years old the interquartile range is 65 minus 35 equals 30 years sample variance is calculated as the average of squared deviations of values from the mean as shown here we square the differences from the mean to provide equal weight to observations below the mean versus those above the mean because we square the difference values that are further away from the mean get higher weight than those close to the mean standard deviation is the most commonly used measure of variation it shows the variation around the mean and has the same units as the original data it is calculated by finding the square root of the variance here's an example of the standard deviation using age data note that sample standard deviation is represented by the symbol s X bar represents the sample mean the standard deviation is an extremely useful measure it tells us how close or far apart data are from the mean the higher the standard deviation the greater the spread of the data here in red is an example of a moderate standard deviation you can see that the data is spread pretty evenly the purple shows a low standard deviation in which the data is concentrated near the middle the blue example shows a high standard deviation where the data is concentrated on the outside these formulas are important to know well while software can compute these for you it's important to know how it's done using simple numbers whenever you work with data you'll have variables that have a center and spread a very useful rule to know is that no matter what the shape of the distribution 75% of values will lie within two standard deviations of the mean while 89 percent will lie three standard deviations from the mean so if someone gives you just these two pieces of information you can make some predictions on where a new data point will lie however what's even better in statistics is knowing that for large samples data are distributed symmetrically and follow the bell curve the 6895 99.7 rule states that 68% of the area of a normal curve lies within one standard deviation of the mean 95% of the area lies within two standard deviations of the mean and ninetynine point seven percent of the area lies within three standard deviations of the mean this rule works for all normal curves no matter their shape that concludes our video on measures of central tendency including mean median and mode variation and dispersion range quartiles and interquartile range sample variance standard deviation the normal curve and the 6895 99.7 rule you
