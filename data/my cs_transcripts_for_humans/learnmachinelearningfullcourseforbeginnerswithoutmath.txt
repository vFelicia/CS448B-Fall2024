With timestamps:

00:00 - hello and welcome to this free course in
00:03 - data science and machine learning for
00:05 - beginners made my AI Sciences Academy
00:07 - this easy-to-understand course is
00:10 - dedicated for beginners who need to
00:11 - learn the A to Z fundamentals of data
00:14 - science and machine learning through a
00:15 - gradual and segmented approach ok here
00:20 - are the four main parts we will cover
00:21 - during this course
00:24 - we will start with an introduction part
00:26 - two data science and machine learning in
00:29 - this part we will answer some questions
00:30 - that you may ask like what is data
00:33 - science and machine learning why data
00:35 - science now and when I can apply data
00:38 - science and machine learning techniques
00:41 - in learning part 1 we will learn the
00:44 - preliminary to understand data science
00:46 - and machine learning in this part we
00:48 - will learn some vital concepts in data
00:50 - science and machine learning
00:53 - a learning bar - we will start the
00:56 - machine learning part where I will
00:58 - explain to you how the machine learning
01:00 - models work we'll also discuss about
01:02 - regression classification and clustering
01:04 - in learning part three we will talk
01:08 - about how to evaluate the performance of
01:11 - the model and choose the best one based
01:13 - on some indicators in learning part four
01:16 - of this course we will discuss about
01:18 - some best practices in data science and
01:21 - machine learning at the end of this
01:23 - course you will receive a gift so please
01:26 - follow this course until the end this
01:28 - gift will help you on your learning
01:30 - journey so ready let's start
01:40 - you
01:42 - data science is not a straightforward
01:44 - easy to define field like most
01:46 - traditional fields it's rather a
01:49 - multidisciplinary field which means that
01:51 - it combines different areas such as
01:53 - computer science mathematics and
01:55 - statistics because data science can be
01:58 - applied and used in various applications
02:00 - and fields it requires domain expertise
02:03 - in each particular area ok for example
02:07 - if we use data science to develop a
02:09 - medical analysis application then we
02:11 - will need an expert in medicine to help
02:14 - define the system and interpret the
02:16 - results
02:18 - data scientists explore the data
02:21 - visualize it and calculate important
02:24 - statistics from it then depending on
02:26 - these steps and the nature of the
02:28 - problem itself they develop a machine
02:30 - learning model to identify the patterns
02:32 - so machine learning and deep learning
02:35 - are the subfields of data science
02:39 - so you might ask what is the difference
02:41 - between data science data analytics and
02:44 - big data well big data means huge
02:48 - volumes of various types of data we
02:50 - differentiate big data by its four V's
02:54 - which are the characteristics that are
02:56 - distinct from ordinary data they are
02:59 - volume velocity variety and veracity
03:06 - the sheer volume is the main
03:09 - characteristic that makes data big
03:11 - velocity is the frequency of incoming
03:14 - data that needs to be processed variety
03:17 - means different forms of data veracity
03:21 - refers to the trustworthiness of the
03:23 - data on the other hand data analytics is
03:26 - more about extracting information from
03:29 - the data by calculating statistical
03:31 - measures and visualizing the
03:33 - relationship between the different
03:34 - variables and how they are used to solve
03:37 - the problem it's like descriptive
03:39 - statistics
03:41 - why data science now that's an
03:44 - interesting question
03:45 - first there is currently plenty of data
03:48 - more than at any time before in history
03:51 - and it just keeps growing exponentially
03:53 - second now we have much better computers
03:55 - and computational power than ever before
03:58 - a task that can be finished in a few
04:00 - seconds nowadays would have required
04:02 - days with the computers that existed
04:04 - just a few years ago and finally we have
04:07 - more advanced algorithms for pattern
04:09 - recognition and machine learning than we
04:11 - did just a few years ago so in one
04:14 - sentence if you want to know why data
04:15 - science has become our focus right now
04:17 - is because we have a lot more data
04:19 - better algorithms and better hardware
04:23 - let us see where data science and
04:26 - machine learning is applied
04:28 - I want to say that data science is
04:30 - everywhere the number of data science
04:33 - applications is countless it's because
04:35 - we have data everywhere and there are
04:38 - dozens of algorithms developed each year
04:40 - to solve these tasks however we will
04:43 - talk about a few famous use cases of
04:45 - machine learning in data science in our
04:47 - daily lives
04:49 - as you can see machine learning and data
04:51 - science are currently being used in many
04:53 - fields such as healthcare finance
04:55 - transport social media ecommerce and
04:58 - virtual assistant apps among others in
05:01 - healthcare machine learning is currently
05:04 - used in disease diagnosis it provides
05:07 - higher accuracy compared to professional
05:09 - physicians it is also undergoing
05:11 - extensive research and drug discovery
05:14 - another application is robotic surgery
05:17 - where we have an AI robot helping and
05:20 - performing the surgery with a precision
05:22 - that is actually higher than that of the
05:25 - best surgeons
05:27 - in transport Tesla used machine learning
05:30 - algorithms in its self-driving cars
05:32 - machine learning is also used for air
05:35 - traffic control
05:37 - in finance I worked for two years as the
05:40 - KPMG consultant in one of the French
05:42 - banks to develop different machine
05:44 - learning algorithms predicting customer
05:46 - defaults and bank capital requirements
05:49 - many banks are currently using machine
05:51 - learning powered software for fraud
05:53 - detection also banks are using machine
05:56 - learning for algorithmic trading in
05:59 - social media I think all social media
06:01 - platforms today use machine learning for
06:03 - both spam filtering and sentiment
06:05 - analysis Facebook also uses machine
06:08 - learning image recognition
06:11 - in e-commerce many online shopping
06:14 - websites such as Amazon eBay and udemy
06:17 - use machine learning for customer
06:19 - support targeted advertising and product
06:21 - recommendation machine learning is also
06:24 - used in virtual assistant apps many
06:26 - startups are founded based on the idea
06:28 - of developing a machine learning powered
06:30 - assistant in one particular field this
06:33 - assistant can be a chatbot for example
06:35 - which can intelligently reply and answer
06:37 - nearly any inquiries in that field
06:41 - these are just a few broad in general
06:43 - applications of data science and machine
06:44 - learning you can develop your own
06:46 - application in any field that you find
06:48 - interesting and have some experiences
06:50 - and by the end of this course you will
06:52 - be equipped with the knowledge necessary
06:54 - to create an app in the area of your
06:56 - choice
06:58 - let's talk just a little bit about the
07:00 - history of data science the term data
07:02 - science has been appearing in various
07:04 - contexts over the past thirty years but
07:07 - did not become an established term until
07:08 - relatively recently in its early usage
07:12 - it was used as a substitute for computer
07:14 - science since 1960 when Peter now are
07:17 - first mentioned the term
07:19 - how about the future of data science and
07:21 - data scientists we can clearly see that
07:24 - the future of data science is very
07:26 - bright another evidence for that is the
07:28 - cloud services that have appeared in the
07:30 - last two or three years being extremely
07:32 - cheap and fast they can help develop
07:34 - more advanced machine learning
07:36 - applications in all fields so it will
07:39 - now be surprising to see many tasks that
07:41 - we considered science fiction such as
07:43 - assistant robots and self-driving cars
07:45 - already in use in our daily lives
07:49 - how about the future of data scientists
07:51 - let me share some statistics with you as
07:53 - you can see in the figure on your screen
07:55 - at the end of 2016 the percentage of
07:58 - jobs for data scientists was 474 percent
08:02 - larger than those for statisticians
08:04 - today in the USA the work of a data
08:08 - scientist is one of the high Penguin's
08:10 - learning data science maybe the best
08:13 - decision you've ever made in our
08:14 - learning company ai sciences is here to
08:17 - help you achieve this goal now how can
08:20 - you get the ultimate benefit from this
08:22 - course
08:24 - I highly recommend you take this course
08:26 - very seriously
08:28 - and follow it step by step we highly
08:31 - suggest that you go through the study
08:32 - materials that you used in your high
08:34 - school undergraduate and graduate
08:36 - programs and revised topics such as
08:38 - linear algebra calculus and statistics
08:41 - we will not go too deep into the math
08:43 - behind algorithms in this course but we
08:46 - will cover basic ideas logic and in some
08:48 - cases formulas to understand our main
08:50 - topic of interest better also try to
08:54 - finish every single project provided in
08:57 - the course on your own and then check
08:59 - the solution
09:01 - finally we encourage you to go through
09:03 - any further reading material that you
09:04 - will find it will give you an overview
09:06 - of what you can learn next after
09:08 - finishing the course okay in this lesson
09:11 - we will explore in detail some important
09:13 - terms in data science you ready let's go
09:17 - in the following lesson we will talk
09:19 - more about data and variables so what is
09:22 - the data data are basically collections
09:26 - of facts measurements observations
09:28 - numbers words etc that have been
09:30 - transformed into a form that can be
09:32 - processed by computers data are stored
09:34 - in columns and rows the convention is
09:37 - that each row represents one observation
09:39 - case or example and each column
09:42 - represents one feature or variable got
09:45 - it we also have two types of variables
09:48 - based on their value numerical and
09:51 - categorical variable if the value is a
09:53 - number and we can compute the mean for
09:56 - this variable we call it a numerical
09:58 - variable but if the but if the value is
10:02 - a factor or label and we can't compute
10:05 - its mean we have a categorical variable
10:09 - so we can also talk about dependent and
10:12 - independent variables a dependent
10:14 - variable is one that we need to predict
10:16 - and the independent variables or X
10:19 - variables will help us to predict a y
10:22 - variable it is essential to know that X
10:25 - variables need to be independent of each
10:27 - other and that is why we call them
10:29 - independent variables or predictors
10:33 - couple other important terms in data
10:35 - science are population and sample and
10:37 - data science the whole population is our
10:40 - target but due to a lack of resources a
10:43 - data scientist can't work in the entire
10:46 - population because of that we have to
10:48 - choose a representative sample of the
10:51 - data from the population the goal of
10:53 - machine learning algorithms is to find
10:55 - parameters that can do the mapping on
10:58 - the whole population
10:59 - based on the given sample as you can see
11:02 - we have our population and we will
11:04 - choose a sample by using the sampling
11:07 - technique
11:08 - now let's talk about outlier and missing
11:12 - data in data science an outlier is a
11:15 - data point that differs from other
11:16 - observations an outlier may occur due to
11:19 - variability in the measurement or it may
11:21 - indicate an experimental error outliers
11:25 - can alter the performance of many
11:26 - machine learning algorithms as we can
11:30 - see there in the image we can detect
11:31 - outliers by visualizing the data in this
11:35 - example employee number two and nineteen
11:37 - are outliers based on their profile
11:42 - how to deal with outliers we can drop
11:45 - them altogether cap them with the
11:47 - threshold assign new values based on the
11:50 - mean of the data set for example or
11:52 - apply a transformation on the data set
11:55 - itself
11:58 - okay one way to handle missing data is
12:00 - by dropping the observations another way
12:03 - to handle them is to use data imputation
12:06 - techniques so if we have a numeric
12:09 - feature we can replace the missing value
12:12 - with a mean median or mode or we can
12:16 - select random observations from the data
12:18 - set and replace its feature value in the
12:21 - observation that has missing values the
12:24 - last imputation technique can be
12:26 - performed by regressing the missing
12:28 - feature on the other features and making
12:30 - a prediction of the missing value
12:34 - if we have a categorical feature we can
12:36 - use the mode replacing or also we can
12:39 - use the KNN model to predict the feature
12:42 - that has a missing value
12:45 - if you still don't know much about the
12:47 - regression and KNN model that worries
12:49 - David will present them to you in the
12:51 - machine learning section
12:53 - that's it for this lesson in this class
12:56 - we will learn about the link between AI
12:58 - machine learning and deep learning dl
13:02 - you'll find out how a machine learns and
13:04 - the types of learning there are ready
13:06 - let's go
13:08 - first here's the link between artificial
13:11 - intelligence machine learning and deep
13:13 - learning let's clear the confusion
13:15 - between these three essential terms so
13:17 - what are they I ml and DL
13:22 - by looking at the image it is clear that
13:24 - ml is a subfield of AI and DL is a
13:28 - subfield of ML
13:32 - machine learning algorithms were
13:33 - developed with the goal of finding a
13:35 - useful prediction function among machine
13:38 - learning algorithms let's mention the
13:40 - artificial neural network
13:42 - an artificial neural network consists of
13:45 - a collection of neurons connected to
13:47 - each other in a specific way however the
13:49 - use of neural network was limited
13:51 - because of the lack of computational
13:53 - power and the lack of proper
13:55 - optimization algorithms for neural
13:57 - networks this is where deep learning
13:59 - came in it is a simpler algorithm that
14:03 - uses more neurons and layers to perform
14:05 - a lot of learning tasks like image
14:08 - recognition and natural language
14:10 - processing today deep learning is used
14:13 - in a lot of areas such as the high-tech
14:15 - industry Tesla self-driving cars and the
14:18 - Seattle Amazon store are constructed
14:20 - based on the deep learning algorithms
14:22 - combined with computer vision
14:25 - now let's see how a machine learning
14:28 - algorithm learns
14:31 - okay Before we jump into how a machine
14:33 - learns let us first try to understand
14:35 - how a human baby learns think for
14:38 - example of a one-year-old human baby a
14:42 - baby does not know the difference
14:43 - between an apple and an orange for him
14:46 - all fruit is the same orange Apple
14:48 - bananas cucumbers in his first phase of
14:52 - learning called phase one of learning in
14:54 - the figure he builds an intuition that
14:56 - oranges and apples are of one shape and
14:59 - bananas and cucumbers are of another
15:01 - shape once a baby is comfortable with
15:04 - the shapes of fruit he goes into
15:06 - learning phase two phase two of learning
15:08 - in the figure
15:11 - by introducing another property like
15:13 - color now he knows that a fruit that is
15:17 - round in shape and red in color means
15:20 - that it's an apple in a round shape and
15:23 - an orange color means it's an orange in
15:26 - the face 3 the baby will gather a lot of
15:30 - data as shown in the table with these
15:32 - two properties and based on these data
15:34 - in the future he will know the
15:36 - difference between fruits
15:38 - machine learning models learn the same
15:40 - way in machine learning the properties
15:43 - of the fruit such as shape and color are
15:45 - called
15:46 - the features the fruit type is called
15:48 - the label each instant of an
15:51 - input/output pair is called an
15:53 - observation
15:55 - so depending on the features and labels
15:57 - enter to a machine learning algorithm
15:59 - learning is classified into three main
16:03 - categories supervised learning
16:05 - unsupervised learning and reinforcement
16:08 - learning we also have semi-supervised
16:11 - and instant based learning but in this
16:13 - lesson we're just going to focus on the
16:15 - main three learning types in supervised
16:18 - learning we train with labeled
16:20 - observations that means that for each
16:22 - observation of training data the input
16:25 - and output are known as you can see in
16:27 - the image we try to predict the output
16:30 - from the input by training our machine
16:32 - learning model
16:34 - classification is one example of
16:37 - supervised learning where the goal is to
16:38 - classify objects regression is another
16:42 - example where we try to understand the
16:44 - relationships among variables
16:47 - in a glance in supervised learning we
16:50 - have the Y and the X variables and we
16:53 - want to make the prediction of why
16:56 - okay now in unsupervised learning the
16:59 - trainer does not provide a labeled
17:01 - output in the learning data set the
17:04 - machine learning algorithm learns from
17:06 - unlabeled data and gathers information
17:08 - from it as you can see
17:11 - it's short in unsupervised learning we
17:13 - only have X variables and we need to
17:15 - gather the observations and groups based
17:18 - on the information given by X
17:21 - unsupervised learning is used mainly for
17:23 - clustering tasks where we organize the
17:26 - observation into clusters
17:29 - in reinforcement learning an agent
17:32 - learns by interacting with the
17:33 - environment so in this type of learning
17:36 - the agent performs an action in the
17:38 - environment this action takes the
17:40 - environment to a new state and gives a
17:42 - reward to the agent the reward can be
17:44 - negative or positive for multiple
17:47 - iterations and the rewards the agent
17:49 - learns based on his past experiences
17:53 - reinforcement learning is mainly used in
17:55 - skill acquisition tasks such as robot
17:58 - navigation or games
18:02 - okay to sum up we have three main types
18:05 - of learning supervised when we have X
18:07 - and y variables unsupervised when we
18:10 - only have X variables and reinforcement
18:14 - learning in this case the algorithm
18:16 - learns by rewards that it gets as a
18:18 - result of its action in an evolving
18:21 - environment
18:22 - now you know how a machine learns and
18:24 - what are the different types of learning
18:26 - I guess now you know a lot more about
18:28 - basic data science and machine learning
18:29 - terms in this lesson let me explain some
18:32 - important modeling terms to you alright
18:35 - you ready let's start
18:37 - in machine learning we always split our
18:40 - data into two so we have a training data
18:43 - set and a test data set the training
18:47 - data will help us to train our model and
18:49 - the test data set will help us evaluate
18:51 - our model performance and accuracy a
18:54 - typical machine learning model learns
18:56 - from the training data set and applies
18:58 - the learning to the test data set so if
19:01 - the model is able to make correct
19:03 - predictions on the test data set then
19:06 - the model is able to generalize the
19:08 - learning to any new data and then we can
19:10 - say we have a good model now what is an
19:14 - over fitted model when we obtain a model
19:18 - that works very well on the training
19:19 - data set but is not able to generalize
19:22 - to the test data sets we call such a
19:25 - model an over fitted model in this case
19:27 - the testing error is large because the
19:30 - model is very complex the model is under
19:33 - fitted when the training error is large
19:35 - because the model is too simple and just
19:37 - can't capture the true complexity of the
19:39 - data so you may wonder can we control
19:43 - these issues the answer is yes the
19:46 - solution for under fitting is either to
19:48 - increase the size of the data set or to
19:51 - increase the complexity of the model but
19:54 - the solution for overfitting is a bit
19:56 - trickier the first solution is to gather
19:58 - more data this solution is not always
20:01 - feasible the second solution is to use
20:04 - penalty terms in the model and that is
20:07 - called the regularization technique
20:10 - and the last one is to make
20:12 - cross-validation okay in short you need
20:15 - to understand that our goal as data
20:17 - scientists is to come up with a model
20:19 - that is not too generalized and not too
20:22 - focused on training data this model is
20:25 - called the best fit model this is
20:27 - basically a trade-off between an under
20:29 - fit and an over fit model like the
20:32 - middle image
20:34 - as you're already familiar with
20:36 - overfitting and underfitting the concept
20:38 - of bias and variance will be very easy
20:40 - to digest but before we start talking
20:43 - about bias and variance let us classify
20:45 - the type of models errors first we have
20:50 - the irreducible error which comes from
20:52 - the nature of the data itself for
20:54 - example the noise when you talk through
20:56 - your mobile phone the second kind of
20:59 - error is the reducible error we have to
21:02 - reducible errors the bias error and the
21:04 - variance error
21:06 - okay the bias error is the difference
21:09 - between the average prediction of our
21:11 - model and the correct value which we are
21:13 - trying to predict the bias error is high
21:16 - if the model is oversimplified the
21:19 - variance error is the variability of
21:22 - model predictions for the given data the
21:25 - variance error is high if the model is
21:27 - not generalizing well on new data okay
21:31 - look at the figure at the right the blue
21:33 - points represent how far we are from the
21:35 - minimum error which is represented by
21:38 - the small red circle in the case of low
21:40 - bias the blue points are not very far
21:43 - from the minimum error in the case of
21:46 - low variance the blue points are near
21:48 - each other of course we want our model
21:51 - outputs to be as close as possible to
21:53 - the minimum error which means low bias
21:56 - and low variance but it is impossible to
21:59 - have both low bias and low variance
22:01 - there is a trade-off between bias and
22:04 - variance because as we decrease the
22:06 - model bias we make it more complex and
22:09 - we increase its variance and when we
22:12 - decrease the variance we increase the
22:15 - bias looking back to overfitting and
22:18 - underfitting we can say that when the
22:20 - model is under fitted it has low
22:23 - variance and high bias when the model is
22:25 - over fitted then it has high variance
22:28 - and low bias
22:31 - as you can see in machine learning it is
22:33 - important to make a bias-variance
22:35 - tradeoff and to choose a middle model
22:39 - in data science we have different types
22:41 - and qualities of data and we need to
22:43 - make them usable in our model number one
22:46 - feature extraction and feature
22:48 - engineering will help us transform raw
22:51 - data into features suitable for modeling
22:53 - and to feature selection will help us
22:57 - remove unnecessary features during the
22:59 - data processing step
23:01 - in this part of the course we will learn
23:03 - various machine learning models which
23:05 - are commonly used for prediction
23:07 - classification clustering etc as I
23:11 - already mentioned machine learning is
23:13 - broadly classified into supervised and
23:16 - unsupervised learning supervised
23:19 - learning means that the algorithms are
23:21 - supervised during the training phase so
23:23 - in other words in order to train these
23:26 - algorithms we need data that have
23:28 - labelled targets for example if a model
23:31 - is being created for predicting house
23:33 - prices then the historical data that is
23:35 - used to train the model should have a
23:37 - target column stating the price of a
23:39 - house
23:41 - unsupervised learning is when we don't
23:44 - have a target labeled data in this case
23:47 - the algorithm classifies objects based
23:50 - on some existing features supervised
23:53 - machine learning problems have two
23:54 - categories of learning regression and
23:56 - classification
24:00 - regression algorithms are used to
24:03 - identify a relationship between a
24:05 - dependent variable target and
24:08 - independent variables predictor /
24:10 - features so in regression the target is
24:14 - always a continuous variable and the
24:17 - predictors can be continuous or discrete
24:19 - in nature regression is best used for
24:23 - finding causal effect relationships
24:25 - between the variables forecasting time
24:28 - series modeling etc in regression
24:31 - analysis the model tries to fit a curve
24:35 - to the data points in such a manner that
24:36 - the difference between the data point
24:38 - and the curve is at a minimum
24:45 - on the other side we have classification
24:48 - models the classification models are
24:51 - used to predict the target that has
24:53 - discrete values for so for example class
24:57 - of fruits orange pineapple and lime or
24:59 - predicting whether a patient is
25:01 - suffering from cancer or not
25:04 - for this kind of use machine learning
25:06 - models collect insights from the
25:08 - historical labeled data and use these
25:11 - insights to predict the target class
25:14 - okay listen up don't forget this
25:16 - regression and classifications are all
25:18 - supervised learning models because we
25:21 - have a labeled output which we need to
25:23 - predict if this labeled output is
25:25 - continuous we use regression and if it's
25:28 - categorical we use classification
25:34 - now the most used unsupervised learning
25:37 - models are clustering and Association
25:40 - analysis clustering is the most
25:43 - important unsupervised learning model it
25:45 - deals with finding a structure in a
25:47 - collection of unlabeled data so a loose
25:51 - definition of clustering could be the
25:53 - process of organizing objects into
25:56 - groups whose members are similar in some
25:58 - way I will present clustering in more
26:00 - detail later
26:03 - Association analysis models discover
26:06 - relationships in large datasets hidden
26:08 - data relationships will be expressed as
26:11 - a collection of Association rules and
26:14 - frequent itemsets with Association
26:17 - analysis Association analysis isn't
26:20 - frequently used we will not focus on
26:22 - them in this course
26:25 - what are the machine learning models
26:27 - associated with each of these learning
26:29 - types I mean which models are dedicated
26:32 - to regression classification and
26:34 - clustering
26:36 - for the regression purpose we commonly
26:39 - use linear regression decision trees for
26:42 - regression support vector machines for
26:45 - regression SVR or neural networks for
26:48 - the classification purpose we use
26:51 - logistic regression decision trees for
26:54 - classification support vector machines
26:58 - classifier SVC nearest neighbor or
27:01 - neural networks in unsupervised learning
27:05 - we use the k-means clustering
27:09 - so before we discuss these models in
27:12 - detail I need to explain the difference
27:15 - between two terms which can be confusing
27:17 - when you get started in machine learning
27:19 - the terms model parameter and model
27:23 - hyper parameter I'll talk more about
27:25 - these two terms in a few minutes so I
27:28 - think it will be better to explain the
27:30 - difference between these two terms now
27:38 - a model parameter is a configuration
27:40 - variable that is internal to the model
27:43 - and whose value can be estimated from
27:46 - data it's required by the model when
27:48 - making predictions and it is estimated
27:51 - or learned from data model parameters
27:55 - are key to machine learning algorithms
27:57 - they are the part of the model that is
27:59 - learned from historical training data so
28:02 - some examples of model parameters
28:04 - include the weights in an artificial
28:08 - neural network the coefficients in a
28:10 - linear regression or logistic regression
28:17 - on the other side a model hyper
28:19 - parameter is a configuration that is
28:22 - external to the model and whose value
28:24 - can't be estimated from data they are
28:27 - often only help estimate the model
28:29 - parameters and they are often specified
28:31 - by the practitioner
28:35 - [Music]
28:40 - linear regression is one of the most
28:42 - used supervised machine learning
28:43 - algorithms and now I'm gonna explain
28:46 - what it is exactly how it works and show
28:48 - you the logic behind it first of all
28:50 - what is linear regression let's say that
28:54 - two years ago I made $10,000 on my job
28:57 - and that last year I earned 20,000 with
29:00 - that in mind what do you think how much
29:02 - would I make this year
29:04 - if you answered $30,000 you just applied
29:07 - linear regression we make predictions
29:10 - all the time and most of them follow the
29:12 - logic of linear regression linear
29:16 - regression is a machine learning model
29:17 - which was designed to help you to
29:19 - specify a linear relationship to predict
29:22 - the numerical value of a dependent
29:24 - variable we will call it Y in this
29:26 - course for a given value of independent
29:29 - variables we will call them X by using a
29:32 - straight line called the regression line
29:34 - does that sound complicated it's not
29:37 - I'll show you
29:38 - we can say that linear regression will
29:41 - help us make a prediction based on some
29:43 - information prediction equals dependent
29:47 - variable Y some information equals
29:50 - independent variables X we use linear
29:54 - regression to answer the following
29:56 - questions is there a linear relationship
29:58 - between the two variables x and y which
30:02 - X variable contributes the most let's
30:06 - write this model as an equation it goes
30:07 - like this y equals B 0 plus B X plus e
30:15 - b0 is the value of y even if the value
30:19 - of x is 0 it's called the intercept B is
30:23 - a coefficient associated to X it will be
30:26 - a vector of coefficients e for error
30:29 - denotes all remaining information about
30:31 - why that hasn't been explained by the X
30:34 - variables of course the linear model is
30:37 - not perfect and it will not predict all
30:39 - the data accurately
30:42 - okay we have two types of linear
30:44 - regression the simple linear regression
30:46 - and the multiple linear regression in
30:49 - simple linear regression we use a single
30:52 - independent variable to predict the
30:54 - value of a dependent variable the model
30:57 - in this case will be y equals B 0 plus B
31:03 - 1 X 1 plus e a multiple linear
31:09 - regression we use two or more
31:11 - independent variables to predict the
31:13 - value of a dependent variable the
31:15 - difference between the two is the number
31:17 - of independent variables X y equals B 0
31:22 - plus B 1 X 1 plus B 2 X 2 plus dot dot
31:30 - dot plus E
31:34 - to keep this lesson simple and to help
31:36 - you understand the rest of the course
31:37 - right now we will focus on the simple
31:39 - linear regression only the same thing
31:41 - will be replicated in multiple linear
31:43 - regression the only thing that will
31:45 - change is the number of X variables here
31:49 - are some examples of simple linear
31:51 - regression and multiple linear problems
31:54 - the estimation of the average student
31:56 - score based on the number of hours they
31:58 - have spent studying 30 hours please note
32:01 - that all the students who pass the exam
32:03 - will receive at least two points for
32:05 - their presence in this problem the
32:08 - dependent variable Y will be the average
32:10 - student score the independent variable
32:13 - is the hours of study and it equals 30
32:16 - the intercept equals B 0 equals 1 the
32:22 - regression model we want for the score
32:24 - prediction is score equals B 0 plus B 1
32:29 - times our study score equals B 0 plus B
32:35 - 1 times 30 in this model the b0 and b1
32:41 - are coefficients these coefficients are
32:44 - what we need in order to make
32:45 - predictions about our score if we add
32:47 - the number of exercises that the student
32:50 - has completed to the model it will
32:52 - become a multiple linear regression
32:54 - model score equals b0 plus b1 times
32:59 - our study plus b2 times exercises and B
33:04 - plus
33:06 - for the prediction of the score we need
33:09 - to estimate three parameters B 0 B 1 and
33:12 - B 2 B 0 B 1 and B 2 parameters need to
33:17 - be estimated based on our historical
33:19 - data
33:21 - to estimate the linear regression
33:23 - coefficient we need to minimize the
33:25 - least squares or the sum of individual
33:28 - squared errors in other words that's the
33:31 - difference between the actual value and
33:33 - the prediction the error of the
33:36 - individual I is easily calculated as the
33:38 - difference between the real value of y I
33:41 - equals y hat i AI equals y I minus y hat
33:47 - I we square the error for two reasons
33:50 - one the prediction can be either above
33:53 - or below the true value resulting in a
33:55 - negative or positive difference
33:56 - respectively if we did not square the
33:59 - errors the sum of errors could decrease
34:01 - because of negative differences and not
34:03 - because the model is a good fit
34:06 - to squaring the errors penalize as large
34:09 - differences and so the minimizing the
34:11 - squared errors guarantees a better model
34:13 - let's look at a graph to understand it
34:15 - better in the graph the green dots
34:18 - represent the true data and the yellow
34:20 - line is a linear model the dotted red
34:23 - lines illustrate the errors between the
34:25 - predicted and the true values in
34:27 - practice we use the OLS algorithm it's
34:30 - ordinary least squares eater ative Li in
34:34 - each iteration the algorithm calculates
34:36 - the sum of the individual squared errors
34:39 - and in the next iteration the algorithm
34:42 - updates model parameters to shift the
34:44 - line from the previous position to
34:46 - reduce the squared error finally the
34:49 - best OLS estimators of the coefficients
34:52 - are
34:53 - in these equations X bar and y bar
34:56 - represent the mean
34:58 - now how to make a prediction we will use
35:01 - the existing data to estimate the values
35:03 - of B 0 B 1 through B K we can do that in
35:09 - Excel our Python etc after that we can
35:12 - make all the predictions we want
35:14 - please note that linear regression is
35:17 - considered to be one of the most
35:18 - straightforward machine learning
35:20 - techniques and an easy model to
35:21 - interpret but it has its disadvantages
35:24 - it will only work if the relationship
35:27 - between the dependent and independent
35:28 - variables is linear that's it now you
35:31 - know exactly what linear regression is
35:33 - and how it works now present the
35:36 - decision trees model a decision tree is
35:39 - like a series of if-else conditions that
35:42 - lead to a decision it can be used for
35:44 - classification or regression use cases
35:47 - it works for both categorical and
35:49 - continuous input and output variables
35:53 - decision tree breaks down a data set
35:55 - into smaller and smaller subsets while
35:58 - at the same time an Associated decision
36:01 - tree is incrementally developed the
36:04 - final result is a tree with decision
36:07 - nodes so consider a scenario where we
36:11 - want to decide whether a loan should be
36:13 - approved for an applicant or not to
36:16 - decide we will ask the applicant a
36:18 - series of questions we might start off
36:20 - with whether the applicant has any other
36:22 - existing loans if the answer is yes then
36:25 - the next question might be whether he is
36:27 - a defaulter or not with this kind of
36:30 - series of questions we can narrow down
36:32 - the search and make a robust decision we
36:35 - build the model above by hand to make a
36:38 - decision about whether a loan
36:39 - application should be approved or not
36:42 - alternatively a machine learning model
36:44 - could perform supervised learning using
36:47 - the data set to arrive at a decision the
36:49 - decision tree model in machine learning
36:52 - can learn these decisions or conditions
36:54 - from the data set quickly and build the
36:56 - model
36:58 - okay decision trees have three types of
37:01 - nodes
37:03 - a decision node has two or more branches
37:07 - a leaf node represents a classification
37:09 - or decision
37:12 - we also have the topmost decision node
37:15 - in a tree which corresponds to the best
37:17 - predictor called root node
37:22 - how do decision trees work in practice
37:24 - well that's an interesting question in
37:27 - practice there are three important steps
37:29 - in the building of a decision tree
37:32 - the first one is splitting a decision
37:36 - tree can have a sub-branch or a subtree
37:40 - as well and the process of creating a
37:42 - sub branch is called splitting have a
37:45 - look at the figure to visualize these
37:47 - notations
37:51 - so the root node has the full data set
37:54 - and at each decision though the test is
37:57 - conducted to split the data set decision
38:01 - nodes are executed to split the data
38:02 - until it reaches the leaf node a leaf
38:06 - node contains a single target value a
38:08 - single class or a single regression
38:11 - value a leaf node that contains only one
38:14 - target value is considered pure
38:18 - the second one is pruning or shortening
38:22 - of branches of the tree
38:25 - decision trees are prone to overfitting
38:27 - if the parameters are set to favor all
38:30 - pure leaf nodes which means that all
38:32 - data points in the training data set are
38:35 - correctly classified so to reduce
38:37 - overfitting the following strategies are
38:40 - commonly followed one pre pruning
38:44 - stopping the creation of the tree and
38:46 - this is achieved by limiting the maximum
38:48 - depth of the tree is so limiting the
38:52 - maximum number of leaf nodes or defining
38:54 - the minimum number of points required to
38:57 - split it further to post pruning just
39:01 - trimming the nodes that contain less
39:03 - information
39:05 - as you can see a pruned tree has less
39:08 - nodes and has less sparsity than a nun
39:11 - pruned decision tree
39:16 - then the last one is the tree selection
39:19 - in this step the models are looking for
39:22 - the smallest tree that fits the data
39:25 - usually this is the tree that yields the
39:27 - lowest cross validated error decision
39:32 - tree models work really well if the
39:34 - training data set is in a binary format
39:36 - but this is not a limitation for
39:39 - continuous features the decision
39:42 - condition can be applied in the form of
39:44 - greater than or less than a certain
39:46 - threshold for example X is greater than
39:50 - 0.7 a prediction on a new data point is
39:54 - made by traversing through the decision
39:57 - tree and checking at each decision node
39:59 - whether the condition is met
40:01 - or not
40:04 - one of the key factors in the decision
40:07 - tree is entropy so let me explain to you
40:10 - why
40:12 - it's important to carefully consider
40:14 - which feature will be used to split each
40:16 - node because decision trees with a
40:18 - different split node may result in
40:20 - different predictions and accuracy we
40:24 - can utilize a statistical method to
40:26 - identify the feature that should be
40:27 - selected as the root node the feature
40:30 - that has the most information gain
40:32 - should be selected as the root node
40:36 - so information gain measures how well a
40:38 - certain feature distinguishes among
40:40 - different target classifications
40:43 - information gain is measured in terms of
40:45 - the expected reduction in the entropy or
40:48 - impurity of the data the entropy of a
40:51 - set of probabilities is H of P in the
40:55 - formula where P is the probability of
40:59 - outcome event so if the sample is
41:02 - completely homogeneous the entropy is 0
41:05 - and if the sample is an equally divided
41:08 - one it has an entropy of 1
41:13 - to understand how entropy and
41:14 - information gain is calculated let's
41:16 - take a look at the following example
41:18 - okay a training data set has 500
41:21 - observations of these 500 observations
41:24 - 300 are of positive class and the
41:28 - remaining 200 are of negative class
41:31 - positive class ratio equals 300 divided
41:35 - by 500 equals zero point 6 negative
41:39 - class ratio equals 200 divided by 500
41:44 - equals 0.4
41:48 - entropy of the target variable will be
41:52 - entropy equals minus 0.6 times log to
41:59 - 0.6 plus 0.4 times log to 0.4 equals 0.9
42:09 - 7:02
42:11 - a feature X in the dataset is split as X
42:15 - is greater than 347 120 positive and 80
42:20 - negative X is less than or equal to 347
42:25 - 240 positive and 60 negative
42:29 - entropy of X is greater than three
42:31 - hundred and forty seven equals E 1
42:34 - equals negative 120 divided by 200 log
42:40 - two 120 divided by 200 minus 80 divided
42:46 - by 200 log two eighty divided by two
42:49 - hundred and ruvi of X less than or equal
42:54 - to three hundred and forty-seven equals
42:56 - e 2 equals negative 240 divided by 300
43:01 - log two 240 divided by 300 minus 60
43:07 - divided by 300 log 260 divided by 300
43:11 - entropy of x equals two hundred divided
43:14 - by 500 times entropy one plus 300
43:19 - divided by 500 times entropy two
43:23 - information gained for feature X equals
43:25 - entropy total minus entropy of X
43:30 - whichever feature in the note has the
43:32 - maximum information gain will be
43:34 - selected for the split we can also use
43:37 - other indicators like the misc
43:38 - classification rate the Gini index and
43:42 - Inter of Dakota miser 393 for
43:45 - calculating the information gain of a
43:47 - feature what are the hyper parameters of
43:52 - this model free routing parameters are
43:55 - the main hyper parameters of a decision
43:58 - tree model for example the maximum depth
44:01 - of the tree the maximum number of leaf
44:02 - nodes and minimum number of data points
44:04 - required to split the node further a
44:07 - combination of these hyper parameters
44:09 - can be used to build the decision tree
44:11 - which is generalized over the data set
44:14 - and provides good accuracy
44:16 - what are the advantages and the
44:18 - disadvantages of a decision tree
44:21 - decision trees are computationally cheap
44:24 - to use easy for humans to understand
44:27 - results and it can deal with irrelevant
44:29 - features its disadvantages are it is
44:33 - prone to overfitting and provides poor
44:36 - generalization performance sometimes it
44:39 - gives low prediction accuracy
44:42 - training in post pruning strategies are
44:45 - implemented in decision trees to reduce
44:47 - overfitting but still decision tree
44:50 - models tend to overfit so to overcome
44:54 - this data scientists use an advanced
44:56 - modeling technique called ensemble the
44:59 - idea of ensemble is to build many trees
45:01 - all of which predict well and overfit in
45:04 - their own way and average the results to
45:07 - reduce overfitting ensemble means
45:10 - assembling many machine learning models
45:12 - to create a more powerful and robust
45:14 - model these machine learning models also
45:16 - known as base estimators or base learner
45:19 - because they are combined together
45:23 - the most popular ensemble techniques are
45:26 - bagging and random forest bagging
45:29 - combines sampling techniques and
45:31 - aggregation to form an ensemble model
45:34 - and practice multiple samples are chosen
45:38 - randomly with replacement within the
45:40 - training data set let me explain how the
45:43 - sampling is made so suppose that a
45:45 - sample of 10 observations is drawn from
45:48 - a training data set of 100 observations
45:51 - these observations are then returned to
45:53 - the training data set before another
45:55 - sample is drawn so the next sample of 10
45:59 - observations is drawn from a training
46:01 - data set of 100 observations in simple
46:04 - terms at any point all of the training
46:07 - data set observations will be available
46:09 - for a sample to be drawn this sampling
46:12 - technique is called the bootstrap
46:16 - for each of these samples a decision
46:19 - tree or other bass learners is created
46:22 - finally these decision trees or based
46:25 - learners are aggregated to achieve an
46:27 - efficient predictor typically the
46:29 - combined estimator is better than any
46:32 - one of the single decision trees
46:36 - question how does the algorithm choose
46:40 - the output at the end of this
46:41 - aggregation
46:43 - okay the output is chosen based on
46:46 - voting for classification or on an
46:49 - averaging for regression
46:54 - now how about the random forest model
46:59 - so you learned how bagging works random
47:02 - forests also uses the same bagging
47:05 - technique with a slight modification
47:07 - during bagging all features of the
47:10 - training data set are used on sampled
47:13 - data to create the decision trees or the
47:15 - base estimators because of this sampling
47:18 - techniques used in bagging the datasets
47:21 - in each sample are quite similar HBase
47:24 - estimator usually breaks at the same
47:26 - feature this results in quite similar
47:28 - base estimators
47:29 - this means that weak features will not
47:32 - be incorporated to avoid this in random
47:35 - forest model the samples are created
47:38 - with a subset of features selected
47:40 - randomly for each node in the decision
47:43 - tree this selection of a subset of
47:46 - features is repeated separately in each
47:49 - node so that each node in a tree can
47:52 - make a decision using a different subset
47:54 - of features this process of randomly
47:57 - selecting sampled data and a number of
48:00 - features for a split at each node
48:02 - ensures that all decision trees in the
48:05 - random forest are different
48:08 - similar to the decision tree the random
48:11 - forest also provides feature importance
48:14 - which is computed by aggregating feature
48:16 - importance over the trees in the forest
48:19 - typically the feature importance
48:22 - provided by random forests is more
48:24 - reliable than the one provided by a
48:26 - single tree
48:28 - the maximum number of features to split
48:30 - at each node determines how random each
48:33 - tree is and a smaller value reduces
48:36 - overfitting
48:39 - as a rule of thumb this parameter can be
48:42 - set to the square root of a number of
48:44 - features for classification and for
48:46 - regression use cases
48:50 - Criterion Gini entropy number of
48:54 - decision trees maximum number of
48:56 - features maximum depth of the decision
48:59 - tree minimum number of samples required
49:01 - at each leaf node minimum number of
49:04 - samples required to split a node and
49:06 - maximum number of leaf nodes in each
49:08 - decision tree are all hyper parameters
49:11 - of ensemble models this hyper parameter
49:15 - can be tuned to get a generalized and
49:17 - accurate machine learning model
49:21 - benefits of random forest okay what I
49:25 - can say is that ensemble models are very
49:28 - powerful and often work without
49:30 - parameter tuning but the bad news is it
49:33 - is difficult to understand thousands of
49:35 - trees and explain the decision-making
49:37 - process that is why this model is
49:40 - considered like a black box another
49:43 - disadvantage is that ensemble methods
49:46 - need more computing resources and take
49:48 - more time to learn from data ok they're
49:51 - machine learning model is boosting
49:54 - boosting is another model that makes a
49:56 - bass estimator like decision tree more
49:59 - powerful by making a sequential
50:01 - execution and each subsequent estimator
50:04 - focuses on the weakness of the previous
50:07 - estimator boosting incrementally builds
50:10 - an ensemble by training each model with
50:12 - the same data set but where the model
50:15 - coefficients of estimators are adjusted
50:17 - according to the error of the last
50:20 - prediction several weak models team up
50:23 - to produce a powerful ensemble model the
50:26 - main idea of boosting is to focus on the
50:29 - observations that are hard to predict
50:30 - boosting can reduce bias without
50:32 - incurring higher variance
50:36 - here are the popular boosting algorithms
50:38 - Aida boost and gradient boosting let me
50:42 - explain to you how they work
50:43 - Aida boost is adaptive boosting where
50:46 - more attention is given to the records
50:48 - that are not correctly predicted after
50:50 - each iteration weights of the wrongly
50:53 - predicted observations are increased so
50:56 - that these records will be picked up
50:58 - more in the next iteration to gain
51:00 - better accuracy
51:02 - gradient boosting is another popular
51:05 - boosting algorithm it works by
51:06 - sequentially adding the previous
51:08 - predictors under fitted predictions to
51:11 - the ensemble ensuring errors made
51:14 - previously are corrected
51:17 - here's what you need to know boosting
51:20 - does not introduce randomness to the
51:22 - decision trees or any based learners
51:25 - however it uses strong techniques to
51:28 - build accurate predictors in most cases
51:31 - the maximum depth for boosting models
51:33 - has kept a 5 models this makes the model
51:36 - faster and the model consumes less
51:38 - memory the hyper parameters for these
51:41 - models are the number of decision trees
51:44 - maximum depth and learning rate
51:49 - a lowered learning rate means more trees
51:51 - are required and a higher learning rate
51:53 - means less trees are required to build a
51:55 - model these hyper parameters should be
51:58 - tuned to get an optimized machine
52:00 - learning model boosting models are more
52:04 - sensitive to hyper parameters but once
52:06 - the hyper parameters are tuned properly
52:08 - these models provide very good accuracy
52:11 - and generalization
52:14 - so in general ensemble models are very
52:17 - powerful and widely used however like
52:20 - the bagging models it is difficult to
52:22 - understand thousands of trees and
52:24 - explain the decision-making process I
52:26 - just don't recommend it for dimensional
52:28 - sparse data they don't do very well in
52:31 - high dimensional data
52:33 - well the SVM's model or support vector
52:36 - machines support vector machines are one
52:40 - of the supervised learning algorithms
52:42 - mostly used for classification tasks
52:44 - however SVM algorithms can be used for
52:47 - regression as well a support vector
52:50 - machine for classification is called the
52:52 - support vector classifier SVC and for
52:56 - regression it's called the support
52:57 - vector regressor or SVR svms are based
53:03 - on the idea of finding a hyperplane that
53:05 - best divides a data set into two classes
53:08 - as shown in the image below
53:11 - you may ask what the hyperplane is okay
53:15 - take this example say we want to
53:17 - classify a task with only two features
53:19 - you can think of a hyperplane as a line
53:22 - that linearly separates and classifies
53:25 - our data into intuitively the further
53:29 - from the hyperplane our of data points
53:31 - lie the more confident we are that they
53:34 - have been correctly classified we
53:36 - therefore want our data points to be as
53:38 - far away from the hyperplane as possible
53:40 - while still being on the correct side of
53:42 - it so when new testing data is added
53:46 - whatever side of the hyperplane it lands
53:49 - on will decide the class that we will
53:51 - assign to it now how do we find the
53:54 - right hyperplane or in other words how
53:57 - do we best segregate the two classes
53:58 - within the data before we answer this
54:01 - question we need to understand what is a
54:03 - margin a margin is equal to the distance
54:07 - between the hyperplane and the nearest
54:09 - data point from either set now the goal
54:13 - is to choose a hyperplane with the
54:15 - greatest possible margin between the
54:17 - hyperplane and any point within the
54:19 - training set and giving a greater chance
54:21 - of new data being classified correctly
54:23 - as you can see in two dimensions it is
54:27 - very easy to classify using SVM but
54:30 - sometimes it's harder to identify
54:32 - clearly the hyperplane because the data
54:35 - is rarely ever as clean as our simple
54:37 - example above a data set will often look
54:41 - more like the jumbled balls which
54:43 - represent a linearly non-separable data
54:46 - set look at this case
54:50 - in order to classify a data set like
54:52 - this one it's necessary to move away
54:54 - from a two dimension view of the data to
54:57 - a three dimension view explaining this
55:00 - is easiest with another simplified
55:02 - example imagine that our two sets of
55:05 - colored balls above are sitting on a
55:07 - sheet and this sheet is lifted suddenly
55:09 - launching the balls into the air while
55:12 - the balls are up in the air you use the
55:14 - sheet to separate them this lifting of
55:18 - the balls represents the mapping of data
55:20 - into a higher dimension this is also
55:23 - known as kernel
55:26 - okay because we are now in three
55:28 - dimensions our hyperplane can no longer
55:30 - be a line it must now be a plane as
55:32 - shown in the example the idea is that
55:35 - the data will continue to be mapped into
55:38 - higher and higher dimensions until a
55:40 - hyperplane can be formed to segregate it
55:43 - so that's how SVM's work and produce the
55:46 - output
55:49 - so what are the benefits and the
55:51 - disadvantages of SVM's SVM's can give a
55:55 - great accuracy and can work well on
55:57 - smaller cleaner datasets nevertheless if
56:00 - you have a larger data set it isn't
56:02 - suited as the training time can be very
56:04 - high
56:09 - another machine learning algorithm is
56:11 - the K nearest neighbors K and n the K
56:16 - nearest neighbors knn algorithm belongs
56:18 - to the family of instance-based
56:21 - competitive learning and lazy learning
56:24 - algorithms
56:26 - it's a lazy learning algorithm because
56:29 - the calculation is delayed until a
56:31 - prediction is required it is called the
56:33 - localized model because only the data
56:36 - points that are near new data points are
56:38 - used for model calculation and for
56:40 - predicting classes of new data points so
56:43 - let me explain how this works what a
56:45 - prediction is required for an unseen
56:48 - data point the knn algorithm will search
56:50 - through the training data set for the k
56:53 - most similar neighbor the prediction
56:57 - attribute of the most similar data
56:59 - points is summarized and returned as the
57:01 - prediction for the unseen instance
57:05 - so in this model K is the number of
57:07 - neighbors we want to check to classify a
57:10 - new data point if K is greater than one
57:12 - the model uses voting to classify the
57:15 - new data point in short the class that
57:18 - is in the majority is assigned to the
57:20 - new data point for example okay so if
57:22 - the value of K is set to three the model
57:26 - will check the three nearest data points
57:27 - to classify the new data point the
57:29 - default value of K is one which means
57:32 - that the vanilla KNN model classifies
57:35 - the new data point according to the
57:37 - class of the nearest neighbor
57:40 - the yellow and purple circles represent
57:42 - the data points from the training data
57:44 - set and we want to predict the class for
57:46 - the red data point if the value of K is
57:50 - set to three then the model will check
57:53 - the three nearest data points inner
57:55 - circle and then classify the red data
57:58 - point if the value of K is set to six
58:01 - then the model will check for the
58:02 - nearest six data points outer circle and
58:05 - then classify the red data point
58:08 - in the figure above the red point will
58:10 - be classified as follows if K equals
58:13 - three Class B two votes for Class B and
58:18 - one vote for Class A if K equals six
58:21 - Class A two votes for Class B and four
58:25 - votes for Class A
58:28 - okay in a multi-class data set we count
58:31 - how many data points belong to each
58:33 - class and the class that is in the
58:35 - majority is predicted for the new data
58:37 - point so how does the algorithm choose
58:40 - the nearest points
58:43 - the KNN uses the distance to evaluate
58:46 - which point is near to the new data for
58:49 - continuous features Euclidean distance
58:51 - is calculated and for categorical
58:53 - features another distance called hamming
58:55 - distance is calculated
59:00 - the important hyper parameter for KN n
59:02 - is the number of neighbors it holds the
59:05 - value of K the default value of this
59:07 - parameter is 5
59:13 - k-nearest neighbor has a lot of benefits
59:15 - among them its simplicity and
59:17 - flexibility it also works well with
59:20 - enough representative data
59:23 - the problem that we can face sometimes
59:25 - is that the can and algorithm is space
59:28 - consuming because for each prediction
59:30 - the calculation is done separately
59:35 - okay first of all logistic regression is
59:38 - among the most commonly used and best
59:40 - known algorithms that we can use to
59:41 - solve a classification problem it's
59:44 - named the logistic regression because of
59:46 - the logit function
59:49 - which is used in this method of
59:51 - classification other than that
59:52 - logistic regression is pretty much the
59:54 - same as linear regression the purpose of
59:58 - logistic regression is to detect a
60:00 - relationship between features and find
60:03 - the probability of a particular outcome
60:04 - in a way it extends the idea of linear
60:08 - regression to a situation where the
60:10 - outcome variable is categorical
60:13 - for example let's try to predict whether
60:16 - a student will pass or fail an exam the
60:18 - number of hours spent studying is given
60:20 - as a feature and the response variable
60:23 - has two values passed and failed okay in
60:27 - the equation below you can see that we
60:29 - need to predict the Y variable which can
60:33 - take two values 0 or 1 0 for failed and
60:36 - one for passed it turns out that it is
60:39 - virtually impossible to predict Y with
60:43 - the following model y equals B 0 plus B
60:46 - 1 X 1 plus dot dot B K X K that's
60:52 - because Y is a categorical value and B 0
60:57 - plus B 1 X 1 plus dot B K XK will give a
61:02 - continual value as the result therefore
61:05 - instead of predicting this categorical
61:07 - variable we're going to predict the
61:09 - probability of the realization of y
61:12 - equals 1 B equals probability of y
61:15 - equals 1 in order to do that we need a
61:19 - link function the logit link function
61:21 - okay a link function is basically a
61:24 - function of the mean of the response
61:26 - variable Y that we use as the response
61:30 - instead of Y itself it means that when Y
61:32 - is categorical we use the logit of Y as
61:35 - the response in our regression equation
61:37 - instead of just Y the logit function is
61:40 - the natural log of the odds that y
61:43 - equals one of the categories for
61:45 - mathematical simplicity we're going to
61:47 - assume Y has only two categories encode
61:50 - them as 0 and 1 P is the probability
61:53 - that y equals 1 so for instance those
61:56 - X's could be specific hours spent
61:59 - studying number of completed exercises
62:02 - and the score in the first exams while P
62:05 - would be the probability that a student
62:07 - would pass an exam as in our first
62:09 - example linear regression versus
62:12 - logistic regression instead of linear
62:16 - regression the line between y and X the
62:19 - relationship between X and the
62:21 - probability P is a logistic distribution
62:27 - how to estimate the logistic regression
62:29 - coefficients at this point we don't know
62:32 - the coefficients B 0 B 1 B K of the
62:37 - model so we must estimate them in order
62:40 - to make predictions unlike the linear
62:42 - regression model logistic regression
62:44 - uses ordinary least square for parameter
62:47 - estimation the estimation is done by
62:49 - using maximum likelihood due to its more
62:53 - general nature and statistical features
62:55 - there can be an infinite set of
62:59 - regression coefficients the maximum of
63:02 - the log likelihood estimate is that set
63:05 - of regression coefficients for which the
63:07 - probability of getting the data we have
63:09 - observed is maximum in other terms we
63:13 - must make estimates for the coefficients
63:15 - that predictions are as close as
63:17 - possible to the originally observed
63:20 - value so how to make a prediction
63:23 - well the prediction is made using the
63:25 - original logistic function and the
63:27 - estimated coefficients from the maximum
63:30 - likelihood function with the observed
63:32 - data to compute the estimated
63:34 - probability of P if the probability of P
63:38 - is below 0.5 0 the predicted value of y
63:43 - is 0 otherwise it will be 1 in our
63:47 - example the student will fail based on
63:50 - the value of x in the figure we make a
63:53 - prediction by using a different set of X
63:55 - variables the first set gives us a red
63:57 - point with P equals 0.2 9 in the first
64:02 - case the value of y is equal to 0 as the
64:05 - predicted value of P is less than 0.5 0
64:09 - in the second case the green point P
64:12 - equals zero point 9 0 the predicted
64:15 - value of y will be 1 compared to other
64:19 - models logistic regression is rather
64:21 - simple and efficient however it can't
64:23 - handle a large number of categorical
64:25 - variables successfully that's it now you
64:29 - know exactly what logistic regression is
64:32 - and how it works in our next tutorial
64:34 - you'll learn how to apply it in Python
64:36 - and our click on the video above to see
64:39 - this tutorial
64:41 - thanks for your time and hey if you like
64:42 - our videos please like it share it and
64:45 - subscribe to our Channel oh and click on
64:47 - the notification button so you can
64:49 - receive notifications for our next
64:50 - course enjoy machine learning
64:54 - [Music]
65:00 - first though let's see what a neuron is
65:02 - a neuron in the neural networks field is
65:06 - something that takes some input applies
65:08 - some logic and outputs the result we
65:11 - call it a function for example if we
65:14 - have F X equals y X is the input and Y
65:20 - is the output and F is a function
65:24 - to illustrate so let's say I'm trying to
65:26 - understand the relationship between the
65:28 - length of the video we produce on our
65:30 - channel and the time that people
65:32 - actually spend watching the video we
65:34 - collect data from some of our videos I
65:36 - mean we have the video duration let's
65:38 - call it X and the watching time let's
65:41 - call it Y and we imagine there is some
65:44 - relationship between them denoted by F
65:47 - after that I inform the Machine about
65:50 - the relationship I expect to see between
65:52 - these two variables
65:53 - I can choose a linear function between X
65:56 - and Y or a nonlinear function this
65:59 - function is what we call a neuron then
66:03 - we can predict the time people would
66:06 - spend watching a video lesson precisely
66:08 - based on our neuron and the video
66:11 - duration
66:14 - now let's see what a neural network is
66:17 - well in one sentence a neural network is
66:20 - a network of neurons it means that we
66:23 - have many neurons and all their inputs
66:25 - and outputs are intertwined and they
66:27 - feed each other in this figure you can
66:31 - see the difference between a neuron and
66:33 - a neural network as you can see a neuron
66:36 - is a basic unit of learning and a neural
66:38 - network is a bunch of interconnected
66:40 - neurons
66:42 - neural networks help us cluster and
66:44 - classify they helped a group data
66:46 - according to similarities among the
66:49 - example inputs and they classify data
66:51 - when they have the output variable in
66:53 - the existing data set to learn from it
66:56 - the questions you may ask at this point
66:58 - will probably be
67:00 - question 1 what kind of problems do
67:03 - neural networks solve neural networks
67:06 - could be applied for spam filtering
67:08 - fraud detection customer relationship
67:11 - management angry customers or happy
67:13 - customers image recognition self-driving
67:15 - etc
67:17 - question two which functions will I use
67:20 - in each neuron we can use linear or
67:22 - nonlinear depending on the complexity of
67:25 - the problem
67:26 - question 3 what is the architecture of
67:29 - the network we have different types of
67:32 - neural networks a perceptron a recurrent
67:35 - neural network or RNN a convolutional
67:38 - neural network CNN etc
67:43 - now how we can run our neural network in
67:47 - the first place the neural network learn
67:49 - to recognize patterns just like a human
67:52 - we show them examples of correct inputs
67:55 - and outputs in the hope that when we
67:57 - give it a new example input that it's
68:00 - never seen before it will know how to
68:02 - give the correct output that's what we
68:04 - call training on existing data sets
68:07 - don't forget machine learning equals
68:10 - learning from examples
68:13 - let me present you the most basic neural
68:15 - network the perceptron and discuss how
68:19 - it processes inputs and produces an
68:21 - output so suppose we use our neural
68:23 - network for a froot image recognition I
68:26 - have two inputs for that purpose the
68:28 - color and the shape of some fruits and
68:31 - our data sets and a single binary output
68:33 - which is the fruit name
68:37 - once the machine has learned all these
68:39 - properties I can give it a new image of
68:41 - a fruit when it hasn't seen before and
68:43 - it will hopefully classify it correctly
68:46 - and be able to tell me whether it is an
68:48 - orange or a banana the perceptron learns
68:51 - from the existing data and knows which
68:53 - information will be most important in
68:55 - decision making to decide between
68:57 - multiple information it uses something
69:00 - called weights the weights are just
69:03 - numerical representations of these
69:05 - preferences a higher weight means our
69:07 - perceptron considers that input more
69:09 - important compared to other inputs so
69:13 - for our example let's deliberately set
69:15 - suitable weights for our two inputs two
69:18 - for the fruit shape and four for the
69:21 - fruit color
69:23 - now how does the perceptron calculate
69:26 - the output it simply multiplies the
69:28 - input with its respective weight and
69:31 - sums up all the values it gets from all
69:33 - the inputs let's consider that we have
69:36 - two shaped round and long if the shape
69:39 - is round the input one value is one and
69:42 - if it's not round the value is zero
69:45 - we'll repeat the same thing with the
69:46 - color red takes the value of one and the
69:49 - yellow color the value of zero based on
69:53 - this information if the fruit is round
69:54 - in red our perceptron would do the
69:57 - following calculation total equals round
70:01 - times shape weight plus red times color
70:05 - weight so total equals one times two
70:09 - plus one times four equals six
70:14 - this calculation is known as a linear
70:16 - combination now let's see what this
70:18 - value 6 means we first needed to find
70:21 - the threshold value because the
70:24 - perceptrons output is either 0 or 1 0
70:27 - for a banana and 1 for an orange this
70:30 - output is determined like this if the
70:32 - value of the linear combination is
70:34 - higher than the threshold value then the
70:37 - output is 1 and if it is not the output
70:40 - is 0 so let's say the threshold value is
70:43 - 3 which means that if the calculation
70:45 - gives you a number less than 3 we have a
70:48 - banana but if it's equal to or more than
70:52 - 3 then we have an orange
70:56 - that's how perceptron works it uses a
70:59 - linear combination and produces the
71:01 - output in reality we set the weights to
71:04 - random values and then the network
71:06 - adjusts those weights based on the
71:08 - output errors it made using the previous
71:11 - weights that is called training the
71:13 - neural network
71:15 - in the mathematical language the
71:17 - perceptron algorithms work like this the
71:21 - output is equal to zero if the sum of
71:23 - the weight times the value of the
71:24 - variable is smaller than a threshold in
71:27 - the same way the output will be equal to
71:29 - one if the sum of the weight times the
71:32 - value of the variables is bigger than a
71:35 - threshold
71:39 - to make things just a little simpler for
71:41 - training the threshold is sometimes move
71:44 - to the other side of the inequality and
71:46 - replaced with what's known as the
71:48 - neurons bias now with bias we only need
71:53 - to make changes to the left side of the
71:55 - equation while the right side can remain
71:57 - constant at zero the left side of the
72:00 - equation is a function it is a function
72:02 - that transforms the values or states the
72:05 - conditions for the decision of the
72:07 - output neuron it is known as an
72:09 - activation function
72:12 - the formula above is just one of several
72:14 - activation functions and and the
72:16 - simplest one used in deep learning and
72:19 - it is called the Heaviside step function
72:24 - in reality we can also use other
72:26 - activation functions for example we can
72:29 - use the sigmoid function the tan
72:31 - function and the softmax functions each
72:35 - of them has a purpose and we'll present
72:36 - each of these functions in a special
72:38 - video dedicated just to that
72:41 - that's it for our first introductory
72:43 - video on neural network now you have a
72:46 - solid understanding of the basics of
72:48 - neural networks and how perception works
72:50 - in our next video in neural networks we
72:54 - will explain how they multi layers
72:56 - perceptron works and we will present the
72:58 - concept of hidden layers
73:01 - okay let's present an unsupervised
73:04 - machine learning model the clustering in
73:07 - general and the k-means clustering in
73:09 - particular clustering models are used to
73:12 - analyze unlabeled data and get useful
73:15 - insights from it clustering is equal to
73:18 - grouping things means the data points
73:20 - which are similar in some way and
73:22 - different from other data points are
73:24 - grouped together each group of the data
73:26 - points is known as a cluster
73:30 - in the example data points are divided
73:33 - into four clusters based on the
73:35 - geometrical distance between two data
73:37 - points the data points which are close
73:40 - to each other are assigned to one
73:42 - cluster this approach of creating
73:44 - clusters is known as distance based
73:47 - clustering another approach of creating
73:50 - clusters is the conceptual clustering in
73:54 - this approach clusters are based on
73:56 - conceptual similarity so for example in
73:59 - a data set of oranges and apples two
74:02 - clusters will be created one for all
74:05 - apples and another one for all oranges
74:07 - this approach uses properties of data
74:10 - points to distinguish one data point
74:12 - from another and to group data points
74:15 - with similar properties together
74:17 - clustering is a very subjective area of
74:21 - discussion it is difficult to determine
74:23 - how one clustering is better than the
74:26 - other one
74:27 - machine learning algorithms learn the
74:30 - insights from the data and apply these
74:32 - insights to create clusters since we
74:35 - don't know those insights it is hard to
74:38 - say the clustering done by the machine
74:39 - learning model is best or not so the
74:43 - user must provide an appropriate
74:45 - criterion to get suitable clusters as an
74:48 - output from the machine learning model
74:51 - consider a data set that contains green
74:54 - color balls red color balls green color
74:57 - apples and red color apples now if we
75:01 - provide color as the clustering criteria
75:04 - to the machine learning model then it
75:06 - will create two clusters one of the
75:08 - green color balls and green color apples
75:11 - and the second one of course of red
75:13 - color balls and red color apples however
75:16 - if the criteria provided is edible then
75:21 - the machine learning model will create
75:22 - two clusters one of all apples and
75:25 - another one of all balls
75:30 - clustering algorithms can be applied in
75:32 - many industries marketing like finding
75:35 - groups of customers with similar
75:37 - behavior or customer segmentation
75:39 - biology for plants classification
75:42 - insurance fraud detection city planning
75:46 - earthquake studies document
75:48 - classification identifying crime
75:50 - localities cyber profiling criminals etc
75:56 - now let me explain to you how K means
75:58 - clustering works k-means clustering is
76:02 - one of the most useful clustering
76:04 - algorithms it tries to find cluster
76:07 - centers that represent certain regions
76:10 - of the data k-means clustering is a
76:12 - two-step process one assign each data
76:16 - point to the closest data center to set
76:21 - the cluster center as the mean of the
76:23 - data points assigned to the cluster
76:26 - the algorithm keeps on executing these
76:29 - two steps until the assignment of data
76:31 - points two clusters no longer changes
76:34 - once we decide how many clusters are
76:36 - required the number of clusters is
76:39 - passed to the algorithm so let's say for
76:41 - a given data set we want three clusters
76:44 - in the initiation step the algorithm
76:47 - randomly selects three cluster centers
76:50 - each data point is assigned to the
76:53 - cluster Center it is closest to
76:56 - we then update the cluster Center with
76:58 - the mean of the cluster the previous two
77:01 - steps are repeated until the assignment
77:03 - of data points two clusters no longer
77:05 - changes
77:08 - clustering is similar to classification
77:10 - the only difference is that
77:13 - classification use cases have labeled
77:16 - data I mean the classes are defined and
77:19 - clustering creates various clusters or
77:22 - classes from the data set
77:27 - one of the major benefits of the k-means
77:29 - algorithm is the implementation k-means
77:33 - is a relatively efficient method however
77:36 - we need to specify the number of
77:38 - clusters in advance and the results are
77:41 - sensitive to initialization and often
77:44 - terminates at a local optimum
77:47 - unfortunately there's no global
77:49 - theoretical method to find the optimal
77:51 - number of clusters
77:53 - a practical approach is to just compare
77:56 - the outcomes of multiple runs with
77:58 - different K and choose the best one
78:00 - based on a predefined criteria in
78:03 - general a large K probably decreases the
78:06 - error but increases the risk of
78:09 - overfitting
78:12 - you
78:14 - in the previous part of our course we
78:16 - learned the basics of data science and
78:19 - machine learning and the most important
78:21 - machine learning models in practice we
78:24 - can apply manually or automatically
78:26 - various models on a given data set
78:29 - before finalizing and choosing the right
78:31 - model and the more efficient one
78:34 - but how do we evaluate the performance
78:36 - of a model to determine the one model
78:39 - that is performing better in this part
78:42 - of the course we will answer this
78:43 - question by presenting some performance
78:46 - indicators to evaluate the model
78:50 - let's start with the R squared R squared
78:55 - is the most common way to evaluate the
78:58 - performance of a linear model it is a
79:00 - statistical measure of determining how
79:02 - close the data is to the fitted
79:05 - regression line it is a proportion of
79:08 - explained variance to total variance
79:11 - r2 equals explained variance over total
79:16 - variance
79:18 - you can see how it's represented
79:20 - graphically here
79:23 - typically the r-squared value lies
79:26 - between zero and one
79:29 - zero means the model does not explain
79:31 - any variability of the data around it's
79:34 - me
79:36 - one means the model explains all the
79:39 - variability of the data around it's me
79:42 - so the higher the value of r-squared the
79:46 - better the model fits the data and
79:48 - better the model explains the variance
79:50 - of the observed data around its meat
79:54 - however it is difficult to tell whether
79:56 - an increase in the r-squared value is a
80:00 - result of better model performance or
80:02 - more features in the model
80:08 - that's why in multiple linear regression
80:11 - it is better to choose the adjusted
80:14 - r-square
80:16 - adjusted r-squared is a modified version
80:18 - of r-squared that has been adjusted for
80:21 - the number of features in the model
80:24 - adjusted r-squared penalize --is the
80:27 - r-squared if the choice of the feature
80:30 - newly added to model he's not good
80:36 - so in classification scenarios having
80:39 - good r-squared or adjusted r-squared
80:42 - values is not important until both the
80:45 - classes are unidentified with similar
80:48 - accuracy this becomes a big issue when
80:52 - the data is not equally distributed for
80:54 - the targeted classes okay for example
80:57 - consider a data set with 98% observation
81:02 - of Class A and 2% observation of Class B
81:06 - the model can easily get 98% training
81:09 - accuracy by simply predicting that every
81:12 - sample belongs to Class A but the
81:15 - prediction accuracy for Class B is very
81:17 - poor
81:19 - to overcome this situation model
81:22 - performance should be evaluated for each
81:24 - class and then aggregated to get the
81:27 - overall performance of the model
81:29 - confusion matrix provides the right
81:32 - tools to get the individual and overall
81:34 - performance of a classification model so
81:37 - let's take this example with two class
81:40 - positive and negative classes
81:44 - in the figure we have the classifier the
81:47 - prediction outcome and actual values the
81:50 - total number of positive classes is P
81:53 - and the total number of negative classes
81:56 - is n TP true positive in the top left
82:00 - corner of the figure equals actual
82:03 - classes positive and predicted class is
82:05 - also positive this states how many
82:08 - positive classes correctly predicted
82:10 - this is also known as the power of the
82:12 - model FN false negative in the top right
82:16 - corner of the figure actual class is
82:18 - positive and predicted class is negative
82:21 - this states how many positive classes
82:24 - are predicted as negative this is also
82:26 - known as a type 2 error miss
82:31 - FP falls positive in the bottom left
82:34 - corner in the figure actual classes
82:36 - negative and predicted class is positive
82:39 - this states how many negative classes
82:42 - are predicted as positive this is also
82:45 - known as a type 1 error false alarm
82:48 - TN true negative in the bottom right
82:52 - corner of the figure actual class is
82:54 - negative and predicted class is negative
82:56 - this states how many negative classes
82:59 - are correctly predicted
83:01 - so correctly predicted classes is equal
83:04 - to TP plus TN incorrectly predicted
83:08 - classes is equal to FP plus FN actual
83:13 - positive classes is equal to P equals TP
83:17 - plus FN actual negative classes is equal
83:22 - to N equals FP plus TN
83:28 - so we compute the proportion of records
83:31 - that are correctly classified called
83:33 - the accuracy of the classification model
83:36 - and it is calculated as follows
83:38 - accuracy equals T P plus TN over TP plus
83:45 - TN plus FP plus FN
83:52 - so another important validation
83:54 - technique is the cross validation cross
83:57 - validation is a tool that utilizes the
83:59 - training data set in a better way to
84:02 - reduce overfitting and underfitting it
84:04 - is a model validation technique for
84:07 - assessing how the results of statistical
84:10 - analysis will generalize to an
84:12 - independent data set the purpose of
84:15 - using cross-validation is to increase
84:18 - confidence in the model trained by the
84:20 - training data set
84:21 - so without cross validation our model
84:24 - may perform well on the training data
84:25 - set but their performance decreases when
84:28 - applied to the testing data set the
84:31 - testing data set is precious and should
84:33 - only be used once so the solution is to
84:36 - separate one small part of the training
84:38 - data set as a test of the trained model
84:41 - which is the validation data set
84:46 - okay here are two cross validation
84:48 - techniques k-fold cross-validation this
84:53 - involves splitting the training data set
84:55 - into K subsets of data also known as
84:58 - folds the machine learning model is
85:01 - trained on k1 subsets and then evaluated
85:04 - on the subset that was not used for
85:06 - training this process is repeated k
85:09 - times with a different subset reserved
85:12 - for evaluation and excluded from
85:14 - training each time so once the model has
85:17 - been executed for all training subsets
85:19 - the average of error in each run is
85:22 - calculated and represented as the
85:25 - cross-validation error
85:29 - leave one out cross validation this is
85:33 - another technique used for cross
85:34 - validation it's a logical extreme of
85:37 - k-fold cross-validation where K equals n
85:41 - the number of observations so for each
85:44 - run only one observation is left with a
85:47 - validation data set this approach leads
85:49 - to higher variation and testing model
85:52 - effectiveness because testing is done
85:54 - against one observation only hence the
85:57 - estimation is highly influenced by the
85:59 - validation observation if the validation
86:02 - observation is an outlier it can lead to
86:05 - higher variation
86:10 - at this last part of the course I will
86:13 - give you some best practices in data
86:15 - science and machine learning okay you
86:18 - need to be aware that the data set that
86:20 - we will get for machine learning
86:22 - problems is not always as clean as we
86:25 - can expect we might have to clean the
86:28 - data set and transform it into a data
86:30 - set that can be used to build a model
86:32 - for this purpose we might have to go
86:34 - through multiple processes like data
86:37 - processing feature engineering and
86:40 - feature extraction feature scaling and
86:43 - selection
86:47 - this set of processes is a major part of
86:50 - a machine learning project because the
86:52 - model performance is based on the data
86:54 - on which the model is trained garbage in
86:56 - garbage out we already talked a little
87:00 - bit about feature engineering and
87:02 - feature extraction
87:04 - I'm going to give you some best
87:05 - practices that we can follow to tackle
87:08 - the data sets and machine learning use
87:09 - cases
87:12 - another important concept is one hot
87:15 - encoding
87:17 - machine learning cannot handle
87:19 - non-numeric features by themselves so
87:21 - the question that you may ask is how do
87:23 - we transform the non numeric features
87:26 - let's take our example in the employee
87:29 - salary suppose that in the data set
87:31 - gender is maintained as a non numeric
87:34 - feature and this feature can have two
87:36 - values male or female for an
87:39 - organization and employees gender is
87:41 - meaningful information since it is a non
87:43 - numeric feature if we try to train a
87:46 - model on the gender feature then the
87:48 - model will not be able to interpret
87:50 - anything meaningful from it to make the
87:53 - gender column meaningful to the model we
87:55 - must transform it into a numeric column
87:58 - one hot encoding is one of the
88:01 - transformation techniques that can be
88:03 - used to transform categorical features
88:05 - into numeric categorical features so if
88:09 - the feature has only two categories then
88:12 - those two categories can be replaced by
88:14 - 0 & 1 in our employee data set example
88:18 - we can replace male with 0 and female
88:20 - with 1
88:23 - now consider a scenario where the
88:25 - categorical column has more than two
88:27 - categories in other words in our data
88:29 - set
88:30 - gender have three values male female and
88:34 - unknown so what hot encoding can be
88:38 - extended to take care of this multi
88:40 - category features it creates a new
88:42 - feature for each category and for each
88:45 - observation the value one will be
88:48 - assigned to the newly created feature to
88:50 - which the category belongs to
88:54 - other newly-created features will be set
88:56 - to zero for example in our employee
88:58 - gender example three new features will
89:01 - be created like in the table
89:05 - binning in a few machine learning
89:08 - scenarios continuous features cannot be
89:10 - used directly to train a model these
89:12 - features should be converted into
89:14 - categorical features and then one hot
89:18 - encoding should be applied to make these
89:19 - features important for the machine
89:22 - learning model and our employee data set
89:24 - example employee age ranges from 21 to
89:28 - 60 years these employees can be
89:31 - categorized into four age brackets 21 to
89:34 - 30 31 to 40 41 to 50 and 51 to 60 the
89:40 - technique of converting a continuous
89:42 - feature into multiple bins and creating
89:45 - a new feature from it is known as
89:47 - binning or bucket ization it's also
89:50 - known as quantization binning transforms
89:53 - a continuous feature into a categorical
89:56 - feature and categorical feature
89:58 - engineering might need to be performed
90:01 - before using this feature in modeling we
90:05 - can create a new age group feature and
90:07 - map each employee with one of these
90:09 - brackets for bidding we can use the
90:12 - domain expertise as well as a few
90:15 - statistical methods to correctly
90:17 - determine the number of Bin's
90:18 - / buckets and the boundaries for each
90:21 - bin so some of the common methods of
90:24 - bidding are one fixed width binning in
90:27 - this technique width is decided for each
90:30 - bin based on domain knowledge rules or
90:33 - constraints to quantile based binning
90:36 - this technique divides the data into Q
90:39 - equal partitions if q equals 4 then the
90:43 - parts are quartiles divide data into
90:46 - four equal partitions three two way
90:49 - ANOVA analysis of variance tests this is
90:54 - used to find similarity between the
90:55 - various data points of the feature these
90:58 - similar data points can be grouped
90:59 - together to partition the data set
91:03 - you
91:04 - first let's discuss what feature
91:07 - engineering is features are the
91:10 - independent variables don't forget we
91:12 - will use this term a lot feature feature
91:17 - equals variable once again independent
91:20 - variables or features are used to
91:22 - predict the dependent variable
91:24 - frequently these features have hidden
91:26 - information that the machine learning
91:28 - models cannot utilize to negate this
91:32 - situation in the data pre-processing
91:34 - stage we just applied the domain
91:36 - knowledge and create new informative
91:38 - features out of the existing features
91:40 - available in the data set
91:43 - these features should be created
91:45 - carefully though otherwise the model may
91:47 - over fit the set of features on which
91:50 - the model is supposed to run should be
91:52 - selected wisely because good features
91:54 - with good quality data can yield a less
91:57 - complex model with better results
92:01 - feature engineering is a recursive
92:04 - process that can be divided into the
92:06 - following steps
92:08 - understand data set
92:11 - brainstorm features
92:14 - create new features
92:17 - validate what impact these features have
92:20 - on the prediction result
92:23 - restart from step one until the desired
92:26 - accuracy and other metrics are achieved
92:28 - so for example consider a model designed
92:31 - to predict salary hikes for employees in
92:34 - an organization the data set contains
92:37 - employee ID geographical location date
92:41 - of birth employment start date career
92:44 - start date etc in this data set date of
92:47 - birth and career start date might not be
92:50 - useful for a data set if we derive two
92:53 - new features for example employee age
92:56 - and years of experience these two
92:58 - features could play a key role in salary
93:00 - hike prediction the process of
93:04 - identifying creating these derived
93:05 - features is called feature engineering
93:08 - in reality though feature engineering is
93:11 - an art and it comes with domain
93:13 - knowledge and experience
93:17 - feature scaling some machine learning
93:20 - algorithms do not perform well when all
93:22 - features in the data set are not on the
93:25 - same scale so coming back to our
93:27 - employee data set example salary may
93:30 - range from $40,000 to $200,000 and age
93:34 - ranges from 21 to 60 days two features
93:38 - do not have the same scale rescaling
93:41 - these features can improve the
93:43 - performance of some machine learning
93:45 - models
93:47 - now how we can make feature scaling we
93:50 - have two common techniques for feature
93:51 - scaling the first one is normalization
93:55 - this technique transformed the feature
93:57 - into a 0 to 1 scale the great thing
94:00 - about normalization is to reduce also
94:03 - the effective outliers the new value of
94:07 - the feature called exchanged is equal to
94:10 - the value of X minus the minimum x value
94:13 - and dividing by the maximum x value
94:16 - minus minimum x value exchanged equals X
94:21 - minus X min over X max minus X min
94:28 - the second one is standardization which
94:30 - transforms the features so that the mean
94:32 - of the distribution becomes zero and the
94:35 - variability becomes one standardization
94:38 - is not significantly affected by
94:40 - outliers
94:43 - look at the figures below they show how
94:46 - the transformed feature will be after
94:49 - implementing normalization and
94:51 - standardization
94:55 - well that's it for this free AI Sciences
94:58 - course hopefully this course has
95:00 - demystified the notion of data science
95:03 - and machine learning that is just the
95:06 - beginning so now that you are familiar
95:08 - with the logic behind the different
95:09 - types of learning you know how to create
95:12 - simple models it's time to move on but
95:15 - you will not be alone on that path we
95:17 - have more content courses and books to
95:19 - reinforce your efforts and guide you at
95:21 - every stage machine learning is a
95:24 - crucial development of today's world the
95:27 - concepts behind it have been around for
95:28 - more than a decade but the age of
95:30 - machine learning and related models such
95:33 - as artificial intelligence data science
95:35 - and more is now the change is just
95:39 - happening and it is fast you've made a
95:43 - great decision to start your journey
95:44 - into the world of machine learning with
95:46 - this free course today the knowledge and
95:48 - the ability to use machine learning is a
95:51 - competitive advantage tomorrow it will
95:54 - be a mere necessity machine learning
95:57 - techniques have already started to
95:59 - change the world of business by creating
96:01 - a new value for data the future will be
96:04 - even more exciting very soon most of the
96:07 - devices and apps that we use daily will
96:09 - be fueled by machine learning algorithms
96:11 - many of them already are now though you
96:15 - have a chance to become a part of this
96:17 - major development Congrats on your
96:19 - decision and don't forget to check out
96:21 - our courses ebooks and contents now as I
96:25 - promised you there is a gift for you
96:27 - it's an e-book available online where
96:29 - you will learn about machine learning
96:31 - you can find it at this link HTTP colon
96:36 - forward slash forward slash WWI Sciences
96:41 - dotnet forward slash gift
96:44 - - eBook - machine - learning
96:50 - we highly recommend that you visit our
96:52 - website AI Sciences dotnet and subscribe
96:55 - to our email list you'll receive all of
96:58 - our books free in an eBook format and
97:01 - you will be informed about all our
97:03 - promotions and offers if you have any
97:06 - feedback please let us know by sending
97:08 - an email to review at AI Sciences net
97:13 - this feedback is highly valued and we
97:15 - really look forward to hearing from you
97:18 - if you enjoyed this video please like it
97:20 - share it subscribe to our Channel AI
97:22 - sciences and make sure you click on the
97:25 - notification button so you can receive a
97:27 - notification when our next course is
97:29 - ready see you at the next video
97:37 - [Music]

Cleaned transcript:

hello and welcome to this free course in data science and machine learning for beginners made my AI Sciences Academy this easytounderstand course is dedicated for beginners who need to learn the A to Z fundamentals of data science and machine learning through a gradual and segmented approach ok here are the four main parts we will cover during this course we will start with an introduction part two data science and machine learning in this part we will answer some questions that you may ask like what is data science and machine learning why data science now and when I can apply data science and machine learning techniques in learning part 1 we will learn the preliminary to understand data science and machine learning in this part we will learn some vital concepts in data science and machine learning a learning bar we will start the machine learning part where I will explain to you how the machine learning models work we'll also discuss about regression classification and clustering in learning part three we will talk about how to evaluate the performance of the model and choose the best one based on some indicators in learning part four of this course we will discuss about some best practices in data science and machine learning at the end of this course you will receive a gift so please follow this course until the end this gift will help you on your learning journey so ready let's start you data science is not a straightforward easy to define field like most traditional fields it's rather a multidisciplinary field which means that it combines different areas such as computer science mathematics and statistics because data science can be applied and used in various applications and fields it requires domain expertise in each particular area ok for example if we use data science to develop a medical analysis application then we will need an expert in medicine to help define the system and interpret the results data scientists explore the data visualize it and calculate important statistics from it then depending on these steps and the nature of the problem itself they develop a machine learning model to identify the patterns so machine learning and deep learning are the subfields of data science so you might ask what is the difference between data science data analytics and big data well big data means huge volumes of various types of data we differentiate big data by its four V's which are the characteristics that are distinct from ordinary data they are volume velocity variety and veracity the sheer volume is the main characteristic that makes data big velocity is the frequency of incoming data that needs to be processed variety means different forms of data veracity refers to the trustworthiness of the data on the other hand data analytics is more about extracting information from the data by calculating statistical measures and visualizing the relationship between the different variables and how they are used to solve the problem it's like descriptive statistics why data science now that's an interesting question first there is currently plenty of data more than at any time before in history and it just keeps growing exponentially second now we have much better computers and computational power than ever before a task that can be finished in a few seconds nowadays would have required days with the computers that existed just a few years ago and finally we have more advanced algorithms for pattern recognition and machine learning than we did just a few years ago so in one sentence if you want to know why data science has become our focus right now is because we have a lot more data better algorithms and better hardware let us see where data science and machine learning is applied I want to say that data science is everywhere the number of data science applications is countless it's because we have data everywhere and there are dozens of algorithms developed each year to solve these tasks however we will talk about a few famous use cases of machine learning in data science in our daily lives as you can see machine learning and data science are currently being used in many fields such as healthcare finance transport social media ecommerce and virtual assistant apps among others in healthcare machine learning is currently used in disease diagnosis it provides higher accuracy compared to professional physicians it is also undergoing extensive research and drug discovery another application is robotic surgery where we have an AI robot helping and performing the surgery with a precision that is actually higher than that of the best surgeons in transport Tesla used machine learning algorithms in its selfdriving cars machine learning is also used for air traffic control in finance I worked for two years as the KPMG consultant in one of the French banks to develop different machine learning algorithms predicting customer defaults and bank capital requirements many banks are currently using machine learning powered software for fraud detection also banks are using machine learning for algorithmic trading in social media I think all social media platforms today use machine learning for both spam filtering and sentiment analysis Facebook also uses machine learning image recognition in ecommerce many online shopping websites such as Amazon eBay and udemy use machine learning for customer support targeted advertising and product recommendation machine learning is also used in virtual assistant apps many startups are founded based on the idea of developing a machine learning powered assistant in one particular field this assistant can be a chatbot for example which can intelligently reply and answer nearly any inquiries in that field these are just a few broad in general applications of data science and machine learning you can develop your own application in any field that you find interesting and have some experiences and by the end of this course you will be equipped with the knowledge necessary to create an app in the area of your choice let's talk just a little bit about the history of data science the term data science has been appearing in various contexts over the past thirty years but did not become an established term until relatively recently in its early usage it was used as a substitute for computer science since 1960 when Peter now are first mentioned the term how about the future of data science and data scientists we can clearly see that the future of data science is very bright another evidence for that is the cloud services that have appeared in the last two or three years being extremely cheap and fast they can help develop more advanced machine learning applications in all fields so it will now be surprising to see many tasks that we considered science fiction such as assistant robots and selfdriving cars already in use in our daily lives how about the future of data scientists let me share some statistics with you as you can see in the figure on your screen at the end of 2016 the percentage of jobs for data scientists was 474 percent larger than those for statisticians today in the USA the work of a data scientist is one of the high Penguin's learning data science maybe the best decision you've ever made in our learning company ai sciences is here to help you achieve this goal now how can you get the ultimate benefit from this course I highly recommend you take this course very seriously and follow it step by step we highly suggest that you go through the study materials that you used in your high school undergraduate and graduate programs and revised topics such as linear algebra calculus and statistics we will not go too deep into the math behind algorithms in this course but we will cover basic ideas logic and in some cases formulas to understand our main topic of interest better also try to finish every single project provided in the course on your own and then check the solution finally we encourage you to go through any further reading material that you will find it will give you an overview of what you can learn next after finishing the course okay in this lesson we will explore in detail some important terms in data science you ready let's go in the following lesson we will talk more about data and variables so what is the data data are basically collections of facts measurements observations numbers words etc that have been transformed into a form that can be processed by computers data are stored in columns and rows the convention is that each row represents one observation case or example and each column represents one feature or variable got it we also have two types of variables based on their value numerical and categorical variable if the value is a number and we can compute the mean for this variable we call it a numerical variable but if the but if the value is a factor or label and we can't compute its mean we have a categorical variable so we can also talk about dependent and independent variables a dependent variable is one that we need to predict and the independent variables or X variables will help us to predict a y variable it is essential to know that X variables need to be independent of each other and that is why we call them independent variables or predictors couple other important terms in data science are population and sample and data science the whole population is our target but due to a lack of resources a data scientist can't work in the entire population because of that we have to choose a representative sample of the data from the population the goal of machine learning algorithms is to find parameters that can do the mapping on the whole population based on the given sample as you can see we have our population and we will choose a sample by using the sampling technique now let's talk about outlier and missing data in data science an outlier is a data point that differs from other observations an outlier may occur due to variability in the measurement or it may indicate an experimental error outliers can alter the performance of many machine learning algorithms as we can see there in the image we can detect outliers by visualizing the data in this example employee number two and nineteen are outliers based on their profile how to deal with outliers we can drop them altogether cap them with the threshold assign new values based on the mean of the data set for example or apply a transformation on the data set itself okay one way to handle missing data is by dropping the observations another way to handle them is to use data imputation techniques so if we have a numeric feature we can replace the missing value with a mean median or mode or we can select random observations from the data set and replace its feature value in the observation that has missing values the last imputation technique can be performed by regressing the missing feature on the other features and making a prediction of the missing value if we have a categorical feature we can use the mode replacing or also we can use the KNN model to predict the feature that has a missing value if you still don't know much about the regression and KNN model that worries David will present them to you in the machine learning section that's it for this lesson in this class we will learn about the link between AI machine learning and deep learning dl you'll find out how a machine learns and the types of learning there are ready let's go first here's the link between artificial intelligence machine learning and deep learning let's clear the confusion between these three essential terms so what are they I ml and DL by looking at the image it is clear that ml is a subfield of AI and DL is a subfield of ML machine learning algorithms were developed with the goal of finding a useful prediction function among machine learning algorithms let's mention the artificial neural network an artificial neural network consists of a collection of neurons connected to each other in a specific way however the use of neural network was limited because of the lack of computational power and the lack of proper optimization algorithms for neural networks this is where deep learning came in it is a simpler algorithm that uses more neurons and layers to perform a lot of learning tasks like image recognition and natural language processing today deep learning is used in a lot of areas such as the hightech industry Tesla selfdriving cars and the Seattle Amazon store are constructed based on the deep learning algorithms combined with computer vision now let's see how a machine learning algorithm learns okay Before we jump into how a machine learns let us first try to understand how a human baby learns think for example of a oneyearold human baby a baby does not know the difference between an apple and an orange for him all fruit is the same orange Apple bananas cucumbers in his first phase of learning called phase one of learning in the figure he builds an intuition that oranges and apples are of one shape and bananas and cucumbers are of another shape once a baby is comfortable with the shapes of fruit he goes into learning phase two phase two of learning in the figure by introducing another property like color now he knows that a fruit that is round in shape and red in color means that it's an apple in a round shape and an orange color means it's an orange in the face 3 the baby will gather a lot of data as shown in the table with these two properties and based on these data in the future he will know the difference between fruits machine learning models learn the same way in machine learning the properties of the fruit such as shape and color are called the features the fruit type is called the label each instant of an input/output pair is called an observation so depending on the features and labels enter to a machine learning algorithm learning is classified into three main categories supervised learning unsupervised learning and reinforcement learning we also have semisupervised and instant based learning but in this lesson we're just going to focus on the main three learning types in supervised learning we train with labeled observations that means that for each observation of training data the input and output are known as you can see in the image we try to predict the output from the input by training our machine learning model classification is one example of supervised learning where the goal is to classify objects regression is another example where we try to understand the relationships among variables in a glance in supervised learning we have the Y and the X variables and we want to make the prediction of why okay now in unsupervised learning the trainer does not provide a labeled output in the learning data set the machine learning algorithm learns from unlabeled data and gathers information from it as you can see it's short in unsupervised learning we only have X variables and we need to gather the observations and groups based on the information given by X unsupervised learning is used mainly for clustering tasks where we organize the observation into clusters in reinforcement learning an agent learns by interacting with the environment so in this type of learning the agent performs an action in the environment this action takes the environment to a new state and gives a reward to the agent the reward can be negative or positive for multiple iterations and the rewards the agent learns based on his past experiences reinforcement learning is mainly used in skill acquisition tasks such as robot navigation or games okay to sum up we have three main types of learning supervised when we have X and y variables unsupervised when we only have X variables and reinforcement learning in this case the algorithm learns by rewards that it gets as a result of its action in an evolving environment now you know how a machine learns and what are the different types of learning I guess now you know a lot more about basic data science and machine learning terms in this lesson let me explain some important modeling terms to you alright you ready let's start in machine learning we always split our data into two so we have a training data set and a test data set the training data will help us to train our model and the test data set will help us evaluate our model performance and accuracy a typical machine learning model learns from the training data set and applies the learning to the test data set so if the model is able to make correct predictions on the test data set then the model is able to generalize the learning to any new data and then we can say we have a good model now what is an over fitted model when we obtain a model that works very well on the training data set but is not able to generalize to the test data sets we call such a model an over fitted model in this case the testing error is large because the model is very complex the model is under fitted when the training error is large because the model is too simple and just can't capture the true complexity of the data so you may wonder can we control these issues the answer is yes the solution for under fitting is either to increase the size of the data set or to increase the complexity of the model but the solution for overfitting is a bit trickier the first solution is to gather more data this solution is not always feasible the second solution is to use penalty terms in the model and that is called the regularization technique and the last one is to make crossvalidation okay in short you need to understand that our goal as data scientists is to come up with a model that is not too generalized and not too focused on training data this model is called the best fit model this is basically a tradeoff between an under fit and an over fit model like the middle image as you're already familiar with overfitting and underfitting the concept of bias and variance will be very easy to digest but before we start talking about bias and variance let us classify the type of models errors first we have the irreducible error which comes from the nature of the data itself for example the noise when you talk through your mobile phone the second kind of error is the reducible error we have to reducible errors the bias error and the variance error okay the bias error is the difference between the average prediction of our model and the correct value which we are trying to predict the bias error is high if the model is oversimplified the variance error is the variability of model predictions for the given data the variance error is high if the model is not generalizing well on new data okay look at the figure at the right the blue points represent how far we are from the minimum error which is represented by the small red circle in the case of low bias the blue points are not very far from the minimum error in the case of low variance the blue points are near each other of course we want our model outputs to be as close as possible to the minimum error which means low bias and low variance but it is impossible to have both low bias and low variance there is a tradeoff between bias and variance because as we decrease the model bias we make it more complex and we increase its variance and when we decrease the variance we increase the bias looking back to overfitting and underfitting we can say that when the model is under fitted it has low variance and high bias when the model is over fitted then it has high variance and low bias as you can see in machine learning it is important to make a biasvariance tradeoff and to choose a middle model in data science we have different types and qualities of data and we need to make them usable in our model number one feature extraction and feature engineering will help us transform raw data into features suitable for modeling and to feature selection will help us remove unnecessary features during the data processing step in this part of the course we will learn various machine learning models which are commonly used for prediction classification clustering etc as I already mentioned machine learning is broadly classified into supervised and unsupervised learning supervised learning means that the algorithms are supervised during the training phase so in other words in order to train these algorithms we need data that have labelled targets for example if a model is being created for predicting house prices then the historical data that is used to train the model should have a target column stating the price of a house unsupervised learning is when we don't have a target labeled data in this case the algorithm classifies objects based on some existing features supervised machine learning problems have two categories of learning regression and classification regression algorithms are used to identify a relationship between a dependent variable target and independent variables predictor / features so in regression the target is always a continuous variable and the predictors can be continuous or discrete in nature regression is best used for finding causal effect relationships between the variables forecasting time series modeling etc in regression analysis the model tries to fit a curve to the data points in such a manner that the difference between the data point and the curve is at a minimum on the other side we have classification models the classification models are used to predict the target that has discrete values for so for example class of fruits orange pineapple and lime or predicting whether a patient is suffering from cancer or not for this kind of use machine learning models collect insights from the historical labeled data and use these insights to predict the target class okay listen up don't forget this regression and classifications are all supervised learning models because we have a labeled output which we need to predict if this labeled output is continuous we use regression and if it's categorical we use classification now the most used unsupervised learning models are clustering and Association analysis clustering is the most important unsupervised learning model it deals with finding a structure in a collection of unlabeled data so a loose definition of clustering could be the process of organizing objects into groups whose members are similar in some way I will present clustering in more detail later Association analysis models discover relationships in large datasets hidden data relationships will be expressed as a collection of Association rules and frequent itemsets with Association analysis Association analysis isn't frequently used we will not focus on them in this course what are the machine learning models associated with each of these learning types I mean which models are dedicated to regression classification and clustering for the regression purpose we commonly use linear regression decision trees for regression support vector machines for regression SVR or neural networks for the classification purpose we use logistic regression decision trees for classification support vector machines classifier SVC nearest neighbor or neural networks in unsupervised learning we use the kmeans clustering so before we discuss these models in detail I need to explain the difference between two terms which can be confusing when you get started in machine learning the terms model parameter and model hyper parameter I'll talk more about these two terms in a few minutes so I think it will be better to explain the difference between these two terms now a model parameter is a configuration variable that is internal to the model and whose value can be estimated from data it's required by the model when making predictions and it is estimated or learned from data model parameters are key to machine learning algorithms they are the part of the model that is learned from historical training data so some examples of model parameters include the weights in an artificial neural network the coefficients in a linear regression or logistic regression on the other side a model hyper parameter is a configuration that is external to the model and whose value can't be estimated from data they are often only help estimate the model parameters and they are often specified by the practitioner linear regression is one of the most used supervised machine learning algorithms and now I'm gonna explain what it is exactly how it works and show you the logic behind it first of all what is linear regression let's say that two years ago I made $10,000 on my job and that last year I earned 20,000 with that in mind what do you think how much would I make this year if you answered $30,000 you just applied linear regression we make predictions all the time and most of them follow the logic of linear regression linear regression is a machine learning model which was designed to help you to specify a linear relationship to predict the numerical value of a dependent variable we will call it Y in this course for a given value of independent variables we will call them X by using a straight line called the regression line does that sound complicated it's not I'll show you we can say that linear regression will help us make a prediction based on some information prediction equals dependent variable Y some information equals independent variables X we use linear regression to answer the following questions is there a linear relationship between the two variables x and y which X variable contributes the most let's write this model as an equation it goes like this y equals B 0 plus B X plus e b0 is the value of y even if the value of x is 0 it's called the intercept B is a coefficient associated to X it will be a vector of coefficients e for error denotes all remaining information about why that hasn't been explained by the X variables of course the linear model is not perfect and it will not predict all the data accurately okay we have two types of linear regression the simple linear regression and the multiple linear regression in simple linear regression we use a single independent variable to predict the value of a dependent variable the model in this case will be y equals B 0 plus B 1 X 1 plus e a multiple linear regression we use two or more independent variables to predict the value of a dependent variable the difference between the two is the number of independent variables X y equals B 0 plus B 1 X 1 plus B 2 X 2 plus dot dot dot plus E to keep this lesson simple and to help you understand the rest of the course right now we will focus on the simple linear regression only the same thing will be replicated in multiple linear regression the only thing that will change is the number of X variables here are some examples of simple linear regression and multiple linear problems the estimation of the average student score based on the number of hours they have spent studying 30 hours please note that all the students who pass the exam will receive at least two points for their presence in this problem the dependent variable Y will be the average student score the independent variable is the hours of study and it equals 30 the intercept equals B 0 equals 1 the regression model we want for the score prediction is score equals B 0 plus B 1 times our study score equals B 0 plus B 1 times 30 in this model the b0 and b1 are coefficients these coefficients are what we need in order to make predictions about our score if we add the number of exercises that the student has completed to the model it will become a multiple linear regression model score equals b0 plus b1 times our study plus b2 times exercises and B plus for the prediction of the score we need to estimate three parameters B 0 B 1 and B 2 B 0 B 1 and B 2 parameters need to be estimated based on our historical data to estimate the linear regression coefficient we need to minimize the least squares or the sum of individual squared errors in other words that's the difference between the actual value and the prediction the error of the individual I is easily calculated as the difference between the real value of y I equals y hat i AI equals y I minus y hat I we square the error for two reasons one the prediction can be either above or below the true value resulting in a negative or positive difference respectively if we did not square the errors the sum of errors could decrease because of negative differences and not because the model is a good fit to squaring the errors penalize as large differences and so the minimizing the squared errors guarantees a better model let's look at a graph to understand it better in the graph the green dots represent the true data and the yellow line is a linear model the dotted red lines illustrate the errors between the predicted and the true values in practice we use the OLS algorithm it's ordinary least squares eater ative Li in each iteration the algorithm calculates the sum of the individual squared errors and in the next iteration the algorithm updates model parameters to shift the line from the previous position to reduce the squared error finally the best OLS estimators of the coefficients are in these equations X bar and y bar represent the mean now how to make a prediction we will use the existing data to estimate the values of B 0 B 1 through B K we can do that in Excel our Python etc after that we can make all the predictions we want please note that linear regression is considered to be one of the most straightforward machine learning techniques and an easy model to interpret but it has its disadvantages it will only work if the relationship between the dependent and independent variables is linear that's it now you know exactly what linear regression is and how it works now present the decision trees model a decision tree is like a series of ifelse conditions that lead to a decision it can be used for classification or regression use cases it works for both categorical and continuous input and output variables decision tree breaks down a data set into smaller and smaller subsets while at the same time an Associated decision tree is incrementally developed the final result is a tree with decision nodes so consider a scenario where we want to decide whether a loan should be approved for an applicant or not to decide we will ask the applicant a series of questions we might start off with whether the applicant has any other existing loans if the answer is yes then the next question might be whether he is a defaulter or not with this kind of series of questions we can narrow down the search and make a robust decision we build the model above by hand to make a decision about whether a loan application should be approved or not alternatively a machine learning model could perform supervised learning using the data set to arrive at a decision the decision tree model in machine learning can learn these decisions or conditions from the data set quickly and build the model okay decision trees have three types of nodes a decision node has two or more branches a leaf node represents a classification or decision we also have the topmost decision node in a tree which corresponds to the best predictor called root node how do decision trees work in practice well that's an interesting question in practice there are three important steps in the building of a decision tree the first one is splitting a decision tree can have a subbranch or a subtree as well and the process of creating a sub branch is called splitting have a look at the figure to visualize these notations so the root node has the full data set and at each decision though the test is conducted to split the data set decision nodes are executed to split the data until it reaches the leaf node a leaf node contains a single target value a single class or a single regression value a leaf node that contains only one target value is considered pure the second one is pruning or shortening of branches of the tree decision trees are prone to overfitting if the parameters are set to favor all pure leaf nodes which means that all data points in the training data set are correctly classified so to reduce overfitting the following strategies are commonly followed one pre pruning stopping the creation of the tree and this is achieved by limiting the maximum depth of the tree is so limiting the maximum number of leaf nodes or defining the minimum number of points required to split it further to post pruning just trimming the nodes that contain less information as you can see a pruned tree has less nodes and has less sparsity than a nun pruned decision tree then the last one is the tree selection in this step the models are looking for the smallest tree that fits the data usually this is the tree that yields the lowest cross validated error decision tree models work really well if the training data set is in a binary format but this is not a limitation for continuous features the decision condition can be applied in the form of greater than or less than a certain threshold for example X is greater than 0.7 a prediction on a new data point is made by traversing through the decision tree and checking at each decision node whether the condition is met or not one of the key factors in the decision tree is entropy so let me explain to you why it's important to carefully consider which feature will be used to split each node because decision trees with a different split node may result in different predictions and accuracy we can utilize a statistical method to identify the feature that should be selected as the root node the feature that has the most information gain should be selected as the root node so information gain measures how well a certain feature distinguishes among different target classifications information gain is measured in terms of the expected reduction in the entropy or impurity of the data the entropy of a set of probabilities is H of P in the formula where P is the probability of outcome event so if the sample is completely homogeneous the entropy is 0 and if the sample is an equally divided one it has an entropy of 1 to understand how entropy and information gain is calculated let's take a look at the following example okay a training data set has 500 observations of these 500 observations 300 are of positive class and the remaining 200 are of negative class positive class ratio equals 300 divided by 500 equals zero point 6 negative class ratio equals 200 divided by 500 equals 0.4 entropy of the target variable will be entropy equals minus 0.6 times log to 0.6 plus 0.4 times log to 0.4 equals 0.9 702 a feature X in the dataset is split as X is greater than 347 120 positive and 80 negative X is less than or equal to 347 240 positive and 60 negative entropy of X is greater than three hundred and forty seven equals E 1 equals negative 120 divided by 200 log two 120 divided by 200 minus 80 divided by 200 log two eighty divided by two hundred and ruvi of X less than or equal to three hundred and fortyseven equals e 2 equals negative 240 divided by 300 log two 240 divided by 300 minus 60 divided by 300 log 260 divided by 300 entropy of x equals two hundred divided by 500 times entropy one plus 300 divided by 500 times entropy two information gained for feature X equals entropy total minus entropy of X whichever feature in the note has the maximum information gain will be selected for the split we can also use other indicators like the misc classification rate the Gini index and Inter of Dakota miser 393 for calculating the information gain of a feature what are the hyper parameters of this model free routing parameters are the main hyper parameters of a decision tree model for example the maximum depth of the tree the maximum number of leaf nodes and minimum number of data points required to split the node further a combination of these hyper parameters can be used to build the decision tree which is generalized over the data set and provides good accuracy what are the advantages and the disadvantages of a decision tree decision trees are computationally cheap to use easy for humans to understand results and it can deal with irrelevant features its disadvantages are it is prone to overfitting and provides poor generalization performance sometimes it gives low prediction accuracy training in post pruning strategies are implemented in decision trees to reduce overfitting but still decision tree models tend to overfit so to overcome this data scientists use an advanced modeling technique called ensemble the idea of ensemble is to build many trees all of which predict well and overfit in their own way and average the results to reduce overfitting ensemble means assembling many machine learning models to create a more powerful and robust model these machine learning models also known as base estimators or base learner because they are combined together the most popular ensemble techniques are bagging and random forest bagging combines sampling techniques and aggregation to form an ensemble model and practice multiple samples are chosen randomly with replacement within the training data set let me explain how the sampling is made so suppose that a sample of 10 observations is drawn from a training data set of 100 observations these observations are then returned to the training data set before another sample is drawn so the next sample of 10 observations is drawn from a training data set of 100 observations in simple terms at any point all of the training data set observations will be available for a sample to be drawn this sampling technique is called the bootstrap for each of these samples a decision tree or other bass learners is created finally these decision trees or based learners are aggregated to achieve an efficient predictor typically the combined estimator is better than any one of the single decision trees question how does the algorithm choose the output at the end of this aggregation okay the output is chosen based on voting for classification or on an averaging for regression now how about the random forest model so you learned how bagging works random forests also uses the same bagging technique with a slight modification during bagging all features of the training data set are used on sampled data to create the decision trees or the base estimators because of this sampling techniques used in bagging the datasets in each sample are quite similar HBase estimator usually breaks at the same feature this results in quite similar base estimators this means that weak features will not be incorporated to avoid this in random forest model the samples are created with a subset of features selected randomly for each node in the decision tree this selection of a subset of features is repeated separately in each node so that each node in a tree can make a decision using a different subset of features this process of randomly selecting sampled data and a number of features for a split at each node ensures that all decision trees in the random forest are different similar to the decision tree the random forest also provides feature importance which is computed by aggregating feature importance over the trees in the forest typically the feature importance provided by random forests is more reliable than the one provided by a single tree the maximum number of features to split at each node determines how random each tree is and a smaller value reduces overfitting as a rule of thumb this parameter can be set to the square root of a number of features for classification and for regression use cases Criterion Gini entropy number of decision trees maximum number of features maximum depth of the decision tree minimum number of samples required at each leaf node minimum number of samples required to split a node and maximum number of leaf nodes in each decision tree are all hyper parameters of ensemble models this hyper parameter can be tuned to get a generalized and accurate machine learning model benefits of random forest okay what I can say is that ensemble models are very powerful and often work without parameter tuning but the bad news is it is difficult to understand thousands of trees and explain the decisionmaking process that is why this model is considered like a black box another disadvantage is that ensemble methods need more computing resources and take more time to learn from data ok they're machine learning model is boosting boosting is another model that makes a bass estimator like decision tree more powerful by making a sequential execution and each subsequent estimator focuses on the weakness of the previous estimator boosting incrementally builds an ensemble by training each model with the same data set but where the model coefficients of estimators are adjusted according to the error of the last prediction several weak models team up to produce a powerful ensemble model the main idea of boosting is to focus on the observations that are hard to predict boosting can reduce bias without incurring higher variance here are the popular boosting algorithms Aida boost and gradient boosting let me explain to you how they work Aida boost is adaptive boosting where more attention is given to the records that are not correctly predicted after each iteration weights of the wrongly predicted observations are increased so that these records will be picked up more in the next iteration to gain better accuracy gradient boosting is another popular boosting algorithm it works by sequentially adding the previous predictors under fitted predictions to the ensemble ensuring errors made previously are corrected here's what you need to know boosting does not introduce randomness to the decision trees or any based learners however it uses strong techniques to build accurate predictors in most cases the maximum depth for boosting models has kept a 5 models this makes the model faster and the model consumes less memory the hyper parameters for these models are the number of decision trees maximum depth and learning rate a lowered learning rate means more trees are required and a higher learning rate means less trees are required to build a model these hyper parameters should be tuned to get an optimized machine learning model boosting models are more sensitive to hyper parameters but once the hyper parameters are tuned properly these models provide very good accuracy and generalization so in general ensemble models are very powerful and widely used however like the bagging models it is difficult to understand thousands of trees and explain the decisionmaking process I just don't recommend it for dimensional sparse data they don't do very well in high dimensional data well the SVM's model or support vector machines support vector machines are one of the supervised learning algorithms mostly used for classification tasks however SVM algorithms can be used for regression as well a support vector machine for classification is called the support vector classifier SVC and for regression it's called the support vector regressor or SVR svms are based on the idea of finding a hyperplane that best divides a data set into two classes as shown in the image below you may ask what the hyperplane is okay take this example say we want to classify a task with only two features you can think of a hyperplane as a line that linearly separates and classifies our data into intuitively the further from the hyperplane our of data points lie the more confident we are that they have been correctly classified we therefore want our data points to be as far away from the hyperplane as possible while still being on the correct side of it so when new testing data is added whatever side of the hyperplane it lands on will decide the class that we will assign to it now how do we find the right hyperplane or in other words how do we best segregate the two classes within the data before we answer this question we need to understand what is a margin a margin is equal to the distance between the hyperplane and the nearest data point from either set now the goal is to choose a hyperplane with the greatest possible margin between the hyperplane and any point within the training set and giving a greater chance of new data being classified correctly as you can see in two dimensions it is very easy to classify using SVM but sometimes it's harder to identify clearly the hyperplane because the data is rarely ever as clean as our simple example above a data set will often look more like the jumbled balls which represent a linearly nonseparable data set look at this case in order to classify a data set like this one it's necessary to move away from a two dimension view of the data to a three dimension view explaining this is easiest with another simplified example imagine that our two sets of colored balls above are sitting on a sheet and this sheet is lifted suddenly launching the balls into the air while the balls are up in the air you use the sheet to separate them this lifting of the balls represents the mapping of data into a higher dimension this is also known as kernel okay because we are now in three dimensions our hyperplane can no longer be a line it must now be a plane as shown in the example the idea is that the data will continue to be mapped into higher and higher dimensions until a hyperplane can be formed to segregate it so that's how SVM's work and produce the output so what are the benefits and the disadvantages of SVM's SVM's can give a great accuracy and can work well on smaller cleaner datasets nevertheless if you have a larger data set it isn't suited as the training time can be very high another machine learning algorithm is the K nearest neighbors K and n the K nearest neighbors knn algorithm belongs to the family of instancebased competitive learning and lazy learning algorithms it's a lazy learning algorithm because the calculation is delayed until a prediction is required it is called the localized model because only the data points that are near new data points are used for model calculation and for predicting classes of new data points so let me explain how this works what a prediction is required for an unseen data point the knn algorithm will search through the training data set for the k most similar neighbor the prediction attribute of the most similar data points is summarized and returned as the prediction for the unseen instance so in this model K is the number of neighbors we want to check to classify a new data point if K is greater than one the model uses voting to classify the new data point in short the class that is in the majority is assigned to the new data point for example okay so if the value of K is set to three the model will check the three nearest data points to classify the new data point the default value of K is one which means that the vanilla KNN model classifies the new data point according to the class of the nearest neighbor the yellow and purple circles represent the data points from the training data set and we want to predict the class for the red data point if the value of K is set to three then the model will check the three nearest data points inner circle and then classify the red data point if the value of K is set to six then the model will check for the nearest six data points outer circle and then classify the red data point in the figure above the red point will be classified as follows if K equals three Class B two votes for Class B and one vote for Class A if K equals six Class A two votes for Class B and four votes for Class A okay in a multiclass data set we count how many data points belong to each class and the class that is in the majority is predicted for the new data point so how does the algorithm choose the nearest points the KNN uses the distance to evaluate which point is near to the new data for continuous features Euclidean distance is calculated and for categorical features another distance called hamming distance is calculated the important hyper parameter for KN n is the number of neighbors it holds the value of K the default value of this parameter is 5 knearest neighbor has a lot of benefits among them its simplicity and flexibility it also works well with enough representative data the problem that we can face sometimes is that the can and algorithm is space consuming because for each prediction the calculation is done separately okay first of all logistic regression is among the most commonly used and best known algorithms that we can use to solve a classification problem it's named the logistic regression because of the logit function which is used in this method of classification other than that logistic regression is pretty much the same as linear regression the purpose of logistic regression is to detect a relationship between features and find the probability of a particular outcome in a way it extends the idea of linear regression to a situation where the outcome variable is categorical for example let's try to predict whether a student will pass or fail an exam the number of hours spent studying is given as a feature and the response variable has two values passed and failed okay in the equation below you can see that we need to predict the Y variable which can take two values 0 or 1 0 for failed and one for passed it turns out that it is virtually impossible to predict Y with the following model y equals B 0 plus B 1 X 1 plus dot dot B K X K that's because Y is a categorical value and B 0 plus B 1 X 1 plus dot B K XK will give a continual value as the result therefore instead of predicting this categorical variable we're going to predict the probability of the realization of y equals 1 B equals probability of y equals 1 in order to do that we need a link function the logit link function okay a link function is basically a function of the mean of the response variable Y that we use as the response instead of Y itself it means that when Y is categorical we use the logit of Y as the response in our regression equation instead of just Y the logit function is the natural log of the odds that y equals one of the categories for mathematical simplicity we're going to assume Y has only two categories encode them as 0 and 1 P is the probability that y equals 1 so for instance those X's could be specific hours spent studying number of completed exercises and the score in the first exams while P would be the probability that a student would pass an exam as in our first example linear regression versus logistic regression instead of linear regression the line between y and X the relationship between X and the probability P is a logistic distribution how to estimate the logistic regression coefficients at this point we don't know the coefficients B 0 B 1 B K of the model so we must estimate them in order to make predictions unlike the linear regression model logistic regression uses ordinary least square for parameter estimation the estimation is done by using maximum likelihood due to its more general nature and statistical features there can be an infinite set of regression coefficients the maximum of the log likelihood estimate is that set of regression coefficients for which the probability of getting the data we have observed is maximum in other terms we must make estimates for the coefficients that predictions are as close as possible to the originally observed value so how to make a prediction well the prediction is made using the original logistic function and the estimated coefficients from the maximum likelihood function with the observed data to compute the estimated probability of P if the probability of P is below 0.5 0 the predicted value of y is 0 otherwise it will be 1 in our example the student will fail based on the value of x in the figure we make a prediction by using a different set of X variables the first set gives us a red point with P equals 0.2 9 in the first case the value of y is equal to 0 as the predicted value of P is less than 0.5 0 in the second case the green point P equals zero point 9 0 the predicted value of y will be 1 compared to other models logistic regression is rather simple and efficient however it can't handle a large number of categorical variables successfully that's it now you know exactly what logistic regression is and how it works in our next tutorial you'll learn how to apply it in Python and our click on the video above to see this tutorial thanks for your time and hey if you like our videos please like it share it and subscribe to our Channel oh and click on the notification button so you can receive notifications for our next course enjoy machine learning first though let's see what a neuron is a neuron in the neural networks field is something that takes some input applies some logic and outputs the result we call it a function for example if we have F X equals y X is the input and Y is the output and F is a function to illustrate so let's say I'm trying to understand the relationship between the length of the video we produce on our channel and the time that people actually spend watching the video we collect data from some of our videos I mean we have the video duration let's call it X and the watching time let's call it Y and we imagine there is some relationship between them denoted by F after that I inform the Machine about the relationship I expect to see between these two variables I can choose a linear function between X and Y or a nonlinear function this function is what we call a neuron then we can predict the time people would spend watching a video lesson precisely based on our neuron and the video duration now let's see what a neural network is well in one sentence a neural network is a network of neurons it means that we have many neurons and all their inputs and outputs are intertwined and they feed each other in this figure you can see the difference between a neuron and a neural network as you can see a neuron is a basic unit of learning and a neural network is a bunch of interconnected neurons neural networks help us cluster and classify they helped a group data according to similarities among the example inputs and they classify data when they have the output variable in the existing data set to learn from it the questions you may ask at this point will probably be question 1 what kind of problems do neural networks solve neural networks could be applied for spam filtering fraud detection customer relationship management angry customers or happy customers image recognition selfdriving etc question two which functions will I use in each neuron we can use linear or nonlinear depending on the complexity of the problem question 3 what is the architecture of the network we have different types of neural networks a perceptron a recurrent neural network or RNN a convolutional neural network CNN etc now how we can run our neural network in the first place the neural network learn to recognize patterns just like a human we show them examples of correct inputs and outputs in the hope that when we give it a new example input that it's never seen before it will know how to give the correct output that's what we call training on existing data sets don't forget machine learning equals learning from examples let me present you the most basic neural network the perceptron and discuss how it processes inputs and produces an output so suppose we use our neural network for a froot image recognition I have two inputs for that purpose the color and the shape of some fruits and our data sets and a single binary output which is the fruit name once the machine has learned all these properties I can give it a new image of a fruit when it hasn't seen before and it will hopefully classify it correctly and be able to tell me whether it is an orange or a banana the perceptron learns from the existing data and knows which information will be most important in decision making to decide between multiple information it uses something called weights the weights are just numerical representations of these preferences a higher weight means our perceptron considers that input more important compared to other inputs so for our example let's deliberately set suitable weights for our two inputs two for the fruit shape and four for the fruit color now how does the perceptron calculate the output it simply multiplies the input with its respective weight and sums up all the values it gets from all the inputs let's consider that we have two shaped round and long if the shape is round the input one value is one and if it's not round the value is zero we'll repeat the same thing with the color red takes the value of one and the yellow color the value of zero based on this information if the fruit is round in red our perceptron would do the following calculation total equals round times shape weight plus red times color weight so total equals one times two plus one times four equals six this calculation is known as a linear combination now let's see what this value 6 means we first needed to find the threshold value because the perceptrons output is either 0 or 1 0 for a banana and 1 for an orange this output is determined like this if the value of the linear combination is higher than the threshold value then the output is 1 and if it is not the output is 0 so let's say the threshold value is 3 which means that if the calculation gives you a number less than 3 we have a banana but if it's equal to or more than 3 then we have an orange that's how perceptron works it uses a linear combination and produces the output in reality we set the weights to random values and then the network adjusts those weights based on the output errors it made using the previous weights that is called training the neural network in the mathematical language the perceptron algorithms work like this the output is equal to zero if the sum of the weight times the value of the variable is smaller than a threshold in the same way the output will be equal to one if the sum of the weight times the value of the variables is bigger than a threshold to make things just a little simpler for training the threshold is sometimes move to the other side of the inequality and replaced with what's known as the neurons bias now with bias we only need to make changes to the left side of the equation while the right side can remain constant at zero the left side of the equation is a function it is a function that transforms the values or states the conditions for the decision of the output neuron it is known as an activation function the formula above is just one of several activation functions and and the simplest one used in deep learning and it is called the Heaviside step function in reality we can also use other activation functions for example we can use the sigmoid function the tan function and the softmax functions each of them has a purpose and we'll present each of these functions in a special video dedicated just to that that's it for our first introductory video on neural network now you have a solid understanding of the basics of neural networks and how perception works in our next video in neural networks we will explain how they multi layers perceptron works and we will present the concept of hidden layers okay let's present an unsupervised machine learning model the clustering in general and the kmeans clustering in particular clustering models are used to analyze unlabeled data and get useful insights from it clustering is equal to grouping things means the data points which are similar in some way and different from other data points are grouped together each group of the data points is known as a cluster in the example data points are divided into four clusters based on the geometrical distance between two data points the data points which are close to each other are assigned to one cluster this approach of creating clusters is known as distance based clustering another approach of creating clusters is the conceptual clustering in this approach clusters are based on conceptual similarity so for example in a data set of oranges and apples two clusters will be created one for all apples and another one for all oranges this approach uses properties of data points to distinguish one data point from another and to group data points with similar properties together clustering is a very subjective area of discussion it is difficult to determine how one clustering is better than the other one machine learning algorithms learn the insights from the data and apply these insights to create clusters since we don't know those insights it is hard to say the clustering done by the machine learning model is best or not so the user must provide an appropriate criterion to get suitable clusters as an output from the machine learning model consider a data set that contains green color balls red color balls green color apples and red color apples now if we provide color as the clustering criteria to the machine learning model then it will create two clusters one of the green color balls and green color apples and the second one of course of red color balls and red color apples however if the criteria provided is edible then the machine learning model will create two clusters one of all apples and another one of all balls clustering algorithms can be applied in many industries marketing like finding groups of customers with similar behavior or customer segmentation biology for plants classification insurance fraud detection city planning earthquake studies document classification identifying crime localities cyber profiling criminals etc now let me explain to you how K means clustering works kmeans clustering is one of the most useful clustering algorithms it tries to find cluster centers that represent certain regions of the data kmeans clustering is a twostep process one assign each data point to the closest data center to set the cluster center as the mean of the data points assigned to the cluster the algorithm keeps on executing these two steps until the assignment of data points two clusters no longer changes once we decide how many clusters are required the number of clusters is passed to the algorithm so let's say for a given data set we want three clusters in the initiation step the algorithm randomly selects three cluster centers each data point is assigned to the cluster Center it is closest to we then update the cluster Center with the mean of the cluster the previous two steps are repeated until the assignment of data points two clusters no longer changes clustering is similar to classification the only difference is that classification use cases have labeled data I mean the classes are defined and clustering creates various clusters or classes from the data set one of the major benefits of the kmeans algorithm is the implementation kmeans is a relatively efficient method however we need to specify the number of clusters in advance and the results are sensitive to initialization and often terminates at a local optimum unfortunately there's no global theoretical method to find the optimal number of clusters a practical approach is to just compare the outcomes of multiple runs with different K and choose the best one based on a predefined criteria in general a large K probably decreases the error but increases the risk of overfitting you in the previous part of our course we learned the basics of data science and machine learning and the most important machine learning models in practice we can apply manually or automatically various models on a given data set before finalizing and choosing the right model and the more efficient one but how do we evaluate the performance of a model to determine the one model that is performing better in this part of the course we will answer this question by presenting some performance indicators to evaluate the model let's start with the R squared R squared is the most common way to evaluate the performance of a linear model it is a statistical measure of determining how close the data is to the fitted regression line it is a proportion of explained variance to total variance r2 equals explained variance over total variance you can see how it's represented graphically here typically the rsquared value lies between zero and one zero means the model does not explain any variability of the data around it's me one means the model explains all the variability of the data around it's me so the higher the value of rsquared the better the model fits the data and better the model explains the variance of the observed data around its meat however it is difficult to tell whether an increase in the rsquared value is a result of better model performance or more features in the model that's why in multiple linear regression it is better to choose the adjusted rsquare adjusted rsquared is a modified version of rsquared that has been adjusted for the number of features in the model adjusted rsquared penalize is the rsquared if the choice of the feature newly added to model he's not good so in classification scenarios having good rsquared or adjusted rsquared values is not important until both the classes are unidentified with similar accuracy this becomes a big issue when the data is not equally distributed for the targeted classes okay for example consider a data set with 98% observation of Class A and 2% observation of Class B the model can easily get 98% training accuracy by simply predicting that every sample belongs to Class A but the prediction accuracy for Class B is very poor to overcome this situation model performance should be evaluated for each class and then aggregated to get the overall performance of the model confusion matrix provides the right tools to get the individual and overall performance of a classification model so let's take this example with two class positive and negative classes in the figure we have the classifier the prediction outcome and actual values the total number of positive classes is P and the total number of negative classes is n TP true positive in the top left corner of the figure equals actual classes positive and predicted class is also positive this states how many positive classes correctly predicted this is also known as the power of the model FN false negative in the top right corner of the figure actual class is positive and predicted class is negative this states how many positive classes are predicted as negative this is also known as a type 2 error miss FP falls positive in the bottom left corner in the figure actual classes negative and predicted class is positive this states how many negative classes are predicted as positive this is also known as a type 1 error false alarm TN true negative in the bottom right corner of the figure actual class is negative and predicted class is negative this states how many negative classes are correctly predicted so correctly predicted classes is equal to TP plus TN incorrectly predicted classes is equal to FP plus FN actual positive classes is equal to P equals TP plus FN actual negative classes is equal to N equals FP plus TN so we compute the proportion of records that are correctly classified called the accuracy of the classification model and it is calculated as follows accuracy equals T P plus TN over TP plus TN plus FP plus FN so another important validation technique is the cross validation cross validation is a tool that utilizes the training data set in a better way to reduce overfitting and underfitting it is a model validation technique for assessing how the results of statistical analysis will generalize to an independent data set the purpose of using crossvalidation is to increase confidence in the model trained by the training data set so without cross validation our model may perform well on the training data set but their performance decreases when applied to the testing data set the testing data set is precious and should only be used once so the solution is to separate one small part of the training data set as a test of the trained model which is the validation data set okay here are two cross validation techniques kfold crossvalidation this involves splitting the training data set into K subsets of data also known as folds the machine learning model is trained on k1 subsets and then evaluated on the subset that was not used for training this process is repeated k times with a different subset reserved for evaluation and excluded from training each time so once the model has been executed for all training subsets the average of error in each run is calculated and represented as the crossvalidation error leave one out cross validation this is another technique used for cross validation it's a logical extreme of kfold crossvalidation where K equals n the number of observations so for each run only one observation is left with a validation data set this approach leads to higher variation and testing model effectiveness because testing is done against one observation only hence the estimation is highly influenced by the validation observation if the validation observation is an outlier it can lead to higher variation at this last part of the course I will give you some best practices in data science and machine learning okay you need to be aware that the data set that we will get for machine learning problems is not always as clean as we can expect we might have to clean the data set and transform it into a data set that can be used to build a model for this purpose we might have to go through multiple processes like data processing feature engineering and feature extraction feature scaling and selection this set of processes is a major part of a machine learning project because the model performance is based on the data on which the model is trained garbage in garbage out we already talked a little bit about feature engineering and feature extraction I'm going to give you some best practices that we can follow to tackle the data sets and machine learning use cases another important concept is one hot encoding machine learning cannot handle nonnumeric features by themselves so the question that you may ask is how do we transform the non numeric features let's take our example in the employee salary suppose that in the data set gender is maintained as a non numeric feature and this feature can have two values male or female for an organization and employees gender is meaningful information since it is a non numeric feature if we try to train a model on the gender feature then the model will not be able to interpret anything meaningful from it to make the gender column meaningful to the model we must transform it into a numeric column one hot encoding is one of the transformation techniques that can be used to transform categorical features into numeric categorical features so if the feature has only two categories then those two categories can be replaced by 0 & 1 in our employee data set example we can replace male with 0 and female with 1 now consider a scenario where the categorical column has more than two categories in other words in our data set gender have three values male female and unknown so what hot encoding can be extended to take care of this multi category features it creates a new feature for each category and for each observation the value one will be assigned to the newly created feature to which the category belongs to other newlycreated features will be set to zero for example in our employee gender example three new features will be created like in the table binning in a few machine learning scenarios continuous features cannot be used directly to train a model these features should be converted into categorical features and then one hot encoding should be applied to make these features important for the machine learning model and our employee data set example employee age ranges from 21 to 60 years these employees can be categorized into four age brackets 21 to 30 31 to 40 41 to 50 and 51 to 60 the technique of converting a continuous feature into multiple bins and creating a new feature from it is known as binning or bucket ization it's also known as quantization binning transforms a continuous feature into a categorical feature and categorical feature engineering might need to be performed before using this feature in modeling we can create a new age group feature and map each employee with one of these brackets for bidding we can use the domain expertise as well as a few statistical methods to correctly determine the number of Bin's / buckets and the boundaries for each bin so some of the common methods of bidding are one fixed width binning in this technique width is decided for each bin based on domain knowledge rules or constraints to quantile based binning this technique divides the data into Q equal partitions if q equals 4 then the parts are quartiles divide data into four equal partitions three two way ANOVA analysis of variance tests this is used to find similarity between the various data points of the feature these similar data points can be grouped together to partition the data set you first let's discuss what feature engineering is features are the independent variables don't forget we will use this term a lot feature feature equals variable once again independent variables or features are used to predict the dependent variable frequently these features have hidden information that the machine learning models cannot utilize to negate this situation in the data preprocessing stage we just applied the domain knowledge and create new informative features out of the existing features available in the data set these features should be created carefully though otherwise the model may over fit the set of features on which the model is supposed to run should be selected wisely because good features with good quality data can yield a less complex model with better results feature engineering is a recursive process that can be divided into the following steps understand data set brainstorm features create new features validate what impact these features have on the prediction result restart from step one until the desired accuracy and other metrics are achieved so for example consider a model designed to predict salary hikes for employees in an organization the data set contains employee ID geographical location date of birth employment start date career start date etc in this data set date of birth and career start date might not be useful for a data set if we derive two new features for example employee age and years of experience these two features could play a key role in salary hike prediction the process of identifying creating these derived features is called feature engineering in reality though feature engineering is an art and it comes with domain knowledge and experience feature scaling some machine learning algorithms do not perform well when all features in the data set are not on the same scale so coming back to our employee data set example salary may range from $40,000 to $200,000 and age ranges from 21 to 60 days two features do not have the same scale rescaling these features can improve the performance of some machine learning models now how we can make feature scaling we have two common techniques for feature scaling the first one is normalization this technique transformed the feature into a 0 to 1 scale the great thing about normalization is to reduce also the effective outliers the new value of the feature called exchanged is equal to the value of X minus the minimum x value and dividing by the maximum x value minus minimum x value exchanged equals X minus X min over X max minus X min the second one is standardization which transforms the features so that the mean of the distribution becomes zero and the variability becomes one standardization is not significantly affected by outliers look at the figures below they show how the transformed feature will be after implementing normalization and standardization well that's it for this free AI Sciences course hopefully this course has demystified the notion of data science and machine learning that is just the beginning so now that you are familiar with the logic behind the different types of learning you know how to create simple models it's time to move on but you will not be alone on that path we have more content courses and books to reinforce your efforts and guide you at every stage machine learning is a crucial development of today's world the concepts behind it have been around for more than a decade but the age of machine learning and related models such as artificial intelligence data science and more is now the change is just happening and it is fast you've made a great decision to start your journey into the world of machine learning with this free course today the knowledge and the ability to use machine learning is a competitive advantage tomorrow it will be a mere necessity machine learning techniques have already started to change the world of business by creating a new value for data the future will be even more exciting very soon most of the devices and apps that we use daily will be fueled by machine learning algorithms many of them already are now though you have a chance to become a part of this major development Congrats on your decision and don't forget to check out our courses ebooks and contents now as I promised you there is a gift for you it's an ebook available online where you will learn about machine learning you can find it at this link HTTP colon forward slash forward slash WWI Sciences dotnet forward slash gift eBook machine learning we highly recommend that you visit our website AI Sciences dotnet and subscribe to our email list you'll receive all of our books free in an eBook format and you will be informed about all our promotions and offers if you have any feedback please let us know by sending an email to review at AI Sciences net this feedback is highly valued and we really look forward to hearing from you if you enjoyed this video please like it share it subscribe to our Channel AI sciences and make sure you click on the notification button so you can receive a notification when our next course is ready see you at the next video
