00:00 - how we all doing this morning oh I like
00:02 - the woo let's hear some noise get some
00:04 - energy flowing um welcome to day2 of
00:08 - cson I hope you all had an amazing day
00:11 - one uh I was in some of the sessions
00:13 - that were really engaging a lot of great
00:15 - conversation a lot of great questions um
00:17 - a lot of really awesome hallway
00:20 - discussions as well so um look forward
00:22 - to more of that today we're going to
00:24 - hopefully fill your cup today with lots
00:26 - of things to talk about um I heard there
00:29 - were folks that some things last night
00:30 - went to the sphere uh had a chance to um
00:34 - take in some of the sight so hopefully
00:35 - you all got a chance to also participate
00:37 - in the fun the city has to offer us so
00:40 - uh I'm Kim megi I'm the chief product
00:42 - officer with code.org and I'm going to
00:44 - be hosting a panel this morning on the
00:47 - topic of AI and the future of work so
00:50 - we'll get going with that um so as all
00:52 - good talks uh on AI uh du these days I
00:55 - started by playing a bit with AI to help
00:57 - me um prepare for uh this panel
01:00 - um so I prompted AI to basically uh
01:03 - Envision the AI enabled worker of the
01:05 - future uh I'm going to I'm going to read
01:07 - for you what it says here um some of you
01:09 - might not be able to read this so this
01:10 - is chat GPT and Dolly 2 that help me do
01:12 - this so meet Sarah uh whose day
01:15 - seamlessly incorporates AI her morning
01:18 - starts with AI sorting her inbox
01:20 - flagging urgent emails summarizing key
01:22 - points in meetings AI takes notes
01:26 - generates action items and suggests
01:27 - Solutions based on past discussion
01:30 - when she digs into work AI analyzes vast
01:32 - amounts of data offers customer insights
01:35 - forecast future Trends and potential
01:38 - Market shifts empowering her to make
01:39 - informed decisions on opportunities or
01:41 - risks that would otherwise take hours to
01:44 - uncover AI has become Sarah's
01:47 - indispensable partner amplifying her
01:50 - productivity insights and confidence
01:53 - confidence in the modern work landscape
01:56 - so there's some really interesting words
01:57 - there that AI generated for you and I
01:59 - did this in like one shot with a little
02:00 - bit of eding and that image was a first
02:02 - shot image so um not bad I was I was
02:04 - pretty actually was mildly surprised
02:06 - honestly what happened um so we've all
02:09 - seen headlines like these predicting the
02:11 - future of work in an age of AI from job
02:14 - loss to reskilling to automation to
02:17 - rizen Global GDP occurring at the same
02:19 - time the headlines are a wash with
02:22 - fortune tellers and doomsayers on the
02:24 - future of work in an age of
02:26 - AI um I'm sure many of you have also
02:28 - seen data like this don't worry I'm not
02:30 - going to give you a quiz on what the
02:32 - data points say um you probably can't
02:34 - see them but what you should see here is
02:36 - a lot of dots to the right of that line
02:39 - that says average so this is basically a
02:41 - visualization that the Washington Post
02:43 - created based on a research study that
02:46 - looked at the potential impact of AI on
02:48 - various occupations and what they did
02:50 - was they measured the exposure of that
02:52 - occupation given the tasks that the job
02:55 - uh entails and what AI is good at and so
02:58 - here you know um you're probably not
03:00 - going to be surprised management
03:01 - business and finance higher possibility
03:05 - of exposure and impact science and
03:07 - computer science what a lot of folks in
03:09 - this room are talking about um greater
03:11 - chance of impact there healthcare
03:14 - workers another area where we see the
03:15 - potential for impact and education and
03:18 - Social Services things that we might
03:20 - care about in this room um so you might
03:22 - not agree to what extent the state is
03:24 - pointing at I don't know if anyone
03:26 - really does know at this stage but I
03:27 - think we can probably agree that AI will
03:31 - have a meaningful impact on
03:32 - knowledge-based
03:34 - work another set of interesting
03:36 - headlines which we've all seen that
03:38 - compare artificial and human
03:40 - intelligence so here assessments which
03:42 - are you know obviously a key tool for us
03:44 - as a society to ensure that
03:46 - practitioners are skilled and
03:47 - knowledgeable for their future
03:49 - occupations yeah can not now not only
03:51 - match Human Performance but in many
03:54 - cases exceed Human Performance here's
03:56 - some data from um open AI themsel that
03:58 - quantify on a percentile basis where AI
04:01 - would have scored relative to Human
04:04 - Performance so um and then you can also
04:06 - see here uh if you squint the blue is
04:10 - GPT 3.5 and the green adds on what
04:13 - happens when we um sort of throw gp4 at
04:16 - those tools so in this session we're
04:19 - going to discuss the how AI will impact
04:21 - the future of work our goal is to leave
04:23 - you with Food For Thought we're not
04:25 - going to give you the answer to the
04:26 - question because I don't think any of us
04:28 - know we were talking about that this
04:29 - morning so open questions lots of great
04:30 - discussions to have but we are going to
04:32 - leave you with some food for thought and
04:34 - maybe some questions for you all to
04:35 - consider as you bring this very
04:36 - important topic into discussions and the
04:39 - work that you do uh okay so with that
04:42 - I'd like your help in welcoming our
04:43 - panelists to the stage uh and I will
04:46 - start with Nicole Cario who is director
04:49 - of AI and Society at the Allen Institute
04:51 - for AI sh give it up thank you welcome
04:57 - Nicole uh Shena hope who who's the
04:59 - director of education for social impact
05:02 - at
05:03 - Google you guys can make noise come on
05:05 - let's make some noise there we go thank
05:08 - you welcome shenica and Michael trano a
05:11 - visiting fellow at the bkings
05:14 - institution there we go all
05:18 - right
05:20 - awesome so we're gonna start off here by
05:23 - getting to know our panelist a little
05:24 - bit um so I'd love uh to do a warm-up
05:27 - question with you all okay so the
05:29 - question is is what what was your aha
05:31 - moment um with generative AI in your
05:34 - work so specifically what I mean is when
05:36 - did you personally or maybe viscerally
05:39 - realized the potential for AI to change
05:42 - work for good or for otherwise maybe
05:44 - maybe for not good uh I'll let you all
05:46 - decide what what that moment was I'll
05:48 - start with my moment my moment was so I
05:50 - have a computer science and an
05:51 - engineering background but I haven't
05:53 - written code in 15 years so uh and I
05:55 - wanted to do a little little thing at
05:57 - work where I was uh trying to break a
05:59 - little bit of Technology into a um a
06:02 - piece of work that I was doing and I
06:04 - thought well let me let me see what AI
06:05 - can do here and in about an hour which
06:08 - blew me away I was able to essentially
06:10 - solve this problem that probably would
06:12 - have taken me multiple days to kind of
06:14 - res skill myself on the tools of coding
06:17 - and go through that whole process so
06:19 - that was the moment when I was like okay
06:20 - this is going to be a really meaningful
06:22 - change for us so um who wants to go next
06:25 - how about Nicole yeah um I think for me
06:27 - so I work at an artifici intelligence
06:30 - Research Institute and so the teams that
06:31 - I work with are the ones creating
06:33 - technology that's powering a lot of the
06:34 - things that are happening today and so
06:35 - for me it was actually seeing their
06:37 - response um and watching them say oh
06:40 - this is a big giant leap forward um you
06:43 - know our one of our teams at ai2 was
06:45 - focused on their their charge was
06:47 - reasoning and how do we help this
06:49 - technology pass SC an eighth grade
06:52 - science exam for example they achieved
06:54 - that in 2019 that was not that long ago
06:58 - and so to see the Giant leap that um you
07:01 - know chat GPT brought forth and also to
07:03 - see the giant momentum that it had
07:05 - within just pop culture um that's when
07:08 - it was the moment for me yeah I would I
07:11 - would Echo um both of your comments I
07:13 - think um the work that I lead at Google
07:16 - we are focused on understanding those
07:19 - factors that drive or create kind of
07:21 - LeapFrog momentum and ensuring that
07:24 - there is Equitable access uh and
07:26 - participation to Computing and I think
07:29 - when and it was actually in November of
07:31 - last year when chat GPT came onto the
07:34 - education scene we were floored by the
07:38 - rapid and the accelerated uh visibility
07:41 - and adoption um of that that tool and
07:45 - one of the things that we did um is we
07:48 - actually did some analysis around
07:50 - standards alignment work uh specifically
07:53 - focused on trying to understand
07:55 - efficiencies that it could potentially
07:56 - bring into the classroom for educators
07:59 - and I've my my in a previous life prior
08:02 - to Google 20 years in public education
08:05 - I've led a number of curriculum um
08:08 - experiences where we adopting new
08:09 - curriculum standards Etc moving it into
08:12 - public comment we were able to move
08:14 - through a standards alignment um
08:15 - exercise that normally would take months
08:18 - in a matter of hours and that was it
08:21 - just floored us with 80 plus percent
08:24 - accuracy by the way pretty incredible
08:26 - and so for me that was probably one of
08:28 - the first like ah oh my goodness wow
08:30 - this is can be pretty powerful can
08:32 - create a bit of a superpower for the
08:34 - work and the things that we're trying to
08:35 - accomplish uh not just in the classroom
08:37 - but in the workforce as
08:39 - well yeah thanks km uh and thanks
08:41 - everyone uh yeah I mean my personal AA
08:45 - moment um I've been playing around with
08:47 - genit of AI tools for a while uh in ways
08:50 - that I thought could help me be creative
08:52 - uh where I couldn't be because I just
08:54 - didn't have the skills but I think my
08:56 - real aha moment with Chachi BT um you
08:59 - know we all have drudgery in parts of
09:01 - our jobs parts of our lives and I worked
09:03 - at the World Bank for 26 years and I
09:05 - would constantly be asked to do last
09:06 - minute briefings for important people
09:09 - and I got a note uh from someone uh last
09:12 - December and they said you know we've
09:13 - got some important person coming in we
09:15 - need a briefing sheet in the next hour
09:17 - and it was a topic I worked on and but I
09:20 - hadn't worked on it for quite a while so
09:22 - I just had uh my previous had chat GPT
09:25 - summarized my previous work gave me a
09:28 - quick briefing sheet I just um annotated
09:30 - a little bit sent along and I thought
09:31 - you know this is a part of my job that I
09:33 - hate and now I well hopefully won't have
09:36 - to do it as often um and that was really
09:38 - my aha moment I guess I would say love
09:41 - thank you all for sharing those yeah so
09:42 - many interesting uh ways in which we all
09:45 - experience AI in this early stage um so
09:48 - I want to get to sort of one of the hard
09:49 - of the questions we're going to talk
09:50 - about today um but I want to I want to
09:53 - set it up a little bit so this morning
09:54 - we came in a little bit early to to uh
09:56 - just you know mic test and whatnot we
09:58 - got in a nice chat about about um about
10:00 - TV shows that we watch you're going to
10:02 - hear a little bit about that maybe
10:03 - throughout the session one of the TV
10:05 - shows that we were talking about was
10:06 - Black Mirror I I can't I can barely see
10:08 - you all but just even like wave your
10:10 - hand Black Mirror does that have you
10:12 - seen it do are you familiar with it okay
10:14 - so you know kind of a little bit
10:15 - technophobia dystopian view of the
10:18 - future with technology um and so you
10:21 - know I think a little bit of even this
10:22 - conversation might touch on that so I'm
10:25 - going to hearken to that conversation
10:26 - and ask the first question which is
10:28 - overall are you bullish or bearish on
10:31 - the impacts that AI can have on work and
10:34 - and tell us a little bit a little bit
10:35 - about why and maybe shenica if you can
10:36 - lead us off yeah sure I would I would
10:39 - say I am quite bullish about the
10:42 - opportunity of AI in terms of
10:44 - transformation in the workforce um you
10:47 - know we we know and recognize and
10:49 - understand that technology has shaped
10:51 - and changed the anatomy of Workforce um
10:54 - for for for decades but the opportunity
10:57 - of artificial intelligence I think you
10:59 - know this is new for us in terms of
11:02 - really understanding the scope the size
11:05 - the speed the depth of that
11:07 - transformation that's something that
11:08 - we're all still learning and trying to
11:10 - understand together but when you take a
11:12 - look at a lot of the early signals and a
11:13 - lot of the research that's coming out of
11:15 - thing places like the world economic
11:16 - Forum some of the headlines that km uh
11:19 - showed earlier you look at some of the
11:21 - work that Mackenzie has put out what we
11:22 - do what we're finding uh based off of
11:25 - some of these early signals is that in
11:27 - the next 10 years 70% of the workforce
11:31 - Global Workforce will be touched in some
11:33 - way shape or form by generative AI
11:37 - specifically the knowledge work sectors
11:41 - there's based on early signals we're
11:43 - seeing that there will be roughly a 50%
11:46 - impact in terms of productivity and
11:48 - efficiency in the knowledge Workforce
11:51 - and when we say knowledge these are
11:53 - typically the careers that are typically
11:55 - mid High skill typically High wage um
11:59 - where you often times see or uh the
12:02 - types of skills that are being asked of
12:03 - knowledge work uh is that of synthesis
12:07 - analysis um pulling together information
12:10 - uh design in terms of like first drafts
12:14 - generative AI based on the early signals
12:16 - that we're seeing will absolutely have a
12:18 - s substantial impact on those kinds of
12:20 - SE on those sectors um and that's also
12:23 - tied to what generative AI actually is
12:26 - it's based on large language models that
12:28 - are pulling in a a lot of information
12:30 - using algorithms that are leaning in on
12:33 - probability and statistics to try to
12:36 - predict the next set of the next words
12:39 - sets of words and so when you think
12:41 - about that and you think about what the
12:43 - knowledge sector does it would make
12:45 - sense that generative AI would have
12:47 - implications in the knowledge sector
12:49 - probably uh at at at greater
12:51 - implications than in other spaces and
12:53 - again when I say transformation or
12:55 - change we're talking about productivity
12:56 - and efficiency and so that's not the
12:58 - create fear for those types of
13:01 - Industries what it really is it's to
13:02 - frame it to help us to understand that
13:04 - the parts of the work that are that can
13:06 - be unique to the human it frees us up to
13:10 - focus in on more of the complex and more
13:12 - sophisticated and nuanced aspects of our
13:14 - work and take uh Leverage The Power of
13:17 - artificial intelligence to help with
13:19 - automation love that point yeah awesome
13:22 - uh Nicole Michael your thoughts bullish
13:24 - or bearish yeah so um yes yes yes okay
13:30 - and I'll say I'm so I've worked for you
13:32 - know quarter Century in institutions
13:34 - filled with economists I'm not one uh
13:36 - and with labor economists I'm not a
13:38 - labor Economist so uh some of my
13:40 - colleagues will no doubt take issue with
13:41 - some of the things I say um but I know
13:44 - that they'll take would take greater
13:45 - issue with things that they say uh
13:47 - between each other given the way
13:48 - economists are so with that caveat out
13:50 - of the out of the way um I mean I'm
13:52 - bullish for Education yeah I mean I
13:54 - think the potential Returns on education
13:56 - and the importance and centrality of
13:57 - Education I don't see how
13:59 - um uh how not to be bullish with that um
14:03 - on the flip side um you know inequality
14:06 - which is the defining feature of our
14:07 - society our societies around the world
14:09 - today and the real worry I think
14:11 - legitimate worry is that we're going to
14:13 - be increasingly polarized as there are
14:15 - increased returns to certain types of
14:17 - Labor what are we going to do as a
14:19 - society with people who are whatever
14:21 - Left Behind who lose jobs I I'm overall
14:24 - bullish but of course as with any sort
14:26 - of automation or with techn olical
14:29 - advances they're going to be certain
14:30 - groups they going to be certain
14:31 - geographies who really suffer a lot and
14:33 - with what's happening with AI generative
14:35 - AI now um some of this will happen
14:37 - quickly I think most of it will happen
14:39 - over the medium term you know with uh
14:42 - autonomy and vehicular autonomy you know
14:45 - there was a big prediction that oh truck
14:47 - drivers are going to go away and that's
14:48 - going to well I mean truck driving
14:50 - typically isn't the greatest it's it's a
14:52 - tough job yeah and in the 10 years since
14:55 - a lot of big predictions were made about
14:56 - that you know we've had huge um we need
14:59 - more truck drivers we don't have enough
15:00 - and some of the things we can automate
15:02 - away would be great to automate away so
15:04 - I'm both bearish and bullish but
15:06 - recognizing that um the impact as you
15:10 - know since the uh Weavers in Silesia um
15:13 - you know destroyed their Looms in the in
15:15 - the early 1900s it's really going to
15:17 - impact 1800s impact people uh in very
15:20 - different ways love that balance that's
15:22 - an awesome uh awesome way to answer the
15:24 - question Nicole how about your thoughts
15:25 - yeah I agree with the yes I feel like
15:27 - I'm kind of in the middle on certain
15:28 - things in in the you know on one side
15:30 - and the other I think my challenge is
15:32 - that there are you know the tool is the
15:34 - tool it's not the thing that's going to
15:35 - cause disruption it's the decision
15:37 - makers that are going to cause a
15:38 - disruption and I'm not sure how much
15:40 - faith I have in the system that we've
15:42 - seen fail before on other technology
15:44 - technological advancements so what I
15:47 - think is that there's huge potential and
15:50 - huge opportunity and we as a society are
15:53 - at this Reckoning moment where we have
15:55 - to view it as this real turning point so
15:58 - I think think um you know there are you
16:00 - know to the point about autonomous
16:02 - vehicles truck drivers if they are
16:04 - displaced that's then what will they do
16:07 - right so I agree it's a hard job and
16:09 - it's a job that we have a lot of people
16:10 - in and so there's a lot of decisions
16:12 - around um you know social safety nets
16:14 - and things that we don't always have
16:16 - these really rich conversations about I
16:18 - also think you know college costs are
16:21 - rising and rising and Rising um so if
16:24 - we're asking people to now take on more
16:26 - college debt or asking people to get
16:28 - more education then we're kind of
16:30 - setting people up to fail so I'm
16:32 - pointing this out because I agree with
16:34 - all of the things you said but I I also
16:36 - go back to the um you know how are we
16:39 - making sure that this technology is
16:41 - distributed in a way that is Equitable
16:43 - and isn't driving these bigger divides
16:45 - and that is you know when we're relying
16:48 - on a government that's not the most
16:50 - functional to make some of these choices
16:53 - and we're relying on businesses who are
16:54 - incentivized by their financial returns
16:57 - to make some of these decisions um
17:00 - that's where I get that is what I am
17:02 - concerned about not the technology I
17:04 - think the technology can do so many
17:06 - things so yeah if I could just quickly
17:09 - build on that thanks Kim um Nicole makes
17:12 - an an incredibly important point and we
17:13 - had an earlier discussion as part when
17:15 - we first came in to do the walkthrough
17:17 - uh Beyond just the Golden Girls and the
17:19 - the the mirror uh um television program
17:23 - but we we also talked about to to that
17:25 - very point that the technology and the
17:27 - potential of the technology is
17:29 - absolutely incredible um it it can in
17:32 - fact create a bit of a superpower for
17:34 - the strengths that a human and and our
17:36 - knowledge um that we can bring to bear
17:38 - to any any sector right but it's going
17:41 - to take a collective Coalition and
17:43 - that's all of us in the room public and
17:45 - private to ensure that we're putting
17:47 - sufficient safeguards around the
17:49 - technology so that as we the as with any
17:53 - technology as it begins to to be adopted
17:56 - into uh Society that there are we are
18:00 - thinking intentionally and early about
18:02 - how do we make sure that we're upscaling
18:04 - people sufficiently how are we making
18:06 - sure that we can help PE individuals
18:08 - understand the technology is augmenting
18:09 - your work now in this new way and so
18:12 - we're also going to invest back in in
18:14 - you to ensure that you can use the
18:15 - technology so that you can make that
18:17 - necessary pivot I also took a point
18:19 - about the cost of Education one of the
18:21 - things that we were talking about is how
18:23 - the technology and through the education
18:24 - space there's power and potential in
18:27 - using it as a of a co-pilot and for
18:29 - tutoring and so that you can integrate
18:31 - it so it's real time and near time one
18:33 - of the things that uh one of the
18:34 - economic reports I think the world for
18:36 - and I think it was actually one of the
18:37 - headlines that that showed that one of
18:39 - the things that we were trying to figure
18:41 - out uh back to the early signals and
18:43 - just the implications on the workforce
18:45 - they're going to be four primary sectors
18:47 - where we see 70% of some of that
18:49 - transformation happening and again these
18:51 - are early signals so you know let's be
18:54 - let's let's be you know let's take it
18:57 - for what it's it is but those four
18:59 - sectors are going to be around customer
19:01 - service and support marketing and sales
19:05 - software development and R&D again these
19:09 - are knowledge knowledge creation sectors
19:12 - and what the early signals are telling
19:14 - us in those four sectors is that yes
19:17 - there's going to be pivots that are
19:18 - going to be required to happen in these
19:20 - in these industries but it's not about
19:22 - displacing people it's about a clear
19:24 - recognition that pivots must happen but
19:28 - we need to di to move that human the
19:30 - unique human strength on the more
19:33 - nuanced complex complex aspects of the
19:36 - work and so I really believe that's what
19:38 - we have to lean in on is training people
19:41 - and readying them for the the the the
19:44 - understanding that this technology is
19:46 - going to augment the way of work that's
19:49 - a great point and it's also a really
19:51 - good lead into where i' love to take the
19:52 - conversation um about sort of Skilling
19:55 - displacement and so um so the question I
19:57 - want to POS POS is you know we've heard
19:59 - a lot about job loss right um at least
20:02 - headlines talk about job loss when you
20:03 - get a little bit more under the hood and
20:05 - start to look at both the research and
20:07 - and and some of the the details behind
20:10 - um the headline you start to see that
20:11 - there's there's more Nuance in this
20:13 - conversation so but I'm going to ask the
20:15 - non- Nuance question let you all do the
20:16 - nuancing part so okay do you believe
20:19 - we'll see job
20:20 - creation for job loss what what where
20:23 - are you at on this and and Nicole maybe
20:25 - you can start us off on this one yeah um
20:26 - I think we're going to see both and
20:28 - going to be different based on the
20:30 - sector and it's going to be different
20:32 - based on the skill level um and it's
20:34 - also going to be different based on
20:35 - globally where you are because I think
20:37 - you know I I often tend to as a US
20:40 - citizen over index on the us-centric
20:42 - approach to this and so I will try to be
20:43 - mindful of that um and I have a great
20:45 - example I'll share later about brilliant
20:47 - work happening in the global South that
20:49 - we create um in fact I'm just going to
20:51 - go there now um job creation so data is
20:56 - so valuable if we don't have data we
20:59 - don't have ai and so there is a
21:01 - tremendous amount of work around data
21:04 - collection curation annotation that's
21:06 - happening um the example I was going to
21:08 - share is there's an amazing organization
21:10 - out of India called karia and they are
21:13 - moving generations of people out of
21:15 - poverty by paying them an above living
21:19 - wage to share their data which is such a
21:22 - valuable resource I think for so long
21:24 - we've heard around about data privacy
21:27 - and we've heard about you know you be be
21:29 - careful of your data um and that is
21:31 - still all true but at the if you speak a
21:34 - language that is not heavily resourced
21:36 - in the data um if you speak a language
21:39 - that is in danger of going away you are
21:42 - now like the most valuable golden shiny
21:44 - object because data is so valuable so I
21:47 - think there's there is a huge um need
21:50 - and job creation opportunity there um
21:54 - Kia is doing incredible work finding
21:56 - it's it's largely women that they're
21:57 - working with with these women are now
21:59 - making more than their husbands they're
22:00 - having to have secret bank accounts so
22:02 - that they can put money it's tremendous
22:04 - what is happening and the confidence and
22:06 - the empowerment that is coming with this
22:09 - is literally life-changing and it's
22:11 - changing the face of these communities
22:13 - and they're expanding throughout the
22:14 - global South to capture more and more
22:16 - languages so that is a tremendous job
22:19 - creation
22:20 - opportunity um I would also say you know
22:22 - I have friends who are digital creatives
22:25 - who have you know they're on their 400th
22:28 - thousandth application and they're not
22:29 - getting a job and so we can't Overlook
22:33 - the impact of certain industries um a
22:36 - creative you know that I worked with for
22:37 - years and years who is a brilliant
22:39 - person in her career um and advanced in
22:42 - her career is just like I now have to
22:44 - make a change I this is no longer the
22:46 - career for me so if we can get ahead of
22:49 - helping people see when their job is at
22:52 - risk because there are jobs at risk and
22:54 - helping them res skill upskill find new
22:58 - things that has to be a priority now
23:00 - because the the Technology's
23:02 - capabilities today are not such that you
23:04 - should be firing your copywriting
23:07 - teams five years from now it might be I
23:09 - mean five who knows when that's going to
23:11 - happen but that's that's coming where
23:13 - you know you won't have to fact check
23:15 - your briefing as as well um that that
23:18 - time is coming and so how do we get
23:20 - ahead of it and that's where we'll get
23:22 - into more job creation um and so to your
23:25 - to answer your question it's going to be
23:26 - both and I think it's going to have
23:29 - tremendous impacts in both so I think
23:31 - that context point is really important I
23:33 - love the example really good good use
23:35 - case to understand that it's not a it's
23:37 - not a yes or no it's a timeline it's a
23:40 - yes and it's both that are going to
23:42 - occur depending upon context so U
23:44 - Michael Shenika how would you like to so
23:47 - I think so who knows right uh the
23:50 - problem with predicting the future is we
23:52 - don't know it yet um I'm I'm confident
23:55 - in stating that I think people who you
23:57 - use AI in their jobs will replace people
23:59 - who don't use AI in their jobs I mean
24:01 - that's one general statement I would
24:03 - make um you know uh you know I talked to
24:06 - business owners and and they they might
24:09 - say um you know look at what chat gbt
24:12 - and these tools um look all the
24:14 - marketing copy copy that they can put
24:16 - out I don't need as many people to do
24:18 - marketing I tend to
24:19 - think I I I don't think you know there's
24:21 - a sort of the the um the lump fallacy
24:25 - that there's only so much work that can
24:26 - be done I think people do more marketing
24:29 - yeah and I think um that there are all
24:32 - sorts of jobs you know when ATMs came
24:34 - out they said that's that you know we
24:36 - won't have any more bank tellers and
24:38 - what happened for 30 years is we had a
24:40 - rise in the number of bank tellers
24:42 - because we could put banks in all sorts
24:44 - of new places and supplement them with
24:46 - and we saw we 10 years ago we heard
24:47 - about radiology and don't have your kid
24:50 - become a radiologist because that's
24:51 - going to be obsolute we have more
24:52 - Radiologists today than we do did 10
24:54 - years ago who knows what it'll be in in
24:55 - 20 years um so just another I guess
24:58 - caution about making too many
25:00 - predictions but uh you know I think this
25:02 - idea that uh so the Scottish Economist
25:06 - um you know um William Stanley jivon
25:08 - talked about a paradox when he saw that
25:10 - um when the steam engine was created and
25:13 - it made the use of coal much more
25:14 - efficient the idea is okay then we won't
25:16 - need to use as much coal and they saw
25:18 - the exact opposite we're using coal much
25:19 - more I tend to think with a lot some of
25:21 - the jobs that we're talking about um
25:23 - that won't be replaced it will just
25:25 - allow us to do more of that and and the
25:28 - challenge I think of fear of a lot of
25:30 - people is that um you know the first
25:31 - wave of it automation took away um a lot
25:35 - of uh or threatened a lot of jobs that
25:38 - relied on uh routine cognitive tasks
25:42 - yeah those could be and now we're
25:43 - worried it's coming for the the quote
25:45 - unquote real knowledge workers who are
25:47 - doing non-routine cognitive work um
25:50 - maybe I think it may be we may see a
25:53 - redefinition of what um non-routine
25:55 - cognitive work means you know if you're
25:57 - graphic designer yeah and I just need to
25:59 - hire somebody to give me a picture of a
26:01 - mountain fair enough uh machine can do
26:03 - that is that a non-routine cognitive
26:06 - task or are we going to change our sort
26:08 - of definition how we think about these
26:09 - things yeah I love the the the point
26:11 - you're you're making about redefining
26:13 - work and that's where I want to go next
26:14 - actually I think this is a really
26:15 - critical uh topic for this group to be
26:18 - discussing um with with that before we
26:20 - jump into that next question what I'd
26:22 - like to do is share a video clip um from
26:24 - a professional learning series uh that
26:26 - code.org created uh partnership with KH
26:28 - Academy IST and ETS it's called AI 101
26:31 - for teachers uh and and the clip is
26:34 - going to be a conversation between Sal
26:36 - Con uh from KH Academy and um Cod org
26:40 - CEO hadti partovi and they're going to
26:41 - talk a little bit about the industry
26:43 - impact um on AI and it touches on this
26:45 - question so um if you can queue up the
26:47 - uh industry impact
26:52 - video C of AI is going to touch every
26:56 - industry at at every every possible part
26:58 - of it now I do want to make clear a lot
26:59 - of people have started to differentiate
27:01 - which I think is an important thing
27:02 - between jobs and tasks I think it's
27:04 - going to allow a lot of folks to do more
27:06 - tasks more productively but it's not
27:09 - going to replace in many cases whole
27:11 - whole jobs so pretty much any job where
27:13 - you have to do any type of writing I
27:16 - think you're going to have a
27:16 - productivity Improvement at least 2x
27:18 - probably more any job where you have to
27:21 - create presentations you have to create
27:24 - any type of artifact uh probably any job
27:28 - that involves working with a computer in
27:30 - any way is going to become in some way
27:33 - not replaced by AI but Easier by AI
27:37 - which means whether you're writing code
27:39 - whether you're reviewing contracts if
27:42 - you're writing a patent if you're
27:44 - reviewing a patent if you're creating
27:46 - marketing if you're posting to social
27:48 - media uh if you're WR creating a
27:50 - spreadsheet or analyzing data with a
27:53 - spreadsheet everywhere from marketing to
27:55 - sales to accounting you name it
27:58 - AI isn't going to replace your job it'll
28:00 - make your job easier because certain
28:02 - tasks that are repetitive will become
28:03 - completely automated certain tasks where
28:06 - you might have writers block AI might
28:08 - make the first draft for you or help you
28:10 - out or give you Corrections uh to
28:13 - basically reduce the time you need to
28:14 - spend to get the job done one of the
28:15 - things is important is when you think
28:17 - about how much all digital work is going
28:20 - to be impacted by AI is recognizing how
28:22 - critical it is that our education system
28:25 - teaches students what this is is how to
28:28 - use it how it works and what are its
28:30 - risks what are its strengths and
28:32 - weaknesses because right now most adults
28:35 - just treat it as a sort of magical tool
28:37 - like oh it just came out and it's magic
28:39 - uh but it's important to actually
28:41 - understand the underpinnings of what is
28:43 - a large language model realizing that
28:45 - it's using statistics to generate its
28:47 - thoughts to realize that that
28:49 - statistical generation is why it can
28:51 - make mistakes uh and recognizing that
28:54 - it's been trained on data so the data
28:56 - that it's used to figure out what it's
28:58 - creating could have biases could have
29:00 - you know missing gaps in the data things
29:03 - like that uh it's important for students
29:06 - to as they get prepared for this
29:08 - Workforce of the future to know how the
29:11 - technology they're using in these future
29:14 - jobs is actually
29:17 - working so um so I'd love to pick up uh
29:20 - on both uh the conversation s and how
29:22 - had and and Michael's last Point um and
29:25 - Shena maybe you can lead us off on this
29:27 - so how how do you see the way in which
29:29 - we work changing like if we think about
29:31 - redefining what work means um kind of uh
29:34 - even though we're not suers go ahead and
29:37 - and look into that crystal ball a little
29:38 - bit so I'll wave the wand um no and but
29:43 - but seriously I
29:44 - think I think look we we as a collective
29:48 - Society have have been here
29:51 - before our our society has seen first
29:55 - wave uh our society has encountered
29:57 - inventions new
29:59 - technology um and our society has
30:02 - learned to adapt and
30:04 - adjust I have a lot of Hope and Faith
30:07 - and in us as society and our Collective
30:10 - Global society that said when I think
30:13 - about the future of work I think it's
30:15 - exactly what um what both Sal and and
30:19 - hottie mentioned I see the technology
30:22 - and every all the signals that we see
30:23 - today is that the technology is going to
30:25 - be used as a collaborator and as a
30:27 - co-pilot um to help create greater
30:30 - efficiencies to help increase
30:32 - productivity and it's going to augment
30:34 - the things that we as humans do to make
30:37 - us more efficient to allow us to Pivot
30:39 - our energy our and our our uniqueness as
30:42 - humans to a task allow us to really
30:45 - index and focus our time and effort
30:47 - there um that's what I see in terms of
30:50 - the transformation based on all of the
30:51 - signals that we see I do believe that it
30:54 - will be incumbent on us to lean in on on
30:57 - the basic building blocks that we know
31:00 - happen in the early years the formative
31:03 - years we need to think critically we
31:05 - need to understand how to communicate
31:07 - effectively we need to understand how to
31:09 - collaborate into team um that those you
31:12 - know the 21st century skills when I was
31:14 - a teacher in the classroom these are the
31:16 - things those building blocks will not
31:19 - change and we must help our young people
31:22 - have a very strong foundation around
31:25 - computational thinking right because
31:27 - again these are tools these tools are
31:30 - also created by flawed individuals human
31:33 - beings and so it's incumbent on us as we
31:35 - think about today's Workforce and the
31:37 - future Workforce is that we're
31:40 - teaching students and professionals to
31:44 - interrogate the outputs from the
31:47 - technology the technology is a
31:48 - collaborator and a co-pilot that's what
31:50 - I believe and see I see it as augmenting
31:53 - the strengths that are uniquely human
31:56 - love that point
31:58 - Nicole yeah I would Echo the um
32:01 - executive functioning skills as like the
32:03 - North Star I said this in a panel
32:04 - yesterday we you know I would amend
32:07 - hotti statement slightly by saying
32:09 - people need to know how to use this
32:10 - technology and understand this
32:12 - technology they also have to understand
32:13 - how to exist with this technology
32:15 - because it's not going away and that is
32:17 - the executive functioning skills that
32:18 - should be so front and center um I'd
32:22 - also say this is so important of a
32:25 - moment where AI literacy comes in
32:27 - because if we don't understand the
32:29 - capabilities of the technology then we
32:31 - may be laying people off who don't need
32:32 - to be laid off because we're over
32:34 - indexing and putting too much trust in
32:36 - the technology so I the the AI literacy
32:39 - component of this just can't be
32:41 - understated um if we if people are
32:44 - tasked with you know bringing big
32:47 - returns to their shareholders then
32:49 - that's their charge and if they think
32:51 - that the way to do that is not by
32:53 - augmenting but by supplementing or
32:55 - replacing then that's what's going to
32:56 - happen so there's a literacy an AI
32:59 - understanding what can the technology do
33:01 - the challenge with that um I did a study
33:03 - in 2021 of you know people's
33:06 - understanding of AI capabilities and
33:09 - surprisingly we found you know only 16%
33:11 - had a a passing understanding of AI
33:13 - literacy I'm redoing that survey now and
33:16 - I often consult with people across the
33:18 - globe that I collaborate with and we are
33:20 - getting so into the details on does AI
33:22 - understand cause and effect this person
33:24 - says yes this person says no so it's
33:27 - even within the field of AI there are
33:29 - these big questions about the current
33:32 - capabilities um and so we need to solve
33:35 - for a general level of understanding so
33:38 - that people can really make smart and
33:40 - informed choices with how they're
33:42 - deploying how they're collaborating um
33:44 - and how this technology is is being used
33:47 - can I just jump in really quick before
33:48 - Mike answers I I just want to build off
33:50 - a point that Nicole made thank you
33:51 - you're gracious to me um you know when
33:54 - you think about in the space of
33:55 - mathematics this thought came to mind
33:57 - and and our one of our leaders in Google
33:59 - Maggie Johnson um a who understands
34:02 - deeply computer science but also
34:04 - Computer Science Education and one of
34:06 - the points that she she recently made
34:08 - which I thought was exactly right which
34:09 - is think about in the space of
34:10 - mathematics when the you know the ad you
34:13 - know the calculator coming into the
34:15 - classroom and and just Mass adoption um
34:19 - it didn't wipe away our need to
34:20 - understand how to add and subtract what
34:22 - it did do is it brought us together to
34:24 - understand that we need to have a a
34:26 - number sent what is the number sense the
34:29 - basic knowledge that every individual
34:31 - needs to have in light of these tools
34:33 - that are available to us in mathematics
34:35 - I think it's a similar thing back to
34:37 - your point about artificial intelligence
34:39 - um AI literacy we need to have a an AI
34:44 - sense that is generally understood so
34:47 - that we understand what this is that
34:48 - it's not magic and by the way it is
34:51 - flawed right and so with us having a a
34:54 - baseline understanding of AI back to to
34:56 - your literacy point it will set us up
34:58 - well to answer some of these harder
35:00 - questions around things like what values
35:02 - are being placed that are invisible into
35:05 - the algorithms and into the technology
35:07 - and into the infrastructure so these are
35:09 - the things I cannot stress enough your
35:11 - point Nicole about the literacy Baseline
35:13 - it's it's a
35:15 - must so um it's about time for Q&A but
35:19 - um thinking you know I'm worried that a
35:22 - lot of the discussions around Ai and the
35:25 - workforce quickly be discussions around
35:28 - efficiency yeah and so there are
35:29 - efficiency arguments and effic
35:31 - efficiency analyses we can do and we
35:32 - should be doing um but um I mean all of
35:35 - us here were involved in the grand
35:37 - experiment in remote learning and remote
35:40 - work uh during the fabulous thing we all
35:43 - went through a couple years ago and I
35:45 - don't know hopefully very few or none of
35:47 - you in this room uh had new software put
35:51 - on your computers AI enabled software to
35:54 - essentially surveil what you were doing
35:56 - uh in the uh the worry that you weren't
35:58 - working or that and and then people say
36:02 - oh we can look at this now we can
36:04 - actually measure efficiency of our
36:05 - workers in new ways um My worry is that
36:08 - when some of these AI tools enable uh us
36:11 - to be treated like machines ourselves um
36:14 - those are the exact types of function
36:15 - type work that can be automated away and
36:18 - I think you know educ we're all
36:19 - Educators in this room I mean education
36:21 - is fundamentally a human endeavor and in
36:23 - talking with uh policy makers around the
36:26 - world
36:27 - especially in places where there aren't
36:28 - teachers or there are insufficiently
36:30 - supported teachers and Ministers of
36:32 - Education have said to me I mean we
36:34 - don't have teachers in these places or
36:35 - they're young or they they they don't
36:39 - know their subjects very well and
36:40 - wouldn't be great if AI could complement
36:42 - and maybe um where they don't exist
36:44 - where they could do the job of teachers
36:46 - and it's yeah okay yeah there's there's
36:48 - a there's a place um for that but um My
36:51 - worry is that the you talked about the
36:53 - next digital divide and education is
36:54 - about access to AI who has access to it
36:57 - and who doesn't My worry is that it'll
36:59 - become about who has access to people
37:00 - and who only has access to technology
37:03 - and hopefully that's not the world that
37:04 - we're going towards and if we are we
37:07 - have we well we will always have time
37:09 - hopefully but we have time now to think
37:10 - about what we might want to do to
37:12 - prevent that you're describing a Black
37:13 - Mirror episode yeah exactly yeah we we
37:17 - to bring it home Michael thank you
37:18 - describing actual conversations I've had
37:20 - with Ministers of Education uh
37:21 - addressing very real needs
37:24 - today and you know tomorrow's needs are
37:27 - subordinate to today's needs can I just
37:29 - add one point too um even if you're
37:31 - relying whatever you're relying it on it
37:33 - for today it's also predominantly
37:36 - English it's predominantly developed by
37:38 - a very specific group of people and so
37:40 - depending on where you know you're
37:41 - you're over indexing on a certain group
37:43 - already because again the technolog is
37:45 - not Advanced enough to be
37:46 - representational and diverse and so
37:49 - efficiency for who what does that even
37:51 - look like so yeah that's awesome um so I
37:53 - do want to make sure we have a little
37:54 - bit of time for some Q&A from you all
37:56 - but I'm going to ask for one last
37:58 - sentence from each person and the
37:59 - question when it POS is uh actually it
38:02 - picks up even on this last uh discussion
38:05 - so we have a a room full of very
38:08 - influential people here that can steer a
38:11 - good portion of the discussion we've
38:13 - just had what the future of society and
38:14 - the future of work looks like to the
38:16 - work they do uh in education what's a
38:19 - piece of advice you're going to you're
38:20 - GNA give to these folks and maybe Nicole
38:22 - you can lead us off yeah um I would say
38:25 - there
38:27 - I I said this yesterday on a panel and
38:29 - I'll say it again as a start like ask
38:30 - the dumb questions because somebody else
38:32 - will have them I ask dumb questions all
38:35 - the time um because this technolog is
38:37 - new there's not consensus around a lot
38:39 - of things and so if you're using a tool
38:42 - and it seems to be underperforming or
38:44 - overperforming for a specific
38:45 - demographic call that out like pull the
38:48 - fire alarm these are the things that I
38:50 - think are so important as this
38:51 - technology develops and becomes more and
38:53 - more ubiquitous and then make sure that
38:56 - there are spaces that are being created
38:58 - for learning about this technology about
39:01 - the potential about the use cases um you
39:04 - know it's it's a tool it's it is it's a
39:06 - tool to be used it shouldn't be the
39:09 - dictator it shouldn't be the the place
39:11 - of a human there should always be a
39:13 - human decision maker at the end um of
39:16 - the use of a tool like this thanks
39:18 - Nicole Shena yeah I would I would go to
39:21 - the point of a coalition um it's going
39:23 - to take all of us in this room um public
39:26 - priv
39:27 - to collectively explore the
39:29 - opportunities of this technology but
39:32 - also to put sufficient safeguards in
39:34 - place to ensure that we're harnessing
39:36 - the technology for good um and so that's
39:39 - what I would say we need Collective
39:41 - action together to solve for this it's
39:43 - not for engineers to solve it's not for
39:46 - you know uh government to solve we have
39:48 - to do it do it together the last thing I
39:51 - would also say is there's I know that
39:53 - there's also some energy around fear
39:55 - around this techn technology and to to
39:59 - coin something I'm going to terribly
40:00 - paraphrase Marie cury um but her point
40:03 - this point that she made in a quote
40:04 - which is fear is can be driven out
40:07 - through knowledge and so take the time
40:10 - to build your understanding of the
40:11 - opportunity and get caught doing doing
40:15 - the work to try to make sure that we're
40:17 - putting the safeguards in place together
40:20 - so that we're not leaving anyone behind
40:23 - yeah Nicole said if you're using the
40:24 - tools I mean hopefully you're using the
40:25 - tools yeah I mean that's the big deal um
40:28 - I was talking with an expert Ai and
40:30 - education expert and he was talking me
40:32 - about some of the image Generation stuff
40:33 - and talking about using mid journey and
40:34 - I mentioned Discord and he had no idea
40:36 - what I was talking about I'm wondering
40:38 - what this guy is actually doing I later
40:39 - found out that you know he was an expert
40:41 - in crypto uh year and a half ago and so
40:43 - there are people moving into these SP
40:44 - but just you know I put aside an hour a
40:47 - day which I don't have literally to just
40:49 - be immersed in tools I mean immers in
40:51 - the tools all day long anyway but um if
40:53 - this new world of work is Dawning on us
40:56 - and and it's going to happen we we may
40:58 - as well be in it as soon as we can and
40:59 - just just use the stuff ask dumb
41:01 - questions and and and look dumb too
41:03 - because you got to look dumb before you
41:05 - look less
41:06 - dumb well we'll we're gonna we're going
41:08 - to end on that note so you all have your
41:10 - called to actions out there and uh let's
41:12 - go to some uh some Q&A from yall we've
41:14 - got just under 10 minutes so and I can't
41:17 - see anything so if the lights can go up
41:19 - that'd be great hi looks like we have a
41:22 - sorry I think they've handed me a mic so
41:24 - I'm going to start way over here on the
41:25 - corner oh my God there you are hello hi
41:27 - uh my name is leizer I'm the executive
41:29 - director of computer science for all um
41:32 - and I love the conversation thank you so
41:34 - much for thinking about this
41:36 - intersection of um work which is
41:39 - actually the foundational way many
41:41 - people pay to live in this country and
41:44 - exist with some kind of subsistence um
41:47 - and I really love that notion of
41:48 - computational thinking um combined with
41:51 - computational questioning it's really
41:53 - important that we understand not just
41:56 - that technology can be right or wrong
41:59 - I'm curious about how we bring into the
42:01 - conversation how it's wrong the notion
42:04 - of instead of an error rate an error
42:06 - Matrix right if you're trying to detect
42:10 - breast cancer you want it to be wrong
42:13 - more saying somebody might have a
42:15 - problem than saying they're all clear
42:18 - right just like if we're trying to leap
42:19 - over a Chasm we'd rather leap too far
42:22 - than Too Short right and so how do we
42:25 - bring the conversation of how it's wrong
42:27 - not just that it's wrong into the bigger
42:32 - picture
42:34 - thoughts I I mean so how do we do one I
42:36 - think is one thing we do is we insist in
42:39 - various ways or agitate for evangelize
42:41 - for um tools that would help us
42:44 - understand what these tools are actually
42:46 - doing so things transparency if we are
42:49 - going to um have set up these matrices
42:51 - and be I mean to actually understand
42:53 - what's happening understand where there
42:55 - might be I mean alignment problems
42:57 - imagine everyone in the in the room is
42:59 - familiar with that term I mean I think
43:01 - one thing that we can all do
43:02 - collectively and that we can only do
43:04 - collectively really is educate uh um for
43:07 - more transparency um you know this there
43:10 - was a tool released maybe early this
43:12 - week or last week around transparency of
43:14 - some of these AI tools that found them
43:16 - all basically wanting in ways that were
43:18 - both subtle and profound and I think
43:20 - absent some of uh ability to have
43:22 - insights into what's happening that will
43:24 - constrain our ability to do exactly what
43:26 - you
43:28 - suggest yeah I would add too um there
43:31 - has to be more space and patience for
43:36 - when things come out as wrong and the
43:37 - example that I go back to is you know
43:39 - Amazon we all heard Amazon had a hiring
43:41 - tool that they were they were developing
43:44 - that was over indexing on rejecting
43:46 - women and people of color because if you
43:48 - said you were part of a specific team or
43:51 - you said the word women in your resume
43:52 - it was just it was over indexing on
43:54 - rejecting those applications the tool
43:57 - was a brilliant case study in bias in
44:02 - the it wasn't rolled out so they didn't
44:03 - use it and it was instead of them diving
44:07 - in and like publicly having that
44:09 - conversation it was like the you know
44:11 - the pr team or whoever said you know we
44:14 - got to shut it down and we need more of
44:16 - those learnings because that is a
44:18 - brilliant case study in so many
44:20 - different types of learning and you know
44:22 - right now ai can read a lung cancer um
44:26 - or you know give a lung cancer diagnosis
44:28 - better than a radiologist that's one of
44:29 - the things it does very well um and to
44:32 - get there it had to be wrong a lot and
44:34 - it had to be right a lot and so you know
44:36 - those are the things that are sort of
44:38 - happening behind the scenes but the more
44:39 - we publicly share those I think the
44:41 - better and play around if you ask chat
44:44 - GPT today and you don't have the extra
44:46 - boosted gp4 version um you're very
44:49 - likely going to get really madeup
44:50 - citations you know that's a great way to
44:52 - play around check the citations be use
44:55 - your critical think check the the um
44:58 - check their work and so I think um
45:01 - playing around with the tools is another
45:02 - way to see how they're wrong and to
45:05 - really under give get an understanding
45:06 - of um what is happening under the covers
45:09 - because right now we also you know we
45:11 - could have a whole other panel on you
45:12 - know open models versus closed models
45:14 - and transparency and what that means and
45:17 - at ai2 we really push for you know
45:19 - moving from a black box to a glass box
45:22 - because in order to solve some of these
45:23 - open research questions um we need to be
45:26 - able to get into the full stack of the
45:28 - model in order to understand what it's
45:30 - doing and when and why and so also
45:33 - moving towards this more open and
45:35 - transparent research is critical could I
45:39 - just say 30 seconds quickly uh just
45:41 - really quickly uh I would point to a
45:42 - number of Frameworks so Google has
45:44 - responsible uh a responsible AI
45:46 - framework uh Singapore has released a
45:48 - framework uh as well and what's lovely
45:51 - about a number of these Frameworks is
45:52 - they provide a whole host of questions
45:54 - because net net
45:56 - our philosophy is that those closest to
45:58 - the problem space must be armed with a
46:01 - set of questions and they must determine
46:03 - this idea and this notion of right and
46:05 - wrong right so really I would lean in on
46:07 - the Frameworks that provide some guiding
46:09 - questions to help lead towards this this
46:11 - this idea of having a conversation about
46:13 - what's right what's wrong in terms of
46:15 - these inputs and outputs I would also
46:17 - add the people who are impacted should
46:19 - also be at the table from the beginning
46:21 - and that's not always what's happening
46:23 - um you know unfortunately and so that s
46:25 - that solves a lot of different problems
46:27 - even in the executive order that came
46:28 - out this week you know the voices of
46:30 - workers was part of the the order that
46:33 - Biden had put forth um and so bringing
46:35 - people to the table sooner in the
46:36 - process versus getting just the
46:38 - application or just the way it's used is
46:41 - a tremendous opportunity all right I'm
46:43 - going to try and make sure we get two
46:44 - more questions in here so I think
46:46 - David's got a question right here yes
46:49 - thank you uh Senator Wiman from uh
46:51 - Washington State and I chair uh
46:53 - education um but I started my career on
46:55 - Main frame so I go by back and to me I
46:58 - think a lot of what we have to do is to
47:00 - translate what is happening for people
47:03 - whether it's within our schools whether
47:04 - it's within our legislators to get them
47:06 - to move things that they have to move
47:09 - and for me um you know I go back to uh
47:11 - warock and gki and say hey postcript was
47:14 - AI postcript took what we uh had to use
47:19 - machines to code and it turned it into
47:21 - something that went to a printer and and
47:23 - did what it did but also within two
47:26 - years 1500 typ Setters in the United
47:28 - States were gone they were no longer
47:30 - needed that job was over and I think you
47:33 - know that's really important so we go
47:35 - way back with AI it's different with
47:38 - generative AI where we actually can pose
47:41 - the question um the rest of us can now
47:44 - have that added capability aui is an AI
47:48 - um you know that we've had for years and
47:50 - I think U just to say there is a movie
47:53 - that I think would be very useful just
47:55 - to to take a look at which is called
47:57 - knowledge Navigator and it talks about
47:59 - the interaction with the human and and
48:03 - Ai and it was produced by Apple in
48:06 - 1987 um it's on YouTube and it's it's I
48:10 - think you know I think we have to make
48:11 - it comfortable for people as well
48:13 - because this can get really scary fast
48:16 - so just some comments yeah thank you for
48:18 - adding those comments and great
48:19 - resources and great points that you made
48:21 - um let's go uh how about over in this
48:23 - side of the room uh we'll take question
48:26 - I see a hand kind of in the middle over
48:27 - here maybe uh David okay thank
48:34 - you Ali Gallagher from policy analysis
48:37 - for California education my question is
48:40 - actually about this concept of
48:42 - coalitions and getting the end user
48:45 - involved and I wonder um given the
48:47 - comments that you made before about the
48:50 - nature of the governments that we have
48:52 - right now and also the incentives that
48:54 - the private sector has how will those
48:57 - spaces get created and how are people
48:59 - who feel like they are end users what
49:02 - paths do you see to empowering them to
49:04 - get into those spaces if they don't see
49:06 - where they
49:07 - exist shenica do you want to feel that
49:10 - one I love that you threw me that
49:13 - question I think that's an incredible
49:14 - question I wish I had the answers I do I
49:16 - do not I I think from from my vantage
49:19 - point it's about deliberate action I
49:21 - think a lot of it's going to have to be
49:22 - Grassroots I think it also has to be
49:24 - anchored in and uh intentional knowledge
49:26 - and understanding the space um I think
49:30 - look I think we this group we have a lot
49:32 - of power we have more power than we
49:34 - realize and I think it I think it's
49:36 - going to require us to take some risks
49:39 - ask hard questions of both the industry
49:41 - but also the government um but I I I
49:45 - don't I don't know if I have the answer
49:47 - I just know that we can't solve this
49:49 - unless we try to solve it together um
49:52 - I'll throw that over to my other
49:53 - colleagues I would add so as when we
49:56 - work on Research when we work on
49:58 - creating models we we need to do a
50:01 - better job of including people and we
50:02 - are starting to move that way we just
50:04 - rolled out some new processes that
50:06 - involve people at the beginning so we're
50:08 - you know I one of the things that's
50:10 - interesting is people who are creating
50:12 - the foundation models today are not
50:14 - always thinking about the use cases
50:16 - because it is a baseline technology and
50:18 - so when you look at the full pipeline um
50:22 - there are so many different inflection
50:23 - points so I think there is
50:25 - responsibility throughout that process
50:27 - we have a responsibility to as we're
50:29 - creating a model what are the potential
50:31 - use cases and how are we bringing in
50:33 - those voices I'm working on a project
50:35 - now where we are um doing a a survey and
50:38 - a study with people to brainstorm
50:40 - potential use cases and talk about
50:43 - values aligned with those types of
50:44 - things um that's one example of where
50:47 - like we should have ownership as a
50:49 - research organization um people who are
50:51 - developing technology and developing
50:53 - apps built on top of these models
50:56 - absolutely should be having
50:57 - conversations with people who will be
50:59 - impacted and then there are you know
51:00 - again back to the executive order nist
51:02 - is going to be putting out safety
51:03 - standards right in the public comment
51:05 - like there will be I'm sure a public
51:06 - comment process do it it seems like you
51:09 - may be lost in the shuffle but they read
51:11 - them and so putting like participating
51:14 - in the Democracy that we are in in the
51:16 - US as an example to be us-centric is a
51:18 - way to get your voice to be heard and so
51:21 - I would encourage that um having just
51:22 - written a copyright comment that just
51:24 - was due last week so a thought just came
51:27 - to mind and I see you I was just going
51:28 - to mention Brooking Institute actually
51:30 - did a survey uh uh not quite a year ago
51:33 - 20 countries and states uh Global um
51:36 - they actually looked at a coalition
51:38 - model around how Computer Science
51:40 - Education how these specific geographies
51:42 - were able to drive adoption I would say
51:44 - take a look at it it's a Brooking
51:46 - Institute uh recent report I think it's
51:48 - called skills for everyone that and it
51:51 - did this Global survey and Arkansas
51:53 - actually was spotlighted as one that had
51:55 - done in a an effective job of building a
51:57 - coalition uh they were called out in
51:59 - that report so I'm I'm sorry we we got
52:01 - to we got to wrap up here the the
52:03 - countdown is getting pretty tight here
52:05 - and we've got folks but but the good
52:07 - news is um all three of our wonderful
52:10 - panelists are going to be here a little
52:12 - bit after the panel and here throughout
52:14 - the day so please follow up with them on
52:16 - the questions that we didn't get to and
52:18 - even questions that we did that we want
52:19 - to we want to dig into please um help
52:22 - help me thank our panelists Nicole Shena
52:24 - and for doing this
52:28 - panel