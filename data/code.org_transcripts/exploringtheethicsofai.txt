01:18 - hello everyone my name is pat
01:21 - youngbraddit i'm cod.org's chief
01:23 - academic officer and i want to welcome
01:25 - you to our webinar today
01:27 - exploring the ethics of ai
01:29 - without further ado
01:31 - i'd like our panelists to introduce
01:33 - themselves
01:35 - uh panelists please share your name
01:37 - where do you work what you do
01:40 - as well as your favorite example of
01:43 - artificial intelligence
01:46 - let's go ahead and start with amanda
01:51 - hi i'm amanda askall i'm a research
01:54 - scientist on the policy team openai
01:56 - um my background is in ethics but i
01:58 - currently work on the evaluation of ai
02:00 - systems
02:02 - so a cool example of ai i think as
02:04 - someone who spends a lot of time writing
02:06 - i'm quite excited by like language
02:08 - models that can generate and summarize
02:10 - text
02:11 - oh yeah
02:12 - i'm sure
02:13 - the students watching right now wish
02:15 - they had a language model at their
02:17 - disposal
02:18 - right now
02:19 - and actually they just might we'll talk
02:21 - about that later uh let's go to deb deb
02:25 - could you please introduce yourself
02:30 - hi i'm deb raji
02:32 - i'm currently a mozilla fellow i'm also
02:34 - a fellow at the algorithmic justice
02:36 - league um and i do a lot of work um
02:38 - auditing deployed algorithms um usually
02:41 - government deployments of different
02:43 - algorithms or various machine learning
02:45 - products
02:46 - um
02:47 - my i guess favorite uh
02:50 - application of ai um i'm really excited
02:54 - by a lot of machine learning for
02:55 - healthcare applications um i have a
02:57 - least favorite application which is
02:59 - facial recognition which is which has
03:00 - been the subject for a lot of our audit
03:02 - work so far yes and we'll be definitely
03:05 - talking about that later thanks deb
03:07 - uh mayron could you introduce yourself
03:10 - sure my name is maron sahami i'm a
03:12 - professor at stanford university and a
03:14 - lot of my time is spent thinking about
03:16 - education around ai and other topics in
03:18 - computer science
03:20 - and i'd say having two kids that are
03:21 - getting near driving age one of my
03:23 - favorite applications of ai is
03:25 - self-driving cars they may be part of
03:27 - the first generation that actually never
03:29 - ends up having to drive themselves wow
03:33 - and last but not least natasha
03:36 - hi everyone i'm natasha crampton and i'm
03:39 - microsoft's chief responsible ai officer
03:42 - at microsoft i put our ai principles to
03:45 - work across the company uh by making
03:48 - sure that we build and us and make
03:51 - available our technologies consistent
03:53 - with those principles so on a day-to-day
03:55 - basis i do things like helping to write
03:57 - policies helping teams work through
04:00 - challenges that don't have obvious
04:02 - answers
04:03 - and i also do some work on law reform so
04:07 - helping to
04:09 - contribute to the conversation that
04:11 - needs to be had about what you norms and
04:13 - laws and standards do we need in this
04:15 - space
04:17 - my favorite application of ai is through
04:20 - an organization called wild me that
04:23 - we've partnered with so as as you
04:26 - probably know the um possible extinction
04:29 - of animals is a real thing and in fact
04:31 - if we don't take action by 20 100 we
04:35 - could be facing extinction of about 38
04:39 - of of the world species so what this
04:41 - application does is it combines some ai
04:44 - technology that helps identify
04:48 - wildlife species and images and combines
04:51 - the power of that with citizen
04:52 - scientists like you or me um to help
04:56 - track uh wildlife and and make sure that
04:58 - we're um identifying you know activities
05:01 - that would be uh you know inconsistent
05:04 - with their their long-term survivals i
05:06 - think that's a pretty cool um
05:07 - application of ai that makes a real
05:10 - difference in the real world
05:12 - thank you natasha natasha mehran deb and
05:15 - amanda thank you so much for joining us
05:17 - today
05:19 - i actually want to start off with some
05:20 - questions for the audience
05:23 - and i want to see how well they can
05:25 - identify these
05:27 - famous
05:28 - movie ais
05:30 - so let's start off with the first poll
05:33 - and audience get your fingers ready
05:35 - because you're going to be answering
05:36 - some questions
05:42 - so
05:43 - who is this
05:45 - famous ai i'll give you five seconds
05:48 - to answer it
05:55 - oh too easy right
05:57 - yep
05:59 - too easy you are correct it is c-3po
06:02 - from star wars let's go to the second
06:05 - question
06:06 - who is this famous ai
06:21 - and uh the majority has it again the
06:24 - matrix
06:25 - this third question is a little bit
06:27 - harder
06:28 - so let's try one more time identify this
06:31 - famous ai
06:45 - i i knew people would have a harder time
06:48 - with this
06:49 - and actually if i looked at this i'd be
06:50 - i would have thought oh it's the nest uh
06:53 - learning thermostat
06:54 - um but actually it is
06:56 - hal 9000 from 2001 a space odyssey
07:00 - uh good job audience um
07:04 - now i want to start off with uh a uh
07:08 - very important question for our audience
07:10 - um audience uh we're probably we
07:13 - probably have a mix of students teachers
07:15 - and adults but pretend that i'm a uh 13
07:18 - year old
07:21 - and i asked you the question what is ai
07:24 - how would you describe ai how would you
07:27 - define ai for a 13 year old and anyone
07:30 - can take this
07:39 - well maybe i'll jump in um it's good
07:41 - that we saw a few examples of ai in the
07:43 - movies because i think you know one way
07:44 - to think about ai is trying to get
07:46 - computers in the real world to behave
07:48 - more like computers in the movie or you
07:51 - know if we want to be more specific is
07:53 - kind of thinking about what are the
07:54 - things that involve activities that
07:56 - require your thought to do what we want
07:59 - to try to do with ai is to have
08:01 - computers be able to do more of those
08:03 - kinds of things so when we think about
08:05 - driving or making decisions or playing
08:07 - games those are all things that require
08:09 - some level of human thought and so if we
08:11 - can get a computer to do some of those
08:13 - things we might consider that ai okay
08:15 - thanks miran
08:20 - i'll jump in and say that one of the
08:22 - interesting sort of narratives that we
08:24 - see in science fiction is this kind of
08:27 - idea of you know humans versus machines
08:30 - and there's going to be these
08:32 - super human robots that are going to
08:34 - take over our lives and i wanted to jump
08:37 - in and say that that is not what today's
08:40 - ai is capable of and in fact
08:43 - really the
08:44 - the ai is a collection of different
08:46 - techniques and and approaches to
08:49 - um
08:50 - you know empowering machines to do the
08:52 - sorts of things that humans do but one
08:54 - of those sort of prominent approaches
08:56 - today that is really
08:58 - gaining steam has been used in the real
09:01 - world is something called
09:03 - machine learning
09:04 - and the way i like to think about
09:06 - machine learning is to sort of contrast
09:08 - it with uh
09:10 - more traditional approaches to software
09:13 - development so more traditional software
09:15 - development
09:16 - it's sort of like writing a recipe you
09:18 - need to know what the steps are you need
09:21 - to know the ingredients you write down
09:23 - the steps and it leads to a certain
09:25 - outcome
09:26 - the thing about machine learning is that
09:28 - it's much more like learning from
09:31 - experience so in the same way that you
09:34 - might tell a toddler again and again do
09:36 - not touch that hot stove
09:39 - uh they will probably do that and they
09:41 - will learn from experience that that
09:43 - stove is hot and then they might be able
09:45 - to extrapolate from that experience to
09:48 - figure out that touching a toaster is
09:50 - probably going to be hot
09:53 - as well and might burn them
09:55 - and so this machine learning process
09:58 - where we're using lots of data to teach
10:02 - software how to learn from that data and
10:05 - find patterns is a really exciting
10:07 - development and that's really um you
10:09 - know a lot of what today's ai is about
10:13 - i want to uh
10:15 - highlight what you said uh that there is
10:18 - a difference between ai and
10:21 - fake ai or like just the like this
10:23 - general
10:24 - movie ai
10:26 - and what we see these days which is
10:27 - mostly machine learning so kids keep
10:29 - that in mind
10:30 - amanda deb
10:32 - um you know
10:33 - we see a lot of uh ai in the movies as
10:36 - as we saw from the polls which are real
10:39 - what aspects are real and what aspects
10:41 - are fake
10:43 - um yeah i can maybe start um also i um
10:47 - you know i definitely um
10:49 - interact more i think that there's
10:51 - definitely a branch of researchers that
10:54 - um uh
10:55 - whether they're the pr their primary
10:57 - concern their primary work is more
10:59 - speculative work so there are people
11:00 - thinking about
11:02 - um you know the ai systems and the
11:04 - movies and trying to understand and
11:05 - replicate those that level of human
11:08 - thinking with machines a lot of the work
11:10 - that i do is looking at products that we
11:13 - have today so like natasha mentioned a
11:15 - lot of those products that are deployed
11:17 - affecting people's lives today
11:19 - are machine learning products um and
11:21 - they have very specific characteristics
11:23 - that are very different from the sci-fi
11:25 - movie versions of ai um the way i like
11:28 - to describe it is kind of if anyone's
11:30 - ever seen a roomba um you know that's
11:33 - the most widely disseminated robot uh
11:36 - uh in the us right now and uh it
11:38 - definitely does not look like like um
11:41 - you know the robot in the movies it's
11:42 - much simpler um but you know if there
11:45 - was a systematic sort of issue with the
11:47 - roomba there was some kind of mistake
11:49 - that was made in creating a roomba that
11:51 - would affect a lot of households it
11:52 - would affect you know millions of people
11:54 - so it's really important even if it's a
11:56 - really simple
11:57 - it's if it's simpler than you know the
11:59 - the image that we see uh in the movies
12:01 - we still have to pay attention to to the
12:03 - ways that we build these things so for
12:06 - me um you know i have a 12 year old
12:08 - sister and she doesn't understand what i
12:10 - do so i try to explain it to her and the
12:12 - way that i um describe the difference
12:15 - between the models that
12:17 - i'm doing my audit work on and um
12:20 - you know what she's seeing on in movies
12:22 - is that um for one thing a lot of
12:25 - machine learning models even though it's
12:26 - described as learning um
12:29 - really often don't have this continuous
12:32 - learning in the way that you know a
12:34 - human will you know learn um you know
12:36 - the alphabet and then you know maybe
12:38 - learn to write words and just
12:39 - continuously be taking in feedback a lot
12:42 - of these um machine learning models as
12:44 - we call them um like natasha mentioned
12:47 - they'll
12:48 - be defined by um you know
12:50 - uh information and they will be
12:53 - initially sort of uh
12:55 - set up using information that's been
12:57 - provided but ultimately um
13:00 - once they're defined then they don't
13:02 - actually keep adapting and keep evolving
13:04 - in the way that we as humans keep
13:06 - adapting and keep evolving continuously
13:08 - so that's something that i think is
13:10 - often a little bit difficult for people
13:11 - to understand just because of the
13:13 - branding of learning we think like oh
13:15 - like learning is a continuous thing but
13:16 - it's sort of another way to think about
13:18 - it is if you train a dog to do a very
13:20 - particular trick um you know that dog
13:23 - can do that trick
13:24 - and if you want the dog to do a
13:26 - different trick you have to train it to
13:27 - do a different trick so machine learning
13:29 - models work that way right now
13:32 - rather than sort of the way humans work
13:34 - where we're constantly learning new
13:36 - things and being very creative and
13:38 - innovative so we're not quite there yet
13:40 - and that's something that's really
13:41 - important for um especially younger
13:42 - people to understand
13:44 - thank you dad amanda what do you say
13:48 - yeah i think the problem is the ai is
13:49 - used in this really general way to refer
13:52 - to a lot of different things so maybe
13:54 - i'd make the division as something like
13:56 - real ai realistic ai and then something
13:59 - more like implausible ai so real ai is
14:02 - like the stuff that we actually have
14:03 - just now and you know these are the
14:05 - applications that people have focused on
14:07 - so
14:08 - it's more narrow it's more machine
14:09 - learning based it's doing things like um
14:11 - translation and search
14:13 - and it's just the stuff that you already
14:14 - kind of interact with even if you don't
14:16 - know that you're interacting with it and
14:17 - then there's like realistic applications
14:19 - which we don't yet have but that don't
14:21 - seem kind of out of the realm of uh
14:23 - things that ai researchers will work on
14:25 - and possibly solve so like we've already
14:28 - heard like self-driving cars that's
14:29 - something that we don't have yet but we
14:31 - can imagine having that and it also
14:33 - includes things like um better
14:35 - translators it's a better machine
14:37 - translation maybe even things like
14:39 - computer assistants and teachers um or
14:42 - things that like um can you know
14:44 - summarize like papers for you uh so
14:46 - those are things that we don't
14:47 - necessarily have like in the world but
14:50 - they seem pretty plausible
14:51 - um and i think the same is true of like
14:53 - just more general systems so actually
14:55 - having systems that can do more than one
14:57 - thing and so they don't have to just be
14:59 - like tailored to a very narrow
15:00 - application but it can like teach you
15:02 - about mathematics if you ask questions
15:04 - about mathematics but it can also like
15:05 - summarize a book if you ask it to
15:07 - summarize a book
15:08 - um and then i think they're the ones
15:09 - that are
15:10 - like more implausible so i think a lot
15:13 - of like movie depictions of ai is kind
15:15 - of they're both like human-like and
15:17 - they're very often robots um and so
15:20 - it can be really easy to kind of think
15:22 - that what a very powerful ai would look
15:24 - like like a very general ai it must look
15:27 - like a lot like a human and have like a
15:28 - person's kind of like motives and so
15:31 - that's why you know to make it
15:32 - interesting we make it really malicious
15:33 - for example you know and i think that
15:36 - you know
15:38 - that is like i was kind of trying to
15:39 - make something you know very interesting
15:41 - but it's not necessarily the case that
15:43 - we're going to see something that is
15:44 - like very human-like um even if you see
15:46 - something that can like learn in these
15:47 - really general ways that doesn't mean
15:49 - it's gonna be like a human-like system
15:51 - thank you thanks
15:52 - amanda uh
15:54 - i have a question uh on behalf of again
15:57 - the kids um
15:59 - on our uh webinar today uh
16:03 - students out there uh i bet you're
16:05 - wondering why should i care about ai i
16:07 - mean other than it being cool in the
16:09 - movies and uh and all that and it's some
16:11 - type of technical trend right now
16:14 - like maybe you're wondering why should i
16:15 - care about ai why do i really need to
16:17 - know about it how might it affect my
16:19 - life let's start off with mehron what do
16:21 - you think you you have like you said you
16:23 - have uh
16:25 - two um kids who are about to learn how
16:27 - to drive i believe or learning how to
16:29 - drive
16:30 - so other than other than the fact that
16:33 - they might not have to drive and that's
16:34 - something that they might look forward
16:36 - to or not why should kids in general
16:39 - care about ai
16:41 - well i think if you spend any time
16:42 - online these days which is basically all
16:45 - of us you're interacting with ai in a
16:47 - bunch of ways many of which may not be
16:49 - clear that what's actually power
16:52 - powering something underneath the hood
16:54 - is actually something we would consider
16:55 - ai
16:56 - but some simple examples for you know
16:58 - for example or you know if you watch
17:00 - videos if you watch youtube if you watch
17:01 - netflix how do they make recommendations
17:04 - about other things you might like the
17:06 - you know technology of the powers that
17:07 - we would consider that a form of ai
17:10 - if you play video games oftentimes the
17:11 - characters you're interacting with are
17:14 - trying to take some novel action
17:15 - depending on what you're doing in the
17:16 - game so we would think of that as a form
17:18 - of ai if you send email your email is
17:21 - being filtered for things like spam
17:22 - using ai if you're on a social network
17:25 - things like friend recommendations are
17:26 - powered by ai so ai's out there in a
17:29 - bunch of different ways whether or not
17:31 - you're really actively thinking about it
17:32 - or not it's actually interacting with
17:35 - you in a lot of different ways and so
17:36 - the more you know about it the more you
17:38 - can make informed decisions in those
17:40 - interactions
17:42 - thanks meyron and anyone else would like
17:44 - to to add to why a
17:46 - a student these days would should care
17:48 - about ai or care about learning about ai
17:53 - i'd add that uh you know at times uh
17:56 - satya nadallah our ceo talks about ai
17:59 - has been you know one of the most
18:00 - transformative technologies of our time
18:03 - so akin to other
18:06 - advances in technology things like the
18:09 - printing press or electricity or the
18:11 - internet
18:13 - and with all of those new advances have
18:15 - been new issues that have been have
18:17 - resulted as well so i think
18:19 - my first reason would be this is game
18:22 - changing and you want to be a part of a
18:25 - of a game-changing change in technology
18:28 - and to understand its impact on society
18:30 - so you can be involved right
18:33 - as miran was just saying you're going to
18:35 - be consuming
18:36 - ai powered services you're going to be
18:40 - impacted by ai powered services and so i
18:43 - think it's fantastic to really want to
18:45 - understand those uh uh those services
18:49 - you know ai is not just uh sort of
18:52 - magically created it just doesn't turn
18:54 - up it's made by human beings and there
18:57 - are lots and lots of decisions that are
18:59 - made along the way so i think you know
19:01 - if you come to understand how the
19:03 - technology works and how it and its
19:06 - impacts on people and society you can
19:08 - have a voice in in how
19:12 - technology serves society going forward
19:15 - and so i think that's a really exciting
19:16 - reason to be involved
19:19 - thanks natasha
19:23 - yeah i definitely um agree with
19:25 - everything that's been said i guess i'll
19:27 - add that
19:28 - um you know there are um algorithms in
19:32 - you know programs that we choose to
19:34 - engage with such as recommendation
19:35 - systems
19:36 - uh you know when we watch netflix or
19:38 - when we're um looking through a social
19:41 - media feed but there's also a lot of
19:43 - algorithms being used um you know by
19:46 - government agencies being used by your
19:48 - schools maybe
19:50 - um
19:51 - that you know you might want to learn
19:53 - more about uh algorithms that you might
19:55 - not necessarily be fully aware of unless
19:58 - you educate yourself about um you know
20:00 - how they show up in your life to affect
20:02 - your life um and the life of others in
20:04 - your community and um i think that's
20:07 - incredibly incredibly important if
20:09 - you're hoping to be just someone that
20:11 - understands more of how decisions are
20:13 - made about you and about important
20:15 - things in your life um so i definitely
20:17 - think that you know for anyone that's
20:19 - curious just to understand better the
20:22 - systems that um
20:24 - affect them uh it's super
20:26 - important or super relevant to to care
20:28 - about ai and to care about these
20:30 - algorithms and understand how they work
20:32 - gotcha thank you
20:36 - yeah i think another issue is that maybe
20:39 - people can it's easy to think that like
20:41 - ai is only going to be related to things
20:43 - that are very like math and sciencey i
20:45 - think that's like a kind of bias we
20:46 - might have but actually if you look at a
20:47 - lot of like our ai applications now it's
20:50 - being applied to like art to music to
20:52 - text to just like things that you
20:54 - wouldn't consider like very traditional
20:56 - kind of like science and mathematics
20:57 - domains so it's really easy to think
20:59 - well it won't affect me unless i want to
21:00 - have like i want to go into the sciences
21:02 - that might affect you if you actually
21:04 - want to go into like law or to art or to
21:06 - writing um and so
21:08 - you know historically there were times
21:10 - when people were like well why should i
21:11 - learn computers like i'm not going into
21:13 - science like so i don't need to learn
21:14 - how to use a computer and now we would
21:16 - look back on that and say that was
21:17 - really naive like computers were
21:18 - actually going to affect a huge number
21:20 - of industries and i think we should feel
21:22 - kind of similarly about ai just now it's
21:24 - not just something that's going to
21:25 - affect a kind of like narrow domain of
21:27 - occupations um but rather it might end
21:29 - up affecting kind of like a whole host
21:31 - of things that you might want to do
21:32 - yeah thanks amanda
21:35 - natasha you mentioned um
21:37 - an organization called was it wild team
21:40 - is that what what the name of it
21:42 - uh it's called wild me
21:44 - wild me
21:46 - wild me and uh and they were using ai to
21:50 - address um animal extinction
21:53 - um
21:55 - uh
21:56 - for all the panelists um
21:58 - you know how can ai be used uh for
22:02 - social social justice or social good you
22:04 - know uh just as a little plug code.org
22:07 - ai for oceans tutorial
22:09 - um follows our theme for the hour code
22:13 - this year i think it was the hour code
22:15 - being last year as well cs for good
22:18 - um
22:19 - and what we're talking about right now
22:20 - is you know ai for good ai for social
22:23 - justice but
22:25 - what are those examples of how ai can be
22:28 - used or is already being used for social
22:30 - for social justice and social good
22:35 - yeah an example i really like um is a
22:38 - project from google ghana where they
22:41 - um they have an issue and
22:44 - there's an issue that a lot of the
22:45 - farmers in that country face where they
22:47 - try to
22:48 - understand um you know how crop diseases
22:51 - show up in their plants and it's very
22:53 - difficult to just you know by by sight
22:57 - um understand what kind of
22:59 - disease might be affecting different
23:01 - types of crops um so there was a project
23:03 - that they did where they collected a
23:05 - data set of you know affected cassava
23:07 - plants and plants that were not affected
23:09 - and
23:10 - it was a really interesting project
23:11 - because it was so connected to
23:13 - um a real issue in that community and um
23:16 - computer vision in this case which is
23:18 - sort of the ability of the
23:20 - machine learning model to be able to
23:22 - distinguish between different images um
23:24 - was super helpful in uh figuring out
23:27 - which plants were sick and which plants
23:29 - um and thus needed more care or which
23:30 - plants were okay um and it was it was
23:33 - sort of a great example of you know
23:34 - collecting data sets that are useful for
23:36 - the community um to address a problem
23:38 - that that community was really facing so
23:40 - that's a really
23:41 - uh that's the example i often think of
23:43 - when i think of ai for social gain
23:45 - thank you
23:48 - other examples out there
23:50 - we got addressing animal extension
23:53 - crops
23:56 - and maybe another one to throw in is
23:58 - i'm looking at how to
24:00 - help different countries develop
24:02 - economically so there's you know some
24:04 - work the united nations does for example
24:06 - to try to figure out uh where economic
24:09 - development is happening and so for
24:10 - example where do you need to send food
24:12 - shipments where do you need to send
24:14 - supplies what kind of aid to send and
24:16 - that's a hard thing if you're trying to
24:18 - monitor that on a global level but it
24:20 - turns out if you get some satellite
24:22 - imagery from that image where you can do
24:24 - the kind of vision analysis work that
24:26 - the dev was talking about to be able to
24:28 - understand for example where there's
24:30 - electricity usage because you actually
24:32 - see lights where you have different
24:34 - kinds of agricultural development
24:35 - because you can identify what parts of
24:37 - the satellite and major crops and how
24:39 - much they are where there's water and
24:41 - how it's being used and that gives you
24:44 - indications as to what where different
24:46 - areas are developing and how quickly
24:48 - because you can look at these satellite
24:49 - images over time and then help figure
24:52 - out where aid needs to be sent in the
24:54 - future
24:55 - awesome
24:58 - i think there are also examples that um
25:00 - might feel slightly indirect but seem
25:02 - pretty important so like recently you
25:05 - know uh deep mind released alpha fold
25:07 - which is helping us kind of uh see the
25:09 - shape of proteins um and things like
25:12 - that have like down the line
25:14 - applications so these like science and
25:15 - medicine um applications of ml uh which
25:18 - could mean things like assisting with
25:19 - new drug discovery um and that's a way
25:21 - of doing like a lot of social good kind
25:23 - of but you're doing so early in the
25:24 - process um
25:26 - i think another one that's kind of uh we
25:29 - might not think of is like machine
25:30 - translation um these things that we've
25:32 - had around for a while but as they get
25:34 - better they just like let more people
25:36 - like access um things like just like
25:38 - documents on the internet or be able to
25:40 - talk with one another and that seems
25:41 - like a really important thing for social
25:43 - good to me
25:47 - yeah so you know
25:49 - uh
25:50 - students and teachers out there
25:53 - obviously there are examples
25:55 - [Music]
25:56 - of ai and machine learning being used
25:59 - for social good
26:01 - but as you know uh and from the news as
26:04 - well sometimes there are issues in the
26:07 - way ai is used and some unintended
26:09 - consequences
26:10 - um panelists
26:12 - uh what are
26:14 - some of the uh potential misuses
26:17 - of ai or
26:20 - another way to say it is
26:22 - you know everyone's trying to use ai uh
26:24 - for a positive uh outcome but sometimes
26:28 - there are unintended negative outcomes
26:30 - what are those outcomes what are we what
26:32 - are we seeing already and what's what's
26:35 - what scary things are even possible if
26:37 - they're not happening already
26:42 - yeah so like natasha mentioned um you
26:45 - know one of the big
26:46 - differentiators between
26:49 - machine learning or ai and
26:51 - traditional software is that traditional
26:53 - software there was a lot of control of
26:56 - the software engineer to be able to
26:58 - define the rules um to make or to
27:01 - automate a decision
27:03 - so um as a result of that um there was a
27:06 - certain amount of visibility and control
27:08 - around
27:09 - um you know what are the steps involved
27:11 - or what are the what are the steps of
27:13 - the recipe to get to the cake
27:14 - effectively whereas with machine
27:16 - learning a lot of those steps are
27:18 - defined by data and that causes you know
27:22 - that raises a bunch of different issues
27:24 - for one
27:25 - um a lot of the models that we use today
27:27 - are quite large and resource intensive
27:29 - so they require a lot of data
27:32 - and sometimes it's data that we don't
27:34 - understand that we don't necessarily
27:36 - think um or we don't necessarily give
27:38 - permission to be used as part of a
27:39 - machine learning model so there's a lot
27:41 - of concerns there um and there's also
27:43 - the fact that just because sometimes the
27:45 - data sets are so large
27:47 - it becomes very difficult
27:49 - to understand
27:51 - what part of the data is defining the
27:54 - program
27:55 - and the decisions that the program is
27:56 - going to make
27:58 - and as a result of that there's a lot of
27:59 - challenges with trying to understand
28:02 - the steps that end up coming out of the
28:04 - model and the recipe
28:06 - that ends up
28:07 - sort of being developed using that data
28:10 - um and as a result of that we have a lot
28:12 - of difficulty and this is a lot of the
28:14 - work that i do we have a lot of
28:15 - difficulty even
28:18 - properly evaluating or understanding um
28:21 - you know what it means for the system to
28:23 - work or not work just because some of
28:25 - those
28:25 - um some of those consequences of
28:28 - deploying that model uh could be
28:30 - something that we can only observe you
28:32 - know days later years later
28:35 - wow
28:36 - yep
28:39 - to add to what
28:41 - deb says
28:43 - you know in addition to thinking about
28:45 - um the the data and the in the
28:48 - challenges the data bias challenges
28:51 - the the challenges that can come when
28:54 - you know algorithms are not trained in a
28:56 - really thoughtful way
28:58 - there are some challenges that come when
29:02 - people are not thoughtful about the use
29:05 - cases to which they push
29:08 - the ai technologies because some of the
29:11 - technologies if we take facial
29:12 - recognition as an example
29:14 - it can be put to really wildly different
29:17 - uses right you might have unlocked your
29:20 - device this morning by using facial
29:23 - recognition um and that's a pretty
29:26 - constrained use case um the consequences
29:30 - of something going wrong um if if the
29:33 - phone or your device can't recognize you
29:36 - are not all that great you might be put
29:38 - to a bit of inconvenience because you
29:40 - might have to enter your pen or you
29:42 - might have to call the building security
29:44 - because you can't get into the building
29:46 - but that's a fundamentally different
29:49 - thing to using
29:51 - facial recognition for
29:53 - mass surveillance of
29:55 - people at a protest or to
29:58 - persecute a marginalized group
30:02 - so there are essentially a lot of
30:04 - choices that you have to make when
30:06 - you're building an ai system and there
30:09 - are a lot of choices that you have to
30:11 - make when you're deploying an ai system
30:14 - and if you don't think through both of
30:16 - those things very carefully and think
30:18 - very broadly about the stakeholders
30:20 - you're um you're impacting if you don't
30:23 - think about the readiness of the
30:25 - technology things can go wrong at both
30:27 - of those end so you really have to
30:30 - bake in safeguards from the very
30:32 - beginning and that then allows you to
30:34 - realize the potential of the technology
30:38 - got it
30:40 - yeah anyone else want to add something
30:42 - mehran i was going to say i think in a
30:44 - lot of ways ai amplifies certain aspects
30:47 - of society and some of those aspects can
30:49 - be good when we try to think about you
30:51 - know addressing problems of social good
30:53 - but there's also aspects of society that
30:55 - we don't want to have amplified that you
30:57 - know if we have bias in the world for
30:59 - example that's not something we want to
31:01 - amplify
31:02 - it's also the fact that it makes some
31:03 - things easier and that can be good or
31:05 - bad it could be you know it's great when
31:07 - it helps you around the house because
31:09 - now you might have a personal vacuum
31:10 - like a roomba that helps clean up but at
31:13 - that same time if you think about
31:14 - autonomous technology you can also mount
31:17 - a weapon on that and you can engage in
31:19 - warfare and at one level we might say
31:21 - that's good because then we don't have
31:22 - people dying on the battlefield and
31:24 - another level if it makes it easier to
31:26 - engage in warfare that might not be
31:28 - something we want and so you know as
31:30 - natasha saying we need to be very
31:32 - judicious not only about what ai we can
31:34 - build but you know what are the things
31:37 - that we actually want to amplify in our
31:39 - society to think about what are the
31:41 - right problems for ai to address
31:43 - yeah you know i want to um
31:45 - amanda this question's for you is
31:47 - starting with you but then for everyone
31:49 - else as well let's let's let's talk
31:50 - about the ethics right
31:52 - uh and i know
31:54 - amanda a lot of your research is into
31:55 - the ethics of using ai
31:58 - uh what are the the principles or ethics
32:01 - that should govern how people develop or
32:03 - use ai
32:07 - yeah i've thought about this a fair bit
32:10 - i think the first example that i'd give
32:12 - of a principle that feels important but
32:14 - is maybe kind of an unusual one that
32:15 - people don't talk about here is patience
32:18 - so i think
32:20 - in part
32:21 - one of the issues is
32:23 - we're seeing like a lot of new
32:24 - technology where we're trying to predict
32:26 - the consequences and i think there's a
32:28 - temptation to try to develop and deploy
32:30 - things really quickly and in reality
32:32 - what you want to do is take the time
32:34 - that is required to slowly rule out
32:37 - things see if they have any consequences
32:39 - that you didn't expect make sure that
32:41 - they're like ruled out in a way that's
32:42 - like fairly restrictive at first so that
32:44 - um as other
32:45 - as others have said uh you know the
32:47 - consequences of something going wrong
32:49 - aren't too high and then if that's okay
32:51 - you like you do that again and you and
32:53 - rather than having some kind of like
32:54 - sweeping change you just have like a
32:56 - kind of slow roll out where you can kind
32:58 - of like control and understand the
32:59 - consequences of your system and that
33:01 - requires like an environment where we're
33:03 - all willing to be a little bit more
33:04 - patient about how things are rolled out
33:06 - and how they're developed and where we
33:07 - kind of create an environment where
33:08 - developers aren't just incentivized to
33:10 - like put something out to market
33:11 - straight away in this like really like
33:13 - large and kind of uncontrolled way so i
33:15 - think that like yeah maybe an under
33:17 - recognized principle here is just a
33:18 - principle of patience
33:20 - all right so uh again i know there are a
33:22 - lot of students on this webinar write
33:24 - down patience
33:25 - as one of those
33:26 - those ethical principles for
33:29 - developing or using ai patience all
33:32 - right deb mayra natasha let's add to
33:34 - that list
33:42 - uh yeah i was gonna say i really love
33:43 - that i think that's excellent um i
33:46 - i also find that um a lot of the
33:49 - um latasha mentioned there does need to
33:52 - be a little bit of like caution in terms
33:54 - of before you even build something you
33:57 - need to really reflect there's
33:59 - it can't always be a reactive situation
34:02 - um you have to sort of be very you have
34:04 - to take initiative uh to think about
34:06 - what could go wrong before you build the
34:08 - thing i think that's very connected to
34:09 - patients that idea of caution and being
34:12 - careful
34:13 - um but i also just really like that idea
34:15 - of patients as well because i find that
34:17 - um
34:18 - uh like like you were just mentioning um
34:20 - you know
34:21 - a lot of the challenges we have with
34:24 - machine learning deployments is when
34:25 - people kind of rush it so they they
34:27 - don't do a good job collecting data in a
34:29 - way that's respectful or they don't take
34:31 - the time to properly you know label the
34:34 - data or properly um you know scope out
34:37 - the context that it can be deployed
34:38 - within or communicate that document
34:41 - everything so um yeah i i actually agree
34:44 - with amanda and i'm just gonna just use
34:46 - another synonym for patients which is
34:48 - caution
34:49 - thanks dad
34:52 - i'll uh jump in with with two um
34:56 - fairness and
34:58 - transparency
35:00 - so we really want our ai systems that we
35:03 - build to be fair and to be fair in a
35:06 - couple of dimensions right we don't want
35:08 - a situation where a system works well
35:12 - for one group of people and works really
35:15 - badly for another group of people
35:17 - we also don't want a system that hands
35:20 - out
35:21 - resources or opportunities to certain
35:25 - people more than others so on that type
35:27 - of system you can think about you know a
35:30 - hiring system or a loan application
35:33 - system we don't want certain people
35:35 - getting all the resources and
35:37 - opportunities and others consistently
35:39 - missing out
35:40 - and we also don't want to do
35:43 - that thing that miran was talking about
35:45 - earlier which is we want to try and
35:48 - minimize the possibility that we would
35:51 - be
35:52 - perpetuating stereotypes and that's that
35:55 - is a really important objective and it's
35:57 - also a really hard
36:00 - practical
36:01 - problem that many people are working on
36:03 - today so that's
36:05 - some ideas about fairness
36:08 - on transparency
36:10 - because these systems are really
36:12 - affecting our lives in
36:15 - many ways big and small it's really
36:17 - important that we understand as
36:19 - stakeholders for those systems
36:22 - why systems are behaving in the way that
36:25 - they do
36:27 - because if i am being impacted by an ai
36:31 - system that's
36:33 - helping to
36:34 - with a hiring process and i miss out on
36:37 - making the candidate list i want to know
36:40 - why and so there's you know a sort of
36:43 - understanding that we have to
36:44 - communicate uh through ai systems so
36:47 - that they're not just black boxes and
36:49 - nobody knows how they operate so i'd say
36:52 - that that transparency is a really
36:53 - important principle too
36:56 - thank you
36:58 - and just to add to that i would say that
37:00 - sometimes in technology we think about
37:03 - you know what we can build and an
37:05 - important equally important or perhaps
37:07 - more important question to ask is who
37:09 - are we building for
37:11 - and part of that you know even though
37:12 - i'm excited about self-driving cars is
37:14 - understanding who's actually implant
37:16 - impacted by technology right there's a
37:18 - lot of people who will be positively
37:20 - impacted by self-driving cars because
37:22 - they may be safer people who may not
37:24 - necessarily otherwise have mobility
37:26 - options now have easier ways of getting
37:28 - around but at the same time there's a
37:30 - lot of people whose livelihood is made
37:32 - through transportation people like taxi
37:34 - drivers or truck drivers and if they're
37:37 - potentially displaced in their jobs
37:38 - because of ai we need to think through
37:40 - what the full set of impacts are
37:43 - and there is a place where you know
37:44 - having some deliberation and some early
37:48 - forethought about well how do we help
37:50 - out the people who might be adversely
37:52 - affected by ai helps us reach better
37:54 - outcomes for everyone in the end
37:57 - speaking of
37:58 - of future the future and jobs
38:01 - a lot of our audience will be starting
38:03 - their career in probably five to ten
38:04 - years
38:06 - um what kind of jobs do you all think
38:08 - will be available and won't be available
38:11 - anymore so let's let's help these these
38:13 - young people out
38:15 - uh what kind of jobs will be available
38:16 - and won't be available anymore what do
38:19 - you think
38:25 - that's a tough question i don't want to
38:27 - be on record saying anything um i was
38:29 - gonna say i i do think something i
38:32 - wanted to point out was the point amanda
38:33 - had made earlier around um you know who
38:36 - can be in the ai space i think very
38:39 - um earlier there was an assumption that
38:41 - you had to be a computer scientist or
38:43 - you had to be a mathematician to have
38:45 - something interesting and important to
38:46 - say about
38:47 - ai and how it's developed and how it's
38:49 - built and how it's managed um and uh and
38:53 - how it's deployed and i think now we're
38:54 - realizing that we need the help of all
38:56 - these
38:57 - different types of people with different
38:59 - interests and different skills so um i
39:01 - think that there's going to definitely
39:02 - be this interesting new opportunity for
39:05 - you know anthropologists and social
39:07 - scientists and um you know
39:10 - people with all kinds of different
39:12 - skills and um and uh expertise to be
39:16 - able to participate in this field so
39:18 - even if you're someone that you know you
39:20 - think ai is really cool you think
39:22 - machines are really cool but you care
39:25 - about how they impact society and you
39:27 - want to study that or you think that
39:28 - that's a more interesting question
39:30 - i think in the future there will
39:31 - definitely be more
39:33 - opportunity for for that kind of that
39:35 - kind of job or that kind of role
39:37 - yeah ethicists
39:38 - philosophers
39:40 - they're all needed
39:43 - i i completely agree with that it's
39:45 - actually one of the parts of my job that
39:47 - i love most which is getting to work
39:50 - with people from all different works of
39:52 - life from all sorts of different
39:54 - disciplines
39:55 - they make me think about things that i
39:57 - haven't thought about before and in
39:58 - different dimensions and i think that's
40:01 - that's really exciting
40:02 - um certainly you might
40:04 - end up with a job like mine that didn't
40:06 - exist
40:08 - even a couple of years ago so i think
40:10 - there is going to be you know a whole
40:12 - new um
40:14 - sort of discipline that um evolves
40:17 - around
40:18 - the practice of of responsible ai you
40:21 - know how what are the sorts of things
40:24 - that you need to think about when you
40:26 - are
40:26 - baking responsible ai considerations
40:29 - into the way that you're building
40:31 - systems how do you move from you know a
40:34 - principle to practice so i think there's
40:36 - going to be a whole new discipline of of
40:39 - people
40:40 - working on the hands-on ways in which we
40:42 - can
40:44 - help make positive choices when we are
40:46 - building these systems and there's going
40:48 - to be a whole discipline of people who
40:51 - who look at these systems from the
40:52 - outside in and provide a perspective
40:55 - about how they doing the things that we
40:56 - want them to do or that we expect them
40:58 - to do and kind of auditing systems and
41:01 - and providing that outside-in
41:03 - perspective and deb's done some of this
41:05 - really cutting edge work already but i
41:07 - can really imagine a whole new
41:10 - profession arising out of that type of
41:12 - work
41:17 - got it
41:19 - yeah i think one of the reasons it's
41:21 - hard to give a very definitive answer
41:22 - here is that when we think of automation
41:24 - i think a lot of people think of the
41:26 - automation of like low skilled work um
41:28 - and they think of like robotics but one
41:30 - of the interesting things about ai is
41:32 - that you actually see like when ai can
41:35 - do tasks it's often like high skilled
41:36 - tests or knowledge work um and so i
41:39 - think that that should just make us
41:40 - aware of the fact that this is something
41:42 - that's going to affect a whole host of
41:43 - professions and so it's actually
41:45 - probably better to just not have this
41:47 - sense that it's like well here are the
41:48 - jobs that are completely safe and hear
41:50 - the jobs that are like um that are at
41:52 - risk and obviously we should think about
41:54 - that but you shouldn't kind of like i
41:56 - think that it's better for us to think
41:58 - actually assume that this is going to
41:59 - kind of affect everyone how are we going
42:01 - to deal with that in a way that makes
42:02 - sure that no one is left out
42:04 - thanks amanda
42:06 - yeah i think amanda's point is super
42:08 - important you know part of the reason
42:10 - why people learn about things like
42:12 - science and physics is not necessarily
42:14 - because we need a bunch more people
42:15 - studying black holes but it's because
42:17 - when you understand something about that
42:19 - it allows you to make better decisions
42:20 - about a lot of other things that that
42:22 - impact your life
42:24 - and you know in terms of future jobs
42:25 - there'll be a lot more people who will
42:27 - be not necessarily building ai but we'll
42:30 - be thinking about how they can use the
42:32 - kinds of ai that exists in terms of
42:34 - enhancing the jobs that they do and so
42:37 - to that extent for someone to learn a
42:39 - little bit about ai no matter what they
42:41 - do will be important because it'll
42:43 - probably be harnessed in their job in
42:45 - some way in the future probably in a lot
42:47 - of ways we can't even see right now but
42:49 - understanding something about the
42:51 - technology helps you make better choices
42:53 - about how to use it and deploy it
42:55 - yeah thank you um audience uh raise your
42:59 - hand i think there's probably some
43:00 - button where you can raise your hand
43:03 - um raise your hand if you want to learn
43:05 - more about ai
43:09 - panelists can you see the the number of
43:10 - hands being raised
43:14 - awesome okay so let's let's help them
43:16 - out
43:17 - uh
43:18 - if i'm a kid who wants to learn about ai
43:20 - where would i start
43:26 - uh i really like to find a project that
43:29 - i think is interesting and so i
43:31 - recommend looking at things like kaggle
43:33 - competitions they just have like data
43:35 - sets and problems and i think it's good
43:37 - to acquire skills by like finding a
43:39 - project you care about and then
43:41 - acquiring the skills in order to like
43:42 - solve that project and there's lots of
43:44 - like online resources like fastai that
43:46 - will help you do that so yeah i
43:48 - recommend finding a problem you want to
43:49 - solve and then like finding the tools
43:52 - that help you solve it
43:53 - thank you
43:56 - yeah i was going to recommend like
43:57 - online
43:59 - courses like fast ai um
44:01 - also a lot of coursera
44:03 - offerings just make it very accessible
44:06 - to just
44:07 - start hearing the terminology and
44:09 - learning what the words mean and
44:11 - navigating your way through i think um
44:14 - this field is really interesting in the
44:15 - sense that a lot of the like
44:17 - foundational knowledge is
44:19 - not completely inaccessible it's
44:21 - something that people have worked really
44:22 - hard to make
44:24 - um tangible in certain ways so there's a
44:26 - lot of resources even for
44:28 - um younger kids to explain some of these
44:31 - concepts at a very basic level so i
44:33 - think that it's a great place to just
44:34 - start
44:35 - and once you're getting familiar with
44:37 - the concepts and you have an intuition
44:38 - as to you know some of these questions
44:40 - of what is machine learning for example
44:42 - what is data how does how those things
44:44 - are connected you know when you start um
44:47 - understanding these basic concepts then
44:49 - um once you're ready to you know to
44:51 - start learning the math or start coding
44:53 - um you'll feel much more confident
44:55 - thanks bud
44:59 - i i would
45:00 - endorse
45:02 - both of those uh bits of guidance i
45:04 - would also say sometimes you might find
45:06 - some interesting challenges in places
45:08 - where you are already so for example um
45:11 - i know that with our minecraft education
45:14 - offering we have
45:16 - some challenges within uh minecraft that
45:19 - help you understand ai concepts through
45:22 - trying to solve for real problems like
45:24 - climate change so uh always keep your
45:26 - eyes open i would say another way to try
45:29 - and um sort of develop your thinking
45:33 - about how uh these sorts of ethical
45:36 - challenges could materialize is to pay
45:38 - attention to the media and when you see
45:41 - um you know the increasing reporting
45:43 - that there is today of um of situations
45:47 - where um you know ai's potential has
45:50 - been realized equally situations where
45:53 - things have gone wrong you can ask
45:55 - yourself questions about who might be
45:58 - impacted how
46:00 - could that have panned out differently
46:02 - does this remind us of a challenge that
46:05 - we might have encountered in the past or
46:07 - is this a whole new challenge i think by
46:10 - sort of having that critical mind when
46:12 - you're just hearing about
46:14 - um
46:15 - events as they unfold from your from
46:18 - your news sources that can help start to
46:20 - build your
46:21 - your muscle and thinking about the
46:23 - implications of of these new
46:26 - technologies thank you natasha
46:28 - any resources you point our students to
46:30 - maron
46:31 - well there are a tremendous amount of
46:33 - online resources as many other folks
46:36 - have alluded to on coursera edx if you
46:39 - do other searches or uh you know
46:41 - available ai resources at a variety of
46:44 - levels and i would just the one thing i
46:46 - would add is the the hardest part is
46:47 - often just starting um and sometimes
46:50 - when you start this was true for me it's
46:52 - true for a lot of students i work with
46:54 - sometimes when you start it can feel a
46:56 - little bit overwhelming because you feel
46:57 - like there's all these terms you need to
46:59 - know and sometimes someone throws in
47:01 - some math that's complicated or whatever
47:03 - the case may be and the thing to keep in
47:05 - mind is that a lot of people actually
47:07 - when they're learning uh
47:09 - have challenges or have struggles when
47:11 - they begin with and the important thing
47:12 - is just to work through it and you'll
47:13 - get there and i see that and lots of
47:15 - students who you know if they feel that
47:18 - it's hard and they just keep at it and
47:20 - they work through it and they they do
47:22 - well so understanding something about
47:24 - what you don't yet know when you start
47:26 - jumping in but if you feel like it's
47:28 - overwhelming don't worry many people
47:31 - feel that way and it's it turns out if
47:33 - you just work at it over time all the
47:35 - pieces begin to fit and make sense so i
47:37 - just keep at it
47:38 - yeah and that's exactly why code.org has
47:41 - created a bunch of resources around ai
47:43 - you can find them at code.org
47:46 - ai or just
47:47 - type into a search engine code.org space
47:50 - ai and you'll you'll find the page
47:52 - uh there are videos there lesson plans a
47:55 - lesson plan about ethics and you'll
47:57 - actually find like i said videos but
47:59 - videos featuring our panelists as well
48:00 - they're they're famous folks uh beyond
48:03 - just their their uh day-to-day jobs
48:06 - um
48:07 - so uh with that i want to thank our
48:09 - panelists for joining us today
48:12 - this is obviously going to be recorded
48:14 - so
48:14 - whoever's watching this in the future
48:16 - hello
48:17 - um
48:18 - uh but if you're not watching this in
48:20 - the future you can check out this
48:22 - afternoon's episode of code bites dance
48:24 - party it's uh happening at 1 30 pacific
48:26 - 4 30 eastern you can go to code.org code
48:29 - bytes b-y-t-e-s
48:31 - for details and again i talked about
48:33 - code.org ai
48:36 - uh panelists audience thank you for
48:38 - joining us today
48:39 - we will see you later have a happy
48:41 - computer science education week bye
48:55 - bye