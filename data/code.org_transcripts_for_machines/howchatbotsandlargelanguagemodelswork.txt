hi I'm Mira morati I'm the chief technology officer at open AI the company that created child GPT I really wanted to work on AI because it has the potential to really improve almost every aspect of life and help us tackle really hard challenges hi I'm Crystal Valenzuela CEO and cofounder of Runway Runway is a research company that builds AI algorithms for storytelling and video creation chatbots like Chad gbt are based on a new type of AI technology that's called large language models so instead of a typical neural network which trains on a specific task like how to recognize faces or images a large language model is trained on the largest amount of information possible such as everything available on the internet it's raining to then be able to generate completely new information like to write essays or poems have conversations or even write code the possibilities seem endless but how does this work and what are its shortcomings let's Dive In while a chatbot built on a large language model may seem magical it works based on some really simple ideas in fact most of the magic of AI is based on very simple math concepts from statistics applied billions of times using fast computers the AI uses probabilities to predict the text that you wanted to produce based on all the previous texts that it has been trained on suppose that we want to train a large language model to read every play written by William Shakespeare so that it could write new plays in the same style we'd start with all the texts from Shakespeare's plays stored letter by letter in a sequence next we'd analyze each letter to see what letter is most likely to come next after an eye the next most likely letters that show up in Shakespeare plays are s or n after an s T C or h and so on this creates a table of probabilities with just this we can try to generate new writing we pick a random letter to start starting with the first letter we can see what's most likely to come next we don't always have to pick the most popular choice because that will lead to repetitive Cycles instead we pick randomly once we have the next letter we repeat the process to find the next letter and then the next one and so on okay well that doesn't look at all like Shakespeare it's not even English but it's a first step this simple system might not seem even remotely intelligent but as we build up from here you have to be surprised where it goes the problem in the last example is that at any point the AI only considers a single letter to pick what comes next that's not enough context and so the output is not helpful what if we could train it to consider a sequence of letters like sentences or paragraphs to give it more context to pick the next one to do this we don't use a simple table of probabilities we use a neural network a neural network is a computer system that is Loosely inspired by the neurons in the brain it is trained on a body of information and with enough training it it can learn to take in new information and give simple answers the answer is always include probabilities because there can be many options now let's take a neural network and train it on all the letter sequences in Shakespeare's plays to learn what letter is likely to come next at any point once we do this the neural network can take any new sequence and predict what could be a good next letter sometimes the answer is obvious but usually it's not it turns out this new approach works better much better by looking at a long enough sequence of letters the AI can learn complicated patterns and it uses those to produce all new texts it starts the same way with a starting letter and then using probabilities to pick the next letter and so on but this time the probabilities are based on the entire context of what came beforehand as you see this works surprisingly well now a system like chat GPT uses a similar approach but with three very important additions first instead of just training on Shakespeare it looks at all the information you can find on the internet including all the articles on Wikipedia or all the code on GitHub second instead of learning and predicting letters from just the 26 choices in the alphabet it looks at tokens which are either full words or word parts or even codes and third difference is that a system of this complexity needs a lot of human tuning to make sure it produces reasonable results in a wide variety of situations while also protecting against problems like producing highly biased or even dangerous content even after we do this tuning it's important to note that this system is still just using random probabilities to choose words a large language model can produce unbelievable results that seem like magic but because it's not actually magic it can often get things wrong and when you get things wrong people ask does a large language model have actual intelligence questions about AI often spark philosophical debates about the meaning of intelligence some argue that a neural network producing words using probabilities doesn't have real intelligence but what isn't under debate is that large language models produce amazing results with applications in many fields this technology is already been used to create apps and websites help produce movies and video games and even discover new drugs the rapid acceleration of AI will have enormous impacts on society and it's important for everybody to understand this technology what I'm looking forward to is the amazing things people will create with AI and I hope you dive in to learn more about how AI works and explore what you can build with it foreign