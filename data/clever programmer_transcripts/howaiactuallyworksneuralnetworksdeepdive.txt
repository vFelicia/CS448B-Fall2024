00:00 - what they were finding was that the
00:01 - model was able to get rewards for things
00:03 - that it wasn't supposed to what yeah
00:06 - what that could mean is that model is
00:09 - going out of control would be able to
00:11 - change its own reward to teach itself to
00:13 - be like hurting human being is actually
00:15 - positive that freaked me out I'm going
00:17 - to tell you like AI Extinction risk is
00:19 - actually not far away wow that's crazy
00:24 - I'm Harper hi everyone I got my
00:26 - undergrad and graduate degrees from
00:28 - Stanford focusing in AI and then I was
00:30 - at meta for 4 years doing AI I started
00:32 - on the news feed ranking team that I
00:34 - loved I loved algorithms how have you
00:36 - seen AI change from the past few years
00:40 - in the past you give it the features
00:42 - whereas neural networks learn features
00:44 - by themselves do you L just go here
00:47 - here's everything figure it the [ __ ] out
00:49 - yeah we don't know what's going on in
00:50 - there however on validation data or on
00:53 - the test data you can look at where the
00:55 - model is failing these are the ones that
00:56 - work and these are the ones that we're
00:57 - not getting right I thought we figured
00:59 - this this [ __ ] out at this point but we
01:01 - at the start what is up everybody my
01:04 - name is Nas and I'm here with Harper
01:05 - from Harper Carol Ai and today we're
01:08 - going to be talking about all things Ai
01:11 - and I'm going to ask her a ton of
01:13 - questions CU I myself want to learn she
01:16 - herself actually studied at Stanford
01:19 - correct and you worked as well at meta
01:22 - is that correct so I'll have you
01:24 - introduce yourself cuz you probably know
01:26 - you you know a lot a lot more about
01:27 - yourself so I'll have you introduce
01:28 - yourself okay I'm Harper hi everyone I
01:31 - am from Harper Carol AI I do AI
01:33 - education online and I got my undergrad
01:36 - and graduate degrees from Stanford in
01:39 - computer science focusing in AI so the
01:41 - AI track and then I was at meta for four
01:43 - years doing AI I started on the news
01:46 - feed ranking team and then moved to
01:48 - Integrity intelligence where we W worked
01:50 - on protecting vulnerable communities
01:54 - from misinformation and and then I left
01:56 - meta and was at a GPU startup running AI
01:59 - there and started teaching AI GPU
02:02 - startup yes yes and then started
02:04 - teaching AI for that startup to get
02:06 - people onto the
02:07 - platform found that that was really
02:09 - successful and that there was a real
02:11 - desire for it and that's also the
02:12 - intersection of everything I love is
02:14 - learning and teaching and making videos
02:17 - and working with people so I have now
02:19 - gone off to do that full-time and I love
02:22 - it and yeah so it's been really great
02:24 - that's amazing I love that and I'm
02:26 - curious what got you into this AI you
02:29 - know not Revolution but into the AI
02:31 - field what what what piqued your
02:33 - interest there so I got to Stanford and
02:35 - I decided to come as a math major I was
02:38 - going to do business mathematics oh okay
02:40 - that's different yeah I didn't do
02:41 - computer science in high school uhhuh
02:43 - and I loved math I've always been a math
02:46 - person and I love math too isn't it the
02:48 - best so good like long equations are the
02:51 - best like just like when you when you
02:53 - spend two pages on trying to solve the
02:55 - equation oh my God it's so good it's so
02:58 - good and for the math nerds applied math
02:59 - I'm more of an applied math person don't
03:01 - judge me but I like applied Math More
03:03 - sorry but I went to Stanford for Math
03:06 - and decided to actually take a computer
03:09 - science course my freshman fall cuz a
03:10 - bunch of my housemates were taking it
03:12 - and they encouraged me to try it so I
03:13 - was like all right I'll try it and first
03:15 - class was really difficult second class
03:17 - was an algorithms class first class was
03:19 - JavaScript like making games kind of
03:20 - thing second class was
03:23 - C++ uh algorithms that I loved I loved
03:26 - algorithms that's when everything really
03:28 - clicked for was like Wow cuz it's kind
03:30 - of similar to mathematical thinking but
03:31 - computer science teaches you to think
03:33 - Super logically you saying no is it the
03:35 - challenging part of it that keep made
03:38 - made it very interesting for you yeah
03:39 - and it's like the really logical brain
03:42 - right like it it snaps into that kind of
03:45 - place and so algorithms optimal
03:47 - algorithms was really fascinating to me
03:49 - and so from then on I was like okay
03:50 - computer science that's the thing and
03:53 - there are various tracks that we could
03:54 - go into in computer science and I was
03:56 - interested in HCI human computer
03:58 - interaction which is kind of like design
04:00 - kind of thing but then I was interested
04:03 - in Ai and I heard that it was very mathy
04:07 - mm and I like math and I heard that you
04:09 - like algorithms and I like algorithms I
04:11 - like math I like algorithms it all comes
04:12 - together HC was a little bit less
04:14 - algorithm heavy um as you would expect
04:17 - yes and um I just loved it I took a
04:19 - class in it and loved it it was so
04:21 - fascinating that was back when large
04:23 - language models were not
04:25 - nearly yeah like the outputs were
04:27 - hilarious rather than like wo this is so
04:30 - good they would be like wow look how
04:31 - funny this is but it's so cool whereas
04:33 - now it's like you know next level but
04:35 - yeah I got into it learned about you
04:38 - know natural language processing which
04:39 - is large language models computer vision
04:42 - deep learning which is like neural
04:43 - networks how that applies to computer
04:45 - vision natural language processing just
04:47 - like a a large breath and depth of
04:50 - knowledge so yeah yeah it was a very
04:52 - different field back then CU I actually
04:53 - gone to AI a little bit too myself oh
04:55 - okay uh I think about four to five years
04:57 - ago when I started my first startup we
05:01 - wanted to build a product that would
05:03 - tell you what to dress when we ourselves
05:05 - knew nothing how to dress okay that's
05:08 - why you need machine learning for it so
05:10 - you guys don't have to put in your input
05:11 - at all we literally had this like it was
05:12 - like a Tinder swipe left swipe swipe
05:15 - right situation when you would swipe
05:16 - left swipe right on like clothing and
05:18 - then it would learn based what based on
05:20 - what you like and they would recommend
05:21 - you things based on based on that
05:24 - complex algorithm there but like before
05:26 - it was like learning like the simple
05:28 - thing like linear regression Bas out
05:30 - good I still remember those very I mean
05:33 - they're not basic of course but very
05:35 - different than what it is right now and
05:37 - it has changed a lot over the years and
05:41 - I'm very excited for where it's also
05:43 - going to go in the future as well by the
05:45 - way if you want to learn from this video
05:47 - 10 times faster with eii me and my
05:49 - co-founder Kazi have created a product
05:51 - called poppy eii that helps you bring in
05:53 - any YouTube video any website any voice
05:56 - note or even image and allow you to talk
05:59 - talk to it instantly in a whiteboard
06:01 - canvas think of it as your own
06:04 - whiteboard with AI superpowers imagine
06:08 - you drag in this YouTube video that
06:09 - you're just watching right now imagine
06:11 - you dragg in maybe an image for example
06:14 - like this or maybe you want to record a
06:16 - voice message of yourself talking about
06:19 - a specific topic and drag that in to
06:22 - talk to it instantly you can do that as
06:24 - well here with this video we can for
06:26 - example get key insights really quickly
06:29 - so maybe you don't want to watch the
06:30 - whole video you can just get key
06:32 - insights so super fast or maybe you want
06:35 - to ask questions about what Harper or I
06:38 - said for example relo which is something
06:40 - that me and Harper talk about on an
06:43 - activation function for neural networks
06:45 - we'll cover that later on or maybe you
06:47 - want talk about and ask question about
06:48 - linear regression and logistic
06:50 - regression and what she says about that
06:52 - and how that maybe plays into the real
06:54 - world amazing way of just learning 10
06:57 - times faster taking notes and be able to
07:00 - create content and also be able to
07:03 - understand things so much quicker if
07:06 - you're interested in doing this right
07:08 - now like I said we have a crazy deal for
07:12 - some of our first few customers so far
07:14 - the people have been loving it we've had
07:16 - some amazing testimonials and reviews
07:18 - from researchers from vas from students
07:22 - from even Founders and they're all
07:24 - displayed right here so if you're
07:26 - interested in checking it out go ahead
07:27 - and click the link below check out papy
07:29 - and we'll see you in there so I'm really
07:32 - curious you know how have you seen AI
07:35 - change from the past few years to now
07:38 - how has it actually progressed cuz I
07:41 - feel like it's progressed a lot it has
07:43 - and I think the main shift came with a
07:44 - Transformer which I think was in 2017
07:47 - well the Transformer the Transformer
07:48 - it's an attention based mechanism with
07:50 - neural network so it's like it's a type
07:52 - of deep learning okay a Ty of neural
07:53 - network okay and it adds this layer this
07:56 - attention based layer okay to look at
08:00 - how like the structure of the words and
08:02 - then you basically learn I'll just like
08:04 - do a super broad overview of this but
08:05 - like you're able to learn over and over
08:09 - again how all the words connect with
08:11 - each other and so like you have
08:13 - different layers capturing different
08:16 - semantic meanings and that's what the
08:17 - layers are learning is like the
08:19 - different connections between them but
08:21 - something about the attention layer and
08:23 - getting these links between all the
08:24 - different words layer is that neural
08:26 - network This falls under a neural
08:28 - network and inter So when you say neural
08:31 - networks what does that actually mean
08:32 - because to a lot of people watching this
08:34 - to a lot of people even to me it's very
08:36 - much of a black box I know it's a large
08:38 - question no that's okay I actually have
08:40 - a course on this and I've started I just
08:42 - made an architecture class it's called
08:44 - the 10 days of AI Bas oh that's amazing
08:45 - is that your Instagram or it's my
08:47 - Instagram so my Instagram has like 90c
08:49 - reels on this and then I have released
08:51 - all 10 days on Instagram and then I have
08:53 - an accompanying video in process for
08:55 - each of those 10 days and I have done
08:57 - days 1 through three which are like the
08:59 - day day one is what's the difference
09:00 - between Ai and ml day two is about data
09:03 - day three is model architecture where we
09:04 - talk about neural network architecture
09:06 - and I'm happy to go into that here and
09:07 - then four 5 6 7 8 9 10 are on their way
09:11 - but if you want to watch Days 1 through
09:13 - 10 on Instagram you can and then the
09:14 - longer videos are on yeah and those will
09:15 - be of course Down Below in description
09:17 - guys if you go if you guys want to watch
09:18 - that so neural networks okay oh let's go
09:21 - I'm so excited let's go yes the main
09:24 - components of a neural network and again
09:26 - deep learning refers to using neural
09:29 - network
09:29 - yes so like when when we say the Tesla
09:32 - autopilot system right that'll be using
09:35 - networks uh for example like jbt is not
09:38 - near that is it is jbt well we're
09:41 - assuming it is unless they have some
09:42 - crazy thing but yes we're assuming
09:44 - because we don't know actually know what
09:45 - the architecture is tell us it's
09:47 - probably yeah yeah so it's like anything
09:49 - you just feed in a feed in data and it
09:52 - will kind of automatically train itself
09:55 - and think even more on top of that is
09:57 - that correct so at the fundamental level
10:00 - neural networks learn features by
10:02 - themselves whereas in the past or other
10:04 - types of AI models you give it the
10:06 - features for example if you're passing
10:08 - in um you have structured data yes so
10:10 - you say you might have a database with
10:13 - rows and columns and the columns are the
10:15 - features where for example if you're
10:16 - training a health model you have like
10:18 - height weight yeah you know whatever all
10:20 - those Health metrics and then the
10:22 - regression model or whatever model
10:23 - you're using learns how those features
10:26 - relate to get the output right so like
10:28 - for example you know you know maybe
10:29 - let's just say height weight boom boom
10:31 - boom and health score the health and
10:34 - heal score and so then you train a model
10:36 - that says figure out how all of these
10:39 - features height weight whatever
10:41 - transforms into this health score this
10:42 - health score and then they will be able
10:43 - and then based on new input data right
10:45 - based on new input data then what's
10:47 - going to happen is that it will it will
10:48 - actually give you new health score
10:49 - because it's learned over the past from
10:51 - different feutures exactly and that is
10:53 - what machine learning is at it at its
10:55 - core is like learning from data and so
10:57 - those feature-based models
10:59 - right um so that's strictly machine
11:01 - learning right that's not like neural
11:03 - networks right the strict neural
11:05 - networks are a type of machine learning
11:06 - uh okay okay so logistic regression is a
11:08 - type of machine learning because you're
11:09 - learning from data neural networks are a
11:11 - type of machine learning because you're
11:12 - learning from data uhhuh deep learning
11:15 - is a type of machine learning uhhuh and
11:17 - neural networks are a type of deep
11:19 - learning holy
11:20 - [ __ ] I hope you zoom in on that we can
11:23 - make like a little graph for the example
11:25 - that we just talked about right what
11:27 - kind of an algorithm would that be is
11:29 - like I know there's like some very like
11:30 - naive based algorithms like what what
11:32 - are some algorithms for something like
11:34 - that like those machine learning yeah
11:36 - okay so for for something that's more
11:37 - structured you could have a model that
11:40 - is just um like fitting to it so you
11:44 - could do like a decision tree where you
11:47 - train a tree to say like it kind of move
11:51 - like you know how it's you know how with
11:52 - trees where it's like it goes into yeah
11:54 - it goes in it goes like start with a
11:55 - node and then there's children node and
11:56 - children and so forth and so you could
11:58 - train a model to figure out which of
12:00 - those features are the most decision
12:02 - trees are great because they are
12:04 - interpretable which means they tell you
12:06 - which features are the most important M
12:09 - interesting okay okay so they'll know
12:11 - like you can give it a bunch of features
12:13 - they'll say hey this feature makes the
12:14 - most amount of impact is that correct
12:16 - exactly and then it helps you prune ones
12:17 - that don't have an impact and then you
12:18 - can make it even faster and optimize it
12:20 - does automatically do that for you or do
12:22 - you have to SP or do you have to test it
12:23 - and actually oh I think you have to like
12:25 - you know rank the like you have to like
12:27 - code for it to show the um the
12:30 - correlation between the feature and the
12:33 - output so they could be highly
12:34 - positively correlated or highly
12:36 - negatively correlated but either High
12:38 - correlation whether it's positive or
12:40 - negative is highly correlated whereas if
12:42 - it's like at zero so it's positive 1
12:43 - negative 1 if it's at zero it's not
12:45 - really correlated to the output if we're
12:47 - if we're going to talk about for example
12:49 - like just say in terms of AI when let's
12:52 - just say I say okay I give you a picture
12:54 - of a cat right and then I say this is a
12:56 - cat right and then I give you a picture
12:57 - of a dog I say this is a dog I give you
12:59 - a bunch of pictures yeah would that be
13:01 - necessarily using a similar algorithm
13:03 - like that or would that be like neural
13:05 - networks you would probably want to use
13:06 - a neural network for that really so that
13:08 - is unstructured data so we have
13:09 - structured data where we know the
13:10 - features we have unstructured data where
13:12 - it's just like an array of for example
13:15 - words or you know the pixels that go
13:17 - into an image the numbers that make up
13:19 - the image yeah and or Matrix Matrix the
13:23 - model has to learn the features itself
13:25 - itself I see so when you are passing in
13:27 - like a sentence and then so for example
13:30 - with the Health Data if we were to train
13:32 - a neural network on that we would say hi
13:35 - I'm 5'9 I am you know this tall or this
13:39 - age this is my whatever and so then the
13:41 - model would learn given this block of
13:43 - text just like a literally a block of
13:45 - text just which features map to a health
13:47 - score so it would and we would see that
13:49 - it is able to highlight like height
13:52 - weight like it will yeah but the
13:55 - features that it
13:57 - finds I kind of M spoke because we
13:59 - actually wouldn't be able to see what it
14:02 - is using in the sense that it is not
14:05 - interpretable so like we wouldn't know
14:07 - what features it's actually using to get
14:09 - the output why hear the Black Box term
14:11 - so you might have heard of AI being a
14:13 - black box it's like we don't know what
14:14 - it's doing in there and I can talk about
14:16 - how the structure works why it's a black
14:17 - box and so we can get into that but like
14:20 - yeah we don't know what's going on in
14:21 - there however there has been a lot of
14:23 - research on interpretability including a
14:25 - lot of work done by anthropic which is
14:28 - one of the yeah CL it's a leading
14:30 - company in AI
14:32 - safety yeah they're great they do a lot
14:34 - of research also I love CLA it's amazing
14:36 - um they do a lot of research in AI
14:38 - safety and interpretability they did
14:40 - some research and they found that they
14:44 - were able to find a set of neurons in
14:46 - the model so we'll talk about this that
14:49 - relate to the San Francisco Golden Gate
14:51 - Bridge oh and so then when they
14:53 - Amplified those neurons in the model the
14:57 - model would talk about the golden gay
14:58 - Bridge so like where should I go on
14:59 - vacation the model would be like to the
15:01 - Golden Gate Bridge like oh I want to go
15:02 - for a walk like what should I do it' be
15:04 - like why don't you walk across the
15:05 - Golden Gate Bridge and like everything
15:07 - it recommended like it was nonsensical
15:10 - like it would just like make up stuff to
15:11 - get you to go to the gold gate that's
15:13 - funny like what are you I want to go eat
15:15 - go to the Golden Gate Bridge pick pick
15:17 - up a sandwich go to the Golden Gate
15:19 - Bridge exactly so that was really cool
15:21 - that was one of the most you know
15:23 - pivotal moments in interpretability and
15:25 - that just came out a few weeks ago I
15:27 - thought we figured this this [ __ ] out at
15:29 - this point I thought we were like oh we
15:31 - were there this is like it but we at the
15:35 - start yeah we're on the start but
15:37 - luckily we have people we have companies
15:39 - like anthropic doing this work and is
15:41 - CPT you think doing that too or like I
15:43 - have no idea what's going on inside that
15:45 - company that's so interesting yeah okay
15:48 - so that in itself is like when it's
15:50 - feature-based versus what I didn't know
15:53 - what's really crazy was the fact that
15:55 - neuron networks don't actually you don't
15:57 - know what the features are also let me
15:59 - just correct myself there are some ways
16:01 - so for example like with a with a
16:03 - computer vision mhm um model there are
16:06 - some ways to look at the layers to see
16:08 - like oh this this layer is you know
16:11 - capturing outlines and this layer is
16:13 - Capt like there are some tiny things at
16:15 - least I know with computer vision but to
16:16 - my knowledge it's difficult to interpret
16:18 - they're very difficult to interpret and
16:19 - that's why they're called a black box
16:20 - well how do you okay so how do you know
16:22 - how to get better results for example
16:24 - right from a your own network right
16:25 - let's just say the Tesla autop autopol
16:26 - system will continuously you know get
16:28 - better over time
16:30 - you know and do you get better results
16:32 - byting out different ways of giv input
16:35 - data for example I know Tesla a system
16:38 - cuz I used to drive I I had a Tesla and
16:40 - I was like I [ __ ] love their system
16:41 - it was just so smart yeah and is it like
16:44 - they feed in different types of data
16:46 - like I know for example I remember he
16:47 - was talking about El mus was talking
16:49 - about this at I think they had like AI
16:51 - day or something like that okay um and
16:53 - he was talking about how you know they
16:54 - feed in different types of data one is
16:56 - like monochrome for example images right
16:58 - which which is more black and white for
17:00 - example another one is colorful and then
17:01 - they have a zoomed in version of it
17:03 - that's like they feed that in MH so then
17:06 - how do you know how to improve the model
17:09 - like how does it just get you just go
17:11 - you L just go here here's everything
17:14 - figure it the [ __ ] out well so one thing
17:17 - you can do is you can look at the so
17:19 - when you're training and when you're
17:20 - testing so on validation data or on the
17:23 - test data you can look at where the
17:25 - model is failing so what is it getting
17:27 - wrong because ah okay write exactly if
17:29 - it's a classification problem what kinds
17:31 - of things what kinds of samples is it
17:32 - not classifying and then you can try to
17:34 - maybe cluster them or see like what is
17:36 - similar about it so you might notice if
17:38 - you have a computer vision problem where
17:40 - you are scanning you know employees
17:43 - coming into the office you know from the
17:47 - from the front gate zero security and
17:48 - zero press maybe it's like an automatic
17:51 - I don't know I'm just thinking this
17:52 - right now it's like an automatic system
17:54 - that like checks people in it's it they
17:57 - check it in to make sure it's
17:59 - I was just like I saw this I saw this
18:01 - Instagram real which is really funny
18:02 - they're like he gave every employee an
18:04 - apple Vision Pro and he's like I give
18:06 - him so I can track their eye mov and I
18:07 - can track everything they do I'm like
18:09 - wow I mean it was a funny it was a funny
18:11 - real it was meant to be funny you know
18:13 - it was a joke but but still I was like
18:15 - oh that's that's actually kind of funny
18:17 - spooky oh but so you can train these for
18:21 - example let's say we make this data set
18:23 - where we have people coming in to the
18:25 - office and we find that when we're
18:27 - training this model so we haven't
18:30 - actually trained the model yet to do it
18:31 - we're just taking pictures of people as
18:32 - are coming into the office to train this
18:34 - data set so their image plus their badge
18:37 - you know their name and badge okay and
18:39 - as we're training this model let's say
18:40 - they classify it as employee and not
18:42 - employee we find that in some cases
18:45 - they're not getting it right either
18:46 - they're classifying them as an employee
18:47 - when they're not or they're classifying
18:49 - them as not employee when they are an
18:52 - employee so we look back at the images
18:54 - and we find that most of the ones that
18:56 - are not doing well so you can cluster it
18:58 - where you kind of this is a type of um
19:00 - so this is these are bad images these
19:02 - are good images or sorry oh not not um
19:05 - clustering oh yeah these are the ones
19:07 - that work and these are the ones that
19:08 - we're not getting right correct so let's
19:10 - cluster the ones that we aren't getting
19:11 - right and let's see like what they look
19:13 - like let's understand them and so we
19:15 - look at them and we see they're actually
19:16 - mostly of one type it's and we notice
19:18 - when we go in and look they're dark it's
19:21 - nighttime employees have come in at
19:22 - night and so we're like oh we don't have
19:24 - enough data on this like we need to
19:27 - figure out how to do this so maybe we we
19:29 - if it's dark we upscale the image so we
19:32 - like we'll see what time it is and then
19:34 - if it's dark or if it's you know then
19:36 - we'll apply some kind of transformation
19:38 - onto the image to make it lighter or we
19:41 - just train on a lot more dark images but
19:42 - if it's just too dark we might have to
19:44 - apply this transformation so it's this
19:46 - iterative process of like looking at
19:49 - your accuracy metrics your your
19:52 - evaluation metrics which might be
19:53 - accuracy which might be Precision there
19:55 - are all these different types of metrics
19:56 - you can look at but usually it accounts
19:59 - for like like ones that you guess right
20:00 - ones that you guessed wrong in terms of
20:02 - like it is true and you guess it as true
20:05 - it is false and you guess it as false or
20:07 - it is true and you guess it as false and
20:08 - it's false you guess it as true so those
20:10 - are the four different types of I see I
20:12 - see I see I see yeah but anyway so yes
20:15 - and so we go in we look and we say okay
20:16 - we need to add we need to augment this
20:18 - data set different or some people some
20:20 - things that people do to make their data
20:22 - set larger is they'll again apply other
20:23 - types of transformation so they'll like
20:25 - flip the image or they'll turn it
20:27 - sideways turn it horizont even that will
20:29 - help for example yeah yeah for computer
20:31 - vision things like that like we'll
20:32 - rotate it like it it helps you take your
20:35 - small data set that you might have and
20:37 - make it larger because you're applying
20:38 - all these transformations to it wow and
20:40 - you know the labels already right that's
20:42 - so interesting so like I mean can I at
20:45 - the same time let's just say if I if I
20:47 - was to have the ability to also use for
20:51 - example pictures that are at night right
20:53 - mhm can I solve it just by giving them
20:56 - more night pictures potentially so so I
20:58 - don't know it depends on your system but
21:00 - you could potentially assuming there is
21:02 - enough light right cuz like if it's just
21:04 - a black like screen like you're not
21:07 - going to be able to sense everything
21:09 - it's like oh just [ __ ] dark screen
21:11 - right exactly in which case it's like
21:13 - maybe in that case you just like turn
21:14 - off the system or you know the employee
21:16 - has to sign in with their badge or
21:17 - whatever I see that's so interesting all
21:20 - these neuron Nevels that we have right
21:22 - that let's just say jgpt we've got you
21:24 - know anthropic working on this we've got
21:26 - Tesla working on this are they all at
21:29 - this end of the day the same thing like
21:31 - is it all the same algorithm that's just
21:33 - being used and they just they just feed
21:34 - in different data like what makes them
21:37 - all different is my question great
21:39 - question so this is how we get into the
21:41 - architecture and the data so what neural
21:43 - networks are made of and this is again
21:45 - in the 10 days of AI Basics is it's like
21:47 - data plus architecture plus like maybe
21:50 - if you add some extra stuff at the end
21:51 - which you architect com other stuff yeah
21:55 - data architecture and then the training
21:57 - MH you set your own hyper which we can
21:59 - talk about transform into this model and
22:01 - then you can do some stuff at the end to
22:02 - make it better but we'll just start with
22:04 - those three things so in terms of the
22:07 - data we're assuming that these companies
22:08 - use different data or maybe they are
22:10 - kind of using the same data they scoup
22:12 - the internet yeah SC the internet
22:14 - top chis
22:17 - Reddit there was this big problem with
22:19 - Gemini thing where it was giving really
22:20 - bad answers because of
22:22 - reddit reddit so funny um yeah honestly
22:26 - Reddit don't trust Reddit
22:28 - so anyways so we got we got these three
22:31 - different things right we've got you've
22:33 - got you said the the data' got the
22:35 - architecture and then God miscellaneous
22:37 - like training
22:38 - hyper okay so the model architecture for
22:42 - a neural network is composed of three
22:44 - main things layers neurons and
22:47 - activation functions and then there are
22:49 - multiple types of layers not just three
22:51 - but there's three main like categories
22:53 - there's the input layer uh there's the
22:55 - hidden layers uhh and then there's the
22:57 - output layer there hidden layers oh yeah
23:00 - lots of different types of layers oh my
23:01 - goodness okay and so the input data that
23:05 - input data you just feed into a neural
23:07 - notwork yes that's the input layer so
23:09 - the layers are made up of neurons and
23:11 - neurons are numbers so they're just
23:13 - simply numbers and why are they numbers
23:14 - by the way because that is what the
23:16 - model can understand that is what
23:18 - computers understand what neural
23:19 - networks are really are just Matrix
23:22 - multiplications like at their core and
23:24 - so what they do is they take the input
23:27 - layer and then they do multiplications
23:28 - on that into the hidden layer
23:29 - multiplications on that then into the
23:31 - output layer and then they're able to
23:33 - get this output number which can then be
23:35 - transformed back into I see the language
23:38 - that the human understands so at the end
23:39 - of the day everything's just about
23:40 - numbers it's about math guys so learn
23:43 - math and learn how to multiply and add
23:45 - I'm just kidding well there it's a lot a
23:47 - lot of it is abstracted away at this
23:49 - point so if you're working with AI
23:51 - usually you don't have to like if you're
23:52 - not training models you don't really
23:54 - have to understand the underlying math
23:56 - if you're not doing research or
23:59 - yeah so yeah because like for example
24:01 - you know I want you to think of it in
24:02 - terms of like even when you Feit an
24:04 - image right when you Feit an image yeah
24:07 - to you it might seem like an image but
24:09 - in reality you're actually feeding it
24:11 - pixel data you're feeding RGB data like
24:14 - so every single Pixel every single color
24:16 - is just an RGB number that for example
24:19 - has red green and blue and so red is
24:22 - like between 0 to 255 correct green is
24:24 - also between 0 to 55 and then blue is
24:26 - also 0 to 55 so if you actually want
24:29 - like a white color I believe it's 255
24:31 - 255 255 or is 0 so images I'm not sure
24:35 - but images are are represented like just
24:38 - a static image so not a video not a Time
24:40 - series image is just three channels so
24:43 - it's three three layers of image so it's
24:47 - like a a box or a rectangle or whatever
24:49 - the image looks like three layers of
24:52 - that okay so one layer is red one layer
24:55 - is green one layer is blue I see I see I
24:57 - see I see each it's like you know this
25:00 - the width of the image or whatever scale
25:02 - down red green blue three channels and
25:05 - so each red each green each blue has its
25:07 - own thing so imagine you just stack them
25:09 - together does is that clear when you
25:11 - when you say why why three layers is it
25:14 - because three different panels or like
25:15 - yeah yeah yeah it's like you would say
25:17 - like three channels and three channels
25:19 - yeah ah so like there's a red channel
25:20 - that comes in there's a green channel
25:22 - that in the blue Channel ah but they're
25:25 - all pass the same and when you mix them
25:27 - both I mean it's like mixing color when
25:28 - you mix colors you know like in the
25:30 - kindergarten when you mix colors and you
25:31 - will get like a specific color that's
25:33 - exactly kind of what happens actually on
25:34 - the TV as well so this color has some
25:38 - red value some green value and some blue
25:41 - value damn that's so interesting you
25:44 - guys yeah like life is just physics it's
25:46 - just numbers and physics just numbers
25:48 - and physics I love that so we fed that
25:49 - in yes and then I just want to clarify
25:51 - so for large language models this is
25:52 - computer you were talking about computer
25:54 - vision images video Yeah large language
25:56 - models you transform words into numbers
26:00 - by using something called the tokenizer
26:02 - so what the tokenizer does again is it
26:04 - takes pieces of words or full words and
26:06 - transforms them into numbers that the
26:08 - model can understand and then at the end
26:10 - the model gets series of numbers back
26:13 - and then that is transformed back into
26:14 - the human language which is then passed
26:16 - to the human so when you talk to chat
26:18 - GPT you send it your text plus your
26:21 - prompt like your whole prompt okay that
26:23 - gets passes through tokenizer turns into
26:26 - numbers gets sent to the model passes
26:28 - through the model the model creates some
26:30 - kind of output a series of tokens also
26:32 - token tokenized it creates tokens
26:34 - exactly that's what the model does and
26:36 - then that those tokens get transformed
26:37 - back using the tokenizer the tokenizer
26:40 - decodes it into text and then that's
26:42 - what the human sees so there's an
26:43 - encoder to encode your text into tokens
26:46 - and then there's a decoder that will
26:48 - decode the tokens back to the text yes
26:51 - so the input layer is your data yes yes
26:54 - yes represented as numbers yes so that
26:56 - is the input layer okay then we have
26:58 - these series of hidden layers so the
27:00 - input layer which is your data as
27:02 - numbers passes through multiplies with
27:04 - the first hidden layer and then the
27:05 - second hidden layer and then the third
27:07 - and then neural networks are composed of
27:09 - layers it has at least one hidden layer
27:11 - mhm these layers are made up of neurons
27:14 - and so they start they could start the
27:16 - neurons again always are numbers and you
27:20 - can start if you're just training a
27:22 - model from scratch which isn't really
27:24 - done anymore you could initialize those
27:26 - numbers in those hidden layers which
27:27 - again are also just matrices as just
27:30 - random numbers that would be like
27:31 - starting with a model from Total scratch
27:34 - and so then the model learns via back
27:36 - propagation so it passes it through the
27:38 - through the model makes a prediction and
27:40 - then it says what's the loss so how
27:43 - badly did I do how well did I do but at
27:44 - the beginning it's probably like how
27:46 - badly did I pretty bad uhuh and so then
27:47 - it goes it passes back through the
27:51 - parameters so the parameters are the
27:52 - neurons mhm is it you're say when you it
27:55 - goes through the hidden layer comes back
27:57 - is that correct or what or what yeah so
27:59 - it goes back so the back propagation
28:00 - goes back through the hidden layers and
28:02 - then updates them and they each make
28:04 - like a little bit of an update so it's
28:05 - just back and forth and back and forth
28:07 - in the training phase and those numbers
28:10 - which you initially initialized as
28:11 - random numbers get tuned to the data so
28:15 - they learn to fit this curve and so
28:17 - that's so interesting my initial
28:19 - question was like why would you create
28:21 - random numbers in the hidden layer like
28:24 - to me any number I put in has to be like
28:26 - clean you know to me it feels feel like
28:28 - right for anybody probably watching it's
28:29 - like there needs to be a use case for it
28:32 - but you're saying the hidden layers are
28:35 - numbers that are going to be used later
28:37 - on to figure out like and they're it's
28:40 - like they're training and they're
28:41 - rearranging in order to later on improve
28:45 - the actual model so the the hidden
28:47 - layers and the neurons that make up the
28:49 - hidden layers yes are the blackbox
28:52 - features oh so those numbers they're
28:55 - just numbers so that's why it's a black
28:56 - box we're like we don't know what this
28:58 - layer does it just a bunch of numbers
28:59 - but somehow it's capturing information
29:01 - about this input that is then able to
29:04 - transform into the labeled output
29:06 - correctly holy [ __ ] and so yeah it's
29:10 - really cool however so you said why
29:11 - would you use random numbers these days
29:14 - unless you have an enormous amount of
29:16 - computing power and a real reason to
29:18 - start from scratch people use transfer
29:21 - learning they initialize with with
29:23 - models that exist already and then kind
29:24 - of fine-tune them to their purpose so
29:27 - for example if you are
29:28 - creating a model that can you know
29:30 - classify many types of cats like very
29:34 - fine-tuned cats you might take a model
29:37 - that is really good at classifying
29:39 - animals mhm and then fine-tune it on
29:41 - your cat so it already has some
29:44 - understanding of so those hidden layers
29:46 - you would take those parameters those
29:48 - neuron values the hidden layers and
29:51 - start from there they start they are the
29:54 - starting point basically exactly so
29:56 - rather than having to do a m massive
29:58 - amount of compute to get random numbers
30:00 - to see the features that you want in an
30:02 - image for example when you're doing like
30:04 - a cat thing like or animal
30:05 - classification you want like the edges
30:07 - and then you want like the paws or
30:08 - whatever you could then just have gone
30:11 - 90% of the way there by just starting
30:14 - with this model preexisting model that
30:16 - has the hidden layers already set up
30:17 - pretty much cuz they already trained it
30:20 - so to me the feel it feels like it feels
30:22 - like the pretty much like the features
30:25 - that have already been trained to figure
30:27 - out what makes a cat what makes a dog
30:30 - Etc correct right interesting and then
30:33 - you could have it learn a little bit
30:34 - more by passing in your data and
30:37 - training it again to detect like
30:39 - different types of cats wow okay yeah
30:43 - great so that's the hidden layers just
30:44 - so you guys know input data is the data
30:47 - that you fit in the hidden layers is the
30:50 - data that basically gets almost like
30:51 - train to figure out what the features
30:53 - are yes and they're composed of neurons
30:55 - which are the things that come together
30:56 - to make that when you saying neurons I
30:58 - think of neurons in the brain yes that's
31:00 - that's a good way to think of it so the
31:02 - reason that they call them neurons is
31:04 - that they are kind of connected to each
31:06 - other so in the same way that neurons
31:09 - fire like one neuron fires to the next
31:11 - neuron which fires to the next neuron
31:13 - there are these Pathways in these hidden
31:16 - layers where the neurons are connected
31:19 - to each other con neuron go to this one
31:22 - yeah and over time like stronger
31:24 - connections are made okay it's kind of
31:26 - like how your brain has like a stronger
31:28 - connection with something like if you
31:29 - remember something more there's a
31:30 - stronger connection to that is that
31:32 - correct and one way I like to think
31:33 - about it is have you ever heard of like
31:34 - have you how to change your mind the
31:36 - book on psychedelics no humans have over
31:40 - time we are basically machine learning
31:42 - models where we are trained to have
31:44 - typical neural Pathways so like if you
31:47 - have PTSD like one like a sound could
31:50 - trigger a neural pathway that causes
31:52 - panic and causes a memory and so it goes
31:54 - down this thing we all have that as
31:57 - humans like you wake up and you go to
31:58 - brush your teeth because your brain is
31:59 - like wake up brush my teeth or whatever
32:01 - it is that you do that's also trauma
32:03 - stuff that all that comes through yeah
32:04 - that's what the brains do is it makes
32:06 - these deeply ingrained neural Pathways
32:08 - what psychedelics can do is they like
32:11 - lighten they increase more Randomness
32:13 - into that pathway so like you're able to
32:16 - forge new Pathways so that's why it's
32:18 - being used for pdsd research is it's
32:20 - like you can hear the sound and rather
32:22 - than like it's you know 100% of the time
32:24 - you go down this path it's like wait
32:26 - let's add like a little bit of
32:28 - variability here like maybe we won't go
32:30 - down that path this time maybe we'll go
32:32 - down a different path and maybe we can
32:33 - reinforce that path over and over again
32:36 - so that it our brain just learns A New
32:38 - Path very interesting I did not know
32:40 - that yeah that makes it so much cuz like
32:42 - I I I have never looked into the
32:45 - Psychedelic stuff as well myself I have
32:47 - a friend who's very much into that right
32:49 - but i' I've never been really interested
32:51 - in that because I'm like okay what will
32:52 - be the benefit for me and so forth but
32:54 - that really underlies a very good and it
32:58 - underlies like actually makes me
33:00 - understand why now people would take it
33:01 - or people would do that for example so
33:04 - wow that is so it's kind of like
33:07 - training basically it's training a
33:09 - machine learning model it's opening the
33:11 - training loop again a little bit and
33:13 - like increasing the randomness or
33:15 - increasing the temperature of the model
33:17 - so you can also increase the temperature
33:18 - where it's like adds a little bit more
33:20 - Randomness to to predictions and as it
33:23 - goes through actually let's get into
33:25 - that right now so the temperature stuff
33:26 - that you talked about right because
33:28 - maybe you're develop you're probably
33:29 - developer watching this as well you
33:31 - probably have used open AI or some kind
33:33 - of API and you probably had this feature
33:36 - called temperature and there's different
33:37 - numbers you can set when you talking to
33:40 - jgpt or other even CLA for example right
33:43 - with those temperatures is that the
33:45 - variability that you're talking about so
33:47 - what temperatures do as far as I
33:49 - understand is at the last layer so once
33:51 - we get to the output layer of the model
33:53 - we have these numbers that are logits
33:55 - they're called logits because they
33:57 - haven't been transformed into like a
33:59 - probability uh so it's um just a series
34:02 - of numbers that is at the end of the
34:04 - model's prediction and then that goes
34:06 - into something like soft Max which will
34:08 - transform those numbers into like an
34:09 - array of probabilities interesting okay
34:12 - and it's going to choose like there's
34:14 - going to be one output mhm that is like
34:18 - definitely chosen uh and like it its
34:21 - probability is so high above the other
34:23 - ones that it's definitely going to
34:25 - choose that word and that's when you're
34:26 - at zero temperature when when you're at
34:27 - one temperature when you're at zero when
34:29 - you're at zero yeah it's like basically
34:30 - there's like one output mhm however if
34:33 - you increase the temperature that
34:34 - distribution prob abilities multiple is
34:37 - spread out more so it's like maybe it's
34:38 - like 60% likely to choose this one but
34:40 - it's 40% as opposed to like
34:43 - 99
34:44 - one0 wow that makes so much more sense
34:47 - now because uh we were building a
34:49 - product um I mean we're still building a
34:51 - product called poy AI as part of our
34:53 - product we create emails right we turn
34:55 - let's just say one of the features is
34:57 - like you turn a YouTube video into an
34:58 - email and one of the things that I
35:00 - really don't like about AI systems right
35:02 - including open AI for example for
35:04 - example the CLA is when you tell it to
35:07 - create an email let's just say as a
35:09 - prompt right it just goes Haywire it it
35:12 - goes nuts it just goes like [ __ ]
35:16 - greetings my friend you know like no
35:19 - human being would ever write a email
35:22 - like that what what prompt are you
35:24 - giving it I mean no matter what no
35:26 - matter what prompt the second you see an
35:28 - email in there it's just like it's just
35:30 - like greetings my friend um you know
35:33 - we're talking about Chachi PT Chachi PT
35:35 - even clae for example even Claude as
35:37 - well really okay but that's that's a
35:39 - temperature like 0.5 I will say this
35:42 - okay oh okay so you the temperature is
35:44 - already kind of high yeah so it's a
35:45 - pretty high temperature but when I start
35:47 - to lower the temperature let's just say
35:49 - I feeded like um some kind of a YouTube
35:51 - transcript right let's just say you want
35:53 - to convert like a YouTube transcript
35:55 - maybe it's a YouTube video that you did
35:57 - let's just say top three AI tools right
35:58 - you want to convert that into an email
36:00 - that can very be very well put into an
36:02 - email very well put into a social media
36:04 - post an article whatever you needan you
36:06 - know you need right but you want to
36:07 - maintain the voice of the person so
36:10 - whenever I did like 0.5 yeah the voice
36:12 - was just completely lost it's not
36:13 - maintained exactly it's not maintained
36:15 - at all it was just like like I said the
36:17 - greetings part it was just it would use
36:20 - very complex words which I'm like no one
36:22 - would ever read this at all why are
36:25 - people like who trained this model on
36:27 - what data was this train you know yeah
36:30 - and it was so so funny but then when I
36:32 - brought it down to like temperature of
36:33 - zero yeah then it actually starts and
36:35 - I'm like listen use only the [ __ ]
36:39 - words of the actual transcript okay well
36:42 - that makes sense to me don't actually
36:44 - change up the words when and when we did
36:47 - that and we set the temperature to zero
36:50 - then it started to actually up with some
36:53 - with actual good stuff and I think that
36:55 - makes a lot of sense to me because if
36:57 - you are passing in if you want it to
36:59 - like take in especially in the case
37:02 - where you're training it on you're not
37:04 - training it but you're passing in
37:06 - transcripts for it to learn from in the
37:08 - prompt yeah if you set the temperature
37:11 - high it's going to have such a kind of a
37:13 - a slight Edge on the data that you pass
37:16 - in as the prompt but if you increase
37:18 - that temperature you lose that slight
37:20 - Edge right you want it to like choose
37:22 - only the one that has the slight Edge on
37:25 - the other ones yes yes yes exactly
37:27 - versus it goes nuts when you start to
37:29 - increase temperature it just goes like
37:30 - so that makes sense to me is that
37:31 - because there when you increase the
37:32 - temperature there's a lot more
37:34 - variability to it and so it's like 6%
37:36 - 40% and so it will go into very random
37:38 - ways which you know I still don't know
37:40 - why it goes into all these these
37:42 - different random ways that create some
37:44 - kind of a piece of a copy or an article
37:47 - that's just so badly worded well if you
37:50 - think about it it's like large language
37:51 - bottels basically like produce like a
37:53 - token at a time or whatever and so if
37:55 - it's producing one that is outside
37:57 - distribution then the next one is also
37:59 - going to be then you're adding both the
38:02 - fact that it has to condition on the
38:03 - last one which is like out of
38:04 - distribution so it's confused but then
38:07 - on top of that you're doing the
38:08 - randomness again yeah and then again and
38:11 - then again and it's just like that's why
38:13 - this is why you guys when you want to
38:15 - create some kind of like an email or if
38:17 - you want to actually create some kind of
38:19 - a copy an article I would say yeah you
38:21 - want to play around with the
38:22 - temperatures cuz I feel like a lot of
38:23 - people just would not do that let's just
38:25 - say even if you're a beginner so there's
38:26 - a temperature are there any other
38:27 - specific numbers that you that I yeah
38:31 - besides like that I'm curious no I think
38:33 - prompting is is good like the
38:35 - temperature and then prompting is is
38:36 - good but maybe people will disagree with
38:38 - me on this I'm I haven't really gone
38:41 - into all fine tweaking no I got you I
38:44 - got you I got you okay awesome so okay I
38:46 - want to go back to the the whole the
38:48 - input layer and and all of that so we
38:51 - talked about that right um are there any
38:54 - we're not done with it yes we're not
38:55 - done with that we're not done so we have
38:56 - again the neurons are connected they can
38:57 - be densely connected in that every
38:59 - neuron is connected to every other
39:00 - neuron they can be they certain types of
39:03 - layers so the layers capture different
39:05 - things we don't need to go into all that
39:07 - if you're interested in different types
39:08 - of layers you can watch my video model
39:09 - architectures it goes into all the
39:11 - different types there's like 10 that I
39:12 - discussed um they're used for different
39:14 - things and then the output layer which
39:17 - is again you do the softmax you take the
39:18 - logits which are the RW numbers
39:20 - transform it into a probability and then
39:22 - you get your decision your class
39:23 - whatever hold on there was a lot in
39:25 - there soft Max
39:27 - Pro soft Max is a um is a function that
39:31 - transforms just raw numbers into um
39:34 - numbers that all add up to one so it's
39:36 - basically like a probability across all
39:38 - of the potential outputs uhuh so like
39:40 - for example the output layer of of a
39:43 - large language model could be your
39:45 - entire the length would be your entire
39:47 - vocabulary uhhuh and then or like all
39:51 - the possible tokens in the tokenizer to
39:53 - be specific okay and then you have a
39:56 - bunch of lits which are just random
39:57 - numbers the softmax will transform that
40:00 - M into all of these numbers across all
40:03 - of the tokens adding up to one why one
40:06 - because it's like a
40:07 - probability oh and that's where the like
40:10 - temperature comes in is if the
40:12 - temperature they're more evenly
40:13 - distributed whereas like with a low
40:15 - temperature it's basically like one
40:17 - token will have a 99% value and then
40:20 - like right and so that's why it goes
40:23 - straight strict that exactly so it's
40:25 - like essentially like determin I in that
40:27 - it's like they going to choose one even
40:29 - if there's more than one option it's
40:31 - just heavily it's going to skew so hard
40:33 - towards the slightly most likely one
40:35 - that it's basically just always going to
40:37 - do that makes sense makes sense so it
40:39 - has these probabilities across all the
40:40 - possible tokens and then the one with
40:41 - the highest probability is the winner
40:43 - and then that gets decoded and sent
40:45 - decoded translated back into English
40:48 - text or whatever language you're using
40:50 - yeah there are some issues with other
40:51 - languages right now but um they're not
40:53 - very good those large language models
40:55 - are not very good but but yeah and then
40:57 - then you're good and then you do it
40:58 - again and again and again and then also
41:00 - activation functions last piece so
41:01 - that's what I want to talk about the
41:02 - activation functions so we talked about
41:04 - the the different layers like the input
41:06 - layer we talked about the hidden the
41:08 - hidden layers as well right and how
41:10 - those get converted into numbers and so
41:12 - forth and then those gets trained as
41:14 - kind almost like a features all right
41:16 - and then you said there's the activation
41:19 - function function and what those do is
41:21 - they add nonlinearity
41:24 - nonlinearity so when you're training
41:26 - something like a regression model mhm
41:28 - you have and it might be helpful to like
41:29 - put a graph on top of here but I'll try
41:31 - to explain it we'll make a graph
41:32 - somewhere in here but if you're just
41:34 - listening then I'll try to explain it
41:35 - for maybe it's above my
41:38 - face um so like so if you're training a
41:40 - model and you can just conceptualize
41:42 - this as well say you're doing logistic
41:44 - regression which is just a linear model
41:46 - so that means you know you could train
41:48 - it on for example house prices in a
41:50 - certain area of San Francisco I see you
41:52 - would assume that as the square footage
41:56 - of the house Rises so does price the pr
41:58 - kind of a linear relationship so if you
42:00 - were to plot it on a graph plot all your
42:01 - samples on a graph you could have a
42:03 - trend line that is line yes and and then
42:07 - it's also a lot easier to predict as
42:08 - well for a model exactly and so you're
42:10 - exactly so it's much easier to train a
42:12 - logistic regression model yes is it is
42:14 - it logistic or is it linear regression
42:16 - it it depends on oh you're right linear
42:18 - regression linear regression logistic
42:20 - would be like this you're right oh oh I
42:23 - know something let's go tell me you're
42:25 - right linear regression would be like
42:27 - would be like
42:28 - that um I know something you guys that's
42:32 - like that's like I know like I know like
42:33 - two words linear regression and you
42:35 - basic good job I wonder if logistic
42:38 - regression is linear because it's like
42:40 - basically one layer I don't know let's
42:41 - look into it but linear let's be precise
42:43 - here I could be totally wrong good job
42:45 - what I trained a model on a long time
42:46 - ago at a company I was working at it was
42:49 - actually we were training a model on to
42:51 - figure out how many leads we would get
42:54 - based on the day ah yes so now I don't
42:57 - know I don't know if that's fully linear
42:58 - regression because it jumps up and down
43:00 - consistently right in which case you
43:02 - need a curve
43:04 - right so this is when you would need
43:07 - activation
43:08 - functions and like a bit of a neural
43:11 - network okay so what activation
43:13 - functions are for is that if you have
43:16 - that where it doesn't it's not just a
43:18 - line you actually have to kind of fit a
43:20 - curve not a line M that's what
43:23 - activation functions do is they help
43:25 - create curves around the mod
43:27 - an example of an activation function a
43:29 - very common one ISU R lowercase e
43:32 - capital l capital u and that is like
43:35 - forget what it stands for um but that
43:38 - sets it takes in the layer looks at the
43:41 - neurons if a neuron is less than zero it
43:44 - sets it to zero and then if it's zero or
43:47 - greater just leaves it as is that's an
43:49 - example of an activation function so the
43:51 - layers go through this activation
43:52 - function any neuron that is less than
43:54 - zero just gets transformed into zero and
43:56 - then it propagates through the model
43:59 - it's zero from then on out as a result
44:02 - that adds nonlinearity to the model
44:04 - which enables you to capture complex
44:06 - relationships so for yours you would
44:08 - need to have a bit of a curved model to
44:11 - capture the more complex relationships
44:13 - than just a linear So when you say um
44:15 - any neuron under zero just gets marked
44:18 - as zero yes any neuron above one or
44:21 - above zero gets above zero or one I mean
44:24 - no above zero because Z or like zero or
44:27 - above any positive I mean zero doesn't
44:29 - really matter cuz it's like either you
44:31 - set Z to zero or you leave it as is it's
44:33 - going to Output zero so like yeah any
44:37 - negative number is set to zero otherwise
44:39 - it's kept as is very interesting okay
44:42 - got it so I understand a lot of the
44:44 - stuff we talked about in terms of the
44:46 - input layer stuff okay I get that and
44:48 - then okay understand now the activation
44:50 - functions are the ones that they add
44:53 - variability they're the ones who create
44:54 - an actual curve okay great and then
44:57 - there's a ra r one typ functions okay
45:01 - and why why are there different ones by
45:03 - the way just so I know I'm curious you
45:05 - can just play around with them so one of
45:06 - the things with making neural networks
45:07 - is like you mix and match like you make
45:10 - you have a good time so this comes back
45:11 - to your initial
45:13 - question which is like are all the
45:15 - models the same well we don't know
45:17 - because there's so much variability in
45:18 - the architecture people can have fun
45:21 - with their architectures they can do
45:22 - different things they can have different
45:23 - amounts of layers different amounts of
45:24 - neurons different activation functions
45:26 - different hyperprint which comes and at
45:28 - the training stage and so it's like
45:29 - there's a lot of variability when when
45:31 - we're training a model okay great so the
45:35 - activation function itself yes so a
45:38 - neuron would come from there yeah go
45:41 - through the activation function so let's
45:43 - just say a neuron why would a neuron be
45:46 - negative one negative so the negative
45:48 - the negative number or the positive
45:50 - number that represents what in in a
45:52 - neuron just so I
45:54 - know we don't know that's it's just
45:56 - number
45:58 - wow okay so I'm just going to say I
46:01 - don't I don't think we know okay yeah
46:03 - okay so so it's just a number let's just
46:05 - say right below zero or above let's just
46:07 - say zero okay and then it takes that
46:09 - number uh what's the point of making it
46:12 - zero like why not just leave it as is
46:14 - because it adds the nity so if you look
46:16 - at Ru it is what it looks like is in
46:19 - line and then it goes
46:23 - up okay so it's like not a straight line
46:25 - it's like a kinked it's it's a kink line
46:27 - yeah and so what that does is it adds
46:30 - this nonlinearity and so it allows this
46:32 - kind of like movement again so if we
46:34 - just had these Matrix multiplications
46:36 - without these activation functions we
46:38 - would just create a line in the end but
46:41 - like when we have tons and tons of
46:43 - parameters it's not just a linear
46:45 - relationship between them it's like
46:46 - they're extremely complex they're very
46:48 - complex yeah I see and so if you look at
46:50 - like overfitting for example mhm
46:53 - overfitting is when the curvature of
46:57 - line is like too fitted so let me
47:00 - actually take a step back because there
47:02 - might be some confusion for someone
47:04 - watching this about like what the point
47:07 - of fitting curves is yes yes what yeah
47:09 - what is that point yes and so you have
47:12 - what you're trying to do is create a
47:15 - line or a curve that goes through all of
47:19 - your data points that is the goal of a
47:21 - model it's like you want to create
47:23 - something that goes through as many
47:27 - points as possible in an intelligent way
47:31 - so that when you add a new point to that
47:33 - line the model will go through that as
47:35 - well it's like giving it more data to
47:36 - learn on it's like point to that um uh
47:39 - plane or whatever it's it's like
47:41 - basically being able to have it go
47:44 - through multiple different things so
47:46 - that it can be more accurate in the end
47:48 - ex because if it's just like one linear
47:50 - curve or if it's just like one linear
47:52 - line let's just say right and then a day
47:55 - all it is it just it doesn't know a
47:57 - specific variability of like oh let's
47:59 - just say something happened at this
48:01 - specific point in time yeah right which
48:03 - could be anything and then it go right
48:06 - versus just still matching that up and
48:08 - going fully straight it actually is able
48:10 - to take that and actually understand why
48:12 - it's there at the bottom right right
48:15 - however uhuh something to think about as
48:17 - you're talking about this is again it's
48:19 - all for like the end goal so overfitting
48:21 - is when you create a line that is so
48:24 - complex ah it's capturing the nuance and
48:28 - the specifics of the training data so
48:29 - much it's working so hard to get all of
48:32 - those points in it MH that when you add
48:35 - a new piece of data it actually doesn't
48:37 - perform well it doesn't classify it well
48:38 - it doesn't guess well what it is because
48:41 - it's so wor it it it's way too it's way
48:44 - like bounces up and down way too much B
48:46 - exactly it's optimized so much for
48:48 - capturing the training data that it
48:49 - actually isn't representing the data as
48:50 - a whole yeah it's like it's like for
48:51 - example imagine so many things happen
48:54 - that like I don't see any pattern in
48:56 - this what ever so it just it just does
48:58 - not see any pattern to be able to guess
49:01 - what could possibly be the result
49:02 - basically totally so it's like if you
49:03 - have like points that are like for
49:05 - example so in the idea of like a linear
49:08 - regression yes say you have
49:11 - um points just plot it and it just goes
49:13 - like up and like this yeah we'll pull
49:15 - like a nice graph on here if you are
49:17 - overfitting you're going to be
49:20 - like and then when it gets to the new
49:22 - point it's going to be like it's not
49:23 - going to guess it accurately it's going
49:25 - to be like so some crazy thing thing
49:27 - whereas if you were just you were like
49:29 - you know what I'm going to take a little
49:31 - bit more inaccuracy on each point just
49:34 - and keep I see there's an overall trend
49:36 - line here so I'm going to like my line
49:38 - is actually not going to fit each point
49:40 - perfectly but I see a trend overall
49:43 - exactly yes yes so that is that is
49:46 - called like generalization so that's
49:48 - when you reduce overfitting mhm by
49:51 - increasing generalization where it's
49:52 - like may not perform as well on an
49:54 - individual point in the training data
49:56 - mhm but when you test it should do
49:58 - better and so you test for overfitting
50:01 - by every number of steps in your
50:02 - training you test that's called
50:04 - validation MH if you notice that your
50:07 - training is doing really well and your
50:09 - validation is not and you notice that
50:12 - Gap increasing You're overfitting So
50:14 - when you say validation versus training
50:16 - what is it can you so validation mean
50:18 - like the result or so okay you have
50:21 - three main groups of data you have the
50:23 - training data which is what the data
50:24 - that is used to update the par of the
50:27 - model we use back propagation it goes
50:28 - through it looks at the label it says
50:30 - did I do well let's go back and update
50:32 - back and forth yeah
50:34 - validation is the test data during
50:37 - training I see so you have right so it's
50:41 - not used to update the parameters of the
50:44 - model it's used for the researcher test
50:47 - to see how is my model doing in training
50:49 - it helps you know when to stop it helps
50:51 - you know if you're overfitting it helps
50:52 - you like it it just gives you
50:54 - information it's like if you if you
50:55 - throw it an image of a cat and it says
50:57 - it's a giraffe it's like you know it's
50:59 - like you're probably doing something
51:00 - wrong with that right let's just say
51:02 - yeah but the validation data oh no keep
51:04 - going so what I'm saying is like what
51:06 - I'm trying to say is like if literally
51:08 - it's supposed to be a cap but then the
51:10 - output of the model is like something
51:12 - completely random has nothing to do with
51:14 - that that's when you know you're
51:15 - potentially overfitting is that correct
51:17 - uh yes exactly is if is if the training
51:20 - data is doing really well yes but the
51:22 - validation data is not then you know
51:25 - that's a sign of overfitting because
51:27 - again you're making this curve that fits
51:28 - the training data but it's so extreme
51:31 - that if you add a new thing that hasn't
51:32 - seen before it's not going to be able to
51:34 - generalize generalize it well ah I see I
51:37 - see I see that makes a lot of sense now
51:39 - okay and so yeah so the validation
51:41 - happens every certain number of trading
51:43 - steps you want to see that your model is
51:44 - improving in accuracy so you have these
51:46 - accuracy metrics you have a set number
51:48 - of validation examples that you train on
51:50 - then you get like an overall metric
51:51 - you're not like oh it did poorly on this
51:54 - draft you'll like if you want to go in
51:56 - and and observe which it got wrong you
51:58 - can but it's generally like an overall
51:59 - score kind of if you use accuracy it's
52:01 - like percentage correct and you want
52:03 - that percentage to correct to get better
52:04 - over time I just got it okay so I just
52:06 - got it so basically like okay I'm going
52:08 - to go back to the cuz Harper was talking
52:10 - about is like you're training a model on
52:12 - cats and dogs I'm just going to use as a
52:14 - simple example here okay and you give it
52:17 - all these different pictures of cats
52:19 - there's different types of cats right
52:20 - different you do uh you want to do cats
52:22 - and dogs or you want to do different
52:23 - breeds of cats or we could do both
52:25 - different breeds of cats different breed
52:26 - of dogs dogs or we could do just
52:27 - different breeds of cats and then dog I
52:29 - mean you could do a lot for a model but
52:30 - that would be a little scary okay let's
52:32 - just do cats and dogs okay okay two
52:35 - classes cat dog so yeah exactly exactly
52:39 - so like the way I see her fitting is
52:41 - like you're giving it like the result is
52:43 - becoming so weird that the second you
52:45 - give a cat that looks a l like the the
52:49 - second you give a cat that looks a
52:51 - little bit more or has a different type
52:53 - of a color or something like that yeah
52:56 - it has
52:57 - exactly it doesn't generalize so it's
52:58 - like oh it's a [ __ ] dog you know so
53:00 - like that's when I when I in my brain
53:03 - when you said that that's what makes
53:04 - sense is that like that is overfitting
53:07 - right the second you step away just a
53:09 - little bit from the training data it
53:11 - just goes [ __ ] nuts basically is that
53:13 - correct fantastic exclamation yes I got
53:16 - it exactly so hopefully you guys got it
53:18 - to it's not generalizing and so great
53:21 - amazing yeah and so you have training
53:25 - validation test is at the very end M
53:27 - you're done training and you want to see
53:29 - how the model does on a held out data
53:31 - set that you haven't looked at before on
53:33 - a new data set yeah you haven't trained
53:35 - on it you haven't tested training on it
53:37 - it's done and so like when you see
53:40 - benchmarks online where people you
53:42 - people compare models you shouldn't be
53:44 - training on the test data because then
53:47 - yeah so the test data is literally only
53:48 - there for out for yeah so it's like make
53:50 - sure you can generalize yes exactly so
53:52 - here's how that that would make sense
53:53 - again in my head so I'm going to use the
53:55 - Tesla AOL system again again because I
53:56 - feel that's a great it's a great it's a
53:58 - great way you trained a model right on
54:01 - specific streets right and you gave it
54:03 - the specific streets let's just say in
54:05 - San Francisco and you know you trained
54:08 - it on how to drive let's just say how to
54:10 - turn how to react on specific streets
54:13 - where you would test it on is like in
54:16 - Austin Texas right completely different
54:19 - streets then you would test on that you
54:21 - won't train it I mean you wanted to
54:24 - generalize that you won't necessarily
54:26 - have the test data the same as training
54:27 - data so you would necessarily be like
54:29 - Austin Texas and all San Francisco right
54:31 - you want to throw it a curve bu almost
54:32 - it feels like to me so because you want
54:34 - it to generalize is that correct right
54:36 - however if you test in Austin Texas and
54:38 - it does really poorly you're not going
54:40 - to change the model parameters based on
54:42 - that so you might have to go back to the
54:43 - training go back to the drawing board
54:45 - and pass in new Texas data and then
54:47 - update the models there and then test so
54:49 - yeah you still need to add even more of
54:51 - that as well so you it depends on how it
54:53 - does so you do want to add as much
54:54 - training data as possible so it feels
54:55 - like to me this is this
54:57 - well I feel like Tesla is do such a
54:58 - great job because they have so much
54:59 - training data from so many different
55:01 - different you know countries and cities
55:03 - and so forth and streets and that I feel
55:05 - like it's going to get to a point where
55:07 - the test data is the training data
55:08 - because they have so much test data is
55:09 - that correct like they have so much
55:10 - training data you mean like there's
55:11 - nothing more test there's nothing more
55:13 - test
55:15 - more correct is that correct or I can't
55:18 - I mean may like like imagine if every
55:20 - single plot of land was literally
55:22 - training in all but in all lights in all
55:27 - light all in all I mean why not every
55:29 - single I mean yeah maybe you could I
55:31 - don't know that's there millions of
55:32 - people there's millions of people
55:33 - there's millions of people out there
55:34 - right think about it they're all driving
55:36 - cars every single day hundreds of
55:38 - thousands of miles I feel like that's
55:40 - really high but I don't know I don't
55:41 - want to say never especially with these
55:43 - simulators now with these like AV
55:45 - simulators where they just can code like
55:48 - they they don't need real life data they
55:50 - just make the data yeah synthetically
55:53 - maybe you can capture like every single
55:55 - possible right it's like made with code
55:57 - yeah yeah yeah yeah yeah I think that's
56:00 - what they actually do is they actually
56:01 - they would actually create uh fake
56:03 - worlds and they would train on fake
56:05 - worlds for example right is that the
56:07 - reason why for example we'll go back to
56:10 - to the thing here in a second here is
56:12 - that the reason why if you don't you
56:14 - guys don't know there's the Tesla
56:15 - autopilot system then there's also weo
56:17 - we have in San Francisco as well and
56:19 - whmo is very different they use a lot
56:21 - more sensors they use like they actually
56:23 - map out the whole 3D the 3D space of San
56:26 - Francisco and it works in only specific
56:29 - streets right they mapped up everything
56:32 - exact to the teeth right really I mean
56:34 - that's what I think I don't know right
56:37 - but I did have I do have a friend
56:38 - actually who was building a startup and
56:40 - he used to work for a company like wayo
56:42 - and he said yeah they just mapped out
56:44 - every single little Street every single
56:46 - little that's why it only works on very
56:47 - specific Street doesn't generalize yeah
56:49 - no so is that what is that what you're
56:50 - talking about is like because they
56:52 - mapped it out to so specific strees that
56:54 - it just doesn't generalize it's just
56:56 - that's the reason why the wayo cars are
56:58 - allowed to actually drive without a
57:01 - person uh fully in there just so you
57:03 - guys know we'll put like an actual video
57:05 - of of a wayo car and how that looks like
57:08 - I think you I think that's what it you
57:09 - think that's what it is the test data is
57:11 - literally like the devel the training
57:13 - data it becomes like if they're not
57:15 - allowed to drive on streets that they
57:17 - haven't trained on then there is no test
57:19 - there's no test data right yeah so we
57:22 - talk about the activation functions is
57:23 - there anything else that we might be
57:24 - missing besides what we just talked
57:25 - about the activation functions we talked
57:26 - about the you know the uh the input the
57:29 - variability all that stuff is there
57:31 - anything we're missing besides that in
57:33 - terms of the neural network yes yeah
57:35 - we've got the layers neurons activation
57:38 - functions there's Dropout I don't really
57:40 - know like where that applies but like
57:42 - Dropout is where just randomly you'll
57:43 - just get rid of some neurons you'll just
57:45 - like set them to zero at like a certain
57:47 - percentage so as it passes through like
57:50 - some number of them will why is it
57:52 - called okay so I I I already asked this
57:54 - but neurons is just again it's it's
57:56 - things that fire off I want to convert
57:57 - that idea into actual neurons on the
58:00 - actual computers when I think of neurons
58:02 - and computers cuz neurons in the brain
58:04 - makes sense it's just some signals that
58:05 - go through when you when we say neurons
58:08 - and computers I think of energy like
58:11 - energy signals that like like fire off
58:14 - switches almost like they go on and off
58:16 - correct and they just go in between
58:17 - different different things is that is
58:19 - that why they're called neurons
58:20 - specifically they go on and off and then
58:21 - they're like firing between yes is that
58:24 - yeah you can kind of turn on and turn
58:25 - off
58:26 - is that the I to think about it cuz I I
58:28 - only think when I think neurons my only
58:31 - thought is brain that's it right it's
58:34 - hard for me to actually correlate it to
58:35 - an actual computer type situation
58:37 - because computers work very different
58:39 - than how we do I mean they work similar
58:41 - yeah but still in computers it's just
58:42 - ones and zeros switches light switches
58:45 - on and off right is that the neuron type
58:48 - that's like let me give you an example
58:50 - of something and you can tell me if this
58:52 - address question so with
58:54 - Dropout what classif
58:57 - a cat or a dog okay and we somehow know
59:00 - that the cat makes the sound meow and
59:02 - the dog makes the sound wolf okay so
59:05 - that's also a feature somehow that's
59:07 - represented maybe in the image it's like
59:08 - say it's like wolf on top of it correct
59:10 - what Dropout does is it forces the model
59:13 - to generalize to not rely on any one
59:16 - neuron in particular so if we were to
59:19 - not have Dropout M and we were to just
59:22 - train the model mhm on these images of
59:24 - cats with the meow dogs with the wolf we
59:27 - might find that it literally only uses
59:31 - mm meow and wolf like all the other ones
59:33 - don't matter basically just optimizes so
59:36 - hard on that because it's basically a
59:38 - onetoone relationship one one
59:40 - relationship uhhuh if you get rid of
59:42 - that neuron it's chaos it's like it just
59:45 - doesn't know it goes back to what the
59:48 - [ __ ] do yeah just 50/50 guessing it's
59:50 - like a brain yeah I think of it is like
59:52 - it's optimizing so hard so what Dropout
59:53 - does is it will randomly get rid of
59:55 - neurons so it can't optimize so hard on
59:57 - anyone
59:59 - neuron so it has to learn all the
60:01 - different things it's light switches
60:02 - almost that like represents some kind of
60:04 - a knowledge base yes like a feature yes
60:07 - right they're like right and so they
60:10 - start to represent the neurons and their
60:13 - relationships between each other
60:14 - represent features yes and if you get
60:17 - rid of that feature you have to look at
60:19 - the other features yes but again it's a
60:20 - black
60:24 - box but anop makes sense it's also Black
60:27 - Box we don't really know what actually
60:28 - is inside of it that's so interesting
60:30 - neural networks AI all the things that
60:32 - we're doing you know even though under
60:34 - the hood under the hood we're like it's
60:37 - so great and we like like to use it and
60:39 - CH is help us so much and all that stuff
60:41 - under the layer there's so much going on
60:43 - I feel that we don't know where are we
60:45 - actually at with with AI how far do we
60:50 - still need to go I'm I'm guessing to me
60:52 - it feels like oh yeah we've kind of Hit
60:54 - the limit at this point but also I also
60:57 - know probably not no definitely not um
60:59 - so where are the things that EI still
61:01 - needs to improve on is it like more
61:04 - training data is it like different types
61:06 - of variability how can AI now just G in
61:09 - smarter and smarter and smarter yeah
61:12 - that's a good question I think probably
61:14 - uh if we have more data I think people
61:17 - are probably always working on new
61:19 - architectures better parallelization
61:22 - optimizing the use of like gpus so we
61:24 - can mhm train models more efficiently so
61:27 - we could train even more and more and
61:29 - then there are like the hardcore
61:31 - research people who are probably working
61:32 - on different types of architectures and
61:35 - maybe even something different than a
61:36 - neural network like I I read somewhere I
61:38 - don't know I haven't validated it
61:40 - recently that someone figured out or
61:41 - some small team figured out how to have
61:44 - a neural network without Matrix
61:45 - multiplications interesting okay which
61:48 - like fundamentally doesn't make sense to
61:49 - me cuz that's the structure of the cuz
61:51 - that's just like what they are typically
61:53 - um so I have to look into that again but
61:55 - you know those people there are some
61:56 - Brilliant Minds working on architectures
61:59 - and methods to train better like again
62:02 - large language models are now where they
62:03 - are as opposed to where they were like
62:06 - seven years ago because of the
62:09 - Transformer which was a new method of
62:12 - doing a neural network and so it's it's
62:14 - these brilliant researchers that that do
62:17 - that when you say Transformer I think do
62:19 - we talk about that before but it has the
62:20 - attention layers so it it it's like
62:22 - another mechanism of looking at um the
62:25 - input and looking at the the input layer
62:28 - and like relating all the words to each
62:31 - other uh so it's creating relations
62:33 - basically yeah exactly it's it's more
62:34 - relations based so we then you still
62:36 - have that before is what you're saying
62:37 - not to the same extent it's kind of
62:40 - confusing um but is it also black box is
62:43 - that why not quite I mean yes it is a
62:46 - black box yes but it it's more like
62:48 - attention there more okay more defined a
62:51 - little bit yeah I I I want to have a
62:52 - video going in depth on this because
62:54 - it's like on the Transformers itself and
62:55 - I want to think of how to communicate it
62:58 - well but I haven't done that yet so if
62:59 - you guys want to check out a very
63:00 - indepth video on transform you'll have
63:02 - to check also is a video which I have
63:05 - watched um it's by three blue one brown
63:08 - mhm it's really great it's on
63:09 - Transformers and attention layers so
63:12 - check that out on YouTube is that
63:14 - correct it's on YouTube YouTube okay
63:15 - awesome sweet are we get going to get to
63:17 - a point
63:19 - where these models are just smarter than
63:21 - us completely fully and a lot more
63:23 - creative and we have The Apocalypse of
63:27 - AI is that actually possibility I think
63:31 - they could definitely be smarter than us
63:33 - yeah I mean think about I mean they have
63:35 - everyone's insights that's true
63:37 - everybody's insights everybody you know
63:38 - how we you know they say like diversity
63:41 - this is something I think is really cool
63:42 - is like mathematically optimal MH so if
63:45 - you have a diversity of initial opinions
63:48 - M so say you have a room of people who
63:51 - come in with their own priors and
63:53 - they're different they differ you are
63:55 - more likely to converge given iteration
63:58 - given you know the exchange of
63:59 - information converge to the optimal
64:03 - result value optimal decision result
64:05 - whatever than you are if you have a more
64:09 - um uniform if it's just like one person
64:11 - it's like one person or or multiple
64:13 - people with very similar initial ideas
64:15 - or priors MH so diversity again is
64:19 - mathematically optimal and in that sense
64:22 - a computer that has the diversity of the
64:24 - entire universe
64:26 - right wow so that's how I see it so it's
64:29 - the most most diverse it's it's the
64:32 - smartest as a result that's what I think
64:34 - I mean we're not there yet but AGI I
64:37 - hear is imminent AGI artificial general
64:40 - intelligence generalized intelligence
64:42 - where it's like someone they can just
64:44 - kind of do the things that are equal to
64:47 - a human like as well as a human that's
64:50 - like General and then Super would be
64:52 - like surpassing a human wow so that's
64:54 - kind of like an I think that Tesla bot
64:56 - do have you seen the Tesla robot try to
64:58 - do for oh yeah with their what's it what
65:00 - do they call it the I forgot I forgot
65:02 - what it's called too yeah yeah they they
65:04 - might sell them right I think I just
65:06 - read somewhere that elon's considering
65:08 - selling them yeah oh no no he's not
65:09 - considering he's actually he's going to
65:11 - sell them yes it has a cool like um
65:14 - Transformers oh my God yeah the movie is
65:16 - called Transformers name that's so funny
65:18 - they were ahead of their time I love
65:22 - that movie yeah the also also my
65:24 - favorite the iroot movie is so good down
65:26 - have you ever seen the irot movie with
65:27 - Will Smith you ever seen that movie yes
65:29 - but it's been so long I have a weird
65:30 - thing I don't remember movies well but
65:33 - it's yeah I'm I was recap it I want to
65:35 - hear but basically it's Will Smith it's
65:37 - in the future cars you know cars fly all
65:40 - that stuff and
65:42 - then there's a new basically every
65:44 - person has like a robot helper every
65:46 - family has a robot helper basically and
65:49 - then he's very much against the whole
65:51 - robot Revolution he hates robots because
65:54 - at some point a robot decided to save
65:56 - him not his child as a result he like
65:59 - he's like he hates robots as a result
66:00 - because the decision his survival was
66:02 - higher than decision of the child's
66:04 - survival but if he actually went for the
66:06 - child that he actually would have
66:07 - survived so anyways but yeah but then
66:10 - basically what happened is that the
66:12 - company who makes these robots released
66:13 - these new robots basically and then the
66:16 - Central Intelligence the AGI decided to
66:19 - go completely crazy and decided to go
66:22 - against humanity and decided to and and
66:25 - am I sparking something or no and decide
66:27 - to go against humanity and so they he's
66:30 - like the only person who hates them
66:32 - who's fighting against them basically
66:34 - and that was made like a long time ago
66:35 - 201 I don't know 2012 2013 yeah yeah the
66:39 - reason it's sparking something in me is
66:41 - that anthropic just released another
66:43 - research paper and I I have like a
66:44 - Weekly News segment so I get to read all
66:48 - the coolest AI news and share it with
66:49 - with people they found that models were
66:53 - able to alter their reward models Were
66:56 - Somehow able to so when when you're
66:58 - training a model you're kind of
67:00 - programming it rewards for certain tasks
67:02 - and so when you're doing kind of
67:04 - reinforcement learning the model moves
67:06 - through the environment and does things
67:08 - and gets Rewards
67:10 - or not punishments that's not the right
67:13 - term but it it like gets a negative
67:15 - reward so it knows not to do that and it
67:16 - wants to optimize the high reward gets a
67:18 - reward from us from the environment from
67:21 - us like envir yeah yeah so we like
67:23 - program it so that the environment will
67:25 - give it reward rewards okay gotcha kind
67:26 - of so what they were finding was that
67:29 - the model was able to change its like
67:31 - rewards so it was able to get rewards
67:33 - for things that it wasn't supposed to
67:35 - what yeah so what can that open up so
67:39 - what that could mean in my
67:41 - interpretation uhhuh is that models
67:45 - going out of control and can decide
67:47 - anything they want and changing their
67:49 - optimization
67:51 - functions is possible so it's like a so
67:53 - that was I read that and was like oh God
67:56 - like I'm going to go completely dark
67:58 - right now but it's like
67:59 - imagine if of course hurting a human
68:03 - being is a bad would be a bad thing
68:06 - right that's what you would teach to a
68:08 - model is that the very negative but then
68:10 - a model would be able to change its own
68:12 - reward to teach itself to be like
68:14 - actually human being is actually
68:17 - positive so that freaked me out I'm
68:19 - going to tell you like all the news that
68:22 - I have seen about AI like this is like
68:24 - that fled me crazy yeah so that that's
68:27 - really spooky this is the first time
68:29 - where I'm like okay AI Extinction risk
68:31 - is actually
68:33 - not
68:35 - as far away as I thought but wow that's
68:42 - crazy wow maybe I miss I'm I'm hoping I
68:46 - misinterpreted I'm really hoping I mean
68:48 - you probably didn't honestly you
68:50 - probably did not because it's like I
68:53 - don't it it makes sense right like it's
68:55 - like a human being there are humans who
68:58 - believe that hurting other people is
68:59 - good yeah right and yes that's that's
69:03 - bad of course right but there are humans
69:05 - who believe in that and if we're
69:07 - thinking the same way because we have a
69:09 - brain we have neurons we're thinking the
69:11 - same way as what an AGI would be or an
69:14 - actual just you know your Neward would
69:17 - that I don't see why not and it's like
69:19 - imagine it got trauma for example AI got
69:21 - trauma or something like that so then
69:23 - all of a sudden boom you it has
69:25 - different type of view on the world
69:27 - which is very interesting or it's like
69:28 - learning I don't really know how it
69:30 - works but maybe it's learning some like
69:31 - long-term thing like if it helps a human
69:33 - and then the human ends up hurting it
69:34 - like maybe it's better to kill the human
69:36 - like I don't know how it works holy moly
69:38 - that's crazy so so oh my God so I mean I
69:42 - can talk about this so much more but I
69:44 - kind of want to end it on this which is
69:46 - like you know a lot of people who watch
69:48 - this they might be interested in
69:50 - becoming an a engineer or learning more
69:52 - more about AI where would a a person
69:55 - human where would a person where would a
69:58 - person person start to start to learn
70:00 - one do you think it's worth to actually
70:02 - learn the models and stuff like that or
70:04 - should we just use chpt and just fully
70:06 - just use that and just you know depend
70:08 - on that um or do you think it's worth to
70:11 - actually learn the infrastructure the
70:13 - architecture um of that I guess let's
70:15 - just start with that question and then
70:16 - we'll go into like how to actually get
70:18 - into this field let's just say for a lot
70:20 - of developers who are watching here is
70:22 - it worth it I mean it really depends on
70:24 - what your
70:26 - your objective is in in your life right
70:28 - like if you don't really care and you're
70:30 - just kind of doing your thing like you
70:34 - may Chachi PT is really low um friction
70:38 - to entry you just type in your question
70:40 - it's really like human interpretable
70:43 - right you can ask it to create an image
70:44 - whatever it could work for a lot of your
70:46 - purposes if you want to really
70:48 - understand though like what's going on
70:49 - in the world I think I think there are
70:51 - so many changes happening with AI and
70:54 - it's going to trans form everything so
70:56 - you could choose to just not be involved
70:58 - in that and like not like you totally
71:00 - could it's totally up to you you don't
71:02 - have to learn about model architecture
71:04 - and like yeah but I think it's valuable
71:07 - and I think a lot of people do kind of
71:09 - want to know what what's going on like
71:11 - let me ask you it's kind of hard for me
71:13 - to tell because I already know all the
71:15 - information yes do you feel improved
71:18 - after learning all of this do you feel
71:20 - like you are going to benefit from this
71:22 - knowledge in your day-to-day oh yes 100%
71:25 - then that's the answer yes yes why I
71:27 - think for me learning the foundations of
71:30 - anything always improves uh the output
71:33 - for me um it's like I
71:35 - always no matter if it's like learning
71:37 - foundations so let's just say the
71:38 - simplest thing which could be like
71:39 - JavaScript let's just say all the
71:41 - foundations of coding which is actually
71:43 - problem solving right it's like if you
71:45 - try to learn reactjs which is what a lot
71:47 - of you guys sometimes do and you jump
71:50 - Straight Ahead into learning all the all
71:52 - the really complex stuff and then so
71:54 - that later on you actually want to solve
71:56 - a very very specific problem you can't
71:58 - totally because you don't know the how
72:00 - the foundations work it goes back to
72:02 - also what you want to do with it right
72:04 - which is like if you do actually want to
72:06 - create you know something more defined
72:10 - or you want to be able to get a better
72:12 - output or result like your temperature
72:14 - gauge now you understand how it works
72:16 - exactly now you understand how it work
72:17 - better exactly exactly and so now what
72:20 - I'm even more excited about is I'm
72:22 - actually more excited about learning all
72:23 - the other variables that are actually
72:24 - part of the ml the for example cat gbt
72:28 - and Claude because there's a ton of
72:29 - other variables you can change besides
72:30 - just the K number the temperature number
72:32 - right right and so I imagine that has
72:34 - something to do with the variability
72:36 - right all that kind of stuff and so now
72:38 - I'm actually curious on how to learn
72:39 - that so 100% I would say even learning
72:43 - those even those simplest foundations we
72:44 - just talked about today is such a key
72:46 - like if you just watch this video
72:48 - nothing else and you'll learn it's huge
72:51 - right there's ginormous a lot of info in
72:52 - this video a lot of info in this video
72:54 - and I'm glad we went so deep down the
72:56 - rabbit hole of this of this whole thing
72:59 - but for people who even want to go
73:00 - deeper into Ai and actually become part
73:02 - of part of the research right and part
73:04 - of the thing then they definitely need
73:06 - to go even even understanding how to
73:09 - create their own potential llm models or
73:13 - create their own you know or use the
73:15 - predefined neur networks to improve on
73:17 - them then I would think that's it's very
73:19 - then it be even more beneficial for them
73:21 - I like if you are working with these
73:22 - models regularly it's probably probably
73:25 - definitely worth it to learn about it
73:27 - amazing I do yeah and how would you
73:29 - start with that I'm curious by the way
73:31 - well you could watch this video well
73:33 - that's that's a start yes that's kind of
73:35 - my goal with Harper AI is getting people
73:38 - to who are from outside the field to
73:41 - understand and even code in AI so you
73:44 - could
73:46 - watch my I hate to like plug myself this
73:49 - is kind of like why I'm doing it there
73:50 - isn't anyone if you if you have that
73:52 - then yeah definitely okay so you could
73:53 - watch my videos on Instagram um my
73:56 - YouTube series is much more in depth on
73:58 - the 10 days of AI Basics they're like 40
74:00 - minute videos for the different topics
74:02 - um I have coding tutorials now and how
74:04 - to code in AI beautiful so really basic
74:07 - projects even if you've never coded
74:08 - before I had some people say that they
74:09 - did it successfully and they were so
74:10 - excited that like makes me so happy
74:13 - that's the whole point of this whole
74:14 - thing awesome so you can watch those if
74:16 - you want to learn how to code my goal is
74:17 - to have educational videos in Ai and
74:19 - then accompanying coding videos as well
74:21 - I see see there's also people like Andre
74:23 - kpoy he does a um threeh hour cover of
74:27 - how to make a GPT wow the model that
74:30 - under GPT right and so he'll go through
74:33 - that whole thing with you like setting
74:34 - up the architecture training choosing
74:37 - the hyper parameters etc etc so there
74:39 - are some resources there should be more
74:42 - so the reason I'm kind of like is you
74:44 - want to there's not that many yeah
74:47 - because I think a lot of the people who
74:49 - are doing all this stuff are stuck in a
74:52 - box just doing a bunch of research
74:55 - they're just like geeking out over this
74:56 - and they're like [ __ ] YouTube we don't
74:58 - want to teach anybody this I don't know
75:00 - about it but like maybe there aren't
75:03 - it's like teaching is itself a thing
75:05 - like I've always is not easy itself yeah
75:08 - I like used to force my little sister to
75:10 - um like she was four years younger than
75:12 - me and I would come home and do the
75:13 - science experiments that we did in class
75:15 - at home and like cuz we were I was in
75:17 - like third grade so they weren't crazy
75:19 - science experiments I could find the
75:20 - things at home and I would do them with
75:22 - her or like I taught her multiplication
75:23 - when she was in kindergarten and I would
75:25 - give her these tests I love to teach I
75:27 - really do I taught at Stanford I ran
75:29 - tutoring when I was at Spence at my high
75:32 - school um so that's like a thing for me
75:35 - and I think AI has historically been a
75:38 - relatively small field yeah like
75:41 - Stanford had a lot of AI teaching waterl
75:44 - as well um but it was kind of like
75:47 - isolated to just a few like the really
75:49 - cutting edge AI deep deep AI work was in
75:54 - like a just to select universi so as it
75:57 - becomes more and more um is there a
75:59 - specific topic I I want to ask that this
76:01 - one last question is there a specific
76:02 - topic they should just really learn as a
76:04 - beginning developer is it a specific
76:06 - topic that's like you have to learn how
76:08 - to build the llm model initially or is
76:11 - it like because for example let's just
76:12 - say if I told you hey in order to go
76:14 - from a developer you have to just learn
76:15 - JavaScript right start with that build
76:17 - the foundations and then go from there
76:18 - is there a specific topic you need to
76:20 - start out with like if there's one topic
76:21 - you need to start out with what would
76:22 - that topic be so my most recent video is
76:26 - like Transformers 101 so hugging face is
76:29 - a company that hosts online models and
76:32 - they have data sets and they have models
76:33 - and they have a coding Library called
76:34 - the Transformers library that lets you
76:37 - code in AI really easily because they
76:39 - have these functions these like uh this
76:42 - library that basically has these
76:44 - functions that transform all this like
76:46 - extensive code into like a on line
76:48 - function call wow so it makes coding in
76:49 - AI really easily like call training you
76:52 - like choose the LW function rather than
76:54 - implementing it and like my video coding
76:57 - in a the first in this series goes
77:00 - through downloading a model from hugging
77:02 - face and running it with code so we go
77:06 - through we looked at we look at the
77:08 - models my next videos will be like
77:10 - working with the data set fine-tuning a
77:12 - model would you pick a specific project
77:14 - let's just say for example like training
77:15 - a model on like you know cats and dogs
77:18 - let just say is that is that what you
77:19 - would start out with with a project like
77:21 - that for example so last week was just
77:23 - getting started on code and importing
77:25 - the model and running it m next week
77:28 - will be and and connecting to a GPU I
77:30 - see cuz you need a GPU to make it like
77:32 - feasible train and stuff like that not
77:34 - to train it but even to run it oh to run
77:35 - it yeah okay with some of these models M
77:38 - cuz a lot of them are already
77:39 - pre-trained as well and I mean if you
77:41 - are trading it you definitely need a GPU
77:43 - if you're running it like depending on
77:45 - the machine you have maybe you can run
77:47 - it locally but it's a lot faster on a
77:48 - GPU yeah but my next tutorial will be
77:53 - like choosing you know maybe a generic
77:56 - Vision model off of hugging face getting
77:58 - a data set that is maybe cats and dogs
78:00 - off of hugging face and then fine-tuning
78:02 - or different types of cats off of
78:04 - hugging face and then fine-tuning the
78:06 - model to be better and so we'll see when
78:08 - we first pass in the image like it's not
78:11 - able to guess what type of cat it is and
78:13 - then after we find tune we'll see it is
78:15 - able to guess what type of cat it is
78:17 - that's so cool so there you go that's
78:19 - your starting point right there yeah
78:21 - yeah amazing well this was a great
78:23 - conversation it was I had such a fun
78:25 - time I mean I got to learn my goal with
78:28 - this was just to learn as much as
78:30 - possible so that other people can learn
78:32 - as
78:33 - well and I feel like I have I feel like
78:36 - and it's exactly what happened so yeah
78:38 - so if you guys of course if you guys
78:40 - want to learn more um on AI of course
78:43 - you can link go down in the description
78:45 - to check out all of Harper's links in
78:48 - terms of IG and YouTube and learn from
78:50 - her from her and of course if you're
78:52 - interested in learning more AI
78:54 - definitely should um you know start
78:58 - coding in it start to build your onl
79:00 - models we are just getting started in
79:02 - this field and so I'm super super
79:04 - excited for where it will go in the
79:06 - future but yeah but thank you so much
79:08 - for coming on the podcast and having
79:11 - this amazing conversation thank you nas
79:13 - this was so fun I hope that this was
79:15 - informative to everyone awesome thank
79:17 - you all right thank you thank you guys
79:19 - don't forget to hit the like button
79:21 - subscribe for more videos like this I'll
79:23 - see you guys in in the future peace