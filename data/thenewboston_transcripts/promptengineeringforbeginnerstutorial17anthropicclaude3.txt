00:00 - hey welcome back everybody this is Ian
00:01 - bringing you another video in this AI
00:03 - series with the New Boston so in today's
00:06 - video we're going to talk about
00:07 - anthropics clae 3 API in the previous
00:10 - video we talked about CLA 2 we did
00:12 - mention that we were going to talk about
00:13 - clae 2.1 but CLA 3 is better faster came
00:17 - out right after we made that last video
00:19 - so we thought what the heck let's just
00:20 - go straight to CLA 3 everything that
00:22 - we're going to teach you here can
00:24 - actually translate over to claw 2.1 if
00:26 - you for whatever reason need to go back
00:28 - and use it but CL three is going to be
00:31 - the latest and greatest so we're going
00:33 - to go ahead and stay up to date with
00:34 - that by introducing it in this video
00:36 - this that we're looking at here on my
00:38 - screen is the blog post that talks about
00:40 - the release of Claude 3 the clae 3 Model
00:43 - family there's actually three different
00:44 - models that are included and that's
00:46 - because they've got a version called
00:49 - Haiku that is cheaper than the other two
00:51 - versions and faster in most cases but
00:55 - not quite as intelligent so it'll do a
00:56 - lot of really great stuff but it might
00:58 - not be what you need for all of your us
01:00 - cases and then kind of right in the
01:01 - middle ground they have one called
01:02 - sonnet which is a little more expensive
01:06 - and potentially a little slower but more
01:08 - intelligent and then Opus is probably
01:10 - the slowest of the three but is way more
01:12 - intelligent and also more expensive so
01:15 - it can do a huge range of things but it
01:18 - may not necessarily be the best for
01:20 - whatever your project is based on your
01:22 - project requirements so you'll want to
01:24 - go through this article and figure out
01:25 - based on the information in it which one
01:28 - is the best to use we are going to use
01:30 - hi cou because it is simple it's fast
01:33 - and it is the cheapest but you can
01:35 - change the values for which model to use
01:37 - and we'll show you how to do that in
01:38 - this video so yeah go ahead and go
01:40 - through this you can see like I
01:42 - described to you the kind of XY here for
01:44 - intelligence versus cost and where each
01:46 - of these three models in the model
01:48 - family kind of fall in you can see the
01:50 - one that we're going to use ha coup on
01:51 - the left being the least intelligent but
01:53 - also the cheapest and then Sonet right
01:55 - in the middle and then Opus over here on
01:57 - the right where it costs more but it's
01:59 - very intelligent
02:00 - bunch of cool stuff in here about
02:01 - benchmarking I'm not going to go through
02:03 - all that but you can go through and read
02:04 - all the cool stuff that they have
02:06 - available for you to read and then right
02:07 - here they talk about the specifics of
02:10 - each model so Opus being the most
02:12 - intelligent and the cost per million
02:14 - tokens for input and output the content
02:16 - W context window and potential uses so
02:20 - again if you have like a task automation
02:22 - thing then maybe Opus is the right thing
02:24 - for you if you have some R&D sort of
02:26 - thing for research then maybe it's Opus
02:28 - again you can go down to the next one
02:30 - you can read about Sonet and you can see
02:32 - okay this one strikes the ideal balance
02:34 - between intelligence and speed and
02:36 - particularly for Enterprise workloads so
02:38 - then go down to potential uses you can
02:40 - see oh it's actually pretty good for rag
02:41 - use cases or it's good for things like
02:44 - sales forecasting targeting marketing
02:46 - Etc so after that the faster one is
02:49 - going to be I think we skipped it is
02:52 - haiku which is the fastest most compact
02:54 - model for near instant responsiveness so
02:56 - as you might assume this one's going to
02:58 - be pretty good for like chat Bo bots so
03:00 - you can see down here for potential use
03:02 - cases customer interactions chat Bots
03:04 - content moderation more chat type stuff
03:07 - and uh cost saving tasks so we are going
03:10 - to use this one again just because it's
03:12 - affordable and it's really fast and it's
03:15 - still pretty powerful you can also see
03:17 - that it is uh the cheapest based on per
03:20 - million token usage okay great so feel
03:23 - free to look through this we will share
03:25 - a link to this in the description or the
03:27 - comments but let's hop over to our code
03:30 - mode great so this is the main pie file
03:33 - inside of the 02 dog toes folder which
03:36 - is inside of our anthropic examples
03:38 - folder and of course at the very top all
03:40 - this stuff is just boiler plate so uh
03:42 - I'm not going to go into that but
03:44 - basically that's just how we bring in
03:45 - anthropic how we include our API keys
03:47 - and we get it hooked up to prompt layer
03:50 - we are going to be using prompt layer in
03:51 - this video it's just so easy to hook it
03:53 - up to things like open Ai and anthropic
03:55 - where we can track all of our calls and
03:58 - track our prompts and create templates
04:00 - so it's just a no-brainer to have that
04:01 - included all right cool so here on line
04:04 - eight is where we're actually getting
04:05 - into the meat and potatoes of this
04:06 - lecture and that is how we can send a
04:09 - request using the messages API for clae
04:12 - 3 now previously I believe we used the
04:14 - text completions which is their older
04:17 - version that they started out with now
04:19 - for cloud 2.1 and Claude 3 they're using
04:22 - the messages API so that's what we're
04:24 - going to use here so we use the SDK to
04:27 - send the request to the API through
04:29 - through this client. messages. create
04:31 - method and the result of that is going
04:33 - to be the message the assistant rooll
04:35 - message that gets sent back from cloud 3
04:38 - so we'll assign that message object to
04:40 - the completion variable so that we can
04:42 - then look at it here at the bottom of
04:44 - our file all right so which arguments
04:46 - does the create method take for calling
04:49 - this messages API the first one here is
04:51 - the messages list inside the list
04:54 - there's going to be some dictionaries
04:56 - now the main rule here that you have to
04:57 - remember is that the first dictionary
04:59 - has to be a role of user no matter what
05:02 - so you might be used to open AI where
05:04 - they had their first one being the
05:06 - system role that is not how they do it
05:09 - with anthropics apis in fact you can
05:11 - have a system argument but it's going to
05:14 - be a top level argument to the doc
05:15 - create method as opposed to being
05:17 - included within the messages list so
05:20 - within the messages list the first
05:21 - dictionary that you'll see here is going
05:23 - to have two properties the first one is
05:24 - content which will be the text that
05:26 - you're actually sending to the API with
05:29 - whatever prompt is the next property
05:32 - here is going to be role there's two of
05:34 - those so anything that's coming from us
05:36 - the user will be a user role anything
05:38 - that comes back from the API will be an
05:40 - assistant role once we get that message
05:42 - back if we want to append it to this
05:44 - messages list and some sort of Loop and
05:46 - create some type of chat interaction
05:48 - then we can do that so here for
05:50 - Simplicity sake we're just sending a one
05:53 - dictionary request one message request
05:56 - to the API and we're going to expect a
05:58 - single response message object back and
06:01 - that will be a assistant roll message so
06:04 - then we have our temperature this should
06:05 - look familiar to you coming from our
06:07 - previous videos with open AI but in
06:09 - anthropic they do things similarly where
06:11 - they do have this thing called
06:12 - temperature which is going to control
06:14 - the creativity of the response so it
06:16 - starts at 0.0 and goes up to 1.0 as it
06:19 - gets closer to 1.0 it gets more creative
06:22 - so you can mess with that and just
06:24 - judging on the responses that you get
06:26 - back you can decide where to put that
06:28 - value so so that is the value that
06:30 - you're going to use most commonly they
06:32 - have some other stuff like topor P that
06:35 - you can use alternatively to temperature
06:38 - you can read about that in the
06:39 - documentation it's a little more
06:40 - advanced you would use one or the other
06:42 - but not both in this case we're just
06:44 - going to do temperature so then you can
06:46 - see I have an argument commented out
06:48 - here called system and this is just like
06:50 - the system roll that we've used in
06:52 - previous videos with open AI apis
06:54 - anthropic has it here as a top level
06:56 - argument like I said before but we can
06:58 - uncomment this and we can add some
07:00 - additional context or Guide to the
07:03 - overall request so rather than just
07:06 - saying how many toes does a dog have or
07:08 - do dogs have rather we can say respond
07:11 - only in Yoda speak so yes it'll answer
07:14 - us this question how many toes do dogs
07:16 - have but it'll answer it as if it's
07:18 - coming from Yoda the Star Wars character
07:20 - so we'll see how that works and then
07:22 - we'll comment it out and we'll see how
07:23 - it works without it this is optional and
07:27 - you can use it depending on your various
07:29 - use cases okay so then the model here
07:32 - you can go to the documentation to get
07:34 - the string that you use for the various
07:35 - models like I said there's three for
07:37 - cloud 3 there's Haiku which we have here
07:40 - and there's going to be another one for
07:41 - Sonet and another one for Opus so then
07:45 - Max tokens you can put this to whatever
07:47 - you want but this of course is going to
07:48 - control the combination of your input
07:50 - and output tokens so that you don't
07:51 - spend too much money you can keep things
07:53 - within budget so the last thing that we
07:55 - have here is actually coming from prompt
07:58 - layer so PL stands for prompt layer and
08:00 - we have plore tags is equal to this list
08:03 - of strings here we only have a single
08:05 - string you could include more if you
08:06 - wanted to but we have it set to animal-
08:09 - toes so this is just a tag for this
08:11 - specific request so that when we see it
08:13 - over in our promp layer dashboard we can
08:15 - more easily organize things and say okay
08:17 - yeah these were all the requests about
08:19 - animal toes versus these were all the
08:21 - requests about you know which presidents
08:23 - of the United States were who during
08:24 - which time and so on and so forth okay
08:27 - great so that's it for sending your
08:29 - request to the anthropic messages API so
08:32 - down here we're actually going to print
08:34 - out the response which is that message
08:36 - object and then we're going to drill
08:38 - down into it and get that first content
08:40 - object which is like a Content block
08:43 - object type something like that it
08:44 - doesn't really matter but it has two
08:46 - properties one is text and the other is
08:47 - type we want the text from that object
08:51 - the way we get that is we say
08:52 - completion. content which is a list we
08:55 - get the first item in the list using
08:56 - zero and then text so we'll see that
08:59 - here here in a second all these other
09:00 - print statements with nothing inside of
09:01 - them are just there for formatting
09:03 - purposes so let's jump over to our
09:05 - terminal and send our first request of
09:06 - course you want to make sure that you
09:08 - have your virtual environment activated
09:09 - all of your dependencies installed
09:11 - everything like that once you have that
09:12 - ready to go then you can just run python
09:15 - or Python 3 depending on how your python
09:17 - is set up and then
09:20 - main.py so this shouldn't take long
09:22 - because we're using hu sure enough came
09:24 - right back and you can see that the
09:26 - first thing we have here is the message
09:29 - object so if we go look at our code
09:31 - that's that completion
09:32 - variable and so here we have message so
09:35 - it has an ID property a Content property
09:38 - and then after content it's kind of hard
09:40 - to read this but there's one for model
09:43 - and then there's the role note that the
09:46 - role is no longer User it's assistant
09:48 - right we're going to alternate between
09:49 - user and assistant we send the user
09:51 - requests we get back the assistant
09:53 - response the stop reason is end turn
09:55 - that just means that it came to a
09:56 - natural stopping point stop sequence
09:59 - there's no stop sequence included that's
10:00 - a more advanced thing where you can
10:02 - include a list of strings and if it
10:04 - matches a string it will stop based on
10:06 - that match lastly we have the type
10:09 - message and usage and within usage we
10:12 - have the input tokens how many were used
10:15 - output tokens how many were used then of
10:16 - course a combination of that would be
10:18 - how many tokens you used Al together and
10:21 - that's going to be less than whatever
10:22 - your max tokens was less than or equal
10:24 - to okay great so then what I did is I
10:26 - drilled down into the complete
10:30 - got that first dictionary within the
10:31 - content list and then printed out the
10:33 - text so here we have this array and then
10:36 - we have this object and it has a text
10:38 - property we print it out here and what
10:40 - we get back is dogs typically have four
10:42 - toes on each paw so a total of 16 toes
10:45 - that's actually a great answer it
10:46 - actually goes on right and we gave it
10:49 - 256 tokens so it thought what the heck
10:52 - you know they didn't tell me explicitly
10:54 - to keep it to a certain number of
10:55 - characters or anything like that so
10:57 - here's my much lengthier response so
11:01 - based on this output you may decide yeah
11:03 - that's not exactly what I want and then
11:04 - you can just dial it back you can change
11:06 - the way that you prompted it maybe by
11:08 - adding something to the system or to the
11:10 - initial user content and make it to
11:14 - where it comes back formatted or limited
11:17 - to a certain number of characters based
11:19 - on whatever your needs are all right
11:20 - cool so it says include some other stuff
11:22 - here the Paw of a dog has the following
11:25 - toe configuration blah blah blah so the
11:27 - total number of toes on a dog is four
11:28 - toes time Four Paws equals 16 toes this
11:32 - is a standard number of toes for most
11:33 - dog Breeze blah blah blah BL blah
11:35 - actually interesting fact right so there
11:37 - are some rare exceptions where dogs may
11:39 - have extra toes polyl dogs uh I didn't
11:42 - even know that so that's kind of cool
11:43 - that we learned that but again it might
11:44 - not be what you need for your specific
11:46 - use case so then you can just modify
11:48 - your prompt to get back something a
11:49 - little closer to what you want So
11:51 - speaking of modifying our prompt let's
11:53 - go back to our request and you'll notice
11:56 - that system for respond only in Yoda
11:59 - speed is there uncommented but I haven't
12:01 - saved it yet so that's why we didn't see
12:03 - it the version when we ran the code this
12:05 - is what it saw where it was commented
12:07 - out so we have it uncommented here we
12:10 - have our system argument we're saying
12:11 - respond only in Yoda speak so Yoda being
12:14 - the character from Star Wars series and
12:16 - he talks in a very specific way so we
12:19 - are going to run this back
12:21 - again and the response should be the way
12:24 - that Yoda talks so it starts out with h
12:27 - which he's always doing that little
12:29 - thing before he says anything and then
12:31 - dogs have four toes on each pod they do
12:33 - a total of 16 toes a dog has so that is
12:36 - definitely how Yoda talks so great even
12:39 - CLA 3 Haiku which is the most
12:41 - inexpensive and least intelligent of the
12:43 - three models in the CLA 3 Model family
12:45 - was able to do great with those
12:47 - instructions so the last thing that we
12:49 - want to check out here of course is
12:51 - because we are dialed into prompt layer
12:54 - we have this all connected we should be
12:56 - able to go over to our prompt layer
12:58 - dashboard and and sure enough you can
13:00 - see the responses from all the requests
13:02 - that we've made so the most recent one
13:04 - is right here with the respond only in
13:06 - Yoda speak system here's the user prompt
13:09 - how many toes the dogs have and then
13:11 - here's the assistant response you can
13:13 - see that we've got our tag up here you
13:15 - can even add your own tags if you want
13:17 - to so we have our tags added
13:18 - programmatically but we can also add
13:20 - them through the graphical user
13:21 - interface here even after the fact and
13:24 - then it calculated the total tokens for
13:27 - us which is nice it even calculated the
13:29 - cost which is really cool and then you
13:30 - can take this over to the playground and
13:32 - keep messing with it from there if you
13:33 - want to do it from a graphical user
13:35 - interface or you can jump back into your
13:38 - code and you can do things
13:40 - programmatically likewise it has a
13:42 - history of all of our requests and the
13:43 - responses so you can see the one before
13:45 - it where we didn't have the system you
13:47 - can see that the system is omitted we
13:49 - just have the user prompt and then we
13:50 - have that longer assistant prompt that
13:52 - came back both of these the one that we
13:55 - just looked at with the Yoda speak and
13:56 - the one that we're looking at currently
13:58 - have the same tag of animal toes so
14:00 - they're going to be grouped in with the
14:01 - same tag we can organize things that way
14:04 - so that's it for this video teaching you
14:07 - the introductory Basics to sending a
14:09 - request with the messages API for Claude
14:12 - 3 model with specifically the hon model
14:15 - but they do have the Sonet and the Opus
14:17 - models that you can play around with for
14:19 - the anthropic API thanks so much for
14:21 - watching this video can't wait to see
14:22 - you in the next one until next time
14:24 - peace