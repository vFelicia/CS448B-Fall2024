00:00 - hey welcome back everybody this is Ian
00:02 - and in this video I'm going to show you
00:03 - how to make a chat GPT like chat bot
00:06 - from scratch so we're going to use this
00:08 - guide over on medium.com this article
00:10 - that was written by the team over at
00:11 - prompt flayer and it's going to show us
00:13 - how to do this step by step so let's go
00:16 - ahead and get into it so at the top here
00:18 - Jared the author of the article talks
00:20 - about the history of chat gbt talks
00:23 - about how in the earliest versions it
00:25 - was just a simple system prompt and we
00:27 - have an example of that here Ur chat GB
00:30 - t a large language model trained by open
00:32 - AI based on the GPT 3.5 architecture
00:35 - this is your knowledge cuto off date
00:37 - 20221 and then the current date was 2023
00:40 - 1101 so it says here that technically
00:42 - the first version of chat GPT actually
00:44 - used gpt3 which was completion based not
00:47 - chat and the system prompt is a paradigm
00:50 - that began with GPT 3.5 not longer after
00:53 - chat gpt's initial release so you can
00:55 - learn more about that in this little
00:56 - link right here but we're going to go
00:58 - ahead and move on to the point here
01:00 - where he talks about how even though it
01:02 - started out really simple very quickly
01:04 - after getting some user input and also
01:07 - seeing how there were some malicious
01:09 - requests and injections and getting
01:11 - feedback ideas and feature requests that
01:14 - the prompts the little simple one that
01:16 - we saw above actually ballooned into
01:19 - this giant prompt that you see here so
01:21 - feel free to go through and read this on
01:23 - your own time there is a lot of stuff in
01:25 - here but essentially they're trying to
01:27 - create a bunch of safeguards that are
01:29 - saying like hey if you get this type of
01:30 - input make sure that you don't do this
01:32 - or you do do that and just trying to
01:34 - cover all their bases so there's a lot
01:36 - of stuff that got added there this is
01:38 - what Jared's referring to as prompt debt
01:41 - and even companies like open AI are not
01:43 - immune to this sort of thing so with a
01:45 - little bit of history behind us let's go
01:47 - ahead and dive into building my chat gbt
01:50 - which is going to be our little chat bot
01:52 - here all right so we have some steps
01:54 - here we're actually going to break this
01:55 - tutorial up into two parts and in
01:58 - today's part we're going to go over over
02:00 - writing our system prompt building the
02:02 - chat interface and connecting to logs
02:04 - for debugging and then in the future
02:06 - video we're going to provide my chat gbt
02:08 - with World information things like
02:10 - today's date and the location of the
02:13 - user then we're going to add calculator
02:15 - tools to offset where the llm falls
02:17 - short so we'll be able to connect to a
02:21 - function that can then do some
02:22 - operations for us lastly we're going to
02:25 - talk about different ways to make
02:26 - development easier so production deploys
02:28 - Snippets and reg ression test all right
02:31 - cool so let's go ahead and get started
02:35 - one thing that we want to point out here
02:36 - is that much has already been written
02:38 - about chat gbt this particular tutorial
02:41 - will focus on some of the best practices
02:43 - of building a chat bot like this my chat
02:46 - gbt that we're going to build Jared adds
02:48 - here that he believes the best way to
02:50 - approach prompt engineering is to treat
02:51 - the llm as a blackbox it's simply just
02:54 - writing English and checking the output
02:56 - we will build our development
02:57 - environment around this principle
02:59 - primarily optimizing for Speed of
03:00 - development where quick prompt
03:02 - iterations and feedback Cycles are a
03:04 - necessity the first thing we want to do
03:06 - is write our system prompt so let's
03:08 - start there basically we need to tell
03:11 - our chatbot about itself we need to give
03:14 - it some context so it understands what
03:17 - it's supposed to do okay so that's going
03:19 - to be achieved with a prompt layer in
03:22 - this case but we're going to set a
03:24 - system prompt so that's the starting
03:26 - point for this you can see the example
03:28 - here there's a link prompt layer if you
03:30 - don't have a free account you can go
03:31 - there and sign up and you can create
03:34 - your first prompt template so this shows
03:36 - you step by step what you need to do
03:38 - you're going to create a new prompt
03:40 - template it's going to default to chat
03:41 - you're going to paste in the system
03:43 - prompt and then you're going to set an
03:44 - initial user message with an input
03:46 - variable called question so we're going
03:49 - to scroll down here we actually get to
03:50 - copy this prompt for the system prompt
03:53 - which is pretty nice so we'll go ahead
03:54 - and copy that and if you want you can
03:57 - link The Prompt layer right here this is
04:00 - what prompt ler looks like when you have
04:01 - it opened up and you're signed in and
04:04 - you can follow instructions here or you
04:05 - can just follow along with me but
04:06 - essentially we want to create a new prom
04:08 - template so the way we can do that is
04:10 - head over to our registry and you can
04:13 - see I've already created one here so
04:14 - I'll go ahead and let's see if we can
04:16 - just rename this to number two and then
04:19 - we'll create a brand new one where's my
04:21 - button we'll move that here's this blue
04:24 - button here create template so we'll
04:27 - click on
04:28 - that and and then we have our system
04:30 - prompt so you'll notice that it's
04:32 - defaulted to chat completion is the
04:34 - older version the way that it used to be
04:36 - done now in newer versions of you know
04:39 - gbt 3 and a half turbo and gbt 4 gbt 4
04:42 - Omni we use chat all right so on the
04:45 - left-and side here for the system we're
04:46 - going to paste in what we copied from
04:48 - the tutorial essentially telling the bot
04:51 - that you are an AI assistant called my
04:53 - chat gbt you are a large language model
04:56 - please be as helpful as possible if you
04:58 - don't know the answer don't be afraid to
05:00 - say I don't know and then we need to
05:02 - create a new message here and we're
05:03 - going to use single curly brackets here
05:05 - because you'll notice we have F string
05:07 - selected here if you did Ginga I think
05:08 - you would do double curly brackets but
05:10 - with FST string selected for our
05:12 - template we're going to add in a
05:15 - variable called question and you'll
05:17 - notice right away under input variables
05:19 - it actually identifies that variable
05:21 - named question and now it's going to
05:23 - expect us to pass in a variable called
05:25 - question with some Dynamic value so down
05:28 - here under parameters we'll click on set
05:31 - parameters and here you can decide which
05:33 - model provider you want which model you
05:36 - want from that provider and you can
05:37 - tweak the various settings so I'm going
05:39 - to leave all the defaults open AI gbt 3
05:41 - and a half turbo temperature one maximum
05:43 - tokens 256 and so on so with that set
05:47 - we'll go ahead and click on create
05:48 - template here and here's where you can
05:51 - set your initial prompt version message
05:54 - like a commit message if you're coming
05:56 - from a programming background you're
05:57 - probably familiar with commits from
05:58 - something like git for Version Control
06:01 - this is optional you don't have to do it
06:03 - uh it'll create a message for you if you
06:05 - don't but since it's the initial version
06:08 - I just use the anit commit syntax and
06:12 - say next so it's going to ask if you
06:14 - want to run an evaluation we don't need
06:16 - to do that just yet so we're going to go
06:18 - ahead and save this by clicking on
06:20 - confirm and it's reminding me to set a
06:23 - title thank you so up here at the top
06:25 - there's the title and this part is very
06:27 - important because we're going to use
06:28 - this title to actually fetch this
06:30 - template whenever we're writing out our
06:32 - code here in a moment so we're going to
06:34 - call it my with a capital M chat with a
06:37 - Capital C and then all caps GPT my chat
06:40 - GPT all right cool so now if we try to
06:42 - create the template it should go through
06:44 - commit
06:45 - message again I'm just going to say AIT
06:48 - commit and click on next and then click
06:49 - on confirm and here we go I've got my
06:53 - template so we're going to return to
06:54 - this in a bit we'll actually be viewing
06:56 - the history of all of our requests on
06:58 - the left here clicking on requests as
07:00 - they come through and viewing
07:01 - information about those but for now
07:03 - let's head back over to the tutorial all
07:05 - right here we are back at the tutorial
07:07 - we've already copied this system roll
07:09 - message right here and we've put it into
07:11 - our prompt template the next step is
07:14 - going to be building the chat interface
07:16 - so it talks about creating a new GitHub
07:17 - repo you're welcome to do that if you
07:19 - want to track your project on GitHub I'm
07:22 - going to go ahead and skip that but
07:23 - essentially you'll want to create a new
07:26 - project and if you do it on GitHub
07:27 - you'll want to clone it down otherwise
07:29 - we'll just just want to create a new
07:30 - project inside of your code
07:32 - editor once you have your project ready
07:35 - we're going to go ahead and use poetry
07:36 - for the virtual environment if you want
07:38 - to use poetry great if you don't have it
07:39 - installed go ahead and look it up
07:41 - otherwise feel free to use whatever you
07:43 - want for your virtual environment these
07:45 - commands here if you have po installed
07:47 - are going to go ahead and initialize the
07:48 - project and then they're going to add
07:50 - our dependencies so let's head over to
07:51 - our code editor you'll notice that I
07:54 - have an app.py file already created and
07:56 - open but it's empty so if I LS and of my
08:00 - my chat gbt demo folder you'll see that
08:03 - I have that one file all right so let's
08:04 - run these commands real quick poetry
08:07 - andit poetry add prompt layer poetry add
08:10 - open aai and poetry add
08:13 - python. so running that it's actually
08:15 - going to start by asking you some
08:16 - questions about your poetry project you
08:18 - can just run through all these until you
08:21 - get to the
08:23 - end and then it's going to update
08:25 - install resolve all the
08:27 - dependencies great so once that's done
08:30 - you get back to your prompt you can
08:32 - check and see that you have a poetry
08:33 - lock file and a p project toml file
08:36 - great so let's head over to the tutorial
08:38 - and go ahead with the next step so now
08:41 - that we have all of our dependencies
08:42 - installed and our virtual environment
08:44 - set up we want to go ahead and create
08:46 - aemv file and this is where we're going
08:49 - to set our open AI API key and our
08:51 - prompt layer API key so if you're not
08:53 - familiar with where to get those with
08:55 - prompt layer you just go to your
08:57 - settings for your account and it'll have
08:59 - a a button for getting your API key
09:01 - click on that generate a new key and
09:03 - copy it and you can bring it over to
09:04 - your EMV file and paste it there
09:06 - likewise for open AI you go log into
09:09 - your account and then go to your
09:10 - settings and you can get your API key
09:12 - from there as well so once you have both
09:14 - of those inside of your EMV file and
09:16 - you've set them equal to these two
09:17 - variable names open a API key and prompt
09:20 - player API key then you're ready to go
09:22 - with the next step so inside of your
09:25 - app.py file we're going to set up some
09:27 - boiler plate code for bringing in
09:30 - EMV OS open Ai and prompt layer so we'll
09:34 - go ahead and copy this code right
09:36 - here and paste this in so again we're
09:39 - going to import OS and we're going to
09:41 - get EMV that's going to allow us to pull
09:43 - in our environment variable for promp
09:45 - layer the one for open AI gets pulled in
09:48 - for us automatically so we don't have to
09:49 - worry about that we import open Ai and
09:52 - we import prompt layer we load the EMV
09:55 - file and then we go ahead and set up our
09:57 - promp layer client by setting it equal
09:59 - to to prompt layer and then you can pass
10:01 - in your prompt layer API key so then
10:03 - we're going to set up our open aai
10:04 - client first thing we do is we actually
10:06 - pull open AI from prompt layer client
10:09 - and once we have that we go ahead and
10:10 - initialize our client by setting it
10:12 - equal to open AI open close parenthesis
10:15 - so that'll invoke the class there and
10:17 - create this new instance which is our
10:19 - open AI client and we're ready to go so
10:21 - let's head back over to the tutorial the
10:23 - first thing it wants us to do is get
10:25 - some user input so we're going to copy
10:27 - this here head back over here and
10:30 - basically we're saying user uncore input
10:32 - is equal to the input method this is
10:34 - built into Python and then inside the
10:37 - input method we just pass it the prompt
10:39 - that we want to display right before we
10:41 - actually get the input from the user so
10:43 - in this case welcome to my chat GPT how
10:46 - can I help and then a new line with a
10:48 - little carrot that indicates that the
10:50 - user can now type
10:52 - something so we'll go ahead and save
10:54 - that head back to the
10:56 - tutorial so now that we have collected
10:59 - to the user input the next thing we want
11:01 - to do is actually get our prom template
11:04 - so we can do that programmatically using
11:05 - the library inside of our code so of
11:07 - course we'll use promp layer to do that
11:09 - you can see the syntax here is just
11:11 - promp layer client. templates. getet and
11:13 - then you pass in the name of the
11:14 - template that's why I said it was
11:16 - important that we name it specifically
11:18 - capital M my Capital C chat Capital GPT
11:22 - the next thing you do is you pass in a
11:24 - dictionary with some values for the
11:26 - provider including your input variables
11:28 - that get injected into to that template
11:30 - the benefit of using prompt layer for
11:31 - the prompt template is that in addition
11:33 - to being able to track all of your
11:35 - responses in a dashboard automatically
11:38 - we can also tweak our template in one
11:41 - place and have it update anywhere where
11:43 - that template is being used so we don't
11:45 - have to go out and modify our code
11:48 - anywhere we literally just go to our
11:49 - dashboard change the template as we see
11:51 - fit and then we can continue running our
11:53 - code without any changes there pretty
11:55 - cool so let's go ahead and borrow this
11:58 - code right here
12:00 - head over to our code and paste this in
12:04 - and then let's talk about what it's
12:07 - doing so here we have the my chat GPT
12:11 - prompt variable set equal to prompt
12:13 - layer client. templates. getet and again
12:16 - this is why it's important that we name
12:18 - this my chat gbt with the exact spelling
12:20 - and the exact capitalization is because
12:23 - we're actually using that name of the
12:25 - prompt template to pull it in to our
12:27 - code so then after that we pass in a
12:30 - dictionary here with the provider in
12:32 - this case open Ai and then any input
12:34 - variables so we have the one user input
12:36 - that we got above we're going to inject
12:38 - that as the question variable so if you
12:40 - recall from our prompt template that we
12:42 - created in prompt layer earlier we did
12:44 - have for the initial user roll content
12:48 - we had a question variable that this is
12:51 - where we actually inject a value into
12:54 - all right so after that we want to
12:55 - actually send the request out to the API
12:58 - so we use the open client variable we
13:00 - use chat completions create and here
13:03 - we're taking all the values from this my
13:05 - chat GPT prompt llm quars and we are
13:09 - going to dump them into this create
13:13 - method call and so it's going to go
13:15 - through and it's going to take each key
13:16 - value pair and it's going to put it in
13:18 - here for us so that's what this double
13:21 - asterisk is doing here it's taking those
13:22 - key word arguments and it's injecting
13:24 - them into create and then lastly we're
13:27 - going to include plore tags and we're
13:29 - going to put a little tag in here you
13:31 - will note that this is a list in Python
13:33 - and so we can include any number of tags
13:35 - that we want so we will see when we go
13:37 - back to the dashboard for promp player
13:39 - in a little bit that will have this tag
13:41 - right here and this one is specifically
13:43 - for development but if you are doing
13:45 - things in production you could add a tag
13:47 - or change the tag out for production
13:49 - that way you could keep track of which
13:50 - requests and which response are what
13:53 - cool so we assign the response of that
13:55 - to the response variable and then we're
13:57 - going to Traverse down into that
13:59 - dictionary that comes back it has a
14:01 - choices list we take the first element
14:04 - from that list it has a message
14:06 - dictionary which has a Content key and
14:09 - that's going to have a string that
14:11 - points to the actual response so we're
14:14 - going to print that out and we should
14:16 - now be able to run our code we can check
14:18 - with the tutorial just to make sure that
14:21 - yep they are actually testing it so in
14:23 - order for us to test it we're going to
14:25 - use poetry so because poetry is our
14:27 - virtual environment we just run poetry
14:29 - shell it's going to put us into the
14:31 - virtual environment and then we can run
14:32 - python app.py let's go ahead and try
14:34 - that now make sure that your file is
14:36 - saved and then we'll open up our
14:38 - terminal down here so we'll just run
14:41 - poetry showell here it'll get us into
14:43 - our virtual environment and then we can
14:45 - run python
14:47 - app.py okay so it says welcome to my
14:49 - chat gbt how can I help and then we have
14:51 - our little carrot here with the space
14:53 - and then we have our cursor indicating
14:54 - that we can start typing so we'll just
14:56 - ask at something really simple we'll say
14:58 - uh what what is CSS so cascading
15:03 - stylesheets wait a second for the
15:05 - response and there it is CSS stands for
15:07 - cascading stylesheets blah blah blah
15:09 - blah blah awesome so we are able to
15:12 - interact with the bot simply by asking
15:14 - it a question and then it gives us a
15:15 - response that's cool but what would be
15:18 - really awesome is if we could ask it
15:20 - questions back to back and actually have
15:22 - a conversation a conversation such that
15:24 - it actually has some memory of not just
15:27 - our questions in the past but also its
15:29 - responses that's how most chat Bots
15:31 - operate and that's how we want to set
15:33 - this one up so there's a couple
15:34 - different ways to achieve this we'll see
15:36 - what the tutorial shows us let's go see
15:38 - what the next step is O making a loop
15:42 - Okay cool so we were able to ask one
15:44 - question we're able to get back one
15:45 - answer but what would be really cool is
15:47 - if we could have a loop that would allow
15:50 - us to continue having a conversation
15:52 - with this chat bot until we decide that
15:53 - we're done so the way that we're going
15:55 - to achieve this is with a W Loop every
15:57 - time the llm responds with a message
15:59 - we'll ask the user for a response and
16:01 - we'll pend both new messages so their
16:03 - response and our question into our
16:06 - prompt so there's a couple drawbacks of
16:08 - this of course our prompts can get
16:09 - really long really really quickly but
16:12 - this is just an MVP so in the future
16:16 - alternatively if you want to you could
16:17 - summarize the conversation you can
16:19 - inject it back into the system prompt
16:20 - context there's a couple other
16:22 - strategies for this we're not going to
16:23 - go into them in this tutorial for now
16:25 - what we're going to do is we're just
16:26 - going to append their response resp in
16:29 - our next question back into the messages
16:32 - list so let's see how to do that all
16:34 - right so we have some code here and it
16:36 - says just add this Loop that we've
16:38 - created with this wall Loop here right
16:40 - after we're done printing the first llm
16:42 - response from above so the code that we
16:43 - already have in there we're just going
16:45 - to add this right after it so we'll copy
16:47 - this here we'll head back to our code so
16:50 - we've printed the response this is our
16:52 - initial request initial response and now
16:54 - we're actually going to paste in this
16:56 - Loop here so I'll make this a little bit
16:58 - smaller so you can kind of see the whole
17:00 - thing
17:01 - here but basically it starts with
17:04 - prepare the new prompt a messages
17:07 - variable is equal to my chat GPT prompt
17:10 - now if you remember my chat GPT prompt
17:12 - is the variable we created whenever we
17:13 - pulled the template and we injected the
17:16 - very first user input value so it had
17:18 - that thing llm quars that we were able
17:20 - to dump into this create method for the
17:23 - client Jack completions call and we're
17:26 - going to access messages if you think
17:28 - back to our template that we created
17:30 - with prompt layer you'll recall that we
17:32 - had the system role and then we had our
17:34 - user role the user just had the question
17:36 - variable and the system had its initial
17:39 - prompt so that's what's going to be
17:41 - inside of the messages list at least the
17:43 - first time that we make our initial
17:45 - request it's going to have that first
17:47 - object with the system role and its
17:50 - content and then the next object is
17:52 - going to be the user role with the
17:54 - message for its content so now that we
17:57 - have that messages list the next thing
17:59 - we want to do is get the response
18:01 - message from the request that we just
18:02 - made so we have that response variable
18:04 - from the request and we're going to use
18:06 - choices bracket 0. message to get access
18:10 - to the first message this is going to be
18:13 - the assistant roles message that came
18:15 - back from our initial request okay so
18:17 - we're going to take the assistant
18:19 - response to our initial question and
18:22 - we're going to append it to the end of
18:24 - the existing messages list the one we
18:26 - just talked about that had the system
18:28 - rle and the initial user so at this
18:30 - point it goes system user assistant and
18:33 - then what follows is just going to be a
18:35 - back and forth between user and
18:37 - assistant all right so inside the loop
18:39 - is how we're going to facilitate that at
18:41 - this point we need to get more user
18:44 - input okay so we're going to say user
18:46 - uncore input the same variable that we
18:47 - used before is equal to again we're
18:50 - going to use the input method and then
18:51 - here we're just going to have the little
18:53 - bracket or the little carrot with a
18:55 - space indicating that the user can now
18:57 - type something now now that we have the
18:59 - user input we're going to append that
19:01 - after the assistant role that we just
19:03 - put into our messages list so here you
19:05 - can see messages. append and then we
19:07 - have this dictionary the role is now
19:09 - user the content is the user input that
19:11 - we just gathered from the user all right
19:13 - now we want to send all this to the llm
19:15 - so the first thing we do is we're going
19:16 - to access that my chat gbt prompt this
19:19 - is our prompt template we're going to go
19:20 - down into the llm quars and we're going
19:23 - to go over into our messages list here
19:25 - on this object and we're going to
19:27 - overwrite it with with the messages that
19:29 - we have been appending the assistant and
19:32 - user objects to so essentially we're
19:35 - just updating the my chat gbd prompt llm
19:38 - quarks and the reason we're doing that
19:40 - is because we're actually going to pass
19:42 - that into our next request to open AI
19:45 - API so you can see on the next line
19:46 - we're getting the response from sending
19:48 - a new request via client chat
19:51 - completions a creation of a chat
19:53 - completion so sending out this request
19:55 - to the openai uh API and again we're
20:00 - just dumping the contents of my chat gbt
20:02 - prompt llm quars and this is going to
20:05 - fill in things like which model we're
20:07 - using what the temperature is what the
20:09 - messages list is which at this point is
20:11 - updated and anything else that's
20:14 - included there so after that we have our
20:17 - plore tags again if you wanted to do
20:19 - this in you know staging or production
20:21 - or whatever the case you can put any
20:23 - kind of tags you want in here one or
20:24 - more it's up to you you can even leave
20:26 - it empty if you want for us specifically
20:28 - we're saying my chat gbt hyen Dev for
20:30 - our tag and now we're getting back the
20:33 - response message from that request so
20:36 - again we get the response we assigned it
20:38 - on 44 and we're just going down into the
20:40 - choices list and pulling that message
20:42 - dictionary and assigning that to
20:44 - response message that way we can go back
20:46 - to our messages list which is keeping
20:48 - track of all the different messages so
20:51 - again we start with our system we go to
20:54 - user then we go to assistant and then
20:55 - user and assistant at this point this
20:57 - response is going to be an assistant
20:59 - because it's a response from the API and
21:02 - we're going to append that dictionary
21:04 - stored in response message to the
21:05 - messages list now we're actually going
21:08 - to print that and then we're going to
21:09 - start the loop over again so at this
21:11 - point what we're printing is the content
21:14 - of the message dictionary from the
21:17 - assistant role which is going to have
21:19 - the response to the user's last question
21:21 - and now the user has an opportunity to
21:23 - continue the conversation because we
21:25 - start at the top of the loop again
21:26 - awesome all right let's go ahead and
21:28 - test this out in our terminal so with
21:29 - our virtual environment activated let's
21:31 - go ahead and run Python
21:33 - app.py and it says welcome to my chat
21:36 - GPT how can I help so here we're going
21:38 - to say what is and let's ask what is
21:41 - HTML all right cool so when it comes
21:44 - back with its answer you'll notice that
21:46 - we have this little carrot here it's
21:48 - waiting for our next question so we can
21:50 - continue on now we can keep asking it
21:53 - questions that's really cool but what's
21:55 - even cooler is that it has this
21:56 - short-term memory and it recalls
21:58 - information about our past questions and
22:00 - its past responses so let's try this
22:03 - let's say my name is Ian what is your
22:08 - name and it says hello Ian my name is my
22:10 - chat gbt how can I assist you today all
22:13 - right cool so we've told it something
22:14 - about ourselves now let's say what was
22:18 - my name
22:20 - again and it recalls that our name is
22:23 - Ian it says your name is Ian how can I
22:25 - assist you today Ian awesome so it's
22:28 - actually able to recall information from
22:30 - the history of our conversation and now
22:33 - I can ask it something else you know
22:35 - what is
22:38 - CSS and it gives me my answer so when
22:41 - we're done here we don't have a
22:42 - mechanism for exiting just yet so you
22:44 - can just do control D to exit in that
22:47 - way and it'll take you back to your
22:49 - terminal all right cool let's jump back
22:51 - out here to the tutorial and we've
22:53 - already run it we've tested it out so
22:55 - you can see these examples here the last
22:57 - thing we want to do before we go here is
22:59 - add some logging so everything we've
23:01 - done is really neat but as is mentioned
23:04 - here any sufficiently complicated AI
23:07 - application can expect to go through a
23:08 - lot of prompt iterations so being able
23:11 - to track which ones work and which ones
23:13 - don't work is going to be critical so we
23:16 - can do that with prompt layer and
23:18 - actually the original inspiration behind
23:19 - prompt layer was because the author of
23:21 - this article Jared lost one of his good
23:24 - prompts after making a few edits so you
23:26 - may have experienced this similar fr
23:28 - ation where you're working with the
23:30 - prompt and it's working really well for
23:31 - you but then you edit it a couple times
23:33 - trying to get it to be better and now
23:36 - maybe it's performing worse but you
23:38 - can't remember exactly what that
23:39 - original prompt was and maybe you don't
23:41 - have a history of it anymore so with
23:42 - prompt layer we're getting the tracking
23:44 - for every single request that we send
23:46 - which is great that's built in if we
23:48 - head over here we can actually see all
23:50 - these requests that we've sent have
23:52 - actually been tracked here so you can
23:54 - see what is HTML and I said my name is
23:57 - Ian there's it response if you go to the
23:59 - next one it updates with the next part
24:00 - of the conversation and then the last
24:02 - one you can see the full conversation so
24:04 - system rooll user assistant user
24:06 - assistant user assistant user and then
24:08 - the final assistant all that worked
24:10 - great and in fact we can see our tag
24:12 - here my chat gbt hyen Dev we can even
24:15 - add some tags here if there weren't tags
24:17 - that we added whenever we first created
24:19 - the request but one thing that's not
24:21 - happening is it's not actually connected
24:23 - to the prompt template so you see no
24:25 - template here in the top left corner and
24:28 - we want to connect it with the template
24:29 - that we're using in this case my chat
24:31 - jpt so that's going to be what we're
24:33 - going to do now let's head over here and
24:36 - if we scroll down you can see there's a
24:39 - link to a way to track which logs were
24:42 - generated with which prompt versions so
24:44 - you can go ahead and click on the
24:45 - enrichment described here link if we do
24:48 - this it actually takes us over to the
24:49 - promp layer documentation has examples
24:51 - for python JavaScript and rest in this
24:54 - case we're going to use the one for
24:55 - python so we're going to go ahead and
24:57 - copy this head back over to our code
25:00 - what I'm going to do is I'm going to put
25:01 - it right after our request so we send
25:03 - our request we assign the response to
25:05 - the response variable and now right here
25:09 - we're going to paste in this prompt
25:12 - layer client track
25:14 - prompt and it's looking for a variable
25:17 - called plore request ID that's actually
25:20 - going to come from the request here
25:22 - because the client if you recall even
25:24 - though it's open AI it's being pulled
25:26 - from prompt layer and so it has some
25:28 - additional features so what we can do
25:30 - here is we can say response comma plore
25:35 - request ID and then this part I can't
25:37 - remember exactly it's like
25:40 - PL request
25:43 - ID is equal to
25:45 - true but I'm not 100% sure so we're
25:48 - going to jump over here and see if we
25:51 - can find that so under request ID is
25:53 - under
25:55 - logging if you scroll down here there it
25:58 - is it's actually return plore ID so
26:02 - we're going to go back to our code and
26:04 - we're going to say return plore ID so
26:09 - what that does if we add return plore ID
26:12 - equal true inside of this chat
26:14 - completions create method and then we
26:17 - set a comma plore requestor ID up here
26:21 - in the assignment for the variables is
26:24 - that this will actually return a tuple
26:27 - with two values the first being the
26:29 - response and the second being the ID of
26:31 - the prompt layer request and the good
26:33 - thing about that is now we can pull the
26:34 - response and we can get whatever
26:36 - information we need about that but we
26:37 - also have access to our prompt layer
26:39 - request ID so that we can pass it into
26:41 - this code and be able to track the usage
26:44 - of our prompt template so the next thing
26:47 - after setting the requestor ID equal to
26:50 - the pl request ID is the prompt name so
26:53 - you'll want to update this value to be
26:55 - the name of your prompt template in our
26:57 - case it's my chat GPT capital m capital
27:00 - c Capital
27:01 - GPT and then here you've got your prompt
27:04 - input variables so this could be a
27:08 - dictionary with your input varibles so
27:11 - for example we know we have one input
27:13 - variable question and then we know we
27:15 - set that equal to the user input now if
27:18 - you're using this anywhere else then you
27:21 - could just create a dictionary and you
27:23 - could add it there but for Simplicity
27:25 - sake here we're just going to say this
27:27 - is a literal dictionary with the key of
27:29 - question we know we only have one
27:31 - variable that we're using and then at
27:33 - this point the user input is whatever
27:35 - the user is inputed right here cool so
27:39 - then the last value here is this version
27:41 - equals 2 the two is just an example but
27:44 - you would set this to whichever version
27:45 - of your prom template you want to use
27:48 - for this tracking now if you don't have
27:50 - a specific version or if you want to use
27:53 - the latest version rather what you can
27:55 - do is you can just submit this and it'll
27:57 - always default to whatever the latest
27:59 - version is of that template so I don't
28:01 - think we have any versions yet you would
28:03 - get versions if you went over to your
28:06 - registry and you went inside of here you
28:08 - see we have version one and if you made
28:10 - an edit so you can see there's an edit
28:12 - button here and you go change some
28:14 - things and then you update and then you
28:16 - do a commit message and what it would do
28:18 - is it would create a second version here
28:21 - that you could then pass in the number
28:22 - two for version two if you had multiple
28:24 - versions you can do it that way there's
28:26 - also labels that can be used so if you
28:28 - want to do that you can refer to the
28:30 - documentation but it's pretty versatile
28:32 - again if you omit the
28:35 - version so if we take this out it's just
28:38 - going to use the latest version of The
28:40 - Prompt with the name in this case my
28:42 - chat GPT so we'll go ahead and remove it
28:45 - save that and run our code again and see
28:49 - if it
28:50 - works cool so it's saying hey welcome to
28:52 - my chat chbt how can I help and I'll say
28:54 - who was the first president of the
28:58 - United
29:01 - States and it says first president was
29:03 - George Washington with some information
29:05 - about when he served and then I'll do my
29:07 - name is
29:09 - Ian what's
29:11 - yours says Hi Ian I'm my chat GPT we can
29:16 - say what was my name
29:19 - again and it can say your name is Ian
29:22 - how can I help you today Ian awesome so
29:25 - let's head over to prompt ler here you
29:27 - can see our most recent tracking here
29:29 - but notice the difference here when we
29:31 - click on the latest one sure it has the
29:33 - system Ro and it's got the user and
29:35 - assistant user assistant user and then
29:37 - the final assistant so this may look the
29:39 - same as it was before down here but
29:42 - you'll notice where it says no template
29:44 - we actually have a template now you can
29:46 - see if I click on this it goes straight
29:48 - over to my registry for my chat gbt the
29:51 - template that we used and if we look
29:53 - down here we can actually see there's
29:55 - some tracking here and if you Mouse over
29:57 - the question variable you can actually
29:59 - see what the dynamic value was for the
30:01 - variable at the time of this specific
30:03 - request and a preview of the response
30:06 - from the assistant for that specific
30:08 - request and then if we go back over here
30:11 - you can see if you Mouse over the text
30:13 - in the most recent user message the
30:15 - content of it is blue and if you Mouse
30:17 - over it it actually does a little tool
30:19 - tip telling you the name of the variable
30:22 - assigned to that one in this case
30:24 - question so pretty cool now we have some
30:26 - more fine grain tracking of our prompt
30:29 - template and anytime we want to go over
30:31 - to it we can click on that we can make
30:33 - edits to it we can have different
30:34 - versions We can track which version
30:36 - we're using for which prompt requests
30:38 - that we have over inside of our code
30:40 - just a lot of Versatility added on to
30:43 - what you would normally be doing if you
30:44 - were writing a chatbot so really cool
30:46 - that promp layer gives us all those
30:47 - features makes it a lot easier for us to
30:49 - build these types of applications and
30:51 - then iterate on them quickly that's it
30:54 - for this video if you go back to the
30:56 - tutorial you can see that the next thing
30:58 - that we're going to do in the next video
30:59 - here is we're going to provide our
31:01 - chatbot with some real world information
31:04 - for example the current date and then
31:06 - the location of the user thanks a lot
31:09 - and we'll catch you all in the next
31:10 - video peace