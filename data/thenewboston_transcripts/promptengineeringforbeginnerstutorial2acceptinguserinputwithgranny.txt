00:00 - hey everyone welcome back this is Ian
00:03 - here bringing you this AI series with
00:05 - the New Boston we're in video two of the
00:07 - series so glad to have you here very
00:10 - excited to bring you this video so let's
00:11 - get into it in this particular video
00:14 - we're going to build on what we did in
00:15 - the first video we're going to use open
00:17 - AI chat completion API to interact with
00:20 - it and get back some cool output right
00:23 - so we're going to have a conversation
00:25 - with open ai's GPT 3.5 turbo model and
00:29 - we're going going to take control of
00:31 - exactly what that conversation is going
00:33 - to look like normally when we're out
00:35 - there on chat gbt or similar generative
00:37 - AI tools we just type in something we
00:40 - might tell it like hey you know pretend
00:42 - you're this and give me a response back
00:44 - you know write me a poem in the voice of
00:46 - Shakespeare or something like that and
00:48 - that's great but in this case we're
00:50 - actually writing programs that allow our
00:53 - users to interact with chat but we have
00:57 - full control over the pretext of
00:58 - everything so we can really build some
01:00 - cool things here in this use case we're
01:03 - going to augment what we did previously
01:06 - where we hardcoded a question to the
01:08 - model we said something like tell me the
01:11 - NHL team for Philadelphia and it came
01:15 - back with a response very
01:16 - straightforward answer we had a low
01:18 - temperature setting in our request
01:21 - parameters so it was giving us the same
01:23 - answer over and over again that's great
01:25 - now what we're going to do is we're
01:26 - going to take in Dynamic input from the
01:28 - user in the Contex
01:30 - of the program where it's executed in
01:32 - this case the terminal and we're going
01:34 - to use that Dynamic input to send it
01:38 - through with our chat completion request
01:41 - and what that will do is make it to
01:42 - where the user now has the ability to
01:44 - ask the question so instead of just
01:45 - hardcoding it and having the same
01:47 - question over and over again the user
01:49 - can start driving the conversation we're
01:51 - going to go a step further though
01:52 - instead of just having the model act as
01:54 - a default helpful assistant we're going
01:57 - to tell the system role that hey you you
01:59 - are a sweet old helpful grandma and what
02:02 - will happen there is that the response
02:04 - back from the API is going to be in the
02:07 - voice in the context of a sweet helpful
02:09 - old grandma so it's going to add a
02:11 - little bit of flare and uniqueness to
02:13 - those responses so let's take a look at
02:15 - it on the first two lines here we simply
02:18 - have our import for the OS the import
02:21 - for the open AI library and then setting
02:24 - that open AI API key to the result of
02:26 - using OS with the get EMV method to get
02:30 - the actual value of our open AI API key
02:33 - environment variable I know that's a lot
02:35 - hopefully you're familiar with setting
02:36 - up environment variables if you're not
02:38 - go ahead and look that up it's really
02:40 - straightforward just make sure that you
02:41 - look it up for your operating system or
02:43 - your environment you can do it with
02:45 - python or you can do it with Windows you
02:46 - can do it with Mac or Linux there's a
02:48 - bunch of different ways and they're all
02:49 - very straightforward so once you have
02:51 - that set up now you're ready to interact
02:53 - with open ai's API the next thing we
02:56 - want to do is get some Dynamic user
02:58 - input and we we can do that with
03:00 - Python's built-in input method so the
03:03 - input method takes an argument that's a
03:05 - string and it'll print that out into the
03:07 - console and then the user will be
03:09 - prompted to respond to that they're
03:11 - going to type on their keyboard and when
03:13 - they press enter or return the result of
03:15 - that is going to be stored in this
03:17 - variable here in this case the variable
03:18 - is called user text so what we'll see as
03:22 - output in the console or in the terminal
03:24 - is what can granny help you with today
03:27 - so we're already setting the pretext
03:29 - here so that the user knows that they're
03:30 - communicating with Granny they're no
03:32 - longer communicating with just a regular
03:33 - helpful assistant now we can use that
03:36 - user text that we gathered from our user
03:39 - and we can pass it into our chat
03:41 - completion API request down here on line
03:44 - nine you can see everything here looks
03:46 - pretty similar to what we did in the
03:47 - previous video right we're using open
03:49 - ai's chat completion API we're creating
03:52 - a new chat completion object behind the
03:54 - scenes it knows which endpoints to send
03:56 - the request to and the headers and
03:58 - everything we don't have to worry about
03:59 - any of that all we have to do is make
04:00 - sure that we pass in values for
04:03 - parameters that we want to use so we
04:05 - pass them in as arguments in this case
04:07 - model messages temperature and Max
04:09 - tokens we talked about those in the
04:11 - previous video but let's go ahead and
04:12 - review exactly what they're doing here
04:14 - so model is going to dictate which of
04:17 - the open AI models that we're going to
04:19 - use llm large language model in this
04:22 - case GPT 3.5 turbo it's more affordable
04:25 - it's still very powerful it's great for
04:26 - just getting your feet wet and having
04:28 - some fun with this API without spending
04:30 - a bunch of money on tokens okay messages
04:33 - messages is going to have different
04:35 - objects inside of it it's a list right
04:37 - an array and each object has a role
04:40 - there's three roles we have system user
04:42 - and we have assistant the system is that
04:44 - very first object that just tells the
04:46 - model like hey this is how you need to
04:48 - act this is the role you need to take on
04:51 - whenever you're responding to me in
04:52 - future responses as an assistant and
04:55 - then anything from the user is going to
04:57 - be us our input anything from the the
04:59 - assistant is going to be the responses
05:01 - that we get back and we can append these
05:04 - objects into this list as you'll see in
05:06 - the later video so that we can continue
05:07 - building on this history so every time
05:09 - we go back and communicate with the API
05:12 - it knows exactly the context of the
05:14 - conversation for now we're just setting
05:16 - the pretext we're saying hey system you
05:19 - are a sweet old helpful Grandma so the
05:21 - role of system the content is you are a
05:23 - sweet old helpful Grandma easy enough
05:26 - that's our first object in the messages
05:27 - list the next one is roll of user again
05:30 - this is the input from us the user or
05:32 - the users that are using our program the
05:35 - content is going to be set to some type
05:37 - of string now previously we hardcoded
05:39 - this now we're going to set it to the
05:41 - variable user text which was executed
05:43 - right when our program was run initially
05:46 - we'll see that in a moment and it asks
05:48 - the user for some input what can granny
05:50 - help you with today so then the user
05:51 - enters something in there and we use
05:54 - that inside of our request here so that
05:56 - the chat completions API can say okay
05:58 - I'm a sweet old helpful Grandma here's
06:00 - your question let me build a response
06:02 - for you and then send it back when we
06:04 - get it back we store it in this response
06:06 - variable and then we can take a look at
06:08 - it of course we have our temperature
06:10 - that's going to determine how
06:11 - deterministic how variable how creative
06:15 - the responses are in this case we're
06:16 - pretty close to zero at 0.5 the scale is
06:19 - 0 to two the closer you are to zero the
06:22 - less random it'll be the closer you are
06:24 - to two uh the more creative but
06:26 - potentially more random and error prone
06:28 - it could be Max token
06:29 - this is the number of tokens that we
06:31 - will allow the response to go up to in
06:34 - the prompt response this isn't including
06:37 - the tokens that are being used for the
06:39 - input this is just for the prompt
06:40 - response so the total tokens will be the
06:43 - max tokens whatever gets used from those
06:46 - and whatever tokens are inherently uh
06:49 - included in our inputs so we'll see more
06:51 - of that in a second so you see you'll
06:53 - see exactly what I'm talking about okay
06:55 - so that's it whenever we pass in all of
06:57 - these arguments and we send them to the
06:59 - chat completion API it does its thing
07:02 - and then it sends us back a response so
07:04 - we're going to print that response it's
07:05 - just going to be an object with some key
07:07 - value pairs we're going to print a empty
07:10 - line just for formatting purposes and
07:12 - then we're going to actually dive down
07:14 - into the response object we're going to
07:15 - go look at what's called choices you can
07:18 - set a choices argument to give you
07:20 - multiple choices to look at in this case
07:22 - we omitted it so we're only going to
07:23 - have one and therefore we're going to
07:25 - access that one choice at the zeroth
07:28 - index inside of the choices list once
07:31 - inside of there we're going to have
07:32 - another object to look at it's going to
07:33 - have a message field or attribute and
07:35 - when we look inside there it'll have a
07:37 - Content attribute and when we look
07:38 - inside of that ultimately we're going to
07:40 - get back our string of text and that's
07:42 - what we're going to print to the console
07:44 - here so let's open up our code editor
07:48 - and I should mention before we run this
07:49 - code that if you're following along and
07:52 - you've got the repository with all the
07:54 - solution code for this entire series uh
07:56 - from the description of course on your
07:59 - computer computer there's going to be a
08:00 - couple folders in that main repository
08:02 - the one that we are in currently is the
08:04 - open aior examples this is the second
08:08 - video in the series so we are inside of
08:09 - 02 user input all the code that we just
08:13 - looked at is inside main.py of course
08:15 - you're going to want to make sure that
08:16 - if you're doing this locally you have a
08:17 - virtual environment set up you have all
08:19 - your dependencies installed and you have
08:20 - your open AI API key set as an
08:23 - environment variable so with all of that
08:25 - ready to go then you can run python
08:28 - main.py from inside of the O2 or 02 user
08:31 - input folder so we're going to run it of
08:34 - course it's going to hit that line where
08:35 - it wants the input so the prompt for us
08:38 - is what can granny help with help you
08:40 - with today and we're going to say we're
08:43 - going to keep it simple at first okay
08:45 - because this is super powerful you can
08:46 - ask open-ended questions and you can get
08:48 - massive responses but just remember
08:51 - large responses means lots of tokens
08:52 - lots of tokens means lots of money okay
08:54 - so let's do something simple like what
08:58 - is the capital of New
09:01 - York now if we had asked this question
09:04 - with our previous implementation from
09:05 - the first video it would just be very
09:07 - straightforward very simple it would
09:09 - just say the capital of New York is
09:11 - Albany in this case remember this is a
09:14 - helpful granny so our responses can be a
09:16 - little more creative it says oh my dear
09:18 - the capital of New York is Albany okay
09:21 - very similar to what it would have been
09:22 - but now it's calling us my dear and then
09:25 - it goes on to say it's a lovely city
09:28 - located along the river is there
09:30 - anything else you like to know so again
09:32 - helpful old grandma she's following up
09:34 - with us she's saying is there anything
09:35 - else you like to know she's calling us
09:36 - dear you know I feel like I'm I'm at
09:39 - Grandma's house right now and she's
09:40 - helping me out now let's look at the
09:42 - response object here it's going to be
09:44 - very similar to what we saw before in
09:46 - our initial implementation from the
09:47 - first video it has a unique ID for this
09:50 - chat completion object it says that the
09:52 - object type is that of chat completion
09:55 - the created Property here is going to be
09:56 - a Unix timestamp the model is the model
09:59 - that we use GPT 3.5 Turbo with the
10:02 - version number the choices because we
10:04 - omitted choices as a argument in our
10:06 - initial request it's only going to give
10:07 - us one so we have one choice object like
10:10 - we talked about before the index is zero
10:13 - the message is going to have the role
10:15 - that it is if it's coming back to us
10:17 - it's the assistant right and then it has
10:18 - the content so the content is what we're
10:20 - really uh concerned with is what we want
10:22 - to look at so we'll see here in a second
10:25 - how we access that here it is the string
10:28 - that's what we
10:29 - right and then the finished reason of
10:31 - stop that's good we want a finished
10:33 - reason of stop if we modify our tokens
10:36 - so that we're creating kind of a
10:37 - threshold that you can only go up to so
10:39 - many tokens in your response that's fine
10:41 - but just know that if the question is
10:44 - open-ended and so the response is a
10:45 - longer response and if it meets its
10:48 - token length at Max tokens then it's
10:50 - actually just going to cut off the
10:51 - response and the Finish reason is not
10:52 - going to be stop it's going to be length
10:55 - so we can look at that more in a second
10:56 - before we wrap up the video the usage of
10:59 - course is useful because it shows us
11:01 - what the total number of prompt tokens
11:03 - were prompt tokens being the response
11:05 - the completion tokens being the result
11:08 - back to us the chat completion and then
11:10 - the total tokens of course is the
11:11 - combination of those two so 61 is super
11:13 - low that's great we asked a really
11:15 - simple question we got a pretty simple
11:16 - answer didn't use a lot of tokens now we
11:19 - can ask a not so simple open-ended
11:21 - question and it can use all 10,24 of the
11:25 - tokens that we set in our Max tokens
11:27 - that may not be what we want so we might
11:30 - tune it down to like a
11:32 - 100 but then if we do something where we
11:34 - run the program it says what can granny
11:37 - help you with today and we say tell me a
11:39 - story granny and we run
11:42 - it what's going to happen most likely is
11:45 - it's not going to be able to tell us a
11:46 - full story in 100 tokens for the
11:48 - completion and so it'll cut it off and
11:51 - so we'll see what that looks like in the
11:53 - response
11:54 - object okay so completion tokens this is
11:58 - the content ENT of the assistant
12:00 - responding to us that's our completion
12:02 - is 100 tokens okay total tokens of
12:06 - course is the combination of the prompt
12:07 - tokens plus the completion tokens so
12:09 - when you do maxcore tokens you're saying
12:11 - your completion can only use this many
12:13 - tokens in this case 100 and then it says
12:16 - okay well in addition to the completion
12:18 - your prompt your input use this many
12:21 - tokens as well so here's the total for
12:23 - that okay great now because in our code
12:27 - down at the very bottom we Traverse down
12:29 - into the object choices zero message
12:31 - content on the
12:32 - response and we print that we get that
12:36 - printed out really nicely here right so
12:38 - we can see it here but it's included in
12:39 - this larger object with all this you
12:41 - know distraction so if we go down here
12:43 - and we look at that output we can see it
12:46 - starts telling us a story which is what
12:48 - we asked it to do and it's doing it as
12:50 - if it's a helpful Grandma however
12:53 - because we limited the max tokens to 100
12:56 - it got to 100 tokens and it stopped you
12:59 - see here curiosity peaked that's the
13:01 - beginning of a sentence but it's not a
13:03 - full thought we don't have a period we
13:05 - don't have an end of the sentence so
13:07 - there's going to be some let's just call
13:09 - it an artifact of limiting your tokens
13:13 - and so when you're building your
13:14 - applications you have to take that into
13:15 - account if I'm limiting my tokens could
13:17 - the response not be fully finished how
13:21 - do I determine that well that's where
13:23 - the Finish reason comes into play if it
13:25 - says finish reason is stop that means it
13:27 - came to a natural stopping point that's
13:29 - great if it's length then you probably
13:31 - want to do a check for that and you want
13:33 - to follow up with some type of response
13:35 - or output or message or something
13:37 - logging whatever you choose to do to
13:39 - have the best user experience possible
13:42 - based on the limitations of your program
13:45 - just something to think about let's go
13:47 - ahead and do a quick overview of
13:49 - everything we just talked about because
13:50 - even though it's only a few lines of
13:52 - code we just did something really
13:53 - powerful and now you have the ability to
13:55 - go in here augment it and really have
13:56 - some fun kind of moving things around
13:58 - and making it do exactly what you want
14:00 - it to do so again boiler plate code at
14:02 - the top get that environment variable
14:04 - set it on your open AI as the API key
14:07 - that's all good to go now get that user
14:10 - input assign it to a variable make a
14:13 - call to the API assign the result to a
14:15 - variable and then print out the entire
14:18 - result print an empty space for
14:20 - formatting purposes and then print the
14:23 - actual content so Traverse down into
14:24 - that object until you get to the content
14:26 - attribute and print its output now more
14:29 - specifically inside of the open AI chat
14:31 - completion API call just remember that
14:34 - our system is no longer a helpful
14:36 - assistant we said you are a sweet old
14:38 - helpful grandma have fun with this mess
14:40 - around with it make it do something
14:41 - different right see what kind of output
14:43 - you can get based on whatever you put
14:45 - inside the content here and then again
14:48 - you can change your input here according
14:50 - to whatever you set the context of the
14:53 - content string for your system rle
14:56 - you're going to end up passing that text
14:58 - down in here it's going to make things
14:59 - Dynamic and in future videos we're going
15:01 - to show you how to keep that
15:02 - conversation going so that the user can
15:04 - keep going back and forth with the model
15:07 - and actually having a full conversation
15:08 - with a tracked history and the model
15:11 - understands the context of the continued
15:13 - conversation that's it for now really
15:15 - enjoying teaching this to you all thank
15:17 - you so much please give me any comments
15:18 - or feedback questions in the comments
15:21 - below and we will see you all in the
15:23 - next video thanks a lot