00:00 - all righty y'all welcome back to another
00:02 - video and in this one we are going to be
00:05 - taking a look at how to add long-term
00:07 - memory to your llm
00:09 - chatbot now let me go ahead and give you
00:12 - an example of what the heck I'm talking
00:14 - about so let's say talk in the chat GPT
00:17 - and I wanted to let's say tell me a joke
00:21 - about
00:23 - pizza all right so let's see what it
00:25 - comes up with why did the pizza apply
00:28 - for a job because it on it to make some
00:30 - dough not bad okay so funny joke uh now
00:34 - let's pretend I you know take a step
00:37 - away from my computer go grab a cup of
00:39 - water come back and okay talking to Jet
00:41 - gbt again uh tell me that
00:46 - joke about pizza again man I need a good
00:50 - laugh sure wait what's this why didn't
00:53 - the slice cuz it couldn't okay that's a
00:56 - pretty funny joke but no I wanted the
00:59 - joke that you told me last time that was
01:01 - my favorite joke well uh let me see
01:06 - no the one from last time okay that must
01:10 - be able to clear it up what type of okay
01:13 - so clearly chat GPT or whatever I'm
01:16 - talking to right now it has no
01:18 - recollection of our previous
01:20 - conversation and honestly it makes me
01:22 - feel a little bit hurt inside because
01:24 - it's like yeah we had this whole
01:26 - relationship going and now you're
01:28 - pretending like you don't even know
01:29 - about our past and well honestly I just
01:32 - don't like feeling this way so let's
01:33 - figure out how we can add some long-term
01:36 - memory to ya give her a little bit
01:38 - improvement over chat GPT so how are we
01:42 - going to be doing this well we are going
01:44 - to be using this little thingy called
01:47 - embeddings so what are embeddings
01:50 - embeddings are how we can turn text into
01:55 - numbers okay that sounds interesting not
01:58 - sure why we would want to do that but
02:00 - let's go ahead and scroll down see if we
02:01 - can figure this out so first of all how
02:04 - do we get these magical embeddings well
02:07 - it looks like we use this function right
02:09 - here and we give it some text and then
02:13 - after that let's see what we get back
02:16 - huh okay so it will contain the
02:18 - embedding Vector along with some other
02:20 - data and this embedding Vector it just
02:23 - looks like a big array of numbers okay
02:28 - so indeed these embeddings are a
02:32 - numerical
02:33 - representation of this
02:36 - text interesting but then the question
02:39 - Still Remains why do we want a big
02:41 - number that represents some text well
02:45 - let's put that thought on the back
02:46 - burner for just a second and hop over
02:49 - and talk about space for just a little
02:52 - bit so here what we're looking at is
02:56 - onedimensional space we only have one
02:58 - axis the x- axis
03:00 - and if I were to say something like
03:04 - let's find something on at location 8
03:07 - let's say uh let me get another shape
03:09 - here so we can have something to draw
03:12 - with all right just this red I'll make
03:15 - this uh a red circle it can be my little
03:17 - pointer I guess but let's say we wanted
03:20 - to uh like just signify something at
03:23 - location eight so this is zero one two 3
03:27 - four five six 7 eight okay
03:30 - so in onedimensional space eight you
03:34 - just count over eight spots so let's you
03:37 - know make this a little bit fancier and
03:40 - now let's go to two-dimensional space
03:43 - well now I can give you two coordinates
03:45 - like location 48 so what we do is count
03:48 - over four and then we would count up
03:50 - eight so we have two axises or two axes
03:54 - two coordinates okay simple enough same
03:58 - thing with three dimension iions so if
04:01 - we have three dimensions I could give
04:02 - you a three coordinate system an X
04:04 - location A Y location and a z location
04:08 - now honestly anything more than three
04:11 - dimensions it's kind of hard for me to
04:14 - comprehend however thankfully for
04:16 - computers it's not so what does all this
04:21 - have to do with vectors well this Vector
04:25 - representation right here believe it or
04:28 - not they're a lot more than these four
04:30 - numbers in this embedding there are
04:33 - like, 1500 right around there I think
04:34 - it's like 15,000 or
04:37 - 1567 whatever this actually maps to and
04:41 - I know this terminology is like not
04:43 - exactly right but just to uh kind of lay
04:46 - down the concepts this maps to a point
04:49 - in like 1500 dimensional space so
04:53 - basically whenever you have a piece of
04:56 - text right here at very high dimensional
04:59 - space that can be converted to a point
05:02 - that's out there somewhere uh actually
05:05 - let's say that some text is converted
05:11 - to this point in whatever dimensional
05:15 - space and we'll say another bit of text
05:17 - is converted to this point in whatever
05:20 - dimensional space all right that's
05:22 - interesting so I guess we're getting
05:24 - somewhere we know that different text
05:27 - kind of aligns with different points in
05:29 - space but I still don't see how this is
05:32 - going to help us with that long-term
05:34 - memory Pizza problem we were talking
05:37 - about earlier so check this out let me
05:39 - just go ahead and get rid of this and
05:41 - now that we had our little side quest
05:44 - talking about space let's go ahead and
05:46 - take a look at a more realistic example
05:49 - of what's going to
05:50 - happen so in this example instead
05:54 - of this text right here I am going to be
05:58 - converting
05:59 - these little messages to vectors so
06:03 - let's just go ahead and take this one
06:05 - real quick that says I love pizza and
06:08 - we'll kind of just start mapping this
06:10 - out whoa it's pretty big okay so let me
06:14 - zoom out a little bit and we'll say that
06:16 - this text right here I love pizza would
06:18 - you say that this ended up here in space
06:21 - again it doesn't really matter exactly
06:23 - where like up down left right I just
06:25 - want to show you something real quick so
06:27 - we'll say that this text I love pizza is
06:29 - in space right here now let's get
06:32 - another one of my comments I need to
06:34 - work out more and let's just make
06:37 - another shape for this so this is I need
06:40 - to work out more now the thing about
06:43 - Vector embeddings which make them very
06:45 - interesting is this if two pieces of
06:49 - text are similar they're going to end up
06:52 - close to each other in this whatever
06:54 - dimensional space now if these two
06:56 - pieces of text are not similar at all
06:59 - they don't have any concepts any words
07:00 - related they're going to be far apart so
07:04 - where would I really love pizza and I
07:06 - need to work out more be in relation to
07:09 - each other my guess is that they're
07:11 - going to be pretty dang far apart so
07:14 - we'll put them like this far apart so
07:16 - let's keep going through our examples
07:18 - and do this one I'm hungry a big cheese
07:21 - pizza sounds great okay so let me go
07:24 - ahead and add a little shape for
07:26 - this and where would this be I'm hungry
07:29 - hungry a big cheese pizza sounds great
07:32 - so this is pretty closely related to
07:35 - Pizza doesn't really have anything to do
07:38 - with working out so I would put it like
07:40 - right around here I would say and we can
07:43 - just do one more for fun does anyone
07:45 - know a good book about Bitcoin so okay
07:49 - let me just make the shape for
07:51 - this um has nothing to do with pizza
07:55 - nothing to do with working out really so
07:57 - maybe it would just be like out here
08:00 - somewhere okay so right now if I was
08:05 - storing these embeddings or these
08:08 - vectors then it would look something
08:11 - like this okay interesting but still not
08:17 - quite sure how this helps us with the
08:18 - pizza problem well check this out now
08:21 - things are going to start coming
08:22 - together let's say that we have a chat
08:24 - bot and all of our previous messages are
08:28 - already indexed they're already stored
08:29 - and we'll take a look at exactly how to
08:31 - do that in just a bit but they're stored
08:33 - in our data store just like we're seeing
08:35 - right here so now we got a new message
08:37 - that comes in and it says I'm hungry
08:40 - what kind of pizza should I eat so
08:43 - imagine here we come in and we say I'm
08:46 - hungry what kind of pizza should I eat
08:49 - right now chat PT or ya it doesn't have
08:52 - any context about our past it doesn't
08:54 - know if I talked to it before if I
08:56 - talked to it a million times before and
08:58 - if so what we talked about it basically
09:01 - has no idea of any memory of me and each
09:04 - message is like a brand new moment to it
09:08 - but check this out if I take this and
09:12 - I'm going to convert it to another shape
09:14 - because what we're going to be doing
09:15 - with this message the new one that comes
09:18 - in is we're also be going to be
09:20 - converting it to an embedding Vector
09:22 - just like we have been and then let me
09:25 - just give this a different outline so
09:26 - you guys can see what's going on just
09:28 - outline it uh
09:30 - green or
09:32 - something so this new message that comes
09:36 - in once it gets converted to a vector
09:38 - it's going to be placed somewhere in
09:42 - space so then what we can do is we can
09:45 - say okay this is where you are what we
09:48 - want to do from here is we want to go
09:51 - ahead and get the I'll just say x
09:54 - nearest messages and I say x because it
09:58 - could be like the two near nearest ones
10:00 - the 10 nearest ones this depends on how
10:02 - you want to configure it but let's just
10:04 - say we want to configure it to also get
10:06 - the two nearest messages so that's going
10:09 - to be this one and this one so this is
10:13 - going to be your original query that
10:15 - comes in and these are going to be the
10:18 - two little bits of information that
10:20 - found that is close to this message so
10:23 - what we can do from here is we can take
10:25 - all of these messages and we can include
10:28 - it in the prompt
10:29 - and then when we say I'm kind of hungry
10:32 - what kind of pizza should I eat it's
10:34 - going to know that one I really love
10:36 - pizza and two I particularly like cheese
10:40 - pizza right here so with that the desire
10:46 - is with this additional information
10:48 - these additional memories we can get a
10:50 - better response from the llm so sounds
10:53 - like a pretty cool concept now how do we
10:55 - actually do this well believe it or not
10:57 - you cannot store these embedded vectors
11:00 - on Lucid chart you have to use a another
11:03 - data store and one of the most popular
11:05 - ones is pine cone so with pine cone what
11:09 - you do is you first create an index now
11:13 - an index is basically just like a big
11:15 - bucket or data store that you can use to
11:18 - store
11:19 - vectors and then once you have your
11:21 - index you can pretty much just store
11:23 - vectors and it's pretty simple actually
11:25 - it's very simple actually so let me go
11:28 - ahead I already wrot a little bit of
11:29 - source code and I'll talk you through
11:31 - this so this little example right here
11:34 - is going to be using open Ai and pine
11:37 - cone again open AI is what we're going
11:40 - to be using to generate these
11:43 - vectors and then we're going to be
11:45 - storing them in Pine Cone basically our
11:47 - database so that's why I configured
11:50 - everything right here and now here what
11:53 - I'm doing is I'm just creating that
11:54 - index I already have it created as you
11:57 - guys can see I just named it post since
11:59 - it's going to store post
12:01 - vectors and in this example since I
12:03 - already have it created I just commented
12:05 - this out so you know I don't want to
12:07 - create it again it's already created but
12:09 - then what I'm going to do and again this
12:12 - would probably be coming from an actual
12:14 - database but because I didn't want to
12:16 - make this uh example too complicated I
12:18 - just made a quick dict and then I just
12:21 - gave it 10 random
12:24 - messages uh the example that I'm going
12:27 - to be using eventually is I'm hungry
12:29 - what kind of pizza should I eat same
12:30 - thing as we saw in Lucid chart and I
12:33 - have two messages about pizza right here
12:35 - I really love pizza and I'm hungry a Big
12:38 - Cheese Pizza I think that's the other
12:39 - one uh yeah so hopefully if everything
12:43 - works we should be getting these two
12:45 - messages back but either way continuing
12:48 - on what we're going to do next is I'm
12:51 - just looping through this messages
12:54 - dictionary and for each of those
12:56 - messages or posts I probably should have
12:58 - called
13:00 - um what I'm going to do is I'm pretty
13:01 - much just going to generate a vector
13:03 - from
13:04 - it right here and then once I have all
13:08 - those vectors or embedded embedding
13:11 - vectors generated I'm just going to
13:13 - insert them all into my post Index right
13:18 - here and that's how you see you have 10
13:22 - matches or 10 posts in here because I
13:24 - had uh 10 matches and another thing that
13:28 - I want to point out before I go any
13:30 - further we'll get back to that code
13:32 - example in just a bit is you see
13:34 - whenever I create this index and maybe
13:36 - you didn't see that I skimmed over this
13:39 - but whenever I create this index I need
13:41 - to explicitly say how big are the
13:45 - vectors that I'm going to be storing in
13:47 - it because each Vector has a set size
13:51 - whenever I use this function from open
13:53 - AI it generates vectors in this size
13:56 - right here 1 53 36 now in this index I
14:02 - can only store vectors with that size
14:05 - and that's because of how the math works
14:07 - you can't compare a vector let's say of
14:11 - a length of three to a vector of a
14:13 - length of eight whenever you're
14:14 - comparing vectors and getting the
14:16 - distance between them to find out how
14:18 - similar the text is the vectors need to
14:20 - be the exact same size so I just want to
14:23 - point that out if you ever like come
14:24 - across another function that can get you
14:26 - a vector and it's not compatible just uh
14:29 - yeah something to watch out for but
14:31 - anyways heading back to this example now
14:34 - so what I did is I basically inserted
14:39 - converted all my posts to vectors and
14:41 - then I inserted them into my Vector
14:43 - Store Pine Cone and now with this this
14:48 - code right here is basically simulating
14:50 - what would happen whenever a new chat
14:53 - message came in so I'm a user and I just
14:56 - typed I'm hungry what kind of pizza
14:58 - should I eat
14:59 - now in order to find the most similar
15:01 - post related to this I first need to
15:04 - conver what the heck was that noise
15:06 - conver I need to convert this to an
15:11 - embedding now once I have that I can
15:14 - pretty much use this query right here to
15:17 - say okay from this Vector that I just
15:20 - created basically a numerical
15:21 - representation of this string right here
15:24 - get me the two nearest vectors and then
15:28 - it actually doesn't return the string it
15:31 - Returns the
15:32 - ID one of these numbers so from the ID
15:36 - all I have to do is pretty much just uh
15:38 - get the related messages from it but if
15:41 - all this works then what we should see
15:44 - is actually let's run this right here
15:47 - let me bump this up a little bit so you
15:49 - guys can see all right look at this so
15:53 - it printed out the two most closely
15:56 - related posts and it was this one it was
16:00 - the closest one I am hungry a big big
16:03 - cheese pizza sounds great and this one
16:06 - right here I really love
16:08 - pizza so there you go what you would do
16:12 - from here is of course take this
16:14 - original question and these two other
16:18 - related ones and throw this into your
16:20 - prompt template and then hopefully the
16:23 - llm can come back with a much more
16:25 - tailored accurate memorable response so
16:29 - yeah that is it for this tutorial I'm
16:32 - not exactly sure what we're going to be
16:34 - covering in the next one but I'm sure
16:36 - that is going to be awesome so I will
16:37 - see you then