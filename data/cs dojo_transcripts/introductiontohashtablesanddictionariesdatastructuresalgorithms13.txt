00:00 - hey everyone in this video i'm going to
00:02 - give you an introduction to
00:03 - hash tables and dictionaries first of
00:06 - all let me explain what a dictionary is
00:08 - and then i'm going to explain how it can
00:10 - be implemented using a hash table
00:13 - so here's an example of a dictionary as
00:16 - you might guess this is
00:17 - essentially a table that shows different
00:20 - people's age
00:22 - and you can ask this table or this
00:24 - dictionary how old is
00:25 - paul for example then you get the number
00:27 - 29 right away
00:29 - or you can ask it how old is chloe and
00:32 - you get 88
00:34 - and that operation can be called search
00:37 - because you're looking for a specific
00:40 - key
00:40 - for example paul to find the
00:44 - corresponding
00:45 - value uh in that case 29
00:48 - and these pairs are often called key
00:50 - value pairs by the way
00:52 - so a dictionary is basically a
00:54 - collection of these key value pairs
00:56 - or a data structure that can store these
00:58 - key value pairs
01:00 - so that you can retrieve the value of
01:02 - any of those keys quickly
01:03 - and in additionally you can define a few
01:06 - more operations
01:08 - one is insert and that would be adding a
01:12 - new entry to this
01:13 - table by saying bob is eight for example
01:18 - another one is delete
01:21 - and that would be dealing an existing
01:23 - entry for example you might say well i
01:25 - don't want
01:26 - this data about chloe anymore
01:29 - when you implement a dictionary you
01:31 - should be able to implement it
01:33 - ideally so that all of these operations
01:36 - take only one in time on average
01:40 - and a hash table is a good way to do
01:42 - that
01:43 - to build a hash table the first thing
01:45 - you'll need is
01:46 - an array so here i have an array of
01:50 - eight elements just as an example
01:53 - and let's say that we want to use the
01:55 - array or the
01:56 - hash table to represent the dictionary
01:58 - that we saw earlier this one
02:00 - to do that we're going to put each key
02:03 - value pair
02:04 - in one of these slots but to do that
02:08 - we need a way to decide which key value
02:11 - pair
02:12 - is going to go into which slot of this
02:14 - array
02:15 - one way to do that would be to look at
02:18 - the first
02:19 - letter the first character of each key
02:22 - and compare it to the letter a
02:26 - and compute how many characters away it
02:29 - is
02:30 - from the letter a so for example for
02:32 - this key
02:33 - paul you can look at the first letter p
02:36 - convert it to the lowercase p and
02:39 - compare it to
02:40 - the lowercase a and in ascii code
02:43 - you'll be able to see that p is 15
02:46 - characters away
02:47 - from a but 15 wouldn't be an index of
02:50 - this array because that would be out of
02:52 - range
02:53 - so you would need to use for example the
02:56 - model operator
02:58 - mod 8 mod of the length of the array
03:01 - to get the desired range that would be 0
03:04 - to
03:04 - 7 inclusive and with that method you
03:07 - would get
03:09 - 7. so at that point you can put
03:12 - this key value pair paul 29 over here
03:16 - at index 7. and just like that we can
03:19 - decide
03:20 - which index of the array we want to use
03:22 - for each
03:23 - key value pair so jane
03:26 - would be over here at index one chloe
03:29 - would be over here
03:30 - and alex would be
03:34 - over here and actually what i showed you
03:36 - here
03:37 - is already a hash table so basically to
03:40 - construct a hash table
03:42 - you need an array and the dictionary you
03:45 - want to represent
03:46 - and a way to decide which index of the
03:48 - array you want to use
03:49 - for each key value pair another way to
03:52 - describe the same thing
03:53 - would be to say we need a function that
03:56 - turns
03:57 - each of these keys whether they're
03:59 - strings or anything else
04:01 - into an index of this array
04:04 - that we constructed and we could call
04:07 - that function
04:08 - for example h1 and with the method that
04:11 - i just showed you
04:12 - h1 of paw would be 7. and this function
04:16 - is usually called a hash function and
04:18 - that's why this whole thing is called a
04:20 - hash table
04:21 - but this particular hash function that i
04:23 - just showed you
04:24 - might not be ideal for a few reasons one
04:27 - of them
04:28 - is this if you consider english names
04:31 - there might be a lot of names that start
04:33 - with j
04:35 - and if you put for example josh
04:39 - in this dictionary or this hash table it
04:41 - would try to
04:42 - go into the same bucket as jane and the
04:45 - same thing with
04:46 - jennifer and that would be called a
04:49 - collision
04:50 - when multiple keys would try to go into
04:52 - the same
04:53 - spot of this array and there is a way to
04:56 - deal with collisions and we're going to
04:58 - talk about those
04:59 - but for now you should know that we want
05:01 - to avoid
05:02 - collisions as much as possible to keep
05:05 - your
05:06 - hash table efficient and so one way to
05:09 - deal with a problem like that would be
05:11 - to consider
05:12 - most of the letters or many of the
05:14 - letters in the given key
05:15 - if not all of the letters and one such
05:18 - function
05:19 - is called djb2 and i'm going to put a
05:23 - link to some information about that in
05:25 - the description just in case you're
05:26 - curious about it
05:27 - anyway when you're choosing a hash
05:29 - function for your hash table
05:31 - there are a few things that you should
05:33 - consider one is that it should be
05:35 - fast to compute and the other one is
05:38 - that
05:39 - is to try to avoid collisions as much as
05:41 - possible
05:42 - and that's pretty much it when it comes
05:44 - to the criteria
05:46 - in some textbooks they might say your
05:49 - hash functions should be uniformly
05:50 - distributed
05:51 - or random looking or something like that
05:54 - but
05:55 - it's really not necessary for practical
05:57 - purposes
05:58 - and it's not necessarily better than
06:01 - non-uniformly distributed
06:03 - functions so if you're choosing a hash
06:05 - function for your hash table
06:07 - you should really only consider these
06:09 - two criteria
06:11 - and when you're choosing a hash function
06:13 - for security purposes
06:14 - you might have other concerns but here
06:16 - we're only talking about a hash function
06:19 - for a hash table
06:20 - okay let's now talk about how to do with
06:24 - collisions we're going to talk about two
06:26 - families of methods for dealing with
06:28 - collisions in this video
06:30 - and the first one is called chaining
06:33 - with this method instead of storing the
06:36 - key value pairs
06:37 - directly in the array we're going to
06:41 - store them in a linked list and from
06:44 - each element of the array we're going to
06:46 - have a pointer
06:48 - to that linked list and that linked list
06:51 - is going to contain
06:52 - all the key value pairs that were
06:54 - assigned to that particular
06:56 - slot in the array so for example
07:00 - if you have another key value pair that
07:02 - was assigned
07:03 - to this same slot
07:06 - then what we'll need to do is we'll need
07:08 - to
07:09 - put the new key value pair at the
07:12 - beginning
07:13 - or at the top of this linked list
07:16 - just like that and
07:20 - if you have another key value pair that
07:23 - was assigned
07:24 - to an empty slot then we'll need to
07:26 - create a new
07:27 - linked list containing this single
07:29 - element
07:31 - and then have a pointer that points
07:34 - to the new linked list from that slot
07:37 - and with chaining insertion
07:40 - only takes oh one in time or constant
07:44 - amount of time
07:45 - and what about search
07:48 - well to explain that i'll need to first
07:51 - define a few variables
07:54 - and here is the number of elements that
07:57 - we
07:58 - have put in so far in this hash table
08:01 - and m is the length of the array
08:05 - so this alpha which is n over m is going
08:08 - to show how full
08:09 - this hash table is so right now because
08:12 - n is 4 and m is 8 alpha is exactly a
08:16 - half
08:17 - and with this you can show that
08:20 - search only takes o 1 plus alpha
08:23 - in time what this means is that if you
08:27 - keep
08:27 - alpha below a certain number below let's
08:31 - say 1
08:32 - search would only take a constant amount
08:34 - of time and this o
08:36 - 1 plus alpha is the average time
08:40 - and here you might say what if i don't
08:42 - want to use this
08:43 - extra data structure outside of this
08:45 - array
08:47 - then the approach you might want to use
08:49 - is called
08:50 - open addressing and there are a few
08:52 - different flavors for it
08:54 - i'm going to explain the simplest one
08:56 - first which is
08:58 - linear probing with linear probing or
09:01 - with open addressing in general we store
09:04 - all the key value pairs within the array
09:06 - itself just like you can see here
09:10 - let's say here that we have another key
09:12 - value pair
09:14 - that collides with this one then with
09:18 - linear probing
09:18 - all we need to do is we'll need to check
09:21 - the element
09:22 - that's directly to the right of the
09:24 - collision
09:25 - and if it's empty we can just put it
09:27 - there
09:28 - and if another element collides with
09:32 - this one again we'll need to check
09:35 - this element and then this element next
09:38 - until we find an empty element and then
09:40 - we can put it in there
09:42 - so just like that if this new q
09:46 - value pair collides with this one we'll
09:48 - need to keep checking the elements
09:50 - to the right until we find an empty one
09:54 - so i would say linear probing is an okay
09:57 - approach
09:58 - but it could be inefficient when you
10:00 - have a lot of elements
10:02 - and that's because these elements are
10:04 - likely to
10:06 - start forming clusters when you have a
10:07 - lot of them so for example you have
10:11 - a cluster of five elements here and when
10:14 - you have a cluster of five elements or
10:15 - maybe a lot more elements it would take
10:18 - you know extra time to go through all of
10:20 - them
10:21 - and to find an empty spot
10:24 - and one way to solve that issue is
10:28 - called double hashing so let me explain
10:30 - how double hashing works
10:33 - let's say that this
10:36 - key value pair happens to collide with
10:39 - this one then what we're going to do is
10:42 - similar to linear probing
10:44 - in a way that we're going to jump ahead
10:46 - and check other elements to see if
10:47 - they're empty
10:48 - but instead of jumping ahead by one
10:51 - element
10:53 - we're going to pick a number here let's
10:55 - say 3
10:57 - to determine how many elements we want
10:59 - to check ahead
11:01 - so if we pick three here we're going to
11:03 - check
11:04 - one two three this element
11:07 - the third element and we're gonna check
11:10 - every third element
11:11 - ahead of that so since this is empty
11:13 - we're going to put it here
11:15 - but if another pair
11:18 - collides with this one and if we happen
11:21 - to pick
11:22 - three again we're gonna check the third
11:25 - element
11:26 - and then we're going to jump ahead by
11:28 - three elements again so that would be
11:30 - one
11:30 - two it would be this almond but since
11:34 - it doesn't exist we're gonna jump back
11:36 - here
11:37 - and the nice thing about double hashing
11:40 - is that every time we have a collision
11:42 - uh depending on the key or depending on
11:44 - the starting point
11:46 - we're going to produce a slightly
11:48 - different
11:49 - sequence every time the sequence of the
11:52 - elements that we're going to check
11:54 - so let's say that this new pair collides
11:57 - with this one
11:59 - we might pick uh one
12:02 - for the number of elements that we're
12:04 - going to drop ahead if we pick one
12:06 - we're going to just go to this element
12:08 - and find that this is
12:10 - empty so we don't necessarily jump
12:14 - from here to here and have another
12:17 - equation
12:19 - and that's why we're less likely to have
12:21 - clusters
12:22 - in double hashing and that's why it can
12:25 - be more efficient than linear probing
12:27 - to summarize this we first
12:31 - pick our initial index for the given key
12:35 - with a hash function h1 with the model
12:39 - operator
12:40 - the length of the array in this
12:42 - particular case 8
12:44 - and then the next index that we're going
12:47 - to check is going to be
12:48 - the original index plus c
12:51 - the number that we're going to pick for
12:53 - the particular key
12:56 - mod 8. and the next one after that is
12:59 - going to be i plus 2c
13:00 - mod 8 and so on and here i think the
13:03 - natural question would be
13:05 - how do we pick this number c well one
13:08 - condition
13:08 - that we need to satisfy is that
13:12 - gcd of c and m or the greatest common
13:15 - divisor or the greatest common factor of
13:17 - c
13:17 - and m should be 1 and m
13:20 - is the length of the array here and
13:22 - that's because by
13:24 - satisfying this condition we can make
13:26 - sure that
13:27 - this sequence of indices
13:30 - will eventually cover the entire array
13:34 - and one convenient way to make sure
13:36 - that's true
13:37 - is to always set m the length of the
13:40 - array to be a prime number
13:42 - and c to be a positive integer and that
13:45 - way
13:45 - gcd of c and m will automatically be 1.
13:49 - okay so how do we pick c uh here's
13:52 - one way of picking it assuming that m is
13:56 - a prime number
13:57 - we're going to use a second hash
13:59 - function which we're going to call
14:01 - h2 and then we're going to put a key
14:04 - into that function and then do some
14:07 - operations here
14:08 - so let me explain what we're doing here
14:10 - here we're applying the
14:12 - mod operation with m minus 1
14:16 - to the result of the hash function
14:19 - and that way the range of
14:23 - the results that we can get from this
14:26 - whole
14:27 - expression is going to be
14:30 - 0 to n minus 2
14:33 - inclusive and by adding 1
14:37 - to that result we're going to get the
14:39 - range
14:40 - 1 to n minus 1
14:44 - inclusive and that's the range that we
14:46 - want
14:47 - and here the natural question after that
14:50 - is
14:51 - how do we pick h2 for that i ran
14:54 - an experiment and i tried a few
14:56 - different approaches
14:58 - so here's uh the first approach i tried
15:02 - we have h1 the original hash function
15:05 - and to make h2 i simply appended
15:09 - a letter which i picked it could be
15:10 - anything but i picked d
15:12 - here to the key so if the key
15:15 - is jane i just put jane
15:18 - d to h1 and then i used that
15:21 - as h2 and it actually seemed to perform
15:26 - pretty well and by the way the h1 i used
15:30 - for this one is the default hash
15:32 - function
15:33 - of python which seems to be based on dj
15:36 - b2
15:37 - which i mentioned earlier and the second
15:39 - approach i tried
15:41 - is simply this so i used exactly the
15:44 - same
15:45 - hash function as the original one as the
15:47 - second hash function
15:49 - and so much surprisingly it performed as
15:52 - well as
15:53 - the first approach but i would say if
15:55 - you want to try implementing double
15:57 - hashing
15:58 - yourself you should try a few different
16:00 - hash functions
16:02 - because the performance probably depends
16:04 - on your particular
16:05 - environment and the particular kind of
16:07 - input data that you get
16:09 - anyway with double hashing you can show
16:12 - that with
16:12 - a few assumptions that to complete
16:16 - either the search operation or the
16:18 - insertion operation
16:20 - you need to check almost this
16:23 - number of elements on average
16:27 - that's 1 over 1 minus alpha where alpha
16:30 - is n over m so again n is the number of
16:34 - elements that we've put in so far in the
16:36 - hash table and
16:37 - m is the length of the array so just
16:40 - like i said before
16:41 - alpha shows how full your hash table is
16:46 - so let's say that alpha is two thirds
16:50 - then this expression one over one minus
16:53 - alpha
16:53 - becomes three so that means that
16:56 - to complete search or insertion you need
16:59 - to check
17:00 - at most three elements on average
17:04 - so basically if you keep alpha below a
17:06 - certain number
17:08 - let's say two thirds again you'll be
17:10 - able to complete
17:11 - search or insertion in
17:14 - constant time so what i would suggest if
17:17 - you're implementing
17:18 - double hashing by yourself well at least
17:20 - one way
17:21 - to do that would be to pick m to be a
17:25 - prime number
17:26 - let's say 7 or 701
17:30 - and then as soon as alpha becomes uh
17:33 - greater than two-thirds resize the array
17:36 - pick a larger prime number than the
17:39 - original m
17:41 - and then transfer all the elements to
17:44 - the new array
17:46 - and that way resizing the array takes
17:49 - extra time but at least for search and
17:52 - insertion
17:54 - it's only going to take a constant
17:56 - amount of time as long as alpha
17:58 - stays low enough okay so that's my
18:02 - introduction to hash tables and
18:04 - dictionaries but there are a few things
18:06 - i wanted to mention before i go
18:08 - one is that there is a coding interview
18:09 - problem that i covered a while ago on
18:11 - this channel
18:12 - and for that problem you can actually
18:14 - use one of these concepts to
18:15 - solve it so i'll put a link to that
18:17 - video in the description below just in
18:19 - case you want to watch it
18:21 - the other one is that i tried
18:23 - implementing a hash table
18:24 - in python so i'm going to put a link to
18:27 - that code in the description below as
18:28 - well
18:29 - and from that code you'll be able to see
18:31 - how i ran the experiment i mentioned
18:33 - earlier too
18:34 - anyway thank you as always for watching
18:36 - my videos and i'll see you guys in the
18:38 - next one