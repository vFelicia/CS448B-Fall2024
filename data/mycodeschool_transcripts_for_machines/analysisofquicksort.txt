In our previous lesson, we learnt about Quicksort algorithm. Now in this lesson, we will analyze quick sort. We will first look at some properties of quick sort and then we will go ahead and try to analyze its time and space complexity, so maybe try to see how efficient it is in terms of running time and space or memory requirement. Okay! so let's get started. The properties of Quick sort that I want to talk about are First of all Quick sort just like Merge sort is a divide and conquer algorithm. Divide and Conquer is an algorithm design paradigm, in which we break a problem into subproblems and then from solutions to the subproblems problems we construct the solution to actual problem. Okay, next property of Quick sort is that its a recursive algorithm. In programming as we know recursion is a function calling itself. Recursion is a natural choice in Divide and Conquer strategy. A Divide and Conquer algorithm is mostly implemented using the recursion, moving on next property is that Quick sort is not a stable sorting algorithm. We have discussed what a stable sorting algorithm is in our previous lessons. In a stable sorting algorithm the relative order of records with equal key is preserved. When we learn a sorting algorithm, we sort a list of integers but a sorting algorithm can be used to sort any list of any data type. In case of a complex data types, we sort on some property of the records on some key. For example let's say we want to sort a list of points in Cartesian plane. Each record in the list here is a pair of integers, first integer is Xcoordinate and second integer is Ycoordinate, and let's say if you want to sort in increasing order of xcoordinate. We have two records here with equal Xcoordinate. If we will use are stable sorting algorithm then point (4,5) which is before point (4,3) in the original list will be before (4,3) in sorted arrangement also but this is not guaranteed if we will use an unstable sorting algorithm. In the second arrangement that I'm showing here, Elements are sorted in increasing order of Xcoordinate but (4,3) is coming before (4,5). Partitioning logic in Quicksort algorithm does not ensure stability. Okay! Let's now talk about time complexity of Quicksort. Time complexity of Quicksort is O(n logn) in best or average case and its O(n^2) in worstcase, but the worst case can almost always be avoided by using what we call randomized version of Quicksort let's now see how quick sort is O(n logn) in best or average case and O(n^2) in worstcase. This is psedocode for a quick sort function that he had written earlier. Let's try to calculate the running time, the first statement here is a this if condition its a simple statement the cost of executing this statement will be some constant let's say the cost of executing this C1 if we will go inside the if condition and then the have this call to partition function. Now this is not a simple statement we first need to figure out what will be the running time of partition function. This is the partition function that we had written in our previous lesson if you can see there are some simple statements here that will take constant time, these four statements are simple statements together they will take some constant time apart from these statements we have one forloop the statements inside the for loop once again are simple statements and will together take some constant time, let's say these statements inside the for loop will take some constant time A and the statements 1 2 3 and 4 let's say will cost us B, so time taken will be A(N)+B. Let's say and this constant A is also taking care of this statement where we are incrementing I for the loop on this particular line will actually this particular statement on the forloop will run one extra time actually but we ignore such small costs while calculating running time because we are interested in a calculating the rate of growth for very high values of N, okay so time expression here is A(N)+B where A and B are constant, so in other words this is O(N). Remember, N here is the lenght of subarray that we are partitioning, okay coming back to our Quicksort function. The partition function here is going to cost us O(N). We can say that the cost for executing this statement will be some constant times N plus we can add some constant here but it will not matter because for very high values of N, the added constant term will be negligible compared to this A*N term. We have an assignment here to this variable Pindex that once again will be some constant time. So now let's say we are taking B' constant time, so overall cost of executing this statement is A*N + B' and now we have two recursive calls, let's see what will be the cost of these two recursive calls. In partition function, we choose a element as Pivot in our partition function we are always choosing the last element in the segment of array, pass to quick sort or element at end index as pivot. I'm drawing an array of integers here and let's say the whole array is passed to the Quicksort function. So start is 0 and end is 7. Four will be picked as pivot and we will rearrange the array such that all the elements lesser than the pivot will lie towards its left and all the elements greater than the pivot will lie towards its right and now after the partition we can make two recursive calls one for the segment of array to the left of pivot and another for the segment of array to the right of pivot. In this recursive approach, A balanced partitioning will be best case for us. In balanced partitioning both subarrays towards the left of pivot and towards the right of pivot will have length equal to or almost equal to N/2 where N is the number of elements in original array, so if time taken for this quick sort funtion is let's see T(N) then both these quicksort calls in bestcase partitioning will take time T(N/2) each. So T(N) in all will be 2T(N/2) plus we will add A*N+B' and C1, remember this is the best case scenario for us. When we are seeing bestcase what we mean is all the partitions in all that recusive calls will be balanced, all the partitions will break a segmented into 2 sub segments of equal length. Here in this recurrence relation T(N)=T(N/2)+(A*N)+B'+C1 these two constant terms B' and C1 will be negligible compared to A*N and for very high values of N and when we analyze time complexity, we always look at running time for very high values of N so they can safely ignore B' and C1 and instead of A I'll write C for constant because C looks good When I'm saying constant, so time taken in the best case is equal to 2T(N/2)+C*N if N > 1 for N = 1, we will not go inside this if condition only cost will be execution of this ifs statement so I'm saying that for N=1 my costs is C1 so T(1) is C1. This recurrence relation that we're getting here for time is the same that we had got for Mergesort. We can solve this recurrence and express T(N) in terms of T(1) which is known to us T(N) is 2T(N/2)+C*N. We can write T(N/2) as 2T(N/4)+C*(N/2) and we will have C*N outside. This overall will be 4T(N/4) + 2C*N and now we can write T(N/4) in terms of T(N/8) like this this'll be 8T(N/8) + 3C*N. In terms of some generic K we can reduce this expression like 2^K*T(N/(2^K)) and this will happen if we will go reducing using k steps. In fourth step, after this step when K be equal to 4 this will be 16*T(N/16)+ 4*C*N, I'm saying this is the first step this is the second step this is to third step and says the Kth step, This is Kth reduction, now we know T(1) and we want to express T(N) in terms of T(1) so in that case in N/(2^k) = 1. If we will solve this than K= log n the base to 2. So here T(N) can be written as 2^(log n the base 2) * T(1) + C*N * log N to to the base 2, 2^(log n the base 2) will equal to 1 T(1) is C1 oops! sorry and 2^(log n to the base 2) will be N T(1) is C1 this will be C*N *log n to the base 2 and if base is understood and I can read write that as log n so this is what I'm getting. This expression is O(N logN) N*C1 one is lower order term here. For very high values of N it will be negligible compared to C*N*logN. For high values of and the rate of growth of time taken will be very close to C(N) logN, for some constant C so we can say that time complexity will be a O(N logN). Actually it would be better to say Ɵ(N logN). If you're not getting any of this then we have whole series on time complexity analysis you can find a link to it in a description of this video. Ɵ(thetha) notation is actually better metric. Anyway, in best case, we are O(N logN) or Ɵ(N logN). Now let's try to analyze the worstcase. Worst case for us will be when we have totally unbalanced partitioning like for this array which is already sorted if we will pick the last element as Pivot. After partition, we will have only one segment. There will be no right segment One of the recursive calls for left subarray will cost us cost T(n1) if we are seeing that T(N) is the cost for array and other recursive call will simply return control will not go inside the If conditions so there will be some constant cost for the second recursive call if we will have unbalanced partitioning in all cases in all recursive calls and the recurrence relation to solve is this. T(N)= T(N1) + C*N where C is a constant I'm ignoring other constants that would add up they will be negligible compared to this term C*N for higher values of N ofcourse we also have a basecase T(1) which will be equal to C1 this T(N) is for all N > 1. In this recurrence relation its really simple to solve we can go and reducing T(N1) can be written in terms of T(N2) so it will be T(N2)+ C*(N1) plus ofcourse we will have C*N here so this will be T(N2) + 2C*(N)C and we can go on reducing, T(N2) can be written as T(N3) plus C*(N2) plus we will have the terms 2*(C*N)C so now this is T(N3) + 3(C*N)3*C. If I'll reduce this further this will be T(N4) + 4*C*N 6C. In terms of some K if we will go reducing K steps then this will T(NK) plus K*C*N minus (1+2+3+... all the way till K 1) * C. This part 1+2+3 till K1 will be K*(K1)/2. This is a simple arithmetic progression, so this is what we're getting, now if I want to write T(N) in terms 0f T(1) that a know, then in that case (NK) = 1 that will imply K will be equal to N. This expression will now be T(1) + K is now N and so this will be C*(N^2) minus N(N1)/2 * C. Some my T(N) and finally is T(1) is constant C1 plus I can right rest of the part as C*N and I'm going to take this out inside it will be N((N1)/2) and the part will reduced to (N+1)/2 overall this is what I'm getting. I'm getting are quadratic expression of the form A(N^2)+B(N)+C where A,B and C are some constants A here is C/2 and B here are C/2 and C is C1 this time expression is definitely O(N^2). To write in terms of Big O notation we will drop the law and order terms and the constant, O(N^2) is really bad but as I had said earlier worst case in quicksort can be avoided. In the partition function that I have written here I'm always choosing the last element in the segment as pivot with this strategy if my array is already sorted, I'm always getting unbalanced partitioning what we can do is we can have a strategy in which we can choose pivot randomly so during this whole recursive process, we will not have a rule that only the end element should be picked up as pivot any element randomly should be picked up as pivot. What I'm going to do is I'm going to write a function named randomized partition that once again is taking the Array and a start and end argument Now, in this function I'm first going to pick up pivot index between start and end, start and end included. By making a call to a function named random, this function will give me an index between start and end picked up randomly almost all the language library would have a random number generator function, now the element at this pivot index is my pivot. I'll first swap this element at pivot index with last element in the segment and then I can go a with rest of the logic that I have in the partition function. So I'll simply make a call to partition function in the left that i have written here. In my quicksort function now instead of calling partition directly I'll call randomized partition, now with this strategy probability of hitting worstcase is almost Zero. Coming back to quicksort function instead of calling partition here we are going to call a randomized partition, I'm short of space here so i'll write it like this this is randomized partition function it's always wise to called randomized partition. Now, let's try to analyze the average case. Once again in our basecase T(1) is C1, now what will be my T(N)? In average case will say that we can have all kind of split with equal probability. let's say our array is split such that pivot lies at index i the time taken for to the left half in this case will be T(I 1) because there will be (I 1) elements in left half and there will be NI elements in right half. This I can be any index between start and end with equal probability, so I'm going to say that T(N) will be average of all possible partitions or all possible values of I, apart from this average cost to the cost of partition is some constant times N so we have this recurrence relation to solve I'm writing it down here we are taking an average of all possible partitions here by summing up time taken in all possible splits and then dividing by N dividing the sum by N I'm not going to solve this one you can check the description of this video for a link that has all the maths. This will also evaluate this will also reduced to an expression that will be O(N log N). With randomized partition we can get average case running time of quicksort with very high probability and this makes quick sort really cool because unlike merge sort its space complexity is very less. let's now discuss space complexity of Quicksort. Space complexity as we know is the measure of rate of growth of extra space needed extra space means space or memory apart from the memory used to store the original array, the original list so space complexities is measure a rate of growth of extra space with input. I'm not going to derive this one for a quicksort you can check the description of this video for link to a lesson where we have described how the can derive space complexity in case of recursion. The space complexity of quicksort in average case is O(log N). In worstcase its O(N) but as we saw worstcase can always be a avoided now logN is such small rate of growth that that you can say that its almost negligible extra space requirement. We can say that quick sort in place sorting algorithm and in place sorting algorithm should take constant extra memory. Extra space requirement must not grow with input but but log N for all practical values of N is very small so we discount quicksort here. Okay I'll stop here now. In coming lesson, we will see some more sorting algorithms. This is it for this lesson. Thanks for watching.