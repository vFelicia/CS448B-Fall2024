In our previous lesson, we had explained merge sort algorithm. Now, in this lesson we are going to analyze merge sort algorithm. We will look at various properties of merge sort algorithm. We will try to see how efficient it is terms of time and space complexity. Some of the properties of merge sort algorithm that I want to talk about are, first of all merge sort falls in the class of algorithms that we call divide and conquer algorithms. In divide and conquer strategy, we break a problem into subproblems and then we first find out the solutions to subproblems and from solutions to subproblems we construct the solution of the actual problem. Second property of merge sort algorithm is that its a recursive algorithm. Programmatically, recursion is a function calling itself, but generally recursion is problem reducing itself in a selfsimilar manner. Merge sort is also a stable sorting algorithm. When we explain a sorting algorithm, we try to sort a list of integers in increasing order of value, but a sorting algorithm can be applied to a collection of any data type. When we are sorting complex records, we sort them on the basis of some property or some key. For example, lets say we want to sort points in Cartesian plane where each point is given as a pair of integers with x and y coordinates and we want to sort this list in increasing order of x coordinate. So, the key here is x coordinate. For this list, there can be 2 possible sorted arrangements. We have 2 records, 2 points that have equal x coordinate. So, in the sorted arrangement, either of these could come first. But, in a stable sorting algorithm, the relative order of records with the same key is preserved. So, if (2,5) is occurring before (2,3) in the original list, (2,5) must also occur before (2,3) in the sorted arrangement. Merge sort is a stable sorting algorithm. So, it preserves the relative order of records with same key. The next property that we want to talk about is merge sort is not an inplace sorting algorithm. An inplace sorting algorithm takes constant amount of extra memory to sort a list. Previous sorting algorithms like bubble sort, insertion sort and selection sort, all of them were using only some temporary variables to store indices, the extra space taken was not dependent upon the size of the list. But in merge sort, when we divide an array into sublists , two sublists left and right, we create two entirely new arrays. So, definitely the amount of extra space that we will take will be proportional to the size of the list, the number of elements in the list. The space complexity of merge sort is bigoh of n. In fact we should say theta n. Technically theta notation is what we should be using most of the time, but because bigoh notation is more famous, we use bigoh notation. What this means in simple terms is the memory or the space consumption is proportional to the number of elements in the list. the time complexity of merge sort algorithm is bigoh of n*logn in worst case. Once again, we should be using the theta notation to be more precise. if you are not aware of bigoh and theta notation, we have a series on time complexity analysis. You can find a link to it in the description of this video. Let us now try to deduce and understand time and space complexity of merge sort. Let us first try to analyze the time complexity. We have the pseudocode for merge sort algorithm here that we had written in our previous lesson. Lets say the time taken for an array of size n by merge site algorithm will be T(n). Let us now see how we can find an approximate expression for this T(n) . Some of the... talking about some of the fundamentals of time complexity analysis, we always try to approximate the worst case and we try to see the rate of growth of time taken for very high values of n and then we try to classify the time expression as bigoh or theta of some common function. Here in this algorithm, lets say, when we are executing for n greater than 1, all these statements, they are simple statements. Simple statements are statements that have simple operations like arithmetic or logical operations, assignments or comparisons and simple statements execute in constant time. So lets say these set of statements, these set of simple statements, in the worst case will take some constant time c1. If there are no loops or function calls and there are only simple statements, then the time taken will not be a function of the input size. Now, these two loops, these two assignments here will have same cost and these two loops together will run n times. So, we can say that they will together cost us some constant c2, lets say c2 is the cost of executing one of the statements, then the cost will be c2*n. Sometimes we directly talk in terms of the asymptotic notations only like we can directly say that this particular segment of the code is theta(n) in terms of time complexity. but lets talk like this that this will execute in c2*n where c2 is some constant and then we have this recursive call MergeSort for left. If we are saying that we will take T(n) time for MergeSort for an array of size n, the left will be an array of size n/2 . So, this will be T(n/2) cost for us. And then we have another merge sort call that will again be T(n/2). And then we have a call to Merge. Now for this one, we will have to look at the merge function, what is the complexity of Merge function. if you remember the Merge function, all Merge function was doing was it was picking up one element from either left or right at a time and writing it in another array. So, in all loops were running for total length of left subarray plus total length of right subarray. So, in all loops were running for n times. So, if you would analyze the running time for Merge function also, it will be of some form like some constant times n plus some other constant . i am not going into all the detailed calculations here. So, for n > 1, T(n) will be equal to 2*T(n/2) plus we can add all these costs up. It will be something like this. if n is less than 2, or in other words if n is equal to 1, that is the only case that we will have , when we will go and simply return. In this case there is only one simple comparison and we return. So, the cost will be some constant. We can simplify this equation further. Lets say this is equal to 2T(n/2)+ C'n + C'' Once again C' and C'' are some constants. For very high values of n , this constant term C'' will be negligible compared to C' * n and in time complexity analysis we are mostly worried about the rate of growth of function for very high values of n. So, for the sake of simplifying our calculation here, we can get rid of this plus c'' and we can say that T(n) = 2T(n/2) plus some constant time n and now we can try to solve this recurrence. T(n) = 2T(n/2) + c'n. Now, T(n/2) can also be written as 2T(n/4) +c' * (n/2) and of course we will have one more C'n outside here. And this will evaluate to 4T(n/4) + 2C'n. Once again T(n/4) can be written as 2T(n/8) + c'.(n/4) and we will have this 2c' outside. And this will evaluate to 8T(n/8) + 3c'n and we can go on reducing like this. We can write this as 16T(n/16) + 4c'n and if I have to write this in form of some generic k, this will be equal to 2^k*T(n/2^k) + Kc'n . Now we want to reduce this in terms of T(1) because that is what we know. In that case, n/2^k will be equal to 1 that will mean 2^k = n and k will be equal to log n to the base 2. So, if k is equal to log n to the base 2., we will have this expression in terms of T(1). So, finally we can write this as 2^logn to the base 2 times T(1) plus log n to the base 2 into c' into n. T(1) is equal to c. And 2 to the power log n to the base 2 will be equal to n. So, this will be equal to nC + C'*nlogn and now if I have to write this term in terms of bigoh notation, then what we typically do is we drop the lower order terms. n is a lower order term that n*logn. So, for very high values of n, we can ignore this part, this n*c and we can drop the constants from this part to say that this is equal to O(nlogn). In fact this will also be equal to Theta(nlogn). Theta of n log n here means that for some constants C1 and C2, T(n) which is equal to nc + C'*nlogn will be greater than or equal to c1*nlogn and less than or equal to c2*nlogn. if n is greater than or equal to some constant n0. And bigoh of nlogn means that T(n) will be less than or equal to some constant times nlogn, if n is greater than or equal to some constant n0. So, this function T(n) here is both Theta(nlogn) as well as bigoh of nlogn. When we say that Merge sort is theta(nlogn) in practical sense, we mean that the rate of growth of time taken by the algorithm for very high values of n is as close to the growth of function nlogn as possible. And by bigoh notation in practical sense we mean that the rate of growth will not be more than the rate of growth of nlogn. We will not go into all the details of why we safely ignore the lower order terms and constants and say that this is bigoh or theta of a particular function. its beyond the scope of this lesson. Now, let us try to understand the space complexity requirement for merge sort algorithm. I'll take this example once again that we had used previously to describe the algorithm. When we are talking about space complexity, we are talking about the extra memory that we will consume while executing MergeSort. The extra memory that we will consume will mostly be in the form of these auxiliary arrays left and right that we will create. If you see, once the merge process has finished, we do not need these arrays. So, either they can be cleared automatically. If they are on the stack section of application's memory, once the function finishes they will be erased from memory or we can explicitly erase them by writing some extra statements. but its important, its really needed that these arrays which are being used as scratch space are deleted at the end of function. There can be one variation of MergeSort in which instead of using these auxiliary arrays left and right, instead of using this extra memory for left and right sublists in the MergeSort function, we can use extra memory in the merge function, but lets analyze our implementation only. First think about this. Lets say we do not erase left and right at the end of MergeSort function. So, what will be the memory that we will consume. If you see here in this particular structure, we start with an array of size n and then we split it into two arrays of size n/2 and then we split it into four arrays of size n/4. Lets call this level zero and lets call this level 1 and lets call this level 2 where we have sublists of size 2 each and lets call this level 3. For an array of size n, what will be the maximum number of levels. Its like at level 0, you have a list with n elements and then you go to 2 lists with n/2 elements each and then you go to n/4 elements each and you go on like this till you reach sublists with 1 element each. Lets say this will take k steps. So, it will mean that n/2^k , we will need k divisions by 2 will be equal to 1. So, k will be equal to logn to the base 2. So, we will have logn levels after level 0, And at each step we will have, at each level we will have extra memory used to store n elements. 8 elements are there at level 0 and there are 4+4 = 8 at 1 and there are 2*4 = 8 at 2 and so on. For this example, n is 8 , log n would be 3, so we take 3 steps. We have L1 L2 and L3. So, if we do not clear extra memory used for left and right, we will take space for n integers each for logn level. So, space complexity will be Theta of nlogn , the space requirement, extra space requirement will be theta of nlogn. Now, if we assume that we will clear left and right arrays once we are done using the MergeSort function, then to explain this scenario, I'll simulate the MergeSort calls once again using this array as example. When we will make a call to MergeSort passing this array, we will create these two sublists left and right and then we will pause the execution of the first call and go ahead and make another call on the left sublist. Now, for this call also, two sublists will be created and we will go on to first sort the left part which will again create two sublists. Except this original array, all these extra arrays that are being created are using extra memory. Now, once we reach this stage, we are not creating any extra sublist for these sublists with one element each. We will start wrapping up. We will merge these two 2 and 4. And once the merge is done, we can get rid of the two arrays with one element each and we can go on to sort the other half. So, if we will clear the auxiliary arrays left and right once we are done using them, in the worst case we will use space for n integers at level 1. We will use space for n/2 integers at level 2 and we will use space for n/4 integers at Level 3 and if we will have more elements, we will go on like this. So, the extra space taken will be n + n/2 + n/4 + we will go on in a geometric progression. For this example here, once we will be done merging this sublist 1,2,4,6, we will clear the sublists created for it and then we will go to the other side. So, any time at a level, we will use half the space that we would be using at the parent level. Let us now try to solve this geometric progression here. We can write this as n *(1 + 1/2 + 1/4 + ...) and in all we will have log n terms in this geometric progression. but even if we take this geometric progression 1+ 1/2+1/4+1/8 and so on till infinity, if we take this till infinity, then the sum will be 1 upon 11/2 which will be equal to 2. So, overall this expression, if we call this function S(n), then S(n) is definitely less than or equal to 2n and S(n) is definitely greater than or equal to 1n. So, S(n) is clearly theta n. So, space complexity of merge sort is theta n. So, this was time and space complexity analysis of merge sort algorithm. Thanks for watching !