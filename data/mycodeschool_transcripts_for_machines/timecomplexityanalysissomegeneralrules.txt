So, in the previous lessons we learnt how to calculate the running time of an algorithm and we also learnt about asymptotic notations as a way to classify these running times into some generic classes now in this lesson we will learn about some techniques and some rules using which we can avoid doing all these complex calculations and still derive the time complexity expression in terms of bigoh or theta notation before we start discussing these techniques i will make two statements and the statements are that we analyze time complexity in most cases for (a) very large input size and (b) worstcase scenario what can be a worst case scenario apart from very large input size is something that we will discuss later in some examples now if we have a function defining the running time of an algorithm like T(n) equal to n^3 + 3n^2 + 4n + 2 then for very large input sizes these lower order terms like 3n^2 + 4n + 2 will become insignificant in comparison to the n^3 term, so this will be almost equal to n^3 for very large values of n Or we can also say that thing like when n tends to infinity and that's why we say that this function after some time will not grow any faster than function cn^3 where c is some constant and we say the same thing as this particular function is bigoh of n^3 so the first rule that i want to state is that if you want to calculate the bigoh notation from a polynomial expression like this, then you need to drop all the lower order terms, and you also need to drop the constant multiplier and you will get the bigoh expression for example say if T(n) is equal to 17n^4 + 3n^3 + 4n + 8, then you first need to drop the lower order terms and just keep the highest power which is n to the power 4 and then you also need to drop this constant multiplier seventeen so this will be bigoh of n to the power 4l and we may also sometimes encounter time expressions like T(n) equal to say 16n + log(n) and for higher values of n log n will become insignificant and you can always try to deduce a mathematical proof but this will be equal to bigoh of n so you just ignored the lower order term which will be insignificant in comparison ton. log n will be insignificant in comprising to n and we drop the constant 16 now let use see some other rules and the rule is that we can calculate the running time of an algorithm by summing up that running time of the fragments in the program and we will discuss this using some examples shortly what we will basically teach you is that by looking at a code you shouldfirst look at that running time for different fragments and then calculate the overall running time if a fragment of code is a group of simple statements like some declaration for example we declare int a and then we say that a =5 then we say that a should be incremented then all of these are simple expressions so fragment of code with all these simple expressions will always run in a constant time so the time complexity is O(1) now if we have a loop, say for example and let's say this is a single loop and inside the loop we have simple statements and no complex function call or another loop so the running time of a single loop would be the number of times the loop runs multiplied by the running time of the statements inside, so in this case this loop runs n times and the simple statements take constant time so the total time taken is definitely proportional to n, so the time complexity of this fragment, a fragment like this would be bigoh bigoh of n and I will create some space for another example now let's say we have a fragment which is a nested loop now in this case again the outer loop will run n times and corresponding to each run of the outer loop, the inner loop will also run n times so the simple statements will be actually executed n cross n times and the running time of this particular code will be some constant times n square so clearly you can say that this is bigoh of n^2, this fragment is O(n^2) in terms of Time Complexity similarly if there were three nested loops, then in that case the complexity would have been bigoh of n cube Now, let's assume we have a program like this now in this case we have a fragment which is just a bunch of simple statements we have a fragment which is uh... single loop and we have a fragment which is nested double loop and we have picked up this example from our previous slide now the running time of the first fragment as we saw was O(1), so this is O(1) and this is O(n) and this is O(n^2) and the overall running time of this particular function would be sum of the running time of all these fragments, so this is O(1) + O(n) + O(n^2) now once again when we analyze time complexity, we want to analyze it for very large input sizes so for very large input sizes for very large values of n this part will again become insignificant so, the time taken is actually O(n^2), so Thus in a typical program the maximum running time, the fragment which has the maximum running time actually decides the overall running time of the program now let's say we have a function like this where we have some conditional statements, so the program is like if some condition is true then we have a single loop which have which will have a time complexity O(n) and if this condition is not true we have a nested double loop which will have a time complexity of O(n^2) now if control of the program goes to this particular part It will execute with a complexity O(n) but if the control goes to the else part it will execute with a time complexity of O(n^2) as i had mentioned earlier, when we try to analyze time complexity we always try to analyze it in the worst case so in this case if we are not lucky then we will go into that else condition the control of the program will go into the else condition and the complexity would be O(n^2) so that's why we say that the time complexity for this particular program O(n^2), because that's how it is in the worst case scenario so in case of the conditional statements, you do not simply add up the fragments or try to calculate the time taken as the some of the two running times but you pick up the maximum of the two so we have this rule for conditional statements that pick complexity of the condition which is the worstcase so in this lesson we saw some rules to analyze the time complexity of some very basic programs in next couple of lessons we will see different time complexity functions and their comparison and, we will also see howto analyze the time complexity of recursive programs So, thanks for watching !