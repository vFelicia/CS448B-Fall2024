With timestamps:

00:01 - hello and welcome to this complete
00:03 - Docker course by the end of this course
00:06 - you'll have a deep understanding of all
00:08 - the main Concepts and also a great big
00:11 - picture overview of how Docker is used
00:13 - in the whole software development
00:16 - process the course is a mix of animated
00:19 - theoretic explanations but also Hands-On
00:22 - demos for you to follow along so get
00:25 - your first hands-on experience and
00:27 - confidence using Docker in your
00:30 - projects so let's quickly go through the
00:33 - topics I'll cover in this course we will
00:35 - start with the basic concepts of what
00:38 - Docker actually is and what problems it
00:40 - solves also we'll understand the
00:42 - difference between Docker and virtual
00:44 - machine and after installing Docker we
00:47 - will go through all the main Docker
00:49 - commands to start and stop containers
00:51 - debug containers Etc after that we'll
00:54 - see how to use Docker in practice by
00:58 - going through a complete workflow with a
01:00 - demo project so first we'll see how to
01:03 - develop locally with containers then
01:06 - we'll run multiple containers or
01:08 - services with Docker compos we'll build
01:11 - our own Docker image with Docker file
01:13 - and we'll push that built image into a
01:15 - private Docker repository on AWS and
01:18 - finally we'll deploy our containerized
01:22 - application last but not least we'll
01:24 - look at how to persist data in Docker
01:27 - learning the different volume types and
01:29 - afterwards configure persistence for our
01:32 - demo project if you get stuck anywhere
01:36 - just comment under the video and I will
01:38 - try my best to answer your questions
01:40 - also you can join the private Tech
01:42 - worldwi community group on Facebook
01:45 - which is there to exchange your
01:47 - knowledge with others and connect with
01:49 - them if you like the course by the end
01:52 - of the video be sure to subscribe to my
01:54 - channel for more related content so
01:57 - let's get started
02:01 - so we'll talk about what a container is
02:04 - and what problems it solves we will also
02:07 - look at a container repository which is
02:09 - basically a storage for containers we'll
02:11 - see how a container can actually make
02:13 - the development process much easier and
02:16 - more efficient and also how they solve
02:18 - some of the problems that we have in the
02:20 - deployment process of
02:22 - applications so let's dive right into it
02:25 - what a container is a container is a way
02:27 - to package applications with everything
02:29 - they need inside of that package
02:31 - including the dependencies and all the
02:33 - configuration necessary and that package
02:36 - is portable just like any other artifact
02:38 - is and that package can be easily shared
02:41 - and moved around between a development
02:43 - team or development and operations Steam
02:46 - and that portability of containers plus
02:51 - everything packaged in one isolated
02:53 - environment gives it some of the
02:55 - advantages that makes development and
02:57 - deployment process more efficient
03:01 - and we'll see some of the examples of
03:03 - how that works in later
03:05 - slides so as I mentioned containers are
03:08 - portable so there must be some kind of a
03:10 - storage for those containers so that you
03:13 - can share them and move them around so
03:16 - containers live in a container
03:18 - repository this is a special type of
03:20 - storage for containers many companies
03:23 - have their own private repositories
03:25 - where they host or the where they store
03:28 - all the containers and this will look
03:31 - something like this where you you can
03:33 - push all of the containers that you have
03:36 - but there is also a public repository
03:38 - for Docker containers where you can
03:41 - browse and probably find any application
03:44 - container that you want so let's head
03:47 - over to the browser and see how that
03:49 - looks like so if I here search for a
03:52 - dockerhub which is the name of the
03:54 - public repository for Tucker I will see
03:58 - this official website
04:00 - so here if you scroll down you see that
04:03 - there are more than 100,000 container
04:06 - images of different applications hosted
04:10 - or stored in this Docker repository so
04:13 - here you see just some of the examples
04:15 - and for every application there's this
04:17 - official Docker container or Docker
04:20 - container image um but if you are
04:22 - looking for something else you can
04:25 - search it here and I see there's an
04:28 - official image for let's say Jenkins uh
04:31 - but there's also a lot of non-official
04:34 - images or container images that
04:36 - developers
04:38 - or or even from Jenkins itself that they
04:42 - actually store it here so public
04:44 - repository is where you usually get
04:47 - started when you're using or when you're
04:50 - starting to use the containers where you
04:52 - can find any application
04:55 - image so now let's see how containers
04:58 - improve the development process by
05:01 - specific
05:02 - examples how did we develop applications
05:05 - before the containers usually when you
05:08 - have a team of developers working on
05:10 - some application you would have to
05:13 - install most of the services on your
05:16 - operating system directly right for
05:18 - example you are developing some
05:20 - JavaScript application and you need a
05:22 - postp ql and you need red for messaging
05:26 - and every developer in the team would
05:29 - then have to go and install the binaries
05:32 - of those services
05:34 - and configure them and run them on their
05:37 - local development environment and
05:41 - depending on which operating system
05:43 - they're using the installation process
05:45 - will look actually different also
05:49 - another thing with installing services
05:51 - like this is that you have multiple
05:54 - steps of installation so you have a
05:57 - couple of commands that you have to
05:58 - execute and the chances of something
06:02 - going wrong and error happening is
06:04 - actually pretty high because of the
06:06 - number of steps required to install each
06:08 - service and this approach or this
06:11 - process of setting up a new environment
06:13 - can actually be pretty tedious depending
06:16 - on how complex your application is for
06:18 - example if you have 10 services that
06:20 - your application is using then you would
06:23 - have to do that 10 times on each
06:25 - operating system environment so now
06:27 - let's see how containers solve some of
06:30 - these problems with containers you
06:33 - actually do not have to install any of
06:36 - the services directly on your operating
06:38 - system because the container is its own
06:41 - isolated operating system layer with
06:44 - Linux based image as we saw in the
06:46 - previous slides you have everything
06:48 - packaged in one isolated environment so
06:51 - you have the posis ql with a specific
06:54 - version packaged with the configuration
06:57 - and the start script inside of one
06:59 - container so as a developer you don't
07:02 - have to go and look for the binaries to
07:04 - download on your machine but rather you
07:06 - just go ahead and check out the
07:09 - container repository to find that
07:11 - specific container and download on your
07:14 - local machine and the download step is
07:18 - just one Docker command which fetches
07:20 - the container and starts it at the same
07:23 - time and regardless of which operating
07:25 - system you're on the command the docker
07:28 - command for starting the container will
07:30 - not be different it will be exactly the
07:32 - same so if you have 10 applications that
07:34 - your JavaScript application uses and
07:37 - depends on you would just have to run 10
07:40 - Docker commands for each container and
07:42 - that will be it which makes the setting
07:47 - up your local development environment
07:50 - actually much easier and much more
07:53 - efficient than the previous version also
07:55 - as we saw in the demonstration before
07:58 - you can actually have different versions
08:00 - of the same application running on your
08:03 - local environment without having any
08:07 - conflict so now let's see how containers
08:10 - can improve the deployment process
08:12 - before the containers a traditional
08:14 - deployment process will look like this
08:17 - development team will produce artifacts
08:20 - together with a set of instructions of
08:22 - how to actually install and configure
08:25 - those artifacts on the server so you
08:28 - would have a jar file or something
08:30 - similar for your application and in
08:32 - addition you would have some kind of a
08:34 - database service or some other service
08:36 - also with a set of instructions of how
08:39 - to configure and set it up on the
08:42 - server so development team would give
08:45 - those artifacts over to the operations
08:48 - team and the operations team will handle
08:51 - setting up the environment to deploy
08:53 - those
08:54 - applications now the problem with this
08:57 - kind of approach is that first first of
08:59 - all you need to configure everything and
09:01 - install everything directly on the
09:04 - operating system which we saw in the
09:06 - previous example that could actually
09:08 - lead to conflicts with dependency
09:11 - versions and multiple Services running
09:13 - on the same host another problem that
09:15 - could arise from this kind of process is
09:18 - when there is misunderstanding between
09:20 - the development team and operations
09:23 - because everything is in a textual guide
09:26 - as instructions so there could be cases
09:29 - where developers forget to mention some
09:31 - important point about
09:33 - configuration or maybe when operations
09:36 - team misinterpret some of those
09:39 - instructions and when that fails the
09:42 - operations team has to go back to the
09:44 - developers and ask for more details and
09:48 - this could lead to some back and forth
09:49 - communication until the application is
09:52 - successfully deployed on the server with
09:55 - containers this process is actually
09:58 - simplified because now you have the
10:01 - developers and operations working in one
10:03 - team to package the whole configuration
10:07 - dependencies inside the application just
10:09 - as we saw previously and since it's
10:12 - already encapsulated in one single
10:15 - environment and you don't have to
10:16 - configure any of this directly on the
10:19 - server so the only thing you need to do
10:22 - is run a Docker command that pulls that
10:25 - container image that you've stored
10:28 - somewhere in the repos repository and
10:30 - then run it this is of course a
10:33 - simplified version but that makes
10:35 - exactly the problem that we saw on the
10:37 - previous slide much more easier no
10:41 - environmental configuration needed on
10:43 - the server the only thing of course you
10:45 - need to do is you have to install and
10:47 - set up the docker runtime on the server
10:49 - before you will be able to run
10:51 - containers there but that's just onetime
10:54 - effort
11:00 - now that you know what a container
11:02 - concept is let's look at what a
11:03 - container is technically so technically
11:06 - container is made up of images so we
11:09 - have layers of stacked images on top of
11:11 - each other and at the base of most of
11:13 - the containers you would would have a
11:16 - Linux based image which is either Alpine
11:18 - with a specific version or it could be
11:20 - some other Linux distribution and it's
11:22 - important for those base images to be
11:24 - small that's why most of them are
11:26 - actually Alpine because that will make
11:29 - make sure that the containers stay small
11:31 - in size which is one of the advantages
11:33 - of using container so on top of the base
11:36 - image you would have application image
11:39 - and this is a simplified diagram usually
11:41 - you would have these intermediate images
11:43 - that will lead up to the actual
11:46 - application image that is going to run
11:48 - in the container and of course on top of
11:50 - that you will have all this
11:51 - configuration data so now I think it's
11:54 - time to dive into a practical example of
11:58 - how you you can actually use a Docker
12:00 - container and how it looks like when you
12:02 - install it and download it and run it on
12:06 - your local
12:07 - machine so to give you a bit of an idea
12:11 - of how this works let's head over to
12:14 - dockerhub
12:15 - and search for postris
12:23 - ql so here which is a Docker official
12:26 - image I can see some of the versions and
12:28 - let's say say I'm looking specifically
12:31 - for older version I don't know 96
12:35 - something so I'm going to pull that one
12:39 - so this is a dock repository so that I
12:42 - can actually go ahead and pull the
12:44 - containers from that repository directly
12:47 - and because it's a public repository I
12:50 - don't have to log into it I don't have
12:52 - to provide any authentication
12:54 - credentials or anything I can just get
12:57 - started with simple Docker command
13:01 - without doing or configuring anything to
13:04 - access dockerhub so on my terminal I can
13:07 - just do
13:10 - Docker Pole or I can even do Docker run
13:13 - and then just copy the the image name
13:17 - and if I don't specify any version it
13:21 - will just give me the latest but I want
13:23 - a specific version so I'm just I'm going
13:25 - to go with
13:27 - 9.6 actually just to demonstrate so I
13:32 - can provide the version like this with a
13:35 - column and I can
13:37 - start run so as you see the first line
13:41 - says unable to find image locally so it
13:43 - knows that it has to go to dockerhub and
13:46 - pull it from there and the next line
13:49 - says pulling from library
13:52 - postgress and here you see a lot of
13:55 - hashes that says
13:57 - downloading and the this is what I
14:00 - mentioned earlier which is Docker
14:02 - containers or any containers are made up
14:04 - of layers right you have the Linux image
14:07 - layer you have the application layers
14:09 - and so on so what what you see here are
14:12 - actually all those layers that are
14:16 - separately downloading from the
14:18 - dockerhub on my machine right and the
14:21 - advantage of splitting those
14:23 - applications and layers is that actually
14:25 - for example if the image changes or I
14:28 - have to download a newer version of
14:30 - pogress what happens is that the layers
14:33 - they're the same between those two
14:36 - applications two versions of posis will
14:40 - not be downloaded again but only those
14:44 - layers that are different so for example
14:47 - now it's going to take around 10 or 15
14:49 - minutes to download this one image
14:52 - because I don't have any pogress locally
14:54 - but if I were to download the next
14:56 - version it will take a little bit less
14:58 - time
14:59 - because some layers already exist on my
15:02 - local
15:03 - machine so now you see that it's already
15:07 - logging because it this command that I
15:11 - ran here the docker run with the
15:13 - container name and version it fetches or
15:16 - it pulls the the container but it also
15:20 - starts it so it executes the start
15:22 - script right away as soon as it
15:24 - downloads it and here you see the output
15:27 - of the starting of the application so it
15:29 - just gives some output about um starting
15:32 - the server and doing some configuration
15:35 - stuff and here you see database system
15:39 - is ready to accept connections and
15:41 - launcher started so now let's open the
15:45 - new tab and see with Docker PS command
15:48 - you can actually see all the running
15:52 - containers so here you see that
15:55 - postgress 96 is running and it actually
15:59 - says image so this is another important
16:02 - thing to understand when we're talking
16:04 - about containers there are two technical
16:06 - terms image and a container and a lot of
16:09 - people confuse those two I think and
16:11 - there is actually a very easy
16:13 - distinction between the two so image is
16:16 - the actual package that we saw in one of
16:18 - those previous slides so the application
16:21 - package together with the configuration
16:23 - and the dependencies and all these
16:25 - things this is actually the artifact
16:27 - that is movable around is is actually
16:30 - the
16:31 - image container is when I pull that
16:34 - image on my local machine and I actually
16:37 - started so the application inside
16:39 - actually starts that creates the
16:42 - container environment so if it's not
16:44 - running basically it's an image it's
16:46 - just an artifact that's lying around if
16:49 - I start it and actually run it on my
16:51 - machine it is a container so that is the
16:55 - distinction so here it says the active
16:58 - running containers with a container ID
17:01 - image that it's running from and some
17:04 - entry commands that it executed and some
17:07 - other status
17:09 - information so this means that poql is
17:12 - now running on my local machine simple
17:15 - as that if I were now to uh need let's
17:19 - say another version of pogress to run at
17:21 - the same time on my local machine I
17:23 - could just go ahead and say let's go
17:27 - back and let's say I want want to have
17:30 - 9.6 and
17:33 - 10.10 running at the same time on my
17:35 - local machine I just do run
17:44 - postgress and
17:46 - run again it doesn't find it locally so
17:49 - it pushes and this is what I actually
17:52 - explained to you earlier
17:55 - because it's the same application but
17:57 - with just a different version version
17:59 - some of the layers of the image are the
18:02 - same so I don't have to fetch those
18:05 - again because they are already on my
18:07 - machine and it just fetches the layers
18:09 - that are different so that saves a
18:12 - little bit of uh time and I think it's
18:16 - it could be actually good
18:20 - Advantage so now we'll wait for other
18:23 - image layers to load so that we have the
18:26 - second
18:29 - postgress version running and now you
18:31 - see I have postgress 9.6 running in this
18:38 - uh command line tab and I have postgress
18:41 - version 10.10 running in the next one so
18:45 - I have two postes with different
18:47 - versions running and I can actually
18:49 - output them here have both of them
18:51 - running and there's no conflict between
18:53 - those two like I can actually run any
18:57 - number of applications with different
19:00 - versions maybe of the same application
19:02 - with no problem at all and we going to
19:05 - go through how to use those containers
19:07 - in your application and the port
19:09 - configuration and some of the other
19:11 - configuration stuff later in this
19:13 - tutorial when we do a deep dive but this
19:15 - is just for you to get the first visual
19:19 - image of how Docker containers actually
19:21 - work how they look like and how easily
19:24 - you can actually start them on your
19:26 - local machine without having to
19:28 - implement lement a specific version of
19:30 - postgress application and do all the
19:33 - configuration
19:41 - yourself when I first started learning
19:44 - Docker after understanding some of the
19:46 - main Concepts my first question was okay
19:49 - so what is the difference between Docker
19:51 - and an Oracle uh virtual box for example
19:55 - and the difference is quite simple I
19:56 - think and in this short video I'm I'm
19:58 - going to cover exactly that and I'm
20:00 - going to show you the difference by
20:02 - explaining how docu works on an
20:04 - operating system level and then
20:06 - comparing it to how virtual machine
20:09 - works so let's get
20:12 - started in order to understand how
20:14 - Docker works on the operating system
20:16 - level let's first look at how operating
20:19 - system is made up so operating systems
20:23 - have two layers operating system kernel
20:26 - and the applications layer so so as you
20:29 - see in this diagram the kernel is the
20:31 - part that communicates with the hardware
20:34 - components like CPU and memory Etc and
20:37 - the applications run on the Kernel layer
20:41 - so they are based on the Kernel uh so
20:44 - for example we you all know Linux
20:46 - operating system and there are lots of
20:49 - distributions of Linux out there there's
20:51 - buonto and Debian and there's Linux Mint
20:53 - Etc there are hundreds of distributions
20:56 - they all look different so the graphical
20:58 - user the interface is different the file
21:00 - system is maybe different so a lot of
21:02 - applications that you use are different
21:04 - because even though they use the same
21:08 - Linux kernel they use different or they
21:11 - Implement different applications on top
21:14 - of the kernel so as you know Docker and
21:17 - virtual machine they're both
21:19 - virtualization tools so the question
21:21 - here is what parts of the operating
21:24 - system they
21:26 - virtualize so docker
21:30 - virtualizes the applications layer so
21:33 - when you download a Docker image it
21:35 - actually contains the applications layer
21:38 - of the operating system and some other
21:40 - applications installed on top of it and
21:42 - it uses the kernel of the host because
21:45 - it doesn't have its own kernel the
21:48 - virtual box or the virtual machine on
21:50 - the other hand has the applications
21:52 - layer and its own kernel so it
21:56 - virtualizes the complete operating
21:58 - system which means that when you
22:01 - download a virtual machine image on your
22:04 - host it doesn't use your host kernel it
22:06 - boots up its own so what does this
22:09 - difference between Docker and virtual
22:11 - machine actually mean so first of all
22:14 - the size of Docker images are much
22:16 - smaller because they just have to
22:18 - implement one layer so Docker images are
22:22 - usually couple of megabytes uh virtual
22:24 - machine images on the other hand can be
22:26 - couple of gigabytes large a second one
22:29 - is the speed so you can run and start
22:31 - Docker containers much faster than the
22:34 - VMS because they every time you start
22:36 - them you have they have to put the
22:39 - operating system kernel and the
22:41 - applications on top of it the third
22:43 - difference is compatibility so you can
22:46 - run a virtual machine image of any
22:50 - operating system on any other operating
22:53 - system host but you can't do that with
22:56 - Docker so what is the problem exactly
22:59 - let's say you have a Windows operating
23:01 - system with a kernel and some
23:03 - applications and you want to run a Linux
23:06 - based Docker image on that Windows host
23:10 - the problem here is that a Linux based
23:12 - Docker image might not be compatiable
23:14 - with the windows kernel and this is
23:17 - actually true for the windows versions
23:19 - below 10 and also for the older Mac
23:22 - versions which if you have seen how to
23:24 - install Docker on different operating
23:26 - systems you see that the first step is
23:28 - to check whether your host can actually
23:31 - run Docker natively which basically
23:34 - means is the kernel compatible with the
23:36 - docker images so in that case a
23:39 - workaround is that you install a
23:41 - technology called Docker toolbox which
23:44 - abstracts away the kernel to make it
23:46 - possible for your host to run different
23:49 - Docker
23:56 - images so in this video I will show you
23:59 - how to install Docker on different
24:00 - operating systems the installation will
24:03 - differ not only based on the operating
24:05 - system but also the version of that
24:07 - operating system so you can actually
24:09 - watch this video selectively depending
24:11 - on which OS and the version of that OS
24:14 - you have I will show you how to find out
24:16 - which installation step applies to you
24:19 - in the before installing section which
24:20 - is the first one so once you find that
24:23 - out you can actually directly skip to
24:25 - that part of the video where I explained
24:27 - that into details I will put the minute
24:29 - locations of each part in the
24:31 - description part of the video and also I
24:34 - will put all the links that I use in the
24:36 - video in the description um so that you
24:39 - can easily access them also if you have
24:41 - any questions during the video or if you
24:44 - get stuck installing the docker on your
24:46 - system please post your question or
24:48 - problem in the comment section so that I
24:51 - can um get back to you and help you
24:53 - proceed or maybe someone from the
24:55 - community will uh so with that said
24:58 - let's let's dive right into
25:00 - it so if you want to install Docker you
25:03 - can actually Google it and you get an
25:06 - official documentation of Docker um it's
25:09 - important to note that there are two
25:10 - additions of Docker there is a community
25:12 - and Enterprise additions um for us to
25:15 - begin with Community Edition will be
25:18 - just fine in the docker Community
25:21 - Edition uh tab there there's a list of
25:24 - operating systems and distributions in
25:26 - order to install docker
25:28 - so for example if we start with Mac we
25:31 - can click in here and we see the
25:34 - documentation of how to install it on
25:36 - Mech which is actually one of the
25:38 - easiest but we'll see some other ones as
25:44 - well so before you install Docker on
25:47 - your Mac or Windows computer there are
25:50 - prerequisites to be considered so for
25:53 - mac and
25:54 - windows there has to be some criteria of
25:58 - the operating system and the hardware
26:00 - met in order to support running
26:03 - Docker if you have Mech go through the
26:06 - system requirements to see if your U
26:09 - Mech version is actually supporting
26:12 - Docker if you have Windows then you can
26:15 - go to the windows Tab and look at the
26:18 - system requirements there or what to
26:20 - know before you install for example one
26:23 - thing to note is that Docker natively
26:26 - runs only on Windows 10
26:29 - so if you have a Windows version Which
26:31 - is less than 10 then Docker cannot run
26:35 - natively on your
26:37 - computer so if your computer doesn't
26:40 - meet the requirements to run Docker
26:42 - there is a workaround for that which is
26:44 - called Docker toolbox instead of Docker
26:47 - you basically just have to install a
26:49 - Docker toolbox that will become a sort
26:51 - of a bridge between your operating
26:53 - system and the docker and that will
26:55 - enable you to run Docker on your leg
26:58 - Legacy computer so if that applies to
27:02 - you then skip ahead in this video to the
27:05 - part where I explain how to install
27:07 - Docker toolbox on Mac and on
27:14 - windows so let's install Docker for Mac
27:18 - as you see here there are two um
27:20 - channels that you can download the
27:23 - binaries from or the application from we
27:25 - will go with the stable Channel and
27:27 - other things to can see if you have an
27:29 - older version of Mech either software or
27:32 - the hardware please go through the
27:34 - system requirements to see if you can
27:37 - actually install Docker so here there is
27:40 - a detailed description of what make
27:42 - version you need um to be able to run
27:45 - Docker and also you need at least four
27:48 - gab of RAM and by installing Docker you
27:52 - will actually have the whole package of
27:54 - it which is a Docker engine uh which is
27:58 - important or which is necessary to run
28:01 - the docker containers on your laptop the
28:04 - docker command line client which will
28:06 - enable you to execute some Docker
28:08 - commands Docker compose if you don't
28:10 - know it yet don't worry about it but
28:12 - it's just technology to orchestrate if
28:15 - you have multiple containers um and some
28:18 - other stuff that we're not going to need
28:21 - in this tutorial but you will have
28:22 - everything in a package installed so go
28:25 - ahead and download the stable
28:29 - version well I already have Docker
28:32 - installed from The Edge channel so I
28:36 - won't be installing it again but it
28:38 - shouldn't matter because the steps of
28:39 - installation are the same for
28:41 - both so once the docker DMG file is
28:45 - downloaded you just double click on it
28:48 - and it will pop up this window just drag
28:53 - the docker whale app into the
28:56 - applications and it will be installed on
28:59 - your Mach as the next step you will see
29:02 - Docker installed in your applications so
29:06 - you can just go ahead
29:08 - and start
29:12 - it so as you see the docker sign or icon
29:17 - is starting here if you click on it you
29:19 - see the status that Docker is
29:22 - running and you can configure some
29:25 - preferences and check the docker version
29:29 - and so on and if you want to stop Docker
29:32 - or quit it on your Mech you can just do
29:36 - it from here um an important maybe
29:39 - interesting note here is that if let's
29:41 - say you download or install Docker and
29:45 - you have uh more than one accounts on
29:48 - your
29:49 - laptop you will actually get some errors
29:52 - or conflicts if you run Docker at the
29:55 - same time or multiple accounts so what I
30:00 - do for example is that if I switch to
30:02 - another account where I'm also going to
30:04 - need Docker I quit it from here and then
30:07 - I start it from other account so that I
30:10 - don't get any
30:11 - errors so that might be something you
30:14 - need to consider if you use multiple
30:19 - accounts so let's see how to install
30:22 - Ducker for Windows the first step as I
30:24 - mentioned before is to go to the before
30:27 - you install section and to see that your
30:30 - operating system and your computer meets
30:33 - all the criteria to run Docker natively
30:36 - so if you're installing Docker for the
30:38 - first time don't worry about most of
30:40 - these parts like Docker toolbox and
30:42 - Docker machine there are two things that
30:44 - are important one is to double check
30:47 - that your windows version is compatible
30:50 - for Docker and the second one is to have
30:53 - virtualization
30:54 - enabled virtualization is by default
30:57 - always enable abled um other than you
30:59 - manually disabled it so if you're unsure
31:03 - then you can check it by going to the
31:05 - task manager performance CPU Tab and
31:09 - here you can see the status of the
31:12 - virtualization so once you have checked
31:14 - that and made sure that these two
31:17 - prerequisites are met then actually you
31:20 - can scroll up and download the windows
31:23 - installer for from the stable
31:26 - Channel once they install installer is
31:28 - downloaded you can just click on it and
31:30 - follow the installation wizard to
31:34 - install Docker for Windows once the
31:36 - installation is completed you have to
31:39 - explicitly start Docker because it's not
31:41 - going to start automatically so for that
31:44 - you can just go and search for the
31:46 - docker for Windows app on your windows
31:49 - just click on it and you will see the
31:52 - docker whale icon um starting and if you
31:55 - click on that icon you can actually see
31:57 - the status that says stalker is now up
32:00 - and running so this is basically it for
32:03 - the
32:07 - installation now let's see how to
32:09 - install Docker on different Linux
32:12 - distributions and this is where things
32:14 - get a little bit more complicated so
32:17 - first of all you see that in this menu
32:20 - on the on the left you see that for
32:23 - different Linux distributions the
32:25 - installation steps will differ but also
32:28 - for example if we just click on auntu
32:31 - for the guide you can see that in the
32:33 - prerequisites section there is also
32:37 - differentiation between the versions of
32:39 - the same Linux distribution and there
32:42 - may be some even more complicated
32:43 - scenarios where the combination of the
32:45 - version of the distribution and the
32:48 - architecture it's running in um also
32:50 - makes some difference into how to set up
32:53 - Docker on that specific environment
32:58 - because of that I can't go through a
33:00 - Docker installation process of every
33:03 - Linux environment because there are just
33:05 - too many combinations so instead what
33:07 - we'll do is just go through a general
33:09 - overview of the steps and configuration
33:12 - process to get Docker running on your
33:15 - Linux environment and you can just
33:17 - adjust it then for your specific setup
33:20 - so these are some general steps to
33:22 - follow in order to install Docker on
33:24 - your Linux Linux environment first of
33:27 - all all you have to go through the
33:29 - operating system requirements part on
33:32 - the relevant Linux distribution um that
33:35 - applies for you a second step in the
33:38 - documentation to is to uninstall old
33:40 - versions however if it's the first time
33:43 - you installing Docker then you don't
33:45 - have to worry about that you also don't
33:47 - have to worry about the supported
33:49 - storage
33:50 - drivers and you can skip ahead to the
33:52 - part of installing Docker Community
33:55 - Edition
33:58 - so for any Linux distribution here the
34:01 - steps will be or the options for
34:03 - installing Docker will be the same so
34:06 - first option is basically to set up a
34:09 - repository and download the docker from
34:12 - and install it from the docker
34:14 - repository um the second option is to
34:18 - install the packages manually however I
34:21 - wouldn't recommend it and I think the
34:23 - documentation doesn't recommend it
34:25 - either because then you'll have to do a
34:27 - lot of steps of the installation and the
34:29 - maintenance of the versions manually so
34:32 - I wouldn't do that the third one is just
34:35 - for the testing purposes it may be
34:37 - enough for the development purposes as
34:39 - well but I would still not do it which
34:41 - is basically just downloading some
34:43 - automated scripts that will install and
34:47 - setup Docker on your Linux environment
34:50 - however again I wouldn't go with it I
34:53 - would actually just do the first option
34:56 - which is just download Lo in the docker
34:59 - from the repository so in order to
35:02 - install Docker using the first option
35:05 - which is downloading it from the docker
35:07 - repositories you have two main steps so
35:11 - the first one is to set up the
35:14 - repository uh which differs a little bit
35:17 - depending on which distribution you have
35:20 - and then install the docker CE from that
35:23 - repository so from abunto and Debian the
35:28 - steps for setting up the repository are
35:31 - generally just updating your app package
35:33 - then setting up an https connection with
35:36 - the repository and adding the docker's
35:39 - official gpg key which only aono and dbn
35:44 - need you don't have to do this um steps
35:47 - for SOS and Fedora they just have to
35:50 - install
35:52 - the required packages and the last step
35:56 - uh for setting up the repository is
35:58 - basically setting up the stable
36:02 - repository of Docker which we saw
36:04 - previously on the overview that there
36:06 - are two channels which is a stable and
36:08 - Edge here you always have to set up the
36:10 - stable repository optionally you can
36:12 - also set up the edge repository but I'll
36:15 - just do uh stable this time and here
36:19 - also something to notice depending on
36:22 - architecture you have to actually set it
36:26 - or you have to set that as a parameter
36:28 - when you set up the repository so if you
36:31 - have for example a different
36:32 - architecture you can use those steps to
36:34 - display the correct command for it and
36:38 - um I guess that applies to other Linux
36:41 - distributions as well like for example
36:43 - here you also have the second tab where
36:45 - you see a separate command for it so
36:49 - these steps should actually um set up
36:52 - the repository so that as a Next Step
36:54 - you can then install the docker C from
36:57 - those
36:58 - repositories so installing Docker from
37:00 - the setup repository is actually pretty
37:02 - straightforward the steps are same for
37:05 - or similar to all the distributions
37:08 - basically just update the app package
37:11 - and then you just say install Docker CE
37:14 - so this command will just download the
37:16 - latest Docker version if you want to
37:19 - install a specific one which you will
37:22 - need to do in a production environment
37:25 - then you can just uh provide a version
37:27 - like like this to just say Docker minus
37:30 - C equals some specific versions and
37:34 - using this command you can actually look
37:36 - up what versions are available in that
37:38 - repository that you just and with this
37:41 - command actually Docker will be
37:43 - installed um on your Linux environment
37:46 - and then you can just verify using PSE
37:49 - sudo Docker run hello world which is
37:52 - this demo image of Docker you can verify
37:55 - that Docker is running and this will
37:57 - start hello world Docker container on
38:01 - your
38:05 - environment so as I mentioned previously
38:08 - for environments um that do not support
38:12 - running Docker natively there is an
38:15 - workaround which is called Docker
38:17 - toolbox so Docker toolbox is basically
38:20 - an installer for Docker environment
38:23 - setup on those systems
38:27 - so this is how to install uh Docker
38:30 - toolbox on your Mac um this is the whole
38:33 - package that comes with the installation
38:35 - of Docker toolbox which is basically the
38:37 - docker command line Docker machine
38:39 - Docker compose basically all the
38:41 - packages that we saw in the native
38:43 - installation and in on top of that you
38:45 - also get the Oracle VM virtual box so in
38:48 - order to install the docker toolbox it's
38:51 - actually pretty straightforward on this
38:53 - website you can go to the toolbox
38:55 - releases where we have all the leas of
38:58 - latest releases you just take the uh
39:01 - latest release and here you see two
39:03 - assets this one is for Windows obviously
39:06 - and you just download the package for
39:08 - mac and once it's downloaded you just
39:11 - click on it and go through the
39:13 - installation wizard leave all the
39:16 - options by default as they are do not
39:18 - change anything and after the
39:20 - installation you can just validate that
39:23 - the installation is successful and you
39:26 - can actually run Docker so so after
39:28 - seeing the installation was successful
39:29 - screen just go and look up in your
39:31 - launch pad dock quick start terminal and
39:35 - once you open it you should be able to
39:37 - run uh Docker commands and you can just
39:39 - try Docker run hello world which should
39:42 - just start up or bring up um this hello
39:45 - world Docker container on your
39:50 - environment so now let's see how to
39:52 - install Docker toolbox on Windows here
39:55 - see that you get the whole package of
39:57 - Docker Technologies with a toolbox which
39:59 - are basically the same package which you
40:02 - get on the uh Native Docker installation
40:05 - and on top of that you get Oracle VM
40:07 - virtual box which is the tool that
40:10 - enables Docker to run on an older system
40:13 - so before you install Docker tool books
40:15 - you have to make sure that you meet some
40:17 - of the preconditions number one you have
40:20 - to make sure your Windows system
40:22 - supports virtualization and that
40:25 - virtualization must be enabled
40:28 - otherwise Docker Docker won't start so
40:30 - depending on which Windows version you
40:33 - have looking up or checking the
40:35 - virtualization status will be different
40:38 - so I just suggest you Google it and look
40:40 - it up of how to find the virtualization
40:43 - status to see that it's enabled once you
40:46 - have that checked also make sure that
40:48 - your Windows operating system is 64 bits
40:53 - so if those two criteria are met then
40:55 - you can go ahead and install the Locker
40:57 - toolbox the place where you see the
40:59 - releases or the release artifacts is
41:01 - toolbox releases link here which I have
41:04 - open so it's basically a list of the
41:07 - releases you just take the latest one
41:09 - which has two artifacts this is the one
41:12 - for Windows you just download this
41:14 - executable file click on it and go
41:17 - through the installation wizard once the
41:19 - installation is completed there are just
41:21 - couple of steps here you can verify that
41:24 - Docker was installed or the toolbox was
41:26 - installed
41:27 - by just looking up the docker quick
41:29 - start terminal on your windows that app
41:32 - must be installed and once you click on
41:34 - it and open it you should be able to run
41:36 - Docker commands in the terminal so the
41:38 - basic Docker command that you can test
41:41 - will be Docker run hello world which
41:44 - will just fetch this basic uh Docker
41:47 - container from the public registry and
41:49 - run it on your computer if that command
41:53 - is successful it means that Docker was
41:55 - successfully installed on your computer
41:58 - and now you can proceed with the
42:04 - tutorial so in this video um I'm going
42:07 - to show you some basic Docker commands
42:10 - at the beginning I'm going to explain
42:11 - what the difference between container
42:13 - and images because that's something a
42:15 - lot of people confuse then very quickly
42:17 - go through version and tag and then show
42:19 - you a demo of how to use the basic
42:22 - Docker commands um commands that will be
42:24 - enough to pull an image locally to start
42:26 - a container to configure a container and
42:29 - even debug the container so with that
42:32 - said let's get
42:35 - started so what is the difference
42:37 - between container and image mostly
42:39 - people use those terms interchangeably
42:42 - but actually there is a fine difference
42:43 - between the two to see
42:46 - theoretically container is just the part
42:49 - of a container runtime so container is
42:52 - the running environment for an image so
42:56 - as you see in this
42:58 - graphic the application image that runs
43:01 - the application could be postgress redis
43:04 - some other application needs let's say a
43:07 - file system where it can save the log
43:10 - files or where it can store some
43:12 - configuration files it also needs some
43:15 - environmental configuration like
43:16 - environmental variables and so on so all
43:20 - of this environmental stuff are provided
43:23 - by
43:24 - container and container also has a Port
43:27 - that is binded to it uh which makes it
43:30 - possible to talk to the application
43:32 - which is running inside of a container
43:35 - and of course it should be noted here
43:37 - that the file system is virtual in
43:39 - container so the container has its own
43:42 - abstraction of an operating system
43:45 - including the file system and the
43:47 - environment which is of course different
43:49 - from the file system and environment of
43:51 - the host machine so in order to see the
43:55 - difference between container and image
43:57 - in action let's head over to the docker
44:01 - Hub and find for example a rice image
44:05 - another thing is that dockerhub all the
44:08 - artifacts that are in the docker Hub are
44:11 - images so we're not talking about
44:13 - containers here all of these things are
44:16 - images Docker official image so we're
44:19 - going to go ahead and pull a rad image
44:22 - out of the doah Hub to my laptop
44:28 - so you see the different layers of the
44:30 - image are
44:34 - downloaded and this will take a couple
44:37 - of
44:39 - minutes so once the download is complete
44:42 - I
44:43 - can check all the existing images on my
44:47 - laptop using Docker images command so I
44:50 - see I have two images radi and postgress
44:54 - with text image IDs and so on another
44:57 - important aspect of images is that they
45:01 - have texts or versions so for example if
45:04 - we go back to the docker Hub each one
45:07 - each image that you look up in the
45:10 - docker Hub uh will have any different
45:13 - versions the latest is always the one
45:17 - that you get when you don't specify the
45:19 - version of course if you have a
45:21 - dependency on a specific version you can
45:23 - actually choose the version you want and
45:27 - specified and you can select one from
45:29 - here so this is what you see here the
45:32 - tag is basically the version of the
45:35 - image so I just downloaded the latest
45:38 - and I can also see the size of the image
45:42 - so now to this point we have only worked
45:46 - with images there is no container
45:48 - involved and there is no redish running
45:51 - so now let's say I need red running so
45:55 - that my application can connect to it
45:57 - I'll have to create a container of that
46:00 - redice image that will make it possible
46:03 - to connect to the redis application and
46:07 - I can do it by running the redis image
46:12 - so if I say Docker Run
46:15 - Red this will actually start the image
46:19 - in a container so as I said before
46:22 - container is a running environment of an
46:25 - image so now if I open a new tab and do
46:30 - Docker
46:31 - PS I will get status of all the running
46:36 - Docker containers so I can see the
46:38 - container redis is running with a
46:40 - container ID based on the image of redis
46:45 - and some other information about it for
46:48 - example the port that it's running on
46:51 - and so
46:53 - on so as you see here the docker run
46:56 - redis command will start the redis
46:59 - container in the terminal um in an
47:02 - attached mode so for example if I were
47:04 - to terminate this with the control
47:08 - C you see that red application stops and
47:12 - the container will be stopped as well so
47:15 - if I do Docker PS again I see that no
47:18 - container is running so there is an
47:21 - option for Docker run command that make
47:26 - makes it able
47:27 - makes it possible to run the container
47:29 - in a detached mode and that is minus D
47:32 - so if I do dock run minus D
47:35 - redis I will just get the ID of the
47:38 - container as an output and the container
47:40 - will stop running so if we check again
47:43 - Docker PS I see the container with the
47:46 - ID starting with 838 which is the same
47:50 - thing here is running so this is how you
47:54 - can start it in the detached mode now
47:57 - for example if you would want to restart
48:00 - a container because I don't know some uh
48:02 - the application crashed inside or some
48:05 - error happened so you want to restart it
48:08 - you would need the doc container ID so
48:11 - just the first part of it not the whole
48:15 - string and you can simply say Docker
48:18 - stop ID of the container and that will
48:21 - stop the docker container nothing
48:24 - running if you want to start it again
48:27 - you can use the same
48:30 - ID to start
48:36 - again so let's say you stop Docker
48:41 - container at the end of the day you go
48:44 - home you come back the next day open
48:46 - your laptop and you want to restart the
48:49 - stopped container right so if you do
48:51 - Docker PS there is uh the output is
48:54 - empty you don't see any containers so
48:56 - what you can do alternative to just
48:59 - looking up your history command line
49:01 - history is you can do Docker
49:04 - PS minus a which will show you all the
49:08 - containers which uh are running or not
49:11 - running so here you see the container ID
49:15 - again and you can restart
49:20 - it okay so let's try another thing let's
49:23 - say you have two parallel applications
49:26 - that both use redish but in different
49:30 - versions so you would need two redish
49:32 - containers with different image versions
49:36 - running on your laptop right at
49:38 - different times maybe or at the same
49:40 - time so here we have the latest one
49:44 - which is radius
49:47 - 56 and let's head over to the dockerhub
49:51 - and select uh version let's say you need
49:54 - version 4
49:57 - o so remember the first time that we
50:01 - downloaded the redis image we did Docker
50:03 - pull redis um however if you run Docker
50:08 - if you use Docker
50:09 - run with redice image and the tech which
50:13 - was
50:14 - 4.0 it will pull the image and start the
50:18 - container right away after it so it does
50:21 - two commands basically in one so it's
50:24 - Docker pole that Docker start in one one
50:26 - command so if I do this it says it can't
50:30 - find the image locally so it goes and
50:33 - pulls the image from the repository to
50:37 - my
50:39 - laptop and again we see some layers are
50:42 - downloaded and the container is started
50:45 - right away and now if I do Docker
50:49 - PS you see that I have two radices
50:53 - running so this is where it gets
50:56 - interesting
50:57 - now how do you actually use any
51:01 - container that you just started so in
51:03 - this output we you also see the ports
51:06 - section which specifies on which Port
51:10 - the container is listening to the
51:13 - incoming requests so both containers
51:16 - open the same port which is what was
51:19 - specified in the image
51:22 - so in the logs of the container you can
51:26 - see the information running mode stand
51:28 - loone Port
51:31 - 6379 so how does that actually work and
51:34 - how do we not have conflicts while both
51:37 - are running on the same port so to
51:39 - explain that let's head over to our
51:41 - slide and see how this works as you know
51:45 - container is just the virtual
51:47 - environment running on your host and you
51:50 - can have multiple containers running
51:52 - simultaneously on your host which is
51:54 - your laptop PC whatever you working on
51:58 - and your laptop has certain ports
52:01 - available that you can open for certain
52:04 - applications so how it works is that you
52:07 - need to create a so-called binding
52:10 - between a port that your laptop your
52:13 - host machine has and the container so
52:17 - for example in the first container part
52:20 - here you see container is listening on
52:23 - Port 5000 and you bind your laptops Port
52:28 - 5,000 to that containers now you will
52:31 - have conflict if you open two 5,000
52:36 - ports on your host because you will get
52:38 - a message the port is already bound or
52:41 - is already in use you can do that um
52:46 - however you can have two containers as
52:48 - you see the second and third containers
52:51 - are both listening on Port 3000 which is
52:54 - absolutely okay as long as you bind them
52:58 - to two different ports from your host
53:01 - machine so once the port binding between
53:04 - the host and the container is already
53:06 - done you can actually connect to the
53:08 - running container using the port of the
53:11 - host so in this example URI you would
53:14 - have some app Local Host and then a port
53:17 - of the host and the host then will know
53:21 - how to forward the request to The
53:23 - Container using the port binding
53:27 - so if we head
53:29 - back here you see that containers have
53:33 - their ports and they're both running on
53:35 - the same one however we haven't made any
53:38 - binding between my laptop's port and the
53:41 - container port and because of that the
53:44 - container is basically unreachable by
53:46 - any application so I won't be able to
53:48 - use it so the way we actually do that is
53:53 - by specifying The Binding of the ports
53:56 - during the Run command so I'm going to
53:59 - break this and check that there is just
54:02 - one container running now I'm going
54:05 - to stop the other one as well so we can
54:08 - start the
54:09 - menu okay so we see both containers are
54:15 - here so now we want to start them using
54:18 - The Binding between the host and the
54:22 - container ports but again we have two
54:25 - Rices so we need to bind them to two
54:28 - different ports on my laptop so the way
54:31 - to do it is you do Docker run and you
54:35 - specify with minus P the port of the
54:39 - host that's the first
54:41 - one so let's go with 6,000 it doesn't
54:45 - really matter in this case and the
54:48 - second one is the port that you're
54:50 - binding this two which is the container
54:53 - Port so we know the container Port will
54:56 - be
54:57 - 6379 and this is where we bind our so my
55:02 - laptop's port
55:05 - 60002 and if I do this I forgot ra
55:10 - here so and now if I do Docker
55:16 - PS let's actually clean this Docker PS
55:20 - again here you see The Binding here all
55:24 - right so your laptops 6,000 Port is
55:28 - bound to the containers 6 37 9
55:34 - so now let's do another thing and let's
55:38 - start it in a detached
55:40 - mode so like this let's check
55:45 - again it's running again and now I want
55:48 - to start the second container let's
55:51 - clear this again
55:59 - so here you see it created a bunch of
56:01 - containers because uh when I specified
56:05 - different options with the port binding
56:06 - it actually created new containers um
56:09 - that's why you see a couple of more here
56:12 - so I'm going to copy the image name with
56:15 - the tag for uh.
56:18 - o minus
56:20 - P so for example if I were to do this
56:24 - now
56:26 - and I would try to
56:29 - run the other
56:32 - redis the second redis container with
56:35 - the same port on my laptop I would get
56:39 - an error saying Port is already
56:42 - allocated so I can do
56:45 - 60001 and run it again I'll run it in
56:49 - detached mode so that I'm see
56:53 - port and if I go go over here and say
56:57 - Docker PS I see that I have two
57:01 - different radi versions running both of
57:04 - them bound to different ports on my
57:07 - laptop and the containers themselves
57:11 - listening to request on the same
57:17 - port so so far we have seen a couple of
57:20 - basic Docker commands we have seen
57:22 - Docker pull which pulls the image from
57:24 - the repository to local environment we
57:28 - also saw run which basically combines
57:31 - Docker pull and Docker start pulls the
57:34 - image if it's not locally available and
57:36 - then starts it right away then we saw
57:39 - Docker start and Docker stop which makes
57:43 - it possible to restart a container if um
57:46 - you made some changes and you want to um
57:49 - create a new version which makes
57:51 - possible to restart a container if you
57:52 - need to um we also saw docu run with
57:56 - options the one option that we saw was D
58:00 - minus D which is detach so you can run
58:03 - the container in DET detached mode so
58:05 - you can use terminal again minus P
58:09 - allows you to bind Port of your host to
58:12 - The Container so very important to
58:15 - remember minus P then comes the port of
58:18 - your host and then comes the port of
58:21 - your um container whatever it might be
58:26 - we also saw doap PS doap PS minus a
58:31 - which basically gives you all the
58:33 - containers no matter if they're running
58:34 - currently or not we also saw Docker
58:37 - images which gives you all the images
58:40 - that you have um locally so for example
58:43 - if after a couple of months you decide
58:46 - to clean up your space and get rid of
58:49 - some stale images you can actually check
58:51 - them check the list and then go through
58:53 - them and uh delete them you can do the
58:56 - same with stale Docker containers that
58:58 - you don't use anymore or you don't need
59:00 - anymore you can also get rid of them so
59:02 - the final part of the docker basic
59:04 - commands are commands for
59:06 - troubleshooting which are very very
59:08 - useful if something goes wrong in the
59:09 - container you want to see the logs of
59:12 - the container or you want to actually
59:14 - get inside of container get the terminal
59:17 - and execute some commands on it so let's
59:20 - see Docker PS we have two containers
59:23 - running right now we don't have any out
59:26 - we don't see any locks here so let's say
59:29 - something um happens your application
59:31 - cannot connect to redies and you don't
59:33 - know what's happening so ideally you
59:36 - would want to see what logs radi
59:40 - container is producing right the way to
59:42 - do that is very easy you just say Docker
59:44 - locks and you specify the container ID
59:48 - and you see the locks you can also do
59:51 - the lock locks if you don't want to uh
59:54 - remember the container ID or to Docker
59:57 - PS all the time you can remember the
59:59 - name of the container and you can get
60:02 - the logs using the
60:04 - name so a little side note here um as
60:07 - we're talking about the names of the
60:09 - containers so here as you see when a
60:11 - container is created you just get some
60:13 - random name like this so you can name
60:16 - your containers as you want um using
60:19 - another option of the docker run which
60:21 - might be pretty useful sometimes if you
60:23 - don't want to work with the container
60:25 - IDs and you just want to remember the
60:27 - names um or if you just want to
60:29 - differentiate between the containers so
60:31 - for example let's create a new container
60:35 - from r4.0 image using a different name
60:39 - that we choose so I'm going to stop this
60:44 - container and I'm going to create a new
60:47 - one from the same image so let's run it
60:50 - in the detached mode Let's Open the
60:54 - port th000 1 2 6 3 7
61:01 - 9 and give the name to the container and
61:05 - let's call since it's the older um
61:08 - version let's call it red
61:12 - older and we need to specify the image
61:15 - so remember this will create a new
61:17 - container since we're running the docker
61:21 - one command again so if we execute this
61:25 - and check again
61:27 - we see the redis 4.0 image based
61:32 - container is created which is um a fresh
61:36 - new you can see in it created and the
61:38 - name is red is older and we can do the
61:40 - same
61:43 - for the other container so that we kind
61:46 - of know which uh container is what so
61:50 - I'll stop this
61:53 - one and I will use the same command here
61:57 - this will be the latest and I will call
62:00 - this radius
62:02 - latest and since find another Port so
62:07 - I'm going to run it and let's
62:10 - see so here I have two containers
62:13 - running now I know R is older R is
62:16 - latest so for example if the older
62:19 - version has some problems I can just do
62:22 - logs R is older and I can get get my
62:26 - locks so another very useful command in
62:30 - debugging is Docker exit so what we can
62:34 - do with Docker exit is we can actually
62:37 - get the terminal of a running container
62:40 - so let's check again we have two
62:41 - containers running and let's say there
62:43 - is some problem with the latest ready
62:48 - latest container and I want to get a
62:50 - terminal of that container and to maybe
62:54 - navigate a directory inside check uh the
62:57 - lock file or maybe check the
62:58 - configuration file or uh print out the
63:01 - environmental variables um whatever so
63:04 - in order to do that we use Docker exit
63:07 - command with minus t which stands for
63:10 - interactive terminal then I specify the
63:13 - container
63:16 - ID and I say in so I get the B and here
63:22 - you see that the the cursor changed so
63:25 - I'm in inside of the container as a root
63:29 - user and here if I say LS okay the data
63:32 - is empty I can also print out in which
63:35 - directory I am I can go to home
63:37 - directory see what's there um so I have
63:40 - my virtual file system inside of a
63:43 - container and here I can um navigate the
63:46 - different directories and I can check
63:48 - stuff I can also print all the
63:50 - environmental variables to see that
63:52 - something is set correctly um and do all
63:55 - kinds of stuff
63:57 - here and this could be really useful if
64:01 - you have a container with some complex
64:04 - configuration or if for example you are
64:07 - running your own application that you
64:08 - wrote in a container H and you have some
64:12 - complex configuration there um or some
64:14 - kind of setup and you want to validate
64:17 - that everything um is correctly set in
64:20 - order to exit the terminal you just do
64:24 - exit and you're out
64:26 - you can also do the same using the name
64:29 - again if you don't want to work with the
64:31 - IDS and you just want to remember the
64:33 - names of the container to make it easier
64:36 - you can do it with the name as well same
64:38 - thing since most of the container images
64:42 - are based on some lightweight Linux
64:44 - distributions you wouldn't have much of
64:46 - their Linux um commands or applications
64:50 - installed here for example you wouldn't
64:52 - have a curl or some other stuff so you
64:55 - were a little bit more limited in that
64:57 - sense so you can execute a lot of stuff
65:00 - from the docker containers for most of
65:03 - the debugging work um it should be
65:05 - actually enough so the final part to
65:08 - review the difference between Docker run
65:10 - and Docker start which might be
65:12 - confusing for some people let's revisit
65:14 - them so basically Docker run is where
65:18 - you create a new container from an image
65:22 - so Docker run will take an image with a
65:25 - specific version or just latest right as
65:29 - option or as an attribute with Docker
65:32 - start you not working with images but
65:35 - rather with containers so for example um
65:38 - as we saw Docker run has a lot of
65:40 - options you specify with minus D and
65:44 - minus P the port binding and then you
65:46 - have this name of the container and all
65:48 - this stuff so basically you tell Docker
65:52 - at the
65:53 - beginning what kind of container with
65:56 - what attributes name and so on to create
65:58 - from a specific image but once the
66:02 - container is
66:03 - created and you can see that using a con
66:07 - uh the command so for example here the
66:09 - last ones that we
66:10 - created and if you stop it and you want
66:13 - to restart it you just need to use the
66:16 - command do start and specify the
66:19 - container ID and when you start it the
66:22 - container will retain all the attributes
66:25 - that we defined when creating the
66:27 - container using Docker run so Docker run
66:32 - is to create a new container Docker
66:34 - start is to restart a stopped
66:41 - container so once you've learned the
66:43 - docker basic concepts and understood how
66:45 - it works uh it's important to see how
66:48 - Docker is actually used in practice so
66:50 - in software development workflow you
66:52 - will know you have this uh classical
66:55 - steps of development and continuous
66:57 - delivery or continuous integration uh
67:00 - and then eventually gets deployed on
67:02 - some environment right could be a test
67:03 - environment develop environment so it's
67:05 - important to see how Docker actually
67:08 - integrates in all those steps so in the
67:11 - next couple of videos I'm going to
67:13 - concentrate exactly on that so we're
67:14 - going to see some overview of the flow
67:17 - and then we're going to zoom in on
67:19 - different parts and see how Docker is
67:21 - actually used in those individual steps
67:26 - so let's consider a simplified scenario
67:28 - where you are developing a JavaScript
67:30 - application on your laptop right on your
67:33 - local development
67:34 - environment your JavaScript
67:37 - application uses a mongodb database and
67:41 - instead of installing it on your laptop
67:44 - you download a Docker container from the
67:46 - docker Hub so you connect your
67:49 - JavaScript application with the mongodb
67:52 - and you start developing so now let's
67:54 - say you develop the application first
67:57 - version of the application locally and
67:59 - now you want to test it or you want to
68:01 - deploy it on the uh development
68:04 - environment where a tester in your team
68:07 - is going to test it so you commit your
68:10 - JavaScript application in git or in some
68:13 - other version control system uh that
68:16 - will trigger a continuous um integration
68:20 - a Jenkins build or whatever you have
68:22 - configured and Jenkins build will
68:24 - produce artifacts from your application
68:28 - so first it will build your JavaScript
68:31 - application and then create a Docker
68:34 - image out of that JavaScript artifact
68:38 - right so what happens to this Docker
68:40 - image once it gets created by Jenkins
68:43 - build it gets pushed to a private Docker
68:47 - repository so usually in a company you
68:50 - would have a private repository because
68:52 - you don't want other people to have
68:54 - access to your image images so you push
68:56 - it there and now as a Next Step could be
69:00 - configured on Jenkins or some other
69:03 - scripts or tools that Docker image has
69:07 - to be deployed on a development server
69:10 - so you have a development server that
69:12 - pulls the image from the private
69:15 - repository your JavaScript application
69:17 - image and then pulls the mongodb that
69:21 - your JavaScript application depends on
69:23 - from a dockerhub and now you have two
69:25 - containers one your custom container and
69:29 - a publicly available mongodb container
69:32 - running on dev server and they talk to
69:34 - each other you have to configure it of
69:36 - course they talk and communicate to each
69:38 - other and run as an app so now if a
69:42 - tester for example or another developer
69:45 - logs in to a Dev server they be they
69:49 - will be able to test the
69:51 - application so this is a simplified
69:54 - workflow how Brer will work in a real
69:56 - life development process in the next
69:59 - videos I'm going to show you Hands-On
70:02 - demo of how to actually do all this in
70:08 - practice so in this video we are going
70:10 - to look at some practical example of how
70:12 - to use Docker in a local development
70:14 - process so what we're going to do is a
70:17 - simple demo of a JavaScript and nodejs
70:19 - application in the back end to simulate
70:22 - the local development process and then
70:24 - we're going to connect it to a Docker
70:25 - container with a mongodb database in it
70:28 - so let's get
70:32 - started so in this video we're going to
70:34 - see how to work with Docker containers
70:37 - When developing applications so the
70:39 - first step will be is we're going to
70:41 - develop a very simple UI backend uh
70:45 - application using JavaScript very simple
70:48 - HTML structure and nodejs in the back
70:50 - end and in order to integrate all of
70:53 - this in the database we are going to use
70:55 - a Docker container of a mongodb database
70:58 - and um also to make working with the
71:01 - mongodb much easier so we don't have to
71:04 - execute commands in in the terminal
71:06 - we're going to deploy a Docker container
71:09 - of a UI which is called the
71:11 - Express where we can see the database
71:13 - structure and all the updates that our
71:15 - application is making in the
71:19 - database so this development setup
71:21 - should give you an idea of um how Docker
71:24 - containers are actually used in
71:26 - development
71:30 - process so I've already prepared some
71:32 - very simple JavaScript application um so
71:36 - in order to see the code basically we
71:38 - have this index HTML that is very simple
71:42 - code and we have some JavaScript here
71:44 - and we're using nodejs backend that just
71:46 - serves that index HTML file and listens
71:49 - on Port 3000 so we have the server
71:52 - running here in the back end and we have
71:54 - the UI that looks like this so basically
71:57 - it's just a user profile page with some
71:59 - user information and user can edit their
72:02 - profile so if I for example change the
72:04 - name here um and if I change the email
72:10 - address and do changes like this I can
72:12 - save my profile and I have my updates
72:15 - here um however if I refresh the page of
72:19 - course the changes will be lost because
72:21 - it's just JavaScript no JS so there is
72:23 - no persistent compon component in this
72:25 - application so in order to have that
72:28 - which is actually how real life
72:30 - applications work you'll know that you
72:32 - need to integrate the application with a
72:34 - database so using that example I will
72:37 - try to Showcase you how you can actually
72:39 - use the docker containers to make the
72:41 - development process Easier by just
72:44 - pulling one of the databases and
72:45 - attaching it or connecting it to the
72:48 - application so in this case we're going
72:49 - to go with the mongodb application and
72:53 - uh in addition to mongodb contain
72:55 - container we're going to also deploy a
72:57 - mongodb UI which is its own container
73:00 - it's called Express where we can
73:02 - manage or see the database insights and
73:06 - updates from our application much easier
73:09 - so now let's see how that all
73:15 - works so in order to get started let's
73:17 - go to dockerhub and find our uh mongodb
73:21 - image
73:25 - so here let's go to
73:27 -  and we have mongodb
73:32 - here actually and the Express
73:35 - which is another dock container that
73:37 - we're going to use for the UI so first
73:40 - let's pull the mongodb official
73:50 - image so I I already have mongodb latest
73:53 - so pulling doesn't take longer on my
73:56 - laptop but you're going to need a couple
73:59 - of seconds probably and the next one
74:02 - we're going to pull is the docker
74:04 - Express which I also have I believe so
74:07 - let's
74:09 - see yes it's also fast so if I check
74:14 - locally I have mongod TB and
74:17 - Express images so the next step is to
74:20 - run both and Express uh
74:23 - containers in order to make the mongod
74:26 - DB database available for our
74:28 - application and also to connect the
74:31 -  Express with the mongod DB
74:33 - container so let's do the connection
74:36 - between those two
74:40 - first in order to do that we have to
74:42 - understand another Docker concept Docker
74:45 - Network so how it works is that Docker
74:47 - creates its isolated Docker
74:51 - Network where the containers are running
74:53 - in so so when I deploy two containers in
74:57 - the same docken network in this case
75:00 -  and Express they can talk to
75:03 - each other using just the container name
75:06 - without Local Host port number Etc just
75:09 - the container name because they're in
75:11 - the same
75:13 - network and the applications that run
75:16 - outside of doer like our nodejs which
75:18 - just runs from node server is going to
75:21 - connect to them from outside or from the
75:24 - host using Local Host and the port
75:27 - number so later when we package our
75:30 - application into its own Docker image
75:33 - what we're going to have is again Docker
75:36 - network with mongodb container
75:38 - Express container and we're going to
75:40 - have a nodejs application which we wrote
75:44 - including the index HTML and JavaScript
75:47 - for front end in its own doc container
75:50 - and it's going to connect to the mongodb
75:52 - and the browser which is running on the
75:55 - host but outside the docker network is
75:58 - going to connect to our JavaScript
76:00 - application again using host name and
76:02 - the port number so Docker by default
76:05 - already provides some networks so if we
76:08 - say Docker
76:10 - Network LS we can already see these
76:14 - autogenerated Docker networks so we have
76:16 - four of them with different names and
76:18 - the drivers we're not going to go into
76:20 - details here but what we're going to do
76:22 - is create its own network for the
76:25 - mongodb and the Express and we're
76:28 - going to call it Network so let's
76:31 - do this right
76:32 - away going to say Docker Network create
76:37 - and we are going to call it
76:42 - Network so now if I do dock
76:46 - Network LS again I see my docken network
76:50 - has been created so now in order to make
76:54 - our m B container and the Express
76:57 - container run in this Network we
77:00 - have to provide this network option when
77:03 - we run the container in the docker run
77:07 - command so let's start with the so
77:11 - we all know that Docker run is the
77:13 - command to start a container from an
77:15 - image right so we have Docker run
77:17 - which is the basic Docker run command
77:20 - however in this case we want to specify
77:22 - a couple of things um as you learned
77:24 - from the previous videos you have to
77:27 - specify something called Port so we need
77:30 - to open a port of mongodb the default
77:34 - Port of mongodb is
77:37 - 27,7 so we will take that Port actually
77:41 - for both host and container so
77:43 - will run at this port inside of a
77:46 - container and we open the same port on
77:48 - the host so that will take care of the
77:50 - port then we will run it in a detach
77:53 - mode in addition to that there are a
77:55 - couple of things that we can specify
77:57 - when starting up the container and these
77:59 - are environmental variables of the
78:01 - mongot TB let's see um in the official
78:05 - image description you actually have
78:07 - couple of documentation about how to use
78:09 - the image which is very helpful to kind
78:12 - of understand what kind of configuration
78:14 - you can uh apply to it here you see some
78:17 - environmental variables so basically on
78:20 - Startup you can Define what the root
78:22 - username and the password will be which
78:25 - is very um handy because we're going to
78:27 - need those two for the express to
78:30 - connect to the and you can also
78:32 - specify the init database we're just
78:35 - going to provide the username and
78:36 - password because we can create the
78:38 - database from the Express UI later
78:40 - so let's do that and the way you can
78:43 - specify the environmental variables you
78:45 - can actually see here as well is by
78:50 - just let's copy this one so here you say
78:55 - environmental variable that's what the
78:57 - minus E Flex stands for and root
79:00 - username we'll say it's admin
79:04 - and another variable which is the
79:07 - password will be just password so in
79:10 - this way we can actually overwrite what
79:13 - the default username and password will
79:15 - be so two more things that we need to
79:17 - configure in this uh command our
79:20 - container name because we're going to
79:24 - need that container name to connect with
79:26 - the Express so we'll call this one
79:29 -  DB let's say and another one we
79:34 - need is the
79:36 - network that we created which was called
79:39 -  Network so in order to make this
79:42 - command a little bit more structured do
79:44 - it on multiple lines so let's
79:50 - see so it's more readable so basically
79:54 - all the
79:55 - options or all these flags that we set
79:58 - um to go one more time through them it
80:01 - it's going to start in detached mode uh
80:04 - we are opening the port on the host um
80:07 - username and password that we want
80:10 - mongodb to use uh in the startup process
80:13 - we're going to rewrite or overwrite the
80:16 - name of the container and this container
80:18 - is going to run in a
80:20 - Network and this should actually start
80:23 - the container
80:27 - okay so if you want to see whether it
80:30 - was successful we can log the container
80:33 - and see what's happening
80:35 - inside so as we see was started
80:39 - and everything actually looks good
80:43 - waiting for connections on Port
80:46 - 27,7 okay so now let's start
80:50 - Express we want Express to connect
80:53 - to the running mongod DB be container on
80:56 - Startup and here we have an example of
80:59 - how to run it and here we have the list
81:01 - of environmental variables that we can
81:03 - configure so let's quickly look at them
81:06 - username password we don't need
81:08 - them however we need the admin username
81:12 - and admin password of the motb this is
81:14 - actually what we overwrote with admin
81:16 - and password so we're going to use them
81:19 - because Express will need some username
81:21 - password to authenticate with the
81:23 - mongodb and to connect it the port is by
81:25 - default the correct one so we don't need
81:27 - to change that um and this is an
81:30 - important part this is the mongodb
81:32 - server right so basically this is the
81:35 - container name that Express will use to
81:39 - connect to the docker and because they
81:41 - running in the same network only because
81:44 - of that this configuration will work if
81:47 - I didn't if I hadn't specify the network
81:50 - then I could have I could specify the
81:53 - name correct name here of the container
81:55 - but it wouldn't work so with that said
81:57 - let's actually create the docker run
82:00 - command for Express as
82:03 - well so let's clear the history and
82:07 - let's start so again we run it in
82:09 - detached mode and let's see what
82:11 - parameters we need so first of all Port
82:15 - let's say what is the default Port that
82:17 - the express runs on that's 80
82:20 - 81 so we'll take that so basically it's
82:24 - going to run on our laptop on Port
82:27 - 8081 the next option would be these two
82:32 - and remember environmental variables
82:34 - need to be specified with minus E and
82:37 - this is the username of mongodb admin
82:40 - which is admin because we specified it
82:44 - when we started the mongodb container
82:46 - this is the password let's set this one
82:49 - as
82:50 - well don't forget the network minus
82:53 - minus net
82:59 - Network we have the name we can also
83:02 - call it
83:07 - Express and let's see what else we might
83:11 - need here yes this is important one
83:16 - um and our container name let's actually
83:21 - see it again toer PS the one running
83:25 - it's called mongodb that's the container
83:27 - name and this is what we need to specify
83:29 - here so I'm going to write this here and
83:32 - finally the image is called
83:36 - Express so I'm just going to copy this
83:37 - one here
83:39 - and that is it so basically with these
83:43 - commands do Express should be able
83:45 - to connect to the mongod Deb container
83:48 - so let's run it and just to make sure
83:52 - let's log the container and see what's
83:54 - happening
83:56 - there waiting for mongodb welcome to
83:59 -  express it looks like it connected
84:02 - successfully um it says here database
84:04 - connected and the Express is
84:07 - available at Port
84:10 - 8081 so let's
84:13 - check the Express out at the Port
84:17 - 881 so actually let's close these tabs
84:20 - we don't need them anymore and here if I
84:23 - say Local Host
84:24 - 881 I should be able to see the manga
84:28 - Express so these are the databases that
84:31 - already exist by default in or
84:33 - which are created on Startup and using
84:36 - the UI we can create our own database as
84:39 - we saw previously we could have
84:41 - specified an environmental variable init
84:43 - DB on mongodb Startup and that would
84:46 - have created a new database however it
84:49 - doesn't matter we will just create a
84:51 - database name here so we will call it
84:55 - user um
84:57 - account database so let's create one and
85:01 - now we can actually use it or connect to
85:05 - this database from node.js
85:07 - so let's see how that
85:12 - works so now we have the mongodb
85:15 - container and the Express
85:17 - container running so let's check that we
85:20 - have both of them we'll have to connect
85:23 - nodejs with the database so the way to
85:25 - do it is usually to give a protocol of
85:28 - the database and the URI and the URI for
85:31 - a mongodb database would be Local Host
85:33 - and the port that it's accessible at I
85:37 - already went ahead and prepared the code
85:39 - for node.js so basically we are going to
85:42 - use a client here which is a node
85:44 - module and using that client we
85:47 - are connecting to the mongodb database
85:51 - so this is the protocol the host and the
85:54 - port that we just saw that the mongodb
85:57 - is listening at and username and
86:00 - password of the root user of mongodb of
86:03 - course usually you wouldn't put the
86:05 - password here or not use an admin or
86:07 - root uh username password to connect to
86:10 - a database but this is for just the
86:12 - demonstration purposes and these are
86:14 - username and password that we set as
86:17 - environmental variables when we created
86:19 - the docker mongodb container so
86:24 - let's check that so this is the mongodb
86:26 - uh container command and this is the
86:29 - username root and root password that we
86:33 - specified and this is what we are going
86:35 - to use in the code as I said for
86:38 - demonstration purposes I will write the
86:40 - password directly here so then we
86:43 - connect to the database um so I also
86:46 - went ahead and in the Express user
86:49 - account database and inside that I
86:52 - created collection which is like a table
86:54 - in my SQL world called users so here I
86:58 - connect to user account database and I
87:01 - query The Collection users and this is a
87:04 - get request so I'm just fetching
87:06 - something from the database and this is
87:08 - update uh request same thing I connect
87:12 - to the database using the same URI and
87:14 - the database name and I update or insert
87:18 - something in the collection so now let's
87:20 - see how all that works so let's head
87:22 - over to the UI so in the users
87:24 - collection there is no data it's empty
87:28 - so we're going to refresh it and edit
87:30 - the data so I'm going to write here
87:34 - some and update
87:37 - and refresh we see that a new insert was
87:41 - made so this is the update profile
87:45 - section here so all this was executed it
87:48 - connected to the mongodb and now we have
87:51 - one entry which is email
87:54 - coding name that we changed so if I'm
87:57 - going to refresh it now I fetched the
87:59 - newly inserted user data in the UI and I
88:02 - displayed it here and also if you want
88:04 - to see what the mongod container
88:05 - actually logs during this process we can
88:08 - actually look at the logs so I'm going
88:10 - to say docks and log using the container
88:14 - ID so let's say if I wanted to see just
88:18 - the last part of it because I want to
88:20 - see what the last activity was I can
88:22 - also let's clear this
88:25 - and I can also do tail so I can just
88:28 - display the the last part of it or if I
88:31 - want it I could also stream the logs so
88:34 - I'll clear this again and I'll
88:37 - say stream the logs so I want have to do
88:40 - dockal logs all the time so if I make a
88:42 - line here for example to Mark the last
88:45 - logs I can refresh it let's make some
88:49 - other changes let's change it to
88:52 - own and
88:55 - save profile so I'm going to see some
88:58 - activity here as well so these
89:00 - connections are new and it also says
89:03 - received client metadata and this is
89:05 - where the nodejs request comes in with
89:08 - the nodejs and its
89:09 - version and at the end of each
89:12 - communication there is an end connection
89:14 - because we end the database connection
89:17 - at the end so we see that also in the
89:20 - logs so if for example something wasn't
89:22 - working properly you could always check
89:24 - them in the logs
89:26 - here so with that I have a fully
89:29 - functional JavaScript nodejs application
89:31 - which has a persistence in the mongodb
89:33 - database and we also have uh UI
89:36 - both of them running in a Docker
89:38 - container so this would be uh somehow an
89:41 - realistic example of how local
89:44 - development using Docker containers
89:46 - would look
89:51 - like so in the last video we created and
89:54 - started two Docker containers mongodb
89:57 - and Mong Express and these are the
89:59 - commands that we used to make it happen
90:01 - right the first we created a network
90:04 - where these two containers can can talk
90:06 - to each other using just the container
90:08 - name and no host Port Etc is necessary
90:11 - for that um and then we actually ran two
90:15 - Docker run commands with all the options
90:18 - and environmental variables Etc set now
90:22 - uh this way of starting containers all
90:26 - the time is a little bit tedious and you
90:28 - don't want to execute these run commands
90:30 - all the time on the command line
90:32 - terminal especially if you have a bunch
90:34 - of Docker containers to run you probably
90:37 - want to automate it or just make it a
90:39 - little bit easier and there's a tool
90:42 - that's that makes running multiple
90:44 - Docker containers with all this
90:46 - configuration much easier than with
90:49 - Docker run commands and that is Docker
90:51 - compose if you already know Docker comp
90:54 - post and you are wondering why is it
90:56 - useful and what it actually does then
90:58 - bear with me in the next slide I'm going
91:00 - to explain
91:02 - that so this is a Docker run command of
91:05 - the mongodb that we executed
91:07 - previously so basically with Docker
91:10 - compose file what we can do is we
91:13 - can take the whole command with its
91:17 - configuration and map it into a file so
91:20 - that we have a structured commands so if
91:23 - you have have let's say 10 Docker
91:25 - containers that you want to run for your
91:27 - application and they all need to talk to
91:29 - each other and interact with each other
91:31 - you can basically write all the Run
91:33 - commands for each container in a
91:35 - structured way in the docker compos and
91:37 - we'll see how that structure actually
91:39 - looks like so on the right side in the
91:42 - docker compos example the first two tags
91:46 - are always there right version three
91:49 - that's the latest version of the compose
91:52 - Docker compose and then we have the
91:54 - services this is where the container
91:56 - list goes so the first one is mongodb
91:59 - and that Maps actually to The Container
92:01 - name right this is going to be a part of
92:03 - container name when Docker creates a
92:05 - container out of this configuration
92:08 - blueprint the next one is actually the
92:13 - image right so we need to know which
92:15 - image that container is going to be
92:17 - built from and of course you can specify
92:19 - a version tag here um next to the name
92:23 - the next one one is Port so we can also
92:26 - specify which ports is going to open
92:29 - first one is on the host and the second
92:31 - one after the colum is on the container
92:35 - so the port mapping is there and of
92:38 - course the environmental variables can
92:41 - be also mapped in the docker compose and
92:43 - this is how actually the structure of
92:46 - Docker compose looks like for one
92:48 - specific commands let's actually see the
92:50 - second container command for
92:52 - Express that we executed it and how to
92:55 - map that so now again we have a Docker
92:58 - run command for Express and let's
93:01 - see how we can map it into a Ducker
93:02 - compose so as I said services will list
93:06 - the containers that we want to create
93:09 - and
93:10 - again names Express will map map
93:13 - to The Container name the next one will
93:16 - be the image again you can add a tag
93:19 - here if you want to be um have a
93:22 - specific one then you have the ports 80
93:26 - to
93:28 - 8080 and then you have all the
93:30 - environmental variables again under the
93:33 - attribute environment and this is how
93:36 - the docker compose will look like so
93:39 - basically Docker compos is just a
93:41 - structured way to contain very normal
93:45 - common Docker commands and of course
93:47 - it's it's going to be easier for you to
93:49 - edit the the file uh if you want to
93:52 - change some variables s if you want to
93:54 - change the ports or if you want to add
93:56 - some new options to the Run command so
93:59 - to say and maybe you already noticed the
94:03 - network configuration is not there in
94:05 - the docker compost so this Network
94:07 - that we created we don't have to do it
94:09 - in a Docker compose we go to the next
94:13 - slide because we have the same concept
94:16 - here we have containers that will talk
94:19 - to each other using just the container
94:21 - name so what docker compose will do is
94:25 - actually take care of creating a common
94:27 - Network for these containers so we don't
94:31 - have to create the network and specify
94:33 - in which network these containers will
94:36 - run in and we're going to see that in
94:38 - action right
94:43 - away so let's actually create a Docker
94:46 - compost file so I'm going to paste all
94:50 - my contents here and this is exactly
94:52 - what we saw on the slide
94:55 - and I'm going to save it as a
95:00 - yl and we see the highlighting as well
95:03 - be very aware of the indentation they
95:06 - have to be correct so this is the list
95:08 - of all the containers on the same level
95:12 - and then each container has its
95:14 - configuration inside
95:16 - that so now compared to Docker run
95:19 - commands it will be very easy for me to
95:21 - go here and change these environment
95:23 - variables or add some new configuration
95:26 - options
95:28 - Etc so here again for demonstration we
95:31 - actually save the docker compose in the
95:33 - code so it's part of the application
95:36 - code so now that we have a Docker
95:38 - compose file the question is how do I
95:41 - use it or how do I start the containers
95:43 - using that so let's go to the command
95:45 - line and start Docker containers using
95:49 - this Docker compose file so the way to
95:52 - use it is using Docker
95:55 - compose command now if you've installed
95:59 - Docker on your laptop it usually gets
96:02 - installed with the docker compose
96:05 - packaged inside so you should have both
96:08 - Docker and Docker compose commands
96:10 - installed as a package so Docker compos
96:13 - command takes an argument which is the
96:15 - file so I'm going to specify which file
96:18 - I want to execute and in my case it's
96:21 - called yl and and at the end I
96:25 - want to say what I want to do with this
96:27 - file in this case the command is up
96:30 - which will start all the containers
96:32 - which are in the yl so let's
96:35 - actually check before that there there
96:37 - are no containers running so I don't
96:39 - have anything running here and I'm going
96:41 - to start those two
96:52 - containers okay so there are couple of
96:54 - interesting things here in this output
96:57 - so let's scroll all the way up so we've
96:59 - talked about Docker Network and how we
97:02 - created our own network at the beginning
97:04 - to run the containers inside and I said
97:07 - the docker compos takes care of it and
97:08 - here we see the output where it actually
97:12 - created a network called my app default
97:16 - this is the name of the network and it's
97:18 - going to run those two containers these
97:21 - are actually the names of the containers
97:23 - the do compos created this is what we
97:26 - specified and it just added prefix and
97:29 - suffix to it and it created those two
97:31 - containers uh in that Network so if I
97:34 - actually go here and do Docker Network
97:38 - LS I see the my app default is
97:43 - here so that's one important thing
97:46 - another one is that logs of both
97:49 - containers actually mixed because we are
97:52 - starting both at the the same time as
97:55 - you see the Express has to wait
97:58 - for mongod DB to start because it needs
98:01 - to establish a connection so we here see
98:05 - the locks so mongodb is
98:08 - starting we still get connection refus
98:11 - because it's not started uh completely
98:14 - and somewhere here when mongodb is
98:17 - started and listening for connections
98:19 -  Express is able to connect to it
98:22 - so this is something that you can also
98:24 - do with Docker compose uh when you have
98:27 - two containers that where one depends on
98:30 - another one starting you can actually
98:32 - configure this waiting logic in the
98:35 - docker compos okay so now let's see
98:39 - actually that the docker containers are
98:42 - running so we have both of them here you
98:44 - see the container names that Docker
98:46 - compos gave them and one thing here to
98:50 - note is that the Express actually
98:54 - started on Port 8081 inside the
98:57 - container so we can see that here so we
99:01 - are opening a port 8080 on my laptop
99:05 - that actually forwards the request to
99:07 - container at port 8080 one just so that
99:10 - you don't get confused because it was
99:12 - 8080 on the slides so now that we have
99:16 - restarted the containers let's actually
99:18 - check the first one which is
99:21 - Express
99:24 - so it's running on
99:27 - 8080 in the previous example we created
99:29 - a database and the collection which is
99:33 - gone because we restarted the container
99:35 - this is actually another very important
99:38 - concept of containers to understand when
99:40 - you restart a container everything that
99:43 - you configured in that container's
99:45 - application is gone so data is lost so
99:50 - to say there is no data persistence in
99:52 - the containers
99:54 - itself of course that is very
99:56 - inconvenient you want to have some
99:58 - persistence especially when you're
100:00 - working with a database and there is a
100:02 - concept we're going to learn later
100:04 - called
100:06 - volumes uh that makes it possible to
100:09 - have persistency between the container
100:12 - restarts okay so let's actually create
100:15 - the database again because we need
100:18 - it and inside the database we had
100:20 - actually users collection let's create
100:23 - that one as well and that is empty now
100:26 - let's actually start our
100:36 - application and there you go so now if I
100:40 - were to modify this one
100:44 - here and update I should see the updated
100:48 - entry here so the connectivity with
100:51 - mongodb works so now what do I do if I
100:54 - want to stop those
100:56 - containers of course I could go there
100:59 - and say Docker stop and I can provide
101:01 - all the IDS as we did previously or with
101:04 - Docker compose it's actually easier I
101:07 - can do Docker compose
101:10 - again specify the file and instead of up
101:14 - I'm going to say down and that will go
101:17 - through all the containers and shut them
101:20 - all and in addition to remove removing
101:23 - the containers or stopping them removing
101:25 - the containers it also removes the
101:28 - network so the next time we restarted
101:31 - it's going to recreate so let's actually
101:33 - check that the network
101:36 - LS that default my app default Network
101:40 - case G and when I do
101:45 - up see it gets recreated that should
101:48 - give you a good idea of what dock
101:50 - compose is and how to use it the next
101:54 - we're going to build our own Docker
101:56 - image from our node.js JavaScript
102:02 - application so now let's consider the
102:04 - following scenario you have developed an
102:06 - application feature you have tested it
102:09 - and now you're ready to deploy it right
102:11 - to deploy it your application should be
102:14 - packaged into its own Docker container
102:17 - so this means that we are going to build
102:19 - an Docker image from our JavaScript
102:23 - no JS backend application and prepare it
102:27 - to be deployed on some environment to
102:29 - review this diagram that we saw at the
102:31 - beginning of the tutorial so we have
102:33 - developed a JavaScript application we
102:35 - have used the mongodb docker container
102:38 - to use it and now it's time to commit it
102:41 - to the git right so in this case we're
102:43 - going to simulate this steps on the
102:46 - local environment but still I'm going to
102:48 - show you how these steps actually work
102:51 - so after commit you have a continuous
102:53 - integration that runs so the question is
102:56 - what does actually Jenkins do with this
102:59 - application when it builds the
103:01 - application so the JavaScript
103:03 - application using the npm build Etc it
103:08 - packages it then in a Docker
103:11 - image and then pushes it into a Docker
103:15 - repository so we're going to actually
103:17 - simulate what Jenkins does with their
103:20 - application and how it actually packages
103:23 - it into a Docker image on the local
103:26 - environment so I'm going to do all this
103:28 - on my laptop but it's basically the same
103:30 - thing that Jenkins will do and then on
103:32 - later step we can actually push the
103:35 - built image into a Docker
103:39 - repository in order to build a Docker
103:42 - image from an application we basically
103:45 - have to copy the contents of that
103:47 - application into the docker file it
103:49 - could be an artifact that we built in
103:51 - our case we just have three files so
103:53 - we're going to copy them directly in the
103:55 - image and we're going to configure it
103:57 - and in order to do that we're going to
103:59 - use a blueprint for building images
104:02 - which is called a Docker file so let's
104:05 - actually see what is a Docker file and
104:07 - how it actually looks like so as I
104:10 - mentioned Docker file is a blueprint for
104:13 - creating Docker images a syntax of a
104:17 - Docker file is super simple so the first
104:20 - line of every Docker file is
104:23 - from image so whatever image you
104:26 - building you always want to base it on
104:29 - another image in our case we have a
104:32 - JavaScript application with no JS
104:34 - backend so we are going to need node
104:37 - inside of our container so that it can
104:40 - run our node
104:42 - application instead of basing it on a
104:45 - Linux Alpine or some other lower level
104:48 - image because then we would have to
104:50 - install node ourselves on it so we are
104:53 - taking a ready node
104:55 - image and in order to see that let's
104:57 - actually go to dockerhub
105:00 - and search node here and here you see
105:04 - there is a ready node image that we can
105:07 - base our own image from so here we have
105:11 - a lot of different text so we can
105:12 - actually use one specific one or we can
105:15 - just go with the latest if we don't
105:18 - specify any take so what that actually
105:21 - means basing our own image on a node
105:25 - image is that we're going to have node
105:29 - installed inside of our image so when we
105:32 - start a container and we actually get a
105:35 - terminal of the container we can see
105:37 - that node command is available because
105:39 - there's node installed there this is
105:41 - what from node actually gives us so the
105:45 - next one
105:46 - is we can configure environmental
105:49 - variables inside our Docker file now as
105:52 - you know we have already done this in
105:54 - the using the docker on commands or the
105:57 - docker compos so this will be just an
106:00 - alternative to defining environmental
106:02 - variables in a Docker compos for example
106:05 - I would say it's better to define the
106:08 - environmental variables externally in a
106:10 - Docker compos file because if something
106:12 - changes you can actually overwrite it
106:16 - you can change the docker compos file
106:17 - and override it instead of rebuilding
106:20 - the image but this is an option so this
106:22 - n command basically would translate to
106:25 - setting the environment of variables
106:28 - inside of the image
106:30 - environment the next one is run so all
106:34 - these Capital case words that you see
106:37 - from in and run they're basically part
106:40 - of a syntax of a Docker file so using
106:43 - run basically you can execute any kind
106:46 - of Linux commands so you see make
106:50 - directory is a Linux command that creat
106:52 - creates home slome app um directory very
106:57 - important to note here this directory is
106:59 - going to leave inside of a container so
107:02 - when I start a container from this image
107:05 - the slh home/ app directory will be
107:09 - created inside of the container and not
107:11 - on my laptop not on the host so all
107:14 - these commands that you have in Docker
107:16 - file will apply to The Container
107:18 - environment none of them will be
107:21 - affecting my host environment or my
107:24 - laptop environment so with run basically
107:27 - you can execute any Linux commands that
107:29 - you want so that's probably one of the
107:31 - most used ones and we also have a copy
107:35 - command now you would probably ask I can
107:38 - execute a copy command a Linux copy
107:40 - command using run yes you could but the
107:43 - difference here is that as I said all
107:46 - these commands in run for example they
107:50 - apply to they get executed in inside of
107:53 - the container the copy command that you
107:56 - see here it actually uh executes on the
108:00 - host and you see the first parameter is
108:02 - Dot and the second parameter is slome
108:06 - app so source and the Target so I can
108:10 - copy files that I have on my host inside
108:13 - of that container
108:15 - image because if I were to execute run
108:20 - CP Source destination that that command
108:23 - would execute inside of the docker
108:25 - container but I have the files that I
108:27 - want to copy on my
108:29 - host in the last one so from and CMD or
108:34 - command is always part of Docker file
108:37 - what command does is basically executes
108:40 - an entrypoint Linux command so this line
108:43 - with the command actually translates to
108:47 - node server JS so remember here we
108:50 - actually do node server JS so we start a
108:52 - node server with the nodejs this is
108:55 - exactly what it does but inside of the
108:57 - container so once we copy our server JS
109:01 - and other files inside of a container we
109:03 - can then execute node server.js and we
109:06 - are able to do it because we are basing
109:09 - on the Node image that already has node
109:12 - pre-installed and we are going to see
109:14 - all this inaction so another question
109:17 - here what is the difference between run
109:19 - and CMD because I could also say run
109:22 - node server.js the difference again is
109:25 - that CMD is an entry point command so
109:28 - you can have multiple run commands with
109:31 - different Linux commands but CMD is just
109:33 - one and that marks for Docker file that
109:37 - this is the command that you want to
109:39 - execute as an entry point so that
109:41 - basically runs the server and that's
109:46 - it so now let's actually create the
109:49 - docker file and just like the docker
109:51 - compos file
109:53 - Docker file is part of the application
109:55 - code so I'm going to create a new file
109:57 - here and I'm going
110:00 - to paste here the contents so again
110:05 - we're basing off Note image and actually
110:09 - instead of just having the latest node
110:11 - I'm going to specify a node version so
110:14 - I'm going to take 13 minus Alpine so all
110:18 - these that you see here are Texs so I
110:21 - can use any of them as a TCH so I'm
110:23 - going to say 13 minus
110:28 - Alpine like this so this is going to be
110:32 - a specific node image that I'm going to
110:36 - use as my base image let's actually stop
110:38 - here for a moment and take a little bit
110:41 - of a deep dive on this line so since we
110:44 - saw that Docker file is a blueprint for
110:48 - any Docker image that should actually
110:51 - mean that every docker ER image that
110:53 - there is on dockerhub should be built on
110:56 - its own Docker file right so if we
110:59 - actually go to Let's actually look at
111:03 - one of the latest versions which is 13
111:05 - minus Alpine and let's click inside and
111:08 - as you
111:09 - see this specific image has its own
111:12 - Docker file and here as you see we have
111:15 - the same from that we just saw and this
111:19 - is what this node official image is
111:22 - based of which is a base image Alpine
111:26 - 3.10 right and then we have this
111:28 - environmental variable set and all these
111:30 - Linux commands using
111:33 - run and some other environmental
111:36 - variable and you have this entry point
111:39 - which is a script so you can also
111:41 - execute the whole shell script instead
111:44 - of separate commands and you have this
111:46 - final Command right so you don't have to
111:50 - understand any of this I just want to
111:51 - demonstrate that every image is based of
111:55 - another base image right so in order to
111:59 - actually visually comprehend how this
112:01 - layer stacking works with images let's
112:04 - consider this simplified
112:07 - visualization so our own image that
112:10 - we're building app with a version 1.0 is
112:14 - going to be based on a node image with a
112:16 - specific version that's why we're going
112:17 - to specify from node 13 Alpine and the
112:22 - node 13 Alpine image as we saw in the
112:26 - docker file is based on Alpine base
112:29 - image with a version 3.10 that's why it
112:31 - specifies from Alpine 3.10 so Alpine is
112:36 - a lightweight base image then we install
112:39 - node on top of it and then we install
112:41 - our own application on top of it and
112:43 - basically this is how all the images are
112:45 - built so now let's go back and complete
112:47 - our Docker file so we have the from
112:51 - specified we have the environmental
112:53 - variables specified and in just a second
112:56 - we're going to actually see these
112:59 - commands in action so let's copy that
113:02 - and this is also very important Docker
113:05 - file has to be called exactly like that
113:07 - you can just give it any name it is
113:09 - always called Docker file starting with
113:12 - a capital D and that's it it's a simple
113:15 - text file so just save it like this and
113:18 - here you even see the highlighting and
113:21 - this Docker icon
113:27 - so now that we have a Docker file ready
113:29 - let's see how to actually use it so how
113:31 - do we build an image out of it so in
113:35 - order to build an image using the docker
113:37 - file we have to provide two parameters
113:41 - one is we want to give our image a name
113:44 - in the tag just like all the other
113:46 - images have so we are going to do it
113:49 - using minus t so we are going to call
113:52 - our image my app and we're going to give
113:55 - it a tag of 1.0 the tag could be
113:59 - anything you can even call it actually
114:01 - version one it wouldn't matter so we're
114:04 - going to do 1
114:06 - point0 and the second required parameter
114:10 - actually is a location of a Docker file
114:14 - because we want to tell Docker here
114:16 - build an image using this Docker file
114:19 - and in this case because we're in the
114:21 - same fold as the docker file we're just
114:23 - going to say current directory when we
114:26 - execute this we're going to see that
114:29 - image is built and this is an ID of the
114:32 - image that was
114:35 - built because I already have node 13
114:38 - Alpine on my laptop this just use the
114:41 - the one I have lying around locally for
114:44 - you if it's the first time you will
114:46 - actually see that it's pulling node
114:49 - image from the dockerhub so now with the
114:52 - docker images I can actually see that my
114:56 - image is here it says created two days
114:59 - ago I don't know why but anyways so I
115:02 - have the image name which is this one
115:04 - here and I have the name of the image
115:08 - and the tag of the image so if we go
115:10 - back to this diagram that we saw in the
115:13 - review so basically we've got all these
115:15 - steps or we have simulated some of the
115:17 - steps we've built the JavaScript
115:19 - application using a Docker containers
115:21 - and one once the the application is
115:23 - ready let's say we made the commit and
115:27 - we we just simulated what Jenkins server
115:30 - also does so what Jenkins does is
115:33 - actually it takes the docker file that
115:36 - we create so we have to commit the
115:39 - docker file into the repository with the
115:41 - code and Jenkins will then build a
115:45 - Docker image based on the docker file
115:48 - and what is an important Point here is
115:50 - that usually you don't develop long you
115:52 - are in the team so other people might
115:55 - want to have access to that upto-date
115:57 - image of your application that you
115:59 - developed it could be a tester maybe who
116:01 - wants to pull that image and test it
116:04 - locally or you want that image to be
116:07 - deployed on a development server right
116:09 - and in order to do that you have to
116:11 - actually share the image so it is pushed
116:14 - into a Docker repository and from there
116:18 - either people can take it for example a
116:21 - tester maybe want to download the image
116:23 - from there and test it locally or a
116:25 - development server can actually pull it
116:28 - from
116:31 - there so let's actually just run a
116:34 - container I'm just going to say Docker
116:37 - run the image name
116:40 - obviously and a
116:42 - tag like this and in this case I'm not
116:46 - going to specify any other options
116:48 - because we just want to see what's going
116:49 - on inside of the container so I'm just
116:51 - going to run it
116:53 - okay so the problem is that it can't
116:55 - find the server JS file which is
116:58 - actually logical because we are not
117:01 - telling it to look in the correct
117:02 - directory so since we are copying all
117:05 - the resources in this home/ home/ app
117:09 - directory server JS is going to be there
117:11 - as well and this is another topic
117:15 - whenever you adjust a Docker file you
117:17 - have to rebuild an image because the old
117:20 - image cannot be over written so to say
117:23 - so what I'm going to do now is actually
117:25 - I'm going to delete the one that I built
117:28 - so I'm going to I'm going to actually
117:30 - take the image this is how you delete an
117:35 - image but I can delete it because as as
117:38 - it says the docker is used by a stopped
117:42 - container so if I do Docker PS minus
117:46 - a actually let's crap to my app like
117:53 - this I have to first delete the
117:55 - container so this is how you delete a
117:58 - container it's doer
117:59 - RM and once I've deleted the container I
118:03 - can delete an image so the image
118:05 - deletion is
118:07 - RMI like this so if I do images now I
118:11 - see my image isn't there okay so we'
118:14 - have modified the docker file so let's
118:17 - rebuild it now so Docker build
118:25 - again and let's see the image is here so
118:30 - let's start it
118:33 - again so it's my app
118:37 - 1.0 and let's run it and as you see the
118:40 - problem is fixed app listening on Port
118:43 - 3000 so our app is running so this one
118:47 - here I app 1.0 first of all we can see
118:50 - the logs here
118:53 - like this we see that the EP is
118:56 - listening on Port 3000 we know
118:58 - everything is cool to actually just get
119:00 - a little bit more inside let's enter the
119:03 - containers or let's get the terminal the
119:05 - command line terminal of the container
119:08 - and look around there so I'm going to
119:11 - say Docker exit interactive terminal I'm
119:15 - going to specify the container
119:17 - ID
119:19 - in like this
119:22 - and since bin bash doesn't work we can
119:25 - actually try shell so this is something
119:29 - you will also encounter because some
119:33 - containers do not have bash installed so
119:35 - we'll have to connect using bin
119:40 - sh so one of them has to work always so
119:44 - let's see in which directory we are so
119:46 - we are in the root directory and we see
119:49 - our virtual file system there and as you
119:52 - see the cursor changed as well so that
119:54 - means we're inside of a container so now
119:58 - let's actually check some of the stuff
120:00 - so first of all we specified some
120:02 - environmental variables here in the
120:05 - docker file and this means that these
120:08 - environmental variables have to be set
120:11 - inside the docker environment so if we
120:13 - do inv we actually see the mongodb
120:17 - username this one here and mongodb
120:19 - password are set and there are some
120:21 - other
120:22 - environmental variables automatically
120:24 - said we don't care about them so another
120:27 - thing we can check is this directory
120:29 - because remember because with this line
120:32 - we actually created this slome app
120:35 - directory so let's see
120:38 - slome Slash app and as you can see the
120:43 - directory was created and with the next
120:45 - Land We copied everything in the current
120:49 - folder so if we actually go and see
120:52 - reveal in
120:54 - finder so this is where the docker file
120:56 - resides so basically we copied
120:59 - everything that is inside of this
121:02 - directory so all of these into the
121:05 - Container now we don't actually need to
121:08 - have Docker file and Docker compose and
121:12 - uh this other stuff in here because the
121:14 - only thing we need are the JavaScript
121:17 - files or if we build a JavaScript
121:19 - application artifact just the artifact
121:22 - so let's go ahead and improve that so
121:25 - what I'm going to do is I'm going to
121:27 - create an app
121:29 - directory and I'm going to copy just the
121:32 - files that I'm going to need for
121:35 - starting an application inside of a
121:37 - container so I'm going to take
121:41 - those and the images as well so all
121:45 - these are just external ones we don't
121:47 - need them there and images the index
121:49 - HTML file package Jon server JS and node
121:53 - modules are inside of app so what we can
121:56 - do it now is instead of copying the
121:58 - whole directory where where the docker
122:00 - file is I just want to copy all the
122:03 - contents of EP folder so what I'm going
122:07 - to do is I'm going to say copy all the
122:10 - app
122:12 - contents and again because we modified a
122:14 - Docker file we need to recreate the
122:17 - image in order to leave the docker
122:20 - Container Terminal can actually exit so
122:23 - now we are on the host again so if I do
122:27 - Docker images again I have to first
122:31 - delete the container and then image but
122:33 - in order to delete the container I have
122:35 - to first stop
122:37 - it so now I can remove the
122:41 - container and now I can actually remove
122:43 - the
122:45 - image that the container was based
122:48 - on and let's check again so let's ex
122:52 - execute that build command
122:57 - again so now that we have the image
123:00 - built let's actually run it so I'm going
123:03 - to say my
123:05 - app
123:07 - 1.0 and of course I could have executed
123:10 - with a minus D in a detached mode it
123:12 - doesn't matter now and if I do a Docker
123:15 - PS I see my um image container running
123:19 - and now let's actually enter the
123:21 - container G so
123:24 - it and as we learned it
123:28 - was in
123:30 - sh and again we're going to see
123:34 - the home
123:37 - app and here we just have the contents
123:41 - of app directory so no unnecessary
123:44 - Docker file Docker compose Etc files
123:47 - which is actually how it's supposed to
123:49 - be or as I said because I just had a
123:52 - couple of files here I copied all of
123:54 - them but usually if you have this huge
123:56 - application you would want to compress
123:59 - them and package them into an artifact
124:02 - and then copy that artifact into a
124:05 - Docker image
124:07 - container okay but as I said this was
124:09 - just for demonstration purposes because
124:12 - I just wanted to show you um how you can
124:14 - actually start it as a container and how
124:17 - it should to look inside and in this
124:18 - case we improved a couple of things but
124:20 - usually we would start this container
124:22 - from a Docker compose as well together
124:25 - with all the other Docker images that
124:27 - the application uses and it's also
124:29 - doesn't have any ports open so uh this
124:32 - is just for demonstration
124:36 - purposes so in this video we're going to
124:39 - create a private repository for Docker
124:41 - images on AWS ECR there are many more
124:45 - options for Docker Registries among them
124:48 - Nexus and digital ocean so we're going
124:51 - to see how to create a registry there
124:53 - build and tag an image so that we can
124:55 - push them into that repository and in
124:57 - order to push the images into a private
124:59 - repository you first have to log into
125:02 - that repository so let's see how it all
125:09 - works so the first step is to actually
125:12 - create a private repository for Docker
125:14 - it's also called Docker registry in this
125:17 - case we're going to do it on AWS
125:21 - so let's
125:25 - see so I already have an account on
125:28 - AWS so the service that we're going to
125:32 - use is called elastic container registry
125:36 - so
125:37 - ECR doer container
125:40 - registry and because I don't have a
125:42 - repository there yet I am presenting
125:45 - with the screen so in order to create a
125:48 - repository click on get started and here
125:52 - we have a repository name and we're
125:55 - actually going to name it the name of
125:57 - the application that we have so I'm
125:59 - actually going to name it my app this is
126:02 - the domain of the registry from AWS and
126:05 - this is the repository name which is the
126:08 - same as my image
126:10 - name and don't worry about the other
126:12 - stuff right now and just create a
126:14 - repository it's as simple as that now
126:18 - one thing I think specific to Amazon
126:21 - container service is that here you
126:25 - create a Docker repository per image so
126:28 - you don't have a repository where you
126:30 - have where you can actually push
126:32 - multiple images of different
126:33 - applications but rather for each image
126:36 - you have its own repository and you go
126:39 - inside of the repository here it's empty
126:41 - now but what you store in a repository
126:44 - are the different tags or different
126:46 - versions of the same image so this is
126:49 - how the Amazon container service
126:51 - actually works there are other dep
126:54 - Docker Registries that work differently
126:56 - for example you create a repository and
126:57 - you can just throw all of your container
126:59 - images inside of that one repository so
127:02 - I think this is more or less specific
127:04 - for AWS so anyways we have repository
127:08 - which is called my app and let's
127:10 - actually see how we can push the image
127:12 - that we have locally so actually check
127:15 - that once more so we want to push this
127:18 - image here into that
127:22 - repository so how do we do that if you
127:25 - click on this one the view push commands
127:29 - will be highlighted this is different
127:30 - for each registry but basically what you
127:33 - need to do in order to push an image
127:35 - into repository are two things one you
127:40 - have to log in into the private
127:41 - repository because you have to
127:43 - authenticate yourself so if you are
127:45 - pushing from your local laptop or local
127:48 - environment you have to tell that
127:50 - private reposit
127:51 - hey I have access to it this is my
127:54 - credentials if a Docker image is built
127:57 - and pushed from a Jenkins server then
128:00 - you have to give Jenkins credentials to
128:02 - login into the repository so Docker
128:05 - login is always the first step that you
128:08 - need to do so here AWS actually provides
128:14 - a Docker login command for AWS so it
128:18 - doesn't say Docker login but in the
128:20 - background it uses one
128:24 - so I'm going to execute this login
128:26 - command for AWS Docker repository uh so
128:29 - in the background it uses actually
128:31 - Docker login to authenticate so in order
128:34 - to be able to execute this you need to
128:36 - have AWS command line interface and the
128:39 - credentials configured for it so if you
128:43 - don't I'm going to put a link to the
128:45 - guide of how to do that in the
128:46 - description I have configured both of
128:49 - them so I can execute this command
128:52 - and I should be logged in successfully
128:54 - to the docker repository so now I have
128:57 - authenticated myself to the docker
128:59 - repository here so I'm able to push the
129:04 - image that I have locally to that
129:07 - repository but before I do that there is
129:09 - one step I need to do so I've already
129:11 - built my image so that's fine and now I
129:15 - have to tag my image and if this command
129:18 - here looks a little bit too complicated
129:20 - for you or too strange let's actually go
129:22 - and look at image naming Concepts in
129:24 - Docker
129:29 - repositories so this is the naming in
129:32 - Docker Registries this is how it works
129:35 - the first part of the image name the
129:38 - image full name is the registry domain
129:41 - so that is the host Port Etc slash
129:45 - repository or image name and the tag now
129:49 - you may be wondering every time we were
129:51 - pulling an image out of dockerhub we
129:53 - actually never had this complex long
129:57 - name of the image right so when we were
130:00 - pulling an image it looked like this
130:02 - Docker pole 4.2 the thing is with
130:06 - dockerhub we're actually able to pull an
130:09 - image with a short hand without having
130:12 - to specify a registry domain but this
130:14 - command here is actually a shorthand for
130:18 - this command what actually gets executed
130:20 - in the background when we say Docker
130:21 - pole is Docker pole the registry
130:25 - domain so docker.io library is a
130:28 - registry domain then you have the image
130:29 - name and then you have the tag so
130:32 - because we we were working with
130:33 - dockerhub we were able to use a shortcut
130:36 - so to say in a private Registries we can
130:40 - just skip that part because there is no
130:43 - default configuration for it so in our
130:46 - case in AWS ECR what we going to do is
130:50 - we're going to
130:51 - execute Docker pull the full registry
130:54 - domain of the repository this is what
130:57 - we're going to see here and a tag and
131:00 - this is how AWS just generates the
131:03 - docker registry name that's why we see
131:06 - this long image name with the tag here
131:10 - and we have to tag our image like this
131:13 - so let's go back and take a look at our
131:15 - images our image that we built again and
131:19 - under the repository it says my app now
131:23 - the problem is we can just push an image
131:26 - with this name because when we say
131:28 - Docker
131:29 - push my app like this Docker wouldn't
131:33 - know to which repository we're trying to
131:36 - push by default it will actually assume
131:39 - we're trying to push to dockerhub but
131:42 - it's not going to work obviously because
131:44 - we want to push it to AWS so in order to
131:47 - tell Docker you know what I want this
131:49 - image to be pushed to AWS repository
131:53 - with the name my app we have to tag the
131:57 - image so we have to include that
132:00 - information in the name of the image and
132:03 - that is why we have to tag the image tag
132:06 - basically means that we are renaming our
132:09 - image to include the repository domain
132:12 - or address and the name okay and AWS
132:17 - already gives
132:18 - us the command that we can execute
132:22 - we want to use the specific version so
132:25 - I'm going to use 1.0 in both so what
132:30 - this is going to do is it's going to
132:31 - rename this is what tech does my app 1.0
132:37 - this is what we have locally this is
132:39 - what the name is to this one here so
132:42 - let's execute that and let's see what
132:44 - the outcome is and as you see it took
132:49 - the image that we had it made a copy and
132:52 - renamed it into this one so these two
132:55 - are identical images they're just called
132:57 - in a different way and
132:59 - now when we go back we see the docker
133:03 - push command so basically this thing
133:06 - here is a same as Docker
133:09 - push and name of the
133:13 - image and the take so this push command
133:19 - will tell Docker you know what I want
133:21 - you to take the image with tag
133:25 - 1.0 and push it into a repository at
133:29 - this address so when I execute this
133:32 - command see the push command will
133:36 - actually push those layers of the docker
133:39 - image one by one this is the same thing
133:42 - as when we're pulling it we also pulled
133:45 - the images layer by layer and this is
133:48 - what happens in the reverse Direction
133:50 - when we push
133:51 - it so this is also going to take a
133:54 - little
133:56 - bit great so the push command was
133:59 - complete and we should be able to see
134:03 - that image in the AWS repository now so
134:07 - if I go
134:08 - inside see I have image tag with 1.0
134:12 - this is our tag here
134:15 - and push the time the digest which is
134:18 - the unique hash of that image
134:21 - and the image
134:23 - URI which is again the name of the image
134:27 - using the the
134:29 - repository address image name or
134:32 - repository name in this case and the
134:37 - tag so now let's say I made some changes
134:40 - in the docker file you know let's say I
134:43 - renamed this home slome to node app
134:51 - like
134:52 - this or what could also lead to need to
134:56 - recreate an image is obviously when I
134:59 - change something in the code right so
135:02 - you know let's say I were to delete this
135:05 - line because I don't want to console
135:08 - log to be in my
135:12 - code and now I have a different version
135:15 - of the application where I have changes
135:17 - in the application so now I want to have
135:20 - those changes in the new Docker image so
135:23 - now let's build a new Docker image out
135:26 - of
135:27 - it so Docker build let's call it my
135:31 - app with a version
135:34 - 1.1 and a path to a Docker
135:40 - file and now I have a second
135:43 - image which is called my app with
135:46 - version
135:47 - 1.1 so now again because I want to push
135:50 - this to repository I have to rename it
135:53 - to include the repository address inside
135:56 - of it so I'm going to do Docker
135:58 - tag the first parameter is the image
136:03 - that I want to rename and the second one
136:06 - is the name of that
136:08 - image a new name so it's going to be the
136:13 - same as the previous one because the
136:14 - repository name and the address is the
136:17 - same remember we have one repository for
136:21 - one image but for different versions so
136:25 - we're building a version 1.1 so it
136:27 - should end up in the same
136:29 - repository so now here we have 1.1 and
136:33 - if I tag that and images I have a second
136:38 - image
136:39 - here so I'm going to copy that and I'm
136:43 - going to do Docker
136:47 - build and do not forget that tag it's
136:50 - important because because it's part of
136:51 - the complete name sorry it's Docker
136:58 - push and now some of the layers that I
137:01 - already pushed are there only the ones
137:04 - that changed are being rep pushed sort
137:06 - of say and also know that I just have to
137:10 - do Docker login once at the beginning
137:13 - and then I
137:14 - can pull and push images uh from this
137:18 - repository as many times as I want so do
137:21 - login is done
137:23 - once so now that is complete let's
137:26 - actually reload
137:28 - this so my repository now has two
137:32 - versions so this is pretty practical if
137:34 - you are for example testing with
137:36 - different versions and you want to have
137:39 - a history of those image Texs if you
137:42 - want to for example test a previous
137:45 - version and I think in AWS the repos
137:48 - each repository has a capacity of hold
137:50 - holding up to 1,000 uh image versions so
137:54 - for example my app here can have
137:57 - thousand different tags or of the same
138:00 - image okay so now going to compare it to
138:04 - the initial diagram that we saw for this
138:07 - complete flow let's actually switch back
138:09 - to it
138:10 - quickly so here what we did is basically
138:13 - simulate how Jenkins would push an image
138:16 - to a Docker repository so whatever we
138:19 - did on our lap top will be the same
138:22 - commands executed on a Docker on the
138:24 - Jenkins server and again Jenkins user or
138:28 - Jenkins server user has to have
138:30 - credentials to the docker repository to
138:33 - execute Docker login depending on the
138:36 - registry or repository configuration
138:38 - will look different and Jenkins needs to
138:41 - tag the image and then push it to the
138:43 - repository and this is how it it's done
138:47 - and the next step of course we need to
138:49 - use that image that is lying now in the
138:51 - repository and we're going to see how
138:53 - it's pulled from that repository and
138:56 - again we're going to do it on the local
138:57 - environment but it's the same thing that
139:00 - a development server or any other
139:03 - environment will actually
139:07 - execute so in this video we're going to
139:09 - see how to deploy an application that we
139:12 - built into a Docker image so after you
139:15 - package your application in a Docker
139:17 - image and save it in the private
139:19 - repository you need to somehow deploy it
139:21 - on a development server or integration
139:24 - server or whatever other environment and
139:27 - we're going to use Docker compose to
139:29 - deploy that application so let's
139:31 - imagine we have logged in to a
139:35 - development
139:36 - server and we want to run our image that
139:41 - we just push the repository so our my
139:44 - app image and the mongodb image uh both
139:48 - the database and the Express on
139:51 - the development server so the my app
139:53 - image will be pulled from private
139:55 - repository of AWS the in the two
139:59 - containers will be pulled from the
140:01 - docker Hub so let's see actually how
140:04 - that would work so usually again you
140:07 - have developed your application you done
140:10 - with it and you have created uh your own
140:15 - Docker image right now in order to start
140:19 - an application on development server you
140:21 - would need all the containers that make
140:24 - up that application environment okay so
140:26 - we have mongodb and Express
140:28 - already so what we are going to do is
140:31 - here we're going to
140:32 - add a new container in the list which is
140:38 - going to be our own image so let's go
140:41 - ahead and copy the
140:43 - image from our
140:46 - repository so let's actually use the 1.0
140:56 - so again remember we said that this
140:59 - image name is a shortcut for having a
141:02 - docker.io do library SL with like
141:09 - a specific version so instead of that
141:11 - because we are pulling these images from
141:13 - a Docker Hub we can actually skip that
141:18 - repository domain in front of the images
141:21 - but here because we're pulling it from a
141:23 - private repository so if we were to
141:25 - specify our image like this Docker will
141:28 - think that our image resides on
141:30 - dockerhub so we try to pull it from
141:32 - dockerhub and of course it won't find it
141:34 - because we have to tell Docker go and
141:37 - look at this repository with this
141:39 - repository name and this TCH and of
141:42 - course in order to be able to pull this
141:44 - image or the docker composed to be able
141:46 - to pull this image the environment where
141:49 - you execute this Docker compost file has
141:52 - to be logged into a Docker repository so
141:56 - here as the development server has to
141:59 - pull the image from the repository what
142:01 - we would need to do on the development
142:03 - server is actually do a dock login
142:06 - before we execute the docker compose and
142:09 - obviously you don't need a Docker login
142:11 - for Docker Hub those images will
142:14 - be pulled
142:16 - freely okay so the next thing that we
142:19 - have to configure are the ports because
142:22 - obviously want to open the
142:24 - ports if we go back we see that our
142:28 - application runs on Port 3000 so the
142:32 - port of the container or the where the
142:34 - container is listening on is 3,000 and
142:38 - here we can open the port on the host
142:41 - machine so it's going to be 3,000 me to
142:44 - 3,000 we have actually the environment
142:46 - variables inside of the docker file but
142:51 - obviously we could have configured them
142:52 - in the docker compose just like this so
142:55 - it's an alternative so this will be a
142:58 - complete Docker compost file that will
143:00 - be used on the development server to
143:04 - deploy all the all the applications
143:06 - inside so again if we're trying to
143:08 - simulate a development server the first
143:10 - step will be to do the docker login in
143:12 - this
143:14 - case you have this on command for
143:17 - logging into the AWS Repository which I
143:21 - have done already in this terminal so
143:24 - the next step is to have the docker
143:26 - compos file available on this
143:27 - development server because we have to
143:30 - execute the docker compost file because
143:33 - we're simulating here the way I would do
143:35 - it is I'm going to create a yl
143:38 - file in the current directory where I am
143:42 - I'm going to copy
143:45 - this and
143:47 - save so now I have my ml file and
143:52 - now we can start all three containers
143:55 - using Docker compose
143:58 - command minus
144:00 - f
144:07 - up and here we see that app started on
144:11 - 3000 and mongodb and express started as
144:18 - well so let's check again
144:23 - now and here we saw that database is
144:26 - lost every time we recreate a container
144:29 - and of course that's not good and we're
144:30 - going to learn how to preserve the
144:33 - database data between the container
144:34 - restarts using Docker volumes in the
144:36 - later tutorials because this is not an
144:39 - ideal State okay so now that we have
144:41 - database in a collection let's actually
144:43 - refresh and our application works as
144:46 - well let's check
144:54 - awesome so application works let's
144:58 - refresh this one as well and there is
145:00 - actually one thing that I needed to
145:02 - change in the code to connect nodejs
145:04 - with mongodb so let's actually go and
145:07 - look at that these are my handlers you
145:09 - know nodejs where I connect to the
145:11 - mongodb database so the uis are the same
145:15 - and what I changed here is that it was a
145:17 - local host
145:19 - before so instead of Local Host I
145:21 - changed it to
145:23 - mongodb because this actually is a name
145:27 - of the
145:28 - container or of the service that we
145:31 - specify here so this actually leads back
145:35 - to the docker Network and how Docker
145:37 - compos takes care of it is that in the
145:39 - URI or when I connect one application in
145:43 - a Docker container with another one in
145:45 - another Docker container I don't have to
145:47 - use this uh Local Host anymore actually
145:51 - I wouldn't even need to use the port
145:53 - even because I have all that information
145:57 - so the host name and the port number in
146:01 - that configuration so my application
146:04 - will be able to connect to mongodb using
146:07 - the service name and because of that you
146:09 - don't have to specify here a local host
146:11 - and a port number which is actually even
146:14 - more Advantage when you consider using
146:17 - Docker containers to run all of your
146:19 - applic ations because it makes the
146:22 - connectivity between them even more
146:27 - easier and that actually concludes the
146:30 - this uh diagram that we saw previously
146:33 - we have gone through all of the steps
146:35 - where we saw uh how to develop uh a
146:38 - JavaScript application locally with
146:41 - Docker containers then we saw how to
146:43 - build them into an image uh just like a
146:46 - continuous integration build will do it
146:49 - then we push it into a private
146:51 - repository and we simulated a
146:53 - development server where we pulled the
146:55 - image from U private repository and the
146:58 - other images from the dockerhub where we
147:00 - started the whole application setup with
147:02 - our uh own application and the two
147:06 - applications uh using a Docker compose
147:09 - which is how you would deploy an
147:11 - application on a Dev server so that now
147:13 - testers or other developers will be able
147:16 - to um access the development server and
147:19 - actually try out the applic that you
147:20 - just deployed or you can also use it for
147:28 - demos so in this video we're going to
147:30 - learn about Docker volumes in a nutal
147:33 - Docker volumes are used for data
147:36 - persistence in Docker so for example if
147:38 - you have databases or other stateful
147:41 - applications you would want to use
147:43 - Docker volumes for that so what are the
147:46 - specific use cases when you need Docker
147:49 - volumes so a container runs on a host
147:52 - let's say we have a database container
147:54 - and a container has a virtual file
147:57 - system where the data is usually
148:00 - stored but here there is no persistence
148:02 - so if I were to remove the container or
148:06 - stop it and restart the container then
148:08 - the data in this virtual file system is
148:11 - gone and it starts from a fresh state
148:14 - which is obviously not very practical
148:16 - because I want to save the changes that
148:18 - my application is making in the database
148:21 - and that's where I need Docker volumes
148:23 - so what are the docker volumes exactly
148:26 - so on a host we have a physical file
148:29 - system right and the way volumes work is
148:33 - that we plug the physical file system
148:37 - path it could be a folder a directory
148:40 - and we plug it into the containers file
148:43 - system path so in simple terms a
148:46 - directory a folder on a host file system
148:48 - is mounted in into a directory or folder
148:52 - in the virtual file system of Docker so
148:55 - what happens is that when a container
148:58 - writes to its file system it gets
149:00 - replicated or automatically written on
149:03 - the host file system directory and vice
149:06 - versa so if I were to change something
149:08 - on the host file system it automatically
149:10 - appears in the container as well so
149:13 - that's why when a container restarts
149:14 - even if it starts from a fresh state in
149:17 - its own virtual file system it gets the
149:19 - data automatically from the from the
149:21 - host because the data is still there and
149:23 - that's how data is populated on a
149:25 - startup of a container every time you
149:28 - restart now there are different types of
149:30 - Docker volumes and so different ways of
149:33 - creating them usually the way to create
149:35 - Docker volumes is using Docker run
149:37 - command so in the docker run there is an
149:40 - option called minus V and this is where
149:42 - we Define the connection or the
149:45 - reference between the host directory and
149:49 - the container directory and this type of
149:52 - volume definition is called host volume
149:55 - and the main characteristic of this one
149:56 - is that you decide where on the host
150:00 - file system that reference is made so
150:03 - which folder on the host file system you
150:06 - mount into the Container so the second
150:08 - type is where you create a volume just
150:11 - by referencing the container directory
150:15 - so you don't specify which uh directory
150:18 - on the host should be mounted but that's
150:21 - taking care of the docker itself so that
150:24 - directory is first of all automatically
150:26 - created by Docker under the VAR leap
150:28 - Docker volumes so for each container
150:31 - there will be a folder generated that
150:33 - gets mounted automatically to the
150:35 - container and this type of volumes are
150:38 - called Anonymous volumes because you
150:39 - don't have a reference to this
150:42 - automatically generated folder basically
150:45 - you just have to know the path and the
150:47 - third volume type is actually an
150:49 - improvement of the anonymous volumes and
150:52 - it specifies the name of the folder on
150:56 - the host file system and the name is up
150:59 - to you it's just to reference the
151:01 - directory and that type of volumes are
151:03 - called named volumes so in this case
151:06 - compared to Anonymous volumes you H you
151:08 - can actually reference that volume just
151:11 - by name so you don't have to know
151:13 - exactly the path so from these three
151:15 - types the mostly used one and the one
151:18 - that you should be using using in
151:20 - production is actually the named volumes
151:23 - because there are additional benefits to
151:25 - letting Docker actually manage those uh
151:29 - volume directories on the host now they
151:31 - showed how to create Docker volumes
151:33 - using Docker run commands but if you're
151:35 - using Docker compose it's actually the
151:38 - same so this actually shows how to use
151:41 - volume definitions in a Docker compose
151:44 - and this is pretty much the same as in
151:46 - Docker run commands so we have volumes
151:48 - attribute and underneath you define your
151:51 - volume definition just like you would in
151:53 - this minus V option and here we use a
151:55 - named volume so db- data will be the
151:59 - name reference name that you can just
152:02 - think of could be anything and in vly
152:06 - MySQL data is the path in the container
152:09 - then you may have some other containers
152:11 - and at the end so on the same level as
152:14 - the services you would actually list all
152:17 - the volumes that you have defined you
152:19 - def find a list of volumes that you want
152:21 - to mount into the containers so if you
152:25 - were to create volumes for different
152:26 - containers you would list them all here
152:29 - and on the container level then you
152:32 - actually Define under which path that
152:35 - specific volume can be mounted and the
152:37 - benefit of that is that you can actually
152:40 - mount a reference of the same uh folder
152:43 - on a host to more than one containers
152:47 - and that would be beneficial if those
152:49 - containers need to share the data in
152:52 - this case you would Mount the same
152:53 - volume name or reference to two
152:56 - different containers and you can mount
152:58 - them into different path inside of the
153:01 - container
153:04 - even in this video we are going to look
153:07 - at Docker volumes in practice and this
153:09 - is a simple nodejs mongodb application
153:13 - uh that we're going to attach the volume
153:15 - to so that we don't lose the database
153:17 - data every time we restart start the
153:20 - mongodb
153:22 - container so let's head over to the
153:24 - console and I'm going to start the
153:26 - mongodb with the docker compose so this
153:29 - is how the docker compose looks like
153:31 - we're going to start the mongod TB uh
153:32 - container and the Express
153:34 - container so that we have a UI to
153:37 - it so I'm going to execute the docker
153:43 - compost which is going to start mongodb
153:46 - and the Express
153:51 - so when it started I'm going to check
153:53 - that Express is running on port
153:56 - 8080 and here we see just the default
153:59 - databases so these are just created by
154:02 - default on Startup um and we're going to
154:05 - create our own one for the node.js
154:08 - application and inside of that database
154:11 - I'm going to create users collection so
154:15 - these are the prerequisites or these are
154:17 - the things that my node.js applic ation
154:20 - needs so this one here in order to
154:23 - connect to the database might DB this is
154:27 - what we just created ITB and inside of
154:31 - that to the collection called users so
154:34 - let's start the
154:37 - application which is running on Port
154:39 - 3000 so
154:42 - here and this is our app which when I
154:46 - edit something here
154:51 - we'll write the changes to my database
154:55 - now if I were to restart now the mongodb
154:59 - container I would lose all this data so
155:02 - because of that we're going to use named
155:04 - volumes inside of the docker compos file
155:07 - to persist all this data in the
155:11 - mongodb let's head over to dock compose
155:15 - so the first step is to Define what
155:17 - volumes I'm going to be using in any of
155:19 - my containers and I'm going to do that
155:22 - on the services level so
155:25 - here I Define the list of all the
155:28 - volumes that I'm going to need in any of
155:30 - my containers and since we need data
155:32 - persistency for mongod TB we're going to
155:35 - create uh data volume here now
155:40 - this is going to be the name of the
155:42 - volume reference uh but we also need to
155:45 - provide here a
155:47 - driver local so the actual store storage
155:50 - path that we're going to see later once
155:51 - it's created it's is actually created by
155:54 - Docker itself and this is kind of an
155:57 - information additional information for
155:59 - Docker to create that physical storage
156:01 - on a local file
156:03 - system so once we have a name reference
156:06 - to the volume defined we can actually
156:09 - use it in the container so
156:13 - here I'm going to say
156:15 - volumes and here I will Define a mapping
156:18 - between the
156:20 - data volume that we have on our host and
156:24 - the second one will be the path inside
156:28 - of the mongodb container it has to be
156:30 - the path where mongodb explicitly
156:33 - persists its data so for example if you
156:36 - check it out online you see that the
156:39 - default path where mongodb stores its
156:42 - data is data/ dat DB and we can actually
156:46 - check that out so if I say docker s and
156:50 - go inside the
156:54 - container it's minus
156:59 - it I can actually see data DB and here
157:05 - is all the data that mongodb actually
157:07 - holds but this is of course only the
157:10 - container so when the container restarts
157:13 - the data get regenerated so nothing
157:15 - persists here so this is the path inside
157:20 - of the container not on my host that we
157:22 - need to reference in the volumes here so
157:25 - we're attaching our volume on the host
157:29 - to
157:31 - data/ dat DB inside of a container so
157:36 - for example for MySQL it's going to be
157:38 - um VAR leap MySQL for postgress it's
157:42 - also going to be VAR leap postris SQL SL
157:46 - data so each database will have its own
157:48 - so you have to actually find the right
157:51 - one so what this means is that all the
157:54 - data with that we just saw here all of
157:56 - this will be replicated on a container
157:58 - startup on our host on this persistent
158:02 - volume that we defined here and vice
158:04 - versa meaning when a container restarts
158:07 - all the data that is here will be
158:10 - replicated inside of that directory
158:12 - inside of a
158:13 - container so now that we have defined
158:16 - that let's actually restart the docum
158:18 - compose
158:23 - and restart
158:28 - it so once we create the
158:32 - data and I'm going to the collection and
158:37 - let's actually change this
158:43 - one
158:44 - here and update it so we have a data
158:48 - here
158:51 - so now that we have the persistent
158:52 - volume defined if I were to restart all
158:56 - these containers these data should be
158:58 - persisted so in the next restart I
159:01 - should see the database my DB collection
159:04 - and the entry here so let's do
159:09 - that I'm going to do
159:16 - down great so let's check
159:20 - see the database is here the collection
159:22 - is here and the entry has
159:29 - persisted so now let's actually see
159:31 - where the docker volumes are located on
159:34 - our local machine and that actually
159:37 - differs between the operating systems
159:39 - for example on a Windows laptop or a
159:41 - computer uh the path of the docker
159:44 - volume will be at program data Docker SL
159:48 - volumes the program data Docker folder
159:50 - actually contains all the other
159:53 - container information so you would see
159:55 - other folders in this Docker directory
159:59 - besides the volumes on Linux the path is
160:02 - actually /ar leap Docker volumes which
160:05 - is comparable to the windows path so
160:08 - this is where the docker saves all this
160:10 - configuration and the data and on the
160:12 - mech it's also the same one inside of
160:16 - this volumes directory you actually have
160:18 - a list of all the volumes that one or
160:20 - many containers are using and each
160:23 - volume has its own hash which is or
160:26 - which has to be unique and then slore
160:30 - data will actually contain all the files
160:32 - and all the data that is uh persisted
160:36 - let's head over to the command line and
160:38 - actually see um the volumes that we
160:41 - persisted for mongodb now interesting
160:44 - note here is that if I were to go to
160:47 - this path that I just showed you in the
160:49 - presentation which is VAR Le Docker see
160:54 - there is no such directory so that could
160:57 - be a little little bit confusing but the
160:59 - way it works on Mac specifically on
161:02 - Linux you would actually have that path
161:05 - directly on your host but on Mech it's a
161:07 - little bit different so basically what
161:09 - happens is that Docker for Mech
161:11 - application seems to uh actually create
161:14 - a Linux VM uh in the background and
161:17 - store all the docker inform or doer data
161:20 - about the containers and the volumes Etc
161:23 - inside of that vm's storage so if we
161:26 - execute this command here so this is
161:28 - actually the physical storage on my
161:30 - laptop that I have where all the data is
161:33 - stored but if I execute this command I
161:36 - actually get the terminal of that
161:39 - VM and inside here if I look I have a
161:44 - virtual different virtual file
161:46 - system and I can find that path that I
161:49 - showed you here so it's VAR leap
161:54 - Docker see so I have all this Docker
161:57 - information here I have the containers
161:59 - folder and I have volumes folder so this
162:02 - is the one we
162:04 - need sort of that actually go to the
162:07 - volumes and this is a list of volumes
162:10 - that um I have created and this is the
162:13 - one that came from our Docker compose
162:17 - right this is the name of our app this
162:19 - is do this is what Docker compose
162:21 - actually takes as the name we can
162:23 - actually take a look here so when it's
162:27 - creating these containers it depends
162:30 - these name as a prefix and then there is
162:32 - mongodb and our volume has the same
162:35 - pattern it has the prefix and then
162:38 - mongod data this is the name that we
162:40 - defined here so now if we look inside of
162:45 - that mongod data volume directory
162:49 - we see that underscore
162:53 - data this would be the anonymous volumes
162:56 - so basically here you don't have a name
162:58 - reference it's just some random uh
163:00 - unique ID but it's the same kind of
163:03 - directory as this one here the
163:06 - difference being that this one has a
163:08 - name so it's more it's easier to
163:11 - reference it with a name so this is an
163:13 - onymous volume this is a named
163:15 - volume but the content will be used in
163:18 - the same way
163:20 - um so here as you see in this underscore
163:22 - data we have all the data that mongodb
163:25 - uses so this will be where it gets the
163:29 - default databases and the changes that
163:31 - we make through our application inside
163:33 - and if I go inside of the container so
163:35 - remember this volume is attached
163:40 - to
163:42 - mongodb and is replicated inside of the
163:45 - container under path SL dat DB so if we
163:48 - go go inside of the
163:51 - container actually it
163:54 - here
164:02 - PS SL DP we'll see actually the same
164:06 - kind of data here so we have all this
164:07 - index and collection um files just like
164:12 - we did in this one so now whenever we
164:14 - make changes to our application for
164:16 - example we change it to SM
164:20 - whatever
164:22 - and this will make the container update
164:25 - its data and that will Cascade into this
164:28 - volumes directory that we have here so
164:30 - that on the next startup of a container
164:32 - when the SL data/ DB is totally empty it
164:35 - will actually populate this directory
164:37 - with the data from this uh persistent
164:40 - volume so that we will see all the data
164:43 - that we uh created through our
164:46 - application again on Startup and that's
164:48 - how do loer volumes
164:51 - work in order to end that screen session
164:55 - that we just started because exit
164:57 - doesn't work in this case uh somehow on
165:00 - Mac you can actually click on control a
165:03 - k and then just type Y and the session
165:07 - will be closed so when you do screen LS
165:10 - you should see actually it's
165:13 - terminating congratulations you made it
165:15 - till the end I hope you learned a lot
165:17 - and got some valuable Knowledge from
165:19 - this course now that you've learned all
165:21 - about containers and Docker technology
165:24 - you can start building complex
165:26 - applications with tens or even hundreds
165:28 - of containers of course these containers
165:31 - would need to be deployed across
165:33 - multiple servers in a distributed way
165:36 - you can imagine what overhead and
165:38 - headache it would be to manually manage
165:40 - those hundreds of containers so as a
165:43 - Next Step you can learn about container
165:45 - orchestration tools and kubernetes in
165:48 - particular which is the most popular
165:50 - tool to automate this task if you want
165:53 - to learn about kubernetes be sure to
165:56 - check out my tutorials on that topic And
165:58 - subscribe to my channel for more content
166:00 - on modern devops tools also if you want
166:03 - to stay connected you can follow me on
166:06 - social media or join the private
166:08 - Facebook group I would love to see you
166:11 - there so thank you for watching and see
166:13 - you in the next video

Cleaned transcript:

hello and welcome to this complete Docker course by the end of this course you'll have a deep understanding of all the main Concepts and also a great big picture overview of how Docker is used in the whole software development process the course is a mix of animated theoretic explanations but also HandsOn demos for you to follow along so get your first handson experience and confidence using Docker in your projects so let's quickly go through the topics I'll cover in this course we will start with the basic concepts of what Docker actually is and what problems it solves also we'll understand the difference between Docker and virtual machine and after installing Docker we will go through all the main Docker commands to start and stop containers debug containers Etc after that we'll see how to use Docker in practice by going through a complete workflow with a demo project so first we'll see how to develop locally with containers then we'll run multiple containers or services with Docker compos we'll build our own Docker image with Docker file and we'll push that built image into a private Docker repository on AWS and finally we'll deploy our containerized application last but not least we'll look at how to persist data in Docker learning the different volume types and afterwards configure persistence for our demo project if you get stuck anywhere just comment under the video and I will try my best to answer your questions also you can join the private Tech worldwi community group on Facebook which is there to exchange your knowledge with others and connect with them if you like the course by the end of the video be sure to subscribe to my channel for more related content so let's get started so we'll talk about what a container is and what problems it solves we will also look at a container repository which is basically a storage for containers we'll see how a container can actually make the development process much easier and more efficient and also how they solve some of the problems that we have in the deployment process of applications so let's dive right into it what a container is a container is a way to package applications with everything they need inside of that package including the dependencies and all the configuration necessary and that package is portable just like any other artifact is and that package can be easily shared and moved around between a development team or development and operations Steam and that portability of containers plus everything packaged in one isolated environment gives it some of the advantages that makes development and deployment process more efficient and we'll see some of the examples of how that works in later slides so as I mentioned containers are portable so there must be some kind of a storage for those containers so that you can share them and move them around so containers live in a container repository this is a special type of storage for containers many companies have their own private repositories where they host or the where they store all the containers and this will look something like this where you you can push all of the containers that you have but there is also a public repository for Docker containers where you can browse and probably find any application container that you want so let's head over to the browser and see how that looks like so if I here search for a dockerhub which is the name of the public repository for Tucker I will see this official website so here if you scroll down you see that there are more than 100,000 container images of different applications hosted or stored in this Docker repository so here you see just some of the examples and for every application there's this official Docker container or Docker container image um but if you are looking for something else you can search it here and I see there's an official image for let's say Jenkins uh but there's also a lot of nonofficial images or container images that developers or or even from Jenkins itself that they actually store it here so public repository is where you usually get started when you're using or when you're starting to use the containers where you can find any application image so now let's see how containers improve the development process by specific examples how did we develop applications before the containers usually when you have a team of developers working on some application you would have to install most of the services on your operating system directly right for example you are developing some JavaScript application and you need a postp ql and you need red for messaging and every developer in the team would then have to go and install the binaries of those services and configure them and run them on their local development environment and depending on which operating system they're using the installation process will look actually different also another thing with installing services like this is that you have multiple steps of installation so you have a couple of commands that you have to execute and the chances of something going wrong and error happening is actually pretty high because of the number of steps required to install each service and this approach or this process of setting up a new environment can actually be pretty tedious depending on how complex your application is for example if you have 10 services that your application is using then you would have to do that 10 times on each operating system environment so now let's see how containers solve some of these problems with containers you actually do not have to install any of the services directly on your operating system because the container is its own isolated operating system layer with Linux based image as we saw in the previous slides you have everything packaged in one isolated environment so you have the posis ql with a specific version packaged with the configuration and the start script inside of one container so as a developer you don't have to go and look for the binaries to download on your machine but rather you just go ahead and check out the container repository to find that specific container and download on your local machine and the download step is just one Docker command which fetches the container and starts it at the same time and regardless of which operating system you're on the command the docker command for starting the container will not be different it will be exactly the same so if you have 10 applications that your JavaScript application uses and depends on you would just have to run 10 Docker commands for each container and that will be it which makes the setting up your local development environment actually much easier and much more efficient than the previous version also as we saw in the demonstration before you can actually have different versions of the same application running on your local environment without having any conflict so now let's see how containers can improve the deployment process before the containers a traditional deployment process will look like this development team will produce artifacts together with a set of instructions of how to actually install and configure those artifacts on the server so you would have a jar file or something similar for your application and in addition you would have some kind of a database service or some other service also with a set of instructions of how to configure and set it up on the server so development team would give those artifacts over to the operations team and the operations team will handle setting up the environment to deploy those applications now the problem with this kind of approach is that first first of all you need to configure everything and install everything directly on the operating system which we saw in the previous example that could actually lead to conflicts with dependency versions and multiple Services running on the same host another problem that could arise from this kind of process is when there is misunderstanding between the development team and operations because everything is in a textual guide as instructions so there could be cases where developers forget to mention some important point about configuration or maybe when operations team misinterpret some of those instructions and when that fails the operations team has to go back to the developers and ask for more details and this could lead to some back and forth communication until the application is successfully deployed on the server with containers this process is actually simplified because now you have the developers and operations working in one team to package the whole configuration dependencies inside the application just as we saw previously and since it's already encapsulated in one single environment and you don't have to configure any of this directly on the server so the only thing you need to do is run a Docker command that pulls that container image that you've stored somewhere in the repos repository and then run it this is of course a simplified version but that makes exactly the problem that we saw on the previous slide much more easier no environmental configuration needed on the server the only thing of course you need to do is you have to install and set up the docker runtime on the server before you will be able to run containers there but that's just onetime effort now that you know what a container concept is let's look at what a container is technically so technically container is made up of images so we have layers of stacked images on top of each other and at the base of most of the containers you would would have a Linux based image which is either Alpine with a specific version or it could be some other Linux distribution and it's important for those base images to be small that's why most of them are actually Alpine because that will make make sure that the containers stay small in size which is one of the advantages of using container so on top of the base image you would have application image and this is a simplified diagram usually you would have these intermediate images that will lead up to the actual application image that is going to run in the container and of course on top of that you will have all this configuration data so now I think it's time to dive into a practical example of how you you can actually use a Docker container and how it looks like when you install it and download it and run it on your local machine so to give you a bit of an idea of how this works let's head over to dockerhub and search for postris ql so here which is a Docker official image I can see some of the versions and let's say say I'm looking specifically for older version I don't know 96 something so I'm going to pull that one so this is a dock repository so that I can actually go ahead and pull the containers from that repository directly and because it's a public repository I don't have to log into it I don't have to provide any authentication credentials or anything I can just get started with simple Docker command without doing or configuring anything to access dockerhub so on my terminal I can just do Docker Pole or I can even do Docker run and then just copy the the image name and if I don't specify any version it will just give me the latest but I want a specific version so I'm just I'm going to go with 9.6 actually just to demonstrate so I can provide the version like this with a column and I can start run so as you see the first line says unable to find image locally so it knows that it has to go to dockerhub and pull it from there and the next line says pulling from library postgress and here you see a lot of hashes that says downloading and the this is what I mentioned earlier which is Docker containers or any containers are made up of layers right you have the Linux image layer you have the application layers and so on so what what you see here are actually all those layers that are separately downloading from the dockerhub on my machine right and the advantage of splitting those applications and layers is that actually for example if the image changes or I have to download a newer version of pogress what happens is that the layers they're the same between those two applications two versions of posis will not be downloaded again but only those layers that are different so for example now it's going to take around 10 or 15 minutes to download this one image because I don't have any pogress locally but if I were to download the next version it will take a little bit less time because some layers already exist on my local machine so now you see that it's already logging because it this command that I ran here the docker run with the container name and version it fetches or it pulls the the container but it also starts it so it executes the start script right away as soon as it downloads it and here you see the output of the starting of the application so it just gives some output about um starting the server and doing some configuration stuff and here you see database system is ready to accept connections and launcher started so now let's open the new tab and see with Docker PS command you can actually see all the running containers so here you see that postgress 96 is running and it actually says image so this is another important thing to understand when we're talking about containers there are two technical terms image and a container and a lot of people confuse those two I think and there is actually a very easy distinction between the two so image is the actual package that we saw in one of those previous slides so the application package together with the configuration and the dependencies and all these things this is actually the artifact that is movable around is is actually the image container is when I pull that image on my local machine and I actually started so the application inside actually starts that creates the container environment so if it's not running basically it's an image it's just an artifact that's lying around if I start it and actually run it on my machine it is a container so that is the distinction so here it says the active running containers with a container ID image that it's running from and some entry commands that it executed and some other status information so this means that poql is now running on my local machine simple as that if I were now to uh need let's say another version of pogress to run at the same time on my local machine I could just go ahead and say let's go back and let's say I want want to have 9.6 and 10.10 running at the same time on my local machine I just do run postgress and run again it doesn't find it locally so it pushes and this is what I actually explained to you earlier because it's the same application but with just a different version version some of the layers of the image are the same so I don't have to fetch those again because they are already on my machine and it just fetches the layers that are different so that saves a little bit of uh time and I think it's it could be actually good Advantage so now we'll wait for other image layers to load so that we have the second postgress version running and now you see I have postgress 9.6 running in this uh command line tab and I have postgress version 10.10 running in the next one so I have two postes with different versions running and I can actually output them here have both of them running and there's no conflict between those two like I can actually run any number of applications with different versions maybe of the same application with no problem at all and we going to go through how to use those containers in your application and the port configuration and some of the other configuration stuff later in this tutorial when we do a deep dive but this is just for you to get the first visual image of how Docker containers actually work how they look like and how easily you can actually start them on your local machine without having to implement lement a specific version of postgress application and do all the configuration yourself when I first started learning Docker after understanding some of the main Concepts my first question was okay so what is the difference between Docker and an Oracle uh virtual box for example and the difference is quite simple I think and in this short video I'm I'm going to cover exactly that and I'm going to show you the difference by explaining how docu works on an operating system level and then comparing it to how virtual machine works so let's get started in order to understand how Docker works on the operating system level let's first look at how operating system is made up so operating systems have two layers operating system kernel and the applications layer so so as you see in this diagram the kernel is the part that communicates with the hardware components like CPU and memory Etc and the applications run on the Kernel layer so they are based on the Kernel uh so for example we you all know Linux operating system and there are lots of distributions of Linux out there there's buonto and Debian and there's Linux Mint Etc there are hundreds of distributions they all look different so the graphical user the interface is different the file system is maybe different so a lot of applications that you use are different because even though they use the same Linux kernel they use different or they Implement different applications on top of the kernel so as you know Docker and virtual machine they're both virtualization tools so the question here is what parts of the operating system they virtualize so docker virtualizes the applications layer so when you download a Docker image it actually contains the applications layer of the operating system and some other applications installed on top of it and it uses the kernel of the host because it doesn't have its own kernel the virtual box or the virtual machine on the other hand has the applications layer and its own kernel so it virtualizes the complete operating system which means that when you download a virtual machine image on your host it doesn't use your host kernel it boots up its own so what does this difference between Docker and virtual machine actually mean so first of all the size of Docker images are much smaller because they just have to implement one layer so Docker images are usually couple of megabytes uh virtual machine images on the other hand can be couple of gigabytes large a second one is the speed so you can run and start Docker containers much faster than the VMS because they every time you start them you have they have to put the operating system kernel and the applications on top of it the third difference is compatibility so you can run a virtual machine image of any operating system on any other operating system host but you can't do that with Docker so what is the problem exactly let's say you have a Windows operating system with a kernel and some applications and you want to run a Linux based Docker image on that Windows host the problem here is that a Linux based Docker image might not be compatiable with the windows kernel and this is actually true for the windows versions below 10 and also for the older Mac versions which if you have seen how to install Docker on different operating systems you see that the first step is to check whether your host can actually run Docker natively which basically means is the kernel compatible with the docker images so in that case a workaround is that you install a technology called Docker toolbox which abstracts away the kernel to make it possible for your host to run different Docker images so in this video I will show you how to install Docker on different operating systems the installation will differ not only based on the operating system but also the version of that operating system so you can actually watch this video selectively depending on which OS and the version of that OS you have I will show you how to find out which installation step applies to you in the before installing section which is the first one so once you find that out you can actually directly skip to that part of the video where I explained that into details I will put the minute locations of each part in the description part of the video and also I will put all the links that I use in the video in the description um so that you can easily access them also if you have any questions during the video or if you get stuck installing the docker on your system please post your question or problem in the comment section so that I can um get back to you and help you proceed or maybe someone from the community will uh so with that said let's let's dive right into it so if you want to install Docker you can actually Google it and you get an official documentation of Docker um it's important to note that there are two additions of Docker there is a community and Enterprise additions um for us to begin with Community Edition will be just fine in the docker Community Edition uh tab there there's a list of operating systems and distributions in order to install docker so for example if we start with Mac we can click in here and we see the documentation of how to install it on Mech which is actually one of the easiest but we'll see some other ones as well so before you install Docker on your Mac or Windows computer there are prerequisites to be considered so for mac and windows there has to be some criteria of the operating system and the hardware met in order to support running Docker if you have Mech go through the system requirements to see if your U Mech version is actually supporting Docker if you have Windows then you can go to the windows Tab and look at the system requirements there or what to know before you install for example one thing to note is that Docker natively runs only on Windows 10 so if you have a Windows version Which is less than 10 then Docker cannot run natively on your computer so if your computer doesn't meet the requirements to run Docker there is a workaround for that which is called Docker toolbox instead of Docker you basically just have to install a Docker toolbox that will become a sort of a bridge between your operating system and the docker and that will enable you to run Docker on your leg Legacy computer so if that applies to you then skip ahead in this video to the part where I explain how to install Docker toolbox on Mac and on windows so let's install Docker for Mac as you see here there are two um channels that you can download the binaries from or the application from we will go with the stable Channel and other things to can see if you have an older version of Mech either software or the hardware please go through the system requirements to see if you can actually install Docker so here there is a detailed description of what make version you need um to be able to run Docker and also you need at least four gab of RAM and by installing Docker you will actually have the whole package of it which is a Docker engine uh which is important or which is necessary to run the docker containers on your laptop the docker command line client which will enable you to execute some Docker commands Docker compose if you don't know it yet don't worry about it but it's just technology to orchestrate if you have multiple containers um and some other stuff that we're not going to need in this tutorial but you will have everything in a package installed so go ahead and download the stable version well I already have Docker installed from The Edge channel so I won't be installing it again but it shouldn't matter because the steps of installation are the same for both so once the docker DMG file is downloaded you just double click on it and it will pop up this window just drag the docker whale app into the applications and it will be installed on your Mach as the next step you will see Docker installed in your applications so you can just go ahead and start it so as you see the docker sign or icon is starting here if you click on it you see the status that Docker is running and you can configure some preferences and check the docker version and so on and if you want to stop Docker or quit it on your Mech you can just do it from here um an important maybe interesting note here is that if let's say you download or install Docker and you have uh more than one accounts on your laptop you will actually get some errors or conflicts if you run Docker at the same time or multiple accounts so what I do for example is that if I switch to another account where I'm also going to need Docker I quit it from here and then I start it from other account so that I don't get any errors so that might be something you need to consider if you use multiple accounts so let's see how to install Ducker for Windows the first step as I mentioned before is to go to the before you install section and to see that your operating system and your computer meets all the criteria to run Docker natively so if you're installing Docker for the first time don't worry about most of these parts like Docker toolbox and Docker machine there are two things that are important one is to double check that your windows version is compatible for Docker and the second one is to have virtualization enabled virtualization is by default always enable abled um other than you manually disabled it so if you're unsure then you can check it by going to the task manager performance CPU Tab and here you can see the status of the virtualization so once you have checked that and made sure that these two prerequisites are met then actually you can scroll up and download the windows installer for from the stable Channel once they install installer is downloaded you can just click on it and follow the installation wizard to install Docker for Windows once the installation is completed you have to explicitly start Docker because it's not going to start automatically so for that you can just go and search for the docker for Windows app on your windows just click on it and you will see the docker whale icon um starting and if you click on that icon you can actually see the status that says stalker is now up and running so this is basically it for the installation now let's see how to install Docker on different Linux distributions and this is where things get a little bit more complicated so first of all you see that in this menu on the on the left you see that for different Linux distributions the installation steps will differ but also for example if we just click on auntu for the guide you can see that in the prerequisites section there is also differentiation between the versions of the same Linux distribution and there may be some even more complicated scenarios where the combination of the version of the distribution and the architecture it's running in um also makes some difference into how to set up Docker on that specific environment because of that I can't go through a Docker installation process of every Linux environment because there are just too many combinations so instead what we'll do is just go through a general overview of the steps and configuration process to get Docker running on your Linux environment and you can just adjust it then for your specific setup so these are some general steps to follow in order to install Docker on your Linux Linux environment first of all all you have to go through the operating system requirements part on the relevant Linux distribution um that applies for you a second step in the documentation to is to uninstall old versions however if it's the first time you installing Docker then you don't have to worry about that you also don't have to worry about the supported storage drivers and you can skip ahead to the part of installing Docker Community Edition so for any Linux distribution here the steps will be or the options for installing Docker will be the same so first option is basically to set up a repository and download the docker from and install it from the docker repository um the second option is to install the packages manually however I wouldn't recommend it and I think the documentation doesn't recommend it either because then you'll have to do a lot of steps of the installation and the maintenance of the versions manually so I wouldn't do that the third one is just for the testing purposes it may be enough for the development purposes as well but I would still not do it which is basically just downloading some automated scripts that will install and setup Docker on your Linux environment however again I wouldn't go with it I would actually just do the first option which is just download Lo in the docker from the repository so in order to install Docker using the first option which is downloading it from the docker repositories you have two main steps so the first one is to set up the repository uh which differs a little bit depending on which distribution you have and then install the docker CE from that repository so from abunto and Debian the steps for setting up the repository are generally just updating your app package then setting up an https connection with the repository and adding the docker's official gpg key which only aono and dbn need you don't have to do this um steps for SOS and Fedora they just have to install the required packages and the last step uh for setting up the repository is basically setting up the stable repository of Docker which we saw previously on the overview that there are two channels which is a stable and Edge here you always have to set up the stable repository optionally you can also set up the edge repository but I'll just do uh stable this time and here also something to notice depending on architecture you have to actually set it or you have to set that as a parameter when you set up the repository so if you have for example a different architecture you can use those steps to display the correct command for it and um I guess that applies to other Linux distributions as well like for example here you also have the second tab where you see a separate command for it so these steps should actually um set up the repository so that as a Next Step you can then install the docker C from those repositories so installing Docker from the setup repository is actually pretty straightforward the steps are same for or similar to all the distributions basically just update the app package and then you just say install Docker CE so this command will just download the latest Docker version if you want to install a specific one which you will need to do in a production environment then you can just uh provide a version like like this to just say Docker minus C equals some specific versions and using this command you can actually look up what versions are available in that repository that you just and with this command actually Docker will be installed um on your Linux environment and then you can just verify using PSE sudo Docker run hello world which is this demo image of Docker you can verify that Docker is running and this will start hello world Docker container on your environment so as I mentioned previously for environments um that do not support running Docker natively there is an workaround which is called Docker toolbox so Docker toolbox is basically an installer for Docker environment setup on those systems so this is how to install uh Docker toolbox on your Mac um this is the whole package that comes with the installation of Docker toolbox which is basically the docker command line Docker machine Docker compose basically all the packages that we saw in the native installation and in on top of that you also get the Oracle VM virtual box so in order to install the docker toolbox it's actually pretty straightforward on this website you can go to the toolbox releases where we have all the leas of latest releases you just take the uh latest release and here you see two assets this one is for Windows obviously and you just download the package for mac and once it's downloaded you just click on it and go through the installation wizard leave all the options by default as they are do not change anything and after the installation you can just validate that the installation is successful and you can actually run Docker so so after seeing the installation was successful screen just go and look up in your launch pad dock quick start terminal and once you open it you should be able to run uh Docker commands and you can just try Docker run hello world which should just start up or bring up um this hello world Docker container on your environment so now let's see how to install Docker toolbox on Windows here see that you get the whole package of Docker Technologies with a toolbox which are basically the same package which you get on the uh Native Docker installation and on top of that you get Oracle VM virtual box which is the tool that enables Docker to run on an older system so before you install Docker tool books you have to make sure that you meet some of the preconditions number one you have to make sure your Windows system supports virtualization and that virtualization must be enabled otherwise Docker Docker won't start so depending on which Windows version you have looking up or checking the virtualization status will be different so I just suggest you Google it and look it up of how to find the virtualization status to see that it's enabled once you have that checked also make sure that your Windows operating system is 64 bits so if those two criteria are met then you can go ahead and install the Locker toolbox the place where you see the releases or the release artifacts is toolbox releases link here which I have open so it's basically a list of the releases you just take the latest one which has two artifacts this is the one for Windows you just download this executable file click on it and go through the installation wizard once the installation is completed there are just couple of steps here you can verify that Docker was installed or the toolbox was installed by just looking up the docker quick start terminal on your windows that app must be installed and once you click on it and open it you should be able to run Docker commands in the terminal so the basic Docker command that you can test will be Docker run hello world which will just fetch this basic uh Docker container from the public registry and run it on your computer if that command is successful it means that Docker was successfully installed on your computer and now you can proceed with the tutorial so in this video um I'm going to show you some basic Docker commands at the beginning I'm going to explain what the difference between container and images because that's something a lot of people confuse then very quickly go through version and tag and then show you a demo of how to use the basic Docker commands um commands that will be enough to pull an image locally to start a container to configure a container and even debug the container so with that said let's get started so what is the difference between container and image mostly people use those terms interchangeably but actually there is a fine difference between the two to see theoretically container is just the part of a container runtime so container is the running environment for an image so as you see in this graphic the application image that runs the application could be postgress redis some other application needs let's say a file system where it can save the log files or where it can store some configuration files it also needs some environmental configuration like environmental variables and so on so all of this environmental stuff are provided by container and container also has a Port that is binded to it uh which makes it possible to talk to the application which is running inside of a container and of course it should be noted here that the file system is virtual in container so the container has its own abstraction of an operating system including the file system and the environment which is of course different from the file system and environment of the host machine so in order to see the difference between container and image in action let's head over to the docker Hub and find for example a rice image another thing is that dockerhub all the artifacts that are in the docker Hub are images so we're not talking about containers here all of these things are images Docker official image so we're going to go ahead and pull a rad image out of the doah Hub to my laptop so you see the different layers of the image are downloaded and this will take a couple of minutes so once the download is complete I can check all the existing images on my laptop using Docker images command so I see I have two images radi and postgress with text image IDs and so on another important aspect of images is that they have texts or versions so for example if we go back to the docker Hub each one each image that you look up in the docker Hub uh will have any different versions the latest is always the one that you get when you don't specify the version of course if you have a dependency on a specific version you can actually choose the version you want and specified and you can select one from here so this is what you see here the tag is basically the version of the image so I just downloaded the latest and I can also see the size of the image so now to this point we have only worked with images there is no container involved and there is no redish running so now let's say I need red running so that my application can connect to it I'll have to create a container of that redice image that will make it possible to connect to the redis application and I can do it by running the redis image so if I say Docker Run Red this will actually start the image in a container so as I said before container is a running environment of an image so now if I open a new tab and do Docker PS I will get status of all the running Docker containers so I can see the container redis is running with a container ID based on the image of redis and some other information about it for example the port that it's running on and so on so as you see here the docker run redis command will start the redis container in the terminal um in an attached mode so for example if I were to terminate this with the control C you see that red application stops and the container will be stopped as well so if I do Docker PS again I see that no container is running so there is an option for Docker run command that make makes it able makes it possible to run the container in a detached mode and that is minus D so if I do dock run minus D redis I will just get the ID of the container as an output and the container will stop running so if we check again Docker PS I see the container with the ID starting with 838 which is the same thing here is running so this is how you can start it in the detached mode now for example if you would want to restart a container because I don't know some uh the application crashed inside or some error happened so you want to restart it you would need the doc container ID so just the first part of it not the whole string and you can simply say Docker stop ID of the container and that will stop the docker container nothing running if you want to start it again you can use the same ID to start again so let's say you stop Docker container at the end of the day you go home you come back the next day open your laptop and you want to restart the stopped container right so if you do Docker PS there is uh the output is empty you don't see any containers so what you can do alternative to just looking up your history command line history is you can do Docker PS minus a which will show you all the containers which uh are running or not running so here you see the container ID again and you can restart it okay so let's try another thing let's say you have two parallel applications that both use redish but in different versions so you would need two redish containers with different image versions running on your laptop right at different times maybe or at the same time so here we have the latest one which is radius 56 and let's head over to the dockerhub and select uh version let's say you need version 4 o so remember the first time that we downloaded the redis image we did Docker pull redis um however if you run Docker if you use Docker run with redice image and the tech which was 4.0 it will pull the image and start the container right away after it so it does two commands basically in one so it's Docker pole that Docker start in one one command so if I do this it says it can't find the image locally so it goes and pulls the image from the repository to my laptop and again we see some layers are downloaded and the container is started right away and now if I do Docker PS you see that I have two radices running so this is where it gets interesting now how do you actually use any container that you just started so in this output we you also see the ports section which specifies on which Port the container is listening to the incoming requests so both containers open the same port which is what was specified in the image so in the logs of the container you can see the information running mode stand loone Port 6379 so how does that actually work and how do we not have conflicts while both are running on the same port so to explain that let's head over to our slide and see how this works as you know container is just the virtual environment running on your host and you can have multiple containers running simultaneously on your host which is your laptop PC whatever you working on and your laptop has certain ports available that you can open for certain applications so how it works is that you need to create a socalled binding between a port that your laptop your host machine has and the container so for example in the first container part here you see container is listening on Port 5000 and you bind your laptops Port 5,000 to that containers now you will have conflict if you open two 5,000 ports on your host because you will get a message the port is already bound or is already in use you can do that um however you can have two containers as you see the second and third containers are both listening on Port 3000 which is absolutely okay as long as you bind them to two different ports from your host machine so once the port binding between the host and the container is already done you can actually connect to the running container using the port of the host so in this example URI you would have some app Local Host and then a port of the host and the host then will know how to forward the request to The Container using the port binding so if we head back here you see that containers have their ports and they're both running on the same one however we haven't made any binding between my laptop's port and the container port and because of that the container is basically unreachable by any application so I won't be able to use it so the way we actually do that is by specifying The Binding of the ports during the Run command so I'm going to break this and check that there is just one container running now I'm going to stop the other one as well so we can start the menu okay so we see both containers are here so now we want to start them using The Binding between the host and the container ports but again we have two Rices so we need to bind them to two different ports on my laptop so the way to do it is you do Docker run and you specify with minus P the port of the host that's the first one so let's go with 6,000 it doesn't really matter in this case and the second one is the port that you're binding this two which is the container Port so we know the container Port will be 6379 and this is where we bind our so my laptop's port 60002 and if I do this I forgot ra here so and now if I do Docker PS let's actually clean this Docker PS again here you see The Binding here all right so your laptops 6,000 Port is bound to the containers 6 37 9 so now let's do another thing and let's start it in a detached mode so like this let's check again it's running again and now I want to start the second container let's clear this again so here you see it created a bunch of containers because uh when I specified different options with the port binding it actually created new containers um that's why you see a couple of more here so I'm going to copy the image name with the tag for uh. o minus P so for example if I were to do this now and I would try to run the other redis the second redis container with the same port on my laptop I would get an error saying Port is already allocated so I can do 60001 and run it again I'll run it in detached mode so that I'm see port and if I go go over here and say Docker PS I see that I have two different radi versions running both of them bound to different ports on my laptop and the containers themselves listening to request on the same port so so far we have seen a couple of basic Docker commands we have seen Docker pull which pulls the image from the repository to local environment we also saw run which basically combines Docker pull and Docker start pulls the image if it's not locally available and then starts it right away then we saw Docker start and Docker stop which makes it possible to restart a container if um you made some changes and you want to um create a new version which makes possible to restart a container if you need to um we also saw docu run with options the one option that we saw was D minus D which is detach so you can run the container in DET detached mode so you can use terminal again minus P allows you to bind Port of your host to The Container so very important to remember minus P then comes the port of your host and then comes the port of your um container whatever it might be we also saw doap PS doap PS minus a which basically gives you all the containers no matter if they're running currently or not we also saw Docker images which gives you all the images that you have um locally so for example if after a couple of months you decide to clean up your space and get rid of some stale images you can actually check them check the list and then go through them and uh delete them you can do the same with stale Docker containers that you don't use anymore or you don't need anymore you can also get rid of them so the final part of the docker basic commands are commands for troubleshooting which are very very useful if something goes wrong in the container you want to see the logs of the container or you want to actually get inside of container get the terminal and execute some commands on it so let's see Docker PS we have two containers running right now we don't have any out we don't see any locks here so let's say something um happens your application cannot connect to redies and you don't know what's happening so ideally you would want to see what logs radi container is producing right the way to do that is very easy you just say Docker locks and you specify the container ID and you see the locks you can also do the lock locks if you don't want to uh remember the container ID or to Docker PS all the time you can remember the name of the container and you can get the logs using the name so a little side note here um as we're talking about the names of the containers so here as you see when a container is created you just get some random name like this so you can name your containers as you want um using another option of the docker run which might be pretty useful sometimes if you don't want to work with the container IDs and you just want to remember the names um or if you just want to differentiate between the containers so for example let's create a new container from r4.0 image using a different name that we choose so I'm going to stop this container and I'm going to create a new one from the same image so let's run it in the detached mode Let's Open the port th000 1 2 6 3 7 9 and give the name to the container and let's call since it's the older um version let's call it red older and we need to specify the image so remember this will create a new container since we're running the docker one command again so if we execute this and check again we see the redis 4.0 image based container is created which is um a fresh new you can see in it created and the name is red is older and we can do the same for the other container so that we kind of know which uh container is what so I'll stop this one and I will use the same command here this will be the latest and I will call this radius latest and since find another Port so I'm going to run it and let's see so here I have two containers running now I know R is older R is latest so for example if the older version has some problems I can just do logs R is older and I can get get my locks so another very useful command in debugging is Docker exit so what we can do with Docker exit is we can actually get the terminal of a running container so let's check again we have two containers running and let's say there is some problem with the latest ready latest container and I want to get a terminal of that container and to maybe navigate a directory inside check uh the lock file or maybe check the configuration file or uh print out the environmental variables um whatever so in order to do that we use Docker exit command with minus t which stands for interactive terminal then I specify the container ID and I say in so I get the B and here you see that the the cursor changed so I'm in inside of the container as a root user and here if I say LS okay the data is empty I can also print out in which directory I am I can go to home directory see what's there um so I have my virtual file system inside of a container and here I can um navigate the different directories and I can check stuff I can also print all the environmental variables to see that something is set correctly um and do all kinds of stuff here and this could be really useful if you have a container with some complex configuration or if for example you are running your own application that you wrote in a container H and you have some complex configuration there um or some kind of setup and you want to validate that everything um is correctly set in order to exit the terminal you just do exit and you're out you can also do the same using the name again if you don't want to work with the IDS and you just want to remember the names of the container to make it easier you can do it with the name as well same thing since most of the container images are based on some lightweight Linux distributions you wouldn't have much of their Linux um commands or applications installed here for example you wouldn't have a curl or some other stuff so you were a little bit more limited in that sense so you can execute a lot of stuff from the docker containers for most of the debugging work um it should be actually enough so the final part to review the difference between Docker run and Docker start which might be confusing for some people let's revisit them so basically Docker run is where you create a new container from an image so Docker run will take an image with a specific version or just latest right as option or as an attribute with Docker start you not working with images but rather with containers so for example um as we saw Docker run has a lot of options you specify with minus D and minus P the port binding and then you have this name of the container and all this stuff so basically you tell Docker at the beginning what kind of container with what attributes name and so on to create from a specific image but once the container is created and you can see that using a con uh the command so for example here the last ones that we created and if you stop it and you want to restart it you just need to use the command do start and specify the container ID and when you start it the container will retain all the attributes that we defined when creating the container using Docker run so Docker run is to create a new container Docker start is to restart a stopped container so once you've learned the docker basic concepts and understood how it works uh it's important to see how Docker is actually used in practice so in software development workflow you will know you have this uh classical steps of development and continuous delivery or continuous integration uh and then eventually gets deployed on some environment right could be a test environment develop environment so it's important to see how Docker actually integrates in all those steps so in the next couple of videos I'm going to concentrate exactly on that so we're going to see some overview of the flow and then we're going to zoom in on different parts and see how Docker is actually used in those individual steps so let's consider a simplified scenario where you are developing a JavaScript application on your laptop right on your local development environment your JavaScript application uses a mongodb database and instead of installing it on your laptop you download a Docker container from the docker Hub so you connect your JavaScript application with the mongodb and you start developing so now let's say you develop the application first version of the application locally and now you want to test it or you want to deploy it on the uh development environment where a tester in your team is going to test it so you commit your JavaScript application in git or in some other version control system uh that will trigger a continuous um integration a Jenkins build or whatever you have configured and Jenkins build will produce artifacts from your application so first it will build your JavaScript application and then create a Docker image out of that JavaScript artifact right so what happens to this Docker image once it gets created by Jenkins build it gets pushed to a private Docker repository so usually in a company you would have a private repository because you don't want other people to have access to your image images so you push it there and now as a Next Step could be configured on Jenkins or some other scripts or tools that Docker image has to be deployed on a development server so you have a development server that pulls the image from the private repository your JavaScript application image and then pulls the mongodb that your JavaScript application depends on from a dockerhub and now you have two containers one your custom container and a publicly available mongodb container running on dev server and they talk to each other you have to configure it of course they talk and communicate to each other and run as an app so now if a tester for example or another developer logs in to a Dev server they be they will be able to test the application so this is a simplified workflow how Brer will work in a real life development process in the next videos I'm going to show you HandsOn demo of how to actually do all this in practice so in this video we are going to look at some practical example of how to use Docker in a local development process so what we're going to do is a simple demo of a JavaScript and nodejs application in the back end to simulate the local development process and then we're going to connect it to a Docker container with a mongodb database in it so let's get started so in this video we're going to see how to work with Docker containers When developing applications so the first step will be is we're going to develop a very simple UI backend uh application using JavaScript very simple HTML structure and nodejs in the back end and in order to integrate all of this in the database we are going to use a Docker container of a mongodb database and um also to make working with the mongodb much easier so we don't have to execute commands in in the terminal we're going to deploy a Docker container of a UI which is called the Express where we can see the database structure and all the updates that our application is making in the database so this development setup should give you an idea of um how Docker containers are actually used in development process so I've already prepared some very simple JavaScript application um so in order to see the code basically we have this index HTML that is very simple code and we have some JavaScript here and we're using nodejs backend that just serves that index HTML file and listens on Port 3000 so we have the server running here in the back end and we have the UI that looks like this so basically it's just a user profile page with some user information and user can edit their profile so if I for example change the name here um and if I change the email address and do changes like this I can save my profile and I have my updates here um however if I refresh the page of course the changes will be lost because it's just JavaScript no JS so there is no persistent compon component in this application so in order to have that which is actually how real life applications work you'll know that you need to integrate the application with a database so using that example I will try to Showcase you how you can actually use the docker containers to make the development process Easier by just pulling one of the databases and attaching it or connecting it to the application so in this case we're going to go with the mongodb application and uh in addition to mongodb contain container we're going to also deploy a mongodb UI which is its own container it's called Express where we can manage or see the database insights and updates from our application much easier so now let's see how that all works so in order to get started let's go to dockerhub and find our uh mongodb image so here let's go to and we have mongodb here actually and the Express which is another dock container that we're going to use for the UI so first let's pull the mongodb official image so I I already have mongodb latest so pulling doesn't take longer on my laptop but you're going to need a couple of seconds probably and the next one we're going to pull is the docker Express which I also have I believe so let's see yes it's also fast so if I check locally I have mongod TB and Express images so the next step is to run both and Express uh containers in order to make the mongod DB database available for our application and also to connect the Express with the mongod DB container so let's do the connection between those two first in order to do that we have to understand another Docker concept Docker Network so how it works is that Docker creates its isolated Docker Network where the containers are running in so so when I deploy two containers in the same docken network in this case and Express they can talk to each other using just the container name without Local Host port number Etc just the container name because they're in the same network and the applications that run outside of doer like our nodejs which just runs from node server is going to connect to them from outside or from the host using Local Host and the port number so later when we package our application into its own Docker image what we're going to have is again Docker network with mongodb container Express container and we're going to have a nodejs application which we wrote including the index HTML and JavaScript for front end in its own doc container and it's going to connect to the mongodb and the browser which is running on the host but outside the docker network is going to connect to our JavaScript application again using host name and the port number so Docker by default already provides some networks so if we say Docker Network LS we can already see these autogenerated Docker networks so we have four of them with different names and the drivers we're not going to go into details here but what we're going to do is create its own network for the mongodb and the Express and we're going to call it Network so let's do this right away going to say Docker Network create and we are going to call it Network so now if I do dock Network LS again I see my docken network has been created so now in order to make our m B container and the Express container run in this Network we have to provide this network option when we run the container in the docker run command so let's start with the so we all know that Docker run is the command to start a container from an image right so we have Docker run which is the basic Docker run command however in this case we want to specify a couple of things um as you learned from the previous videos you have to specify something called Port so we need to open a port of mongodb the default Port of mongodb is 27,7 so we will take that Port actually for both host and container so will run at this port inside of a container and we open the same port on the host so that will take care of the port then we will run it in a detach mode in addition to that there are a couple of things that we can specify when starting up the container and these are environmental variables of the mongot TB let's see um in the official image description you actually have couple of documentation about how to use the image which is very helpful to kind of understand what kind of configuration you can uh apply to it here you see some environmental variables so basically on Startup you can Define what the root username and the password will be which is very um handy because we're going to need those two for the express to connect to the and you can also specify the init database we're just going to provide the username and password because we can create the database from the Express UI later so let's do that and the way you can specify the environmental variables you can actually see here as well is by just let's copy this one so here you say environmental variable that's what the minus E Flex stands for and root username we'll say it's admin and another variable which is the password will be just password so in this way we can actually overwrite what the default username and password will be so two more things that we need to configure in this uh command our container name because we're going to need that container name to connect with the Express so we'll call this one DB let's say and another one we need is the network that we created which was called Network so in order to make this command a little bit more structured do it on multiple lines so let's see so it's more readable so basically all the options or all these flags that we set um to go one more time through them it it's going to start in detached mode uh we are opening the port on the host um username and password that we want mongodb to use uh in the startup process we're going to rewrite or overwrite the name of the container and this container is going to run in a Network and this should actually start the container okay so if you want to see whether it was successful we can log the container and see what's happening inside so as we see was started and everything actually looks good waiting for connections on Port 27,7 okay so now let's start Express we want Express to connect to the running mongod DB be container on Startup and here we have an example of how to run it and here we have the list of environmental variables that we can configure so let's quickly look at them username password we don't need them however we need the admin username and admin password of the motb this is actually what we overwrote with admin and password so we're going to use them because Express will need some username password to authenticate with the mongodb and to connect it the port is by default the correct one so we don't need to change that um and this is an important part this is the mongodb server right so basically this is the container name that Express will use to connect to the docker and because they running in the same network only because of that this configuration will work if I didn't if I hadn't specify the network then I could have I could specify the name correct name here of the container but it wouldn't work so with that said let's actually create the docker run command for Express as well so let's clear the history and let's start so again we run it in detached mode and let's see what parameters we need so first of all Port let's say what is the default Port that the express runs on that's 80 81 so we'll take that so basically it's going to run on our laptop on Port 8081 the next option would be these two and remember environmental variables need to be specified with minus E and this is the username of mongodb admin which is admin because we specified it when we started the mongodb container this is the password let's set this one as well don't forget the network minus minus net Network we have the name we can also call it Express and let's see what else we might need here yes this is important one um and our container name let's actually see it again toer PS the one running it's called mongodb that's the container name and this is what we need to specify here so I'm going to write this here and finally the image is called Express so I'm just going to copy this one here and that is it so basically with these commands do Express should be able to connect to the mongod Deb container so let's run it and just to make sure let's log the container and see what's happening there waiting for mongodb welcome to express it looks like it connected successfully um it says here database connected and the Express is available at Port 8081 so let's check the Express out at the Port 881 so actually let's close these tabs we don't need them anymore and here if I say Local Host 881 I should be able to see the manga Express so these are the databases that already exist by default in or which are created on Startup and using the UI we can create our own database as we saw previously we could have specified an environmental variable init DB on mongodb Startup and that would have created a new database however it doesn't matter we will just create a database name here so we will call it user um account database so let's create one and now we can actually use it or connect to this database from node.js so let's see how that works so now we have the mongodb container and the Express container running so let's check that we have both of them we'll have to connect nodejs with the database so the way to do it is usually to give a protocol of the database and the URI and the URI for a mongodb database would be Local Host and the port that it's accessible at I already went ahead and prepared the code for node.js so basically we are going to use a client here which is a node module and using that client we are connecting to the mongodb database so this is the protocol the host and the port that we just saw that the mongodb is listening at and username and password of the root user of mongodb of course usually you wouldn't put the password here or not use an admin or root uh username password to connect to a database but this is for just the demonstration purposes and these are username and password that we set as environmental variables when we created the docker mongodb container so let's check that so this is the mongodb uh container command and this is the username root and root password that we specified and this is what we are going to use in the code as I said for demonstration purposes I will write the password directly here so then we connect to the database um so I also went ahead and in the Express user account database and inside that I created collection which is like a table in my SQL world called users so here I connect to user account database and I query The Collection users and this is a get request so I'm just fetching something from the database and this is update uh request same thing I connect to the database using the same URI and the database name and I update or insert something in the collection so now let's see how all that works so let's head over to the UI so in the users collection there is no data it's empty so we're going to refresh it and edit the data so I'm going to write here some and update and refresh we see that a new insert was made so this is the update profile section here so all this was executed it connected to the mongodb and now we have one entry which is email coding name that we changed so if I'm going to refresh it now I fetched the newly inserted user data in the UI and I displayed it here and also if you want to see what the mongod container actually logs during this process we can actually look at the logs so I'm going to say docks and log using the container ID so let's say if I wanted to see just the last part of it because I want to see what the last activity was I can also let's clear this and I can also do tail so I can just display the the last part of it or if I want it I could also stream the logs so I'll clear this again and I'll say stream the logs so I want have to do dockal logs all the time so if I make a line here for example to Mark the last logs I can refresh it let's make some other changes let's change it to own and save profile so I'm going to see some activity here as well so these connections are new and it also says received client metadata and this is where the nodejs request comes in with the nodejs and its version and at the end of each communication there is an end connection because we end the database connection at the end so we see that also in the logs so if for example something wasn't working properly you could always check them in the logs here so with that I have a fully functional JavaScript nodejs application which has a persistence in the mongodb database and we also have uh UI both of them running in a Docker container so this would be uh somehow an realistic example of how local development using Docker containers would look like so in the last video we created and started two Docker containers mongodb and Mong Express and these are the commands that we used to make it happen right the first we created a network where these two containers can can talk to each other using just the container name and no host Port Etc is necessary for that um and then we actually ran two Docker run commands with all the options and environmental variables Etc set now uh this way of starting containers all the time is a little bit tedious and you don't want to execute these run commands all the time on the command line terminal especially if you have a bunch of Docker containers to run you probably want to automate it or just make it a little bit easier and there's a tool that's that makes running multiple Docker containers with all this configuration much easier than with Docker run commands and that is Docker compose if you already know Docker comp post and you are wondering why is it useful and what it actually does then bear with me in the next slide I'm going to explain that so this is a Docker run command of the mongodb that we executed previously so basically with Docker compose file what we can do is we can take the whole command with its configuration and map it into a file so that we have a structured commands so if you have have let's say 10 Docker containers that you want to run for your application and they all need to talk to each other and interact with each other you can basically write all the Run commands for each container in a structured way in the docker compos and we'll see how that structure actually looks like so on the right side in the docker compos example the first two tags are always there right version three that's the latest version of the compose Docker compose and then we have the services this is where the container list goes so the first one is mongodb and that Maps actually to The Container name right this is going to be a part of container name when Docker creates a container out of this configuration blueprint the next one is actually the image right so we need to know which image that container is going to be built from and of course you can specify a version tag here um next to the name the next one one is Port so we can also specify which ports is going to open first one is on the host and the second one after the colum is on the container so the port mapping is there and of course the environmental variables can be also mapped in the docker compose and this is how actually the structure of Docker compose looks like for one specific commands let's actually see the second container command for Express that we executed it and how to map that so now again we have a Docker run command for Express and let's see how we can map it into a Ducker compose so as I said services will list the containers that we want to create and again names Express will map map to The Container name the next one will be the image again you can add a tag here if you want to be um have a specific one then you have the ports 80 to 8080 and then you have all the environmental variables again under the attribute environment and this is how the docker compose will look like so basically Docker compos is just a structured way to contain very normal common Docker commands and of course it's it's going to be easier for you to edit the the file uh if you want to change some variables s if you want to change the ports or if you want to add some new options to the Run command so to say and maybe you already noticed the network configuration is not there in the docker compost so this Network that we created we don't have to do it in a Docker compose we go to the next slide because we have the same concept here we have containers that will talk to each other using just the container name so what docker compose will do is actually take care of creating a common Network for these containers so we don't have to create the network and specify in which network these containers will run in and we're going to see that in action right away so let's actually create a Docker compost file so I'm going to paste all my contents here and this is exactly what we saw on the slide and I'm going to save it as a yl and we see the highlighting as well be very aware of the indentation they have to be correct so this is the list of all the containers on the same level and then each container has its configuration inside that so now compared to Docker run commands it will be very easy for me to go here and change these environment variables or add some new configuration options Etc so here again for demonstration we actually save the docker compose in the code so it's part of the application code so now that we have a Docker compose file the question is how do I use it or how do I start the containers using that so let's go to the command line and start Docker containers using this Docker compose file so the way to use it is using Docker compose command now if you've installed Docker on your laptop it usually gets installed with the docker compose packaged inside so you should have both Docker and Docker compose commands installed as a package so Docker compos command takes an argument which is the file so I'm going to specify which file I want to execute and in my case it's called yl and and at the end I want to say what I want to do with this file in this case the command is up which will start all the containers which are in the yl so let's actually check before that there there are no containers running so I don't have anything running here and I'm going to start those two containers okay so there are couple of interesting things here in this output so let's scroll all the way up so we've talked about Docker Network and how we created our own network at the beginning to run the containers inside and I said the docker compos takes care of it and here we see the output where it actually created a network called my app default this is the name of the network and it's going to run those two containers these are actually the names of the containers the do compos created this is what we specified and it just added prefix and suffix to it and it created those two containers uh in that Network so if I actually go here and do Docker Network LS I see the my app default is here so that's one important thing another one is that logs of both containers actually mixed because we are starting both at the the same time as you see the Express has to wait for mongod DB to start because it needs to establish a connection so we here see the locks so mongodb is starting we still get connection refus because it's not started uh completely and somewhere here when mongodb is started and listening for connections Express is able to connect to it so this is something that you can also do with Docker compose uh when you have two containers that where one depends on another one starting you can actually configure this waiting logic in the docker compos okay so now let's see actually that the docker containers are running so we have both of them here you see the container names that Docker compos gave them and one thing here to note is that the Express actually started on Port 8081 inside the container so we can see that here so we are opening a port 8080 on my laptop that actually forwards the request to container at port 8080 one just so that you don't get confused because it was 8080 on the slides so now that we have restarted the containers let's actually check the first one which is Express so it's running on 8080 in the previous example we created a database and the collection which is gone because we restarted the container this is actually another very important concept of containers to understand when you restart a container everything that you configured in that container's application is gone so data is lost so to say there is no data persistence in the containers itself of course that is very inconvenient you want to have some persistence especially when you're working with a database and there is a concept we're going to learn later called volumes uh that makes it possible to have persistency between the container restarts okay so let's actually create the database again because we need it and inside the database we had actually users collection let's create that one as well and that is empty now let's actually start our application and there you go so now if I were to modify this one here and update I should see the updated entry here so the connectivity with mongodb works so now what do I do if I want to stop those containers of course I could go there and say Docker stop and I can provide all the IDS as we did previously or with Docker compose it's actually easier I can do Docker compose again specify the file and instead of up I'm going to say down and that will go through all the containers and shut them all and in addition to remove removing the containers or stopping them removing the containers it also removes the network so the next time we restarted it's going to recreate so let's actually check that the network LS that default my app default Network case G and when I do up see it gets recreated that should give you a good idea of what dock compose is and how to use it the next we're going to build our own Docker image from our node.js JavaScript application so now let's consider the following scenario you have developed an application feature you have tested it and now you're ready to deploy it right to deploy it your application should be packaged into its own Docker container so this means that we are going to build an Docker image from our JavaScript no JS backend application and prepare it to be deployed on some environment to review this diagram that we saw at the beginning of the tutorial so we have developed a JavaScript application we have used the mongodb docker container to use it and now it's time to commit it to the git right so in this case we're going to simulate this steps on the local environment but still I'm going to show you how these steps actually work so after commit you have a continuous integration that runs so the question is what does actually Jenkins do with this application when it builds the application so the JavaScript application using the npm build Etc it packages it then in a Docker image and then pushes it into a Docker repository so we're going to actually simulate what Jenkins does with their application and how it actually packages it into a Docker image on the local environment so I'm going to do all this on my laptop but it's basically the same thing that Jenkins will do and then on later step we can actually push the built image into a Docker repository in order to build a Docker image from an application we basically have to copy the contents of that application into the docker file it could be an artifact that we built in our case we just have three files so we're going to copy them directly in the image and we're going to configure it and in order to do that we're going to use a blueprint for building images which is called a Docker file so let's actually see what is a Docker file and how it actually looks like so as I mentioned Docker file is a blueprint for creating Docker images a syntax of a Docker file is super simple so the first line of every Docker file is from image so whatever image you building you always want to base it on another image in our case we have a JavaScript application with no JS backend so we are going to need node inside of our container so that it can run our node application instead of basing it on a Linux Alpine or some other lower level image because then we would have to install node ourselves on it so we are taking a ready node image and in order to see that let's actually go to dockerhub and search node here and here you see there is a ready node image that we can base our own image from so here we have a lot of different text so we can actually use one specific one or we can just go with the latest if we don't specify any take so what that actually means basing our own image on a node image is that we're going to have node installed inside of our image so when we start a container and we actually get a terminal of the container we can see that node command is available because there's node installed there this is what from node actually gives us so the next one is we can configure environmental variables inside our Docker file now as you know we have already done this in the using the docker on commands or the docker compos so this will be just an alternative to defining environmental variables in a Docker compos for example I would say it's better to define the environmental variables externally in a Docker compos file because if something changes you can actually overwrite it you can change the docker compos file and override it instead of rebuilding the image but this is an option so this n command basically would translate to setting the environment of variables inside of the image environment the next one is run so all these Capital case words that you see from in and run they're basically part of a syntax of a Docker file so using run basically you can execute any kind of Linux commands so you see make directory is a Linux command that creat creates home slome app um directory very important to note here this directory is going to leave inside of a container so when I start a container from this image the slh home/ app directory will be created inside of the container and not on my laptop not on the host so all these commands that you have in Docker file will apply to The Container environment none of them will be affecting my host environment or my laptop environment so with run basically you can execute any Linux commands that you want so that's probably one of the most used ones and we also have a copy command now you would probably ask I can execute a copy command a Linux copy command using run yes you could but the difference here is that as I said all these commands in run for example they apply to they get executed in inside of the container the copy command that you see here it actually uh executes on the host and you see the first parameter is Dot and the second parameter is slome app so source and the Target so I can copy files that I have on my host inside of that container image because if I were to execute run CP Source destination that that command would execute inside of the docker container but I have the files that I want to copy on my host in the last one so from and CMD or command is always part of Docker file what command does is basically executes an entrypoint Linux command so this line with the command actually translates to node server JS so remember here we actually do node server JS so we start a node server with the nodejs this is exactly what it does but inside of the container so once we copy our server JS and other files inside of a container we can then execute node server.js and we are able to do it because we are basing on the Node image that already has node preinstalled and we are going to see all this inaction so another question here what is the difference between run and CMD because I could also say run node server.js the difference again is that CMD is an entry point command so you can have multiple run commands with different Linux commands but CMD is just one and that marks for Docker file that this is the command that you want to execute as an entry point so that basically runs the server and that's it so now let's actually create the docker file and just like the docker compos file Docker file is part of the application code so I'm going to create a new file here and I'm going to paste here the contents so again we're basing off Note image and actually instead of just having the latest node I'm going to specify a node version so I'm going to take 13 minus Alpine so all these that you see here are Texs so I can use any of them as a TCH so I'm going to say 13 minus Alpine like this so this is going to be a specific node image that I'm going to use as my base image let's actually stop here for a moment and take a little bit of a deep dive on this line so since we saw that Docker file is a blueprint for any Docker image that should actually mean that every docker ER image that there is on dockerhub should be built on its own Docker file right so if we actually go to Let's actually look at one of the latest versions which is 13 minus Alpine and let's click inside and as you see this specific image has its own Docker file and here as you see we have the same from that we just saw and this is what this node official image is based of which is a base image Alpine 3.10 right and then we have this environmental variable set and all these Linux commands using run and some other environmental variable and you have this entry point which is a script so you can also execute the whole shell script instead of separate commands and you have this final Command right so you don't have to understand any of this I just want to demonstrate that every image is based of another base image right so in order to actually visually comprehend how this layer stacking works with images let's consider this simplified visualization so our own image that we're building app with a version 1.0 is going to be based on a node image with a specific version that's why we're going to specify from node 13 Alpine and the node 13 Alpine image as we saw in the docker file is based on Alpine base image with a version 3.10 that's why it specifies from Alpine 3.10 so Alpine is a lightweight base image then we install node on top of it and then we install our own application on top of it and basically this is how all the images are built so now let's go back and complete our Docker file so we have the from specified we have the environmental variables specified and in just a second we're going to actually see these commands in action so let's copy that and this is also very important Docker file has to be called exactly like that you can just give it any name it is always called Docker file starting with a capital D and that's it it's a simple text file so just save it like this and here you even see the highlighting and this Docker icon so now that we have a Docker file ready let's see how to actually use it so how do we build an image out of it so in order to build an image using the docker file we have to provide two parameters one is we want to give our image a name in the tag just like all the other images have so we are going to do it using minus t so we are going to call our image my app and we're going to give it a tag of 1.0 the tag could be anything you can even call it actually version one it wouldn't matter so we're going to do 1 point0 and the second required parameter actually is a location of a Docker file because we want to tell Docker here build an image using this Docker file and in this case because we're in the same fold as the docker file we're just going to say current directory when we execute this we're going to see that image is built and this is an ID of the image that was built because I already have node 13 Alpine on my laptop this just use the the one I have lying around locally for you if it's the first time you will actually see that it's pulling node image from the dockerhub so now with the docker images I can actually see that my image is here it says created two days ago I don't know why but anyways so I have the image name which is this one here and I have the name of the image and the tag of the image so if we go back to this diagram that we saw in the review so basically we've got all these steps or we have simulated some of the steps we've built the JavaScript application using a Docker containers and one once the the application is ready let's say we made the commit and we we just simulated what Jenkins server also does so what Jenkins does is actually it takes the docker file that we create so we have to commit the docker file into the repository with the code and Jenkins will then build a Docker image based on the docker file and what is an important Point here is that usually you don't develop long you are in the team so other people might want to have access to that uptodate image of your application that you developed it could be a tester maybe who wants to pull that image and test it locally or you want that image to be deployed on a development server right and in order to do that you have to actually share the image so it is pushed into a Docker repository and from there either people can take it for example a tester maybe want to download the image from there and test it locally or a development server can actually pull it from there so let's actually just run a container I'm just going to say Docker run the image name obviously and a tag like this and in this case I'm not going to specify any other options because we just want to see what's going on inside of the container so I'm just going to run it okay so the problem is that it can't find the server JS file which is actually logical because we are not telling it to look in the correct directory so since we are copying all the resources in this home/ home/ app directory server JS is going to be there as well and this is another topic whenever you adjust a Docker file you have to rebuild an image because the old image cannot be over written so to say so what I'm going to do now is actually I'm going to delete the one that I built so I'm going to I'm going to actually take the image this is how you delete an image but I can delete it because as as it says the docker is used by a stopped container so if I do Docker PS minus a actually let's crap to my app like this I have to first delete the container so this is how you delete a container it's doer RM and once I've deleted the container I can delete an image so the image deletion is RMI like this so if I do images now I see my image isn't there okay so we' have modified the docker file so let's rebuild it now so Docker build again and let's see the image is here so let's start it again so it's my app 1.0 and let's run it and as you see the problem is fixed app listening on Port 3000 so our app is running so this one here I app 1.0 first of all we can see the logs here like this we see that the EP is listening on Port 3000 we know everything is cool to actually just get a little bit more inside let's enter the containers or let's get the terminal the command line terminal of the container and look around there so I'm going to say Docker exit interactive terminal I'm going to specify the container ID in like this and since bin bash doesn't work we can actually try shell so this is something you will also encounter because some containers do not have bash installed so we'll have to connect using bin sh so one of them has to work always so let's see in which directory we are so we are in the root directory and we see our virtual file system there and as you see the cursor changed as well so that means we're inside of a container so now let's actually check some of the stuff so first of all we specified some environmental variables here in the docker file and this means that these environmental variables have to be set inside the docker environment so if we do inv we actually see the mongodb username this one here and mongodb password are set and there are some other environmental variables automatically said we don't care about them so another thing we can check is this directory because remember because with this line we actually created this slome app directory so let's see slome Slash app and as you can see the directory was created and with the next Land We copied everything in the current folder so if we actually go and see reveal in finder so this is where the docker file resides so basically we copied everything that is inside of this directory so all of these into the Container now we don't actually need to have Docker file and Docker compose and uh this other stuff in here because the only thing we need are the JavaScript files or if we build a JavaScript application artifact just the artifact so let's go ahead and improve that so what I'm going to do is I'm going to create an app directory and I'm going to copy just the files that I'm going to need for starting an application inside of a container so I'm going to take those and the images as well so all these are just external ones we don't need them there and images the index HTML file package Jon server JS and node modules are inside of app so what we can do it now is instead of copying the whole directory where where the docker file is I just want to copy all the contents of EP folder so what I'm going to do is I'm going to say copy all the app contents and again because we modified a Docker file we need to recreate the image in order to leave the docker Container Terminal can actually exit so now we are on the host again so if I do Docker images again I have to first delete the container and then image but in order to delete the container I have to first stop it so now I can remove the container and now I can actually remove the image that the container was based on and let's check again so let's ex execute that build command again so now that we have the image built let's actually run it so I'm going to say my app 1.0 and of course I could have executed with a minus D in a detached mode it doesn't matter now and if I do a Docker PS I see my um image container running and now let's actually enter the container G so it and as we learned it was in sh and again we're going to see the home app and here we just have the contents of app directory so no unnecessary Docker file Docker compose Etc files which is actually how it's supposed to be or as I said because I just had a couple of files here I copied all of them but usually if you have this huge application you would want to compress them and package them into an artifact and then copy that artifact into a Docker image container okay but as I said this was just for demonstration purposes because I just wanted to show you um how you can actually start it as a container and how it should to look inside and in this case we improved a couple of things but usually we would start this container from a Docker compose as well together with all the other Docker images that the application uses and it's also doesn't have any ports open so uh this is just for demonstration purposes so in this video we're going to create a private repository for Docker images on AWS ECR there are many more options for Docker Registries among them Nexus and digital ocean so we're going to see how to create a registry there build and tag an image so that we can push them into that repository and in order to push the images into a private repository you first have to log into that repository so let's see how it all works so the first step is to actually create a private repository for Docker it's also called Docker registry in this case we're going to do it on AWS so let's see so I already have an account on AWS so the service that we're going to use is called elastic container registry so ECR doer container registry and because I don't have a repository there yet I am presenting with the screen so in order to create a repository click on get started and here we have a repository name and we're actually going to name it the name of the application that we have so I'm actually going to name it my app this is the domain of the registry from AWS and this is the repository name which is the same as my image name and don't worry about the other stuff right now and just create a repository it's as simple as that now one thing I think specific to Amazon container service is that here you create a Docker repository per image so you don't have a repository where you have where you can actually push multiple images of different applications but rather for each image you have its own repository and you go inside of the repository here it's empty now but what you store in a repository are the different tags or different versions of the same image so this is how the Amazon container service actually works there are other dep Docker Registries that work differently for example you create a repository and you can just throw all of your container images inside of that one repository so I think this is more or less specific for AWS so anyways we have repository which is called my app and let's actually see how we can push the image that we have locally so actually check that once more so we want to push this image here into that repository so how do we do that if you click on this one the view push commands will be highlighted this is different for each registry but basically what you need to do in order to push an image into repository are two things one you have to log in into the private repository because you have to authenticate yourself so if you are pushing from your local laptop or local environment you have to tell that private reposit hey I have access to it this is my credentials if a Docker image is built and pushed from a Jenkins server then you have to give Jenkins credentials to login into the repository so Docker login is always the first step that you need to do so here AWS actually provides a Docker login command for AWS so it doesn't say Docker login but in the background it uses one so I'm going to execute this login command for AWS Docker repository uh so in the background it uses actually Docker login to authenticate so in order to be able to execute this you need to have AWS command line interface and the credentials configured for it so if you don't I'm going to put a link to the guide of how to do that in the description I have configured both of them so I can execute this command and I should be logged in successfully to the docker repository so now I have authenticated myself to the docker repository here so I'm able to push the image that I have locally to that repository but before I do that there is one step I need to do so I've already built my image so that's fine and now I have to tag my image and if this command here looks a little bit too complicated for you or too strange let's actually go and look at image naming Concepts in Docker repositories so this is the naming in Docker Registries this is how it works the first part of the image name the image full name is the registry domain so that is the host Port Etc slash repository or image name and the tag now you may be wondering every time we were pulling an image out of dockerhub we actually never had this complex long name of the image right so when we were pulling an image it looked like this Docker pole 4.2 the thing is with dockerhub we're actually able to pull an image with a short hand without having to specify a registry domain but this command here is actually a shorthand for this command what actually gets executed in the background when we say Docker pole is Docker pole the registry domain so docker.io library is a registry domain then you have the image name and then you have the tag so because we we were working with dockerhub we were able to use a shortcut so to say in a private Registries we can just skip that part because there is no default configuration for it so in our case in AWS ECR what we going to do is we're going to execute Docker pull the full registry domain of the repository this is what we're going to see here and a tag and this is how AWS just generates the docker registry name that's why we see this long image name with the tag here and we have to tag our image like this so let's go back and take a look at our images our image that we built again and under the repository it says my app now the problem is we can just push an image with this name because when we say Docker push my app like this Docker wouldn't know to which repository we're trying to push by default it will actually assume we're trying to push to dockerhub but it's not going to work obviously because we want to push it to AWS so in order to tell Docker you know what I want this image to be pushed to AWS repository with the name my app we have to tag the image so we have to include that information in the name of the image and that is why we have to tag the image tag basically means that we are renaming our image to include the repository domain or address and the name okay and AWS already gives us the command that we can execute we want to use the specific version so I'm going to use 1.0 in both so what this is going to do is it's going to rename this is what tech does my app 1.0 this is what we have locally this is what the name is to this one here so let's execute that and let's see what the outcome is and as you see it took the image that we had it made a copy and renamed it into this one so these two are identical images they're just called in a different way and now when we go back we see the docker push command so basically this thing here is a same as Docker push and name of the image and the take so this push command will tell Docker you know what I want you to take the image with tag 1.0 and push it into a repository at this address so when I execute this command see the push command will actually push those layers of the docker image one by one this is the same thing as when we're pulling it we also pulled the images layer by layer and this is what happens in the reverse Direction when we push it so this is also going to take a little bit great so the push command was complete and we should be able to see that image in the AWS repository now so if I go inside see I have image tag with 1.0 this is our tag here and push the time the digest which is the unique hash of that image and the image URI which is again the name of the image using the the repository address image name or repository name in this case and the tag so now let's say I made some changes in the docker file you know let's say I renamed this home slome to node app like this or what could also lead to need to recreate an image is obviously when I change something in the code right so you know let's say I were to delete this line because I don't want to console log to be in my code and now I have a different version of the application where I have changes in the application so now I want to have those changes in the new Docker image so now let's build a new Docker image out of it so Docker build let's call it my app with a version 1.1 and a path to a Docker file and now I have a second image which is called my app with version 1.1 so now again because I want to push this to repository I have to rename it to include the repository address inside of it so I'm going to do Docker tag the first parameter is the image that I want to rename and the second one is the name of that image a new name so it's going to be the same as the previous one because the repository name and the address is the same remember we have one repository for one image but for different versions so we're building a version 1.1 so it should end up in the same repository so now here we have 1.1 and if I tag that and images I have a second image here so I'm going to copy that and I'm going to do Docker build and do not forget that tag it's important because because it's part of the complete name sorry it's Docker push and now some of the layers that I already pushed are there only the ones that changed are being rep pushed sort of say and also know that I just have to do Docker login once at the beginning and then I can pull and push images uh from this repository as many times as I want so do login is done once so now that is complete let's actually reload this so my repository now has two versions so this is pretty practical if you are for example testing with different versions and you want to have a history of those image Texs if you want to for example test a previous version and I think in AWS the repos each repository has a capacity of hold holding up to 1,000 uh image versions so for example my app here can have thousand different tags or of the same image okay so now going to compare it to the initial diagram that we saw for this complete flow let's actually switch back to it quickly so here what we did is basically simulate how Jenkins would push an image to a Docker repository so whatever we did on our lap top will be the same commands executed on a Docker on the Jenkins server and again Jenkins user or Jenkins server user has to have credentials to the docker repository to execute Docker login depending on the registry or repository configuration will look different and Jenkins needs to tag the image and then push it to the repository and this is how it it's done and the next step of course we need to use that image that is lying now in the repository and we're going to see how it's pulled from that repository and again we're going to do it on the local environment but it's the same thing that a development server or any other environment will actually execute so in this video we're going to see how to deploy an application that we built into a Docker image so after you package your application in a Docker image and save it in the private repository you need to somehow deploy it on a development server or integration server or whatever other environment and we're going to use Docker compose to deploy that application so let's imagine we have logged in to a development server and we want to run our image that we just push the repository so our my app image and the mongodb image uh both the database and the Express on the development server so the my app image will be pulled from private repository of AWS the in the two containers will be pulled from the docker Hub so let's see actually how that would work so usually again you have developed your application you done with it and you have created uh your own Docker image right now in order to start an application on development server you would need all the containers that make up that application environment okay so we have mongodb and Express already so what we are going to do is here we're going to add a new container in the list which is going to be our own image so let's go ahead and copy the image from our repository so let's actually use the 1.0 so again remember we said that this image name is a shortcut for having a docker.io do library SL with like a specific version so instead of that because we are pulling these images from a Docker Hub we can actually skip that repository domain in front of the images but here because we're pulling it from a private repository so if we were to specify our image like this Docker will think that our image resides on dockerhub so we try to pull it from dockerhub and of course it won't find it because we have to tell Docker go and look at this repository with this repository name and this TCH and of course in order to be able to pull this image or the docker composed to be able to pull this image the environment where you execute this Docker compost file has to be logged into a Docker repository so here as the development server has to pull the image from the repository what we would need to do on the development server is actually do a dock login before we execute the docker compose and obviously you don't need a Docker login for Docker Hub those images will be pulled freely okay so the next thing that we have to configure are the ports because obviously want to open the ports if we go back we see that our application runs on Port 3000 so the port of the container or the where the container is listening on is 3,000 and here we can open the port on the host machine so it's going to be 3,000 me to 3,000 we have actually the environment variables inside of the docker file but obviously we could have configured them in the docker compose just like this so it's an alternative so this will be a complete Docker compost file that will be used on the development server to deploy all the all the applications inside so again if we're trying to simulate a development server the first step will be to do the docker login in this case you have this on command for logging into the AWS Repository which I have done already in this terminal so the next step is to have the docker compos file available on this development server because we have to execute the docker compost file because we're simulating here the way I would do it is I'm going to create a yl file in the current directory where I am I'm going to copy this and save so now I have my ml file and now we can start all three containers using Docker compose command minus f up and here we see that app started on 3000 and mongodb and express started as well so let's check again now and here we saw that database is lost every time we recreate a container and of course that's not good and we're going to learn how to preserve the database data between the container restarts using Docker volumes in the later tutorials because this is not an ideal State okay so now that we have database in a collection let's actually refresh and our application works as well let's check awesome so application works let's refresh this one as well and there is actually one thing that I needed to change in the code to connect nodejs with mongodb so let's actually go and look at that these are my handlers you know nodejs where I connect to the mongodb database so the uis are the same and what I changed here is that it was a local host before so instead of Local Host I changed it to mongodb because this actually is a name of the container or of the service that we specify here so this actually leads back to the docker Network and how Docker compos takes care of it is that in the URI or when I connect one application in a Docker container with another one in another Docker container I don't have to use this uh Local Host anymore actually I wouldn't even need to use the port even because I have all that information so the host name and the port number in that configuration so my application will be able to connect to mongodb using the service name and because of that you don't have to specify here a local host and a port number which is actually even more Advantage when you consider using Docker containers to run all of your applic ations because it makes the connectivity between them even more easier and that actually concludes the this uh diagram that we saw previously we have gone through all of the steps where we saw uh how to develop uh a JavaScript application locally with Docker containers then we saw how to build them into an image uh just like a continuous integration build will do it then we push it into a private repository and we simulated a development server where we pulled the image from U private repository and the other images from the dockerhub where we started the whole application setup with our uh own application and the two applications uh using a Docker compose which is how you would deploy an application on a Dev server so that now testers or other developers will be able to um access the development server and actually try out the applic that you just deployed or you can also use it for demos so in this video we're going to learn about Docker volumes in a nutal Docker volumes are used for data persistence in Docker so for example if you have databases or other stateful applications you would want to use Docker volumes for that so what are the specific use cases when you need Docker volumes so a container runs on a host let's say we have a database container and a container has a virtual file system where the data is usually stored but here there is no persistence so if I were to remove the container or stop it and restart the container then the data in this virtual file system is gone and it starts from a fresh state which is obviously not very practical because I want to save the changes that my application is making in the database and that's where I need Docker volumes so what are the docker volumes exactly so on a host we have a physical file system right and the way volumes work is that we plug the physical file system path it could be a folder a directory and we plug it into the containers file system path so in simple terms a directory a folder on a host file system is mounted in into a directory or folder in the virtual file system of Docker so what happens is that when a container writes to its file system it gets replicated or automatically written on the host file system directory and vice versa so if I were to change something on the host file system it automatically appears in the container as well so that's why when a container restarts even if it starts from a fresh state in its own virtual file system it gets the data automatically from the from the host because the data is still there and that's how data is populated on a startup of a container every time you restart now there are different types of Docker volumes and so different ways of creating them usually the way to create Docker volumes is using Docker run command so in the docker run there is an option called minus V and this is where we Define the connection or the reference between the host directory and the container directory and this type of volume definition is called host volume and the main characteristic of this one is that you decide where on the host file system that reference is made so which folder on the host file system you mount into the Container so the second type is where you create a volume just by referencing the container directory so you don't specify which uh directory on the host should be mounted but that's taking care of the docker itself so that directory is first of all automatically created by Docker under the VAR leap Docker volumes so for each container there will be a folder generated that gets mounted automatically to the container and this type of volumes are called Anonymous volumes because you don't have a reference to this automatically generated folder basically you just have to know the path and the third volume type is actually an improvement of the anonymous volumes and it specifies the name of the folder on the host file system and the name is up to you it's just to reference the directory and that type of volumes are called named volumes so in this case compared to Anonymous volumes you H you can actually reference that volume just by name so you don't have to know exactly the path so from these three types the mostly used one and the one that you should be using using in production is actually the named volumes because there are additional benefits to letting Docker actually manage those uh volume directories on the host now they showed how to create Docker volumes using Docker run commands but if you're using Docker compose it's actually the same so this actually shows how to use volume definitions in a Docker compose and this is pretty much the same as in Docker run commands so we have volumes attribute and underneath you define your volume definition just like you would in this minus V option and here we use a named volume so db data will be the name reference name that you can just think of could be anything and in vly MySQL data is the path in the container then you may have some other containers and at the end so on the same level as the services you would actually list all the volumes that you have defined you def find a list of volumes that you want to mount into the containers so if you were to create volumes for different containers you would list them all here and on the container level then you actually Define under which path that specific volume can be mounted and the benefit of that is that you can actually mount a reference of the same uh folder on a host to more than one containers and that would be beneficial if those containers need to share the data in this case you would Mount the same volume name or reference to two different containers and you can mount them into different path inside of the container even in this video we are going to look at Docker volumes in practice and this is a simple nodejs mongodb application uh that we're going to attach the volume to so that we don't lose the database data every time we restart start the mongodb container so let's head over to the console and I'm going to start the mongodb with the docker compose so this is how the docker compose looks like we're going to start the mongod TB uh container and the Express container so that we have a UI to it so I'm going to execute the docker compost which is going to start mongodb and the Express so when it started I'm going to check that Express is running on port 8080 and here we see just the default databases so these are just created by default on Startup um and we're going to create our own one for the node.js application and inside of that database I'm going to create users collection so these are the prerequisites or these are the things that my node.js applic ation needs so this one here in order to connect to the database might DB this is what we just created ITB and inside of that to the collection called users so let's start the application which is running on Port 3000 so here and this is our app which when I edit something here we'll write the changes to my database now if I were to restart now the mongodb container I would lose all this data so because of that we're going to use named volumes inside of the docker compos file to persist all this data in the mongodb let's head over to dock compose so the first step is to Define what volumes I'm going to be using in any of my containers and I'm going to do that on the services level so here I Define the list of all the volumes that I'm going to need in any of my containers and since we need data persistency for mongod TB we're going to create uh data volume here now this is going to be the name of the volume reference uh but we also need to provide here a driver local so the actual store storage path that we're going to see later once it's created it's is actually created by Docker itself and this is kind of an information additional information for Docker to create that physical storage on a local file system so once we have a name reference to the volume defined we can actually use it in the container so here I'm going to say volumes and here I will Define a mapping between the data volume that we have on our host and the second one will be the path inside of the mongodb container it has to be the path where mongodb explicitly persists its data so for example if you check it out online you see that the default path where mongodb stores its data is data/ dat DB and we can actually check that out so if I say docker s and go inside the container it's minus it I can actually see data DB and here is all the data that mongodb actually holds but this is of course only the container so when the container restarts the data get regenerated so nothing persists here so this is the path inside of the container not on my host that we need to reference in the volumes here so we're attaching our volume on the host to data/ dat DB inside of a container so for example for MySQL it's going to be um VAR leap MySQL for postgress it's also going to be VAR leap postris SQL SL data so each database will have its own so you have to actually find the right one so what this means is that all the data with that we just saw here all of this will be replicated on a container startup on our host on this persistent volume that we defined here and vice versa meaning when a container restarts all the data that is here will be replicated inside of that directory inside of a container so now that we have defined that let's actually restart the docum compose and restart it so once we create the data and I'm going to the collection and let's actually change this one here and update it so we have a data here so now that we have the persistent volume defined if I were to restart all these containers these data should be persisted so in the next restart I should see the database my DB collection and the entry here so let's do that I'm going to do down great so let's check see the database is here the collection is here and the entry has persisted so now let's actually see where the docker volumes are located on our local machine and that actually differs between the operating systems for example on a Windows laptop or a computer uh the path of the docker volume will be at program data Docker SL volumes the program data Docker folder actually contains all the other container information so you would see other folders in this Docker directory besides the volumes on Linux the path is actually /ar leap Docker volumes which is comparable to the windows path so this is where the docker saves all this configuration and the data and on the mech it's also the same one inside of this volumes directory you actually have a list of all the volumes that one or many containers are using and each volume has its own hash which is or which has to be unique and then slore data will actually contain all the files and all the data that is uh persisted let's head over to the command line and actually see um the volumes that we persisted for mongodb now interesting note here is that if I were to go to this path that I just showed you in the presentation which is VAR Le Docker see there is no such directory so that could be a little little bit confusing but the way it works on Mac specifically on Linux you would actually have that path directly on your host but on Mech it's a little bit different so basically what happens is that Docker for Mech application seems to uh actually create a Linux VM uh in the background and store all the docker inform or doer data about the containers and the volumes Etc inside of that vm's storage so if we execute this command here so this is actually the physical storage on my laptop that I have where all the data is stored but if I execute this command I actually get the terminal of that VM and inside here if I look I have a virtual different virtual file system and I can find that path that I showed you here so it's VAR leap Docker see so I have all this Docker information here I have the containers folder and I have volumes folder so this is the one we need sort of that actually go to the volumes and this is a list of volumes that um I have created and this is the one that came from our Docker compose right this is the name of our app this is do this is what Docker compose actually takes as the name we can actually take a look here so when it's creating these containers it depends these name as a prefix and then there is mongodb and our volume has the same pattern it has the prefix and then mongod data this is the name that we defined here so now if we look inside of that mongod data volume directory we see that underscore data this would be the anonymous volumes so basically here you don't have a name reference it's just some random uh unique ID but it's the same kind of directory as this one here the difference being that this one has a name so it's more it's easier to reference it with a name so this is an onymous volume this is a named volume but the content will be used in the same way um so here as you see in this underscore data we have all the data that mongodb uses so this will be where it gets the default databases and the changes that we make through our application inside and if I go inside of the container so remember this volume is attached to mongodb and is replicated inside of the container under path SL dat DB so if we go go inside of the container actually it here PS SL DP we'll see actually the same kind of data here so we have all this index and collection um files just like we did in this one so now whenever we make changes to our application for example we change it to SM whatever and this will make the container update its data and that will Cascade into this volumes directory that we have here so that on the next startup of a container when the SL data/ DB is totally empty it will actually populate this directory with the data from this uh persistent volume so that we will see all the data that we uh created through our application again on Startup and that's how do loer volumes work in order to end that screen session that we just started because exit doesn't work in this case uh somehow on Mac you can actually click on control a k and then just type Y and the session will be closed so when you do screen LS you should see actually it's terminating congratulations you made it till the end I hope you learned a lot and got some valuable Knowledge from this course now that you've learned all about containers and Docker technology you can start building complex applications with tens or even hundreds of containers of course these containers would need to be deployed across multiple servers in a distributed way you can imagine what overhead and headache it would be to manually manage those hundreds of containers so as a Next Step you can learn about container orchestration tools and kubernetes in particular which is the most popular tool to automate this task if you want to learn about kubernetes be sure to check out my tutorials on that topic And subscribe to my channel for more content on modern devops tools also if you want to stay connected you can follow me on social media or join the private Facebook group I would love to see you there so thank you for watching and see you in the next video
