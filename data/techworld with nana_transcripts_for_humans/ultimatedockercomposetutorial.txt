With timestamps:

00:00 - in this video you will learn everything
00:02 - you need to know to get started with
00:04 - using Docker compose we'll go over what
00:07 - it is exactly what problems Docker
00:10 - compose was designed to solve its common
00:12 - use cases and of course we will do some
00:14 - Hands-On demos to learn actually using
00:17 - Docker compose in practice I am super
00:20 - excited to teach you all this so let's
00:22 - jump into
00:24 - it now in order to understand Docker
00:27 - compos you need to First understand
00:29 - docker and have some basic experience
00:31 - with it if you don't I recommend you
00:34 - pause and watch my Docker crash course
00:36 - first and then continue with this one
00:39 - because Docker compost is essentially a
00:41 - tool that is supposed to manage and work
00:44 - with Docker containers so you need to
00:46 - understand that part first so that you
00:48 - understand the context for learning
00:50 - Docker compost so in the docker video I
00:53 - break down what the containers are what
00:55 - images are what problems Docker solves
00:57 - and what use case it it has dockerizing
00:59 - your application with Docker file and
01:00 - all the concepts you need to understand
01:02 - Docker itself so based on that knowledge
01:05 - we can Now understand why Docker compos
01:06 - was created along with Docker and when
01:09 - we want to use
01:12 - it now applications are composed of many
01:16 - different parts you can have apis
01:18 - databases any Services your application
01:21 - depends on and even within the
01:23 - application you may have a microservice
01:25 - application which is basically an
01:28 - application broken down into multip
01:30 - micro applications or microservices and
01:33 - when you're creating containerized
01:35 - applications all of these different
01:37 - application components must be deployed
01:40 - and run together because they have
01:42 - dependencies on each other so basically
01:44 - you have a set of containers which are
01:47 - running different services and
01:49 - applications within them that need to
01:51 - run together that need to talk to each
01:53 - other and so on so Docker compose is
01:56 - basically a tool that allows you to
01:59 - Define and run multiple services and
02:02 - applications that belong together in one
02:05 - environment so simply put if you want to
02:07 - deploy multiple Docker containers where
02:10 - each container may have its different
02:11 - configuration options you can use Docker
02:14 - compose to do this to manage these
02:16 - containers way more easily now this is
02:19 - just a general definition to give you
02:22 - some idea of what Docker compose is but
02:25 - of course we want to understand this
02:27 - with specific examples and specific
02:30 - demonstration so that you really
02:32 - understand these Concepts and the actual
02:34 - use cases of using doer compose and not
02:38 - just a general abstract explanation of
02:41 - what it is and because of that we're
02:43 - going to jump right into that demo where
02:45 - I'm going to explain the concepts the
02:47 - use cases using those demonstrations so
02:50 - let's get
02:51 - started as a first step we're going to
02:54 - start two Services as Docker containers
02:57 - using just the docker command so we're
02:59 - not going to use Docker compos as a
03:01 - first step so we can see and compare the
03:03 - before after States first we're going to
03:05 - create a Docker Network where these two
03:07 - containers will run and talk to each
03:09 - other using just the container name and
03:11 - then we're going to start two containers
03:13 - one is going to be a mongodb container
03:16 - and another one is going to be
03:17 - Express container which is basically a
03:20 - UI for the mongodb database very simple
03:23 - use case and we're going to run both
03:25 - containers using Docker run commands so
03:27 - that's our first very simple
03:29 - demonstration let's go ahead and do that
03:31 - so I'm going to switch to a terminal
03:34 - because we're going to execute those
03:35 - docket run commands on the terminal and
03:38 - you probably see this is a fancy fun
03:40 - looking terminal that I have been using
03:43 - since recently and this is an
03:45 - application or a terminal app called
03:47 - warp which is actually a sponsor of this
03:50 - video I actually played around with warp
03:53 - and love using it it's free it's easy to
03:55 - install on your computer so I will be
03:57 - using warp throughout the entire day
03:59 - demo because it also helps highlight
04:01 - some of the commands and stuff better so
04:03 - it's going to be easier for you guys to
04:04 - follow what I'm showing you however if
04:06 - you want to install warp yourself on
04:08 - your computer you can go ahead and check
04:10 - out the link to get started in the video
04:12 - description where I'm going to provide
04:14 - all the relevant links for this crash
04:16 - course including the warp installation
04:19 - so to run our Docker containers of
04:21 - course we need to have Docker installed
04:23 - and running so I'm going to start up
04:25 - Docker and then we can start the
04:27 - containers so the docker service is up
04:30 - and running let's go ahead and create
04:33 - the docker Network first so I'm going to
04:35 - do Docker Network and since we're going
04:38 - to run mongodb and Express
04:41 - containers we can call this network
04:43 -  Network and let's create and now
04:47 - if I do Docker Network LS so basically
04:51 - list all the networks available these
04:53 - are the default ones basically that you
04:55 - get out of the box when you install
04:56 - Docker and this is the Network
04:58 - that we just created awesome so the
05:00 - network is there now let's run our two
05:03 - containers and if you know Docker if you
05:06 - followed my Docker crash course
05:08 - basically you know all this stuff Docker
05:10 - run and we're going to execute this in
05:12 - the
05:14 - background and we have the and
05:19 - Express image documentation so we can
05:21 - actually reference
05:23 - this so first I'm going to define the
05:25 - port uh mongodb's default Port is $27
05:30 - 07 so we're going to map that to the
05:34 - same port so we we're going to bind that
05:36 - to the same port on the host then we're
05:39 - going to define those two environment
05:41 - variables to basically set the admin or
05:44 - the root user and password so we're
05:46 - going to copy those and we're going to
05:49 - call this
05:56 - admin and this is some password we're
06:00 - going to set this to super secret so all
06:03 - these should be actually be a refresher
06:05 - from Docker we also want to specify that
06:08 - it should run in this network
06:11 - network so we're going to
06:13 - do
06:16 - Network run in this
06:20 - one we're also going to name our
06:22 - container instead of having Docker just
06:25 - create a random container name so we're
06:28 - going to call this
06:30 - DB and finally we need to specify the
06:34 - image right and this is the name of the
06:39 - image and that's basically our Docker
06:41 - command so I'm going to
06:44 - execute and this will fetch or pull the
06:47 - latest image from dockerhub
06:50 - repository and run it in a detached
06:57 - mode perfect so we should have have our
07:00 - mongodb container running and now let's
07:03 - start Express container and I can
07:05 - actually bring up my previous command
07:08 - and we're going to adjust it for the
07:11 -  Express right here we see that
07:14 -  Express is running on port 8080 so
07:17 - that's what we're going to set here
07:19 - there you go we also have different
07:22 - environment variables so basically
07:25 - Express is just a UI for mongodb and
07:30 - in order for us to use it it needs to
07:33 - connect and authenticate with mongodb so
07:36 - we need to provide it the credentials as
07:38 - well that we set for mongodb database
07:41 - and we're passing those also as
07:43 - environment variables but in this case
07:46 - the environment variables are named
07:47 - differently so that's what we're using
07:49 - referring to the official documentation
07:50 - which you always should do to get the
07:52 - most up toate data and you also see the
07:55 - default values for those environment
07:57 - variables the port is correct because
07:59 - that's what we binded it to on our host
08:02 - and mongod to be server which is going
08:04 - to be the mongod to be container name in
08:06 - our case it's different because we
08:07 - called our container mongod beam so
08:09 - we're going to set this environment
08:11 - variable as well so right here I'm going
08:14 - to add this and we're going to set these
08:16 - to mongodb let's not forget the
08:19 - backwards slash here so the ports are
08:22 - correct the environment variables are
08:24 - correct we are going to run it also in
08:26 - the Network we're going to name
08:28 - this Express so that's going to be
08:31 - the name of the container and let's see
08:33 - what the actual name of the image is
08:36 - just going to copy that so that I don't
08:38 - make spelling mistake and that's it
08:40 - let's execute this as
08:45 - well and seems like it started without
08:48 - any problems let's see perfect it's
08:51 - running and now to test that it was
08:54 - actually able to connect without any
08:57 - issues to the mongodb database container
09:00 - we're going to access it in our browser
09:04 - so we opened it on Port 881 on our host
09:09 - and it is asking for basic
09:10 - authentication in the browser and we can
09:13 - actually get those in the locks let's do
09:16 - that do
09:19 - logs of Express and here we have
09:23 - the credentials so admin pass should
09:28 - work
09:31 - and there you go so that's a test and a
09:34 - proof that it was able to connect to our
09:37 - database since we didn't have any
09:39 - connection errors here and we're able to
09:41 - access the application here so this was
09:44 - basically just to demonstrate how you
09:45 - would start containers that belong to
09:48 - each other so Express container
09:50 - actually depends on mongodb because we
09:53 - don't need it without the database in
09:54 - the background so kind of start
09:56 - containers that belong together that
09:58 - should run together
09:59 - using just plain Docker and also
10:02 - starting them in the same network so
10:04 - they can talk to each other in that
10:06 - isolated virtual Network now obviously
10:10 - these are just two containers but if we
10:12 - have microservice application with 10
10:14 - different services that has a messaging
10:15 - service maybe two databases that it
10:18 - belongs to maybe those databases have
10:19 - their own UI services that we want to
10:22 - run in addition so now these are lots of
10:26 - containers that we need to start and
10:28 - manage using just plain Docker commands
10:31 - and now imagine if you need to stop
10:32 - those containers because you don't want
10:34 - to have them running all the time or you
10:36 - want to make changes and restart them
10:38 - again this is going to be a lot of
10:40 - manual tedious work and you don't want
10:42 - to execute these commands all the time
10:45 - on the command line terminal especially
10:47 - when you have tens of containers so you
10:49 - want an easier way to manage to stop
10:52 - start configure containers that you want
10:55 - to start together and that's exactly
10:57 - where Docker compose comes into the
10:59 - picture so Docker compos basically makes
11:01 - running multiple Docker containers with
11:03 - all this configuration that we just
11:05 - defined on those containers so you have
11:07 - the environment variables you have ports
11:09 - maybe you have multiple ports on the
11:11 - same container same application that you
11:12 - want to open maybe you want to configure
11:15 - additional volumes for example for data
11:17 - persistence so that's the main use case
11:19 - of Docker compose so with Docker compose
11:22 - basically you have a file a yaml file
11:25 - where you define all this configuration
11:28 - a list of contain ERS or services that
11:30 - you want to start together and all their
11:32 - configuration in one central place in a
11:34 - file that you can modify configure and
11:37 - use to start and stop those containers
11:40 - so let's see how the file looks like and
11:42 - how these Docker run commands actually
11:45 - map to the docker compost so how can we
11:47 - migrate or map all of these and write a
11:50 - Docker compost file that starts those
11:53 - two containers with exactly the same
11:55 - configuration that we defined
11:57 - here
11:59 - so this is a Docker run command of the
12:01 - mongod beam that we executed previously
12:03 - so basically with Docker compos file
12:06 - what we can do is we take the whole
12:08 - command with this configuration and map
12:11 - it into a file so we have that command
12:13 - defined in a structured way so if you
12:15 - have let's say 10 20 Docker containers
12:18 - that you want to run for your
12:19 - application and they all need to talk to
12:22 - each other and interact with each other
12:24 - you can basically write all the Run
12:26 - commands for each container in a
12:28 - structured way in Docker compose and
12:30 - Define the entire configuration there
12:32 - and this is how the structure in Docker
12:35 - compose will actually look like so the
12:37 - first two lines are required attributes
12:39 - of Docker compose file with the first
12:41 - line we basically Define the version of
12:43 - Docker compose which is the latest
12:45 - version that should be compatible with
12:47 - the docker compose that you have
12:49 - installed locally so the latest Docker
12:51 - compose tool installed on your computer
12:54 - will be able to read the latest Docker
12:56 - compose file version and then we have
12:58 - the services and Docker compose is super
13:01 - simple Services is basically an
13:03 - attribute where you can list all the
13:05 - services or all the containers that you
13:07 - want to run as part of this doer compos
13:09 - file so in this case the first service
13:11 - we want to Define is mongodb and that
13:13 - Maps actually to The Container name or
13:15 - rather this is going to be part of the
13:17 - container name when the services are
13:20 - created as Docker containers and for
13:22 - each service like Mong TB we have all
13:25 - the configuration for that specific
13:27 - container so the first one is obviously
13:29 - image because we are building the
13:31 - container from the image so we need to
13:33 - know which image that container is going
13:34 - to be built from and of course you can
13:36 - specify version Tech here next to the
13:38 - name the next one is the list of ports
13:42 - because you can open multiple ports on a
13:44 - container if there are multiple
13:45 - processes running inside the container
13:47 - but mostly you would just have one so
13:49 - this is where we Define the port
13:51 - mappings so mapping a container port to
13:54 - the host so just like in Docker command
13:56 - the first Port refers to the host the
13:58 - second one refers to the port inside
14:00 - container then we have the environment
14:03 - variables listed under an environment
14:06 - attribute like this and this is actually
14:09 - how the structure of Docker compose
14:11 - looks like for one specific command now
14:14 - let's actually add the second container
14:16 - command for Express and how that
14:18 - Maps into our Docker compost file so
14:21 - again we have the service which we can
14:23 - call Express and by the way the
14:25 - service names are completely up to you
14:26 - you can call them whatever you want just
14:28 - like the container names you can call
14:30 - the containers whatever you want and
14:32 - under that Express we have the
14:35 - same exact configuration options we have
14:37 - the image which refers to Express
14:39 - image again you can have a TCH here if
14:41 - you want to have a specific one then we
14:43 - have the port and all the environment
14:45 - variables that we defined with Docker
14:48 - run command under the environment
14:49 - attribute and this is how Docker compos
14:52 - will look like with multiple Services
14:54 - defined inside so basically Docker
14:57 - compos is just a structured way to
14:59 - contain very normal common Docker
15:02 - commands and of course it's going to be
15:03 - easier for you to edit this file if you
15:06 - want to change some variables or if you
15:08 - want to change the ports or if you want
15:10 - to add more services with those services
15:13 - and as part of everything as code Trend
15:16 - dock compose is basically a code that
15:19 - defines how your services should run
15:21 - within a file that you can check in to a
15:24 - code repository and multiple people can
15:27 - work on it together
15:29 - compared to a command that you just
15:31 - execute manually on your computer with
15:33 - individual Docker run commands the final
15:35 - thing here which you may already noticed
15:38 - is the network configuration is not
15:40 - defined in the docker compost so we
15:42 - didn't map that part from the docker run
15:45 - commands so this Monga Network that we
15:47 - created we don't have to explicitly
15:49 - create or Define it in Docker compost
15:51 - because Docker compose will actually
15:54 - take care of creating a shared network
15:56 - for all the containers from from the
15:58 - services list that it's going to run
16:01 - when we execute this file so we don't
16:03 - have to create the network specifically
16:04 - and then specify that all the containers
16:07 - run in that Network Docker compose will
16:09 - automatically take care of it and we're
16:11 - actually going to see that in action
16:13 - right away so now let's actually go and
16:15 - create a Docker compos file in a code
16:18 - editor so in this projects directory so
16:21 - basically where I'm in the terminal I
16:23 - created this mongos services. yl file
16:26 - which is my Docker compos file with
16:29 - those two Services defined here so
16:31 - exactly the same code that you just saw
16:33 - we have our credentials all our
16:35 - environment variables defined and since
16:37 - this is a yl format please make sure
16:40 - that your indentations are correct
16:42 - because yl is a very simple language but
16:44 - it's very strict on indentation so the
16:47 - services need to be on the same level
16:49 - and then inside that service you need to
16:51 - have correct indentation for the
16:53 - configuration attributes so now compared
16:56 - to the docker commands it's going to be
16:57 - easier for me to go here to this file
17:00 - first of all see what services I'm
17:02 - running with what configuration edit
17:04 - those make any changes add any new
17:06 - services that I want to run and now
17:08 - let's actually execute this Docker
17:10 - compos file and see how it works back to
17:13 - my warp terminal I'm actually going to
17:16 - stop all the
17:18 - containers because we want
17:21 - to start them using Docker
17:25 - compost so that's the first one
17:29 - let's stop
17:32 - them we can actually remove
17:34 - them and we can also
17:46 - remove the dock
17:52 - Network and there you go so we have a
17:55 - clean State no containers running and
17:58 - now how do we execute a Docker compost
18:00 - file with Docker compost good news is if
18:03 - you have Docker installed on your
18:05 - computer that means you automatically
18:08 - also have Docker compose installed so
18:10 - you don't have to do that separately
18:12 - that means we should have Docker compose
18:14 - command already available as part of
18:17 - Docker and Docker compos takes one
18:20 - attribute which is the file name
18:25 - Services there you go and
18:29 - the command which is up which basically
18:31 - means go through the docker compost file
18:34 - provided here and Run start all the
18:37 - services configured right here so let's
18:40 - execute this and we're going to see the
18:43 - result awesome so now there are a couple
18:45 - of interesting things that I want to
18:47 - point out and highlight in the output
18:50 - that we got and also explain some of the
18:52 - interesting Concepts behind so let's go
18:54 - through them one by one I'm going to
18:56 - scroll all the way up to the beginning
18:58 - of the output which is right here when
19:00 - we executed Docker compose command the
19:02 - first one is I mentioned that Docker
19:05 - compose takes care of creating a
19:07 - dedicated Network for all the containers
19:10 - and here we see in the output that it
19:12 - actually created Network called projects
19:15 - uncore default so this is the name of
19:18 - the network and it's going to run those
19:20 - two containers in that Network so if I
19:23 - open another terminal and if I do Docker
19:25 - Network LS we're going to see projects
19:29 - default network was created another
19:31 - interesting thing to point out is the
19:34 - container names for those two containers
19:36 - so in the docker compose we actually
19:38 - called those services mongodb and
19:40 - Express however as you see Docker
19:43 - compose actually added a prefix projects
19:46 - and a suffix at the end to each service
19:49 - so this is basically the folder that
19:53 - contains the docker compos file where
19:55 - the docker compos file is located as you
19:57 - see right here so Docker compose always
20:00 - takes the name of the folder where the
20:03 - docker compose file is executed and it
20:06 - uses it as a prefix of the container and
20:09 - then you have one as a suffix so we have
20:12 - one instance of each container and
20:15 - that's how the containers are called and
20:16 - we can also check our
20:19 - containers and here you see the names
20:24 - projects mongodb
20:26 - 1 another interesting thing to point out
20:29 - is that you see that the logs of those
20:31 - two containers are actually mixed so we
20:34 - have the mongod be logs Express
20:36 - then mongod be
20:38 - again and so on because we're starting
20:41 - both containers at the same
20:43 - time so if you had 20 Services defined
20:46 - here they will all start at the same
20:47 - time and you will see the logs basically
20:49 - just mixed together on Startup however
20:52 - when you have multiple Services where
20:55 - some Services actually depend on the
20:57 - others in our case Express depends
21:00 - on mongodb because it cannot establish a
21:02 - connection the initial connection with
21:05 - the service until mongodb is fully up
21:08 - and running so we may have such
21:11 - dependencies or we may have an
21:12 - application our custom web application
21:15 - that also needs to connect to the
21:17 - database when we actually start the
21:19 - application to fetch some initial data
21:21 - and so on however if the database is not
21:23 - up and running when the application
21:26 - starts the application will fail with an
21:28 - error because it won't be able to
21:30 - connect to the database because it's not
21:31 - ready for the connection yet and you may
21:34 - have lots of such dependencies when
21:36 - you're running multiple Services as part
21:39 - of one application and this is something
21:41 - that you can Define in Docker compose
21:43 - with a depends on attribute so you can
21:45 - explicitly say this service actually
21:48 - needs to wait for another service or
21:51 - container to be fully up and running
21:53 - until this container is created with a
21:55 - very simple dependson attribute so
21:58 - basically we can say the express
22:01 - service depends on and we can have
22:03 - multiple dependencies so for example we
22:05 - can say an application depends on two
22:07 - different databases to start plus an
22:09 - authentication Service so all of those
22:11 - should be up and running until we start
22:13 - the application because otherwise it's
22:14 - not going to be able to connect to those
22:17 - on the initial startup so dependon takes
22:19 - a list of the services and it basically
22:23 - says wait until all the dependent
22:26 - services are fully up and run running
22:28 - before you start this service so we can
22:30 - fix it very easily using this attribute
22:34 - and now since we have both Services up
22:36 - and running again I'm going to refresh
22:39 - here and we should see Express
22:41 - accessible from the browser and we can
22:43 - actually do something here so we can
22:45 - change something in the database so for
22:47 - example I can create a mydb
22:52 - database and inside that I can create my
22:55 - collection collection I'm very bad with
22:57 - with names and not very creative so
23:00 - that's all we got we have my DB and my
23:02 - collection and this actually creates
23:04 - those in the actual mongodb database
23:08 - cool and if I go back to the terminal we
23:09 - should actually see all these change
23:11 - logs from Express and in mongodb
23:15 - basically
23:17 - logs new entries in the database that it
23:21 - created cool now what do we do if we
23:24 - want to stop those containers or maybe
23:27 - we want to change some
23:29 - configuration in do compose and restart
23:33 - those containers right now since we have
23:35 - the dock compos process running in the
23:37 - terminal we're going to need to do
23:39 - contrl c to basically break out of the
23:42 - process and this is going to stop both
23:44 - of the containers however just like with
23:46 - Docker run commands we have the detached
23:48 - mode we can actually run Docker compose
23:52 - in the detached mode like this so we'll
23:56 - start the containers in the background
23:58 - however now if we want to stop the
23:59 - containers we could stop them using
24:01 - Docker stop
24:04 - commands and providing the ID of the
24:07 - container however again if we have 20
24:10 - containers running this is not going to
24:12 - be a efficient way to do it and with do
24:15 - compose it's also very simple
24:20 - actually instead of up we do down and
24:23 - what this will do if we have 20 Services
24:25 - defined here that are running as
24:27 - containers
24:28 - it's going to go through all of those
24:30 - and it will actually not only stop those
24:33 - containers but also remove them so now
24:35 - if I do Docker PS a so this shows
24:41 - running and stopped containers so all
24:43 - the containers in any state you see that
24:45 - we have no containers because they have
24:47 - been removed completely and you also see
24:50 - the network itself was removed so
24:52 - basically with Docker composed down you
24:55 - have a very easy way to clean up the
24:57 - entire state so you don't have any
24:59 - leftovers of containers and networks
25:01 - that you created previously everything
25:03 - will be completely removed however when
25:06 - you're running containers and when you
25:08 - make changes like we did in the database
25:10 - for testing you may want to retain those
25:13 - changes the state or the data in those
25:16 - containers so you don't want to
25:17 - completely remove the containers you
25:19 - just want to stop them and then restart
25:21 - them and as you've learned in the docker
25:23 - crash course containers are ephemeral
25:26 - they have no persistence so all the data
25:28 - is gone when you remove the container
25:31 - because by default it doesn't have any
25:32 - persistence unless you configure that
25:35 - persistence with volumes however if you
25:38 - just stop the containers and restart
25:40 - them you will still have the state and
25:42 - data because the container itself was
25:43 - not removed it actually stayed locally
25:46 - so to demonstrate that let's do up
25:50 - again and with Docker compos you can
25:53 - execute stop command which simply stops
25:55 - the containers and if I do docker PSA
25:58 - you see that the containers are still
26:01 - available locally they're just not
26:02 - running they're in an exited
26:06 - status and we can start them again using
26:10 - Docker compose start
26:13 - command and if we refresh our mydb
26:17 - database and collection are gone we can
26:20 - create them
26:24 - again like
26:26 - this
26:31 - we can restart using Docker compose and
26:34 - the data should still be there so that's
26:36 - basically the difference between up and
26:38 - down commands compared to start and stop
26:41 - and obviously both have their different
26:43 - use cases and one more thing since we
26:46 - are executing Docker compose
26:48 - commands very
26:50 - often like this one for example we can
26:53 - actually go ahead and bookmark this like
26:56 - this
26:59 - so if we have too many commands in the
27:02 - history for example and if we are
27:04 - scrolling around which basically creates
27:06 - this visual marker and you can just
27:09 - click inside and it jumps directly to
27:11 - that command we can then copy that
27:14 - command and execute here perfect so now
27:18 - before we move on to the next part of
27:21 - this demo where we connect our own
27:24 - custom application to the mongodb
27:26 - database and run it also as part of
27:29 - Docker compos service let's go to the
27:33 - database and in our new collection let's
27:36 - actually create a new document that our
27:39 - application is going to need it's going
27:41 - to be a very simple document let's add
27:44 - two more attributes here so we're going
27:46 - to have let's call this my ID again as
27:49 - you see I'm very uncreative with names
27:52 - so this is going to be an ID that we can
27:55 - reference in addition to this generated
27:58 - ID and then we're going to have the
27:59 - actual data which is going to be a
28:01 - string and we're just going to write
28:03 - here some Dynamic data loaded from DB so
28:08 - when we load this from our application
28:10 - we know that it's coming from the
28:12 - database so I'm going to save this
28:14 - document in the collection you see it
28:16 - was created here here are the values the
28:18 - generated ID my ID literally my ID and
28:22 - this data um text okay and we're going
28:26 - to make this a little bit more
28:28 - interesting so we're going to use a
28:30 - custom JavaScript application which is a
28:32 - super simple application with just one
28:34 - file that simply connects to the mongodb
28:37 - database and displays the data in the
28:39 - browser so we can see some of the
28:42 - concepts in action and we're going to
28:44 - containerize our JavaScript application
28:47 - and run it as part of the docker compos
28:50 - services and of course I'm going to
28:52 - provide the link to the git repository
28:54 - where this JavaScript application is
28:56 - hosted in the the video description so
28:58 - you can just clone it locally to follow
29:01 - along and by the way you will also find
29:03 - the docker compost file in that
29:05 - repository so all the code that we write
29:07 - in this demo will be
29:10 - there so I have cloned my own
29:12 - application locally in the
29:15 - projects I've called it Docker compos
29:17 - crash course so let's switch inside and
29:22 - to show you how simple the application
29:24 - is I have opened it in the visual studio
29:26 - code so I don't have the docker compos
29:28 - here yet this is the entire application
29:31 - we basically have the server JS which is
29:33 - a node.js backend and index.html which
29:37 - has the style the JavaScript code which
29:40 - is basically just one function and the
29:43 - HTML code in one file so the simplest
29:47 - app ever created so first of all you
29:49 - don't need to understand any part of
29:52 - this code we're just going to
29:53 - concentrate on the configuration and the
29:56 - dockerization part part of this app so
29:58 - even if it's simple app you don't need
30:00 - to understand the code but just to run
30:02 - through the logic on a high level this
30:05 - backand basically connects to the
30:07 - database logic we have this index.html
30:10 - file which shows two lines of data we
30:13 - have some static data which is hardcoded
30:16 - in the file itself and then we have data
30:20 - that is supposed to come from a database
30:23 - so this is empty so we're going to set
30:25 - it dynamically from the data that we get
30:28 - from the database which is going to be
30:31 - this data right here and the way we do
30:34 - that
30:35 - is in this JavaScript section when we
30:38 - load this index HTML page it basically
30:42 - the front end basically sends a request
30:45 - to our server JS backand and it says
30:48 - fetch the data and in server JS we
30:52 - accept that request right here we
30:56 - connect to the database base using this
30:58 - logic right here and now that we are
31:01 - using my DB and my collection as the
31:04 - database and collection name that's why
31:06 - we created them in the database and it
31:09 - connects to this collection and it
31:11 - basically grabs the element that has
31:15 - this key value pair inside my ID one so
31:18 - it's going to get this data here from
31:21 - the collection and it's going to send
31:23 - that the whole object back to the front
31:26 - end as a response and then we're going
31:28 - to grab the data attribute from that
31:32 - response that's the data this is the
31:34 - value of the data and we're going to set
31:36 - it as the value for this second line so
31:39 - that's how the whole thing is going to
31:40 - work and in order to connect to the
31:43 - database because remember we actually
31:45 - set username and password on mongodb so
31:47 - our application needs to have that same
31:49 - username and password just like the
31:52 -  Express container had to have
31:55 - those credentials so we are providing
31:57 - those also as environment variables so
32:00 - just like Express had to receive
32:03 - those values as environment variables
32:05 - our application is also going to receive
32:07 - those as environment variables with
32:10 - these names so Mong to be username M Tob
32:13 - password and we use those to connect to
32:15 - the database that's the entire logic so
32:17 - now our goal is to take this application
32:20 - to build a Docker container out of it
32:23 - using the docker file blueprint which is
32:25 - right here also very simple simple
32:27 - because it's a nodejs application we use
32:29 - node as a base image uh we basically
32:32 - take the code that we have here in the
32:34 - app folder we copy it into the image we
32:38 - run npm install to download the
32:40 - dependencies inside the
32:43 - image and then we just start the
32:46 - application using node command which
32:48 - comes from here and server.js file which
32:52 - is this file right
32:54 - here so it starts the application on
32:57 - Port 3000 and logs this as a first log
33:01 - of the
33:02 - application so we want to use Docker
33:05 - file and again you learn Docker file in
33:08 - the docker crash course how to use it so
33:10 - all this should be familiar to you so we
33:12 - want to build our custom JavaScript
33:15 - application as a container and run it as
33:18 - part of Docker compose along with
33:21 - mongodb and Express Services
33:24 - that's the goal how do we do that first
33:26 - of all we need to copy that Docker
33:28 - compos that we created into this
33:31 - application code and remember another
33:33 - interesting point to highlight here
33:35 - Docker compose is part of the
33:37 - application code so developers work on
33:39 - Docker compose just like they work on
33:41 - Docker file and other parts of the
33:42 - application code which is the best
33:44 - practice to have all this logic together
33:46 - in one repository instead of scripts and
33:49 - commands spread on laptops and computers
33:52 - of different developers everything is in
33:54 - a central place so we created the
33:57 - this Docker compose file in the projects
34:01 - folder and we want to copy this into
34:03 - this folder so let's do a simple
34:11 - copy there you go
34:15 - and here we have our Services yl.
34:19 - compost file and as I said we want to
34:22 - add our application as a third service
34:26 - which is going to run as a container
34:28 - service but we don't have the image yet
34:30 - so we need to build the image as well in
34:32 - do compost what you can actually do you
34:35 - can Define both in one configuration so
34:38 - we can build and then start the
34:40 - container with Docker compose so right
34:43 - here I'm going to add the service for
34:45 - our nodejs application and let's call
34:48 - this my app because great with names and
34:53 - instead of image because we want to
34:55 - build that image first we're going to
34:57 - Simply provide build attribute and the
35:01 - build context which is current directory
35:03 - and this basically points to where the
35:06 - docker file is located as well as the
35:09 - entire build context for that image and
35:12 - the rest of the configuration is going
35:14 - to be the same as for other services so
35:17 - we have the ports in our case we're
35:19 - starting this application on Port 3000
35:22 - so that's what we're going to Define
35:25 - right here so Port 3000 inside the
35:27 - container we're going to bind it on
35:29 - 3,000 on our host and as we saw we have
35:32 - the environment variables defined here
35:35 - as well so we need to set
35:39 - those so that our application will be
35:42 - able to connect to the database using
35:45 - those credentials and that's basically
35:48 - the entire configuration this will build
35:51 - our node.js application image using
35:55 - Docker file as a blueprint for the image
35:58 - and it will start it as a container on
36:01 - this port and pass in those environment
36:03 - variables that our application will read
36:06 - here and use it to connect to the
36:08 - database now we don't have to configure
36:11 - depend on here because the application
36:13 - doesn't actually connect to the database
36:15 - when it starts up so this is the startup
36:18 - logic so here we don't have any
36:20 - connection it only connects to the
36:22 - database when we load the application in
36:25 - the front end in the browser this
36:27 - function gets executed or this script
36:29 - gets
36:30 - executed and because of that we don't
36:32 - need to do depends on here and now let's
36:37 - go back to the terminal let's first of
36:39 - all see whether we have containers
36:43 - running let's get
36:45 - our Command to stop those containers so
36:49 - we're not going to remove them because
36:50 - we need the database collection and the
36:53 - data inside for our application and now
36:56 - I'm going to go into the docker compose
36:59 - crash course folder where we have the
37:01 - new Docker compose and I'm going to
37:05 - execute Docker compose up and let's
37:10 - execute and as you see it is actually
37:12 - building the new Docker image from the
37:15 - Noe base image and that was actually
37:18 - pretty fast and now we should have all
37:20 - three containers running let's check
37:24 - that and we have really bad names for
37:27 - our containers because the name of the
37:29 - folder is very descriptive large name
37:33 - which was used as a prefix for
37:35 - containers but that's fine and this were
37:38 - created from scratch so
37:43 - our previous
37:46 - containers with this prefix are not
37:49 - actually running instead it created the
37:51 - new ones and that brings me to another
37:54 - concept which is you can actually over
37:56 - IDE the value that is used as a prefix
37:59 - so maybe you want to reuse the same
38:02 - containers but you have moved the docker
38:04 - compost file to another location so
38:06 - let's
38:10 - actually remove those containers that we
38:13 - just
38:14 - started like
38:18 - this so now we only have those two and
38:22 - the way we can
38:24 - override is use using a flag on Docker
38:28 - compose so we can add an additional flag
38:32 - here minus P or also
38:37 - project name so essentially the name of
38:39 - the folder is assumed to be the project
38:42 - name so we can overwrite that project
38:44 - name value using this flag and we can
38:47 - call this projects which was the
38:50 - previous one like this and let's start
38:53 - the
38:54 - container let's do drps as you see our
38:59 - old instances of mongodb and
39:03 - Express were restarted instead of new
39:05 - ones being created plus the network
39:08 - called projects default that was already
39:10 - there and that means if I refresh this
39:16 - we still have our MB and my collection
39:19 - and the data inside for our application
39:22 - which means if I visit the application
39:24 - on Local Host 3000 which should see our
39:28 - awesome
39:30 - application and Let me refresh this once
39:33 - again so we can see the network traffic
39:35 - here we refresh so this was the fetch
39:38 - data
39:40 - request which we execute right here that
39:44 - basically returns this
39:48 - object that we created here from in the
39:51 - database back to the front end so if we
39:55 - go to preview or our response we see
39:58 - this object with my ID one this is the
40:02 - ID from the database and the
40:05 - data some Dynamic data loaded from DB
40:09 - and we're using that to set this line
40:13 - right here so if I actually went there
40:15 - and changed
40:18 - this like this and let save I'm going to
40:22 - refresh
40:23 - again you see that now we get this
40:26 - updated data from the database so the
40:29 - entire connection Works our application
40:31 - is connected to the database and
40:34 - displays that information right here now
40:37 - I mentioned that dock compose is part of
40:40 - the application code which means it gets
40:43 - committed and stored in a git
40:46 - repository so that everyone can work on
40:48 - it it's available for the entire team if
40:51 - a new engineer joins the team and they
40:53 - download the code they have docu compos
40:55 - so they know know exactly what services
40:57 - are running as part of that application
40:59 - and they can easily start that locally
41:01 - however that also means that it's really
41:04 - bad that we are hardcoding our secret
41:08 - credentials in the docker compost file
41:10 - because the best practice for security
41:13 - is that you shouldn't hardcode any
41:15 - sensitive data in the application code
41:18 - so it doesn't end up in the git
41:20 - repository and even if you remove it
41:22 - later if you accidentally checked it in
41:25 - and removed it it's still going to in
41:26 - the commit history so you shouldn't have
41:28 - any hardcoded values here so how do we
41:31 - solve this because we need those
41:32 - credentials to be passed on to Services
41:35 - well for that we can actually use
41:36 - variables or placeholders in Docker
41:39 - compost instead of the actual values and
41:42 - we can set the values of those variables
41:45 - as environment variables on the
41:46 - operating system so let's see how it
41:48 - works first of all we're going to remove
41:50 - all those hard-coded values and instead
41:52 - we're going to define the variables in
41:55 - Docker compos which has a syntax of
41:58 - dollar sign and then curly braces and
42:00 - inside that we can name the variable
42:02 - whatever we want I'm going to call this
42:04 -  admin user because it's the admin
42:07 - user in mongod to be and by the way this
42:09 - could be lowercase you can call this
42:12 - really what you want but I'm using a
42:14 - standard environment variable name
42:16 - convention here with all upper cases so
42:19 - we have the admin user let's call this
42:21 - admin pass for password and we're going
42:25 - to reuse use those
42:31 - everywhere which is another advantage of
42:33 - using variables because if you change
42:35 - those values like if you change the
42:37 - password value for example you just have
42:39 - to change it or set it once and it
42:41 - automatically gets updated everywhere so
42:43 - now this do compost does not have any
42:45 - hardcoded sensitive data and it's safe
42:48 - to check it in the G repository however
42:51 - we still need to set those actual values
42:55 - so to test that let's go back to the
42:57 - terminal first of all I'm going to stop
43:03 - those stop the containers so we can test
43:06 - that everything works and on the first
43:08 - Docker compos command execution we get a
43:10 - warning that says the variables are not
43:13 - set the containers were stopped however
43:15 - we need to set those as variables in our
43:19 - terminal session
43:21 - so we set them here export admin
43:25 - user
43:27 - let's set the other
43:32 - one like this and the same way as we did
43:37 - with up command we actually need to
43:39 - specify which containers we're stopping
43:41 - so by default it's going to look for
43:43 - containers that start with this name the
43:46 - name of the folder so we need to
43:47 - overwrite that projects tag
43:50 - again and there you go and I'm actually
43:53 - going to bookmark this one as well
44:01 - like
44:04 - this and if we
44:06 - refresh we should see that the pages are
44:09 - not working because the containers are
44:11 - stopped and then let's start them
44:15 - again and if we start them again with
44:17 - those environment variables
44:21 - set everything should
44:23 - work same as before
44:26 - let's wait there you go now I want to
44:29 - mention here that Docker compos actually
44:30 - has a concept of Secrets which is
44:34 - another functionality to manage the
44:35 - secrets especially when you're running
44:37 - do compos in a production environment
44:39 - which is exactly for this use case where
44:42 - you need to pass in credentials or any
44:43 - sensitive data to the services defined
44:47 - in Docker compose so you can use Docker
44:49 - compose Secrets as an alternative to
44:51 - this method basically awesome we have
44:53 - just learned the fundamentals of docker
44:55 - compose and more importantly you
44:57 - understand its core use case and by the
45:00 - way I want to highlight the importance
45:02 - of learning tools like Docker and Docker
45:04 - compos or generally cloud and devops
45:07 - Technologies because nowadays it is
45:09 - becoming more and more needed for
45:11 - software developers to learn those tools
45:13 - to become more valuable in their roles
45:16 - especially in the current tense job
45:18 - market where we have layoffs and
45:20 - companies hiring less as more and more
45:22 - companies are adopting devops it is a
45:25 - great way to Stand Out Among developers
45:28 - who only focus on programming and are
45:31 - not interested to learn new Concepts and
45:33 - tools that are being adopted in the
45:35 - industry so I think it's definitely more
45:37 - important than ever to educate yourself
45:40 - keep learning and with devops or Cloud
45:42 - engineering skills you will definitely
45:43 - be ahead of 90% of developers in fact
45:47 - most of our devop boot camp students are
45:50 - actually software developers or software
45:51 - Engineers since many companies do not
45:54 - have a separate devops engineer role but
45:57 - often their responsibility lies on
45:59 - senior developers to set up the devops
46:01 - processes like release pipelines for
46:03 - example so even if you are a junior
46:06 - software engineer learning devops and
46:08 - Cloud skills and technologies will
46:10 - absolutely accelerate your career to a
46:13 - senior engineer so if you want to get a
46:15 - complete education on devops to take
46:18 - over devops tasks at your work then
46:21 - definitely check out our devops boot
46:23 - camp you will learn various Technologies
46:25 - from zero to an advanced level to be
46:28 - able to build real world Davos processes
46:32 - at your job and if you need some
46:33 - guidance before you can also contact us
46:36 - there with your questions so check out
46:38 - the information below and let's move on
46:40 - to the next
46:42 - part so now we are building and running
46:44 - our JavaScript application as a
46:46 - container along with mongodb service and
46:50 - the mongodb UI but usually that's a
46:52 - testing environment as you learn in the
46:54 - docker crash course eventually you want
46:56 - the JavaScript application your custom
46:58 - application container to be stored
47:01 - centrally in a Docker registry or rather
47:04 - the image to be stored in Docker
47:06 - registry so we can start and deploy it
47:07 - as a container on the end environment on
47:10 - a actual deployment server where end
47:12 - users will access it so we need to build
47:14 - the image and push that to the
47:16 - repository like a dockerhub repository
47:19 - or whatever other Docker repository want
47:22 - and now the interesting question is
47:24 - after we push the image to private
47:25 - Docker repository how do we reference
47:28 - our Custom Image from our private Docker
47:31 - repository in Docker compose and also
47:34 - note that when we run this on an actual
47:37 - deployment server we're going to copy
47:40 - the docker compose on that server where
47:42 - we have Docker and Docker compose
47:43 - installed and when we run this Docker
47:45 - compost file or execute this it will go
47:47 - through all the services and it will
47:49 - pull all the images defined here and run
47:51 - them as containers with all this
47:52 - configuration so we pull the official
47:54 - images from docker H public repository
47:57 - and any custom images from the private
48:00 - repositories so let's actually see how
48:02 - it works it's actually very very easy
48:04 - and again building image pushing it to
48:07 - the repository the whole thing you
48:08 - should already know it from the docker
48:09 - course so this should be a refresher for
48:12 - you so let's actually see that in action
48:15 - right here so first of all I'm going to
48:17 - log into my dockerhub
48:19 - account and I'm actually going to create
48:21 - a new private repository in dockerhub
48:24 - like this
48:26 - let's call this my app because it's
48:28 - generic I may use it for some other
48:31 - demonstration later
48:33 - so there you go and now we're going to
48:38 - build our image using the docker file
48:42 - and we're going to push that image to
48:44 - this specific private
48:46 - repository so let's execute those
48:48 - commands I'm going to build using Docker
48:51 - build command I'm going to tag this and
48:53 - this is again a refresher from Docker
48:56 - course we need to take the image with
48:59 - the name of the repository so that
49:01 - Docker knows on push command which
49:03 - repository to push that image to so
49:06 - that's going to be the entire name so
49:08 - the image name itself includes the
49:11 - repository name and we're just going to
49:13 - tag it with a simple
49:15 - 1.0 and we need to provide the build
49:18 - context which is the current directory
49:20 - where Docker file is located and that's
49:22 - basically it let's
49:24 - execute again
49:26 - super first let's list all the images so
49:30 - we can see what we have built locally
49:32 - and there you go this is our image with
49:35 - an image tag and this is exactly the
49:38 - image we want to push now to the private
49:41 - reposer and you know when we want to
49:43 - push or pull from a private repository
49:45 - we need to be logged in to that
49:47 - repository so we need to do Docker login
49:50 - and the username is actually your
49:53 - dockerhub username name and dockerhub
49:59 - user password and this is actually
50:02 - different for other Docker repositories
50:05 - so if you have a ECR or some other
50:08 - Docker repository the process may be
50:10 - different with dockerhub it's very
50:12 - simple that's why I use it for the demos
50:14 - mostly so we are now logged in to
50:17 - dockerhub and what I can do now is
50:22 - basically push that image that we just
50:24 - created
50:26 - using simple Docker push and the image
50:29 - full name with the
50:32 - tag and there you
50:35 - go and if I refresh we should see one
50:40 - tag here 1.0 for our my app image
50:44 - repository perfect and one thing I
50:46 - wanted to show you here is that whenever
50:49 - you are building and pushing an image
50:51 - you basically have the same commands all
50:53 - the time for the specific action you
50:56 - build the image you log in if you're not
50:58 - logged in already you push the image and
51:00 - so on and I have actually used a feature
51:03 - called workflows in workp for these kind
51:05 - of use cases which can be really helpful
51:08 - if you want to if you have a set of
51:09 - commands that you always need for the
51:12 - same type of workflow you can basically
51:14 - Define that as one unit so you can group
51:17 - those commands in one unit called the
51:20 - workflow and you can save it here and
51:23 - whenever you need that you can just
51:25 - execute all those commands with one
51:26 - click which I personally found super
51:29 - cool so in warp drive you have some
51:32 - options here you create a new
51:34 - workflow and I'm going to call this
51:37 - Docker push and here you can list the
51:40 - different commands basically so let's do
51:43 - Docker login and we have
51:46 - username which was this one right here
51:49 - and obviously we don't want to provide a
51:51 - password hardcoded here so we're going
51:53 - to pass that as very
51:56 - variable or
52:00 - argument and we're going to read that
52:03 - from the standard input again this is a
52:05 - refresher from Docker this is how should
52:08 - use Docker login so you don't type in
52:09 - the password directly and with worp you
52:13 - can actually use
52:15 - arguments like this so whenever you run
52:18 - this command or list of commands before
52:21 - it runs it will actually tell you how
52:22 - you should fill out this argument so
52:24 - we're going to use an AR arent here and
52:26 - then after Docker login we're going to
52:27 - do Docker build like we
52:31 - did with an image
52:34 - tag we can also make this an argument
52:38 - let's make this an argument number two
52:41 - and then push
52:45 - that like this you can also set default
52:48 - values let's actually do 1.0
52:51 - here and this way you don't have to type
52:54 - out all the commands for the same
52:56 - workflow so let me check all these
52:59 - commands so we need a build context at
53:01 - the end and the rest looks pretty good
53:05 - and let's save the workflow and the way
53:06 - it works is now I have the workflow
53:09 - right
53:10 - here and whenever I need that workflow
53:13 - to execute I just click on it and it
53:17 - fills out the terminal basically with
53:19 - all these commands and it highlights the
53:22 - the arguments that I need to set so I'm
53:24 - actually going
53:25 - to put my password here as an argument
53:30 - and then let's say we want 1.1 as a
53:33 - second argument and we can execute all
53:36 - the commands like this and this will
53:38 - actually have pushed another tag with
53:41 - 1.1 so this is a cool feature that you
53:43 - can use on warp to make your life a
53:45 - little bit more convenient and that
53:48 - means now we have our image with two
53:50 - different Texs in a private
53:54 - Repository and now if we go back to the
53:57 - dock compost we don't need to build it
53:59 - locally we can again this is convenient
54:02 - for testing because when you're testing
54:05 - on a local environment as a software
54:06 - developer as an engineer you may want to
54:08 - just do very quick local changes in a
54:11 - Docker file or in application test it
54:13 - quickly and you don't want to be
54:15 - building and pushing and pulling image
54:18 - all the time so this is a very good
54:21 - functionality for local testing however
54:23 - on an end environment obviously we need
54:25 - to Define an image that comes from a
54:28 - repository maybe we have scanned that
54:30 - image already and made sure that it's
54:32 - secure and properly configured and so on
54:35 - and now we want to actually use it and
54:38 - how do we reference our Custom Image
54:40 - from a private repository in Docker
54:42 - compose very
54:44 - simple basically do it just like any
54:47 - other image in dockerhub or any other
54:49 - repository like this with a specific
54:52 - image tag that is available let's do 1.0
54:55 - and how will Docker compos be able to
54:57 - pull that image from a private
54:59 - repository or basically authenticate
55:01 - with the private repository to pull the
55:03 - image it actually uses the same Docker
55:05 - login that we use to do Docker push so
55:09 - Docker login basically creates after
55:11 - successful login authentication with the
55:14 - docker Hub it actually creates a Docker
55:17 - Json file locally that creates the
55:20 - authentication credentials or tokens in
55:23 - that file and Docker compos in the
55:25 - background is using Docker to run those
55:28 - containers so it's going to be the exact
55:29 - same process pulling or pushing the
55:31 - images from Docker compose so that means
55:35 - if you have done Docker login already to
55:37 - that repository then you should be able
55:39 - to pull any images defined in Docker
55:42 - compose from that repository that means
55:45 - that's the configuration this should
55:47 - work now in order to test that let's
55:50 - actually
55:52 - stop our containers
56:02 - so they're all stopped and I'm actually
56:04 - going to remove the application
56:07 - container because we want to simulate
56:09 - that the container is recreated from the
56:12 - new image that we
56:14 - pull and now if we
56:22 - do up again in DET mode
56:26 - and as you see my app image was pulled
56:31 - and the container my app was started
56:34 - from that if we check the running
56:37 - containers we see that this is the image
56:40 - that was used to create this container
56:45 - awesome and again we can check that our
56:48 - application still works and there you go
56:51 - that's how we can reference our Custom
56:55 - Image from a Docker repository in the
56:58 - docker compose and in case when you're
57:01 - executing those commands so let's say
57:03 - we're doing Docker build and you forget
57:06 - one of the arguments or use a wrong flag
57:10 - and so
57:11 - on first of all you get an error but you
57:14 - can also do a troubleshooting feature
57:17 - within the terminal to actually give you
57:21 - a pretty good tips and notes on what the
57:24 - error actually is because sometimes we
57:27 - make spelling mistakes sometimes we
57:28 - forget an argument or whatever so it
57:31 - could be helpful for a tool to actually
57:33 - tell you what the actual problem is so
57:35 - you can fix it and warp has this AI
57:38 - assistant which is pretty cool so for
57:41 - this specific error if I open this warp
57:44 - AI which you can see on every command
57:47 - block so you have this bookmark and you
57:49 - have this warp AI so if I click on this
57:53 - it actually autog generates the question
57:54 - question because it knows that this is
57:57 - an error and you can ask it how to fix
58:00 - it so you can modify your question or
58:03 - example and if I hit enter here it gives
58:06 - me an answer that the docker build
58:08 - command requires an argument which
58:10 - should be the path to the docker file
58:12 - and gives me an example with the correct
58:14 - one so change the directory that has
58:16 - Docker file and then execute this
58:19 - command which has dot at the end so I
58:21 - found this feature also pretty cool
58:23 - which means if any of the commands give
58:25 - you an error while you're following this
58:28 - demo you can actually use this to find
58:31 - out what the error is about and and
58:33 - ideally how to fix it so this is going
58:35 - to help you troubleshoot your issues and
58:38 - finally I want to actually add a few
58:40 - very interesting and important Concepts
58:43 - regarding docu compose and kind of what
58:46 - the next steps are so this final small
58:49 - section may be really really interesting
58:51 - for you so basically as you see the main
58:53 - use case of Docker composed was to have
58:55 - a central place to manage containers
58:58 - that were supposed to run together like
59:00 - your application and all the services
59:02 - that it depends on and we configure all
59:04 - the environment variables or any other
59:06 - configuration for those services in that
59:09 - one file and also start them in one
59:11 - isolated Docker Network and it makes it
59:14 - super easy for us to clean up all the
59:16 - resources so Engineers took Docker and
59:19 - they containerized their applications to
59:21 - a whole new scale that was not a
59:24 - standard before and Docker was
59:26 - especially perfect to use as a host for
59:29 - microservice applications where you have
59:32 - even more applications and more
59:34 - containers now running in one
59:36 - environment and again if you don't know
59:38 - about microservices I have a separate
59:39 - video about them but essentially it's
59:42 - when you have all the services needed to
59:44 - run one application but split into
59:47 - separate micro applications or services
59:49 - and they can be scaled independently and
59:52 - run independently as independent
59:53 - containers so Docker was a perfect host
59:56 - for that so we ended up with lots of
59:59 - applications lots of microservices
60:01 - applications with hundreds or thousands
60:03 - or tens of thousands of containers that
60:06 - is pretty much a standard nowadays such
60:09 - a scale actually led to Docker compost
60:12 - actually not being able to handle such
60:16 - large scale of containers and more
60:18 - importantly Engineers will have to still
60:20 - manually manage running and operating
60:22 - those containers with do compose like if
60:25 - containers die or crash or have
60:27 - connectivity issues ETC you have to
60:30 - manually detect and then debug and
60:32 - restart the services now Docker compose
60:34 - actually made some improvements on that
60:35 - there tags like restart and so on but
60:38 - it's still a lot of operational effort
60:41 - to run the containers with this kind of
60:44 - scale where you have thousands of them
60:46 - using Docker compose and that's where
60:49 - kubernetes kind of came into the picture
60:51 - to solve exactly these two main issues
60:54 - initially scaling to thousands or tens
60:57 - of thousands of containers with
60:59 - kubernetes you can basically merge
61:00 - hundreds of servers into one huge server
61:03 - to deploy all the containers that belong
61:05 - to the same application in that
61:07 - environment they will all run as if they
61:09 - were running on the same server so it
61:10 - naturally makes it easier to scale your
61:12 - applications and to run thousands of
61:15 - instances and the second one was the
61:17 - automatic operations or making the
61:19 - operations of applications easier or
61:21 - also called kubernetes Auto healing
61:23 - feature which basically manages starting
61:26 - and restarting containers if they crash
61:29 - and has mechanisms to manage operations
61:31 - of a large number of containers in an
61:33 - automated way when manual effort isn't
61:36 - it's just impossible or not feasible
61:38 - anymore and that led to C is becoming so
61:41 - popular so Docker compose is kind of
61:44 - like a intermediary step if you have
61:46 - smaller set of containers but with
61:49 - today's standards when you want to work
61:51 - with very complex applications with a
61:53 - large scale then Docker compose has its
61:56 - limits so that's where kubernetes
61:58 - basically comes into the picture so if
62:00 - you're learning this containerized
62:02 - containerization and container
62:04 - orchestration Concepts then I would
62:06 - actually recommend to use this road map
62:08 - of learning the docker using the docker
62:11 - crash course then learning the docker
62:13 - compose with this course like you did
62:16 - and then you can move on to the
62:17 - kubernetes and if you want to learn
62:19 - kubernetes I also very conveniently have
62:21 - a kubernetes crash course to get you
62:23 - started in kubernetes very easily so if
62:26 - you want to get started with that you
62:28 - can check out any of the many videos
62:30 - that I have on my YouTube channel but I
62:32 - would recommend to start with the
62:34 - kubernetes crash course so I hope you
62:36 - learned a lot of new Concepts you
62:38 - obviously learned Docker compose as a
62:40 - new tool new technology thank you for
62:43 - watching till the end let me know in the
62:45 - comments how this video actually helped
62:47 - you in your work or maybe in your job
62:49 - application I'm always happy to hear and
62:52 - read that feedback from our viewers to
62:54 - know that my videos are helpful in
62:56 - actual job environment you can also
62:59 - share any other tips and learnings about
63:02 - dock compose that you have from your
63:03 - practical experience so that other
63:06 - viewers can read and benefit from it as
63:08 - well and with that thank you for
63:10 - watching and see you in the next video

Cleaned transcript:

in this video you will learn everything you need to know to get started with using Docker compose we'll go over what it is exactly what problems Docker compose was designed to solve its common use cases and of course we will do some HandsOn demos to learn actually using Docker compose in practice I am super excited to teach you all this so let's jump into it now in order to understand Docker compos you need to First understand docker and have some basic experience with it if you don't I recommend you pause and watch my Docker crash course first and then continue with this one because Docker compost is essentially a tool that is supposed to manage and work with Docker containers so you need to understand that part first so that you understand the context for learning Docker compost so in the docker video I break down what the containers are what images are what problems Docker solves and what use case it it has dockerizing your application with Docker file and all the concepts you need to understand Docker itself so based on that knowledge we can Now understand why Docker compos was created along with Docker and when we want to use it now applications are composed of many different parts you can have apis databases any Services your application depends on and even within the application you may have a microservice application which is basically an application broken down into multip micro applications or microservices and when you're creating containerized applications all of these different application components must be deployed and run together because they have dependencies on each other so basically you have a set of containers which are running different services and applications within them that need to run together that need to talk to each other and so on so Docker compose is basically a tool that allows you to Define and run multiple services and applications that belong together in one environment so simply put if you want to deploy multiple Docker containers where each container may have its different configuration options you can use Docker compose to do this to manage these containers way more easily now this is just a general definition to give you some idea of what Docker compose is but of course we want to understand this with specific examples and specific demonstration so that you really understand these Concepts and the actual use cases of using doer compose and not just a general abstract explanation of what it is and because of that we're going to jump right into that demo where I'm going to explain the concepts the use cases using those demonstrations so let's get started as a first step we're going to start two Services as Docker containers using just the docker command so we're not going to use Docker compos as a first step so we can see and compare the before after States first we're going to create a Docker Network where these two containers will run and talk to each other using just the container name and then we're going to start two containers one is going to be a mongodb container and another one is going to be Express container which is basically a UI for the mongodb database very simple use case and we're going to run both containers using Docker run commands so that's our first very simple demonstration let's go ahead and do that so I'm going to switch to a terminal because we're going to execute those docket run commands on the terminal and you probably see this is a fancy fun looking terminal that I have been using since recently and this is an application or a terminal app called warp which is actually a sponsor of this video I actually played around with warp and love using it it's free it's easy to install on your computer so I will be using warp throughout the entire day demo because it also helps highlight some of the commands and stuff better so it's going to be easier for you guys to follow what I'm showing you however if you want to install warp yourself on your computer you can go ahead and check out the link to get started in the video description where I'm going to provide all the relevant links for this crash course including the warp installation so to run our Docker containers of course we need to have Docker installed and running so I'm going to start up Docker and then we can start the containers so the docker service is up and running let's go ahead and create the docker Network first so I'm going to do Docker Network and since we're going to run mongodb and Express containers we can call this network Network and let's create and now if I do Docker Network LS so basically list all the networks available these are the default ones basically that you get out of the box when you install Docker and this is the Network that we just created awesome so the network is there now let's run our two containers and if you know Docker if you followed my Docker crash course basically you know all this stuff Docker run and we're going to execute this in the background and we have the and Express image documentation so we can actually reference this so first I'm going to define the port uh mongodb's default Port is $27 07 so we're going to map that to the same port so we we're going to bind that to the same port on the host then we're going to define those two environment variables to basically set the admin or the root user and password so we're going to copy those and we're going to call this admin and this is some password we're going to set this to super secret so all these should be actually be a refresher from Docker we also want to specify that it should run in this network network so we're going to do Network run in this one we're also going to name our container instead of having Docker just create a random container name so we're going to call this DB and finally we need to specify the image right and this is the name of the image and that's basically our Docker command so I'm going to execute and this will fetch or pull the latest image from dockerhub repository and run it in a detached mode perfect so we should have have our mongodb container running and now let's start Express container and I can actually bring up my previous command and we're going to adjust it for the Express right here we see that Express is running on port 8080 so that's what we're going to set here there you go we also have different environment variables so basically Express is just a UI for mongodb and in order for us to use it it needs to connect and authenticate with mongodb so we need to provide it the credentials as well that we set for mongodb database and we're passing those also as environment variables but in this case the environment variables are named differently so that's what we're using referring to the official documentation which you always should do to get the most up toate data and you also see the default values for those environment variables the port is correct because that's what we binded it to on our host and mongod to be server which is going to be the mongod to be container name in our case it's different because we called our container mongod beam so we're going to set this environment variable as well so right here I'm going to add this and we're going to set these to mongodb let's not forget the backwards slash here so the ports are correct the environment variables are correct we are going to run it also in the Network we're going to name this Express so that's going to be the name of the container and let's see what the actual name of the image is just going to copy that so that I don't make spelling mistake and that's it let's execute this as well and seems like it started without any problems let's see perfect it's running and now to test that it was actually able to connect without any issues to the mongodb database container we're going to access it in our browser so we opened it on Port 881 on our host and it is asking for basic authentication in the browser and we can actually get those in the locks let's do that do logs of Express and here we have the credentials so admin pass should work and there you go so that's a test and a proof that it was able to connect to our database since we didn't have any connection errors here and we're able to access the application here so this was basically just to demonstrate how you would start containers that belong to each other so Express container actually depends on mongodb because we don't need it without the database in the background so kind of start containers that belong together that should run together using just plain Docker and also starting them in the same network so they can talk to each other in that isolated virtual Network now obviously these are just two containers but if we have microservice application with 10 different services that has a messaging service maybe two databases that it belongs to maybe those databases have their own UI services that we want to run in addition so now these are lots of containers that we need to start and manage using just plain Docker commands and now imagine if you need to stop those containers because you don't want to have them running all the time or you want to make changes and restart them again this is going to be a lot of manual tedious work and you don't want to execute these commands all the time on the command line terminal especially when you have tens of containers so you want an easier way to manage to stop start configure containers that you want to start together and that's exactly where Docker compose comes into the picture so Docker compos basically makes running multiple Docker containers with all this configuration that we just defined on those containers so you have the environment variables you have ports maybe you have multiple ports on the same container same application that you want to open maybe you want to configure additional volumes for example for data persistence so that's the main use case of Docker compose so with Docker compose basically you have a file a yaml file where you define all this configuration a list of contain ERS or services that you want to start together and all their configuration in one central place in a file that you can modify configure and use to start and stop those containers so let's see how the file looks like and how these Docker run commands actually map to the docker compost so how can we migrate or map all of these and write a Docker compost file that starts those two containers with exactly the same configuration that we defined here so this is a Docker run command of the mongod beam that we executed previously so basically with Docker compos file what we can do is we take the whole command with this configuration and map it into a file so we have that command defined in a structured way so if you have let's say 10 20 Docker containers that you want to run for your application and they all need to talk to each other and interact with each other you can basically write all the Run commands for each container in a structured way in Docker compose and Define the entire configuration there and this is how the structure in Docker compose will actually look like so the first two lines are required attributes of Docker compose file with the first line we basically Define the version of Docker compose which is the latest version that should be compatible with the docker compose that you have installed locally so the latest Docker compose tool installed on your computer will be able to read the latest Docker compose file version and then we have the services and Docker compose is super simple Services is basically an attribute where you can list all the services or all the containers that you want to run as part of this doer compos file so in this case the first service we want to Define is mongodb and that Maps actually to The Container name or rather this is going to be part of the container name when the services are created as Docker containers and for each service like Mong TB we have all the configuration for that specific container so the first one is obviously image because we are building the container from the image so we need to know which image that container is going to be built from and of course you can specify version Tech here next to the name the next one is the list of ports because you can open multiple ports on a container if there are multiple processes running inside the container but mostly you would just have one so this is where we Define the port mappings so mapping a container port to the host so just like in Docker command the first Port refers to the host the second one refers to the port inside container then we have the environment variables listed under an environment attribute like this and this is actually how the structure of Docker compose looks like for one specific command now let's actually add the second container command for Express and how that Maps into our Docker compost file so again we have the service which we can call Express and by the way the service names are completely up to you you can call them whatever you want just like the container names you can call the containers whatever you want and under that Express we have the same exact configuration options we have the image which refers to Express image again you can have a TCH here if you want to have a specific one then we have the port and all the environment variables that we defined with Docker run command under the environment attribute and this is how Docker compos will look like with multiple Services defined inside so basically Docker compos is just a structured way to contain very normal common Docker commands and of course it's going to be easier for you to edit this file if you want to change some variables or if you want to change the ports or if you want to add more services with those services and as part of everything as code Trend dock compose is basically a code that defines how your services should run within a file that you can check in to a code repository and multiple people can work on it together compared to a command that you just execute manually on your computer with individual Docker run commands the final thing here which you may already noticed is the network configuration is not defined in the docker compost so we didn't map that part from the docker run commands so this Monga Network that we created we don't have to explicitly create or Define it in Docker compost because Docker compose will actually take care of creating a shared network for all the containers from from the services list that it's going to run when we execute this file so we don't have to create the network specifically and then specify that all the containers run in that Network Docker compose will automatically take care of it and we're actually going to see that in action right away so now let's actually go and create a Docker compos file in a code editor so in this projects directory so basically where I'm in the terminal I created this mongos services. yl file which is my Docker compos file with those two Services defined here so exactly the same code that you just saw we have our credentials all our environment variables defined and since this is a yl format please make sure that your indentations are correct because yl is a very simple language but it's very strict on indentation so the services need to be on the same level and then inside that service you need to have correct indentation for the configuration attributes so now compared to the docker commands it's going to be easier for me to go here to this file first of all see what services I'm running with what configuration edit those make any changes add any new services that I want to run and now let's actually execute this Docker compos file and see how it works back to my warp terminal I'm actually going to stop all the containers because we want to start them using Docker compost so that's the first one let's stop them we can actually remove them and we can also remove the dock Network and there you go so we have a clean State no containers running and now how do we execute a Docker compost file with Docker compost good news is if you have Docker installed on your computer that means you automatically also have Docker compose installed so you don't have to do that separately that means we should have Docker compose command already available as part of Docker and Docker compos takes one attribute which is the file name Services there you go and the command which is up which basically means go through the docker compost file provided here and Run start all the services configured right here so let's execute this and we're going to see the result awesome so now there are a couple of interesting things that I want to point out and highlight in the output that we got and also explain some of the interesting Concepts behind so let's go through them one by one I'm going to scroll all the way up to the beginning of the output which is right here when we executed Docker compose command the first one is I mentioned that Docker compose takes care of creating a dedicated Network for all the containers and here we see in the output that it actually created Network called projects uncore default so this is the name of the network and it's going to run those two containers in that Network so if I open another terminal and if I do Docker Network LS we're going to see projects default network was created another interesting thing to point out is the container names for those two containers so in the docker compose we actually called those services mongodb and Express however as you see Docker compose actually added a prefix projects and a suffix at the end to each service so this is basically the folder that contains the docker compos file where the docker compos file is located as you see right here so Docker compose always takes the name of the folder where the docker compose file is executed and it uses it as a prefix of the container and then you have one as a suffix so we have one instance of each container and that's how the containers are called and we can also check our containers and here you see the names projects mongodb 1 another interesting thing to point out is that you see that the logs of those two containers are actually mixed so we have the mongod be logs Express then mongod be again and so on because we're starting both containers at the same time so if you had 20 Services defined here they will all start at the same time and you will see the logs basically just mixed together on Startup however when you have multiple Services where some Services actually depend on the others in our case Express depends on mongodb because it cannot establish a connection the initial connection with the service until mongodb is fully up and running so we may have such dependencies or we may have an application our custom web application that also needs to connect to the database when we actually start the application to fetch some initial data and so on however if the database is not up and running when the application starts the application will fail with an error because it won't be able to connect to the database because it's not ready for the connection yet and you may have lots of such dependencies when you're running multiple Services as part of one application and this is something that you can Define in Docker compose with a depends on attribute so you can explicitly say this service actually needs to wait for another service or container to be fully up and running until this container is created with a very simple dependson attribute so basically we can say the express service depends on and we can have multiple dependencies so for example we can say an application depends on two different databases to start plus an authentication Service so all of those should be up and running until we start the application because otherwise it's not going to be able to connect to those on the initial startup so dependon takes a list of the services and it basically says wait until all the dependent services are fully up and run running before you start this service so we can fix it very easily using this attribute and now since we have both Services up and running again I'm going to refresh here and we should see Express accessible from the browser and we can actually do something here so we can change something in the database so for example I can create a mydb database and inside that I can create my collection collection I'm very bad with with names and not very creative so that's all we got we have my DB and my collection and this actually creates those in the actual mongodb database cool and if I go back to the terminal we should actually see all these change logs from Express and in mongodb basically logs new entries in the database that it created cool now what do we do if we want to stop those containers or maybe we want to change some configuration in do compose and restart those containers right now since we have the dock compos process running in the terminal we're going to need to do contrl c to basically break out of the process and this is going to stop both of the containers however just like with Docker run commands we have the detached mode we can actually run Docker compose in the detached mode like this so we'll start the containers in the background however now if we want to stop the containers we could stop them using Docker stop commands and providing the ID of the container however again if we have 20 containers running this is not going to be a efficient way to do it and with do compose it's also very simple actually instead of up we do down and what this will do if we have 20 Services defined here that are running as containers it's going to go through all of those and it will actually not only stop those containers but also remove them so now if I do Docker PS a so this shows running and stopped containers so all the containers in any state you see that we have no containers because they have been removed completely and you also see the network itself was removed so basically with Docker composed down you have a very easy way to clean up the entire state so you don't have any leftovers of containers and networks that you created previously everything will be completely removed however when you're running containers and when you make changes like we did in the database for testing you may want to retain those changes the state or the data in those containers so you don't want to completely remove the containers you just want to stop them and then restart them and as you've learned in the docker crash course containers are ephemeral they have no persistence so all the data is gone when you remove the container because by default it doesn't have any persistence unless you configure that persistence with volumes however if you just stop the containers and restart them you will still have the state and data because the container itself was not removed it actually stayed locally so to demonstrate that let's do up again and with Docker compos you can execute stop command which simply stops the containers and if I do docker PSA you see that the containers are still available locally they're just not running they're in an exited status and we can start them again using Docker compose start command and if we refresh our mydb database and collection are gone we can create them again like this we can restart using Docker compose and the data should still be there so that's basically the difference between up and down commands compared to start and stop and obviously both have their different use cases and one more thing since we are executing Docker compose commands very often like this one for example we can actually go ahead and bookmark this like this so if we have too many commands in the history for example and if we are scrolling around which basically creates this visual marker and you can just click inside and it jumps directly to that command we can then copy that command and execute here perfect so now before we move on to the next part of this demo where we connect our own custom application to the mongodb database and run it also as part of Docker compos service let's go to the database and in our new collection let's actually create a new document that our application is going to need it's going to be a very simple document let's add two more attributes here so we're going to have let's call this my ID again as you see I'm very uncreative with names so this is going to be an ID that we can reference in addition to this generated ID and then we're going to have the actual data which is going to be a string and we're just going to write here some Dynamic data loaded from DB so when we load this from our application we know that it's coming from the database so I'm going to save this document in the collection you see it was created here here are the values the generated ID my ID literally my ID and this data um text okay and we're going to make this a little bit more interesting so we're going to use a custom JavaScript application which is a super simple application with just one file that simply connects to the mongodb database and displays the data in the browser so we can see some of the concepts in action and we're going to containerize our JavaScript application and run it as part of the docker compos services and of course I'm going to provide the link to the git repository where this JavaScript application is hosted in the the video description so you can just clone it locally to follow along and by the way you will also find the docker compost file in that repository so all the code that we write in this demo will be there so I have cloned my own application locally in the projects I've called it Docker compos crash course so let's switch inside and to show you how simple the application is I have opened it in the visual studio code so I don't have the docker compos here yet this is the entire application we basically have the server JS which is a node.js backend and index.html which has the style the JavaScript code which is basically just one function and the HTML code in one file so the simplest app ever created so first of all you don't need to understand any part of this code we're just going to concentrate on the configuration and the dockerization part part of this app so even if it's simple app you don't need to understand the code but just to run through the logic on a high level this backand basically connects to the database logic we have this index.html file which shows two lines of data we have some static data which is hardcoded in the file itself and then we have data that is supposed to come from a database so this is empty so we're going to set it dynamically from the data that we get from the database which is going to be this data right here and the way we do that is in this JavaScript section when we load this index HTML page it basically the front end basically sends a request to our server JS backand and it says fetch the data and in server JS we accept that request right here we connect to the database base using this logic right here and now that we are using my DB and my collection as the database and collection name that's why we created them in the database and it connects to this collection and it basically grabs the element that has this key value pair inside my ID one so it's going to get this data here from the collection and it's going to send that the whole object back to the front end as a response and then we're going to grab the data attribute from that response that's the data this is the value of the data and we're going to set it as the value for this second line so that's how the whole thing is going to work and in order to connect to the database because remember we actually set username and password on mongodb so our application needs to have that same username and password just like the Express container had to have those credentials so we are providing those also as environment variables so just like Express had to receive those values as environment variables our application is also going to receive those as environment variables with these names so Mong to be username M Tob password and we use those to connect to the database that's the entire logic so now our goal is to take this application to build a Docker container out of it using the docker file blueprint which is right here also very simple simple because it's a nodejs application we use node as a base image uh we basically take the code that we have here in the app folder we copy it into the image we run npm install to download the dependencies inside the image and then we just start the application using node command which comes from here and server.js file which is this file right here so it starts the application on Port 3000 and logs this as a first log of the application so we want to use Docker file and again you learn Docker file in the docker crash course how to use it so all this should be familiar to you so we want to build our custom JavaScript application as a container and run it as part of Docker compose along with mongodb and Express Services that's the goal how do we do that first of all we need to copy that Docker compos that we created into this application code and remember another interesting point to highlight here Docker compose is part of the application code so developers work on Docker compose just like they work on Docker file and other parts of the application code which is the best practice to have all this logic together in one repository instead of scripts and commands spread on laptops and computers of different developers everything is in a central place so we created the this Docker compose file in the projects folder and we want to copy this into this folder so let's do a simple copy there you go and here we have our Services yl. compost file and as I said we want to add our application as a third service which is going to run as a container service but we don't have the image yet so we need to build the image as well in do compost what you can actually do you can Define both in one configuration so we can build and then start the container with Docker compose so right here I'm going to add the service for our nodejs application and let's call this my app because great with names and instead of image because we want to build that image first we're going to Simply provide build attribute and the build context which is current directory and this basically points to where the docker file is located as well as the entire build context for that image and the rest of the configuration is going to be the same as for other services so we have the ports in our case we're starting this application on Port 3000 so that's what we're going to Define right here so Port 3000 inside the container we're going to bind it on 3,000 on our host and as we saw we have the environment variables defined here as well so we need to set those so that our application will be able to connect to the database using those credentials and that's basically the entire configuration this will build our node.js application image using Docker file as a blueprint for the image and it will start it as a container on this port and pass in those environment variables that our application will read here and use it to connect to the database now we don't have to configure depend on here because the application doesn't actually connect to the database when it starts up so this is the startup logic so here we don't have any connection it only connects to the database when we load the application in the front end in the browser this function gets executed or this script gets executed and because of that we don't need to do depends on here and now let's go back to the terminal let's first of all see whether we have containers running let's get our Command to stop those containers so we're not going to remove them because we need the database collection and the data inside for our application and now I'm going to go into the docker compose crash course folder where we have the new Docker compose and I'm going to execute Docker compose up and let's execute and as you see it is actually building the new Docker image from the Noe base image and that was actually pretty fast and now we should have all three containers running let's check that and we have really bad names for our containers because the name of the folder is very descriptive large name which was used as a prefix for containers but that's fine and this were created from scratch so our previous containers with this prefix are not actually running instead it created the new ones and that brings me to another concept which is you can actually over IDE the value that is used as a prefix so maybe you want to reuse the same containers but you have moved the docker compost file to another location so let's actually remove those containers that we just started like this so now we only have those two and the way we can override is use using a flag on Docker compose so we can add an additional flag here minus P or also project name so essentially the name of the folder is assumed to be the project name so we can overwrite that project name value using this flag and we can call this projects which was the previous one like this and let's start the container let's do drps as you see our old instances of mongodb and Express were restarted instead of new ones being created plus the network called projects default that was already there and that means if I refresh this we still have our MB and my collection and the data inside for our application which means if I visit the application on Local Host 3000 which should see our awesome application and Let me refresh this once again so we can see the network traffic here we refresh so this was the fetch data request which we execute right here that basically returns this object that we created here from in the database back to the front end so if we go to preview or our response we see this object with my ID one this is the ID from the database and the data some Dynamic data loaded from DB and we're using that to set this line right here so if I actually went there and changed this like this and let save I'm going to refresh again you see that now we get this updated data from the database so the entire connection Works our application is connected to the database and displays that information right here now I mentioned that dock compose is part of the application code which means it gets committed and stored in a git repository so that everyone can work on it it's available for the entire team if a new engineer joins the team and they download the code they have docu compos so they know know exactly what services are running as part of that application and they can easily start that locally however that also means that it's really bad that we are hardcoding our secret credentials in the docker compost file because the best practice for security is that you shouldn't hardcode any sensitive data in the application code so it doesn't end up in the git repository and even if you remove it later if you accidentally checked it in and removed it it's still going to in the commit history so you shouldn't have any hardcoded values here so how do we solve this because we need those credentials to be passed on to Services well for that we can actually use variables or placeholders in Docker compost instead of the actual values and we can set the values of those variables as environment variables on the operating system so let's see how it works first of all we're going to remove all those hardcoded values and instead we're going to define the variables in Docker compos which has a syntax of dollar sign and then curly braces and inside that we can name the variable whatever we want I'm going to call this admin user because it's the admin user in mongod to be and by the way this could be lowercase you can call this really what you want but I'm using a standard environment variable name convention here with all upper cases so we have the admin user let's call this admin pass for password and we're going to reuse use those everywhere which is another advantage of using variables because if you change those values like if you change the password value for example you just have to change it or set it once and it automatically gets updated everywhere so now this do compost does not have any hardcoded sensitive data and it's safe to check it in the G repository however we still need to set those actual values so to test that let's go back to the terminal first of all I'm going to stop those stop the containers so we can test that everything works and on the first Docker compos command execution we get a warning that says the variables are not set the containers were stopped however we need to set those as variables in our terminal session so we set them here export admin user let's set the other one like this and the same way as we did with up command we actually need to specify which containers we're stopping so by default it's going to look for containers that start with this name the name of the folder so we need to overwrite that projects tag again and there you go and I'm actually going to bookmark this one as well like this and if we refresh we should see that the pages are not working because the containers are stopped and then let's start them again and if we start them again with those environment variables set everything should work same as before let's wait there you go now I want to mention here that Docker compos actually has a concept of Secrets which is another functionality to manage the secrets especially when you're running do compos in a production environment which is exactly for this use case where you need to pass in credentials or any sensitive data to the services defined in Docker compose so you can use Docker compose Secrets as an alternative to this method basically awesome we have just learned the fundamentals of docker compose and more importantly you understand its core use case and by the way I want to highlight the importance of learning tools like Docker and Docker compos or generally cloud and devops Technologies because nowadays it is becoming more and more needed for software developers to learn those tools to become more valuable in their roles especially in the current tense job market where we have layoffs and companies hiring less as more and more companies are adopting devops it is a great way to Stand Out Among developers who only focus on programming and are not interested to learn new Concepts and tools that are being adopted in the industry so I think it's definitely more important than ever to educate yourself keep learning and with devops or Cloud engineering skills you will definitely be ahead of 90% of developers in fact most of our devop boot camp students are actually software developers or software Engineers since many companies do not have a separate devops engineer role but often their responsibility lies on senior developers to set up the devops processes like release pipelines for example so even if you are a junior software engineer learning devops and Cloud skills and technologies will absolutely accelerate your career to a senior engineer so if you want to get a complete education on devops to take over devops tasks at your work then definitely check out our devops boot camp you will learn various Technologies from zero to an advanced level to be able to build real world Davos processes at your job and if you need some guidance before you can also contact us there with your questions so check out the information below and let's move on to the next part so now we are building and running our JavaScript application as a container along with mongodb service and the mongodb UI but usually that's a testing environment as you learn in the docker crash course eventually you want the JavaScript application your custom application container to be stored centrally in a Docker registry or rather the image to be stored in Docker registry so we can start and deploy it as a container on the end environment on a actual deployment server where end users will access it so we need to build the image and push that to the repository like a dockerhub repository or whatever other Docker repository want and now the interesting question is after we push the image to private Docker repository how do we reference our Custom Image from our private Docker repository in Docker compose and also note that when we run this on an actual deployment server we're going to copy the docker compose on that server where we have Docker and Docker compose installed and when we run this Docker compost file or execute this it will go through all the services and it will pull all the images defined here and run them as containers with all this configuration so we pull the official images from docker H public repository and any custom images from the private repositories so let's actually see how it works it's actually very very easy and again building image pushing it to the repository the whole thing you should already know it from the docker course so this should be a refresher for you so let's actually see that in action right here so first of all I'm going to log into my dockerhub account and I'm actually going to create a new private repository in dockerhub like this let's call this my app because it's generic I may use it for some other demonstration later so there you go and now we're going to build our image using the docker file and we're going to push that image to this specific private repository so let's execute those commands I'm going to build using Docker build command I'm going to tag this and this is again a refresher from Docker course we need to take the image with the name of the repository so that Docker knows on push command which repository to push that image to so that's going to be the entire name so the image name itself includes the repository name and we're just going to tag it with a simple 1.0 and we need to provide the build context which is the current directory where Docker file is located and that's basically it let's execute again super first let's list all the images so we can see what we have built locally and there you go this is our image with an image tag and this is exactly the image we want to push now to the private reposer and you know when we want to push or pull from a private repository we need to be logged in to that repository so we need to do Docker login and the username is actually your dockerhub username name and dockerhub user password and this is actually different for other Docker repositories so if you have a ECR or some other Docker repository the process may be different with dockerhub it's very simple that's why I use it for the demos mostly so we are now logged in to dockerhub and what I can do now is basically push that image that we just created using simple Docker push and the image full name with the tag and there you go and if I refresh we should see one tag here 1.0 for our my app image repository perfect and one thing I wanted to show you here is that whenever you are building and pushing an image you basically have the same commands all the time for the specific action you build the image you log in if you're not logged in already you push the image and so on and I have actually used a feature called workflows in workp for these kind of use cases which can be really helpful if you want to if you have a set of commands that you always need for the same type of workflow you can basically Define that as one unit so you can group those commands in one unit called the workflow and you can save it here and whenever you need that you can just execute all those commands with one click which I personally found super cool so in warp drive you have some options here you create a new workflow and I'm going to call this Docker push and here you can list the different commands basically so let's do Docker login and we have username which was this one right here and obviously we don't want to provide a password hardcoded here so we're going to pass that as very variable or argument and we're going to read that from the standard input again this is a refresher from Docker this is how should use Docker login so you don't type in the password directly and with worp you can actually use arguments like this so whenever you run this command or list of commands before it runs it will actually tell you how you should fill out this argument so we're going to use an AR arent here and then after Docker login we're going to do Docker build like we did with an image tag we can also make this an argument let's make this an argument number two and then push that like this you can also set default values let's actually do 1.0 here and this way you don't have to type out all the commands for the same workflow so let me check all these commands so we need a build context at the end and the rest looks pretty good and let's save the workflow and the way it works is now I have the workflow right here and whenever I need that workflow to execute I just click on it and it fills out the terminal basically with all these commands and it highlights the the arguments that I need to set so I'm actually going to put my password here as an argument and then let's say we want 1.1 as a second argument and we can execute all the commands like this and this will actually have pushed another tag with 1.1 so this is a cool feature that you can use on warp to make your life a little bit more convenient and that means now we have our image with two different Texs in a private Repository and now if we go back to the dock compost we don't need to build it locally we can again this is convenient for testing because when you're testing on a local environment as a software developer as an engineer you may want to just do very quick local changes in a Docker file or in application test it quickly and you don't want to be building and pushing and pulling image all the time so this is a very good functionality for local testing however on an end environment obviously we need to Define an image that comes from a repository maybe we have scanned that image already and made sure that it's secure and properly configured and so on and now we want to actually use it and how do we reference our Custom Image from a private repository in Docker compose very simple basically do it just like any other image in dockerhub or any other repository like this with a specific image tag that is available let's do 1.0 and how will Docker compos be able to pull that image from a private repository or basically authenticate with the private repository to pull the image it actually uses the same Docker login that we use to do Docker push so Docker login basically creates after successful login authentication with the docker Hub it actually creates a Docker Json file locally that creates the authentication credentials or tokens in that file and Docker compos in the background is using Docker to run those containers so it's going to be the exact same process pulling or pushing the images from Docker compose so that means if you have done Docker login already to that repository then you should be able to pull any images defined in Docker compose from that repository that means that's the configuration this should work now in order to test that let's actually stop our containers so they're all stopped and I'm actually going to remove the application container because we want to simulate that the container is recreated from the new image that we pull and now if we do up again in DET mode and as you see my app image was pulled and the container my app was started from that if we check the running containers we see that this is the image that was used to create this container awesome and again we can check that our application still works and there you go that's how we can reference our Custom Image from a Docker repository in the docker compose and in case when you're executing those commands so let's say we're doing Docker build and you forget one of the arguments or use a wrong flag and so on first of all you get an error but you can also do a troubleshooting feature within the terminal to actually give you a pretty good tips and notes on what the error actually is because sometimes we make spelling mistakes sometimes we forget an argument or whatever so it could be helpful for a tool to actually tell you what the actual problem is so you can fix it and warp has this AI assistant which is pretty cool so for this specific error if I open this warp AI which you can see on every command block so you have this bookmark and you have this warp AI so if I click on this it actually autog generates the question question because it knows that this is an error and you can ask it how to fix it so you can modify your question or example and if I hit enter here it gives me an answer that the docker build command requires an argument which should be the path to the docker file and gives me an example with the correct one so change the directory that has Docker file and then execute this command which has dot at the end so I found this feature also pretty cool which means if any of the commands give you an error while you're following this demo you can actually use this to find out what the error is about and and ideally how to fix it so this is going to help you troubleshoot your issues and finally I want to actually add a few very interesting and important Concepts regarding docu compose and kind of what the next steps are so this final small section may be really really interesting for you so basically as you see the main use case of Docker composed was to have a central place to manage containers that were supposed to run together like your application and all the services that it depends on and we configure all the environment variables or any other configuration for those services in that one file and also start them in one isolated Docker Network and it makes it super easy for us to clean up all the resources so Engineers took Docker and they containerized their applications to a whole new scale that was not a standard before and Docker was especially perfect to use as a host for microservice applications where you have even more applications and more containers now running in one environment and again if you don't know about microservices I have a separate video about them but essentially it's when you have all the services needed to run one application but split into separate micro applications or services and they can be scaled independently and run independently as independent containers so Docker was a perfect host for that so we ended up with lots of applications lots of microservices applications with hundreds or thousands or tens of thousands of containers that is pretty much a standard nowadays such a scale actually led to Docker compost actually not being able to handle such large scale of containers and more importantly Engineers will have to still manually manage running and operating those containers with do compose like if containers die or crash or have connectivity issues ETC you have to manually detect and then debug and restart the services now Docker compose actually made some improvements on that there tags like restart and so on but it's still a lot of operational effort to run the containers with this kind of scale where you have thousands of them using Docker compose and that's where kubernetes kind of came into the picture to solve exactly these two main issues initially scaling to thousands or tens of thousands of containers with kubernetes you can basically merge hundreds of servers into one huge server to deploy all the containers that belong to the same application in that environment they will all run as if they were running on the same server so it naturally makes it easier to scale your applications and to run thousands of instances and the second one was the automatic operations or making the operations of applications easier or also called kubernetes Auto healing feature which basically manages starting and restarting containers if they crash and has mechanisms to manage operations of a large number of containers in an automated way when manual effort isn't it's just impossible or not feasible anymore and that led to C is becoming so popular so Docker compose is kind of like a intermediary step if you have smaller set of containers but with today's standards when you want to work with very complex applications with a large scale then Docker compose has its limits so that's where kubernetes basically comes into the picture so if you're learning this containerized containerization and container orchestration Concepts then I would actually recommend to use this road map of learning the docker using the docker crash course then learning the docker compose with this course like you did and then you can move on to the kubernetes and if you want to learn kubernetes I also very conveniently have a kubernetes crash course to get you started in kubernetes very easily so if you want to get started with that you can check out any of the many videos that I have on my YouTube channel but I would recommend to start with the kubernetes crash course so I hope you learned a lot of new Concepts you obviously learned Docker compose as a new tool new technology thank you for watching till the end let me know in the comments how this video actually helped you in your work or maybe in your job application I'm always happy to hear and read that feedback from our viewers to know that my videos are helpful in actual job environment you can also share any other tips and learnings about dock compose that you have from your practical experience so that other viewers can read and benefit from it as well and with that thank you for watching and see you in the next video
