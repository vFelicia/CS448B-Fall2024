With timestamps:

00:00 - so kubernetes is an orchestration tool
00:03 - which means that if you have a complex
00:05 - application made up of multiple
00:08 - containers kubernetes tool will help you
00:11 - achieve high availability scalability
00:14 - and disaster recovery of your
00:17 - application setup but what does all
00:20 - these things actually mean and how does
00:22 - kubernetes help you achieve all of these
00:25 - things in order to answer these
00:26 - questions we're gonna go through a
00:28 - simplified visualization how kubernetes
00:31 - cluster really works so first let's talk
00:39 - about high availability and scalability
00:42 - here let's say we have two worker nodes
00:44 - of kubernetes cluster server one and
00:47 - server two with each server holding a
00:49 - replica of my application and a database
00:52 - application and we also have an ingress
00:55 - component which basically handles every
00:57 - incoming request to your application so
01:00 - if someone accessed my app website on a
01:03 - browser the request would come in to
01:05 - ingress an ingress component will
01:07 - actually be their own parts on each
01:11 - server so they are replicated as well so
01:14 - now when a user visits my app website on
01:18 - browser a request is made and handled by
01:21 - ingress first and as we just saw ingress
01:24 - is load-balanced so we have replicas of
01:27 - ingress on multiple servers ingress will
01:31 - then forward that request to a service
01:34 - for my application and service is a load
01:38 - balancer as well that will then direct
01:40 - that request to the respective replicas
01:44 - of the pot if let's say for that request
01:46 - a database access was necessary my app
01:50 - will then make another request to
01:53 - database service which is also load
01:55 - balanced and pass a request to one of
01:58 - the replicas and of course this is a
02:00 - simplistic view you can have 10 servers
02:03 - and 10 replicas of your database my
02:06 - April ingress applications but what this
02:09 - setup demonstrates is that from
02:12 - entry point of the request into the
02:15 - cluster til the last end point which is
02:18 - a database every component is replicated
02:21 - and load-balanced which means that in
02:24 - this whole setup there is no bottleneck
02:26 - where a request handling for example
02:29 - could stop the whole application and
02:31 - make the responses slower for a user but
02:34 - of course it must be noted here that
02:36 - your application or the way your
02:39 - application is designed should also
02:42 - support this replication and request
02:45 - handling because this is just tools that
02:47 - kubernetes offers you to make your
02:49 - properly designed application highly
02:53 - available in highly scalable so with
02:55 - this setup
02:55 - if server 2 completely crashed and all
02:59 - the parts that are running on it died
03:01 - you would still have replicas of your
03:03 - application running so there will be no
03:06 - downtime and in the meantime a
03:08 - kubernetes master process called
03:10 - controller manager would actually
03:12 - schedule new replicas of the died pods
03:15 - on another worker node let's say server
03:18 - 3 and recover the previous load-balanced
03:22 - and replicated application state which
03:24 - means while the node servers actually do
03:28 - the work of running the applications the
03:31 - master processes on the master nodes
03:33 - actually monitor the cluster state and
03:36 - make sure that if a pod dies it
03:39 - automatically gets restarted or if
03:41 - something crashes in the cluster that it
03:43 - automatically gets recovered an
03:45 - important master component that is used
03:48 - to manage the cluster mechanism to run
03:51 - properly is the Etsy D store which
03:54 - stores the cluster state like the
03:55 - resources available on the notes and the
03:58 - pod state etc at any given time so for
04:01 - example if a pod diet and it CD so would
04:06 - be updated about it and that's how the
04:08 - controller management would know that it
04:10 - should intervene and make sure new pot
04:12 - gets restarted so when that happens
04:14 - again it CD store gets updated
04:19 - and because etsy D always has the
04:22 - current state of the cluster it's also a
04:25 - crucial component in disaster recovery
04:27 - of kubernetes clustered applications and
04:30 - the way disaster recovery mechanism can
04:33 - be implemented how kubernetes is to
04:36 - create debt CD backups and store them
04:39 - into remote storage and these backups
04:42 - are in form of 8 CD snapshots now
04:45 - kubernetes doesn't manage or take care
04:48 - of baking updated CD snapshots on remote
04:52 - storage this is responsibility of the
04:55 - kubernetes cluster administrator so this
04:58 - storage could be completely outside the
05:00 - cluster on a different server or maybe
05:04 - even cloud storage an important note
05:06 - here is that HCD
05:08 - doesn't store database or application
05:11 - data that data is usually also stored on
05:14 - remote storage where the application
05:17 - thoughts actually have reference to the
05:20 - storage so that they can read and write
05:23 - the data from and this remote storage
05:25 - just like the HDD snapshots backup
05:28 - location isn't managed by kubernetes and
05:31 - again must be reliably backed up and
05:34 - stored outside of the cluster this is
05:37 - usually how production clusters are set
05:40 - up so now considering a reliable backup
05:43 - and replication of HDD snapshot and
05:46 - application data is in place if the
05:49 - whole cluster were to crash including
05:51 - the worker nodes and the master nodes
05:54 - themselves it would be possible to
05:56 - recover the cluster state on completely
05:59 - new machines with new worker nodes and
06:02 - also new master node using the HDD
06:06 - snapshot and the application data and of
06:09 - course you can even avoid any downtime
06:11 - between the cluster crash and a new
06:14 - cluster creation by keeping a backup
06:18 - cluster so to say that can immediately
06:20 - take over when the active cluster or the
06:24 - current cluster crashes or dies
06:28 - there is one thing I should note here
06:30 - you can actually achieve this setup with
06:32 - load balancers and replicas also without
06:35 - kubernetes for example on AWS instances
06:40 - using the AWS load balancer etc however
06:44 - there are a couple of advantages that
06:46 - you have with kubernetes that you don't
06:49 - have with other tools or if you would
06:51 - create this setup yourself
06:53 - one is the replication is made much
06:57 - easier using kubernetes the only thing
07:00 - that you have to do is just declare how
07:03 - many replicas of a certain application
07:05 - beat your own application or a database
07:08 - application you need and the kubernetes
07:10 - component takes care of actually
07:13 - replicating it second one is the
07:15 - kubernetes self-healing feature so what
07:18 - it basically means is that if a pod dies
07:21 - there should be processed that monitors
07:23 - the state that detects that a replica
07:27 - died and automatically restarts a new
07:29 - one and again you have this feature out
07:32 - of the box from kubernetes third is
07:35 - smart scheduling feature of kubernetes
07:38 - which means that for example if you have
07:40 - 50 worker servers that your application
07:43 - containers will run on with kubernetes
07:46 - you don't have to decide where to run
07:48 - your container you just say you need a
07:50 - new replica of a pod and kubernetes
07:53 - smart scheduler basically goes and finds
07:55 - the best fitting work node among those
07:59 - 50 worker nodes maybe to schedule your
08:02 - container by comparing how much
08:05 - resources a worker node has available or
08:08 - free and overall many features that you
08:12 - could also do elsewhere on other
08:15 - platforms like as I mentioned AWS is
08:18 - made simpler or it's easier to create
08:22 - and configure in kubernetes like service
08:25 - as a load balancer for example thanks
08:27 - for watching the video I hope it was
08:29 - helpful and if it was don't forget to
08:31 - like it this is a video series so I will
08:34 - create a new one every week so if you
08:36 - want to be notified whenever and you
08:38 - comes out then subscribe to my channel
08:40 - if you have any questions if something
08:43 - wasn't clear in the video please post
08:45 - them in a comment section below and I
08:47 - will try to answer them so thank you and
08:49 - see you in the next video

Cleaned transcript:

so kubernetes is an orchestration tool which means that if you have a complex application made up of multiple containers kubernetes tool will help you achieve high availability scalability and disaster recovery of your application setup but what does all these things actually mean and how does kubernetes help you achieve all of these things in order to answer these questions we're gonna go through a simplified visualization how kubernetes cluster really works so first let's talk about high availability and scalability here let's say we have two worker nodes of kubernetes cluster server one and server two with each server holding a replica of my application and a database application and we also have an ingress component which basically handles every incoming request to your application so if someone accessed my app website on a browser the request would come in to ingress an ingress component will actually be their own parts on each server so they are replicated as well so now when a user visits my app website on browser a request is made and handled by ingress first and as we just saw ingress is loadbalanced so we have replicas of ingress on multiple servers ingress will then forward that request to a service for my application and service is a load balancer as well that will then direct that request to the respective replicas of the pot if let's say for that request a database access was necessary my app will then make another request to database service which is also load balanced and pass a request to one of the replicas and of course this is a simplistic view you can have 10 servers and 10 replicas of your database my April ingress applications but what this setup demonstrates is that from entry point of the request into the cluster til the last end point which is a database every component is replicated and loadbalanced which means that in this whole setup there is no bottleneck where a request handling for example could stop the whole application and make the responses slower for a user but of course it must be noted here that your application or the way your application is designed should also support this replication and request handling because this is just tools that kubernetes offers you to make your properly designed application highly available in highly scalable so with this setup if server 2 completely crashed and all the parts that are running on it died you would still have replicas of your application running so there will be no downtime and in the meantime a kubernetes master process called controller manager would actually schedule new replicas of the died pods on another worker node let's say server 3 and recover the previous loadbalanced and replicated application state which means while the node servers actually do the work of running the applications the master processes on the master nodes actually monitor the cluster state and make sure that if a pod dies it automatically gets restarted or if something crashes in the cluster that it automatically gets recovered an important master component that is used to manage the cluster mechanism to run properly is the Etsy D store which stores the cluster state like the resources available on the notes and the pod state etc at any given time so for example if a pod diet and it CD so would be updated about it and that's how the controller management would know that it should intervene and make sure new pot gets restarted so when that happens again it CD store gets updated and because etsy D always has the current state of the cluster it's also a crucial component in disaster recovery of kubernetes clustered applications and the way disaster recovery mechanism can be implemented how kubernetes is to create debt CD backups and store them into remote storage and these backups are in form of 8 CD snapshots now kubernetes doesn't manage or take care of baking updated CD snapshots on remote storage this is responsibility of the kubernetes cluster administrator so this storage could be completely outside the cluster on a different server or maybe even cloud storage an important note here is that HCD doesn't store database or application data that data is usually also stored on remote storage where the application thoughts actually have reference to the storage so that they can read and write the data from and this remote storage just like the HDD snapshots backup location isn't managed by kubernetes and again must be reliably backed up and stored outside of the cluster this is usually how production clusters are set up so now considering a reliable backup and replication of HDD snapshot and application data is in place if the whole cluster were to crash including the worker nodes and the master nodes themselves it would be possible to recover the cluster state on completely new machines with new worker nodes and also new master node using the HDD snapshot and the application data and of course you can even avoid any downtime between the cluster crash and a new cluster creation by keeping a backup cluster so to say that can immediately take over when the active cluster or the current cluster crashes or dies there is one thing I should note here you can actually achieve this setup with load balancers and replicas also without kubernetes for example on AWS instances using the AWS load balancer etc however there are a couple of advantages that you have with kubernetes that you don't have with other tools or if you would create this setup yourself one is the replication is made much easier using kubernetes the only thing that you have to do is just declare how many replicas of a certain application beat your own application or a database application you need and the kubernetes component takes care of actually replicating it second one is the kubernetes selfhealing feature so what it basically means is that if a pod dies there should be processed that monitors the state that detects that a replica died and automatically restarts a new one and again you have this feature out of the box from kubernetes third is smart scheduling feature of kubernetes which means that for example if you have 50 worker servers that your application containers will run on with kubernetes you don't have to decide where to run your container you just say you need a new replica of a pod and kubernetes smart scheduler basically goes and finds the best fitting work node among those 50 worker nodes maybe to schedule your container by comparing how much resources a worker node has available or free and overall many features that you could also do elsewhere on other platforms like as I mentioned AWS is made simpler or it's easier to create and configure in kubernetes like service as a load balancer for example thanks for watching the video I hope it was helpful and if it was don't forget to like it this is a video series so I will create a new one every week so if you want to be notified whenever and you comes out then subscribe to my channel if you have any questions if something wasn't clear in the video please post them in a comment section below and I will try to answer them so thank you and see you in the next video
