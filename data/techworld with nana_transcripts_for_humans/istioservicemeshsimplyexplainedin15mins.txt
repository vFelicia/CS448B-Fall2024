With timestamps:

00:00 - in this video you will learn about
00:02 - service mesh and one of its
00:03 - implementations
00:04 - which is istio in order to understand
00:07 - the concepts
00:08 - we will first look at the challenges of
00:10 - a microservices application
00:12 - and then we will see how different
00:14 - features of a service mesh
00:17 - solve these challenges we will look at
00:20 - how istio implements service mesh
00:22 - and learn about istio architecture as
00:25 - well as how to configure
00:26 - istio for our microservices application
00:29 - istio is
00:30 - a service mesh so in order to understand
00:33 - istio we need to understand what service
00:35 - mesh is
00:36 - service mesh is a popular solution for
00:38 - managing communication between
00:40 - individual microservices in a micro
00:43 - service application
00:44 - so why do we need a dedicated tool for
00:47 - microservices communication
00:49 - and what are the challenges here
00:55 - now when we move from monolith to
00:57 - microservices application
00:59 - we introduced a couple of new challenges
01:01 - that we didn't have
01:03 - with a monolith application and let's
01:05 - say we have
01:06 - an online shop application which is made
01:08 - up of several micro services
01:10 - we have the web server that gets the ui
01:12 - requests
01:14 - payment microservice that handles the
01:15 - payment logic
01:17 - let's say we have a shopping cart
01:19 - product inventory
01:20 - and database and probably some more
01:22 - services
01:23 - and we're deploying our micro service
01:25 - application inside a kubernetes cluster
01:29 - now what does our micro service
01:30 - application setup need
01:32 - to run successfully or what are some of
01:34 - the required
01:35 - configurations for such an application
01:39 - first of all each micro service has its
01:41 - own business logic right
01:43 - payment service handles the payment
01:45 - logic web server handles ui requests
01:47 - database persist data and so on now
01:50 - services need to talk to each other
01:52 - when user puts stuff in the shopping
01:54 - cart request is received by the web
01:56 - server
01:56 - which hands it over to the shopping cart
01:58 - microservice
02:00 - which will talk to the database to
02:01 - persist the data so how do services
02:04 - know how to talk to each other what is
02:06 - the endpoint of each
02:08 - service all the service endpoints that
02:10 - web server talks to
02:12 - must be configured for web server so
02:14 - when we add a new micro service
02:16 - we need to add the endpoint of that new
02:20 - service to all the microservices
02:22 - that need to talk to it so we have that
02:24 - information as part of the application
02:27 - deployment
02:27 - code now what about security in our
02:30 - microservice application setup
02:32 - generally a common environment in many
02:35 - projects will look like this
02:37 - you have firewall rules set up for your
02:39 - kubernetes cluster
02:40 - maybe you have a proxy as entry point
02:43 - that gets the request first so cluster
02:45 - can't be
02:45 - accessed directly so you have security
02:48 - around the cluster
02:49 - however once request gets inside the
02:51 - cluster the communication is insecure
02:54 - microservices talk to each other over
02:56 - http or some other insecure protocol
02:59 - also services talk to each other freely
03:02 - every service inside the cluster can
03:04 - talk to any other service so there are
03:06 - no restrictions on that
03:07 - so this means that from security
03:10 - perspective
03:10 - if an attacker gets inside the cluster
03:13 - it can do anything because we don't have
03:15 - any additional security inside
03:17 - and maybe for small applications they
03:19 - don't really have any sensitive user
03:21 - data
03:21 - it may be okay but for more important
03:24 - applications like
03:25 - online banks or apps with lots of
03:27 - personal user data
03:29 - a higher level of security is very
03:31 - important
03:32 - so you want everything to be as secure
03:34 - as possible
03:35 - so again additional configuration inside
03:38 - each application is needed to secure
03:42 - communication between
03:43 - services within the cluster you also
03:45 - need
03:46 - retry logic in each microservice to make
03:49 - the whole application
03:51 - more robust if one microservice is
03:54 - unreachable or you lose connection for a
03:56 - bit you want to retry the connection
03:58 - so developers will add this retry logic
04:01 - also to the services what about metrics
04:04 - for your services you want to be able to
04:07 - monitor how the services are performing
04:09 - what http errors are you getting how
04:11 - many requests are your microservices
04:13 - receiving
04:14 - or sending or how long does a request
04:17 - take
04:18 - to identify the bottlenecks in your
04:20 - application
04:21 - so development team may add a monitoring
04:24 - logic
04:24 - for prometheus for example using
04:26 - prometheus client library
04:28 - and collect tracing data using a tracing
04:32 - library like zipkin for example
04:34 - so as you see teams of developers of
04:36 - each microservice
04:38 - need to add all this logic to each
04:41 - service
04:41 - and maybe configure some additional
04:44 - stuff in the cluster to handle
04:46 - all these very important challenges in
04:48 - the microservices application
04:50 - and this means that developers of
04:51 - microservices are not working
04:54 - on the actual service logic but are busy
04:57 - adding network logic for metrics and
05:00 - security and communication etc
05:02 - for each microservice which also adds
05:05 - complexity to the services instead of
05:07 - keeping them simple and lightweight
05:13 - now wouldn't it make more sense to
05:15 - extract all the non-business logic
05:17 - out of the microservices and into its
05:20 - own
05:20 - small sitecar application that handles
05:24 - all these logic and acts as a proxy and
05:27 - this small application
05:28 - is a third party application the cluster
05:30 - operators
05:31 - can easily configure through a simple
05:33 - api without worrying about how the logic
05:36 - is implemented and developers can now
05:39 - focus on developing the
05:40 - actual business logic and note that you
05:43 - don't have to add this sidecar
05:45 - configuration to your
05:46 - micro service deployment yaml file
05:49 - because
05:50 - service mesh has a control plane that
05:52 - will automatically
05:53 - inject this proxy in every microservice
05:57 - pod
05:58 - so now the microservices can talk to
06:00 - each other
06:01 - through those proxies and the network
06:04 - layer
06:05 - for service to service communication
06:07 - consisting of
06:09 - control plane and the proxies is a
06:12 - service mesh
06:16 - in addition to the above features one of
06:19 - the most important features of a service
06:21 - mesh
06:22 - is traffic split configuration so what
06:25 - is a traffic split
06:26 - when changes are made to a payment
06:28 - microservice for example
06:30 - a new version is built tested and
06:33 - deployed to a production environment
06:35 - right
06:35 - now of course you can rely on tests to
06:38 - validate
06:38 - the new version but what if the new
06:41 - version has a bug that you couldn't
06:43 - catch with the tests
06:44 - happens very often depending on the test
06:46 - coverage so in this
06:48 - case you don't want to end up with a new
06:50 - version of payment service in production
06:52 - that doesn't work it may cost your
06:54 - company a lot of money
06:55 - so you want to send maybe only one
06:58 - percent or 10
06:59 - traffic to the new version for a period
07:01 - of time
07:02 - to make sure it really works so with
07:05 - service mesh
07:06 - you can easily configure a web server
07:09 - micro service
07:10 - to direct 90 of traffic to the payment
07:14 - service
07:14 - version 2.0 and 10 of traffic
07:18 - to the version 3.0 which is also known
07:22 - as
07:22 - canary deployment
07:27 - and as i mentioned at the beginning
07:29 - service mesh
07:30 - is just a pattern or a paradigm
07:33 - and istio is one of its implementations
07:36 - and in istio architecture the proxies
07:40 - are invoice proxies which is an
07:42 - independent open source project
07:44 - that istio as well as many other service
07:47 - mesh implementations also use
07:49 - and the control plane component in istio
07:51 - is
07:52 - istio d which manages and injects the
07:55 - envoy proxies
07:57 - in each of the microservice pods
08:00 - now note here that in earlier versions
08:03 - of
08:03 - istio up to version 1.5
08:07 - istio control plane was a bundle of
08:09 - multiple
08:10 - components you had the citadel
08:14 - mixer galley and some other components
08:17 - so you had multiple pods when you
08:19 - deployed istio
08:20 - however in version 1.5 all of these
08:24 - separate components were combined back
08:26 - into
08:27 - one single stod component to make it
08:31 - easier
08:32 - for the operators to configure and
08:35 - operate istio so if you have read
08:38 - articles or watch videos where all these
08:40 - components are explained
08:41 - separately note that this is only
08:43 - relevant for the earlier versions now
08:45 - you only worry about
08:47 - one single istio d component
08:50 - so istio architecture is comprised of
08:53 - the control
08:54 - plane which has esteod component and
08:57 - control plane manages a data plane
09:00 - which is group of all the invoice
09:06 - proxies
09:08 - so now the question is how do we
09:10 - configure all these above features
09:13 - for our micro services in istio as i
09:16 - mentioned
09:16 - you don't have to adjust deployment and
09:19 - service dml files for your micro
09:21 - services
09:22 - so all the configuration for easter
09:24 - components will be done in istio itself
09:27 - again having a clear separation between
09:29 - the application logic
09:31 - and configuration and the service mesh
09:33 - logic and configuration
09:35 - and the great thing is that istio can be
09:37 - configured
09:38 - with kubernetes yaml files because it
09:40 - uses
09:41 - crds by extending kubernetes api
09:45 - crd is basically a custom resource or
09:48 - custom component in kubernetes that can
09:51 - be used
09:52 - to allow configuring these third-party
09:55 - technologies like istio
09:57 - prometheus etc using the same kubernetes
10:00 - yaml files
10:01 - and apply them using cube ctl without
10:05 - having to learn a technology specific
10:07 - configuration language
10:09 - and adjusting that configuration
10:10 - directly inside istio for example
10:13 - so using a few sdo crds
10:16 - we can configure different traffic
10:18 - routing rules
10:19 - between our microservices like which
10:22 - services can talk to each other
10:24 - traffic split configuration the retry
10:27 - rules
10:28 - timeouts and many other network
10:31 - configurations
10:32 - and there are two main crds for
10:35 - configuring
10:36 - service to service communication virtual
10:39 - service
10:39 - which configures how to route the
10:42 - traffic to a specific service
10:44 - and once that traffic is actually routed
10:46 - to that service
10:47 - on top of that using destination rule
10:50 - component
10:51 - we can configure some policies on that
10:54 - traffic light what kind of load
10:55 - balancing to use
10:56 - to talk to the pods behind the
10:58 - destination service
11:00 - so overall as you see we create
11:03 - these crds custom resource definitions
11:06 - in kubernetes
11:07 - that istiod component which is is the
11:10 - control plane
11:11 - will read and convert into
11:15 - invoice specific configuration and send
11:18 - that configuration
11:19 - out to all the invoice proxies so we
11:22 - don't configure
11:23 - proxies we configure control plane and
11:25 - control plane itself
11:27 - will then push that configuration out to
11:30 - all individual
11:31 - invoice proxies and the proxies
11:33 - themselves
11:34 - can now communicate with each other by
11:37 - applying this configuration
11:38 - that we define without having
11:42 - to go back to the easter control plane
11:45 - so they can independently talk to each
11:48 - other
11:49 - because they have all the logic and
11:51 - configuration they need
11:52 - without talking to the control plane
11:58 - in addition to configuring the proxies
12:01 - stod also has a central registry for all
12:04 - the
12:05 - microservices so instead of statically
12:08 - configuring the endpoints
12:09 - for each microservice when a new
12:12 - microservice
12:13 - gets deployed it will automatically get
12:16 - registered
12:16 - in the service registry without the need
12:19 - of any additional configuration from our
12:21 - site because
12:22 - istio automatically detects the services
12:24 - and endpoints in the cluster
12:26 - and using this service registry the
12:29 - envoy proxies can now
12:31 - query the endpoints to send the traffic
12:33 - to the relevant services
12:35 - in addition to this dynamic service
12:38 - discovery feature
12:40 - istiod also acts as a ca as a
12:43 - certificate authority
12:44 - and generates certificates for all the
12:46 - microservices
12:48 - in the cluster to allow secured tls
12:50 - communication between
12:52 - proxies of those microservices and
12:55 - finally sdod gets metrics and tracing
12:59 - data
13:00 - from the invoice proxies that it
13:04 - gathers they can be later consumed by
13:07 - monitoring server like
13:08 - prometheus or tracing servers etc
13:11 - to have out-of-the-box metrics and
13:14 - tracing
13:15 - data for your whole microservice
13:18 - application
13:22 - istio has another component called istio
13:25 - ingress gateway
13:26 - that basically is an entry point into
13:30 - your kubernetes cluster
13:32 - you can think of the istio ingress
13:34 - gateway as an alternative
13:36 - to nginx ingress controller so istio
13:39 - gateway
13:40 - runs as a pod in your cluster and
13:43 - acts as a load balancer by accepting
13:46 - incoming traffic in your cluster
13:50 - and gateway will then direct traffic to
13:52 - one of your microservices inside the
13:55 - cluster
13:56 - using virtual service component
13:59 - and you can configure istio gateway
14:01 - using
14:02 - a gateway crd
14:08 - so now the traffic flow in your
14:10 - kubernetes cluster
14:12 - with all these istio components will
14:14 - look like this
14:16 - so user will initiate a request to
14:19 - a web server microservice in your
14:22 - kubernetes cluster
14:23 - the request will first hit the gateway
14:25 - because it's that entry point
14:27 - of the cluster gateway will then
14:30 - evaluate
14:31 - the virtual service rules about how to
14:34 - route the traffic
14:36 - and will send it to web server
14:39 - microservice and finally that request
14:42 - will reach
14:42 - the proxy the invoice proxy inside your
14:45 - web server
14:47 - micro service the invoice proxy will
14:49 - evaluate the request
14:51 - and forward it to the actual
14:54 - web server container within the same pod
14:58 - using localhost now the web server will
15:01 - initiate another request to a payment
15:04 - microservice for example
15:06 - so the request will move from
15:09 - web server container to the web server
15:11 - proxy
15:12 - which will then by applying the virtual
15:14 - service rules as well as
15:16 - destination rules and maybe some other
15:18 - configuration
15:20 - will communicate with the proxy
15:23 - envoy proxy of payment microservice
15:26 - using mutual tls and the same will
15:29 - repeat for communication between the
15:32 - payment service and
15:33 - database and all the way back the
15:36 - response will be
15:37 - returned to the ui and during this
15:39 - overall request flow
15:41 - the proxies will gather all the metrics
15:43 - and tracing information
15:45 - about the requests and send it back to
15:48 - the control plane
15:49 - so we automatically have monitoring for
15:52 - our application
15:54 - so that's it for this video subscribe to
15:56 - my channel for
15:58 - more content like this and if you want
16:00 - to see
16:01 - behind the scenes content and previews
16:04 - follow me on instagram as well
16:05 - thank you and see you in the next video

Cleaned transcript:

in this video you will learn about service mesh and one of its implementations which is istio in order to understand the concepts we will first look at the challenges of a microservices application and then we will see how different features of a service mesh solve these challenges we will look at how istio implements service mesh and learn about istio architecture as well as how to configure istio for our microservices application istio is a service mesh so in order to understand istio we need to understand what service mesh is service mesh is a popular solution for managing communication between individual microservices in a micro service application so why do we need a dedicated tool for microservices communication and what are the challenges here now when we move from monolith to microservices application we introduced a couple of new challenges that we didn't have with a monolith application and let's say we have an online shop application which is made up of several micro services we have the web server that gets the ui requests payment microservice that handles the payment logic let's say we have a shopping cart product inventory and database and probably some more services and we're deploying our micro service application inside a kubernetes cluster now what does our micro service application setup need to run successfully or what are some of the required configurations for such an application first of all each micro service has its own business logic right payment service handles the payment logic web server handles ui requests database persist data and so on now services need to talk to each other when user puts stuff in the shopping cart request is received by the web server which hands it over to the shopping cart microservice which will talk to the database to persist the data so how do services know how to talk to each other what is the endpoint of each service all the service endpoints that web server talks to must be configured for web server so when we add a new micro service we need to add the endpoint of that new service to all the microservices that need to talk to it so we have that information as part of the application deployment code now what about security in our microservice application setup generally a common environment in many projects will look like this you have firewall rules set up for your kubernetes cluster maybe you have a proxy as entry point that gets the request first so cluster can't be accessed directly so you have security around the cluster however once request gets inside the cluster the communication is insecure microservices talk to each other over http or some other insecure protocol also services talk to each other freely every service inside the cluster can talk to any other service so there are no restrictions on that so this means that from security perspective if an attacker gets inside the cluster it can do anything because we don't have any additional security inside and maybe for small applications they don't really have any sensitive user data it may be okay but for more important applications like online banks or apps with lots of personal user data a higher level of security is very important so you want everything to be as secure as possible so again additional configuration inside each application is needed to secure communication between services within the cluster you also need retry logic in each microservice to make the whole application more robust if one microservice is unreachable or you lose connection for a bit you want to retry the connection so developers will add this retry logic also to the services what about metrics for your services you want to be able to monitor how the services are performing what http errors are you getting how many requests are your microservices receiving or sending or how long does a request take to identify the bottlenecks in your application so development team may add a monitoring logic for prometheus for example using prometheus client library and collect tracing data using a tracing library like zipkin for example so as you see teams of developers of each microservice need to add all this logic to each service and maybe configure some additional stuff in the cluster to handle all these very important challenges in the microservices application and this means that developers of microservices are not working on the actual service logic but are busy adding network logic for metrics and security and communication etc for each microservice which also adds complexity to the services instead of keeping them simple and lightweight now wouldn't it make more sense to extract all the nonbusiness logic out of the microservices and into its own small sitecar application that handles all these logic and acts as a proxy and this small application is a third party application the cluster operators can easily configure through a simple api without worrying about how the logic is implemented and developers can now focus on developing the actual business logic and note that you don't have to add this sidecar configuration to your micro service deployment yaml file because service mesh has a control plane that will automatically inject this proxy in every microservice pod so now the microservices can talk to each other through those proxies and the network layer for service to service communication consisting of control plane and the proxies is a service mesh in addition to the above features one of the most important features of a service mesh is traffic split configuration so what is a traffic split when changes are made to a payment microservice for example a new version is built tested and deployed to a production environment right now of course you can rely on tests to validate the new version but what if the new version has a bug that you couldn't catch with the tests happens very often depending on the test coverage so in this case you don't want to end up with a new version of payment service in production that doesn't work it may cost your company a lot of money so you want to send maybe only one percent or 10 traffic to the new version for a period of time to make sure it really works so with service mesh you can easily configure a web server micro service to direct 90 of traffic to the payment service version 2.0 and 10 of traffic to the version 3.0 which is also known as canary deployment and as i mentioned at the beginning service mesh is just a pattern or a paradigm and istio is one of its implementations and in istio architecture the proxies are invoice proxies which is an independent open source project that istio as well as many other service mesh implementations also use and the control plane component in istio is istio d which manages and injects the envoy proxies in each of the microservice pods now note here that in earlier versions of istio up to version 1.5 istio control plane was a bundle of multiple components you had the citadel mixer galley and some other components so you had multiple pods when you deployed istio however in version 1.5 all of these separate components were combined back into one single stod component to make it easier for the operators to configure and operate istio so if you have read articles or watch videos where all these components are explained separately note that this is only relevant for the earlier versions now you only worry about one single istio d component so istio architecture is comprised of the control plane which has esteod component and control plane manages a data plane which is group of all the invoice proxies so now the question is how do we configure all these above features for our micro services in istio as i mentioned you don't have to adjust deployment and service dml files for your micro services so all the configuration for easter components will be done in istio itself again having a clear separation between the application logic and configuration and the service mesh logic and configuration and the great thing is that istio can be configured with kubernetes yaml files because it uses crds by extending kubernetes api crd is basically a custom resource or custom component in kubernetes that can be used to allow configuring these thirdparty technologies like istio prometheus etc using the same kubernetes yaml files and apply them using cube ctl without having to learn a technology specific configuration language and adjusting that configuration directly inside istio for example so using a few sdo crds we can configure different traffic routing rules between our microservices like which services can talk to each other traffic split configuration the retry rules timeouts and many other network configurations and there are two main crds for configuring service to service communication virtual service which configures how to route the traffic to a specific service and once that traffic is actually routed to that service on top of that using destination rule component we can configure some policies on that traffic light what kind of load balancing to use to talk to the pods behind the destination service so overall as you see we create these crds custom resource definitions in kubernetes that istiod component which is is the control plane will read and convert into invoice specific configuration and send that configuration out to all the invoice proxies so we don't configure proxies we configure control plane and control plane itself will then push that configuration out to all individual invoice proxies and the proxies themselves can now communicate with each other by applying this configuration that we define without having to go back to the easter control plane so they can independently talk to each other because they have all the logic and configuration they need without talking to the control plane in addition to configuring the proxies stod also has a central registry for all the microservices so instead of statically configuring the endpoints for each microservice when a new microservice gets deployed it will automatically get registered in the service registry without the need of any additional configuration from our site because istio automatically detects the services and endpoints in the cluster and using this service registry the envoy proxies can now query the endpoints to send the traffic to the relevant services in addition to this dynamic service discovery feature istiod also acts as a ca as a certificate authority and generates certificates for all the microservices in the cluster to allow secured tls communication between proxies of those microservices and finally sdod gets metrics and tracing data from the invoice proxies that it gathers they can be later consumed by monitoring server like prometheus or tracing servers etc to have outofthebox metrics and tracing data for your whole microservice application istio has another component called istio ingress gateway that basically is an entry point into your kubernetes cluster you can think of the istio ingress gateway as an alternative to nginx ingress controller so istio gateway runs as a pod in your cluster and acts as a load balancer by accepting incoming traffic in your cluster and gateway will then direct traffic to one of your microservices inside the cluster using virtual service component and you can configure istio gateway using a gateway crd so now the traffic flow in your kubernetes cluster with all these istio components will look like this so user will initiate a request to a web server microservice in your kubernetes cluster the request will first hit the gateway because it's that entry point of the cluster gateway will then evaluate the virtual service rules about how to route the traffic and will send it to web server microservice and finally that request will reach the proxy the invoice proxy inside your web server micro service the invoice proxy will evaluate the request and forward it to the actual web server container within the same pod using localhost now the web server will initiate another request to a payment microservice for example so the request will move from web server container to the web server proxy which will then by applying the virtual service rules as well as destination rules and maybe some other configuration will communicate with the proxy envoy proxy of payment microservice using mutual tls and the same will repeat for communication between the payment service and database and all the way back the response will be returned to the ui and during this overall request flow the proxies will gather all the metrics and tracing information about the requests and send it back to the control plane so we automatically have monitoring for our application so that's it for this video subscribe to my channel for more content like this and if you want to see behind the scenes content and previews follow me on instagram as well thank you and see you in the next video
