With timestamps:

00:00 - If you want
to understand
00:02 - what a service mesh is and learn
one of its implementations,
00:06 - Consul as well as understand
why this concept is so popular
00:09 - in cloud and DevOps space.
00:11 - And get your first hands
on practice with it,
00:14 - then this crash course
is exactly for you.
00:18 - You definitely want to stick
around till the end,
00:20 - because this is going
to be a value packed,
00:22 - super exciting crash course with
lots of interesting concepts.
00:26 - First, we will see why we even
need a service mesh
00:30 - technology like Consul
and what it does exactly.
00:33 - We'll then see its
practical use cases,
00:35 - including how it's used
in multi data center
00:39 - and multi-cluster environments.
00:42 - We will understand Consul
architecture so how it works
00:46 - and how it does all that.
00:47 - And finally we'll see a really
interesting demo use case
00:51 - of deploying microservices
application across two
00:55 - Kubernetes clusters on two
different cloud platforms
00:59 - and configuring the connectivity
of these
01:01 - services across different
environments using Consul.
01:06 - And in case this sounds
like complex
01:08 - topics and use cases,
01:10 - remember you are on Tech World
with nano channel,
01:13 - so you can be sure that I will
break down all
01:16 - the complex topics
into a simple and easy
01:19 - to understand examples
and explanations.
01:22 - So let's get into it.
01:28 - Let's say we have an e-commerce
01:30 - application like Amazon,
01:32 - which is a complex
microservices application.
01:35 - It has services for various
functionalities
01:38 - like product catalog to manage
product information, pricing,
01:42 - product images, etcetera.
01:43 - We have a shopping cart
service that allows
01:47 - adding products, removing them,
maybe saving for later.
01:50 - We also have order management
service to handle
01:53 - all the orders.
01:55 - We have user authentication
and authorization services,
01:58 - obviously to manage
the registration,
02:00 - user login and so on. You know,
reviews and rating service,
02:03 - recommendation service.
02:05 - And let's say it also
integrates with bunch
02:08 - of supplier APIs and allows
others to set up their
02:13 - own shops or a payment
gateway to integrate
02:16 - with external
payment processors.
02:18 - So a bunch of stuff
is going on where
02:21 - this huge application logic
is broken down
02:24 - into microservices.
02:26 - And if you don't know
what microservices are exactly
02:29 - and how they are designed,
02:31 - I actually have a separate video
on that which I will
02:34 - link in here.
02:35 - And of course these
microservices
02:36 - are interconnected.
02:38 - They need to talk to each other
like when product is added
02:41 - to shopping cart product
service needs to update
02:44 - the stock information on how
many products are left.
02:47 - Shopping cart needs
to talk to the payment
02:50 - service or user account.
02:52 - The user authentication
service will talk to other
02:55 - services that require
user authentication
02:57 - like verify user identity,
doing order placement,
03:01 - or payment. For example,
03:03 - recommendation engine
service will communicate
03:06 - with User authentication Service
to personalize
03:10 - recommendations based
on user preferences,
03:12 - as well as talk to product
catalog service to fetch product
03:17 - details for recommended items.
So as you see,
03:21 - it is a complex network
of services that all need
03:24 - to talk to each other
without issues to make
03:27 - sure that the entire application
works properly
03:30 - and the user experience
is smooth.
03:33 - And while moving from monolith
to microservices,
03:36 - architecture introduced
a lot of flexibility
03:39 - in developing and scaling
such complex applications.
03:43 - One of the main challenges
it introduced
03:46 - was the connectivity
between those services.
03:49 - In monolith application
is just one
03:51 - application and one code base,
03:53 - so it's all function
calls between different
03:56 - parts of the application.
But in microservices,
03:59 - you have multiple isolated
micro applications,
04:03 - which introduces a couple
of questions and challenges
04:06 - like how do they talk
to each other,
04:09 - on which endpoints,
04:11 - what communication
channel do they use?
04:13 - Do they send Http requests
to each other?
04:16 - Do they use message
broker synchronous
04:19 - or asynchronous communication?
04:21 - What about
the communication bottlenecks?
04:23 - When one service
is completely overwhelmed
04:26 - with requests from all
other services?
04:30 - How to deal with a situation
where one service is down
04:33 - and not responsive
to other services?
04:36 - Like what happens if shopping
cart service
04:38 - is down but other services
depend on it to do their jobs
04:41 - and it's just not responding?
04:43 - And how do we even monitor
such application?
04:46 - How do we know which services
are up and running,
04:50 - which ones are having issues,
04:52 - which services are overloaded
with requests or not responding
04:56 - or just slow in their response?
04:59 - So all of these are challenges
that microservices
05:02 - infrastructure introduced.
05:04 - Now let's say we have our
microservice application
05:07 - deployed within a Kubernetes
cluster in ECS
05:11 - in one of the US regions.
05:13 - Let's say we are a US
based company and we have
05:15 - mostly American users,
05:17 - but we grow and become super
popular in Europe and Asia.
05:21 - So now we need to deploy
our application
05:24 - in those regions as well,
05:25 - geographically closer to our
new users,
05:29 - to make their user
experience better.
05:32 - So we need the instance
of our application in other
05:36 - geographic regions to make
sure that our application
05:39 - loads fast and people
in those regions have
05:43 - good user experience. Now,
05:45 - this is another layer
of complexity because now
05:49 - the question is how do we manage
communication between
05:52 - services across
multiple regions,
05:54 - in multiple data centers?
05:56 - That introduces a whole
different level
05:59 - of challenges of operating
your microservices application,
06:03 - like networking and connectivity
between those regions,
06:07 - making sure those connections
are also secure,
06:09 - making sure the data
is in sync between
06:12 - regions and data centers.
06:14 - And you don't have data
inconsistencies. Now,
06:18 - as if that wasn't enough,
06:19 - our microservice is using two
different databases
06:23 - for different purposes,
and those are managed centrally.
06:27 - By database engineers
on separate VMs,
06:31 - because let's say those
databases are used and shared
06:35 - by the whole company.
It's a legacy system.
06:38 - Everything's interconnected.
06:40 - So it's not easy to move away
from virtual machines
06:43 - and migrate the database
to a Kubernetes cluster.
06:47 - Let's say it's a large
database with more powerful
06:51 - machines and many replicas.
06:53 - So it would be an enormous
effort to migrate all
06:57 - that to a
Kubernetes environment.
06:59 - So microservice application
has to communicate
07:02 - with database services
running on virtual machines
07:07 - in on premise data centers.
So we have a hybrid environment.
07:12 - And that is even more
challenging than managing
07:15 - connections between
services on two
07:17 - different Kubernetes clusters.
However,
07:20 - it is a very common real use
case among many
07:23 - companies and projects.
So that's a real challenge.
07:26 - Now our story continues.
07:28 - Let's say one day right before
the holidays,
07:32 - AWS has an outage in multiple
regions around the world,
07:36 - and we lose lots of business
in our e-commerce application.
07:41 - So management decided to make
sure this never happens again,
07:45 - to have a failover to a
different cloud provider,
07:49 - like a backup,
07:50 - in case the main cloud provider
has issues.
07:52 - So they replicate
the entire application
07:56 - on Google Kubernetes Engine,
07:58 - which is a Google's managed
solution of Kubernetes,
08:01 - because the chances of both
and Google Cloud going down
08:06 - at the same time is very low.
08:08 - And this is great for business
in case of any such issues.
08:12 - But a new headache
and challenges
08:15 - for the engineers. Again,
08:17 - connectivity across multiple
cloud providers now security,
08:21 - network configuration and so on.
08:26 - So as you see, the operations
of microservices,
08:31 - especially in the modern,
08:32 - highly complex environments
is a challenge.
08:36 - And that's where the service
mesh technology like Consul
08:40 - comes in as the communication
channel or communication
08:43 - network layer
between microservices.
08:46 - That solves many of the
above challenges.
08:48 - Now that's a bit simplified
definition of service mesh.
08:52 - But essentially service mesh
like Consul,
08:54 - is the whole infrastructure
layer that has different
08:58 - features to solve these
challenges of operating
09:01 - microservices applications
and enabling
09:04 - communication between them
across multiple environments.
09:08 - Great. So now that we
understand conceptually
09:10 - what Consul is and why we even
need a service mesh technology,
09:15 - let's actually understand how
it works and how it solves these
09:19 - challenges that I talked about.
09:21 - And the great thing is that most
service mesh technologies
09:24 - are pretty similar in
their functionalities.
09:26 - So understanding the concepts
of how Consul
09:29 - works will make it much easier
for you to understand any other
09:34 - service mesh technology as well.
09:36 - So I'm a big fan of concepts
before technologies concept
09:40 - to understand what problem
you are solving
09:42 - and what is the need.
09:44 - And technology is then a tool
for solving that problem
09:48 - and fulfilling that need.
09:49 - So let's understand all
that to understand the way
09:54 - a service mesh
like Consul works.
09:56 - Let's imagine we have a city
with different
09:59 - buildings and roads,
10:00 - and those apartment
buildings have a bunch
10:02 - of residents in apartments,
10:04 - and those residents
each do their tasks,
10:07 - and sometimes they need
information from other
10:10 - residents to complete those.
10:12 - So they send messages to each
other to communicate.
10:15 - And each resident has an own
registry book,
10:19 - like an old address book
with a list of all
10:22 - the residents they talk
to and their addresses,
10:25 - where they can send
the messages.
10:27 - The city is the
Kubernetes cluster,
10:29 - buildings are the nodes,
10:31 - and the apartments are pods
of each microservice,
10:36 - while the residents
are the service
10:37 - containers within the pods.
10:39 - And that phonebook is like
a configuration file
10:42 - for the applications
in a container where
10:45 - they provide information
of all service endpoints
10:47 - with service name
and port number.
10:50 - So basically where they have
a list of all
10:52 - the services they talk to. Okay.
10:53 - So that's what we
are working with.
10:55 - No Consul no service mesh
in our imaginary city yet.
10:59 - Now let's see how our
city residents or pods
11:03 - operate without a mesh.
11:05 - If a resident moves
to another building,
11:08 - all residents who were
sending messages
11:11 - to her need to now
update their address
11:14 - book with the new address.
Otherwise,
11:16 - they will be sending
the messages to the wrong
11:19 - address and wonder why
they never get an answer back.
11:22 - And this would happen
when a microservice
11:24 - or database service gets
a new endpoint or service
11:29 - name or port changes.
11:30 - Now the city
is managed centrally,
11:33 - like when someone
is administering
11:35 - Kubernetes cluster.
11:37 - So the administrators want
all communication data going
11:42 - through the services to be
transparent and gathered
11:45 - in one place so they can
identify if there are any
11:48 - issues in the communication
between the residents
11:51 - and fix those issues to see
maybe how secure the city is,
11:55 - how responsive the residents
are to each other, and so on.
11:58 - So this residents need
to keep a protocol
12:01 - and report the city
monitoring service
12:04 - about their communications.
12:06 - Each and every one of them
needs to do that.
12:09 - So making sure each
resident does their job
12:11 - properly and consistently
and while keeping
12:15 - up with the messages,
12:16 - they do all these other
administrative tasks as well,
12:19 - which is overwhelming
for a lot of residents.
12:22 - So they're working over time.
12:24 - I know this sounds
like a weird city
12:26 - with surveillance system
monitoring its residents
12:29 - and making them work 24
over seven,
12:31 - but we are in a Kubernetes city,
so it's fine.
12:34 - And in Kubernetes cluster,
12:36 - this is equivalent to adding
a monitoring endpoint in our
12:41 - applications to expose metrics.
12:43 - So we can scrape and collect
those metrics in Prometheus,
12:47 - for example,
12:48 - or adding logic to each
microservice for how to handle
12:51 - communication with other
services
12:53 - when they don't respond or when
they get an error reply,
12:57 - like how do they retry
the request, and so on.
13:00 - And some residents might
miss to track some data.
13:04 - Some of them will just track
part of the data
13:06 - and not all of it.
13:08 - They may all write in different
formats or not
13:10 - readable handwriting's,
13:12 - so the central service will
not be able to fit those
13:16 - reports or that metadata about
the communication
13:19 - itself together,
13:20 - because they're all
in different formats.
13:23 - So essentially anything
related to communicate.
13:26 - With other services,
13:28 - making sure the addresses
and endpoints are up to date,
13:31 - proper handling of when
they don't get
13:33 - a response back or if there
are any communication issues,
13:36 - and so on.
13:38 - The residents are responsible
about all these themselves.
13:41 - Now to optimize this city
13:47 - and release some workload
13:49 - from our residents and let
13:51 - them focus on their main tasks.
As city administrators,
13:55 - we introduce a service
mesh like Consul.
13:58 - Basically in every apartment
for every resident,
14:01 - we add a personal assistant.
14:04 - This assistants or agents
are now saying to the resident,
14:08 - I will send all those messages
for you. In fact,
14:12 - you don't even need to know
the exact addresses of other
14:16 - residents that you
are talking to.
14:18 - You just write their name
on the envelope and I will
14:21 - find out where they live,
and I will deliver the message.
14:24 - And when they respond back,
14:25 - I will receive the incoming
messages and forward
14:30 - them to you.
14:31 - I will also keep
a protocol of any
14:33 - information that goes
through me.
14:35 - So all these administrative
tasks around sending
14:38 - the messages are taken care
of by those
14:41 - agents or assistants,
14:43 - and residents can focus
on their main activities
14:47 - and the actual contents
of the message.
14:49 - In Consul this assistants
are envoy
14:53 - sidecar proxy containers
injected into the pod
14:57 - of each service. As you know
from Kubernetes,
14:59 - we have a service container
running in Pod,
15:02 - and we can run helper or sidecar
containers that will
15:06 - run alongside to support
that main service
15:10 - container in its job.
15:11 - So these envoy proxies will
act as those assistants living
15:16 - in the pod along the service.
And by the way,
15:19 - I have another video
where I explain service
15:22 - mesh with example of Istio.
15:24 - I explain the same concept
but from a different angle,
15:28 - so you can check it out as well.
15:30 - To get even better understanding
and compare two
15:33 - different service mesh tools.
Also,
15:35 - if you are new to these
concepts like running
15:38 - microservices applications
in a complex
15:40 - Kubernetes environment,
15:42 - I actually have a complete
DevOps bootcamp where you can
15:46 - learn all these
with practical hands
15:49 - on projects as well
as separate course
15:51 - focused specifically
on microservices with mono repo,
15:56 - poly repo structures,
15:57 - and building a CI CD pipeline
for the microservice
16:00 - application on the git.
16:02 - And in our latest program
about DevSecOps,
16:05 - you can also learn
the security focused
16:09 - approach of working
with containerized applications,
16:12 - Kubernetes cluster,
16:12 - automating security
and compliance
16:15 - checks for applications,
16:17 - and also learn about
the production grade
16:20 - deployment of microservices
application with a service
16:24 - mesh in Kubernetes using
the security best practices,
16:27 - along with tons
of other concepts.
16:30 - So if you are at the point
where you want
16:32 - to take your engineering
career to the next level,
16:35 - definitely check out our courses
and trainings to really dive
16:39 - in and learn these tools
and practices properly.
16:42 - And all these for a fraction
of the price
16:45 - of what engineers with this
skill set earn
16:48 - in salary anywhere in the world.
16:50 - So how do these assistants do
their job?
16:53 - Because now they need to have
an address book and a way
16:57 - to keep the protocol of things,
17:00 - instead of each resident
having its own address book.
17:04 - Assistants actually
have a shared network
17:07 - with a shared address book,
17:09 - so each assistant will add
the information about their
17:13 - resident or their service
and how to talk
17:16 - to it in this central registry
so other assistants
17:21 - can read that information
as well.
17:23 - So when a new pod gets scheduled
with a new microservice,
17:27 - it will get assigned
the assistant automatically.
17:30 - So proxy will be automatically
injected by service mesh
17:33 - and proxy will say
to all the other
17:36 - proxies or the shared network.
Hey, we are new here.
17:40 - Me and my service.
17:41 - And this is how you can
contact me
17:44 - if you want to talk
to the service
17:46 - that I'm assisting.
17:47 - And I will then forward
your message to the service.
17:50 - Now, when any service
wants to talk to any
17:53 - other service,
17:55 - they can say to their
personal assistant. Hey,
17:58 - I want to send this request
or message
18:01 - to this service called payment.
Please deliver it.
18:04 - The proxy looks at the shared
registry to find
18:08 - the location of the intended
service based on its
18:13 - name or tags,
18:14 - and it will send the message
to the services address,
18:17 - where the agent or assistant
of that service will open
18:21 - the door and accept the message,
18:23 - and that agent will then deliver
it to the actual service
18:27 - inside the apartment or pod.
18:29 - So essentially that means
services don't need to know
18:33 - each other's endpoints at all.
18:35 - They have the assistance
for that.
18:37 - So we free the individual
services from having
18:40 - to even know this information
and extract it completely
18:43 - into the service mesh.
18:44 - And when we have new
tenants in new apartments
18:48 - and when old ones move out
of the building or the city,
18:51 - agents update
this information dynamically.
18:54 - So instead of a static
configuration
18:57 - file with endpoints,
18:59 - we have what's called
a dynamic service registry
19:02 - that is always kept up to date
by those Consul agents.
19:06 - Now let's say a resident
gets sick like they get
19:10 - a burnout from too much work,
19:13 - in which case their
assistant will update
19:16 - the information in the registry
and say my service is sick.
19:21 - They can't receive and reply
to any messages for now,
19:25 - and I will let you know
when they're
19:27 - healthy and responsive again.
19:29 - So now if we have pod
replicas of the same
19:32 - service on same or different
nodes represented by buildings,
19:38 - other proxies will know to talk
to one of the other healthy
19:42 - replicas of that service and not
send the traffic
19:45 - to the unhealthy replica
by reading this health
19:49 - information from the
shared registry.
19:52 - And again,
19:53 - all of this is handled just
between the assistance services.
19:57 - Don't need to worry about
handling any of this logic,
20:00 - or keeping up to date
with which service
20:03 - replicas are healthy or not,
20:05 - and trying to retrieve these
health
20:07 - information from somewhere.
20:08 - They're completely
unaware of all this.
20:14 - Now, let's say while agents
are carrying these
20:17 - messages back and forth between
services in different buildings,
20:21 - some malicious actors
like hackers managed
20:24 - to sneak into our city.
20:26 - So they entered our Kubernetes
cluster or our
20:29 - infrastructure and got access
to our network somehow.
20:33 - Now we have these
malicious actors
20:35 - on the streets roaming
around freely who want
20:38 - to sniff these messages
being sent between services,
20:42 - especially if they
contain private,
20:44 - sensitive data.
20:45 - Maybe they want to steal payment
information or personal
20:47 - user data.
20:48 - Maybe they want to mess
up the systems.
20:51 - So if they snatch the envelope
from the agents,
20:55 - open it and read it,
20:56 - they will see all
the information inside.
20:58 - So ideally we want to encrypt
that communication
21:02 - between the services.
21:03 - So even if hackers managed
to get into our
21:06 - system and network
and they were able
21:09 - to see those messages,
21:10 - they can't understand anything
because instead of plain text,
21:14 - it's all encrypted and only our
agents can decrypt them
21:18 - because they have
the decryption keys.
21:21 - So that's another feature.
21:23 - Service mesh offers
encrypting end to end
21:26 - communication between services
using mutual TLS without mesh.
21:31 - If you had 20 microservices
and you wanted to implement
21:35 - encryption between them,
21:36 - you would have to change
the application code
21:38 - in every single service
to implement encrypting the data
21:43 - or receiving encrypted data.
Terminating encryption.
21:47 - You would have to implement
the management
21:48 - of the encryption keys
and certificates to make
21:51 - sure that they are also
securely created and stored.
21:55 - And it's a lot of work on the
development side. Again,
21:58 - that extra administrative
work around secure
22:01 - communication that has nothing
to do
22:04 - with the business
logic directly. Plus,
22:07 - mostly these are the things
that developers
22:09 - are probably not the most
knowledgeable in and not
22:13 - the best at implementing
this stuff.
22:15 - You kind of need specialized
knowledge to implement
22:19 - this with proper security.
22:20 - So the fact that you get these
out of the box in service
22:23 - mesh is pretty powerful.
And this is interesting,
22:26 - the microservice itself is still
sending the traffic,
22:30 - which is unencrypted,
22:32 - but before it leaves
the apartment or pod,
22:35 - it's captured by the proxy.
22:37 - The proxy has a TLS certificate
with encryption
22:41 - key to encrypt the message.
22:43 - So when the request
leaves the pod,
22:45 - it's fully encrypted.
22:47 - When that encrypted request
reaches the target
22:50 - service or in Consul term,
22:53 - upstream dependency
of that service,
22:55 - that services proxy will
then receive the message.
22:59 - And we'll do that TLS
termination before
23:01 - routing it to its host service,
23:05 - which basically means it will
decrypt the message
23:07 - with its encryption key
and pass the plain
23:10 - text message to the microservice
within the pod. So now,
23:14 - even if someone infiltrates
did Kubernetes
23:17 - network and was sniffing
the traffic between the pods,
23:20 - they won't be able
to read the messages
23:22 - because they are all encrypted.
And again,
23:24 - the microservices themselves
have no idea that all
23:27 - this encryption decryption
is happening.
23:29 - From their perspective,
23:30 - they're just sending
and receiving
23:31 - unencrypted messages.
23:33 - The biggest advantage
of service mesh
23:35 - is that all that functionality
is built in the mesh itself,
23:39 - which means it doesn't matter
how your applications
23:42 - are programed or how other
people's third party
23:44 - applications are programed over
which you anyways
23:47 - have no control.
23:48 - You can still use all
these like end to end
23:51 - encryption and error handling,
23:53 - etcetera with Consul
without relying
23:55 - on the applications
implementing this logic
23:58 - or having support for TLS,
for example.
24:02 - And that is super powerful
and helpful when you're
24:05 - operating complex,
heterogeneous systems.
24:12 - Now, these end to end
encryption or mutual
24:15 - TLS between services
gives us one more thing.
24:19 - Since each service proxy
gets its own individual
24:23 - certificate to establish secure
connection with other services.
24:27 - This individual certificate can
also be used
24:30 - to uniquely identify the service
and validate its identity. So,
24:35 - for example,
24:36 - each resident or service gets
their own
24:40 - unique stamp or certificate.
So when they send the message,
24:44 - the assistant stamps
the envelope
24:47 - with the stamp or encrypts
the message with their key.
24:50 - So when the receiving agent
or proxy gets the message,
24:54 - that agent can verify
with the central registry,
24:58 - is this stamp real or fake?
25:00 - Was it tampered with and
which service
25:03 - does it belong to?
25:04 - So we know this message
really comes from the proxy
25:08 - of the payment service.
For example,
25:10 - since it's signed
by its certificate,
25:12 - we can now use this information
to define
25:16 - rules about who can talk to who,
25:19 - like define whether payment
service is allowed to talk
25:22 - to user authentication
service and frontend
25:24 - service is not allowed to talk
to the database service,
25:27 - for example.
25:28 - So after the identity
is verified as a second step,
25:32 - proxies will check
the communication rules.
25:35 - So this is really a payment
service sending
25:38 - this message that's verified.
25:40 - But is it actually
allowed to talk to my
25:43 - user authentication service.
25:45 - So is this resident allowed
to talk to my resident
25:48 - or does it maybe have
a restraining
25:50 - order if proxy sees oh,
25:52 - it's not supposed to be sending
message to my service,
25:55 - then it can block the message
and the connection
25:58 - so it won't be forwarded to the
service at all.
26:01 - If the rule allows it,
then everything is verified,
26:04 - so it will forward
the decrypted message.
26:07 - This is also called micro
network segmentation.
26:11 - So instead of having a firewall
on the security
26:14 - group level or a subnet level,
26:16 - we have firewall on an
individual service level,
26:19 - which gives us a more granular
control of who can talk
26:23 - to our services on which ports.
ET cetera.
26:26 - That's why the term micro
network segmentation.
26:30 - Now remember,
26:32 - city wants to have protocols
of who is talking
26:34 - to whom,
26:35 - especially when we limit those
connections with strict rules.
26:38 - We want to see who is trying
to break the rules and talk
26:42 - to the services
that they are not
26:44 - supposed to be talking to,
26:46 - or generally which tenants
are unhealthy maybe?
26:49 - Or who is sending and receiving?
How much traffic?
26:52 - Are there any bottlenecks
in the system?
26:54 - What is the error rate
and what error responses
26:57 - are we getting
from different services?
26:59 - Maybe a few services are getting
too many requests
27:03 - and are overloaded and on
the verge of a burnout
27:07 - and proxies by being located
exactly
27:10 - in that traffic path where
the data exchange is happening,
27:13 - automatically end up
with rich telemetry data,
27:17 - which they can then expose
to an external
27:20 - system like Datadog
or Prometheus.
27:22 - And here's a great thing
about proxies being the ones
27:26 - that collect and expose
this data.
27:28 - Consul proxies are all
the same service,
27:31 - which is envoy proxy.
27:33 - So when they collect
and expose the metrics
27:36 - in different services,
27:37 - they all do it in the same way
because it's
27:39 - the same application.
27:40 - So they collect and expose
the same metrics in the same
27:43 - format across all services.
27:45 - So it's easy to put together
metrics of all services
27:49 - and build unified
dashboards from them
27:52 - in Prometheus and Grafana,
which are the monitoring tools.
27:55 - So this architecture gives
us immense power to change
27:59 - and control things
in the network
28:01 - without having to do any
changes in the applications,
28:05 - which means we are flexible
to do whatever
28:08 - we want very fast,
28:10 - and configure things
very fast in a unified
28:14 - way for all the services.
28:17 - Now you're probably thinking
proxies have a shared
28:20 - address list of all
other services.
28:23 - They have certificate data
and they know who can
28:26 - talk to who based on the
rules configuration
28:29 - that they all share access to.
28:32 - So the question is how
do they get all this data?
28:36 - Or when a new proxy starts up,
28:38 - who provides it with all
this information?
28:41 - And where is this shared
database and storage
28:45 - of information and certificates?
Where is it located?
28:48 - And that's where the Consul
servers come in.
28:51 - With our analogy.
28:53 - Imagine these assistants all
worked for the same company,
28:57 - and they had
a headquarter office
28:59 - in the city in its own building,
separate from the assistance.
29:03 - This office is the
Consul server.
29:06 - You can have a single
room in a building,
29:09 - like a single Consul server
instance or a single
29:12 - pod replica.
29:13 - But if you are managing many
services and their proxies,
29:17 - you might need a bigger office.
29:19 - So maybe 3 or 5 Consul
server pods.
29:23 - So these Consul servers push
out all the needed data
29:27 - to the proxies
or Consul clients,
29:30 - like service
registry information,
29:32 - the configuration certificates.
29:35 - So we don't have to do anything
to get certificates
29:38 - in all this data to the proxies.
29:40 - All of this is done and managed
automatically
29:43 - by the Consul servers.
29:45 - So we have basically automated
operations of the mesh itself.
29:50 - And as I mentioned,
29:51 - those personal assistants have
a network.
29:54 - They talk to each other
exchanging
29:56 - information and so on.
29:57 - And that network of personal
assistants is called data plane.
30:01 - And the central office
or cluster of Consul servers
30:07 - that manage these assistants
network is called control plane.
30:11 - This means the data plane
is managed centrally by Consul
30:14 - servers or the control plane,
30:16 - so that they too can focus
on doing their job
30:20 - of handling the communication
between services
30:23 - and if something changes
or gets updated,
30:26 - like the address
of a service or new
30:28 - service gets added or removed,
certificates get rotated,
30:32 - they will get the update
from the central
30:35 - office automatically.
30:36 - It's like these proxies
are all working
30:38 - for the same organization,
30:39 - having access to the centrally
managed resources and data
30:43 - so they can all do
their work easily.
30:46 - And this control plane
leaves separately,
30:49 - maybe in the same city,
30:50 - which would be the same
Kubernetes cluster.
30:52 - Or maybe they even have
an office in a different city,
30:56 - which means you can spin up
a dedicated Kubernetes
30:59 - cluster just for the Consul
control plane,
31:02 - and then connect it to
the Kubernetes cluster
31:04 - where the data plane is running.
31:09 - Now let's say we have
31:11 - multiple Kubernetes clusters
31:12 - with our microservices
31:14 - like in different
31:15 - geographic regions,
31:16 - maybe replicated on different
cloud platforms even.
31:19 - And this is like having allied
cities or city allies where
31:24 - cities form a network
and decide, you know,
31:27 - let's form a partnership.
31:28 - So we will allow your services
to talk to ours and vice versa.
31:33 - In this case,
31:34 - you can have Consul control
plane in each cluster.
31:38 - So own control plane
office in each city.
31:40 - Or again you may have one
dedicated cluster or the main
31:44 - headquarter Consul
control plane.
31:46 - And it will manage all other
clusters data planes from there
31:51 - which is a common setup.
31:53 - This way you can avoid
replicated
31:55 - offices and resources.
31:57 - For example in Consul
is especially powerful
32:01 - in such multi cluster multi data
center environments.
32:06 - Connecting services across
different environments
32:09 - which can be a really big
networking and security
32:12 - challenge if you're doing
this without a service
32:14 - mesh tool.
32:15 - So how does this happen
with Consul?
32:17 - Think of Consul planting guards
at the exit and entry
32:22 - of the city
in Kubernetes cluster.
32:24 - This guard is called
a mesh gateway.
32:27 - So if payment service
from cluster one wants
32:30 - to talk to user authentication
service in cluster two,
32:33 - it will be the same process
for the services where
32:36 - the payment service just says
to its proxy, hey,
32:39 - send this message to the user
service, please.
32:42 - I don't know where it's running.
Also, I don't care.
32:45 - You will figure out how
to deliver this message.
32:48 - Proxy will have the list
of available services
32:51 - provided by the Consul server,
32:53 - including services in all
allied cities
32:57 - where it says okay,
32:59 - this user authentication
service lives in another city,
33:02 - so it could hand the message
over to the city guard.
33:05 - The guard will take
it to the other cluster
33:08 - and hand it over to the guard
at the entry of that cluster,
33:12 - which is going to be the mesh
gateway of the second cluster,
33:15 - which will then deliver
it to the proxy of the user
33:18 - service inside the cluster.
33:19 - And finally it will be
forwarded to the user
33:23 - service within the pod itself.
33:25 - And the response will
flow the same way back
33:27 - to the payment service.
33:29 - And we're going to see
an example of this specific
33:32 - use case in the demo part,
33:34 - where we will connect two
Kubernetes clusters on two
33:38 - different cloud platforms
with each other using Consul.
33:41 - And the good thing is,
33:43 - it doesn't matter which cloud
platform you use.
33:45 - It pretty much works
the same all the time.
33:47 - Now when we talk about multi
data center environments,
33:51 - it's not just
Kubernetes cluster.
33:52 - Many companies,
33:53 - especially large
established companies,
33:56 - have tons of applications
that run
33:58 - on legacy systems
directly on VMs.
34:01 - Or they have a large company
wide database that database
34:04 - engineers team is managing
centrally that also run on VMs.
34:08 - And often that team already
has a strong expertise of how
34:13 - to manage and operate those
services
34:14 - on the virtual machines.
34:16 - So the overhead
of learning Kubernetes
34:19 - and then learning how
to migrate and operate
34:21 - the service on Kubernetes
is often too large.
34:25 - Or if it's a small
legacy application,
34:27 - maybe the overhead
is just not worth it.
34:29 - So these are real use cases
where companies still have
34:33 - services that will run on VMs
and may not be migrated
34:37 - to Kubernetes or cloud
anytime soon,
34:41 - but these companies and projects
still want to take
34:44 - advantage of the modern tools
like Kubernetes and containers.
34:48 - So the teams in that company
deploy their microservices
34:52 - in Kubernetes cluster,
34:53 - which now has to connect
to the database on the VMs
34:57 - or connect with other legacy
backends still running
35:01 - on premise virtual machines.
35:03 - And if connecting multiple
Kubernetes
35:05 - clusters is a challenge,
35:07 - try throwing VMs in that mix
that becomes even larger.
35:11 - Challenge of how do we connect
those networks? And again,
35:15 - service mesh tools make
that easier by abstracting
35:19 - this low level network
configuration
35:21 - and letting you manage that on
a service mesh level.
35:24 - What's great with Consul
specifically
35:26 - is that while other
service mesh tools
35:28 - also have this capability,
35:30 - Consul actually treats the VM
environment
35:33 - with the same importance
or as a first class citizen,
35:37 - same as
the Kubernetes environment,
35:39 - and doesn't treat
it as an uninvited
35:42 - or undesired guest
serving it just
35:45 - because it's there
and it has to.
35:47 - So how does Consul work on VMs?
If we use our analogy again,
35:53 - an on premise data center
would be its own city
35:56 - with a bunch of private houses
where each house is a VM,
36:01 - the application or service
will be the only
36:05 - resident in the house
and the Consul proxy and.
36:08 - Consul client will be living
in that house
36:12 - along the resident,
36:13 - and we will have its
own house for the Consul
36:16 - server as main office.
36:18 - You will then configure
the communication channel
36:21 - so that Consul server running
on the VM can connect
36:24 - to the Consul server
in the Kubernetes cluster,
36:27 - so they can share information
and create
36:29 - connection channel
for their residents.
36:33 - So now again you have
Mesh gateway
36:36 - in the Virtual Machine City
as well.
36:39 - Who will communicate with Mesh
Gateway in Kubernetes cluster.
36:43 - This way it can connect to other
allied cities like other VMs
36:48 - or Kubernetes clusters.
36:50 - And once the trust
and secure communication
36:52 - channel is established
between them.
36:55 - Now the residents of both
cities can talk to each other
36:59 - through that secure channel.
So again,
37:01 - now with payment service
in Kubernetes,
37:03 - cluster wants to talk
to the database on VM.
37:05 - They go through the same
exact process where
37:08 - payment just says to its proxy,
hey,
37:11 - send this message
to a database please.
37:14 - Proxy will have the list
of available services
37:16 - provided by the Consul server,
37:18 - and it sees their database lives
in another city.
37:22 - So through the mesh gateways
the message will
37:25 - get transported all the way
to the database running on VM
37:29 - in a different data center.
So as you see,
37:32 - a service mesh like Consul
is essentially the whole
37:35 - infrastructure layer
that has different
37:38 - features to solve these
challenges of operating
37:41 - microservices applications
and enabling
37:45 - communication between them.
37:49 - So in this demo part we're
37:52 - going to create a Kubernetes
37:54 - cluster on AWS
37:56 - using ECS service.
37:58 - So that's going to be
our very first step.
38:01 - Once we have the Kubernetes
cluster we're going
38:03 - to deploy a microservices
application with lots
38:07 - of services inside the cluster.
38:09 - And we're going to use an open
source microservices
38:13 - application project from Google.
38:15 - So it's a little
bit more realistic,
38:18 - like a more
complex microservice.
38:19 - And not just two services
for the demo.
38:22 - And once we have
that all set up,
38:25 - we're going to deploy
Consul on ECS,
38:27 - and we're going to see how
the proxies will be injected
38:30 - in each one
of those microservices.
38:32 - What configuration changes
we're going to have to make
38:35 - to the microservices
Kubernetes manifest
38:38 - files in order for Consul
to work in Kubernetes,
38:42 - and also explore a couple
of features of Consul
38:46 - and what it gives
us out of the box.
38:48 - Once we have all of that set up,
38:51 - we're going to create
another Kubernetes cluster
38:54 - on a different cloud platform.
38:56 - And we're going to use fairly
simple Kubernetes managed
39:01 - service on Linode
Cloud platform.
39:04 - I like using because it's
super simple
39:07 - to spin up a cluster there
to create it manually.
39:10 - It's just very little
effort compared to ECS,
39:13 - and it's also very fast.
39:15 - So we're going to use
that as a demonstration
39:17 - for another Kubernetes cluster.
39:18 - But it could really be
any other Kubernetes
39:20 - cluster that you want. So the
concepts are the same.
39:23 - And then in that linode
Kubernetes managed cluster,
39:27 - we're going to deploy
the same exact
39:29 - microservice and same exact
Consul configuration.
39:33 - And once we have that we're
going to connect those two
39:36 - clusters together using Consul.
39:38 - And we're going to see
the demo of or simulation
39:42 - of a service going down
or crashing inside the cluster.
39:48 - And it failing over to the same
service inside the Elk cluster.
39:55 - So basically a multi cluster
environment
39:58 - with a service failover
to another Kubernetes cluster.
40:03 - So let's go ahead and do
that step by step.
40:08 - Where along the way I'm
going to explain lots
40:11 - of different concepts related
to Consul service mesh.
40:15 - So the first step is we're
going to create an EKS cluster.
40:20 - But of course we don't want
to do that manually
40:23 - because it's a lot
of effort and it's not
40:26 - the easiest thing to do.
40:28 - So we're going to use
infrastructure as code
40:31 - using Terraform and all the code
configuration files,
40:33 - the Terraform script,
the microservices application,
40:36 - all of that will be linked
in the video description.
40:39 - So you can clone those
repositories and follow along.
40:43 - So this is one repository
where I'm going to have all
40:47 - my Kubernetes manifest files
that we're going to use
40:49 - to configure Consul and deploy
our microservices application.
40:54 - So all of that is going
to be here.
40:57 - And we have the Terraform
folder inside
40:59 - with the Terraform script
for creating the EKS cluster.
41:02 - And I have this repository
cloned locally
41:05 - so that I can work on it using
my code editor with terminal.
41:11 - So this is where we're going
to be doing most of the work
41:14 - of configuring stuff.
41:16 - And I also have my account ready
where we're
41:18 - going to be creating the Elastic
Kubernetes Service.
41:23 - Right now we don't have any.
So let's create one.
41:26 - And before we do,
41:28 - let's actually go through
a little bit of the Terraform
41:31 - script and what we're
doing here.
41:33 - It's pretty
straightforward actually.
41:35 - I'm using the modules
to make my job easier.
41:37 - So I'm using the VPC
module to create a new
41:41 - VPC for the EKS cluster.
41:43 - Our EKS cluster will
be publicly accessible.
41:46 - That's very important.
41:47 - And therefore we have the public
subnet in addition
41:49 - to the private subnet
in our VPC.
41:52 - And then I'm just using the X
module to create the cluster
41:57 - obviously referencing this VPC.
42:00 - And we're basically configuring
it with a bunch of parameters
42:04 - to configure our cluster.
42:06 - So first of all as I said
I want my Kubernetes cluster
42:09 - to be accessible externally.
42:12 - So with this attribute we can
actually
42:14 - create a public endpoint.
42:16 - Or we can let create
a public endpoint for our
42:20 - Kubernetes API server.
42:22 - So we can connect
to the cluster using
42:25 - kubectl for example or browser
whatever from outside the VPC.
42:30 - Right. So I'm setting
this to true.
42:33 - To achieve that the
next attribute
42:36 - is adding some security
group rules.
42:39 - And this is actually
important for Consul
42:42 - to be able to do its job.
42:44 - And there are some specific
ports that we need to open
42:48 - on the worker nodes themselves,
42:49 - where the Consul processes will
be running
42:52 - in order to allow Consul
components to talk
42:56 - to each other,
42:57 - and for the Kubernetes
control plane components
43:00 - to reach Consul
processes as well.
43:02 - And I'm actually going
to reference
43:04 - to the list of ports
that we need to open
43:08 - for Consul.
43:09 - So you see what they are and why
those ports are needed. However,
43:14 - just to make things simpler
and to make sure
43:18 - that you guys do not have any
networking issues with Consul,
43:22 - and just to make sure
that things go smoothly,
43:25 - what I'm going to do is,
43:27 - as you see in the Terraform
configuration itself,
43:29 - I'm actually going to open all
the ports on my worker nodes,
43:33 - and I want to stress that I'm
actually doing it for the demo,
43:36 - because the security
best practice is to have
43:39 - only those ports open that you
actually need exposed,
43:43 - and only those internal
or external processes
43:46 - that need access to whatever
service is running
43:49 - on that port needs to have
access to that port
43:53 - and nothing else. However,
this is a demonstration,
43:56 - and I just want to make
it easier for you guys
43:59 - to follow along and to make
sure you don't have
44:01 - any networking problems
when deploying Consul.
44:04 - And finally, this is the managed
node groups.
44:07 - So these are the actual worker
nodes or the worker node group
44:10 - configuration for the cluster.
44:12 - And here we're basically just
choosing the small instances.
44:16 - And we're going to have three
nodes or three of those
44:19 - instances in the cluster.
44:20 - And finally we have these two
configuration pieces
44:26 - which are also needed
for Consul deployment.
44:29 - So these are basically
the configuration
44:31 - for the dynamic volume
configuration because Consul
44:35 - is a stateful application.
44:38 - So it actually deploys
a stateful set
44:40 - component and it needs to store
and persist some data.
44:44 - So it needs to create
the volumes on whatever
44:48 - platform it gets deployed.
And in this case,
44:50 - we are making sure that creation
of
44:53 - Amazon Elastic Block Storage
is enabled
44:57 - for the cluster by giving
permission to processes on these
45:01 - nodes to create the storage.
And in addition to this role,
45:07 - we have to enable what's called
an add on on EKS cluster,
45:11 - which allows for automatic
provisioning of the storage.
45:15 - And once the cluster is created,
45:16 - I'm actually going to show
you all this information
45:18 - so we can see that visually
as well. Apart from that,
45:21 - we have variables
that we are setting
45:25 - and I have added some default
values for most
45:28 - of the variables,
45:29 - so you don't have to set
them in the TF vars file.
45:33 - So these are basically
just Cidr blocks for VPC
45:36 - private and public subnets.
The Kubernetes version.
45:38 - That's very important
to make sure to choose
45:40 - the one of the latest ones.
45:42 - This is the latest
one as of now.
45:45 - So that's what I'm using
the cluster name
45:48 - because we use that in a couple
of places
45:51 - within the main configuration.
45:52 - So I extracted that as
a variable region.
45:54 - You can set whatever
region is close to you.
45:58 - And there are two pieces
of information
46:00 - or variables that you have
to set
46:02 - yourself to execute the script.
46:04 - Everything else is configured
and set already.
46:07 - So before you are able to
execute this Terraform script,
46:10 - make sure to go to your AWS
account and for your user,
46:16 - create an access key pair
and you have to set those
46:20 - values for the Terraform.
Object.
46:23 - So I have defined them
and referenced them right here
46:27 - in the provider configuration,
46:29 - which means I can just set
those variable values in my
46:33 - Terraform dot vars file,
which I have done already.
46:37 - And once you have that,
46:38 - you should be good
to go and terraform.
46:41 - Tfrs is a simple text
file with key value pairs.
46:44 - So you have the key name,
which is.
46:46 - This one equals whatever
your key id is in quotes.
46:51 - And same for the access key.
46:52 - So set those two values in the
tf vars
46:55 - file and we are good to go.
46:58 - That's the provider
configuration.
47:00 - That's the version
that I'm using.
47:02 - And now we can actually
execute this Terraform
47:05 - script to create
the EKS cluster.
47:08 - So I'm going to switch to the
Terraform folder and I'm
47:12 - going to do terraform init.
47:14 - So terraform init basically
downloads any providers
47:18 - that are specified here
just like you download
47:21 - dependencies of your code.
47:22 - For example in order
to run your project.
47:25 - As you see it creates
this dot terraform folder
47:28 - where the modules and providers
will be downloaded.
47:32 - Those modules and this provider
and everything is green,
47:36 - which means our Terraform
has been initialized.
47:39 - We also have the Terraform
log file. And now.
47:44 - We can execute Terraform apply.
47:48 - You can do Terraform
plan for the preview.
47:51 - But I'm going to do terraform
apply immediately.
47:54 - And we have to pass
in the variables file
47:57 - that defines any missing
variables. So.
48:03 - VAR file is terraform tf vars.
And let's apply.
48:15 - And this is our preview.
48:16 - All the things that will
be created.
48:18 - I don't need to look
through that.
48:20 - I'm going to confirm,
48:21 - and this is going to take
a couple of minutes
48:24 - to create everything,
48:26 - because lots of components
and things are being created.
48:30 - And once the cluster has been
created and fully initialized,
48:34 - we can continue from there.
48:38 - So the Terraform script
48:40 - was executed and it took
48:43 - some time, but it
successfully executed.
48:46 - So now if I switch back to my
AWS account in the region
48:52 - that you have basically set
in the variables file,
48:56 - I chose the EU central region,
which is closest to me.
49:01 - So this is my EKS cluster
that was created
49:04 - in the Frankfurt region.
49:06 - And if we go inside and check
out the detailed view,
49:10 - I'm going to show you a couple
of things that we have
49:14 - configured in our Terraform
script that we can
49:16 - see in the UI as well.
49:18 - So I want to point out a couple
of configuration details.
49:21 - First of all we have the cluster
configuration details.
49:24 - So we have the role the IAM role
for the cluster
49:29 - which is this one right here,
49:30 - as well as security groups
for the cluster like this.
49:35 - And then we have
the configuration details
49:38 - on the node groups or the worker
nodes themselves.
49:41 - So if I go to compute we're
going to see the three nodes
49:44 - that were created because that
is our configuration.
49:48 - We have defined three nodes.
49:49 - So these are basically
the work node instances
49:53 - that are running in our account.
So if we go to EC2 dashboard,
50:00 - we're going to see these three
instances here.
50:03 - And we have the security group
configuration on the worker
50:08 - node level, which is this one
right here.
50:11 - And note that this security
group additional rule
50:16 - actually applies
to the worker nodes.
50:18 - So this is the same security
group that all the nodes share.
50:21 - So it will be same
for each worker node.
50:24 - And we have also configured
this additional
50:27 - policy for the IAM role
which also applies
50:33 - to the node groups.
50:34 - So now the cluster
the EKS cluster role
50:37 - or the control plane role.
50:39 - But the node group role
that apply to the worker nodes.
50:43 - And again we can see that right
here in
50:47 - the instance configuration.
This is the role.
50:50 - And we should see
this Amazon EBS,
50:53 - CSI driver policy listed here.
50:56 - And I'm pointing this out
because first of all,
50:59 - you need to understand
that these two things
51:02 - are configured separately.
51:03 - You have the control plane
configuration with EKS
51:06 - which is actually running in its
own network.
51:09 - And then you have the worker
node configuration with its
51:12 - own role on port's own firewall
configuration and so on.
51:16 - So if you have any networking
issues and so on,
51:18 - this should help
you troubleshoot and know where
51:21 - to look for things basically.
51:23 - And finally last thing
I want to show you is these
51:27 - EBS CSI driver.
51:29 - Add on that we activated
on our cluster.
51:32 - And you are going to see that in
the evidence
51:36 - tab for the cluster.
51:39 - And right here we have
Amazon EBS CSI driver
51:43 - which basically is needed
in order to automatically
51:48 - provision the elastic block
storage for persistent
51:52 - volumes inside the cluster.
51:54 - So in our case Consul
stateful set actually
51:57 - needs a persistent volume.
51:59 - So this allows the cluster
to automatically
52:02 - provision the Amazon block
storage for those volumes.
52:06 - Awesome. So the cluster
is already active.
52:09 - So we can connect to it using
kubectl and deploy
52:12 - our application inside.
52:19 - So I'm going to switch
52:20 - back to my code editor.
52:23 - And I'm actually going
to use the terminal here.
52:25 - So we have everything
in one place.
52:27 - And we don't need
the Terraform script
52:29 - anymore because we executed
the provisioning already.
52:33 - So now the next step
is to actually
52:35 - connect to our EKS cluster.
52:37 - And we do that using AWS
command line interface.
52:40 - So this is basically
a secure way to retrieve
52:44 - a cube config file from the EKS
cluster without exposing any
52:50 - credentials and without having
to download
52:52 - this kube config file,
52:53 - and so on using
a simple command,
52:56 - which means you have to have
installed. If you don't,
53:00 - it's pretty easy.
53:01 - Just go ahead and install
command line interface
53:04 - on whatever operating system
you have.
53:06 - And once you have that,
53:08 - you need to also configure
your CLI to use the access keys,
53:14 - which can be the same access
keys that your Terraform
53:17 - is using for this demo use case,
53:19 - because command line interface
will need the access
53:22 - credentials to connect
to the AWS account. Right?
53:27 - And I have already configured
all of that with AWS
53:30 - configure command.
53:33 - So just make sure that the
default region
53:36 - and credentials configured here
are for the same
53:40 - account as for Terraform,
and you should be good to go.
53:43 - So. With that setup I'm going
to execute AWS X command.
53:50 - Update. Kube config.
53:53 - And you can actually provide
the region here as well
53:57 - for where the cluster
is running.
54:01 - So central one and we are going
to need
54:05 - the cluster name as well.
54:07 - And we have that here
we call the cluster.
54:11 - This generic name.
54:13 - So I'm going to copy
the cluster name.
54:15 - So basically what update kube
config subcommand does
54:19 - is it fetches the cube
config file
54:22 - which is like a credentials file
for Kubernetes cluster from AWS.
54:27 - And it stores it locally
into a default
54:31 - cube config location,
54:32 - where cube CTL will look
for it and the location
54:36 - is on your user's home directory
in dot cube folder.
54:41 - So after executing this command
you should find
54:44 - cube config file in there.
So let's execute.
54:47 - And there you go.
54:48 - You see the output that the cube
config was.
54:52 - Or the context of the cluster
was added in this location
54:56 - in dot cube slash config.
54:58 - So if you don't have the dot
cube folder already,
55:02 - it will basically create
one and add the cube config
55:06 - file configuration in there.
Or if you already have one,
55:08 - it will just append
to the existing config
55:12 - because you may be connected
to multiple clusters.
55:14 - So all of those configuration
will be right here.
55:17 - That's how it works
for Kubernetes in general.
55:19 - So nothing is specific here.
55:21 - And that means we now
should be able to connect
55:25 - to the cluster using
kubectl command.
55:27 - So let's see. Kubectl get node.
And there you go.
55:33 - We have our three work nodes
with this Kubernetes version
55:38 - which we have defined here.
Awesome.
55:41 - The first step is done
as a next step.
55:43 - We want to deploy
microservices application
55:46 - into this Kubernetes cluster.
55:48 - So for that I'm going
to actually switch
55:50 - to the Kubernetes folder
where I have my manifests.
55:56 - And I'm going to close
55:58 - this up and expand this.
55:59 - So these are all the config
files we're
56:01 - going to need in this demo.
56:03 - But we're going to start
with the simplest one.
56:06 - So that's all we need to deploy
our microservices.
56:09 - So actually we don't need
the repository
56:11 - of the microservice
application itself.
56:13 - We just need a reference
to the images.
56:15 - So this is a Kubernetes
config file that references
56:19 - images of all
those microservices.
56:21 - But of course I'm going to link
the microservices repository
56:25 - in the video description
as well.
56:26 - So this is an open
source microservices
56:30 - demo repository from Google.
56:32 - And all those images are public
which makes it easy
56:36 - to use it for demos.
56:38 - And this currently happens
to be the latest version.
56:41 - If the version has changed,
56:43 - you can check in the
provided link
56:45 - and you can just update
the version basically.
56:48 - So super simple actually,
56:49 - we just have a bunch
of deployments for each service.
56:52 - The configuration is pretty
similar for each microservice.
56:56 - They just run on different
ports and have different names.
56:59 - All of them have cluster IP
services which are basically
57:03 - internal services.
57:04 - And we have one entry
point microservice,
57:08 - which is the front end that will
then route the traffic
57:12 - to all the other microservices.
57:14 - And here we see basically
the frontend
57:17 - talks to all other services,
57:19 - and it is the only one
that has an external
57:22 - service of type load balancer.
57:25 - And all those microservices
also share a Redis
57:28 - memory database. Again,
pretty simple setup.
57:31 - Nothing crazy here.
57:33 - So that means once we apply
this configuration file,
57:37 - all the images will
be downloaded,
57:39 - all the pods will be created,
deployment services and so on.
57:43 - And we're going to have one
entry point service
57:46 - to the cluster through
load balancer.
57:48 - Now for simplicity I'm not
going to deploy an ingress
57:51 - controller in the cluster.
57:52 - So we're just going to use
the load balancer service
57:55 - directly to access
our application,
57:58 - which is going to be
enough for our demo.
58:01 - So let's go ahead and apply
this config file.
58:09 - Config dot Yaml. That's all
we have to do.
58:12 - And by the way,
58:13 - there is one service
that is misconfigured
58:16 - slightly on purpose, which is
a payment service.
58:18 - It basically is missing
one environment variable.
58:21 - So it's not going to be able
to successfully start in a pod,
58:25 - but we're going to need
that to demo that later
58:28 - in Consul.
58:29 - So let's execute
and see the result.
58:32 - And we have the output of all
the stuff that was created.
58:36 - So we have quite a few
microservices here,
58:38 - and it will need a little
bit of time.
58:41 - And we're going to check
kubectl get pod.
58:45 - So they will all be created
in the default namespace.
58:48 - So if we do kubectl get
pod we should see all our
58:53 - pods are up and running
except for the payment
58:55 - service which is going to stay
in the error
58:58 - state which is fine.
59:00 - We're going to use
that as an example
59:02 - of an error in microservice
to see it in Consul. Okay.
59:06 - So that was pretty easy.
59:08 - Now what we actually want
to see what we deployed.
59:11 - So I'm going to do
kubectl get services.
59:16 - And as you see all our services
are cluster IP type except
59:21 - for the frontend external.
59:23 - That means if you have watched
my other Kubernetes tutorials,
59:26 - you would know that load
balancer external service
59:29 - gets the internal cluster IP,
59:32 - but also the external IP
because we need to be
59:35 - able to access it externally.
59:37 - And this is the external
domain name for the load
59:42 - balancer that will then map
to the external IP.
59:45 - So where does it come from.
59:47 - That's also pretty easy
as it works in Kubernetes.
59:50 - When you create a load
balancer on whatever
59:52 - cloud platform,
59:53 - you are creating this load
balancer service.
59:56 - It will in the background
use that Cloud Platforms load
60:00 - balancer service to create
the native load balancer there.
60:04 - So that's where the external
IP comes from.
60:06 - And that means if I switch
back to AWS and go
60:13 - to EC2 service.
60:16 - That's where we have
the load balancers.
60:18 - We should see our load
balancer for the front
60:22 - end external right here.
60:24 - And this is basically the DNS
name that we see right here
60:28 - right ending in 138
configured on Http port
60:34 - forwarding to this port which is
configured right here.
60:39 - So pretty simple.
60:40 - We just grab this DNS
or external DNS name.
60:43 - And since port 80 is the default
Http port it will
60:48 - just open it like this.
And there you go.
60:52 - Our microservice is deployed.
60:55 - Now the next step
is to actually deploy
60:58 - Consul in our cluster and use
Consul for our microservices.
61:02 - So how do we deploy Consul.
61:05 - There are several ways
to install Consul
61:07 - on Kubernetes cluster.
61:10 - One of them is using
Consul's Kubernetes CLI
61:14 - and another one
is using Consuls.
61:16 - Official helm chart
for Kubernetes,
61:18 - which is what I'm going to use.
So if we search for Consul,
61:23 - helm, chart and open
the installation guide on their
61:27 - official documentation,
61:28 - always try to refer to the
official documentation
61:31 - instead of some blog posts
because they are most
61:34 - up to date. So for
the latest version,
61:36 - these are the instructions.
61:37 - We basically add the HashiCorp
helm repository.
61:42 - We install the Consul chart
and provide any
61:47 - parameters we need.
61:49 - And if you have watched my helm
chart videos,
61:52 - you know that helm charts
are configurable so we can
61:55 - actually provide any parameters,
61:57 - any configuration options
in order to configure
62:00 - the service using those
parameters. Right.
62:03 - So for example,
62:04 - with Consul service mesh
I mentioned
62:06 - it has multiple features.
62:07 - And depending on which features
you actually need,
62:10 - you can enable them
by configuring them
62:13 - as the chart values or passing
them as chart values,
62:17 - and to see what values
are available
62:20 - to be set and parameterized.
62:22 - Obviously we need to see
the values yaml file.
62:25 - So we have a chart
reference here.
62:28 - So that's basically the chart
values reference documentation.
62:32 - I usually actually prefer
the values stored
62:35 - file in the repository.
So for example this one.
62:38 - But as you see the repository
has been archived.
62:42 - So this documentation is up
to date and something
62:45 - that you should reference.
62:47 - But even though it's an
advantage to have
62:50 - the chart highly configurable
so you can
62:52 - actually tweak it to
whatever desired
62:55 - configuration you have in mind.
62:57 - If you don't know
what you're doing,
62:58 - there's this huge list of values
that you have to basically
63:02 - understand what they're doing.
63:04 - And it's pretty difficult
to understand all
63:08 - these configuration.
63:09 - So we're going to use a couple
of these configuration options.
63:12 - And I'm going to explain
to you what they're
63:14 - actually doing.
63:15 - But if you need any
additional configuration
63:17 - options you can find those
listed here with descriptions.
63:20 - I don't think it's the most
comprehensive
63:22 - and understandable,
63:23 - but at least you have
some reference point.
63:26 - So let's switch back
to the project.
63:28 - And I'm going to walk
you through the values file
63:31 - that I have configured for our
specific Consul installation.
63:38 - Which is actually
63:39 - pretty simple configuration.
63:41 - We have this global attribute
which applies to all
63:45 - the components that are part
of the chart. As you know,
63:48 - chart usually holds
or is a bundle
63:50 - of multiple components
of different Kubernetes native
63:53 - components as well as custom
resource definitions and so on.
63:57 - So this applies
to multiple components.
63:59 - First of all,
64:01 - we have the Consul
image with the version.
64:04 - We are enabling TLS
communication between
64:06 - the components and services.
64:08 - And since we want to connect
multiple clusters
64:12 - or multi data center
environments
64:14 - basically with each other,
64:15 - we are also enabling
the peering.
64:19 - Then we have the Consul
server configuration.
64:22 - As I explained,
64:23 - the Consul server
is the control plane
64:25 - that manages all the proxies,
the Consul client and so on.
64:29 - And usually in a production
environment you want to have
64:32 - at least three replicas,
not just one,
64:34 - because if one replica dies,
you want to have a failover.
64:37 - In our case,
64:39 - we're just going to use one
server for our demo purpose.
64:42 - So you can configure that here.
64:44 - This is an important
configuration option.
64:46 - Connect inject is basically
the name,
64:49 - the technical term
of the service mesh
64:51 - functionality of Consul.
64:53 - And what this configures
is basically
64:56 - if we enable connect inject
it will allow Consul
65:00 - to automatically
inject the proxies,
65:04 - those helper containers,
as I mentioned,
65:06 - into the pods of services.
65:09 - So that's what enabled
true does.
65:11 - And then we have
the second configuration
65:14 - that says default false
which is actually
65:16 - the default value.
65:17 - But I wanted to specifically
configure this.
65:20 - So if this is set to false
then you would need an extra
65:24 - annotation inside
the Kubernetes manifest
65:27 - files or deployment manifest
files in order to actually
65:32 - inject the proxy into that pod.
If we set this to true as well,
65:37 - like this,
65:39 - then it will actually
inject those proxies,
65:42 - even if we don't have
the Consul
65:44 - annotations in the deployments.
65:47 - And this is a good thing to have
control over,
65:50 - because it could be
that you have pods
65:52 - running in your cluster,
65:54 - that you don't want to have
any proxy services in them,
65:57 - or you need to,
65:58 - or maybe you have some
namespaces that should not
66:01 - have any Consul
proxies applied.
66:04 - So if you set it to false,
66:06 - you basically decide per
deployment per application
66:10 - where you want
the proxy injected.
66:13 - So I'm going to set it to true.
So we can see how that works.
66:16 - And we actually want
all our services to have
66:19 - the proxy injected.
66:20 - And we don't want to add
annotations one
66:23 - by one on each deployment.
So I'm going to set it to true.
66:28 - And that's the connect
inject configuration.
66:32 - Then we have mesh gateway,
66:34 - which as I explained
is basically a connector
66:37 - between multiple environments.
66:39 - So mesh gateway is like a guard
that is standing at
66:42 - the entry or exit of the city
with our analogy.
66:46 - And you can also have multiple
replicas of the mesh gateway,
66:49 - because if one of them goes
down or has an issue
66:53 - or maybe becomes a bottleneck
because of the number
66:56 - of requests,
66:56 - you can actually scale up
the replicas.
66:58 - And again,
66:59 - it's a feature that you can
enable if you actually need to.
67:03 - So if you don't have a multi
cluster environment
67:06 - or just for security purposes,
67:08 - you need to stay
within the cluster,
67:11 - then of course you want
enable it.
67:14 - And finally we have UI
which gives us a Consul,
67:17 - dashboard or UI where we can
see the services and where
67:22 - we can even configure stuff
which we are actually going
67:24 - to be using in our use
case and type load
67:28 - balancer basically means
that it will create a service,
67:30 - an external load balancer type
of service,
67:33 - so we can access the UI.
And we are also enabling that.
67:37 - So that's the configuration
that we want to apply to Consul
67:40 - in our cluster to one,
67:43 - or inject the proxies in our
microservices pods. And second,
67:47 - allow us to connect
to Kubernetes clusters
67:51 - with each other and also have
a dashboard where we can see
67:55 - stuff and configure some things.
67:57 - So I'm going to go ahead
and actually install the Consul
68:00 - helm chart with these values.
And let's see what we get.
68:04 - So I'm going to copy the first
command and let's
68:08 - add HashiCorp repository.
There you go.
68:11 - And now we can actually install
helm install.
68:15 - And we're going to give our
Consul chart deployment a name.
68:18 - You can call it whatever
you want.
68:20 - I'm actually going to call
this X to basically
68:23 - differentiate it later from
the Linode
68:26 - Kubernetes Engine deployment.
68:28 - So I'm going to go
with that name.
68:30 - And then obviously we need
the chart name.
68:33 - I'm going to copy it from here.
68:34 - And actually when components
are created
68:36 - it prepend Consul in the name.
68:38 - So that's why I'm not
using X Consul or Consul
68:41 - in the chart name itself.
68:42 - And I'm actually going
to add some additional
68:45 - parameters which can also
be configured in the values.
68:48 - But I'm going to set them
separately because I'm going
68:51 - to use the same values file
for the Consul
68:54 - deployment as well.
68:55 - So I'm going to pass in the
version first of all.
68:59 - And we're going to use. Version
one. And then of course,
69:05 - we have to pass the values file.
Consul values dot yaml.
69:11 - That's our file.
69:12 - And finally the last
configuration is I'm
69:15 - going to set.
69:18 - A global config.
69:20 - So one of those global
attributes called data center.
69:27 - And that's basically the name.
69:29 - So you can name the environment
where Consul is running.
69:33 - You can give it a name
of a data center.
69:35 - And I'm also going to call
this x. And that's basically it.
69:40 - So I'm going to execute
this command.
69:43 - And this should install all
the Consul
69:46 - components in our cluster.
69:48 - And this gets executed
pretty quickly actually I'm
69:51 - going to do kubectl get pod.
69:53 - So first of all I'm going
to check the pods.
69:56 - And now that I'm deploying
the Consul components
69:58 - in the same default namespace.
70:00 - But you could also have them
in a separate namespace,
70:03 - especially if you have
multiple applications.
70:05 - That would make sense.
Let's check again.
70:08 - So we have these four pods.
The first one is server.
70:12 - Obviously we need the server
or the control
70:14 - plane to manage the proxies
to inject the proxies and so on.
70:20 - And the chart name was taken
as the prefix
70:23 - as you see here for all
those pods.
70:25 - Then we have the mesh gateway.
70:28 - One instance of it we have
the connect
70:31 - injector that is the one
that is responsible
70:34 - for injecting pods.
70:35 - And we have the webhook
certification manager.
70:39 - And as you see all the pods
are running successfully.
70:43 - You can also check any other
components that were deployed.
70:46 - So we have this stateful set
which is a server itself
70:50 - and those deployments
and we have the Consul
70:54 - services themselves.
70:56 - And one of them is we saw
is the Consul UI
70:59 - which is created as a load
balancer service type.
71:02 - And it also gets its own
load balancer component on AWS
71:08 - with its own DNS name.
71:09 - So we can use that to access
the Consul UI.
71:14 - So let's go ahead and do that.
71:16 - And I'm going to make
this a little bit broader.
71:19 - And one thing I want to point
out here
71:21 - is that this external service
of Consul UI is actually
71:26 - accessible at the Https
port 443.
71:30 - That means we have to access
the service using
71:34 - Http as protocol. Like this.
71:38 - And of course our browser
doesn't know the certificate
71:42 - which is signed by Consul CA
so we can say it's all fine,
71:47 - we allow it. And there you go.
This is our Consul UI.
71:52 - And as you see we have only
two services displayed here.
71:56 - So basically Consul now
is aware only of two services.
72:01 - One of them is a mesh
gateway which we deployed
72:03 - as part of Consul.
72:04 - And the other one is Consul
server itself.
72:07 - That means another
interesting point.
72:09 - The proxies have not been
injected yet in any
72:13 - of the microservices
because we need
72:16 - to restart or recreate those
services or deployments.
72:21 - So that's what we're
going to do next.
72:28 - So I'm actually going
72:29 - to delete our
72:31 - microservice
72:31 - deployment completely.
72:33 - And I'm going to redeploy
it with a little
72:35 - bit adjusted configuration.
So let's do config dot Yaml.
72:41 - There you go.
72:43 - And let's see. Looks good. Now,
72:46 - I actually have already
prepared a config file
72:56 - with a couple of changes
for Consul
72:59 - deployment specifically.
73:01 - So I'm going to open this file
and let's go through
73:04 - it and understand those changes.
73:07 - So first of all in order
to configure anything
73:10 - in our deployment that is
Consul relevant.
73:13 - So most of the things
that are actually
73:15 - relevant for how Consul
will treat our deployments,
73:19 - or whether it will inject
proxies or how
73:22 - it will handle the communication
between services, etcetera,
73:26 - we can configure those
using the annotations is,
73:29 - you know,
73:30 - annotations are part
of metadata of Kubernetes
73:33 - components like deployments.
73:34 - And these are going to be
the annotations
73:36 - on the pod level.
73:37 - So inside the template metadata
in the annotations we can add
73:42 - the Consul
annotations basically.
73:44 - So this will communicate our
desired configuration of our
73:50 - microservice sees to Consul,
73:52 - which is a pretty easy
way to manage that using
73:56 - the Kubernetes native way.
So the first annotation,
73:59 - which is probably
the most used one,
74:02 - most seen one is connect inject.
True.
74:05 - So basically this specific
configuration is not relevant
74:09 - for us because we set
the connect
74:13 - inject default to true.
Basically,
74:16 - as I said we can say we don't
want auto injection in every
74:20 - part in every namespace.
74:22 - We want to be able
to decide which pods
74:24 - actually get those proxies.
74:26 - And we decide that by adding
this annotation
74:29 - to the pod metadata, right.
74:32 - So every part that we want
to have proxy injected,
74:36 - we can add this annotation
and it will take care of it.
74:39 - However, when this is set
to true
74:41 - which is what we configured.
74:43 - And that's what Consul
that is deployed in our
74:46 - cluster actually knows,
we don't need that.
74:49 - So I'm just going
to comment this out.
74:51 - Let's go to
the next microservice.
74:56 - So we have the same
connect inject.
74:58 - We don't need this.
75:00 - And this is another
annotation called
75:02 - Connect Service up streams
which is another core
75:06 - annotation which basically
defines which services
75:10 - does this service talk to.
75:12 - And how are those
services called.
75:15 - Remember I mentioned that once
the proxy is there,
75:19 - the service inside the pod
does not or should not
75:22 - care about where the destination
services are located
75:26 - and what their addresses are.
75:27 - They can just hand over
the request with the name
75:31 - of the service,
75:32 - and then proxy will figure out
where that service
75:35 - actually runs in.
75:36 - Kubernetes services are already
referenced in this easy way,
75:40 - using the service name and port
instead of static IP addresses.
75:45 - So this is basically not
a huge improvement
75:47 - in this case because Kubernetes
already manages that. However,
75:51 - we do need to communicate
to Consul which services
75:55 - this one will be talking to.
75:57 - So that's kind of the
metadata definition.
75:59 - And when we have this configured
there is one
76:02 - more change we're
going to do here,
76:04 - which is there is an environment
variable that points
76:08 - to the service
that recommendations
76:12 - service is talking to.
76:13 - And in the previous
configuration
76:16 - this was actually the Kubernetes
service name and the port.
76:19 - And again going back to my
previous explanation,
76:23 - the service does not need
to know what the Kubernetes
76:27 - service name
of that microservices
76:30 - that he talks to because
it talks
76:32 - to its proxy.
76:33 - And the proxy will listen
to it to this request
76:37 - on localhost because it's within
the pod.
76:39 - So the containers within
the pod communicate via
76:42 - localhost and the same
port where the upstream
76:46 - service is configured.
76:48 - So this request will basically
go to the proxy instead
76:52 - of the Kubernetes service.
76:54 - But proxy knows that whatever
that request
76:58 - points to is located here,
77:02 - which is the Kubernetes
service name. So as I said,
77:05 - it's not like a huge game
changer here
77:08 - because Kubernetes already
manages the service
77:11 - names pretty well, but that's
basically it.
77:14 - So that's the annotation
that I have
77:17 - configured in services.
77:19 - That's the only annotation
we use here.
77:22 - So I'm going to scroll
through and comment out all
77:26 - of those connect inject
annotations just to demonstrate
77:29 - that we don't need them.
Like this.
77:37 - So I basically just
77:39 - commented out that annotation,
77:40 - but I'm still going to leave
it in the configuration file
77:43 - for your reference in case
you want to use that as well.
77:47 - And then we have two last
services that have a bunch
77:50 - of upstream services
they talk to.
77:53 - And it's the same idea.
77:54 - You can just provide a list
with service names and ports.
77:59 - And then all of those will be
accessible on localhost
78:03 - on different ports.
78:04 - So basically the service
the checkout service will
78:07 - always be talking
to the proxy instead
78:10 - of talking to all those
different services
78:13 - as it was doing
before right here.
78:15 - It will now only
talk to the proxy.
78:17 - And then proxy will forward all
those requests
78:21 - to the respective services
based on whichever port
78:25 - the proxy receives that request.
78:27 - And that means obviously
those ports need to be
78:30 - different within the localhost
and the same for front end.
78:35 - So front end is the last
service which also
78:39 - has a bunch of upstream
services exactly
78:43 - the same concept. Change
the localhost here.
78:45 - And there is one more annotation
that we are using here for front
78:49 - end to actually be accessible,
78:51 - which is transparent proxy
annotation set to false.
78:56 - So transparent proxy
or transparent proxy true
79:00 - is a feature of Consul
that makes it possible
79:05 - for services to communicate
with each
79:07 - other through those proxies,
79:09 - without being aware that those
proxies are actually there.
79:13 - So they are thinking
that they're sending the request
79:16 - to the service,
79:18 - and proxies are capturing
those requests in the middle,
79:20 - but services are unaware
of that.
79:22 - And with transparent proxy set
to false, basically,
79:26 - we are saying that the service
needs to be aware of the proxy,
79:31 - and it has to explicitly send
the traffic to the proxy
79:34 - and route the traffic
through it.
79:36 - And that's our slightly
modified configuration.
79:40 - And I'm going to apply.
79:43 - This config Consul
file to deploy our
79:47 - microservices so CTL apply.
Config.
79:55 - Consul and let's see.
80:01 - And let's give it a couple
of seconds for the pods
80:03 - to come up. And. Let's do
kubectl get pod.
80:10 - And there you go.
80:17 - So let's see what we have here.
80:19 - These are all the pods
from the microservices.
80:22 - And you probably notice that for
each service or each
80:26 - pod of the service,
80:28 - we have two out of two
containers running
80:31 - inside instead of one.
80:32 - And that second container
is basically the injected proxy.
80:37 - And we can just log one of the
service containers to see
80:40 - what the proxy is doing.
So I'm going to do kubectl.
80:44 - Logs and let's just
take the edX service.
80:47 - So I'm going to do this.
80:50 - So basically when you have two
containers it takes a default
80:53 - which is the main container.
80:54 - So this is
the microservice itself.
80:57 - However we want to log
the proxy container logs.
81:00 - And this is actually
the Consul data plane.
81:04 - So this is the proxy that was
injected in the pod.
81:07 - And we also have
an init container.
81:10 - So you need container
as you know already
81:13 - from Kubernetes concepts
is basically a container
81:15 - that starts up before
the standard
81:18 - containers actually run.
81:20 - So it's kind of preparing
the environment before
81:23 - the rest of the actual
containers will run
81:26 - and init container exits
once it's done its job.
81:30 - And then the other containers
in the pod will
81:32 - run and the init container.
So this is the Consul process.
81:36 - Basically it prepares
the environments for the proxy.
81:40 - So remember I told
you that proxy needs
81:43 - information about what other
services are there.
81:45 - So that if its host service
wants to talk to other services,
81:49 - it knows how to reach them.
81:51 - It also gets the TLS certificate
for the secure
81:55 - encrypted communication.
81:56 - So all of that is actually
handled by the init
82:00 - container that injects all
this information
82:03 - into the pod so that the proxy
has access to them.
82:07 - So let's actually log the proxy
container and what it does.
82:11 - And we're just going to provide
the container name like this.
82:14 - And. There you go.
As I mentioned,
82:18 - Consul's data plane
is an envoy container.
82:21 - That's the technology behind it.
82:23 - So we're seeing the envoy logs
and the proxy
82:27 - basically on the startup.
82:29 - What it's doing is it tries
to find the service
82:31 - associated with the pod.
So for the edge service,
82:35 - for example,
82:36 - it will find
the associated service
82:38 - and it will register
it with Consul so other
82:41 - proxies can talk to it. Awesome.
82:44 - That means if it doesn't find
any
82:46 - related or associated service,
82:48 - it will actually give
you an error
82:50 - that it couldn't find a service
associated with that pod.
82:53 - And this now means that all
those proxies actually did
82:56 - the work of registering these
microservice pods with Consul.
83:02 - So this means if we go
back to the Consul UI,
83:07 - we're going to see all
the services listed here.
83:10 - Because now Consul knows
about them.
83:12 - And for each service
it also shows you how
83:15 - Consul is aware
of those services.
83:17 - And in our case they have
been registered with proxies.
83:21 - And we actually have a pod
in our cluster that is crashing.
83:26 - So the proxy is up.
83:28 - However the service itself
is not able to start up.
83:32 - That's why we have just one
container which is the proxy
83:35 - up and running. And we see
that here as well.
83:38 - Basically it's failing
the health check which means
83:41 - it's not accessible.
83:42 - This means that all the other
proxies will actually know
83:46 - about the issue of the service,
83:47 - without even sending a request
to it.
83:54 - And we can click
83:55 - inside the service.
83:57 - And what this basically displays
is not which service
84:01 - talks to which other service,
84:02 - because payment service
is obviously not
84:04 - talking to all of them,
84:05 - but rather which service
is allowed to talk
84:09 - to which service.
84:10 - And right now we have no
rules in the cluster
84:13 - that limit any service to talk
to any other service.
84:17 - And that's why everything
is allowed. However,
84:20 - we can change that.
So for example,
84:21 - if we go back
to our configuration.
84:24 - And I'm going to go
to the payment service.
84:26 - You actually see that the
payment service
84:29 - is not initiating communication
with any
84:32 - other service.
84:33 - So there is no service
that payment
84:35 - service directly talks to.
However,
84:38 - there are services that send
the request or initiate
84:41 - request to the payment service,
which is the checkout service.
84:45 - So this is actually the only
service actually
84:47 - that talks to the payment
service that initiates
84:51 - the request.
84:52 - That means all of these other
connections here
84:55 - that we are allowing
are actually not necessary.
84:58 - So by limiting those
connections and basically
85:01 - saying payment service
should not talk to anything
85:04 - other than the checkout service,
85:06 - we are reducing the attack
surface in our cluster.
85:09 - So that means if there was a bug
in the payment service,
85:13 - like a huge
security vulnerability,
85:15 - we would actually limit
the damage that someone
85:18 - can do by exploiting
the vulnerability
85:21 - in the cluster,
85:22 - because we're limiting
what payment service can do
85:25 - within the cluster
and who it can
85:27 - communicate to and talk to.
85:28 - And we're going to use
the concept of intentions here.
85:32 - So this is the micro
network segmentation
85:34 - that I already explained,
85:36 - in which Consul basically
allows us to define
85:40 - firewall configurations
in a granular way
85:42 - on a service level.
85:44 - And that feature is called
Intentions in Consul.
85:47 - And there is an intention
CRD file that you can create
85:50 - as a Kubernetes manifest file,
85:51 - which is pretty simple
to configure.
85:54 - And we're going to create
an intention where we're going
85:57 - to say that the checkout service
is going to be able to initiate
86:04 - communication to the
payment service,
86:07 - which is what we have
right here,
86:10 - because checkout Service will be
talking to the payment service,
86:13 - not vice versa. And that's it.
86:17 - No other service is allowed
to do that.
86:19 - And let's create that.
86:21 - And we can create another
intention that says all
86:25 - the services are denied to talk
to the payment service.
86:30 - So basically by default
we disallow any communication
86:34 - and we only allow
it for checkout
86:36 - service as an exception.
86:38 - If I go back you see that this
diagram has changed.
86:42 - And we see checkout service
is the only one
86:44 - that can talk to it.
And going back,
86:47 - we can also create another
rule where we can say
86:50 - the payment service itself
to all other
86:53 - services is also denied.
So let's save this.
86:57 - Go back to topology.
And there you go.
87:00 - And as you see,
87:02 - this is on a specific
service level. However,
87:04 - you can do this for your entire
microservices application group.
87:08 - And we even have a warning here
that basically tells us
87:11 - to configure that for all
the services.
87:13 - And you can do that here
directly as well. But as I said,
87:16 - if you want to automate this,
if you want to have that again,
87:19 - configuration as code,
87:21 - which is the recommended
way of working,
87:24 - you would create the Crds
for Consul called
87:29 - service intentions.
87:30 - So we have successfully
deployed Consul
87:33 - in the EKS cluster
and configured it to inject
87:38 - proxies into all
the microservices,
87:40 - plus the ready service
that we're using,
87:42 - which is a third party
service obviously.
87:44 - So it lets us have the proxy
application in any service,
87:49 - whether it's our own
third party or whatever,
87:53 - to have that consistency
in the communication between
87:56 - the services which is
actually great
87:58 - because we can apply
the same kind of rules
88:01 - on third party applications
as we can on our
88:04 - own application,
88:05 - so we can decide which services
can talk
88:07 - to the database
using intentions,
88:09 - and we can encrypt
the connection
88:11 - with the database
the same way as we do
88:13 - within our own applications.
Now, as a next step,
88:17 - we're going to repeat
the same exact deployment
88:21 - in another Kubernetes
cluster on another
88:24 - cloud platform.
88:25 - So we're going to recreate
the same exact
88:27 - state in another cluster.
88:29 - And then we're going
to connect those two
88:32 - to simulate a failover.
88:34 - When a service here fails
that the same service
88:38 - in another cluster
on another cloud
88:40 - platform can take over its job.
88:43 - So we're going to deploy
the same
88:45 - application in an Elk cluster.
88:49 - So I'm going to go ahead and log
in into my linode account.
88:57 - So if you don't have
88:59 - a linode account yet,
you can sign up. But also,
89:01 - as I mentioned previously,
89:02 - you can actually create
this cluster wherever you want.
89:05 - The concept will
be exactly the same,
89:07 - so it's not anything
that is ECS or specific.
89:12 - This could work with any
two Kubernetes clusters.
89:15 - I personally like linode
because it makes the cluster
89:18 - creation super fast
and super easy.
89:21 - That's why use it.
89:22 - But you can use
whatever you want.
89:24 - So going to Kubernetes I'm
going to create a cluster I'm
89:27 - going to call these Elk Consul.
Not very creative.
89:32 - And I'm going to create these
also in the Frankfurt region,
89:34 - but it could be in a different
region as well.
89:37 - It really doesn't matter.
89:38 - Just choose the latest
Kubernetes version.
89:40 - I'm going to choose no Aicha
for the control plane,
89:43 - because we just
need a demo here.
89:46 - And let's choose the cheaper
linode machines
89:50 - by switching to the shared CPU.
89:52 - And let's take the four
gigabyte sized VMs,
89:57 - and let's choose two nodes.
Let's confirm that.
90:02 - Create a cluster.
90:04 - And this should be up
and running pretty fast.
90:09 - So let's wait
for the provisioning
90:12 - of those worker nodes.
90:13 - Our two nodes are running
and we can now
90:16 - download the cube config
file to access our cluster.
90:20 - So I'm going to go back
to my Visual Studio Code
90:24 - and open a new terminal.
90:27 - And let's go
into the Kubernetes.
90:31 - Folder again,
90:33 - and we're going to have
to export that cube config
90:35 - file to connect to the cluster.
90:38 - So right here in this
execution environment
90:41 - basically we have executed
the AWS command to add
90:46 - the cube config file
in a default location
90:48 - where kubectl will look for it.
90:49 - So wherever I execute kubectl
now in my local environment
90:54 - it will connect to the eks
cluster because that's set
90:58 - in the dot cube
default location.
91:01 - So basically we're going
to configure this specific
91:04 - terminal session or this
environment to point
91:08 - to the cube config file of LCC.
91:10 - And then when we execute
kubectl commands here
91:12 - specifically it's going
to connect to the LCC.
91:15 - And that's very easy.
91:16 - We're just going to export
an environment
91:18 - variable called cube config.
91:21 - But again kubectl will
pick up on and we're
91:25 - going to set its location
to wherever that downloaded.
91:30 - Alki kube config file is that's
what it's called.
91:34 - And there you go.
91:36 - So now if I do kubectl
get node this should point
91:41 - me to those nodes. Very simple.
91:45 - So basically kubectl will
first check is kube config
91:49 - environment variable set
and pointing to a specific
91:53 - kube config file.
91:54 - If it is then that's the cluster
it connects to.
91:56 - If the environment
variable is not said,
91:58 - it's just going to look
in the default location
92:01 - dot kube and try to find
the cube config file there.
92:04 - And for security reasons
we would limit
92:08 - the permissions on this file.
92:09 - So right now it's readable
not only for the owner but also
92:13 - for group and any other user.
So I'm going to do change mode.
92:20 - We can set it to 700.
92:21 - So basically remove any
permissions from the file
92:24 - to anyone other than the owner.
And that's it. Awesome.
92:28 - So we are connected
to the cluster,
92:30 - which means we're going
to repeat the same exact steps.
92:33 - Install or deploy the Consul
helm chart inside the cluster
92:37 - and deploy our microservices
application
92:40 - with the annotations.
Let's do it.
92:42 - And this is going
to be a command here.
92:44 - We're calling this helm
installation
92:48 - of the Consul chart. We're
using the same version.
92:54 - The same values file.
92:56 - And this time we are setting
the data center to l k value.
93:01 - So calling the cluster
where Consul will run.
93:07 - So let's execute this and
let's check.
93:14 - Everything that was created.
We have our deployments.
93:19 - The four pods starting up
the same exact thing is in ECS.
93:25 - What Consul also deploys along
which I briefly
93:29 - mentioned are the Crds.
93:31 - So we can also check those
get CRD.
93:36 - And there are a bunch of kids
here that are
93:40 - from the LCC itself, so we can
actually filter. And.
93:46 - Let's find only the things
that have Consul in it.
93:48 - So basically you have
those service intentions
93:53 - that I showed you and so on.
93:55 - So it actually creates a bunch
of crds in the cluster.
93:59 - So you can configure Consul
and different components
94:02 - of Consul in the Kubernetes
native way with manifest files.
94:06 - And you can find
the configuration of all
94:09 - the Crds in their official
documentation as well.
94:12 - So let's give it some
time to start up.
94:15 - Let's see if the pods are ready.
There you go.
94:17 - And now we can deploy.
The microservices application.
94:24 - Apply config Consul and enter.
94:35 - In the same way,
94:37 - we see that two containers
are starting up
94:40 - in every part of microservices,
94:42 - which means the proxy
containers were injected
94:45 - in each service. And while
this is starting up,
94:49 - let's actually. Check
the services.
94:56 - Because with
the same configuration,
94:58 - we have the Consul UI
for this cluster
95:01 - as well as the front end
service for this cluster.
95:05 - And as I told you,
95:06 - it doesn't really matter
which cloud platform
95:09 - you use because the concepts
are really similar.
95:11 - So the same way is on AWS.
95:14 - The load balancer component
in Kubernetes actually links
95:19 - to the cloud native
load balancer in linode
95:23 - called node balancer.
95:25 - So this one right
here was created.
95:27 - That's the IP address
the external IP address.
95:30 - And we see that here.
95:32 - This is for the frontend
on port 80.
95:36 - And then we have the Consul UI
which is accessible with Https
95:42 - on this endpoint.
This one right here.
95:45 - So exactly same configuration,
95:48 - which means we can actually
access this and see
95:51 - the Consul UI in LKY.
95:57 - And as you see,
95:59 - that's what we called
the data center.
96:02 - That's the name we gave
to the cluster environment
96:05 - when we deployed Consul.
96:07 - So it says okay here and here
it says x. So it's going
96:11 - to make it a little bit easier
for us to differentiate
96:15 - when we do stuff
to connect those to.
96:18 - And as you see all the services
are listed here.
96:22 - And all those parts
96:24 - except for the payment
96:25 - service have
96:26 - successfully started. Awesome.
96:32 - Now we have basically recreated
96:35 - the same exact environment
96:36 - in a different cluster,
96:39 - on a different cloud
platform that can allow
96:41 - us to now have a failover.
96:43 - So if something happens
to the cluster,
96:45 - we can always fall back to the
cluster. However,
96:50 - we need to first establish
that connection
96:52 - because obviously now these
are two separate clusters.
96:55 - So we need to connect them
so that the services
96:59 - in EKS cluster can communicate
with services
97:02 - in cluster.
97:04 - And for that we are going
to create
97:06 - what's called a peer connection.
97:08 - So we're going to make
those two clusters peers.
97:11 - And since both of them have
full Consul deployments
97:15 - inside on the Consul level,
97:17 - we are going to connect
them so that services
97:19 - inside those two clusters can
communicate with each other.
97:22 - And it is actually pretty easy
to do. Again,
97:26 - there is an option to do
this using the CRD components.
97:31 - for this demonstration we're
going to use a more visual
97:34 - and simpler approach of Consul
UI to establish
97:40 - that peer connection. And in
the Consul values file,
97:43 - remember we enable
the mesh gateways.
97:45 - These are the components
that are actually going
97:47 - to help us connect those two
clusters together.
97:50 - So the mesh gateway in EKS
cluster will connect to the mesh
97:54 - gateway in cluster.
97:56 - So those two are the
connection links.
97:59 - And when we have Consul
clusters on different
98:02 - networks like we do here,
completely different networks,
98:05 - different cloud platforms,
98:06 - then we're going to need
to set the Consul
98:09 - servers or the control plane,
98:11 - basically to use mesh gateways
to request or accept
98:16 - the peering connection.
98:17 - So on both sides we're going
to actually configure Consul
98:22 - to use the mesh gateway
component to send or accept
98:27 - the peering connection
from the other cluster.
98:29 - And we can do that with one
of the crds called mesh.
98:33 - So this one here,
98:35 - and we're going to have to apply
that on both
98:38 - Consul deployments.
So going back to my code.
98:43 - I actually have
that configuration file
98:46 - already prepared right here.
98:48 - And you see how simple
it looks like.
98:50 - This is the CRD from HashiCorp.
By the way,
98:53 - make sure to check
the latest version
98:56 - in the documentation
if you are watching
98:58 - this a little bit later.
98:59 - So we're creating the mesh
component with a specification
99:04 - that enables or basically
tells Consul to use
99:08 - the mesh gateways for the
peering connection.
99:10 - And we're going to apply
this on both clusters
99:14 - which means I'm going to do
kubectl apply. Consul.
99:21 - Mesh gateway right here.
99:25 - And I'm going to switch to and
apply it here as well.
99:29 - There you go.
99:30 - We can also check that the CD
was created.
99:38 - And as you see,
99:40 - we have this mesh component
in the cluster, which,
99:44 - as I said,
99:45 - will allow to route
the peering traffic
99:48 - through the mesh gateways.
99:50 - And now we are actually
ready to pair
99:53 - those two clusters.
99:54 - So first going to the ECS
Consul deployment.
99:59 - So that's our main data center.
So to say our main cluster.
100:03 - Not technically but just
theoretically for us.
100:06 - And we're going to go
to the peers section.
100:09 - And we're going to add
a peer connection.
100:11 - And we can give the peer a name.
I'm going to use LCM.
100:16 - That's going to be the peer.
100:18 - So that's the name that the peer
will be represented by.
100:20 - And we're going to click
on Generate Token.
100:22 - And this is basically a secure
token that will allow the other
100:28 - peer to connect to this one.
100:30 - So I'm going to copy
this and let's close.
100:33 - And as you see this is
the peer name.
100:35 - And it's pending
because the appear
100:37 - basically has to make
a connection as well
100:40 - using that token.
100:41 - So we're going to go to the now
the peers at Peer Connection.
100:46 - And now instead of generate
token we're going
100:48 - to do establish peering
again name of the peer.
100:52 - We're going to call the other
peer X and the token
100:56 - that I just copied at peer.
As you see, super simple.
101:01 - And here we have this status
as well as the health check.
101:06 - Is the peer accessible or not?
101:08 - And if I switch back this one
is active as well.
101:12 - Very simple and straightforward
as you see.
101:14 - Now as the next step we're
going to add an exported
101:18 - service configuration
in the LCK.
101:21 - So basically we're going to use
an example
101:25 - of one specific service.
101:27 - And we're going to expose
that service or export
101:31 - the service from this pier
to make it accessible
101:34 - for the EKS cluster.
101:37 - That means the services here
in the cluster will be able
101:41 - to talk to that
exported service.
101:44 - And we're going to use
an example of the shipping
101:47 - service actually,
101:48 - which means we're going
to export this shipping
101:52 - service from the cluster to make
it accessible from ECS services.
101:57 - And I also have a configuration
for that.
102:00 - Let's go back right here.
I have the exported service.
102:03 - And as you see the configuration
is also pretty simple.
102:06 - We have this CD called
Exported Services.
102:09 - This is the name of the service
that we are exporting.
102:12 - And this is the name
of the peer that is going
102:15 - to consume the service.
Pretty straightforward.
102:21 - So basically just
102:23 - to demonstrate how this is going
102:24 - to work right now, if the
shipping service failed,
102:28 - a specific feature related
to that service
102:30 - will not work anymore
because the pod
102:33 - is not available, the service
is not available.
102:36 - So let's click in one
of the items.
102:39 - And if I click on Add to Cart,
as you see,
102:42 - everything works
because shipping
102:43 - service is up and running.
Let's go back.
102:47 - I'm actually going to. Yeah,
102:49 - I'm going to switch back
to the EKS cluster and I'm
102:54 - going to delete deployment.
Let me check the name.
103:04 - Shipping.
103:07 - Service.
103:11 - There you go.
103:13 - The shipping service pod
103:15 - should be gone and should be
103:21 - gone from here as well.
103:22 - And now let's actually try
to access the same
103:26 - function again. And as you see,
103:29 - it's not working
because the shipping
103:31 - service is not available.
You get the error here as well.
103:35 - So what we're going to do
now is that if this happens,
103:39 - like some of the services
in this cluster basically
103:43 - crash and they're not available,
103:45 - we're going to direct or we're
going to forward the traffic
103:49 - to the peer cluster that has
the same service.
103:53 - So the shipping service
of the cluster will
103:56 - basically take over instead
of that deleted
104:00 - or crashed shipping service
that was running here.
104:03 - So that's what we want
to achieve.
104:05 - So I'm going to bring up
the shipping service deployment.
104:08 - Again I'm just going to apply.
The config again.
104:16 - Like this.
104:19 - There you go. It works again.
104:22 - And now let's configure
that failover.
104:24 - So in the LCK.
104:26 - So we have this exported
service for shipping service.
104:29 - And we're going to. Apply this.
104:38 - And if I switch
104:40 - back to my Consul.
UI for deployment.
104:46 - You see that we now have one
exported service,
104:49 - which is this shipping
service basically.
104:51 - It also shows the topology
of the connections and the same
104:55 - way in each cluster.
104:57 - It shows that as an imported
service from that cluster.
105:01 - So now if we go back to the list
of all the services,
105:05 - so we have all those other
services through proxy,
105:09 - and we have this one
here that shows
105:12 - that the service is actually
coming from the peer connection.
105:16 - So we have two shipping
services available
105:19 - for this cluster. Now.
105:20 - Now there's one more thing
that we need to do from the case
105:24 - side to create what's called
a service resolver.
105:28 - And again service resolver
is its own CRD.
105:33 - And this is the configuration
for the service
105:37 - resolver component.
105:39 - So basically we're configuring
the service
105:41 - resolver for the shipping
service since we have
105:43 - two shipping services now.
105:47 - Right here.
105:48 - And we're saying that we're
going to use the failover
105:52 - to the peers shipping
service service.
105:58 - So we are going to need
to apply the service
106:00 - resolver in the EKS cluster,
106:03 - because we have those two
instances of shipping
106:05 - service in the EKS cluster.
106:07 - And we are defining a service
resolver for them,
106:11 - saying that this should be
basically a failover.
106:13 - So switching back to EKS I'm
going to apply.
106:21 - The service resolver.
Let's do that. Create it.
106:25 - And now, the moment of truth.
106:27 - I'm going to delete
the shipping service
106:29 - deployment again in the cluster.
106:32 - And that add to cart feature
should still be working.
106:37 - So let's do that again
in the EKS cluster.
106:41 - I'm going to.
106:43 - Delete the deployment
shipping service again.
106:48 - Switch back as you see
that one is gone.
106:51 - And now let's actually
refresh again just in case.
106:56 - Going to any product. And if
I click on Add to Cart,
106:59 - it should work by failing
over to this imported service.
107:06 - And let's do that. Awesome.
As you see,
107:09 - it used the service from a pure
cluster as a failover. Awesome.
107:15 - So that was basically our demo.
107:17 - I hope I was able to give
you lots of new insights
107:21 - and lots of new knowledge
about service
107:24 - mesh technology generally,
107:25 - as well as what concepts
and small
107:27 - details are involved
in all of this.
107:30 - If you made it till
the end of the video,
107:32 - congratulations on gaining
a lot of valuable insights
107:36 - and knowledge in this area.
107:38 - We put a lot of work and effort
in creating this video,
107:41 - so I will absolutely appreciate.
If you like this video,
107:44 - leave a comment with your
feedback and even share
107:48 - with your colleagues or anyone
who you think will benefit
107:50 - from learning these concepts.
And with that,
107:53 - thank you for watching
till the end and see
107:56 - you in the next video.

Cleaned transcript:

If you want to understand what a service mesh is and learn one of its implementations, Consul as well as understand why this concept is so popular in cloud and DevOps space. And get your first hands on practice with it, then this crash course is exactly for you. You definitely want to stick around till the end, because this is going to be a value packed, super exciting crash course with lots of interesting concepts. First, we will see why we even need a service mesh technology like Consul and what it does exactly. We'll then see its practical use cases, including how it's used in multi data center and multicluster environments. We will understand Consul architecture so how it works and how it does all that. And finally we'll see a really interesting demo use case of deploying microservices application across two Kubernetes clusters on two different cloud platforms and configuring the connectivity of these services across different environments using Consul. And in case this sounds like complex topics and use cases, remember you are on Tech World with nano channel, so you can be sure that I will break down all the complex topics into a simple and easy to understand examples and explanations. So let's get into it. Let's say we have an ecommerce application like Amazon, which is a complex microservices application. It has services for various functionalities like product catalog to manage product information, pricing, product images, etcetera. We have a shopping cart service that allows adding products, removing them, maybe saving for later. We also have order management service to handle all the orders. We have user authentication and authorization services, obviously to manage the registration, user login and so on. You know, reviews and rating service, recommendation service. And let's say it also integrates with bunch of supplier APIs and allows others to set up their own shops or a payment gateway to integrate with external payment processors. So a bunch of stuff is going on where this huge application logic is broken down into microservices. And if you don't know what microservices are exactly and how they are designed, I actually have a separate video on that which I will link in here. And of course these microservices are interconnected. They need to talk to each other like when product is added to shopping cart product service needs to update the stock information on how many products are left. Shopping cart needs to talk to the payment service or user account. The user authentication service will talk to other services that require user authentication like verify user identity, doing order placement, or payment. For example, recommendation engine service will communicate with User authentication Service to personalize recommendations based on user preferences, as well as talk to product catalog service to fetch product details for recommended items. So as you see, it is a complex network of services that all need to talk to each other without issues to make sure that the entire application works properly and the user experience is smooth. And while moving from monolith to microservices, architecture introduced a lot of flexibility in developing and scaling such complex applications. One of the main challenges it introduced was the connectivity between those services. In monolith application is just one application and one code base, so it's all function calls between different parts of the application. But in microservices, you have multiple isolated micro applications, which introduces a couple of questions and challenges like how do they talk to each other, on which endpoints, what communication channel do they use? Do they send Http requests to each other? Do they use message broker synchronous or asynchronous communication? What about the communication bottlenecks? When one service is completely overwhelmed with requests from all other services? How to deal with a situation where one service is down and not responsive to other services? Like what happens if shopping cart service is down but other services depend on it to do their jobs and it's just not responding? And how do we even monitor such application? How do we know which services are up and running, which ones are having issues, which services are overloaded with requests or not responding or just slow in their response? So all of these are challenges that microservices infrastructure introduced. Now let's say we have our microservice application deployed within a Kubernetes cluster in ECS in one of the US regions. Let's say we are a US based company and we have mostly American users, but we grow and become super popular in Europe and Asia. So now we need to deploy our application in those regions as well, geographically closer to our new users, to make their user experience better. So we need the instance of our application in other geographic regions to make sure that our application loads fast and people in those regions have good user experience. Now, this is another layer of complexity because now the question is how do we manage communication between services across multiple regions, in multiple data centers? That introduces a whole different level of challenges of operating your microservices application, like networking and connectivity between those regions, making sure those connections are also secure, making sure the data is in sync between regions and data centers. And you don't have data inconsistencies. Now, as if that wasn't enough, our microservice is using two different databases for different purposes, and those are managed centrally. By database engineers on separate VMs, because let's say those databases are used and shared by the whole company. It's a legacy system. Everything's interconnected. So it's not easy to move away from virtual machines and migrate the database to a Kubernetes cluster. Let's say it's a large database with more powerful machines and many replicas. So it would be an enormous effort to migrate all that to a Kubernetes environment. So microservice application has to communicate with database services running on virtual machines in on premise data centers. So we have a hybrid environment. And that is even more challenging than managing connections between services on two different Kubernetes clusters. However, it is a very common real use case among many companies and projects. So that's a real challenge. Now our story continues. Let's say one day right before the holidays, AWS has an outage in multiple regions around the world, and we lose lots of business in our ecommerce application. So management decided to make sure this never happens again, to have a failover to a different cloud provider, like a backup, in case the main cloud provider has issues. So they replicate the entire application on Google Kubernetes Engine, which is a Google's managed solution of Kubernetes, because the chances of both and Google Cloud going down at the same time is very low. And this is great for business in case of any such issues. But a new headache and challenges for the engineers. Again, connectivity across multiple cloud providers now security, network configuration and so on. So as you see, the operations of microservices, especially in the modern, highly complex environments is a challenge. And that's where the service mesh technology like Consul comes in as the communication channel or communication network layer between microservices. That solves many of the above challenges. Now that's a bit simplified definition of service mesh. But essentially service mesh like Consul, is the whole infrastructure layer that has different features to solve these challenges of operating microservices applications and enabling communication between them across multiple environments. Great. So now that we understand conceptually what Consul is and why we even need a service mesh technology, let's actually understand how it works and how it solves these challenges that I talked about. And the great thing is that most service mesh technologies are pretty similar in their functionalities. So understanding the concepts of how Consul works will make it much easier for you to understand any other service mesh technology as well. So I'm a big fan of concepts before technologies concept to understand what problem you are solving and what is the need. And technology is then a tool for solving that problem and fulfilling that need. So let's understand all that to understand the way a service mesh like Consul works. Let's imagine we have a city with different buildings and roads, and those apartment buildings have a bunch of residents in apartments, and those residents each do their tasks, and sometimes they need information from other residents to complete those. So they send messages to each other to communicate. And each resident has an own registry book, like an old address book with a list of all the residents they talk to and their addresses, where they can send the messages. The city is the Kubernetes cluster, buildings are the nodes, and the apartments are pods of each microservice, while the residents are the service containers within the pods. And that phonebook is like a configuration file for the applications in a container where they provide information of all service endpoints with service name and port number. So basically where they have a list of all the services they talk to. Okay. So that's what we are working with. No Consul no service mesh in our imaginary city yet. Now let's see how our city residents or pods operate without a mesh. If a resident moves to another building, all residents who were sending messages to her need to now update their address book with the new address. Otherwise, they will be sending the messages to the wrong address and wonder why they never get an answer back. And this would happen when a microservice or database service gets a new endpoint or service name or port changes. Now the city is managed centrally, like when someone is administering Kubernetes cluster. So the administrators want all communication data going through the services to be transparent and gathered in one place so they can identify if there are any issues in the communication between the residents and fix those issues to see maybe how secure the city is, how responsive the residents are to each other, and so on. So this residents need to keep a protocol and report the city monitoring service about their communications. Each and every one of them needs to do that. So making sure each resident does their job properly and consistently and while keeping up with the messages, they do all these other administrative tasks as well, which is overwhelming for a lot of residents. So they're working over time. I know this sounds like a weird city with surveillance system monitoring its residents and making them work 24 over seven, but we are in a Kubernetes city, so it's fine. And in Kubernetes cluster, this is equivalent to adding a monitoring endpoint in our applications to expose metrics. So we can scrape and collect those metrics in Prometheus, for example, or adding logic to each microservice for how to handle communication with other services when they don't respond or when they get an error reply, like how do they retry the request, and so on. And some residents might miss to track some data. Some of them will just track part of the data and not all of it. They may all write in different formats or not readable handwriting's, so the central service will not be able to fit those reports or that metadata about the communication itself together, because they're all in different formats. So essentially anything related to communicate. With other services, making sure the addresses and endpoints are up to date, proper handling of when they don't get a response back or if there are any communication issues, and so on. The residents are responsible about all these themselves. Now to optimize this city and release some workload from our residents and let them focus on their main tasks. As city administrators, we introduce a service mesh like Consul. Basically in every apartment for every resident, we add a personal assistant. This assistants or agents are now saying to the resident, I will send all those messages for you. In fact, you don't even need to know the exact addresses of other residents that you are talking to. You just write their name on the envelope and I will find out where they live, and I will deliver the message. And when they respond back, I will receive the incoming messages and forward them to you. I will also keep a protocol of any information that goes through me. So all these administrative tasks around sending the messages are taken care of by those agents or assistants, and residents can focus on their main activities and the actual contents of the message. In Consul this assistants are envoy sidecar proxy containers injected into the pod of each service. As you know from Kubernetes, we have a service container running in Pod, and we can run helper or sidecar containers that will run alongside to support that main service container in its job. So these envoy proxies will act as those assistants living in the pod along the service. And by the way, I have another video where I explain service mesh with example of Istio. I explain the same concept but from a different angle, so you can check it out as well. To get even better understanding and compare two different service mesh tools. Also, if you are new to these concepts like running microservices applications in a complex Kubernetes environment, I actually have a complete DevOps bootcamp where you can learn all these with practical hands on projects as well as separate course focused specifically on microservices with mono repo, poly repo structures, and building a CI CD pipeline for the microservice application on the git. And in our latest program about DevSecOps, you can also learn the security focused approach of working with containerized applications, Kubernetes cluster, automating security and compliance checks for applications, and also learn about the production grade deployment of microservices application with a service mesh in Kubernetes using the security best practices, along with tons of other concepts. So if you are at the point where you want to take your engineering career to the next level, definitely check out our courses and trainings to really dive in and learn these tools and practices properly. And all these for a fraction of the price of what engineers with this skill set earn in salary anywhere in the world. So how do these assistants do their job? Because now they need to have an address book and a way to keep the protocol of things, instead of each resident having its own address book. Assistants actually have a shared network with a shared address book, so each assistant will add the information about their resident or their service and how to talk to it in this central registry so other assistants can read that information as well. So when a new pod gets scheduled with a new microservice, it will get assigned the assistant automatically. So proxy will be automatically injected by service mesh and proxy will say to all the other proxies or the shared network. Hey, we are new here. Me and my service. And this is how you can contact me if you want to talk to the service that I'm assisting. And I will then forward your message to the service. Now, when any service wants to talk to any other service, they can say to their personal assistant. Hey, I want to send this request or message to this service called payment. Please deliver it. The proxy looks at the shared registry to find the location of the intended service based on its name or tags, and it will send the message to the services address, where the agent or assistant of that service will open the door and accept the message, and that agent will then deliver it to the actual service inside the apartment or pod. So essentially that means services don't need to know each other's endpoints at all. They have the assistance for that. So we free the individual services from having to even know this information and extract it completely into the service mesh. And when we have new tenants in new apartments and when old ones move out of the building or the city, agents update this information dynamically. So instead of a static configuration file with endpoints, we have what's called a dynamic service registry that is always kept up to date by those Consul agents. Now let's say a resident gets sick like they get a burnout from too much work, in which case their assistant will update the information in the registry and say my service is sick. They can't receive and reply to any messages for now, and I will let you know when they're healthy and responsive again. So now if we have pod replicas of the same service on same or different nodes represented by buildings, other proxies will know to talk to one of the other healthy replicas of that service and not send the traffic to the unhealthy replica by reading this health information from the shared registry. And again, all of this is handled just between the assistance services. Don't need to worry about handling any of this logic, or keeping up to date with which service replicas are healthy or not, and trying to retrieve these health information from somewhere. They're completely unaware of all this. Now, let's say while agents are carrying these messages back and forth between services in different buildings, some malicious actors like hackers managed to sneak into our city. So they entered our Kubernetes cluster or our infrastructure and got access to our network somehow. Now we have these malicious actors on the streets roaming around freely who want to sniff these messages being sent between services, especially if they contain private, sensitive data. Maybe they want to steal payment information or personal user data. Maybe they want to mess up the systems. So if they snatch the envelope from the agents, open it and read it, they will see all the information inside. So ideally we want to encrypt that communication between the services. So even if hackers managed to get into our system and network and they were able to see those messages, they can't understand anything because instead of plain text, it's all encrypted and only our agents can decrypt them because they have the decryption keys. So that's another feature. Service mesh offers encrypting end to end communication between services using mutual TLS without mesh. If you had 20 microservices and you wanted to implement encryption between them, you would have to change the application code in every single service to implement encrypting the data or receiving encrypted data. Terminating encryption. You would have to implement the management of the encryption keys and certificates to make sure that they are also securely created and stored. And it's a lot of work on the development side. Again, that extra administrative work around secure communication that has nothing to do with the business logic directly. Plus, mostly these are the things that developers are probably not the most knowledgeable in and not the best at implementing this stuff. You kind of need specialized knowledge to implement this with proper security. So the fact that you get these out of the box in service mesh is pretty powerful. And this is interesting, the microservice itself is still sending the traffic, which is unencrypted, but before it leaves the apartment or pod, it's captured by the proxy. The proxy has a TLS certificate with encryption key to encrypt the message. So when the request leaves the pod, it's fully encrypted. When that encrypted request reaches the target service or in Consul term, upstream dependency of that service, that services proxy will then receive the message. And we'll do that TLS termination before routing it to its host service, which basically means it will decrypt the message with its encryption key and pass the plain text message to the microservice within the pod. So now, even if someone infiltrates did Kubernetes network and was sniffing the traffic between the pods, they won't be able to read the messages because they are all encrypted. And again, the microservices themselves have no idea that all this encryption decryption is happening. From their perspective, they're just sending and receiving unencrypted messages. The biggest advantage of service mesh is that all that functionality is built in the mesh itself, which means it doesn't matter how your applications are programed or how other people's third party applications are programed over which you anyways have no control. You can still use all these like end to end encryption and error handling, etcetera with Consul without relying on the applications implementing this logic or having support for TLS, for example. And that is super powerful and helpful when you're operating complex, heterogeneous systems. Now, these end to end encryption or mutual TLS between services gives us one more thing. Since each service proxy gets its own individual certificate to establish secure connection with other services. This individual certificate can also be used to uniquely identify the service and validate its identity. So, for example, each resident or service gets their own unique stamp or certificate. So when they send the message, the assistant stamps the envelope with the stamp or encrypts the message with their key. So when the receiving agent or proxy gets the message, that agent can verify with the central registry, is this stamp real or fake? Was it tampered with and which service does it belong to? So we know this message really comes from the proxy of the payment service. For example, since it's signed by its certificate, we can now use this information to define rules about who can talk to who, like define whether payment service is allowed to talk to user authentication service and frontend service is not allowed to talk to the database service, for example. So after the identity is verified as a second step, proxies will check the communication rules. So this is really a payment service sending this message that's verified. But is it actually allowed to talk to my user authentication service. So is this resident allowed to talk to my resident or does it maybe have a restraining order if proxy sees oh, it's not supposed to be sending message to my service, then it can block the message and the connection so it won't be forwarded to the service at all. If the rule allows it, then everything is verified, so it will forward the decrypted message. This is also called micro network segmentation. So instead of having a firewall on the security group level or a subnet level, we have firewall on an individual service level, which gives us a more granular control of who can talk to our services on which ports. ET cetera. That's why the term micro network segmentation. Now remember, city wants to have protocols of who is talking to whom, especially when we limit those connections with strict rules. We want to see who is trying to break the rules and talk to the services that they are not supposed to be talking to, or generally which tenants are unhealthy maybe? Or who is sending and receiving? How much traffic? Are there any bottlenecks in the system? What is the error rate and what error responses are we getting from different services? Maybe a few services are getting too many requests and are overloaded and on the verge of a burnout and proxies by being located exactly in that traffic path where the data exchange is happening, automatically end up with rich telemetry data, which they can then expose to an external system like Datadog or Prometheus. And here's a great thing about proxies being the ones that collect and expose this data. Consul proxies are all the same service, which is envoy proxy. So when they collect and expose the metrics in different services, they all do it in the same way because it's the same application. So they collect and expose the same metrics in the same format across all services. So it's easy to put together metrics of all services and build unified dashboards from them in Prometheus and Grafana, which are the monitoring tools. So this architecture gives us immense power to change and control things in the network without having to do any changes in the applications, which means we are flexible to do whatever we want very fast, and configure things very fast in a unified way for all the services. Now you're probably thinking proxies have a shared address list of all other services. They have certificate data and they know who can talk to who based on the rules configuration that they all share access to. So the question is how do they get all this data? Or when a new proxy starts up, who provides it with all this information? And where is this shared database and storage of information and certificates? Where is it located? And that's where the Consul servers come in. With our analogy. Imagine these assistants all worked for the same company, and they had a headquarter office in the city in its own building, separate from the assistance. This office is the Consul server. You can have a single room in a building, like a single Consul server instance or a single pod replica. But if you are managing many services and their proxies, you might need a bigger office. So maybe 3 or 5 Consul server pods. So these Consul servers push out all the needed data to the proxies or Consul clients, like service registry information, the configuration certificates. So we don't have to do anything to get certificates in all this data to the proxies. All of this is done and managed automatically by the Consul servers. So we have basically automated operations of the mesh itself. And as I mentioned, those personal assistants have a network. They talk to each other exchanging information and so on. And that network of personal assistants is called data plane. And the central office or cluster of Consul servers that manage these assistants network is called control plane. This means the data plane is managed centrally by Consul servers or the control plane, so that they too can focus on doing their job of handling the communication between services and if something changes or gets updated, like the address of a service or new service gets added or removed, certificates get rotated, they will get the update from the central office automatically. It's like these proxies are all working for the same organization, having access to the centrally managed resources and data so they can all do their work easily. And this control plane leaves separately, maybe in the same city, which would be the same Kubernetes cluster. Or maybe they even have an office in a different city, which means you can spin up a dedicated Kubernetes cluster just for the Consul control plane, and then connect it to the Kubernetes cluster where the data plane is running. Now let's say we have multiple Kubernetes clusters with our microservices like in different geographic regions, maybe replicated on different cloud platforms even. And this is like having allied cities or city allies where cities form a network and decide, you know, let's form a partnership. So we will allow your services to talk to ours and vice versa. In this case, you can have Consul control plane in each cluster. So own control plane office in each city. Or again you may have one dedicated cluster or the main headquarter Consul control plane. And it will manage all other clusters data planes from there which is a common setup. This way you can avoid replicated offices and resources. For example in Consul is especially powerful in such multi cluster multi data center environments. Connecting services across different environments which can be a really big networking and security challenge if you're doing this without a service mesh tool. So how does this happen with Consul? Think of Consul planting guards at the exit and entry of the city in Kubernetes cluster. This guard is called a mesh gateway. So if payment service from cluster one wants to talk to user authentication service in cluster two, it will be the same process for the services where the payment service just says to its proxy, hey, send this message to the user service, please. I don't know where it's running. Also, I don't care. You will figure out how to deliver this message. Proxy will have the list of available services provided by the Consul server, including services in all allied cities where it says okay, this user authentication service lives in another city, so it could hand the message over to the city guard. The guard will take it to the other cluster and hand it over to the guard at the entry of that cluster, which is going to be the mesh gateway of the second cluster, which will then deliver it to the proxy of the user service inside the cluster. And finally it will be forwarded to the user service within the pod itself. And the response will flow the same way back to the payment service. And we're going to see an example of this specific use case in the demo part, where we will connect two Kubernetes clusters on two different cloud platforms with each other using Consul. And the good thing is, it doesn't matter which cloud platform you use. It pretty much works the same all the time. Now when we talk about multi data center environments, it's not just Kubernetes cluster. Many companies, especially large established companies, have tons of applications that run on legacy systems directly on VMs. Or they have a large company wide database that database engineers team is managing centrally that also run on VMs. And often that team already has a strong expertise of how to manage and operate those services on the virtual machines. So the overhead of learning Kubernetes and then learning how to migrate and operate the service on Kubernetes is often too large. Or if it's a small legacy application, maybe the overhead is just not worth it. So these are real use cases where companies still have services that will run on VMs and may not be migrated to Kubernetes or cloud anytime soon, but these companies and projects still want to take advantage of the modern tools like Kubernetes and containers. So the teams in that company deploy their microservices in Kubernetes cluster, which now has to connect to the database on the VMs or connect with other legacy backends still running on premise virtual machines. And if connecting multiple Kubernetes clusters is a challenge, try throwing VMs in that mix that becomes even larger. Challenge of how do we connect those networks? And again, service mesh tools make that easier by abstracting this low level network configuration and letting you manage that on a service mesh level. What's great with Consul specifically is that while other service mesh tools also have this capability, Consul actually treats the VM environment with the same importance or as a first class citizen, same as the Kubernetes environment, and doesn't treat it as an uninvited or undesired guest serving it just because it's there and it has to. So how does Consul work on VMs? If we use our analogy again, an on premise data center would be its own city with a bunch of private houses where each house is a VM, the application or service will be the only resident in the house and the Consul proxy and. Consul client will be living in that house along the resident, and we will have its own house for the Consul server as main office. You will then configure the communication channel so that Consul server running on the VM can connect to the Consul server in the Kubernetes cluster, so they can share information and create connection channel for their residents. So now again you have Mesh gateway in the Virtual Machine City as well. Who will communicate with Mesh Gateway in Kubernetes cluster. This way it can connect to other allied cities like other VMs or Kubernetes clusters. And once the trust and secure communication channel is established between them. Now the residents of both cities can talk to each other through that secure channel. So again, now with payment service in Kubernetes, cluster wants to talk to the database on VM. They go through the same exact process where payment just says to its proxy, hey, send this message to a database please. Proxy will have the list of available services provided by the Consul server, and it sees their database lives in another city. So through the mesh gateways the message will get transported all the way to the database running on VM in a different data center. So as you see, a service mesh like Consul is essentially the whole infrastructure layer that has different features to solve these challenges of operating microservices applications and enabling communication between them. So in this demo part we're going to create a Kubernetes cluster on AWS using ECS service. So that's going to be our very first step. Once we have the Kubernetes cluster we're going to deploy a microservices application with lots of services inside the cluster. And we're going to use an open source microservices application project from Google. So it's a little bit more realistic, like a more complex microservice. And not just two services for the demo. And once we have that all set up, we're going to deploy Consul on ECS, and we're going to see how the proxies will be injected in each one of those microservices. What configuration changes we're going to have to make to the microservices Kubernetes manifest files in order for Consul to work in Kubernetes, and also explore a couple of features of Consul and what it gives us out of the box. Once we have all of that set up, we're going to create another Kubernetes cluster on a different cloud platform. And we're going to use fairly simple Kubernetes managed service on Linode Cloud platform. I like using because it's super simple to spin up a cluster there to create it manually. It's just very little effort compared to ECS, and it's also very fast. So we're going to use that as a demonstration for another Kubernetes cluster. But it could really be any other Kubernetes cluster that you want. So the concepts are the same. And then in that linode Kubernetes managed cluster, we're going to deploy the same exact microservice and same exact Consul configuration. And once we have that we're going to connect those two clusters together using Consul. And we're going to see the demo of or simulation of a service going down or crashing inside the cluster. And it failing over to the same service inside the Elk cluster. So basically a multi cluster environment with a service failover to another Kubernetes cluster. So let's go ahead and do that step by step. Where along the way I'm going to explain lots of different concepts related to Consul service mesh. So the first step is we're going to create an EKS cluster. But of course we don't want to do that manually because it's a lot of effort and it's not the easiest thing to do. So we're going to use infrastructure as code using Terraform and all the code configuration files, the Terraform script, the microservices application, all of that will be linked in the video description. So you can clone those repositories and follow along. So this is one repository where I'm going to have all my Kubernetes manifest files that we're going to use to configure Consul and deploy our microservices application. So all of that is going to be here. And we have the Terraform folder inside with the Terraform script for creating the EKS cluster. And I have this repository cloned locally so that I can work on it using my code editor with terminal. So this is where we're going to be doing most of the work of configuring stuff. And I also have my account ready where we're going to be creating the Elastic Kubernetes Service. Right now we don't have any. So let's create one. And before we do, let's actually go through a little bit of the Terraform script and what we're doing here. It's pretty straightforward actually. I'm using the modules to make my job easier. So I'm using the VPC module to create a new VPC for the EKS cluster. Our EKS cluster will be publicly accessible. That's very important. And therefore we have the public subnet in addition to the private subnet in our VPC. And then I'm just using the X module to create the cluster obviously referencing this VPC. And we're basically configuring it with a bunch of parameters to configure our cluster. So first of all as I said I want my Kubernetes cluster to be accessible externally. So with this attribute we can actually create a public endpoint. Or we can let create a public endpoint for our Kubernetes API server. So we can connect to the cluster using kubectl for example or browser whatever from outside the VPC. Right. So I'm setting this to true. To achieve that the next attribute is adding some security group rules. And this is actually important for Consul to be able to do its job. And there are some specific ports that we need to open on the worker nodes themselves, where the Consul processes will be running in order to allow Consul components to talk to each other, and for the Kubernetes control plane components to reach Consul processes as well. And I'm actually going to reference to the list of ports that we need to open for Consul. So you see what they are and why those ports are needed. However, just to make things simpler and to make sure that you guys do not have any networking issues with Consul, and just to make sure that things go smoothly, what I'm going to do is, as you see in the Terraform configuration itself, I'm actually going to open all the ports on my worker nodes, and I want to stress that I'm actually doing it for the demo, because the security best practice is to have only those ports open that you actually need exposed, and only those internal or external processes that need access to whatever service is running on that port needs to have access to that port and nothing else. However, this is a demonstration, and I just want to make it easier for you guys to follow along and to make sure you don't have any networking problems when deploying Consul. And finally, this is the managed node groups. So these are the actual worker nodes or the worker node group configuration for the cluster. And here we're basically just choosing the small instances. And we're going to have three nodes or three of those instances in the cluster. And finally we have these two configuration pieces which are also needed for Consul deployment. So these are basically the configuration for the dynamic volume configuration because Consul is a stateful application. So it actually deploys a stateful set component and it needs to store and persist some data. So it needs to create the volumes on whatever platform it gets deployed. And in this case, we are making sure that creation of Amazon Elastic Block Storage is enabled for the cluster by giving permission to processes on these nodes to create the storage. And in addition to this role, we have to enable what's called an add on on EKS cluster, which allows for automatic provisioning of the storage. And once the cluster is created, I'm actually going to show you all this information so we can see that visually as well. Apart from that, we have variables that we are setting and I have added some default values for most of the variables, so you don't have to set them in the TF vars file. So these are basically just Cidr blocks for VPC private and public subnets. The Kubernetes version. That's very important to make sure to choose the one of the latest ones. This is the latest one as of now. So that's what I'm using the cluster name because we use that in a couple of places within the main configuration. So I extracted that as a variable region. You can set whatever region is close to you. And there are two pieces of information or variables that you have to set yourself to execute the script. Everything else is configured and set already. So before you are able to execute this Terraform script, make sure to go to your AWS account and for your user, create an access key pair and you have to set those values for the Terraform. Object. So I have defined them and referenced them right here in the provider configuration, which means I can just set those variable values in my Terraform dot vars file, which I have done already. And once you have that, you should be good to go and terraform. Tfrs is a simple text file with key value pairs. So you have the key name, which is. This one equals whatever your key id is in quotes. And same for the access key. So set those two values in the tf vars file and we are good to go. That's the provider configuration. That's the version that I'm using. And now we can actually execute this Terraform script to create the EKS cluster. So I'm going to switch to the Terraform folder and I'm going to do terraform init. So terraform init basically downloads any providers that are specified here just like you download dependencies of your code. For example in order to run your project. As you see it creates this dot terraform folder where the modules and providers will be downloaded. Those modules and this provider and everything is green, which means our Terraform has been initialized. We also have the Terraform log file. And now. We can execute Terraform apply. You can do Terraform plan for the preview. But I'm going to do terraform apply immediately. And we have to pass in the variables file that defines any missing variables. So. VAR file is terraform tf vars. And let's apply. And this is our preview. All the things that will be created. I don't need to look through that. I'm going to confirm, and this is going to take a couple of minutes to create everything, because lots of components and things are being created. And once the cluster has been created and fully initialized, we can continue from there. So the Terraform script was executed and it took some time, but it successfully executed. So now if I switch back to my AWS account in the region that you have basically set in the variables file, I chose the EU central region, which is closest to me. So this is my EKS cluster that was created in the Frankfurt region. And if we go inside and check out the detailed view, I'm going to show you a couple of things that we have configured in our Terraform script that we can see in the UI as well. So I want to point out a couple of configuration details. First of all we have the cluster configuration details. So we have the role the IAM role for the cluster which is this one right here, as well as security groups for the cluster like this. And then we have the configuration details on the node groups or the worker nodes themselves. So if I go to compute we're going to see the three nodes that were created because that is our configuration. We have defined three nodes. So these are basically the work node instances that are running in our account. So if we go to EC2 dashboard, we're going to see these three instances here. And we have the security group configuration on the worker node level, which is this one right here. And note that this security group additional rule actually applies to the worker nodes. So this is the same security group that all the nodes share. So it will be same for each worker node. And we have also configured this additional policy for the IAM role which also applies to the node groups. So now the cluster the EKS cluster role or the control plane role. But the node group role that apply to the worker nodes. And again we can see that right here in the instance configuration. This is the role. And we should see this Amazon EBS, CSI driver policy listed here. And I'm pointing this out because first of all, you need to understand that these two things are configured separately. You have the control plane configuration with EKS which is actually running in its own network. And then you have the worker node configuration with its own role on port's own firewall configuration and so on. So if you have any networking issues and so on, this should help you troubleshoot and know where to look for things basically. And finally last thing I want to show you is these EBS CSI driver. Add on that we activated on our cluster. And you are going to see that in the evidence tab for the cluster. And right here we have Amazon EBS CSI driver which basically is needed in order to automatically provision the elastic block storage for persistent volumes inside the cluster. So in our case Consul stateful set actually needs a persistent volume. So this allows the cluster to automatically provision the Amazon block storage for those volumes. Awesome. So the cluster is already active. So we can connect to it using kubectl and deploy our application inside. So I'm going to switch back to my code editor. And I'm actually going to use the terminal here. So we have everything in one place. And we don't need the Terraform script anymore because we executed the provisioning already. So now the next step is to actually connect to our EKS cluster. And we do that using AWS command line interface. So this is basically a secure way to retrieve a cube config file from the EKS cluster without exposing any credentials and without having to download this kube config file, and so on using a simple command, which means you have to have installed. If you don't, it's pretty easy. Just go ahead and install command line interface on whatever operating system you have. And once you have that, you need to also configure your CLI to use the access keys, which can be the same access keys that your Terraform is using for this demo use case, because command line interface will need the access credentials to connect to the AWS account. Right? And I have already configured all of that with AWS configure command. So just make sure that the default region and credentials configured here are for the same account as for Terraform, and you should be good to go. So. With that setup I'm going to execute AWS X command. Update. Kube config. And you can actually provide the region here as well for where the cluster is running. So central one and we are going to need the cluster name as well. And we have that here we call the cluster. This generic name. So I'm going to copy the cluster name. So basically what update kube config subcommand does is it fetches the cube config file which is like a credentials file for Kubernetes cluster from AWS. And it stores it locally into a default cube config location, where cube CTL will look for it and the location is on your user's home directory in dot cube folder. So after executing this command you should find cube config file in there. So let's execute. And there you go. You see the output that the cube config was. Or the context of the cluster was added in this location in dot cube slash config. So if you don't have the dot cube folder already, it will basically create one and add the cube config file configuration in there. Or if you already have one, it will just append to the existing config because you may be connected to multiple clusters. So all of those configuration will be right here. That's how it works for Kubernetes in general. So nothing is specific here. And that means we now should be able to connect to the cluster using kubectl command. So let's see. Kubectl get node. And there you go. We have our three work nodes with this Kubernetes version which we have defined here. Awesome. The first step is done as a next step. We want to deploy microservices application into this Kubernetes cluster. So for that I'm going to actually switch to the Kubernetes folder where I have my manifests. And I'm going to close this up and expand this. So these are all the config files we're going to need in this demo. But we're going to start with the simplest one. So that's all we need to deploy our microservices. So actually we don't need the repository of the microservice application itself. We just need a reference to the images. So this is a Kubernetes config file that references images of all those microservices. But of course I'm going to link the microservices repository in the video description as well. So this is an open source microservices demo repository from Google. And all those images are public which makes it easy to use it for demos. And this currently happens to be the latest version. If the version has changed, you can check in the provided link and you can just update the version basically. So super simple actually, we just have a bunch of deployments for each service. The configuration is pretty similar for each microservice. They just run on different ports and have different names. All of them have cluster IP services which are basically internal services. And we have one entry point microservice, which is the front end that will then route the traffic to all the other microservices. And here we see basically the frontend talks to all other services, and it is the only one that has an external service of type load balancer. And all those microservices also share a Redis memory database. Again, pretty simple setup. Nothing crazy here. So that means once we apply this configuration file, all the images will be downloaded, all the pods will be created, deployment services and so on. And we're going to have one entry point service to the cluster through load balancer. Now for simplicity I'm not going to deploy an ingress controller in the cluster. So we're just going to use the load balancer service directly to access our application, which is going to be enough for our demo. So let's go ahead and apply this config file. Config dot Yaml. That's all we have to do. And by the way, there is one service that is misconfigured slightly on purpose, which is a payment service. It basically is missing one environment variable. So it's not going to be able to successfully start in a pod, but we're going to need that to demo that later in Consul. So let's execute and see the result. And we have the output of all the stuff that was created. So we have quite a few microservices here, and it will need a little bit of time. And we're going to check kubectl get pod. So they will all be created in the default namespace. So if we do kubectl get pod we should see all our pods are up and running except for the payment service which is going to stay in the error state which is fine. We're going to use that as an example of an error in microservice to see it in Consul. Okay. So that was pretty easy. Now what we actually want to see what we deployed. So I'm going to do kubectl get services. And as you see all our services are cluster IP type except for the frontend external. That means if you have watched my other Kubernetes tutorials, you would know that load balancer external service gets the internal cluster IP, but also the external IP because we need to be able to access it externally. And this is the external domain name for the load balancer that will then map to the external IP. So where does it come from. That's also pretty easy as it works in Kubernetes. When you create a load balancer on whatever cloud platform, you are creating this load balancer service. It will in the background use that Cloud Platforms load balancer service to create the native load balancer there. So that's where the external IP comes from. And that means if I switch back to AWS and go to EC2 service. That's where we have the load balancers. We should see our load balancer for the front end external right here. And this is basically the DNS name that we see right here right ending in 138 configured on Http port forwarding to this port which is configured right here. So pretty simple. We just grab this DNS or external DNS name. And since port 80 is the default Http port it will just open it like this. And there you go. Our microservice is deployed. Now the next step is to actually deploy Consul in our cluster and use Consul for our microservices. So how do we deploy Consul. There are several ways to install Consul on Kubernetes cluster. One of them is using Consul's Kubernetes CLI and another one is using Consuls. Official helm chart for Kubernetes, which is what I'm going to use. So if we search for Consul, helm, chart and open the installation guide on their official documentation, always try to refer to the official documentation instead of some blog posts because they are most up to date. So for the latest version, these are the instructions. We basically add the HashiCorp helm repository. We install the Consul chart and provide any parameters we need. And if you have watched my helm chart videos, you know that helm charts are configurable so we can actually provide any parameters, any configuration options in order to configure the service using those parameters. Right. So for example, with Consul service mesh I mentioned it has multiple features. And depending on which features you actually need, you can enable them by configuring them as the chart values or passing them as chart values, and to see what values are available to be set and parameterized. Obviously we need to see the values yaml file. So we have a chart reference here. So that's basically the chart values reference documentation. I usually actually prefer the values stored file in the repository. So for example this one. But as you see the repository has been archived. So this documentation is up to date and something that you should reference. But even though it's an advantage to have the chart highly configurable so you can actually tweak it to whatever desired configuration you have in mind. If you don't know what you're doing, there's this huge list of values that you have to basically understand what they're doing. And it's pretty difficult to understand all these configuration. So we're going to use a couple of these configuration options. And I'm going to explain to you what they're actually doing. But if you need any additional configuration options you can find those listed here with descriptions. I don't think it's the most comprehensive and understandable, but at least you have some reference point. So let's switch back to the project. And I'm going to walk you through the values file that I have configured for our specific Consul installation. Which is actually pretty simple configuration. We have this global attribute which applies to all the components that are part of the chart. As you know, chart usually holds or is a bundle of multiple components of different Kubernetes native components as well as custom resource definitions and so on. So this applies to multiple components. First of all, we have the Consul image with the version. We are enabling TLS communication between the components and services. And since we want to connect multiple clusters or multi data center environments basically with each other, we are also enabling the peering. Then we have the Consul server configuration. As I explained, the Consul server is the control plane that manages all the proxies, the Consul client and so on. And usually in a production environment you want to have at least three replicas, not just one, because if one replica dies, you want to have a failover. In our case, we're just going to use one server for our demo purpose. So you can configure that here. This is an important configuration option. Connect inject is basically the name, the technical term of the service mesh functionality of Consul. And what this configures is basically if we enable connect inject it will allow Consul to automatically inject the proxies, those helper containers, as I mentioned, into the pods of services. So that's what enabled true does. And then we have the second configuration that says default false which is actually the default value. But I wanted to specifically configure this. So if this is set to false then you would need an extra annotation inside the Kubernetes manifest files or deployment manifest files in order to actually inject the proxy into that pod. If we set this to true as well, like this, then it will actually inject those proxies, even if we don't have the Consul annotations in the deployments. And this is a good thing to have control over, because it could be that you have pods running in your cluster, that you don't want to have any proxy services in them, or you need to, or maybe you have some namespaces that should not have any Consul proxies applied. So if you set it to false, you basically decide per deployment per application where you want the proxy injected. So I'm going to set it to true. So we can see how that works. And we actually want all our services to have the proxy injected. And we don't want to add annotations one by one on each deployment. So I'm going to set it to true. And that's the connect inject configuration. Then we have mesh gateway, which as I explained is basically a connector between multiple environments. So mesh gateway is like a guard that is standing at the entry or exit of the city with our analogy. And you can also have multiple replicas of the mesh gateway, because if one of them goes down or has an issue or maybe becomes a bottleneck because of the number of requests, you can actually scale up the replicas. And again, it's a feature that you can enable if you actually need to. So if you don't have a multi cluster environment or just for security purposes, you need to stay within the cluster, then of course you want enable it. And finally we have UI which gives us a Consul, dashboard or UI where we can see the services and where we can even configure stuff which we are actually going to be using in our use case and type load balancer basically means that it will create a service, an external load balancer type of service, so we can access the UI. And we are also enabling that. So that's the configuration that we want to apply to Consul in our cluster to one, or inject the proxies in our microservices pods. And second, allow us to connect to Kubernetes clusters with each other and also have a dashboard where we can see stuff and configure some things. So I'm going to go ahead and actually install the Consul helm chart with these values. And let's see what we get. So I'm going to copy the first command and let's add HashiCorp repository. There you go. And now we can actually install helm install. And we're going to give our Consul chart deployment a name. You can call it whatever you want. I'm actually going to call this X to basically differentiate it later from the Linode Kubernetes Engine deployment. So I'm going to go with that name. And then obviously we need the chart name. I'm going to copy it from here. And actually when components are created it prepend Consul in the name. So that's why I'm not using X Consul or Consul in the chart name itself. And I'm actually going to add some additional parameters which can also be configured in the values. But I'm going to set them separately because I'm going to use the same values file for the Consul deployment as well. So I'm going to pass in the version first of all. And we're going to use. Version one. And then of course, we have to pass the values file. Consul values dot yaml. That's our file. And finally the last configuration is I'm going to set. A global config. So one of those global attributes called data center. And that's basically the name. So you can name the environment where Consul is running. You can give it a name of a data center. And I'm also going to call this x. And that's basically it. So I'm going to execute this command. And this should install all the Consul components in our cluster. And this gets executed pretty quickly actually I'm going to do kubectl get pod. So first of all I'm going to check the pods. And now that I'm deploying the Consul components in the same default namespace. But you could also have them in a separate namespace, especially if you have multiple applications. That would make sense. Let's check again. So we have these four pods. The first one is server. Obviously we need the server or the control plane to manage the proxies to inject the proxies and so on. And the chart name was taken as the prefix as you see here for all those pods. Then we have the mesh gateway. One instance of it we have the connect injector that is the one that is responsible for injecting pods. And we have the webhook certification manager. And as you see all the pods are running successfully. You can also check any other components that were deployed. So we have this stateful set which is a server itself and those deployments and we have the Consul services themselves. And one of them is we saw is the Consul UI which is created as a load balancer service type. And it also gets its own load balancer component on AWS with its own DNS name. So we can use that to access the Consul UI. So let's go ahead and do that. And I'm going to make this a little bit broader. And one thing I want to point out here is that this external service of Consul UI is actually accessible at the Https port 443. That means we have to access the service using Http as protocol. Like this. And of course our browser doesn't know the certificate which is signed by Consul CA so we can say it's all fine, we allow it. And there you go. This is our Consul UI. And as you see we have only two services displayed here. So basically Consul now is aware only of two services. One of them is a mesh gateway which we deployed as part of Consul. And the other one is Consul server itself. That means another interesting point. The proxies have not been injected yet in any of the microservices because we need to restart or recreate those services or deployments. So that's what we're going to do next. So I'm actually going to delete our microservice deployment completely. And I'm going to redeploy it with a little bit adjusted configuration. So let's do config dot Yaml. There you go. And let's see. Looks good. Now, I actually have already prepared a config file with a couple of changes for Consul deployment specifically. So I'm going to open this file and let's go through it and understand those changes. So first of all in order to configure anything in our deployment that is Consul relevant. So most of the things that are actually relevant for how Consul will treat our deployments, or whether it will inject proxies or how it will handle the communication between services, etcetera, we can configure those using the annotations is, you know, annotations are part of metadata of Kubernetes components like deployments. And these are going to be the annotations on the pod level. So inside the template metadata in the annotations we can add the Consul annotations basically. So this will communicate our desired configuration of our microservice sees to Consul, which is a pretty easy way to manage that using the Kubernetes native way. So the first annotation, which is probably the most used one, most seen one is connect inject. True. So basically this specific configuration is not relevant for us because we set the connect inject default to true. Basically, as I said we can say we don't want auto injection in every part in every namespace. We want to be able to decide which pods actually get those proxies. And we decide that by adding this annotation to the pod metadata, right. So every part that we want to have proxy injected, we can add this annotation and it will take care of it. However, when this is set to true which is what we configured. And that's what Consul that is deployed in our cluster actually knows, we don't need that. So I'm just going to comment this out. Let's go to the next microservice. So we have the same connect inject. We don't need this. And this is another annotation called Connect Service up streams which is another core annotation which basically defines which services does this service talk to. And how are those services called. Remember I mentioned that once the proxy is there, the service inside the pod does not or should not care about where the destination services are located and what their addresses are. They can just hand over the request with the name of the service, and then proxy will figure out where that service actually runs in. Kubernetes services are already referenced in this easy way, using the service name and port instead of static IP addresses. So this is basically not a huge improvement in this case because Kubernetes already manages that. However, we do need to communicate to Consul which services this one will be talking to. So that's kind of the metadata definition. And when we have this configured there is one more change we're going to do here, which is there is an environment variable that points to the service that recommendations service is talking to. And in the previous configuration this was actually the Kubernetes service name and the port. And again going back to my previous explanation, the service does not need to know what the Kubernetes service name of that microservices that he talks to because it talks to its proxy. And the proxy will listen to it to this request on localhost because it's within the pod. So the containers within the pod communicate via localhost and the same port where the upstream service is configured. So this request will basically go to the proxy instead of the Kubernetes service. But proxy knows that whatever that request points to is located here, which is the Kubernetes service name. So as I said, it's not like a huge game changer here because Kubernetes already manages the service names pretty well, but that's basically it. So that's the annotation that I have configured in services. That's the only annotation we use here. So I'm going to scroll through and comment out all of those connect inject annotations just to demonstrate that we don't need them. Like this. So I basically just commented out that annotation, but I'm still going to leave it in the configuration file for your reference in case you want to use that as well. And then we have two last services that have a bunch of upstream services they talk to. And it's the same idea. You can just provide a list with service names and ports. And then all of those will be accessible on localhost on different ports. So basically the service the checkout service will always be talking to the proxy instead of talking to all those different services as it was doing before right here. It will now only talk to the proxy. And then proxy will forward all those requests to the respective services based on whichever port the proxy receives that request. And that means obviously those ports need to be different within the localhost and the same for front end. So front end is the last service which also has a bunch of upstream services exactly the same concept. Change the localhost here. And there is one more annotation that we are using here for front end to actually be accessible, which is transparent proxy annotation set to false. So transparent proxy or transparent proxy true is a feature of Consul that makes it possible for services to communicate with each other through those proxies, without being aware that those proxies are actually there. So they are thinking that they're sending the request to the service, and proxies are capturing those requests in the middle, but services are unaware of that. And with transparent proxy set to false, basically, we are saying that the service needs to be aware of the proxy, and it has to explicitly send the traffic to the proxy and route the traffic through it. And that's our slightly modified configuration. And I'm going to apply. This config Consul file to deploy our microservices so CTL apply. Config. Consul and let's see. And let's give it a couple of seconds for the pods to come up. And. Let's do kubectl get pod. And there you go. So let's see what we have here. These are all the pods from the microservices. And you probably notice that for each service or each pod of the service, we have two out of two containers running inside instead of one. And that second container is basically the injected proxy. And we can just log one of the service containers to see what the proxy is doing. So I'm going to do kubectl. Logs and let's just take the edX service. So I'm going to do this. So basically when you have two containers it takes a default which is the main container. So this is the microservice itself. However we want to log the proxy container logs. And this is actually the Consul data plane. So this is the proxy that was injected in the pod. And we also have an init container. So you need container as you know already from Kubernetes concepts is basically a container that starts up before the standard containers actually run. So it's kind of preparing the environment before the rest of the actual containers will run and init container exits once it's done its job. And then the other containers in the pod will run and the init container. So this is the Consul process. Basically it prepares the environments for the proxy. So remember I told you that proxy needs information about what other services are there. So that if its host service wants to talk to other services, it knows how to reach them. It also gets the TLS certificate for the secure encrypted communication. So all of that is actually handled by the init container that injects all this information into the pod so that the proxy has access to them. So let's actually log the proxy container and what it does. And we're just going to provide the container name like this. And. There you go. As I mentioned, Consul's data plane is an envoy container. That's the technology behind it. So we're seeing the envoy logs and the proxy basically on the startup. What it's doing is it tries to find the service associated with the pod. So for the edge service, for example, it will find the associated service and it will register it with Consul so other proxies can talk to it. Awesome. That means if it doesn't find any related or associated service, it will actually give you an error that it couldn't find a service associated with that pod. And this now means that all those proxies actually did the work of registering these microservice pods with Consul. So this means if we go back to the Consul UI, we're going to see all the services listed here. Because now Consul knows about them. And for each service it also shows you how Consul is aware of those services. And in our case they have been registered with proxies. And we actually have a pod in our cluster that is crashing. So the proxy is up. However the service itself is not able to start up. That's why we have just one container which is the proxy up and running. And we see that here as well. Basically it's failing the health check which means it's not accessible. This means that all the other proxies will actually know about the issue of the service, without even sending a request to it. And we can click inside the service. And what this basically displays is not which service talks to which other service, because payment service is obviously not talking to all of them, but rather which service is allowed to talk to which service. And right now we have no rules in the cluster that limit any service to talk to any other service. And that's why everything is allowed. However, we can change that. So for example, if we go back to our configuration. And I'm going to go to the payment service. You actually see that the payment service is not initiating communication with any other service. So there is no service that payment service directly talks to. However, there are services that send the request or initiate request to the payment service, which is the checkout service. So this is actually the only service actually that talks to the payment service that initiates the request. That means all of these other connections here that we are allowing are actually not necessary. So by limiting those connections and basically saying payment service should not talk to anything other than the checkout service, we are reducing the attack surface in our cluster. So that means if there was a bug in the payment service, like a huge security vulnerability, we would actually limit the damage that someone can do by exploiting the vulnerability in the cluster, because we're limiting what payment service can do within the cluster and who it can communicate to and talk to. And we're going to use the concept of intentions here. So this is the micro network segmentation that I already explained, in which Consul basically allows us to define firewall configurations in a granular way on a service level. And that feature is called Intentions in Consul. And there is an intention CRD file that you can create as a Kubernetes manifest file, which is pretty simple to configure. And we're going to create an intention where we're going to say that the checkout service is going to be able to initiate communication to the payment service, which is what we have right here, because checkout Service will be talking to the payment service, not vice versa. And that's it. No other service is allowed to do that. And let's create that. And we can create another intention that says all the services are denied to talk to the payment service. So basically by default we disallow any communication and we only allow it for checkout service as an exception. If I go back you see that this diagram has changed. And we see checkout service is the only one that can talk to it. And going back, we can also create another rule where we can say the payment service itself to all other services is also denied. So let's save this. Go back to topology. And there you go. And as you see, this is on a specific service level. However, you can do this for your entire microservices application group. And we even have a warning here that basically tells us to configure that for all the services. And you can do that here directly as well. But as I said, if you want to automate this, if you want to have that again, configuration as code, which is the recommended way of working, you would create the Crds for Consul called service intentions. So we have successfully deployed Consul in the EKS cluster and configured it to inject proxies into all the microservices, plus the ready service that we're using, which is a third party service obviously. So it lets us have the proxy application in any service, whether it's our own third party or whatever, to have that consistency in the communication between the services which is actually great because we can apply the same kind of rules on third party applications as we can on our own application, so we can decide which services can talk to the database using intentions, and we can encrypt the connection with the database the same way as we do within our own applications. Now, as a next step, we're going to repeat the same exact deployment in another Kubernetes cluster on another cloud platform. So we're going to recreate the same exact state in another cluster. And then we're going to connect those two to simulate a failover. When a service here fails that the same service in another cluster on another cloud platform can take over its job. So we're going to deploy the same application in an Elk cluster. So I'm going to go ahead and log in into my linode account. So if you don't have a linode account yet, you can sign up. But also, as I mentioned previously, you can actually create this cluster wherever you want. The concept will be exactly the same, so it's not anything that is ECS or specific. This could work with any two Kubernetes clusters. I personally like linode because it makes the cluster creation super fast and super easy. That's why use it. But you can use whatever you want. So going to Kubernetes I'm going to create a cluster I'm going to call these Elk Consul. Not very creative. And I'm going to create these also in the Frankfurt region, but it could be in a different region as well. It really doesn't matter. Just choose the latest Kubernetes version. I'm going to choose no Aicha for the control plane, because we just need a demo here. And let's choose the cheaper linode machines by switching to the shared CPU. And let's take the four gigabyte sized VMs, and let's choose two nodes. Let's confirm that. Create a cluster. And this should be up and running pretty fast. So let's wait for the provisioning of those worker nodes. Our two nodes are running and we can now download the cube config file to access our cluster. So I'm going to go back to my Visual Studio Code and open a new terminal. And let's go into the Kubernetes. Folder again, and we're going to have to export that cube config file to connect to the cluster. So right here in this execution environment basically we have executed the AWS command to add the cube config file in a default location where kubectl will look for it. So wherever I execute kubectl now in my local environment it will connect to the eks cluster because that's set in the dot cube default location. So basically we're going to configure this specific terminal session or this environment to point to the cube config file of LCC. And then when we execute kubectl commands here specifically it's going to connect to the LCC. And that's very easy. We're just going to export an environment variable called cube config. But again kubectl will pick up on and we're going to set its location to wherever that downloaded. Alki kube config file is that's what it's called. And there you go. So now if I do kubectl get node this should point me to those nodes. Very simple. So basically kubectl will first check is kube config environment variable set and pointing to a specific kube config file. If it is then that's the cluster it connects to. If the environment variable is not said, it's just going to look in the default location dot kube and try to find the cube config file there. And for security reasons we would limit the permissions on this file. So right now it's readable not only for the owner but also for group and any other user. So I'm going to do change mode. We can set it to 700. So basically remove any permissions from the file to anyone other than the owner. And that's it. Awesome. So we are connected to the cluster, which means we're going to repeat the same exact steps. Install or deploy the Consul helm chart inside the cluster and deploy our microservices application with the annotations. Let's do it. And this is going to be a command here. We're calling this helm installation of the Consul chart. We're using the same version. The same values file. And this time we are setting the data center to l k value. So calling the cluster where Consul will run. So let's execute this and let's check. Everything that was created. We have our deployments. The four pods starting up the same exact thing is in ECS. What Consul also deploys along which I briefly mentioned are the Crds. So we can also check those get CRD. And there are a bunch of kids here that are from the LCC itself, so we can actually filter. And. Let's find only the things that have Consul in it. So basically you have those service intentions that I showed you and so on. So it actually creates a bunch of crds in the cluster. So you can configure Consul and different components of Consul in the Kubernetes native way with manifest files. And you can find the configuration of all the Crds in their official documentation as well. So let's give it some time to start up. Let's see if the pods are ready. There you go. And now we can deploy. The microservices application. Apply config Consul and enter. In the same way, we see that two containers are starting up in every part of microservices, which means the proxy containers were injected in each service. And while this is starting up, let's actually. Check the services. Because with the same configuration, we have the Consul UI for this cluster as well as the front end service for this cluster. And as I told you, it doesn't really matter which cloud platform you use because the concepts are really similar. So the same way is on AWS. The load balancer component in Kubernetes actually links to the cloud native load balancer in linode called node balancer. So this one right here was created. That's the IP address the external IP address. And we see that here. This is for the frontend on port 80. And then we have the Consul UI which is accessible with Https on this endpoint. This one right here. So exactly same configuration, which means we can actually access this and see the Consul UI in LKY. And as you see, that's what we called the data center. That's the name we gave to the cluster environment when we deployed Consul. So it says okay here and here it says x. So it's going to make it a little bit easier for us to differentiate when we do stuff to connect those to. And as you see all the services are listed here. And all those parts except for the payment service have successfully started. Awesome. Now we have basically recreated the same exact environment in a different cluster, on a different cloud platform that can allow us to now have a failover. So if something happens to the cluster, we can always fall back to the cluster. However, we need to first establish that connection because obviously now these are two separate clusters. So we need to connect them so that the services in EKS cluster can communicate with services in cluster. And for that we are going to create what's called a peer connection. So we're going to make those two clusters peers. And since both of them have full Consul deployments inside on the Consul level, we are going to connect them so that services inside those two clusters can communicate with each other. And it is actually pretty easy to do. Again, there is an option to do this using the CRD components. for this demonstration we're going to use a more visual and simpler approach of Consul UI to establish that peer connection. And in the Consul values file, remember we enable the mesh gateways. These are the components that are actually going to help us connect those two clusters together. So the mesh gateway in EKS cluster will connect to the mesh gateway in cluster. So those two are the connection links. And when we have Consul clusters on different networks like we do here, completely different networks, different cloud platforms, then we're going to need to set the Consul servers or the control plane, basically to use mesh gateways to request or accept the peering connection. So on both sides we're going to actually configure Consul to use the mesh gateway component to send or accept the peering connection from the other cluster. And we can do that with one of the crds called mesh. So this one here, and we're going to have to apply that on both Consul deployments. So going back to my code. I actually have that configuration file already prepared right here. And you see how simple it looks like. This is the CRD from HashiCorp. By the way, make sure to check the latest version in the documentation if you are watching this a little bit later. So we're creating the mesh component with a specification that enables or basically tells Consul to use the mesh gateways for the peering connection. And we're going to apply this on both clusters which means I'm going to do kubectl apply. Consul. Mesh gateway right here. And I'm going to switch to and apply it here as well. There you go. We can also check that the CD was created. And as you see, we have this mesh component in the cluster, which, as I said, will allow to route the peering traffic through the mesh gateways. And now we are actually ready to pair those two clusters. So first going to the ECS Consul deployment. So that's our main data center. So to say our main cluster. Not technically but just theoretically for us. And we're going to go to the peers section. And we're going to add a peer connection. And we can give the peer a name. I'm going to use LCM. That's going to be the peer. So that's the name that the peer will be represented by. And we're going to click on Generate Token. And this is basically a secure token that will allow the other peer to connect to this one. So I'm going to copy this and let's close. And as you see this is the peer name. And it's pending because the appear basically has to make a connection as well using that token. So we're going to go to the now the peers at Peer Connection. And now instead of generate token we're going to do establish peering again name of the peer. We're going to call the other peer X and the token that I just copied at peer. As you see, super simple. And here we have this status as well as the health check. Is the peer accessible or not? And if I switch back this one is active as well. Very simple and straightforward as you see. Now as the next step we're going to add an exported service configuration in the LCK. So basically we're going to use an example of one specific service. And we're going to expose that service or export the service from this pier to make it accessible for the EKS cluster. That means the services here in the cluster will be able to talk to that exported service. And we're going to use an example of the shipping service actually, which means we're going to export this shipping service from the cluster to make it accessible from ECS services. And I also have a configuration for that. Let's go back right here. I have the exported service. And as you see the configuration is also pretty simple. We have this CD called Exported Services. This is the name of the service that we are exporting. And this is the name of the peer that is going to consume the service. Pretty straightforward. So basically just to demonstrate how this is going to work right now, if the shipping service failed, a specific feature related to that service will not work anymore because the pod is not available, the service is not available. So let's click in one of the items. And if I click on Add to Cart, as you see, everything works because shipping service is up and running. Let's go back. I'm actually going to. Yeah, I'm going to switch back to the EKS cluster and I'm going to delete deployment. Let me check the name. Shipping. Service. There you go. The shipping service pod should be gone and should be gone from here as well. And now let's actually try to access the same function again. And as you see, it's not working because the shipping service is not available. You get the error here as well. So what we're going to do now is that if this happens, like some of the services in this cluster basically crash and they're not available, we're going to direct or we're going to forward the traffic to the peer cluster that has the same service. So the shipping service of the cluster will basically take over instead of that deleted or crashed shipping service that was running here. So that's what we want to achieve. So I'm going to bring up the shipping service deployment. Again I'm just going to apply. The config again. Like this. There you go. It works again. And now let's configure that failover. So in the LCK. So we have this exported service for shipping service. And we're going to. Apply this. And if I switch back to my Consul. UI for deployment. You see that we now have one exported service, which is this shipping service basically. It also shows the topology of the connections and the same way in each cluster. It shows that as an imported service from that cluster. So now if we go back to the list of all the services, so we have all those other services through proxy, and we have this one here that shows that the service is actually coming from the peer connection. So we have two shipping services available for this cluster. Now. Now there's one more thing that we need to do from the case side to create what's called a service resolver. And again service resolver is its own CRD. And this is the configuration for the service resolver component. So basically we're configuring the service resolver for the shipping service since we have two shipping services now. Right here. And we're saying that we're going to use the failover to the peers shipping service service. So we are going to need to apply the service resolver in the EKS cluster, because we have those two instances of shipping service in the EKS cluster. And we are defining a service resolver for them, saying that this should be basically a failover. So switching back to EKS I'm going to apply. The service resolver. Let's do that. Create it. And now, the moment of truth. I'm going to delete the shipping service deployment again in the cluster. And that add to cart feature should still be working. So let's do that again in the EKS cluster. I'm going to. Delete the deployment shipping service again. Switch back as you see that one is gone. And now let's actually refresh again just in case. Going to any product. And if I click on Add to Cart, it should work by failing over to this imported service. And let's do that. Awesome. As you see, it used the service from a pure cluster as a failover. Awesome. So that was basically our demo. I hope I was able to give you lots of new insights and lots of new knowledge about service mesh technology generally, as well as what concepts and small details are involved in all of this. If you made it till the end of the video, congratulations on gaining a lot of valuable insights and knowledge in this area. We put a lot of work and effort in creating this video, so I will absolutely appreciate. If you like this video, leave a comment with your feedback and even share with your colleagues or anyone who you think will benefit from learning these concepts. And with that, thank you for watching till the end and see you in the next video.
