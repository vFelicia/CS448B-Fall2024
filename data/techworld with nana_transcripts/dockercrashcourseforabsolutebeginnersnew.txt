00:00 - in this video I will teach you all the
00:02 - main concepts of Docker including
00:04 - getting your first hands-on experience
00:06 - with it so if you have to use Docker at
00:09 - work or if you need to learn Docker to
00:11 - level up your engineering skills and
00:13 - need to get started fast and understand
00:15 - all the main Concepts and learn basics
00:18 - of how to work with Docker this crash
00:20 - course is exactly right for you first
00:23 - we'll start by explaining what Docker is
00:25 - why was it even created basically what
00:27 - problems it solves in engineering and
00:30 - how it helps in software development and
00:32 - deployment process so you will
00:34 - understand exactly why Docker is such a
00:36 - big deal and why it has become so
00:38 - popular and widely used in IT projects
00:41 - and as part of a virtualization solution
00:45 - Docker being an improvement over virtual
00:48 - machines or the next Evolution step I
00:51 - will also explain the difference between
00:52 - virtual machine and Docker and what are
00:55 - the advantages of Docker in this
00:57 - comparison after we've understood why we
00:59 - want to use Docker in the first place we
01:02 - will install Docker and learn how to
01:04 - actually work with it we will learn the
01:06 - concepts of Docker images containers
01:09 - Docker registry public and private
01:11 - Registries and we will run containers
01:14 - locally based on some of the images
01:17 - available on Dockers public registry
01:19 - called Docker Hub we will also learn the
01:22 - concept of creating your own images and
01:24 - learning about a Docker image blueprint
01:26 - called Docker file and of course we will
01:29 - see all these in action and learn all
01:31 - the docker commands for pulling images
01:33 - running containers building your own
01:35 - Docker image Etc we will also learn
01:38 - about versioning images with image text
01:41 - and finally after you've learned how to
01:43 - work with Docker I will also explain
01:45 - with graphical animations how Docker
01:49 - fits in the big picture of software
01:51 - development and deployment process so by
01:54 - the end of this video you will feel way
01:56 - more confident about your knowledge and
01:58 - understanding in Docker and and can
02:01 - easily build on that Foundation
02:02 - knowledge to become a Docker power user
02:05 - if you want to and under the video
02:07 - description I will provide some
02:08 - resources to learn even more about
02:11 - Docker and become more advanced in it
02:14 - but before we jump right in it seems
02:16 - like many of you watching the videos on
02:18 - our channel are still not subscribed so
02:21 - if you're getting some value out of the
02:23 - free tutorials I put out regularly on
02:26 - this channel be sure to subscribe not to
02:28 - miss any future videos or tutorials I
02:31 - would also be happy to connect with you
02:33 - on my other social media accounts where
02:35 - I post behind the scenes content weekly
02:38 - updates and so on so hope to connect to
02:41 - you there as well well I'm super excited
02:44 - to teach you all these so let's get into
02:47 - it let's start with the most important
02:49 - question what is Docker why was it even
02:52 - created and what problem does it solve
02:58 - in simple words Docker is a
03:00 - virtualization software that makes
03:02 - developing and deploying applications
03:05 - very easy much easier compared to how it
03:08 - was done before Docker was introduced
03:10 - and Docker does that by packaging an
03:13 - application into something called a
03:16 - container that has everything the
03:18 - application needs to run like the
03:20 - application code itself its libraries
03:23 - and dependencies but also the runtime
03:26 - and environment configuration so
03:28 - application and its running environment
03:30 - are both packaged in a single Docker
03:34 - package which you can easily share and
03:37 - distribute now why is this a big deal
03:40 - and how are applications actually
03:42 - developed and deployed before Docker was
03:45 - introduced let's see that to understand
03:47 - the benefits of Docker more clearly
03:53 - so how did we develop applications
03:56 - before containers usually when you have
03:59 - a team of developers working on some
04:01 - application they would have to install
04:02 - all the services that application
04:05 - depends on or needs like database
04:07 - Services Etc directly on their operating
04:10 - system right for example if you're
04:12 - developing a JavaScript application and
04:14 - you need a postgresql database maybe you
04:18 - need a redis for caching mosquito for
04:21 - messaging like you have a microservices
04:23 - application now you need all these
04:26 - Services locally on your development
04:28 - environment so you can actually develop
04:30 - and test the application right and every
04:33 - developer in the team would then have to
04:35 - go and install all those Services
04:38 - configure and run them on their local
04:41 - development environment and depending on
04:44 - which operating system they're using the
04:46 - installation process will be different
04:49 - because installing postgresql database
04:51 - on Mac OS is different from installing
04:54 - it on a Windows machine for example
04:56 - another thing with installing Services
04:58 - directly on an operating system
05:00 - following some installation guide is
05:03 - that you usually have multiple steps of
05:06 - installation and then configuration of
05:09 - the service so with multiple commands
05:11 - that you have to execute to install
05:12 - configure and set up the service the
05:15 - chances of something going wrong and
05:18 - error happening is actually pretty high
05:20 - and this approach or this process of
05:22 - setting up a development environment for
05:24 - a developer can actually be pretty
05:27 - tedious depending on how complex your
05:29 - application is for example if you have
05:32 - 10 services that your application is
05:34 - using then you would have to do that
05:37 - installation 10 times for each service
05:40 - and again it will differ within the team
05:43 - based on what operating system each
05:46 - developer is using now let's see how
05:48 - containers solve some of these problems
05:51 - with containers you actually do not have
05:54 - to install any of the services directly
05:56 - on your operating system because with
05:59 - Docker you have that service packaged in
06:02 - one isolated environment so you have
06:04 - postgresql with a specific version
06:07 - packaged with its whole configuration
06:10 - inside of a container so as a developer
06:13 - you don't have to go and look for some
06:16 - binaries to download and install on your
06:19 - machine but rather you just go ahead and
06:21 - start that service as a Docker container
06:24 - using a single Docker command which
06:27 - fetches the container package from
06:30 - internet and starts it on your computer
06:32 - and the docker command will be the same
06:35 - regardless of which operating system
06:37 - you're on and it will also be the same
06:40 - regardless of which service you are
06:42 - installing so if you have 10 services
06:43 - that your JavaScript application depends
06:45 - on you would just have to run 10 Docker
06:48 - commands for each container and that
06:51 - will be it so as you see Docker
06:54 - standardizes the process of running any
06:57 - service on your development environment
06:58 - and makes the whole process much easier
07:01 - so you can basically focus and work more
07:04 - on development instead of trying to
07:05 - install and configure services on your
07:08 - machine
07:09 - and this obviously makes setting up your
07:11 - local development environment much
07:14 - faster and easier than the option
07:17 - without containers plus with the docker
07:21 - you can even have different versions of
07:23 - the same application running on your
07:25 - local environment without having any
07:27 - conflict which is very difficult to do
07:30 - if you are installing that same
07:32 - application with different versions
07:34 - directly on your operating system and we
07:37 - will actually see all of this in action
07:40 - in the demo part of this video now let's
07:42 - see how containers can improve the
07:44 - application deployment process before
07:47 - containers a traditional deployment
07:49 - process would look like this development
07:52 - team would produce an application
07:54 - artifact or a package together with a
07:57 - set of instructions of how to actually
07:59 - install and configure that application
08:01 - package on the server so you would have
08:04 - something like a jar file for Java
08:07 - application or something similar
08:08 - depending on the programming language
08:10 - used and in addition of course you would
08:13 - have some kind of database service or
08:16 - some other services that your
08:17 - application needed also with a set of
08:19 - instructions of how to configure and set
08:22 - it up on the server so that application
08:24 - could connect to it and use it so
08:27 - development team would give that
08:28 - application artifact or package over to
08:31 - the operations team and the operations
08:34 - team would handle installing and
08:37 - configuring the application and all its
08:39 - dependent services like database for
08:41 - example now the problem with this kind
08:43 - of approach is that first of all you
08:45 - need to configure everything and install
08:47 - everything again indirectly on the
08:50 - operating system which I I mentioned in
08:52 - the development context that is actually
08:54 - very error prone and you can have
08:56 - various different problems during the
08:58 - setup process you can also have
09:00 - conflicts with dependency versions where
09:03 - two services are depending on the same
09:05 - library for example but with different
09:07 - versions and when that happens it's
09:09 - going to make the setup process way more
09:11 - difficult and complex so basically a lot
09:13 - of things that can go wrong when
09:16 - operations team is installing and
09:18 - setting up application any services on a
09:21 - server another problem that could arise
09:24 - from this kind of process is when there
09:26 - is a miscommunication between the
09:29 - development team and operations team
09:31 - because since everything is in a textual
09:34 - guide like an instruction list of how to
09:37 - configure and run the application or
09:41 - maybe some kind of checklist there could
09:43 - be cases where developers forget to
09:45 - mention some important step about
09:47 - configuration and when that part fails
09:49 - the operations team have to go back to
09:52 - developers and ask for more details and
09:55 - input and this could lead to some back
09:57 - and forth communication until the
09:59 - application is successfully deployed on
10:01 - the server so basically you have this
10:03 - additional communication overhead where
10:05 - developers have to communicate in some
10:08 - kind of textual graphical whatever
10:10 - format how the application should run
10:13 - and as I mentioned this could lead to
10:15 - issues and miscommunications with
10:18 - containers this process is actually
10:20 - simplified because
10:22 - now developers create an application
10:24 - package that doesn't only include the
10:27 - code itself but also all the
10:29 - dependencies and the configuration for
10:32 - the application so instead of having to
10:34 - write that in some textual format and
10:36 - document they basically just package all
10:38 - of that inside the application artifact
10:40 - and since it's already encapsulated in
10:44 - one environment the operations people
10:46 - don't have to configure any of this
10:48 - stuff directly on the server so it makes
10:50 - the whole process way easier and there
10:53 - is less room for issues that I mentioned
10:56 - previously so the only thing now that
10:58 - operations team need to do in this case
11:00 - is to run a Docker command that gets the
11:04 - container package that developers
11:06 - created and runs it on the server the
11:09 - same way operations team will run any
11:11 - services that application needs also as
11:14 - Docker containers and that makes the
11:16 - deployment process way easier on the
11:19 - operation side now of course the
11:20 - operations team will have to install all
11:22 - and set up the docker runtime on the
11:24 - server before they will be able to run
11:26 - containers but that's just one-time
11:29 - effort for one service or one technology
11:32 - and once you have Docker runtime
11:34 - installed you can simply run Docker
11:37 - containers on that server
11:41 - now at the beginning I mentioned that
11:44 - Docker is a virtualization tool just
11:47 - like a virtual machine and virtual
11:49 - machines have been around for a long
11:51 - time so why did Docker become so widely
11:54 - adopted what advantage is it has over
11:57 - virtual machines and what is the
11:59 - difference between the two for that we
12:01 - need to see a little bit of how Docker
12:04 - works on a technical level I also said
12:06 - that with Docker you don't need to
12:08 - install Services directly on operating
12:10 - system but in that case how does Docker
12:14 - run its containers on an operating
12:16 - system now in order to understand all
12:19 - this let's first look at how an
12:22 - operating system is made up operating
12:24 - systems have two main layers you have
12:26 - the operating system kernel and the
12:29 - operating system Apple locations layer
12:31 - and kernel is the part that communicates
12:34 - with the hardware components like CPU
12:38 - memory storage Etc so when you have a
12:41 - physical machine with all these
12:43 - resources and you install operating
12:45 - system on that physical machine the
12:48 - kernel of the operating system will
12:50 - actually be the one talking to the
12:52 - hardware components to allocate
12:54 - resources like CPU memory storage Etc to
12:58 - the applications then running on that
13:00 - operating system and those applications
13:02 - are part of the applications layer and
13:05 - they run on top of the kernel layer so
13:08 - kernel is kind of a middleman between
13:09 - the applications that you see when you
13:12 - interact with your computer and the
13:15 - underlying Hardware of your computer and
13:18 - now since Docker and virtual machine are
13:21 - both virtualization tools the question
13:24 - is what part of the operating system
13:28 - they actually virtualize and that's
13:31 - where the main difference between Docker
13:33 - and virtual machines actually lie so
13:35 - Docker virtualizes the applications
13:37 - layer this means when you run a Docker
13:40 - container it actually contains the
13:42 - applications layer of the operating
13:45 - system and some other applications
13:47 - installed on top of that application
13:49 - layer this could be a Java runtime or
13:51 - python or whatever and it uses the
13:54 - kernel of the host because it doesn't
13:57 - have its own kernel the virtual machine
14:00 - on the other hand has the applications
14:02 - layer and its own kernel so it
14:05 - virtualizes the complete operating
14:07 - system which means that when you
14:09 - download a virtual machine image on your
14:12 - host it doesn't use the host kernel it
14:15 - actually puts up its own so what is this
14:18 - difference between Docker and virtual
14:20 - machine actually mean first of all the
14:23 - size of the docker packages or images
14:25 - are much smaller because they just have
14:28 - to implement one layer of the operating
14:31 - system so Docker images are usually a
14:34 - couple of megabytes large virtual
14:36 - machine images on the other hand can be
14:38 - a couple of gigabytes
14:40 - this means when working with Docker you
14:42 - actually save a lot of disk space
14:44 - you can run and start Docker containers
14:46 - much faster than virtual machines
14:49 - because virtual machine has to put up a
14:52 - kernel every time it starts while Docker
14:54 - container just reuses the host kernel
14:57 - and you just start the application layer
14:59 - on top of it so while virtual machine
15:01 - needs a couple of minutes to start up
15:02 - Docker containers usually start up in a
15:05 - few milliseconds the third difference is
15:08 - compatibility so you can run virtual
15:10 - image of any operating system on any
15:14 - other operating system host so on a
15:17 - Windows machine you can run a Linux
15:19 - virtual machine for example but you
15:21 - can't do that with Docker at least not
15:24 - directly so what is the problem here
15:26 - let's say you have a Windows operating
15:28 - system with Windows kernel and its
15:31 - application layer and you want to run a
15:34 - Linux based Docker image directly on
15:37 - that Windows host the problem here is
15:39 - that Linux based Docker image cannot use
15:42 - the windows kernel it wouldn't need a
15:45 - Linux kernel to run because you can run
15:47 - a Linux application layer on a Windows
15:49 - kernel so that's kind of an issue with
15:52 - Docker however when you're developing on
15:55 - Windows or Mac OS you want to run
15:59 - various Services because most containers
16:01 - for the popular services are actually
16:04 - Linux based also interesting to note
16:07 - that Docker was originally written and
16:10 - built for Linux but later Docker
16:13 - actually made an update and developed
16:16 - what's called Docker desktop for Windows
16:20 - and Mac which made it possible to run
16:24 - Linux based containers on Windows and
16:27 - Mac computers as well so the way it
16:30 - works is that Docker desktop uses a
16:33 - hypervisor layer with a lightweight
16:36 - Linux Distribution on top of it to
16:38 - provide the needed Linux kernel and this
16:41 - way make running Linux based containers
16:45 - possible on Windows and Mac operating
16:48 - systems and by the way if you want to
16:50 - understand more about virtualization and
16:52 - how virtual machines work and what a
16:54 - hypervisor for example is you can watch
16:56 - my other video where I explain all of
16:58 - that in detail
17:00 - so this means for local development as
17:03 - an engineer you would install Docker
17:05 - desktop on your Windows or Mac OS
17:08 - computer to run Linux based images which
17:11 - as I mentioned most of the popular
17:13 - Services databases
17:15 - Etc are mostly Linux based so you would
17:19 - need that and that brings us to the
17:22 - installation of Docker in order to do
17:25 - some demos and learn Docker in practice
17:27 - you would first need to install it so in
17:30 - order to install Docker you just go to
17:32 - their official page for installation
17:34 - guide and follow the steps because
17:36 - Docker gets updated all the time the
17:39 - installation changes so instead of me
17:41 - just giving you some comments that may
17:43 - work now but we'll get updated in the
17:46 - future you should always refer to the
17:48 - latest documentation for installation
17:50 - guide for any tool so if we search for
17:54 - Docker desktop
17:56 - installation
17:58 - click on one of those links like install
18:01 - on windows so that's the docker desktop
18:04 - the tool that I mentioned that solves
18:06 - this problem of running Linux based
18:08 - images on a different operating system
18:10 - but it actually includes a lot of other
18:12 - things when you install it so what are
18:13 - you exactly installing with Docker
18:16 - desktop
18:17 - and you see exactly what's included in
18:19 - there so basically get the docker
18:21 - service itself it's called Docker engine
18:23 - that's the main part of the docker that
18:27 - makes this virtualization possible but
18:29 - when we have a service we need to
18:31 - communicate with that right so we need a
18:33 - client that can talk to that service so
18:36 - Docker desktop actually comes with a
18:39 - command line interface client which
18:41 - means we can execute Docker commands on
18:44 - a command line to start containers to
18:47 - create containers start stop them remove
18:50 - them Etc and do all kinds of things and
18:53 - it also comes with a graphical user
18:56 - interface client so if you're not
18:58 - comfortable working with command line
19:00 - you can actually use the graphical using
19:03 - interface where you can do all these
19:05 - things but in a nice user-friendly UI so
19:09 - you get all these things when you
19:10 - install Docker desktop basically
19:12 - everything that you need to get started
19:13 - with Docker and of course depending on
19:16 - which operating system you're on you're
19:18 - going to choose that one Mac windows on
19:20 - Linux so let's click on one of those and
19:23 - you basically just follow the
19:25 - instructions you have some system
19:27 - requirements you have to check things
19:29 - like the the version of your Mac OS
19:32 - how much resources you're going to need
19:34 - and you also have the options for Mac
19:38 - with Intel or Mac with apple silicon so
19:41 - you can toggle between those and
19:43 - basically just choose the guide that
19:46 - matches your computer specifications and
19:49 - once you have that check the system
19:51 - requirements go ahead and click on one
19:54 - of those in my case I have mac with
19:57 - Intel chip so I would click on this one
19:59 - and that's actually the docker desktop
20:01 - installer so if I click it's going to
20:04 - download this DMG image and once it's
20:07 - downloaded you basically just follow the
20:10 - steps described here right you double
20:12 - click on it open the application and so
20:15 - on
20:16 - and same for Windows if your windows you
20:19 - basically click on this one and download
20:20 - Docker desktop for Windows and make sure
20:23 - to check the system requirements and
20:26 - kind of prepare everything you need for
20:29 - starting Docker generally for latest
20:32 - versions of Windows Mac or whatever
20:34 - operating system it should be pretty
20:37 - easy and straightforward to install
20:39 - Docker so go ahead and do that once
20:42 - you're done with installation you can
20:44 - simply start the service by searching
20:47 - Docker and if I click on it you will see
20:50 - right here that it's actually
20:53 - starting up Docker service for Docker
20:56 - engine
20:58 - and there you go it's running and this
21:02 - view here that you're seeing this window
21:04 - is actually the graphical user interface
21:07 - of Docker that I mentioned so that's the
21:09 - client that you can use to interact with
21:12 - the docker engine so you have a list of
21:14 - containers running currently so there's
21:16 - no list same with images if I switched
21:19 - images I have cleaned up my environment
21:21 - so I'm starting with scratch with empty
21:24 - State just like you so we're ready to
21:27 - start using Docker But first you may be
21:30 - wondering what are images and that's
21:32 - what I'm gonna explain next because it's
21:34 - a very important Concept in docker
21:40 - now mentioned that Docker allows to
21:43 - package the application with its
21:45 - environment configuration in this
21:47 - package that you can share and
21:49 - distribute easily so just like an
21:51 - application artifact file like when we
21:53 - create a zip or tar file or a jar file
21:56 - which you can upload to a artifact
21:59 - storage and then download on the server
22:02 - or locally whenever you need it and then
22:04 - package or artifact that we produce with
22:06 - Docker is called a Docker image so it's
22:10 - basically an application artifact but
22:13 - different from jar file or from other
22:16 - application artifacts it not only has
22:19 - the compiled application code inside but
22:21 - additionally has information about the
22:24 - environment configuration it has the
22:27 - operating system application layer as I
22:29 - mentioned plus the tools like node npm
22:33 - or Java runtime installed on that
22:35 - depending on what programming language
22:37 - your application was written in for
22:39 - example you have a JavaScript
22:41 - application you would need node.js and
22:44 - npm to run your application right so in
22:47 - the docker image you would actually have
22:50 - node and npm installed already you can
22:53 - also add environment variables that your
22:55 - application needs for example you can
22:58 - create directories you can create files
23:00 - or any other environment configuration
23:02 - whatever you need around your
23:04 - application so all of the information is
23:08 - packaged in the docker image together
23:10 - with the application code and that's the
23:13 - great advantage of Docker that we talked
23:15 - about and as I said the package is
23:17 - called an image
23:19 - so if that's an image what is a
23:21 - container then well we need to start
23:24 - that application package somewhere right
23:26 - so when we take that package or image
23:28 - and download it to server or your local
23:32 - computer laptop we want to run it on
23:35 - that computer the application has to
23:37 - actually run
23:38 - and when we run that image on an
23:40 - operating system and the application
23:43 - inside starts in the pre-configured
23:45 - environment that gives us a container so
23:49 - a running instance of an image is a
23:52 - container so a container is basically a
23:55 - running instance of an image and from
23:58 - the same image from one image you can
24:01 - run multiple containers which is a
24:03 - legitimate use case if you need to run
24:06 - multiple instances of this same
24:08 - application for increased performance
24:10 - for example and that's exactly what we
24:12 - were seeing here so we have the images
24:14 - these are the application packages
24:17 - basically and then from those images we
24:19 - can start containers which we will see
24:22 - listed right here which are running
24:24 - instances of those images and I also
24:28 - said that in addition to the graphical
24:30 - user interface we get a command line
24:33 - interface client Docker client that can
24:35 - talk to Docker engine and since we
24:38 - installed Docker desktop we should have
24:40 - that Docker CLI also available locally
24:43 - which means if you open your terminal
24:45 - you should be able to execute Docker
24:47 - commits and Doc recommends we can do
24:50 - anything for example we can check what
24:52 - images we have available locally so if I
24:55 - do Docker images that will give me a
24:58 - list of images that I have locally which
25:00 - in this case I don't have any which we
25:02 - saw in the graphical user interface and
25:04 - I can also check the containers using a
25:07 - command docker occur
25:09 - PS
25:11 - and again I don't have any running
25:13 - containers yet now before moving on I
25:16 - want to give a shout out to Ned Hopper
25:18 - net Hopper's Cloud platform called
25:21 - kubernetes application operations offers
25:23 - an easy way for devops teams to deliver
25:26 - manage upgrade connect secure and
25:30 - monitor applications in one or more
25:32 - kubernetes clusters with this platform
25:35 - they basically create this virtual
25:37 - Network layer that connects multiple
25:39 - environments for example if you have
25:41 - multiple Cloud platforms and multiple
25:44 - kubernetes clusters even your own
25:46 - on-premise data center where your
25:48 - application gets deployed you can
25:50 - connect all these in one virtual Network
25:53 - so you can deploy and operate your
25:56 - kubernetes workloads as if it was in one
25:59 - cluster or one infrastructure
26:01 - environment and the GitHub Centric
26:03 - approach they use offers the visibility
26:05 - to know who did what and when for both
26:08 - your infrastructure and application so
26:10 - with net Hopper Enterprises can automate
26:13 - their operations and instead of building
26:15 - an own platform devops teams can focus
26:19 - on what matters the most which is
26:21 - releasing more application features
26:23 - faster so check them out you can
26:25 - actually sign up for a free account and
26:28 - take it for a spin to see if net Hopper
26:30 - is the right solution for you
26:36 - now it's clear that we get containers by
26:38 - running images but how do we get images
26:41 - to run containers from
26:44 - let's say we want to run
26:46 - a database container or redis or some
26:48 - log collector service container how do
26:51 - we get their Docker images well that's
26:53 - where Docker Registries come in so there
26:57 - are ready Docker images available online
27:00 - in image storage or registry so
27:03 - basically this is a storage specifically
27:05 - for Docker image type of artifacts and
27:09 - usually the company is developing those
27:11 - services like redis mongodb Etc as well
27:15 - as Docker Community itself will create
27:18 - what's called official images so you
27:21 - know this mongodb image was actually
27:23 - created by mongodb itself or the docker
27:26 - community so you know it's an official
27:28 - verified image from Docker itself and
27:32 - Docker itself offers the biggest Docker
27:35 - registry called Docker Hub where you can
27:38 - find any of these official images and
27:41 - many other images that different
27:43 - companies or individual developers have
27:46 - created and uploaded there
27:48 - so if we search for Docker hub right
27:52 - here you see Docker Hub container image
27:55 - Library
27:57 - and that's how it looks like and you
28:00 - don't actually have to register or sign
28:02 - up on Docker Hub to find those official
28:04 - images so anyone can go on this website
28:07 - and basically browse the container
28:09 - images and here in search bar you can
28:12 - type any service that you're looking for
28:15 - for example redis that I mentioned and
28:18 - if I hit enter you will basically see a
28:22 - list of various radius related images as
28:25 - well as the ready service itself as a
28:28 - Docker image and here you have this
28:30 - batch or label that says Docker official
28:33 - image for example for the reddish image
28:35 - that we are going to choose here you see
28:37 - that it is actually maintained by Docker
28:39 - Community the way it works is that
28:42 - Docker has a dedicated team that is
28:44 - responsible for reviewing and Publishing
28:47 - all content in the docker official
28:49 - images and this team works in the
28:51 - collaboration with the technology
28:54 - creators or maintainers as well as
28:56 - security expert words to create and
28:59 - manage those official Docker images
29:02 - so this way it is ensured that not only
29:05 - the technology creators are involved in
29:08 - the official image creation but also all
29:11 - the docker security best practices and
29:13 - production best practices are also
29:15 - considered in the image creation and
29:18 - that's basically the description page
29:21 - with all the information about how to
29:24 - use this Docker image what it includes
29:27 - Etc
29:28 - and again as I said Docker Hub is the
29:31 - largest Docker image registry so you can
29:33 - find images for any service that you
29:36 - want to use on Docker Hub now of course
29:39 - technology changes and there are updates
29:43 - to Applications those Technologies so
29:45 - you have a new version of redis or
29:48 - mongodb and in that case a new Docker
29:52 - image will be created so images are
29:55 - versioned as well and these are called
29:58 - image tags and on the page of each image
30:03 - you actually have the list of versions
30:06 - or tags of that image listed right here
30:09 - so this is for redis and if I search for
30:13 - postgres for example
30:17 - foreign
30:19 - you will see different image tags for
30:21 - postgres image also listed here
30:24 - so when you're using a technology and
30:26 - you need a specific version you can
30:28 - choose a Docker image that has that
30:31 - version of the technology and there is a
30:34 - special tag that all images have called
30:38 - latest so right here you see this latest
30:41 - tag or here as well in the recent text
30:44 - so latest tag is basically the latest
30:48 - the last image that was built so if you
30:51 - don't specify or choose a version
30:53 - explicitly you basically get the latest
30:56 - image from the docker Hub so now we've
30:59 - seen what images are and where you can
31:02 - get them so now the question is how do
31:04 - we actually get the image from Docker
31:07 - Hub and download it locally on our
31:10 - computer so we can start a container
31:12 - from that image so first we locate the
31:16 - image that we want to run as a container
31:18 - locally for our demo I'm going to use an
31:21 - nginx image so go ahead and search for
31:24 - nginx which is basically a simple web
31:26 - server and it has a UI so we will be
31:30 - able to access our container from the
31:33 - browser to validate the container has
31:35 - started successfully that's why I'm
31:36 - choosing nginx and here you have a bunch
31:40 - of image tags that you can choose from
31:42 - so the second step after locating the
31:45 - image is to pick a specific image tag
31:49 - and note that selecting a specific
31:52 - version of image is the best practice in
31:55 - most cases and let's say we choose
31:57 - version
31:59 - 1.23 so we're choosing this tag right
32:02 - here and to download an image we go back
32:05 - to our terminal and we execute docker
32:09 - pull comment and we specify the name of
32:13 - the image which is nginx
32:16 - so you have that whole command here as
32:18 - well so that's basically the name of the
32:20 - image that you have written here so
32:22 - that's nginx and then we specify the
32:26 - image tag by separating it with a column
32:30 - and then the version 1.23 that's what we
32:33 - chose that's the whole command so Docker
32:36 - client will contact Docker Hub and it
32:39 - will say I want to grab the nginx image
32:43 - with this specific tag and download it
32:45 - locally so let's execute
32:49 - and here we see that it's pulling the
32:52 - image from the image registry Docker Hub
32:56 - and the reason why we don't have to tell
32:58 - Docker to find that image on Docker Hub
33:01 - is because Docker Hub is actually the
33:04 - default location where Docker will look
33:06 - for any images that we specify right
33:09 - here so it's automatically configured as
33:12 - a location for downloading the images
33:15 - from and the download happened and now
33:18 - if we execute Docker images command
33:21 - again as we did here we should actually
33:23 - see one image now locally which is nginx
33:27 - with an image tag 1.23 and some other
33:32 - information like the size of the image
33:34 - which is usually in megabytes as I
33:37 - mentioned so we have an image now
33:39 - locally and if we pull an image without
33:43 - any specific tag so we do this basically
33:47 - Docker pull name of the image if I
33:50 - execute this you see that it is pulling
33:52 - the latest image automatically and now
33:55 - if I do Docker images again we're going
33:58 - to see two images of nginx with two
34:03 - different texts right so these are
34:05 - actually two separate images with
34:07 - different versions cool now we have
34:10 - images locally but obviously they're
34:12 - only useful when we run them in a
34:15 - container environment how can we do that
34:16 - also super easy we pick the image we
34:20 - already have available locally with the
34:23 - tag so let's say we want to run this
34:26 - image as a container and we execute
34:28 - Docker run command and with the name of
34:32 - the image and the tag
34:36 - super easy and let's execute and that
34:39 - command actually starts the container
34:41 - based on the image and we know the
34:43 - container started because we see the
34:46 - logs of nginx service starting up inside
34:50 - the container so these are actually
34:51 - container logs that we see in the
34:53 - console so it's launching a couple of
34:55 - scripts
34:57 - and right here we have start worker
35:00 - processes and the container is running
35:03 - so now if I open a new terminal session
35:07 - like this and
35:10 - to Docker PS
35:13 - I should actually see one container this
35:17 - one here in the running container list
35:20 - and we have some information about the
35:22 - container we have the ID we have the
35:24 - image that the container is based on
35:26 - including the tag when it was created
35:29 - and also the name of the container so we
35:32 - have the ID and name of the container
35:34 - this is the name which Docker actually
35:37 - automatically generates and assigns to a
35:41 - container when it's created so it's a
35:43 - random generated name
35:45 - now if I go back here you see that these
35:49 - locks the container logs actually are
35:51 - blocking the terminal so if I want to
35:54 - get the terminal back and do Ctrl C exit
35:57 - the container exits and the process
36:00 - actually dies so now if I do Docker PS
36:02 - you will see that there is no container
36:05 - running
36:06 - but we can start a container in the
36:08 - background without it blocking the
36:10 - terminal by adding
36:12 - a flag called minus D which stands for
36:15 - detached so it detaches the docker
36:18 - process from terminal
36:20 - if I execute this you see that it's not
36:23 - blocking the terminal anymore and
36:26 - instead of showing the logs from nginx
36:28 - starting up inside the container it just
36:30 - locks out the full ID of the container
36:33 - so now if I do Docker PS here in the
36:36 - same terminal I should see that
36:38 - container running again and that's
36:41 - basically the ID or the part of the this
36:45 - full ID string shown here but when we
36:49 - start a container in the background in a
36:51 - detached mode you may still want to see
36:53 - the application logs inside the
36:55 - container so you may want to see how did
36:58 - nginx start up what did it log actually
37:00 - so for that you can use another Docker
37:03 - command called Docker locks with the
37:07 - container ID like this
37:10 - and it will print out the application
37:12 - logs from the container now in order to
37:15 - create the container the nginx container
37:18 - we first pull the image and then we
37:20 - created a container from that image but
37:23 - we can actually save ourselves the pull
37:25 - command and execute run command directly
37:29 - even if the image is not available
37:32 - locally so right now we have these two
37:35 - images available locally but in the
37:38 - docker run command you can actually
37:39 - provide any image that exists on Docker
37:42 - Hub it doesn't necessarily have to exist
37:45 - locally on your computer so you don't
37:47 - have to pull that first so if I go back
37:49 - we can actually choose a different image
37:52 - version let's choose
37:54 - 1.22 Dash Alpine so
37:58 - this image tag which we don't have
38:01 - locally or of course this can be
38:04 - completely different service it doesn't
38:06 - matter so basically any image that we
38:09 - don't have locally you can run directly
38:12 - using Docker run command so what it does
38:15 - is first it will try to locate that
38:18 - image locally and if it doesn't find it
38:21 - it will go to Docker Hub by default and
38:25 - pull the image from there automatically
38:27 - which is very convenient so it does both
38:30 - in one command basically so it
38:32 - downloaded the image with this tag and
38:36 - started the container and now if we do
38:39 - Docker PS we should have two containers
38:43 - running with different nginx versions
38:46 - and remember I said Docker solves the
38:49 - problem of running different versions of
38:51 - the same application at once so that's
38:54 - how simple it is to do that with Docker
38:56 - so we can actually quit this container
38:59 - and now again we have that one nginx
39:03 - container with this version
39:09 - now the important question is how do we
39:12 - access this container well we can't
39:15 - right now because the container is
39:18 - running in the closed Docker Network so
39:22 - we can't access it from our local
39:24 - computer browser for example we need to
39:27 - First expose the container to our local
39:30 - network which may sound a little bit
39:32 - difficult but it's super easy so
39:35 - basically we're going to do what's
39:36 - called a port binding the container is
39:40 - running on some Port right and each
39:42 - application has some standard port on
39:45 - which it's running like nginx
39:48 - application always runs on Port 80
39:50 - radius runs on Port
39:54 - 6379 so these are standard ports for
39:57 - these applications so that's the port
39:59 - where container is running on and for
40:03 - nginx we see the ports under the list of
40:05 - ports here application is running on
40:07 - Port 80 inside the container so now if I
40:10 - try to access nginx container on this
40:14 - port on Port 80 from the browser
40:18 - and let's try to do that we're eating
40:22 - and hit enter you see that nothing is
40:25 - available on this port on localhost so
40:28 - now we can tell Docker hey you know what
40:31 - bind that container Port 80
40:34 - to our local host on any port that I
40:38 - tell you on some specific Port like 8080
40:41 - or 9000 it doesn't actually matter so
40:44 - that I can access the container or
40:46 - whatever is running inside the container
40:48 - as if it was running on my Local Host
40:51 - Port 9000 and we do that with an
40:55 - additional flag when creating a Docker
40:58 - container so what we're going to do is
41:02 - first we're going to stop this container
41:05 - and create a new one so we're going to
41:07 - do Docker stop which
41:10 - basically stops this running container
41:13 - and we're going to create a new
41:15 - container
41:16 - so we're going to do Docker run nginx
41:20 - the same version and we're going to find
41:23 - it in the background in detached mode
41:25 - now we're going to do the port binding
41:27 - with an additional flag minus p and it's
41:31 - super easy we're telling Docker the
41:34 - nginx application Port inside container
41:38 - which is 80. please take that and find
41:41 - that on a host localhost on Port
41:45 - whatever 9000 for example right that's
41:48 - the port I'm choosing so this flag here
41:51 - will actually expose the container to
41:55 - our local network or localhost so these
41:59 - nginx process running in container will
42:01 - be accessible for us on Port 9000. so
42:04 - now if I execute this let's see that
42:07 - container is running
42:09 - and in the port section we see a
42:12 - different value so instead of just
42:14 - having 80 we have this port binding
42:17 - information so if you forgot which Port
42:20 - you chose or if you have 10 different
42:22 - containers with Docker PS you can
42:24 - actually see on which Port each
42:26 - container is accessible on your Local
42:28 - Host so this will be the port
42:31 - so now if I go back to the browser and
42:35 - instead of localhost 80 we're going to
42:37 - type in localhost 9000.
42:41 - and hit enter
42:42 - there you go we have the welcome to
42:45 - nginx page so it means we are actually
42:48 - accessing our application and we can see
42:51 - that in the logs as well Docker locks
42:54 - container ID
42:55 - and there you go this is the log uh that
42:59 - nginx application produced that it got a
43:02 - request from MEC or Mac OS machine
43:05 - Chrome browser so we see that our
43:08 - request actually reached the nginx
43:11 - application running inside the container
43:14 - so that's how easy it is to run a
43:17 - service inside container and then access
43:20 - it locally now as I said you can choose
43:22 - whatever Port you want but it's also
43:25 - pretty much a standard to use the same
43:28 - port on your host machine as the
43:32 - container is using so if I was running a
43:34 - MySQL container which started at Port
43:38 - 3306. I would bind it on localhost
43:43 - 3306. so that's kind of a standard
43:50 - now there's one thing I want to point
43:52 - out here which is that Docker run
43:55 - command actually creates a new container
43:58 - every time it doesn't reuse the
44:01 - container that we created previously
44:03 - which means since we executed Docker run
44:06 - command a couple of times already we
44:08 - should actually have multiple containers
44:10 - on our laptop however if I do Docker PS
44:14 - I only see the running container I don't
44:17 - see the ones that I created but stopped
44:20 - but those containers actually still
44:22 - exist so if I do Docker PS
44:26 - with a Fleck a
44:29 - and execute this gives you actually a
44:32 - list of all containers whether they are
44:35 - running or stopped so this is the active
44:38 - container that is still running and
44:41 - these ones are the stopped ones it even
44:43 - says exited 10 minutes ago six minutes
44:46 - ago whatever so we have four containers
44:49 - with different configuration
44:51 - and previously I showed you Docker stop
44:53 - command which basically stops an
44:56 - actively running container so we can
44:58 - stop this one
45:00 - and now it will show it as a stopped
45:03 - container as well exited one second ago
45:05 - but the same way you can also restart a
45:08 - container that you created before
45:10 - without having to create a new one with
45:12 - Docker run command so for that we have a
45:14 - Docker start
45:16 - and that takes the ID of the container
45:21 - and starts the container again and again
45:23 - you can start multiple
45:25 - containers at once if you want
45:28 - like this
45:31 - and they have two containers running now
45:34 - you saw that we use ID of the container
45:36 - in various Docker commands so to start
45:39 - the container to restart it to check the
45:41 - logs Etc but ID is hard to remember and
45:44 - you have to look it up all the time so
45:47 - as an alternative you can also use
45:48 - container name for all these commands
45:51 - instead of the ID which gets
45:53 - auto-generated by Docker but we can
45:55 - actually rewrite that and we can give
45:58 - our containers a more meaningful names
46:01 - when we create them
46:04 - so we can stop those two containers
46:06 - using the ID
46:08 - or the name
46:10 - like this so these are two different
46:12 - containers one with the ID one with name
46:14 - and we're going to stop both of them
46:17 - there you go now when we create a new
46:20 - container we can actually give it a
46:22 - specific name and there is another flag
46:24 - for that which is dash dash name and
46:27 - then we provide the name that we want to
46:29 - give our container let's say this is a
46:33 - web app so that's what we're going to
46:35 - call our container
46:37 - and let's execute
46:39 - if I do Docker PS you see that the name
46:42 - is not some auto-generated random thing
46:45 - but instead our container is called web
46:47 - app so now we can do Docker locks and
46:50 - name of our container like this
46:57 - now we've learned about Docker Hub which
47:00 - is actually what's called a Public Image
47:03 - registry which means those images that
47:06 - we used are visible and available for
47:09 - public but when a company creates their
47:12 - own images of their own applications of
47:15 - course they don't want it to be
47:16 - available publicly so for that there are
47:20 - what's called private Docker Registries
47:22 - and there are many of them almost all
47:24 - Cloud providers have a service for
47:27 - private Docker registry for example AWS
47:30 - is ECR or elastic container registry
47:32 - service Google Azure they all have their
47:36 - own Docker Registries Nexus which is a
47:39 - popular artifact storage service has
47:42 - Docker registry even Docker Hub has a
47:45 - private Docker registry so on the
47:47 - landing page of Docker Hub you saw this
47:50 - get started form so basically if you
47:53 - want to store your private Docker images
47:54 - on Docker Hub you can actually create a
47:57 - private registry on Docker Hub or even
48:00 - create a public registry and upload your
48:02 - images there so that's why I actually
48:04 - have an account because I have uploaded
48:05 - a couple of images on Docker Hub that my
48:09 - students can download for different
48:11 - courses and there is one more concept I
48:13 - want to mention related to registry
48:15 - which is something called a repository
48:17 - which you also often hear Docker
48:19 - repository Docker registry so what is
48:21 - the difference between them very simply
48:23 - explained AWS ECR is a registry so
48:27 - basically that's a service that provides
48:29 - storage for images and inside that
48:31 - registry you can have multiple
48:33 - repositories for all your different
48:35 - application images so each application
48:37 - gets its own repository and in that
48:40 - repository you can store different image
48:42 - versions or tags of that same
48:44 - application the same way dockerhub is a
48:47 - registry it's a service for storing
48:49 - images and on Docker Hub you can have
48:52 - your public repositories for storing
48:54 - images that will be accessible publicly
48:56 - or you can have private repositories for
48:59 - different applications and again you can
49:01 - have repository dedicated for each
49:03 - application so that's a side note there
49:05 - if you hear these terms and Concepts and
49:08 - you know what is the difference between
49:09 - them
49:12 - now I mentioned that companies would
49:15 - want to create their own custom images
49:17 - for their applications so how does that
49:20 - actually work how can I create my own
49:22 - Docker image for my application and the
49:25 - use case for that is when I'm done with
49:28 - development the application is ready it
49:31 - has some features and we want to release
49:33 - it to the end users so we want to run it
49:36 - on a deployment server and to make the
49:38 - deployment process easier once you
49:40 - deploy our application as a Docker
49:42 - container along with the database and
49:44 - other services that are also going to
49:47 - run as Docker containers so how can we
49:49 - take our created deployed application
49:51 - code and package it into a Docker image
49:55 - for that we need to create a definition
49:58 - of how to build an image from our
50:01 - application and that definition is
50:04 - written in a file called a Docker file
50:07 - so that's how it should be called
50:08 - creating a simple Docker file is very
50:10 - easy and in this part we're going to
50:12 - take a super simple node.js application
50:15 - that I prepared and we're going to write
50:18 - a Docker file for that application to
50:20 - create a Docker image out of it and as I
50:23 - said it's very easy to do so this is the
50:25 - application it is extremely simple I
50:28 - just have one server.js file which
50:32 - basically just starts the application on
50:35 - Port 3000 and then it just says welcome
50:38 - when you access it from the browser and
50:41 - we have one package of Json file which
50:44 - contains this dependency but the express
50:46 - library that we use here to start the
50:49 - application super lean and simple and
50:52 - that's the application from which we're
50:54 - going to create a Docker image and start
50:56 - it as a Docker container so let's go
50:59 - ahead and do that
51:00 - so in the root of the application we're
51:03 - going to create a new file called Docker
51:05 - file
51:06 - so that's the name and you see that most
51:10 - code editors actually detect Docker file
51:13 - and we get this Docker icon here so in
51:16 - this Docker file we're going to write a
51:18 - definition of how the image should be
51:21 - built from this application so what does
51:23 - our application need it needs a node
51:25 - installed because node should run our
51:27 - application right
51:28 - so if I wanted to start this application
51:31 - luckily for my terminal I would execute
51:33 - node SRC so the source folder and
51:37 - server.js command to start the
51:40 - application so we need that node command
51:42 - available inside the image and that's
51:45 - where the concept of Base image comes in
51:48 - so each Docker image is actually based
51:50 - on this base image which is mostly a
51:54 - lightweight Linux operating system image
51:57 - that has the node npm or whatever tool
52:00 - you need for your application installed
52:02 - on top of it so for a JavaScript
52:04 - application you would have node base
52:06 - image if you have Java application we
52:08 - will use an image that has Java runtime
52:10 - installed again Linux operating system
52:13 - with Java installed on top of it and
52:17 - that's the base image and we Define the
52:19 - base image using a directive in Docker
52:21 - file called from we're saying build this
52:24 - image from the base image and if I go
52:26 - back to Docker Hub and search for node
52:30 - you will see that we have an image which
52:32 - has node and npm installed inside and
52:37 - base images are just like other images
52:38 - so basically you can pile and build on
52:41 - top of the images in Docker so they're
52:44 - just like any other image that we saw
52:45 - and they also have text or image
52:48 - versions so we're going to choose node
52:51 - image and a specific version and let's
52:54 - actually go for 19-alpine
53:00 - so that's our base image and our first
53:03 - directive in the docker file so again
53:04 - this will just make sure that when our
53:07 - node.js application starts in a
53:09 - container it will have a node and npm
53:12 - commands available inside to run our
53:14 - application now if we start our
53:15 - application with this command we will
53:18 - see that we get an error because we need
53:20 - to First install dependencies of an
53:22 - application we just have one dependency
53:24 - which is press Library which means we
53:26 - would have to execute npm install
53:28 - command which will check the
53:31 - package.json file read all the
53:33 - dependencies defined inside and install
53:35 - them locally in node modules folder so
53:39 - basically we're mapping the same thing
53:41 - that we would do to run the application
53:43 - locally we're making that inside the
53:45 - container so we would have to run npm
53:48 - install command also inside the
53:51 - container so as I mentioned before most
53:54 - of the docker images are Linux based
53:56 - Alpine is a Linux a lightweight Linux
53:58 - operating system distribution so so in
54:01 - Docker file you can write any Linux
54:02 - commands that you want to execute inside
54:04 - the container and whenever we want to
54:06 - run any command inside the container
54:08 - whether it's a Linux command or node
54:10 - command npm command whatever we executed
54:13 - using a run directive so that's another
54:16 - directive and you see that directives
54:18 - are written in all caps and then comes
54:21 - the command so npm install which will
54:24 - download dependencies inside the
54:27 - container and create a node modules
54:29 - folder inside the container before the
54:32 - application gets started so again think
54:35 - of a container as its own isolated
54:36 - environment it has a simple Linux
54:38 - operating system with node and npm
54:42 - installed and we're executing npm
54:44 - install however we need application code
54:46 - inside the container as well right so we
54:49 - need the server.js inside and we need
54:52 - the package.json because that's what npm
54:54 - command will need to actually read the
54:57 - dependencies and that's another
54:59 - directive where where we take the files
55:02 - from our local computer and we paste
55:06 - them copy them into the container
55:09 - and that's a directive called copy and
55:12 - you can copy individual files like
55:14 - package.json from here into the
55:18 - container and we can say where in
55:20 - container on which location in the file
55:24 - system it should be copied to and let's
55:27 - say it should be copied into a folder
55:29 - called slash app
55:32 - inside the container so this is on our
55:35 - machine right we have package.json here
55:38 - this is inside the container it's a
55:41 - completely isolated system from our
55:44 - local environment
55:46 - so we can copy individual files and we
55:49 - can also copy the complete directories
55:51 - so we also need our application code
55:53 - inside obviously to run the application
55:55 - so we can copy this whole Source
55:58 - directory so we have multiple files
56:00 - inside we can copy the whole directory
56:03 - into the Container again in slash app
56:07 - location and the slash at the end is
56:10 - also very important so the docker knows
56:13 - to create this folder if it doesn't
56:15 - exist in the container yet so the roots
56:19 - of Linux file system app folder inside
56:22 - and then slash so now all the relevant
56:25 - application files like package.json and
56:27 - the whole Source directory are copied
56:29 - into the container on this location the
56:33 - next thing we want to do before we can
56:35 - execute npm install command is to
56:38 - actually change into that directory
56:40 - right so in Linux we have this CD right
56:43 - to change into a directory in order to
56:45 - execute the following commands inside
56:47 - the directory in Docker file we have a
56:50 - directive for that called work
56:53 - dear it's a working directory which is
56:57 - an equivalent of changing into a
56:59 - directory to execute all the following
57:01 - commands in that directory so we can do
57:05 - slash app here so it sets this path as
57:08 - the default location for whatever comes
57:12 - afterwards okay so we're copying
57:14 - everything into the Container then we
57:17 - are setting the working directory or the
57:20 - default directory inside the container
57:22 - and then we're executing npm install
57:24 - again within the container to download
57:26 - all the dependencies that application
57:28 - needs that are defined here and finally
57:31 - we need to run the application right so
57:34 - after npm install the node command
57:37 - should be executed and we learned to
57:40 - execute commands we use the Run
57:42 - directive however if this is the last
57:46 - command in the docker file so something
57:49 - that actually starts the process itself
57:51 - the application inside we have a
57:53 - different directive for that called CMD
57:56 - so that's basically the last command in
57:58 - the docker file and that starts the
58:00 - application and the Syntax for that is
58:03 - the command which is node and the
58:07 - parameter
58:08 - gserver.js so we copied everything into
58:12 - slash app so we have the server.js
58:14 - inside the app directory and we're
58:16 - starting it or running it using node
58:18 - commit that's it that is the complete
58:22 - Docker file which will create a Docker
58:25 - image for our node.js application which
58:28 - we can then start as a container
58:34 - so now we have the definition in Docker
58:36 - file it's time to actually build the
58:39 - image from this definition I'm going to
58:41 - clear this up and without changing to
58:43 - the terminal we can actually reuse this
58:45 - one we can execute a Docker command to
58:48 - build a Docker image which is super easy
58:50 - we just do Docker build
58:52 - then we have a couple of options that we
58:54 - can provide the first one is the name of
58:57 - the image so just like all those images
58:59 - have names right like node release
59:03 - Etc and the text we can also name our
59:07 - image and give it some specific tag and
59:10 - we do that using this Dash T option and
59:14 - we can call our application node app
59:16 - maybe with Dash doesn't matter and we
59:20 - can give it a specific tag like 1.0 for
59:24 - example and the last parameter is the
59:27 - location of dockerfile so we're telling
59:29 - Docker build an image with this name
59:32 - with this tag from the definition in
59:36 - this specific Docker file right so this
59:39 - is a location of Docker file in this
59:41 - case we are in the directory where
59:43 - Docker file is located so it's going to
59:46 - be the current directory so this dot
59:49 - basically refers to the current folder
59:51 - where Docker file is located so now if
59:55 - we execute this as you see Docker is
59:58 - actually building the image from our
60:01 - Docker file
60:03 - and it looks like it succeeded where it
60:06 - started building the image you see those
60:09 - steps those directives that we defined
60:11 - here so we have the first one from
60:13 - directive got executed then we have the
60:16 - copy as a second step then we have copy
60:19 - The Source folder setting work directory
60:22 - and running npm install and then the
60:25 - last one just started the application so
60:27 - now
60:28 - if I do Docker images in addition to
60:31 - those nginx images we downloaded
60:34 - previously from Docker Hub we should
60:37 - actually see the image that we just
60:38 - created this is the node app image with
60:41 - tag 1.0 and some other information so
60:45 - that's our image and now we can start
60:47 - this image and work with it just like we
60:50 - work with any other image downloaded
60:53 - from Docker Hub so we're going to go
60:54 - ahead and run container from this node
60:57 - app image and make sure that the
60:59 - application inside is actually working
61:01 - so we're going to do Docker run node app
61:06 - image with
61:08 - 1.010 and we're going to pass in
61:12 - parameter to start in detach mode and
61:14 - also we want to expose the port right we
61:18 - want to be able to access the
61:20 - application the node application from
61:22 - localhost and we know that the
61:24 - application inside the container will
61:26 - start on Port 3000 because that's what
61:29 - we have defined here so the application
61:31 - itself will be running on Port 3000 so
61:34 - that's inside container and we can bind
61:38 - it to whatever Port we want on localhost
61:41 - and we can do 3000 the same as in the
61:44 - container so this is the host port and
61:47 - this is container port and now if I
61:50 - execute
61:51 - command and do Docker PS we should see
61:55 - our node app running on Port 3000 and
62:00 - now the moment of truth going back to
62:02 - the browser
62:03 - and opening localhost 3000
62:07 - there is our welcome to my awesome app
62:12 - message from our application and we can
62:15 - even check the logs by grabbing the ID
62:19 - of our not app
62:21 - and doing Docker blocks with the ID
62:24 - and that's the output of our application
62:28 - inside the container so that's how easy
62:31 - it is to take your application package
62:33 - it into a Docker image using Docker file
62:36 - and then run it as a container
62:42 - and finally going back to this graphical
62:45 - user interface client that Docker
62:47 - desktop actually provides us with now we
62:50 - are able to see other containers and
62:52 - images here as well and that's how this
62:55 - UI actually looks like it gives you a
62:57 - pretty good overview of what containers
62:59 - you have which ones are currently
63:02 - running which ones are stopped with
63:04 - their names and so on and you even have
63:06 - some controls here to start a stop
63:09 - container like this or even stop it
63:12 - again
63:14 - restart container deleted whatever and
63:17 - the same way you have a list of images
63:19 - including our own image and you can also
63:23 - create containers directly from here
63:24 - using some controls so I personally
63:28 - prefer the command line interface to
63:30 - interact with Docker but some feel more
63:32 - comfortable using the visual UI so
63:35 - whichever you prefer you can actually
63:37 - choose to work with either
63:42 - now we've learned a lot of basic
63:45 - building blocks of Docker however it's
63:48 - also interesting to see how Docker
63:50 - actually fits in in the complete
63:52 - software development and deployment
63:53 - process with lots of other Technologies
63:57 - as well so in which steps throughout
64:00 - this whole process is Docker relevant so
64:03 - in this final part of the crash course
64:04 - we're gonna see Docker in big picture
64:07 - view of software development life cycle
64:10 - so let's consider a simplified scenario
64:13 - where you're developing a JavaScript
64:15 - application on your laptop right on your
64:17 - local development environment
64:20 - your JavaScript application uses a
64:23 - mongodb database and instead of
64:26 - installing it on your laptop you
64:28 - download a Docker container from the
64:31 - docker hub so you connect your
64:33 - JavaScript application with the mongodb
64:35 - and you start developing
64:37 - so now let's say you developed the
64:39 - application first version of the
64:41 - application locally and now you want to
64:43 - test it or you want to deploy it on the
64:47 - development environment where a tester
64:50 - in your team is gonna test it so you
64:53 - commit your JavaScript application in
64:56 - git or in some other version control
64:58 - system
64:59 - that will trigger a continuous
65:01 - integration a Jenkins build or whatever
65:04 - you have configured and Jenkins build
65:07 - will produce artifacts from your
65:10 - application so first you will build your
65:13 - JavaScript application and then create a
65:17 - Docker image out of that JavaScript
65:20 - artifact right so what happens to this
65:23 - Docker image once it gets created by
65:25 - Jenkins build it gets pushed to a
65:29 - private Docker repository so usually in
65:32 - a company you would have a private
65:34 - repository because you don't want other
65:36 - people to have access to your images so
65:39 - you push it there and now is the next
65:42 - step could be configured on Jenkins or
65:45 - some other scripts or tools that Docker
65:49 - image has to be deployed on a
65:51 - development server so you have a
65:54 - development server that pulls the image
65:56 - from the private repository your
65:59 - JavaScript application image and then
66:01 - pulls the mongodb that your JavaScript
66:04 - application depends on from a Docker Hub
66:06 - and now you have two containers one your
66:10 - custom container and a publicly
66:12 - available mongodb container running on
66:15 - dev server and they talk to each other
66:17 - you have to configure it of course they
66:19 - talk and communicate to each other and
66:22 - run as an app so now if a tester for
66:25 - example or another developer logs in to
66:29 - a Dev server they will be able to test
66:31 - the application so this is a simplified
66:34 - workflow how Docker will work in a real
66:37 - life development process so in a short
66:39 - time we actually learn all the basic
66:42 - building blocks the most important parts
66:43 - of Docker so you understand what images
66:46 - are how to start containers how they
66:48 - work and how to access them as well as
66:50 - how to actually create your own Docker
66:52 - image and run it as a container but if
66:55 - you want to learn more about Docker and
66:58 - practice your skills even more like how
67:00 - to connect your application to a Docker
67:02 - container learn about Docker compose
67:05 - Docker volumes Etc you can actually
67:07 - watch my full Docker tutorial and if you
67:11 - want to learn Docker in the context of
67:13 - devops and really really Master it with
67:17 - things like private Registries using
67:19 - Docker to run Jenkins integrate Docker
67:22 - in cicd pipelines and use it with
67:26 - various other Technologies like
67:27 - terraform ansible Etc you can check out
67:30 - our complete devops bootcamp where you
67:32 - learn all these and much more