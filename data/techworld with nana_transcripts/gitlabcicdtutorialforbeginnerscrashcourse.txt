00:00 - hello and welcome to the gitlab crash
00:02 - course where i will teach you everything
00:04 - you need to know to get started with
00:06 - gitlab cicd in one hour i am nada and i
00:09 - have taught hundreds of thousands of
00:11 - people how to advance their devops
00:13 - skills through my youtube channel and
00:16 - online courses as well as the complete
00:18 - devops educational program if you're new
00:21 - to my channel be sure to subscribe
00:23 - because i upload new videos about
00:25 - different devops technologies and devops
00:27 - concepts all the time now in this crash
00:30 - course you will learn how to build a
00:33 - basic ci city pipeline that will run
00:36 - tests build a docker image and push to
00:39 - docker hub's private repository and then
00:41 - finally deploy the newly built image to
00:44 - a remote ubuntu server and while
00:46 - building the pipeline you will learn the
00:49 - core concepts of how gitlab cicd works
00:52 - and the main building blocks such as
00:54 - jobs stages runners variables etc now of
00:59 - course you can learn only so much in one
01:02 - hour right and this will help you build
01:04 - foundational knowledge and get started
01:06 - with the gitlab cicd platform but if you
01:08 - want to dive deeper and start building
01:11 - real life devops pipelines for releasing
01:14 - your applications i actually have a
01:16 - complete gitlab cicd course on it which
01:19 - i have linked in the video description
01:21 - so if you're interested you can check it
01:22 - out there
01:23 - so with that let's get started
01:26 - first of all what is gitlab cicd and why
01:29 - should you even care
01:30 - now gitlab platform generally is
01:34 - striving to become the devops platform
01:37 - or a one-stop shop for building devos
01:39 - processes for your applications
01:41 - so they have exactly this roadmap and
01:43 - they're working towards that which means
01:45 - they're actually integrating and
01:46 - creating new features to basically give
01:50 - you
01:50 - everything in one platform to build
01:53 - complete devops processes and big part
01:55 - of those processes is a ci cd pipeline
01:59 - so first of all what is cicd in simple
02:03 - words
02:04 - csid stands for continuous integration
02:06 - and continuous deployment or continuous
02:09 - delivery and what it basically means is
02:12 - automatically and continuously
02:15 - testing
02:16 - building
02:17 - and releasing code changes to the
02:20 - deployment environment
02:22 - so that means when a developer commits a
02:24 - new code into the gitlab repository
02:27 - gitlab will automatically execute a cicd
02:30 - pipeline that you have configured for
02:33 - your project to release those code
02:36 - changes to the end environment where the
02:38 - end users can access them but this icd
02:41 - concept is a topic of its own so if you
02:45 - want to understand it on a deeper level
02:47 - then you can check out my other video
02:49 - about devops and ci city pipeline where
02:52 - i explain this in more detail but as i
02:55 - said in simple terms cicd is to
02:58 - continuously release your code changes
03:01 - to the end environment and in this crash
03:04 - course we will be building a simplified
03:06 - version of a ci cd pipeline using gitlab
03:09 - ci cd
03:13 - and of course there are many ci cd tools
03:16 - one of the most used ones in the
03:18 - industry still being jenkins and gitlab
03:21 - cicd is just one of those other csd
03:24 - tools and all of them have their
03:26 - advantages and disadvantages but a big
03:29 - advantage of using gitlab to build csd
03:32 - pipelines for your applications is that
03:35 - you already have your code on gitlab so
03:38 - this is an extension of your software
03:41 - development processes in your team where
03:44 - you can also build ci city pipelines on
03:47 - the same platform so your team already
03:49 - works with gitlab you have your code
03:51 - there so this is basically additional
03:53 - feature that you can extend your
03:55 - workflows on gitlab with and you don't
03:57 - need a separate tool for that apart from
04:00 - that gitlab makes that extension very
04:03 - seamless by allowing you to get started
04:05 - without any setup effort and also having
04:08 - your pipeline as part of your
04:11 - application code
04:12 - compared to jenkins for example where
04:15 - you have to set up and configure the
04:17 - jenkins server create a pipeline and
04:19 - then connect it to the git project
04:22 - with gitlab you can start without any of
04:24 - this configuration effort and we will
04:26 - see that in the demo part
04:28 - now if you don't have to set up anything
04:30 - and configure any servers to run the
04:33 - pipelines how does it actually work and
04:36 - this leads us to the topic of gitlab
04:38 - architecture and how it works you have a
04:41 - gitlab instance or gitlab server
04:44 - that hosts your application code and
04:47 - your pipelines and basically the whole
04:49 - configuration so it knows what needs to
04:52 - be done and connected to that gitlab
04:55 - instance you have multiple gitlab
04:57 - runners which are separate machines
04:59 - connected to the gitlab server machine
05:02 - which are actually the ones executing
05:04 - the pipelines so gitlab server knows
05:07 - what needs to be done and gitlab runner
05:09 - actually does that
05:11 - and gitlab.com is actually a managed
05:13 - gitlab instance that offers multiple
05:16 - managed runners already out of the box
05:19 - so these are all maintained by gitlab
05:22 - and that's why you can start running
05:23 - your pipelines without any setup and
05:25 - configuration effort
05:27 - using this managed setup and this is
05:30 - already enough for starting out but of
05:32 - course for your own organization you may
05:35 - want to manage the runners or the whole
05:37 - setup yourself so you can create
05:40 - partially or completely self-managed
05:43 - gitlab setup as well in this crash
05:45 - course we will use gitlab's managed
05:47 - infrastructure and free features to
05:51 - build our release pipeline so we will
05:53 - not need to configure anything ourselves
05:56 - in my gitlabcacity complete course
05:58 - however you actually learn to create and
06:00 - connect your own runners to the
06:02 - gitlab.com instance so now we know what
06:06 - gitlab csd is how it works how the
06:08 - architecture works so with that let's
06:11 - get started with the demo project and
06:13 - learn how to build a ci city pipeline
06:19 - and for the demo project we're going to
06:21 - use a python application so we're going
06:24 - to be building a simple ci cd pipeline
06:27 - for a python app
06:29 - where we execute the tests we build a
06:32 - docker image from the python application
06:34 - and then we deployed and run it so it's
06:37 - a web application and the code is from
06:40 - this repository on github i will link
06:42 - this as well in the video description so
06:45 - that's basically the original
06:46 - application it's a demo app that
06:48 - provides some system information and
06:50 - that's how the ui looks like you don't
06:52 - have to understand the code behind we
06:54 - are just going to concentrate on how to
06:56 - take this application and deploy it
06:59 - using a ci cd pipeline i have made just
07:02 - a couple of adjustments here and i have
07:04 - created the repository on gitlab because
07:06 - that's where we're going to be working
07:07 - with and i'm going to highlight any part
07:10 - of this application code that is
07:12 - relevant for us in order to build the
07:14 - cicd pipeline the rest of them you don't
07:16 - need to worry about you don't even need
07:18 - to understand how python applications
07:20 - are written so what i'm going to do
07:22 - first is i'm going to clone this
07:23 - repository locally so that i can show
07:26 - you how to run this application locally
07:28 - and how to execute the tests because
07:30 - we're going to be running tests on the
07:32 - pipeline so we need to know how that's
07:34 - done so i'm going to grab the clone url
07:38 - so switching back to the terminal
07:41 - i'm going to do
07:42 - git clone
07:44 - and the project url
07:46 - and
07:47 - we called it gitlab cicd crash course
07:51 - and that's basically our application now
07:54 - i'm going to open these in a visual
07:55 - studio code so that we have a better
07:57 - visualization
08:08 - make it bigger and let's see exactly
08:10 - what parts
08:11 - of this application code is relevant for
08:14 - us so first as i said we're going to be
08:16 - running tests so in this application we
08:19 - have a couple of tests which we will run
08:22 - during the pipeline which is there to
08:25 - make sure that application code is
08:26 - correct so any changes that we make
08:28 - didn't break the application and the
08:30 - tests can validate that and in the
08:32 - source folder right here in app
08:35 - we have a folder called tests and inside
08:39 - there we have these two test files that
08:42 - include the test again you don't need to
08:44 - understand how these tests are written
08:46 - what they actually do
08:48 - we're using it as an example to
08:51 - demonstrate how to run tests for any
08:54 - technology any application inside the
08:57 - pipeline so you can apply this knowledge
08:59 - for any other application written in any
09:02 - other language so we have these tests i
09:04 - think there are four of them and they're
09:07 - using this configuration file in the
09:09 - test folder
09:10 - um to execute those tests so as a first
09:13 - step let's actually see how the tests of
09:16 - this specific project can be executed
09:18 - and i'm going to open a terminal for
09:20 - that and again in this specific project
09:23 - to execute the tests we have this make
09:25 - file that includes a couple of commands
09:27 - and one of them is test so using make
09:30 - test
09:32 - we can execute
09:34 - the tests basically so let's run it
09:39 - and there you go so first of all it's a
09:41 - general concept in any application with
09:43 - any programming language um applications
09:45 - have dependencies so these are
09:48 - third-party libraries that we included
09:50 - in our applications so we don't have to
09:52 - code them from script so we include code
09:54 - that other people wrote that we can use
09:56 - in our applications and these
09:57 - dependencies have to be downloaded from
09:59 - internet right because they live in some
10:01 - code repository so they need to be
10:03 - downloaded and made available locally so
10:06 - that we can actually run our application
10:09 - right and we have dependencies for tests
10:11 - as well so
10:12 - in python specifically those
10:14 - dependencies are
10:16 - defined in a file called
10:17 - requirements.txt
10:19 - again different languages have different
10:21 - files for that and at the beginning all
10:24 - the dependencies are downloaded that are
10:26 - needed for the test and then tests are
10:29 - executed
10:30 - you see here internally a pi test
10:33 - command gets executed and then we have
10:35 - result of text execution we have four
10:38 - tests and all of them passed so that's
10:40 - how it looks like and this is also a
10:42 - general principle when you run an
10:44 - application whether it's java or node.js
10:46 - or python application you always need
10:49 - that technology the tool installed
10:51 - locally right so if i run python
10:53 - application i need obviously python to
10:55 - be available on my machine if i run
10:58 - java application i need java to be
11:01 - installed and available locally so i
11:03 - have python i also need to have peep
11:05 - installed which is a package manager for
11:08 - python which is the tool that actually
11:11 - downloads and fetches those dependencies
11:13 - from the internet again other languages
11:15 - have their own tools for that
11:18 - and finally because we have a make file
11:20 - here to execute those commands we need
11:22 - make command as well to be available on
11:25 - the machine and this means that we need
11:28 - these three tools to also be available
11:31 - inside our pipeline when the tests are
11:34 - executed
11:36 - and we're going to see that a bit later
11:38 - now i'm going to do one more thing which
11:39 - is actually start the application
11:41 - locally and see how it looks in the
11:44 - browser and for that we have another
11:46 - command called make run
11:48 - and this will start the application on
11:51 - port 5000 if i want to change the port
11:54 - and i actually do want to change the
11:55 - port because
11:56 - i have already something running on port
11:58 - 5000 so i'm going to set port
12:02 - to let's do 5004
12:06 - and
12:07 - make run
12:08 - and this will start the application
12:10 - locally and then i can access that
12:12 - localhost
12:14 - port
12:15 - 5004.
12:17 - let's see that
12:20 - and there you go that's how the
12:21 - application looks like we have
12:24 - the monitoring dashboard info
12:28 - and some other stuff so
12:30 - that's the application that we want to
12:32 - release
12:33 - using a csd pipeline on a deployment
12:35 - server
12:36 - so now we know how the application looks
12:38 - like how to run the tests so let's
12:41 - actually go ahead and build a simple ci
12:43 - cd pipeline for this demo application
12:50 - so i'm going to stop this let's actually
12:52 - remove the terminal
12:54 - and now the question is how do we create
12:56 - a gitlab cicd pipeline
12:58 - well following the concept of
13:00 - configuration is code the whole pipeline
13:03 - will be written in code and hosted in
13:07 - the applications git repository itself
13:10 - in a simple yaml file and the file has
13:13 - to be called dot gitlab
13:15 - dash ci dot yemo so that gitlab can
13:19 - automatically detect that pipeline code
13:21 - and execute it without any extra
13:23 - configuration effort from our site so in
13:26 - the root of the project's repository
13:28 - we're going to create this yaml file and
13:31 - we're going to write all the pipeline
13:32 - configuration inside and we can actually
13:35 - do that directly in the gitlab ui as
13:37 - well so we don't have to switch back and
13:39 - forth from the editor to gitlab so i'm
13:41 - going to do that directly here
13:43 - create new file
13:46 - and as i said it has to be called dot
13:50 - gitlab
13:51 - dash ci dot
13:54 - yml and as soon as i type that in you
13:57 - see that it automatically fills out
14:00 - the rest because it detected that we're
14:02 - creating the pipeline code
14:04 - and now let's write our pipeline in a
14:07 - simple yaml format
14:12 - so the tasks in the cicd pipeline such
14:15 - as running tests building an image
14:18 - deploying to a server etc are configured
14:21 - as what's called jobs
14:23 - so let's create jobs for all those tasks
14:26 - so first let's create a job that will
14:29 - run the tests
14:31 - and we have a super simple syntax for
14:33 - that we simply write the name of the job
14:35 - so let's call it run
14:38 - test and underscore to separate the
14:41 - words is a standard syntax in the
14:44 - pipeline configuration code on gitlab so
14:47 - that's what we're going to use so that's
14:48 - the job name
14:50 - and inside the job we have a couple of
14:52 - parameters or a couple of attributes or
14:55 - things that we want to configure for the
14:57 - job and again know the syntax of yaml
15:00 - where we have the indentation for each
15:02 - attribute so the first attribute and the
15:05 - required attribute of a job is what's
15:07 - called a script
15:09 - and script is basically where we list
15:12 - any comments that should be executed for
15:15 - that job so for example run tests right
15:18 - make test is a command that needs to be
15:21 - executed in order to run the test so
15:23 - we're going to write that command right
15:25 - here so that's actually a job
15:27 - configuration to run the tests simple as
15:31 - that we have the name of the job and
15:33 - inside that we have a script that tells
15:35 - what should this job actually do and
15:37 - we're saying that it should run command
15:40 - called make test which will execute the
15:42 - tests just like we did locally but in
15:45 - order for this to run successfully we
15:47 - need to do a couple of things remember i
15:49 - told you that the make test command to
15:51 - be successful needs first of all make
15:54 - command to be available wherever this
15:56 - job runs but also in the background it
15:59 - will execute
16:00 - peep to install the dependencies and it
16:03 - will also execute python because tests
16:05 - are written in python so we need that to
16:08 - be available as well so
16:10 - these three things need to be available
16:13 - on the machine
16:14 - where this will run and now we come to
16:17 - the question of
16:19 - where will this job actually be executed
16:21 - on which machine on which environment
16:23 - is it a linux machine is it windows
16:26 - what is it
16:27 - as i mentioned in the beginning pipeline
16:29 - jobs are executed on gitlab runners
16:32 - and gitlab runners can be
16:35 - installed on different environments it
16:37 - could be different operating system like
16:39 - windows linux different
16:41 - distribution of linux etc and that's
16:44 - what's called a shell executor it's a
16:47 - simplest type of executor simplest
16:49 - environment what we know from jenkins
16:51 - for example where you have a simple
16:52 - server on linux machine and you execute
16:55 - shell commands on them as part of your
16:58 - job directly on the operating system
17:01 - but another common execution environment
17:03 - on githlab is dockercontainer so instead
17:06 - of executing the jobs directly on the
17:08 - operating system like on a linux machine
17:12 - for example
17:13 - we execute the jobs inside containers so
17:16 - the gitlab runner is installed on some
17:18 - linux machine and on that machine gitlab
17:22 - runner creates docker containers to run
17:24 - the jobs and the managed runners from
17:27 - gitlab that we get available out of the
17:30 - box actually use docker container as the
17:33 - execution environment so all our jobs
17:35 - that we write here will be executed
17:37 - inside docker containers
17:40 - but as you know containers run based on
17:42 - a certain image right you can't run a
17:45 - container without an image and depending
17:47 - on which image you use you're going to
17:49 - have different tools available inside
17:51 - that container so if i use a mysql image
17:54 - to start a container i'm going to have
17:56 - my sql inside plus some other tools that
17:59 - the image has if i have no js image then
18:02 - i'm going to have nodejs and npm
18:04 - available inside the container if i use
18:06 - a basic alpine image that i'm going to
18:10 - have basic linux commands and tools
18:12 - available inside so which image are
18:15 - gitlab's
18:16 - managed runners actually using to start
18:18 - those containers well by default
18:21 - currently at this moment
18:23 - gitlab's managed runners are actually
18:25 - using ruby image to start the containers
18:30 - that will run the jobs now in our case
18:32 - this is not going to work because
18:34 - for our test execution we actually need
18:38 - a container or an image
18:40 - that has
18:41 - python
18:42 - peep and make available inside them
18:45 - right so instead of ruby image we want
18:48 - to use a python image to execute this
18:51 - job and the good thing is that we can
18:54 - actually overwrite which image is used
18:57 - by the runner for each specific job so
19:00 - for each job we can say
19:02 - you know what forget ruby use this image
19:05 - instead and we can do that using an
19:07 - attribute on the job called image and
19:09 - then specifying whichever image we want
19:12 - and as i said in our case we're going to
19:15 - take python image
19:17 - from the docker hub
19:19 - so let's actually find the
19:22 - official python image
19:26 - so it's going to be called python and we
19:29 - can specify a tag here and it's actually
19:32 - a best practice to specify a tag instead
19:34 - of leaving it to the latest because you
19:37 - want your pipeline to be consistent
19:39 - right so instead of always fetching the
19:41 - latest image and since the latest image
19:43 - gets updated all the time you may get
19:46 - some unexpected behavior at some point
19:48 - when the latest image has some changes
19:51 - so we want to always fixate the version
19:54 - of the image and you have the list of
19:57 - those texts to select from here in our
19:59 - case we actually need a specific version
20:02 - of 3.9
20:04 - because that's what our application
20:06 - actually expects so i'm actually going
20:08 - to go ahead and select this image tag
20:12 - and make sure that you do the same
20:14 - so that's the version of the python
20:17 - image our application needs so now when
20:19 - the gitlab runner gets this job in order
20:21 - to execute it will see that we have
20:23 - overwritten the image so instead of
20:27 - taking ruby it will
20:29 - fetch python image with this image tag
20:31 - and it will start the container with
20:33 - that image in order to execute
20:35 - the script logic inside that container
20:38 - now this image
20:41 - makes python and peep tools available
20:43 - inside the container
20:45 - however we still don't have the make
20:47 - commit so we need that also inside the
20:50 - python container to be able to execute
20:52 - this command so how can we do that a
20:55 - simple way to do that is to install the
20:58 - make command inside that python
21:00 - container before the script gets
21:03 - executed
21:04 - and we can do that using another
21:06 - attribute on the job called before
21:10 - underscore script
21:13 - so this is basically the same as a
21:15 - script with the same syntax you can
21:17 - execute the same commands here as in
21:19 - script but gitlab runner knows that
21:21 - whatever is defined here
21:23 - should run before the script so this is
21:26 - usually used in jobs to prepare
21:29 - environment like said environment
21:31 - variables create any temporary files
21:33 - maybe fetch the data from somewhere or
21:36 - in this case install something
21:38 - that the main script actually needs for
21:41 - execution
21:42 - so that's what we're going to do here
21:44 - and we're going to install make just
21:45 - like you would install it on any linux
21:47 - server with a package manager apt-get so
21:51 - we're going to do apt-get update
21:54 - and
21:55 - apt-get
21:56 - install make
21:59 - so this will take care of
22:01 - installing make inside that container so
22:03 - we will have all three tools that we
22:05 - need available in order to execute this
22:08 - command
22:09 - so now
22:11 - with this configuration we already have
22:14 - a working pipeline with just one job
22:16 - which runs tests but still we have a
22:18 - valid working pipeline that we can
22:21 - execute for our application
22:23 - so how do we actually execute the
22:25 - application the only thing we need to do
22:28 - is simply commit this file and as soon
22:31 - as we commit the changes
22:33 - gitlab will actually detect
22:36 - that we have added a pipeline
22:38 - configuration and it will automatically
22:41 - execute the pipeline in the background
22:43 - so the pipeline is actually already
22:45 - running
22:46 - now where do we see the pipeline
22:48 - execution well right here we are in the
22:51 - repository section which lets you manage
22:53 - the files
22:55 - commits branches and so on for the ci cd
22:59 - there is a separate section
23:01 - that lets you manage the cicd part of
23:04 - the project and the good thing is you
23:06 - don't have to switch to some other view
23:08 - you stay inside the project you have all
23:10 - the configuration in one place and
23:13 - the first tab here is pipelines and this
23:17 - is a list view so this is going to be a
23:19 - list of all the pipeline executions for
23:21 - your project and we see that our first
23:24 - pipeline was successful it has passed
23:27 - state and if i click inside
23:31 - that's the detail view of the pipeline
23:34 - we actually see all the jobs that ran
23:37 - for the pipeline in our case we just
23:39 - have one job we also have a separate job
23:42 - section where you can see the list of
23:43 - the jobs
23:44 - and if i click inside the job you will
23:46 - see the logs which is obviously useful
23:49 - if the job fail for example for
23:52 - troubleshooting and debugging so let's
23:55 - actually take a look at our logs
23:59 - to highlight some interesting and
24:01 - important information first of all right
24:04 - here you see that the job
24:06 - is being executed using docker
24:09 - right here you see using docker executor
24:11 - with image and this is the image that we
24:14 - specified so by default it would have
24:15 - used ruby but since we overwrote that it
24:18 - is using image that we defined and it's
24:21 - pulling the image and starting the
24:22 - container from it
24:24 - once the container is created now it of
24:27 - course has to fetch the whole git
24:29 - repository so these projects code
24:32 - basically in that docker container where
24:35 - we have the test files dependencies file
24:38 - and everything else to be able to
24:40 - execute tests right
24:43 - but before the main script the before
24:45 - script gets executed which installs make
24:50 - and once that's installed then make test
24:53 - command gets executed then we have pip
24:55 - install the dependencies
24:57 - and
24:58 - at the end we have
25:00 - our four tests that all run and they are
25:03 - in the past state so the first step or
25:07 - first job of our pipeline is configured
25:10 - and working before moving on i want to
25:12 - give a shout out to twin gate who made
25:14 - this video possible twingate is a remote
25:17 - access service that allows you to easily
25:20 - access private services within any cloud
25:23 - or in your home lab for ci city
25:26 - workflows that execute outside of your
25:29 - environment twin gate includes a service
25:31 - account feature that allows
25:33 - secure and automated access to your
25:36 - private resources
25:37 - twin get is actually a modern
25:39 - alternative to a vpn that is easier to
25:42 - set up offers better performance and is
25:45 - more secure if you need to create a csd
25:48 - workflow to deploy into your private
25:50 - kubernetes clusters or if you have
25:52 - internal databases you need to access
25:55 - then it is an excellent choice with
25:58 - twingate you don't need to open any
25:59 - ports on your firewall or manage any
26:02 - ipwhitelists you will also find
26:04 - providers for terraform and polumi so
26:07 - you can fully automate it into your
26:09 - existing stack
26:11 - if you want to try it you can use their
26:13 - free tier and get started for free or
26:15 - twin gate has actually provided a
26:17 - special offer for my viewers
26:19 - you can use the code nana to get three
26:22 - months free of the business tier in
26:24 - addition to their 14-day free trial
26:30 - so now let's go back to the pipeline
26:32 - and at the next job that will build
26:36 - and push a docker image of our python
26:40 - application
26:41 - and to switch back to our pipeline code
26:44 - we can go to the repository and
26:48 - basically
26:49 - in the gitlab ci file
26:51 - or we actually have a shortcut here in
26:53 - the ci cd section
26:55 - with the editor tab that takes you
26:58 - directly
26:59 - to your gitlab ci yml code in the edit
27:02 - mode so you can continue working here on
27:06 - your configuration now let's create the
27:08 - next job where we build a docker image
27:10 - and push it to docker hub's private
27:13 - repository
27:14 - so first of all to have a private
27:16 - repository on docker hub you can simply
27:18 - sign up and create an account by default
27:20 - in the free version basically you get uh
27:24 - one private repository so that's what
27:26 - we're gonna use and i have some other
27:28 - apps here but that's what i'm gonna use
27:30 - for the demo so that's the address of my
27:32 - private image registry on the docker hub
27:36 - and i'm going to be pushing an image
27:39 - that the pipeline builds into this
27:41 - repository
27:45 - now in order to push an image to a
27:48 - private image repository we need to log
27:52 - in into that repository first because of
27:54 - course we don't want to allow anyone to
27:56 - pull and push images from a private
27:58 - repository it has its credentials and
28:01 - you have to log in before you actually
28:04 - push an image or pull an image from it
28:07 - so we would need repository credentials
28:10 - for this repository to be available on
28:13 - gitlab so that gitlab runner can
28:16 - actually log into the registry before it
28:18 - pushes the image
28:20 - and the credentials are actually
28:22 - username and password of your docker hub
28:25 - account so this is what we're going to
28:27 - use to log in to the private repository
28:31 - and of course we don't want to hard code
28:33 - those credentials that username and
28:34 - password inside the pipeline code
28:37 - because again this is part of the
28:38 - repository so anyone that has access to
28:42 - the repository we'll be able to see that
28:44 - plus it's going to be in plain text and
28:46 - so on so it's not secure
28:48 - so we want to create what's called
28:51 - secret type of variables
28:53 - that will be available in our pipeline
28:56 - and the way it works on gitlab is in the
28:59 - projects settings so
29:01 - if you scroll all the way down you have
29:03 - this settings tab here
29:05 - where you can actually
29:07 - configure some of the administration
29:09 - settings
29:10 - for different parts of your project so
29:13 - you have settings for the repository you
29:15 - have settings for ci cd and so on and if
29:17 - you're wondering why it is actually
29:20 - separated so why don't i have the
29:22 - settings here directly in the ci cd or
29:24 - the repository settings here directly in
29:27 - the repository is that this would
29:29 - actually be two different roles so for
29:32 - your project you may have
29:34 - project administrators to actually
29:36 - administer
29:37 - and manage the settings of the project
29:40 - and you will have the project users
29:42 - these are your developers junior
29:44 - developers
29:46 - interns senior developers whoever needs
29:48 - access to the project to work in it
29:51 - right so you may want to separate those
29:53 - permissions and have just a dedicated
29:56 - people who have access to the settings
29:58 - responsible for that part and then your
30:00 - developers or most of the developers do
30:02 - not see the settings at all so that's
30:05 - basically the idea of having this cicd
30:08 - setting separately and
30:10 - the pipeline configuration separately so
30:13 - in the settings of the ci cd if i click
30:16 - inside
30:17 - and we can leave the edit mode of the
30:19 - pipeline that's fine
30:21 - right here we have
30:22 - again different settings for our csd
30:26 - pipelines for the project which an
30:28 - administrator of the project can
30:29 - configure
30:30 - so obviously this would be someone more
30:33 - experienced and knowledgeable about how
30:35 - to manage all these that's why you want
30:37 - to give maybe a limited set of people
30:40 - permission to see and modify settings of
30:43 - the project so right here we have
30:46 - multiple things we can configure and one
30:49 - of them is project variables and this is
30:52 - where we can create custom variables
30:55 - this could be secret variables like
30:57 - password username but also just normal
31:00 - variables right and this will be then
31:03 - available
31:04 - inside the
31:06 - pipeline code so we can actually
31:07 - reference those variables in our
31:10 - pipeline configuration so if i expand
31:12 - this we can create
31:14 - variables here
31:16 - click on add variable and we're gonna
31:18 - define docker
31:21 - user
31:22 - or this could also be
31:25 - registry user
31:27 - and here we're gonna have
31:28 - the value so i'm actually gonna copy
31:31 - that
31:33 - and this is the
31:34 - docker id
31:36 - so your username on dockerhub and
31:39 - the type variable and we're gonna click
31:43 - on mask variable so what this will do is
31:45 - whenever the variable is referenced and
31:47 - used inside a job for example job
31:50 - execution of the pipeline the value will
31:53 - be masked so it's not going to be
31:55 - visible in the job logs which is of
31:57 - course more secure so for secret or
32:01 - sensitive data we're gonna check that
32:04 - so that's it that's gonna create our
32:06 - first variable
32:08 - and now let's also create registry
32:10 - password
32:12 - let's call it registry pass
32:14 - and again mask the variable
32:17 - and
32:18 - for that you're gonna grab the password
32:25 - and there you go we have those two
32:27 - variables and now we can actually
32:29 - reference them inside our
32:32 - pipeline configuration
32:36 - so i'm going to go back to the editor
32:39 - and right here we're going to create a
32:41 - job called
32:42 - build
32:43 - image obviously you can call it whatever
32:45 - but i'm going to call it build image
32:48 - and let's define the main script so what
32:51 - is the main logic
32:52 - that we want to
32:54 - do in this job
32:56 - well first we want to build a docker
32:58 - image of our python application from
33:01 - dockerfile and we actually have a
33:03 - dockerfile as well in the project so
33:05 - right here in the root of the project we
33:08 - have this file
33:10 - that defines the python base image which
33:12 - is by the way the same as we used right
33:16 - here
33:16 - to run tests
33:19 - and the rest of the configuration is
33:20 - simply copying the relevant files inside
33:24 - the image
33:25 - installing all the dependencies from the
33:27 - requirements file and then finally
33:29 - starting the application on port 5000 so
33:32 - that's what's happening we already have
33:34 - the docker file so we're going to use
33:36 - that to build the image and as i said it
33:38 - is in the root of the project so we're
33:41 - going to do docker build and since our
33:45 - gitlabci.yaml file is also in the root
33:47 - this is going to be the current location
33:49 - of dockerfile which is the current
33:51 - directory
33:52 - now in order to build an image that we
33:55 - can later push into our repository we
33:58 - have to take that image using the
34:01 - repository name so i'm gonna copy that
34:04 - and i'm gonna add minus t so that's for
34:08 - tagging the image with the repository
34:10 - name so in docker the name of the image
34:12 - includes the repository name so that
34:15 - docker knows where to push that image
34:17 - when you execute docker push command and
34:19 - you also need to have a tag just like
34:22 - here right
34:23 - and in this case let's hard code a value
34:26 - here let's do python
34:29 - app 1.0
34:32 - and this will be the complete
34:34 - name of our image
34:36 - and then
34:37 - once we build that image
34:39 - we're gonna do docker push because we
34:41 - need to push that image
34:43 - and
34:45 - i'm gonna copy the name so that's going
34:48 - to be the push image and as i said in
34:51 - docker
34:52 - the way docker knows where you want to
34:53 - push that image or the address of the
34:55 - repository is inside the image name
34:58 - itself so it knows on this docker
35:00 - registry you have a repository called
35:03 - this and you want to take the image
35:05 - using
35:06 - this specific tag but before we can do
35:09 - docker push obviously we need to
35:10 - authenticate
35:12 - with the repository otherwise it's not
35:14 - going to work so right here before push
35:16 - we need to execute docker login command
35:19 - which takes user name and password as
35:21 - parameters so we have
35:24 - dash u for username and
35:27 - now instead of hard coding the value
35:29 - here we can reference the variable that
35:31 - we created in the settings called
35:34 - registry user and we can reference it
35:36 - super simply using the dollar sign and
35:39 - then name of the variable registry
35:42 - underscore user
35:44 - and we also have the password
35:47 - and that's going to be
35:49 - registry
35:51 - pass that's what we called it
35:53 - and for doc hub so if we're using docker
35:55 - hub we don't have to specify the docker
35:58 - registry because docker hub is the
36:00 - default but if we're logging into some
36:03 - other docker registry like on aws ecl
36:05 - for example
36:06 - then we would have to specify the
36:09 - registry right here but as i said docker
36:12 - hub is a default so we don't have to do
36:13 - that and since the docker build and push
36:16 - our main commands and docker login needs
36:18 - to be executed before we can push to the
36:20 - repository we can actually set it
36:23 - in before script
36:25 - section
36:30 - like this
36:32 - so we have our main commands and
36:35 - supporting commands so to say separated
36:37 - like this
36:38 - so that's our job configuration
36:40 - and by the way when we're repeating the
36:42 - values inside our pipeline we can also
36:46 - extract them
36:47 - into custom variables so for example the
36:50 - repository name and the image tag could
36:53 - be variables
36:55 - inside the pipeline then we can then
36:57 - reference here instead of hard coding
36:59 - them and then repeating the value
37:01 - multiple times and for that you don't
37:03 - have to go to settings and create these
37:05 - global variables you can simply define
37:07 - them here inside the pipeline
37:09 - configuration either on a job level like
37:12 - this
37:14 - so we would have
37:15 - something like image name
37:21 - and this will be the image name and
37:23 - image tag
37:26 - like this and then we just reference
37:28 - both variable values using the same
37:31 - syntax we used to reference the
37:32 - variables from the ci cd settings so
37:35 - with dollar sign variable name
37:37 - and
37:38 - image
37:40 - tag
37:43 - and the same here
37:45 - and now that the uppercase letters with
37:48 - underscore is just a standard you can
37:49 - obviously call it whatever you want it
37:51 - could be also image name it doesn't
37:53 - matter
37:54 - and as i said you can define the
37:56 - variables on a job level or if you have
37:58 - other jobs that also reference this
38:01 - image name and tag for example in a
38:03 - deploy stage for example if we also need
38:06 - the same variables we can extract that
38:08 - and make it available
38:10 - on a pipeline level for all the jobs
38:14 - like this so even though our job logic
38:17 - is complete we still have to make sure
38:20 - that the job execution environment of
38:23 - this specific job has all the needed
38:26 - tools to actually execute these commands
38:29 - so as we know on managed gitlab runners
38:32 - all our jobs will run in the docker
38:34 - container and in that docker container
38:36 - we need to have the commands whatever
38:39 - we're executing inside the job available
38:42 - otherwise it's going to say it cannot
38:44 - find the command right so in this case
38:47 - we have a little bit of specific
38:49 - situation
38:51 - where we need docker to be available
38:53 - inside a docker container and that is
38:56 - what's called docker in docker concept
38:59 - so within a docker container you have
39:01 - docker daemon and docker client both
39:04 - available in order to execute docker
39:06 - command and again the default is a ruby
39:08 - image but we need an image that will
39:10 - have docker available
39:12 - inside the docker container and there is
39:14 - actually an official image
39:17 - of docker
39:20 - right here
39:25 - that is
39:26 - meant exactly for that scenario where
39:28 - you have docker in docker so you build
39:30 - the docker container with a docker image
39:33 - which makes again docker commands or
39:36 - docker client and daemon available
39:38 - inside your container in order to
39:40 - execute docker commands so
39:42 - that's what gitlab cicd official
39:45 - documentation also references whenever
39:47 - you have this kind of use case and the
39:49 - configuration is actually pretty simple
39:51 - the first thing we need to do is
39:53 - obviously set the image to this docker
39:55 - image and we can actually take one of
39:57 - the latest tags
40:00 - so right here we're gonna do docker
40:03 - and the image tag so this will take care
40:06 - of starting a docker container from
40:08 - docker image so we have the docker
40:11 - client available inside however we also
40:13 - need docker daemon available inside
40:16 - right so docker demon is the process
40:18 - that lets the client actually execute
40:21 - those commands connect to the docker
40:22 - registry pull the image push the image
40:25 - etc so in order to make that available
40:28 - we actually need to configure another
40:30 - attribute which is called services on
40:32 - gitlab so let's actually define that and
40:34 - i'm going to explain what that means so
40:36 - service is basically an additional
40:39 - container that will start at the same
40:41 - time as
40:42 - the job container
40:44 - and the job container can use that
40:46 - service
40:47 - during the build time so for example if
40:50 - your test execution needs some kind of
40:52 - service like a database service for
40:54 - example mysql then you can actually
40:57 - start a mysql service or mysql container
41:01 - during your job execution
41:03 - in addition to this python container and
41:06 - the service attribute will make sure
41:08 - that these containers are linked
41:10 - together so they run in the same network
41:12 - and they can talk to each other directly
41:14 - so this way python container will
41:17 - actually have access and can use the
41:20 - mysql service from the other container
41:23 - so that's basically what the concept of
41:25 - services means in gitlab cicd so right
41:28 - here what we're doing is we have this
41:30 - docker container with docker client
41:33 - inside and we want to start another
41:35 - docker container with docker daemon
41:37 - inside so the client can actually
41:39 - connect to that daemon
41:41 - the docker server and then execute
41:44 - these commands and then image for the
41:47 - docker demon is actually
41:49 - this tag right here
41:53 - so that's going to be docker
41:56 - and
41:57 - the same version so that's the client
42:00 - that's the daemon with dind which is
42:04 - docker in docker
42:05 - so these two
42:07 - will basically give us the complete set
42:09 - of docker client and server in the same
42:12 - job execution environment
42:15 - so these two containers will be linked
42:16 - to each other and
42:18 - they will be able to communicate with
42:20 - each other there is one final thing we
42:22 - need to do here which is
42:24 - to make sure these two can communicate
42:26 - with each other using docker
42:28 - certificates so these two needs to have
42:31 - the same certificate so that can
42:33 - authenticate with each other and talk to
42:35 - each other and for that we want those
42:37 - two containers that will start to share
42:40 - that certificates directory so they can
42:42 - read from the same certificates
42:45 - folder
42:46 - and we can do that
42:48 - by defining a variable or environment
42:51 - variable called docker tls
42:54 - cert dear and setting that to slash
42:56 - search so what this will do again
42:59 - is this will tell docker to create the
43:01 - certificates in this location and then
43:03 - this certificate will be shared between
43:05 - the service container and the job
43:08 - container so with this configuration we
43:10 - have a full docker setup
43:12 - of client and a server or docker daemon
43:15 - and they can talk to each other
43:17 - authenticate with each other and so on
43:19 - and this way we're going to have docker
43:20 - commands available inside our job
43:26 - great so now let's actually commit our
43:29 - changes and gitlab will automatically
43:31 - trigger a pipeline for us
43:35 - you see that pipeline execution here
43:37 - if i do
43:39 - view pipeline
43:41 - now you see that two jobs are being
43:43 - executed inside the pipeline
43:46 - both running at the same time
43:48 - so let's wait for them to finish
43:50 - and there you go both jobs were
43:52 - successful which means we should
43:54 - actually already see an image
43:57 - in our docker hub registry so i'm going
43:59 - to go to my repository
44:02 - and
44:03 - right here
44:04 - pushed a few seconds ago we have python
44:07 - app 1.0
44:10 - so we have successfully built and pushed
44:13 - a docker image
44:15 - to
44:16 - docker registry using gitlabcd pipeline
44:23 - however there is one issue here which is
44:26 - that both jobs get executed at the same
44:28 - time and if we add
44:30 - deploy job it will also run
44:33 - in parallel to these two jobs and this
44:36 - makes no sense for us because we want to
44:38 - run these jobs in order right so we want
44:41 - to run the tests and only if the tests
44:44 - are successful we want to build and push
44:47 - the image and only if build and push job
44:50 - was successful
44:51 - then we want to deploy that image
44:53 - because if
44:55 - build
44:56 - fails we have nothing to deploy right
44:58 - so how can we
45:00 - force this order in which the jobs will
45:02 - execute in the pipeline well we can do
45:05 - that using what's called stages
45:08 - so the stages can be used to organize
45:10 - the pipeline by grouping
45:12 - related jobs
45:14 - that can run in parallel together so for
45:17 - example if you have different tasks that
45:19 - you run for your applications like unit
45:21 - tests lead tests whatever they can all
45:25 - run in parallel by being in the same
45:27 - stage so let's see how that works
45:30 - going back to the editor
45:33 - we want to put the run test in one stage
45:36 - and then have build image as the next
45:38 - stage so build image will basically wait
45:41 - for the run test to execute and only if
45:43 - the run test was successful it will
45:46 - execute the build image job
45:48 - and it's super easy we have the stages
45:52 - attribute
45:53 - where we can create
45:55 - a list of stages and we can call them
45:57 - whatever we want i'm going to create
45:59 - test stage
46:00 - and
46:02 - build stage
46:03 - and then
46:04 - from those stages that you have defined
46:06 - here you can reference them within your
46:09 - jobs so right here we can say
46:12 - run tests in test stage
46:15 - and build image in
46:18 - build stage
46:20 - that's it that's the whole configuration
46:22 - so
46:23 - i'm going to commit that
46:26 - and let's see the result so the pipeline
46:28 - got triggered if i go to pipelines
46:33 - the first difference right away that you
46:35 - see here is we have two stages now so we
46:37 - have test stage and build stage whereas
46:41 - here by default when you don't specify
46:43 - any stages gitlab gives you a test stage
46:46 - so everything gets executed in that test
46:48 - stage and if we go in the detail view of
46:52 - the pipeline
46:54 - you also see a visualization of those
46:57 - stages so instead of the jobs being just
47:00 - listed here within one stage you now
47:02 - have
47:03 - the stages listed next to each other and
47:06 - the jobs in the build stage will wait
47:09 - for any jobs in the test stage to
47:11 - complete before they run so that's how
47:14 - the stages looks like and when we add
47:16 - more stages they will basically
47:17 - disappear next to each other
47:22 - so these two steps are configured now
47:24 - let's add the final job to our pipeline
47:28 - to deploy that newly built docker image
47:31 - to an ubuntu server
47:33 - and run that docker application there
47:36 - for that we first need a deployment
47:38 - server right we need a server that we're
47:40 - gonna deploy to and run our application
47:46 - to make this easy we will create a
47:48 - simple ubuntu server on a digitalocean
47:51 - platform
47:53 - it's way easier to set up and configure
47:55 - than aws if you don't have an aws
47:58 - account so that's the platform we're
48:00 - going to use
48:01 - now if you have a ubuntu server or other
48:03 - server you're welcome to use that as
48:06 - well so this is not specific to any
48:08 - cloud platform we just need an ubuntu
48:11 - server that is connected to internet
48:13 - that we can deploy to and on
48:15 - digitalocean actually you can sign up a
48:18 - new account with a hundred dollar credit
48:20 - so make sure to take advantage of that
48:22 - if you register here so that you can
48:24 - work through the demo for free i already
48:26 - have an account so i'm gonna log in
48:29 - and i'm starting with a completely clear
48:31 - state i don't have anything configured
48:33 - yet
48:34 - so we're going to do all this together
48:36 - first of all when we create the server
48:38 - we will access that remote server on
48:41 - digitalocean
48:42 - from our local machine using an ssh
48:45 - command into ssh into a server obviously
48:49 - we need an ssh key
48:51 - and on digitalocean we can add our own
48:54 - ssh key in the settings here so in
48:57 - settings security
49:00 - you have a section here that says ssh
49:02 - keys and you can add one right so you
49:04 - can generate an ssh key pair
49:07 - you can upload the public key of that
49:10 - keep here to digitalocean and then all
49:13 - the servers that you create on
49:14 - digitalocean will be accessible using
49:17 - the public key that you uploaded here so
49:20 - that's how it's going to work so what
49:22 - we're going to do is
49:24 - actually create a new ssh key locally
49:28 - which is very simple
49:32 - i'm going to leave the project and right
49:34 - here i'm going to execute a simple ssh
49:37 - key gen command
49:39 - and this will basically generate an ssh
49:42 - key pair for us so enter and this is the
49:44 - default location where your ssh keys are
49:47 - and this is the default key pair that is
49:50 - always used when you ssh so we're gonna
49:54 - use a different name inside the dot ssh
49:58 - folder in your users directory and let's
50:01 - call this digital
50:03 - ocean
50:04 - key
50:06 - we're gonna leave the passphrase empty
50:10 - repeat
50:11 - and there you go that was it
50:13 - and now
50:14 - inside that ssh
50:16 - folder
50:17 - we should actually see our digitalocean
50:20 - keypair so we should have two keys one
50:23 - of them is public and this is the
50:25 - private one so that's the one we need to
50:28 - upload to
50:30 - the platform and then we're gonna be
50:32 - able to connect to any server created on
50:35 - the platform using the private key and
50:37 - to upload that we're simply gonna
50:40 - cut this
50:42 - and obviously i need the whole path
50:46 - like this
50:48 - and that's our
50:51 - public key so i'm gonna copy that
50:54 - to new ssh key
50:56 - copy the contents let's give it a name
50:59 - let's call it a server
51:01 - or deployment
51:02 - server
51:06 - key and
51:09 - add ssh key
51:11 - and that's it
51:12 - so now
51:14 - i'm going to go back to
51:15 - the droplets
51:18 - and create a deployment server so click
51:20 - on create droplet
51:23 - i'm going to choose ubuntu with the
51:25 - latest version
51:26 - basic plan
51:27 - regular with ssd and i'm going to select
51:30 - the smallest server with the smallest
51:32 - resources because we're just going to be
51:34 - deploying one application container
51:36 - which is our python app so we don't need
51:40 - much resources and finally we're gonna
51:43 - choose the
51:44 - region
51:45 - which is closest to our location
51:48 - and that's it all the other stuff will
51:50 - stay the same
51:51 - right here in the authentication
51:53 - you already see that ssh keys is
51:57 - selected so that's going to be
51:59 - how we connect to the server and that's
52:02 - the key that we edit
52:03 - so we leave everything else
52:06 - with defaults
52:08 - we are creating one droplet
52:10 - and
52:12 - create
52:14 - and let's wait for our droplet to be
52:16 - fully initialized
52:19 - there you go
52:20 - and once it is initialized we're gonna
52:23 - need to configure one thing
52:25 - on that server before we are able to
52:28 - deploy
52:29 - and run our docker image to that server
52:32 - which is
52:33 - we need to install docker because we're
52:35 - going to be running docker container on
52:37 - it so we need docker available and for
52:39 - that we're going to grab the public ip
52:41 - address of our droplet of our server
52:45 - which is this one right here let's copy
52:47 - and we're going to connect to that
52:49 - machine locally and we're going to ssh
52:52 - into the server using an ssh command
52:55 - and the ip address of the server which
52:58 - is a public ip address
53:00 - but of course when we ssh into a remote
53:02 - server we need to authenticate ourselves
53:05 - right so we need to provide credentials
53:07 - in our case username
53:09 - or linux username and private key
53:13 - so the linux user on droplet servers is
53:16 - root so that's the user
53:19 - we're connecting with and here we're
53:21 - going to specify the private key that we
53:24 - want the ssh command to use to connect
53:26 - to the server using dash i option
53:30 - and we need the location so that's the
53:34 - home directory of my user dot ssh
53:38 - and
53:40 - digitalocean key
53:43 - so with this command we should now be
53:45 - able to
53:46 - connect to the server so let's execute
53:50 - and this is a confirmation to save that
53:52 - remote server locally as a known host so
53:55 - let's confirm
53:57 - and there you go we are inside our
53:59 - droplet connected as a root user and as
54:03 - i said we need to install docker on this
54:05 - machine so if i do docker obviously
54:08 - right now it's not available and i'm
54:10 - going to use this suggested command to
54:12 - install docker but before that we're
54:14 - going to do
54:15 - apt
54:16 - update
54:19 - and now we can do apt install docker
54:24 - there you go
54:27 - and now we should have docker commands
54:29 - available let's do docker ps to check
54:32 - and there you go currently nothing is
54:34 - running but we're going to deploy to the
54:36 - server
54:37 - from our pipeline so we can exit from
54:40 - the server our work is done here
54:47 - and we're going to go back to our
54:49 - pipeline configuration
54:51 - and add a third job that will deploy to
54:54 - that server
54:55 - so how is gitlab going to deploy a
54:57 - docker image to the droplet server or
55:00 - how is it going to connect to that
55:01 - server in order to deploy the docker
55:04 - image well actually the same way as we
55:07 - did locally using ssh command so again
55:10 - gitlab runner will start a container for
55:12 - the deploy job and inside the container
55:15 - we're going to execute ssh command to
55:17 - connect to the remote server
55:21 - and
55:22 - for that ssh command we're going to need
55:24 - exactly the same information right so
55:26 - we're going to have the root user and
55:28 - we're going to need that private key to
55:31 - connect to the server which means we
55:33 - need to make this private key also
55:35 - available to gitlab and just like we
55:38 - created variables secret variables for
55:41 - docker hub private repository we're
55:43 - going to create a secret variable for
55:46 - the private key
55:48 - in the cicd settings so going to
55:51 - settings cicd
55:54 - in the variables section we're going to
55:57 - add a third variable and we're going to
55:59 - call this
56:01 - ssh
56:02 - key
56:03 - and the value of that key will actually
56:06 - be the contents of
56:09 - that
56:10 - key file right the private key file so
56:17 - i'm simply going to cut that
56:19 - print that out to the console
56:21 - like this
56:22 - and copy this whole private key
56:25 - and paste it in here
56:27 - so these are the file contents of the
56:29 - ssh key and right here in the type field
56:33 - if i click inside this drop down you see
56:36 - that we have two options we have
56:37 - variable which is a simple text variable
56:40 - and we have a file in this case we
56:42 - actually want to have a file variable
56:46 - because we're going to reference this as
56:47 - an ssh key file so let's select that one
56:52 - and there's one kind of a workaround so
56:54 - that gitlab can create a temporary file
56:57 - from this contents with a correct format
57:00 - and it's kind of a weird workaround
57:02 - to fix the issue so it could be a bug in
57:05 - gitlab i'm not really sure so what we're
57:07 - going to do is we're going to add a new
57:09 - line here at the end of the contents so
57:12 - that a correct private key format is
57:15 - created by gitlab so what gitlab will do
57:18 - in the job execution environment it will
57:20 - take actually these contents of the
57:22 - variable and it will create a temporary
57:24 - file
57:25 - from this because we have a file type
57:28 - specified so it will create a temporary
57:30 - file
57:31 - with the contents that we have here
57:33 - we're going to add the variable so now
57:36 - this ssh key file will be available
57:38 - inside the pipeline so let's go to
57:41 - editor
57:43 - and
57:44 - first i'm going to add a new stage here
57:46 - let's call this deploy
57:48 - and here i'm going to create a new job
57:51 - called deploy
57:53 - you can call it deploy to
57:55 - development whatever
57:57 - i'm going to keep it simple this is
57:58 - going to run in deploy stage
58:01 - and now we have our script
58:06 - and i can actually copy this whole
58:08 - command
58:10 - and modify the respective values
58:13 - here so that's command we're going to be
58:16 - executing because we want to ssh
58:18 - from the job environment to the remote
58:22 - server
58:23 - so the public ip address obviously will
58:25 - be the same the root user and this is
58:28 - the file that we have as a reference
58:31 - using a variable so we're going to
58:33 - reference that variable dollar sign ssh
58:37 - key
58:38 - and remember this confirmation we had to
58:40 - make here
58:41 - to
58:42 - save that remote server
58:44 - in the list of known hosts and that's
58:46 - actually an interactive step right so it
58:48 - needs some manual input and if we don't
58:51 - provide a manual input this will not
58:52 - work so want to actually skip this step
58:55 - because in the pipeline execution we
58:57 - don't have a manual step
59:00 - so we want to tell the ssh command you
59:03 - know what forget about that check just
59:06 - connect to the server and that's it and
59:08 - for that we're going to add an option
59:10 - here called strict host key checking
59:14 - equals no
59:16 - so it will skip that so no manual input
59:19 - will be required here so that's an ssh
59:21 - command to connect to the server but
59:24 - once we connect to the server obviously
59:26 - we need to do something right we need to
59:28 - start a container using this image that
59:31 - we just built right
59:33 - so we need to execute docker run or some
59:36 - kind of similar command
59:38 - and
59:39 - we can do that passing that docker
59:41 - command to the ssh command so we're
59:44 - saying once the ssh please then execute
59:46 - whatever is defined here to a line break
59:48 - here
59:49 - and
59:50 - let's write our docker run command so
59:53 - first of all our application is running
59:55 - on port 5000
59:57 - so we're gonna expose that port on the
59:59 - host
60:01 - like this
60:02 - then we have the image and since we have
60:05 - parameterized this we can just copy
60:08 - those values so that's the image
60:10 - now there are three more things that we
60:12 - need to configure in order to make this
60:14 - deploy drop successful
60:16 - first of all
60:17 - this command will be executed on a
60:19 - droplet server and it will actually pull
60:22 - the image that we specify here from the
60:25 - private registry
60:27 - and that means we need to authenticate
60:29 - with that registry to be able to pull
60:31 - the image
60:32 - so just like we had to do docker login
60:35 - here in order to push an image we need
60:37 - to do docker login here in order to pull
60:39 - the image so we're going to copy this
60:41 - thing
60:42 - and
60:44 - edit here
60:45 - before we run the image and since these
60:48 - two are going to execute inside the ssh
60:51 - we need to add these ampersands here so
60:54 - we are kind containing multiple ssh
60:56 - commands together so that's one thing
60:59 - which will take care of authenticating
61:00 - with the docker registry and pulling the
61:03 - image from the repository so this is the
61:06 - first one the second one is that
61:08 - when we execute this pipeline the first
61:11 - time it will actually succeed right it
61:13 - will create and start the docker
61:15 - container on the server and that's it
61:17 - but on the second execution or any
61:19 - following execution it will try to
61:22 - create and start a new container on the
61:25 - same host port and obviously this is
61:27 - going to fail because you can't run
61:29 - multiple processes on the same port
61:31 - right so before each docker run command
61:34 - we also need to make sure to stop and
61:36 - remove any existing containers or at
61:39 - least stop any currently running
61:41 - containers on port 5000 so that a new
61:45 - one can be created so that's the second
61:47 - thing we need to do and because we know
61:49 - that this server will only host and run
61:52 - our application we know that there is
61:54 - going to be one container and we just
61:56 - want to stop that and we're going to do
61:58 - that using the following commit we're
61:59 - going to do docker ps
62:02 - dash
62:02 - a so this will list all the containers
62:05 - whether they're running or stopped
62:07 - doesn't matter and it's going to list
62:09 - them by id so with dash aq version we're
62:13 - going to have a list of all containers
62:15 - again in our case it's just going to be
62:16 - one
62:17 - and then we're going to
62:19 - pipe that command and we're going to
62:21 - stop that container using docker stop
62:24 - command so we're going to take whatever
62:27 - this displays as an argument
62:30 - for docker
62:32 - stop commit like this and we're also
62:35 - gonna
62:36 - take that list as an argument
62:38 - for docker remove command so this will
62:42 - stop any containers
62:44 - using their ids and it will also remove
62:46 - those containers and then we're going to
62:48 - create a new one with a new image and
62:51 - let's not forget
62:53 - the end here to change the commands
62:56 - and the third thing we need to do is
62:59 - relate it to the ssh key file so when
63:02 - gitlab creates a temporary file from
63:05 - this ssh key variable that we created it
63:08 - will actually set access permissions on
63:10 - that ssh private key file and those
63:13 - access permissions by default are to
63:15 - open so anyone can
63:17 - read and write to that file
63:20 - and when we try to connect with a
63:22 - private key file which has
63:24 - open access permissions so anyone can
63:26 - read and write to it we're gonna get an
63:28 - error that it's insecure that the file
63:31 - is not restricted and it has
63:34 - two loose or open access permissions so
63:37 - we're gonna need to fix that and to
63:40 - shortly explain to you what that means
63:42 - so if i go to ssh
63:44 - and
63:45 - our digital ocean key and if i do ls
63:49 - with
63:50 - option
63:51 - l
63:52 - so long input this will actually show me
63:55 - who has access to this file right so
63:57 - these are permissions for the owner
64:00 - which is my user the group
64:02 - and anyone else so for this file the
64:05 - owner has read and write permissions but
64:08 - no one else can actually read or write
64:10 - or access this file in any way so that's
64:13 - a restricted access and that's what we
64:15 - need to do
64:17 - here on gitlab as well and as i said by
64:19 - default gitlab actually gives everyone
64:22 - a read write permission when it creates
64:25 - a temporary file from the file type of
64:27 - variable
64:28 - and we can fix that very easily
64:31 - by
64:32 - setting the permission to a stricter
64:34 - access
64:35 - before the script will run
64:37 - inside a before script section
64:40 - so right here we're going to do change
64:42 - mode
64:44 - of the file
64:46 - the temporary file to
64:49 - access 400 so 400 will actually be
64:53 - read so without write and then 0 0 so no
64:57 - access for anything else
64:59 - and that's actually strict enough so
65:02 - that's the third thing we need to do for
65:03 - this
65:04 - job to run successfully and we also need
65:07 - to run the docker run command in a
65:10 - background or in a detached mode with
65:13 - minus d
65:14 - because otherwise it will block our
65:16 - terminal or the jobs terminal and it
65:19 - will basically just endlessly wait and
65:22 - with minus d or detached mode it will
65:25 - just send the command in the background
65:27 - and will complete the job okay so that's
65:31 - the whole configuration now let's
65:33 - actually commit that and trigger a
65:35 - pipeline which will deploy our
65:37 - application
65:39 - to a deployment server so let's go to
65:41 - the pipelines
65:42 - and let's wait for the execution
65:45 - awesome so our pipeline went through we
65:48 - have a successful deploy job let's
65:50 - actually check the locks
65:51 - and first of all i want to mention that
65:53 - because we didn't overwrite the image
65:56 - the default ruby image was used to
65:58 - create
65:59 - the job container
66:03 - then we have our change mod before
66:05 - script command and here is a result of
66:08 - docker login command and then the
66:11 - container has also started
66:15 - and we also have three stages now test
66:17 - build and deploy stage with
66:20 - each one having its respective job
66:22 - so we can now validate that our
66:24 - application is running first by going to
66:27 - the remote server
66:29 - let's ssh into it
66:32 - and
66:34 - checking that the docker container is
66:37 - running
66:37 - and there you go we have our python app
66:41 - that's the image
66:42 - running on port 5000
66:44 - and the second way to validate of course
66:47 - we want to
66:48 - access this web application from a
66:50 - browser right so on the droplets public
66:54 - ip address
66:56 - on port 5000 because that's where the
66:59 - application is exposed
67:01 - there you go we have our application
67:04 - accessible from the browser so with this
67:07 - we have actually successfully built a
67:09 - basic cicd pipeline for our python
67:13 - application and as i said at the
67:15 - beginning this actually applies to any
67:17 - application written in any programming
67:19 - language these two jobs are anyways very
67:22 - generic and for this one you would just
67:24 - have to find a respective image and
67:27 - comment and that's basically it also
67:30 - when you're done with the demo make sure
67:31 - to go to your digitalocean account
67:34 - and delete your droplet so you don't get
67:36 - charged for the server resources
67:39 - so just wanted to remind you of that and
67:41 - with that our demo project is complete
67:45 - and we have learned a few core concepts
67:47 - of gitlab cicd now of course you can
67:50 - configure much more in your github
67:52 - pipeline code and as i said this is a
67:54 - basic pipeline and you can already work
67:56 - with this knowledge because you
67:58 - basically learn the core concepts of
68:00 - gitlab cicd and you can extend on it
68:02 - however there is much more to gitlab
68:05 - cicd platform much more features with
68:08 - more advanced use cases of using
68:10 - artifacts caching deploying with docker
68:13 - compose deploying to kubernetes cluster
68:16 - creating pipelines for microservices
68:18 - applications using
68:20 - job templates to avoid code duplication
68:23 - and much much more so if you want to
68:26 - really dive into gitlab ci city and
68:29 - learn all these features and concepts
68:31 - and become an expert in that as
68:33 - mentioned i have a complete kit lab csd
68:36 - course so be sure to check out the video
68:39 - description to see exactly what's in the
68:41 - course and to enroll there i hope you
68:44 - enjoyed the video and you learned a lot
68:46 - let me know in the comments what is your
68:48 - experience and your use case with gitlab
68:51 - cicd and whether you're using it at work
68:53 - or any other projects and with that
68:56 - thank you for watching and see you in
68:58 - the next video