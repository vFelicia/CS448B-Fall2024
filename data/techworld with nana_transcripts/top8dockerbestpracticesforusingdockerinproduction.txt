00:00 - in this video we're gonna talk about
00:02 - eight best practices for using docker in
00:05 - production docker is obviously a
00:07 - technology that became a standard and a
00:10 - technology that everyone is familiar
00:12 - with however not everyone is using
00:14 - docker according to the best practices
00:17 - so in this video i want to show you
00:20 - eight ways you can use docker in a right
00:22 - way in your projects to improve security
00:25 - optimize the image size and take
00:28 - advantage of some of the useful darker
00:30 - features and also write cleaner and more
00:33 - maintainable docker files the first best
00:36 - practice is
00:37 - to use an official and verified docker
00:40 - image whenever available
00:43 - let's say you are developing a node.js
00:45 - application and want to build it and run
00:48 - it as a docker image instead of taking a
00:51 - base operating system image and
00:54 - installing node.js npm and whatever
00:57 - other tools you need for your
00:58 - application
00:59 - use the official node image for your
01:02 - application this will not only make your
01:05 - docker file cleaner but also let you use
01:08 - an official and verified image which is
01:10 - already built with the best practices
01:16 - okay so we have selected the base image
01:19 - but now when we build our applications
01:21 - image from this docker file it will
01:24 - always use the latest tag of the node
01:27 - image
01:28 - now why is this a problem because this
01:31 - means you might get a different image
01:33 - version as in the previous build and the
01:36 - new image version may break stuff or
01:38 - cause an unexpected behavior so latest
01:42 - tag is basically unpredictable you don't
01:44 - know exactly which image you are getting
01:46 - so instead of a random latest image tag
01:50 - you want to fixate the version and just
01:52 - like you deploy your own application
01:54 - with a specific version you want to use
01:56 - the official image with a specific
01:59 - version and the rule here is the more
02:01 - specific the better this also gives you
02:04 - and your team a transparency to know
02:07 - exactly what version of the base image
02:09 - you're using in your docker file
02:14 - now looking at all the image tags or
02:17 - versions here you see that for node.js
02:20 - there are multiple official images not
02:22 - only with different version numbers but
02:25 - also with different operating system
02:27 - distributions so the question is which
02:30 - one do you choose and that's an
02:31 - important point if the image is based on
02:34 - a full-blown operating system
02:36 - distribution like ubuntu or centos which
02:39 - has a bunch of tools already packaged in
02:43 - the image size will be larger right but
02:45 - you don't need most of these tools in
02:48 - your application images in contrast
02:51 - having smaller images means you need
02:53 - less storage space in image repository
02:56 - as well as on a deployment server and of
02:59 - course you can transfer the images
03:01 - faster when pulling or pushing them from
03:04 - the repository now in addition to the
03:06 - size there is another issue with images
03:09 - based on full-blown operating systems
03:12 - with lots of tools installed inside and
03:15 - that is a security issue because such
03:18 - base images usually contain hundreds of
03:21 - known vulnerabilities and basically
03:24 - create a larger attack surface to your
03:27 - application image
03:29 - and this way you basically end up
03:30 - introducing unnecessary security issues
03:34 - from the beginning to your image in
03:36 - comparison by using smaller images with
03:39 - leaner operating system distributions
03:42 - which only bundle the necessary system
03:44 - tools and libraries you're also
03:47 - minimizing the attack surface and making
03:49 - sure that you build more secure images
03:52 - so the best practice here would be to
03:54 - select an image with a specific version
03:57 - based on a leaner operating system
04:00 - distribution like alpine for example
04:02 - alpine has everything you need to start
04:04 - your application in a container but is
04:07 - much more lightweight and for most of
04:09 - the images that you look on a docker hub
04:12 - you will see
04:13 - a version tag with alpine distribution
04:16 - inside it is one of the most common and
04:19 - popular base images for docker
04:22 - containers
04:23 - so the best practice here is
04:25 - that if your application does not
04:27 - require any specific system utilities or
04:30 - libraries make sure to choose the leaner
04:33 - and smaller images from the selection
04:36 - the next best practice with docker is
04:39 - optimizing caching for image layers when
04:43 - building an image
04:44 - so what are image layers and what does
04:47 - caching and image layer mean
04:50 - it's very simple actually now docker
04:52 - image is built based on a docker file
04:54 - right in docker file each command or
04:57 - instruction creates an image layer let's
05:00 - look at a simple docker file based on a
05:02 - node alpine image so every docker image
05:05 - is made up of layers
05:08 - this means when we use a base image of
05:10 - node alpine like in this example it
05:13 - already has layers because it was
05:15 - already built using its own docker file
05:19 - and you can see that actually in the
05:20 - documentation when you go to the image
05:23 - page in docker hub and click in one of
05:26 - the text you will see the layers that
05:29 - make up the image so this is how the
05:32 - image was built
05:34 - and plus in our docker file on top of
05:36 - that we have a couple of other comments
05:39 - that each will add a new layer to this
05:42 - image and again this is how every docker
05:46 - image is created using this multiple
05:48 - layers and once you build your own
05:50 - application you can also see all the
05:52 - image layers of the final application
05:54 - image on a command line using docker
05:58 - image history command with the image
06:00 - name so this will display you the image
06:03 - layers with the corresponding commands
06:06 - that created this layer okay now what
06:08 - about caching well each layer will get
06:11 - cached by docker so when you rebuild
06:14 - your image if your docker file hasn't
06:16 - changed
06:17 - a docker will just use the cached layers
06:21 - to build the image this of course makes
06:23 - building the image much faster
06:25 - caching is also useful and important
06:28 - when pulling and pushing an image so if
06:30 - i pull a new image version of the same
06:33 - application and let's say two new layers
06:36 - have been edited in the new version the
06:38 - whole image doesn't need to be
06:40 - downloaded only the newly edited layers
06:42 - will be downloaded the rest are already
06:45 - locally cached by docker so they will be
06:47 - reused from the cache so going back to
06:50 - our simple docker file example what
06:52 - we're doing here is that we're copying
06:54 - all the files from the project into the
06:56 - image using the copy command and then we
06:59 - are executing npm install to install all
07:02 - the project dependencies inside now what
07:05 - happens if we make some code changes in
07:08 - our application
07:09 - since we are copying everything into the
07:11 - image this means copy
07:14 - command will be executed again because
07:16 - it needs to
07:17 - copy the change files right all the
07:19 - changes that we made in the code but the
07:22 - next line of npm install is also
07:26 - re-executed even though we didn't change
07:28 - anything in the dependencies
07:31 - now why is that why isn't it used from
07:33 - cache well that's because
07:35 - once a layer changes all the following
07:39 - or downstream layers have to be
07:42 - recreated as well in other words when
07:44 - you change the contents of one line in
07:47 - docker file caches of all the following
07:50 - lines or layers will be busted and
07:52 - invalidated so each layer from that
07:55 - point will be rebuilt however instead we
07:58 - want to take advantage of docker cache
08:01 - and we want things that haven't changed
08:03 - to be reused from cache again giving us
08:07 - an advantage that the image will be
08:09 - built fast so in our case we don't want
08:11 - to rerun npm install dependencies every
08:15 - time
08:15 - some file in the project changes we only
08:18 - want to run it when package.json
08:21 - file contents have changed which
08:24 - includes all the application
08:25 - dependencies so let's restructure the
08:28 - docker file to only copy
08:31 - package.json file and then run npm
08:34 - install and only after that
08:36 - run the copy
08:38 - all the files comment in this case if we
08:41 - add or remove a dependency in
08:43 - package.json file it will be copied and
08:46 - npm install will be executed if we
08:49 - change any other file in project these
08:52 - two layers will be reused from cache
08:55 - so npm install will not get re-executed
08:59 - and you can see that in the output of
09:01 - docker build command as well where you
09:03 - have the information that a layer has
09:06 - been reused from cache or a layer has
09:09 - been rebuilt so the rule here and the
09:12 - best practice is that you should order
09:15 - your commands in the docker file from
09:18 - the least to the most frequently
09:20 - changing commands to take advantage of
09:23 - caching and this way optimize how fast
09:27 - the image gets built before moving on i
09:30 - want to give a shout out to castin who
09:32 - made this video possible
09:35 - castings k10 is the data management
09:38 - platform for kubernetes
09:40 - k10 basically takes off most of the load
09:43 - of doing backup and restore in
09:45 - kubernetes from the cluster
09:47 - administrators it has a very simple ui
09:50 - so it's super easy to work with and has
09:53 - an intelligent logic which does all the
09:55 - heavy lifting for you and with my link
09:58 - you can download k10 for free and get 10
10:01 - notes free forever to do your kubernetes
10:04 - backups so make sure to check out the
10:06 - link in the video description and now
10:08 - let's continue
10:10 - now usually when we build the image we
10:13 - don't need everything we have in the
10:15 - project to run the application inside we
10:18 - don't need the auto-generated folders
10:20 - like targets or build folder we don't
10:23 - need the readme file etc so how do we
10:26 - exclude such content from ending up in
10:29 - our application image in order to reduce
10:32 - the image size and that's our next best
10:36 - practice to use a dot docker ignore file
10:39 - and it's pretty straightforward we
10:41 - basically just create this docker ignore
10:43 - file we list all the files and folders
10:47 - that we want to be ignored and when
10:49 - building the image docker will look at
10:52 - the contents and ignore anything
10:54 - specified inside
10:58 - but now let's say there are some
11:00 - contents in your project that you need
11:03 - for building the image so during the
11:05 - build process but you don't need them in
11:08 - the final image itself to run the
11:10 - application
11:11 - the way it works is that while you build
11:14 - an image from a docker file many
11:16 - artifacts actually get created which are
11:19 - required only during the build time and
11:22 - this could be development tools and
11:24 - libraries needed for compiling the
11:26 - application or this could be
11:28 - dependencies needed to run unit tests
11:31 - could also be some temporary files and
11:33 - so on if you keep these artifacts in
11:35 - your final image even though they're
11:37 - absolutely unnecessary for running the
11:40 - application it will again result in an
11:43 - increased image size and increased
11:45 - attack surface a specific example for
11:48 - this is a package.json or palm.xml or
11:52 - any other dependencies file
11:54 - which specifies all the dependencies for
11:56 - the project and are needed
11:59 - to install those dependencies however
12:02 - once the dependencies are installed we
12:05 - don't need these files in the image
12:06 - itself to run the application another
12:09 - more interesting use case is when
12:12 - building java based applications for
12:14 - example
12:15 - we need jdk to compile the java source
12:18 - code but jdk is not needed to run the
12:22 - java application itself in addition to
12:24 - that you might be using tools like maven
12:27 - or gradle to build your java application
12:31 - those are also not needed in the final
12:33 - image
12:34 - so how do we
12:36 - separate
12:37 - the build stage from the runtime stage
12:40 - in other words how do we exclude the
12:43 - build dependencies from the image while
12:46 - still having them available while
12:48 - building the image well for that you can
12:50 - use what's called multi-stage builds
12:53 - the multi-stage builds feature allows
12:56 - you to use multiple temporary images
13:00 - during the build process but keep only
13:03 - the latest image as the final artifact
13:06 - let's see how that works
13:07 - this is an example docker file with two
13:10 - build stages the first stage which we
13:13 - call build specified like this is used
13:17 - to build the java application using
13:20 - maven tool and in the second stage which
13:23 - starts from here
13:25 - with directive from tomcat we use the
13:29 - files generated in the previous build
13:31 - stage
13:32 - and copy them in the final image
13:35 - so the final application image is
13:37 - created only in the last stage
13:40 - using these two lines
13:42 - and all the files and tools
13:45 - that we used in the first stage will be
13:47 - discarded
13:48 - once it's completed
13:50 - and also we talked about layers in our
13:52 - case the final two commands of this
13:54 - docker file will create
13:57 - the layers of the final image again all
14:00 - these previous steps will be discarded
14:03 - so this basically helps us separate the
14:05 - build tools and dependencies from what's
14:08 - needed for a runtime and gives us images
14:11 - which have way less dependencies and are
14:13 - much smaller in size
14:18 - now when we create this image and
14:20 - eventually run it as a container
14:23 - which operating system user will be used
14:26 - to start the application inside well by
14:28 - default when a docker file does not
14:30 - specify a user it uses a root user but
14:34 - in reality there is mostly no reason to
14:37 - run containers with root privileges and
14:40 - there is also a bad security practice
14:43 - this basically introduces a security
14:45 - issue because when container starts on
14:47 - the host it will potentially have root
14:50 - access on the docker host so running an
14:53 - application inside the container with a
14:56 - root user will make it easier for an
14:58 - attacker to escalate privileges on the
15:01 - host and basically get hold of the
15:05 - underlying host and its processes not
15:07 - only the container itself especially if
15:10 - the application
15:11 - inside the container is vulnerable to
15:14 - exploitation
15:16 - to avoid this the best practice is to
15:18 - simply create a dedicated user and a
15:20 - dedicated group in the docker image to
15:23 - run the application
15:25 - to create the user in its group you can
15:27 - simply run user add and group ad linux
15:30 - commands like this and once you have
15:33 - that user to also run the application
15:37 - inside the container with that user you
15:39 - can use a directive called user
15:42 - with the username and then start the
15:45 - application conveniently some images
15:48 - already have a generic user
15:51 - bundled in which you can use so you
15:54 - don't have to create a new one for
15:56 - example node.js image already bundles a
15:59 - generic user called node
16:02 - which you can simply use to run the
16:04 - application inside the container
16:09 - finally how do you make sure and
16:11 - validate the image you build has a few
16:15 - or no security vulnerabilities and so
16:18 - the next and final best practice is once
16:21 - you build the image to scan it for
16:24 - security vulnerabilities using the
16:26 - docker scan command
16:28 - here note that you have to be logged
16:30 - into docker hub to be able to scan your
16:33 - images so you can do a simple docker
16:35 - login on your command line and then
16:38 - execute docker scan
16:41 - command with image name as a parameter
16:44 - and in the background docker actually
16:45 - uses a service called sneak to do the
16:49 - vulnerability scanning
16:51 - of the images the scan uses a database
16:54 - of vulnerabilities which gets constantly
16:57 - updated so new ones get discovered and
17:00 - edit all the time for different images
17:03 - and this is basically an example output
17:05 - of docker scan command where you see the
17:07 - type of vulnerability a url for more
17:10 - information but also what's
17:13 - very useful and interesting you see
17:15 - which version of the relevant library
17:19 - actually fixes that vulnerability so you
17:21 - can update your libraries to get rid of
17:24 - these issues and in addition to scanning
17:27 - your images with docker scan command on
17:29 - a command line interface you can also
17:31 - configure docker hub to scan the images
17:34 - automatically when they get pushed to
17:36 - the repository and you can see the
17:38 - results of that scanning in docker hub
17:41 - or docker desktop
17:43 - and of course you can integrate this
17:45 - check in your ci cd pipeline when
17:47 - building your docker images
17:50 - so these are eight best practices
17:53 - that you can apply today to make your
17:55 - docker images leaner and more secure of
17:58 - course there are many more best
18:00 - practices related to docker but applying
18:03 - these will already give you great
18:05 - results when using
18:06 - docker in production do you know some
18:09 - other best practices which you think are
18:11 - super important and have to be mentioned
18:13 - please share them in the comments for
18:15 - others as well and finally make sure to
18:17 - check out the video description for more
18:20 - learning resources and with that thank
18:23 - you for watching and see you in the next
18:25 - video