00:00 - in this video we will look at fluentd
00:03 - which is an open source log data
00:05 - collector
00:06 - to understand what this actually means i
00:08 - will first explain
00:10 - why we actually need logs the challenges
00:13 - of
00:13 - collecting and consuming application
00:16 - logs
00:17 - how fluenty works and how it solves all
00:20 - those challenges
00:21 - and finally how to configure fluenty as
00:24 - a user
00:28 - let's say we have a microservices
00:30 - application deployed in a kubernetes
00:32 - cluster
00:33 - two applications in node.js a couple of
00:36 - python applications
00:37 - maybe databases a message broker and
00:39 - other services
00:41 - all these applications talk to each
00:43 - other and produce
00:45 - log data so each of these services
00:48 - is logging information about what the
00:50 - application is doing
00:52 - now what are some of the information
00:55 - these applications are logging
00:57 - and why do we need this log data this
00:59 - may be some compliance data for example
01:02 - like if you're required to log some
01:04 - specific information
01:05 - depending on your industry in order to
01:07 - be compliant
01:08 - it could be for your application
01:10 - security for
01:12 - example to detect suspicious requests in
01:15 - your application
01:16 - by logging all xs attempts with ip
01:20 - address and user id etc
01:22 - or log who is accessing what and when
01:26 - and an obvious usage for log data is
01:29 - debugging your application when there is
01:30 - an
01:31 - error analyzing all application logs to
01:34 - find the cause
01:35 - these are some of the examples why log
01:38 - data is so important
01:40 - now the question is how do applications
01:43 - log this data there are few options
01:46 - first one is
01:48 - applications right to a file which is a
01:51 - common way of logging in applications
01:53 - however
01:54 - as you can imagine it's difficult to
01:56 - analyze
01:57 - loads of data in raw log files
02:01 - so it's not really for human consumption
02:04 - and without user interface or
02:06 - visualization for this data
02:08 - how do you analyze logs properly
02:10 - especially across
02:12 - applications by checking each
02:14 - application's
02:16 - log file and trying to look for similar
02:19 - times to check across applications
02:21 - also logs will be in different formats
02:24 - coming from different applications
02:26 - like the timestamps and log levels etc
02:29 - another option could be to log directly
02:32 - into a log database like elastic for
02:35 - example
02:35 - to then have a visualization of this
02:38 - data however in this case
02:40 - each application developer must add a
02:42 - library
02:43 - for elastic search and configure it to
02:46 - connect to elastic and
02:47 - send those logs and also each developer
02:50 - must configure the proper format
02:52 - so again there's some challenges with
02:55 - this option as well
02:56 - now what about the third-party
02:58 - applications in your cluster like
03:00 - databases
03:01 - and message broker also in kubernetes
03:04 - requests go through
03:05 - nginx controller so what if you want to
03:08 - see those locks too
03:09 - or what about system logs you can't
03:12 - control how they look so how do you
03:14 - collect
03:15 - logs from all these different data
03:17 - sources
03:18 - all of these are challenges of
03:20 - collecting and consuming
03:22 - logs in complex applications with tons
03:26 - of useful data
03:27 - because you have loads of data which you
03:29 - can't really consume and analyze
03:31 - because you don't have it all in one
03:34 - place in a unified format
03:36 - to be able to visualize them properly so
03:39 - lots of valuable data is kind of wasted
03:42 - so what is a good solution to that
03:44 - challenge
03:45 - a technology that lets you collect all
03:47 - the data
03:48 - regardless of where they come from and
03:51 - transform
03:52 - in a unified format all in one place
03:55 - so that you can then use that data again
03:58 - for compliance or debugging etc
04:00 - and that's exactly what fluentd does
04:04 - and fluentd does that reliably meaning
04:07 - if there is a network outage or data
04:10 - spikes
04:11 - this shouldn't mess up data collection
04:13 - right so fluentd
04:15 - handles such cases as well so how does
04:18 - fluentd work
04:19 - and how does it do all this fluency gets
04:22 - deployed into the cluster
04:24 - and it starts collecting logs from all
04:27 - the applications
04:28 - it can be your own applications
04:30 - third-party applications
04:32 - all of it now these logs that fluently
04:34 - collected
04:35 - will be of different forms and formats
04:38 - right like json format
04:39 - nginx format some custom format maybe
04:43 - and so on so fluentd will process them
04:46 - and reformat them into a uniform way now
04:49 - on top of that
04:50 - you can enrich your data with fluentd so
04:54 - you can add additional information
04:56 - to each log entry like pod name
04:59 - namespace container name and so on
05:02 - so for example you can later group logs
05:05 - of the same pod or logs of the same
05:07 - namespace
05:09 - or you can even modify the data in a log
05:11 - so now you're streaming your logs from
05:14 - all the applications into one unified
05:17 - format
05:17 - through fluentd what happens to these
05:20 - logs after
05:21 - fluentd processes them well obviously in
05:24 - most cases
05:25 - the goal is to nicely visualize them
05:27 - right so we can do some analysis on it
05:30 - well fluentd can send these logs to any
05:33 - destination you want
05:34 - this could be elasticsearch mongodb s3
05:38 - kafka etc now what if you want your
05:41 - python application logs
05:43 - to go to mongodb storage for data
05:45 - analysis
05:46 - and all other application logs to go to
05:49 - elasticsearch
05:50 - or what if you want that node.js logs
05:53 - also go to the mongodb in addition to
05:55 - elasticsearch
05:57 - you can actually very easily configure
05:59 - that routing
06:00 - in fluency which is a great thing about
06:03 - fluendy because it gives you
06:05 - such flexibility compared to alternative
06:07 - tools
06:08 - so you can send any data from any data
06:11 - source
06:11 - to any destination or storage
06:15 - and this flexibility also comes from the
06:17 - fact that fluenty is not
06:19 - tied to any particular back end so you
06:22 - have a wide choice of such
06:24 - destination targets without a vendor
06:27 - looking
06:28 - when using fluentd now you're probably
06:30 - wondering
06:31 - what you as a fluent user need to
06:34 - configure
06:35 - and how you can actually use fluentd
06:38 - first
06:38 - you must install fluentd in kubernetes
06:40 - as a daemon set
06:42 - daemon set is a component that runs on
06:46 - each kubernetes node so if you have five
06:49 - nodes
06:50 - they will all have a fluency pod running
06:53 - on them
06:53 - you can configure fluenty using a
06:55 - fluency configuration file
06:57 - now fluency configuration may be a bit
07:00 - complex to get started
07:02 - with but it's very powerful in terms of
07:04 - processing and reformatting your data
07:07 - and for that you will use fluency
07:09 - plugins fluently has
07:11 - tons of plugins for different use cases
07:14 - first of all you can define the data
07:16 - sources these are all the applications
07:19 - from which fluenty will start collecting
07:21 - the logs
07:22 - so first you configure which application
07:24 - logs
07:25 - you want fluenty to start collecting
07:28 - second you configure how
07:30 - these data entries will be processed
07:32 - line by line
07:33 - so you parse each log as an individual
07:36 - key value pair
07:37 - you have log level message date
07:40 - user id ip address etc
07:44 - and you do that in fluentd using parsers
07:47 - after that you can enrich the data using
07:49 - record transformers
07:51 - again to have even more information on
07:53 - that data
07:54 - or you can even modify the data a great
07:57 - use case would be
07:58 - if you want to anonymize personal data
08:01 - in the logs
08:01 - for data protection for example and
08:04 - finally you have
08:05 - the output where should the logs go
08:09 - and for each such output target
08:12 - there is a plugin like elasticsearch
08:15 - mongodb
08:16 - and so on and as you see here in the
08:18 - example configuration file
08:20 - fluendy has a concept of tags
08:23 - which you can use to group together logs
08:26 - or to filter logs
08:28 - so using these tags you can say i want
08:31 - all logs with tag my app to be parsed
08:34 - like that or i want logs with the tag my
08:37 - service
08:38 - to go to elasticsearch and so on and
08:41 - also using these tags
08:42 - you can easily filter out any unneeded
08:46 - logs to save resources for example
08:48 - in the flexible routing that i mentioned
08:50 - before
08:52 - that's why it's easy to configure
08:53 - because using this text
08:55 - you can very easily configure which logs
08:58 - should go where
08:59 - so that's basically how you can use
09:01 - fluency
09:02 - for your logs now one big advantage of
09:05 - fluency is its built-in reliability
09:08 - when fluentd collects and processes the
09:11 - data
09:11 - it saves it on hard drive until it sends
09:14 - that process data to the configured
09:16 - output destination this means that if
09:19 - fluency pod
09:20 - restarts in the middle of collecting or
09:22 - processing
09:24 - the data or the whole server restarts
09:26 - the data will still be there
09:28 - and when fluency starts again it can
09:30 - pick up from where it left off
09:32 - and it also means you don't have to
09:35 - configure any additional storage for
09:36 - fluency
09:37 - like a radius database and so on
09:40 - what can also happen is when the backend
09:44 - the output target is not accessible can
09:47 - happen that
09:48 - elasticsearch is down or mongodb isn't
09:50 - accessible
09:51 - in that case fluentd will handle that by
09:54 - automatically
09:55 - retrying to send logs until that
09:58 - endpoint becomes available again
10:00 - and in addition to that you can also
10:02 - cluster your fluency setup
10:04 - to make it even more performant and
10:06 - highly available
10:07 - i should mention here that this is one
10:10 - of the use cases of fluency
10:12 - which is logging in kubernetes however
10:15 - logging is
10:16 - a very important topic in iot
10:18 - applications too
10:20 - or in non-containerized applications
10:23 - running on bare metal servers for
10:25 - example and many projects are using
10:27 - fluenty for
10:28 - those use cases as well so fluenty can
10:31 - be used in many different environments
10:34 - if you like this video subscribe for
10:36 - more content like this
10:37 - also if you're interested in behind the
10:40 - scenes and preview content
10:42 - you can follow me on instagram we'll be
10:44 - happy to connect with you
10:46 - there and with that thank you and see
10:48 - you in the next video