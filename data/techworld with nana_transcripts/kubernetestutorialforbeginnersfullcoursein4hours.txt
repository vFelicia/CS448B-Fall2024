00:00 - hello and welcome to this complete
00:03 - kubernetes course
00:05 - the course is a mix of animated
00:07 - theoretic explanations but also Hands-On
00:10 - demos for you to follow along
00:13 - so let's quickly go through the topics
00:15 - I'll cover in this course
00:17 - the first part gives you a great
00:20 - introduction to kubernetes we'll start
00:22 - with the basic concepts of what
00:24 - kubernetes actually is what problems it
00:26 - solves in the kubernetes architecture
00:28 - you will learn how you can use
00:30 - kubernetes by showcasing all the main
00:33 - components after learning the main
00:35 - Concepts we will learn and install
00:37 - minicube for a local kubernetes cluster
00:40 - and we will go through the main commands
00:42 - of creating debugging and deleting pods
00:46 - using cubectl which is kubernetes
00:49 - command line tool after knowing cubectl
00:52 - main commands I will explain kubernetes
00:54 - yaml configuration files which we will
00:57 - use to create and configure components
01:00 - then we will go through a practical use
01:03 - case where we'll deploy a simple
01:05 - application setup in kubernetes cluster
01:08 - locally to get your first hands-on
01:11 - experience with kubernetes and feel more
01:13 - confident about the tool in the second
01:15 - part we will go into more advanced and
01:19 - important Concepts like organizing your
01:22 - components using namespaces how to make
01:25 - your app available from outside using
01:28 - kubernetes Ingress and learn about Helm
01:31 - which is the package manager for
01:33 - kubernetes in addition we will look at
01:36 - three components in more detail first
01:39 - how to persist data in kubernetes using
01:41 - volumes second how to deploy stateful
01:44 - applications like databases using
01:46 - stateful set component and lastly
01:50 - we will look at the different kubernetes
01:52 - service types for different use cases if
01:56 - you like the course be sure to subscribe
01:57 - to my channel for more videos like this
01:59 - and also check out the video description
02:02 - for more related courses on udemy Etc
02:06 - if you guys have any questions during
02:08 - the course or after the course or you
02:10 - want to Simply stay in touch I would
02:12 - love to connect with you on social media
02:14 - so be sure to follow me there as well
02:22 - so in this video I'm going to explain
02:24 - what kubernetes is we're going to start
02:26 - off with the definition to see what
02:28 - official definition is and what it does
02:31 - then we're going to look at the problem
02:32 - solution case study of kubes basically
02:36 - why the kubernetes even come around and
02:38 - what problems does it solve so let's
02:42 - jump in right into the definition what
02:44 - is kubernetes
02:45 - so kubernetes is an open source
02:47 - container orchestration framework which
02:50 - was originally developed by Google so on
02:52 - the foundation it manages container
02:55 - speed Docker containers or from some
02:57 - other technology
02:58 - which basically means that kubernetes
03:01 - helps you manage applications that are
03:04 - made up of hundreds or maybe thousands
03:07 - of containers
03:09 - and it helps you manage them in
03:11 - different environments
03:13 - like physical machines virtual machines
03:16 - or Cloud environments or even hybrid
03:19 - deployment environments so what problems
03:22 - does kubernetes solve and what are the
03:24 - tasks of a container orchestration tool
03:27 - actually so to go through this
03:29 - chronologically the rise of
03:32 - microservices cost increased usage of
03:34 - container Technologies because the
03:37 - containers actually offer the perfect
03:38 - host for small independent applications
03:43 - like microservices
03:45 - and the rise of containers in the
03:47 - microservice technology actually
03:49 - resulted in applications that are now
03:52 - comprised of hundreds or sometimes maybe
03:54 - even thousands of containers now
03:56 - managing those loads of containers
03:58 - across multiple environments using
04:02 - scripts and self-made tools can be
04:04 - really complex and sometimes even
04:07 - impossible so that specific scenario
04:10 - actually caused the need for having
04:12 - container orchestration Technologies so
04:16 - what those orchestration tools like
04:18 - kubernetes do is actually guarantee
04:21 - following features one is high
04:24 - availability in simple words High
04:27 - availability means that the application
04:29 - has no downtime so it's always
04:31 - accessible by the users a second one is
04:35 - scalability which means that application
04:37 - has high performance
04:39 - it loads fast and the users have a very
04:44 - high response rates from the application
04:46 - and the third one is disaster recovery
04:49 - which basically means that if an
04:50 - infrastructure has some problems like
04:52 - data is lost or the servers explode or
04:55 - something bad happens with the service
04:56 - center the infrastructure has to have
04:59 - some kind of mechanism to pick up the
05:01 - data and to restore it to the latest
05:03 - state so that application doesn't
05:05 - actually lose any data and the
05:07 - containerized application can run from
05:10 - the latest state after the recovery and
05:13 - all of these are functionalities that
05:15 - container orchestration Technologies
05:17 - like kubernetes offer
05:23 - so in this video I want to give you an
05:25 - overview of the most basic fundamental
05:28 - components of kubernetes but just enough
05:31 - to actually get you started using
05:33 - kubernetes in practice either as a
05:36 - devops engineer or a software developer
05:39 - now kubernetes has tons of components
05:41 - but most of the time you're going to be
05:43 - working with just a handful of them
05:46 - so I'm gonna build a case of a simple
05:49 - JavaScript application with a simple
05:52 - database and I'm going to show you step
05:54 - by step how each component of kubernetes
05:57 - actually helps you to deploy your
05:59 - application and what is the role of each
06:03 - of those components
06:06 - so let's start with the basic setup of a
06:09 - worker node or in kubernetes terms a
06:12 - node which is a simple server a physical
06:16 - or virtual machine and the basic
06:18 - component or the smallest unit of
06:21 - kubernetes is a pod so what pod is is
06:24 - basically an abstraction over a
06:27 - container so if you're familiar with
06:29 - Docker containers or container images so
06:32 - basically what pod does is it creates
06:34 - this running environment or a layer on
06:38 - top of the container and the reason is
06:40 - because kubernetes wants to abstract
06:42 - away the container runtime or container
06:45 - Technologies so that you can replace
06:48 - them if you want to and also because you
06:51 - don't have to directly work with Docker
06:54 - whatever container technology you use in
06:58 - a kubernetes so you only interact with
06:59 - the kubernetes layer so we have an
07:01 - application pod which is our own
07:03 - application and that will maybe use a
07:06 - data database pod with its own container
07:09 - and this is also an important concept
07:11 - here pot is usually meant to run one
07:14 - application container inside of it you
07:17 - can run multiple containers inside one
07:19 - pod but usually it's only the case if
07:22 - you have one main application container
07:24 - and a helper container or some side
07:27 - service that has to run inside of that
07:30 - pod and as you say this is nothing
07:33 - special you just have one server and two
07:35 - containers running on it with a
07:37 - abstraction layer on top of it so now
07:39 - let's see how they communicate with each
07:41 - other in kubernetes world so kubernetes
07:43 - offers out of the box a virtual Network
07:45 - which means that each pod gets its own
07:49 - IP address not the container the Pod
07:51 - gets the IP address and each pod can
07:55 - communicate with each other using that
07:57 - IP address which is an internal IP
07:59 - address obviously it's not the public
08:01 - one so my application container can
08:05 - communicate with database using the IP
08:07 - address however pod components in
08:10 - kubernetes also an important concept are
08:13 - ephemeral which means that they can die
08:15 - very easily and when that happens for
08:19 - example if I lose a database container
08:21 - because the container crashed because
08:24 - the application crashed inside or
08:26 - because the nodes the server that I'm
08:29 - running them on uh ran out resources the
08:33 - Pod will die
08:34 - and a new one will get created in its
08:37 - place and when that happens it will get
08:39 - assigned a new IP address which
08:42 - obviously is inconvenient if you are
08:44 - communicating with the database using
08:46 - the IP address because now you have to
08:47 - adjust it every time pod restarts and
08:50 - because of that another component of
08:53 - kubernetes called service is used
08:57 - so service is basically a static IP
09:00 - address or permanent IP address that can
09:03 - be attached so to say to each pod so my
09:07 - app will have its own service and
09:08 - database pod will have its own service
09:11 - and the good thing here is that the life
09:14 - cycles of service and the Pod are not
09:17 - connected so even if the Pod dies the
09:20 - service and its IP address will stay so
09:24 - you don't have to change that endpoint
09:27 - anymore
09:28 - so now obviously you would want your
09:31 - application to be accessible through a
09:33 - browser right and for this you would
09:35 - have to create an external service so
09:37 - external Services a service that opens
09:40 - the communication from external sources
09:43 - but obviously you wouldn't want your
09:45 - database to be open to the public
09:48 - requests and for that you would create
09:50 - something called an internal service so
09:53 - this is a type of a service that you
09:56 - specify when creating one however if you
09:59 - notice the URL of the external service
10:03 - is not very practical so basically what
10:06 - you have is an HTTP protocol with a node
10:11 - IP address so often node not the service
10:14 - and the port number of the service which
10:17 - is
10:18 - good for test purposes if you want to
10:20 - test something very fast but not for the
10:22 - end product so usually you would want
10:24 - your url to look like this if you want
10:26 - to talk to your application with a
10:29 - secure protocol and a domain name and
10:32 - for that there is another component of
10:35 - kubernetes called Ingress so instead of
10:38 - service the request goes first to
10:40 - Ingress and it does the forwarding then
10:42 - to the service
10:44 - so now we saw some of the very basic
10:46 - components of kubernetes and as you see
10:49 - this is a very simple setup we just have
10:51 - one server and a couple of containers
10:55 - running and some Services nothing really
10:58 - special where kubernetes advantages or
11:02 - the actual cool features really come
11:05 - forward but we're gonna get there step
11:07 - by step so let's continue
11:10 - so as we said pots communicate with each
11:13 - other using a service so my application
11:16 - will have a database endpoint let's say
11:19 - called mongodb service that it uses to
11:22 - communicate with the database but where
11:25 - do you configure usually this database
11:27 - URL or endpoint usually you would do it
11:30 - in application properties file or as
11:34 - some kind of external environmental
11:36 - variable but usually it's inside of the
11:39 - built image of the application so for
11:42 - example if the endpoint of the service
11:44 - or service name in this case changed to
11:47 - mongodb you would have to adjust that
11:50 - URL in the application so usually you'd
11:53 - have to rebuild the application with a
11:55 - new version and you have to push it to
11:57 - the repository and now you'll have to
12:00 - pull that new image in your pod and
12:03 - restart the whole thing so a little bit
12:06 - tedious for a small change like database
12:09 - URL so for that purpose kubernetes has a
12:13 - component called configmap so what it
12:16 - does is it's basically your external
12:17 - configuration to your application so
12:20 - config map would usually contain
12:22 - configuration data like URLs of a
12:25 - database or some other services that you
12:27 - use and in kubernetes you just connect
12:30 - it to the Pod so that pod actually gets
12:34 - the data that configmap contains and now
12:37 - if you change the name of the service
12:38 - the endpoint of the service you just
12:41 - adjust the config map and that's it you
12:43 - don't have to build a new image and have
12:45 - to go through this whole cycle now part
12:48 - of the external configuration can also
12:51 - be database username and password right
12:54 - which may also change in the application
12:57 - deployment process but putting a
13:00 - password or other credentials in a
13:02 - config map in a plain text format would
13:04 - be insecure even though it's an external
13:07 - configuration so for this purpose
13:09 - kubernetes has another component called
13:12 - secret so secret is just like config map
13:16 - but the difference is that it's used to
13:18 - store secret data credentials for
13:21 - example and it's stored not in a plain
13:24 - text format of course but in base 64
13:27 - encoded format so secret would contain
13:30 - things like credentials and of course I
13:33 - mean database user you could also put in
13:35 - config map but what's important is the
13:37 - passwords certificates things that you
13:40 - don't want other people to have access
13:42 - to would go in the secret and just like
13:45 - config map you just connect it to your
13:47 - pod so that pod can actually see those
13:50 - data and read from the secret you can
13:52 - actually use the data from config map or
13:55 - Secret inside of your application pod
13:58 - using for example environmental
14:00 - variables or even as a properties file
14:03 - so now to review we've actually looked
14:06 - at almost all mostly used kubernetes
14:09 - basic components we've looked at the pod
14:12 - we've seen how services are used what is
14:16 - ingress component useful for and we've
14:19 - also seen external configuration using
14:22 - config map and secrets
14:27 - so now let's see another very important
14:30 - concept generally which is data storage
14:33 - and how it works in kubernetes so we
14:36 - have this database part that our
14:37 - application uses and it has some data
14:40 - regenerate some data with this setup
14:42 - that you see now if the database
14:44 - container or the Pod gets restarted
14:47 - the data would be gone and that's
14:51 - problematic and inconvenient obviously
14:53 - because you want your database data or
14:56 - log data to be persisted reliably long
14:59 - term
15:00 - and the way you can do it in kubernetes
15:02 - is using another component of kubernetes
15:05 - called volumes
15:07 - and how it works is that it basically
15:09 - attaches a physical storage on a hard
15:12 - drive to your pod and that storage could
15:16 - be either on a local machine meaning on
15:18 - the same server node where the Pod is
15:21 - running or it could be on the remote
15:22 - storage meaning outside of the
15:24 - kubernetes cluster it could be a cloud
15:27 - storage or it could be your own premise
15:29 - storage which is not part of the
15:32 - kubernetes cluster so you just have an
15:34 - external reference on it so now when the
15:37 - database pod or container gets restarted
15:39 - all the data will be there persisted
15:42 - it's important to understand the
15:44 - distinction between the kubernetes
15:46 - cluster and all of its components and
15:49 - the storage regardless of whether it's a
15:52 - local or remote storage think of a
15:54 - storage as an external hard drive
15:56 - plugged in into the kubernetes cluster
15:59 - because the point is kubernetes
16:02 - clustered explicitly doesn't manage any
16:04 - data persistence which means that you as
16:07 - a kubernetes user or an administrator
16:10 - are responsible for backing up the data
16:12 - replicating and managing it and making
16:15 - sure that it's kept on a proper Hardware
16:18 - Etc because it's not taking care of
16:21 - kubernetes
16:23 - so now let's see everything is running
16:25 - perfectly and a user can access our
16:28 - application through a browser
16:30 - now with this setup what happens if my
16:33 - application pod dies right crashes or I
16:37 - have to restart the Pod because I built
16:41 - a new container image basically I would
16:44 - have a downtime where a user can reach
16:47 - my application which is obviously a very
16:50 - bad thing if it happens in production
16:53 - and this is exactly the advantage of
16:56 - distributed systems and containers so
16:59 - instead of relying on just one
17:01 - application pod and one database part
17:04 - Etc we are replicating everything
17:07 - on multiple servers so we would have
17:11 - another node where a replica or clone of
17:14 - our application would run which will
17:17 - also be connected to the service so
17:20 - remember previously we said the service
17:21 - is like an persistent static IP address
17:25 - with a DNS name so that you don't have
17:27 - to constantly adjust the endpoint when a
17:31 - pod dies but service is also a load
17:34 - balancer which means that the service
17:36 - will actually catch the request and
17:38 - forward it to whichever part is list
17:40 - busy so it has both of these
17:42 - functionalities
17:43 - but in order to create the the second
17:45 - replica of the my application pod you
17:48 - wouldn't create a second part but
17:51 - instead you would Define a blueprint for
17:53 - a my application pod and specify how
17:56 - many replicas of that pod you would like
17:59 - to run and that component or that
18:01 - blueprint is called deployment which is
18:03 - another component of kubernetes and in
18:07 - practice you would not be working with
18:09 - pulse or you would not be creating pods
18:11 - you would be creating deployments
18:13 - because there you can specify how many
18:16 - replicas and you can also scale up or
18:19 - scale down number of replicas of parts
18:22 - that you need so with pod we said that
18:24 - part is a layer of abstraction on top of
18:27 - containers and deployment is another
18:30 - abstraction on top of pods which makes
18:33 - it more convenient to interact with the
18:35 - pods replicate them and do some other
18:38 - configuration
18:39 - so in practice you would mostly work
18:42 - with deployments and not with pots so
18:44 - now if one of the replicas of your
18:47 - application pod would die the service
18:49 - will forward the requests to another one
18:52 - so your application would still be
18:54 - accessible for the user so now you're
18:56 - probably wondering what about the
18:57 - database pod because if the database
19:00 - part diet your application also wouldn't
19:02 - be accessible so we need a database
19:06 - replica as well however we can't
19:08 - replicate database using a deployment
19:11 - and the reason for that is because
19:13 - database has a state which is its data
19:17 - meaning that if we have clones or
19:20 - replicas of the database they would all
19:22 - need to access the same shared data
19:27 - storage and there you would need some
19:29 - kind of mechanism that manages which
19:32 - pods are currently writing to that
19:34 - storage or which pods are reading from
19:37 - that storage in order to avoid the data
19:40 - inconsistencies and that mechanism in
19:44 - addition to replicating feature is
19:47 - offered by another kubernetes component
19:49 - called stateful set so this component is
19:53 - meant specifically for applications like
19:56 - databases so MySQL mongodb
20:00 - elasticsearch or any other stateful
20:02 - applications or databases should be
20:06 - created using stateful sets and not
20:08 - deployments
20:09 - it's a very important distinction
20:12 - and stateful said just like deployment
20:14 - would take care of replicating the pots
20:17 - and scaling them up or scaling them down
20:20 - but making sure the database reads and
20:23 - writes are synchronized so that no
20:25 - database inconsistencies are offered
20:28 - however I must mention here that
20:31 - deploying database applications using
20:34 - stateful sets in kubernetes cluster can
20:37 - be somewhat tedious so it's definitely
20:40 - more difficult than working with
20:42 - deployments where you don't have all
20:45 - these challenges that's why it's also a
20:47 - common practice to host database
20:50 - applications outside of the kubernetes
20:52 - cluster and just have the deployments or
20:55 - stateless applications that replicate
20:58 - and scale with no problem inside of the
21:01 - kubernetes cluster and communicate with
21:03 - the external database so now that we
21:05 - have two replicas of my application pod
21:08 - and two replicas of the database and
21:10 - they're both load balanced our setup is
21:13 - more robust which means that now even if
21:16 - Node 1 the whole node server was
21:18 - actually rebooted or crashed and nothing
21:22 - could run on it we would still have a
21:24 - second node with application and
21:26 - database pods running on it and the
21:29 - application would still be accessible by
21:31 - the user until these two replicas get
21:34 - recreated so you can avoid downtime so
21:38 - to summarize we have looked at the most
21:41 - used kubernetes components we start with
21:43 - the pods and the services in order to
21:45 - communicate between the parts and the
21:48 - Ingress component which is used to Route
21:51 - traffic into the cluster we've also
21:53 - looked at external configuration using
21:55 - config maps and secrets and data
21:58 - persistence using volumes and finally
22:01 - we've looked at pod blueprints with
22:04 - replicating mechanisms like deployments
22:07 - and stateful sets where stateful set is
22:10 - used specifically for stateful
22:12 - applications like databases and yes
22:15 - there are a lot more components that
22:17 - kubernetes offers but these are really
22:19 - the core the basic ones just using these
22:23 - core components you can actually build
22:25 - pretty powerful kubernetes clusters
22:33 - video we're gonna talk about basic
22:36 - architecture of kubernetes
22:38 - so we're going to look at two types of
22:41 - nodes that kubernetes operates on one is
22:44 - master and another one is slave and
22:46 - we're going to see what is the
22:48 - difference between those and which role
22:50 - each one of them has inside of the
22:52 - cluster and we're going to go through
22:54 - the basic concepts of how kubernetes
22:57 - does what it does and how the cluster is
23:00 - self-managed and self-healing and
23:02 - automated and how you as a operator of
23:05 - the kubernetes cluster should end up
23:08 - having much less manual effort
23:13 - and we're going to start with this basic
23:16 - setup of one node with two application
23:19 - Parts running on it so one of the main
23:21 - components of kubernetes architecture
23:23 - are its worker servers or nodes and each
23:27 - node will have multiple application pods
23:30 - with containers running on that node and
23:33 - the way kubernetes does it is using
23:36 - three processes that must be installed
23:38 - on every node that are used to schedule
23:41 - and manage those parts so nodes are the
23:44 - cluster servers that actually do the
23:47 - work that's why sometimes also called
23:49 - worker nodes so the first process that
23:52 - needs to run on every node is the
23:55 - container runtime in my example I have
23:57 - Docker but it could be some other
23:59 - technology as well so because
24:00 - application pods have containers running
24:02 - inside a container runtime needs to be
24:05 - installed on every node but the process
24:08 - that actually schedules those can those
24:11 - pods and the containers in underneath is
24:14 - cubelet which is a process of kubernetes
24:17 - itself unlike container runtime that has
24:21 - interface with both container runtime
24:24 - and the Machine the node itself because
24:27 - at the end of the day cubelet is
24:29 - responsible for taking that
24:31 - configuration and actually running a pod
24:34 - or starting a pod with a container
24:36 - inside and then assigning resources from
24:40 - that node to The Container like CPU RAM
24:43 - and storage resources so usually
24:45 - kubernetes cluster is made up of
24:47 - multiple nodes which also must have
24:50 - container runtime and cubelet services
24:52 - installed and you can have hundreds of
24:55 - those worker nodes which will run other
24:57 - pods and containers and replicas of the
25:00 - existing parts like my app and database
25:03 - pods in this example and the way that
25:05 - communication between them works is
25:08 - using Services which is sort of a load
25:11 - balancer that basically catch matches
25:14 - the request directed to the part or the
25:16 - application like database for example
25:18 - and then forwards it to the respective
25:21 - part and the third process that is
25:23 - responsible for forwarding requests from
25:26 - services to pods is actually Cube proxy
25:29 - that also must be installed on every
25:33 - node and Q proxy has actually
25:35 - intelligent forwarding logic inside that
25:39 - makes sure that the communication also
25:41 - works in a performant way with low
25:44 - overhead for example if an application
25:46 - my app replica is making a requested
25:49 - database instead of service just
25:52 - randomly forwarding the request to any
25:54 - replica it will actually forward it to
25:57 - the replica that is running on the same
25:59 - node as the Pod that initiated the
26:02 - request thus this way avoiding the
26:05 - network overhead of sending the request
26:07 - to another machine so to summarize two
26:12 - kubernetes processes cubelet and Cube
26:15 - proxy must be installed on every
26:18 - kubernetes worker node along with an
26:21 - independent container runtime in order
26:23 - for kubernetes cluster to function
26:25 - properly but now the question is how do
26:28 - you interact with this cluster or do you
26:30 - decide on which node a new application
26:33 - pod or database pod should be scheduled
26:36 - or if a replica part dies what process
26:40 - actually monitors it and then
26:42 - reschedules it or restarts it again or
26:44 - when we add another server how does it
26:47 - join the cluster to become another node
26:50 - and get pods and other components
26:51 - created on it and the answer is all
26:54 - these managing processes are done by
26:57 - Master nodes
26:59 - so Master servers or masternodes have
27:02 - completely different processes running
27:05 - inside and these are four processes that
27:08 - run on every masternode that control the
27:10 - cluster State and the worker nodes as
27:13 - well
27:14 - so the first service is API server so
27:18 - when you as a user want to deploy a new
27:21 - application in a kubernetes cluster you
27:24 - interact with the API server using some
27:26 - client it could be a UI like kubernetes
27:29 - dashboard could be command line tool
27:30 - like cubelet or a kubernetes API so API
27:34 - server is like a cluster Gateway which
27:39 - gets the initial request of any updates
27:42 - into the cluster or even the queries
27:44 - from the cluster and it also acts as a
27:47 - gatekeeper for authentication to make
27:50 - sure that only authenticated and
27:52 - authorized requests get through to the
27:55 - cluster that means whenever you want to
27:58 - schedule new pods deploy new
28:00 - applications create new service or any
28:02 - other components you have to talk to the
28:05 - API server on the master node and the
28:08 - API server then validate your request
28:11 - and if everything is fine then it will
28:14 - forward your request to other processes
28:17 - in order to schedule the Pod or create
28:20 - this component that you requested and
28:23 - also if you want to query the status of
28:25 - your deployment or the cluster Health
28:28 - Etc you make a request to the API server
28:30 - and it gives you the response which is
28:33 - good for security because you just have
28:34 - one entry point into the cluster another
28:37 - Master process is a scheduler so as I
28:40 - mentioned if you
28:41 - send an API server a request to schedule
28:44 - a new pod API server after it validates
28:48 - your request will actually hand it over
28:50 - to the scheduler in order to start that
28:53 - application pod on one of the worker
28:56 - nodes and of course instead of just
28:58 - randomly assigning to any node schedule
29:00 - has this whole intelligent way of
29:04 - deciding on which specific worker node
29:07 - the next pod will be scheduled or next
29:11 - component will be scheduled so first it
29:14 - will look at your request and see how
29:17 - much resources the application that you
29:19 - want to schedule will need how much CPU
29:22 - how much RAM and then it it's going to
29:24 - look at and it's going to go through the
29:27 - worker nodes and see the available
29:29 - resources on each one of them and if it
29:32 - says that OneNote is
29:36 - the least busy or has the most resources
29:38 - available it will schedule the new part
29:41 - on that note an important Point here is
29:44 - that scheduler just decides on which
29:47 - nodes a new pod will be scheduled the
29:50 - process that actually does the
29:52 - scheduling that actually starts that pod
29:55 - with a container is the cubelet so it
29:58 - gets the request from the scheduler and
30:00 - executes the request on that note the
30:04 - next component is controller manager
30:06 - which is another crucial component
30:09 - because what happens when pods die on
30:12 - any node there must be a way to detect
30:15 - that the nodes died and then reschedule
30:18 - those pods as soon as possible so what
30:21 - controller manager does is detect the
30:25 - State changes like crashing of pods for
30:28 - example so when pods die controller
30:31 - manager detects that and tries to
30:33 - recover
30:35 - the cluster State as soon as possible
30:37 - and for that it makes a request to the
30:40 - scheduler to reschedule those dead Parts
30:42 - in the same cycle happens here where the
30:44 - scheduler decides based on the resource
30:47 - calculation which worker nodes should
30:51 - restart those pods again and makes
30:54 - requests to the corresponding cubelets
30:57 - on those worker nodes to actually
30:59 - restart the pods and finally the last
31:02 - Master process is etcd which is a key
31:06 - Value Store of a cluster State you can
31:09 - think of it as a cluster brain actually
31:11 - which means that every change in the
31:14 - cluster for example when a new pod gets
31:17 - scheduled when a pod dies all of these
31:19 - changes get saved or updated into this
31:22 - key Value Store of edcd and the reason
31:25 - why atcd store is a cluster brain is
31:28 - because all of this mechanism with
31:31 - scheduler controller manager Etc works
31:34 - because of its data
31:36 - so for example how does scheduler know
31:39 - what resources are available on on each
31:42 - worker node or how does controller
31:45 - manager know that a cluster stay changed
31:47 - in some way for example pods diet or
31:50 - that cubelet restarted new pods upon the
31:53 - request of a scheduler or when you make
31:56 - a query request to API server about the
31:58 - cluster health or for example your
32:01 - application deployment state where does
32:03 - API server get all this state
32:06 - information from so all of this
32:08 - information is stored in hcd cluster
32:10 - what is not stored in the LCD key value
32:13 - store is the actual application data for
32:17 - example if you have a database
32:19 - application running inside of a cluster
32:21 - the data will be stored somewhere else
32:24 - not in the hcd this is just a cluster
32:27 - State information which is used for
32:30 - master processes to communicate with the
32:32 - work processes and vice versa so now you
32:36 - probably already see that Master
32:38 - processes are absolutely crucial for the
32:41 - cluster operation especially the SCD
32:44 - store which contains some data must be
32:46 - reliably stored or replicated so in
32:49 - practice kubernetes cluster is usually
32:52 - made up of multiple Masters where each
32:55 - Master node runs its Master processes
32:58 - where of course the API server is load
33:01 - balanced and the it's a d store forms a
33:04 - distributed storage across all the
33:06 - master nodes
33:09 - so now that we saw what processes run on
33:13 - worker nodes and masternodes let's
33:15 - actually have a look at at a really
33:17 - stick example of a cluster setup so in a
33:20 - very small cluster you would probably
33:22 - have two masternodes and three worker
33:25 - notes also to note here the hardware
33:27 - resources of Master and nodes servers
33:30 - actually differ the master processes are
33:33 - more important but they actually have
33:35 - less load of work so they need less
33:37 - resources like CPU RAM and storage
33:40 - whereas the worker nodes do the actual
33:42 - job of running those pods with
33:45 - containers inside therefore they need
33:47 - more resources and as your application
33:49 - complexity and its demand of resources
33:52 - increases you may actually add more
33:55 - master and node servers to your cluster
33:58 - and thus forming a more powerful and
34:02 - robust cluster to meet your application
34:05 - resource requirements so in an existing
34:08 - kubernetes cluster you can actually add
34:10 - new master or node servers pretty easily
34:13 - so if you want to add a master server
34:15 - you just get a new bare server you
34:17 - install all the master processes on it
34:20 - and add it to the kubernetes cluster
34:21 - same way if you need two worker nodes
34:24 - you get pair servers you install all the
34:27 - worker node processes like container
34:30 - runtime cubelet and Q proxy on it and
34:33 - add it to the kubernetes cluster that's
34:35 - it and this way you can infinitely
34:37 - increase the power and resources of your
34:40 - kubernetes cluster is your replication
34:43 - complexity and its resource demand
34:45 - increases
34:49 - so in this video I'm going to show you
34:52 - what minicube and Cube CTL are and how
34:55 - to set them up so first of all let's see
34:57 - what is minicube usually in kubernetes
35:01 - world when you're setting up a
35:04 - production cluster it will look
35:06 - something like this so you would have
35:07 - multiple Masters uh at least two in a
35:10 - production setting and you would have
35:12 - multiple worker nodes and masternodes
35:16 - and the worker nodes have their own
35:18 - separate responsibility so as you see on
35:20 - the diagram you would have actual
35:22 - separate virtual or physical machines
35:24 - that each represent a node now if you
35:28 - want to test something on your local
35:30 - environment or if you want to try
35:33 - something out very quickly for example
35:35 - deploying new application or new
35:37 - components and you want to test it on
35:40 - your local machine obviously setting up
35:42 - a cluster like this will be pretty
35:44 - difficult or maybe even impossible if
35:47 - you don't have enough resources like
35:49 - memory and CPU Etc and exactly for the
35:52 - use case there's this open source tool
35:55 - that is called a mini Cube so what a
35:57 - mini cube is is basically one node
36:00 - cluster where the master processes and
36:03 - the work processes both run on one node
36:07 - and this node will have a Docker
36:09 - container runtime pre-installed so you
36:12 - will be able to run the containers or
36:14 - the pods with containers on this node
36:16 - and the way it's going to run on your
36:18 - laptop is through a virtual box or some
36:22 - other hypervisor so basically minicube
36:25 - will create a virtual box on your laptop
36:28 - and the nodes that you see here of this
36:31 - node will run in that virtual box so to
36:33 - summarize minicube is a OneNote
36:36 - kubernetes cluster that runs in a
36:39 - virtualbox on your laptop which you can
36:41 - use for testing kubernetes on your local
36:44 - setup so now that you've set up a
36:46 - cluster or a mini cluster on your laptop
36:49 - or PC on your local machine you need
36:51 - some way to interact with a cluster so
36:54 - you want to create components come
36:56 - configure it Etc and that's where
36:59 - cubectl comes in the picture
37:03 - so now that you have this virtual node
37:05 - on your local machine that represents
37:08 - minicube you need some way to interact
37:11 - with that cluster so you need a way to
37:12 - create pods and other kubernetes
37:15 - components on the Node and the way to do
37:17 - it is using cubectl which is a command
37:20 - line tool for kubernetes cluster so
37:23 - let's see how it actually works remember
37:25 - we said that minicube runs both master
37:27 - and work processes so one of the master
37:30 - processes called API server is actually
37:33 - the main entry point into the kubernetes
37:35 - cluster so if you want to do anything in
37:38 - the kubernetes if you want to configure
37:39 - anything create any component you first
37:42 - had to talk to the API server and the
37:44 - way to talk to the API server is through
37:46 - different clients so you can have a UI
37:48 - like a dashboard you can talk to it
37:50 - using kubernetes API or a command line
37:53 - tool which is Cube CTL and cubectl is
37:57 - actually the most powerful of all the
37:59 - three clients because with qcdl you can
38:02 - basically do anything in the kubernetes
38:04 - that you want and throughout these video
38:06 - tutorials we're going to be using
38:08 - cubectl mostly so once the cube CTL
38:11 - submits commands to the API server to
38:13 - create components delete components Etc
38:16 - the work processes on minicube node will
38:20 - actually make it happen so they will be
38:22 - actually executing the commands to
38:25 - create the pods to destroy the parts to
38:27 - create Services
38:28 - Etc so this is the mini Cube setup and
38:31 - this is how Cube CTL is used to interact
38:33 - with the cluster an important thing to
38:36 - note here is that qctl isn't just for
38:38 - minicube cluster if you have a cloud
38:40 - cluster or a hybrid cluster whatever
38:43 - Cube CTL is the tool to use to interact
38:47 - with any type of kubernetes cluster
38:49 - setup so that's important to note here
38:51 - so now that we know what minicube and
38:53 - Cube CTL are let's actually install them
38:56 - to see them in practice
38:59 - I'm using Mac so the installation
39:01 - process will probably be easier but I'm
39:03 - gonna put the links to the installation
39:05 - guides in the description so you can
39:07 - actually follow them to install it on
39:09 - your operating system just one thing to
39:12 - note here is that minicube needs a
39:15 - virtualization because as we mentioned
39:17 - it's going to run in a virtual box setup
39:20 - or some hypervisor so you will need to
39:23 - install some type of hypervisor it could
39:26 - be virtualbox I'm going to install a
39:27 - hyperkit but it's going to be in those
39:29 - step-by-step instructions as well so I'm
39:32 - going to show you how to install it on a
39:33 - Mac
39:36 - so I have a Mac OS Mojave so I'm going
39:39 - to show you how to install mini Cube on
39:42 - this Macos version and I'm going to be
39:43 - using Brew to install it so pretty
39:46 - update
39:49 - and the first thing is that I'm gonna
39:51 - install
39:54 - um a hypervisor
39:55 - hyperkit
39:57 - so I'm gonna go with the hyperkit
40:01 - go ahead and install it
40:06 - I already had it installed it so with
40:08 - you if you're doing it for the first
40:10 - time it might take a longer because it
40:13 - has to download all these dependencies
40:15 - and stuff and now I'm gonna install
40:17 - minicube
40:22 - and here's the thing mini Cube has Cube
40:26 - CTL as a dependency so when I execute
40:29 - this it's going to install cubectl as
40:33 - well
40:34 - so I don't need to install it separately
40:38 - so let's see here
40:41 - installing dependencies for minicube
40:43 - which is
40:44 - kubernetes CLI this is Cube CTL again
40:48 - because I already had it installed
40:51 - before it still has a local copy of the
40:53 - dependencies that's why it's pretty fast
40:55 - it might take longer if you're doing it
40:58 - for the first time so now that
41:00 - everything is installed let's actually
41:01 - check the commands so Cube CTL
41:05 - command should be working so I get this
41:07 - list of the commands with cubectl so
41:09 - it's there and mini Cube should be
41:12 - working as well and as you see mini Cube
41:14 - comes with this command line tool which
41:16 - is pretty simple so with one command
41:19 - it's gonna bring up the whole kubernetes
41:21 - cluster in this OneNote setup and that
41:26 - you can do stuff with it and you can
41:27 - just stop it or delete it it's pretty
41:30 - easy so now that we have both installed
41:33 - and the commands are there let's
41:34 - actually create a mini Cube kubernetes
41:37 - cluster and as you see there is a start
41:38 - command
41:40 - let's actually clear this so this is how
41:43 - we're going to start a kubernetes
41:44 - cluster Q mini Cube start and here is
41:47 - where the hypervisor installed comes in
41:51 - because since midi Cube needs to run in
41:53 - Virtual environment we're gonna tell
41:55 - minicube which hypervisor it should use
41:59 - to start a cluster so for that we're
42:02 - going to specify an option which is VM
42:05 - driver
42:07 - and here I'm going to set the hyperkey
42:09 - that I installed so I'm telling minicube
42:12 - please use hyperkit
42:14 - hypervisor to start this virtual mini
42:17 - Cube cluster so when I execute this it's
42:20 - going to download some stuff so again it
42:22 - may take a little bit longer if you're
42:25 - doing for the first time
42:28 - and as I mentioned mini Cube has Docker
42:32 - runtime or Docker Daemon pre-installed
42:34 - so even if you don't have Docker on your
42:37 - machine it's still gonna work so you
42:39 - would be able to create containers
42:41 - inside because it already contains
42:44 - Docker which is a pretty good thing if
42:46 - you don't have Docker already installed
42:48 - so done Cube CTL is now configured to
42:51 - use minicube which means the mini Cube
42:53 - cluster is set up and Cube CTL command
42:57 - which is meant to interact with the
42:59 - kubernetes Clusters is also connected
43:02 - with that mini Cube cluster which means
43:05 - if I
43:07 - do Cube CTL get notes which just gets me
43:11 - a status of the notes of the kubernetes
43:14 - cluster it's going to tell me that a
43:17 - mini Cube node is ready and as you see
43:20 - it's the only node and it has a must
43:23 - roll because it obviously has to run the
43:25 - master processes
43:26 - um and I can also get the status with
43:28 - minicube
43:31 - executing mini Cube status
43:33 - so I see host is running cubelet which
43:36 - is a service that actually runs the pods
43:39 - using container runtime is running so
43:42 - basically everything is running and by
43:44 - the way if you want to see kubernetes
43:46 - architecture in more detail and to
43:48 - understand how master and worker nodes
43:51 - actually work and what all these
43:52 - processes are I have a separate video
43:55 - that covers kubernetes architecture so
43:58 - you can check it out on this link
43:59 - and we can also check which version of
44:02 - kubernetes we have installed and usually
44:05 - it's going to be the latest version so
44:08 - with qctl version you actually know what
44:10 - the client version of kubernetes is and
44:12 - what the server version of kubernetes is
44:14 - and here we see we're using
44:17 - 1.17 and that's the kubernetes version
44:20 - that is running in the minicube cluster
44:23 - so if you see both client version and
44:25 - server version in the output it means
44:27 - that minicube is correctly installed so
44:30 - from this point on we're going to be
44:32 - interacting with the mini Cube cluster
44:34 - using cubectl command line tool so mini
44:37 - cube is basically just for the startup
44:39 - and for deleting the cluster but
44:41 - everything else configuring we're going
44:44 - to be doing through Cube CTL and all
44:46 - these commands that I executed here I'm
44:48 - gonna put them in a list in the comment
44:50 - section so you can actually copy them
44:54 - in this video I'm gonna show you some
44:57 - basic Cube CTL commands and how to
44:59 - create and debug Parts in minicube
45:05 - so now we have a mini Cube cluster and
45:08 - cubectl installed and once the cluster
45:10 - is set up you're gonna be using cubectl
45:14 - to basically do anything in the cluster
45:16 - to create components to get the status
45:19 - Etc so first thing we are gonna just get
45:23 - the status of the notes
45:25 - so we see that there is one node which
45:28 - is a muster and everything is going to
45:31 - run on that node because it's a mini
45:32 - Cube
45:33 - so with cubect you'll get I can check
45:36 - the parts and I don't have any that's
45:39 - why no resources I can check the
45:41 - services it keeps it will get services
45:46 - and I just have one default service and
45:49 - so on so this Cube CTL get I can list
45:52 - any kubernetes components
45:55 - so now since we don't have any parts
45:57 - we're going to create one and to create
45:59 - kubernetes components there is a cube
46:03 - CTL create command so if I do
46:06 - help on that Cube CTR uh create command
46:10 - I can see available commands for it so I
46:13 - can create all these components using
46:16 - Cube CTL create but there is no pod on
46:19 - the list because in kubernetes world the
46:22 - way it works is that the Pod is the
46:25 - smallest unit of the kubernetes cluster
46:27 - but usually in practice you're not
46:30 - creating pods or you're not working with
46:32 - the pods directly there is an
46:34 - abstraction layer over the pods that is
46:37 - called deployment so this is what we are
46:40 - going to be creating and that's going to
46:42 - create the parts underneath and this is
46:44 - a usage of qctl create deployment so I
46:48 - need to give a name of the deployment
46:50 - and then provide some options and the
46:52 - option that is required is the image
46:55 - because the Pod needs to be created
46:57 - based on certain some image or some
47:00 - container image so let's actually go
47:02 - ahead and create nginx deployment
47:06 - so Cube CTL create deployment we let's
47:11 - call it nginx
47:12 - deployment
47:15 - um image equals nginx it's just gonna go
47:19 - ahead and download the latest nginx
47:21 - image from
47:23 - Docker Hub that's how it's going to work
47:25 - so when I execute this
47:27 - you see deployment nginx Depot created
47:32 - so now if I do Coop CTO get deployment
47:40 - you see that I have one deployment
47:42 - created I have a status here which says
47:45 - it's not ready yet
47:46 - so if I do Cube CTL get part
47:51 - you see that now I have a pod which has
47:54 - a prefix of the deployment and some
47:56 - random hash here and it says container
47:59 - creating so it's not ready yet
48:02 - so if I do it again it's running
48:05 - and the way it works here is that when I
48:08 - create a deployment
48:10 - deployment has all the information or
48:13 - the blueprint for creating the Pod the
48:16 - for the this is the minimalistic or the
48:18 - most basic configuration for a
48:20 - deployment we're just saying the name
48:22 - and the image that's it the rest is just
48:25 - defaults
48:26 - and between deployment and Nepal there
48:29 - is another layer which is automatically
48:31 - managed by kubernetes deployment called
48:36 - replica set so if I do Cube CTL get
48:39 - replica set
48:41 - written together
48:44 - you see I have an nginx Depot replica
48:47 - set hash and it just gives me a state
48:52 - and if you notice here the Pod name has
48:57 - a prefix of deployment and the replica
49:00 - sets ID and then its own ID so this is
49:04 - how the Pod name is made up and the
49:07 - replica set basically is managing the
49:10 - replicas of a pod you in practice will
49:14 - never have to create replica set or
49:17 - delete a replica set or update in any
49:19 - way
49:20 - you're going to be working with
49:22 - deployments directly which is more
49:24 - convenient because in deployment you can
49:26 - configure the Pod blueprint completely
49:29 - you can say how many replicas of the
49:31 - part you want and you can do the rest of
49:33 - the configuration there here with this
49:36 - command we just created one pod or one
49:39 - replica but if you wanted to have two
49:42 - replicas of the nginx part we can just
49:45 - provide as additional options
49:48 - so this is how the layers work first you
49:51 - have
49:52 - the deployment the deployment manages a
49:55 - replica set a replica set manages all
49:58 - the replicas of that pod and the Pod is
50:02 - again an abstraction of a container and
50:06 - everything below the deployment should
50:08 - be managed automatically by kubernetes
50:10 - you shouldn't have to worry about any of
50:12 - it for example the image that it uses I
50:16 - will have to edit that in a deployment
50:18 - directly and not in the Pod so let's go
50:21 - ahead and do that right away so I'm
50:23 - going to do Cube CTL edit deployment
50:27 - and I'm going to provide the name genix
50:32 - and we get an auto generated
50:35 - configuration file of the deployment
50:37 - because in the command line we just gave
50:39 - two options everything else is default
50:41 - and auto generated by kubernetes
50:44 - um and you don't have to understand this
50:46 - now but I'm going to make a separate
50:47 - video where I break down the
50:49 - configuration file and the syntax
50:52 - of the configuration file for now let's
50:55 - just go ahead and scroll to the image
50:58 - which is somewhere down below
51:01 - and let's say I wanted to fixate the
51:04 - version to 1 16.
51:07 - and save that change
51:12 - and as you see deployment was edited
51:15 - and now when I do Cube CTL get pot
51:20 - I see that the old part
51:23 - so this one here is terminating and
51:26 - another one
51:28 - started 25 seconds ago
51:31 - so if I do it again
51:33 - the old part is gone and the new one got
51:36 - created with the new image
51:38 - and if I do if I get replica set
51:45 - I see that the old one has no pods in it
51:48 - and a new one has been created as well
51:51 - so we just edited the deployment
51:54 - configuration and everything else below
51:58 - that got automatically updated
52:01 - and that's the magic of kubernetes and
52:03 - that's how it works
52:07 - another very practical command is Cube
52:10 - CTL logs which basically shows you what
52:13 - the application running inside the Pod
52:16 - actually locked so if I do Cube CTL logs
52:20 - and I will need the Pod name for this
52:25 - um I will get nothing because nginx
52:27 - didn't log anything so let's actually
52:28 - create another deployment
52:31 - uh
52:33 - from mongodb so let's call it
52:36 - deployment
52:37 - and the image and the image will be
52:41 -  so let's see
52:44 - here
52:48 - part so now I have the mongodb
52:50 - deployment creating so let's go ahead
52:53 - and log
52:55 - that
52:57 - status here means that pod was created
52:59 - but the container inside the Pod isn't
53:03 - running yet and when I try to lock
53:06 - obviously it tells me there is no
53:08 - container running so it can show me and
53:10 - it locks so let's get the status again
53:13 - at this point if I'm seeing that
53:16 - container isn't starting I can actually
53:18 - get some additional information by Cube
53:21 - CTL describe pod and the Pod name
53:25 - which here shows me what state changes
53:28 - happen inside the part so it pulled the
53:31 - image created the container and start a
53:34 - container so Cube CTL get pod it should
53:37 - be running already
53:39 - so now let's log it keeps it here logs
53:44 - and here we see the log output so it
53:47 - took a little bit but this is what the
53:50 - mongodb application container actually
53:53 - locked inside the Pod and obviously if
53:56 - container has some problems it's going
53:59 - to help with debugging to see what the
54:01 - application is actually printing
54:03 - so let's clear that
54:05 - and get the parts again
54:09 - so another very useful command when
54:13 - debugging when something is not working
54:15 - or you just want to check what's going
54:17 - on inside the Pod is Cube CTL exec
54:22 - so basically what it does is that it
54:25 - gets the terminal of that mongodb
54:27 - application container so if I do Cube
54:31 - CTL exec interactive terminal that's
54:34 - what it stands for I will need the Pod
54:37 - name
54:39 - Dash Dash
54:42 - so so with this command
54:45 - I get the terminal of the mongodb
54:47 - application container and as you see
54:49 - here I am inside the container of
54:51 - mongodb as a root user so I'm in a
54:54 - completely different setting now and as
54:56 - I said this is useful in debugging or
54:59 - when you want to test something or try
55:01 - something you can enter the container or
55:03 - get the terminal and execute some
55:05 - comments inside there so we can exit
55:09 - that again
55:12 - and of course with Cube CTL I can delete
55:16 - the pods
55:18 - so if I do get deployment
55:22 - I misspelled it so it keeps it here
55:25 - deployment I see that I have two of them
55:27 - and if I do because it get pod and
55:30 - replica set I have also two of them so
55:33 - let's say if I wanted to get rid of all
55:36 - the pods replica sets underneath I will
55:39 - have to delete
55:40 - the deployment
55:42 - so delete
55:44 - deployment and I'll have to provide the
55:46 - name of the deployment I'm gonna delete
55:50 - let's delete mongodb
55:53 - delete it and now if I'm gonna say Cube
55:56 - CTL get pod the Pod should be
55:58 - terminating and if I do get replica set
56:03 - the mongodb replica set is gone as well
56:07 - and the same if I do delete deployment
56:12 - nginx Deadpool
56:14 - and do the replica set
56:16 - see everything gone so all the crud
56:20 - operations create delete update Etc
56:22 - happens on the deployment level and
56:25 - everything underneath just follows
56:27 - automatically
56:28 - in the similar way way we can create
56:30 - other kubernetes resources like Services
56:32 - Etc however as you notice when we are
56:35 - creating kubernetes components like
56:37 - deployment
56:38 - using cubectl Create deployment
56:43 - um and I misspelled it all the time
56:46 - you'll have to provide all these options
56:48 - on the command line so you'll have to
56:50 - say the name and you'll have to specify
56:52 - the image and then you have this option
56:55 - one option two uh Etc
56:58 - and there could be a lot of things that
57:00 - you want to configure in a deployment or
57:03 - in a pod and obviously it will be
57:05 - impractical to write that all out on a
57:08 - command line
57:09 - so because of that in practice you would
57:12 - usually work with kubernetes
57:14 - configuration files meaning what
57:17 - component you're creating what the name
57:19 - of the component is what image is it
57:22 - based off and any other options they're
57:25 - all gathered in a configuration file and
57:29 - you just tell cubectl to execute that
57:32 - configuration file and the way you do it
57:35 - is using cubectl apply command and apply
57:40 - basically takes the file the
57:42 - configuration file as a parameter and
57:44 - does whatever you have written there so
57:47 - apply takes an option called minus F
57:50 - that stands for file and here you would
57:54 - say the name of the file so this will be
57:56 - the config file dot yaml this is the
58:00 - format that you're usually gonna use
58:03 - for configuration files and this is the
58:06 - command that executes whatever is in
58:08 - that configuration file so let's
58:10 - actually call it configuration file
58:13 - um I don't know nginx deployment
58:17 - and let's go ahead and create a very
58:21 - simplistic super basic uh nginx
58:24 - deployment file so here I'm gonna
58:27 - create that file
58:34 - so this is the basic configuration for
58:38 - the deployment so here I'm just
58:41 - specifying what I want to create I want
58:43 - to create a deployment the name of the
58:45 - deployment
58:46 - you can ignore these labels uh right now
58:49 - uh how many replicas of the parts I want
58:53 - to create and this plug right here the
58:56 - template and specification is a
58:59 - blueprint for the pods so specification
59:01 - for the deployment and specification for
59:05 - a pod and here we're just saying that we
59:07 - want one container inside of the pod
59:10 - with nginx image and we are going to
59:14 - bind that on Port 80. so this is going
59:16 - to be our configuration file and once we
59:20 - have that we can apply that
59:22 - configuration
59:29 - so deployment created so now if I get
59:33 - pod
59:34 - I see that nginx deployment pod was
59:38 - created and it's running and let's also
59:40 - see the deployment was created 52
59:43 - seconds ago and now if I wanted to
59:46 - change something in that deployment I
59:48 - can actually change my local
59:50 - configuration
59:52 - for example I wanted two replicas
59:55 - instead of one
59:59 - I can apply that
60:02 - again
60:06 - deployment nginx deployment configured
60:09 - and as you see the difference here is
60:12 - that
60:13 - kubernetes can detect if the nginx
60:15 - deployment doesn't exist yet it's going
60:17 - to create one
60:19 - but if it already exists
60:21 - and I apply the configuration file again
60:24 - it's going to know that it should update
60:27 - it instead of creating a new one so if I
60:30 - do get deployment I see this is the old
60:34 - one or the old deployment
60:36 - and if I do Cube CTL get part I see the
60:39 - old one is still there and a new one got
60:42 - created because I increased the replica
60:44 - count which means that with Cube CTL
60:46 - apply you can both create and update a
60:49 - component and obviously you can do Coupe
60:52 - CTL with services
60:54 - volumes any other kubernetes components
60:57 - just like we did it with the deployment
60:59 - so in the next video I'm going to break
61:01 - down the syntax of the configuration
61:02 - file which is pretty logical and simple
61:05 - actually to understand and I'm going to
61:07 - explain all the different attributes and
61:09 - what they mean so you can write your own
61:12 - configuration files for different
61:13 - components so to summarize we've looked
61:16 - at a couple of cubectl commands in this
61:19 - video we saw how to create a component
61:22 - like deployment how to edit it and
61:24 - delete it we saw how to get status of
61:27 - PODS deployments replica sets cetera we
61:32 - also logged on the console whatever
61:33 - application is writing it to the console
61:36 - in the Pod and we saw how to get a
61:39 - terminal of a running container using
61:41 - cubectl exec and finally we saw how to
61:44 - use a kubernetes configuration file to
61:47 - create and update components using the
61:51 - cube CTL apply command and last but not
61:54 - least we saw Cube CTL describe command
61:56 - which will win a container isn't
61:58 - starting in a pot and you want to get
62:00 - some additional troubleshooting
62:01 - information about the pod
62:07 - in this video I'm going to show you the
62:09 - syntax and the contents of kubernetes
62:12 - configuration file which is the main
62:14 - tool for creating and configuring
62:16 - components in kubernetes cluster if
62:19 - you've seen large configuration files it
62:21 - might seem overwhelming but in reality
62:23 - it's pretty simple and intuitive and
62:26 - also very logically structured so let's
62:28 - go through it step by step
62:33 - so here I have examples of a deployment
62:36 - and service configuration files side by
62:39 - side so the first thing is that every
62:43 - configuration file in kubernetes has
62:45 - three parts the first part is where the
62:49 - metadata of that component that you're
62:52 - creating resides and one of the metadata
62:55 - is obviously name of the component
62:57 - itself the second part in the
63:00 - configuration file is specification so
63:04 - each components configuration file will
63:06 - have a specification where you basically
63:08 - put every kind of configuration that you
63:11 - want to apply for that
63:14 - component the first two lines here as
63:17 - you see is just declaring what you want
63:20 - to create here we are creating
63:22 - deployment and here we're creating a
63:24 - service
63:25 - and this is basically you have to look
63:28 - up for each component there's a
63:29 - different API version
63:31 - so now inside of the specification part
63:34 - obviously the attributes will be
63:38 - specific to the kind of a component that
63:42 - you're creating so deployment will have
63:44 - its own attributes that only apply for
63:47 - deployment and the service will have its
63:50 - own stuff but I said there are three
63:52 - parts of a configuration file and we
63:56 - just see metadata and the specification
63:58 - so where's the third part so the third
64:01 - part will be a status but it's going to
64:04 - be automatically generated and added by
64:07 - kubernetes so the way it works is that
64:10 - kubernetes will always compare what is
64:13 - the desired State and what is the actual
64:15 - stated or the status of the component
64:18 - and if the status and desired state do
64:20 - not match then kubernetes knows there's
64:23 - something to be fixed there so it's
64:25 - going to try to fix it and this is the
64:28 - basis of the self-healing feature that
64:31 - could kubernetes provides for example
64:33 - here you specify you want two replicas
64:36 - of nginx deployment so when you apply
64:40 - this when you actually create the
64:41 - deployment using this configuration file
64:43 - that's what apply means kubernetes will
64:46 - adhere the status of your deployment and
64:50 - it will update that state continuously
64:52 - so for example if a status at some point
64:55 - will say just one replica is running
64:58 - then kubernetes will compare that status
65:01 - with the specification and we'll know
65:03 - there is a problem there another replica
65:06 - needs to be created sap now another
65:09 - interesting question here is where does
65:11 - kubernetes actually get that status data
65:14 - to automatically add here or update
65:17 - continuously that information comes from
65:19 - the etcd remember the cluster brain one
65:23 - of the master processes that actually
65:25 - stores the cluster data so its CD holds
65:29 - at any time the current state is of any
65:33 - kubernetes component and that's where
65:35 - the status information comes from
65:40 - so as you see the format of the
65:43 - configuration files is yaml that's why
65:46 - the extension here and generally it's
65:49 - pretty straightforward to understand it
65:51 - it's a very simple format but yaml is
65:53 - very strict about the indentations so
65:56 - for example if you have something
65:58 - wrongly indented here your file will be
66:01 - invalid so what I do especially if I
66:05 - have a configuration file that has 200
66:07 - lines it's pretty long I usually use
66:10 - some yaml online validator to see where
66:14 - I need to fix that but other than that
66:17 - it's pretty simple another thing is
66:20 - where do you actually store those
66:22 - configuration files a usual practice is
66:26 - to store them with your code because
66:28 - since the deployment in service is going
66:31 - to be applied to your application it's a
66:34 - good practice to store these
66:35 - configuration files in your application
66:37 - code so usually it will be part of the
66:40 - whole infrastructure as a code concept
66:43 - or you can also have its own git
66:46 - repository just for the configuration
66:48 - files
66:51 - so in the previous video I showed you
66:53 - that deployments manage the parts that
66:56 - are below them so whenever you edit
66:59 - something in a deployment it kind of
67:01 - Cascades down down to all the ports that
67:03 - it manages and whenever you want to
67:06 - create some pods you would actually
67:08 - create a deployment and it will take
67:10 - care of the rest so how does this happen
67:13 - or where is this whole thing defined in
67:15 - the configuration
67:17 - um so here in the specification part of
67:20 - a deployment you see a template
67:22 - and if I expand it you see the template
67:25 - also has its own metadata and
67:28 - specification so it's basically a
67:31 - configuration file inside of a
67:33 - configuration file
67:34 - and the reason for it is that this
67:38 - configuration
67:39 - applies to a pod so pod should have its
67:43 - own configuration inside of deployments
67:46 - configuration file and that's how all
67:49 - the deployments will be defined and this
67:51 - is going to be the blueprint for a pod
67:54 - like which image it should be based on
67:56 - which Port it should open what is going
67:59 - to be the name of the container Etc
68:05 - so the way the connection is established
68:08 - is using labels and selectors
68:11 - so as you see metadata part contains the
68:15 - labels
68:16 - and the specification part contains
68:18 - selectors it's pretty simple in a
68:21 - metadata you give
68:25 - components like deployment or pod a key
68:29 - value pair and it could be any key value
68:31 - pair that you think of in this case we
68:34 - have app nginx
68:35 - and that label just sticks to that
68:39 - component so we give pods
68:43 - created using this blueprint label app
68:46 - engine X and we tell the deployment to
68:50 - connect or to match all the labels
68:54 - with app nginx
68:57 - to create that connection
68:59 - so this way deployment will know which
69:01 - pods belong to it
69:03 - now deployment has its own label app
69:06 - engine X and these two labels are used
69:09 - by the service selector so in the
69:12 - specification of a service we Define a
69:15 - selector which basically makes a
69:17 - connection
69:18 - between the service and the deployment
69:22 - or its parts
69:24 - because service must know which pods are
69:28 - kind of registered with it so which pods
69:31 - belong to that service and that
69:33 - connection is made through the selector
69:36 - of the label and we're going to see that
69:39 - in a demo so another thing that must be
69:42 - configured in the service and pod
69:45 - is the ports
69:47 - so if I expand this I see that service
69:51 - has its ports configuration and the
69:56 - container inside of a pod is obviously
69:59 - running or needs to run its import right
70:02 - so how this is configured is basically
70:05 - service has a port where the service
70:09 - itself is accessible at so if other
70:13 - services sends a request to nginx
70:15 - service here it needs to send it on Port
70:17 - 80 but this service needs to know to
70:21 - which pod it should forward the request
70:24 - but also at which Port is that pod
70:26 - listening and that is the Target Port
70:30 - so this one should match the container
70:33 - port and with that we have our
70:35 - deployment and service basic
70:37 - configurations done and to note here
70:41 - most of these attributes that you see
70:43 - here in both parts are required so this
70:46 - will actually be the minimum
70:48 - configuration for deployment and service
70:50 - so once we have those files
70:53 - let's actually apply them or create
70:56 - components using them
70:58 - so let's head over to the console and
71:02 - here I'm going to create
71:04 - both deployment and service so Cube CTL
71:08 - apply
71:11 - deployment
71:14 - created and nginx service
71:19 - so now if I
71:21 - get the pods I see two replicas are
71:23 - running because that's how I Define it
71:25 - here and we have our service as well
71:30 - which is engine X service this is a
71:33 - default service it's always there this
71:36 - is the one we created
71:38 - and it's listening on Port 80 as we
71:41 - specified now how can we validate that
71:45 - the service
71:46 - has the right parts that it forwards the
71:51 - requests to we can do it using Cube CTL
71:54 - describe service and the service name
72:01 - and here you see the endpoints
72:05 - where you have all these status
72:08 - information here like the things that we
72:11 - Define in the configuration like app
72:13 - selector
72:14 - Etc we have the Target Port that we
72:17 - Define and we have the endpoints here
72:19 - and this must be the IP addresses
72:24 - and ports of the pots
72:27 - that the service must forward
72:31 - the request to so how do we know that
72:33 - these are the IP addresses of the right
72:35 - pods because if qctl get pod you don't
72:39 - get this information so the way we do it
72:42 - or way we find that out is using get pod
72:45 - and then you do Dash o which is for
72:48 - outputs and then we want
72:50 - more information so all white
72:55 - and here we see more columns here so we
72:58 - have the name and Status ready Etc but
73:01 - we also have the IP address so here is
73:05 - the IP address endpoint specified here
73:07 - and this is the other one so we know
73:10 - that the service has right endpoints so
73:13 - now let's see uh the third part of the
73:16 - configuration file which is a status
73:17 - that kubernetes automatically generated
73:20 - and the way to do it is we can get the
73:24 - deployment
73:27 - nginx deployment in a yaml format so
73:32 - when I execute this command I will get
73:34 - the resulting or the updated
73:36 - configuration of my deployment which
73:38 - actually resides in the hcd because etcd
73:42 - stores the status of the whole cluster
73:45 - including every component so if I do
73:48 - this I'll get the yaml output in my
73:51 - console but I want it in the file so I'm
73:53 - gonna save it into
73:56 - nginx deployment
74:02 - result
74:06 - and I'm Gonna Save it there and I'm
74:09 - going to open it in my editor next to
74:11 - the original one so as you see a lot of
74:14 - stuff has been added but let's just see
74:17 - the status part
74:18 - so all this is automatically edit and
74:22 - updated constantly by kubernetes so it
74:25 - says how many replicas are running what
74:28 - the state of those replicas and some
74:31 - other information so this part can also
74:33 - be helpful when debugging so that's the
74:36 - status but also if you noticed
74:40 - other stuff has been added in the
74:42 - metadata and specification part as well
74:45 - so for example uh creation timestamp
74:48 - when was the component created is
74:51 - automatically edited by kubernetes
74:53 - because it is a metadata some unique ID
74:56 - Etc you don't have to care about it and
74:58 - in the specification part it just adds
75:01 - some defaults for that component but
75:04 - again you don't have to
75:06 - care or understand most of these
75:08 - attributes but one thing to note here is
75:10 - that if you for example want to copy a
75:14 - deployment that you already have using
75:18 - um maybe automated scripts you will have
75:21 - to remove and get rid of most of these
75:24 - generated stuff so you have to clean
75:26 - that deployment configuration file first
75:30 - and then you can create another
75:32 - deployment from that blueprint
75:35 - configuration so that's it with this
75:37 - video so from now on we're going to be
75:40 - working with the configuration files so
75:42 - for example if I want to delete the
75:44 - deployment and the service I can do it
75:46 - using the file configuration file as
75:49 - well using delete
75:53 - and
75:59 - like this
76:00 - the deployment will be gone and I can do
76:03 - the same for service
76:07 - all right so using cubectl apply and
76:10 - Cube CTL delete you can basically work
76:13 - with the configuration files
76:18 - in this video we're going to deploy two
76:21 - applications mongodb and Express
76:24 - and I chose these two because it
76:27 - demonstrates really well a typical
76:29 - simple setup of a web application and
76:31 - its database so you can apply this to
76:34 - any similar setup you have so let's see
76:37 - how we're going to do this
76:40 - so first we will create a mongodb pod
76:43 - and in order to talk to that pod we are
76:45 - going to need a service and we're going
76:47 - to create an internal service which
76:49 - basically means that no external
76:51 - requests are allowed to the Pod only
76:53 - components inside the same cluster can
76:55 - talk to it and that's what we want then
76:57 - we're going to create a Express
76:59 - deployment one we're going to need a
77:02 - database URL of mongodb so that
77:05 - Express can connect to it and the second
77:07 - one is credentials so username and
77:10 - password of the database so that it can
77:11 - authenticate so the way we can pass this
77:15 - information to Express deployment
77:16 - is through its deployment configuration
77:19 - file through environmental variables
77:22 - because that's how the application is
77:23 - configured so we're going to create a
77:26 - config map that contains database URL
77:28 - and we're going to create a secret that
77:31 - contains the credentials and we're going
77:33 - to reference both inside of that
77:35 - deployment file so once we have that set
77:37 - up we're going to need Express to
77:39 - be accessed accessible through a browser
77:42 - in order to do that we're going to
77:43 - create an external service that will
77:46 - allow external requests to talk to the
77:49 - Pod so the URL will be HTTP IP address
77:53 - of the node and the service port so with
77:57 - this setup the request flow will now
77:59 - look like this so the request comes from
78:01 - the browser and it goes to the external
78:04 - service of the Express which will
78:06 - then forward it to the Express pod
78:08 - the Pod will then connect to internal
78:12 - service of mongodb that's basically the
78:14 - database URL here and it will forward it
78:18 - then to mongodb pod where it will
78:21 - authenticate the request using the
78:24 - credentials so now let's go and create
78:26 - this whole setup using kubernetes
78:28 - configuration files
78:29 - let's dive right into it and create the
78:32 - whole setup so first of all I have a
78:34 - mini Cube cluster running if I do Cube
78:37 - CTL get all which basically gets me all
78:40 - the components that are inside the
78:42 - cluster I only have a default kubernetes
78:45 - service so my cluster is empty and I'm
78:48 - sorry from scratch so the first thing
78:50 - that I said we're gonna do is create a
78:53 - mongodb deployment
78:55 - I usually create it in an editor
78:58 - so I'm going to go to visual studio code
79:00 - and
79:02 - paste a prepared deployment file there
79:06 - for mongodb and this is how it's going
79:08 - to look like so I have deployment kind
79:11 - and I have some metadata I'm just going
79:14 - to call it mongodb deployment
79:16 - um
79:17 - labels and selectors uh in the previous
79:20 - video I already explained the syntax of
79:22 - kubernetes yaml configuration file so if
79:27 - you want to know what all these
79:29 - attributes mean then you can check out
79:30 - that video and here in the template I
79:33 - have a definition or blueprint for parts
79:36 - that this deployment gonna create and
79:39 - I'm just gonna go with one replica
79:41 - so the container is going to be called
79:44 - mongodb and this is the image that I'm
79:46 - gonna take so let's actually go and
79:49 - check out the image configuration for
79:51 - mongodb
79:57 - and I see
79:59 - this image here
80:01 - let's open this
80:03 - and basically what I'm looking for is
80:05 - how to use that container meaning what
80:09 - ports it's gonna open and what's uh
80:12 - external configuration it's going to
80:14 - take so a default Port of mongodb
80:19 - container is
80:20 - 27017 so I'm gonna use that
80:24 - and we are gonna use variables
80:28 - environmental variables the root
80:30 - username and root password so basically
80:33 - I can on the container startup Define
80:36 - the admin username and password so let's
80:39 - go ahead and configure all of that
80:40 - inside the configuration file
80:43 - so here below the image of mongodbit so
80:47 - we're just gonna leave the name of the
80:49 - image and it's gonna pull the latest one
80:51 - and that's what we want so here I'm
80:53 - gonna specify what port I want to expose
80:56 - so ports that's the attribute name
81:00 - and
81:01 - container
81:03 - port
81:06 - and that's the standard Port so I'm
81:09 - gonna leave it and below that I'm gonna
81:12 - specify those two environmental
81:15 - variables so one is called let's see
81:19 - what it's called it's any DB root
81:22 - username and here is going to be a value
81:25 - so we're gonna actually leave it blank
81:27 - for now and the other one is
81:31 - called initroot password and we're going
81:34 - to leave that blank as well just value
81:37 - and once we have the values here
81:41 - um we're gonna have a complete
81:43 - deployment for mongodb this is basically
81:45 - all we need now note that this is a
81:48 - configuration file that is going to be
81:50 - checked into a repository so usually you
81:53 - wouldn't write admin username and
81:55 - password inside the configuration file
81:57 - so what we're going to do now is we're
82:00 - going to create a secret from where we
82:03 - will reference the values so meaning
82:06 - that the secret is going to leave in
82:09 - kubernetes and nobody will have access
82:11 - to it in a git repository so we're going
82:14 - to save this incomplete deployment file
82:17 - first of all so let's call it
82:21 - deployment
82:23 - or let's just call it yemo and
82:27 - save it
82:28 - here so that we get the syntax highlight
82:31 - and now before we apply this
82:33 - configuration we're going to create the
82:34 - secret where the root username and
82:36 - password will leave
82:40 - so let's create a new file and I'm going
82:42 - to paste in the configuration of a
82:45 - secret which is actually pretty simple
82:47 - so we have a kind secret then we have a
82:50 - metadata which again is just simply the
82:52 - name we're going to call it mongodb
82:55 - secret the type opaque is actually a
82:58 - default type which is the most basic key
83:00 - value secret type other types for
83:02 - example include TLS certificates so you
83:06 - can create a secret specifically with
83:08 - the TLs certificate type and a couple of
83:11 - more types but mostly you're going to
83:13 - use the default one and these are the
83:15 - actual contents so you have the data and
83:18 - here you have key value pairs which of
83:22 - course are the names you come up with so
83:25 - we're going to specify username or we
83:27 - can actually call it
83:31 - root username
83:34 - and we're going to call it root
83:36 - password here's the thing the values in
83:41 - in this key value pairs are not plain
83:44 - text so when we are creating a secret
83:46 - the value must be base64 encoded so the
83:50 - way you can do that the simplest way is
83:53 - go to your terminal so here I'm gonna
83:55 - say Echo minus n very important option
83:58 - don't leave it out otherwise it's not
84:01 - going to work and here I'm gonna put a
84:03 - plain text value that I want so I'm just
84:06 - going to go with just using
84:08 - whatever of course you can have
84:11 - something more secretive here and I'm
84:14 - gonna
84:15 - base64 encoding and the value that I get
84:19 - here
84:20 - I'm gonna copy it into the secret
84:22 - configuration as a value and I'm going
84:25 - to do the same with password
84:27 - so
84:28 - again I'm just gonna go with simple
84:30 - password obviously you want to have
84:31 - something more secure
84:33 - and I'm gonna copy that as a value here
84:37 - and
84:39 - save it is secret Dot yeml
84:46 - okay now we have only written
84:48 - configuration files we haven't created
84:50 - anything yet in the cluster so this is
84:53 - just preparation work and we have to
84:56 - create secret before the deployment if
84:59 - we're gonna reference The Secret inside
85:01 - of this so the order of creation matters
85:04 - because if I'm creating a deployment
85:06 - that references a secret that doesn't
85:08 - exist yet I'm gonna get an error so it's
85:11 - not going to start since we have our
85:13 - first component let's actually go ahead
85:15 - and create our secret from a
85:17 - configuration file so again I'm going to
85:19 - go to my
85:21 - console let's actually clear all this
85:23 - and I'm gonna go into the folder where
85:28 - I'm creating all these configuration
85:29 - files I called it kubernetes
85:32 - configuration and here I have both of my
85:34 - files so I'm do I'm gonna do Cube CTL
85:37 - apply
85:39 - Secret
85:42 - and secret created so I'm gonna do Cube
85:46 - CTL get secret
85:48 - and I should see my secret has been
85:51 - created
85:52 - this is something created by default
85:53 - with a different type and this is our
85:56 - secret here
85:57 - so now that we have our secrets we can
86:00 - reference it inside of our deployment
86:03 - configuration file so let's go back and
86:06 - this is how you reference contents
86:09 - specific key value data of secret so
86:13 - instead of value we're going to say
86:15 - value from
86:18 - and then I'm going to do Secret
86:20 - Key ref
86:22 - no secret key reference
86:24 - and
86:26 - name is going to be the secret name
86:28 - so this one here
86:31 - and key is going to be the key in the
86:34 - data I want the value of this key value
86:38 - pair so I want this part of the data so
86:41 - I'm going to reference it by key so you
86:43 - don't have to learn it by heart
86:45 - obviously all the syntax and attribute
86:47 - names important thing here is that you
86:50 - know approximately how to reference it
86:52 - the actual syntax you can always look up
86:55 - in Google or maybe from previous
86:56 - configuration files but yeah this is how
86:59 - you reference it and we're going to do
87:00 - the same with password so I'm gonna do
87:03 - from and I'm just gonna copy the rest
87:06 - here
87:09 - remember yemo is very strict with the
87:12 - indentation here is the same secret but
87:14 - a different key so I'm gonna use a
87:17 - password key here
87:19 - and
87:21 - that will be it so now we have
87:25 - the root username and password
87:26 - referenced from the secret and no actual
87:30 - values inside the configuration file
87:34 - which is good for security because you
87:35 - don't want your credentials in your code
87:38 - Repository
87:39 - okay so our deployment file is actually
87:42 - ready so let's apply that
87:55 - and the deployment created meaning if I
87:58 - do get all
88:00 - I should see the Pod starting up the
88:03 - deployment and the replica set so let's
88:06 - actually
88:08 - check how pod is doing
88:12 - container creating so
88:14 - let's actually watch it might take some
88:17 - time to create it if it takes long and
88:19 - if you want to see whether there is a
88:22 - problem there you can also do Cube CTL
88:24 - describe pod
88:26 - and the Pod name
88:28 - so at least we know nothing's wrong
88:30 - there so we see that it's just pulling
88:33 - the image so that's what it takes so
88:35 - long
88:37 - so let's see again Cube CTL get pot
88:40 - and as you see it's running so we have
88:44 - mongodb deployment and the Pod one
88:46 - replica of its part running now the
88:49 - second step is we're going to create an
88:51 - internal service so that other
88:53 - components or other ports can talk to
88:56 - this mongodb so let's go ahead and
88:58 - create service configuration
89:01 - so go back to yemo and here we can
89:05 - either create a separate emo
89:06 - configuration file for secret or we can
89:09 - also include it in the same one so in
89:11 - yaml you can actually put multiple
89:13 - documents in one file
89:16 - so if I put three dashes uh that's
89:18 - basically a Syntax for document
89:21 - separation in yaml so a new document is
89:24 - starting so actually I'm going to put
89:25 - both deployment and service in one
89:28 - configuration file because they usually
89:30 - belong together so here I'm gonna
89:33 - paste the service configuration
89:35 - and by the way I'm going to put all
89:37 - these configuration files in git
89:39 - repository and Link the repository in
89:42 - the description of this video
89:44 - so this is a service for mongodb let's
89:46 - go through some of the attributes here
89:48 - so it's the service kind just the name
89:52 - we're going to call it mongodb service
89:54 - selector this is an important one
89:56 - because we want this service to connect
89:59 - to the Pod right and the way to do that
90:02 - is using selector and label so using
90:07 - this here the labels that deployment and
90:10 - pod have service can find the parts that
90:13 - it's going to attach to right so we have
90:16 - the selector here and this is an
90:18 - important part where we expose Service
90:22 - Port so this is going to be the service
90:24 - port and this is going to be the
90:26 - container and since we exposed container
90:28 - Port it this address right here these
90:32 - two have to match so Target Port is
90:35 - container or pod port and this is the
90:39 - service port and obviously these two
90:41 - here can be different but I'm gonna go
90:44 - with the same port and that's basically
90:47 - it that's our service so I'm gonna
90:49 - create the service now
90:52 - this file and go back to my
90:55 - and so and I'm gonna apply
90:59 - the same file that I applied before to
91:02 - create deployment so let's see what
91:04 - happens
91:05 - see both deployment and service
91:06 - configuration but it's going to know
91:08 - that I haven't changed the deployment
91:10 - that's what it means here and a service
91:14 - is created so if I were to edit both for
91:17 - example I can reapply the file and
91:20 - deployment service can be changed so I
91:23 - think using local configuration files is
91:26 - a handy way to edit your components so
91:29 - now let's actually check that our
91:30 - service was created
91:32 - get service
91:36 - and this is our service and it's
91:38 - listening at Port 27017 and I showed it
91:42 - in one of the previous videos but we can
91:44 - actually also validate that the service
91:46 - is attached to the correct pod and to do
91:50 - that I'm gonna do describe
91:55 - service and
91:56 - service name for this
91:59 - here I have the endpoint which is an IP
92:03 - address of a pod and the port where the
92:07 - application inside the Pod is listening
92:10 - it so let's actually check that this is
92:12 - the right pod I mean we just have one
92:14 - but still
92:15 - so if I do get pot and I want additional
92:20 - output to what I get by default
92:22 - one of the columns includes the IP
92:25 - address which is this one right here so
92:29 - 172.17
92:32 - 06 that's the Pod IP address and this is
92:37 - the port where the application inside
92:39 - the Pod is listening at so everything is
92:43 - set up perfectly mongodb deployment and
92:45 - service has been created and by the way
92:48 - if you want to see all the components
92:50 - for one
92:51 - um application you can also display them
92:56 - using cubectl get all that will show all
92:59 - the components and you can filter them
93:02 - by name so Bongo DB
93:07 - and here you see the service
93:10 - deployment replica set and the pod
93:14 - so when you do all that component type
93:17 - will be the first here okay that's just
93:21 - a side info so now the next step we're
93:24 - going to create Express deployment
93:26 - and service and also an external
93:28 - configuration
93:30 - um where we're going to put the database
93:32 - URL for mongodb so let's go ahead and do
93:34 - it
93:35 - I'm going to clear that up and go and
93:39 - create a new file for Express
93:41 - deployment and service
93:43 - so this is the deployment draft of
93:48 - Express
93:49 - same things here Express that's
93:51 - the name
93:52 - and here we have the Pod definition
93:55 - where the image name is Express
93:58 - let's actually go ahead and check that
94:00 - image as well
94:03 - we don't need this this is Express
94:07 - and that's the name of the image
94:09 - Express and let's see the same data here
94:12 - let's see the port the express
94:14 - application inside the container starts
94:17 - at is 8081
94:20 - and
94:22 - these are some of the environmental
94:24 - variables so obviously we need three
94:26 - things for Express we need to tell
94:29 - it which database application it should
94:32 - connect to so obviously we need to tell
94:34 - it the mongodb address database address
94:37 - it should connect to the internal
94:39 - service and we we're gonna need
94:41 - credentials so that mongodb can
94:43 - authenticate that connection and the
94:46 - environmental variables to do that is
94:48 - going to be admin username admin
94:50 - password and the mongodb endpoint will
94:52 - be this here so these three
94:54 - environmental variables we need so let's
94:57 - go ahead and use that so first we're
95:00 - going to open the port
95:02 - again
95:03 - container ports
95:08 - and the reason why you have multiple
95:10 - ports is that inside of the Pod you can
95:12 - actually open multiple ports
95:15 - so that's going to be 8081 and now we're
95:18 - gonna
95:19 - add the environmental variables for the
95:21 - connectivity so the first one is
95:25 - the username and this is going to be
95:27 - obviously the same username and password
95:29 - that we defined right here so what I'm
95:33 - going to do is I'm just going to copy
95:35 - them because it's really the same so the
95:37 - value from we're going to read it from
95:39 - the secret that's already there so I'm
95:43 - gonna paste it here
95:48 - second environmental variable is called
95:50 - admin password
95:52 - and I'm also going to copy that from
95:55 - here
95:58 - and the third one
96:02 - is gonna be the database server and
96:05 - since this is also an external
96:07 - configuration we could either do value
96:09 - here and we could write the mongodb
96:12 - server address directly here or as I
96:15 - showed you in the diagram at the
96:17 - beginning we can put it in a config map
96:19 - which is an external configuration so
96:21 - that it's centralized so it's stored in
96:24 - one place and also other components can
96:27 - also use it so for example if I have two
96:30 - applications that are using mongodb
96:32 - database then I can just reference that
96:36 - external configuration here and if I
96:38 - have to change it at some point I just
96:40 - change it in one place and nothing else
96:42 - gets updated so because of that we're
96:45 - gonna keep this incomplete deployment
96:48 - configuration and we're going to create
96:51 - the config map which will contain the
96:54 - mongodb server address so I'm going to
96:57 - create a new file let's actually save
96:59 - this incomplete deployment let's call it
97:02 -  Express yaml and we're going to
97:05 - come back to it later
97:07 - so save that now we need a config map
97:11 - here
97:12 - so I'm going to copy the configuration
97:14 - and this is also pretty simple just like
97:17 - secret you have the kind which is config
97:21 - map the name and the same construct see
97:25 - just like you saw here data which is key
97:28 - value pair it doesn't have a type
97:30 - because they're just one config map type
97:33 - and that's it and here you again have
97:36 - key value pairs so database URL and
97:39 - server name is actually the name of the
97:42 - service it's as simple as that so what
97:45 - do we call our service we call it
97:46 - mongodb service so I'm going to copy the
97:49 - service name and that's going to be the
97:52 - database server URL
97:54 - so I'm going to copy that and let's
97:56 - actually call it
97:58 - config map for consistency
98:01 - save it
98:02 - and just like with secret the order of
98:06 - execution or creation matters so I have
98:09 - to have a config map already in the
98:11 - cluster so that I can reference it
98:14 - so when we're done
98:16 - I have to con create the config map
98:18 - first and then the deployment so the way
98:22 - that I can reference the config map
98:24 - inside the deployment is very similar to
98:28 - secret so I'm actually going to copy the
98:31 - whole thing from Secret put it here the
98:34 - only thing different here is that
98:36 - instead of secret I'm gonna say
98:39 - config
98:41 - map it's all camel case and obviously
98:44 - the name is gonna be
98:46 - config map that's what we called it I
98:49 - think
98:51 - the name let's actually copied
98:53 - and again the key is the key in the key
98:56 - value pair here so let's copy that as
98:59 - well so now I have our Express
99:01 - deployment this is just standard stuff
99:05 - and this is where the Pod blueprint or
99:07 - container configuration exists we have
99:10 - exposed Port 8081 this is the image with
99:14 - latest tag and these are the three
99:16 - environmental variables that
99:19 - Express needs to connect and
99:21 - authenticate with mongodb so deployment
99:24 - is done and let's go ahead and create
99:27 - config map first and then Express
99:30 - deployment
99:32 - CTL apply
99:38 - config map
99:41 - and I'm gonna do group CDL apply
99:45 - always express
99:49 - and let's see the pod
99:53 - so container creating
99:55 - looks good
99:57 - so let's see the pod
99:59 - and it's running
100:01 - and I actually want to see the Lux
100:04 - so I'm going to lock the Express
100:09 - and here you see that express service
100:11 - started and database connected so now
100:15 - the final step is to access
100:18 - Express from a browser and in order to
100:20 - do that we are going to need an external
100:22 - service for Express so let's go
100:25 - ahead and create that one as well so
100:27 - let's clear this output go back to
100:30 - visual code and as we did last time in
100:32 - the same file as a deployment I'm going
100:35 - to create express service because
100:38 - actually in practice you never have
100:41 - deployment without the service so it
100:43 - makes sense to keep them together
100:45 - and this is Express external
100:48 - service and this configuration right now
100:51 - looks exactly same as the mongodb
100:55 - service configuration and even ports are
100:58 - the same like here I have exposed
101:01 - service port at 8081 and Target Port is
101:04 - where the container Port is listening so
101:08 - how do I make these external service is
101:12 - by doing two things so in the
101:15 - specification section so I'm gonna do it
101:18 - below the selector I'm gonna put a type
101:23 - and a type of this external service is
101:26 - load balancer
101:30 - which I think is a bad name for external
101:33 - service because internal service
101:35 - also acts as a load balancer so I've had
101:38 - two mongodb pods the internal service
101:40 - would also load balance the requests
101:43 - coming to these parts so I think the
101:45 - load balancer type name was chosen not
101:48 - very uh well because it could be
101:51 - confusing but what this type load
101:53 - balancer does basically is it accepts
101:55 - external requests by assigning the
101:59 - service in external IP address so
102:02 - another thing that we're gonna do here
102:03 - to make this service external is right
102:07 - here we're gonna provide third port and
102:10 - this is going to be called node port
102:14 - and what this is basically
102:16 - is the port where this external IP
102:20 - address will be open
102:22 - so this will be the port that I'll have
102:24 - to put in the browser to access this
102:27 - service and this node Port actually has
102:29 - a range and that range is between 30
102:34 - 000 and
102:35 - 32 000 something so I can not give it
102:39 - the same port as here as I said it has
102:41 - to be between that range
102:43 - so I'm just gonna go with the 30 000
102:46 - that's the minimum in that range and
102:49 - that would be it so this configuration
102:51 - here will create an external service
102:54 - let's go ahead and do it and I will show
102:56 - you exactly how these ports differ from
103:00 - each other so I'm gonna apply
103:05 - Express
103:08 - so service created and if I do get
103:12 - service
103:14 - I see that mongodb service that we
103:17 - created previously has a type of cluster
103:21 - IP and the express service that we
103:23 - just created is load balancer which is
103:25 - the type that we specifically defined in
103:28 - internal service we didn't specify any
103:30 - type because cluster IP which is the
103:34 - same as in internal service type is
103:37 - default so you don't have to Define it
103:39 - when you're creating internal service
103:41 - and the difference here is that cluster
103:45 - IP will give the service an internal IP
103:49 - address which is this one right here so
103:52 - this is an internal IP address of the
103:54 - service and load balancer will also give
103:57 - service an internal IP address but in
104:00 - addition to that it will also give the
104:02 - service an external IP address where the
104:05 - external requests will be coming from
104:08 - and here it says pending because we're
104:10 - in minicube and it works a little bit
104:12 - differently in a regular kubernetes
104:14 - setup here you would also see an actual
104:17 - IP address a public one and this is
104:19 - another difference because with internal
104:22 - IP address you just have port for that
104:25 - IP address with both internal and
104:28 - external IP addresses you have ports for
104:30 - both of them and that's why we had to
104:32 - Define third port which was for the
104:35 - external IP address as I said pending
104:37 - means that it doesn't have the external
104:39 - IP address yet so in minicube the way to
104:42 - do that is using the command minicube
104:46 - service and I'm gonna need the name of
104:49 - the service
104:51 - so this command will basically assign my
104:54 - external service a public IP address so
104:57 - I'm going to execute this and the
104:59 - browser window will open
105:01 - and I will see my Express page so
105:04 - if I go back to the command line you see
105:06 - that this command here is signed
105:09 - express service
105:10 - a URL with a public IP address or with
105:13 - an external IP address
105:15 - and the port which is what we defined in
105:19 - the node Port so I can basically copy
105:22 - that command which is the same as this
105:24 - one here
105:25 - and I get the page form on Express so
105:30 - now with this setup the way it's going
105:31 - to work is that when I make changes here
105:34 - for example I'm going to create a new
105:36 - database let's call it test
105:39 - DB whatever and I'm gonna create a
105:42 - request what just happened in background
105:44 - is that this request landed with the
105:49 - external service of Express which
105:52 - then forwarded it to the Express
105:53 - pod and the Express pod connected
105:57 - to the mongodb service an eternal
105:59 - service and mongodb service then
106:02 - forwarded that request finally to the
106:04 - mongodb Pod and then all the way back
106:07 - and we have the changes here
106:10 - so that's how you deploy a simple
106:12 - application setup in a kubernetes
106:14 - cluster
106:18 - in this video we're going to go through
106:20 - the usages of a namespace and the best
106:24 - practices of when and how to use a
106:26 - namespace
106:28 - first of all what is a namespace in
106:30 - kubernetes in kubernetes cluster you can
106:32 - organize resources in namespaces so you
106:36 - can have multiple namespaces in a
106:38 - cluster you can think of a namespace as
106:40 - a virtual cluster inside of a kubernetes
106:43 - cluster now when you create a cluster by
106:46 - default kubernetes gives you namespaces
106:49 - out of the box so in the command line if
106:52 - I type Cube CTL get namespaces I see the
106:55 - list of those out of the box namespaces
106:58 - that kubernetes offers and let's go
107:00 - through them one by one the kubernetes
107:02 - dashboard namespace is shipped
107:05 - automatically in minicube so it's
107:07 - specific to mini Cube installation you
107:09 - will not have this in a standard cluster
107:11 - the first one is Cube system
107:15 - Cube system namespace is not meant for
107:18 - your use so basically you shouldn't
107:20 - create anything or shouldn't modify
107:22 - anything in Cube system namespace the
107:24 - components that are deployed in the
107:26 - namespace are the system processes
107:28 - they're from Master managing processes
107:32 - or cube CTL Etc the next one is Cube
107:36 - public and what Q public contains is
107:38 - basically the publicly accessible data
107:40 - it has a config map that contains
107:44 - cluster information which is accessible
107:46 - even without authentication so if I type
107:50 - here cubectl cluster info this is the
107:52 - output that I get through that
107:54 - information and the third one is
107:57 - cubenode lease which is actually a
108:00 - recent addition to kubernetes and the
108:02 - purpose of that namespace is that it
108:04 - holds information about the heartbeats
108:06 - of nodes so each node basically gets its
108:09 - own object that contains the information
108:12 - about that nodes availability and the
108:15 - fourth namespace is the default
108:17 - namespace and default namespace is the
108:20 - one that you're going to be using to
108:21 - create the resources at the beginning if
108:24 - you haven't created a new namespace but
108:27 - of course you can add and create new
108:29 - namespaces and the way that you can do
108:31 - it is using cubectl Create namespace
108:34 - command with the name of the namespace
108:37 - so I can create my namespace and if I do
108:41 - Cube CTL get namespaces I see that in my
108:45 - list now another way to create
108:47 - namespaces is
108:49 - using a namespace configuration file
108:52 - which I think is a better way to create
108:54 - namespaces because you also have a
108:56 - history in your configuration file
108:59 - repository of what resources you created
109:02 - in a cluster okay so now we saw what
109:04 - namespaces are and that you can create
109:08 - new ones and that kubernetes offers some
109:10 - of them by default but the question is
109:12 - what is the need for namespaces when
109:16 - should you create them and how you
109:17 - should use them and the first use case
109:20 - of using or creating your own namespaces
109:23 - is the following imagine you have only
109:26 - default namespace which is provided by
109:28 - kubernetes and you create all your
109:30 - resources in that default namespace if
109:33 - you have a complex application that has
109:35 - multiple deployments which create
109:37 - replicas of many parts and you have
109:40 - resources like services and config Maps
109:44 - Etc very soon your default namespace is
109:46 - going to be filled with different
109:48 - components and it will be really
109:50 - difficult to have an overview of what's
109:53 - in there especially we have multiple
109:55 - users creating stuff inside so a better
109:57 - way to use namespaces in this case is to
110:02 - group resources into namespaces so for
110:05 - example you can have a database
110:06 - namespace where you deploy your database
110:09 - and all its required resources and you
110:13 - can have a monitoring namespace is where
110:15 - you deploy the promatoes and all the
110:18 - stuff that it needs you can also have
110:20 - elastic stack namespace where all the
110:23 - elasticsearch kibana ETC resources go
110:26 - and you can have nginx Ingress resources
110:29 - so just one way of logically grouping
110:32 - your resources inside of the cluster now
110:35 - according to the official documentation
110:37 - of kubernetes you shouldn't use
110:40 - namespaces if you have smaller projects
110:43 - and up to 10 users
110:46 - I personally think that it's always good
110:49 - idea to group your resources in
110:52 - namespaces because as I said even if you
110:55 - have a small project and 10 users you
110:59 - might still need some additional
111:01 - resources for your application like you
111:03 - know logging system and monitoring
111:05 - system and even with the minimum setup
111:08 - you can already get too much to just
111:11 - throw everything in a default namespace
111:13 - another use case where you will need to
111:15 - use namespaces if you have multiple
111:17 - teams so imagine this scenario you have
111:20 - two teams that use the same cluster and
111:24 - one team deploys an application which is
111:27 - called my app deployment that's the name
111:29 - of the deployment they create and that
111:31 - deployment has its certain configuration
111:34 - now if another team had a deployment
111:37 - that accidentally had the same name but
111:41 - a different configuration and they
111:44 - created the deployment or they applied
111:46 - it they would overwrite the first team's
111:49 - deployment and if they're using for
111:52 - example a Jenkins or some automated way
111:55 - to deploy those that application or to
111:58 - create the deployment they wouldn't even
112:00 - know that they overwrote or disrupted in
112:03 - other team's deployment So to avoid such
112:06 - kind of conflicts again you can use the
112:08 - namespaces so that each team can work in
112:12 - their own namespace without disrupting
112:14 - the other another use case for using
112:17 - namespaces is let's say you have one
112:19 - cluster and you want to host both
112:22 - staging and develop development
112:24 - environment in the same cluster and the
112:26 - reason for that is that for example if
112:28 - you're using something like nginx
112:30 - controller or elastic stack used for
112:34 - logging for example you can deploy it in
112:36 - one cluster and use it for both
112:38 - environments in that way you don't have
112:40 - to deploy this common resources twice in
112:44 - two different clusters so now the
112:46 - staging can use both resources as well
112:49 - as the development environment another
112:51 - use case for using namespaces is when
112:54 - you use blue green deployment for
112:56 - application which means that in the same
112:58 - cluster you want to have two different
113:01 - versions of production so the one that
113:04 - is active that is in production now and
113:07 - another one that is going to be the next
113:09 - production version the versions of the
113:11 - applications in those blue and green
113:13 - production namespaces will be different
113:15 - however the same as we saw before in
113:18 - staging and development this namespaces
113:21 - might need to use the same resource
113:23 - courses like again nginx controller or
113:27 - elastic stack and this way again they
113:30 - can both use this common shared
113:32 - resources without having to set up a
113:35 - separate cluster so one more use case
113:37 - for using namespaces is to limit the
113:41 - resources and access to namespaces when
113:45 - you're working with multiple teams so
113:47 - again we have a scenario where we have
113:49 - two teams working on the same cluster
113:50 - and each one of them has their own
113:53 - namespace so what you can do in this
113:55 - scenario is that you can give the teams
113:57 - access to only their namespace so they
114:00 - can only be able to create updates
114:03 - delete resources in their own namespace
114:06 - but they can't do anything in the other
114:08 - namespaces in this way you even restrict
114:11 - or even minimize the risk of one team
114:14 - accidentally interfering with another
114:17 - team's work so each one has their own
114:19 - secured isolated environment additional
114:22 - thing that you can do on the namespace
114:24 - level is limit the resources that each
114:27 - namespace consumes because if you have a
114:29 - cluster with limited resources you want
114:32 - to give each team a share of resources
114:35 - for their application so if one team
114:38 - let's say consumes too much resources
114:40 - then other teams will eventually have
114:43 - much less and their applications may not
114:45 - schedule because the cluster will run
114:47 - out of the resources so what you can do
114:49 - is that per namespace you can Define
114:51 - resource quotas that limit how much CPU
114:55 - RAM storage resources one namespace can
114:58 - use so I hope walking through these
115:00 - scenarios helped you analyze in which
115:03 - use cases and how you should use
115:05 - namespaces in your specific project
115:07 - there are several characteristics that
115:09 - you should consider before deciding how
115:11 - to group and how to use namespaces the
115:14 - first one is that you can't access most
115:16 - of the resources from another namespace
115:19 - so for example if you have a
115:21 - configuration map in Project a namespace
115:24 - that references the database service you
115:27 - can't use that config map in Project B
115:30 - namespace but instead you will have to
115:33 - create the same config map that also
115:35 - references the database service so each
115:38 - namespace will Define or must Define its
115:42 - own config map even if it's the same
115:44 - reference and the same applies to secret
115:47 - so for example if you have credentials
115:49 - of a shared service you will have to
115:51 - create that secret in each namespace
115:53 - where you are going to need that however
115:56 - a resource that you can share across
115:59 - namespaces is service and that's what we
116:02 - saw in the previous slide so config map
116:04 - in Project B namespace references
116:07 - service that is going to be used
116:09 - eventually in a pod and the way it works
116:12 - is that in a config map definition the
116:14 - database URL in addition to its name
116:16 - which is MySQL service will have
116:19 - namespace at the end so using that URL
116:22 - you can actually access services from
116:25 - other namespaces which is a very
116:28 - practical thing and this is how you can
116:30 - actually use shared resources like
116:33 - elasticsearch or nginx from other
116:36 - namespaces and one more characteristic
116:39 - is that we saw that most of the
116:41 - components resources can be created
116:43 - within a namespace but there are some
116:46 - components in kubernetes they're not
116:48 - namespaced so to say so basically they
116:52 - leave just globally in the cluster and
116:55 - you can't isolate them or put them in a
116:58 - certain namespace and examples of such
117:00 - resources are volume or persistent
117:02 - volume and node so basically when you
117:05 - create the volume it's going to be
117:07 - accessible throughout the whole cluster
117:09 - because it's not in a namespace and you
117:12 - can actually list components that are
117:15 - not bound to a namespace using a command
117:17 - cubectl API resources dash dash
117:20 - namespaced false and the same way you
117:23 - can can also list all the resources that
117:25 - are bound to a namespace using namespace
117:28 - true so now that you've learned what the
117:31 - namespaces are why to use them in which
117:34 - cases it makes sense to use them in
117:36 - which way and also some characteristics
117:39 - that you should consider let's actually
117:41 - see how to create components in a
117:43 - namespace in the last example we've
117:46 - created components using configuration
117:48 - files and nowhere there we have defined
117:51 - a namespace so what happens is by
117:53 - default if you don't provide a namespace
117:55 - to a component it creates them in a
117:58 - default namespace so if I apply this
118:00 - config map component and let's do that
118:03 - actually right now so Cube CD apply
118:05 - minus F config map if I apply that and I
118:09 - do Cube CTL get config map my config map
118:11 - was created in a default namespace
118:14 - and notice that even in the cube CTL get
118:17 - config map command I didn't use a
118:19 - namespace because Cube CTL get or cube
118:23 - CTL commands they take the default
118:25 - namespace as a default so Coupe CTL get
118:29 - config map is actually same as cube CTL
118:32 - get config map dash n or namespace and
118:35 - default namespace so these are the same
118:37 - commands it's just a shortcut because it
118:40 - takes default as a default namespace
118:42 - okay so one way that I can create this
118:44 - config map in a specific namespace is
118:47 - using cubectle apply command but adding
118:50 - flag namespace
118:53 - and the namespace name
118:55 - so this will create config map in my
118:58 - namespace and this is one way to do it
119:01 - another way is inside the configuration
119:03 - file itself so I can adjust this config
119:07 - map configuration file to include the
119:09 - information about the destination
119:11 - namespace itself so in the metadata I
119:14 - can add namespace attribute so if I
119:17 - apply this configuration file again
119:19 - using cubectle apply and now if I want
119:22 - to get the component that I created in
119:24 - this specific namespace then I have to
119:26 - add the option or the flag to cubectl
119:30 - get command because as I said by default
119:33 - it will check only in the default
119:35 - namespace
119:36 - so I recommend using the namespace
119:39 - attribute in a configuration file
119:41 - instead of providing it to the cube CTL
119:44 - command because one it's it's better
119:47 - documented so you know by just looking
119:50 - at the configuration file where the
119:52 - component is getting created because
119:54 - that could be an important information
119:56 - and second if you're using automated
119:59 - deployment where you're just applying
120:02 - the configuration files then again this
120:04 - will be a more convenient way to do it
120:05 - now if for example we take a scenario
120:09 - where one team gets their own namespace
120:11 - and that has to uh work entirely in the
120:14 - namespace it could be pretty annoying to
120:16 - have to add this namespace tag to every
120:19 - cubectl command so in order to make it
120:21 - more convenient there is a way to change
120:24 - this default or active namespace which
120:27 - is default namespace to whatever
120:29 - namespace you choose and kubernetes or
120:33 - cubesatel doesn't have any out of the
120:35 - box solution for that but there's a tool
120:37 - called Cube NS or Cubans and you have to
120:41 - install the tool so on Mac
120:44 - so I'm gonna execute Brew install
120:49 - X so this will install
120:52 - Cubans tool as well
120:54 - so once I have the Cubans installed I
120:56 - can just execute Cuban's command and
121:00 - this will give me a list of all the
121:02 - namespaces and highlight the one that is
121:04 - active which is default right now and if
121:07 - I want to change the active namespace I
121:10 - can do Cube ends
121:14 - space
121:18 - and this will switch the active
121:20 - namespace so if I do Cube ends now I see
121:23 - that active one is my namespace so now I
121:27 - can execute Cube CTL commands without
121:29 - providing my namespace namespace
121:32 - but obviously if you switch a lot
121:34 - between the namespaces this will not be
121:37 - so much convenient
121:39 - for your own operating system and
121:42 - environment there will be different
121:45 - installation process so I'm going to
121:47 - link the cube CTX installation guide in
121:50 - the description below
121:54 - so in this video we're going to talk
121:56 - about what Ingress is and how you should
122:00 - use it and also what are different use
122:02 - cases for Ingress
122:04 - so first of all let's imagine a simple
122:06 - kubernetes cluster where we have a pod
122:09 - of my application and it's corresponding
122:12 - service my app service so the first
122:15 - thing you need for a UI application is
122:17 - to be accessible through browser right
122:19 - so for external requests to be able to
122:22 - reach your application so one way to do
122:26 - that an easy way is through an external
122:28 - service where basically you can access
122:30 - the application using HTTP protocol the
122:34 - IP address of the node and the port
122:37 - however this is good for test cases and
122:41 - if you want to try something very fast
122:43 - but this is not what the final product
122:46 - should look like the final product
122:48 - should be like this so you have a domain
122:50 - name for application and you want a
122:53 - secure connection using https so the way
122:56 - to do that is using kubernetes component
122:58 - called Ingress so you'll have my app
123:01 - Ingress and instead of external service
123:04 - you would instead have an internal
123:07 - service so you would not open your
123:09 - application through the IP address and
123:12 - the port and now if the request comes
123:14 - from the browser it's going to first
123:16 - reach the Ingress and Ingress then will
123:18 - redirect it to the internal service and
123:21 - then it will eventually end up with the
123:23 - Pod so now let's actually take a look
123:25 - and see how external service
123:27 - configuration looks like so that you
123:29 - have a practical understanding so you
123:32 - have the service which is of type load
123:34 - balancer this means we are opening it to
123:37 - public by assigning an external IP
123:39 - address to the service and this is the
123:42 - port number that user can access the
123:45 - application at so basically the IP
123:48 - address the external IP address and the
123:50 - port number that you specify here now
123:53 - with Ingress of course it looks
123:56 - differently so let's go through the
123:58 - syntax of Ingress basically you have a
124:00 - kind Ingress instead of a service and in
124:03 - the specification where the whole
124:05 - configuration happens you have so-called
124:07 - rules or routing rules and this
124:10 - basically defines that the main address
124:13 - or all the requests to that host must be
124:17 - forwarded to an internal service so this
124:21 - is the host that user will enter in the
124:24 - browser and in Ingress users Define a
124:26 - mapping so what happens when that
124:28 - requests to that host gets issued you
124:31 - redirect it internally to a service the
124:34 - path here basically means the URL path
124:36 - so everything after the domain name so
124:40 - slash whatever path comes after that you
124:43 - can define those rules here and we'll
124:45 - see some different examples of the path
124:48 - configuration later and as you see here
124:50 - in this configuration we have a http CP
124:53 - protocol so later in this video I'm
124:57 - gonna show you how to configure https
124:59 - connection using Ingress component so
125:02 - right now in the specification we don't
125:04 - have anything configured for the secure
125:07 - connection it's just adhdp and one thing
125:09 - to note here is that this HTTP attribute
125:12 - here does not correspond to this one
125:15 - here this is a protocol that the
125:18 - incoming request gets forwarded to to
125:21 - the internal service so this is actually
125:23 - the second step and not to confuse it
125:26 - with this one
125:27 - and now let's see how the internal
125:30 - service to that Ingress will look like
125:32 - so basically backhand is the target
125:36 - where the request the incoming request
125:38 - will be redirected and the service name
125:40 - should correspond the internal service
125:42 - name like this and the port should be
125:45 - the internal service port and as you see
125:48 - here the only difference between the
125:50 - external and internal Services is that
125:52 - here in internal service I don't have
125:55 - the third ports which is the note Port
125:57 - starting from 30 000. we now have that
125:59 - attribute here and the type is a default
126:03 - type not a load balancer but internal
126:05 - service type which is cluster IP so this
126:08 - should be a valid domain address so you
126:11 - can just write anything here it has to
126:13 - be first of all valid and you should map
126:16 - that domain name to IP address of the
126:20 - node that represents an entry point to
126:23 - your kubernetes cluster so for example
126:25 - if you decide that one of the nodes
126:27 - inside the kubernetes cluster is going
126:29 - to be the entry point then you should
126:31 - map this to the AP address of that node
126:33 - or and we will see that later if you
126:36 - configure a server outside of the
126:38 - kubernetes cluster that will become the
126:40 - entry point to your kubernetes cluster
126:43 - then you should map this hostname to the
126:46 - IP address of that server so now that we
126:49 - saw what kubernetes Ingress components
126:52 - looks like let's see how to actually
126:53 - configure Ingress in the cluster so
126:57 - remember this diagram I showed you at
126:59 - the beginning so basically you have a
127:01 - pod service and corresponding Ingress
127:04 - now if you create that Ingress component
127:07 - alone that won't be enough for Ingress
127:11 - routing rules to work what you need in
127:14 - addition is an implementation for
127:17 - Ingress and that implementation is
127:19 - called Ingress controller so the step
127:21 - one will be to install an Ingress
127:23 - controllers which is basically another
127:26 - pod or another set of parts that run on
127:30 - your node in your kubernetes cluster and
127:32 - thus evaluation and processing of
127:35 - Ingress rules so the yaml file that I
127:38 - showed you with the Ingress component is
127:41 - basically this part right here and this
127:44 - has to be additionally installed in
127:47 - kubernetes cluster so what is ingress
127:49 - controller exactly the function of
127:52 - Ingress controller is to evaluate all
127:55 - the rules that you have defined in your
127:57 - cluster and this way to manage all the
127:59 - redirections so basically this will be
128:02 - the entry point in the cluster for all
128:05 - the requests to that domain or subdomain
128:08 - rules that you've configured and this
128:11 - will evaluate all the rules because you
128:13 - may have 50 rules or 50 Ingress
128:16 - components created in your cluster it
128:19 - will evaluate all the rules and decide
128:20 - based on that which forwarding rule
128:23 - applies for that specific request so in
128:26 - order to install this
128:28 - implementation of Ingress in your
128:30 - cluster you have to decide which one of
128:33 - many different third-party
128:34 - implementations you want to choose from
128:37 - I'll put a link of the whole list in the
128:39 - description where you see different
128:41 - kinds of Ingress controllers you can
128:43 - choose from there is one from kubernetes
128:45 - itself which is kubernetes nginx ingress
128:48 - controller but there are others as well
128:50 - so once you install Ingress controller
128:53 - in your cluster you're good to go create
128:55 - Ingress rules and the whole
128:58 - configuration is going to work so now
129:00 - that I've shown you how Ingress can be
129:02 - used in a kubernetes cluster there is
129:04 - one thing that I think is important to
129:07 - understand in terms of setting up the
129:09 - whole cluster to be able to receive
129:11 - external requests now first of all you
129:15 - have to consider the environment where
129:17 - you kubernetes cluster is running if you
129:20 - are using some cloud service provider
129:22 - like Amazon web services Google Cloud
129:25 - Leno there are a couple more that have
129:28 - out of the books kubernetes Solutions or
129:32 - they have their own virtualized load
129:34 - balances Etc your cluster configuration
129:37 - would look something like this so you
129:39 - would have a cloud load balancer that is
129:42 - specifically implemented by that cloud
129:44 - provider
129:46 - and external requests coming from a
129:48 - browser will first hit the load balancer
129:51 - and that load balancer then will
129:54 - redirect the request to Ingress
129:57 - controller now this is not the only way
129:59 - to do it even in Cloud environment you
130:01 - can do it in in a couple of different
130:03 - ways but this is one of the most common
130:05 - strategies and advantage of using cloud
130:10 - provider for that is that you don't have
130:12 - to implement a load balancer yourself so
130:16 - with minimal effort probably on most
130:19 - Cloud providers you will have the load
130:21 - balancer up and running and ready to
130:24 - receive those requests and for those
130:26 - requests then to your kubernetes cluster
130:28 - so very easy setup now if you're
130:32 - um deploying your kubernetes cluster on
130:34 - a bare metal environment then you would
130:36 - have to do that part yourself so
130:39 - basically you would have to configure
130:40 - some kind of entry point to your
130:42 - kubernetes cluster yourself and there's
130:44 - a whole list of different ways to do
130:46 - that and I'm gonna put that also in the
130:48 - description
130:49 - but generally speaking either inside of
130:52 - a cluster or outside is a separate
130:55 - server you will have to provide an entry
130:58 - point in one of those types is an
131:02 - external proxy server which can be a
131:05 - software or Hardware solution that will
131:09 - take a role of that load balancer in an
131:11 - entry point to your cluster so basically
131:14 - what this would mean is that you will
131:17 - have a separate server and you would
131:19 - give this a public IP address and you
131:22 - would open the ports in order for the
131:24 - requests to be accepted and this proxy
131:28 - server then will act as an entry point
131:31 - to your cluster so this will be the only
131:33 - one accessible externally so none of the
131:36 - servers in your kubernetes cluster will
131:38 - have publicly accessible IP address
131:40 - which is obviously a very good security
131:43 - practice so all the requests will enter
131:46 - the proxy server and that will then
131:48 - redirect the request to Ingress
131:50 - controller and Ingress controller will
131:53 - then decide which Ingress rule applies
131:56 - to that specific request and the whole
131:59 - internal request forwarding will happen
132:01 - so as I said there are different ways to
132:04 - configure that and to set it up
132:07 - depending on which environment you are
132:08 - and also which approach you choose but I
132:11 - think it's a very important concept to
132:13 - understand how the whole cluster setup
132:15 - works so in my case since I'm using a
132:18 - mini Cube to demonstrate all this on my
132:21 - laptop the setup will be pretty easy and
132:24 - even though this might not apply exactly
132:26 - to your cluster setting still you will
132:28 - see in practice how all these things
132:30 - work so the first thing is to install
132:32 - Ingress controller in minicube and the
132:35 - way to do that is by executing minicube
132:39 - add-ons enable Ingress
132:42 - so what this does is automatically
132:44 - configures or automatically starts the
132:48 - kubernetes nginx implementation of
132:50 - Ingress controller so that's one of the
132:53 - many third-party implementations which
132:55 - you can also safely use in production
132:57 - environments not just mini Cube but this
133:00 - is what a mini Cube actually offers you
133:02 - out of the box so with one simple
133:04 - command Ingress controller will be
133:07 - configured in your cluster and if you do
133:10 - Cube CTL get pod in a cube system
133:12 - namespace you will see the nginx Ingress
133:15 - controller pod running in your cluster
133:18 - so once I have Ingress controller
133:19 - installed now I can create an Ingress
133:22 - rule that the controller can evaluate so
133:25 - let's actually head over to the command
133:27 - line where I'm going to create Ingress
133:28 - rule for kubernetes dashboard component
133:31 - so in my mini Cube cluster I have
133:35 - kubernetes dashboard which is right now
133:38 - not accessible externally so what I'm
133:41 - going to do is since I already have
133:42 - internal service for kubernetes
133:44 - dashboard and a pod for that I'm going
133:48 - to configure an Ingress rule for the
133:50 - dashboard so I can access it from a
133:53 - browser using some domain name
133:57 - so I'm gonna
134:00 - so this shows me all the components that
134:03 - I have in kubernetes dashboard and since
134:05 - I already have internal service for
134:08 - kubernetes dashboard and the pod that's
134:10 - running I can now create an Ingress rule
134:14 - in order to access the kubernetes
134:16 - dashboard using some host name so let's
134:19 - go ahead and do that
134:21 - so I'm going to create an Ingress for
134:24 - kubernetes dashboard so these are just
134:26 - metadata the name is going to be
134:29 - dashboard Ingress and the namespace is
134:31 - going to be in the same namespace as the
134:33 - service and pod so in the specification
134:36 - we are going to define the rules
134:39 - so the first rule is the host name
134:42 - I'm just gonna call
134:45 - I'm going to Define dashboard.com
134:50 - and the HTTP forwarding to internal
134:53 - service
134:55 - path let's leave it at all path
134:59 - and this is the back end of the service
135:01 - so service name will be what we saw here
135:06 - so this is the service name
135:09 - and service port
135:13 - is where the service listens so this is
135:17 - actually 80 right here
135:19 - and this will be it that's the Ingress
135:21 - configuration for
135:23 - forwarding every request that is
135:27 - directed to dashboard.com to internal
135:30 - kubernetes dashboard service and we know
135:33 - it's internal because its type is
135:34 - cluster IP so no external IP address
135:38 - so obviously I just made up
135:41 - hostnamedashboard.com it's not
135:43 - registered anywhere and I also didn't
135:46 - configure anywhere which IP address this
135:50 - host name should resolve to and this is
135:52 - something that you will always have to
135:54 - configure so first of all let's actually
135:57 - create that Ingress rule so Cube CTL
136:00 - apply
136:03 - multiple dashboard Ingress
136:06 - yaml see Ingress was created so if I do
136:10 - get
136:11 - Chris
136:13 - in the namespace
136:18 - I should see my Ingress here and as you
136:21 - see address is now empty because it
136:23 - takes a little bit of time to assign the
136:25 - address to Ingress so we'll have to wait
136:29 - for that to get the IP address that will
136:33 - map to this host so I'm just gonna
136:37 - watch this and it's I see that address
136:40 - was assigned so what I'm going to do now
136:42 - is that I'm going to take that address
136:45 - and in my
136:50 - hosts file
136:55 - at the end I'm gonna Define that mapping
136:57 - so that IP address
137:00 - will be mapped to the dashboard
137:03 - .com and again this works locally if I'm
137:05 - gonna type dashboard.com in the browser
137:08 - this will be the IP address that it's
137:10 - going to be mapped to
137:13 - which basically means that the request
137:15 - will come in to my mini Cube cluster
137:18 - will be handed over to Ingress
137:20 - controller and Ingress controller then
137:22 - we'll go and evaluate this rule that
137:24 - I've defined here and forward that
137:27 - request to service so this is all the
137:30 - configuration we need so now I'm gonna
137:32 - go and
137:34 - and enter dashboard.com
137:38 - and I will see my kubernetes dashboard
137:41 - here
137:42 - so Ingress also has something called a
137:45 - default backend so if I do Cube CTL
137:48 - describe Ingress
137:53 - the name of the Ingress and the
137:55 - namespace
138:00 - I'll get this output and here
138:04 - there's an attribute called default
138:06 - backend that maps to default HTTP
138:09 - backend Port 80. so what this means is
138:13 - that whenever a request comes into the
138:15 - kubernetes cluster that is not mapped to
138:18 - any backend so there is no rule for
138:21 - mapping that request uh to a service
138:24 - then this default backend is used to
138:28 - handle that request so obviously if you
138:31 - don't have this service created or
138:34 - defined in your cluster kubernetes will
138:37 - try to forward it to the service it
138:39 - won't find it and you would get some
138:42 - default error response so for example if
138:46 - I entered some path that I have
138:49 - configured I just get page not found so
138:53 - a good usage for that is to Define
138:55 - custom error messages when a page isn't
138:58 - found when a request comes in that you
139:01 - can handle or the application can handle
139:03 - so that users still see some meaningful
139:06 - error message or just a custom page
139:08 - where you can redirect them to your home
139:10 - page or something like this so all you
139:13 - have to do is create an internal service
139:15 - with the same name so default it should
139:18 - be backend and the port number
139:21 - and also create a pod or application
139:24 - that sends that air custom error message
139:28 - response so till now I have shown you
139:31 - what Ingress is and how you can use it
139:33 - I've also shown you a demo of how to
139:35 - create an Ingress rule in minicube but
139:38 - we've used only a very basic Ingress
139:42 - yaml configuration just a simple
139:44 - forwarding to One internal service with
139:48 - one path but you can do much more with
139:51 - Ingress configuration than just basic
139:55 - forwarding and in the next section we're
139:58 - gonna go through more use cases of how
140:01 - you can define a more fine granular
140:04 - routing for applications inside
140:06 - kubernetes cluster so the first thing is
140:10 - defining multiple path of the same host
140:14 - so consider following use case Google
140:17 - has one domain but has many services
140:20 - that it offers so for example if you
140:23 - have a Google account you can use its
140:25 - analytics you can use it shopping you
140:27 - have a calendar you have a Gmail Etc so
140:30 - all of these are separate applications
140:33 - that are accessible with the same domain
140:36 - so consider you have an application that
140:39 - does something similar so you offer two
140:42 - separate applications that are part of
140:45 - the same ecosystem but you still want to
140:47 - have them on separate URLs so what you
140:50 - can do is that in rules you can Define
140:52 - The Host
140:53 - which is myapp.com and in the path
140:57 - section you can Define multiple path so
140:59 - if user wants to access your analytics
141:02 - application then they have to enter my
141:05 - app.com analytics and that will forward
141:08 - the request to internal and analytics
141:11 - service in the Pod or if they want to
141:14 - access the shopping application then the
141:18 - URL for that would be myapp.com shopping
141:20 - so this way you can do forwarding with
141:24 - one Ingress of the same host to multiple
141:27 - applications using multiple path another
141:30 - use case is when instead of using URLs
141:35 - to make different applications
141:37 - accessible some companies use
141:40 - sub-domains so instead of having my
141:43 - app.com analytics they create a
141:46 - subdomain
141:48 - analytics.myapp.com so if you have your
141:51 - application configured that way your
141:53 - figuration will look like this so
141:55 - instead of having one host like in the
141:57 - previous example and multiple path here
142:00 - inside now you have multiple hosts where
142:03 - each host represents a subdomain and
142:06 - inside you just have one path that again
142:08 - redirects that request to analytic
142:11 - service pretty straightforward so now in
142:14 - the same request setting you have
142:15 - analytics service and a pod behind it
142:19 - now the request will look like this
142:22 - using the subdomain instead of path and
142:25 - one final topic that I mentioned that
142:27 - we'll cover here is configuring TLS
142:30 - certificate till now we've only seen
142:32 - Ingress configuration for HTTP requests
142:35 - but it's super easy to configure https
142:39 - forwarding in Ingress so the only thing
142:42 - that you need to do is define attribute
142:45 - called TLS above the rules section with
142:48 - host which is the same host as right
142:50 - here and the secret name which is a
142:54 - reference of a secret that you have to
142:56 - create in a cluster that holds that TLS
142:59 - certificate so the secret configuration
143:01 - would look like this so the name is the
143:06 - reference right here and the data or the
143:09 - actual contents contain TLS certificate
143:12 - and TLS key
143:14 - if you've seen my other videos where I
143:16 - create different components like secret
143:18 - you probably notice the type additional
143:21 - type attribute here in kubernetes there
143:24 - is a specific type of a secret called
143:27 - TLS so we'll have to use that type when
143:31 - you create a TLS secret and there are
143:33 - three small nodes to be made here one is
143:36 - that the keys of this data have to be
143:40 - named exactly like that the values are
143:43 - the actual file contents of the
143:46 - certificate or key contents and not the
143:48 - file path or location so you have to put
143:51 - the whole content here base64 encode it
143:54 - and the third one is that you have to
143:56 - create the secret in the same namespace
143:58 - as the Ingress component for it to be
144:02 - able to use that otherwise you can't
144:04 - reference a secret formula the namespace
144:07 - and these four lines is all you need to
144:10 - configure mapping of an https request to
144:14 - that host to internal service
144:21 - in this video I'm going to explain all
144:24 - the main concepts of Helm so that you're
144:26 - able to use it in your own projects also
144:29 - Helm changes a lot from version to
144:31 - version so understanding the basic
144:33 - common principles and more importantly
144:35 - its use cases to when and why we use
144:39 - Helm will make it easier for you to use
144:41 - it in practice no matter which version
144:43 - you choose so the topics I'm gonna go
144:46 - through in this video are Helm and Helm
144:48 - charts what they are how to use them and
144:51 - in which scenarios they are used and
144:54 - also what is tiller and what part it
144:56 - plays in the helm architecture
144:58 - so what is Helm Helm has a couple of
145:01 - main features that it's used for the
145:04 - first one is as a package manager for
145:07 - kubernetes so you can think of it as apt
145:10 - or yam or Homebrew for kubernetes
145:13 - so it's a convenient way for packaging
145:16 - collections of kubernetes yaml files and
145:19 - distributing them in public and private
145:21 - registry now these definitions may sound
145:24 - a bit abstract so let's break them down
145:26 - with specific examples
145:29 - so let's say you have deployed your
145:31 - application in kubernetes cluster
145:34 - and you want to deploy elasticsearch
145:36 - additionally in your cluster that your
145:38 - application will use to collect its logs
145:43 - in order to deploy elastic stack in your
145:46 - kubernetes cluster
145:49 - you would need a couple of kubernetes
145:51 - components so you would need a stateful
145:53 - set which is for stateful applications
145:55 - like databases you will need a config
145:58 - map with external configuration you
146:00 - would need a secret where some
146:02 - credentials and secret data are stored
146:05 - you will need to create the kubernetes
146:06 - user with its respective permissions and
146:10 - also create couple of services now if
146:14 - you were to create all of these files
146:16 - manually by searching for each one of
146:20 - them separately on internet will be a
146:22 - tedious job and until you have all these
146:25 - yaml files collected and tested and
146:27 - tried out it might take some time and
146:29 - since elastic stack deployment is pretty
146:32 - much the standard across all clusters
146:35 - other people will probably have to go
146:37 - through the same so it made perfect
146:39 - sense that someone created this yaml
146:42 - files once and packaged them up and made
146:46 - it available somewhere so that other
146:48 - people people who also use the same kind
146:51 - of deployment could use them in their
146:53 - kubernetes cluster and that bundle of
146:56 - yemo files is called Helm chart
146:59 - so using Helm you can create your own
147:01 - Helm charts or bundles of those yaml
147:05 - files and push them to some Helm
147:09 - repository to make it available for
147:11 - others or you can consume so you can use
147:14 - download and use existing Helm charts
147:17 - that other people pushed and made
147:19 - available in different repositories so
147:22 - commonly used deployments like database
147:24 - applications elasticsearch mongodb MySQL
147:28 - or monitoring applications like
147:30 - Prometheus that all have this kind of
147:33 - complex setup
147:35 - all have charts available in some Helm
147:39 - Repository
147:40 - so using a simple Helm install chart
147:43 - name command you can reuse the
147:45 - configuration that someone else has
147:47 - already made without additional effort
147:49 - and sometimes that someone is even the
147:52 - company that created the application and
147:54 - this functionality of sharing charts
147:57 - that became pretty widely used actually
148:00 - was one of the contributors to why Helm
148:03 - became so popular compared to its
148:05 - alternative tools so now if you're if
148:08 - you have a cluster and you need some
148:11 - kind of deployment that you think should
148:13 - be available out there you can actually
148:15 - look it up either using command line so
148:18 - you can do Helm search with a keyword or
148:20 - you can go to either Helms on public
148:24 - repository helmhub or on Helm charts
148:27 - Pages or other repositories that are
148:30 - available and I will put all the
148:33 - relevant links for this video in the
148:35 - description so you can check them out
148:36 - now apart from those public Registries
148:39 - for
148:40 - Helm charts there are also private
148:42 - Registries because when companies start
148:45 - creating those charts they also started
148:47 - Distributing them among or internally in
148:50 - the organization so it made perfect
148:52 - sense to create Registries to share
148:55 - those charts within the organization and
148:58 - not publicly so there are a couple of
149:00 - tools out there they're used as Helm
149:04 - charts private repositories as well
149:06 - another functionality of Helm is that
149:10 - it's a templating engine so what does
149:12 - that actually mean imagine you have an
149:15 - application that is made up of multiple
149:18 - micro services and you're deploying all
149:21 - of them in your kubernetes cluster and
149:24 - deployment and service of each of those
149:26 - microservices are pretty much the same
149:29 - with the only difference that the
149:31 - application name and version are
149:33 - different or the docker image name and
149:36 - version tags are different so without
149:39 - Helm you would write separate yaml files
149:43 - configuration files for each of those
149:44 - micro services so you would have
149:46 - multiple deployment service files where
149:50 - each one has its own
149:52 - application name and version defined but
149:55 - since the only difference between those
149:58 - yaml files are just couple of lines or a
150:01 - couple of values using Helm what you can
150:04 - do is that you can define a common
150:06 - blueprint for all the micro services and
150:09 - the values that are dynamic or the
150:12 - values that are going to change
150:14 - replace by placeholders and that would
150:18 - be a template file so the template file
150:21 - would look something like this you would
150:22 - have a template file which is standard
150:24 - EML but instead of values in some places
150:27 - you would have the syntax which means
150:29 - that you're taking a value from external
150:33 - configuration and that external
150:35 - configuration if you see the syntax here
150:37 - dot values that external configuration
150:40 - comes from an additional yemo file which
150:43 - is called
150:44 - values.yaml and here you can Define all
150:48 - those values that you're gonna use in
150:51 - that template file so for example here
150:53 - those four values are defined in an
150:56 - values yml file and what dot values is
151:00 - it's an object that is being created
151:03 - based on the values that are supplied
151:05 - via value CML file and also through
151:10 - command line using dash dash set flake
151:13 - so whichever way you define those
151:16 - additional values that are combined and
151:18 - put together in dot values object that
151:21 - you can then use in those template files
151:23 - to get the values out so now instead of
151:26 - having yaml files for each microservice
151:28 - you just have one and you can simply
151:31 - replace those values dynamically and
151:35 - this is especially practical when you're
151:37 - using continuous delivery continuous
151:39 - integration for application because what
151:42 - you can do is that in your build
151:44 - pipeline you can use those template DML
151:47 - files and replace the values on the Fly
151:51 - before deploying them another use case
151:54 - where you can use the helm features of
151:57 - package manager and templating engine is
152:00 - when you deploy the same set of
152:03 - applications across different kubernetes
152:05 - clusters so consider a use case where
152:08 - you have your microservice application
152:11 - that you want to deploy on development
152:14 - staging and production clusters so
152:17 - instead of deploying the individual DML
152:20 - files separately in each cluster you can
152:23 - package them up to make your own
152:25 - application chart that will have all the
152:28 - necessary yaml files that that
152:31 - particular deployment needs and then you
152:34 - can use them to redeploy the same
152:38 - application in different kubernetes
152:40 - cluster environments using one command
152:42 - which can also make the whole deployment
152:45 - process easier
152:46 - so now that you know what Helm charts
152:49 - are used for it let's actually look at
152:51 - an example Helm chart structure to have
152:54 - a better understanding so typically
152:57 - chart is made up of such a directory
152:59 - structure so it would have the top level
153:02 - will be the name of the chart and inside
153:04 - the directory you would have following
153:06 - so chart.yaml is basically a file that
153:10 - contains all the meta information about
153:12 - the chart it could be name and version
153:14 - maybe list of dependencies Etc
153:18 - that I mentioned before is place where
153:21 - all the values are con configured for
153:24 - the template files and this will
153:27 - actually be the default values that you
153:30 - can override later
153:32 - the charts directory will have chart
153:34 - dependencies inside meaning that if this
153:38 - chart depends on other charts then those
153:41 - chart dependencies will be stored here
153:43 - and templates folder is basically with a
153:47 - template files are stored so when you
153:50 - execute Helm install command to actually
153:53 - deploy those yaml files into kubernetes
153:56 - the template files from here will be
154:00 - filled with the values from
154:02 - values.yaml producing valid kubernetes
154:05 - manifest that can then be deployed into
154:08 - kubernetes
154:09 - and optionally you can have some other
154:11 - files in this folder like readme or
154:15 - license file
154:17 - Etc
154:18 - so to have a better understanding of how
154:20 - values are injected into Helm templates
154:23 - consider that in values.yaml which is a
154:26 - default value configuration you have
154:28 - following three values image name port
154:31 - and version and as I mentioned the
154:34 - default values that are defined here can
154:37 - be overwritten in a couple of different
154:39 - ways one way is that when executing Helm
154:43 - install command you can provide an
154:46 - alternative evalues yaml file using
154:49 - valuesflake so for example if values
154:54 - yemo file will have following three
154:56 - values which are image name port and
154:58 - version you can Define your own values
155:00 - yaml file called
155:03 - myvalues.yaml and you can override one
155:06 - of those values or you can even add some
155:08 - new attributes there and those two will
155:12 - be merged which will result into a DOT
155:15 - values object that will look like this
155:17 - so it have image name and Port from
155:19 - value.yaml and the one that you
155:22 - overwrote with your own values file
155:25 - alternatively you can also provide
155:28 - additional individual values using set
155:32 - flag where you can Define the values
155:35 - directly on the command line but of
155:37 - course it's more organized and better
155:39 - manageable to have files where you store
155:42 - all those values instead of just
155:43 - providing them on the command line
155:45 - another feature of Helm is release
155:48 - management which is provided based on
155:50 - its setup but it's important to note
155:53 - here the difference between Helm
155:55 - versions 2 and 3. in version 2 of Helm
156:00 - the helm installation comes in two parts
156:02 - you have Helm client and the server and
156:05 - the server part is called tiller so
156:08 - whenever you deploy Helm chart using
156:11 - Helm install my chart help client will
156:14 - send the yaml files to Tiller that
156:17 - actually runs or has to run in a
156:19 - kubernetes cluster and Taylor then will
156:22 - execute these request and create
156:24 - components from these yemo files inside
156:27 - the kubernetes cluster and exactly this
156:30 - architecture offers additional valuable
156:33 - feature of Helm which is release
156:35 - management so the way Helm clients
156:38 - server setup works is that whenever you
156:41 - create or change deployment peeler will
156:44 - store a copy of each configuration
156:47 - client send for future reference thus
156:50 - creating a history of chart executions
156:54 - so when you execute Helm upgrade to
156:57 - chart name
156:59 - the changes will be applied to the
157:01 - existing deployment instead of removing
157:03 - it and creating a new one and also in
157:06 - case the upgrades goes wrong for example
157:09 - some yaml files were false or some
157:12 - configuration was wrong you can roll
157:14 - back that upgrade using Helm rollback
157:17 - chart name command
157:19 - and all this is possible because of that
157:22 - chart execution history that healer
157:25 - keeps whenever you send those requests
157:27 - from Helm client to tiller however this
157:31 - setup has a big caveat which is that
157:33 - tiller has too much power inside the
157:36 - kubernetes cluster it can create update
157:39 - delete components and it has too much
157:42 - permissions
157:43 - and this makes it actually a big
157:46 - security issue and this was one of the
157:48 - reasons why in Helm 3 they actually
157:50 - removed the tiller part and it's just a
157:53 - simple Helm binary now and it's
157:56 - important to mention here because a lot
157:59 - of people have heard of tiller and when
158:01 - you deploy a Helm version 3 you
158:04 - shouldn't be confused that tiller isn't
158:06 - actually there anymore
158:10 - in this video I will show you how you
158:13 - can persist data in kubernetes using
158:15 - volumes we will cover three components
158:18 - of kubernetes storage persistent volume
158:21 - persistent volume claim and storage
158:23 - class and see what each component does
158:26 - and how it's created and used for data
158:29 - persistence
158:30 - consider a case where you have a mySQL
158:33 - database part which your application
158:35 - uses data gets added updated in the
158:38 - database maybe you create a new database
158:41 - with a new user Etc but default when you
158:45 - restart the Pod all those changes will
158:48 - be gone because kubernetes doesn't give
158:50 - you data persistence out of the box
158:53 - that's something that you have to
158:55 - explicitly configure for each
158:57 - application that needs saving data
158:59 - between pod restarts
159:01 - so basically you need a storage that
159:05 - doesn't depend on the Pod life cycle so
159:08 - it will still be there when Paul dies
159:10 - and new one gets created so the new part
159:12 - can pick up where the previous one left
159:15 - off
159:15 - so it will read the existing data from
159:18 - that storage to get up-to-date data
159:21 - however you don't know on which node the
159:24 - new pod restarts so your storage must
159:27 - also be available on all nodes not just
159:30 - one specific one so that when the new
159:33 - pod tries to read the existing data the
159:35 - up-to-date data is there on any node in
159:39 - the cluster
159:41 - and also you need a highly available
159:44 - storage that will survive even if the
159:47 - whole cluster crashed
159:49 - so these are the criteria or the
159:51 - requirements that your storage for
159:54 - example your database storage will need
159:56 - to have to be reliable another use case
160:00 - for persistent storage which is not for
160:02 - database is a directory maybe you have
160:04 - an application that writes and reads
160:06 - files from pre-configured directory this
160:10 - could be session files for application
160:12 - or configuration files Etc and you can
160:15 - configure any of this type of storage
160:18 - using kubernetes component called
160:20 - persistent volume think of a persistent
160:24 - volume as a cluster resource just like
160:26 - Ram or CPU that is used to store data
160:31 - persistent volume just like any other
160:33 - component gets created using kubernetes
160:35 - yemo file where you can specify the kind
160:39 - which is persistent volume and in the
160:42 - specification section you have to Define
160:44 - different parameters like how much
160:47 - storage should be created for the volume
160:49 - but since persistent volume is just an
160:53 - abstract component it must take the
160:56 - storage from the actual physical storage
160:58 - right like local hard drive
161:01 - from the cluster nodes or your external
161:04 - NFS servers outside of the cluster or
161:07 - maybe cloud storage like AWS block
161:10 - storage or from Google cloud storage Etc
161:13 - so the question is where does this
161:15 - storage backend come from local or
161:18 - remotes or on cloud Who configures it
161:21 - who makes it available to the cluster
161:23 - and that's the tricky part of data
161:25 - persistence in kubernetes because
161:28 - kubernetes doesn't care about your
161:30 - actual storage it gives you persistent
161:33 - volume component as an interface to the
161:36 - actual storage that you as a maintainer
161:39 - or administrator have to take care of so
161:42 - you have to decide what type of storage
161:45 - your cluster services or applications
161:48 - would need and create and manage them by
161:51 - yourself managing meaning do backups and
161:55 - make sure they don't get corrupt Etc so
161:58 - think of storage in kubernetes as an
162:00 - external plugin to your cluster whether
162:04 - it's a local storage on actual nodes
162:06 - where the cluster is running or a remote
162:09 - storage doesn't matter they're all
162:11 - plugins to the cluster and you can have
162:13 - multiple storages configured for your
162:16 - cluster where one application in your
162:18 - cluster uses local disk storage another
162:22 - one uses the NFS server and another one
162:25 - uses some cloud storage or one
162:28 - application may also use multiple of
162:31 - those storage types
162:33 - and by creating persistent volumes you
162:36 - can use this actual physical storages so
162:39 - in the persistent volume specification
162:41 - section you can Define which storage
162:45 - backend you want to use to create that
162:49 - storage abstraction or storage resource
162:51 - for your applications so this is an
162:54 - example where we use NFS storage backend
162:57 - so basically we Define how much storage
163:00 - we need some additional parameters so
163:03 - that storage like should it be read
163:05 - write or read only Etc and the storage
163:08 - backend with its parameters
163:11 - and this is another example where we use
163:15 - Google Cloud as a storage backend again
163:18 - with the storage backend specified here
163:21 - and capacity and access modes here now
163:24 - obviously depending on the storage type
163:26 - on the storage backend some of the
163:29 - attributes in the specification will be
163:31 - different because they're specific to
163:34 - the storage type this is another example
163:36 - of a local storage which is on the Node
163:39 - itself which has additional node
163:42 - Affinity attribute now you don't have to
163:44 - remember and know all these attributes
163:46 - at once because you may may not need all
163:49 - of them and also I will make separate
163:52 - videos covering some of the most used
163:54 - volumes and explain them individually
163:57 - with examples and demos so there I'm
164:00 - going to explain in more detail which
164:02 - attributes should be used for these
164:05 - specific volumes and what they actually
164:07 - mean in the official kubernetes
164:08 - documentation you can actually see the
164:10 - complete list of more than 25 storage
164:13 - backends that kubernetes supports note
164:16 - here that persistent volumes are not
164:20 - namespaced meaning they're accessible to
164:23 - the whole cluster and unlike other
164:26 - components that we saw like pods and
164:28 - services they're not in any namespace
164:31 - they're just available to the whole
164:33 - cluster to all the namespaces
164:35 - now it's important to differentiate here
164:37 - between two categories of the volumes
164:39 - local and remote each volume type in
164:43 - these two categories has its own use
164:45 - case otherwise they won't exist and we
164:49 - will see some of these use cases later
164:51 - in this video however the local volume
164:54 - types violate the second and third
164:56 - requirements of data persistence for
164:59 - databases that I mentioned at the
165:02 - beginning which is one not being tied to
165:06 - one specific node but rather to each
165:09 - node equally because you don't know
165:11 - where the new pod will start and the
165:14 - second surviving in cluster crash
165:17 - scenarios because of these reasons for
165:20 - database persistence you should almost
165:23 - always use remote storage
165:25 - so who creates these persistent volumes
165:28 - and when as I said persistent volumes
165:31 - are resources like CPU or Ram so they
165:34 - have to be already there in a cluster
165:37 - when the Pod that depends on it or that
165:39 - uses it is created so a side note here
165:43 - is that there are two main roles in
165:45 - kubernetes there's an administrator who
165:48 - sets up the cluster and maintains it and
165:51 - also makes sure the cluster has enough
165:53 - resources
165:55 - these are usually system administrators
165:57 - or devops engineers in a company
166:00 - and the second role is kubernetes user
166:03 - that deploys the applications in the
166:05 - cluster either directly or through CI
166:07 - pipeline these are Developer devops
166:10 - teams who create the applications and
166:13 - deploy them so in this case the
166:15 - kubernetes administrator would be the
166:17 - one to configure the actual storage
166:20 - meaning to make sure that the NFS server
166:24 - storage is there and configured or maybe
166:27 - create and configure a cloud storage
166:30 - that will be available for the cluster
166:32 - and second create persistent volume
166:35 - components from these storage backends
166:38 - based on the information from developer
166:40 - team of what types of storage their
166:43 - applications would need and the
166:45 - developers then will know that storage
166:48 - is there and can be used by their
166:49 - applications but for that developers
166:52 - have to explicitly configure the
166:55 - application yaml file to use those
166:58 - persistent volume components in other
167:01 - words application has to claim that
167:04 - volume storage and you do that using
167:06 - another component of kubernetes called
167:08 - persistent volume claim persistent
167:12 - volume claims also PVCs are also created
167:15 - with yaml configuration here's an
167:18 - example claim again don't worry about
167:20 - are understanding each and every
167:22 - attribute that is defined here but on
167:25 - the higher level the way it works is
167:27 - that PVC claims a volume with certain
167:31 - storage size or capacity which is
167:33 - defined in the persistent volume claim
167:35 - and some additional characteristics like
167:38 - access type should be read only or read
167:41 - write or the type
167:43 - Etc and whatever persistent volume
167:46 - matches this criteria or in other words
167:49 - satisfies this claim will be used for
167:53 - the application but that's not all you
167:55 - have to now use that claim in your pods
167:58 - configuration like this so in the Pod
168:01 - specification here you have the volumes
168:04 - attribute that references the persistent
168:09 - volume claim with its name so now the
168:13 - Pod and all the containers inside the
168:16 - Pod will have access to that persistent
168:19 - volume storage so to go through those
168:22 - levels of abstraction step by step and
168:25 - parts access storage by using the claim
168:28 - as a volume right so they request the
168:31 - volume through claim the claim then will
168:34 - go and try to find a volume persistent
168:37 - volume in the cluster that satisfies the
168:39 - claim
168:40 - and the volume will have a storage the
168:43 - actual storage backend that it will
168:47 - create that storage resource from in
168:50 - this way the Pod will now be able to use
168:52 - that actual storage packet note here
168:56 - that claims must exist in the same
168:59 - namespace as the Pod using the claim
169:02 - while as I mentioned before persistent
169:04 - volumes are not namespaced
169:06 - so once the Pod finds the matching
169:09 - persistent volume through the volume
169:11 - claim through the persistent volume
169:13 - claim the volume is then mounted into
169:16 - the Pod like this here this is a pod
169:19 - level
169:20 - and then that volume can be mounted into
169:23 - the Container inside the pod which is
169:26 - this level right here and if you have
169:29 - multiple containers here in the Pod you
169:31 - can decide to mount this volume in all
169:35 - the containers or just some of those so
169:38 - now the container
169:40 - and the application inside the container
169:42 - can read and write to that storage and
169:44 - when the Pod dies a new one gets created
169:47 - it will have access to the same storage
169:49 - and see all the changes the previous pod
169:52 - or the previous containers made again
169:55 - the attributes here like volumes and
169:57 - volume mounts Etc and how they're used I
169:59 - will show you more specifically and
170:02 - explain in a later demo video now you
170:05 - may be wondering why so many
170:07 - abstractions for using volume where
170:10 - admin role has to create persistent
170:12 - volume and reuse the role creates a
170:14 - claim on that persistent volume and that
170:16 - is in use in pod can I just use one
170:19 - component and configure everything there
170:21 - well this actually has a benefit because
170:24 - as a user meaning a developer who just
170:26 - wants to deploy their application in the
170:28 - cluster you don't care about where the
170:32 - actual storage is you know you want your
170:34 - database to have persistence and whether
170:37 - the data will leave on a gluster FS or
170:40 - AWS EBS or local storage doesn't matter
170:44 - for you as long as the data is safely
170:47 - stored or if you need a directory
170:49 - storage for files you don't care where
170:52 - the directory actually leaves as long as
170:54 - it has enough space and works properly
170:56 - and you sure don't want to care about
170:59 - setting up these actual storages
171:01 - yourself you just want 50 gigabyte
171:04 - storage for your elastic or 10 gigabyte
171:07 - for your application that's it so you
171:10 - make a claim for storage using PVC and
171:12 - assume that cluster has storage
171:15 - resources already there and this makes
171:19 - deploying the applications easier for
171:22 - developers because they don't have to
171:24 - take care of the stuff Beyond deploying
171:27 - the applications
171:28 - now there are two volume types that I
171:31 - think needs to be mentioned separately
171:32 - because they're a bit different from the
171:34 - rest and these are config map and secret
171:37 - now if you have watched my other video
171:40 - on kubernetes components then you are
171:43 - already familiar with both both of them
171:45 - are local volumes but unlike the rest
171:48 - these two aren't created via PV and PVC
171:52 - but a rather own components and managed
171:55 - by kubernetes itself
171:57 - consider a case where you need a
171:58 - configuration file for your Prometheus
172:01 - pod or maybe a message broker service
172:03 - like mosquito or consider when you need
172:06 - a certificate file mounted inside your
172:09 - application in both cases you need a
172:12 - file available to your pod so how this
172:16 - works is that you create config map or
172:18 - secret component and you can mount that
172:21 - into your pod and into your container
172:23 - the same way as you would Mount
172:25 - persistent volume claim
172:27 - so instead you would have a config map
172:29 - or secret here and I will show you a
172:32 - demo of this in a video where I cover
172:35 - local volume types so to quickly
172:38 - summarize what we've covered so far as
172:41 - we see at its core and volume is just a
172:43 - directory possibly with some data in it
172:46 - which is accessible to the containers in
172:48 - a pod how that directory is made
172:51 - available or what storage medium
172:53 - actually packs that and the contents of
172:56 - that directory are defined by specific
173:00 - volume type you use
173:02 - so to use a volume A Part specifies what
173:05 - volumes to provide for the pod in the
173:09 - specification volumes attribute and
173:12 - inside the part then you can decide
173:14 - where to mount that storage into using
173:18 - volume mounts attribute inside the
173:21 - container section
173:22 - and this is a path inside the container
173:25 - where application can access whatever
173:29 - storage we mounted into the container
173:32 - and as I said if you have multiple
173:34 - containers you can decide which
173:37 - container should get access to that
173:39 - storage
173:40 - interesting note for you is that a pod
173:43 - can actually use multiple volumes of
173:46 - different types simultaneously let's say
173:48 - you have an elasticsearch application or
173:52 - pod running in your cluster that needs a
173:55 - configuration file mounted through a
173:58 - config map needs a certificate let's say
174:01 - client certificate mounted as a secret
174:04 - and it needs database storage let's say
174:07 - which is backed with AWS
174:10 - elastic block storage so in this case
174:13 - you can configure all three inside your
174:17 - pod or deployment
174:19 - so this is the Pod specification that we
174:21 - saw before
174:22 - and here on the volumes level you will
174:25 - just list all the volumes that you want
174:28 - to mount into your pod
174:31 - so let's say you have a persistent
174:33 - volume claim that and the background
174:35 - claims persistent volume from AWS block
174:38 - storage and here you have the config map
174:40 - and here have a secret and here in the
174:43 - volume mounts you can list all those
174:46 - storage mounts using the names right so
174:50 - you have the persistent storage then you
174:53 - have the config map and the secret and
174:55 - each one of them is mounted to a certain
174:57 - path inside the container
175:00 - now we saw that to persist data in
175:02 - kubernetes admins need to configure
175:05 - storage for the cluster create
175:07 - persistent volumes and developers then
175:09 - can claim them using PVCs but consider a
175:12 - cluster with hundreds of applications
175:14 - where things get deployed daily and
175:17 - storage is needed for these applications
175:19 - so developers need to ask admins to
175:21 - create persistent volumes they need for
175:24 - applications before deploying them and
175:27 - admins then may have to manually request
175:29 - storage from cloud or storage provider
175:31 - and create hundreds of persistent
175:34 - volumes for all the applications that
175:37 - need storage manually and that can be
175:40 - tedious time consuming and can get messy
175:44 - very quickly so to make this process
175:46 - more efficient there is a third
175:49 - component of kubernetes persistence
175:51 - called storage class storage class
175:54 - basically creates or Provisions
175:57 - persistent volumes dynamically whenever
176:00 - PVC claims it and this way creating or
176:04 - provisioning volumes in a cluster may be
176:06 - automated
176:07 - storage class also gets created using
176:10 - yaml configuration file so this is an
176:13 - example file where you have the kind
176:15 - storage class
176:17 - storage class creates persistent volumes
176:20 - dynamically in the background so
176:22 - remember we Define storage backend in
176:25 - the persistent volume component now we
176:27 - have to Define it in the storage class
176:29 - component and we do that using the
176:32 - provisioner attribute
176:34 - which is the main part of the storage
176:36 - class configuration because it tells
176:38 - kubernetes which provisioner to be used
176:41 - for a specific storage platform or cloud
176:45 - provider to create the persistent volume
176:48 - component out of it so each storage
176:51 - backend has its own provisioner that
176:53 - kubernetes offers internally which are
176:55 - prefixed with kubernetes DOT IO like
176:58 - this one here
176:59 - and these are internal provisioners and
177:03 - for others or other storage types their
177:06 - external provisioners that you have to
177:09 - then explicitly go and find and use that
177:12 - in your storage class
177:14 - and in addition to provisioner attribute
177:17 - we configure parameters of the storage
177:20 - we want to request for our persistent
177:22 - volume like this one's here so storage
177:24 - class is basically another abstraction
177:27 - level that abstracts the underlying
177:29 - storage provider as well as parameters
177:32 - for that storage or characteristics for
177:34 - this storage like what Disk type or
177:38 - Etc so how does it work or how to use
177:41 - storage class in the Pod configuration
177:44 - same as persistent volume it is
177:46 - requested or claimed by PVC so in the
177:50 - PVC configuration here we add additional
177:54 - attribute that is called storage class
177:56 - name
177:57 - that references the storage class to be
178:01 - used to create a persistent volume that
178:04 - satisfies the claims
178:07 - of this PVC so now when a pod claims
178:11 - storage through PVC the PVC will request
178:15 - that storage from Storage class which
178:17 - then will provision or create persistent
178:21 - volume that meets the needs of that
178:24 - claim using provisioner from the actual
178:27 - storage backend now this should help you
178:30 - understand the concepts of how data is
178:33 - persisted in kubernetes as a high level
178:35 - overview
178:39 - in this video we're going to talk about
178:41 - what stateful set is in kubernetes and
178:44 - what purpose it has so what is stateful
178:47 - set
178:48 - the kubernetes component that is used
178:51 - specifically for stateful applications
178:53 - so in order to understand that first you
178:55 - need to understand what a stateful
178:57 - application is
178:58 - examples of stateful applications are
179:01 - all databases like MySQL elasticsearch
179:04 - mongodb Etc or any application that
179:08 - stores data to keep track of its state
179:10 - in other words these are applications
179:13 - that track state by saving that
179:15 - information in some storage stateless
179:18 - applications on the other hand do not
179:21 - keep records of previous interaction in
179:24 - each request or interaction is handled
179:26 - as a completely new isolated interaction
179:30 - based entirely on the information that
179:32 - comes with it and sometimes stateless
179:35 - applications connect to the stateful
179:37 - application to forward those requests
179:40 - so imagine a simple setup of a node.js
179:43 - application that is connected to mongodb
179:46 - database when a request comes in to the
179:49 - node.js application it doesn't depend on
179:51 - any previous data to handle this
179:53 - incoming request it can handle it based
179:56 - on the payload in the request itself
180:00 - now a typical such request will
180:03 - additionally need to update some data in
180:05 - the database or query the data that's
180:09 - where mongodb comes in so when node.js
180:12 - for words that request mongodb mongodb
180:15 - will update the data based on its
180:18 - previous state or query the data from
180:21 - its storage so for each request it needs
180:23 - to handle data and obviously always
180:25 - depends on the most up-to-date data or
180:28 - state to be available while node.js is
180:31 - just a pass-through for data updates or
180:33 - queries and it just processes code
180:37 - now because of this difference between
180:39 - stateful and stateless applications
180:41 - they're both deployed in different ways
180:44 - using different components in kubernetes
180:48 - stateless applications are deployed
180:51 - using deployment component where
180:52 - deployment is an abstraction of parts
180:55 - and allows you to replicate that
180:57 - application meaning Run 2 5 10 identical
181:01 - parts of the same stateless application
181:03 - in the cluster
181:05 - so while stateless applications are
181:07 - deployed using deployment stateful
181:10 - applications in kubernetes are deployed
181:13 - using stateful set components
181:16 - and just like deployment stateful said
181:18 - makes it possible to replicate the
181:20 - stateful app parts or to run multiple
181:23 - replicas of it in other words they both
181:26 - manage parts that are based on an
181:29 - identical container specification
181:32 - and you can also configure storage with
181:35 - both of them equally in the same way so
181:39 - if both manage the replication of PODS
181:42 - and also configuration of data
181:45 - persistence in the same way the question
181:48 - is what a lot of people ask and are also
181:50 - often confused about what is the
181:53 - difference between those two components
181:54 - why we use different ones for each type
181:57 - of application so in the next section
182:00 - we're going to talk about the
182:01 - differences Now replicating stateful
182:03 - application is more difficult and has a
182:06 - couple of requirements that stateless
182:09 - applications do not have so let's look
182:11 - at this first with the example of a
182:14 - mySQL database let's say you have one
182:16 - mySQL database part that handles
182:19 - requests from a Java application which
182:22 - is deployed using a deployment component
182:24 - and let's say you scale the Java
182:26 - application to three parts so they can
182:29 - handle more client requests in parallel
182:32 - below you want to scale MySQL app so we
182:35 - can handle more Java requests as well
182:37 - scaling your Java application here is
182:40 - pretty straightforward Java applications
182:42 - replica pods will be identical and
182:46 - interchangeable so you can scale it
182:49 - using a deployment pretty easily
182:51 - deployment will create the pods in any
182:53 - order in any random order they will get
182:56 - random hashes at the end of the Pod name
182:58 - they will get one service that load
183:01 - balances to any one of the replica pods
183:03 - for any request and also when you delete
183:06 - them they get deleted in a random order
183:08 - or at the same time right or when you
183:11 - scale them down from three to two
183:13 - replicas for example one random replica
183:16 - part gets chosen to be deleted so no
183:19 - complications there on the other hand
183:22 - MySQL pod replicas cannot be created and
183:25 - deleted at the same time in any order
183:28 - and they can't be randomly addressed and
183:31 - the reason for that is because the
183:33 - replica parts are not identical in fact
183:37 - they each have their own additional
183:39 - Identity On Top of the common blueprint
183:42 - of the part that they get created from
183:46 - and giving each part its own
183:49 - required individual identity is actually
183:52 - what stateful set does different from
183:55 - deployment it maintains a sticky
183:58 - identity for each of its parts and as I
184:00 - said these parts are created from the
184:02 - same specification but they're not
184:04 - interchangeable each has a persistent
184:07 - identifier that it maintains across any
184:10 - rescheduling so meaning when pot dies
184:13 - and it gets replaced by a new part it
184:16 - keeps that identity so the question you
184:19 - may be asking now is why do these parts
184:22 - need their own identities why they can't
184:24 - be interchangeable just like with
184:26 - deployment so why is that and this is a
184:29 - concept that you need to understand
184:30 - about scaling database applications in
184:33 - general
184:34 - when you start with a single MySQL pod
184:37 - it will be used for both reading and
184:39 - writing data
184:41 - but when you add a second one it cannot
184:44 - act the same way because if you allow
184:46 - two independent instances of MySQL to
184:49 - change the same data you will end up
184:51 - with data in consistency so instead
184:54 - there is a mechanism that decides that
184:57 - only one pole is allowed to write or
184:59 - change the data which is shared reading
185:02 - at the same time by multiple Parts MySQL
185:06 - instances from the same data is
185:08 - completely fine and the part that is
185:10 - allowed to update the data is called the
185:13 - master the others are called slaves
185:16 - so this is the first thing that
185:18 - differentiates these parts from each
185:20 - other so not all ports are same
185:22 - identical but there is a must pod and
185:24 - they're the slave pods right and there's
185:27 - also difference between those slave
185:29 - Parts in terms of storage which is the
185:31 - next point so the thing is that these
185:34 - parts do not have access to the same
185:36 - physical storage even though they use
185:39 - the same data they're not using the same
185:42 - physical storage of the data they each
185:44 - have their own replicas of the storage
185:47 - that each one of them can access for
185:50 - itself and this means that each pod
185:52 - replica at any time must have the same
185:55 - data as the other ones and in order to
185:57 - achieve that they have to continuously
185:59 - synchronize their data and since Master
186:02 - is the only one allowed to change data
186:05 - and the slaves need to take care of
186:06 - their own data storage obviously the
186:10 - slaves must know about each such change
186:13 - so they can update their own data
186:15 - storage to be up to date for the next
186:18 - query requests and there is a mechanism
186:21 - in such clustered database setup that
186:24 - allows for continuous data
186:26 - synchronization
186:28 - Master changes data and all slaves
186:30 - update their own data storage to keep in
186:33 - sync and to make sure that each pod has
186:36 - the same state now let's say you have
186:38 - one master and two slave parts of my SQL
186:42 - now what happens when a new pod replica
186:44 - joins the existing setup because now
186:47 - that new part also needs to create its
186:50 - own storage and then take care of
186:53 - synchronizing it what happens is that it
186:55 - first clones the data from the previous
186:57 - part not just any part in the in the
187:00 - setup but always from the previous part
187:02 - and once it has the up-to-date data
187:05 - cloned it starts continuous
187:07 - synchronization as well to listen for
187:09 - any updates by Master pod and this also
187:13 - means and I want to point this out since
187:14 - it's pretty interesting point it means
187:17 - that you can actually have a temporary
187:19 - storage for a stateful application and
187:21 - not persist the data at all since the
187:23 - data gets replicated between the pods so
187:26 - theoretically it is possible to just
187:29 - rely on data replication between the
187:31 - pods but this will also mean that the
187:34 - whole data will be lost when all the
187:37 - parts die so for example if stateful set
187:40 - gets deleted or the cluster crashes or
187:43 - all the nodes where these pod replicas
187:45 - are running crash and every part dies at
187:48 - the same time the data will be gone and
187:50 - therefore it's still a best practice to
187:52 - use data persistence for stateful
187:55 - applications if losing the data will be
187:57 - unacceptable which is the case in most
188:00 - database applications and with
188:02 - persistent storage data Will Survive
188:04 - even if all the parts of the stateful
188:07 - set die or even if you delete the
188:10 - complete stateful set component and all
188:12 - the parts get wiped out as well the
188:15 - persistent storage and the data will
188:18 - still remain because persistent volume
188:21 - lifecycle isn't connected or isn't tied
188:24 - to a life cycle of other components like
188:28 - deployment or stateful set and the way
188:32 - to do this is configuring persistent
188:35 - volumes for your stateful set and since
188:37 - each pod has its own data storage
188:40 - meaning it's their own persistent volume
188:43 - that is then backed up by its own
188:45 - physical storage which includes the
188:47 - synchronized data or the replicated
188:49 - database data but also the state of the
188:53 - Pod so each pod has its own state which
188:56 - has information about whether it's a
188:59 - master pod or a slave or other
189:01 - individual characteristics and all of
189:04 - these gets stored in the pot's own
189:07 - storage and that means when a pot dies
189:10 - and gets replaced the persistent pod
189:13 - identifiers make sure that the storage
189:16 - volume gets reattached to the
189:20 - replacement part is a set because that
189:22 - storage has the state of the pod
189:25 - in addition to that replicated data I
189:27 - mean you can clone the data again there
189:30 - will be no problem but it shouldn't lose
189:32 - its state or identity state so to say
189:35 - and for this reattachment to work it's
189:39 - important to use a remote storage
189:40 - because if the Pod gets rescheduled from
189:44 - one node to another node the previous
189:46 - storage must be available on the other
189:49 - node as well and you cannot do that
189:51 - using local volume storage because they
189:53 - are usually tied to a specific node and
189:56 - the last difference between deployment
189:58 - and stateful set is something that I
190:02 - mentioned before is the Pod identifier
190:05 - meaning that every pod has its own
190:07 - identifier so unlike deployment where
190:10 - pods get random hashes at the end
190:12 - stateful set Parts get fixed ordered
190:15 - names which is made up of the stateful
190:18 - set name and ordinal it starts from zero
190:21 - and each additional part will get the
190:24 - next numeral so if we create a stateful
190:26 - set called MySQL with three replicas
190:29 - you'll have pods with names SQL zero one
190:32 - and two the first one is the master and
190:35 - then come the slaves in the order of
190:37 - startup an important note here is that
190:40 - the stateful set will not create the
190:43 - next pod in the replica If the previous
190:46 - one isn't already up and running if
190:49 - first pod creation for example failed or
190:53 - if it was pending the next one won't get
190:55 - created at all it would just wait and
190:57 - the same order is held deletion but in
191:01 - reversed order so for example if you
191:03 - deleted a stateful set or if you scaled
191:06 - it down to one for example from three
191:08 - the deletion will start from the last
191:11 - part so MySQL 2 will get deleted first
191:14 - it will wait until that pod is
191:16 - successfully deleted and then it will
191:19 - delete MySQL 1 and then it will delete
191:22 - my SQL zero and again all these
191:25 - mechanisms are in place in order to
191:28 - protect the data and the state that the
191:30 - stateful application depends on in
191:33 - addition to this fixed predictable names
191:36 - each pod in a stateful set gets its own
191:39 - DNS endpoint from a service so there's a
191:41 - service name for the saveful application
191:43 - just like for deployment for example
191:45 - that will address any replica pod and
191:50 - plus in addition to that there is
191:51 - individual DNS name for each pod which
191:55 - deployment pods do not have the
191:57 - individual DNS names are made up of pod
192:00 - name and the manage or the governing
192:03 - service name which is basically a
192:06 - service name that you define inside the
192:08 - stateful set so these two
192:11 - characteristics meaning having a
192:13 - predictable or fixed name as well as its
192:17 - fixed individual DNS name means that
192:20 - when pod restarts the IP address will
192:23 - change but the name and endpoint will
192:26 - stay the same that's why it said pods
192:28 - get sticky identities so it gets stuck
192:31 - to it even between the restarts and the
192:34 - sticky identity makes sure that each
192:36 - replica pod can retain its state and its
192:39 - role even when it dies and gets
192:42 - recreated and finally I want to mention
192:44 - an important Point here is you see
192:46 - replicating stateful apps like databases
192:49 - with its persistent storage requires a
192:52 - complex mechanism and kubernetes helps
192:54 - you and supports you to set this whole
192:57 - thing up but you still need to do a lot
192:59 - by yourself where kubernetes doesn't
193:02 - actually help you or doesn't provide you
193:04 - out-of-the-box solutions for example you
193:06 - need to configure the cloning and data
193:08 - synchronization in inside the stateful
193:10 - set and also make the remote storage
193:12 - available as well as take care of
193:15 - managing and backing it up all of this
193:18 - you have to do yourself and the reason
193:20 - is that stateful applications are not a
193:23 - perfect candidate for containerized
193:25 - environments in fact Docker kubernetes
193:28 - and generally containerization is
193:30 - perfectly fitting for stateless
193:32 - applications that do not have any state
193:35 - and data dependency and only process
193:37 - code so scaling and replicating them in
193:40 - containers is super easy
193:45 - in this video I will give you a complete
193:48 - overview of kubernetes services first
193:51 - I'll explain shortly what service
193:52 - component is in kubernetes and when we
193:55 - need it and then we'll go through the
193:58 - different service types cluster IP
194:00 - service headless service node port and
194:03 - load balancer Services I will explain
194:06 - the differences between them and when to
194:08 - use which one so by the end of the video
194:11 - you will have a great understanding of
194:13 - kubernetes services and will be able to
194:15 - use them in practice
194:17 - so let's get started
194:19 - so what is a service in kubernetes and
194:22 - why do we need it in a kubernetes
194:23 - cluster each pod gets its own internal
194:26 - IP address but the parts in kubernetes
194:30 - are ephemeral meaning that they come and
194:32 - go very frequently and when the Pod
194:35 - restarts or when old one dies and the
194:38 - new one gets started in its place it
194:40 - gets a new IP address so it doesn't make
194:43 - sense to use pod IP addresses directly
194:46 - because then you would have to adjust
194:47 - that every time the Pod gets recreated
194:50 - with the service however you have a
194:52 - solution of a stable or static IP
194:55 - address that stays even when the Pod
194:57 - dies so basically in front of each pod
195:00 - we set as service which represents a
195:03 - persistent stable IP address access that
195:06 - pod a service also provides load
195:08 - balancing because when you have pod
195:10 - replicas for example three replicas of
195:13 - your microservice application or three
195:16 - replicas of MySQL application the
195:19 - service will basically get each request
195:22 - targeted to that MySQL or your micro
195:25 - service application and then forward it
195:27 - to one of those pods so clients can call
195:31 - a single stable IP address instead of
195:34 - calling each pod individually so
195:37 - services are a good abstraction for
195:39 - loose coupling for communication within
195:41 - the cluster So within the cluster
195:44 - components or pods inside the cluster
195:46 - but also from external services like if
195:49 - you have browser requests coming to the
195:51 - cluster or if you're talking to an
195:53 - external database for example
195:55 - there are several types of services in
195:59 - kubernetes the first and the most common
196:02 - one that you probably will use most of
196:04 - the time is the cluster IP type this is
196:07 - a default type of a service meaning when
196:10 - you create a service and not specify a
196:13 - type it will automatically take cluster
196:15 - IP as a type so let's see how cluster IP
196:18 - works and where it's used in kubernetes
196:21 - setup
196:22 - imagine we have a microservice
196:25 - application deployed in the cluster so
196:27 - we have a pod with microservice
196:30 - container running inside that pod and
196:33 - beside that micro service container we
196:36 - have a sidecar container that collects
196:39 - the logs of the microservice and then
196:41 - sends that to some destination database
196:43 - so these two containers are running in
196:46 - the Pod and let's say your micro service
196:48 - container is running at pod 3000 and
196:52 - your login container let's say is
196:54 - running on Port 9000. this means that
196:58 - those two ports will be now open and
197:01 - accessible inside the pod
197:03 - and pod will also get an IP address from
197:07 - a range that is assigned to a node so
197:10 - the way it works is that if you have for
197:13 - example three worker nodes in your
197:16 - kubernetes cluster each worker node will
197:19 - get a range of IP addresses which are
197:22 - internal in the cluster so for example
197:24 - the Pod 1 will get IP addresses from a
197:27 - range of
197:29 - 10.2.1 onwards the second worker node
197:33 - will get this IP range and the third
197:36 - worker node will get this one so let's
197:38 - say this pod starts on node 2 so it get
197:40 - an IP address that looks like this if
197:44 - you want to see the IP addresses of your
197:47 - pods in the cluster you can actually
197:49 - check them using cubectl get pod output
197:53 - wide command where you will get some
197:55 - extra information about the pods
197:57 - including its IP address and here you
198:00 - will see the IP address that it got
198:01 - assigned and as I mentioned these are
198:04 - from the IP address range that each
198:07 - worker node in the cluster will get
198:09 - so this is from the first worker node
198:11 - and these are from the second worker
198:13 - node
198:14 - so now we can access those containers
198:16 - inside the Pod at this IP address at
198:19 - these ports if we set the replica count
198:22 - to 2 we're going to have another pod
198:24 - which is identical to the first one
198:26 - which will open the same ports and it
198:28 - will get a different IP address let's
198:31 - say if it starts on worker node one you
198:34 - will get an IP address that looks
198:36 - something like this now let's say this
198:38 - microservice is accessible through a
198:40 - browser so we have Ingress configured
198:43 - and the requests coming in from the
198:46 - browser to the micro service will be
198:49 - handled by Ingress how does this
198:52 - incoming request get forwarded from
198:54 - Ingress all the way to the Pod and that
198:57 - happens through a service a cluster IP
199:00 - or so-called internal service a service
199:03 - in kubernetes is a component just like a
199:06 - pod but it's not a process it's just an
199:09 - abstraction layer that basically
199:11 - represents an IP address so service will
199:13 - get an IP address that it is accessible
199:16 - at and service will also be accessible
199:19 - at a certain Port let's say we Define
199:21 - that port to be 3200 so Ingress will
199:25 - talk to the service or hand over the
199:27 - request to the service at this IP
199:30 - address at this port so this is how
199:34 - service is accessible within the cluster
199:37 - so the way it works is that we Define
199:39 - Ingress rules that forward the request
199:43 - based on the request address to certain
199:47 - services and we Define the service by
199:49 - its name and the DNS resolution then
199:54 - maps that service name to an IP address
199:56 - that the service actually got assigned
199:59 - so this is how Ingress knows
200:02 - how to talk to the service so once the
200:05 - request gets handed over to the service
200:07 - at this address then service will know
200:10 - to forward that request to one of those
200:13 - parts that are registered as the service
200:16 - endpoints now here are two questions how
200:19 - does service know which pods it is
200:22 - managing or which pods to forward the
200:24 - request to and the second one is how
200:27 - does service know which port to forward
200:29 - that request to on that specific pod the
200:34 - first one is defined by selectors a
200:37 - service identifies its member pods or
200:40 - its endpoint pods using selector
200:43 - attribute so in the service
200:45 - specification in the yaml file from
200:48 - which we create the service we specify
200:50 - the selector attribute that has a key
200:53 - value pairs defined as a list now these
200:56 - key value pairs are basically labels
200:58 - that pods should have to match that
201:01 - selector so in the Pod configuration
201:04 - file we assign the part certain labels
201:07 - in the metadata section and these labels
201:10 - can be arbitrary name so we can say my
201:12 - app for example and give it some other
201:15 - labels this is basically something that
201:17 - we Define ourselves we can give it any
201:19 - name that we want these are just key
201:21 - value pairs that identify a set of pots
201:24 - and in the service CML file then we
201:27 - Define a selector to match any part that
201:29 - has all of these labels this means if we
201:32 - have a deployment component that creates
201:34 - three replicas of paths with label
201:37 - called app my app and type microservice
201:42 - for example and in the service selector
201:45 - attribute we Define those two labels
201:47 - then service will match all of those
201:50 - three pod replicas and it will register
201:53 - all three parts as its endpoints and as
201:56 - I said it should match all the selectors
201:58 - not just one so this is how service will
202:01 - know which part
202:02 - belong to it meaning where to forward
202:05 - that request to the second question was
202:09 - if a pod has multiple ports open where
202:12 - two different applications are listening
202:14 - inside the Pod how does service know
202:16 - which port to forward the request to and
202:20 - this is defined in the Target Port
202:22 - attribute so this Target Port attribute
202:26 - so let's say Target Port in our example
202:29 - is 3000 what this means is that when we
202:32 - create the service it will find all the
202:35 - parts that match the selector
202:37 - so these pods will become endpoints of
202:40 - the service and when the service gets a
202:42 - request it will pick one of those pod
202:45 - replicas randomly because it's a load
202:47 - balancer and it will send the request it
202:50 - received to that specific pod on a port
202:55 - defined by Target Port attribute in this
202:59 - case three thousand
203:00 - also note that when you create a service
203:03 - kubernetes creates an endpoints object
203:06 - that has the same name as the service
203:09 - itself and kubernetes will use this
203:11 - endpoints object to keep track of which
203:14 - pods are members of the service or as I
203:18 - said which pods are the end points of
203:21 - the service and since this is dynamic
203:23 - because whenever you create a new pod
203:25 - replica or a pod dice the endpoints get
203:28 - updated so this object will basically
203:30 - track that
203:31 - and note here that the service port
203:34 - itself is arbitrary so you can Define it
203:36 - yourself whereas the Target Port is not
203:39 - arbitrary it has to match the port where
203:43 - container the application container
203:45 - inside the Pod is listening at
203:48 - now let's say our microservice
203:50 - application got its requests from the
203:53 - browser through Ingress and internal
203:56 - cluster IP service and now it needs to
203:59 - communicate with the database to handle
204:01 - that request for example and in our
204:04 - example let's assume that the micro
204:06 - service application uses mongodb
204:09 - database so we have two replicas of
204:12 - mongodb in the cluster which also have
204:15 - their own service endpoint so mongodb
204:18 - service is also of cluster IP and it has
204:22 - its own IP address so now the
204:25 - microservice application inside the Pod
204:27 - can talk to the mongodb database also
204:30 - using the service endpoint so the
204:33 - request will come from one of the parts
204:35 - that gets the request from the service
204:37 - to the mongodb service at this IP
204:41 - address and the port that service has
204:44 - open and then service will again
204:47 - select one of those pod replicas and
204:51 - forward that request to the selected
204:54 - part at the Port the Target Port defined
204:58 - here and this is the port where mongodb
205:01 - application inside the Pod is listening
205:04 - at now let's assume that inside that
205:07 - mongodb pod there is another container
205:09 - running that selects the monitoring
205:11 - metrics for Prometheus for example
205:14 - and that will be a mongodb exporter and
205:17 - that container let's say is running at
205:19 - Port
205:20 - 9216 and this is where the application
205:23 - is accessible at and in the cluster we
205:26 - have a Prometheus application that
205:28 - scrapes the metrics endpoint from this
205:31 - mongodb exporter container from this
205:35 - endpoint now that means that service has
205:38 - to handle two different endpoint
205:41 - requests which also means that service
205:44 - has two of its own ports open for
205:48 - handling these two different requests
205:50 - one from the clients that want to talk
205:53 - to the mongodb database and one from the
205:56 - clients like Prometheus that want to
205:58 - talk to them mongodb exporter
205:59 - application and this is an example of a
206:02 - multi-port service and note here that
206:06 - when you have multiple ports defined in
206:09 - a service you have to name those ports
206:12 - if it's just one port then you can leave
206:14 - did so to say Anonymous you don't have
206:16 - to use the name attribute it's optional
206:18 - but if you have multiple ports defined
206:21 - then you have to name each one of those
206:23 - so these were examples of cluster IP
206:26 - service type now let's see another
206:28 - service type which is called headless
206:30 - service so let's see what headless
206:33 - service type is as we saw each request
206:36 - to the service is forwarded to one of
206:39 - the Pod replicas that are registered as
206:42 - service endpoints
206:44 - but imagine if a client wants to
206:47 - communicate with one of the pods
206:49 - directly and selectively or what if the
206:53 - endpoint Parts need to communicate with
206:56 - each other directly without going
206:58 - through the service obviously in this
207:00 - case it wouldn't make sense to talk to
207:02 - the service endpoint which will randomly
207:04 - select one of the pods because we want
207:06 - the communication with specific parts
207:10 - now what would be such a use case a use
207:13 - case where this is necessary is when
207:15 - we're deploying stateful applications in
207:18 - kubernetes stateful applications like
207:20 - databases MySQL mongodb
207:23 - elasticsearch and so on in such
207:25 - applications that pod replicas aren't
207:28 - identical but rather each one has its
207:31 - individual State and characteristic for
207:34 - example if we're deploying a MySQL
207:36 - application you would have a master
207:38 - instance of MySQL and worker instances
207:41 - of my SQL application and master is the
207:45 - only part allowed to write to the
207:47 - database and the worker pods must
207:50 - connect to the master to synchronize
207:52 - their data after masterpod has made
207:55 - changes to the database so they get the
207:58 - up-to-date data as well and when new
208:00 - worker pod starts it must connect
208:03 - directly to the most recent worker node
208:06 - to clone the data from and also get up
208:09 - to date with the data state so that's
208:12 - the most common use case where you need
208:14 - direct communication with individual
208:17 - pods for such case for a client to
208:20 - connect to all pods individually it
208:23 - needs to figure out the IP address of
208:25 - each individual pod one option to
208:27 - achieve this would be to make an API
208:29 - call to kubernetes API server
208:32 - and it will return the list of PODS and
208:35 - their IP addresses but this will make
208:38 - your application too tight to the
208:41 - kubernetes specific API and also this
208:43 - will be inefficient because you will
208:45 - have to get the whole list of PODS and
208:47 - their IP addresses every time you want
208:50 - to connect to one of the pods but as an
208:52 - alternative solution kubernetes allows
208:54 - clients to discover pod IP addresses
208:57 - through DNS lookups and usually the way
209:01 - it works is that when a client performs
209:03 - a DNS lookup for a service the DNS
209:06 - server returns a single IP address which
209:08 - belongs to the service and this will be
209:11 - the services cluster IP address which we
209:13 - saw previously however if you tell
209:16 - kubernetes that you don't need a cluster
209:18 - IP address of the service by setting the
209:21 - cluster IP field To None when creating a
209:24 - service then the DNS server will return
209:27 - the Pod IP addresses instead of the
209:29 - services IP address and now the client
209:32 - can do a simple DNS lookup to get the IP
209:37 - address of the pods that are members of
209:40 - that service and then client can use
209:42 - that IP address to connect to the
209:45 - specific part it wants to talk to or all
209:47 - of the pods
209:49 - so the way we Define a headless service
209:51 - in a service configuration file is
209:54 - basically setting the cluster IP To None
209:57 - So when we create these surveys from
209:59 - this configuration file kubernetes will
210:01 - not assign the service a cluster IP
210:03 - address
210:04 - and we can see that in the output when I
210:07 - list my services so I have a cluster IP
210:10 - service that I created for the micro
210:13 - service and a headless service and note
210:16 - here that when we deploy stateful
210:18 - applications in the cluster like mongodb
210:20 - for example we have the normal service
210:24 - the cluster IP service that basically
210:27 - handles the communication to mongodb and
210:31 - maybe other container inside the Pod and
210:34 - in addition to that service we have a
210:36 - headless service so we always have these
210:38 - two Services alongside each other so
210:40 - this can do the usual load balancing
210:43 - stuff for this kind of use case and
210:47 - for use cases where client needs to
210:49 - communicate with one of those parts
210:51 - directly like a master node directly to
210:55 - perform the right commands or the pods
210:58 - to talk to each other for data
211:00 - synchronization The Headless service
211:02 - will be used for that
211:04 - when we Define a service configuration
211:07 - we can specify a type of the service and
211:11 - the type attribute can have three
211:13 - different values it could be cluster IP
211:15 - which is a default that's why we don't
211:17 - have to specify that we have a node port
211:20 - and load balancer so type node Port
211:23 - basically creates a service that is
211:25 - accessible on a static port on each
211:29 - worker node in the cluster now to
211:31 - compare that to our previous example the
211:34 - cluster IP service is only accessible
211:36 - within the cluster itself so no external
211:39 - traffic can directly address the cluster
211:43 - IP service the node Port service however
211:46 - makes the external traffic accessible on
211:50 - static or fixed port on each worker node
211:54 - so in this case instead of Ingress the
211:57 - browser request will come directly to
211:59 - the worker node at the Port that the
212:02 - service specification defines and the
212:06 - port that node Port service type exposes
212:08 - is defined in the node Port attribute
212:12 - and here note that the note Port value
212:15 - has a predefined range between 30 000
212:18 - and 32
212:20 - 767 so you can have one of the values
212:25 - from that range as a node Port value
212:28 - anything outside that range won't be
212:30 - accepted
212:31 - so this means that the node Port service
212:33 - is accessible for the external traffic
212:36 - like browser request for example add IP
212:39 - address of the worker node and the node
212:41 - Port defined here however just like in
212:44 - cluster IP we have a port of the service
212:47 - so when we create the node Port service
212:49 - a cluster IP service to which the node
212:52 - Port service will route is automatically
212:54 - created and here as you see if I list
212:57 - the services
212:59 - the note Port will have the cluster IP
213:01 - address and for each IP address it will
213:04 - also have the ports open where the
213:07 - service is accessible at and also note
213:10 - that service spends all the worker nodes
213:13 - so if you have three pod replicas on
213:17 - three different nodes basically the
213:18 - service will be able to handle that
213:20 - request coming on any of the worker
213:23 - nodes
213:24 - and then forward it to one of those pod
213:26 - replicas
213:28 - now that type of service exposure is not
213:30 - very efficient and also not secure
213:32 - because you're basically opening the
213:34 - ports to directly talk to the services
213:37 - on each worker node so the external
213:39 - clients basically have access to the
213:42 - worker nodes directly so if we gave all
213:45 - the services this node Port service type
213:49 - then we would have a bunch of ports open
213:52 - on the worker nodes clients from outside
213:54 - can directly talk to so it's not very
213:57 - efficient and secure way to handle that
213:58 - and as a better alternative there is a
214:01 - load balancer service type and the way
214:03 - it works with load balance or service
214:05 - type is that the service becomes
214:07 - accessible externally through a cloud
214:10 - provider's load balancer functionality
214:12 - so each cloud provider has its own
214:15 - native load balancer implementation and
214:17 - that is created and used whenever we
214:20 - create a load balancer service type
214:22 - Google Cloud platform AWS Azure linode
214:26 - openstack and so on all of them offer
214:29 - this functionality so whenever we create
214:31 - a load balancer service node port and
214:34 - cluster IP services are created
214:36 - automatically by kubernetes to which the
214:39 - external load balancer of the cloud
214:42 - platform will route the traffic 2 and
214:45 - this is an example of how did we Define
214:47 - load balancer service configuration
214:50 - so instead of node Port type we have a
214:52 - load balancer and the same way we have
214:55 - the port of the service which belongs to
214:58 - the cluster IP
214:59 - and we have the node Port which is the
215:03 - port that opens on the worker node but
215:05 - it's not directly accessible externally
215:07 - but only through the load balancer
215:10 - itself so the entry point becomes a load
215:13 - balancer first and it can then direct
215:15 - the traffic to node port on the worker
215:18 - node and the cluster IP the internal
215:21 - service
215:22 - so that's how the flow would work with
215:25 - the load balancer service
215:26 - so in other words the load balancer
215:28 - service type is an extension of the node
215:30 - Port type which itself is an extension
215:34 - of the cluster IP type and again if I
215:37 - create a load balancer service type and
215:40 - list all the services you can see the
215:42 - differences in the display as well where
215:45 - for each service type you see the IP
215:48 - addresses you see the type and you see
215:50 - the ports that the service has opened
215:53 - and I should mention here that in a real
215:56 - kubernetes setup example you would
215:58 - probably not use node port for external
216:01 - connection you would maybe use it to
216:03 - test some service very quickly but not
216:06 - for production use cases so for example
216:08 - if you have a application that is
216:10 - accessible through browser you will
216:12 - either configure Ingress for each such
216:15 - request so you would have internal
216:17 - Services the cluster IP services that
216:19 - Ingress will route to or you would have
216:21 - a load balancer that uses the cloud
216:23 - platform's native load balancer
216:26 - implementation
216:27 - congratulations you made it till the end
216:30 - I hope you learned a lot and got some
216:32 - valuable Knowledge from this course if
216:35 - you want to learn about modern devops
216:37 - tools be sure to check out my tutorials
216:39 - on that topic And subscribe to my
216:41 - channel for more content also if you
216:44 - want to stay connected you can follow me
216:46 - on social media or join the private
216:48 - Facebook group I would love to see you
216:51 - there so thank you for watching and see
216:53 - you in the next video