00:00 - In our previous lesson, we learnt about
Quicksort algorithm.
00:03 - Now in this lesson, we will analyze quick
sort. We will first look at some
00:07 - properties of quick sort and then we
will go ahead and
00:10 - try to analyze its time and space
complexity,
00:13 - so maybe try to see how efficient it is
00:16 - in terms of running time and space
00:20 - or memory requirement. Okay! so let's
get started.
00:24 - The properties of Quick sort that I want
to talk about
00:27 - are- First of all Quick sort just like
Merge sort
00:31 - is a divide and conquer algorithm.
Divide and Conquer is an algorithm
00:36 - design
00:37 - paradigm, in which we break a problem
into subproblems
00:41 - and then from solutions to the subproblems
problems we construct the solution to
00:45 - actual problem.
00:46 - Okay, next property of Quick sort is that
00:50 - its a recursive algorithm.
00:53 - In programming as we know recursion is a function
calling itself.
00:57 - Recursion is a natural choice in
Divide and Conquer strategy.
01:01 - A Divide and Conquer algorithm is mostly
01:04 - implemented using the recursion, moving on
01:07 - next property is that Quick sort
01:10 - is not a stable sorting algorithm.
We have discussed what
01:14 - a stable sorting algorithm is in our
previous lessons.
01:18 - In a stable sorting algorithm the
relative order of records with
01:23 - equal key is preserved. When we learn a
sorting algorithm, we sort
01:27 - a list of integers but a sorting
algorithm can be used
01:30 - to sort any list of any data type.
01:34 - In case of a complex data types, we
sort on some property of the records
01:39 - on some key.
For example let's say we
want to sort
01:43 - a list of points in Cartesian plane.
Each record in the list here
01:48 - is a pair of integers, first integer is
X-coordinate and second integer is
01:53 - Y-coordinate,
01:54 - and let's say if you want to sort in
increasing order of
01:57 - x-coordinate. We have two records here
with equal X-coordinate.
02:02 - If we will use are stable sorting algorithm
02:06 - then point (4,5) which is before point (4,3) in the original list
02:11 - will be before (4,3) in sorted
arrangement also
02:15 - but this is not guaranteed if we will
use
02:18 - an unstable sorting algorithm.
In the second arrangement
02:22 - that I'm showing here, Elements are sorted
in increasing order of X-coordinate but
02:26 - (4,3) is coming before (4,5).
02:28 - Partitioning logic in Quicksort algorithm
02:31 - does not ensure stability. Okay! Let's now
talk about time complexity of Quicksort.
02:37 - Time complexity of Quicksort is
02:41 - O(n logn) in best
02:45 - or average case and its O(n^2)
02:48 - in worst-case,
02:51 - but the worst case can almost always be
avoided
02:55 - by using what we call randomized version
of Quicksort
02:59 - let's now see how quick sort is O(n logn) in best or average case and
03:04 - O(n^2) in worst-case.
This is psedocode for a quick sort function
03:09 - that he had written earlier.
03:11 - Let's try to calculate the running
time, the first statement here
03:15 - is a this if condition its
03:18 - a simple statement the cost of executing
this statement will be some constant
03:22 - let's say the cost of executing this C1
03:25 - if we will go inside the if condition
and then the have this call to 
03:29 - partition function.
03:30 - Now this is not a simple statement we
first need to figure out what will be
03:35 - the running time of partition function.
03:37 - This is the partition function that we
had written in our previous lesson
03:41 - if you can see there are some simple
statements here that will take constant
03:45 - time,
03:46 - these four statements are simple
statements
03:49 - together they will take some constant
time apart from these statements we have
03:54 - one for-loop
03:55 - the statements inside the for loop once
again are simple statements
04:00 - and will together take some constant time,
let's say these statements inside the 
04:05 - for loop
04:06 - will take some constant time A
04:09 - and the statements 1 2 3 and
4 let's say will cost us
04:14 - B, so time taken will be
04:17 - A(N)+B. Let's say and this constant
04:21 - A is also taking care of this statement where
we are
04:25 - incrementing I for the loop
04:28 - on this particular line will actually
this particular statement
04:32 - on the for-loop will run one extra time
actually
04:37 - but we ignore such small costs while
calculating running time because
04:42 - we are interested in a calculating the rate
of growth for very high values of
04:46 - N, okay so time expression here is 
04:50 - A(N)+B where A and B are constant, so
in other words this is
04:54 - O(N).
04:55 - Remember, N here is the lenght of sub-array
that we
05:00 - are partitioning, okay coming back to
our Quicksort function.
05:04 - The partition function here is going to
cost us
05:07 - O(N). We can say that the cost for
executing this statement will be
05:12 - some constant times N plus we can add
some constant here but
05:17 - it will not matter because for very
05:21 - high values of N, the added constant term
05:24 - will be negligible compared to this
05:27 - A*N term. We have an assignment here
to this
05:31 - variable Pindex that once again will
be some constant time. So now
05:37 - let's say we are taking B'
05:38 - constant time, so overall cost of
executing this statement is
05:43 - A*N + B' and now we have
two recursive calls,
05:47 - let's see what will be the cost of these
two recursive calls.
05:51 - In partition function, we choose a
element as Pivot
05:54 - in our partition function we are always 
choosing the last element
05:58 - in the segment of array, pass to quick
sort
06:02 - or element at end index as pivot.
06:05 - I'm drawing an array of integers here and
let's say the whole array is passed to the
06:10 - Quicksort function.
06:11 - So start is 0 and end is 7.
06:15 - Four will be picked as pivot and we will
rearrange the array such that all the
06:19 - elements
06:20 - lesser than the pivot will lie towards its
left and all the elements greater than
06:25 - the pivot will lie
06:26 - towards its right and now after the
partition we can make two recursive
06:30 - calls one for the segment of array
06:32 - to the left of pivot and another
06:35 - for the segment of array to the
right of pivot.
06:39 - In this recursive approach, A balanced
partitioning
06:43 - will be best case for us. In balanced
partitioning both
06:47 - subarrays towards the left of pivot
and towards the right of pivot
06:51 - will have length equal to or almost
equal to
06:55 - N/2 where N is the number of elements
in
06:59 - original array, so if time taken for
this quick sort funtion
07:03 - is let's see T(N) then both these 
quicksort calls
07:09 - in best-case partitioning will take time
07:12 - T(N/2) each. So T(N)
07:16 - in all will be 2T(N/2) plus
07:20 - we will add A*N+B'
07:24 - and C1, remember this is the best
case scenario for us.
07:29 - When we are seeing best-case what we
mean is
07:33 - all the partitions in all that recusive
calls will be balanced,
07:37 - all the partitions will break a
segmented into 2 sub segments of
07:42 - equal length. Here in this recurrence
relation
07:48 - T(N)=T(N/2)+(A*N)+B'+C1
07:50 - these two constant terms B'
07:53 - and C1 will be negligible compared to A*N
07:56 - and for very high values of N and when
we analyze time complexity,
08:01 - we always look at running time for
very high values of N
08:05 - so they can safely ignore B' and C1
08:08 - and instead of A I'll write C for
constant because C
08:12 - looks good When I'm saying constant, so
time taken
08:16 - in the best case is equal to
08:21 - 2T(N/2)+C*N if N > 1
08:24 - for N = 1, we will not go inside
this if condition
08:29 - only cost will be execution of this ifs
statement
08:32 - so I'm saying that for N=1 my
costs is 
08:36 - C1 so T(1) is C1.
08:39 - This recurrence relation that we're getting
here
08:42 - for time is the same that we had got
for Mergesort.
08:46 - We can solve this recurrence and express
08:50 - T(N) in terms of T(1) which is known to
us
08:55 - T(N) is 2T(N/2)+C*N.
08:58 - We can write T(N/2) as
09:02 - 2T(N/4)+C*(N/2) and we will have C*N
09:06 - outside. This overall will be
09:10 - 4T(N/4) + 2C*N
09:13 - and now we can write T(N/4) in
terms of
09:17 - T(N/8) like this
09:20 - this'll be 8T(N/8) + 3C*N.
09:23 - In terms of some generic K we can
reduce
09:27 - this expression like
09:30 - 2^K*T(N/(2^K))
09:34 - and this will happen if we will go reducing
using k steps.
09:38 - In fourth step, after this step
09:42 - when K be equal to 4 this will be
09:45 - 16*T(N/16)+ 4*C*N,
09:49 - I'm saying this is the first step this
is the second step this is to third step
09:53 - and says the Kth step, This is Kth
09:56 - reduction, now we know T(1) and we want to
express T(N) in terms of
10:01 - T(1) so in that case in
10:06 - N/(2^k) = 1. If we will solve this than 
10:12 - K= log n the base to 2.
10:15 - So here T(N) can be written as
10:18 - 2^(log n the base 2) * T(1)
10:22 - + C*N * log N to 
10:26 - to the base 2,
10:29 - 2^(log n the base 2) will equal to 1
T(1) is C1
10:34 - oops! sorry and 2^(log n to the base 2) will be N
10:37 - T(1) is C1
10:41 - this will be C*N *log n to the base 2 and if
10:45 - base is understood and I can read
write that as log n
10:48 - so this is what I'm getting. This
expression
10:51 - is O(N logN)
10:55 - N*C1 one is lower order term here.
10:58 - For very high values of N it will be
negligible compared to C*N*logN.
11:03 - For high values of and the rate of
growth
11:06 - of time taken will be very close to
11:09 - C(N) logN, for some constant C so we can
say that
11:14 - time complexity will be a O(N logN).
11:17 - Actually it would be better to say
11:20 - ÆŸ(N logN). If you're not getting
any of this
11:24 - then we have whole series on time
complexity
11:27 - analysis you can find a link to it
in a description of this video.
11:33 - ÆŸ(thetha) notation is actually better metric.
Anyway,
11:37 - in best case, we are O(N logN) or
ÆŸ(N logN).
11:41 - Now let's try to analyze the worst-case.
Worst case for us will be
11:46 - when we have totally unbalanced
partitioning
11:49 - like for this array which is already
sorted
11:52 - if we will pick the last element
as Pivot. After partition, we will have
11:56 - only one segment. There will be no right
segment
12:00 - One of the recursive calls for left
subarray
12:03 - will cost us cost T(n-1)
12:07 - if we are seeing that T(N) is the cost
for array and other recursive call
12:12 - will simply return control will not go
inside the If conditions so
12:16 - there will be some constant cost for the
second recursive call
12:21 - if we will have unbalanced partitioning
in all cases
12:25 - in all recursive calls and the
recurrence relation to solve
12:28 - is this. T(N)= T(N-1) + C*N where C is
12:34 - a constant I'm ignoring other constants
that would add up
12:38 - they will be negligible compared to this
term C*N for higher values of N
12:44 - ofcourse we also have a base-case
12:46 - T(1) which will be equal to C1 this 
T(N) is for all
12:51 - N > 1. In this recurrence relation
its really simple to solve
12:56 - we can go and reducing T(N-1) can be
13:00 - written in terms of T(N-2)
13:03 - so it will be 
13:06 - T(N-2)+ C*(N-1) plus
13:09 - ofcourse we will have C*N here
13:13 - so this will be
13:16 - T(N-2) + 2C*(N)-C and we can go on
reducing,
13:21 - T(N-2) can be written as T(N-3)
13:24 - plus C*(N-2)
13:27 - plus we will have the terms
2*(C*N)-C
13:33 - so now this is 
13:37 - T(N-3) + 3(C*N)-3*C. If I'll reduce this
further this will be
13:41 - T(N-4) + 4*C*N - 6C.
13:46 - In terms of some K if we will
go reducing
13:50 - K steps then this will 
13:53 - T(N-K) plus K*C*N minus
13:57 - (1+2+3+... all the way
till
14:01 - K -1) * C.
14:04 - This part 1+2+3 till K-1
14:08 - will be K*(K-1)/2.
14:12 - This is a simple arithmetic progression,
14:16 - so this is what we're getting, now if I
want to write
14:19 - T(N) in terms 0f T(1) that a know, then
in that case
14:23 - (N-K) = 1 that
will imply K will be equal to N.
14:29 - This expression will now be
14:33 - T(1) + K is now N
14:36 - and so this will be C*(N^2) minus
14:40 - N(N-1)/2 * C.
14:43 - Some my T(N) and finally
14:46 - is T(1) is constant C1
14:49 - plus I can right rest of the part as C*N
14:53 - and I'm going to take this out inside it
will be 
14:56 - N-((N-1)/2) and
15:00 - the part will reduced to (N+1)/2
15:03 - overall this is what I'm getting. I'm
getting
15:07 - are quadratic expression of the form 
A(N^2)+B(N)+C
15:12 - where A,B and C are some constants A here is C/2
15:16 - and B here are C/2 and C is C1
15:19 - this time expression is definitely O(N^2).
15:23 - To write in terms of Big O notation we
will drop the law and order terms
15:27 - and the constant, O(N^2) is
really bad but
15:31 - as I had said earlier worst case in quicksort can
15:35 - be avoided. In the partition function
that I have written here
15:38 - I'm always choosing the last element in
the segment as pivot
15:42 - with this strategy if my array is already
sorted,
15:45 - I'm always getting unbalanced partitioning
15:49 - what we can do is we can have a strategy
in which
15:52 - we can choose pivot randomly so during
this whole
15:56 - recursive process, we will not have a rule that
only the end element should be picked up
16:00 - as pivot
16:01 - any element randomly should be picked up
as pivot.
16:04 - What I'm going to do is I'm going to
write a function named
16:07 - randomized partition that once again is
taking the Array and a start and end
16:12 - argument
16:13 - Now, in this function I'm first going to
pick up pivot index
16:16 - between start and end, start and end
included.
16:20 - By making a call to a function named
16:23 - random, this function will give me an
index between start
16:27 - and end picked up randomly almost all
the language library would have
16:32 - a random number generator function, now
16:36 - the element at this pivot index is my
pivot. I'll first
16:40 - swap this element at pivot index
with last element in the segment
16:44 - and then I can go a with rest of the
logic that I have in the partition
16:48 - function.
16:48 - So I'll simply make a call to partition
function in the left
16:52 - that i have written here. In my quicksort
function now instead of calling
16:56 - partition directly I'll call
16:58 - randomized partition, now with this
strategy
17:01 - probability of hitting worst-case is
17:05 - almost Zero.
17:07 - Coming back to quicksort function
instead of calling partition here we are going
17:11 - to call a randomized partition, I'm short
of space here
17:14 - so i'll write it like this this is
randomized
17:18 - partition function it's always wise to
called randomized partition.
17:21 - Now, let's try to analyze the average
case. Once again in our base-case
17:26 - T(1) is C1, now what will be my
17:31 - T(N)? In average case will say that we can
have
17:34 - all kind of split with equal probability.
17:37 - let's say our array is split such that
pivot lies at index i
17:41 - the time taken for to the left half in
this case will be
17:46 - T(I -1) because there will be
(I -1) elements in
17:50 - left half and there will be
17:53 - N-I elements in right half.
17:56 - This I can be any index between start and
17:59 - end with equal probability, so I'm going
to say that
18:03 - T(N) will be average of all possible
partitions
18:07 - or all possible values of I, apart
from this
18:11 - average cost to the cost of partition is
some constant times N
18:16 - so we have this recurrence relation to
solve
18:19 - I'm writing it down here we are taking
an average of all possible partitions
18:24 - here
18:24 - by summing up time taken in all possible
splits
18:29 - and then dividing by N dividing the sum
by N
18:33 - I'm not going to solve this one you can
check the description of this video for
18:37 - a link that has all the maths.
18:39 - This will also evaluate this will also
reduced to an expression that will be
18:43 - O(N log N).
18:47 - With randomized partition we can get
average case running time of quicksort
18:51 - with very high probability
18:52 - and this makes quick sort really cool
because unlike merge sort
18:56 - its space complexity is very less.
let's now discuss
19:00 - space complexity of Quicksort.
Space complexity as we know
19:04 - is the measure of rate of growth of extra
space needed
19:08 - extra space means space or memory apart
from
19:11 - the memory used to store the original
array, the original list
19:15 - so space complexities is measure a rate of
growth of extra space
19:19 - with input. I'm not going to derive this
one for a quicksort
19:22 - you can check the description of this
video for link to a lesson where we have
19:27 - described how the can derive space
complexity
19:30 - in case of recursion. The space
complexity of quicksort in average case
19:34 - is O(log N). In worst-case
19:38 - its O(N) but as we saw worst-case
can always be a avoided
19:43 - now logN is such small rate of growth
that
19:47 - that you can say that its almost
negligible extra space requirement.
19:50 - We can say that quick sort in place
sorting algorithm and in place sorting algorithm
19:55 - should take
19:56 - constant extra memory. Extra space
requirement must not grow with input but
20:01 - but log N for all practical values of N
20:04 - is very small so we discount quicksort
here. Okay I'll stop here now.
20:09 - In coming lesson, we will see some more
sorting algorithms.
20:12 - This is it for this lesson. Thanks for
watching.