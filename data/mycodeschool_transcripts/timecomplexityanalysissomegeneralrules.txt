00:00 - So, in the previous lessons we learnt how to calculate the  running time of an algorithm
00:05 - and we also learnt about asymptotic notations as a way to classify these 
00:09 - running times
00:11 - into some generic classes
00:13 - now in this lesson
00:14 - we will learn about some techniques and some rules using which
00:18 - we can avoid doing all these complex calculations
00:21 - and still derive the time complexity expression in terms of
00:26 - big-oh or theta notation
00:29 - before we start discussing these techniques
00:32 - i will make two statements
00:34 - and the statements are that we analyze time complexity
00:38 - in most cases for (a)
00:41 - very large input size
00:44 - and (b)
00:45 - worst-case scenario
00:48 - what can be a worst case scenario apart
from very large input size is something
00:52 - that we will discuss
00:54 - later in some examples
00:57 - now if we have a function defining the
running time of an algorithm like
01:00 - T(n)  equal to
01:02 - n^3 + 3n^2 + 4n + 2
01:05 - then for very large input sizes
01:08 - these lower order terms like 3n^2 + 4n + 2 will become
01:12 - insignificant
01:14 - in comparison to the n^3 term, so this will
01:17 - be almost equal to n^3 for very large values of n
01:21 - Or we can also say that
01:23 - thing like
01:24 - when n tends to infinity
01:27 - and that's why we say that this function 
after some time will not grow
01:31 - any faster
01:33 - than function cn^3
01:36 - where c is some constant and we
01:37 - say the same thing as
01:39 - this particular function is
01:41 - big-oh of  n^3
01:44 - so the first
01:45 - rule that i want to state is that if you want to calculate the big-oh
01:48 - notation from a polynomial expression like this, then
01:52 - you need to drop all the lower order terms, and
01:57 - you also need to drop the constant multiplier
02:00 - and you will get the
02:01 - big-oh
02:02 - expression
02:03 - for example
02:04 - say if T(n) is equal to
02:06 - 17n^4 + 3n^3 + 4n + 8,  then you first
02:12 - need to drop the lower order terms
02:14 - and just keep the
02:15 - highest power which is n to the power 4
02:18 - and then you also need  to drop this
02:20 - constant multiplier seventeen
02:22 - so this will be big-oh of
02:24 - n to the power 4l
02:27 - and we may also sometimes encounter
02:30 - time expressions like
02:32 - T(n) equal to 
02:33 - say 16n + log(n)
02:36 - and for higher values of n log n will
become insignificant
02:41 - and you can always
02:42 - try to deduce a mathematical proof but
this will be equal to
02:47 - big-oh of n
02:48 - so you just
02:49 - ignored the
02:50 - lower order term
02:52 - which will be insignificant in comparison ton. log n will be
02:55 - insignificant in comprising to n and we
drop the constant 16
03:00 - now let use see some other rules
03:02 - and the rule is that we can calculate
the running time
03:08 - of an algorithm by summing up that
running time of
03:13 - the fragments in the program
03:14 - and we will discuss this using some
examples shortly
03:18 - what we will  basically teach you is that
03:20 - by looking at a code you shouldfirst look at
03:23 - that running time
03:24 - for different fragments
03:26 - and then calculate the overall running time
03:29 - if a fragment of code is a group of
simple statements
03:32 - like some declaration
03:34 - for example we declare int a and then we say 
that a =5 then we say
03:39 - that a
03:41 - should be incremented
03:43 - then all of these are simple expressions
03:45 - so fragment of code
03:46 - with all these simple expressions will
always run in a constant time
03:50 - so the time complexity is O(1)
03:54 - now if we have a loop, say for example
03:57 - and let's say
03:58 - this is a single loop and inside the loop we
have simple statements
04:02 - and no complex function call
04:04 - or another loop
04:05 - so the running time of
04:07 - a single loop
04:09 - would be
04:10 - the number of times the loop runs
04:13 - multiplied by
04:14 - the running time of the statements
inside, so in this case
04:17 - this loop runs
04:18 - n times
04:20 - and the simple statements take constant
time so the total time taken is
04:23 - definitely proportional to n, so
04:26 - the time complexity of this fragment, a
fragment like this would be big-oh
04:30 - big-oh of n
04:32 - and I will  create some space for another
example
04:35 - now let's say we have a fragment which is
a nested loop
04:39 - now in this case
04:41 - again the outer loop
04:42 - will run n times
04:44 - and corresponding to each
04:45 - run of the outer loop, the inner loop will also run n times
04:49 - so the simple statements
04:51 - will be actually executed
04:53 - n cross n times
04:55 - and the running time of this particular code
04:59 - will be some constant times
05:01 - n square
05:02 - so clearly
05:03 - you can say that this is big-oh of n^2,
 this fragment is O(n^2)
05:07 - in terms of
05:08 - Time Complexity
05:10 - similarly if there were
05:12 - three nested loops, then in that case
the complexity would have been big-oh
05:16 - of
05:17 - n cube
05:18 - Now, let's assume we have a program like
this
05:21 - now in this case we have a fragment
which is just a bunch of simple
05:25 - statements
05:27 - we have a fragment which is
05:29 - uh... single loop
05:31 - and we have a fragment which is
05:33 - nested double loop and we have picked up
05:36 - this example from our previous slide
05:39 - now the running time of the first fragment as we saw
05:42 - was
05:43 - O(1), so this is O(1) and
05:45 - this is O(n)
05:48 - and this is O(n^2) and the overall running time of this particular function
05:53 - would be sum of the running time of all
these fragments, so this is
05:57 - O(1) + O(n) + O(n^2)
06:01 - now once again
06:02 - when we analyze time complexity, we want to analyze it for very large input
06:06 - sizes
06:06 - so for very large input sizes
06:09 - for very large values of n
06:11 - this part will again become insignificant
06:14 - so, the time taken is actually O(n^2), so
06:19 - Thus in a typical program
06:21 - the maximum running time, the fragment
which has the maximum running time
06:25 - actually
06:26 - decides the
06:27 - overall running time of the program
06:30 - now let's say
06:31 - we have a function like this where we have
some conditional statements, so the
06:35 - program is like
06:36 - if some condition is true
06:39 - then we have a 
06:41 - single loop
06:42 - which have
06:43 - which will have a  time complexity O(n)
06:46 - and if this condition is not true
06:48 - we have a nested double loop
06:50 - which will have a time complexity of O(n^2)
06:54 - now if control of the program goes to
06:56 - this particular part
06:58 - It will execute with a complexity O(n)
07:01 - but if the control goes to the else part
07:04 - it will execute
07:06 - with a time complexity of O(n^2)
07:09 - as i had mentioned earlier, when we try to analyze time complexity
07:13 - we always try to analyze it in the 
worst case
07:15 - so in this case
07:17 - if we are not lucky then
07:19 - we will go into
07:20 - that else condition
07:22 - the control of the program will go into
the else condition and the
07:25 - complexity would be O(n^2)
07:28 - so that's why we say  that the time
complexity for this particular program
07:32 - O(n^2),  because that's how
07:35 - it is in the worst case scenario
07:37 - so in case of the conditional
statements, you do not simply add up
07:41 - the fragments or try to
07:43 - calculate the time taken
07:46 - as the some of the two
07:48 - running times
07:49 - but you
07:50 - pick up the maximum of the two
07:52 - so we have this rule for conditional
statements
07:55 - that pick complexity of the condition
which is the worst-case
08:00 - so in this lesson we saw some rules to
analyze the time complexity of some very
08:03 - basic programs
08:05 - in next couple of lessons
08:07 - we will see different
08:08 - time complexity functions
08:10 - and their comparison
08:11 - and, we will also see
08:13 - how-to
08:14 - analyze the time complexity of
08:16 - recursive programs
08:17 - So, thanks for watching !