00:00 - In this lesson we will learn about the
concept of asymptotic notations
00:05 - as a way
00:06 - to classify the running time of algorithms
00:09 - into some generic and broad classes
00:12 - or sets.
00:14 - In the previous lesson, we had seen
00:15 - how to calculate the running time
00:18 - expression
00:19 - for algorithms using the concept of a hypothetical model machine.
00:24 - So, Why do we really want to
00:25 - classify the running time of algorithms into these broad classes.
00:30 - Let's say we have two algorithms:
00:32 - Algorithm 1
00:34 - has the running time defined by this particular function
00:38 - T (n) = 5n^2 +7 where n is the input size
00:42 - and Algorithm 2
00:44 - has the running time defined by 17n^2 + 6n + 8 and let's say we
00:50 - have deduced these expressions
00:52 - on a model machine.
00:54 - We had defined the concept of a model machine  in the previous lesson.
00:58 - A model machine has
01:00 - some basic operations and
01:02 - each basic operation like arithmetical, logical or assignment operation
01:06 - costs us
01:07 - one unit of time.
01:09 - Now, these functions
01:11 - are corresponding to the model machines
01:13 - but we want some functions or some representation
01:16 - which is true
01:17 - irrespective of the machine and still
gives us the idea about the rate of growth
01:21 - of time.
01:24 - Now couple of things about time
complexity analysis:
01:27 - When we analyze time complexity,
01:29 - we analyze it
01:31 - for really really large input sizes, so
01:34 - we want to analyze
01:36 - the time taken when n
01:37 - tends to infinity.
01:40 - In this case if n tends to infinity
01:42 - this plus seven in the first function
01:45 - and this +6 and +8 in the
second function will become
01:47 - insignificant
01:49 - corresponding (compared) to the n square term
01:52 - and if n tends to infinity this constant multiplier 5 and 17
01:56 - also become insignificant
01:59 - and these two functions will pretty much
02:01 - have the similar
02:03 - rate of growth for very high values of n.
02:06 - So irrespective of the machine
02:08 - used,
02:09 - we can say that
02:11 - these two algorithms have
02:13 - something like a quadratic rate of
growth.
02:16 - Rate of growth, which is defined by a quadratic function,
02:20 - which kind of brings these two
02:23 - functions in the same class.
02:25 - But we have a very formal way of
02:27 - classifying
02:29 - functions into classes,
02:32 - in the form of asymptotic notations, where
02:34 - we do not have all these
02:36 - constants in the expressions
02:38 - and we only have
02:40 - the variable term.
02:42 - And let us now define these asymptotic notations,
02:45 - and the first asymptotic notation that we want to
define is
02:48 - the Big-oh notation.
02:51 - If we have a non-negative function
02:53 - g(n)
02:54 - that takes
02:56 - a non-negative argument n
02:58 - then Big-oh(g(n))
03:01 - is defined as
03:02 - the set of all the functions f(n)
03:06 - for which
03:07 - there exist
03:08 - constants...some constants
03:11 - c and n0
03:14 - such that
03:15 - f(n) is less than or equal to
03:18 - c*g(n)
03:21 - for all n
03:22 - greater than or equal to
03:24 - n0,
03:26 - and it's the easier to understand this
using some example.
03:30 - Let's say we have a function f(n)
03:33 - f(n) = 5n^2 + 2n + 1
03:36 - f(n) = 5n^2 + 2n + 1, and we have
03:39 - g(n) = n^2
03:40 - g(n) = n^2
03:43 - now let's see... this 2n can never
be greater than
03:46 - 2n^2
03:47 - and this 1 can never be greater than
n^2, so if we use
03:53 - c = 5+2+1
03:55 - c = 5+2+1, which is
03:58 - 8
03:59 - than f(n)
04:00 - is always
04:02 - less than or equal to
04:04 - 8n^2
04:07 - for all n
04:09 - greater than or equal to 1.
04:11 - So for c = 8 and 
04:13 - and n0 = 1,
04:16 - we can say that f(n) is an element of
the set
04:19 - Big-oh(n^2)
04:21 - Big-oh(n^2).
04:23 - and we often also say this as
04:25 - f(n) = Big-oh(n^2).
04:28 - And let us now plot these two functions
on a graph.
04:33 - So we have plotted the graph here,
04:35 - and you can see that
04:37 - after n = 1,
04:40 - that is our n0,
04:41 - c*g(n)
04:43 - is always greater than f(n).
04:45 - So this assures that f(n)
04:47 - never grows at a rate
04:49 - faster than c*g(n)
04:51 - after n0.
04:53 - So in this way, 
04:54 - Big-oh notation kind of, gives us
04:56 - an upper bound of
04:58 - the rate of growth of a function,
05:01 - and in time complexity analysis, this is
05:04 - very useful because we can say that
hey
05:06 - this is the upper bound of the
05:08 - rate of growth of time and the...
05:10 - time can not
05:11 - grow faster
05:13 - than this.
05:15 - Now one more important thing here is that
05:17 - this c and n0
05:20 - can be chosen differently so for the
same function f(n) and g(n),
05:24 - we can use,
05:25 - we can choose different c and n0
05:27 - and still our
05:29 - conditions may be valid.
05:32 - So let us now define another asymptotic notation,
05:37 - and this notation is called
05:39 - Omega notation.
05:41 - And the definition for this notation is
that
05:43 - if we have
05:45 - a positive function g(n) that takes positive argument n
05:50 - then Omega(g(n)) is defined as
05:52 - the set off all the functions f(n)
05:55 - such that there exist
05:58 - constant c and n0 for f(n)
06:01 - so that
06:02 - c*g(n)
06:04 - is less than or equal to
06:06 - f(n)
06:07 - for all n
06:08 - greater than
06:09 - or equal to n0.
06:15 - Let's pick up an example again
06:17 - and let's pick up the same example
06:19 - we had picked up the last time
06:22 - Let's say our function f(n) is
06:24 - 5n^2 + 2n +1 and
our function g(n) is
06:29 - n^2.
06:32 - Now, for all n greater than or equal to 0, this
06:35 - 2n+1
06:37 - will be greater than or equal to 1,
06:39 - so if we have c = 5 and
06:41 - n0 = 0,
06:44 - then clearly
06:45 - 5n^2 is less than or equal to
06:48 - f(n)
06:49 - for all n
06:51 - greater than or equal to 0.
06:54 - So, we can say that
06:55 - f(n) is
06:56 - Omega(n^2)
06:58 - Omega(n^2).
07:00 - And let's now plot this on a graph.
07:04 - What this graph tells us, is that c*g(n) will
07:08 - never exceed f(n)
07:10 - for all n
07:11 - greater than or equal to n0.
07:13 - So Omega notation gives us
07:16 - the lower bound of the rate of the growth
of a function
07:20 - and in the context of time complexity
analysis
07:23 - this may be important in the sense that
we may say that
07:25 - that hey
07:26 - the time taken grows,
07:29 - at-least by the rate of growth of this
particular function g(n).
07:35 - And let us now see
07:37 - the third
07:38 - uh... asymptotic notation.
07:40 - which is called
07:42 - Theta notation.
07:43 - And the definition for this function is that,
07:46 - if we have a function,
07:47 - a positive function g(n) that takes
positive argument n
07:51 - then, Theta(g(n))
07:53 - is defined as
07:54 - the set of all the functions f(n)
07:57 - such that
07:58 - there exist
08:00 - constants
08:01 - c1, c2 and n0
08:05 - for f(n)
08:06 - such that
08:07 - c1*g(n)
08:09 - is less than or equal to
08:11 - f(n)
08:12 - which is less than or equal to
08:14 - c2*g(n)
08:16 - for all n greater than or equal to n0.
08:21 - And let's pick up
08:22 - an example again
08:24 - and if we pick up the same example that
we have picked up
08:28 - we had picked up earlier for
08:30 - Big-oh and Omega notation,
08:34 - f(n) = 5n^2 + 2n +1
08:37 - and g(n) = n^2
08:39 - g(n) = n^2.
08:42 - then, we can choose c1 = 5,
08:45 - c2 = 8,
08:47 - and n0
08:49 - n0 = 1,
08:53 - and our inequality will hold and we can say that
08:56 - f(n) is
08:57 - actually
08:58 - Theta(n^2)
08:59 - Theta(n^2).
09:02 - Now of all the three asymptotic notations, Big-oh, Omega, and Theta that we have
09:06 - defined
09:07 - Theta notation best describes
09:09 - or gives the best idea about the rate
of growth of the function
09:13 - because it gives us
09:14 - a tight bound
09:17 - unlike Big-oh and Omega
09:18 - which give us
09:20 - upper bound and lower bound.
09:22 - So, Theta notation kind of tells us that
09:25 - g(n) is as close to f(n) , the rate of growth of g(n) is as close to
09:31 - the rate of growth of f(n) as possible.
09:34 - And let's now plot this on a graph.
09:37 - so we can see in the graph, that f(n) is bound by these two functions
09:42 - c1*g(n)
09:43 - and
09:44 - c2*g(n)
09:45 - after n = n0. So this,
09:48 - f(n) always lies
09:50 - between
09:51 - these two .
09:53 - In time complexity analysis, we should
always try to find the
09:56 - tight bound
09:58 - expression or the Theta notation
10:00 - of the time taken
10:02 - because that gives us the best idea
about the time taken
10:05 - but we also
10:07 - in a lot of cases use the Big-oh notation
10:10 - which gives us
10:11 - an idea about the run time of the algorithm in the worst case.
10:14 - So these were...
10:16 - the three
10:17 - asymptotic notations
10:19 - that we use for time complexity
analysis.
10:21 - In coming lessons we will
10:23 - pick up different kind of
10:25 - algorithms,
10:27 - different kind of complex algorithms as well like recursion
10:30 - and see how-to
10:32 - analyze and deduce these
10:33 - asymptotic notation expressions for them
10:36 - So, Thanks for watching !