00:00 - In our previous lesson, we had explained merge
sort algorithm. Now, in this lesson we are
00:04 - going to analyze merge sort algorithm. We
will look at various properties of merge sort
00:09 - algorithm. We will try to see how efficient
it is terms of time and space complexity.
00:16 - Some of the properties of merge sort algorithm
that I want to talk about are, first of all
00:22 - merge sort falls in the class of algorithms
that we call divide and conquer algorithms.
00:28 - In divide and conquer strategy, we break a
problem into sub-problems and then we first
00:34 - find out the solutions to sub-problems and
from solutions to sub-problems we construct
00:39 - the solution of the actual problem. Second
property of merge sort algorithm is that its
00:44 - a recursive algorithm. Programmatically, recursion
is a function calling itself, but generally
00:51 - recursion is problem reducing itself in a
self-similar manner. Merge sort is also a
00:57 - stable sorting algorithm. When we explain
a sorting algorithm, we try to sort a list
01:03 - of integers in increasing order of value,
but a sorting algorithm can be applied to
01:09 - a collection of any data type. When we are
sorting complex records, we sort them on the
01:14 - basis of some property or some key. For example,
lets say we want to sort points in Cartesian
01:22 - plane where each point is given as a pair
of integers with x and y coordinates and we
01:29 - want to sort this list in increasing order
of x coordinate. So, the key here is x coordinate.
01:37 - For this list, there can be 2 possible sorted
arrangements. We have 2 records, 2 points
01:44 - that have equal x coordinate. So, in the sorted
arrangement, either of these could come first.
01:50 - But, in a stable sorting algorithm, the relative
order of records with the same key is preserved.
01:56 - So, if (2,5) is occurring before (2,3) in
the original list, (2,5) must also occur before
02:03 - (2,3) in the sorted arrangement. Merge sort
is a stable sorting algorithm. So, it preserves
02:09 - the relative order of records with same key.
The next property that we want to talk about
02:16 - is - merge sort is not an in-place sorting
algorithm. An in-place sorting algorithm takes
02:23 - constant amount of extra memory to sort a
list. Previous sorting algorithms like bubble
02:28 - sort, insertion sort and selection sort, all
of them were using only some temporary variables
02:33 - to store indices, the extra space taken was
not dependent upon the size of the list. But
02:40 - in merge sort, when we divide an array into
sub-lists , two sub-lists - left and right,
02:47 - we create two entirely new arrays. So, definitely
the amount of extra space that we will take
02:53 - will be proportional to the size of the list,
the number of elements in the list. The space
02:59 - complexity of merge sort is big-oh of n. In
fact we should say theta n. Technically theta
03:06 - notation is what we should be using most of
the time, but because big-oh notation is more
03:10 - famous, we use big-oh notation. What this
means in simple terms is the memory or the
03:18 - space consumption is proportional to the number
of elements in the list. the time complexity
03:24 - of merge sort algorithm is big-oh of n*logn
in worst case. Once again, we should be using
03:31 - the theta notation to be more precise. if
you are not aware of big-oh and theta notation,
03:38 - we have a series on time complexity analysis.
You can find a link to it in the description
03:44 - of this video. Let us now try to deduce and
understand time and space complexity of merge
03:50 - sort. Let us first try to analyze the time
complexity. We have the pseudo-code for merge
03:55 - sort algorithm here that we had written in
our previous lesson. Lets say the time taken
04:00 - for an array of size n by merge site algorithm
will be T(n). Let us now see how we can find
04:07 - an approximate expression for this T(n) . Some
of the... talking about some of the fundamentals
04:13 - of time complexity analysis, we always try
to approximate the worst case and we try to
04:19 - see the rate of growth of time taken for very
high values of n and then we try to classify
04:25 - the time expression as big-oh or theta of
some common function. Here in this algorithm,
04:33 - lets say, when we are executing for n greater
than 1, all these statements, they are simple
04:39 - statements. Simple statements are statements
that have simple operations like arithmetic
04:44 - or logical operations, assignments or comparisons
and simple statements execute in constant
04:51 - time. So lets say these set of statements,
these set of simple statements, in the worst
04:56 - case will take some constant time c1. If there
are no loops or function calls and there are
05:02 - only simple statements, then the time taken
will not be a function of the input size.
05:08 - Now, these two loops, these two assignments
here will have same cost and these two loops
05:16 - together will run n times. So, we can say
that they will together cost us some constant
05:23 - c2, lets say c2 is the cost of executing one
of the statements, then the cost will be c2*n.
05:30 - Sometimes we directly talk in terms of the
asymptotic notations only like we can directly
05:36 - say that this particular segment of the code
is theta(n) in terms of time complexity. but
05:42 - lets talk like this that this will execute
in c2*n where c2 is some constant and then
05:48 - we have this recursive call MergeSort for
left. If we are saying that we will take T(n)
05:53 - time for MergeSort for an array of size n,
the left will be an array of size n/2 . So,
05:59 - this will be T(n/2) cost for us. And then
we have another merge sort call that will
06:06 - again be T(n/2). And then we have a call to
Merge. Now for this one, we will have to look
06:12 - at the merge function, what is the complexity
of Merge function. if you remember the Merge
06:19 - function, all Merge function was doing was
it was picking up one element from either
06:24 - left or right at a time and writing it in
another array. So, in all loops were running
06:30 - for total length of left sub-array plus total
length of right sub-array. So, in all loops
06:36 - were running for n times. So, if you would
analyze the running time for Merge function
06:41 - also, it will be of some form like some constant
times n plus some other constant . i am not
06:49 - going into all the detailed calculations here.
So, for n > 1, T(n) will be equal to 2*T(n/2)
06:57 - plus we can add all these costs up. It will
be something like this. if n is less than
07:06 - 2, or in other words if n is equal to 1, that
is the only case that we will have , when
07:13 - we will go and simply return. In this case
there is only one simple comparison and we
07:20 - return. So, the cost will be some constant.
We can simplify this equation further. Lets
07:26 - say this is equal to 2T(n/2)+ C'n + C'' Once
again C' and C'' are some constants. For very
07:40 - high values of n , this constant term C''
will be negligible compared to C' * n and
07:48 - in time complexity analysis we are mostly
worried about the rate of growth of function
07:53 - for very high values of n. So, for the sake
of simplifying our calculation here, we can
07:58 - get rid of this plus c'' and we can say that
T(n) = 2T(n/2) plus some constant time n and
08:10 - now we can try to solve this recurrence. T(n)
= 2T(n/2) + c'n. Now, T(n/2) can also be written
08:20 - as 2T(n/4) +c' * (n/2) and of course we will
have one more C'n outside here. And this will
08:32 - evaluate to 4T(n/4) + 2C'n. Once again T(n/4)
can be written as 2T(n/8) + c'.(n/4) and we
08:49 - will have this 2c' outside. And this will
evaluate to 8T(n/8) + 3c'n and we can go on
09:01 - reducing like this. We can write this as 16T(n/16)
+ 4c'n and if I have to write this in form
09:09 - of some generic k, this will be equal to 2^k*T(n/2^k)
+ Kc'n . Now we want to reduce this in terms
09:22 - of T(1) because that is what we know. In that
case, n/2^k will be equal to 1 that will mean
09:31 - 2^k = n and k will be equal to log n to the
base 2. So, if k is equal to log n to the
09:40 - base 2., we will have this expression in terms
of T(1). So, finally we can write this as
09:47 - 2^logn to the base 2 times T(1) plus log n
to the base 2 into c' into n. T(1) is equal
10:00 - to c. And 2 to the power log n to the base
2 will be equal to n. So, this will be equal
10:06 - to nC + C'*nlogn and now if I have to write
this term in terms of big-oh notation, then
10:17 - what we typically do is we drop the lower
order terms. n is a lower order term that
10:23 - n*logn. So, for very high values of n, we
can ignore this part, this n*c and we can
10:30 - drop the constants from this part to say that
this is equal to O(nlogn). In fact this will
10:38 - also be equal to Theta(nlogn). Theta of n
log n here means that for some constants C1
10:47 - and C2, T(n) which is equal to nc + C'*nlogn
will be greater than or equal to c1*nlogn
10:58 - and less than or equal to c2*nlogn. if n is
greater than or equal to some constant n0.
11:07 - And big-oh of nlogn means that T(n) will be
less than or equal to some constant times
11:14 - nlogn, if n is greater than or equal to some
constant n0. So, this function T(n) here is
11:22 - both Theta(nlogn) as well as big-oh of nlogn.
When we say that Merge sort is theta(nlogn)
11:30 - in practical sense, we mean that the rate
of growth of time taken by the algorithm for
11:36 - very high values of n is as close to the growth
of function nlogn as possible. And by big-oh
11:45 - notation in practical sense we mean that the
rate of growth will not be more than the rate
11:50 - of growth of nlogn. We will not go into all
the details of why we safely ignore the lower
11:56 - order terms and constants and say that this
is big-oh or theta of a particular function.
12:02 - its beyond the scope of this lesson. Now,
let us try to understand the space complexity
12:07 - requirement for merge sort algorithm. I'll
take this example once again that we had used
12:12 - previously to describe the algorithm. When
we are talking about space complexity, we
12:16 - are talking about the extra memory that we
will consume while executing MergeSort. The
12:23 - extra memory that we will consume will mostly
be in the form of these auxiliary arrays - left
12:27 - and right that we will create. If you see,
once the merge process has finished, we do
12:33 - not need these arrays. So, either they can
be cleared automatically. If they are on the
12:38 - stack section of application's memory, once
the function finishes they will be erased
12:43 - from memory or we can explicitly erase them
by writing some extra statements. but its
12:50 - important, its really needed that these arrays
which are being used as scratch space are
12:57 - deleted at the end of function. There can
be one variation of MergeSort in which instead
13:02 - of using these auxiliary arrays left and right,
instead of using this extra memory for left
13:09 - and right sub-lists in the MergeSort function,
we can use extra memory in the merge function,
13:18 - but lets analyze our implementation only.
First think about this. Lets say we do not
13:23 - erase left and right at the end of MergeSort
function. So, what will be the memory that
13:28 - we will consume. If you see here in this particular
structure, we start with an array of size
13:34 - n and then we split it into two arrays of
size n/2 and then we split it into four arrays
13:41 - of size n/4. Lets call this level zero and
lets call this level 1 and lets call this
13:49 - level 2 where we have sub-lists of size 2
each and lets call this level 3. For an array
13:55 - of size n, what will be the maximum number
of levels. Its like at level 0, you have a
14:02 - list with n elements and then you go to 2
lists with n/2 elements each and then you
14:08 - go to n/4 elements each and you go on like
this till you reach sub-lists with 1 element
14:15 - each. Lets say this will take k steps. So,
it will mean that n/2^k , we will need k divisions
14:24 - by 2 will be equal to 1. So, k will be equal
to logn to the base 2. So, we will have logn
14:33 - levels after level 0, And at each step we
will have, at each level we will have extra
14:40 - memory used to store n elements. 8 elements
are there at level 0 and there are 4+4 = 8
14:46 - at 1 and there are 2*4 = 8 at 2 and so on.
For this example, n is 8 , log n would be
14:53 - 3, so we take 3 steps. We have L1 L2 and L3.
So, if we do not clear extra memory used for
15:00 - left and right, we will take space for n integers
each for logn level. So, space complexity
15:10 - will be Theta of nlogn , the space requirement,
extra space requirement will be theta of nlogn.
15:17 - Now, if we assume that we will clear left
and right arrays once we are done using the
15:23 - MergeSort function, then to explain this scenario,
I'll simulate the MergeSort calls once again
15:29 - using this array as example. When we will
make a call to MergeSort passing this array,
15:35 - we will create these two sublists - left and
right and then we will pause the execution
15:41 - of the first call and go ahead and make another
call on the left sub-list. Now, for this call
15:48 - also, two sublists will be created and we
will go on to first sort the left part which
15:55 - will again create two sublists. Except this
original array, all these extra arrays that
16:00 - are being created are using extra memory.
Now, once we reach this stage, we are not
16:05 - creating any extra sub-list for these sub-lists
with one element each. We will start wrapping
16:13 - up. We will merge these two - 2 and 4. And
once the merge is done, we can get rid of
16:20 - the two arrays with one element each and we
can go on to sort the other half. So, if we
16:27 - will clear the auxiliary arrays - left and
right once we are done using them, in the
16:31 - worst case we will use space for n integers
at level 1. We will use space for n/2 integers
16:42 - at level 2 and we will use space for n/4 integers
at Level 3 and if we will have more elements,
16:50 - we will go on like this. So, the extra space
taken will be n + n/2 + n/4 + we will go on
16:59 - in a geometric progression. For this example
here, once we will be done merging this sub-list
17:06 - - 1,2,4,6, we will clear the sub-lists created
for it and then we will go to the other side.
17:13 - So, any time at a level, we will use half
the space that we would be using at the parent
17:19 - level. Let us now try to solve this geometric
progression here. We can write this as n *(1
17:27 - + 1/2 + 1/4 + ...) and in all we will have
log n terms in this geometric progression.
17:36 - but even if we take this geometric progression
1+ 1/2+1/4+1/8 and so on till infinity, if
17:44 - we take this till infinity, then the sum will
be 1 upon 1-1/2 which will be equal to 2.
17:51 - So, overall this expression, if we call this
function S(n), then S(n) is definitely less
18:00 - than or equal to 2n and S(n) is definitely
greater than or equal to 1n. So, S(n) is clearly
18:09 - theta n. So, space complexity of merge sort
is theta n. So, this was time and space complexity
18:17 - analysis of merge sort algorithm. Thanks for
watching !