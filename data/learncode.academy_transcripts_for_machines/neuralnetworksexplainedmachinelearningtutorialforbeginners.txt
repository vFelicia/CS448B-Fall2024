in this video we're going to dive deeper into how neural networks and machine learning actually works behind the scenes and if you're learning machine learning you've probably come across a diagram that looks like this which often times if not most of the time confuses people because it's a bunch of circles and arrows and the explanation is usually just as confusing well it's simple you take your input circles and move them through your circles and you end up with your voila neural networks um and so that's not really a very good explanation at least it wasn't very good for me when i was learning so hopefully we're gonna break this down make it really simple because it's not really rocket science going into this it's not uh simple like abc's but it's not super complex either so if you guaranteed if you watch this video a few times it'll give you a big head start on what neural networks are and maybe even you'll be able to go and code some yourself if you have coding experience so here's the picture of the neural network we're actually going to use this diagram later but you're going to understand it so a good way to start is to block off this center layer we're going to call that the black box and this is what we've been doing if you've been watching my last two videos on machine learning let's just stick to the input and output data the training data right given this information here's the outcome these are our input dimensions and the output the outcome dimensions and then somewhere that black box in between runs over that information over and over and over again and each time it has a thousand knobs on that black box and it twists a little knob just a little bit at a time until while the black box is tuned and we can give it new input information and get the correct predicted outcome and so this video is going to dive into the hidden layers area that black box and how we configure it and what it does so there's some common configuration options no matter what machine learning or neural network library you're using there's some common configuration options that you're going to come across for configuring that neural network one how many hidden layers do we put in that black box we know we have an input layer and an output layer how many layers go in the middle and usually one is where you need to start off with that's pretty simple and straightforward and then how many nodes how many neurons go in that hidden layer and there's a few different ways you can kind of give yourself an answer to that there's not an exact science to it a good path is if your input and output dimensions are drastically different go somewhere in between if you have seven input values and two output values then yeah maybe three or four also you definitely want to be less than two times the input nodes if you get bigger than two times the input nodes you can get a situation called overfitting where you're just not gonna get accurate outcome also twothirds the input nodes plus the output nodes that's a good kind of way to go and so if you kind of run the math on these three situations you'll kind of get some ideas on what how many nodes that hidden layer should have an activation function this is very important but i'm not going to go into it right now also learning rate and momentum are important but i'm not going to go into those right now lastly the iterations and the desired error level so iterations is how many times it goes over all the data in your data set um the desired error level is how accurate do you want this thing to be and depending on what you're going for basically training stops until you get to one or the other till you've gone through say 20 000 iterations or until your error level is 0.0001 and it's pretty accurate whatever you set the training is going to go till you get to one or the other of those so that's the configuration options that's actually not too bad that's how you tune your black box so if you have a little bit of knowledge as to what the black box is doing you can usually tune a neural network and move on from there and get good results so let's go to a real example using these circles and arrows that we have from before we have a data set this data set gives us fur color and weight of animals and also tells us if those animals were who's its or what'sits so we want to go through all of that data train our neural network to predict whose it's and what's it's based on fur color and weight um and so we've chosen three hidden layers based on the options that we showed you on the last slide so let's go ahead and then get our neural network initialize so to start off we basically want to create a stupid brain a brain that knows nothing so we're gonna just randomly create a bunch of weights you can see i'm going to get my mouse out here you can see we've got we've assigned a point one way to 0.38 just completely random numbers and these neurons these hidden layer neurons and nodes are going to also get a bias consider them coming in with an opinion already even though we don't know anything about how fur color and weight apply to who's it's and what's it's we're just going to start making guesses based on our bias consider them the liberals conservatives and libertarians and we're going to get them all in a room show them a bunch of reallife data and hopefully at the end of the thing they'll all agree on what is truth i'm you know i'm not going to say anything i no okay nope go not going there moving on so that's our hidden layers we initialize the network with just a bunch of completely random weights and biases and then moving on from there we're going to start with the first entry in our data set so entry number one we have an animal for color 25 and weight of 15 and it's a who's it it's a who's it one it's a what's it zero so now we start moving these input dimensions through the neural network it's called a feedforward neural network for that reason so basically what we're going to do is we're going to take both of the input dimensions we're going to multiply them times the weight and then add the bias so here we go node 1 gets fur times 0.1 plus weight times 0.8 so we have our 0.1 weight and our 0.8 weight and then we're going to add the 0.71 bias so that's node number one so we're basically saying let's randomly guess how these two nodes uh affect our outcome let's just start with a guess and we're going to do that for each node a completely different set of weights and biases here a completely different set of weights and biases here you can see when we get down here we're taking fur times 0.57 plus weight times 1 and we're adding a 0.09 bias so we're basically making a random set of guesses and then we do the magic and then we run all those through an activation function and here's where the activation function is important here's where the magic of neural networks comes in is because we're trying to figure out answers to nonlinear data and so i'm going to do a little bit of a tangent over here and kind of show you the difference between linear and nonlinear data linear data is a straight line we can say that hey um as the weight and the fur collar goes up we know it's a greater chance of being a who's it as it goes down it's a greater chance of being a who's it or what's it or hey if the fur color's high or if the weight is high it's always going to be a who's it right so you could use this with some species if it's big it's a dog if it's small it's a mouse if it's small smaller than a certain size there's no way it's going to be a dog that's linear information you don't really need a neural network to solve answers on these you just need a little bit of math and the data should be pretty clear as to how the input correlates with the output nonlinear is like this it just turns out differently um it may be high here and high here or it may kind of follow some unusual curves that are really difficult to figure out um on your own and so that's basically where your activation functions come in we're going to use a sigmoid or a tan or tan h or a tinch depending on who you are and where you come from uh and so we're going to basically apply that that sum value we're going to run it through one of these functions and introduce nonlinearity to our neural network to help us kind of find out the answers to those questions so we've taken the sum right we've taken the sum times the weights and we've added in our bias and we're going to run that through our activation function now we're going to get a nonlinear guess here and then we're going to continue running through until we get to our outcome and let's just throw out a number here let's say we ran it through ran it through activation function came through again here and we end up with just a random drawn out of the hat guess that uh it's point three five one two that it's a who's it and point seven eight one that it's a what's it completely wrong right our neural network is stupid it came in with just biases and random numbers and made a guess that was way off so then it's back propagation time we're going to calculate the error in the delta which is the difference and we're going to adjust all the weights and the biases we're going to go backwards through each step and we're going to adjust these biases and these weights some we're not going to necessarily adjust them all the way but we're going to adjust them some how do we know how much we're going to adjust them well we do that through our configurations the first one is learning rate learning rate says how much should this step outcome affect our weights and our biases and learning rate is you almost want to think of it as personality types there's the slow calculating type of person where if you were to show them here's a cat here's a dog they're going to say hmm i i have some ideas show me some more and then after you show them a whole bunch they'll slowly lean into the answer and then come up with a very calculated answer that's a low learning rate a high learning rate a ridiculously high learning rate would be oh you showed me a cat and a dog i calculated the differences between the two i know the difference and then you show them a really fluffy cat and they think it's a dog because they had way too high of a learning rate so that's what learning rate is and the momentum also says how should our past outcomes affect our weights and biases so past outcomes could say hey that that initial weight there keeps giving us way too high of an answer so we're going to keep that weight kind of going downward no matter what at least we're going to give it a little bit of momentum into this next step so each step kind of takes learning rate and momentum into account and we don't just want to make snap judgments and we also don't want to learn too slowly because we learn too slowly it takes forever to train our neural network so right it's kind of that balance between how fast do we want to train our data versus how accurate do we want that data to be that's kind of where your learning rate and momentum come from so here's kind of an example formula we're going to take our learning rates we're going to multiply the multiply it by the difference and then multiply it by what the actual value is now and we're going to add that to our momentum times our past change amount and that is our current change amount so that's kind of an example formula for how we use learning rate and momentum to determine how much to change a given weight and a bias so now we go through the next piece of data yay we did it we did one iteration through the first piece of our data set we're not through our third iteration yet we're gonna go to the next piece of our data set where fur color is 15 and weight is 35 and that's a what's it and so we do that through each piece of data in our data set and boom that is considered one iteration so we have an iteration and we have an average error rate there and then we can determine do we go through and do more iterations have you requested more than that um or have you requested that to be an acceptable error rate and if not then we just keep going so that's how you train a neural network you give it inputs through the random weights which get more accurate over time biases activation function which is huge and then you're going to calculate your error and you're going to back propagate some adjustments here to the weights and biases and when you're done you get a set of weights that are accurate a set of biases that are accurate and you can then run any input into it it goes through the weights and biases and you get a pretty good guesstimation of what that output's going to be and that's neural networks and machine learning in a nutshell i hope this video helped in the next video we're gonna actually go back to brain.js and watch all these configurations and all these options in action