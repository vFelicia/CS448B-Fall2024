okay we have two problems to solve before our high availability production docker uh solution is complete so we've got everything running our apps are registering so now we just need to kind of pull that internal lcd registry for changes and reload our nginx configuration and this is actually a decent bit easier than service registration i feel like service registration is always the hardest part me telling you that i exist service discovery there's just some good solutions and tools for that that make it a lot easier we're going to be using conf d for nginx co and fd and what conf d is going to do is it's going to run inside of this container and then it's going to watch the cd registry keys that we tell it to watch and whenever things change it's going to reload the nginx configuration template it's going to template it out and change the configuration file and then it's going to reload nginx and i have made a docker image that makes this very simple it's will r stern slash nginx lb and all you do is you run it you pass it two things you tell it the service name that you're listening for and that's your load balancing for and then you give it another environment variable called scd and this is simply your ip address import that it can access scd from so in this case we're running this command right here to get the internal fcd ip address which is always port 2379 so we're passing this in this won't change the only thing that'll change is what your service name is called so in this case we're load balancing some app and then i'll give it a name some app lb so that's it let's go ahead and run this and then i'll show you kind of what's going on under the hood uh so let's go ahead and submit this service file uh submit what i call it some app lb and then i can just start it now ideally we would probably run two copies of this so i'd i'd make it a some app lb at file and then i'd launch two versions of it because we do want failover we want there to be in case this nginx instance goes down we want there to be another one that can get picked up as well but for now we're just doing one for this example let's look at this fleet control list units so there's my lb it is running on this guy oh it's still activating so it's pulling the image right now it'll probably take about 30 seconds start post and should be done any second there we go it's running so let's go ahead and look at this guy i guess i'll just ssh into here i don't know what port it's running on so gosh ssh all these commands can kind of get to you sometimes there you go docker ps there we go node sample is exposing this port 32770 and again that was on this ip address so let's go to here three two seven seven zero three two seven seven zero sum app one let's see if it load balances am i looking at the wrong one yes i am three two seven seven one i was just going directly to some app one there we go two one two four three excellent so it's load balancing between all four great that's very very good to see so let's go and show you how that's doing what it's doing and then we'll move on to our last problem to solve so here's basically everything it took to create that nginx lb image that i made very very simple to recreate on your own i'm doing it from a boon from ubuntu i'm installing conf d i'm installing nginx and then i'm just basically going to remove a few things i'm going to copy a couple files over i'm going to expose port 80 and then my entry command is just to run comfd watch which is this file right here so this is what gets run when you run that load balancer and i'm doing two things i'm simply looking at this tomo file and i'm looking this tomo file is basically the configuration for conf d and it's very very simple you can see here's my template file here's where i'm putting that template file and here's the keys that i'm watching in fcd and then here's my check command this is important too because once you've templated that out you want to make sure that what got automatically generated is actually um actually valid so it's going to run this first it will not reload nginx unless that template file is valid which is a very good thing and we can probably make that do a very loud and clear alert if for some reason our our template file blows up on us and so then it's going to reload nginx when that template's out here's all the template file is is um we're going to proxy to app we're going to listen on port 80 and we're going to proxy up here to all of our up streams so for everything in services app name upstream we're just going to spit out a new server so we have to do two things for this to work is we have to change app name here here and here and then we have to change the app name that we're listening to so if you remember when i started this i did one thing i gave it a service name environment variable and right now we're calling that sum app so when conf d watch starts up we're going to replace every instance of app name in both our tomo file and our nginx template with uh whatever service name we gave it so it's going to automatically replace all this stuff with some app and it's going to automatically replace all this with some app all this all this right here so there we go it's going to change those and then it's going to start up conf d which is going to watch at cd and whenever it changes it's going to do its thing so that's what's happening and that's running so whenever nodes arrive go away all that nginx automatically reloads its configuration um and that's great and so we can actually use this exact same nginx lb image for as many services as we want we just have to give it a different service name and as long as we adhere to this structure as long as we adhere to this structure in etcd services slash service name slash upstream we can use that same image for everything so now we just have one problem left to solve and that's the problem of we still don't know which ip addresses imports all our nginx load balancers will have so now we have to register these somewhere else and then we have a publicfacing proxy in front of them that automatically points all of our subdomains to our services so let's move on to that last and final piece