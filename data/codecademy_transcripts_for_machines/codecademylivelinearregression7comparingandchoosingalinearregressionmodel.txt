okay we will be on in just a second cool yeah i think we're good to go yeah yeah let's do it all right well welcome everyone um this is the seventh live stream uh event in our series and we're going to take a break um after this week and then we'll be back in two weeks to kind of walk through a whole almost like a summary of everything we've done so far but for today we are going to talk about comparing linear models so up to this point we've talked about a few different methods for creating a linear model for fitting different kinds of models like a more flexible model by adding a polynomial term or an interaction term um but we haven't really talked yet about how we evaluate whether doing those things actually has like a positive impact on your model enables you to like model the data better make better predictions um any of those things so that's what we're uh going to cover today cool cool um all right so i'm just gonna jump right in uh we've got this um this code was uploaded to our github repository this morning um so if uh if you want to go ahead and run it yourself you can um also yeah i see a couple people saying hello on the chat um we are back with the green screen um but make sure to say hello if you have any questions um ask them in the chat and alex will keep an eye i'll also try to keep an eye so um we can we can make sure that we cover any of your questions as well yeah and i see a lot of people watching on twitter right now and so um we are mostly interacting with the the youtube chat uh so if you're over on twitter and are just kind of like randomly seeing this we have a lot of these sessions on um on youtube and so if you go and find codecadmin on youtube you'll be able to find all these sessions and really kind of be able to watch live and talk with us there exactly cool awesome um cool so we'll jump right in uh this week we're going to be working with this bike's data set i actually i got this from the uci machine learning repository this is a um a resource that we've used for a lot of codecademy projects and lessons um and also just for anyone i say just for anyone who's looking for good data sets to practice with this is a really great place to take a look because they've got a good mix of different types of things and different levels of cleaning so you can start anywhere and and get a good data set to kind of practice your skills um so this one it's a a data set about bike share a bike share um and i don't know we can read through a little bit but basically um there's some information about um the the date the season the year whether or not it's a holiday what day of the week it is what the weather looked like the temperature humidity wind speed um and then kind of the thing that you're trying to predict is this count bear variable which is the number of total rental bikes that were um that were rented and so you could imagine like this is the kind of thing that if you worked for this bike share company you might want to be able to predict like how many bikes are going to be rented on a particular day given all this information and so um one knowing how each of these predictors are related to the number of bikes that are rented is useful and then also being able to get accurate predictions is useful so that you know how many bikes to have at each of the stations so yeah so that's what we're gonna kind of work with and i think i'm gonna load this up run run okay so um we're gonna start with a couple of different models actually i'll delete this um and i actually saw a question on one of our earlier live streams this past week about r squared and so i think a good place to start for in terms of evaluating a model is r squared it's probably the most typical thing that you learn first when you take a a course in linear regression and i have this written now but i'm gonna just delete this for a minute um and i'm gonna fit two models here we've got model one which predicts the count variable based on temperature wind speed and holiday whether or not it's a holiday and then i've got another model that predicts based off of humidity the season and the day of the week i was using a bike chair yesterday and it was very humid and boy is that a predictor oh my god i yeah i i walked to work this morning and it was not fun i will tell you that much um and then let's say just to start i want to print out i'm just going to print the whole model one summary we'll print just model one summary to start and then we'll take a look at the other one um well it says some of these so i've been here for some of these sessions and not here for some of these sessions and so this one where you where we have a mix of categorical uh is weekday a categorical variable or is that a binary i actually think weekday is a um is not we can check yeah so if you look at the output you can usually tell um actually let's look at model 2 summary but um it looks like season is categorical right because we're getting this like um true spring true summer true winter and then um fall must be the reference category for this right um but then it looks like humidity on weekday we just have one um one slope i'm pretty sure weekday actually we can print out the initial you can just like print out the like stop head just take a look yeah that's cool seeing that uh that season because that was something that we did in an earlier session of um kind of figuring out what the reference variable is um learning how to use these categorical categorical variables what was that that was like week four or five or something yeah it was a couple weeks ago um yeah exactly so it it all comes back to the same same topic that we've been talking about this whole time but um but yeah so we can see actually weekday it looks like is a number from zero to six is is my guess because i see at least one instance of zero and one instance of six and i assume we're talking about a seven day week cool um yes so so we've got one um and we've got one slope um okay so i see a question in the chat about comparing models using information criteria like bic and aic we're going to get to that as well but i just think it's useful for this first pass to see that you can get this model summary which gives you a whole bunch of information and actually it gives you a lot of the things we're going to discuss today it gives you an r squared value it gives you an adjusted r squared we'll talk about that in a minute um it also gives you this f statistic and um prob f statistic which is this is a p value um and i'll i'll talk a little bit about what this means in a minute but basically this is comparing um this is i believe a comparison to a model where you have no predictors so you're just taking like you're basically making your prediction based on the average um count of bikes that are rented um in a given day and you're saying is this model better than just taking the average and because this p value is very small this like e to the minus 75 means point like set point zero zero zero zero like 75 zeros and then five two so we're like very very much below any threshold you might set for a pvalue so this is like saying that this model is statistically significantly better than just taking the average count which we probably expect um and then we've got log likelihood we'll talk about that today we've got aic and bic which we will also talk about today and so yeah if you use stats models one of the nice things is you get all this information right off the bat for your model that gives you some diagnostics then think about like um how well your model is fitting your data cool um so the r squared um r squared is generally interpreted as the proportion of variance in the outcome variable that is described by this model um so i think actually i should pull up the like we can pull up the r squared formula um right so r squared is one minus the residual sum of squares over the total sum of squares um and i can show really quickly residual sum of squares versus total sum of squares so total sum of squares i believe is just like the total sum difference if you just drew a line at if you drew that regression line where it's just a straight line across at the average um total sum of squares is how far off all the points are um and then well basically a measure of how far off the points are and then residual sum of squares is the sum of the squared residuals um and residuals being how far each point is from the prediction okay so basically we want that number to be so we want r squared to be big right so we want this residuals over total to be a small number and that will be a small number if uh the residuals number is small or if the total sum squared is super big and residuals at least do okay right okay yeah so so r squared basically a measure of the proportion of variation in the data that is explained by the model and if you can explain more variation in the outcome variable then you're doing a better job with your model so um so now let's go back we can print just the r squared for each of these models and compare them and so we see um and i'm just pulling out the r squared by doing like model dot r squared um and we see that the first model with temperature wind speed on holiday is doing a little better we're getting like about 41 42 of the variation in the outcome variable which is count um or cnt of bikes that are rented is slightly higher it's about 42 percent of that variation here and then in our second model we're only explaining about 39 of the variation which i mean i guess makes sense like you we kind of think probably the temperature the wind speed and whether or not it's a holiday i would imagine is better than is our better predictors as a group than humidity season and weekday probably humidity and season are pretty correlated um and then i don't know weekday i imagine that the biggest difference in terms of weekday is just um time like well i was i would say like friday saturday sunday versus the rest right so do you think when you say humidity and season are highly correlated um that makes sense oh in the summer it's going to be more humid than in the winter probably so is your kind of intuition there like oh we don't really need both of them or like we could just use one of them would that be something that you would do is there a downside for using both well let's let's take a look so with both of them in the model um our r squared is about .388 what if we take season out of this model um so actually the r squared dropped by a lot that was actually way more than i was expecting yeah wow and what if we reverse it so if we maybe humidity is contributing like nothing wow apparently humidity is not super useful so this is obviously something you can do you can test different you can have test different predictors you could put all the predictors together so let's say you know we could test all of these in one model and then we could say okay what about all of these in a model and then let's also add humidity interesting so this is also this is one of those things where it the um the relationships between all of these predictors matter so maybe if you know the temperature then humidity is important but if you don't know the temperature then humidity is not important got it i don't know maybe maybe we really want a interaction between temperature and humidity i'm just guessing um does that improve it didn't go up very much about the same um so you get the sense right that you can continue to you can can continue to add more and more to this model and see whether it improves the r squared significantly um and you know you can make decisions about how much addition to the r squared is really meaningful um and then maybe you think if it just goes up by .001 maybe you don't care um okay guys don't make fun of my voice this is kind of rude in the chat right here you know we all we all have uh days when our voice sounds better than others um okay i'll i'll do my best not to get into the croaking all right it's fine um okay so let's um let's see let's like take an example and see how much we can how much complexity we can add to a model before things start to break down because one of the issues that happens is if if you're fitting uh if you're fitting a model and you add more complexity for each additional term that you add you are going to explain your data the variance in your data a little bit better um so actually what happens and i i think maybe if i go back to this and i just take this model exact same model but without the interaction term right and i just compare these two models they're exactly the same except the first one doesn't have an interaction term between temperature and humidity and the second one does and i run them we see okay so my r squared was like 0.54098 and then it went up to 0.54100 okay so this interaction term increase the r squared very slightly but does that really warrant having a more complicated model um not necessarily and one of the things you're gonna find is that actually it's impossible for the rsquared to go down if you add more complexity to the model these are what are called nested models which means that this this model is nested inside this one right like this bigger model contains all of the same predictors as the smaller one plus one additional and it would still be nested if i added even more like i could add numpy that power like i could add a polynomial term for temperature right and i could run it again oop that actually improved the model a bit more but these are still considered nested models because the smaller one contains all the same predictors and it turns out that the larger of two nested models will always have a higher r squared because you're always going to be explaining at least a little bit more variation in the outcome variable but now we're going to talk about why that's not always useful um okay so i'm just checking the chat for questions um would the type of encoding of the categorical variables affect the accuracy for example one hot encoding versus integer encoding that is a great question um i think that is a question that i would turn to um i would turn to some like exploratory data analysis to try to answer so for example we can just do this really quickly um i think if i can remember sns box plot if i do like um the weekday and y is count and um the data is spikes i think you want cnt not count oh right yeah i like said it out loud and then typed exactly okay so um so this is one where i would probably just look at this picture first of all i don't know what weekday equals zero means like i don't know which day of the week is zero probably it could be sunday could be monday um but i would look and i would take a look at something like this and i would say okay is there some sort of linear relationship like is it is it just going up throughout the weekday sorry um you could also look at a scatter plot it might be less useful because it's just going to be well anyway we we would take a look at this and see okay like is this something that we think we want to represent with a line based on looking at this it looks like the median number of bikes that are rented is roughly the same every day of the week maybe it goes up and then goes down a little bit so in this case i think it doesn't look like weekday is honestly the best predictor to put in this model but what i was hoping i could show you is something where like let's say you saw that for saturday and sunday it was a lot higher like the rental there were way more rentals on saturday and sunday than they were on other days of the week then maybe you would want to one hot in code and just say like just have weekend and weekday versus weekday um and if you saw that like there was some order of these where it looks like it's you could fit a line to it like it looks like it's just steadily increasing throughout the week then maybe it makes sense to leave it as zero one two three four five six um if it's not a good predictor at all then um then it's not a good predictor um cool all right hopefully that helps answer the question yes this is a jupiter notebook okay so this is actually a kind of a common demo that people use to demonstrate overfitting so and the reason it's it's kind of nice is because you can visualize the overfitting happening in a single plot so now what we're gonna do is we're gonna we're gonna take a bunch of models and the mod the only thing that we're going to change about the models is we're going to add more polynomial terms to higher powers and we're going to see and try to visualize what happens to the model right and the thinking here is kind of doing what we just did of like oh look the more of this that we add the higher that number goes which is which is better and so we're going to see that this is not necessarily the case if we just keep adding more and more and more um polynomial terms exactly okay so here is a scatter plot of of temperature on the xaxis and count of rentals on the yaxis and actually we see this relationship i'm i guess yeah sometimes it can be negative temperature i guess um so we do see this positive relationship but actually if i were to draw a line through this i would say it probably increases and then it starts decreasing which maybe makes sense right because as the weather gets nicer people are more likely to rent bikes and then it gets really hot and then you're like if it's over 90 degrees out which i don't know what that equates to in centigrade which is bad um but basically you know at a certain point it gets too hot and all of a sudden fewer people are going to rent bikes because they just don't want to be outside anymore um so okay maybe a just a line is not going to be the best here but let's let's fit a line and at least see what it looks like so here is my plot where i fit a line to this this model um and so right my model formula is just count um predicted by temperature so i have one predictor temperature to predict the number of bikes um and then this is just some code to create that line i'm just creating a list of numbers between the minimum temperature and the maximum temperature that number list of numbers contains 100 values and then i'm getting the predicted number of bike rentals based on this model for all of those values and then just plotting them down here um you guys have seen this a few times if you have tuned in to some of the other um the other sessions and so when we plot all of this we end up with this regression line which is what we expected right if we just fit a line we can't model that curve okay so let's say that we did a little bit of research beforehand um thank you um we did a little bit of research beforehand a little bit of exploratory analysis we saw that this relationship look this relationship looks kind of curved and so we think okay like we'll add a polynomial term to this model that might do a better job of fitting the data okay so here we are here's the model where we fit a polynomial term a squared polynomial term so we're taking count as a function of temperature and temperature squared in our model okay so r squared uh this is not uh what was the first one was just it was just count and temperature so this is so that was a subset of this one so this r squared value is guaranteed to be higher than the first one right it is and after we look at all these pictures we'll print out all the r squareds of all of the models so we can see what's happening okay cool okay so we add this square term make the same plot and now we see we're able to model this curve a little bit it goes up and then it kind of levels off maybe goes down a little bit at the very end okay that maybe is a little bit better i would expect that that would improve the r squared somewhat meaningfully we saw it actually before when we added that squared term the the um r squared even in the bigger model that r squared went up a bit so we know that that that probably helps okay now let's add temperature cubed to our model and run it and now we've got a little bit more um a little bit more curve to this like it looks like it it starts out a little bit flatter gets steeper and then um and then at a certain point it curves down even more than it did in the first example so i'll just i'll go back really fast so we've got that versus this i mean maybe better not necessarily terrible yeah that drop off at the end maybe looks a little bit better if it drops off quicker yeah not not too bad okay we're gonna continue here i'm gonna add two at a time we're gonna like pick this up a little bit let's add all the way up to five um a power of five so now we've got squared temperature plus temperature squared plus temperature cubed plus 10 plus temperature to the fourth plus temperature to the fifth all in our model let's run it okay oh i didn't get to that one yet i spoiled it i almost spoiled it okay this one doesn't really look any different from the one above it right like sorry i would guess that the r squared is like almost identical into the one above it yeah so model four compared to model three doesn't look that different maybe it's like a little bit more we've got like even more of a flattening here like it looks like this is more oh you can't see my um you can't see my cursor but basically maybe on the left side of this plot it looks a little bit more curved for the five um fifth power one but it's not super different and then i spoiled it but jumping ahead to this one i think has ten in here so we go all the way up to a power of 10 in this model now we get this and this is kind of the epitome of over overfitting um so what's happening here is that we're allowing as much cur we're basically allowing as much curve in this line as we want we're allowing the the line to go up and down like five times and so we see those bumps right like we see i wish you could see my cursor but you see the the first bump on the left another bump another bump another bump and another one um and and each of those little bumps in the line are really responding to very localized um anomalies so it might be just a couple of points that are that are causing that line to go up a little bit um because we're we're just allowing so much flexibility to fit this exact data that we're not really like thinking about well what is what is the actual relationship between this like what what proportion of these data points represent like some sort of random chance that we we see a relationship but like something could be a little higher a little bit lower by random chance on a particular day and we don't necessarily want to fit like exactly to all of those little anomalies right so like maybe that this uh and again we can't see the pointer but that third dot on the left the one that's a little bit higher at you know like 20 200 and negative two um so that's kind of a random single day it's an outlier but that single point is probably doing is what's causing that point to or the line to be kind of pulled up and then back down and so the idea is that that's great for this data set this this line is better for this data set where this line describes that data better but now if we start to think about okay i just have a new day it's a new random day do we really want that single data point from that one day really kind of shaping what this line is and future predictions yeah exactly and i think the answer is probably no if we're trying to build a predictive model and we're trying to make predictions for the future we don't want like one or two random fluke days where it happened to be a really high high rental day or low rental day just by random chance or whatever there was an event that day we don't want to take that into an account in our model when really we're just trying to model though we're trying to make predictions based on like the overall trend i mean i'll drink some water too um okay so here we go we've got this we've seen that there's this overfitting problem and now let's take a look at the rsquared values and see if they match up to our expectation based off of what we saw in the picture so remember that i named these kind of stupidly but remember that model 5 has all 10 powers in it um from no from just like temperature all the way up to temperature to the 10th power and then model four was the one that just has up to five powers and then three has up to the third power two has up to the second power and one doesn't have is just straight linear um and so down here let's print out i think i've already run this but we'll print out the um the r squareds for all of these models and we can see okay so the first one we had about .39 then by adding the square term we go up to 0.45 so it's up like six we're explaining six percent more of the variation in the outcome variable that's pretty good um we add a cubed term and it maybe helps a little bit we're up from like it increases this by 0.01 um so about one percent more variation then we go up from like from having just the cube term to having the turn the temperature to the fourth and temperature to the fifth so we added two more terms this is just going up to 0.4628 instead of 0.4626 so it's going up by point .0002 um and remember those were the two pictures where we said they looked basically the same it was this one and this one and so visually we're seeing right it's not it's not getting that much better um and then numerically we're seeing we're not doing that much better of a job of fitting the data now i think probably like and you might you might be about to get to this but i think a common question is like how do i know when to stop them like we can look at the pictures and and see that oh yeah this this fifth one certainly looks very bad but if i'm plotting something more complicated if there are more variables and i can't plot it in two dimensions like how do i know when you know adding that second term sure seems like a good idea but uh like was the third term a bad idea because it didn't do that much or was it not like how do you decide um yeah enough is enough that is a great question um so there are a few things first of all um there are a bunch of methods which we're going to talk about in a minute that introduce some sort of penalty for increased complexity so every time you add another term to your model you're adding more complexity to the model and so you can just like basically take r squared and then subtract some number times the number of predictors and say like for every additional predictor i want to i want to penalize this number so that got it we can so that there's some sort of pull and push here like it if we don't do that then we're always going to see these numbers increasing by adding more complexity so let's like add something else that says like wait actually if you add complexity then like if it only goes up 0.1 for that complexity but we subtract out point two for each added term then um then it'll actually go down um so we can see that that's basically what adjusted r squared is it just adds a penalty for each additional predictor which means that if something increases if this r squared is only a little bit better but we had to add a lot more terms to get there then um the adjusted r squared will go down um and so we can print out the adjusted r squared this way um just r squared underscore adj for justin and actually you will see something interesting here so um remember in this first example the r squared went up um for the first three models and then it went up just a little bit and then it went up another percentage point or so here it's going up up up then it goes down so when we add the term to the fourth and the term to the fifth the r squared actually goes down um it only goes down by a little bit so we're not adding a huge penalty here for those two extra terms but at least it goes down um but actually it does go back up um when we add to its highest value when we add all of those terms so you know this is one of those things where you might have to you might have to decide what you're comfortable with yourself like if that term that model with the squared term gets you to .45 and by adding eight other eight other terms you are only getting up to 0.46 maybe you just decide i'm not comfortable with that that seems like too much complexity it's also it also makes the model a lot harder to explain to somebody else obviously when we're just using temperature um it's a little bit easier to explain but you could imagine instead of adding squared terms and cubed terms in terms of the fourth etc you could be adding different predictors into your model and once you have 100 predictors in your model if you have a really big data set and then somebody has to app and then let's say like you're presenting this to your boss and you're saying like okay i have this model to predict how many people are gonna rent bikes on this day um and you're and your boss says like oh like how does the how is the model working like what what are the predictors that are most um that are most predictive of this outcome because like maybe some of those things we can control and so we want to know like if um if we only put out the blue bikes instead of the red bikes like just that um does that change the the number of p or that does that change our prediction like we want to understand it or also even from a perspective of like bias a lot of um for example i've seen this in um papers i've read about like you know like models that they're using in police departments or um jails to predict like recidivism in order which basically means like how likely somebody is to commit a crime again and so they are using these like black box models and just throwing all the data they have into the model they don't really know how the model is making a prediction and so if your data is biased in some way that can creep into the predict the method that the model is using to make that prediction even if you know that's not if the data wasn't biased in the first place this is not the best predictive model and so you won't necessarily if you if you use a black box model or you just create a super complicated model and you just dump all your data in um you might get into a situation where your computer is biased and you don't know and that's that creates an issue of fairness in in many different fields many different industries so i this is why this is part of why i like linear regression i know linear regression gets a bad rap um but actually you know like i've i've done a number of projects where i've compared like a random forest model to a logistic regression which is a basically another a slight modification of linear regression um and uh you know svm and like a bunch and a naive bayes like fit all the different models and compare them in terms of accuracy and the logistic regression like nine times out of ten does almost as well logistic rate that linear regression well in in that case if you're i mean if you're comparing logistic regression to most other most i feel like most machine learning algorithms are not predicting quantitative outcomes they're predicting like whether somebody will click on a link or it's a basically a linear regression with a transformation applied to it but it's basically the same um and mathematically like we're still making the same assumptions and yeah like it i don't know i like that in linear regression you can print out all the you can print out all the um the coefficients and i don't know i also like tree based models because tree based models you can like you can do a similar thing and they're pretty simple it's like tree logic like if this then go down this branch if this then go down this other branch and that's also easy to explain once you get into neural networks you might like have a really successful model but like explaining how the model works to somebody who doesn't have a lot of background in machine learning is going to be super complicated um okay cool so um anyway we see that adjusted r squared does a little bit better than r squared we still have to make some judgments about what we're comfortable with um and then i'm just going to show this really quickly this is another way of basically doing the same thing you can run an anova um comparing all of these models and basically get like a p value that compares each nested model to the one to the one smaller nested model and so here i i threw all of the models in and we see that like this these are the p values so this is the pvalue comparing model 2 to mod like the model with the squared term to the um first model and saying like do at least one of the additional predictors in this model like significantly better describe this data and we have basically a pvalue of zero for the first two it's significantly better and then then we get to a pvalue 0.62 comparing the one with the cube term and the one with the fourth and fifth power ones which was the same here right like we had the adjusted r squared actually went down between those two models so we expect like it's not significantly better um but then comparing that model to the model with all the terms we do have a pretty small pvalue if it would depend on what threshold we were using for this this is like a mole this is a bunch of comparisons at once so we would probably want to use a smaller significance threshold so maybe if we're using 0.01 instead of 0.05 this wouldn't be a statistically significant well and and so that pvalue is comparing the fifth model to the fourth model yeah which um again if we look at those numbers that was like the 0.459 uh r squared or adjusted r squared compared to the 0.464 and so that seems like that's a significant difference but would it be fair to say because model four is not significant compared to model three that even though five isn't gonna compare to four that we just like cut it off at uh yeah i i i surely would um i mean yeah if we see if we compare model 3 to model 5 it's not significantly different because remember like it went this adjusted r squared went down and then back up um so yeah yeah so if you cut out that that step of uh model four really didn't do anything and then model model five improved on model four um you'll see if we just cut out that middle that middle one of uh model four three to five it's okay it's closely significant right it's 0.07 and our significant threshold is 0.05 um yeah but i would think in this well first of all there's no such thing as like the enclosure things are either significantly like significant or not but um but yeah like i would say i don't know because we're doing multiple comparisons like having a significance threshold of 0.05 for all of those comparisons also would mean that your false positive rate is higher than 5 so most people would apply like some sort of either like i don't know some sort of multiple comparisons uh like i'm blanking on the word basically you probably would use a smaller threshold or apply some sort of transformation to those um i can you move your wall water glass and i just want to see um okay have you ever compared a linear regression model with an artificial neural network regressor model um i have i have compared a logistic regression to like a simple neural network and in the one this was like a project they did a while ago but in that project i think they performed equally equally well um but yeah i think it's like it's definitely worthwhile to fit if you have the ability the computing power and like the right amount of data and you can fit a bunch of different models like why not fit a bunch of different models and see which one is the best this is actually related to a question i i tried to answer on discord um last week and i'm still interested if other people have thoughts on it but it's like i don't think there's any i don't think there's anything wrong with fitting a bunch of models and seeing which one is has the best accuracy or has the um the best predictive power and like we can we'll discuss in a second some other ways of of evaluating that for a model um but if you if you do find that a neural network or some other type of model improves the accuracy a lot over your logistic regression or whatever else you're trying to fit if you've if you find that then it's actually like an interesting thing to dig in further because it's picking up on something in the data that you're not able to model with your with your simpler regression model um and maybe you can figure out what that is like maybe there's just some sort of relationship that you're not taking into account um but yeah i'm sure there are situations where where a neural network will do better i mean they're definitely situations um but it comes with comes with some downsides just as adding more complexity to a linear model maybe improves it but comes with downsides and i think one of the things i know i'm going on a tangent here but i think one of the things that is really difficult about data science and that a lot of data scientists disagree about also i don't know what all of these errors are i haven't figured this out um but i think a lot of one something that a lot of people disagree about is like what is the best way to draw lines in the sand and what is what is the best way to accomplish the same goal so like do you where do you sacrifice interpretability for accuracy and where don't you and there's not it's a field where i think there's a lack of there's a lack of agreement among people about like what the right what the right threshold is for for questions like this um yeah exactly it's like harder to interpret and if if you're trying like if your goal is you have to present this at a meeting and be like okay like i made this i built this model um this is the data we have this is how the model works and you want to be able to and you want to be able to look for like understand what the model is doing and understand how bias might be creeping in like a more interpretable model is going to be better has real value and also like if you want to be able to take action based off of the model like we're thinking about the bike share data set like maybe you just want to predict how many bikes are going to be rented um on a particular day and that's your only goal but like that actually seems unlikely like you probably want to be able to make that prediction but you also probably want to be able to understand like what are the factors that predict it because like yeah is it really only the weather that we can't control or is it you know the location of our right yeah you know stations or whatever it is um yeah exactly should we uh so we had the earlier question about um bic and aic i don't know if uh yeah so we'll jump into those right now um so that we don't run out of time so log likelihood um there are a bunch of methods that are likelihood based methods um and the log likelihood function is and i always get this mixed up so maybe i should pull up the uh i should pull up this on codecademy let's just see if i can do that really fast um because i always get it mixed up whether it's like the probability of the data given the model i think that's right um but just so that i don't tell you all the wrong thing um also i this maybe helps because i can show you where this is it's in the choosing a linear regression model um module and then in this lesson you can you can play with this data set yourself um but go to this log likelihood um yeah so it essentially measures the probability of observing our data given a particular model so this is like a probability based um function rather than like a proportion of variance explained um and i'm not going to go into the math of like what that probability looks like but you can kind of imagine um when we think about a model we think about like a data generation process and so like this model let's like think about for a a second the super simple model we had at the beginning where we did just like count as a function of temperature right really what this model is saying is that we believe that the this data is generated based on like around this line where all these points are just randomly like this is our prediction and then there's some random error on any given day and so like we think there's a higher probability that it'll be close this line and a lower probability that a particular data point will be far away from this line in this model like same thing and so for each model we have some probability for each of these data points that like you know uh um one of these points that's really far away from the line is less probable than a point that's close to the line and so you can imagine calculating for all of the data points what's the probability of observing all those data points given the model that we've created and then you want to maximize that probability instead of maximizing the um the variance explained it's just a different method and so but then we use this like we use log um so we take that probability and then we take the log of it um so that we can it basically just makes it easier to search that that space for the um for the like optimal value um and so and that's like a a whole separate thing but basically so log likelihood we want um we want log likelihood to be as as large as possible um but large means usually log likelihood will be negative so a larger log likelihood is actually like it'll be a smaller negative number in most cases it is closer to zero if it's if it's negative yeah it is possible to get positive log likelihood but i i've actually seen it in one instance when i was trying to make this lesson the first data set that i grabbed had like r squared values i couldn't get it over like point one basically and then the um the log like lee hoods were positive um but so we see again here it's um increasing right becoming less negative less negative less negative it just like um regular r squared log likelihood will always increase with more complexity um but we have aic and bic which function much in the same way if a ic and bic are to log likelihood what adjusted r squared is to r squared which is to say that aic and bic are just log likelihood actually they are negative log likelihood so they turn it into a positive number and then minus some number times the number of predictors basically so it's a penalty term for adding more predictors and bic gives you a bigger penalty for each predictor um so we'll see aic is going to decrease decrease then increase and because we so because of the negative it's now negative now a smaller number is better in this case so smaller is better so now we've got it's getting so sorry was i saying decrease or increase no i don't know but basically right okay good um so basically the mod we're saying the model gets better and better and better then it gets worse then it gets better again so the aic is basically giving us the same relationship that we got for adjusted r squared um and then you'll see bic because we have a um a bigger uh penalty for each additional predictor bic goes down for the first one and then it just goes up after that so after just adding the square term bic would have you choose bic would say like every all of these additional terms are worse so bic would have you choose the um just the squared term and i think for that reason a lot of people do prefer bic because it like it helps you find more simple models and it just it gives you a bigger penalty for each piece of added complexity um okay i know i'm like i always do this i'm running out of time but i just want to talk about one other way really quickly and then you can read more about this and learn more on the actual course um but one other way that people assess um model accuracy is they split the data up into a training set and a test set um i think actually alex did was it you that wrote an article about this somewhere and yeah somewhere for the original set of machine learning things we can probably find it yeah because i think there was a nice picture in that and we can do the picture instead of the uh look at the picture instead of the code i think it's somewhere in there we go training set versus validation set versus um okay oh i guess you only have the one for tenfold cross validation but basically the idea is you split the data in some way so let's say just looking at this first ex this first picture under fold one the yellow bar represents some subset of the data right and then the blue bars represent the rest of the data and essentially what you do right is you fit if you fit the data on your training set which is maybe 90 of the data or it could be 80 or 70 whatever you want and then you fit and then you test the model out on the rest of the data to see how well it predicts your outcome and the reason why we might do this is because we know the true outcome value for the test set so like if we split the um the bike's data into into pieces then even for the test set we have the true value of count and so we can use our training set to predict count to create the model then predict that outcome variable for our test data set that we did not use to fit the model and we can see how well it performs maybe we just take like um the different you know the sum of squared differences or the sum of differences between the predictions and the the real values and that is kind of becomes kind of like our measure of accuracy that we can use to evaluate the model and what happens is in a situation like this your model is going to be better at making predictions for your um sorry your bottle is gonna be better at making predictions for your training set but it's gonna be probably worse at making predictions for your test set because this model was so um sensitive to specific data points that that could have messed with it um cool uh yeah i see a note in the chat on facebook um come chat with us in the youtube stream yeah we we can really only have one chat open at a time so we'll probably figure out ways to get facebook as well but uh we're just getting set up in the uh back in the office here so um i'm glad that you at least made it to the youtube chat if you saw that on on facebook yeah we're sorry um but yeah hopefully hopefully everyone was able to um get their questions answered um like i said we i highly recommend that you take a look at the course um itself because it has a lot more information here kind of rush through in an hour um so we'll be back in two weeks to kind of do like a a wrap up and um and apply everything that we saw so far in all of these streams we'll like pick a data set and kind of play with it and make it it'll be a little bit more openended and fun so you can you know give us insights or give us suggestions for what you think we should add to the model or take out and we can kind of do some model building in real time together um and then um on thursday we will have office hours um on discord again um but we recommend like if you have questions um ask them in discord we'll keep an eye on it so that we can make sure that we answer some of those questions give the next two weeks um so yeah ask us questions and we'll we'll make sure we check them before the final live stream so that we can we can answer some of those questions live if you don't have the ability to come to the office hours yeah then uh so in two weeks that's our eighth and final stream for this linear regression series if you have other things that uh you're interested in uh in us doing a live stream on uh let us know and maybe we can make it happen right now we have no plans for a next series but um yeah we would uh i at least would be excited to do another one of these so um yeah if you have ideas on other live streams that you'd like to see uh let us know you can comment on this youtube video if you're if you're watching it uh sometime in the future totally cool all right