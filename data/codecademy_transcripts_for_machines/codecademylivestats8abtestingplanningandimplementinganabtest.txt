waiting for everyone to file in waiting to see if we're live so he did such a good job last week of um not having any awkwardness in the first 20 seconds of the stream i don't know i honestly don't know how i did that but hopefully i don't know hopefully i can repeat it someday yeah the timing right cool let's see why okay cool we are up and running yay awesome hello everyone dr monkey uk it's always the first one the best it's awesome let us know you're here um welcome to the eighth stream at eighth and final stream in this series um potentially we have more content in the master stats with python path coming out so maybe someday in the future we'll do more streams like this but uh this is the last one in this series so we're really excited to jump in today and do some um and have a conversation about a b testing which i feel like is a very uh commonly used but sometimes misunderstood way that statistics gets used in a lot of different industries and a lot of different companies so i'm excited to dig in per usual we are looking at the youtube chat hi jamie um and hi dr monkey uk if anybody else is here say hello let us know if you have any questions um and feel free to join us in coding on your own computer if you would like yeah before we get too far into this one i'll do a plug for our upcoming series um after this one is over next week we're starting another one with uh g1 another curriculum developer is going to be leading it about kind of creative coding and specifically using javascript and a javascript library called p5.js so i'll plug that more at the very end of the show but uh yeah we're uh we're doing another series starting next week it's gonna be super awesome i'm going to watch all of them and finally learn some things so yeah very excited all right i'm going to share my screen uh let me make sure this time that i get the right one cool um can you see that alex yes that looks good to me okay suddenly usually i can see your face somewhere on the screen but now i can't see so you'll just see i'll be here you'll be just a voice um somewhere in the background all right so um so like i said today we're going to be talking about um about a b testing and um and we're going to approach this in a similar way to the other content that we've approached or the other statistical concepts that we've learned in the series we're gonna think about both um a researcher perspective so someone that works at a company and then we're gonna think about um a like allpowerful perspective if you had all the information you could possibly have in the world um what decisions or what would you then be able to glean from this process um so a little bit of background sophie and before we go too far i also wanted to plug that uh for the last 15 minutes or so of this stream we're gonna bring on uh dan layfield who is a product manager here at codecademy he runs a lot of the a b testing that we do and so we can ask him kind of what this looks like in the real world so yeah another thing to think about if you have any questions that you want to ask somebody who's implementing some of these statistical concepts um we'll try to leave like 10 15 minutes for dan exactly yeah i'm super excited because i think i i like to focus on a lot of the theory uh or i have focused on a lot of the theory in these live streams and so to have somebody that is actually implementing this in real time i think is a really nice complement to everything that we've been learning so cool um okay so i guess the thing that we should start with is maybe just talking through what an a b test is or like what an a b test is kind of used to do so alex i mean you're not an expert in av testing but i'm sure you've seen some a b tests before seen some results or heard about them so can you give like a one minute explanation of what you think an a b test is all about sure so i my general sense of what an a b test is is um comparing two different options um of something and comparing the results of those options so um we've talked about this with curriculum development where we don't actively do this but it'll be something great that we would love to be able to do is like oh here's a paragraph about for loops written this way here's a paragraph paragraph about four loops written the other way which one is better um or which one do learners find easier to understand or do better on the quiz or or whatever that metric is that we're interested about um we compare those simultaneously and i think one thing that i think that we could do is run you know run paragraph one for week one and then edit it and run paragraph two for week two but i think one of the main important things about a b testing is that it has to be like run simultaneously right or you try to like keep the groups that are interacting with the content as similar as possible i guess yeah we can definitely talk about that a little bit but yes i think in an ideal world you want to set up your experiments so that your two groups that you're comparing are identical in or your two or more groups that you're comparing are identical in um observable and unobservable ways and the only way that you can guarantee or that you can make the unobservable make the group's balance based on unobservable characteristics is by using randomization um and so that means that like you basically have to have a group of people and then randomly assign them to groups and if you do it based off of something temporal like time like the first 500 people and then the second 500 people um then those groups are no longer randomly um right you could argue that the first 500 people had some different motivation or just a different group of people or maybe you know maybe people who um are accessing codecademy between the hours of 8 a.m to 5 p.m gmt are different than um the ones that are accessing it from 5 p.m to the next right so yeah so um one thing i'm gonna do so that was a great explanation and one thing we're gonna do here is we're gonna actually simulate some data that might look like data that came from an a b test so i've written out some code to do this here but um i'm gonna write out that code in front of you guys right now um again so that we can talk through what it's doing so um let's imagine for a minute that we're interested in um two different versions of a subscribe button and we'll call them like version a and version b um and we want to know whether version b or version a is better in terms of the proportion of people that are gonna click on the button if it surfaced to them so um it's a stupid example but we're just gonna go with it for simplicity what a great plug to subscribe to our youtube channel and academy pro and all of our various things we want you to subscribe to exactly we'll pretend that we're trying to get you to subscribe um and so we can imagine for right now that um we have some subscribe button that we're already surfacing to people so uh let's call that the like a button um in the in the spirit of a b testing so um let's say we've got our a button that is the button that we're using now um and based on historical data about half of people who see that button are are clicking it that would be a very high rate but we're just going to pretend like that's how popular we are um and then let's say we want to know if some new button that we're going to design is going to be better than the button that we already have and let's say that we'll feel like it's a better button if it performs such that 60 percent of people click on it um one quick note and this is just like a vocabulary thing so you'll see right that our existing button has 50 rate our new button we're hoping is 10 percent higher so a 60 click rate the difference between these two numbers is 10 but a lot of the time people will use another word called lift which often or is generally reported as the percent increase of these percentages so that sounds complicated but basically what it means is 60 right is 2 i believe am i doing this right 20 higher than 50 right so so look at looking down i am already like looking ahead to that other block of code where you have one plus lift times a rate so if we want if we want b rate to be 0.6 that lift is going to be it's not going to be 10 because that'll be 1.1 times a rate which would be 0.55 so yeah i think it's going to be 20 but so you could do a little bit of algebra to figure out what that lift is based on what percentage you actually want the b rate to be yeah so i'm not going to go into too much detail on that but um but for the time being let's say that the a rate was the our baseline rate that we already have is about 50 and we want to see if we can increase that to at least 60 um so let's let's simulate some data then um so where i'm going to use the same random functions that we've been using prior to this so i'm going to use the numpy.random.choice function i'll just copy these over but we'll well yeah we'll change things around we'll like hard code some of these numbers just so that you see what's going on so let's say we show this button that we've in our experiment we're going to show this button that we already have to 100 people right and we're gonna simulate some data where for those 100 people they have a 50 probability of clicking the button and a 50 probability of not clicking the button yeah and so this syntax looks really similar to in previous lessons when we were flipping a coin right the coin could either be heads or tails and we said okay a fair coin is 50 50 but sometimes we were flipping an unfair coin where it was like ninety percent ten percent right and so that's what that p value is or that p array is doing is saying the the odds of getting yes is um the first the first value and the odds of getting no is that second value so for a rate it's 50 50 and for b rate it's going to be um what 60 40 i guess 4.6 i guess i can put in the actual numbers in here so later we can kind of code these with things like a rate 1 minus a rate b rate 1 minus b rate but just so that we can read this super easily let's just put this all together so you can see okay so if we simulate some data like this so we're simulating again data where um people have a 50 chance of clicking the a button they have a 60 chance of clicking the b button then we can kind of assemble this in a in a data frame so i'm gonna i'm gonna put together a data frame where i'm gonna have a column that tells me whether or not someone clicked a button i'm gonna have a column that tells me which button they saw so to do that what i'm gonna do is i'm actually going to combine these two arrays into a single array and there's a bunch of different ways to do that i have i have grown to really like lists and pythons in terms of like appending arrays or appending lists and like it's happening so sophie uh sophie joined codecademy and was a huge r fan and i was always the python guy and now now she loves python lists yeah well they are pretty nice i think that bass python is like easier than bass r in terms of like general programming things like but in terms of actually running actually doing an analysis or like running statistical it's really easy to do that in r um okay so if i do this really quickly i'm just gonna show you guys what this looks like if i just add these two lists together all it's gonna do is it's gonna take it's gonna take this list and append this list on to the end so just put all of those together so we just have to know that the first hundred were the people that saw button a and the 700 are people that saw button b exactly so yeah the first hundred are the people that saw button a and so now i'm gonna make another list to go alongside this one um that says so this is gonna be uh clicked or outcome maybe we'll call it and then we'll make a second list where we have a hundred a's and then a hundred b's so it can kind of line up right alongside this one so i'm gonna do list a or sorry list where i put a and i'm gonna this is why i like this you can do that and that will just repeat it a hundred times so let me print what this looks like so what i just did was created a list with a hundred a's in a row and then a hundred b's in a row yeah one thing uh and maybe some python experts in the chat i'm always a little bit hesitant to do that i think with like objects those might all when you use multiplication with a list it might all point to the exact same object so it's like if you edited one you'd be editing all of them oh really but i think strings might be different i and i honestly i don't even know if that's true but i've always been a little bit hesitant to use that um or it's good to check to make sure that these are not all copies of each other if you're using um multiplication like that wow i did not know that um well i know that this works for what i need it to do i turn it back to an empire so i think it will be okay but that's super interesting yeah some and that might not even be accurate but for whatever reason i've always been slightly paranoid about using that uh that multiplication with lists um well then now i don't like python anymore jokes okay um all right here we go let's see if we can this okay so here is our simulated data where the first hundred rows saw group a or stop button a the second hundred rows saw button b and then we have simulated information about whether or not they clicked the button um and every time we run this we're gonna get slightly different a slightly different data set but the probability of clicking in group a versus group b will be the same every time so right now i see yes no yes yes yes and if i run this again now i've got yes no no no no so it's going to be slightly different every time yeah so if you one thing that we haven't really talked about as we've been doing all these exercises where we're simulating stuff by like random distributions is we've never talked about how to like seed your random number generator and why you might want to do that um can you talk a bit about like what it means to to see your random library sure yeah so i think one way and here i'm gonna actually just to make this a little easier to see i'm gonna print um a cross tabulation of this uh data group clicked um so that we can see like four numbers instead of oops yeah so we can see four numbers instead of the whole data set um okay so like ada alex was saying um when we run something that's like when we use a function that is using some sort of random number generator um we sometimes might want to like write our code in a way that another person could replicate it so um if i pass this this code on to alex and i want to say like this is what i did and i want you to see the same output that i got or if i want to have him like replicate exactly what i was doing um that gets really difficult when you've got random functions in your code um so one thing you can do is you can set a seed i think um it's just like random look it up as you're doing this it might be np.random.seed but yeah i think that's like random i don't know let's try it yeah i think that'll do it okay so now you'll see right this time we got 50 we'll just focus on this number 51 but you'll see um now that i've set a seed i should get the same numbers every time i run this so even though it's a random process i'm generating the same random outcome every time i run this by setting the seed and you can set the seat this number to anything and every unique number that you input or every unique seed that you input will give you um a different result but like as long as you use the same random seed you'll get the same result every time yeah compared to if we if we delete that line and just keep running it those numbers are going to be bouncing around right they're they're going to be different each time um i find this like i find this helpful i use this a lot in my like coursework when i'd be working on a problem set that somehow involved um random numbers and you know i was like trying to debug my code or trying to like trace through my code to understand what's happening and it's really difficult if some part of that is changing every time you run your program and so yeah i find it helpful for debugging where it's like okay you know that these numbers are always going to be the same even though they're still random um and that way you can kind of like trace through what's happening nice yeah that's super helpful uh i i feel like i use this a lot um if i'm ever trying to do an analysis where i need to use a function that has some sort of random generator like a lot of machine learning algorithms even have um some like use some sort of random gener random process as part of fitting the algorithm so like trying to find the minimum of the of some surface you'll like have you'll start in random places on the surface or something to try to converge towards the minimum and so you get slightly different numbers every time um and if you want your research to be reproducible which is a really good practice or you want somebody else to review your work then it's a good idea to use a random seed because then somebody else can verify um that they get the same result cool yeah um okay so now that we have this kind of fake data set we can think about what we would do as a researcher if we got this data so if we were working at a company and we saw we ran this experiment and we saw these numbers we saw in group a 47 people did not click our button and 53 people did versus in group b 37 people did not click the button and 63 did you might think okay well this looks at least somewhat convincing that a higher proportion of people are clicking the button in group b and now we want to know is that is that sustainable if we run this experiment again so same way as we were discussing last week and the week before hypothesis testing is really built around this idea that you can only observe a sample but you want to know something about a larger population that you can't observe and in the case of this fake a b test that we're running the larger sample that we can observe is all the users who could ever or all the visitors to youtube that could ever view this subscribe button either way and um and we can't see what all of them would do so we only want to change the button because it takes some effort if we feel reasonably confident based on our sample that we'll still see a higher rate of clicks among the larger population to whom we surface these buttons so um we're gonna run a hypothesis test i don't think we've gone over this hypothesis this particular hypothesis test in um in one of these sessions yet but depending on what kind of a b test and what metric you're interested in you might use different hypothesis tests in this case you can kind of think of this as two categorical variables right what group you're in and whether or not you click something and so the outcome we care about because it's categorical it's like do you click or not click um one of the tests that hypothesis tests we could use here is a chisquare test if the outcome we cared about was something quantitative like how much time they spent watching our youtube video or something like that um then we would need a different kind of test so i'm just gonna demo running this test i'm not gonna go into the kind of specifics of what's involved with the test but um we can come back to that if there's time i just want to make sure that we have time to talk about that the results and why they're interesting um so i believe let me just grab i'm gonna grab the uh the test from here because i always forget what the function is and the order of the parameters sophie do you want to roast me about the chisquare test oh one of my favorite stories or one of my favorite moments from when we were building this stats curriculum is alex the thought that the chi square test which it's totally valid you're not wrong not that it was pronounced chai like the t and i don't know it just made me laugh sophie's the stats expert no i just i mean like kai is a a greek letter right yeah [Laughter] but now i now whenever i think of it i think of like someone eating or drinking two cups of tea and it makes me happy yeah maybe test the tea um okay so what i'm going to do is i'm going to save this contingency table as a b contingency and that's the input that i give to this function so basically when i run this test i input this two by two table and then it's going to give me a p value which is testing the null hypoth hypothesis that there's no association between these two variables so there's no relationship between the group identity and whether or not somebody clicked um i'm seeing some questions in the chat uh are we looking to see that the observed higher rate of clicks it's not just higher lift but is 20 higher before then checking to see if it translates to the greater population with our hypothesis test um so that's a good question so i think it will become a little bit clearer in the next step but for right now let's just imagine that we are we are all powerful and we know that these are the true rates um when we come back to the next piece of it we'll talk through how if you're if you're the researcher um and you're planning an a b test you have to think about how big of a difference you care to measure in your test so once you once you have some data you definitely want to measure what is the rate in group a what is the rate in group b and how how large is that and then run the hypothesis test to see it if that's a significant difference right that you can reject the null hypothesis that the difference or that the association um does not exist or that you can reject the null hypothesis that these numbers are um are the same but so sophie i guess the question that i have then is do you set those rates um you set those rates before you view the the samples right whereas it's not because i could see a world where you're like okay we've like made this change we see that um you know group a got 53 yeses group b got 63 yeses is that a real difference right like i could see coming up with those rates after the fact after getting the two samples but yeah so basically this so i will explain this in the next step but for for a hypothesis test or for an a b test usually you're comparing and we can ask dan a little bit about this later too but oftentimes you're comparing something that already exists to a new um a new version okay and so you'll have some baseline data so you'll probably know this number right you'll know like the thing that already exists how does that perform on the website but then you're not going to know what this number is right the only thing that you need to decide before the test is how big of a difference you care about because for example right there could be a true difference but this could be 0.5001 and like you could have a test that detects that difference right we call that like a effect size and this would be a small effect size or a small lift so you could design a test to to detect that difference but you probably don't care about it so we'll come back to this when we come back to the kind of researcher side of it but for right now let's just imagine that these are known rates okay so if these are known rates we could generate this data set we could run our chisquare test the null hypothesis for the chisquare test is that there's no association between these two things another way of thinking about that is that these two rights are the same and then we can print our pvalue and see whether we reject the null hypothesis that there's no association and in this case we wouldn't oh that's surprising i would think that why am i getting oh there we go so in this one we got .002 it's surprising that we got a pvalue of 0.57 i would have thought that with this big of a difference we would usually reject the null hypothesis and think that there was a significant difference but i guess not could it be sample size of size of 100 isn't big enough maybe we'll have to come back to this so um i i think this is the next interesting question once we get in a little bit further into our simulation let's put this let's make this a bigger difference make it like and then that has to propagate through clicks b oh oh that's right because i hard coded it yeah so now i'm getting smaller i'm getting small p values yeah um right so if this p value is small enough below some significance threshold then we reject the null hypothesis and we say that we we think that these things are associated which in the case of this example would lead us to believe that um the b rate was or the the b button was better at getting people to click on our button okay so let's now zoom back out and let's do what we've done a bunch of times and put this all into a for loop and then think about what happens when we run the same random experiment a bunch of times and each time let's think about whether or not we get a significant result as i'm setting this up um alex do you want to talk through so from i'm thinking from the perspective of a researcher what do you think are some of the pro like some of the things that are some of the considerations that you might want to take in terms of how you design this experiment like do you think that um it's just always good to have as large of a sample size as you could possibly get or do you think you want to prioritize like um how the size of the difference that you're trying to measure you have any ideas for like what what are the considerations that you might have to take into account as you're trying to design this um yeah interesting i don't know if i have any anything that sticks out to the top on the top of my head but yeah i would say that in general i would assume that get getting as large as sample size as possible is good um i would again another thing that i kind of when i hear about a b testing the thing that i hear about is like trying to get the groups as um as random as possible or as similar as possible um and randomly split up so doing things like making sure that they're kind of interacting with this at the same time um what could um what do you think i'm trying to like ask you yeah leave me a question but come up with the right way to frame them um what is so okay so let's say that there's a difference a real difference between two groups like let's say that button b really is better than button a right um but it's only a little bit better and it's it's hard for us to measure it there's kind of there there are two different ways that we could make a mistake or there's in this case there's one way that we could make a mistake we could make a mistake by not finding that difference even though there is one um right so that's that has to do with like pvalue right and what pvalue we consider being significant or not um but on the flip side if there's no uh there's no difference between the groups and we make a mistake and we find that there is a difference then that's also not a great thing and so maybe can you walk through from the perspective of a company like how someone might think through those considerations like what goes wrong when you make each of those kinds of mistakes or what's the what could go wrong if you make either of those kinds of mistakes like practically what could go wrong well so like you're yeah ultimately these tests result in some decision being made right and so if um and i'm not gonna know whether it's type one or type two but if it's um if you say that you found a significant difference and like okay let's go ahead and implement this button and there's really no difference then that's a lot of wasted work to to go and make that change um and i suppose the opposite is true for the other type of error if you find that there's no difference but there really is one then you've missed out on this opportunity yeah so right so exactly there there's two kind of things going on here where you if there's a real difference then you want to be able to detect it but if there's not a real difference and you detect one then you're potentially going to put resources into changing something that doesn't need to be changed or doesn't help you so there's kind of two different types of mistakes that you might want to think about prioritizing or not prioritizing when you're trying to plan an a b test so um i'm gonna just grab because i i'm conscious of time and i want to make sure that we have plenty of time to um ask questions later uh with dan so i'm going to just grab some code from here but essentially what i'm going to do in fact i'm just going to grab all of this and then i'll talk through what it's doing yeah so if you while you do that we have a question from the chat of um and i think i know the answer this but uh maybe i'm wrong can this be used to hypothesize difference in male female wages like can you say okay male wages are group a female female wages or group b um can that be something that you use this for yeah so you could use a different kind of hypothesis test for that in that case you're talking about an association between a categorical variable like male and female or whatever other genders exist right so you've got a categorical variable and then you have or what's usually treated as a categorical variable and then you have um a quantitative variable which is wages and so if you're looking at an association between those two then you would need a different hypothesis test but you could definitely think about it in a similar way you would probably use like a twosample ttest or you could also use a ztest um okay so let me talk through what i've got here so same as we had before now i've rewritten this as lift but i'm gonna just uh i'm gonna just put in point six again there we go point seven um okay so we've got a group a rate a group b rate we've got our sample size which is getting split into the two groups so actually here we've only got 50 in each group maybe i'll up it to 200 so we've got 100 in each group we stimulate our data we run our chisquare test and then we'll save the result in this list called results where we'll save significant if the pvalue is less than our significance threshold and we'll save not significant if it's not less than our significance threshold for right now we'll use a significance threshold of 0.05 and then at the very end we're going to calculate the the proportion of the results that were significant and we'll calculate the proportion that are not significant okay okay so let's run this and so before we run this we expect this to be a significant difference or at least i would expect this to be a significant difference um and so i would expect that proportion to be pretty high if a lot of them are significant and not many of them are not significant yep exactly so in this case we've got 73 percent that are significant and 27 that are not significant so um let's talk through for just a second what situation we're in so in this case right because we have a group a rate in this example that's different from our group b rate so again we're all powerful all knowing we know that these rates are different we're in the situation where we want to reject the null hypothesis because there really is a difference right like the correct answer would be a significant pvalue right so in this case we've got that the proportion of significant results is is 0.73 so 73 of the time we detected this true difference and 27 of the time we did not what do you think will happen like if i make this bigger so the difference is even bigger alex what do you think will happen will this number go up or will it go down i imagine it will go up they seem to be it seems to be like this this is very similar to like how the pvalue was the type one error rate right um so i'm guessing yeah yeah that will be the next question but yeah so exactly so if i make this difference even bigger and i run this now 98 of the time the results are significant so the point is here the larger the difference between the two groups the more likely it is that we'll be able to detect a difference when there really is one so can i ask like the inverse what if these two groups are actually the same and if they're actually the same then shouldn't we expect to get like in if there was no randomization then we would expect to get 100 not significant right so yeah actually uh what do you remember or do you wanna take a venture or venture guess what the uh proportion of significant results will be if i do this is is this the pvalue now is this going to be 0.05 it's not the pvalue but it's going to be the signal or the significant threshold yeah that's what i'm at yeah it should be approximately 0.05 so this is actually what we were going over the other week yeah cool right so you'll see that which is fun um right so we gotta take into account exactly what alex is saying two different types of mistakes if we if the truth is that these two rates are the same then we're going to make a mistake about the same percent of the time as whatever significance threshold we set for our test and if the truth is that the rates are different then we're going to be more able or more likely able to detect that difference if it's large the other thing that will help us detect a difference is the sample size so right now if we had this at 200 and we've got this proportion of significant results that's equal to 73 if we up this to 400 we're gonna get a higher proportion of results that are significant so what does this mean practically when you're like designing a experiment um so practically and that's where we're gonna kind of move in and we'll move into this discussion and we'll move into our discussion then um is practically the things that you need to know as your cal as you're setting up an a b test are the baseline conversion rate the lowest in this case it's the smallest difference that you care to measure so right we saw that as long as the difference between the two groups is some number or bigger your probability of of detecting a real difference is just going to go up right so whenever we calculate a sample size we use something we use a statistical power level of 80 and that corresponds to whoops this uh proportion of results that are significant if there is a real difference oops real difference between the groups so here actually if we had um a sample size of 400 and the real difference between the groups was 20 then our power would be 99 so so this tool is saying if we're only interested in 80 per area and 80 statistical power then we don't need 400 people we only need 100 people or something like that yeah we're basically saying that if we want power of at least 80 for to detect a difference in rates of at least 10 percent for two different variants how big is the sample size that we need and that's what this calculator is is calculating for us and really the things that you need to know to plan an experiment are how big of a sample do we need for each group you might need to calculate the duration so like how much traffic do you have um and how long is it going to take you to show your buttons to that large of the sample size um and then right like how big of and in order to calculate these things you need to know um this is kind of the inverse of that significance threshold so a 5 or a 0.05 significance threshold would be a confidence level of 95 percent this gets written different ways in different online calculators that you might use and then you need to know statistical power and this is really like minimum detectable effect which is the minimum rate difference that you would care to measure okay should we bring dan on sophie i don't know if you see him with the screen share but dan is here oh yay hi actually sophie unless we want to show anything on screen maybe this is best if we cut the screen charge so we can see each other oh yeah now i can see all of you this is so much nicer than just talking into a screen welcome so um alex do you want to give a quick intro and then yeah dan i was wondering if you could introduce yourself um talk a little bit about what you do at codecademy and how yeah how you run a b test yeah sure so great to meet everybody so i'm a product manager here at code academy at bennett code academy about five years been a pm for three years really the job of a product manager and a consumerfocused company like codeacademy is to like pick key parts of our user experience and try to improve them i think like in an ideal sense uh you're getting a chance to do a ton of user research and pull data together and then come up with hypotheses about like what could prove user experience uh frankly hypotheses you're really really confident and you have a bunch of great evidence for you don't ship in a b test because you just want to get the right thing out there as fast as possible but abjs are great at deferring risk so if you're really not sure about which one of two options is better for the users in the business a b tests are a great way of measuring awesome that was a really great segue into i i've prepared a couple of questions so i'll try to watch them but uh it's a good segue into my first question which is i'm curious if you could talk about the kinds of questions that codecademy tries to answer with a b tests like what what are some examples yeah so we've done a lot of testing uh for consumer focused company like ours like the bottom of our funnel is very important so specifically our pricing pages our checkout pages how our pricing looks the pricing of the plans themselves is potentially like a really high leverage point in the experience but also like a potentially really risky thing to change so when we go do a bunch of product development work on things like our pricing page like how exactly should we be conveying what's in the free and the paid product uh is a great place to do testing i don't know which uh sophie which little experiment writeup you have you can probably pull up whichever one uh makes sense and i can we can talk about exactly how that went sure uh i think there's one on a pricing positioning let me share my screen again uh and uh folks in chat if you have questions for dan too uh feel free to yeah happy thanksgiving throw him and chat we can give it to him all right let's see i think give me one second yeah no worries so i think like really the job of the product manager is to like deploy the uh resources of the company in the way that is the best for the business and the user base uh so we're constantly trying to come up with ideas about what could be better an experience uh and then shift them and measure them so here's a good example of a test we ran on our pricing page so to test uh the hypothesis that like users might be more interested in our paid products if we conveyed the amount of content in the paid product versus just that there is a difference in content so if you scroll down i think uh the screenshot's probably conveyed the clearest so really important uh in a b testing is clear hypothesis clear kpis that you want to measure uh and then the control the variant was just like that there is a difference between the free and the paid product and the variant yeah zoom in a little just displays like the number of hours of learning content i think versus the number of courses of learning content yeah so in the control there that first line is hours and then sophie if you scroll down to the other one uh yeah number of courses yeah and if i recall correctly i think the course count lost which kind of makes sense is like hours of content is kind of like an opaque confusing concept uh to users we're really just trying to figure out like what is the best way of showing the difference between the products uh my recollection is number of courses make more sense which is a little bit more intuitive so this is a great example of like something we legitimately didn't know the answer to and sits in like an important part of the application itself so that like therefore we want to test it and uh tests like this also go on to inform other teams like the marketing team and how they display the the paid content to other people because what's really important in testing is not necessarily like the exact designs that went into a controller of variants it's the underlying thing that a user understands at a certain point in the journey so this insight is like likely applicable to other teams of code academy you can help make the experience better for everybody dan i'm curious about when you are designing these experiments are you doing the kinds of things that sophie was showing off of like like using these kind of calculators to determine what's the sample size how how long are we going to run it for like can you talk a little bit about actually implementing one of these yeah sure what that looks like so i think we we kind of start at the product development phase like i was describing earlier we'll pick apart uh the experience and really do as much research as we can to figure out what could be improved uh from them we come up with kind of like a tiered list of like what are we most confident in what are we least confident the super confident things just get chipped uh the things that kind of in the middle that are a little more risky tend to get tested and before we do any tests we want to make sure that the test the page we're running on has enough traffic that we can like get to a statistically significant conclusion in a reasonable amount of time so like when we've honestly screwed this up in the past is like we put too many variants on a page we do the math wrong and we realize it'll take four months to get an answer here which is way too long at a startup like really we want to be coming to conclusions uh in typically under a month unless the test is really really high leverage so like if we realize that we don't have the bandwidth to like ship four variants maybe we cut the variance size down to two or three which then affects like how our design and engineering processes work which means like hopefully we get to an answer faster and we have the designer spin up less variance and maybe we'd ship tests in multiple rounds instead of all at once i'm curious if um you all think a lot about minimum detectable effect before you start an a b test so like do you think about how big of a difference do you care about before you try to plan out the sample size or do you try to estimate how big of a difference you think there will be it's a little bit of like chicken or the egg like if um if we know we don't have a lot of traffic on a page and we're doing something that's riskier you need to make a bigger change to meet the minimum detectable effect threshold so like if uh i want to say in that test you just showed kind of makes sense that a tweak within a pricing grid would only be like a four to five percent change whereas like if we completely overhaul the design of the page that may be a 15 change uh which means it's a riskier test to run because there's more engineering and design hours that go into the variant but we know like if we want to be statistically confident and change where we don't have a big population size like the change has to be order magnitude really large to like observe that effect in that time threshold yeah that's interesting and do you do you consider like the i guess you could touch on it a little bit there but the engineering resources or the design resources of like okay we think that this is going to be a large change and relatively speaking there's not a ton of engineering work that needs to be done here or this change isn't going to be huge but you know uh we can we can ship it within three days if we get some engineers working on it or sometimes yeah definitely i mean as a product manager you're always playing the balancing game of like in an ideal world you're shipping the like lowest effort highest impact the lowest risk work at all times right so like a great example of a change that we didn't a b test uh is on our pricing pages and on our checkout pages we use the dollar sign to denote our prices which makes sense if you're in the us but is confusing to users in australia uh and new zealand and canada and fiji which also use the dollar sign so like that we're just confident in that like denoting the currency with an abbreviation is clearer so we don't test that okay like i ideally in a perfect world like we're finding little stuff like that constantly and just shipping it out really really fast but you inevitably get to areas of development that you just don't know the right path forward and that's where testing is great i'm curious if you can speak at all about once you have run a test how you then decide whether or not you're going to ship something so let's say you run a test and um you detect a difference between the two groups but it's really really small um do you still ship it as long as it's like incrementally better or how do you make what are some of the considerations that you yeah i mean i think in um in tests that are themselves lower risk like in areas of the app that don't get a lot of traffic or like away from the core flows uh to me the biggest risk is you had a winner and you didn't ship it because those incremental wins do add up to bigger wins so like if the overall risk is not that great we probably just default to shipping it and then see if we can catch like a longitudinal difference in just observing the traffic on the page over a long period of time it's like in a really wellrounded testing program like you have the analytics set up to like observe the test groups outside of the initial experimentation windows um because we've run enough tests here that we've definitely seen cases where like we see a variant when win during an experiment but like outside of the initial observation window it flat lines and goes back to being the same because maybe you're running on tests of like existing users and you just tweak something and it looks new more people click on it and those things can be like actually very hard to control for like practically uh like in a startup but there's definitely things that are like higher risk that we'll want to have like really extend the observation window and be confident in our results like if we're changing like pricing or our trial model or like anything that you know we use to pay all the bills around here did you super interesting because i it reminds me we had the conversation before you joined about how this is kind of it's a random process and so even if there's um a difference that you observe in a particular group it doesn't necessarily mean that that difference is true for the entire population because you only observed a very small proportion of people but i didn't think about the other part of that which is that for the people that are just seeing a new uh feature there's like some sort of maybe newness effect that is also playing a role and so it's not fully yeah definitely and i think like in ideal scenarios like you're just shipping things to new users so when we do things in the new new first time user experience or the home page like those are easier to control for because a lot of people going through those i've never seen any of these things before but like if we wanted to improve like the fifth module of a course um like the population size is so much smaller that deep into the experience that realistically it's you'll never be able to control in our size anyway just for new users i'm curious is a b testing a pretty common thing that product managers do i know um or when i think about a b testing at code academy i like think about your team and you know sophie and i were talking about how we wish we did this more in curriculum or it doesn't really happen like we wish we could a b test the curriculum itself that we write so like how what kind of jobs use a b testing is it a lot of product managers and if not like how did you kind of find your way into the space yeah sure um i think it is heavily used in product management and specifically in product management in consumer facing companies so if code academy was a primarily like b2b business like uh a b testing can be helpful but realistically most of your customers are on longterm contracts and it's tough to really measure like a trial or a conversion event and this like signals in the kpi is a little blurrier they definitely still do it uh but really the consumer companies also have the traffic size where you have enough of a population to run a high volume of a b tests like booking.com uh and facebook and uber and google are really like the companies that like have massive size and also like a ton of infrastructure built up um infrastructure like is an interesting thing to mention there because like tests are not cheap to run so you should be running them like as often as you need to to defer risk but no more it's very tempting as like a product manager you don't want to ship everything as an a b test because you don't know like definitively if you're right about something whereas like doing like longitudinal studies are less satisfying because data has variance in it anyway so it's tough to say like if the thing you did is really like moving a business forward but as you mentioned in curriculum like it's it's tough to really control for these things and you need a lot of like infrastructure and data science resources to like make a change shift it's much harder um so there's definitely like a cost to them but they can be super powerful yeah it's super interesting wow thank you for joining us this was really good it's awesome i think like we were saying before it's been a lot of theory in these uh in these live streams but it's good to remember that this is a like a real tool that people are using in a job setting and um using it to answer questions and build a business and uh make a product better so it's really helpful yeah definitely i mean the theory is super important like without understanding like the theory it's tough to like design and sequence these tests but like testing is definitely like an applied science yeah we're like there's a theory of how you're doing it but typically you're in environments that you can't perfectly control so you like adhere as close to the theory as possible uh but frequently there's things in your way yeah for sure yeah it almost reminds me of physics classes where you're like all in all physics classes you're like like wind resistance doesn't exist and you're like learning about theory and then you come to apply it and you're like wait a minute there's a million different factors here though if you could perfectly isolate these things it would be awesome but yeah like in reality you can there's tracking problems right there's other teams working in areas that you can't work in uh so you the theory is key but it's really just like applying it in the best way yeah and also knowing how the theory relates to the the actual practical applications like understanding that your sample is not actually a random or a perfect representation of the population you care about which in this case is exactly all users that might ever access this and you there's no way to ever get a random sample from that population so yeah the the application is very hard like even if you ship tests to like a 95 uh statistical significance level it means one of every test 20 of your tests could be wrong right and if you ship enough tests a year you definitely have a bunch of those that you're measuring wrong just practically yeah yeah and it's i think it's also a huge issue at those bigger companies that do have infrastructure um if they're running like thousands of tests at a time and it's hard to make decisions based off of that because you know that you could be making errors exactly like tests overlap they conflict you get the observations you're seeing don't hold with time like at the uh he's messing with reality but it's still super powerful yeah all right i think we are just about at time but i'm just checking the chat to see if we got any questions i don't see any yeah let me um so let me do a plug so well first of all this is the end of this series so congratulations sophie you did eight weeks in a row of this um so thank you for running really amazing sessions um i i learned a lot and i think you did a great job so thank you sophie and thank you for helping with all of them i just like sit here in the background for most of them and occasionally say something so um that's not accurate at all um next week we are starting another series again it's going to be on um creative coding um some really cool projects that jiwan is going to go through i'm putting a link in the chat right now to our events page um uh you can you can find all the events there which will actually send you to the to the youtube um pages even though they're not publicly yet um but keep an eye out on our youtube page because uh shortly i will publish all of these events um so you can see when we're running them uh current plan is to keep doing them on thursdays at 4pm uh but we might actually uh we might actually change that as we're going so keep an eye out on that events page subscribe to the youtube channel all that good stuff to know when we're doing stuff so all right and with that