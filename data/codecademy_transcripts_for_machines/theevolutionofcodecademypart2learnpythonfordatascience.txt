and we're live hey hi everyone welcome back to our live stream uh this is the second in our threepart journey through codecademy's Evolution and today's topic is a fan favorite you know it you love it it's python for data science I'm Corey Steve Cook Academy's content marketing manager and I'm joined by fetty Garcia Lorca Lorca our community manager who you probably recognize from the forums and Discord as well as Dr Autumn Morse a senior instructional designer in data science who will be walking us through this project step by step and then over in the chat we have Eva savinga a senior curriculum developer in data science who's on hand to answer all of your questions uh in case you missed last week's live stream we're celebrating codecademy's birthday all month long with events and new course launches and lots more next week we're actually going to give you a look at our new course intro to generative AI so be sure to RSV for that live stream at the link in the YouTube description below and we'll see you there um today though Anna is diving into a project from our path Learn Python for data science python is the most popular programming language in our catalog and I'm actually curious how many of you here have taken one of our python courses please let us know in the chat um python is a general purpose language it's used for everything for data analysis to mathematical Computing to machine learning and web development in Anna's demo you'll get to see just how easy to read python syntax is and you'll also see how we work with Jupiter notebook a workspace for data science code and visualizations that's built right in to our data science courses and paths uh keep in mind that you can take this path whenever you want on your own time and it's linked in the description below and this live stream will be available to replay if you get stuck or need hints um before we get into it I want to tell you a little bit more about ADA Dr Autumn Morse has worked on courses across our data science catalog covering topics like python of course Excel data engineering and most recently Ai and prompt engineering she comes from a mathematics background and has a PHD focused on the design of selfassembling DNA nanostructures Ada has also worked as an Actuarial systems analyst she's taught math and statistics at Champlain College and she's conducted research supported by NASA and the Vermont space Grant Consortium Hi Ada we're so excited to do this I wanted to not like bumped this up and ask a question that a lot of people have uh what makes python so great for data science it's a great question um python is first of all a great language in general for beginners because it was designed to match sort of natural spoken English so that it sort of Narrows that distance between you know what you want to do and now how do you write the code to make the computer Do It um so that just makes it a really good choice for beginners in general and then in terms of data science um there's just been such a robust data science Community working in Python so um there's lots of packages of prewritten code so you don't need to sort of start from scratch with all of your data analytics um there's lots of people to ask questions and there's lots of courses available um and data scientists in general use Python so if you want to do data science at some point you're going to have to read python code and work in Python code so why not learn it totally all right take us away Ada sure um let me share my screen and here we go so we're going to be analyzing some highspeed Railway data in Python um light Corey said this comes um sorry um we're going to analyze a real data set from a highspeed rail network in China um so like Corey said this project is part of our new Learn Python for data science curriculum um and I'll show you some of what we just talked about in terms of how python is relatively easy to understand and Jupiter notebook is a pretty good workspace for doing data science um you might be wondering why we created um a new course about python when there's so many available and the reason is we really wanted to develop a curriculum specifically for data science um I don't know about you but I often want to learn something new around data science and then I'm stuck with you know weeks or months of learning sort of basic syntax and not getting to like build the thing I actually wanted to build so this teaches you python from scratch while you're actually analyzing real data sets about technology and sports and uh 11 or 12 others so I won't list them all because we want to get into the code so let's switch over to the code academy learning environment and take a look at this data set um so this is a real data set um I will just say as a disclaimer we made a few alterations to the data set structure so that it would make sense in the course and so that um you know you all could practice specific techniques with it um but we didn't modify any of the data so it's all real data about a highspeed rail network in China we can actually take a look at the raw data set um in Jupiter by clicking the Jupiter icon we have all our data sets in this data set folders you can see we have a whole bunch of different files um and they're all they all have this dot CSV extension which stands for comma separated values so if we take a look at what this data set looks like you'll see we've got a bunch of data in here it's not particularly easy to read um as as human beings but in our first row we've got what are column headers so we've got dates we've got ride IDs if we go down we have info like mileage and different delay statistics um and these are all separated with commas which is what that comma separated values means so we're not going to work with these files on their own because that would be a little bit difficult to do instead we're going to read them into python using a package called pandas and work with them there so let's go back to our notebook um so I've got the first little bit of code already loaded here um this first line import pandas as PD we're going to work with the data science package called pandas in this project um like I said at the beginning one of the advantages of python for data science is that we have all these packages already written and they just give us a bunch of code for standard tasks so that we don't have to rate them ourselves and can get right into Data analysis and because I don't like typing things I'm telling python here that I'm going to call pandas PD so I only have to type two letters instead of six okay um now that we have pandas imported what we can do is we can use functionality that's already been coded so pandas has a read CSV method so that's that same CSV comma separated values that's the file that we were looking at and so this is a method to read that data in and then structure it really nicely in a table called a data frame ah session Timeout on the live stream okay disaster averted everybody um it's just live stream luck so like I said it pulls in that data formats it nicely on a table so we can read it much more easily um and I always like after I read a data set in to just go through all the columns and make sure I understand what's going on so we have the date column um it looks like these are all going to be from January 1st 2020. if you look at the file name of the data set it kind of gives a clue that the file name of the data set contains that date um we have a ride ID a train number and a station order um the eagleeyed might notice that the ride ID is a composite column so we start off with our train number then we've got our date and then we've got the station order and so what's happening here is we're actually tracking a train as it goes from Station to Station along its route um so each row is an individual trip from one station to another station with no breaks in between um and then we're following the same train g1226 as it goes from Station to Station um we have our station name data we also have mileage um which is actually in kilometers so maybe this column should be called kilometer Edge um and then we have delay data so arrival delay and departure delay um you might notice there's some negative numbers which is kind of fun so those are early arrivals um or early departures so this first trip it arrived on time and it departed early um so we'll dig into more of that those details as we go along our last column is major holiday this looks like it's just a Boolean column so Boolean means it's true or false and it just tells us was this on a major holiday um or was it not on a major holiday because that might impact um how the trains work all right um so one thing we have built into this for the course just in case you're taking the course who you know are these toggles where we do basically what I just did and go through the data set and let you know what's going on um I'm not going to look through those in the live stream so our first task whenever we're working with data is to read data into the file um read data into into pandas rather from the file and make sure it's all nice and clean um this is something that's often sort of missing from data science classes my own personal pet peeve a lot of data science is getting data and then structuring it so that we can actually work with it and so this project comes from a section of the course where we're teaching you how to do that so we're actually going to get our hands a little bit dirty here with the raw data and cleaning it up nicely so that we can then get to some you know cool insights later on in the project um so when we looked at the data sets there were a whole bunch of different files and there's one file for each day in January this is a pretty typical structure for data where you know if reports are being generated on a daily or a weekly or a monthly basis um each day week or month is stored in its own file now we want to read in all of those files right we want all of the data that we have and we don't want to do that by hand because there's um actually 27 files so I don't want to type the same code 27 times fortunately python is a robust programming language so we won't have to do that um so what we're going to do instead is we're going to create a variable file names and this is going to store a list of all of the files that we want to read in um and before I I write that just a quick note for anyone who's like the total beginner in Python it can be really easy to get hung up on like do I understand all the syntax that's going on I'll explain everything along the way but also feel free to just relax and see this as an opportunity of seeing like what python can do um you know I wouldn't be too worried if you're like I don't understand the every single tiny bit of bit of syntax um so what I want to do is I want to build a list of all of the file names um and again I don't like typing so I'm gonna I'm gonna try and make this as as short as possible for me so the structure of the file name is pretty much the same for each file in that what we have is we have our data sets folder and then we have the railway delays 2020 and January and so the only thing that's changing for each file is the day um so we have data from January 1st through January 27th so if I wanted January 2nd I would change that to be a two or January 3rd I'd change that to be a three um and so what we're going to do is we're going to use Python to do that change for us automatically um first I just want to show you how I'm going to break this apart for a single file name so what I would do is I'd say okay all of that um at the beginning is going to be the same for every file so I'm going to leave that in its own string and then I'm going to separate off the piece that is changing so the piece that is changing is this number one that's going to change to a two to a 3 to a four and what this plus sign is doing is it's telling python take all these individual pieces and just squish them together the fancy word is concatenate but I'm not so fancy so let's just say we're going to squish them together and if I run this um nothing will happen because I haven't told python to print anything out but if I run it telling Jupiter notebook actually in this case to display that variable at the end you'll see that python has done that sort of concatenation or squishing together we've got our one in there now I just need this to update for every single um file and so the way I do that is introducing another variable so instead of this one I want that to just be whatever the day is right whether it's the first or the second or the third it's going to be a variable that stores the day and then I need to tell python what are the different values that the day can be so the key word here is the word for just because that's what python uses so four day in and then there's this range syntax that lets me specify a range now the funky thing is for basic python is that I want to go up to January 27th that's the data I have the way this range thing works you actually give it the next value 28 I cannot tell you the number of times I've done this wrong just by not thinking about it don't stress too much um it's a pretty common issue to run into and so now if I run this what I just realized is I'm going to get an error let's chat about that um so when I run this I get an error called a type error and if I read down to the description it says you can only concatenate Str not int to Str a little bit confusing but Str stands for string that's like a bunch of text maybe with some numbers like the file name int is an integer and so what's happened here is my day is an integer and python is saying I'm just a computer I can't take a string which I think of as being a bunch of text and I don't know how to add an integer to that so to fix this I need to tell python that day should actually be a string again this is something that's easy to forget I just forgot it um but fortunately the error message was pretty helpful and now if we run this we've got all our file names in a beautiful list um I don't know even now I find it satisfying what can I say um now there are some fancy packages that can sort of shorten the amount of work you need to do to find like all the CSV files in a certain directory some of you might have encountered something called glob or something similar I think one of the cool things about python or any programming languages once you know the basics you can do a lot of things just by being creative um but certainly you can also look for special packages that might might help things out in the meantime we've got all our file names which is pretty exciting and now we just need to read those in as um data fribs so let's give that a shot so again I'm going to Define a variable the variable this is just a name that's going to store all of these data sets because we're going to have to read them in one by one as individual data sets now let's just grab the code from above um that read the data set in so this is the read CSV function now this would just read in a single file right but what I want to do is I want to read in a list so what I'm going to do is I'm going to use these square brackets again to say I'm reading in a list now what goes in here inside the read CSV function is a file name and I want that file name to be changing right I don't want it to be just this one file name so I'm going to do the same trick that I did before to generate the file names and I'm just going to say this is a file name I don't know what it is but it's a file name and once again I need to tell python what are the file names what are the different names of all of my files and that's why I created that file name list so again I can tell python what values to sort of put in for the file name using the word for file name in file names now file names here that's the list that we just defined that has all of our file names and then the file name on its own this is just the variable that's going to be read into all of these different um data sets or they're called Data frames in pandas once we've read the csvn the structure is called a data frame so we can't preview all of them right now but we can do a couple checks to make sure things are looking good so what we can do is we can print out the very first data frame so what I've done is I've taken this variable data Railway delays DFS this is where we're storing all of those data frames the square brackets and the zero says get the first one again you'll remember from before with 28 and 27 python starts counting at zero really fun and really annoying but zero says get the first one and then pandas has this helpful head method which will display the first five rows or so of the data frame so let's run that great our first data frame loaded correctly looks good um just to be certain that we've got 27 of these I'm going to also check the length so python has this function Len which will output the length of a list and beautiful we have 27. now that doesn't actually mean that they all were read incorrectly but I feel I feel confident enough that I can move on to the next step now that I know we've got 27 data sets probably all 27 read in correctly but we still have 27 separate data frames and we want one data frame we want one data set that contains all our data right if we look at our first data set that we pulled in here right this was all January 1st now what I want is I want to Stack this and there's a bunch more rows under this this is just the first five rows I want to Stack this above January seconds data above January thirds data to get all of them in a single data set once again the great thing about pandas is someone has already written this code for us so all we need to do is write the code to call that pandas method so I'm going to create a new variable to store our full data frame um I've created a typo fix that and I'm going to call pandas using PD and now the method is dot concat this is short for concatenate just like we were squishing strings together earlier now we're going to squish data sets together um and what are the data sets I'm going to concatenate well the ones I just imported using read CSV and now um what I would like to do so that works nicely but I would like to print out some information about this um this new new data frame to make sure everything worked out correctly um and the way I'm going to get this information again is the pandasmethod.info this is going to tell me things like what are all the columns are there any missing values all of that kind of exciting information um so the first thing I see in my output is I've got 29 000 total entries um and then in this First Column it looks like I've got 28 000 nonnull so those don't quite match so it looks like there might be some missing data but it actually looks like we just have some other data Integrity issue going on here because these columns are all lower case and then I have a second set of the same columns that are uppercase and then I have this third copy of the station name column which is lowercase but is somehow different from the lowercase version that's above so I think we've got just a little data Integrity issue chances are at some point the way the data sets were encoded or created changed from lowercase to uppercase or from uppercase to lowercase um and for this last station name issue my guess would be that there's some sneaky white space so there's actually like a space before the text or after the text that's causing pandas to read it indifferently so in order to actually get to our analysis we need to fix these problems um so what we're going to do is we're going to go back to our list of all of our data frames and just quickly correct all of the column um column headers make sure that they're all lower case make sure that there's no extra white space the fun thing is we don't actually have to look at any of the data frames on their own we can just have python check and fix them all so what I'm going to do is I'm going to write what's called a for Loop so a for Loop we've seen this a little bit before as we created these file names this starts with the keyword for and it's going to tell python keep doing something over and over and over and like we did before I'm going to do this over all of our data sets so we've read all our data sets in and I'm going to look at every single data frame in that list of data frames DF again it's just sort of the uh the standard shorthand for data frame so what this is telling python is take every single data set that you read in and do something with it and now let's specify what we want to do the first thing I want to do is I want to lower case column names so in pandas we can access the columns using this dot columns attribute and that will return all of the column names and now I want to change the value of the column names right I want to take those columns and I want to lower case them so again I'm going to access the columns and now I'm going to use Str for string this will tell pandas I'm about to do something involving text right so uppercasing lowercasing those are all things that involve text so dot Str says hey text a method is about to happen and now it's very well named it's just lower um to lowercase them the other thing I want to do is I want to remove white space so this is going to be really similar I'm going to access the columns and I'm going to say we're going to modify this using a string or text method um and this one is called strip and that's just going to get rid of any white space that we don't want and now once we've done that each of our individual data frames should have the right column names but we'll double check by putting them all back together again and making sure that everything looks correct um so I'm just going to take our code for putting all of the data frames together and we are going to run that again after we've changed the column names make sure that everything looks good beautiful so now we've got just one set of column titles and we've got the right number of entries so we have a total of 29 000 entries in this data set that's rows in the data set and each of our columns has that number of values so that doesn't necessarily mean there's no missing data there could be something sneaky like if someone put in unknown or something like that as a value but at least there's no null data which means there's just nothing for that entry at all all right so I told you a lot of data science is data cleaning and it's true I promise we're going to get to some analytics in just a minute um but let's go through all of the best practices just to make sure that um we're doing things right so the last thing we're going to do is we're going to do some quick cleaning items on the the new full data frame um a couple of these are just good practices to follow so we're going to make sure that all of the text columns are are have the same case this is a really frequent thing that gets encoded incorrectly and we're going to strip any white space um from them because we want to make sure that that's not going to impact our analysis and those are pretty hard to spot in you know 29 000 rows so let's just take care of all of them um and then the last thing we're going to do is we're going to deal with something involving the numbers um the the numeric data and I'll talk about that in a moment um because I lost my connection I'm not sure what's going on there but let's work on this cleaning so once again I don't want to have to do all of this by hand so I'm going to write a for Loop the for Loop is going to tell python do this to all of my columns so I want to take each column of the data frame and I want python to do some cleaning on each column so remember I can access the columns using dot columns so I can say take each column in the list of columns and now we're going to do stuff with it so the first thing I want to do is I want to uppercase all text just to make sure everything's in the same case obviously not every column is a text column so we're going to have to deal with that as well but the code for actually uppercasing it is pretty straightforward right we would do the same thing we would access the column and then we're going to use that same Str dot in this case upper syntax so the first thing I need to do is correct my variable the actual data frame that we're working off of is our full data frame with all of the data from every single day and so now I'm going to take that data frame and I'm going to access the column these square brackets are just what we use to access columns or rows or specific sets of data within a data frame and then I'm going to say okay what do I want this column to be well I want it to be the same column right so I have this equal sign I want it to be the same column I just want everything to be uppercase now you might wonder why uppercase um no specific reason we just want them all to be the same and the other thing I want to do is I want to strip the text strip any white space so it's pretty much the same syntax so I'm going to copy and paste because that's how we get things done and I'm going to use dot strip now if I run this now I think we're going to get an error let's see we're gonna get an error phew um so the error here is an attribute error and what attribute is doing is it's referring to that str.upper and if you scroll all the way down it's going to tell you you can't use this string accessor so we can't use upper casing on things that aren't strings right on things that aren't text so what we're doing here is we're going over every single column which includes numeric columns and we are trying to upper case a number and python is saying I don't know how to uppercase a number so we need to fix this really quickly you know there's a few different ways to do this if you're super fancy pandas has a builtin like select D types method um for our purposes we can just check what the type is of the column before we actually call any of this code um so we do this in Python using something called if else syntax so this allows us to only execute code if we meet a particular requirement so in this case the requirement we want to meet is that the data type so dot dtype is the data type of the column is an object um object is how pandas stores test so that's the the technical name for it if that happens then we will perform this code if you're new to python the reason I just added a tab there is because python figures out what code belongs where by indentation so because I have indented here um that means that python now knows that all of this string code belongs to this statement and so it'll only try to uppercase things that are actually um are actually text we're really running into the live stream curse here of disconnection um all right I think that's fixed so now it should run and it did run and let's just double check that everything's looking good so I'm just going to print out the first five rows Railway delays boom ah I have a typo beautiful it's always hard to spot those typos I wrote Railways not Railway come on Otto get it together great everything's uppercase that's beautiful we haven't done anything to our numbers now the only other thing I notice in terms of cleaning is that these numbers have decimal points so we have like 1.0 2.0 these should be integers um and it's actually the same with the rest of these numbers they're showing up as floats um float is the the technical term for something that has a decimal but they're actually integers now this doesn't cause too many problems um but when you're working with a lot of data integers are easier to store and there's certain things about how python implements arithmetic that might be different um so we'd like to correct all of those um now we can do that just in the same code um by looking for only numeric data type columns and then changing that to integers so again in order to sort of zoom in on only the columns we're interested in what we can do is we can test the data type um so I'm going to copy and paste this code here so what I'm saying is take our data set access the column we're on check the data type and we're going to check if it's a float um there's some technical I should say there's some kind of technical things about testing numeric data types that are a little tricky uh that we don't have time to get to in this live stream for our purposes today this is this is good good enough um but um just something to be aware of so now if we've got a float which means we've got a decimal what I want to do is I want to take our column and what do I want I want it to be an integer so that just means it's a number that doesn't have that decimal point again this mostly has to do with um storage concerns and and how arithmetic is actually implemented than anything else now this is where it could all go wrong let's run this ah it all went right though we've got integers one two three four five instead of 1.0 2.0 3.0 storage concerns aside it just looks more elegant to me so I like it and we're finally done getting our data set together I know that's occasionally a little bit tedious but in all honesty this is a huge part of any data science job um so it's really important to know how to do um and it's also in my experience a part of a lot of data science interviews how would you clean this data set all right so let's look at some delay statistics um I'm actually going to just in the interest of time um we've got some sort of exploratory things what we're going to do is just start by looking at delays on holidays to see does it look like there's a significant difference in terms of delays on major holidays versus nonmajor holidays you know maybe more people are traveling maybe less people are traveling and maybe that impacts how well the trains run on time so um let's take our Railway delays full so this is our data set um and what we're going to do is we're going to ask python to group all of this data so in one group we're going to have all the major holiday rides in another group we're going to have all the other rides and then we're going to do calculations on those two groups so the way we do this in pandas is this method called Group by so split into groups and then we're telling it what do we want it to split into groups using we want the groups to be based on major holiday so what's going to happen from here is pandas will run through the data set and it'll look at each row and it'll say oh was this a major holiday or was it not and separate them into those groups based on that and then we can use that to perform a calculation so the calculation we're going to perform is we're going to calculate the average of our two delay columns um there's a bunch of different ways to do this just so you know so if you know a different way don't worry um I'm going to use a method called dot AG that stands for Aggregate and that's sort of a technical term for what we're doing when we take a bunch of data and compute something like the average um some people call that aggregation and what we need to feed into this aggregation method are the columns that we want to perform calculations on and the calculation we want to perform um so this takes the form of what's called a dictionary if you're not familiar with dictionaries don't worry about it too much basically what I do is I specify my column name arrival delay a colon and then I tell it what I want it to compute I wanted to compute an average which is also called a mean and so in pandas the the word used is mean and then I tell it hey I also want to look at the departure delay and I want to calculate the mean and now provided I've remembered all my syntax correctly we get a lovely table that breaks us down into two groups so major holiday we're broken down into it's not a major holiday or it is a major holiday and then we have both our arrival delay and our departure delay um and the thing I noticed right off the top is that these look really similar it doesn't look like there's much of a difference um 23.3 for arrival delay if it's uh not a major holiday 23.8 if it is that's not so much of a difference um we could certainly test using statistics to see if that that actually is statistically significant but I don't think anyone would really notice that difference in delay just like standing and waiting for the train so I feel like we can probably say chances are there's not that much of a difference now if we wanted to really verify things we might want to dig into this and say hey is are there fewer rides on holidays so do we not have the same robust quantity of data for both different groups right there's things we could do to examine this further but it looks like holidays don't change things too much for the highspeed rail network in China which I have to say is different than my experience here in the United States um another thing we could look at is the distance between stations so we could say hey if the if it's a longer trip more things could go wrong so there's more likelihood of delay um so to look into this the first thing I'm going to do is I'm going to take a look at the mileage column so again I need to access my data frame first oops and then using square brackets I can I can access a specific column in this case the mileage column and I'm going to use a builtin method in pandas called describe and this is going to tell me some statistics about the mileage column what I am especially interested in is the average so it looks like the average distance between stations is about 88 kilometers um and so what I'm going to do is I'm going to look at okay if we have above average versus below average length how does that impact the delays so it really similar to what we did with holidays where we said okay we want to group by whether it is or is it a major holiday we're going to do the same thing with whether it is or isn't a longer than average trip unfortunately in our data frame we don't have that data right if I do Railway delays full and I look at the first five rows with major holiday we had a column that told us this row belongs to a major holiday or it doesn't we don't have that in our data set um so we're going to have to add it to our data set um so this is one of the fun things about pandas is we can just add columns to our data set essentially willynilly um so I want to create a new column that's going to tell me is this a long distance versus a shorter distance trip now if I just ran this code now I would get an error because this isn't a column yet but I can still Define it to be a column and what I want the information I want is is the mileage value bigger than average or less than average so the first thing I need to do is I need to get that mileage value which I'll do the same way we've been doing it right I'll go into the data set and I'll access the mileage column and then I need to check is it greater than the average which is 88. and so what this code here will do is it will just check every single time is my mileage bigger than the average or not and let's just quickly preview to see how that works I swear I typed real way um amazing so we've got a long distance column now which I love um and all of these are false false false false false false which is good because all of these rows the mileage is below 88. let's actually preview a few more rows and see if we can grab some that are above 88 so I can add this sort of special parameter or argument to my preview code to get 10 rows instead of five and yes I've got a true here for long distance and it's awesome because that's 263 kilometers that's what I want to be happening um that's bigger than 88 that is a long distance trip so I'm very happy about that and now we can do exactly the same thing we did with major holiday we're going to split this data set into two groups is it long distance or short distance and we're going to calculate some averages um so I'm going to cheat and copy and paste the code because Y type more things that are absolutely necessary and it's one of the nice things is that a lot of this code ends up being the same right the only difference here is I want to group by long distance so I still want to calculate the averages of the arrival delay and the departure delay the only thing I'm changing is that I'm grouping by long distance and now let's see what happens so once again we've got a cool little table um that tells us was are we looking at the longer distance or shorter distance group and then what are our averages and here we do actually see a difference so we've got an average of about 16 minutes of delay if you are below that 88 kilometer average compared to 34 minutes of delay if you're above it so that's about twice um and a similar pattern for the departure delay so it does look like our hypothesis may be correct that if you're going for longer distance more things can go wrong you know you're more likely to potentially arrive um later or depart later of course arrival and departure delays are a little bit linked because if you arrive late you're probably departing late um so it's not surprising that we'd see similar patterns across those two um again there's statistical techniques that we could use to try to verify that this is the case um but uh we won't dig into that today um because we're coming I think pretty close to the end of the of the live stream um let's take a real quick look though at the impact of weather because I think this one's pretty interesting um so what we're going to do is import some weather data about each ride now because this is really similar to stuff we've been doing I'm just going to add the code in to import this weather data set um so we have weather data stored again in a CSV file we're going to import it to a weather data frame um and so you can see for each ride we know some stuff about the wind we know some stuff about the weather and we know some stuff about the temperature just for Best Practices we'll do the same sort of data cleaning steps that we did on the other data set which is to uppercase all our text to make sure we don't have any white space handle any incorrect data types um I'm just going to run that code um because we went over it in in detail before it's the exact same code um so we can see now we've got all our text is is now uniform our temperature is being displayed as integers um and it actually is integers in the original data set and so now what we want to do is we want to take this weather data and we want to take our Railway data we want to stick them together um and the way we're going to do this is that both of them have this ride ID column and so we're just going to take each row from the railway data frame and we're going to go over to the weather data frame and we're going to find that right ID and we're going to grab that weather data um so the Syntax for this is a little bit uh finicky but I'm gonna do my best so I'm going to call this Railway delays join weather um and the method in pandas for doing that sort of look up for saying oh I'm going to take the right ID over here in this data set and I'm going to look for the same right ID in the other data set the Syntax for that is it's called a merge um and so what we have to do is we have to give our two data sets so the first one is our Railway I cannot spell delays correctly in this live stream I don't know why and then we the next data frame is weather um and then we tell it what's the column to do the lookups so we're going to be looking up ride ID in both to try and match that data set that data um and we're going to use what's called an inner merge that just means we're only going to keep things where we have data in both data sets which merge to use is kind of a it's a complex question um that you can learn more about in the course um so let's make sure this worked let's take a quick look at the preview oh I'm so excited when things work um so yes we've now got weather data and trained data all in the same data set and so we can produce our very last thing before the end of this uh live stream so what we're going to do is grab Railway delays join weather so this is our big data set that has all of our data and we're going to do the same thing we're going to group by what type of weather it is so we're going to split into groups is it sunny is it snowy is there a blizzard um and we're going to again calculate the average this is the one thing I'll say for for pandas is as you add all these methods on it can get a little bit finicky but no worry um it just gets long we're going to calculate our average arrival delay and departure delays and then if I just did this we would get an invalid syntax note ah so dot AG is what's called a method that means it takes input all of this stuff is the input to dot AG it tells the aggregation what to calculate and so that means it needs parentheses so if I do just this we've got sort of everything's kind of out of order right we don't we don't know um what was the biggest delay or the shortest DeLay So I am also going to sort this um bye and save myself some room by entering I'm going to sort this by the arrival delay and I'm going to sort it um descending so from biggest to delay to smallest delay and there we have it um it does look like weather impacts this um blizzards are the biggest delay um Sunny is sort of near the middle which is kind of interesting and then we have actually things like downpour actually have the shortest delays um which looks a little bit confusing it's not exactly what we'd expect now maybe there's some things some other things going on impacting the data one thing I'll show which is the very last thing we'll do is that um we can take a look at the number of data points for each weather type so I'm adding in I'm counting how many rides are in each data type and if I do that I can see that some of these down here have less data so the fact that moderate to heavy rain has very little delay whereas if you look all the way up here moderate rain has more delay well that might be because we only have 30 rides here and our data set is just January so that restricts the weather um so that's all just to show you you can get some pretty quick insights from pandas and there's also so much more that you could dig into um the whole data set is also available from this project um so that has data for all of the other months um and that's the end of the project um I guess I will turn it over to Tori or Fede if there are questions that's me yeah hey thank you I'm back hello everybody uh yeah thank you for stopping sharing the screen that way everybody can see us both so we are in the Q a section of the live stream now so I just posted in the chat for everybody to go ahead start dropping your questions for ARA and I'm going to start with one of my own while we wait for everybody to chime in I wanted to ask you I know that data science is a very bad vast feel and it can be very overwhelming for beginners that are just trying maybe are curious about data but really don't know much about it so how do you feel is a good way to go about getting to the field learning about it and doing it in a way that feels like at a good Pace not super overwhelming because there's so many tools there's so many ways there's so many things right yeah there's there's so much um it can definitely be overwhelming and and there's so many things you need to learn sort of or it might feel you need to learn in order right um before you get to say if you want to do machine learning or you want to do something else um I think picking sort of one one programming language one sort of stack of tools is one way to start and to say hey I'm going to do python pandas right I'm going to like that's what I'm going to learn I'm not going to worry about should I be learning R should I be learning Julia I'm going to pick one um and then the other thing I would say is to um you know focus on how you can get actionable sort of Data Insights from data sets without doing things that are too fancy so sometimes it can feel like oh but I'm not doing I'm not creating a neural network like I'm not creating uh you know a fancy algorithm um but a lot of the times the the daytoday of working in data science is creating visualizations doing exploratory statistics doing that kind of data cleaning um and you can be pretty effective as a data analyst without knowing all the fancy stuff um so I guess don't be afraid to just start just jump in with data sets yeah just do it okay oh yeah just just going there work with the data I guess uh looking at the chat uh we have Richard asking how much time does it take on average to become very good as a data scientist who I mean it's a really tricky question because what does very good mean and like you know how much time are you devoting to it um you know I think yeah if you're sort of if you're working hard and you're focusing on it you know I don't know you're doing practicing an hour a day or something um you know you could get reasonably effective um if you're pretty focused in you know three months six months like the nice thing like we were talking about before is that there's like early milestones in data science so you know you could take a data visualization course and be able to produce meaningful visualizations within you know one or two months obviously becoming a very good and you know attaining that level is going to take more time um I don't know that I would call myself very good and I feel like I've been doing data science for a while so it's all kind of it's kind of a hard question to answer I feel like that happens to all the experts the more you learn about a field the less confident you feel about it like you go in very strong and then the more you know it's like the more you know that you don't know yeah so we have a related question in the chat from Matthew also asking about the influx of tech jobs seekers in the current market and wondering what are the things that they can do Beyond code academy to stand out and to kind of like be better or more competitive in the job market so what are some of maybe what are some of the tools like sites or things that people can do to go out there and explore more things related to data yeah so I think that um there's a couple different there's a couple different things that you know employers and other other folks are looking for Beyond sort of just knowing the code right so you're on code academy you learn how to do the code now what um and some of that is can you work on a team right um can you um you know can you not only sort of analyze a data set but do that extra bit of thinking of like oh here's what the business needs um and here's how I can go about trying to answer that question sort of that kind of work um unfortunately that's hard to demonstrate without a job I'm sure that's what people are thinking right now as I'm saying that they're like okay that's very nice but I don't have a data science job um you know if you can find people to do a group project with that's a great strategy um building up that portfolio as you know is always really huge um it's difficult to find open source projects on GitHub um but you know doing that kind of work to demonstrate that you can fit in with a group of people and contribute to a larger goal that's really important quick plug for codecademy docs where you can get started with GitHub contributions um right here nice another question here is about getting discouraged by errors and I think that this is probably very common in data especially because you run into a lot of trial and errors so do you have any advice for people that might you know feel like oh it's just so much that is always a bug something that doesn't run the results that don't work the way that you're expecting it's so frustrating so first of all I just want to say like I'm going to give like a very nice answer and all this sort of stuff but also just know that I have sat there with my vs code or Jupiter notebook open trying to tear my hair out um so I understand um you know the thing about errors is the the cliche is it's always an opportunity to learn um but it's really true so if there's an error and you don't know why it's happening right what that means is there's something you don't understand maybe it's something you don't understand about the data set right so maybe it's not a syntax error maybe it's just something's not coming out correctly um that means that's that's a sign right you don't know something about the data set or you don't know something about one of the methods that you're using and as data scientists right it's our responsibility to make sure that we know what's going on right so that when we get an answer it's not just like oh pandas told me that this was the average well I know how it's doing that computation and I know that really deeply so it's super frustrating but the errors are there to help us understand like oh I need to like learn a little bit more about this or I need to look into it a little bit more deeply it sounds like it comes to the territory yeah it just comes with the territory and like don't be embarrassed or ashamed like even I mean you saw I did some really silly stuff in this live stream um and like the reason I was able to correct those errors so quickly is that like seven years ago or however long ago when I was first starting out um I spent a lot of time being frustrated by those errors so it comes with time um yeah just be patient and uh don't be afraid to ask for help uh fair enough maybe on the Discord I'm going to yeah exactly go to the community so I'm going to combine these last two questions into one uh because both users are basically asking about the same thing and it's related to I guess how the field divides uh Specialties and things like there's somebody here asking you know is it better to go into data analytics before going to data science and there's another user asking what is the kind of data science specialization that you will you know you think that is the one that has the most Futures the one that is you know the better one to go into when you're trying to break into data science so I guess the question is you know out of all these you know Specialties within data science what uh how do you see beginners going into it yeah it's a really complicated world just because data science is still a relatively young like field right which statisticians by the way will be very mad at me for saying that because they're like oh it's just statistics we've been doing this forever but you know data science with the Advent of big data right which it wasn't even that long ago um was when data science really became a field and um you know so we're still figuring this out as an industry um I'll say that you know I think oftentimes people find the easiest way to break in is in the like business intelligence area um so that's someone who's mostly doing working in like SQL with databases or Excel or Tableau um you know trying to answer business questions and those sorts of things um often not doing as much in the form of you know sort of more advanced statistics or more advanced machine learning but I I feel like sometimes people look down on it but it's a it's it's a really important field and there's a lot of knowledge and expertise that goes into it but it is sometimes a little easier to break into because um sometimes you don't need to learn as much code um or as as much Advanced statistics and then you have the job and you can learn on the job um not as easy to find a job ever but um you know that's that's sort of what what I'm aware of in terms of where the field is going with Specialties obviously you know AI the AI machine learning landscape has been really shaken up over the last you know year I don't know if you've heard there's some you know each new products out there um I I some of it to be honest comes down to what you like to do I certainly think machine learning is going to be an Ever sort of growing field um you know it's also one that requires a lot of technical expertise um at least to do well um and so it is something that maybe you know you could learn on the job while doing more analytics focused stuff um and then there's the more statistic side of things um so you hear people talk about inference or causality um and those are people who are you know maybe a little bit more research focused a little more academic um trying to do um you know more advanced statistical analyzes um but uh a lot of data scientists will do a little bit of all of this right so many people with data science you know as opposed to like machine learning specific job titles will work with machine learning um yeah it's it's interesting how the landscape changes right like a few years ago all the rates was machine learning for selfdriving cars everybody wanted to get a job and oh how to make a car drive itself now everybody likes large language models and AI so it's still kind of like in the same field but it feels like a totally different thing right like people don't talk about that they all the one too much now but they you know so I definitely get that uh that sometimes there's a big swings in the market it's hard to know or you know if I'm learning now what is going to be you know hot in 12 months right when I'm ready to apply for jobs so it's pretty pretty challenging thing well thank you so much ARA for stopping by today and sharing your knowledge and the project with everybody I think it was awesome everybody really enjoyed it thank you to everybody that attended their live stream today I hope that you enjoyed it it was a good experience for you if you want to get more content like this if you want to get into it don't forget that you all you need is an email account to have a code academy account getting to those courses try them for yourself hope that you like it we put a lot of work and effort into the curriculum that we have at Academy we are very proud of it so hopefully you enjoyed it as much as we do making it also please don't forget to subscribe at YouTube follow us social media that's where we post a lot of good content we have an incredible blog post LinkedIn Facebook Twitter whatever you find us and also don't forget that we have a live stream for our third part of the three series of the evolution of code academy next Thursday same time 1 p.m Eastern and we'll see you all in our next live stream thank you all again thank you everybody and have a good weekend bye