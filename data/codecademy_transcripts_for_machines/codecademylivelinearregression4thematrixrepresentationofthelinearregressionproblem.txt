oh man i think we are live hello everyone um let us know in the chat if you can see us i think uh we should be live by now but it's always good to just see some confirmation we're good we have confirmation um all right so we're gonna get started today and um talk about some of the math behind linear regression but before we get started i want to introduce my uh cohost for the day natya she this is her first live stream so give her a warm welcome um she also yeah and she also wrote the article on codecadme on this topic so um it was one of her first her first content items it went live on the site so it's really exciting that she's here and she can also help us walk through everything um and i am live actually from the code academy office and i even have alex kuntz sitting behind me so it's almost like the pandemic is almost over maybe in this little bubble um cool so we're gonna get started um i'll share my screen and we can jump right into it all right so um all of this code just so you know and all of the math stuff is written out in a file that is on our github so there's going to be lots of notes today don't feel like you have to write everything down it's all available to you there um what i'm gonna do also is just make this a little bigger cool so um as i said today's focus is really on some of the math behind linear regression and just to make sure nobody feels intimidated by this we're not going to get into super complicated equations or anything like that what i mean by math is i want to cover some of the ways that we represent the linear regression problem using mathematical language or notation um and the reason why it's important is because whenever you use any sort of software to fit a regression or fit any model for that matter there's that function is doing something behind the scenes that you're not seeing and it's really hard to figure out what's going wrong if things go if things do go wrong which they often do in the real world it's really hard to figure out what's going on if you don't have at least some sense of what's happening when you press run and so today what we're going to do is we're going to dig in a little bit and see a little bit of what it looks like behind the scenes when we run a linear regression caveat is that depending on what package you're using what function you're using the way that this these coefficients are calculated might vary and that's actually part of your decision making as a data scientist or analyst is like how am i going to fit a model but we're going to focus on the stats model's implementation and in the process we're going to talk a little bit about matrices and how we can represent this problem as a matrix equation so as an example we're going to go through some some data where we need some data from street easy again if you've tuned in before you've probably seen us use this data in the past and it's got a bunch of information about apartments in new york city with rent and some information about the apartments themselves so the number of bedrooms bathrooms the size minutes the subway building age whether or not the apartment has a washer and dryer this is a subset of the original data but this is what we're going to work with today so we're going to imagine that we want to fit a regression to predict rent using all of these predictors cool so here's where we're going to return to some math and i'm actually i've got my ipod all set up so i'm going to switch over to the ipad so we can write this out um and and get started let's see if we can get this to work and i want to remind everyone that if you have any questions feel free to go ahead and post them in chat this is going to be a pretty detailed uh discussion that we're having today so if you have any any questions at all or you want to back up and start something over again just let me know and also lightning um okay so here is our uh our data i've pasted it here so that we can take a look at it and this data set has already the makings of what we would call a matrix right so for anyone who needs a refresher who hasn't seen this before a matrix is just basically like a rectangular grid of values um and those values can be anything and so we're going to represent this using a matrix as well um so let's remind ourselves what we're trying to do when we fit a linear regression so remember that our initial um our initial equation of a line was y equals mx plus b that's what we started with and so we wanted to fit an equation that looks something like this rent equals m times and then let's pick a a predictor i guess i think the most obvious one is size square feet so let's do size square feet plus b plus error and remember that this is the slope so we want to calculate the slope this is the intercept and then this is our error how far off the true rent is from our predicted rent in in the context of our line rate this is this is all we need to plot the regression line but this error term is here because our regression line doesn't perfectly predict all of the data points and what this what this equation actually represents is a whole bunch of equations one for every row in our data set so we've got for example 3600 and this is the first row of our data 3600 equals and then the psi square feet is 900 so m times 900 plus b plus error one and then we've got 3 900 equals m times 1000 plus b plus error two and we've got a bunch of equations like this each one with their own error term and that is how far off our prediction is based off of our model and our goal remember for ols regression is to minimize the sum of the square the squared errors so when we run a regression we want to try to find the value of m and b that works for all of these equations right so we want to find value of m and b such that the sum of the squared errors so error 1 squared plus error 2 squared oh my gosh can't even spell error 2 squared plus error 3 squared and so on all the way up to however many data points there are so that that sum is minimized and that's that's the whole regression problem so let's see if we can represent all of these equations using some matrices so i'm gonna erase everything and we're gonna start trying to build up this matrix all right so we've got our rents oh that is and we can put them in a single column matrix so 3 hundred first one thirty nine hundred twenty seven hundred forty nine hundred thirty nine hundred and then we want those to be equal to our slope times in this case all of the size square feet value so 900 1 900 12 16 1100 now we want to add so remember in our equation right it's like 3 600 equals m times all of these numbers plus b but we want the b's to kind of get added to every single equation so in order to do that but without writing the letter b a bunch of times what i'm going to do is i'm going to write it as the letter b times a bunch of ones so this is this term right here is the same as just writing out b b b b because for matrix multiplication if we multiply by a scalar then we just multiply that scalar by every single value in the matrix and then we've also got our errors over here but it turns out right so this is a pretty ugly way to write this out so we can actually make it a little bit simpler by combining this matrix and this matrix together so let us give that a try sorry guys seems to be not a racing we can combine the ones into this matrix and make it a 5 by or in this case 5 by 2 matrix all at once so we can put the ones in here and then we can sorry i don't know why i haven't been able to get this or we're using it to work properly and then we can combine the m and the b into their own matrix as well the reason that this works is because of how i'm sorry b and then i'm doing this wrong i'm sorry uh nitya do you want to catch my mistake or do you know what i'm doing wrong yes um it's it's bro multiplies column right right um yeah so it has to come um after the first matrix yeah all right now we've got it okay the reason this works is because of how matrix multiplication works so when we multiply matrices what we do is we row we multiply each row by a by the corresponding column in the multi in the second matrix so the first thing that we would do if we were multiplying these two matrices is multiply this row by this column and add each pair of terms so we would do right 1 times b plus 900 times m is our first set of our first equation that comes out of this matrix multiplication the second pair is the second row by this column so that's going to be equal to 1 times b plus 1000 times m and then we just continue down the line so for each row we're going to multiply by this column and add up the add up the terms so we end up with b plus 900 m for the first one because one times b is just b by itself and for the second one b plus a thousand times m and then b plus 12 16 times m and in every case we end up with this mx plus b format although it's written in reverse but it doesn't matter what order we multiply in okay this gets slightly more complicated but not too much more complicated when we switch over to multiple linear regression so remember in multiple linear regression we can have even more predictors and we can have even more and then because we have more predictors we have to have even more coefficients so i'm gonna just write this out but if we wanted to do a multiple linear regression with bedrooms and bathrooms and all of our other uh features in there as predictors we can just add them to this matrix so we can say okay i want bedrooms as a predictor so i'll add three three two one zero for um bedrooms and then bathrooms is two two one one um and then min to subway is four four four six three and so on um i can add all of my predictors i won't add them all right now to write it out and then i can have an entire column here column matrix here where i have all of my thetas so my b0 b1 b2 b3 and b4 and these are just the coefficients on each of these predictors when we fit the model and you can see that if we write this all out i know this is like getting very probably very boring at this point but i promise we'll get back to the the fun part in just a moment right if we multiply each of these out we get a separate equation for each row of our our original data that follows the pattern that we wanted we've got in our first row all of these values multiplied by all these values and so and then terms are added up so we get something like 1 times b 0 which is just b 0 plus 900 times b1 plus 3 times b2 plus 2 times b3 plus 4 times b4 and then plus error 1 at the end and this is our regression equation right 3600 equals all of this plus error is our regression equation for the first row of our data and so on um then we do the same thing for the second row cool hopefully i have not lost everyone at this point um we're gonna go back to to coding this up but i think just seeing this written out is useful so that you can build off of what you see in the code so when you stop did you want to add something just that b0 is the new intercept and like that the rest of the b ones too whatever are your new slopes exactly yeah so um right so when we talked about multiple linear regression we talked a little bit about this but it's worth coming back to because i think the notation can be a little bit confusing here um so yeah just like nichia said when we talk about multiple linear regression we rename the intercept b0 instead of b and then we instead of using m for our slope we use b1 through b whatever because that way we can use the same letter to represent all of the coefficients and just kind of number them cool so um let's go ahead and take a look at what this actually looks like when we fit a model so i'm going to go ahead and run this and here's our data again and you might remember this from when we talked about categorical predictors uh but this is the this patsy.d matrices what this does is it takes the step it looks like uh the screen share is freezing yeah uh or maybe the window is just like really short i don't know i just wanted to let me reshare screen this happened once before when i switched from the ipad to the yeah it looks perfect now okay great um cool so um so essentially right the patsy d matrixes the matrices function happens inside stats models uh from formula ols function and we can use the same the same formulas that we use in in that function in this to see a middle step that happens when we run a regression so i'm going to do this really quickly and i'm going to use this uh this equation so rent is our outcome variable and then building age years and min to subway has washer dryer our um predictors for right now because i just don't want it to take that the whole space so you can see it a little bit more easily but you could include all of the predictors in this model and we're going to return this as a data frame so that we can just see these labels and when we print this x matrix what we get is exactly what we saw in that matrix that we just created so we've got an intercept and then in order to fit the intercept we have this column of ones right because that's how we're going to get that beta0 or b0 or intercept whatever you want to call it in every single equation right so we can imagine that we've got our column of b0 b1 b2 over here and when we multiply this whole row by that column we're going to get a 1 times b 0 here for the first equation and a 1 times b 0 for the next equation and so on so that's why we have this column of ones and then we've got each of our predictors as a separate column in this matrix we also have um as part of this this function right we have this column vector of y's and we'll print them out and that's just our outcome variable that's the run so these are really the x's and y's that are from this equation in this case the y represents this whole column vector of outcome variables the x capital x represents this whole matrix that has a column of ones and then a column for each predictor and then what we're trying to estimate is this column of b03 through bm where m is the number of predictors so here's a better a simpler way to write it right so this is like this equation is the same as this equation just replacing this with a small y small x a b and an epsilon for the errors and so what we're calculating in the d matrices function is just this y and this x so we see that under the hood this regression equation is being represented essentially by a matrix and we saw that a little bit before we talked about um categorical predictors as well so now let's go ahead and let's take a look at um at the model summary oh i need to remind myself what we did um this was actually some very helpful uh code that nitia wrote so normally when we fit a model so i'm gonna just like jump back for a second so in the past we've done this like sm.ols got from formula i think it's like is that right you can grab the you can grab the code from one of the previous lessons yeah so we've got from formula and then we give it our formula and then our data okay and then when we print this out um so this is our that's our model and we're just gonna fit it on all one step and when we print out um model.summary we get this really big see did i miss something oh no oh i put this in quotes there we go okay so we get this like very big output that has a lot of information that we have to parse through and so um at the bottom of all this we've got so at the top we've got some information even about like the time that i ran this model and then some information about the model itself um and then we've got our coefficients which we can also get by just looking at the params attribute and then at the very bottom i've got some warnings and so we're going to look at in a in a moment we're going to look at an example where we get some more warnings but so far we just got standard errors assume that the covariance matrix of errors is correctly specified okay we'll come back um so here this is just some nice code nitia wrote it to print out just the parameters so just this part the coefficients for the intercept and each of these each of these predictors and then also print out the warning so that we can see it um just to demonstrate that we can do everything that stats models did behind the scenes um i think it's also useful to take a look at how we would take the matrix representation and actually solve this equation for the betas that or the i keep calling them betas but the the coefficients right the b0 b1 b2 etc so coming back to this equation remember that this was our simplified equation using the matrix form where y is our column matrix x is our matrix of predictors and this beta which corresponds to this whole column are the things that we want to we want to calculate such that the this set of error terms squared and added all together is minimized and we can calculate the the values of b0 b1 b2 etc that minimize the errors by using a little bit of calculus and the calculus that we can implement and i'm not going to go through the math it's a little bit complex but it gives us this fancy equation for the betas or the b0 b1 b2 etc and this is a lot of matrix algebra that you don't need to know for the purposes of this but this t represents the transpose of a matrix which is basically like flipping the rows and columns um so something that was in the first row is now in the first poll and something that was in the second rows now in the second column um and so off and so forth this is an inverse so we're taking like the transpose of our x matrix multiplying it by the original x matrix taking the inverse of the result and then multiplying it by the transpose of the x matrix and then multiplying that by our y matrix complicated math don't need to understand it for the purposes of this but it is kind of cool to see that that works so you can actually in um in numpy you can create so in this case this x matrix is really just it's the same thing that we got here we just got rid of all the labels and all the indices so it's just the ones the 15 896 all of that condensed down into just this like list of lists it looks like a list of lists basically but it's a um a grid of numbers just like the matrix we saw before and we can use our um we can use some functions in numpy to take the transpose of the matrix find the inverse of um of a matrix we can perform matrix multiplication in numpy and when we do all of that we implement that formula on our x matrix and i think i did not run this we end up getting this matrix of betas and notice that the first one is like 369 6.17 and that's the same as we got for the intercept then we've got negative 5.465 um negative 5.465 negative 25 and 719 and um and go back here negative 25 and 719. so we can actually calculate all of these things once we've written it as a set of matrices we can go back and we can actually perform that math or like use that equation to solve for these numbers okay so now let's go back and let's create some problems um so this is again some code from nitya going to create a new um a new column in our matrix or a new column in our data called seconds to subway and the way that we're going to get the seconds to the subway is we're going to multiply it multiply the minutes to subway column by 60. so actually let's like break this up into a couple of steps so i'm gonna run this and i'm gonna print this back out oh see i get so confused i don't know if anybody else has trouble with this there's all this uh hoopla about like how you are supposed to write onto a column of a matrix i don't know nitya if you have any advice i i have struggled with this every single time i thought that i managed to avoid the the error by doing it this way and clearly i have not oh man pandas um i wonder if we should just write our lambda function that's my usual i mean i think it works so like this gives me all of the all these errors so like yeah just so that everybody sees and then i think there it gives you this um this explainer which i always click on and then i never actually read through it um which at some point i will uh but it i like updated literally a year ago or something and it's just been hard to like keep track yeah um yeah like all i'm trying to do basically in the like simple way the old way that i would have done it is just said like take the create a new column called sex to subway i would just use the dot operator you can do it with like uh um you can do it this way as well right and then just have that set that equal to the min to subway column times 60 um and yeah that gives the same error it still works i think at some point they're going to take away that functionality and i'm never going to be able to figure out how to do it without a lambda function um anyway but what we created was this new uh this new column that tells us the number of seconds it takes to get to the subway so here if something took four minutes or an apartment was four minutes away from the subway it's 240 seconds away from the subway because four times 60 is 240 and if it's 6 minutes away it's 360 seconds away from the from the subway so nichia do you want to take this one do you want to explain why this is a problematic column sure i mean i think for it's kind of like a little um apparent right that we have the same information in both of these columns which is um time taken to the subway um and not only are these two uh columns highly correlated they're in fact collinear which is they're literally um they literally differ from each other by a constant factor right um and i think i think it's sort of apparent that we shouldn't be fitting both of these columns together because what would the model interpret would they both get the same coefficient uh but typically what what you will see the model will do is it will assign a coefficient that's very close to zero um to one of the columns um to like deal with the redundant information but we're going to see that this is going to have uh some mathematical problems um with our matrix algebra and yeah yeah exactly so um right in the same way when we talked about categorical variables and we talked about how um if you have three categories and you want to dummy code those variables then in your model you're only going to end up including two dummy coded variables because the third the third dummy coded variable doesn't provide any more information like you can figure out the values of that column in that column from the other two if you know there are exactly three categories and and we'll show that as well in a moment but in this example the seconds to the subway column and the minutes the subway column provide exactly the same information about an apartment so um i think we theoretically can fit this model um so let's fit the model uh that has minutes to subway and seconds the subway in it along with the other predictors that we had before um and i guess actually this let me look nicer on here if i don't use print i think um cool so uh let's take a look it looks like exactly what nachia said so we've got the coefficient or the slope on min to subway is really really small and then um the coefficient on seconds of subway is also pretty small um and we can compare that actually to our initial model so in our initial one we had a coefficient of negative 25 um on minutes subway which makes a lot more sense if our goal is to interpret this right like as an apartment gets farther away from the subway we probably are paying less money so um for every minute right we would interpret that as like for every minute further away it is um we're paying roughly 25 less in rent holding all other um all of their predictor is constant so now this original pretty sensible uh coefficient is now replaced by it's kind of like nonsensical we've got two negative coefficients they're like both much smaller and they're it seems like we can't really represent this information at all i wonder if the seconds to subway i think the second subway coefficient still makes sense in that i think there's like a factor of 60 there or whatever i guess oh yeah that sort of makes sense yeah it's still dangerous i mean that like yeah we shouldn't be doing this ideally yeah no you're totally right so what natia was saying which is which makes a lot of sense right is that like because the values in this column are going to be larger um we expect this coefficient to be smaller so if we multiply like 0.4 by 60 we'll get like 24 20 like this will be roughly 25 negative 25 which is similar to what um what our original coefficient was for minutes um so this is roughly the same relationship that we modeled in the first equation it's just that now we've got two things in the model where one of them basically has a coefficient of zero and all of the explanatory effect of uh minus the subway that we are representing in that first equation is now going to be attributed to this new column that we created um and then at the bottom we see there are some warnings so this warning was in our original model but now we get this additional warning that says the smallest eigenvalue is 1.18 times e to the negative 27. so this is really this is scientific notation so this number is like 0.0000 with 26 zeros and then one one eight um and then it says this might indicate that there are strong multi collinearity problems on or that the design matrix is singular um and do you wanna like talk a little bit about what this uh what their morning means um so actually it's interesting that um uh it actually when i was writing uh this uh article it was interesting to me that stats model just didn't like throw up an error and refuse to do the calculation because if you actually did paper and pen math um like we showed with numpy with the different matrices uh we're not going to be able to get an answer because um as it turns out when two columns or rows of a matrix um are exact multiples of each other uh that math doesn't work because uh it's what's called a singular matrix um it has eigenvalues that are very close to zero um and it just refuses to compute basically but i think the reason stats model still goes through with it is because because of scientific precision because uh with computers you can still find ways around this um but yeah so basically multicolor culinarity literally means that when it's to subway seconds to subway just differ by a factor of 60 when a matrix has two almost identical columns or columns that are multiples of each other it's determinant doesn't exist or it's a singular matrix so we can't calculate those correlations um yeah yeah i'm trying to grab uh grab this code to do exactly what you said um i think actually when i was getting ready for this um this live stream and i was trying to run this with numpy i think it actually calculated it for me and numpy but it yeah i still think it's got to do with floating point precision and i think that's why um this is so important to keep in mind because i think it's not going to throw an error now but if this is part of your larger calculation and you keep to go keep going with it um it's going to cause serious problems to be able to catch that error is so useful right we'll give it a try so um if we take this oops i'm messing up this uh this whole thing um i think this is what i was okay so we've got now we're recalculating um our x matrix with this equation that has seconds to subway in it let's wash our dryer then we're gonna um we're gonna have to switch this to a numpy array so that it's a matrix and then so we can use numpy functions on it and then um print the betas call it seconds or something okay right we it does do this calculation but we get completely different numbers um even more nonsensical than before because all of these are multiplied by 10 to the like 20th or something like that so we end up with like this is negative 4 bazillion i have no idea what that would be it would be like four one one seven all of these numbers and then like whatever 15 more zeros or 13 more zeros um on the end of that and so we end up getting this really weird thing where uh we can't calculate like nichia said we can't calculate these coefficients if we and if we have um a situation where two of the predictors are contributing exactly the same information um i saw there's a question in the chat in theory would that cause problems when finding the coefficients but there happens to be a strong correlation unknown to the user between two predictors so that's a actually really good question so the answer is yes um so if you have a perfect collinearity between two basically a perfect correlation equal to one between two predictors then you can't even fit the model but if you have two predictors that are highly correlated it then you also are going to encounter some issues especially around interpreting coefficients because if two predictors are contributing mostly the same information even if it's not exactly the same information it won't prevent you from from calculating the inverse of a matrix so it won't prevent you from finding a solution theoretically to that matrix equation but it will mean that it kind of becomes arbitrary which predictor the coefficient gets linked to um so you might have a predictor in your model that has a coefficient that's really small maybe even smaller than this but it's actually a an important predictor of the outcome variable and um like right now nitia is working on some content on feature engineering which is really the process of trying to choose a subset of features that you might want to include in a model and one of the issues or like one of the things you might do right is you might fit a model with all the predictors and then try to figure out which ones are the most important ones based on these coefficients because they tell you something about how much um effect i i shy away from using the word effect but like how much impact on the outcome variable each predictor has um but if you have two highly correlated predictors then you might have one of them with a coefficient of zero even though it's really highly related to the outcome um also yeah so c of j says when features are linearly dependent we are in the situation similar to divide by zero in one dimension just my two cents and that is exactly correct like there we just can't calculate it like you can't divide by zero you can't calculate the inverse of a matrix when you have colinearity perfect linear colinearity um so this comes back to all comes back to another thing that's worthwhile to do before you fit these models is um it's it's helpful to look at the correlation matrix to see how highly correlated each of the predictors are so let's actually um go ahead and i'm gonna like create a new column and then i think we can do um did i load seaborn see if i can remember how to do this but if i do sns.key map of bk dot forum i think that might work no let's um let's get the correlation matrix first is it just one r maybe no it seems to be two r's um let's get this is this is fun um heat map function let's try that um do you have any chance to remember nitia how to do this i think you might need to close the i have a the the syntax the closed parenthesis after that i think uh yeah i think so oh yeah nice um okay so this just helps us visualize this a little bit better but we see the coloring is not great in this so perfect correlation would be this like very light color right and everything is perfectly correlated with itself but also min to subway and seconds to subway are perfectly correlated um but this gives you a sense of how highly some of the other features are correlated i think if i do like endnote equals true it'll give me the actual numbers on top um but we see right that some of these other ones are pretty highly correlated like uh size square feet and um and rent are pretty highly correlated in that case rent is our outcome variable so that's not actually that's actually a good thing means it's a really good predictor probably a very good predictor in this model for rent um but some of these other ones like size square feet i guess the next biggest one is size square feet and bedrooms um which makes sense like something that a apartment that has more bedrooms is probably also going to be bigger something like that has a reasonably large correlation where we might be a little concerned i think 0.68 is okay um but maybe like over i think once it gets over about like .75 for me is where i start to think like maybe these two predictors should be in the model depending on depending on what i'm trying to um trying to fit a model for cool um okay we've got about five more minutes i think another um another useful thing to just see that no tia also covered in that article um and i highly recommend that you all check out the article as well if you're interested um is just to demonstrate what happens if you instead uh if you instead use a categorical variable where you have like two columns or enough columns that there is no reference category so in this case we can create another another column in our data along the same lines as before but this column instead tells us whether there is no washer dryer so remember in our original data we had um washer has washer dryer as a as a variable as a feature and it was one if there is a washer and dryer and zero if there's not and if we create a separate column called no washer dryer um then we essentially have replicated that same information kind of in reverse so i'll just print this out really fast so we can see it it'll give me the warning but it'll do what i want anyway um so if something has a washer dryer or it'll be a one in this column and it'll be a zero otherwise all of these have the value zero for house washer dryer um and so it's true therefore that they have no washer dryer um and then let's see if we can find like um it's like print the whole thing this is what i like about uh this okay so we've got um here we've got has washer dryers equal to one because it has a washer dryer and so this column says false because it's false that there's no washer dryer um so basically right like everything every value in this column could be determined by the value of this column so it's almost like the same thing as before um you can't calculate the correlation between two categorical variables but they are not once again there is multicollinearity in that like all of the information in this column is also contained in this column and again if we use that to fit a model so here we're fitting this model um we get some warnings we get this warning that the smallest eigenvalue is super small this might indicate there's strong collinear um that there are strong multicollinearity problems or that the design matrix is singular um and all we did here was we included both has washer dryer and no washer dryer into our model cool um cool so i hope that this like helped motivate a little bit why understanding this matrix representation is important um i think another thing to demonstrate really quickly or that i think is worthwhile to demonstrate really quickly is a lot of people prefer to run linear regressions or any sort of model fitting in scikitlearn because scikitlearn has a lot of other functionality that sats models does not although i find it's harder to look at the output and really understand what's going on um but for example if you want to fit a model in second learn um it's helpful to know that this like matrix representation is something that you need to kind of do yourself so for example um when we fit a model with squirn uh and this is the code to do it in this uh in this jupyter notebook that you can download on your own um there is some info so we're going to have to create this like x and y matrix by ourself um and so if we do that we end up right like taking the columns that we want out of the original data and and putting them in their own matrix and then separating out the y matrix as well in this case that's just the rent column and then we fit the model um but then scleron is automatically adding that um that column of ones behind the scenes so we are not adding that here um ourselves but there are other things that we do have to add so like if we have a um if we have three categories in a categorical variable and we want um to choose our reference category we're gonna have to like put those as columns in our x matrix and leave one out intentionally it's not going to be done automatically for us um so i think it's it's helpful to think about it in this context so that you can think about like what is happening in this library that's not happening or happening behind the scenes in this library and it can enable you therefore to learn a new technology like or learn how to fit a model in r for example and debug that so you get a little bit more insight into what's going on cool um awesome well we are just about out of time i know this was maybe a less exciting uh topic than usual but i hope that it was helpful at least to see a little bit of this math um and i promise the rest of this will be a little less snappy in um in the uh in the future i see there's a um never do math in a youtube video rule maybe that was maybe we should have followed that rule i don't know um but oh well we didn't so hopefully it didn't scare anyone away it is i think helpful to see um awesome oh this is great so there was a question in the chat about office hours this week um so there are going to be office hours this week but we're going to do them on discord for the first time ever so that is going to involve um that we're gonna have a like a chat where you can type and then we're gonna also have uh video and sound available and so if you have not already joined our discord server we will we'll put that information somewhere we'll put it in the youtube description and also the event which you can find at codecademy.com events oh there's not an event never mind it's not going to be there um we'll put it in the the youtube description so you can find it but come join us on discord happy to answer any questions we also have a chat there you can post questions in there at any time um and you can also post questions on the youtube video directly and we'll keep an eye on it and try to make sure that we answer things awesome all right thank you all i hope everyone has a great rest of their day and thank you for joining me on this math adventure thank you this was so fun