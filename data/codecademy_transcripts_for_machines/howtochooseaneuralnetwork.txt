Hello everyone, my name is Zach, I'm a machine learning engineer based in Seattle, Washington, and I'm especially passionate about applications of deep learning for natural language processing and natural language generation. In this video, I'm going to talk about how we choose the best neural network for a given task. Let's jump right in. The first big question here is should we actually be using a deep learningbased approach to begin with? It's important to remember that neural networks are just one of many different machinelearning algorithms and that they do have their tradeoffs. Let's quickly touch on how we choose whether or not to use deep learning. This largely comes down to two big conditions do we have enough data? And is this task sufficiently hard enough to warrant deep learning? Now to be performant, deep learning approaches require much more data than more conventional machine learning approaches. In fact, neural networks usually require hundreds to even thousands of training examples. So if we don't actually have very much data for our task, we probably can't leverage deep learning. More conventional approaches will likely beat a neural network, which could overfit to a small data set. Another critical consideration is is this a hard task that warrants using a neural network? Neural networks are incredibly powerful tools, so powerful that applying these tools to easier machinelearning tasks is often likened to using a sledgehammer to crack a nut by people in the machinelearning community. Now by easy task here, I mean tasks with either few input features or tasks based on features with a simple structure. For example, neural networks are our best shot at classifying the time of day from images but definitely an overthetop approach for classifying the time of day from just temperature and humidity data. Why is it bad to overuse deep learning? Well for one, deep learning models can take longer to train and finetune. Additionally, with deep learning, we gain power but we sacrifice interpretability so it becomes hard to understand our model's decisions. Okay, so let's say we have enough data and we've deemed this task a sufficiently hard one, it's time to design our neural network. Now, there are two critical decisions that we have to make what will our loss function be and what layers will we use in our network? First, let's talk about loss. Here we need to ask, what is our model's intended output? Do we want to generate continuous outputs? In other words, regression or instead, do we want to label our data categorically? This is called classification. For example, predicting medical costs or raw acceptance rates is regression. In these cases, we usually use mean squared error as our loss. In contrast, in classification tasks, like when classifying galaxies or lung tissue and xrays, we use crossentropy loss. As a quick aside, there are definitely other deep learning use cases that call for specific loss functions, and these tasks don't really fall into regression or classification. However, these cases, like for example, GANs and reinforced learning, represent rare applications of deep learning. Okay, so as an extra aside, if our task is a classification task, we should ask a few followup questions. Is our dataset imbalanced? Meaning, that does each class not occur an equal amount? If so, do we care more about predicting the minority class? This is the case when we are building our models to detect something like credit card fraud or diagnose a rare medical condition. If both of these are true, we should consider adding class weights to our loss at training time. We can use these ways to scale up the cost of making a mistake on the minority class. Now, we have to remember that our models could be making lifealtering decisions. For example, it is so much more important for our model to identify any possible cancer risk in a patient than it is for our model to correctly rule cancer out. Using a weighted loss is one of the ways that helps us start to account for these realworld implications of our models. Okay, so now we have chosen our loss function, it's time to talk about which layers we should build into our model. How do we decide which architecture to use? This really depends on the structure of our data. What does our input look like? Let's consider some of the main possibilities. Very often our data comes from tables of numerical data, raw numbers, and categorical data, data divided into categories. For example, in the implementing neural networks lesson, we used age, a numerical feature, and region categorical to predict medical costs. When using tabular data we should usually construct our model entirely out of feedforward dense layers. Feedforward layers learn linear combinations of input features, and these layers are incredibly powerful, especially when we can't make many assumptions but how the underlying input features should relate to each other. However, as we increase the number of input features, we need to dramatically increase number of parameters in a dense layer. That's a great segue into another common type of data, images. Here, our features are coming from raw pixels. Because there are many pixels in a given image, we usually cannot directly pass image data to feedforward layers. The resulting network would have too many parameters for learning to be stable. Instead, we use convolutional layers. These layers hinge upon two fundamental ideas. One, that local relationships between nearby features matter more, and two, that we should look for the same patterns in different parts of the image. By convolving smaller weight matrices called filters, convolutional layers allow us to adequately process image data, while learning a tractable number of parameters. Once we have used these convolutional layers and max pulling layers to further reduce the dimensional layer input then we can use dense layers in our model. Another common type of data is sequential data, data where the sequence of features is meaningful. For example, text data, music, or time series data like from the stock market. Similarly to how CNNs rely upon properties of images, recurrent neural networks, referred to as RNNs, use fewer parameters by processing data one step at a time, one time step at a time, with the same weights. When processing sequential data, we can use one or two RNNs as the first layers in our network. Now the most popular type of RNN is called an LSTM. That's a long shortterm memory network. At every time step, LSTMs use the same weights to update a hidden state and output, which are then passed on to the next time step inside of the layer. Now, it's worth noting that there are a few other alternative options when it comes to sequential data. The three main possibilities for working with sequential data are RNNs, gated convolutional neural networks, and transformers. Now, each layer type is best applied in specific circumstances. RNNs are an allaround very performant layer, especially when working with long or unbounded sequences of data. When working with a sequential dataset, it's often good to try these first. Gated convolutional neural networks are a version of CNNs that train very quickly but usually require sequences to be a fixed length. Lastly, transformers are great for building large powerful models, as long as you have the data. In fact, the state of the art on many sequential tasks, especially on textrelated tasks, is transformerbased. These models work best with shorterlength sequences. Now in many real use cases, our datasets contain many different types of data. We call this multimodal data. For example, some of our data can be coming from tables, while the rest can be images or text. Consider the task of diagnosing a patient from lab reports, patient summaries, and imaging. Now in this case, right, working with this multimodal data, we can encode our different data sources using dataspecific layers. We can use dense layers for our tabular lab results, convolutional layers for our scans, and RNNs for our text data. We can then concatenate these outputs together and pass them onto dense layers. Now remember, to benefit from deeper networks, we need to apply nonlinear activation functions between our different layers, otherwise, our model is no different from one with a single layer. Usually, we use reLU as our activation between intermediate layers. However, other options include sigmoid, tanh, and some of the reLU variants. Special consideration needs to be given to our output layers' activation function. If we are doing classification, then we probably should use the softmax function to normalize the output of our model into probabilities being 0 and 1. Alternatively, if we are doing regression, we usually want to use a linear activation, in other words, no activation. This allows our model to generate all possible real valued outputs. Okay, now when picking your architecture, it's always worth exploring the neural networks that have already been applied to similar tasks. For one, this can inform your hyperparameters and the actual architecture of your network. But additionally, similar work can provide you with pretrained models. These are usually large models trained on large datasets. Now, once these models have been pretrained, they can then be finetuned on smaller datasets. This allows us to use these pretrained models as rich feature extractors, which we can adapt to our own tasks. We can either finetune the entire pretrained model, retrain it with a small learning rate, or just finetune the last few layers. Using pretrained models works incredibly well, especially on text and image data. Let's consider an example, image classification. There are a few large convolutional neural networks that are accepted as standards in the computer vision community. These models can be trained, for example, on imagenet, an image asset containing labeled images of millions of objects. From there, these large models can be finetuned on any imagerelated task, from classifying food to parsing satellite data. Okay, wow we've covered a lot of material in this video. Let's retrace some of our steps. We started with whether or not we should use deep learning to begin with, then we talk about how to pick the most suitable loss function for our task. After that, we talked about how we can choose layers based on our data types, and what sort of nonlinearities we should add between them. We also talked about how we can finetune models that are pretrained on larger datasets for use on smaller ones. Of course, all the things we discussed in this video are general, flexible guidelines. I want to emphasize there are no hard and fast rules for determining the best architecture or the right hyperparameters, for that matter. At the end of the day, the absolute best way to select a neural network architecture is really via experimentation with your own data. Beyond that, I highly recommend reading papers and blog posts so you can incorporate ideas from other researchers and use those as jumpingoff points for your own work. All right, that's it. Best of luck tackling your own tasks with your own architectures. May the gradients be ever in your favor!