all right everybody thank you so much for stopping by I'm fet your host for today and I'm here with wailing you and we are super excited to bring you a practical introduction to llms we are you know trying to be in Trend with this there's a lot of talk about Ai and gen Ai and we thought that will bring you a little bit of something here to learn alongside you so hello way how you doing today hey B good how are you I'm doing fantastic are you ready to go yep ready to go all right take it away all right hello everyone my name is Wayan tuya and I'm a senior data scientist at code academy and today we'll be talking about a practical introduction to llms so um we're going to start this off pretty simple um with what is artificial intelligence so you know we've heard artificial intelligence as a word thrown around um and I just kind of want to clear this up but it's basically a machine's ability to perform the cognitive functions that we associate with human Minds um and artificial intelligence as a practice uh encompasses machine learning and deep learning uh within it so it's been around since the 1950s although recently as we all know this has been a huge Resurgence of it um and most of the breakthroughs that we've seen today is actually thanks to deep learning So within this very uh smaller scope within artificial intelligence that's really pushing the field forward um so going back to you know artificial intelligence AI I want to talk about a bit about machine learning I'm just going to assume um no prior knowledge of data and machine learning and whatsoever so just kind of give you a brief overview of what machine learning is um and in this diagram on the right it's kind of uh showing you what machine learning in action so it's a machine learning is as simple as the linear regression we might have learned in school and essentially it needs some data points uh some computing power and a mathmatical mathematical formula that you're trying to optimize so on in this GIF to the right you can see that we have our set of data points and uh using um the Computing you know our computers and and uh our form our formula which is basically trying to minimize the distance between data points and the line which is the model we can with many iterations start approximating um that those little dots with our model which is a line and you know you do this long enough and you get uh pretty good impress pretty impressive results and you can you have a model that can predict the future essentially or predict things um and so going back to that diagram with within machine learning there is deep learning which is um basically a subset of machine learning and it's to do with neuron networks so what is Neuron networks what is deep learning it's a discipline within AI that teaches computers to process data in a way that is inspired by the human brain so on the right here you can see you know tons of layers um and that is essentially a NE Network and this uh you can see the the hidden layers within them those are U what makes a neuron Network deep if there are a lot of hidden layers it's a deep neon network if there aren't that many then it's a a shallow neon Network and anytime you hear about modern you know Advanced capabilities of AI it's because we have uh a lot of the a lot of research and compute power that we can run these deep learning uh deep neural networks and um essentially that's what's driving AI forward right now um so now that we know you know what neuro networks are um I want to kind of touch briefly on how we can teach neon networks how to learn English so English I'm going to call it uh so that's natural language um it's basically you know a natural language is any language that we speak as humans but how do we teach an a neuron Network to learn a natural language uh computers are can only process numbers and they're very good at them but um you know that numbers are not the same as natural language so uh we have to find a way to transcribe or uh to decod encode and decode code numbers into language and vice versa so going back um a vector uh so computers are very good at working with vectors and the vectors can be uh in this example you know 0.2 Nega 5.36 a series of numbers right and in order to for them to understand English we have to turn them um into um basically turn English into numbers so what if we say one is hello and 001 is goodbye that essentially could work if we have um a way to to store that information and that mapping right so that is a very basic example of uh an embedding uh so anytime anytime you hear the word embedding within the NLP the natural language processing and uh llm field then just think of transcri um turning a set of uh numbers into English language or natural language or vice versa so this example that I gave was actually kind of simple we can actually do a lot better um one to and 001 is very like arbitrary I just pick those numbers on a whim but uh on this example to the right we can see there is word Toc which is one of the most famous techniques um which was developed I think in mid 20 maybe 2013 14 um but essentially you can see that uh in this we have this space and we have uh map uh man woman king and queen right in this space and you see that there is some structure to that data it's not arbitrarily spaced out um if we take the distance from uh man and woman to king and queen is actually the same distance so it's actually helpful to structure your embeddings in a certain way that makes actual sense to you know someone looking at it and Al and also to the machine because essentially um that structure get does help a machine understand uh language better so now that we've talked about a little bit about embeddings and you know General um uh deep learning we can talk about the rise of large language models uh so since before 2017 there were large language models but they did not exist as we know them today um really this all started with a paper uh called attention is all you need uh that is a paper by Google in 2017 where they describe a transform architecture and essentially what a transform architecture is is I was talking about new networks and how there are many layers in between them a Transformer is basically one of those layers or um I guess a set it could be a set of layers within that neuron Network and the architecture this was pretty novel back in the day allows llms um to kind of pay attention to the entire question at hand instead of um maybe to certain parts um I guess and allows basically this attention mechanism kind of like what we uh know as attention to um for so allows the llm to basically look at each part of the question that you provided and then kind of really understand what it's talking about so this kicked off uh family of models gbt B um now more recently llama 2 llama llama 2 um and fast forward to today we have even more models so um Claude Bloom Falcon all these models that are open source uh so it's super cool to see um this explosion of models in this domain so now I want to kind of talk about how you know given all these models and all this um all these new exciting uh tools and apps and models out there what can we do Beyond chat gbt we've um uh you know we've kind of messaged uh so we've kind of touched upon uh Chad gbt I'm sure a lot of you have used it and you know just go on type a message and get a response but kind of keeps you in the box right like what if we want to do more with with the API um what more can we do so I want to talk about um basically Chad gbt in this case is a closed Source model um and you know of course it's owned by open AI um and on the other side we have this open source um models the the llamas the uh the blooms or sorry the clouds the Falcons that we can U actually tweak a lot and actually um you know do use it and host it ourselves right so what are the strengths of having you and using these open source models well first of all you don't need to send data anywhere outside of your organization you can um actually use all that data within the machine you're hosting it on uh you're not dependent on third party uh changes so if opening ey changes something llama 2 for example if you're using it you know if you have your own setup it doesn't change they can be more customizable so there are tons of tools out there um by the open source communities that you can leverage to you uh these days so it's that's kind of cool and you get more FX uh freedom and flexibility when you use um Lama Tu for example um so one one use case is if you want to use um an open source llm model to write like a horror film script right so if you use LL 2 you might get um it's easier to write uh to get the details versus if you use open AI they might say oh this is too graphic like we can't generate a script like this because it's against their policy so um you know if you're a filmmaker maybe you can get some benefits of using a open source model so now on the other side what are the strengths of openi and closed Source models so if we one good thing is that opena manages a lot of the infrastructure in backend so um it's very easy to use and GPT 3.54 would actually perform much better than your open source models this is because they're running it in the back end in huge data centers and versus if you're running an open source model you're probably running on a smaller computer um You probably don't have as much budget and for what it's worth uh using GPT it's very cheap compared to um the amount of power and uh that you're getting for uh basically a lot of bang for your buck because you're getting a better model and a lot of times it can be cheaper than open source um and you know they say they don't train on your API data which is it's you know it's it's good that they're um they claim that um and however of course if you you're working with really secure data you you don't even want to take that chance of sending your data to someone else so uh pros and cons if you're but essentially if you're you know if your use case is um pretty General and and basic I would say stick to um open AI gbt because of its ease of use its cheap prices and the popup performance but if you have something more um do you want to keep private or you want more customizable then um and open source llm like llama 2 might be the way to go so you know I've been talking about llama 2 and that's because it's one of the best open source models out there that's commercially available uh to the public so basically a compan most companies um Can leverage something like that and use it for themselves um uh it has a bunch of sizes of uh so it comes with a bunch of parameters so from all the way down from 7 billion parameters to 70 70 billion parameters um however it's still not as good and as big as gbt 3.5 which is 175 billion parameters um but it is one of the best options you have if you want to host something locally and you can see um in the open source and closed source marks to the right um that llama 2 performs probably at the top at the of the open source uh models but still has some ways to go uh when compared to GB3 gbt 3.5 and gbt 4 another tool that um I would highly encourage uh y'all to check out if you're interested in llms is hug and face so it's basically a GitHub of models and it allows users to save models on there to share ideas and collaborate better and it's heavily used by the open source people so it's a great way to um you know if you have a model you want to try it out uh it's a great way to just go and hug in face download it um try it out um they have a lot of other features out there um and they have tons of models that you can play around with so that's just another if you're get into this um something worth in and techniques for customizing so now that we've talked about kind of what is AI how it generally works and um the landscape out there I kind of want to touch upon um how we can customize these models to to do our bidding better right um so why would we even want to customize LMS in the first place so chat TBT is great but it's training data ends around September 2021 so it doesn't know anything beyond that point right and so um if you ask it a question about recent event it's not going to know and so you're G to you you want to find some way around that right the good thing is the hard work is done for us uh training gb4 costs a lot of money and with comparatively small incremental changes we can actually um tweak gb4 or any of these open source L models to um do our bidding so uh there are two main ways the first one is called retrial augmented generation the second one is fine tuning um retrieval augmented generation also known as rag is probably the first way uh you always want to consider because it's a simpler approach here we can see kind of um a diagram on the right of what uh retrieve augmented generation is and essentially um when a user asks a question uh you know on the right it's the query arrow pointing uh um the the qu the query first comes to a system where it's basically routed to um a set a data set right so basically you have an llm imagine an llm on the side and you have uh your own data set your own propr proprietary data set domain um on you know kind of on the side right and whenever a user asks a question instead of asking the llm directly so say I have a billing question right and and I'm a working in a customer service department if I have a user ask ask a question it can go to Bas LM but the Bas LM will be like well I don't know what our policies are um and you know it can give a very generic answer so instead we can have that question get routed uh to a specific uh domain a data set first and we can compare that question so if it's a question about billing we can get hey uh the most you know top five most recent most similar questions about billing in in our data set so these are the problems that other users have dealt with and like these these are how they've been solved and then we can get because uh so we do a similarity comparison and we can get the top five and we can feed then feed the the query and the relevant documents to the Bas LM so we're basically saying Hey gbt or llama 2 um based on these you know five similar queries where the user had a problem with their billing how would you solve this uh new problem uh and then that's the query of the user and then it'll generate a much better response because uh then it can it has a context to say oh this is what happened in the past and this is probably what should happen uh now and then that response gets forwarded back to the user so that's retrieval augment generation you're augmenting um you're basically retrieving data documents and you're using those documents to augment uh the generation of text uh using your llm um so now we can talk about Vector databases um going back to um what we're to our example here where we are talking about a domain specific data set that's actually um a vector database and so when we talk about rag basically um we need somewhere to store all our data right and that's why um Vector databases exists so if you're familiar with relational databases which is SQL uh they store tables relational uh Vector databases store vectors and they make it very easy for us to compare certain vectors because they're all um it's optimized to do that right and so in this uh picture on the right we can see you know we can see kind of a word cloud and we can see certain words that are clustered together so this is essentially what we're envisioning um is an embedding and so you can see that you know you look for software uh and then you can see uh Excel Oracle web Microsoft all in that cloud right they're all very close together because they're all related to this software um realm I guess and so um using Vector databases we can really easily find vectors that are similar uh and these vectors can be you know just single words or entire queries um and so uh some popular ones that we can use if you're interested are chroma DB and pinec con which are one of the most those two I feel like are the most popular ones but there are tons out there uh that um are available to try out as well um now we can talk about supervised fine tuning so we talked about uh retrieval argumented generation which is you don't touch the model you just basically feed it additional context based on some other data um but what if we want to do something more what if you want to incorporate your actual data into the model itself so that's you know in some cases that's better because the model then actually you know internalize all that data and like learn from that better so um it's you have one uh you know smarter model than rather than like a pieces of um you know a whole system right and so it's very Sim to find tune something it's very similar uh to uh how you you train a machine learning model you're basically giving it data set so a data set so in this um example we have um an instruction on the right and an output on the left and given a set of instructions you're going to tell it to Output a certain um I guess data in a certain way right so you have you feeded this data and um over time you know given a certain number of iterations and and after process all that data it will have um kind of learned if you did it properly it will it will slowly learn um and be finetuned towards your use case and so libraries on hugging space which I bring up again because they have a lot of libraries um that are useful for um for this and so they're great ways to to um play to find two models and you can um play around with ways that are I I haven't even talked about here but like um it's they're both they're all like uh it's it's super useful and you should check them out basically um and when we talk about fine tuning there's there's uh full fine tuning and there's Laura fine tuning um as well as a bunch of other ways um full fine tuning essentially is um you're like I said before you're training you're training the entire model um again with your data I guess enhancing it with your data but it is expensive because you have to train a lot of parameters and it's infeasible for most people um because that's it gets expensive and it also there risks associated with it like cat just profic forgetting where you're you train a bunch of things but it'll forget some other things if you don't do it properly so here we're going to touch upon luras uh low rank adapters um essentially what they are is it uses a trick so uh let me just talk about this trick first and if you're familiar with linear algebra you might have seen this trick done before but it uses decomposition which is basically if you have a huge Matrix of say A Million numbers a thousand by a thousand Matrix right so a thousand rows th000 columns you multiply those two uh uh numbers together you get a million numbers so that's a Million numbers you have to keep in memory to um remember right uh Laura basically is saying uh is uses a trick where you can actually approximate those Million numbers with two smaller skinny matrices so if you take two a th000 by5 matrices um you can actually you know I say a th000 by five in this case but you can actually make that five 10 or 20 but and that just kind of tweaks how much you're approximating your um your original Matrix but if you take these a th by five matrices and you multiply them together you can actually get uh a million um numbers but you're actually storing them as 10,000 numbers and so this is a cool trick because you're storing 10,000 numbers now versus um you're storing you were storing a Million numbers before so that's an improvement of like a comparison of uh like guess you're now only storing 1% of that data versus a million uh which 100% of that data um so Lowa uses this trick um it freezes whenever you do low you're basically freezing your original llm model so let's say we have let's say we have llama 2 we're taking all our parameters of llama 2 and we're freezing those um so they can't change and we can't mess them up right and then we extract approximations of uh those pretrained weights in a separate uh on the side right and then what we do with that is kind of what I said before you you have this huge table of Weights but then you can actually decompose that to two smaller tables and then you can uh basically you can find you in these two smaller uh matrices and essentially what you're doing is you're cutting the computational load um by uh significantly and um you can get pretty good results that way uh and you can train it yourself um you know on our on consumer Hardware which is a pretty great thing uh and essentially what happens is that you can actually train multiple luras so now you can see that um if you're familiar there there are websites out there that allows you to post your luras up there upload your luras and then anyone else um can actually download them and they have the same model they can kind of slot them in and use that Laura to tweak their results um so essentially you get a lot of modularity with this um if you have for example an angry Laura and a happy Laura you can get your base llama 2 model and then you can slot each one in and if you slot in your happy Laura it can maybe produce uh text with uh you know in a very cheerful note but if you have an angry Laura you can slot it and maybe it'll make your um LM produce text with an angry manner right um so this is kind of cool to to who is to personalize um what your model can do another thing I want to uh touch upon is quantization so um if you're not familiar with this so basically is a models are usually trained in fp32 meaning it takes 32 bits to encode a number right so 30 just 32 pieces information zeros and ones right to represent a number um however it was discovered that we don't actually need to work with all 32 bits all the time and that there are little tricks that we can um work with to save in terms of memory um so we can actually get by with storing our weights in 8 Bits or even four bits and then what we do is we can change them back to 32 bits uh during calculation time um and essentially uh if you uh if this doesn't make sense to you essentially what um what we're doing is taking 32 bits of information and replacing that with eight bits right and we're kind of summarizing that data in eight bits um and so 32 divid 8 is four so you get a four times reduction in memory footprint but you get kind of the gist of your information still saved in there so an example I have um is the cars on that you see on the right you know if we can save the full resolution car um and you know you see the car you see all the pieces of information but if we want to kind of compress it down uh we can actually you know use a blurrier version of image um imagine this blurriness is actually saves us um uh memory so um we can still tell if if you look at the picture on the on the left that it's still a car we just can't tell um you know maybe the details but you can in general make out that it's still a car the gist gets across right it gets across right so um quantization is similar in the way that like it'll get you it summarizes and there's some loss of information but um if you do it properly um it's still very useful and you can uh it can still kind of do your bidding as you want to and lastly I want to talk about Cura which is a combination of Laura and quantization so it's taking that these um little tricks that we've learned and then put them all together which is uh very neat uh it comes well together it comes together very uh nicely and um there is a whole paper out there talking talking about Kora um but uh so if you're interested feel free to go online and search that up but essentially I just kind of want to showcase uh Kora here here how cool it is um we can find TOA 65 bilon parameter model in one single GPU um and that reduces basically this memory footprint from 780 gigabytes to 48 gigabytes so 16 times smaller right um if you're familiar with gpus you know that 70 780 gigabytes of RAM like you can't fit that into any consumer GPU so because of horaa we can actually uh train things ourselves um if we have you know a 48 gab GPU which is still pretty expensive um and the the creators of guanako um which is another chat LM out there um actually used Kora to train the 33 billion parameter llama model and it got to 97 so 98% of chat gbt's performance you know I I feel like this this case um kind of show this example kind of showcases the power of uh lores and quantization and granted you know this is probably the best case um I'm sure there the cases out there where um K actually you know you lose some information and won't uh it won't perform as well as um you know a fully trained fine tune model um but there are cases where um customizing it this way can be very useful and it's evident here is another fun fact here is that the the mod actually beat CPT 3.5 at chess so you know not always not always super helpful but when it does work it can be um uh pretty useful and it can run on consumer Hardware which is what's important and that brings us to the end of our discussion today I hop I hope that everyone learned something uh about llms and I'm happy to take questions now let me take a look at the awesome thank you so much that was uh a lot of information I feel like people are going to be processing that for a little bit um we did have a question earlier somebody said hello with all this Chad GPT Gro llama is it worth it to learn machine learning from the beginning to build AI models um I think when we so again when we talk about machine learning um it's a very big field uh and we're Chad gbt Gro Lama that's specifically that's deep learning um if we're talking about machine learning there is still a lot of use cases of machine learning Beyond simpler cases right so if you want to um you know uh predict uh you know if you want to do linear regression or um for simple use cases machine learning is definitely still useful I would say with all these free tools out there it um if you want to build another free tool such as llama that's going to be very hard unless you're learning machine learning to go work at a big tech company to train um a model there but if you want to train a model yourself that's going to be very hard because you're going to need a lot of funding uh to run big gpus um in the cloud and you're going to need a lot of data and you're going to need to know you know how to do all of that right so one single person it's very hard for them to do that and uh so I would say you should still learn machine learning too you know if you want to join teams that develop this like these tools are developed by entire teams um um and and understanding machine learning is definitely useful but now that these tools are available it definitely lowers the barriers to entry democratizes AI for everyone so people without knowledge of it can use them um you know and to to to be productive do you think that this is a side effect of the way that people learn this sort of uh like the introduction to this kind of field I feel I feel like um when you're a total novice and you're just getting into Tech you might just hear hey go learn machine learning or go learn data science like it's just a very generalistic approach to the field yeah so that might be creating some confusion when people say well is it worth learning the entire field to get into it and I think you are you're saying that it's it's more about learning all the basics so that you can then dig deeper into the part of the field that you're interested in yes yes I would say that um it's a huge field no one is um you know an expert in everything but there are people who are experts in certain segments of it right and as someone who's getting into coding um you might hear about machine learning but um It's actually kind of a mixed discipline right you're to learn machine learning you you have to know coding and how to program but you also have to know uh statistics and math and linear algebra it's very multiis multidisciplinary so um I would say you learn coding first and then on the side you kind of Tinker with machine learning to if you're interested and then um that's like the best way to um go to kind of progress in both ways if you're straight going into machine learning there is a lot of things that you might not know about General programming uh and you might miss out on that um and in a lot of ways machine learning jobs are hard to come by for um selftaught people so um if you're talking about if you're looking at um all these you know research machine learning jobs engineers umot oftentimes they'll require a master's or a PhD uh you can get it with a bachelor's degree as well but like um it's much easier to you know do a boot camp or go through code academy and get a programming job first and then jump to machine learning later on just because it encompasses so many fields that um it's yeah it's it's it's very complicated uh field I I mean what do you think that is if it feels very obscure like it's really hard to just jump in it it sounds like it's more adjacent like you get a job in Tech in somewhere that is somewhat related or it's or you either are already on track from like your college degree like you you know you you already picked a a degree an undergraduate that you knew was going to drive you into a masters which going to drive you into a PhD that was gonna it just sounds like it's a it's um maybe not a straight forward has people that decide to be web Developers for example where it's a very clear here's the stack here's the things you learn that's going to get you a a junior uh you know an entry level job and I think this is connected to one of the questions we had in the chat that said uh there's a barrier to entrylevel jobs in deep learning jobs are llms a thing that can get me into the door um I would say LMS are not a thing that can get you into deep learning jobs because you're trying to build llms in a deep learning job so you can't I guess you can't just use lmms to um do your job for you I mean hopefully at some point in the future we can but for now llms um are not that smart yet and so you can maybe use LMS to learn some Concepts um you know if you have any questions ask chat gbt it's great for learning but it'll it won't do your job for you especially if you're um trying to get a deep learning job right um and I think it's going back to your question of why it's kind obscure I think in some ways a field is progressing so rapidly that um schools haven't caught up with it yet schools just caught up with the software engineering wave and like education this I guess the entire education industry right so now we have boot camps um and programs for software engineers and computer science right um but you know when we look there's nothing for deep learning there's nothing for machine learning just yet and that's because like I said it's a is very multiis disciplinary it's it's a lot of that work is research focused and um when you're you know when you're re when you're hiring someone from research you want someone who's experienced with research and um those people tend to be the ones with phds who have um worked a lot with uh you know professors researching whatever field they are in right so they they're they're familiar with that aspect that's a they are um I guess more uh machine learning engineers and and scientists um that are more on the product side or you know more work more with the business and so those kind of are closer related to software engineering jobs um but you still there still um you still have to know these you know the models and Frameworks and you have to learn SQL as long as Python and and all these you know bunch of other things stack entire stack right and so those are we're seeing more um in boot camp pro programs and we're seeing more in uh Master's programs but um as a researcher I think it requires a lot of Education to get to that point w yeah that's a really good point that it's moving so fast that it's not even clear what is it that you should be learning like I guess even as a professional I guess it's probably a bit of a race to to be on top of everything I mean you know open AI just did like a presentation a couple days ago a lot of new things coming out um we had here a question from earlier from Maria asking the deeper the neural network does that make it a bigger blackbox and how could we mitigate it or find ways to control it right uh yes I would say the deeper neuron Network the bigger the blackbox essentially blackboxes you don't know what's happening right and when you're adding more layers uh into the neuron Network um you're making that very complicated uh it's not you know you can go decipher it but most people will not be able to decipher what's going on or it will take a lot of work to the cipher R and so uh the deeper you make it you're adding complexity it's making a more black boxed approach um actually if you're if you're just learning about neuron networks if you have a very shallow neuron neuro Network it's basically is uh the same as a simple linear regression but just um kind of um visualize in a different way so a very shallow neuron network is the same as a regression and it performs very similarly it can perform um you know predictions on like linear tasks um but when you add more complexity to it that uh line that I kind of showcased at the beginning of like you know you're kind of fitting the line to data that line instead of a straight line can become curvy wavy like it becomes a function right and so um it get can get very complicated but it can also means it can get very good approximating um you know data of different shapes it's uh it almost makes me think of linear regressions vers versus polinomial regressions I guess yes um I guess I think I don't know if I'm wrong with this but I think I remember uh testing out shallow models with matlb it was like a really easy way like a really good environment to like see it in action how you would just have it would be very shallow few nodes and you could actually like see how it was changing the output like very very cool um yeah m one of those tools do you you know you were talking before about data in different you were talking about different parts of data science um it sounds like people that want to go into this field might find themselves first going into more of data analytics jobs or data wrangling jobs where you're sort of like working with data but you're not working in the model that is using the data you're just it's I don't know if you can explain to the people that are watching uh sort of like the faces I think you touched into that in your presentation how you know you ingest the data then you process the data which is like you know the fancy models and stuff and then there's the output which is what you get from chgb at the end right um it sounds like most people are really interested in the middle part like the modeling and the mathematics of it but Mo but realistically most people will end up in the front end of that process right right uh yeah I think so I think in general even it sounds the coolest it's like the coolest part to work on the middle part with all the math but it's also um actually in some ways the most scalable so you need uh you know a few people uh a few smart people uh can make a very good model and then they can just send that out you know basically chat gbt right like uh a company has made a model and they can send it out and everyone else can use it and it doesn't require that many uh other humans to kind of maintain it and or I mean it does but like not in the scale that we're talking about right versus in the beginning end of that cleaning data analyzing data um giving insights that at least llms you know they're starting to be able to do some of those things but um they still can do it very uh fast and very efficiently and like very insightfully right and so that's why I would say yes most people will end up that first uh part as data analyst you're cleaning data you're um visualizing it you're kind of maybe doing experiments on it and like playing with that data and so uh it's still very incredibly useful for a business to to have people do that but um yeah and then most people will not be able to U work with the advanced math concepts directly off the bat and even if you do I think uh there is you know as much demand as there is for that kind of um Talent there comparatively I think there's a lot more work to be done um on the the front side of that or the data cleaning for example example um there's and data visualization and Analysis like there's a lot more demand for you know if you if you're talking to a product manager they're going to be like oh what's uh what's going on with my data like um how are how is onboarding going on right how do you what are the pieces of data you're looking at like all these questions um you want to be able to analyze and give answers to that and um there are a lot of these questions around right uh versus um the other side is like oh can you build me a machine learning model to predict XYZ right if unless you're working in a big tech company that has a budget and wants to develop spend that kind of money into uh researching all these things it's hard to um dive into it straight away because also um this ma the more um mathematical and machine learning part there is you're making something but it might not actually work out as well as you want it to right so um there is more of a risk in that sense awesome great answer uh here going back to the chat somebody was asking about open Ai and saying that they they heard that open AI is able to use specific versions of the llm just to answer specific questions so I guess they just kind of like optimizing you know you don't want to kill flies with a cannon so to speak yeah so uh they were just wondering if you have any any thoughts on that or like how I guess how they're doing that or like what what why is that useful yeah so uh is I'm assuming that's related to the gbts uh that they just released a few a few days ago or yesterday what um but essentially that goes back I haven't read too much into it yet but I'm assuming in some in a lot of ways that goes back to the retrieve augmented generation in which they are taking um essentially um well you can do this yourself and there are tons of companies that actually do this right if you're um you know you know say you want to make a llm for uh a law business right you can take uh your your you know law books and like kind of get all that data and um have um your own database and have llms kind of interact with it and respond based on uh this data that they uh you're returning so I'm assuming the gbts that um open are releasing are uh could be based on on that sort of architecture where there um there's a lot of like expert um knowledge in one place and they're kind of referenc in that or um yeah I guess that that's if you ask me like what's my intuition that's kind of my hunch that that's where they're getting at and I and I know there are tons of startups that we doing that because they're like oh it's so cool can we just use GPT and like plug in our data and then we have a product we can make our own startup right I know a lot of them are uh getting disrupted by by this because now now it's like oh wait people would probably rather just go to gbt open the ey to for that functionality they build an entire business model out of a rapper of chpt and then open eyes said well we can just do that in house too exactly yeah um somebody here okay so there's a question here for Michael asking about uh is there are any methods to assure the responses for llms meet standards or there's ways to track when you change an llm if that's going to affect the way that he huc hallucinates or I guess it puts outputs out yeah I think uh people usually use benchmarks a lot of this LM stuff is um actually so a lot of it requires human feedback for example when you training LM GPT all your um like basically the base GPT model all it does is output and it's a very good com sentence completion right so output words are similar to um to what you kind of like the topic that you asked but it might give you just giberish and to make gbt chat gbt there takes a lot of humans to actually create a data set um that makes sense and uh they use re re reinforcement learning um and human feedback to uh kind of accomplish that so um I'm not the most first in it but I would say I think there it takes a lot of humans to kind of actually um uh I guess bet that and they've actually created Ben marks to to score um you know basically these these sets of tests that models can go through and and take them and like kind of like they get a score at the end to see like oh how good and how helpful you were as a model that's the the the main ways that um I've seen these models being compared as as to how good they are sounds like a lot of M your work it reminds me of Amazon Turks and classifying a thousand cat photos a day just to try and train something I I guess yeah um you you have to do you have to do what you got to do to train these models um I know that some people were talking about uh using AI agents to train the AI but I guess that goes into the whole idea of like well you're just putting a lot of trust in that AI agent doing the right thing right like otherwise they're just going to spiral totally out of control um not knowing really what's gonna like I guess you will have to have a a really high level of confidence and predictability in that AI agents to let it train all AIS right right exactly and I mean it's it's been done for example in the sense of like um you can use in responses to train such as the example I gave about guanako training to J gbt data and you know a lot of models can you can train to chat jbt outputs and inputs and that actually is um that's pretty feasible but if you're talking about like automating that entire process and like doing it large scale I would say um you know the you can still do it but the quality of the data the outputs are not vetted right and so unless you have someone they're ensuring uh that the quality is good you might get predictions and get outputs that are not as good as you expected and yeah I think I think essentially um yeah basically you want um they found that these data sets like a good way to make your model better is by having very high quality data sets and if you're just outputting inputting outputting gibberish are like uh not very helpful very generic uh answers you might not get the answers that you want in your own model I can only wait to see what Gro comes out with uh training on on X and all those tweets uh I don't know if you call that a high quality data set uh I guess we'll see um okay so we can close off with one last question I guess this that be more openended so just feel free to answer however you want somebody was wondering how you believe that the end point of AI or like what's the top reach of AI so maybe if you can imagine the way that AI is progressing right now ji and all these tools maybe looking 10 15 20 years down the road how far as you can imagine uh what do you see AI going towards yeah um I honestly I think there it's as much of a guess as you know my guess as anyone else's um I would say uh there's certainly uh a lot of room for um for progress and like we've come a long way but there still so far to go and there uh we will probably see AI incorporate in all these um other tools and other use cases that we don't see as of now so is it a game changer I definitely think it is um but at the same time I think there are problems that we're um going to run into uh so for example uh one thing right now is like you know our models are huge right and there's this thing about how do we carry this model this huge model and like uh have it inferenced on like Edge devices right so basically your cell phone how does cell phone um how will it be able to process these large models and so um I know there's a this rise of analog Computing um that they talk about which basically it's almost like oh we've you know we've done all this in the digital realm and we want to kind of carry that to um the analog like kind of like real life and like use analog computers which are these like it's it's it's a different way of computing but um I I want to say that we'll see that more so it's like we we've Advanced a lot but now we might have to back track a little bit to say like oh you know these are actually some of the downsides that we see with these models and these are um alternatives to get around it and so we might kind of pull back in some ways and Advance uh in those areas and then all in all though I think we we will see um you know I don't think it'll be it'll spell out Doom or you know I'm not kind of the doom and boom kind of guy but I do think it'll be very uh useful for daytoday tasks in the future nice awesome I think it's like um it's like the way that UI has changed right with like web you look at websites from 2000 and you look websites now how much interacted they are how much media there is in it like and and the you know websites from 20 years ago look Prett historic like it's just text um I I know I I remember seeing somebody online saying in a conversation that AI is going to change the way that we understand how things just work like what we expect things functionally wise to be and it's just going to integrate in those small ways where your app might do things that they couldn't do before your services might answer questions like be do things and then people just get used to it and you don't realize that you know this whole wave of GNA is what enabled uh those little things to to just make your life easier I guess um yeah even if it doesn't change the world in a big way right yeah agree agree yeah all right well thank you so much willing to you for stopping by sharing what you know with the community and with everybody on YouTube If you enjoyed today's programming Please Subscribe and keep up with Co Academy's social media WE Post over there every time that we have a live event every time we have guest speakers and every time that we have new curriculum content going out like uh lately we have upgrade or react courses to V8 for example so we'll see you next time thank you for stopping by and having have a great week everybody byebye and that's