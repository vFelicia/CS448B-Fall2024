um i'll read it out loud it says hi um what i'm wondering most is if the discussion here will have differences with the lessons on linear regression in codecademy that is a good question um there will be some differences just in that this is more of an open discussion and so if there's questions in the chat or there are specific things um that anybody wants to talk about uh we might go on some tangents and um and cover some things that are not in the lesson but for the most part we're gonna start by uh going through the the same content that's in the lesson yeah cool cool we are back in the office with uh our fancy streaming setup so we're trying out like new mics and cameras all that kind of stuff so let us know in the chat if anything is screwed up if you can't hear us if the mic sounds bad um but uh hopefully everything is uh is working fine yeah all right shall we get started let's do it all right so um the topic for today is going to be um interaction terms and polynomial terms which basically are ways that we can alter a linear regression a simple linear regression model or a multiple linear regression model to make it a little bit more flexible and handle some different things that we might see in some data so um i actually think this is exactly the same to the question that was asked earlier this is exactly the same data set that we give you in the lesson on codecademy and i believe it is simulated i didn't write this lesson um but the idea behind this this data is that we're imagining um some sort of study that measures selfreported levels of happiness self reported or maybe like measured levels of stress um whether or not a person exercises um i think sleep is probably hours of sleep make sense 13.8 it's a little extreme maybe and then maybe hours of free time per day i think that's the intention of this data set it's i'm pretty sure simulated though so not not real cool so um what we're gonna get started with is uh visualizing some of this data um you'll notice actually in the lesson on codecademy and i just grabbed this code directly from there you'll notice that we actually use this lm plot function to do a lot of the plotting in this in this lesson the reason for that is partially because uh we haven't updated uh matplotlib and seaborn in a little while so this is an older function in seabourn that does the same thing as the newer scatter plot function um but also it has the option to fit a regression line directly onto the plot if you wanted to um and so i'll just demo that really quickly um here actually i'll demo it in a separate plot so if i say let's look at the relationship between stress and happiness so is there a relationship between how stress someone appears to be and how happy they report being um and i say fit reg equals true and i think that's also the default and i run this i get here are the points and then it'll automatically draw this regression line for me which is kind of cool and it allows you to do this a little do this plotting a little bit more quickly um which is which is always fun there you'll notice that something interesting happens actually if i leave this is fit rug equals true and add q equals exercise right so now it's going to even be a little bit hard to visualize right because there's uh two variables there so three variables right there stress happiness and exercise yeah um it it's pretty smart actually it will do something here actually draw me two regression lines one for exercise equals one and the other for exercise equals zero alex do you notice anything weird about this plot though that's like different from what we might expect based on everything we've covered so far like do you remember i think maybe it was the last stream you joined me on where we started talking about um using this using a categorical variable in our regression and we we plotted um some regression lines when we included a categorical variable in the regression right and the the two lines had something different from what we're seeing here um you remember no i don't recall okay so um so actually the two lines were parallel so um there was no way previously or we didn't have a way previously to draw um to draw regression lines that were like totally just fitting the orange points and the blue points separately we could only change the intercept so if you go back you watch the older video you'll see that in action um and today we're gonna actually learn how to fit these lines um so that they can have different slopes yeah and in that graph is are these lines so say you took out all of the blue circles um and just fit it to the orange axes is that the orange line and similar to the other way around so they're completely independent of each other they're not um they're basically ignoring the other set of points um if you fit yeah if you fit just a regression for the orange line or orange points and just regression for the blue points you'd get these lines okay cool um so i'm gonna now just show you this plot uh without the line i lines i think i kind of gave away via the end goal before we even got started so we'll start back at the beginning and we'll motivate this by saying sometimes when we look at a plot of two quantitative variables and then we color by categorical variable we'll see that if we were to draw a line through just the blue points and if we were to draw a separate line through just the orange points those points would or those lines would probably have different slopes and different intercepts whereas up to this point if we just added um so if we just fit a model where we did like happiness as a function of stress and exercise we'd end up with a model that allows us to draw separate lines for these two sets of points but they would have to have the same slope they could just have different intercepts and we'll i'll show you on the ipad in just a second mathematically why that happened and then i'll show you how we can now fit a regression that visually would look like this cool cool so um what i'm gonna do is i'm going to here let's add a um let's add a chunk and let's fit this regression right off the bat um i'm gonna grab this code from over here and we're gonna talk through it for a second and then we'll come back to this in a minute okay so i'm fitting this model i believe i've already loaded yes um so i'm fitting this model and it's happiness as a function of stress plus exercise plus stress colon exercise now for a second let me remove this this is what we've done so far in previous um live streams so if i run this this is kind of what we've seen before we get an intercept we get a slope for stress and we get a slope for exercise same intercept so these lines aren't parallel it's same intercept and different slopes so this these lines actually are parallel so actually this is a good a good place to kind of like come back to this i hadn't initially planned on it but i think this is like a good reminder so um let me actually you can flip me over but um can we leave that up yes okay great so this is for the um the other one that had the stress colon exercise but i'm just gonna write down the um the values for the other regression for a second so for this one we had an intercept of 10 about we had a slope on stress that was equal to negative 0.7 and then we had a slope on exercise that was equal to about negative 0.9 and we fit this model it was happy as a function of stress plus exercise and what that meant was that we were fitting this model we fit something that looked like happy equals intercept which we called b0 but we'll replace that plus b1 times stress plus b2 times exercise it's gonna abbreviate and so plugging in b0 b1 and b2 we got something like happy equals 10 plus well it's really uh plus negative so minus 0.7 times stress minus 0.9 times exercise but remember that there are only two possible values of exercise someone can exercise or not exercise got it so we ended up with two separate equations one when exercise equals zero we get happy equals 10 minus 0.7 times stress minus 0.9 times 0 but 0.9 times 0 is 0. so that kind of goes away and then the other one we got right down here when exercise equals 1 we get happy equals 10 minus 0.7 times stress minus 0.9 times 1 which is just 0.9 and so then we end up with happy equals um we can take the 10 and subtract the 0.9 right because order of operations we can all of these addition and subtraction things can happen in any order so we end up with like 9.1 minus 0.7 times stress got it so same slope different intercepts and so they both have the same slope on stress but the intercept is 10 when exercise equals zero and the intercept is 9.1 when exercise equals one and so that slope on exercise is really the difference in intercepts for the two lines that we would draw oh i'm now realizing that this is that we are covering the uh oh can't darn it sorry guys oh alex i don't even know it's there all right we'll figure it we're figuring out the green screen sorry guys um hopefully that was clear though uh okay so now let's go back to that second um let's can we switch back to the uh that's just okay yes um okay so that was when we just fit this where we have happy as a function of stress plus exercise but now we're gonna add an interaction term and the way we can do this in stats models is we can just say i want an interaction between stress and exercise by putting a colon between them so now when we run this model run it um now we get an intercept we get a slope on stress and then we also get a slope on exercise just as before but we get now a third slope slope on stress colon exercise so now we're going to take this and we'll go back to the ipad in just a second and talk through what those sorry i'm erasing really quickly um talk through what those translate to in terms of our model okay so now oops well that's okay um so now we see okay we had intercept stress exercise stress colon exercise all an interaction term is is an extra it's almost like having an extra column in your data that is the product of two other columns so it's almost like an extra feature that is just the product of two others so the way this looks like in terms of the model is we're fitting now happy screwed up let's see so now we're fitting happy equals b0 plus b1 times stress plus b2 times exercise plus b3 times stress times exercise times sign okay okay so in like for example in um scleren you have to create a um an interaction term yourself in your data frame and literally what you would do and we could demonstrate this after is you would just multiply everything in the stress column by everything in the exercise column and create a new column of your data frame called stress exercise and then you would just add that into your model as a predictor i think i'm a little bit confused of like what the overall goal is here because so in in this situation isn't this gonna still result in two parallel lines because that times one the time zero is still going to be um i can't even uh that's not pointing at anything it's still gonna be right when when exercises is equal to 0 the b2 times exercise and b3 times stress times exercise all of that is just going to go to zero so isn't this really similar to the last thing that we just looked at it's very similar but let's actually walk through exactly that so let me plug in all of these numbers so we've got happy equals our b0 is 12 that's still our intercept um then we've got b1 it's negative so we're gonna do b1 is negative 0.97 so minus 0.97 times stress and then again the negative sign minus 3.1 times exercise and then we've got plus .36 times stress times exercise okay now let's break this up into two separate um two separate equations feel like this eraser never works the way i wanted to um okay so let's break it up into two separate ones one where exercise equals zero and one where exercise equals one so i think you alex were doing the exercise equals zero one in your head first so um it's a little simpler so let's do that first so when exercise equals zero we've got happy equals 12 minus .97 times stress and then we've got minus 3.1 times zero plus .36 times stress times zero now like you were saying right anything times zero is zero so 3.1 times 0 is 0 but also 0.36 times stress times 0 even though we're multiplying stress in there it's still like a bunch of things multiplied by zero so that's also zero okay so that's zero that's zero so our equation is really just happy equals 12 minus point nine seven times stress okay so we've got our line again for exercise equals zero now let's try exercise equals one this is where it gets a little bit more complicated yeah so we've got um and also if anybody has questions as we go i know this is like again a little bit of math um so ask us questions if they come up but for exercise equals 1 we've got happy equals 12 minus 0.97 times stress just like before then we've got um minus 3.1 times 1 and then we've got plus 0.36 times stress times one got it okay so multiplying by one doesn't change the value of anything right so we can do the same thing as before with our with our intercept rate so like this minus 3.1 times 1 we can just add to the 12 again order of multiple order of operations since this is just like multiplication and addition we can order we can add or subtract these terms in any order so we can combine these two together so i'm going to do that first and then i'll write out the other set but some of you might already kind of see what that's going to be so we've got happy equals and then 12 minus 3.1 is like 8.9 right and then we've got minus .97 times stress and then we also have plus .36 times stress and then i'm gonna just drop the one because anything times one is just equal to itself and now we've got we've got minus 0.97 times stress plus 0.36 times stress and so both of these things are multiplying stress so we can actually factor stress out of this like piece right here so stress is multiplying both things we can factor it out and add the 0.9 minus 0.97 and 0.36 together and so what we get is something that looks like this we get happy equals 8.9 and then we've got um i guess i'll put like plus stress times and then we've got the minus 0.97 plus 0.36 oops 0.36 and that um again like thinking about how you would calculate this or like how you would expand this right you would multiply stress by both things inside the parentheses so you'd get like you'd get back to what you had above you'd have the stress times minus 0.97 that's this term from before and then you'd get the stress times .36 which is this term so we just factored it out but we rewrote that same thing as before and then minus 0.97 plus 0.36 uh 61. yeah um is yeah so we got happy equals 8.9 plus 0.61 or sorry minus 0.61 i guess i shouldn't have switched the order but you can multiply in either order so um okay so that's our equation for exercise equals one that's our equation for exercise equals zero and you'll see that they have different intercepts 8.9 versus 12 and they have different slopes minus 0.61 versus minus 0.97 cool cool and so right the change in slope came from this term that included both stress and exercise that was the thing that ultimately affected the slope when exercise was one exactly um can you briefly describe again how the 0.36 was calculated from the original data right so these are these numbers that are up in the top left corner where do where do those even come from oh so these numbers came from fitting the model um so we'll go back to the jupiter notebook in a second um but basically when we fit the model with that interaction term it calculated a slope on the the interaction between stress and exercise and that is basically like a multiplier right of stress times exercise um so then we just followed that out in the model i also see a really good question in here about it says would we consider the third variable stress times exercise dependent on the values of stress and exercise that you get more accurate models when all your feature variables were independent so that's a really good question i'm not sure how great of an answer i can give but um all to say so yes you you don't want to have uh colinear very or you don't want to have highly correlated variables in um in a model together because they're both if they're both explaining your outcome variable then the model is going to have some trouble estimating the coefficients for both of them because they're both like they both have the same explanatory power in terms of your outcome variable um and it also right if they're highly dependent like if you just take um if you take one of the columns and multiply it by a number theoretically you shouldn't be able to fit that model at all because it's perfectly your new variable is perfectly correlated with your old variable in this case it's not perfect it's not like this new variable that we created by multiplying stress and exercise it's not perfectly correlated with either stress or exercise because we're multiplying by a different value for each so like if you think just about stress whether you multiply it by zero or multiply by one depends on the value of exercise so there's no way you could just take that column without seeing the exercise column and and get the new one right it's not like if you were just doubling stress that would be perfectly correlated with stress but there's um yeah it's either you're sometimes multiplying it and sometimes not depending on if exercise is zero or one yeah but on the flip side there are reasons why you might not want to just create like the most complicated model that you can with as many extra predictors as like this as you possibly could create because you could imagine like if you have i don't know 10 columns of a data set which is even in real life that's like pretty small um you could imagine you could create an interaction term for every possible pair of pair of variables basically pair of features and that would be a very large number that i i don't know what it is but yeah but that would be a very large number and you could create a very complicated model to model like every possible interaction um but that's not always a good idea because you end up what ends up happening is you end up over fitting and that's kind of actually a very important topic in a newer course that we're building right now on feature engineering um and like regularization um but basically if you if you fit your data too well then you're not your model is not going to be very good at predicting outcomes for new data so like this this model if i make it super complicated might perfectly fit my my data that i collected on my like 100 patients or whatever but if i go out and i find another patient like this might not be a good way of explaining the relationship between these things right because you were like so dependent on those original 100 patients exactly um can you describe just like the total takeaway or upshot of of this whole process because what i'm walking away from this is before we were able to draw two lines that were parallel that kind of stinks for some reason and now we can draw two lines that aren't parallel like what is what is like the thing that i should be taking away from creating this uh this new variable will you go back to the the picture or the um yeah the browser the browser yeah so that's a really good question i think so a couple of things first of all um it's just you know if you were to plot your data and you see this picture maybe you see something even more obvious like maybe you see these lines look completely different like maybe you even see that for um people who do exercise the relationship between happiness and stress is this like negative relationship like people who are less happy are more stressed but for people who don't exercise maybe it's like a positive relationship and so what you actually see sometimes and this is sometimes i think referred to as simpsons paradox and we actually have here i can go and then um show you in the lesson as a note so we're using new uh streaming software and your mouse pointer isn't showing up on the uh on the software so if you're ever gesticulating with the mouse uh that's not getting backed up good to know something we can uh play around with is it oh it might not be in this lesson which lesson is it in um let me see if i can find it quickly i think it's in this lesson um yes so this is like a more complicated example here let me like pull this up um but you'll see right like in this in this example we've got um like a positive if we were to just like ignore this um categorical variable and draw a line through all the points we would draw this black line which is like this positive relationship but then if you add this um this like categorical variable to the model and you're trying to draw the lines now based off of um you're basically trying to draw the lines through each set of points in this example they're parallel so like we're not constructing but you'll see right that like the um the lines have negative slopes even though the initial relation like the relationship between these two variables in among all the points is positive if you zoom in you now are getting these like negative relationships and so you can see stuff like that where like maybe right maybe one of these relationships is negative and the other one is positive and if you didn't fit anything like you didn't fit um any sort of exercise term like you'd end up with a flat line you'd think there's like no relationship between happiness and stress but like once you add this the model you suddenly start seeing them but now if you are restricting this so that those lines have to be parallel then you're like ending up with you're gonna end up with maybe like two parallel lines that are flat and that's like not gonna model your data super well so there's definitely situations where adding an interaction term means that you significantly improve your model from where like you basically can't explain anything to um oh it looks like i didn't we didn't switch over maybe um that's possible oh sorry yeah you're good um okay uh but yeah so sometimes it significantly improves your model i think like the flip side or the other thing that maybe is like the other half of the answer to that question is and maybe this gets a little bit more into like when we do this um is we do this when we think that the relationship between to the outcome variable and one of our features is moderated by or like influenced by some other factor um so in this example uh it looks like right the relationship between stress and happiness is maybe like less steep here i'll do it with the lines um is less steep for people who do exercise so like maybe people who are exercising are more like immune to stress in general and um and so like being more stressed has less of an effect on their happiness than people among people who don't exercise it's a steeper line and so we're saying that like we think that you know more stress has like more of a effect i say in quotes because like we're not really depending on how the studies run we don't know if it's like a causal relationship but the effect of stress on happiness is more significant um among people who don't exercise and so we think like there's some exercises moderating this relationship and that's why we might think like we want to we want to be able to model that in some way this is maybe a tricky question or uh this may be a tricky question but like how do you do this in the real world of like so you just described oh we have this situation where one set of data points has like a negative slope one has like a huge positive slope we want to be able to draw these two lines with different slopes in order to capture that and so we want to do something like this but do you know that by plotting the data and looking at it do you like just give both things a shot and like see and see what the models look like after after trying to add it or trying not to add it like how do you actually do this with a data set that you don't know anything about that is a great question um so you can certainly and i highly recommend in fact i'm working right now on some content on exploratory data analysis and it's definitely a good idea to before you fit a model take a look at some of these relationships and try to see whether maybe you have hypotheses like maybe you know something you have some prior research about the relationship between these all of these features and you have some hypotheses about what might be moderating what relationships and then maybe you get some data and you want to take a look at it before you try to fit the model and so that's one thing you would do um the other thing you can do which is more like post talk is you can um you can compare different models so you could fit it both ways and then you could see um you know like the simplest thing is you might calculate like a mean squared error like basically um or a or a total squared error or whatever um basically like how far off are all these points from their respective lines and you could calculate that for both models the one where you don't include the interaction and the one where you do um and then you could say like okay i see that when i add this term to the model um it significantly improves its ability to explain this data um and you'll actually see if you're following along in this linear regression course um there is a a section here on choosing a linear regression model that starts to cover some of those methods that you could use to compare two different models once you fit them um to get a sense for whether whether both of them make sense or whether one makes sense over the other then there's also additional additional things you can do like regularization which nithya is working on right now um that can even further like help you basically like fit the model at the same time as estimating whether like those parameters are useful um so cool that was a bit of a tangent but hopefully um i see a question how do you do feature selection in linear regression i've heard a forward and backward elimination methods but don't know what they do um again this is like a plug for all the content that nitia is currently working on um and that will be i think going live at the end of the summer early fall um but essentially so forward and backward elimination are essentially methods for like sequentially adding um terms to your model so like maybe you want to sequentially add different interaction terms and reevaluate the model every time and then if adding the term improves the model you keep it if adding it doesn't improve the model then you don't keep it and you try something else or you start with like all of them in the model and you sequentially like delete things that are not helping you um that would be backwards elimination and then regularization is essentially a way that you can fit this model um with like an extra term so that you allow some of the coefficients to get really small or even go to zero um and that's kind of a way of like eliminating eliminating those uh from your regression um okay and then suppose i have data can i add two extra decimals on point five six oh you can add as many decimals as you want to run the linear regression um yeah as much i mean i don't know if that's that is that's not the question uh yeah the question is saying like oh if i have this data can i add can i add or the way that i'm interpreting this question is can i add points between the my existing points in like regular steps of what can i add 0.5611.56 like can i add data that falls along that line i think that that's what the question is asking but if you want to clarify the question um so i guess like basically i think you can't add data if you're trying to answer a question about some data or like fit a model to some data then you probably don't want to alter your data before you fit the model um but yeah i mean if it's like something that you could collect so like if these are values of stress and you don't have anyone in your data set who reported like a value of 0.5611 for stress but you can go out and find someone and you can record their happiness score then you could add that to the model if you don't have or you could add that to your data to fit the model if you don't if you can't find someone with that value you shouldn't just like add that to your data set before you hit the model um i got it and the clarification in that question of if 0.56 is occurring multiple times can i add extra decimals to it so i still don't fully understand but yeah if you see multiple people with this value of 0.56 and you are able to measure more specific values um then yeah like if you are able to measure to the fourth decimal place but most like measurement tools have some limits um i also see a question about heteroscedasticity um which is a fun word i think um that is coming in the next live stream so yeah we'll come back to that next week um cool so i think uh we've got like 20 minutes left i want to cover two more things if we if we can fit them in um so one thing i'll kind of go over a little bit more quickly but i will i will pull this over and rerun in our other notebook um so i just want to demonstrate so up to this point we have shown that we can fit this interaction term with a categorical variable um as one of the terms that we're interacting so in that case we ended up with just two lines because there are only two values of exercise so you basically get like a separate line for each value of exercise um but we can also fit interaction terms with two quantitative variables so here's another plot it shows the relationship between happy and stress and then we've colored by this value or this other variable free time which is like the number of hours of free time that you have um and this is also a quantitative variable we're just showing values uh like integer values for the the key but you could imagine this is like a scale where people that have more free time are darker colored dots and people that have less free time are lighter colored dots in this plot and you could imagine like i i can't do this super easily in my head but you can imagine if you just isolated all the people with five hours of free time you might draw oh i am remembering now that you can't see my my pointer but right so if you uh just isolated all people all this like the darkest purple dots the line that you draw through those might be different from the line that you draw through the lightest color dots to people with zero hours of free time and so um so we can fit this for uh this setup as well so i'm gonna go ahead and grab this model again and we'll edit it a little bit so this time let's edit this let's do stress uh plus free time plus stress interacted with free time and then let's fit that and we can fit something again we see we get our intercept slope on stress slope on free time and also a slope on free time colon strap or stress colon free time um and i'll just write this out super quick on our ipad again um okay cool so here we've got those written out for ourselves and let's write out what this model is super fast so the model is exactly the same as what we had before we've got like happy we've got happy equals b0 which is eight plus um and again with the minuses sorry minus point five five times stress plus point 0.12 times free time and then plus 0.04 times stress times free time i'll abbreviate ft okay and then you can see for different values of free time we get different equations so for example when free time equals zero our equation is just like happy equals eight minus point five five times stress and these two other terms go to zero because point one two times zero is zero and .04 times stress times zero so zero and then when free time equals one we get happy equals eight minus point five five times stress and then plus 0.12 times 1 which is just 0.12 and then plus 0.04 times stress times 1 which is just 0.04 times stress and so if we simplify this oops i don't know how to go back did that change yeah you can see it oh well um oh oh no this back button i'm just gonna like reopen it cool um so we've got happy happy equals 0.8 um and then we've got the minus 0.55 plus 0.04 so it's going to be like minus 0.51 times stress and sorry i should have added it's 0.8 plus 0.12 so this is going to be like 0.92 over here so it's going to be 0.92 minus 0.51 times stress and then we can keep doing this for other values of free time so we can even do for free time equals two we're basically going to end up adding another .12 to this um to this intercept because we're going to end up with right one two times two instead of point one two times one in this equation so for free time equals two our equation is going to be happy equals point nine two plus another point one two which is going to be 1.04 and then we're going to also add another 0.04 onto the slope so it's going to be like minus 0.47 right times stress and so we can do this for every value of free time we get a new equation every new equation has its own intercept and its own um sorry our its own intercept and its own slope and we can try to visualize it it's a little bit harder to visualize because now we have you know i only did integer values of this but like you could imagine there's a different line for every possible value of free time so you could also have like free time equals 1.5 theoretically you could also have free time equals negative something um but obviously that's not realistic um but here sorry i can't see you so sorry i could have you yeah back um okay so uh so we've got like infinitely many lines in here but we can visualize what some of them would look like i think i grabbed this and i will um i will really quickly talk through what this code is doing oh um i think in my initial i call this model q okay so here's three lines um those are the three lines for happiness or sorry for free time equals i think zero um three and six exactly um and so you can see what i'm doing is i'm modeling so for each line i'm taking the stress values and then i'm well i'm taking the intercept plus the um the coefficient on stress that's all i need for the the first line when um free time equals zero because remember those other two terms kind of go away but then uh for my line when uh free time equals three i'm basically like writing out this whole equation it's like this is my b0 this is my b1 times the stress values and then this is my b2 times three times um oh sorry this is my b2 times 3 which is the value of free time and then this is my b3 times stress times 3. so i'm basically plugging in a value for of 3 for free time into that equation in order to draw this line um and so that's like the second line and then um the third line for uh for free time equals six is this dark purple line and you see that they all have different intercepts and they all have different slopes cool um okay so that's uh that's it that we're going to cover for um interaction terms for right now um were there any questions that you wanted to cover yes i've been chatting with people in the chat so if you've seen me typing that's what i was doing uh let's see um uh there was one where did it go um what are the aic and bic criteria cool yeah so those we're gonna cover i think two live streams from now and like i said that that's all in this um in this course menu that's in the um choosing a linear regression model lesson you'll find lots of stuff about aic and bic uh here we go here's a exercise on it um so those will those will come up later i also see a question about fixed effect and random effect models um that is uh something so what we're doing right now is fitting fixed effect models um random effects are useful for different kinds of problems like i think the classic one is when you have data where the observations are not independent so like students within a school are not independent and so if like you have school as a predictor in your model um and also like individual students within each school um you want to account for those like that colinearity or that correlation um i i don't think we currently have any content that covers that but um but yeah like a random effect is basically like a it's like an extra term in your model that can that is basically like random error um that's like us but the random errors are correlated within like say a school or something um lots of questions about office hours do you want to plug what we're doing there oh yeah so office hours we we did our first test run last week um and then we'll we'll have another office hours this week um we're doing them on discord and it's basically just a place where you can come and ask questions and we'll we have voice capabilities so we can like answer them speaking um you can also share our screen and show you stuff if if we need and um but it's also we also have a um like a chat in the on discord uh where you can post questions at any point and if you post your questions ahead of time um i think it's called yeah questions bank curriculum now um but we'll be checking that and keeping an eye on it we're not gonna respond like right away probably but if you have questions we'll look there before office hours so that we can make sure we get to anything that is asked ahead of time um cool um yeah we also have someone was saying in chat if we could do office hours directly after this stream that's something we weren't planning uh we weren't planning for this week but if folks would prefer that if you're here anyways watching the stream and and want like more time to chat and ask questions right away um we could potentially make that happen yeah for sure so that's good feedback um similarly it sounds like there's a cool python package that will let us like render math math equations quickly um so yeah more more feedback if uh like i like what sophie is doing with the ipad i think it's helpful to be able to like draw graphs and stuff but um if there are ideas that you have that uh would make this better um yeah definitely let us know cool all right i've got one more thing that i can show you guys since we have time um so last we another kind of flexible model that we can create is um using polynomial terms so i'll show this graph right here again a little bit made up but this is sleep versus happiness and so we see um i guess the um we'll do lm plots hold on i got this we got sleep happy and then what was the data called happiness all right let's try that okay so it's gonna fit this line um if we just leave it as fit rug equals true but you'll notice actually i'll add the like fit rug equals false rerun right you notice that if you were to try to like draw a line through these points you would say okay like as you get more sleep your happiness increases up to a point if you have more than 10 hours of sleep your happiness is starting to like decrease again and so we almost want to be able to fit like a curve to this instead of a straight line and with regular linear regression we don't have a way to do that so we can do it using polynomial terms and this is again a little bit of like a trick of almost adding another um another term into our model or another like column into our data that's based off of another um another column that already exists and uh and it basically allows us to have a model where we have like a squared term in the model or so like a higher order polynomial so i'm going to demonstrate this really quickly with this data um and we want so we want happy as a function of sleep this time and then um we're gonna add a i think it's like like this basically this is taking sleep to the second power let me just make sure that that's oh yeah that's right okay so we're just saying okay i want to add sleep squared into my model and then fit this and then print this out let me get model p for polynomial and then oh model yeah um and then you'll see that we get a intercept we get a slope on sleep and then we get a slope on sleep squared and i'm not going to switch to the ipad now because we're we're kind of running out of time but i'll i'll actually do this like mathematically i'll write it out in python so um so basically what this is doing is it's creating a new equation where we have sleep squared in our model um so let's like i guess save these as b0 b1 and b2 so model.params zero so like the first thing is the intercept that's our b0 b1 is the second thing in this model so that's at index one and then b2 is at index 2. so i'm just saving i'm just saving these numbers as b0 b1 and b2 i'm gonna also just save the um the values of sleep as sleep instead of like happiness dot sleep because it's a column in my happiness data set i'll just save it separately as sleep and then i'm gonna say like predicted happiness based on this model is now equal to b zero plus b one times sleep plus b2 times sleep squared um so i can do that as like sleep um star star 2 that will square all the values and sleep i could also do like numpy dot power sleep comma two um but basically right this is like all the values in this column square and if i plot this now and the result there is going to be a column right because sleep is a full column so you're yeah you're doing that for every value in this sleep column you're doing that equation exactly in fact i can also just show like what this looks like i'm so run this so i have them saved and then yeah basically like this is every value in in sleep squared and then multiplied by b2 i guess we can just do that um this is every value in the sleep column squared so okay so i'm just creating these predicted happiness scores and then what i can do is i can add it to this picture so we can see what it looks like and i'm going to um see that plot sleep and predicted happiness based off of this model and i'll show it i guess actually what is a better way to do this um i guess now i'm realizing this is not pretty because uh it's like drawing the line between all the points when we're doing it on a line all of these were like still on top of the line um so let's see we can just do like lin space um or we can do like sleep equals so sleep is roughly between 0 and 14. so let's instead of grabbing this actual column we can do like is it in numpy so you just want like a list of values from zero to 14. yeah um by 5.5 4.1 here let's just test this really fast thank you here actually i can demo this even though we're inside that linspace and then start stop and then oh so then it's just like the third parameter is the number of values so we'll do like 100 values and that should give me yeah okay sorry guys but hopefully this is helping you see like how we do this in real life and so here we're not using the actual sleep data we're making up a bunch of fake points just to draw what this line looks like right and then yeah so that's a little nicer now we see that that polynomial term allowed us to create this like kind of curve um in our line cool all right well quick question was so when we did um sleep times exercise that was the interaction term and if we're doing this sleep squared is that called the polynomial term it is a polynomial term cool yes and we could even add sleep cubed and sleep to the fourth we can add more and more polynomial terms but again like we probably don't want to if we added another if we added like a cube term that would allow us to get like a another curve in the data or in the line basically um but then i think there's there's some really good examples of this if you just like add lots and lots of polynomial terms you could have like a very squiggly line which is a good example um of overfitting in action cool cool um sophie you're going to be gone for office hours this week yeah i might sign on though okay if i can um i might be i might be gone for office hours though but i'll be back next week and i'll keep an eye on this yeah i'll i'll be in the office hours i am not the stats person that sophie is so i don't know if i'll be able to answer your stats questions but i could probably get a niche yeah we'll find someone yeah that knows what they're talking about andrea actually wrote all this this lesson okay maybe i can get hurt cool all right cool well awesome i'm glad that our first uh stream back in the office i love this green screen and the fact that we're just like yeah yeah i don't know if you noticed but i shrunk us uh in the middle of the stream to uh that's good yeah i like it like tiny people awesome well thank you everyone for coming and if you have questions ask us on discord or ask in the comments on the youtube video