um she'll be live in just a moment okay i believe that we should now be live on youtube um but drop us a note in the chat if you can see us good morning on this beautiful tuesday um i'm sophie and jamie is joining me today to help answer some questions and keep me on track so let us know if you are there in the chat if you have any questions throughout the stream please just drop those questions in the chat jamie will stop me or he'll try to answer them if he can directly um but yeah we always love to know where people are joining from i see someone from greece and from alabama very exciting so let us know cool alex lackson says you're live thank you um awesome so i guess we'll get started um and we'll we'll kind of jump right in it's good good to know that youtube is working um we're back to last week we did a stream in the office this week we're back from home so uh we were really high tech last week a little lower tech this week but hopefully we'll get the office set up up and running uh more frequently in the future so um i'm gonna go ahead and share my screen and then we will kind of jump right in all right can you see that jamie yep okay cool so this is a little bit late on to the github page so if you haven't had a chance to take a look through these notes that's totally fine um i'm actually going to before we even get started with these notes i just want to show you um on codecademy where you can find the article that covers the topic that we're going over today um so if you search linear regression uh you'll see this linear regression in python course this is the course that we're going through in this series and right now we've kind of talked about simple linear regression multiple linear regression um last week we talked about interactions and polynomial terms and then this week uh there's this article in here on log transformations and more um this this stream is going to cover even more of the and more part of that um but if you take a look at this article um there's a bunch of helpful information here and it it explains all this with an actual data set um today i'm going to use some um simulated data and i'm actually going to kind of take you behind the scenes if if you will and show you how i'm simulating data because i actually think that this is a really useful skill as a as a data scientist as a statistician to be able to create data that follows some or that represents some situation that might occur in real life and then see what happens when you try to fit a model on that data so it's really useful for debugging as well so here um i'm just in this code uploading or uploading a bunch of or loading a bunch of libraries i see uh someone said sophie you did not finish the modules um i i did finish the modules i actually have reviewed all of them very very carefully um but but yeah it doesn't when i go through on codecademy it doesn't mark them as complete um i actually wrote some of the ones that it says i haven't completed which is funny um okay so anyway so we're gonna simulate some data so what's happening here the reason i have these random seed functions in the code is just so that if you're downloading this code on your own you get the exact same numbers this will all work without this random seed it'll just generate slightly different data every time now what i'm doing here is i'm just using within the numpy package or numpy library there's this random module that has a bunch of uh basically probability distributions that you can sample from so if i sample from a normal distribution i'm basically just like sampling from a bell curve and i can specify the mean and the standard deviation of that bell curve which i've specified as 167 and three and then um three is the uh variance or the standard deviation and then 150 is how many values i'm sampling so i'm just gonna break this down for a minute so that you can see what i'm doing um but if i run this um and just print out the results i'm just getting a whole bunch of randomly generated numbers that have a mean of 167 and a standard deviation of three so you can see all of these numbers are pretty similar to 167 they vary um around that usually within two standard deviations or three standard deviations so like you're not going to see many numbers that are more than nine greater than 167 so you're not going to see much that's larger than 176 but and then or smaller than 167 minus 9 but basically just randomly generating some numbers and then in this next step what i'm doing is i'm taking um the numbers that i just randomly generated and i'm just like multiplying them by some number and adding some numbers so you might recognize this as basically the equation of a line i actually grabbed these numbers from a regression and then i'm adding some random noise so if i didn't add this in um and you can see a plot down here of the data that i just randomly generated this is the um height and this is weight right that i randomly made up if i didn't add this random noise um these would all be in a straight line because we would be simulating our weight variable to be exactly linear with height so i can actually demonstrate that um really fast if i do something like this i get a bunch of points in a straight line um and then now right i add this back in i've simulated some data it looks like this i'm calling it height and weight just to make this i don't know i guess a little bit more uh grounded and like a physical example but um but yeah that's all we're doing here i see oh yeah i see a question about are we doing the ols without an api um so i assume you're asking about this like oops uh this statsmodels.api uh library if you're not asking about that then you can clarify um but so we actually managed to do it without the library at all uh in the week that nitia was joining me i think that was two weeks ago uh we did it with some matrix algebra in numpy um and then there's definitely some code throughout to do this in scleron as well if you're interested in learning another library um but today we're gonna still stick with the statsmodels.a module because again it gives us some good information that's useful for for debugging models cool all right so today we're going to talk about some of the things that can go wrong when you fit a model and then we're going to talk about some of the ways of fixing them fixing those things um sometimes you're gonna notice that something went wrong uh because you got a warning when you fit the model sometimes you won't get a warning and you'll be checked you'll need to check the assumptions of the model and then you'll see something that surprises you and that will that will clue you in that you need to do a little bit more investigation so um so i'm gonna start by just fitting a model with this data that i randomly simulated and we've seen something like this before i'm gonna fit a model where i predict weight based off of height and then when i fit this model i see and we've seen this the same warning before um but i see this warning that says the condition number is large this might indicate that there are strong multicollinearity or other numerical problems okay so i think last time we saw this um the strong multicollinearity part um we talked about a little bit more in depth and we we discussed how that has to do with having predictors in the model that are colinear with each other or correlated with each other and we talked about why that might not be a good idea to include multiple predictors that are highly correlated um and but in this example we've only fit this model with one predictor so there's no chance that we have correlated predictors um so we need to think about what else could be wrong in this situation what else could be causing this warning and so um one of the things let's look at this picture that you might notice when we we first look at this this graph is that it's automatically in matplotlib is plotting this so that it fits it perfectly fits within the axes but we see that it starts at like 1 60ish or a little below 160 on the xaxis and it starts at like 54. on the yaxis and so if we get this intercept of negative 23 and we talked about this i think in one of the first live streams um in order to visualize what that intercept looks like we really have to zoom out a bunch on this plot so i think i have yeah so here this plot is again i can i'll pull this up here so you can see it um all in one place right so if we just plot this looks like this if we plot it and we change the axis so that the xaxis starts at zero and goes up to 180 because that's roughly how far up it goes on this xaxis and then on the yaxis we have it start at zero and go all the way up to 70. um we're gonna get this kind of like squished plot um which is to say that like the negative 21 and here i can even have the uh the y axis go down to like negative or was it negative 23 yeah so we'll have it go to negative 25. this is coming from the fact that if i drew this line in and we can even let's draw it in um we can do equals dot all right 0 180 and then y i'm doing it the the oldfashioned way we'll grab we'll even grab these so the intercept is negative 23. and then the slope was 0.5166 if we add these to our plot in the form of a line right we see that so this is our regression line and the negative 23 is the yintercept and the reason it's negative 23 is that in order to draw this line we've got to go all the way back to the yaxis which happens at x equals zero or height equals zero and then it hits at negative 23. jamie do you have any um any thoughts about like why this might be problematic or like what um what might happen if the data were to just change slightly like if we changed the the slope of that line just like ever so slightly um because one of we had a point down here um all of a sudden what would what might the issue be um i think like a main issue would be that like a slight change in the slope so like any outlier that like it potentially changes the plot would change the intercept like pretty heavily um so i think like that is one issue i see is that like the manager you're talking about or yeah yeah that is exactly what what i was intending so right so um so yeah so this is like a kind of unstable estimate for the intercept because and i think we talked about this a little bit when we started when we first talked about um the definition of an intercept or like the interpretation of an intercept so um jamie i don't know i don't remember if you joined this uh first live stream or if this is if you're caught up on this which it's totally fine if not but you remember how we would interpret this intercept it's like value of negative 23. yeah yeah so since we were talking about i think it's xaxis is height yaxis is weight or is it yeah um so this would be this interception is basically saying that a person who has a height of zero i guess centimeters has would be expected to have a weight of negative it's like 25 22 or something um pounds or whatever or kilograms or whatever it's um the scale is so it's probably like not super informative here or like not super uh what's it called like sometimes you'll have intercepts that like do make sense um like if you're doing like hours studied versus expected test score like zero hours studying expect tests where like that could make sense um i don't know anyone who who sorry who has a height of zero centimeters who has a negative weight um but so yeah i think i think this one um is potentially a little bit uh less less informative um yeah yeah exactly um i realized it maybe is more useful if i use this like sns scatter plot and um because then i think it like plots the uh or at least gives you the labels and we'll do that let's see if that works better yeah there we go um so yeah so exactly so this is the predicted value of weight when height equals zero but just like jamie said there's no such thing as having a height of zero right we have no data all in this part of the plot so drawing the line there like we can say that we the there's this relationship but really we only want that relationship to want to say we don't really only care about that relationship where the data exists which is only in this part of the plot and so we're estimating something that's not really of interest to us and it's also an estimate of something that's really easily changed by just like small changes in the data because it's so far out from where the data is that it's it's kind of unstable so there's a number of things that we can do to fix that um and so this is maybe one thing that's not in the article that i originally wanted to include in this course and we just didn't have time but but i'm going to demonstrate it now so actually before before we even get there um one thing i also actually sorry i'm changing my mind as we go uh let's yeah let's actually let's fix this first and then we'll we'll go back and talk about some of the other assumptions so um so there's a couple different ways to fix this one way to fix it is simply to center the height variable um and so i'm going to talk you through what centering means in just a second and we can actually do it together so to center something all we do center a variable all we do is we take the mean of that variable and we subtract it from every single value so i'm gonna go ahead and in my data i'm gonna find the mean height so i'll print that out first so the mean height in this data is about 167 centimeters and then what i'm going to do is i'm going to subtract that from every value in the height column and what this does is it just creates a whole list of values of heights um with the means subtracted um so jamie do you want to like talk through what what are these numbers now interpreted as like if we see 4.928 what does that mean uh so i think that means that like this specific like data for this specific data point um it's like 4.9 like the height like this person's height is like 4.9 um i think centimeters greater than the mean so it's basically like the distance just like from the mean exactly so yeah so this person is 4.23 or sorry for 4.93 centimeters above the mean in terms of their height this person in the third row is very very close to the average height in the data set so if they had a value of zero it would mean that they were average height so what we can actually do is we can create a new variable we'll call it height centered that contains these values and then i'll print the um the first few rows of the data again to just take a look um and so all we did was we said okay we've got weight we've got height now i want to create another column that's called height centered and these values are now centered at zero right so a value of zero means average height anything above that means they're above the average height anything below that means they're um they are below the average height they do that right yeah yes okay so now what we can do is we can refit this model but use centered height instead right and you'll notice that that other warning went away so remember the first time we got this the condition number is large um this time that went away and we just get standard errors assume that the covariance matrix of the errors is correctly specified which i believe we get no matter what um that's always an error or a warning here um okay so that's one way and we're just kind of dipping our toes in that we can start to transform our data and by transform i mean take existing data and kind of do something to it so that we can fit a model that is a little bit more stable that doesn't create warnings that is potentially like a better fit to our our data um it's easier to interpret all of the above so i think one thing that is maybe useful before we move on to more complex example is to now look at what this picture looks like so let's go ahead and i'm actually going to use i like this lm plot function sometimes because it just saves us a little bit of time so let's replot on the xaxis now height center and on the yaxis let's replot weight and then the data is just called data and this will automatically plot the regression line show run um cool so now we see that the weight variable looks exactly the same but height centered right like the middle of these data points is around height centered equals zero which makes sense because that's the mean and and let's actually add let's add like a vertical line to this okay the code to do that somewhere um it's basically like this but ax line so this right here where i've drawn this dotted line this is the yaxis now so um so now the place where we cross the yaxis and i think uh we can estimate i i'll fit the model in just a second so we can see what it is but it looks like it's around like 62 point something maybe 63. um that value jamie do you want to take a another stab at interpreting that number um so i think it's like if you have a mean if you have like a height that's equal to like the mean of the data then you're well so i guess it's like when we centered it we're like at zero now so like that is the y that's like the x equals zero point um but i i'm guessing we need to say like if you like if you're at like a height of whatever like that pretransformation was and that's like what you would interpret it so okay it's like if you have like 62 or something or whatever the mean was and you have an expected weight of x of like 62 point something exactly so if your height centered value is zero it means that you are average height so the expected or the average weight of someone who is average height is 62 point whatever is how you would interpret that so now we've got a more interpret interpretable value and it's a little more stable right because like the intercept is now kind of in the middle of all of the data and so if we have another point somewhere it's going to be less likely to change what this value is um than before so um cool actually i think let's really quickly fit the model and just see what this looks like so if we go ahead and fit our model but now we use height center we get right an intercept of 62.7 our um our slope on height centered 0.5166 is exactly the same as our slope on height so the slope of our line didn't change it just changed the intercept and now that 62.7 is average weight of someone who's average height in the data and so we've got a situation where we can interpret it it's more stable we don't get um we don't get errors oh i already did this down here sorry um okay cool uh if there are any questions in the chat please let me know otherwise i'm gonna now move on to some log transform stuff so um i think one of the things that's really challenging about data transformations in general is that there are a ton of different ways that you can transform your data and it's hard to know exactly which one to do in any given situation so um one thing i want to say is like there are no hard and fast rules and there's also way more options than what we're going to cover today um for example we could instead of centering we could just standardize everything so we could also divide by the standard deviation um that would change that the slope and change our interpretation of the slope um but there are definitely situations where we'd want to fully standardize instead of just centering i also see a question can you explain the shaded area on the graph around the best bit line that is a great question um so this lm plot function it automatically creates this like confidence interval around the line which basically shows how confident we are that the line um is where where it's pictured so or like how like how sure we are that the line is here so you'll notice that like on the ends of it um this shaded area is wider here i can zoom in right the shaded area is wider on the ends because in this um like in this uh part of the the graph where there's not a lot of data and so we don't know like if we had one more point we don't know if it would be like close to this if it would be here if it would be up here um and that could potentially shift this part of the line a lot more than in here where the bulk of our data is uh we're more confident that the line goes through this point actually i think it would be potentially interesting we do like a lm plot on this what happens if i do that interesting so it won't even graph it outside of where the data is i'm assuming let's like let's make this like uh 130 or something yeah you can see it's like graphing this line in here it won't even graph it down here but you could imagine and it would be really nice if it did because i think you could visualize that if you see how and i know you have to look kind of closely but if you see how like the um confidence interval is really tight in here and it's it gets wider and wider and wider it would just get wider and wider and wider the further away from the data you went um so you could really see what i was talking about before where if you drew this line all the way down here that confidence interval would get really wide um around here meaning you're like very uncertain about what that intercept is that you're trying to estimate um oh okay i see uh alex noted that sns.lm plot is sns.reg plot and the latest version of seaborne fyi um yes uh the reason we have been using lm plot i think lm plot still hasn't been deprecated as far as i know um and we have we had uh an older version of seaborne on codecademy so we we don't have ragplot i think yet um but that is good to know thank you oh and then i see another comment wiki dd says i think they are still two different options with slight difference figure level or not not exactly the same arguments that is interesting i actually did not know that let's like i'm gonna just look it up really fast i think this is kind of cool um interesting it looks similar but it looks like it has some additional options yeah alum plot still definitely has not been deprecated um i think we used a reg plot for one of the off platform like the off platform data analyst and final portfolio project remember should we just try it see what happens oh no said x y and data right oh it's because i haven't recreated this yet so i guess in this example it basically looks the same but interesting it would definitely be interesting to do a um more careful comparison and see what the what the real differences are okay so um before we kind of move into the second half of this live stream i think it's useful to take this data which besides the fact that um it was we had this issue from before where like the um the intercept was kind of far away from the data points besides that that issue um this data is simulated such that it fits a linear a linear model really nicely in fact um i think it's kind of cool to just see um just prove to yourself that like things are working as they're supposed to um so when i simulated this data i used i used some numbers from a regression but i simulated it to have an intercept of around 21.7 and a slope of about 0.5 and then when i and but i randomly generated the point so it wasn't going to be exactly these numbers but this is what i kind of simulated i simulated an exactly linear relationship with some error um and then when i fit this model the first time my intercept was close to the negative 21 and the height and the slope was close to the 0.5 that i originally kind of set out to use to create the data so with that said right like if i create data that perfectly fits a linear model then i can see kind of what my checks of the assumptions of linear regression should look like if everything is kind of perfectly uh perfectly perfect for for the purposes of um of linear regression so i'm going to take a look at two different uh assumptions of linear regression so the first assumption that we'll talk about is homoscedasticity which is a very fancy word but it's pretty i think hopefully easy er to understand once you take a closer look so homoscedasticity essentially means that there is equal variation of the outcome variable for all values of the predictor so or predictors um if you're in multiple linear regression world so here um for height centered or height either one we notice that like the amount of variation on this part of the line is pretty similar to the amount of variation over here um and so because the variation is roughly the same for all values of height um the variation around the line is roughly the same for all values of height um this data follows the homo homoscedasticity assumption of linear regression um and we actually again we created the data to to do this because we added random noise centered at zero with the same level of variation for all of our values when we first created the data and so the way that we often check um homoscedasticity is we get the fitted values for our model the way we get the fitted values is we just predict um get basically like the predicted values for every for the original data itself um and so i will kind of show this i think we skipped over this in the first week but i think it's like important to kind of come back to now so the fitted values here are just the predicted weights for everyone in the data set then we can calculate the residuals which are the difference between the actual weights of every person and their fitted value or their predicted weight and then we can plot the fitted values against the residuals and we expect that um or sorry the residuals against the fitted values so we these are the residuals right the yaxis is the residuals these are the fitted values and we expect that the residuals should be centered at zero right because on average all of the points should be like equally or basically like some of the points are going to be below the line some of the points are going to be above the line but the line was defined such that um it's kind of going through the middle of all the points it's minimizing that vertical distance between all of the points in the line and so we expect there to be just as many points below the line as above the line um and we want this basically we want this plot to just look like a splatter we don't want to see any pattern any patterning we don't want to see that like the variation on this side is different from the variation on this side um and i'll show you what that would look like in just a second we also generally want to take a look at the distribution of the residuals um and we expect that distribution to be roughly normally distributed um so we don't want to have a lot of skew we don't have like a bunch of really small residuals and then um very few large residuals um so this looks pretty good it looks pretty symmetric there's one kind of hump around zero and and we feel pretty good about this but now let's go into a land where um where this is not the case and then take another look at how we could kind of get around that so going way down okay so here is some more simulated data that violates the homoscedasticity assumption and so basically i simulated this data so that the this error term is related to the one of the predictors and then um that means that like essentially for larger values of of age which i'm going to make my predictor in this made up data i'm adding more variation and for smaller values i have less variation so okay so is that kind of showing up in the i can't tell if i'm like seeing this one or not but um the shaded area for the alum plot it's like a little bit wider for like the data that's more like varied um in like the higher age yeah yeah i think it is that looks right um okay so uh i just honestly made up again some labels so that we would have something to kind of refer to instead of just calling it variable one and variable two so let's just say this is like age of person there are some very old people in this uh in this data but that's fine 120 yeah um and this is like how much they spent maybe should we say like on healthcare per year or something i don't know like making it up totally fake data um but but we see this kind of funneling basically where there's like very little um variation around the line for small values of age and for large values of age there's a lot of variation in spending like people who are 100 years old are spending between 40 and 140 versus like people who are 50 years old are really just spending between like 42 and 47. um so we're not going to get any warnings like if we fit this model predicting spending based on age we're not going to get any warnings at all besides this like our typical standard errors assume that the covariance matrix is correctly specified um but if we were to go through and take a look at that plot that we just made for the last set of data we'd see that we get this kind of like so again this is the residuals against the fitted values we start to see this like funneling effect um i think the the like official term for it is funneling i don't know at least that's what i've seen so um this is happening remember because if we go back to this plot and say and this is the line that we're fitting the residuals are going to be really small down here like these are very small residuals um and the residuals are going to be really big over here like this point for example is going to have a really big residual um and this point is and this point is so we've got smaller residuals for smaller values of age and larger residuals for larger values of age and that's what we're seeing here smaller residuals on one side of this graph larger residuals or a bigger range of residuals on the other side and if we look at a histogram of the residuals they're still roughly normally distributed they're still in this example um residuals that are small and residuals that are large and um there's still more common for small to see small residuals than large residuals um but we're we're violating one of the assumptions the homogeneous to the assumption of linear regression so um i'm going to break for a second because i see that we're going to have we have like 12 minutes left and i'm gonna um try to save a little time i think by looking at this uh article really fast so in this article we see an example where there's both the issue of homoscedasticity is violated because you see there's like a lot more variation over here than over here but also the issue also in this example the residuals are not normally distributed um there's going to be i think a lot more large or yeah a lot more like a lot longer tale of larger residuals than small residuals um is there a point at which like like um for like it to like like hit the like the residuals are normally distributed is there like a point at which like you realize that they're like it doesn't like hit the like it's like not like a good amount of normality um because like sometimes like you'll have a graph there's like some slight skewness versus like a lot of skewness yeah that's a good question so there's actually so there are some more rigorous ways of checking the normality assumption um one is a qq plot which basically allows you to visualize how far off the points are from like what they would be expected to look like if it was a perfect normal distribution um but then there's also a hypothesis test i'm great i'm blanking on the name of the test um but i'll i'll look it up and i'll post it somewhere later um but there is a like a hypothesis test you could run for normality uh which is something that people sometimes do and just like set a um a significance threshold ahead of time and then um and then yeah is it the shapiro wilk yes yep that's it yeah i was like thinking that it started with a w but yeah yes shapiro wilkes um is the one that i've seen most commonly used um okay so the other thing like that i notice about this right off the bat is that um is that this is kind of this data looks kind of uh not linear in that um like if i were to draw a line through this set of points i would draw a curved line right like it would probably start up here and it would like curve around to here um and whenever um and whenever you see kind of like this nonlinear relationship that also can be a indicator that you maybe need a different method to model this relationship whether it be adding a polynomial term or in the case of a log transform log transform can help manage this like homoscedasticity violation which would be called heteroskedasticity i believe um and then also help with this lack of normality among the residuals i also see alex said i think some people also check the two parameters skewness and kurtosis to assess normality yes um yeah so there are summary statistics that you can calculate called skewness and then i think kurtosis is like how um it's like peakedness of the um of the distribution i believe and you can you can calculate those values and then use that to to assess if you want like a not hypothesis test method um yeah for sure so um basically log transform generally what we would do is we would take a take the log of the outcome that variable and then fit that in the model so i'm gonna do that back in my code um instead of having spending i can just like before when i had centered um centered height i can do log spending and i can end up with therefore a new column where i've got the log of my original spending column and then if i plot age versus log spending again you'll see that this looks a little bit better than the original data it still got a little bit of fun funneling but it definitely like improved that slightly so again like this was the original where we've got quite a lot of funneling happening and then this is after we plot age versus log spending instead of age versus spending is there a way to like know that like you should try a lot transformation on a like y variable like is like funneling like a good time to like try a log transformation or yeah so there's lots of reasons why you might try it um one reason so i think like one reason that you might try it is if you see that funneling another reason you might try it is if um if you have skewed residuals so like a lot of times what happens with the skewed residuals is that you you have nonlinear um you have a nonlinear relationship so like you could imagine if these points were even further like above this where it was like points really following like this this kind of a line um that kind of shape you end up with really small residuals down here and then really large residuals up here overall and so you end up with this like weird distribution of residuals um which really comes from this like nonlinear relationship that's another time to try a log transform um and also like right off the bat sometimes you'll just notice that you have a outcome variable that probably benefits from being put on a log scale so a lot of the time that will happen with measurements like salary or or cost of something rental price something like that where you end up with a really skewed distribution of your outcome variable um or like very large numbers down to like very small numbers like in this case we have a whole bunch of values that are close to zero uh in terms of this like phones variable and then we've got some values up to a thousand and so taking the log just helps normalize this like y variable like i'm pretty sure if you were to plot the distribution of phones um it would be skewed cool um all right we are we're running tight on time so i'm actually gonna kind of switch over to showing you well i can show you quickly on this example um so we took the log transform we fit our model with log spending instead of regular spending and then we end up with a lot less funneling in this plot there's still some funneling for sure like it's not perfect um but this definitely helped us um build a slightly better model um i think i forget if there's a picture in this article yeah so now um i think this one is maybe a better example i was trying right before this to simulate some data to kind of show you something that looks more like this but i was being slow to simulate data so i think it's it's maybe better to just show you this um but if we look at this relationship now between log phones and birth rate we see that the relationship between law the log of phones and birth rate is a lot more linear than the relationship between phones and birth rate and so by taking the log we've made this more linear we've addressed the homoscedasticity issue a little bit more there's less of this funneling in the original plot and so when we take a look at a histogram of the residuals and the residuals against the fitted values we end up with something that looks a lot more reasonable this looks a lot more normally distributed and this looks a lot more like a splatter of points we don't get that same funneling as we had before there's a whole bunch more in here that i was hoping to cover today but i don't think we're gonna have time about the interpretation or like how you would interpret a model where you took the log of um of the y variable or the outcome variable it's really just a bunch of math so what you're fitting in this case is like log of the outcome variable is equal to this like intercept plus slope times birth rate and so if you want to solve for just like phones um then you have to take you have to exponentiate both sides so i think in python if you do mp.log it's the natural log so um right you end up having to exponentiate mean like take e to the log of both sides and so you can kind of work out what this what this relationship is if you take if you exponentiate both sides and solve for this um i'll leave that to everybody to read the article on your own but i hope that this live stream gave at least some examples and some first sense of how you might want to transform some of your data so that you can more effectively model it using linear regression well um so with that i will wrap things up um we are we've been having office hours on thursdays on discord so you can join us there if you have any questions i know there was a really good question when i checked yesterday um and i'm going to get to that today try to answer that on discord but we'll discuss a little bit more um on thursday and feel free to join us if you'd like to ask anything um there's also a chat on there so you can leave us any questions um if you want us to like take some time to think about it beforehand and give you a more comprehensive answer cool anything to add jamie um yeah no thanks for letting me join you today sophie um it was very fun and i learned a lot i'm sure everyone watching did too sorry all right well thanks for joining me all right have a good day everyone