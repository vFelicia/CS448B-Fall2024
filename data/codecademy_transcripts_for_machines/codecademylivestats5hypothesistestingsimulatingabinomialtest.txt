all right i think we're good to go hey hello um okay i think we will get started in just a minute but i see that some people are starting to say hello in the youtube chat uh thank you dr monkey uk for always letting us know that you can see us see we're getting better at the awkward silence at the beginning yeah we just really leaned into the awkward silence this time i know i have no idea until after i watched this later how long that was cool um so i well we say this at the beginning of every stream but we're streaming on a few different services but we're keeping an eye on the youtube chat so if you'd like to chat with us ask questions um please go there and and let us know we'd love for this to be as interactive as possible so we love your questions and would love to hear from you if you have any suggestions or ideas cool uh sophie do you want to give a little preview of maybe what we did last episode and then uh what we're doing today yes also thank you for the the haircut shout out my uh my boyfriend did this and i'm it's a little uneven it's pretty good actually yeah i just opted for no haircut in the 11 months um yeah so last week we went over the central limit theorem um and we started to get an understanding for inferential statistics which is a field of statistics that's focused on making inferences or i'm trying to think of a better word than inferences uh like making judgments uh about or guesses about a population based off of a sample that we can observe and that's really the basis for a lot of hypothesis tests and now today we're going to jump into an example of a hypothesis test where we're specifically going to look at the binomial test and the binomial distribution um which is maybe not the most ubiquitous or like most common hypothesis tests you probably or most people would think of like a ttest or a ztest is the most common um but it's a fun one because it's really easy to simulate um and you don't need any math in order to basically write a function that can run a binomial test and i think that when you're first learning something understanding how a function works is really important um and more important than being able to say in python just use a hypothesis testing function like there's lots of functions that exist in various libraries for conducting a ttest or a binomial test or chisquare test or any sort of hypothesis test that you want and all you need is one line of code but if you only ever learned how to implement the one line of code um you might not have a sense for how those functions are really working and that can make it harder to interpret things or to figure out if something went wrong and then it's also just i don't know i think a good opportunity to kind of think about inferential statistics in general what the goals are what the assumptions are and build your build your understanding of of the topic at hand um yeah someone that doesn't have a super strong statistics background i think that this is the kind of stuff that's really helpful for me where it's like i kind of know what these tests are i kind of know in what circumstances to use one over the other but really diving in trying to build this thing yourself seeing the um using the simulation to see what this uh what these functions are actually doing um can help give like a more solid understanding of what they're doing so then when you're out in the real world and you encounter a new situation you can say like okay this is exactly like you know what we did in this lesson versus another lesson exactly so with that i am going to share my screen um let me make sure i get the right screen um we're back to uh jupiter notebooks um these notebooks are in our github repo if you want to download them with the data um also just a heads up we added them like 15 minutes ago 20 minutes ago uh it's a little bit running a little bit late today so if you checked the github earlier today um and didn't see them they should be there now and uh if you're watching this after the fact you can go ahead and download those and run the code on your own computer cool so um today's lesson i'm going to start with the starting code file and load this monthly report file and i'll print it out um so this is actually directly from a codecademy lesson so if you're interested in taking that lesson uh i think maybe alex can post where it is yeah drop the link to the lesson um this this live stream series is all kind of a companion piece to the the new skill path that just launched um like a month or so ago so i'll drop a link to that in the uh in the chat yeah so this uh this is a simulated data set that alex and i actually made we had quite a lot of fun making it um but we're gonna pretend like uh we we work for our company and um and this company is collecting some data on purchases and items that were purchased and so uh we're imagining this this company is selling uh props to use to recreate your favorite scenes from movies um and so this is what alex and i spent probably a on a stupid amount of time on on a day when we were having trouble focusing is coming up with all of these props that you might buy um so you can take a look at the full data set if you're interested and see what other things we came up with there's some there's some funny ones in there um but this is this is the first few rows so we're imagining that um that there's there's data i think um we can get like monthly report dot info and just see a little bit more so there's 500 rows um 500 non nonnull values so nonmissing values for each of these two columns um and each one is telling us why or yes yes or no did the did the customer who visited the website make a purchase and um and then if they did make a purchase so if this is a y where we're saying what did they buy and then the item is listed here as well so um basically we're gonna imagine that we are a um like a marketing team and at this company and we're trying to understand we have some idea for what the purchase rate among visitors should be um and we're gonna see if the purchase rate among this sample of people is lower than normal or higher than normal um and essentially the idea here is that maybe there was like maybe there was something that we were testing out like we were testing out a new feature or a new checkout system or there was a bug that some people saw and we want to know we want to have some understanding of whether that change had an effect on our purchase rate and that and we're thinking about this from the perspective that let's say it was a new feature like um a new checkout system that we that we randomly showed to some subset of our visitors if we did that we want to know whether if we show if we showed that same feature to all of our visitors would the purchase rate really be different from our expectation but in this case we only have a sample we only showed it to a small a small proportion of all of the visitors who could ever arrive at our site and so we don't know whether we don't know what would happen if we showed it to every single person and so with that in mind we need a hypothesis test to try to understand whether the difference that we observe in this sample is large enough that we would expect that difference to persist if we showed this thing to more people so so in this example we have 500 rows maybe that's like the purchase it or the people that visited the website on a on you know a tuesday or whatever the most recent day that we pulled this data we have 41 people bought something and so that's a percentage right it's 41 out of 500 that's some percentage and we're curious does that is that percentage different than what we expect right exactly and where does that that number that we're going to be comparing it to if like what we expect in the real world where does that number come from is that just like historical data of like oh my company the alltime set sales percentage is you know this percentage or that percentage so yeah that's a really good question um i think in general yes it comes from historical data we do have to be careful about thinking about how we use historical data given the sample that we have like we really want our sample in order to make this in order to meet the assumptions of this test and we'll go more into that once we get started we need the sample to be representative of the population and so if we're basing let's say we're basing our historical data off of like an entire year's worth of data but maybe like maybe this purchase rate fluctuates over the course of the year like maybe it's higher in the winter and when people are inside and looking for things to do and lower in the summer when everyone's at the pool and or whatever or outs outdoors um and so maybe you know maybe the change that we're if we're only sampling from one month or one day then um we shouldn't really be comparing to like all historical data uh so we we do have to be careful about our choice there um in real life but yeah it can come from a lot of different places like you'll also see these types of tests if you're if you're taking a sample for um for like a experimental study and you're trying to see whether your sample um is representative of the population and you have some averages or you have some like expectation based on like previous research um that has been done or yeah or sometimes it'll be like a cut off score like if you if 50 of or if you get 50 or higher on this test you pass or something like that so how about like oh if the folks that we're showing this new feature to or you know the new checkout system too if those are like the most recent subscribers or the most recent people to visit the site is that an issue where that's like the most recent people is not representative of the entire population i mean yes it's true that that's all that's always going to be the case but like at some point you do have to uh like kind of give up on meeting all assumptions in real research unless you're running like a very controlled experiment where you are splitting people into two groups and randomizing everything and yeah cool okay cool okay so we are going to get started then um so okay so let's just imagine um that the the expectation is that the purchase rate is usually about 10 so like about 10 of visitors to this website usually make a purchase and we want to know if the purchase rate was significantly different from that so the first thing that i think it makes sense to do if we're going to try to answer this question is figure out what the purchase rate is in our data so we've got 500 visitors the site we want to know if the purchase rate was different from 10 percent so let's let's see if we can calculate the purchase rate in this sample and i'm actually going to divide the purchase rate into two numbers because it's going to help us with this like the way the binomial test is set up so the first thing i'm going to calculate is the the numerator the the top of that fraction for the purchase rate which is the number of people who made a purchase um so there's a few different ways to do that alex do you have i'll do it the way that i that like makes sense to me and then uh you can let me know if there are other ways that you've thought about doing this sure um but so monthly if i do mp.sum and then i take monthly report equal equal y so actually let me break this up what i'm first doing is monthly report is equal to yes is going to return a bunch of oops i should do dot purchase so we just get one column of this is going to return a bunch of trues and falses telling me whether or not that value is equal to y so it goes true false false true false because here we've got yes no no yes no and it turns out that there's this nifty thing where true gets a gets cast to a one if you try to perform any sort of operations on it and a false gets cast to a zero so if you take the sum of all these numbers it's gonna add a one every time there's a true and a zero every time there's a false so when i do this i will get a single number which is 41. yeah funny that you prompted this so because as you were developing the uh the course on codecademy this is something that i remember reviewing of like the sum here seems weird to me the way that i would have done it is i would have gotten all of the rows or like i would have extracted all the rows where that that column is a y and then i would have just counted the total number of rows um so i would have like filtered it as opposed to um summing it i guess so you would have done like yeah that yeah so just to repeat what alex just said he's taking the whole data set he's subsetting it to just the rows where a purchase was made and then he's figuring out how many rows are in that subsetted data set by taking the line so and you'll see we got the same numbers that's good and the length of the whole data set we already saw this from the info section but if we say len monthly report which gives us the row number of rows um that was 500 okay so the purchase rate is going to be 41 over 500 and i'll just print out what that is it's 0.082 okay so remember that we said our expectation was 10 percent so that's 0.1 here i can put this actually as a percentage because it's probably easier to read so it's an 8.2 purchase rate our expectation was 10 so this is a little bit above normal i also just saw somebody posted in the comments um you could also do value counts um with normalize equals true and that's totally true as well there's as per usual there's many many ways to do this i'll also demo that because it's kind of fun that'll give us both the number of yeses and no's so that gave us this 0.082 for the s's and 0.918 for the nose and if we took this normalize equals true piece out we would get the counts so that's cool um okay now and so we've seen that the purchase rate was lower than normal but that doesn't fully answer our question because like we said at the very beginning of this just because the purchase rate is 8.2 percent doesn't mean that there's really a dip in the purchase rate if like ev if we showed the same feature that these people saw or the same bug that these people saw if we showed that same thing to all of our visitors because this is just a sample and there's some random chance involved so one of the things i like to think about and i think like this is probably the natural the natural progression or like the natural way that many people teach the concept of the binomial test is to think about flipping a fair coin um i like it as an example so if you imagine you're flipping a fair coin right if you the sorry the probability of heads is 50 so you would expect that about half of your flips would be heads say so let's say you flip your coin ten times you expect that about five of them are going to be heads but because this is a stochastic process it's a random process it's not going to be exactly five every single time right it's gonna be it's gonna vary each time you do it if you flip a coin ten times you might get four heads and if you do it again you might get five if you do it again you might get seven um but the more you do it the closer you're gonna get to that 0.5 or that half level so we can actually demo this um in python i'm gonna so i'll demo actually the the coin flip first so let's say we do um numpy.random.choice we're gonna choose between heads and tails and we'll for right now we'll do one or actually let's do 10 coin flips and we're going to say p equals 0.5.5 which is to say that the probability of heads is 0.5 and the probability of tails is 0.5 if we do this whoop shoot i think these are supposed to be one list sorry yeah there we go we just flipped a coin 10 times on a computer we got heads heads tails oh my god we got nine heads wow that's crazy let's try it again okay now we got closer wow it's still really funny we're getting like six heads in a row and then four tails actually i was reading about this because we made some prop bets for the uh for the super bowl that apparently it was like i think a bunch of heads or tails in a row it was like a bunch of tails in a row then a bunch of the last five six or the last seven have been tails or something like that of the super bowl like uh coin flips that's funny yeah so you know this this happens right you get a bunch of the same thing in in a row just by random chance um so now i'm going to up this a little bit because i don't want to print it out every time so let's save these as flips and let's just print out let's use this same code that we had up here to every time just print out um flips equals head so the number of heads and now let's up the size to like 100 say okay so not 100 flips you got 46 heads 49 and and so bringing this back to our example of the um basically this is kind of like okay we saw 41 purchases is that you know if it was actually 10 we would have seen 50 right because we had 500 total um or we would have seen yes 50 sorry yeah right because 500 people either said yes or no i'm buying the thing or not we expect it to be 10 so we're saying okay that would be 50 people we saw 41. so is the question here that we're asking like how big of a deal is that 41 is it like getting a 52 here or is it like you know if we if we keep printing this out we're pretty much never going to see a 90 or a 100 right um so that's kind of like the comparison here of like how far away are we from the expected thing that we that we expect to get exactly so right so the question here might be like if you're let's say you uh work at a casino and you're you're checking uh a coin i don't know i guess they don't really use coins at casinos but you're checking to see whether this coin is fair like your friend has decided that has offered to play some game where you win if the coin comes up heads and they win if the coin comes up tails and so you're trying to check whether the coin is fair you can't flip the coin infinitely many times to see what percent of them come up heads but you can take a sample so here we're kind of thinking of the sample as being like a sample of all the coin flips that could ever exist it's a really weird thing to think of like a population as being coin flips and a sample as being some subset of those coin flips but that's kind of how we're thinking of it we're thinking of it as like population is all the coin flips that could ever exist in the world and of all the coin flips that could ever exist in the world of a fair coin half of them would be heads um and this is some subset and based on that subset you're saying okay so we expect some random chance so we we expect that even though this should come out too close to 50 it's not going to come out to exactly 50 every single time we might get some weird numbers like just by random chance that time we got 41 heads this time we got 48. but if we flip the coin a hundred times and 99 times it comes up heads we could be pretty convinced that it's not a fair coin um because that would be kind of outside the realm of possibility here by random chance yeah and i i have a question this might relate to something we you do in a second but like to me it seems like there's two things involved here one is the sample size of like okay we did it a hundred times and it's off by this amount and then there's also the how much is it off by so like are those two connected in some ways of like okay i flipped my coin a hundred times and um you know uh it came up heads 70 times versus i flipped my coin 10 times and it came up seven times like how are those two things connected the the sample size and then also how far away you are from the expected thing what a great question alex that was really good um so actually this comes exactly back to what we discussed last week so last week we were looking at a normal distribution so okay let me actually just show you something so let's say i say flip results equals empty set and we're going to do for i n range thousand um and then we're gonna say or you know what actually i'm gonna instead of showing you all this code i'm just gonna show you a picture that i already have i'm gonna pull it from codecademy um okay we're gonna i forgot i didn't fully share my screen hold on actually okay we'll do this sorry um okay so we've got uh we're gonna do this a thousand times every time i'm gonna flip this coin save the number of heads and then i'm going to append that to to my list and then i'm going to use i'll run this i think i can do it 10 000 times without too much of an issue and then i'm gonna print a histogram of that yeah and i think as you're as you're writing this out a thing that gets kind of confusing here is we're now doing this experiment of flipping the coin 100 times we're doing that whole process 10 000 times so there's kind of like layers within layers here exactly so yeah i think i coded that really quickly but just to repeat what i did so by creating by putting this inside of a for loop with ten thousand iterations i said i'm going to repeat what i just did up here which is flipping a coin a hundred times and recording the number of heads i'm going to do that process 10 000 times so 10 000 iterations of me flipping a coin 100 times and recording the number of heads and then here i'm plotting a histogram of um those results and actually i'm gonna like uh run this again because this is a little well okay it's a little off center but if i did this enough times this would be like perfectly kind of centered around 50. and this looks a lot like that's a really weird one 15 make this look nice yeah i should just be starting to use the seabourn the seabourn one that just automatically does this part for you finds the right number of bins um cool so uh anyway we plotted this histogram of the number of heads and we see kind of like we expect the most common outcome is that it's close the number of heads is close to 50 and it looks like the number of heads is ranging roughly from like 35 to 65 with this one outlier like one time we did it we got 70 heads which was a lot but for the most part it's like roughly between 65 and 35 and each of the farther away you get from 50 it's less likely this is called the binomial distribution um and you might think after if you watch last week sorry last week's stream you might think like well that looks a lot like a normal distribution and it actually really is the only difference between this distribution and a normal distribution is that this is what's called dis a discrete probability distribution or a probability i think it's called probability mass function um where the value the number of flips that you can have that come up heads has to be an integer value like it it can only be 35 36 37 it can't be like 35.5 because you can't have 0.5 heads but otherwise this is really just a normal distribution and actually like the central limit theorem essentially applies here the larger the numbers you use like the higher the number of flips the closer this is to a normal distribution um so it approaches we say a normal distribution um and remember from the central limit theorem that the standard error which is describing how wide this thing is is calculated as the standard deviation of the population divided by the square root of the sample size and so you're dividing by essentially the sample size so as the sample size gets larger the variation that you expect in this distribution gets smaller and that and that's like sounds complicated but that's the whole like i think that's the maybe the most beautiful thing about all of this is that you end up with these really complicated formulas a lot of times in statistics and people like tell you give you the formulas and ask you to ask you to apply them but i feel like if you see it in this sense it just makes sense like it you don't have to think about it as dividing by the square root of the sample size to get the width of that distribution you can think about it as well like of course the more you the more you do this the more certain you're going to be that about like the proportion of heads so if you in in this case is the sample size 100 because we're flipping 100 coins each exactly if we go and change that to flipping 10 uh in theory we'll see the variance be a lot larger right so it's going to be yeah we're not going to see it so easily in this plot because um because we can't change like we'd have to set the range for this we're going gonna adjust the range of the histogram automatically but yes cool cool can uh can we can can i just see can can you change it i just want to see it and then we can try to remember the ring so you're saying like the range right now is going from like 35 to 65. yeah if you don't think it's gonna be instructed then i think it's it's it's gonna be a little weird because as you get closer to zero it gets like it gets squashed okay um but but we did show it we did demonstrate this yesterday or not yesterday last week yeah um so watch the other live stream and you can see this in action cool um all right so let's come back to our question and let's or let's come back to our data set and let's like basically do the exact same thing we just did but let's do this in the context of our research question so before we sorry we calculated that there were 41 purchases out of a sample of 500 and we want to know if that's really different from 10 percent so what we're gonna do is we're gonna kind of we're gonna simulate some more samples that are just like this so here we're going to say like simulated visitors and our simulated visitors can either make a purchase yes or not make a purchase no our sample had 500 visitors so we're going to keep that the same and now for this we're going to set our x we're going to use our expectation so we're going to say like we expected the purchase rate to be um 0.1 or 10 right so we expected the probability of a purchase to be 0.1 and the probability of not a purchase to be um and then we'll call this purchases and that's going to be the sum of when simulated visitors equals yes and we want to print that out and then oops and then let me just do this once so what i did here was i said okay let's imagine for a second that the real probability of a purchase is 10 how much of a range am i going to see in the number of purchases that occur among 500 visitors if the true probability is 10 is equal to my expectation basically so i'm going to do this a few times remember that with a 10 purchase rate we expected that among 500 visitors we would have 50 purchases so we're seeing numbers kind of around 50. we had 59 then we got 45. yeah because in our in our real data set we had 41. in our real data set we had 41. um so we're definitely starting to see that we can get numbers that are kind of far away from 50 by random chance and now we're gonna basically do the same thing we did with our coin flips we're gonna say okay if i do this a bunch of times and then i plot the histogram of these numbers and each of those numbers being the number of purchases in a simulated sample of 500 people where the purchase probability was equal to 10 very complicated sentence then what kind of distribution am i gonna see in terms of the range in the number of purchases that people are are making in this simulated so i'm going to copy this over except i'm going to replace all of so i'm going to call this now i'm gonna call this null purchases i'll explain why i'm calling it that in a second um and we're gonna append purchases to that and then i'm going to grab this code and drop that in there okay alex do you want to like walk through as i run this what this code is doing again right so this is this is similar to the flipping a coin a hundred times getting the how many heads you we had and then uh and then doing that process ten thousand times and trying a histogram based on based on those results this is the exact same thing except for now rather than flipping a coin 100 times we're looking at 500 visitors and rather than the coin being 50 50 we're saying um we only expect 10 of those visitors to purchase the thing exactly okay so when we do that we did it 10 000 times simulated 10 000 samples of 500 in each of our samples of 500 each person had a 10 probability of making a purchase um and we collected for each sample of 500 how many purchases were made in our simulation and then we plotted this histogram and we see that the number of purchases is ranging from like 30 to 70. again the most our expectation which was 50 is the most common result um but we see we see this kind of range and now we can figure out where our observed value was so our observation and we can do we can l t dot a x v line um and then put that line at 41 let me get red um right so here's our observed number of purchases and we can see that it's not not like uh it's not quite as likely as 50 it's not out of the question it's within the certainly within the realm of possibility it's not like way out here at 29 or 20 or something crazy or a way above like 80 right it's it was within the realm of possibility but it's less likely than this kind of like middle range of numbers yeah two things that i i want to say here one is that i think this graph is potentially a little bit misleading if you're just looking at the graph without labels because it kind of looks like it's a percentage right it ranges like between 30 and 70 with 50 right in the middle so remember you know it was possible to have um 500 people buy uh buy the product and it would be possible for us to have like you know a bar over at 500. it just so happens that um because uh the expectation is that 10 of the people buy the buy the thing and we're sampling 500 people the middle happens to be 50 but this in like no way is connected to percentages which i think um could be confusing because we're in this example we like a lot of things that are like percentages coin flips 50 50 right and so it's a little bit uh uh unlucky or you know it's a little bit um confusing that the middle here happens to be right around 50. that is very helpful point thank you um the other thing that i wanted to ask i think when we bring this into the real world like it makes sense with a coin of flipping 50 50. but if we when we bring this into the real world of like yes my person can either buy buy something or no they can't a natural question that i had at least and i think i know the answer this based on the fact that this is called a binomial um test uh can you run this with three values right so is it did they buy like the uh using codecademy as an example did they buy the monthlong version or the yearlong version or do they pass altogether right there's three options there what do you do in that situation is this applicable at all would you combine the the two forms of yeses into one form of yes like what do you do if you have not a binary thing yeah so if you have not a binary thing then it depends on what your question is but if your question is basically is there an association between what somebody saw and what and their probability of making a purchase if that's what you want to answer then you need like basically if you if you don't care if if you care if there's any difference between those three or more groups then you're gonna need a chisquare test so um that's a different kind of hypothesis test we'll cover that in another week when we start talking about hypothesis tests for an association um but yes there's there is a hypothesis test for that the other thing that you could do is you could try to binarize it if you really only care let's say you have like three conditions and two of two of them are um i don't know like fallen like you can kind of combine two of them and you really only care about the one that's that you expect to be different then you can always turn it into a binary variable and use something like this yeah i'm struggling to come up with an example we have a we have a good question in the chat from andrew um i guess could you talk a little bit about the yaxis here and what that's showing where these values are a little bit hard to read of oh i just changed it because i saw that so yeah i just changed this to density equals true um as you were talking so density equals true changes this to a um basically to a probability um it's a little bit tricky because like in the case of the normal distribution it will it will make it such that like the area of each basically it will make it such that the area of each of these bars is a probability of observing the number in that area like or the number in that range so for example um this bar looks like it's about at 0.4 and it runs from like 41 to 47 maybe or 46 and so if you so it's maybe like a bar that's 5 units wide times 0.04 high so if you do 5 times 0.04 it'll give you the area of this bar and then that will tell you the probability of observing between 41 and 46 or whatever this is purchases by random chance if the true purchase rate was 10 yeah and that and that's compared to our owned yaxis if we didn't put in um density equals true then that's just like the counts of how many times we we saw these occur right we ran this experiment 10 000 times and 2 000 times they fell between 40 and 47. exactly that is a very good question um okay so now i'll come back to this so the reason i called this null purchases is that in the context of hypothesis tests we usually frame a hypothesis test as starting with a null hypothesis and an alternative hypothesis so the null hypothesis of this binomial test that we're running is that the probability of a purchase is 10 percent um i should write that down so null hypothesis is that the probability of a purchase is 10 and this is kind of in keeping basically the null hypothesis says that our expectation is true we expected there to be a 10 purchase rate and there is a 10 purchase rate the alternative hypothesis we have a little bit more um leeway to decide what we want this alternative hypothesis to be but we could for example say that the alternative hypothesis is that there's fewer that the purchase rate is below 10 we could also make the alternative hypothesis that the purchase rate is a above 10 or we could just say the purchase rate is not 10 it could be less than or greater than um we have some choice but let's say let's say for right now like we only really cared to measure whether the probability of a purchase was below 10 because let's say what happened was that like a bug got thrown to a randomly selected group of 500 visitors base they all saw a bug and we wanted to know if that bug influenced our um our purchases and so we expect the purchase rate to go down um and so maybe our alternative hypothesis is that the probability of a purchase is less than 10 and this is this is very like pedantic but like can you do that can you say like of course a bug is not gonna result in more purchases or like we expect you know we expect it to go down and so we're only going to check to see if it actually went down um yeah i mean so basically like you should decide if you're running an experiment you should decide what your hype alternative hypothesis is ahead of time and there's some complic sort of complicated math associated with this but the idea is that if let's say i think it's easier to think about like if you are trying to um if you're trying to create a new feature that's going to increase the purchase rate um and look and let's say then like you only really care if your new feature improves your um your your purchase rate because if it doesn't improve the purchase rate then you're definitely not if it like if it decreases the purchase rate you're definitely not going to implement that feature if it if it doesn't change the purchase rate you're not going to then we don't care then you don't care so in that case you might want to run a onesided a onesided test which is to say you might want to make your alternative hypothesis be that the purchase rate is greater than 10 because you don't care about the other situation um as you'll see in a second i hope we get time to do it like if you run if you use the alternative hypothesis where you're saying like it could be greater or equal or sorry greater or less than you're gonna be less likely i make sure i get my words right um you're gonna get a larger p value so you're gonna be less likely to get significant results um but if you do the wrong onesided test like you say it's greater than when it's really less than then you have no it's called like pow your power to reject the null hypothesis is um like goes down so anyway it i feel like i'm getting into too complicated of a topic at this point but yes the like the bigger picture is you should make this decision beforehand and it depends on your research research question um cool okay so what i wanted to say is that the reason that i'm calling this null purchases is that basically the key to any hypothesis test is knowing what the null distribution is and the null distribution is basically the distribution of a sample statistic that you care about if the null hypothesis is true in this case the sample statistic that we care about is the count of the number of purchases and the null hypothesis is that the purchase rate is 10 and so our null distribution is the distribution of like imagined simulated uh samples of 500 with a purchase rate of 10 the number of purchases among those simulated samples this is our null distribution and what i'll say is when you run a binomial test you're running a binomial test because the null distribution is something called a binomial distribution and if you run a t test the reason is because the null distribution is in the shape of a student t distribution which is essentially the same as a normal distribution um if you run a chisquare test it's because the null distribution is chisquare distributed so basically like the magic of statistics is knowing what this distribution is going to look like without actually having to do what we did which is simulate all of those samples right so this is an example where inherently the pro the problem that we're or the question that we have is going to end up with a simula with a distribution like this and like the shortcut that you can take once you learn this stuff is knowing that okay this problem is going to result in a binomial distribution and there therefore i'll run the binomial test exactly so okay so let's um let's actually calculate a pvalue for our for our test so essentially what a pvalue is is and i'm gonna like speak slowly so i get this correct is it's the probability of observing a range of statistics which is or a range of values which is defined by the alternative hypothesis so in this case if our alternative hypothesis is that the purchase rate is less than 10 our pvalue is going to be the probability of observing the number of purchases we got or fewer given that the null hypothesis is true so in this picture what this equates to is like the area if we do density equals true again basically is proportional to the area of this distribution that's to the left or smaller than this red line that we drew and that qualifier of like if the null hypothesis is true that's that means that's the experiment that we ran right that's the experiment of 10 of um you know we have 500 people and 10 of them are going to um buy the thing and so it's going to end up with a distribution like this yeah so basically we take the null distribution and then based on the alternative hypothesis we say what proportion uh or what's the probability of observing the alternative hypothesis or basically what's the probability of observing what we observed or something more extreme in the direction of the alternative hypothesis so in this case less than um more extreme is even less than 41. what's the probability of observing that if the null hypothesis is true and that's basically going to be an area of the null distribution where you draw the null distribution you draw a line at your sample statistic observed sample statistic and then you take the area outside of that um so if i were to eyeball this i would say that that's like maybe 10 that area is like maybe 10ish percent of the uh of the total area under the curve i don't know if it's going to be exactly that but that's kind of what we're looking at right the area to the left of that red line is a small area compared to the rest of the area i think your your estimation cells skills are very good um okay so the p pvalue is going to be the sum of null outcomes or what did i call it null purchases i'm copying this over from codecademy uh so this is right the top of this fraction is the number of values in num purchases the number of simulated purchases that are less than or equal to 41 divided by the total number of simulations which in this case um is 10 000. and so if we print that whoop let's see oh i think you have to array okay yep so this is roughly and i'll do times tens well no i won't do that so basically this is like 10 which is exactly what um alex estimated and so basically we're saying that if we randomly simulated i'm just going to say it again because i feel like it helps to just keep saying it if we randomly simulate 500 visitors and we repeat that process 10 000 times each time recording the number of purchases that were made and in each of those simulations the purchase rate really is 10 where the probability that we observe 41 or fewer purchases given that the purchase rate was 10 is about 10 um all these numbers lining up is probably not super helpful um the twosided pvalue is essentially going to be two times that because theoretically we would just draw another red line on the other side of this that's equidistant so this one's at 41 so we draw another line at like 59 so nine units away from 50 on the other side and add in this area over here um the reason i'm saying that so when i say twosided pvalue i mean if we said that the alternative hypothesis is that the probability of a purchase is not 10 then we would want to calculate this like both the area of both sides both extremes um the probability that the number of purchases was nine fewer or nine more than 50. um and the reason that's important is that right now because we're getting close to the end of time i want to just demonstrate that you can also do this with the builtin function and i want to demonstrate that you get the same thing but the builtin uh python function that at least all the ones i found calculate a twosided pvalue by default so um let's just uh i'm just grabbing the solution code i think it's a scipi function that i've been using but i just want to grab it um okay so let's do let's do this so let's import from scipy dot stats import binome test so now we're going to do this in a single line of code we're going to say give me the so the p value from this this function is equal to binome test the parameters here are the observed value which in this case is 41 the sample size which is 500 and then the null probability of a purchase which is 0.1 if we do this and we print out this value it's going to be about two times as big as that but yeah so it's about 20 or 0.2 but if we do alternative equals less to tell it that we want this alternative hypothesis to be that the probability of a purchase is less than this 10 then we should get about exactly 0.1 about exactly 0.1 so we've seen that basically you can use these builtin functions and they'll give you the same roughly the same value as this simulation that we just did to calculate the pvalue the reason it's not exact is that we used a simulation um if we did more iterations like more than 10 000 the more we did the closer it would get but it's pretty cool that now we can now we can basically write this binome test function if we wanted and actually in the lesson um if you're taking it on codecademy it'll walk you through that whole process of writing your own binome test function which i wish we had time to do right now but yeah so so i had a question about that that idea of like it's off by a little bit the actual binome test function um is using like a formula right we're basically what we just did is we like approximated this whole process by doing a simulation of running this thing 10 000 times and like you said if we crank up that number to a million it's going to take our computer a longer time to to run all those simulations but we're probably going to get closer to that number the real the like scipy version of this function is just plugging it into a formula right that's like derived from this process or similar to this process yeah so basically the real scifi function is instead of using this like simulated null distribution that we made it's using a binomial distribution with parameters n equal 500 and p equals 0.1 and that's gonna give you like it it basically just describes exactly what this shape should look like if you did it a bajillion times infinity times right so really the issue is that our simulation didn't actually capture the binomial distribution perfectly because we didn't do it a billion times um but there's some definition out there of this is what a binomial function looks like with these parameters and we can just plug it into that and get the like the permanent pvalue for this yep cool exactly cool so i think that's about it for today but hopefully you feel like you learned something and you understand the binomial tests a little bit better um here i'll stop sharing my screen is there anything you want to add about a second or two to ask any lastminute questions um sophie do you want to preview what we're doing next week sure i should i actually like need to look up what we're doing next week also everyone tell some people because sophie's you know sophie's running these every single week and prepping for them so good job sophie it's very impressive thank you i love doing this i i wish that more people would find them and come and ask us questions because it is very very fun um and i hope that it's helpful for people as well and hopefully people will find them if they're working through the master statistics path uh massachusetts statistics with python path because i i do think it would be a really good compliment to kind of see someone working on the stuff in action i know i feel like we're all in our homes right now i'm kind of isolated still the isolation fatigue and zuma fatigue is real and so it's nice sometimes i think to like just see some other people talking about stuff um yeah uh what is next week let me see i can look it up if you don't have it i've got a significance threshold and multiple hypothesis tests oh oh yeah so next equal we'll kind of talk about the um the issues that can arise if you run a bunch of hypothesis tests and has to do with the fact that probability like all of these tests are regarding probabilities and the more time you do something the more likely it is that even rare events will happen um so it's kind of it's super important because it has to do with the whole like reproducibility crisis which is the idea that like a lot of published papers are not reproducible with new research um and it's really important for anybody who's looking to go into data science and looking to uh or statistics and like looking to run these kinds of tests in their own research because if you're not aware of the issue then um then you contribute to it so yeah awesome well i i'm glad that people find it helpful finding this helpful um i don't see any other questions so with that i think we'll sign off but this was fun cool glad to have you back alex yeah thank you i'll be back next week too so see you