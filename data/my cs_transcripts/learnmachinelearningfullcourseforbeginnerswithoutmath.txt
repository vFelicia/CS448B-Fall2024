00:00 - hello and welcome to this free course in
00:03 - data science and machine learning for
00:05 - beginners made my AI Sciences Academy
00:07 - this easy-to-understand course is
00:10 - dedicated for beginners who need to
00:11 - learn the A to Z fundamentals of data
00:14 - science and machine learning through a
00:15 - gradual and segmented approach ok here
00:20 - are the four main parts we will cover
00:21 - during this course
00:24 - we will start with an introduction part
00:26 - two data science and machine learning in
00:29 - this part we will answer some questions
00:30 - that you may ask like what is data
00:33 - science and machine learning why data
00:35 - science now and when I can apply data
00:38 - science and machine learning techniques
00:41 - in learning part 1 we will learn the
00:44 - preliminary to understand data science
00:46 - and machine learning in this part we
00:48 - will learn some vital concepts in data
00:50 - science and machine learning
00:53 - a learning bar - we will start the
00:56 - machine learning part where I will
00:58 - explain to you how the machine learning
01:00 - models work we'll also discuss about
01:02 - regression classification and clustering
01:04 - in learning part three we will talk
01:08 - about how to evaluate the performance of
01:11 - the model and choose the best one based
01:13 - on some indicators in learning part four
01:16 - of this course we will discuss about
01:18 - some best practices in data science and
01:21 - machine learning at the end of this
01:23 - course you will receive a gift so please
01:26 - follow this course until the end this
01:28 - gift will help you on your learning
01:30 - journey so ready let's start
01:40 - you
01:42 - data science is not a straightforward
01:44 - easy to define field like most
01:46 - traditional fields it's rather a
01:49 - multidisciplinary field which means that
01:51 - it combines different areas such as
01:53 - computer science mathematics and
01:55 - statistics because data science can be
01:58 - applied and used in various applications
02:00 - and fields it requires domain expertise
02:03 - in each particular area ok for example
02:07 - if we use data science to develop a
02:09 - medical analysis application then we
02:11 - will need an expert in medicine to help
02:14 - define the system and interpret the
02:16 - results
02:18 - data scientists explore the data
02:21 - visualize it and calculate important
02:24 - statistics from it then depending on
02:26 - these steps and the nature of the
02:28 - problem itself they develop a machine
02:30 - learning model to identify the patterns
02:32 - so machine learning and deep learning
02:35 - are the subfields of data science
02:39 - so you might ask what is the difference
02:41 - between data science data analytics and
02:44 - big data well big data means huge
02:48 - volumes of various types of data we
02:50 - differentiate big data by its four V's
02:54 - which are the characteristics that are
02:56 - distinct from ordinary data they are
02:59 - volume velocity variety and veracity
03:06 - the sheer volume is the main
03:09 - characteristic that makes data big
03:11 - velocity is the frequency of incoming
03:14 - data that needs to be processed variety
03:17 - means different forms of data veracity
03:21 - refers to the trustworthiness of the
03:23 - data on the other hand data analytics is
03:26 - more about extracting information from
03:29 - the data by calculating statistical
03:31 - measures and visualizing the
03:33 - relationship between the different
03:34 - variables and how they are used to solve
03:37 - the problem it's like descriptive
03:39 - statistics
03:41 - why data science now that's an
03:44 - interesting question
03:45 - first there is currently plenty of data
03:48 - more than at any time before in history
03:51 - and it just keeps growing exponentially
03:53 - second now we have much better computers
03:55 - and computational power than ever before
03:58 - a task that can be finished in a few
04:00 - seconds nowadays would have required
04:02 - days with the computers that existed
04:04 - just a few years ago and finally we have
04:07 - more advanced algorithms for pattern
04:09 - recognition and machine learning than we
04:11 - did just a few years ago so in one
04:14 - sentence if you want to know why data
04:15 - science has become our focus right now
04:17 - is because we have a lot more data
04:19 - better algorithms and better hardware
04:23 - let us see where data science and
04:26 - machine learning is applied
04:28 - I want to say that data science is
04:30 - everywhere the number of data science
04:33 - applications is countless it's because
04:35 - we have data everywhere and there are
04:38 - dozens of algorithms developed each year
04:40 - to solve these tasks however we will
04:43 - talk about a few famous use cases of
04:45 - machine learning in data science in our
04:47 - daily lives
04:49 - as you can see machine learning and data
04:51 - science are currently being used in many
04:53 - fields such as healthcare finance
04:55 - transport social media ecommerce and
04:58 - virtual assistant apps among others in
05:01 - healthcare machine learning is currently
05:04 - used in disease diagnosis it provides
05:07 - higher accuracy compared to professional
05:09 - physicians it is also undergoing
05:11 - extensive research and drug discovery
05:14 - another application is robotic surgery
05:17 - where we have an AI robot helping and
05:20 - performing the surgery with a precision
05:22 - that is actually higher than that of the
05:25 - best surgeons
05:27 - in transport Tesla used machine learning
05:30 - algorithms in its self-driving cars
05:32 - machine learning is also used for air
05:35 - traffic control
05:37 - in finance I worked for two years as the
05:40 - KPMG consultant in one of the French
05:42 - banks to develop different machine
05:44 - learning algorithms predicting customer
05:46 - defaults and bank capital requirements
05:49 - many banks are currently using machine
05:51 - learning powered software for fraud
05:53 - detection also banks are using machine
05:56 - learning for algorithmic trading in
05:59 - social media I think all social media
06:01 - platforms today use machine learning for
06:03 - both spam filtering and sentiment
06:05 - analysis Facebook also uses machine
06:08 - learning image recognition
06:11 - in e-commerce many online shopping
06:14 - websites such as Amazon eBay and udemy
06:17 - use machine learning for customer
06:19 - support targeted advertising and product
06:21 - recommendation machine learning is also
06:24 - used in virtual assistant apps many
06:26 - startups are founded based on the idea
06:28 - of developing a machine learning powered
06:30 - assistant in one particular field this
06:33 - assistant can be a chatbot for example
06:35 - which can intelligently reply and answer
06:37 - nearly any inquiries in that field
06:41 - these are just a few broad in general
06:43 - applications of data science and machine
06:44 - learning you can develop your own
06:46 - application in any field that you find
06:48 - interesting and have some experiences
06:50 - and by the end of this course you will
06:52 - be equipped with the knowledge necessary
06:54 - to create an app in the area of your
06:56 - choice
06:58 - let's talk just a little bit about the
07:00 - history of data science the term data
07:02 - science has been appearing in various
07:04 - contexts over the past thirty years but
07:07 - did not become an established term until
07:08 - relatively recently in its early usage
07:12 - it was used as a substitute for computer
07:14 - science since 1960 when Peter now are
07:17 - first mentioned the term
07:19 - how about the future of data science and
07:21 - data scientists we can clearly see that
07:24 - the future of data science is very
07:26 - bright another evidence for that is the
07:28 - cloud services that have appeared in the
07:30 - last two or three years being extremely
07:32 - cheap and fast they can help develop
07:34 - more advanced machine learning
07:36 - applications in all fields so it will
07:39 - now be surprising to see many tasks that
07:41 - we considered science fiction such as
07:43 - assistant robots and self-driving cars
07:45 - already in use in our daily lives
07:49 - how about the future of data scientists
07:51 - let me share some statistics with you as
07:53 - you can see in the figure on your screen
07:55 - at the end of 2016 the percentage of
07:58 - jobs for data scientists was 474 percent
08:02 - larger than those for statisticians
08:04 - today in the USA the work of a data
08:08 - scientist is one of the high Penguin's
08:10 - learning data science maybe the best
08:13 - decision you've ever made in our
08:14 - learning company ai sciences is here to
08:17 - help you achieve this goal now how can
08:20 - you get the ultimate benefit from this
08:22 - course
08:24 - I highly recommend you take this course
08:26 - very seriously
08:28 - and follow it step by step we highly
08:31 - suggest that you go through the study
08:32 - materials that you used in your high
08:34 - school undergraduate and graduate
08:36 - programs and revised topics such as
08:38 - linear algebra calculus and statistics
08:41 - we will not go too deep into the math
08:43 - behind algorithms in this course but we
08:46 - will cover basic ideas logic and in some
08:48 - cases formulas to understand our main
08:50 - topic of interest better also try to
08:54 - finish every single project provided in
08:57 - the course on your own and then check
08:59 - the solution
09:01 - finally we encourage you to go through
09:03 - any further reading material that you
09:04 - will find it will give you an overview
09:06 - of what you can learn next after
09:08 - finishing the course okay in this lesson
09:11 - we will explore in detail some important
09:13 - terms in data science you ready let's go
09:17 - in the following lesson we will talk
09:19 - more about data and variables so what is
09:22 - the data data are basically collections
09:26 - of facts measurements observations
09:28 - numbers words etc that have been
09:30 - transformed into a form that can be
09:32 - processed by computers data are stored
09:34 - in columns and rows the convention is
09:37 - that each row represents one observation
09:39 - case or example and each column
09:42 - represents one feature or variable got
09:45 - it we also have two types of variables
09:48 - based on their value numerical and
09:51 - categorical variable if the value is a
09:53 - number and we can compute the mean for
09:56 - this variable we call it a numerical
09:58 - variable but if the but if the value is
10:02 - a factor or label and we can't compute
10:05 - its mean we have a categorical variable
10:09 - so we can also talk about dependent and
10:12 - independent variables a dependent
10:14 - variable is one that we need to predict
10:16 - and the independent variables or X
10:19 - variables will help us to predict a y
10:22 - variable it is essential to know that X
10:25 - variables need to be independent of each
10:27 - other and that is why we call them
10:29 - independent variables or predictors
10:33 - couple other important terms in data
10:35 - science are population and sample and
10:37 - data science the whole population is our
10:40 - target but due to a lack of resources a
10:43 - data scientist can't work in the entire
10:46 - population because of that we have to
10:48 - choose a representative sample of the
10:51 - data from the population the goal of
10:53 - machine learning algorithms is to find
10:55 - parameters that can do the mapping on
10:58 - the whole population
10:59 - based on the given sample as you can see
11:02 - we have our population and we will
11:04 - choose a sample by using the sampling
11:07 - technique
11:08 - now let's talk about outlier and missing
11:12 - data in data science an outlier is a
11:15 - data point that differs from other
11:16 - observations an outlier may occur due to
11:19 - variability in the measurement or it may
11:21 - indicate an experimental error outliers
11:25 - can alter the performance of many
11:26 - machine learning algorithms as we can
11:30 - see there in the image we can detect
11:31 - outliers by visualizing the data in this
11:35 - example employee number two and nineteen
11:37 - are outliers based on their profile
11:42 - how to deal with outliers we can drop
11:45 - them altogether cap them with the
11:47 - threshold assign new values based on the
11:50 - mean of the data set for example or
11:52 - apply a transformation on the data set
11:55 - itself
11:58 - okay one way to handle missing data is
12:00 - by dropping the observations another way
12:03 - to handle them is to use data imputation
12:06 - techniques so if we have a numeric
12:09 - feature we can replace the missing value
12:12 - with a mean median or mode or we can
12:16 - select random observations from the data
12:18 - set and replace its feature value in the
12:21 - observation that has missing values the
12:24 - last imputation technique can be
12:26 - performed by regressing the missing
12:28 - feature on the other features and making
12:30 - a prediction of the missing value
12:34 - if we have a categorical feature we can
12:36 - use the mode replacing or also we can
12:39 - use the KNN model to predict the feature
12:42 - that has a missing value
12:45 - if you still don't know much about the
12:47 - regression and KNN model that worries
12:49 - David will present them to you in the
12:51 - machine learning section
12:53 - that's it for this lesson in this class
12:56 - we will learn about the link between AI
12:58 - machine learning and deep learning dl
13:02 - you'll find out how a machine learns and
13:04 - the types of learning there are ready
13:06 - let's go
13:08 - first here's the link between artificial
13:11 - intelligence machine learning and deep
13:13 - learning let's clear the confusion
13:15 - between these three essential terms so
13:17 - what are they I ml and DL
13:22 - by looking at the image it is clear that
13:24 - ml is a subfield of AI and DL is a
13:28 - subfield of ML
13:32 - machine learning algorithms were
13:33 - developed with the goal of finding a
13:35 - useful prediction function among machine
13:38 - learning algorithms let's mention the
13:40 - artificial neural network
13:42 - an artificial neural network consists of
13:45 - a collection of neurons connected to
13:47 - each other in a specific way however the
13:49 - use of neural network was limited
13:51 - because of the lack of computational
13:53 - power and the lack of proper
13:55 - optimization algorithms for neural
13:57 - networks this is where deep learning
13:59 - came in it is a simpler algorithm that
14:03 - uses more neurons and layers to perform
14:05 - a lot of learning tasks like image
14:08 - recognition and natural language
14:10 - processing today deep learning is used
14:13 - in a lot of areas such as the high-tech
14:15 - industry Tesla self-driving cars and the
14:18 - Seattle Amazon store are constructed
14:20 - based on the deep learning algorithms
14:22 - combined with computer vision
14:25 - now let's see how a machine learning
14:28 - algorithm learns
14:31 - okay Before we jump into how a machine
14:33 - learns let us first try to understand
14:35 - how a human baby learns think for
14:38 - example of a one-year-old human baby a
14:42 - baby does not know the difference
14:43 - between an apple and an orange for him
14:46 - all fruit is the same orange Apple
14:48 - bananas cucumbers in his first phase of
14:52 - learning called phase one of learning in
14:54 - the figure he builds an intuition that
14:56 - oranges and apples are of one shape and
14:59 - bananas and cucumbers are of another
15:01 - shape once a baby is comfortable with
15:04 - the shapes of fruit he goes into
15:06 - learning phase two phase two of learning
15:08 - in the figure
15:11 - by introducing another property like
15:13 - color now he knows that a fruit that is
15:17 - round in shape and red in color means
15:20 - that it's an apple in a round shape and
15:23 - an orange color means it's an orange in
15:26 - the face 3 the baby will gather a lot of
15:30 - data as shown in the table with these
15:32 - two properties and based on these data
15:34 - in the future he will know the
15:36 - difference between fruits
15:38 - machine learning models learn the same
15:40 - way in machine learning the properties
15:43 - of the fruit such as shape and color are
15:45 - called
15:46 - the features the fruit type is called
15:48 - the label each instant of an
15:51 - input/output pair is called an
15:53 - observation
15:55 - so depending on the features and labels
15:57 - enter to a machine learning algorithm
15:59 - learning is classified into three main
16:03 - categories supervised learning
16:05 - unsupervised learning and reinforcement
16:08 - learning we also have semi-supervised
16:11 - and instant based learning but in this
16:13 - lesson we're just going to focus on the
16:15 - main three learning types in supervised
16:18 - learning we train with labeled
16:20 - observations that means that for each
16:22 - observation of training data the input
16:25 - and output are known as you can see in
16:27 - the image we try to predict the output
16:30 - from the input by training our machine
16:32 - learning model
16:34 - classification is one example of
16:37 - supervised learning where the goal is to
16:38 - classify objects regression is another
16:42 - example where we try to understand the
16:44 - relationships among variables
16:47 - in a glance in supervised learning we
16:50 - have the Y and the X variables and we
16:53 - want to make the prediction of why
16:56 - okay now in unsupervised learning the
16:59 - trainer does not provide a labeled
17:01 - output in the learning data set the
17:04 - machine learning algorithm learns from
17:06 - unlabeled data and gathers information
17:08 - from it as you can see
17:11 - it's short in unsupervised learning we
17:13 - only have X variables and we need to
17:15 - gather the observations and groups based
17:18 - on the information given by X
17:21 - unsupervised learning is used mainly for
17:23 - clustering tasks where we organize the
17:26 - observation into clusters
17:29 - in reinforcement learning an agent
17:32 - learns by interacting with the
17:33 - environment so in this type of learning
17:36 - the agent performs an action in the
17:38 - environment this action takes the
17:40 - environment to a new state and gives a
17:42 - reward to the agent the reward can be
17:44 - negative or positive for multiple
17:47 - iterations and the rewards the agent
17:49 - learns based on his past experiences
17:53 - reinforcement learning is mainly used in
17:55 - skill acquisition tasks such as robot
17:58 - navigation or games
18:02 - okay to sum up we have three main types
18:05 - of learning supervised when we have X
18:07 - and y variables unsupervised when we
18:10 - only have X variables and reinforcement
18:14 - learning in this case the algorithm
18:16 - learns by rewards that it gets as a
18:18 - result of its action in an evolving
18:21 - environment
18:22 - now you know how a machine learns and
18:24 - what are the different types of learning
18:26 - I guess now you know a lot more about
18:28 - basic data science and machine learning
18:29 - terms in this lesson let me explain some
18:32 - important modeling terms to you alright
18:35 - you ready let's start
18:37 - in machine learning we always split our
18:40 - data into two so we have a training data
18:43 - set and a test data set the training
18:47 - data will help us to train our model and
18:49 - the test data set will help us evaluate
18:51 - our model performance and accuracy a
18:54 - typical machine learning model learns
18:56 - from the training data set and applies
18:58 - the learning to the test data set so if
19:01 - the model is able to make correct
19:03 - predictions on the test data set then
19:06 - the model is able to generalize the
19:08 - learning to any new data and then we can
19:10 - say we have a good model now what is an
19:14 - over fitted model when we obtain a model
19:18 - that works very well on the training
19:19 - data set but is not able to generalize
19:22 - to the test data sets we call such a
19:25 - model an over fitted model in this case
19:27 - the testing error is large because the
19:30 - model is very complex the model is under
19:33 - fitted when the training error is large
19:35 - because the model is too simple and just
19:37 - can't capture the true complexity of the
19:39 - data so you may wonder can we control
19:43 - these issues the answer is yes the
19:46 - solution for under fitting is either to
19:48 - increase the size of the data set or to
19:51 - increase the complexity of the model but
19:54 - the solution for overfitting is a bit
19:56 - trickier the first solution is to gather
19:58 - more data this solution is not always
20:01 - feasible the second solution is to use
20:04 - penalty terms in the model and that is
20:07 - called the regularization technique
20:10 - and the last one is to make
20:12 - cross-validation okay in short you need
20:15 - to understand that our goal as data
20:17 - scientists is to come up with a model
20:19 - that is not too generalized and not too
20:22 - focused on training data this model is
20:25 - called the best fit model this is
20:27 - basically a trade-off between an under
20:29 - fit and an over fit model like the
20:32 - middle image
20:34 - as you're already familiar with
20:36 - overfitting and underfitting the concept
20:38 - of bias and variance will be very easy
20:40 - to digest but before we start talking
20:43 - about bias and variance let us classify
20:45 - the type of models errors first we have
20:50 - the irreducible error which comes from
20:52 - the nature of the data itself for
20:54 - example the noise when you talk through
20:56 - your mobile phone the second kind of
20:59 - error is the reducible error we have to
21:02 - reducible errors the bias error and the
21:04 - variance error
21:06 - okay the bias error is the difference
21:09 - between the average prediction of our
21:11 - model and the correct value which we are
21:13 - trying to predict the bias error is high
21:16 - if the model is oversimplified the
21:19 - variance error is the variability of
21:22 - model predictions for the given data the
21:25 - variance error is high if the model is
21:27 - not generalizing well on new data okay
21:31 - look at the figure at the right the blue
21:33 - points represent how far we are from the
21:35 - minimum error which is represented by
21:38 - the small red circle in the case of low
21:40 - bias the blue points are not very far
21:43 - from the minimum error in the case of
21:46 - low variance the blue points are near
21:48 - each other of course we want our model
21:51 - outputs to be as close as possible to
21:53 - the minimum error which means low bias
21:56 - and low variance but it is impossible to
21:59 - have both low bias and low variance
22:01 - there is a trade-off between bias and
22:04 - variance because as we decrease the
22:06 - model bias we make it more complex and
22:09 - we increase its variance and when we
22:12 - decrease the variance we increase the
22:15 - bias looking back to overfitting and
22:18 - underfitting we can say that when the
22:20 - model is under fitted it has low
22:23 - variance and high bias when the model is
22:25 - over fitted then it has high variance
22:28 - and low bias
22:31 - as you can see in machine learning it is
22:33 - important to make a bias-variance
22:35 - tradeoff and to choose a middle model
22:39 - in data science we have different types
22:41 - and qualities of data and we need to
22:43 - make them usable in our model number one
22:46 - feature extraction and feature
22:48 - engineering will help us transform raw
22:51 - data into features suitable for modeling
22:53 - and to feature selection will help us
22:57 - remove unnecessary features during the
22:59 - data processing step
23:01 - in this part of the course we will learn
23:03 - various machine learning models which
23:05 - are commonly used for prediction
23:07 - classification clustering etc as I
23:11 - already mentioned machine learning is
23:13 - broadly classified into supervised and
23:16 - unsupervised learning supervised
23:19 - learning means that the algorithms are
23:21 - supervised during the training phase so
23:23 - in other words in order to train these
23:26 - algorithms we need data that have
23:28 - labelled targets for example if a model
23:31 - is being created for predicting house
23:33 - prices then the historical data that is
23:35 - used to train the model should have a
23:37 - target column stating the price of a
23:39 - house
23:41 - unsupervised learning is when we don't
23:44 - have a target labeled data in this case
23:47 - the algorithm classifies objects based
23:50 - on some existing features supervised
23:53 - machine learning problems have two
23:54 - categories of learning regression and
23:56 - classification
24:00 - regression algorithms are used to
24:03 - identify a relationship between a
24:05 - dependent variable target and
24:08 - independent variables predictor /
24:10 - features so in regression the target is
24:14 - always a continuous variable and the
24:17 - predictors can be continuous or discrete
24:19 - in nature regression is best used for
24:23 - finding causal effect relationships
24:25 - between the variables forecasting time
24:28 - series modeling etc in regression
24:31 - analysis the model tries to fit a curve
24:35 - to the data points in such a manner that
24:36 - the difference between the data point
24:38 - and the curve is at a minimum
24:45 - on the other side we have classification
24:48 - models the classification models are
24:51 - used to predict the target that has
24:53 - discrete values for so for example class
24:57 - of fruits orange pineapple and lime or
24:59 - predicting whether a patient is
25:01 - suffering from cancer or not
25:04 - for this kind of use machine learning
25:06 - models collect insights from the
25:08 - historical labeled data and use these
25:11 - insights to predict the target class
25:14 - okay listen up don't forget this
25:16 - regression and classifications are all
25:18 - supervised learning models because we
25:21 - have a labeled output which we need to
25:23 - predict if this labeled output is
25:25 - continuous we use regression and if it's
25:28 - categorical we use classification
25:34 - now the most used unsupervised learning
25:37 - models are clustering and Association
25:40 - analysis clustering is the most
25:43 - important unsupervised learning model it
25:45 - deals with finding a structure in a
25:47 - collection of unlabeled data so a loose
25:51 - definition of clustering could be the
25:53 - process of organizing objects into
25:56 - groups whose members are similar in some
25:58 - way I will present clustering in more
26:00 - detail later
26:03 - Association analysis models discover
26:06 - relationships in large datasets hidden
26:08 - data relationships will be expressed as
26:11 - a collection of Association rules and
26:14 - frequent itemsets with Association
26:17 - analysis Association analysis isn't
26:20 - frequently used we will not focus on
26:22 - them in this course
26:25 - what are the machine learning models
26:27 - associated with each of these learning
26:29 - types I mean which models are dedicated
26:32 - to regression classification and
26:34 - clustering
26:36 - for the regression purpose we commonly
26:39 - use linear regression decision trees for
26:42 - regression support vector machines for
26:45 - regression SVR or neural networks for
26:48 - the classification purpose we use
26:51 - logistic regression decision trees for
26:54 - classification support vector machines
26:58 - classifier SVC nearest neighbor or
27:01 - neural networks in unsupervised learning
27:05 - we use the k-means clustering
27:09 - so before we discuss these models in
27:12 - detail I need to explain the difference
27:15 - between two terms which can be confusing
27:17 - when you get started in machine learning
27:19 - the terms model parameter and model
27:23 - hyper parameter I'll talk more about
27:25 - these two terms in a few minutes so I
27:28 - think it will be better to explain the
27:30 - difference between these two terms now
27:38 - a model parameter is a configuration
27:40 - variable that is internal to the model
27:43 - and whose value can be estimated from
27:46 - data it's required by the model when
27:48 - making predictions and it is estimated
27:51 - or learned from data model parameters
27:55 - are key to machine learning algorithms
27:57 - they are the part of the model that is
27:59 - learned from historical training data so
28:02 - some examples of model parameters
28:04 - include the weights in an artificial
28:08 - neural network the coefficients in a
28:10 - linear regression or logistic regression
28:17 - on the other side a model hyper
28:19 - parameter is a configuration that is
28:22 - external to the model and whose value
28:24 - can't be estimated from data they are
28:27 - often only help estimate the model
28:29 - parameters and they are often specified
28:31 - by the practitioner
28:35 - [Music]
28:40 - linear regression is one of the most
28:42 - used supervised machine learning
28:43 - algorithms and now I'm gonna explain
28:46 - what it is exactly how it works and show
28:48 - you the logic behind it first of all
28:50 - what is linear regression let's say that
28:54 - two years ago I made $10,000 on my job
28:57 - and that last year I earned 20,000 with
29:00 - that in mind what do you think how much
29:02 - would I make this year
29:04 - if you answered $30,000 you just applied
29:07 - linear regression we make predictions
29:10 - all the time and most of them follow the
29:12 - logic of linear regression linear
29:16 - regression is a machine learning model
29:17 - which was designed to help you to
29:19 - specify a linear relationship to predict
29:22 - the numerical value of a dependent
29:24 - variable we will call it Y in this
29:26 - course for a given value of independent
29:29 - variables we will call them X by using a
29:32 - straight line called the regression line
29:34 - does that sound complicated it's not
29:37 - I'll show you
29:38 - we can say that linear regression will
29:41 - help us make a prediction based on some
29:43 - information prediction equals dependent
29:47 - variable Y some information equals
29:50 - independent variables X we use linear
29:54 - regression to answer the following
29:56 - questions is there a linear relationship
29:58 - between the two variables x and y which
30:02 - X variable contributes the most let's
30:06 - write this model as an equation it goes
30:07 - like this y equals B 0 plus B X plus e
30:15 - b0 is the value of y even if the value
30:19 - of x is 0 it's called the intercept B is
30:23 - a coefficient associated to X it will be
30:26 - a vector of coefficients e for error
30:29 - denotes all remaining information about
30:31 - why that hasn't been explained by the X
30:34 - variables of course the linear model is
30:37 - not perfect and it will not predict all
30:39 - the data accurately
30:42 - okay we have two types of linear
30:44 - regression the simple linear regression
30:46 - and the multiple linear regression in
30:49 - simple linear regression we use a single
30:52 - independent variable to predict the
30:54 - value of a dependent variable the model
30:57 - in this case will be y equals B 0 plus B
31:03 - 1 X 1 plus e a multiple linear
31:09 - regression we use two or more
31:11 - independent variables to predict the
31:13 - value of a dependent variable the
31:15 - difference between the two is the number
31:17 - of independent variables X y equals B 0
31:22 - plus B 1 X 1 plus B 2 X 2 plus dot dot
31:30 - dot plus E
31:34 - to keep this lesson simple and to help
31:36 - you understand the rest of the course
31:37 - right now we will focus on the simple
31:39 - linear regression only the same thing
31:41 - will be replicated in multiple linear
31:43 - regression the only thing that will
31:45 - change is the number of X variables here
31:49 - are some examples of simple linear
31:51 - regression and multiple linear problems
31:54 - the estimation of the average student
31:56 - score based on the number of hours they
31:58 - have spent studying 30 hours please note
32:01 - that all the students who pass the exam
32:03 - will receive at least two points for
32:05 - their presence in this problem the
32:08 - dependent variable Y will be the average
32:10 - student score the independent variable
32:13 - is the hours of study and it equals 30
32:16 - the intercept equals B 0 equals 1 the
32:22 - regression model we want for the score
32:24 - prediction is score equals B 0 plus B 1
32:29 - times our study score equals B 0 plus B
32:35 - 1 times 30 in this model the b0 and b1
32:41 - are coefficients these coefficients are
32:44 - what we need in order to make
32:45 - predictions about our score if we add
32:47 - the number of exercises that the student
32:50 - has completed to the model it will
32:52 - become a multiple linear regression
32:54 - model score equals b0 plus b1 times
32:59 - our study plus b2 times exercises and B
33:04 - plus
33:06 - for the prediction of the score we need
33:09 - to estimate three parameters B 0 B 1 and
33:12 - B 2 B 0 B 1 and B 2 parameters need to
33:17 - be estimated based on our historical
33:19 - data
33:21 - to estimate the linear regression
33:23 - coefficient we need to minimize the
33:25 - least squares or the sum of individual
33:28 - squared errors in other words that's the
33:31 - difference between the actual value and
33:33 - the prediction the error of the
33:36 - individual I is easily calculated as the
33:38 - difference between the real value of y I
33:41 - equals y hat i AI equals y I minus y hat
33:47 - I we square the error for two reasons
33:50 - one the prediction can be either above
33:53 - or below the true value resulting in a
33:55 - negative or positive difference
33:56 - respectively if we did not square the
33:59 - errors the sum of errors could decrease
34:01 - because of negative differences and not
34:03 - because the model is a good fit
34:06 - to squaring the errors penalize as large
34:09 - differences and so the minimizing the
34:11 - squared errors guarantees a better model
34:13 - let's look at a graph to understand it
34:15 - better in the graph the green dots
34:18 - represent the true data and the yellow
34:20 - line is a linear model the dotted red
34:23 - lines illustrate the errors between the
34:25 - predicted and the true values in
34:27 - practice we use the OLS algorithm it's
34:30 - ordinary least squares eater ative Li in
34:34 - each iteration the algorithm calculates
34:36 - the sum of the individual squared errors
34:39 - and in the next iteration the algorithm
34:42 - updates model parameters to shift the
34:44 - line from the previous position to
34:46 - reduce the squared error finally the
34:49 - best OLS estimators of the coefficients
34:52 - are
34:53 - in these equations X bar and y bar
34:56 - represent the mean
34:58 - now how to make a prediction we will use
35:01 - the existing data to estimate the values
35:03 - of B 0 B 1 through B K we can do that in
35:09 - Excel our Python etc after that we can
35:12 - make all the predictions we want
35:14 - please note that linear regression is
35:17 - considered to be one of the most
35:18 - straightforward machine learning
35:20 - techniques and an easy model to
35:21 - interpret but it has its disadvantages
35:24 - it will only work if the relationship
35:27 - between the dependent and independent
35:28 - variables is linear that's it now you
35:31 - know exactly what linear regression is
35:33 - and how it works now present the
35:36 - decision trees model a decision tree is
35:39 - like a series of if-else conditions that
35:42 - lead to a decision it can be used for
35:44 - classification or regression use cases
35:47 - it works for both categorical and
35:49 - continuous input and output variables
35:53 - decision tree breaks down a data set
35:55 - into smaller and smaller subsets while
35:58 - at the same time an Associated decision
36:01 - tree is incrementally developed the
36:04 - final result is a tree with decision
36:07 - nodes so consider a scenario where we
36:11 - want to decide whether a loan should be
36:13 - approved for an applicant or not to
36:16 - decide we will ask the applicant a
36:18 - series of questions we might start off
36:20 - with whether the applicant has any other
36:22 - existing loans if the answer is yes then
36:25 - the next question might be whether he is
36:27 - a defaulter or not with this kind of
36:30 - series of questions we can narrow down
36:32 - the search and make a robust decision we
36:35 - build the model above by hand to make a
36:38 - decision about whether a loan
36:39 - application should be approved or not
36:42 - alternatively a machine learning model
36:44 - could perform supervised learning using
36:47 - the data set to arrive at a decision the
36:49 - decision tree model in machine learning
36:52 - can learn these decisions or conditions
36:54 - from the data set quickly and build the
36:56 - model
36:58 - okay decision trees have three types of
37:01 - nodes
37:03 - a decision node has two or more branches
37:07 - a leaf node represents a classification
37:09 - or decision
37:12 - we also have the topmost decision node
37:15 - in a tree which corresponds to the best
37:17 - predictor called root node
37:22 - how do decision trees work in practice
37:24 - well that's an interesting question in
37:27 - practice there are three important steps
37:29 - in the building of a decision tree
37:32 - the first one is splitting a decision
37:36 - tree can have a sub-branch or a subtree
37:40 - as well and the process of creating a
37:42 - sub branch is called splitting have a
37:45 - look at the figure to visualize these
37:47 - notations
37:51 - so the root node has the full data set
37:54 - and at each decision though the test is
37:57 - conducted to split the data set decision
38:01 - nodes are executed to split the data
38:02 - until it reaches the leaf node a leaf
38:06 - node contains a single target value a
38:08 - single class or a single regression
38:11 - value a leaf node that contains only one
38:14 - target value is considered pure
38:18 - the second one is pruning or shortening
38:22 - of branches of the tree
38:25 - decision trees are prone to overfitting
38:27 - if the parameters are set to favor all
38:30 - pure leaf nodes which means that all
38:32 - data points in the training data set are
38:35 - correctly classified so to reduce
38:37 - overfitting the following strategies are
38:40 - commonly followed one pre pruning
38:44 - stopping the creation of the tree and
38:46 - this is achieved by limiting the maximum
38:48 - depth of the tree is so limiting the
38:52 - maximum number of leaf nodes or defining
38:54 - the minimum number of points required to
38:57 - split it further to post pruning just
39:01 - trimming the nodes that contain less
39:03 - information
39:05 - as you can see a pruned tree has less
39:08 - nodes and has less sparsity than a nun
39:11 - pruned decision tree
39:16 - then the last one is the tree selection
39:19 - in this step the models are looking for
39:22 - the smallest tree that fits the data
39:25 - usually this is the tree that yields the
39:27 - lowest cross validated error decision
39:32 - tree models work really well if the
39:34 - training data set is in a binary format
39:36 - but this is not a limitation for
39:39 - continuous features the decision
39:42 - condition can be applied in the form of
39:44 - greater than or less than a certain
39:46 - threshold for example X is greater than
39:50 - 0.7 a prediction on a new data point is
39:54 - made by traversing through the decision
39:57 - tree and checking at each decision node
39:59 - whether the condition is met
40:01 - or not
40:04 - one of the key factors in the decision
40:07 - tree is entropy so let me explain to you
40:10 - why
40:12 - it's important to carefully consider
40:14 - which feature will be used to split each
40:16 - node because decision trees with a
40:18 - different split node may result in
40:20 - different predictions and accuracy we
40:24 - can utilize a statistical method to
40:26 - identify the feature that should be
40:27 - selected as the root node the feature
40:30 - that has the most information gain
40:32 - should be selected as the root node
40:36 - so information gain measures how well a
40:38 - certain feature distinguishes among
40:40 - different target classifications
40:43 - information gain is measured in terms of
40:45 - the expected reduction in the entropy or
40:48 - impurity of the data the entropy of a
40:51 - set of probabilities is H of P in the
40:55 - formula where P is the probability of
40:59 - outcome event so if the sample is
41:02 - completely homogeneous the entropy is 0
41:05 - and if the sample is an equally divided
41:08 - one it has an entropy of 1
41:13 - to understand how entropy and
41:14 - information gain is calculated let's
41:16 - take a look at the following example
41:18 - okay a training data set has 500
41:21 - observations of these 500 observations
41:24 - 300 are of positive class and the
41:28 - remaining 200 are of negative class
41:31 - positive class ratio equals 300 divided
41:35 - by 500 equals zero point 6 negative
41:39 - class ratio equals 200 divided by 500
41:44 - equals 0.4
41:48 - entropy of the target variable will be
41:52 - entropy equals minus 0.6 times log to
41:59 - 0.6 plus 0.4 times log to 0.4 equals 0.9
42:09 - 7:02
42:11 - a feature X in the dataset is split as X
42:15 - is greater than 347 120 positive and 80
42:20 - negative X is less than or equal to 347
42:25 - 240 positive and 60 negative
42:29 - entropy of X is greater than three
42:31 - hundred and forty seven equals E 1
42:34 - equals negative 120 divided by 200 log
42:40 - two 120 divided by 200 minus 80 divided
42:46 - by 200 log two eighty divided by two
42:49 - hundred and ruvi of X less than or equal
42:54 - to three hundred and forty-seven equals
42:56 - e 2 equals negative 240 divided by 300
43:01 - log two 240 divided by 300 minus 60
43:07 - divided by 300 log 260 divided by 300
43:11 - entropy of x equals two hundred divided
43:14 - by 500 times entropy one plus 300
43:19 - divided by 500 times entropy two
43:23 - information gained for feature X equals
43:25 - entropy total minus entropy of X
43:30 - whichever feature in the note has the
43:32 - maximum information gain will be
43:34 - selected for the split we can also use
43:37 - other indicators like the misc
43:38 - classification rate the Gini index and
43:42 - Inter of Dakota miser 393 for
43:45 - calculating the information gain of a
43:47 - feature what are the hyper parameters of
43:52 - this model free routing parameters are
43:55 - the main hyper parameters of a decision
43:58 - tree model for example the maximum depth
44:01 - of the tree the maximum number of leaf
44:02 - nodes and minimum number of data points
44:04 - required to split the node further a
44:07 - combination of these hyper parameters
44:09 - can be used to build the decision tree
44:11 - which is generalized over the data set
44:14 - and provides good accuracy
44:16 - what are the advantages and the
44:18 - disadvantages of a decision tree
44:21 - decision trees are computationally cheap
44:24 - to use easy for humans to understand
44:27 - results and it can deal with irrelevant
44:29 - features its disadvantages are it is
44:33 - prone to overfitting and provides poor
44:36 - generalization performance sometimes it
44:39 - gives low prediction accuracy
44:42 - training in post pruning strategies are
44:45 - implemented in decision trees to reduce
44:47 - overfitting but still decision tree
44:50 - models tend to overfit so to overcome
44:54 - this data scientists use an advanced
44:56 - modeling technique called ensemble the
44:59 - idea of ensemble is to build many trees
45:01 - all of which predict well and overfit in
45:04 - their own way and average the results to
45:07 - reduce overfitting ensemble means
45:10 - assembling many machine learning models
45:12 - to create a more powerful and robust
45:14 - model these machine learning models also
45:16 - known as base estimators or base learner
45:19 - because they are combined together
45:23 - the most popular ensemble techniques are
45:26 - bagging and random forest bagging
45:29 - combines sampling techniques and
45:31 - aggregation to form an ensemble model
45:34 - and practice multiple samples are chosen
45:38 - randomly with replacement within the
45:40 - training data set let me explain how the
45:43 - sampling is made so suppose that a
45:45 - sample of 10 observations is drawn from
45:48 - a training data set of 100 observations
45:51 - these observations are then returned to
45:53 - the training data set before another
45:55 - sample is drawn so the next sample of 10
45:59 - observations is drawn from a training
46:01 - data set of 100 observations in simple
46:04 - terms at any point all of the training
46:07 - data set observations will be available
46:09 - for a sample to be drawn this sampling
46:12 - technique is called the bootstrap
46:16 - for each of these samples a decision
46:19 - tree or other bass learners is created
46:22 - finally these decision trees or based
46:25 - learners are aggregated to achieve an
46:27 - efficient predictor typically the
46:29 - combined estimator is better than any
46:32 - one of the single decision trees
46:36 - question how does the algorithm choose
46:40 - the output at the end of this
46:41 - aggregation
46:43 - okay the output is chosen based on
46:46 - voting for classification or on an
46:49 - averaging for regression
46:54 - now how about the random forest model
46:59 - so you learned how bagging works random
47:02 - forests also uses the same bagging
47:05 - technique with a slight modification
47:07 - during bagging all features of the
47:10 - training data set are used on sampled
47:13 - data to create the decision trees or the
47:15 - base estimators because of this sampling
47:18 - techniques used in bagging the datasets
47:21 - in each sample are quite similar HBase
47:24 - estimator usually breaks at the same
47:26 - feature this results in quite similar
47:28 - base estimators
47:29 - this means that weak features will not
47:32 - be incorporated to avoid this in random
47:35 - forest model the samples are created
47:38 - with a subset of features selected
47:40 - randomly for each node in the decision
47:43 - tree this selection of a subset of
47:46 - features is repeated separately in each
47:49 - node so that each node in a tree can
47:52 - make a decision using a different subset
47:54 - of features this process of randomly
47:57 - selecting sampled data and a number of
48:00 - features for a split at each node
48:02 - ensures that all decision trees in the
48:05 - random forest are different
48:08 - similar to the decision tree the random
48:11 - forest also provides feature importance
48:14 - which is computed by aggregating feature
48:16 - importance over the trees in the forest
48:19 - typically the feature importance
48:22 - provided by random forests is more
48:24 - reliable than the one provided by a
48:26 - single tree
48:28 - the maximum number of features to split
48:30 - at each node determines how random each
48:33 - tree is and a smaller value reduces
48:36 - overfitting
48:39 - as a rule of thumb this parameter can be
48:42 - set to the square root of a number of
48:44 - features for classification and for
48:46 - regression use cases
48:50 - Criterion Gini entropy number of
48:54 - decision trees maximum number of
48:56 - features maximum depth of the decision
48:59 - tree minimum number of samples required
49:01 - at each leaf node minimum number of
49:04 - samples required to split a node and
49:06 - maximum number of leaf nodes in each
49:08 - decision tree are all hyper parameters
49:11 - of ensemble models this hyper parameter
49:15 - can be tuned to get a generalized and
49:17 - accurate machine learning model
49:21 - benefits of random forest okay what I
49:25 - can say is that ensemble models are very
49:28 - powerful and often work without
49:30 - parameter tuning but the bad news is it
49:33 - is difficult to understand thousands of
49:35 - trees and explain the decision-making
49:37 - process that is why this model is
49:40 - considered like a black box another
49:43 - disadvantage is that ensemble methods
49:46 - need more computing resources and take
49:48 - more time to learn from data ok they're
49:51 - machine learning model is boosting
49:54 - boosting is another model that makes a
49:56 - bass estimator like decision tree more
49:59 - powerful by making a sequential
50:01 - execution and each subsequent estimator
50:04 - focuses on the weakness of the previous
50:07 - estimator boosting incrementally builds
50:10 - an ensemble by training each model with
50:12 - the same data set but where the model
50:15 - coefficients of estimators are adjusted
50:17 - according to the error of the last
50:20 - prediction several weak models team up
50:23 - to produce a powerful ensemble model the
50:26 - main idea of boosting is to focus on the
50:29 - observations that are hard to predict
50:30 - boosting can reduce bias without
50:32 - incurring higher variance
50:36 - here are the popular boosting algorithms
50:38 - Aida boost and gradient boosting let me
50:42 - explain to you how they work
50:43 - Aida boost is adaptive boosting where
50:46 - more attention is given to the records
50:48 - that are not correctly predicted after
50:50 - each iteration weights of the wrongly
50:53 - predicted observations are increased so
50:56 - that these records will be picked up
50:58 - more in the next iteration to gain
51:00 - better accuracy
51:02 - gradient boosting is another popular
51:05 - boosting algorithm it works by
51:06 - sequentially adding the previous
51:08 - predictors under fitted predictions to
51:11 - the ensemble ensuring errors made
51:14 - previously are corrected
51:17 - here's what you need to know boosting
51:20 - does not introduce randomness to the
51:22 - decision trees or any based learners
51:25 - however it uses strong techniques to
51:28 - build accurate predictors in most cases
51:31 - the maximum depth for boosting models
51:33 - has kept a 5 models this makes the model
51:36 - faster and the model consumes less
51:38 - memory the hyper parameters for these
51:41 - models are the number of decision trees
51:44 - maximum depth and learning rate
51:49 - a lowered learning rate means more trees
51:51 - are required and a higher learning rate
51:53 - means less trees are required to build a
51:55 - model these hyper parameters should be
51:58 - tuned to get an optimized machine
52:00 - learning model boosting models are more
52:04 - sensitive to hyper parameters but once
52:06 - the hyper parameters are tuned properly
52:08 - these models provide very good accuracy
52:11 - and generalization
52:14 - so in general ensemble models are very
52:17 - powerful and widely used however like
52:20 - the bagging models it is difficult to
52:22 - understand thousands of trees and
52:24 - explain the decision-making process I
52:26 - just don't recommend it for dimensional
52:28 - sparse data they don't do very well in
52:31 - high dimensional data
52:33 - well the SVM's model or support vector
52:36 - machines support vector machines are one
52:40 - of the supervised learning algorithms
52:42 - mostly used for classification tasks
52:44 - however SVM algorithms can be used for
52:47 - regression as well a support vector
52:50 - machine for classification is called the
52:52 - support vector classifier SVC and for
52:56 - regression it's called the support
52:57 - vector regressor or SVR svms are based
53:03 - on the idea of finding a hyperplane that
53:05 - best divides a data set into two classes
53:08 - as shown in the image below
53:11 - you may ask what the hyperplane is okay
53:15 - take this example say we want to
53:17 - classify a task with only two features
53:19 - you can think of a hyperplane as a line
53:22 - that linearly separates and classifies
53:25 - our data into intuitively the further
53:29 - from the hyperplane our of data points
53:31 - lie the more confident we are that they
53:34 - have been correctly classified we
53:36 - therefore want our data points to be as
53:38 - far away from the hyperplane as possible
53:40 - while still being on the correct side of
53:42 - it so when new testing data is added
53:46 - whatever side of the hyperplane it lands
53:49 - on will decide the class that we will
53:51 - assign to it now how do we find the
53:54 - right hyperplane or in other words how
53:57 - do we best segregate the two classes
53:58 - within the data before we answer this
54:01 - question we need to understand what is a
54:03 - margin a margin is equal to the distance
54:07 - between the hyperplane and the nearest
54:09 - data point from either set now the goal
54:13 - is to choose a hyperplane with the
54:15 - greatest possible margin between the
54:17 - hyperplane and any point within the
54:19 - training set and giving a greater chance
54:21 - of new data being classified correctly
54:23 - as you can see in two dimensions it is
54:27 - very easy to classify using SVM but
54:30 - sometimes it's harder to identify
54:32 - clearly the hyperplane because the data
54:35 - is rarely ever as clean as our simple
54:37 - example above a data set will often look
54:41 - more like the jumbled balls which
54:43 - represent a linearly non-separable data
54:46 - set look at this case
54:50 - in order to classify a data set like
54:52 - this one it's necessary to move away
54:54 - from a two dimension view of the data to
54:57 - a three dimension view explaining this
55:00 - is easiest with another simplified
55:02 - example imagine that our two sets of
55:05 - colored balls above are sitting on a
55:07 - sheet and this sheet is lifted suddenly
55:09 - launching the balls into the air while
55:12 - the balls are up in the air you use the
55:14 - sheet to separate them this lifting of
55:18 - the balls represents the mapping of data
55:20 - into a higher dimension this is also
55:23 - known as kernel
55:26 - okay because we are now in three
55:28 - dimensions our hyperplane can no longer
55:30 - be a line it must now be a plane as
55:32 - shown in the example the idea is that
55:35 - the data will continue to be mapped into
55:38 - higher and higher dimensions until a
55:40 - hyperplane can be formed to segregate it
55:43 - so that's how SVM's work and produce the
55:46 - output
55:49 - so what are the benefits and the
55:51 - disadvantages of SVM's SVM's can give a
55:55 - great accuracy and can work well on
55:57 - smaller cleaner datasets nevertheless if
56:00 - you have a larger data set it isn't
56:02 - suited as the training time can be very
56:04 - high
56:09 - another machine learning algorithm is
56:11 - the K nearest neighbors K and n the K
56:16 - nearest neighbors knn algorithm belongs
56:18 - to the family of instance-based
56:21 - competitive learning and lazy learning
56:24 - algorithms
56:26 - it's a lazy learning algorithm because
56:29 - the calculation is delayed until a
56:31 - prediction is required it is called the
56:33 - localized model because only the data
56:36 - points that are near new data points are
56:38 - used for model calculation and for
56:40 - predicting classes of new data points so
56:43 - let me explain how this works what a
56:45 - prediction is required for an unseen
56:48 - data point the knn algorithm will search
56:50 - through the training data set for the k
56:53 - most similar neighbor the prediction
56:57 - attribute of the most similar data
56:59 - points is summarized and returned as the
57:01 - prediction for the unseen instance
57:05 - so in this model K is the number of
57:07 - neighbors we want to check to classify a
57:10 - new data point if K is greater than one
57:12 - the model uses voting to classify the
57:15 - new data point in short the class that
57:18 - is in the majority is assigned to the
57:20 - new data point for example okay so if
57:22 - the value of K is set to three the model
57:26 - will check the three nearest data points
57:27 - to classify the new data point the
57:29 - default value of K is one which means
57:32 - that the vanilla KNN model classifies
57:35 - the new data point according to the
57:37 - class of the nearest neighbor
57:40 - the yellow and purple circles represent
57:42 - the data points from the training data
57:44 - set and we want to predict the class for
57:46 - the red data point if the value of K is
57:50 - set to three then the model will check
57:53 - the three nearest data points inner
57:55 - circle and then classify the red data
57:58 - point if the value of K is set to six
58:01 - then the model will check for the
58:02 - nearest six data points outer circle and
58:05 - then classify the red data point
58:08 - in the figure above the red point will
58:10 - be classified as follows if K equals
58:13 - three Class B two votes for Class B and
58:18 - one vote for Class A if K equals six
58:21 - Class A two votes for Class B and four
58:25 - votes for Class A
58:28 - okay in a multi-class data set we count
58:31 - how many data points belong to each
58:33 - class and the class that is in the
58:35 - majority is predicted for the new data
58:37 - point so how does the algorithm choose
58:40 - the nearest points
58:43 - the KNN uses the distance to evaluate
58:46 - which point is near to the new data for
58:49 - continuous features Euclidean distance
58:51 - is calculated and for categorical
58:53 - features another distance called hamming
58:55 - distance is calculated
59:00 - the important hyper parameter for KN n
59:02 - is the number of neighbors it holds the
59:05 - value of K the default value of this
59:07 - parameter is 5
59:13 - k-nearest neighbor has a lot of benefits
59:15 - among them its simplicity and
59:17 - flexibility it also works well with
59:20 - enough representative data
59:23 - the problem that we can face sometimes
59:25 - is that the can and algorithm is space
59:28 - consuming because for each prediction
59:30 - the calculation is done separately
59:35 - okay first of all logistic regression is
59:38 - among the most commonly used and best
59:40 - known algorithms that we can use to
59:41 - solve a classification problem it's
59:44 - named the logistic regression because of
59:46 - the logit function
59:49 - which is used in this method of
59:51 - classification other than that
59:52 - logistic regression is pretty much the
59:54 - same as linear regression the purpose of
59:58 - logistic regression is to detect a
60:00 - relationship between features and find
60:03 - the probability of a particular outcome
60:04 - in a way it extends the idea of linear
60:08 - regression to a situation where the
60:10 - outcome variable is categorical
60:13 - for example let's try to predict whether
60:16 - a student will pass or fail an exam the
60:18 - number of hours spent studying is given
60:20 - as a feature and the response variable
60:23 - has two values passed and failed okay in
60:27 - the equation below you can see that we
60:29 - need to predict the Y variable which can
60:33 - take two values 0 or 1 0 for failed and
60:36 - one for passed it turns out that it is
60:39 - virtually impossible to predict Y with
60:43 - the following model y equals B 0 plus B
60:46 - 1 X 1 plus dot dot B K X K that's
60:52 - because Y is a categorical value and B 0
60:57 - plus B 1 X 1 plus dot B K XK will give a
61:02 - continual value as the result therefore
61:05 - instead of predicting this categorical
61:07 - variable we're going to predict the
61:09 - probability of the realization of y
61:12 - equals 1 B equals probability of y
61:15 - equals 1 in order to do that we need a
61:19 - link function the logit link function
61:21 - okay a link function is basically a
61:24 - function of the mean of the response
61:26 - variable Y that we use as the response
61:30 - instead of Y itself it means that when Y
61:32 - is categorical we use the logit of Y as
61:35 - the response in our regression equation
61:37 - instead of just Y the logit function is
61:40 - the natural log of the odds that y
61:43 - equals one of the categories for
61:45 - mathematical simplicity we're going to
61:47 - assume Y has only two categories encode
61:50 - them as 0 and 1 P is the probability
61:53 - that y equals 1 so for instance those
61:56 - X's could be specific hours spent
61:59 - studying number of completed exercises
62:02 - and the score in the first exams while P
62:05 - would be the probability that a student
62:07 - would pass an exam as in our first
62:09 - example linear regression versus
62:12 - logistic regression instead of linear
62:16 - regression the line between y and X the
62:19 - relationship between X and the
62:21 - probability P is a logistic distribution
62:27 - how to estimate the logistic regression
62:29 - coefficients at this point we don't know
62:32 - the coefficients B 0 B 1 B K of the
62:37 - model so we must estimate them in order
62:40 - to make predictions unlike the linear
62:42 - regression model logistic regression
62:44 - uses ordinary least square for parameter
62:47 - estimation the estimation is done by
62:49 - using maximum likelihood due to its more
62:53 - general nature and statistical features
62:55 - there can be an infinite set of
62:59 - regression coefficients the maximum of
63:02 - the log likelihood estimate is that set
63:05 - of regression coefficients for which the
63:07 - probability of getting the data we have
63:09 - observed is maximum in other terms we
63:13 - must make estimates for the coefficients
63:15 - that predictions are as close as
63:17 - possible to the originally observed
63:20 - value so how to make a prediction
63:23 - well the prediction is made using the
63:25 - original logistic function and the
63:27 - estimated coefficients from the maximum
63:30 - likelihood function with the observed
63:32 - data to compute the estimated
63:34 - probability of P if the probability of P
63:38 - is below 0.5 0 the predicted value of y
63:43 - is 0 otherwise it will be 1 in our
63:47 - example the student will fail based on
63:50 - the value of x in the figure we make a
63:53 - prediction by using a different set of X
63:55 - variables the first set gives us a red
63:57 - point with P equals 0.2 9 in the first
64:02 - case the value of y is equal to 0 as the
64:05 - predicted value of P is less than 0.5 0
64:09 - in the second case the green point P
64:12 - equals zero point 9 0 the predicted
64:15 - value of y will be 1 compared to other
64:19 - models logistic regression is rather
64:21 - simple and efficient however it can't
64:23 - handle a large number of categorical
64:25 - variables successfully that's it now you
64:29 - know exactly what logistic regression is
64:32 - and how it works in our next tutorial
64:34 - you'll learn how to apply it in Python
64:36 - and our click on the video above to see
64:39 - this tutorial
64:41 - thanks for your time and hey if you like
64:42 - our videos please like it share it and
64:45 - subscribe to our Channel oh and click on
64:47 - the notification button so you can
64:49 - receive notifications for our next
64:50 - course enjoy machine learning
64:54 - [Music]
65:00 - first though let's see what a neuron is
65:02 - a neuron in the neural networks field is
65:06 - something that takes some input applies
65:08 - some logic and outputs the result we
65:11 - call it a function for example if we
65:14 - have F X equals y X is the input and Y
65:20 - is the output and F is a function
65:24 - to illustrate so let's say I'm trying to
65:26 - understand the relationship between the
65:28 - length of the video we produce on our
65:30 - channel and the time that people
65:32 - actually spend watching the video we
65:34 - collect data from some of our videos I
65:36 - mean we have the video duration let's
65:38 - call it X and the watching time let's
65:41 - call it Y and we imagine there is some
65:44 - relationship between them denoted by F
65:47 - after that I inform the Machine about
65:50 - the relationship I expect to see between
65:52 - these two variables
65:53 - I can choose a linear function between X
65:56 - and Y or a nonlinear function this
65:59 - function is what we call a neuron then
66:03 - we can predict the time people would
66:06 - spend watching a video lesson precisely
66:08 - based on our neuron and the video
66:11 - duration
66:14 - now let's see what a neural network is
66:17 - well in one sentence a neural network is
66:20 - a network of neurons it means that we
66:23 - have many neurons and all their inputs
66:25 - and outputs are intertwined and they
66:27 - feed each other in this figure you can
66:31 - see the difference between a neuron and
66:33 - a neural network as you can see a neuron
66:36 - is a basic unit of learning and a neural
66:38 - network is a bunch of interconnected
66:40 - neurons
66:42 - neural networks help us cluster and
66:44 - classify they helped a group data
66:46 - according to similarities among the
66:49 - example inputs and they classify data
66:51 - when they have the output variable in
66:53 - the existing data set to learn from it
66:56 - the questions you may ask at this point
66:58 - will probably be
67:00 - question 1 what kind of problems do
67:03 - neural networks solve neural networks
67:06 - could be applied for spam filtering
67:08 - fraud detection customer relationship
67:11 - management angry customers or happy
67:13 - customers image recognition self-driving
67:15 - etc
67:17 - question two which functions will I use
67:20 - in each neuron we can use linear or
67:22 - nonlinear depending on the complexity of
67:25 - the problem
67:26 - question 3 what is the architecture of
67:29 - the network we have different types of
67:32 - neural networks a perceptron a recurrent
67:35 - neural network or RNN a convolutional
67:38 - neural network CNN etc
67:43 - now how we can run our neural network in
67:47 - the first place the neural network learn
67:49 - to recognize patterns just like a human
67:52 - we show them examples of correct inputs
67:55 - and outputs in the hope that when we
67:57 - give it a new example input that it's
68:00 - never seen before it will know how to
68:02 - give the correct output that's what we
68:04 - call training on existing data sets
68:07 - don't forget machine learning equals
68:10 - learning from examples
68:13 - let me present you the most basic neural
68:15 - network the perceptron and discuss how
68:19 - it processes inputs and produces an
68:21 - output so suppose we use our neural
68:23 - network for a froot image recognition I
68:26 - have two inputs for that purpose the
68:28 - color and the shape of some fruits and
68:31 - our data sets and a single binary output
68:33 - which is the fruit name
68:37 - once the machine has learned all these
68:39 - properties I can give it a new image of
68:41 - a fruit when it hasn't seen before and
68:43 - it will hopefully classify it correctly
68:46 - and be able to tell me whether it is an
68:48 - orange or a banana the perceptron learns
68:51 - from the existing data and knows which
68:53 - information will be most important in
68:55 - decision making to decide between
68:57 - multiple information it uses something
69:00 - called weights the weights are just
69:03 - numerical representations of these
69:05 - preferences a higher weight means our
69:07 - perceptron considers that input more
69:09 - important compared to other inputs so
69:13 - for our example let's deliberately set
69:15 - suitable weights for our two inputs two
69:18 - for the fruit shape and four for the
69:21 - fruit color
69:23 - now how does the perceptron calculate
69:26 - the output it simply multiplies the
69:28 - input with its respective weight and
69:31 - sums up all the values it gets from all
69:33 - the inputs let's consider that we have
69:36 - two shaped round and long if the shape
69:39 - is round the input one value is one and
69:42 - if it's not round the value is zero
69:45 - we'll repeat the same thing with the
69:46 - color red takes the value of one and the
69:49 - yellow color the value of zero based on
69:53 - this information if the fruit is round
69:54 - in red our perceptron would do the
69:57 - following calculation total equals round
70:01 - times shape weight plus red times color
70:05 - weight so total equals one times two
70:09 - plus one times four equals six
70:14 - this calculation is known as a linear
70:16 - combination now let's see what this
70:18 - value 6 means we first needed to find
70:21 - the threshold value because the
70:24 - perceptrons output is either 0 or 1 0
70:27 - for a banana and 1 for an orange this
70:30 - output is determined like this if the
70:32 - value of the linear combination is
70:34 - higher than the threshold value then the
70:37 - output is 1 and if it is not the output
70:40 - is 0 so let's say the threshold value is
70:43 - 3 which means that if the calculation
70:45 - gives you a number less than 3 we have a
70:48 - banana but if it's equal to or more than
70:52 - 3 then we have an orange
70:56 - that's how perceptron works it uses a
70:59 - linear combination and produces the
71:01 - output in reality we set the weights to
71:04 - random values and then the network
71:06 - adjusts those weights based on the
71:08 - output errors it made using the previous
71:11 - weights that is called training the
71:13 - neural network
71:15 - in the mathematical language the
71:17 - perceptron algorithms work like this the
71:21 - output is equal to zero if the sum of
71:23 - the weight times the value of the
71:24 - variable is smaller than a threshold in
71:27 - the same way the output will be equal to
71:29 - one if the sum of the weight times the
71:32 - value of the variables is bigger than a
71:35 - threshold
71:39 - to make things just a little simpler for
71:41 - training the threshold is sometimes move
71:44 - to the other side of the inequality and
71:46 - replaced with what's known as the
71:48 - neurons bias now with bias we only need
71:53 - to make changes to the left side of the
71:55 - equation while the right side can remain
71:57 - constant at zero the left side of the
72:00 - equation is a function it is a function
72:02 - that transforms the values or states the
72:05 - conditions for the decision of the
72:07 - output neuron it is known as an
72:09 - activation function
72:12 - the formula above is just one of several
72:14 - activation functions and and the
72:16 - simplest one used in deep learning and
72:19 - it is called the Heaviside step function
72:24 - in reality we can also use other
72:26 - activation functions for example we can
72:29 - use the sigmoid function the tan
72:31 - function and the softmax functions each
72:35 - of them has a purpose and we'll present
72:36 - each of these functions in a special
72:38 - video dedicated just to that
72:41 - that's it for our first introductory
72:43 - video on neural network now you have a
72:46 - solid understanding of the basics of
72:48 - neural networks and how perception works
72:50 - in our next video in neural networks we
72:54 - will explain how they multi layers
72:56 - perceptron works and we will present the
72:58 - concept of hidden layers
73:01 - okay let's present an unsupervised
73:04 - machine learning model the clustering in
73:07 - general and the k-means clustering in
73:09 - particular clustering models are used to
73:12 - analyze unlabeled data and get useful
73:15 - insights from it clustering is equal to
73:18 - grouping things means the data points
73:20 - which are similar in some way and
73:22 - different from other data points are
73:24 - grouped together each group of the data
73:26 - points is known as a cluster
73:30 - in the example data points are divided
73:33 - into four clusters based on the
73:35 - geometrical distance between two data
73:37 - points the data points which are close
73:40 - to each other are assigned to one
73:42 - cluster this approach of creating
73:44 - clusters is known as distance based
73:47 - clustering another approach of creating
73:50 - clusters is the conceptual clustering in
73:54 - this approach clusters are based on
73:56 - conceptual similarity so for example in
73:59 - a data set of oranges and apples two
74:02 - clusters will be created one for all
74:05 - apples and another one for all oranges
74:07 - this approach uses properties of data
74:10 - points to distinguish one data point
74:12 - from another and to group data points
74:15 - with similar properties together
74:17 - clustering is a very subjective area of
74:21 - discussion it is difficult to determine
74:23 - how one clustering is better than the
74:26 - other one
74:27 - machine learning algorithms learn the
74:30 - insights from the data and apply these
74:32 - insights to create clusters since we
74:35 - don't know those insights it is hard to
74:38 - say the clustering done by the machine
74:39 - learning model is best or not so the
74:43 - user must provide an appropriate
74:45 - criterion to get suitable clusters as an
74:48 - output from the machine learning model
74:51 - consider a data set that contains green
74:54 - color balls red color balls green color
74:57 - apples and red color apples now if we
75:01 - provide color as the clustering criteria
75:04 - to the machine learning model then it
75:06 - will create two clusters one of the
75:08 - green color balls and green color apples
75:11 - and the second one of course of red
75:13 - color balls and red color apples however
75:16 - if the criteria provided is edible then
75:21 - the machine learning model will create
75:22 - two clusters one of all apples and
75:25 - another one of all balls
75:30 - clustering algorithms can be applied in
75:32 - many industries marketing like finding
75:35 - groups of customers with similar
75:37 - behavior or customer segmentation
75:39 - biology for plants classification
75:42 - insurance fraud detection city planning
75:46 - earthquake studies document
75:48 - classification identifying crime
75:50 - localities cyber profiling criminals etc
75:56 - now let me explain to you how K means
75:58 - clustering works k-means clustering is
76:02 - one of the most useful clustering
76:04 - algorithms it tries to find cluster
76:07 - centers that represent certain regions
76:10 - of the data k-means clustering is a
76:12 - two-step process one assign each data
76:16 - point to the closest data center to set
76:21 - the cluster center as the mean of the
76:23 - data points assigned to the cluster
76:26 - the algorithm keeps on executing these
76:29 - two steps until the assignment of data
76:31 - points two clusters no longer changes
76:34 - once we decide how many clusters are
76:36 - required the number of clusters is
76:39 - passed to the algorithm so let's say for
76:41 - a given data set we want three clusters
76:44 - in the initiation step the algorithm
76:47 - randomly selects three cluster centers
76:50 - each data point is assigned to the
76:53 - cluster Center it is closest to
76:56 - we then update the cluster Center with
76:58 - the mean of the cluster the previous two
77:01 - steps are repeated until the assignment
77:03 - of data points two clusters no longer
77:05 - changes
77:08 - clustering is similar to classification
77:10 - the only difference is that
77:13 - classification use cases have labeled
77:16 - data I mean the classes are defined and
77:19 - clustering creates various clusters or
77:22 - classes from the data set
77:27 - one of the major benefits of the k-means
77:29 - algorithm is the implementation k-means
77:33 - is a relatively efficient method however
77:36 - we need to specify the number of
77:38 - clusters in advance and the results are
77:41 - sensitive to initialization and often
77:44 - terminates at a local optimum
77:47 - unfortunately there's no global
77:49 - theoretical method to find the optimal
77:51 - number of clusters
77:53 - a practical approach is to just compare
77:56 - the outcomes of multiple runs with
77:58 - different K and choose the best one
78:00 - based on a predefined criteria in
78:03 - general a large K probably decreases the
78:06 - error but increases the risk of
78:09 - overfitting
78:12 - you
78:14 - in the previous part of our course we
78:16 - learned the basics of data science and
78:19 - machine learning and the most important
78:21 - machine learning models in practice we
78:24 - can apply manually or automatically
78:26 - various models on a given data set
78:29 - before finalizing and choosing the right
78:31 - model and the more efficient one
78:34 - but how do we evaluate the performance
78:36 - of a model to determine the one model
78:39 - that is performing better in this part
78:42 - of the course we will answer this
78:43 - question by presenting some performance
78:46 - indicators to evaluate the model
78:50 - let's start with the R squared R squared
78:55 - is the most common way to evaluate the
78:58 - performance of a linear model it is a
79:00 - statistical measure of determining how
79:02 - close the data is to the fitted
79:05 - regression line it is a proportion of
79:08 - explained variance to total variance
79:11 - r2 equals explained variance over total
79:16 - variance
79:18 - you can see how it's represented
79:20 - graphically here
79:23 - typically the r-squared value lies
79:26 - between zero and one
79:29 - zero means the model does not explain
79:31 - any variability of the data around it's
79:34 - me
79:36 - one means the model explains all the
79:39 - variability of the data around it's me
79:42 - so the higher the value of r-squared the
79:46 - better the model fits the data and
79:48 - better the model explains the variance
79:50 - of the observed data around its meat
79:54 - however it is difficult to tell whether
79:56 - an increase in the r-squared value is a
80:00 - result of better model performance or
80:02 - more features in the model
80:08 - that's why in multiple linear regression
80:11 - it is better to choose the adjusted
80:14 - r-square
80:16 - adjusted r-squared is a modified version
80:18 - of r-squared that has been adjusted for
80:21 - the number of features in the model
80:24 - adjusted r-squared penalize --is the
80:27 - r-squared if the choice of the feature
80:30 - newly added to model he's not good
80:36 - so in classification scenarios having
80:39 - good r-squared or adjusted r-squared
80:42 - values is not important until both the
80:45 - classes are unidentified with similar
80:48 - accuracy this becomes a big issue when
80:52 - the data is not equally distributed for
80:54 - the targeted classes okay for example
80:57 - consider a data set with 98% observation
81:02 - of Class A and 2% observation of Class B
81:06 - the model can easily get 98% training
81:09 - accuracy by simply predicting that every
81:12 - sample belongs to Class A but the
81:15 - prediction accuracy for Class B is very
81:17 - poor
81:19 - to overcome this situation model
81:22 - performance should be evaluated for each
81:24 - class and then aggregated to get the
81:27 - overall performance of the model
81:29 - confusion matrix provides the right
81:32 - tools to get the individual and overall
81:34 - performance of a classification model so
81:37 - let's take this example with two class
81:40 - positive and negative classes
81:44 - in the figure we have the classifier the
81:47 - prediction outcome and actual values the
81:50 - total number of positive classes is P
81:53 - and the total number of negative classes
81:56 - is n TP true positive in the top left
82:00 - corner of the figure equals actual
82:03 - classes positive and predicted class is
82:05 - also positive this states how many
82:08 - positive classes correctly predicted
82:10 - this is also known as the power of the
82:12 - model FN false negative in the top right
82:16 - corner of the figure actual class is
82:18 - positive and predicted class is negative
82:21 - this states how many positive classes
82:24 - are predicted as negative this is also
82:26 - known as a type 2 error miss
82:31 - FP falls positive in the bottom left
82:34 - corner in the figure actual classes
82:36 - negative and predicted class is positive
82:39 - this states how many negative classes
82:42 - are predicted as positive this is also
82:45 - known as a type 1 error false alarm
82:48 - TN true negative in the bottom right
82:52 - corner of the figure actual class is
82:54 - negative and predicted class is negative
82:56 - this states how many negative classes
82:59 - are correctly predicted
83:01 - so correctly predicted classes is equal
83:04 - to TP plus TN incorrectly predicted
83:08 - classes is equal to FP plus FN actual
83:13 - positive classes is equal to P equals TP
83:17 - plus FN actual negative classes is equal
83:22 - to N equals FP plus TN
83:28 - so we compute the proportion of records
83:31 - that are correctly classified called
83:33 - the accuracy of the classification model
83:36 - and it is calculated as follows
83:38 - accuracy equals T P plus TN over TP plus
83:45 - TN plus FP plus FN
83:52 - so another important validation
83:54 - technique is the cross validation cross
83:57 - validation is a tool that utilizes the
83:59 - training data set in a better way to
84:02 - reduce overfitting and underfitting it
84:04 - is a model validation technique for
84:07 - assessing how the results of statistical
84:10 - analysis will generalize to an
84:12 - independent data set the purpose of
84:15 - using cross-validation is to increase
84:18 - confidence in the model trained by the
84:20 - training data set
84:21 - so without cross validation our model
84:24 - may perform well on the training data
84:25 - set but their performance decreases when
84:28 - applied to the testing data set the
84:31 - testing data set is precious and should
84:33 - only be used once so the solution is to
84:36 - separate one small part of the training
84:38 - data set as a test of the trained model
84:41 - which is the validation data set
84:46 - okay here are two cross validation
84:48 - techniques k-fold cross-validation this
84:53 - involves splitting the training data set
84:55 - into K subsets of data also known as
84:58 - folds the machine learning model is
85:01 - trained on k1 subsets and then evaluated
85:04 - on the subset that was not used for
85:06 - training this process is repeated k
85:09 - times with a different subset reserved
85:12 - for evaluation and excluded from
85:14 - training each time so once the model has
85:17 - been executed for all training subsets
85:19 - the average of error in each run is
85:22 - calculated and represented as the
85:25 - cross-validation error
85:29 - leave one out cross validation this is
85:33 - another technique used for cross
85:34 - validation it's a logical extreme of
85:37 - k-fold cross-validation where K equals n
85:41 - the number of observations so for each
85:44 - run only one observation is left with a
85:47 - validation data set this approach leads
85:49 - to higher variation and testing model
85:52 - effectiveness because testing is done
85:54 - against one observation only hence the
85:57 - estimation is highly influenced by the
85:59 - validation observation if the validation
86:02 - observation is an outlier it can lead to
86:05 - higher variation
86:10 - at this last part of the course I will
86:13 - give you some best practices in data
86:15 - science and machine learning okay you
86:18 - need to be aware that the data set that
86:20 - we will get for machine learning
86:22 - problems is not always as clean as we
86:25 - can expect we might have to clean the
86:28 - data set and transform it into a data
86:30 - set that can be used to build a model
86:32 - for this purpose we might have to go
86:34 - through multiple processes like data
86:37 - processing feature engineering and
86:40 - feature extraction feature scaling and
86:43 - selection
86:47 - this set of processes is a major part of
86:50 - a machine learning project because the
86:52 - model performance is based on the data
86:54 - on which the model is trained garbage in
86:56 - garbage out we already talked a little
87:00 - bit about feature engineering and
87:02 - feature extraction
87:04 - I'm going to give you some best
87:05 - practices that we can follow to tackle
87:08 - the data sets and machine learning use
87:09 - cases
87:12 - another important concept is one hot
87:15 - encoding
87:17 - machine learning cannot handle
87:19 - non-numeric features by themselves so
87:21 - the question that you may ask is how do
87:23 - we transform the non numeric features
87:26 - let's take our example in the employee
87:29 - salary suppose that in the data set
87:31 - gender is maintained as a non numeric
87:34 - feature and this feature can have two
87:36 - values male or female for an
87:39 - organization and employees gender is
87:41 - meaningful information since it is a non
87:43 - numeric feature if we try to train a
87:46 - model on the gender feature then the
87:48 - model will not be able to interpret
87:50 - anything meaningful from it to make the
87:53 - gender column meaningful to the model we
87:55 - must transform it into a numeric column
87:58 - one hot encoding is one of the
88:01 - transformation techniques that can be
88:03 - used to transform categorical features
88:05 - into numeric categorical features so if
88:09 - the feature has only two categories then
88:12 - those two categories can be replaced by
88:14 - 0 & 1 in our employee data set example
88:18 - we can replace male with 0 and female
88:20 - with 1
88:23 - now consider a scenario where the
88:25 - categorical column has more than two
88:27 - categories in other words in our data
88:29 - set
88:30 - gender have three values male female and
88:34 - unknown so what hot encoding can be
88:38 - extended to take care of this multi
88:40 - category features it creates a new
88:42 - feature for each category and for each
88:45 - observation the value one will be
88:48 - assigned to the newly created feature to
88:50 - which the category belongs to
88:54 - other newly-created features will be set
88:56 - to zero for example in our employee
88:58 - gender example three new features will
89:01 - be created like in the table
89:05 - binning in a few machine learning
89:08 - scenarios continuous features cannot be
89:10 - used directly to train a model these
89:12 - features should be converted into
89:14 - categorical features and then one hot
89:18 - encoding should be applied to make these
89:19 - features important for the machine
89:22 - learning model and our employee data set
89:24 - example employee age ranges from 21 to
89:28 - 60 years these employees can be
89:31 - categorized into four age brackets 21 to
89:34 - 30 31 to 40 41 to 50 and 51 to 60 the
89:40 - technique of converting a continuous
89:42 - feature into multiple bins and creating
89:45 - a new feature from it is known as
89:47 - binning or bucket ization it's also
89:50 - known as quantization binning transforms
89:53 - a continuous feature into a categorical
89:56 - feature and categorical feature
89:58 - engineering might need to be performed
90:01 - before using this feature in modeling we
90:05 - can create a new age group feature and
90:07 - map each employee with one of these
90:09 - brackets for bidding we can use the
90:12 - domain expertise as well as a few
90:15 - statistical methods to correctly
90:17 - determine the number of Bin's
90:18 - / buckets and the boundaries for each
90:21 - bin so some of the common methods of
90:24 - bidding are one fixed width binning in
90:27 - this technique width is decided for each
90:30 - bin based on domain knowledge rules or
90:33 - constraints to quantile based binning
90:36 - this technique divides the data into Q
90:39 - equal partitions if q equals 4 then the
90:43 - parts are quartiles divide data into
90:46 - four equal partitions three two way
90:49 - ANOVA analysis of variance tests this is
90:54 - used to find similarity between the
90:55 - various data points of the feature these
90:58 - similar data points can be grouped
90:59 - together to partition the data set
91:03 - you
91:04 - first let's discuss what feature
91:07 - engineering is features are the
91:10 - independent variables don't forget we
91:12 - will use this term a lot feature feature
91:17 - equals variable once again independent
91:20 - variables or features are used to
91:22 - predict the dependent variable
91:24 - frequently these features have hidden
91:26 - information that the machine learning
91:28 - models cannot utilize to negate this
91:32 - situation in the data pre-processing
91:34 - stage we just applied the domain
91:36 - knowledge and create new informative
91:38 - features out of the existing features
91:40 - available in the data set
91:43 - these features should be created
91:45 - carefully though otherwise the model may
91:47 - over fit the set of features on which
91:50 - the model is supposed to run should be
91:52 - selected wisely because good features
91:54 - with good quality data can yield a less
91:57 - complex model with better results
92:01 - feature engineering is a recursive
92:04 - process that can be divided into the
92:06 - following steps
92:08 - understand data set
92:11 - brainstorm features
92:14 - create new features
92:17 - validate what impact these features have
92:20 - on the prediction result
92:23 - restart from step one until the desired
92:26 - accuracy and other metrics are achieved
92:28 - so for example consider a model designed
92:31 - to predict salary hikes for employees in
92:34 - an organization the data set contains
92:37 - employee ID geographical location date
92:41 - of birth employment start date career
92:44 - start date etc in this data set date of
92:47 - birth and career start date might not be
92:50 - useful for a data set if we derive two
92:53 - new features for example employee age
92:56 - and years of experience these two
92:58 - features could play a key role in salary
93:00 - hike prediction the process of
93:04 - identifying creating these derived
93:05 - features is called feature engineering
93:08 - in reality though feature engineering is
93:11 - an art and it comes with domain
93:13 - knowledge and experience
93:17 - feature scaling some machine learning
93:20 - algorithms do not perform well when all
93:22 - features in the data set are not on the
93:25 - same scale so coming back to our
93:27 - employee data set example salary may
93:30 - range from $40,000 to $200,000 and age
93:34 - ranges from 21 to 60 days two features
93:38 - do not have the same scale rescaling
93:41 - these features can improve the
93:43 - performance of some machine learning
93:45 - models
93:47 - now how we can make feature scaling we
93:50 - have two common techniques for feature
93:51 - scaling the first one is normalization
93:55 - this technique transformed the feature
93:57 - into a 0 to 1 scale the great thing
94:00 - about normalization is to reduce also
94:03 - the effective outliers the new value of
94:07 - the feature called exchanged is equal to
94:10 - the value of X minus the minimum x value
94:13 - and dividing by the maximum x value
94:16 - minus minimum x value exchanged equals X
94:21 - minus X min over X max minus X min
94:28 - the second one is standardization which
94:30 - transforms the features so that the mean
94:32 - of the distribution becomes zero and the
94:35 - variability becomes one standardization
94:38 - is not significantly affected by
94:40 - outliers
94:43 - look at the figures below they show how
94:46 - the transformed feature will be after
94:49 - implementing normalization and
94:51 - standardization
94:55 - well that's it for this free AI Sciences
94:58 - course hopefully this course has
95:00 - demystified the notion of data science
95:03 - and machine learning that is just the
95:06 - beginning so now that you are familiar
95:08 - with the logic behind the different
95:09 - types of learning you know how to create
95:12 - simple models it's time to move on but
95:15 - you will not be alone on that path we
95:17 - have more content courses and books to
95:19 - reinforce your efforts and guide you at
95:21 - every stage machine learning is a
95:24 - crucial development of today's world the
95:27 - concepts behind it have been around for
95:28 - more than a decade but the age of
95:30 - machine learning and related models such
95:33 - as artificial intelligence data science
95:35 - and more is now the change is just
95:39 - happening and it is fast you've made a
95:43 - great decision to start your journey
95:44 - into the world of machine learning with
95:46 - this free course today the knowledge and
95:48 - the ability to use machine learning is a
95:51 - competitive advantage tomorrow it will
95:54 - be a mere necessity machine learning
95:57 - techniques have already started to
95:59 - change the world of business by creating
96:01 - a new value for data the future will be
96:04 - even more exciting very soon most of the
96:07 - devices and apps that we use daily will
96:09 - be fueled by machine learning algorithms
96:11 - many of them already are now though you
96:15 - have a chance to become a part of this
96:17 - major development Congrats on your
96:19 - decision and don't forget to check out
96:21 - our courses ebooks and contents now as I
96:25 - promised you there is a gift for you
96:27 - it's an e-book available online where
96:29 - you will learn about machine learning
96:31 - you can find it at this link HTTP colon
96:36 - forward slash forward slash WWI Sciences
96:41 - dotnet forward slash gift
96:44 - - eBook - machine - learning
96:50 - we highly recommend that you visit our
96:52 - website AI Sciences dotnet and subscribe
96:55 - to our email list you'll receive all of
96:58 - our books free in an eBook format and
97:01 - you will be informed about all our
97:03 - promotions and offers if you have any
97:06 - feedback please let us know by sending
97:08 - an email to review at AI Sciences net
97:13 - this feedback is highly valued and we
97:15 - really look forward to hearing from you
97:18 - if you enjoyed this video please like it
97:20 - share it subscribe to our Channel AI
97:22 - sciences and make sure you click on the
97:25 - notification button so you can receive a
97:27 - notification when our next course is
97:29 - ready see you at the next video
97:37 - [Music]