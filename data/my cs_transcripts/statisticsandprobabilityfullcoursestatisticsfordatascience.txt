00:04 - well it looks like it's time to get
00:05 - started
00:06 - so let's start with section one of
00:08 - chapter one
00:09 - discussing uh descriptive statistics and
00:13 - some basic terminology
00:14 - of statistics so all right let's uh
00:18 - get to work then so uh
00:22 - so in this chapter in this section we're
00:24 - first going to start out
00:26 - uh with some basic terminology uh let's
00:28 - start let's start out with the
00:30 - data data is what statistics is
00:32 - interested in
00:34 - we're going to say that data is
00:35 - basically a collection of facts
00:37 - we have generally a population
00:41 - or some group of interest for which
00:44 - we want to make some statements so
00:48 - if we were to collect data for the
00:50 - entire population we would have
00:51 - conducted a census
00:52 - uh let's let's have some examples
00:54 - there's the classic example of the
00:56 - united states because
00:58 - right now we're actually in a census
01:01 - the 2020 census being conducted by the
01:03 - census bureau
01:04 - and this is a massive
01:08 - undertaking by the u.s government
01:10 - mandated by the constitution
01:12 - to count every single individual
01:15 - currently living in the united states
01:18 - and with that get
01:22 - to get a count that is supposed to be a
01:23 - complete count not an estimate but a
01:25 - count this becomes the facts
01:27 - of how many people are currently living
01:28 - in the united states
01:31 - on a smaller scale there is
01:34 - this class and i presumably have the
01:39 - grades for this class
01:40 - so if i were interested in the grades of
01:42 - this class i could conduct a census by
01:44 - looking at the grades of every single
01:46 - student
01:47 - and with that i would have the entire
01:50 - data set the entire population where
01:52 - i've defined the population to be
01:54 - this class now that said
01:58 - you may not necessarily have access to
02:00 - the entire population for example
02:03 - in the case of the united states the
02:05 - census is
02:07 - an extremely expensive and complex
02:10 - undertaking that can only be done every
02:12 - 10 years
02:15 - and in the case of this class well i can
02:17 - conduct a census
02:18 - you yourself as a student may not be
02:20 - able to conduct a census
02:22 - so if you were interested in how your
02:25 - other
02:25 - fellow students were doing in the class
02:28 - you would be forced to collect a sample
02:30 - a sample is a subset of the population
02:34 - for which you managed to collect data so
02:38 - in the case of the class if you were to
02:41 - talk to some of your friends which may
02:43 - not be the greatest idea anyway
02:45 - but if you were to talk to some of your
02:46 - friends about uh
02:48 - how and ask them how they were doing in
02:50 - the class then you would have conducted
02:51 - this that you would have
02:53 - uh selected a sample uh whereas um
02:57 - uh in the united states we're regularly
03:00 - uh subjected to surveys where a subset
03:03 - of the american population
03:05 - is going to be examined and you're going
03:08 - to ask questions about them and in the
03:10 - case of the census bureau
03:11 - they have a lesser intensity
03:16 - project such as the american community
03:18 - survey which
03:19 - isn't aimed at getting the entire
03:21 - population i think
03:23 - every year they try to get one percent
03:26 - of the population
03:27 - which is big in and of itself all right
03:30 - so uh in a sample we have
03:34 - observations so maybe these are
03:37 - people but in principle they could be
03:40 - just about anything like they could be
03:41 - petri dishes they could be
03:44 - the price of a stock on a particular day
03:46 - something like that
03:47 - and uh with those observations we have
03:50 - variables so in the case of our little
03:53 - person over here who is an observation
03:54 - in our sample
03:55 - we may have tracked their age we'll say
03:58 - this person is
04:00 - 22 we may track their gender
04:04 - uh we'll say that this person uh
04:07 - identifies
04:08 - as male or maybe we are tracking i don't
04:11 - know their
04:12 - uh occupation
04:15 - and we'll just say that this person is a
04:17 - student
04:19 - all right so univariate data is a data
04:23 - where there's only
04:24 - one variable being tracked so
04:28 - in this case if all we did was track age
04:30 - then this would be a univariate data set
04:32 - but this is actually a multivariate data
04:34 - set because we're not
04:35 - only tracking age we're also tracking
04:37 - gender and occupation
04:38 - so that makes this uh data set
04:40 - multivariate presumably
04:42 - there's more than just this individual
04:45 - in our sample uh maybe there are some
04:48 - under
04:49 - other individuals too for whom we've
04:51 - collected some data in this case this
04:53 - would be a multivariate data set
04:55 - there is kind of a special case for the
04:57 - multivariate case which is the bivariate
04:59 - case
04:59 - and bivariate data is a data where
05:02 - there's two variables
05:03 - and when you have only two variables at
05:05 - least in this class we can start talking
05:07 - about
05:08 - ideas such as correlation how are two
05:11 - uh variables related to each other uh
05:13 - you can do this for
05:15 - general multivariate data too but that's
05:17 - kind of going
05:18 - uh beyond the scope of this course and
05:20 - even then you kind of start out with the
05:22 - bivariate case first
05:24 - so data itself
05:27 - the variables can come in different
05:29 - classes such as there can be categorical
05:31 - variables
05:32 - so an example of categorical would be
05:35 - gender and occupation these would be
05:38 - categorical variables because there
05:40 - are a finite number of categories that
05:42 - these could be in
05:45 - compare that to say age which we would
05:47 - consider
05:49 - quantitative because what's being
05:51 - tracked is
05:53 - a number presumably an unbounded number
05:55 - now you might think well what about
05:57 - say uh the roll of a dice if you're
06:00 - rolling a dice
06:01 - then there's only a finite number of
06:03 - numbers one through six
06:04 - this is pretty much a simplification
06:06 - more with categorical we're thinking
06:08 - with things where
06:10 - there may or may not be an ordering but
06:11 - we really don't want to view categorical
06:13 - data necessarily as
06:15 - numbers per se
06:18 - whereas quantitative are numbers right
06:21 - and there's also
06:22 - alternative formulations alternative
06:24 - breakdowns uh with maybe some more
06:26 - granularity
06:27 - on how you're going to break down data
06:29 - like for example you might say
06:31 - there's nominal data where it's just
06:34 - categories that have no relation amongst
06:36 - each other
06:37 - you might then progress to ordinal data
06:40 - where you do add a little bit more
06:41 - structure
06:42 - where one observation where one value
06:45 - could be considered
06:45 - greater than another maybe we could talk
06:48 - about a person's year
06:50 - in college in which case you would say
06:52 - that sophomore
06:53 - is less than in some sense junior which
06:56 - is less than
06:57 - a senior
07:00 - although you don't necessarily want to
07:02 - attach numbers to
07:04 - uh sophomore freshman sophomore junior
07:07 - senior
07:08 - and then you could have maybe interval
07:10 - where
07:11 - you're allowed to do addition and ratio
07:14 - where you're allowed to do division
07:16 - so uh but but yeah that's
07:20 - an alternative breakdown don't worry too
07:22 - much about it
07:23 - in modern statistics we depend very
07:26 - heavily on probability theory
07:28 - probability theory is a field of
07:30 - mathematics that describes
07:31 - the behavior of objects in the presence
07:34 - of uncertainty
07:35 - the mathematical study of probability
07:37 - we've known about probability as a
07:38 - species for a very long time at least
07:40 - since the greeks
07:41 - but it's only until around the time
07:43 - calculus was being developed and
07:45 - probability
07:46 - uh started to take a mathematical
07:48 - interest and then it was around
07:50 - the beginning of the 20th century where
07:52 - probability became its own
07:54 - proper uh field in mathematics
07:57 - so why do statisticians care about
08:00 - probability how's probability used in
08:02 - statistics
08:03 - well we have this relationship um
08:07 - we have a population and we have
08:13 - a sample
08:18 - so a sample is a subset of the
08:19 - population so the question is
08:22 - how is it that we get a sample
08:26 - from the population how when when we
08:30 - collect a random sample from the
08:32 - population
08:33 - where every individual in the sample is
08:35 - picked
08:36 - with some with equal likelihood how will
08:39 - that sample behave
08:40 - how will statistics computed in that
08:43 - sample such
08:44 - such as for example the sample mean or
08:47 - even then like going beyond mean like
08:49 - let's say we track the smallest
08:51 - age how will statistics like that
08:54 - behave the behavior of those statistics
08:58 - is determined by probability
09:02 - so probability describes how
09:05 - random samples from a population behave
09:08 - and as statisticians we develop
09:11 - probability models
09:12 - for our samples and then use those
09:14 - probability models
09:16 - to describe uh parameters of the
09:18 - population
09:20 - so this would be statistics
09:26 - so probability and statistics are
09:28 - working in inverse directions
09:30 - where probability describes how if you
09:33 - knew the population how will the sample
09:35 - behave
09:36 - whereas statistics is interested in
09:39 - given a sample
09:40 - how can we infer the properties of the
09:42 - population like for example
09:44 - important population parameters
09:48 - so
09:51 - okay
09:54 - all right so uh let's let's continue on
09:57 - uh
09:59 - how we define a population largely
10:01 - depends on
10:02 - our problem for example in the examples
10:05 - that i gave before about the united
10:06 - states and my class
10:09 - we would consider a study involving
10:11 - those populations in numerative studies
10:14 - since in principle there is
10:17 - the population exists the population
10:20 - exists in physical
10:21 - and temporal space so
10:24 - that means that there's no like
10:28 - you can actually find in principle every
10:30 - single individual
10:32 - in the united states and you can find
10:34 - every single
10:35 - individual in my class and a census is
10:37 - in fact possible to conduct
10:39 - now compare that instead to analytic
10:43 - studies where the population may not
10:45 - exist so
10:46 - in my previous example where i was
10:48 - talking about petri dishes as being a
10:50 - potential
10:51 - sample a petri dish for
10:54 - some bacterial culture is probably going
10:56 - to appear in an analytic study
10:58 - a lot of those biological studies are
10:59 - going to be analytic studies because the
11:01 - population isn't necessarily considered
11:03 - existing
11:04 - if or in a simpler case if all i wanted
11:07 - to do was determine
11:09 - whether a single dice that i have was
11:11 - fair
11:12 - in principle you can't really say that
11:17 - all of the possible roles for this dice
11:19 - exist
11:20 - in some physical temporal space i can
11:23 - keep rolling this dice
11:24 - forever so uh
11:28 - you can't really consider an enumerative
11:30 - study so you're just gonna say the
11:31 - population is all possible die
11:33 - rolls for this dice and in the case of a
11:35 - medical study you might say
11:37 - that the population is all humans past
11:40 - present and future and we're studying
11:43 - the behavior
11:44 - of some drug in humans
11:47 - we're we're thinking of humans as some
11:49 - biological
11:51 - as a human species as a biological
11:53 - entity uh not necessarily
11:55 - the current people who are living right
11:57 - now we would probably want to include
11:58 - future people too
11:59 - um all right so
12:03 - uh statistics is it depends very
12:05 - crucially on how the data is collected
12:07 - and we're not actually going to talk
12:09 - very much in this class about how to
12:11 - collect
12:12 - data correctly i'm just going to leave
12:14 - you a few words
12:16 - in this class we're going to assume that
12:18 - data was collected
12:20 - via a simple random sample so
12:23 - if data was in fact collected via a
12:26 - simple random sample
12:27 - then ins there is a sense in which every
12:30 - individual
12:31 - in in the population is equally likely
12:34 - to have been chosen
12:35 - uh the metaphor that i like to think of
12:38 - is
12:38 - we have a hat and that hat has little
12:42 - slips of paper and each of those slips
12:44 - of paper will identify
12:47 - uh individuals in our population we pull
12:50 - one of the slips of paper at random with
12:52 - equal likelihood from the hat
12:54 - and that gives us an individual in the
12:56 - population that will be
12:57 - included in our sample compare that
13:00 - instead
13:01 - to stratified random sampling which is a
13:03 - more complicated procedure
13:05 - and the procedure that the census bureau
13:07 - is actually going to use
13:08 - in these annual american community
13:11 - survey
13:12 - studies where in this case you actually
13:15 - divide up your population according to
13:17 - some known strata
13:19 - a strata is something that you
13:22 - automatically know
13:23 - for individuals in your population for
13:25 - example what state they reside in
13:28 - so you divide up your population into
13:30 - strata
13:31 - and those strata don't necessarily have
13:33 - equal size
13:35 - but you're going to pick a random sample
13:38 - from each strata a simple random sample
13:41 - as i described before
13:44 - and then make inferences
13:48 - this procedure the stratification
13:51 - procedure
13:52 - can increase uh the power or the
13:55 - ability of your statistical procedure
13:59 - without having to collect as much data
14:02 - so it's a nice procedure to have in hand
14:04 - it's the
14:05 - procedure that is being used in more
14:07 - complicated surveys
14:09 - but we're not going to discuss it here
14:11 - the procedures that we discuss here are
14:13 - not appropriate
14:14 - for stratified sampling uh so it's of
14:17 - course
14:18 - very easy to sample badly
14:21 - uh so one instance of bad
14:24 - sampling is convenience sampling where
14:27 - you're basically selecting individuals
14:29 - from the population based on how easy it
14:30 - is for you to get the data
14:32 - in the case of this class if you were
14:35 - interested in how and what the grades of
14:37 - this class were
14:39 - and you decide to ask some of your
14:41 - friends what their grades are
14:43 - that's a convenience sample it is not a
14:45 - random sample
14:47 - because in the end the sample is going
14:50 - to resemble
14:51 - you in some way it's going to be more a
14:55 - reflection of you
14:56 - than it is of the class
14:59 - so you may end up if your friends are
15:02 - like better students and that's because
15:05 - better students like to
15:07 - co-mingle then that could be a problem
15:09 - because you're not going to get an
15:10 - accurate view
15:11 - or let's say we're studying politics
15:14 - and you decide that you want to
15:17 - determine people's um
15:18 - political party affiliation so you stand
15:20 - outside of the marriott library on
15:22 - campus
15:23 - and you start asking students uh what
15:25 - party do you support
15:27 - in elections uh in that case you are not
15:30 - gonna get anyone from st
15:32 - george utah uh almost sure well i mean
15:35 - you might get a couple people but for
15:36 - the most part st
15:37 - george utah will be extremely
15:38 - underrepresented in your sample
15:42 - and that's going to be a problem now you
15:44 - don't have an accurate representation
15:46 - of the state of utah if that if the
15:49 - state of utah were in fact
15:50 - your population of interest
15:54 - and even if the university of utah
15:55 - itself for your population of interest
15:57 - not every student is going to be hanging
15:59 - out around the library so that'll be a
16:00 - problem
16:01 - uh or in the case of a voluntary
16:04 - sampling where
16:05 - like the classic example is a
16:09 - a tv host goes and tells his viewers to
16:14 - participate in a survey is like who do
16:15 - you like the democrats or the
16:16 - republicans
16:18 - and it's like oh my gosh all my viewers
16:20 - like the same party as i do and they
16:22 - agree with me
16:23 - no shocker so that would be a case of
16:26 - a very unreliable study um
16:31 - so yeah that's that's another thing but
16:33 - we're not really going to talk too much
16:34 - about this uh
16:36 - this chapter this section i mean there's
16:38 - a lot that can be said about how to
16:40 - appropriately sample and
16:44 - actually this is one area where
16:48 - current events i would like to say some
16:49 - more about it like the coronavirus
16:52 - uh what are some of the statistical
16:53 - issues surrounding the coronavirus
16:55 - because there's a lot of statistical
16:56 - issues involving the coronavirus and our
16:59 - understanding
17:00 - of it and its effect on the population
17:02 - and i'm thinking i'm going
17:04 - to leave that to a separate video but
17:07 - for now that is the end of this section
17:09 - and uh i'll see you in section two on
17:13 - pictorial tabular methods
17:14 - in descriptive statistics so all right
17:16 - best of luck to you
17:24 - hey students alright so next section
17:28 - is on uh making graphs and making tables
17:31 - for descriptive statistics all right so
17:35 - first let's describe the idea of a
17:37 - distribution
17:38 - when we're talking about random
17:39 - variables there's a couple things that
17:42 - we should keep track of
17:43 - there is what values a random variable
17:47 - could take and there's also how
17:49 - frequently those values are taken
17:51 - those ideas are are
17:54 - captured in the distribution of the data
17:58 - set
18:00 - so um in uh in in this section we're
18:04 - going to see
18:05 - some uh basic ways to understand a
18:07 - distribution
18:08 - of an observed data set and in later
18:11 - sections we're going to
18:12 - talk about the idea of the distribution
18:14 - of a population
18:16 - and how it could possibly uh extend the
18:18 - idea of distribution
18:20 - and one basic way that we could
18:22 - understand a distribution is using a
18:24 - table
18:24 - and we can also use visualizations
18:26 - visualizations can actually
18:28 - be very helpful in understanding a
18:29 - distribution because there are things
18:31 - that our eyes are very good
18:33 - at picking out and when we make pictures
18:36 - of our data set we pick those things
18:38 - out like for example where does the data
18:40 - tend to cluster how
18:42 - spread out is the data are there
18:44 - outliers in the data set
18:46 - influential observations and our eyes
18:48 - are actually very good
18:49 - at noticing those features so
18:53 - creating visualizations of data says can
18:56 - lead to us learning a lot that actually
18:59 - really cannot be learned
19:00 - by numbers on their own in fact there
19:03 - are some
19:04 - uh examples for example there's this
19:06 - thing known as um
19:07 - amscom's quartet maybe i should uh maybe
19:10 - i should very quickly
19:12 - uh have a look at anscombe's quartet
19:15 - because we can do that
19:16 - um very quick
19:21 - okay so and oh let's see
19:25 - wk and scum is it
19:28 - b uh quartet
19:34 - this is close enough it'll probably
19:35 - figure it out
19:42 - is it ants comb
19:53 - and i can't type oh okay so it has a b
19:57 - in it all right so there's this picture
19:59 - right here
20:00 - and this is known as enscom's quartet
20:02 - this is for a bivariate data set
20:04 - uh which is not what we're talking about
20:06 - right now but
20:08 - uh when working with bivariate data sets
20:10 - you want to track
20:11 - uh for example the mean of x
20:14 - and the mean of y and also
20:17 - if you were to come up with a best
20:18 - fitting line a regression line between
20:20 - the two variables
20:21 - uh what would the parameters of that
20:23 - line be and what is the correlation
20:25 - between the variables
20:26 - and numerically every single one of
20:29 - these data sets are exactly the same
20:32 - even though our eyes can see they are
20:36 - not the same at all these are clearly
20:38 - not the same data set
20:39 - and they have very different properties
20:41 - so making visualizations can help us
20:43 - learn a lot in fact this is
20:45 - a subject in and of itself you can take
20:47 - a class
20:48 - from the computer science department on
20:50 - just visualization
20:51 - um but we're not going to be going into
20:54 - that much depth and visualization today
20:56 - um what we learned today is
20:59 - pretty much enough to get you by in life
21:02 - unless you're doing some really serious
21:04 - work
21:05 - so from this point on in this video
21:08 - and future videos i use
21:11 - this notation to describe a data set and
21:16 - this data set has sample size
21:18 - n so the sample size is the number of
21:20 - observations in a data set
21:22 - um the indexing here we have x1 x2 xn
21:26 - the indexing the numbers themselves
21:28 - don't actually matter they're just a way
21:30 - for us to differentiate observations
21:32 - these observations may have the same
21:34 - values or different values
21:36 - uh it so there's it it actually doesn't
21:39 - really say all that much about the data
21:40 - set
21:41 - in particular i don't want you to look
21:43 - at this and think that this data set is
21:45 - now
21:46 - ordered this data set is not assumed to
21:48 - be ordered
21:49 - if the data set ever becomes ordered i
21:51 - will let you know
21:52 - but right now it isn't um
21:55 - i also would like to draw attention to
21:57 - the fact that this is in lower case
21:59 - and it's it's not a
22:02 - it's not a perfect rule i've often
22:05 - violated it
22:07 - uh but it is a hint that lowercase
22:10 - things and this will probably be the
22:11 - case in the in the in the
22:14 - videos for this class um it is generally
22:17 - the case that lower case numbers are
22:19 - constant
22:20 - uh and a data set is constant a data set
22:22 - is something that you observe
22:23 - and because you observed it is now
22:25 - constant whereas
22:26 - capitals often refer to random things
22:29 - which is not what we're going to talk
22:30 - about today
22:32 - so uh the first visualization method
22:34 - that we're going to talk about
22:35 - is very basic it's called a stem and
22:38 - leaf plot
22:39 - so we create a stem leaf plot using the
22:41 - following steps
22:43 - the first thing we're going to do given
22:44 - a data set is we're going to select the
22:46 - number of
22:47 - leading digits to be the stem values
22:51 - so we have a number maybe that number
22:54 - is 123.45
22:59 - and we're going to select some number of
23:02 - digits maybe the first two digits
23:04 - and these will be the stem values the
23:06 - remaining step the remaining
23:08 - values uh let's use a different color
23:10 - for that uh
23:12 - where did red go the remaining values
23:14 - will be
23:15 - the leaf values which in principle you
23:18 - could
23:19 - put like the entire block 345 as a
23:22 - leaf value although a lot of people will
23:25 - just like round
23:26 - to just one digit uh because well you'll
23:29 - see for a second why someone might be
23:31 - inclined to do that
23:32 - um so after you make that selection
23:36 - you're going to draw
23:37 - a vertical line let's not use that color
23:40 - uh we're going to draw a vertical line
23:43 - and we're going to list
23:44 - the stem or possible stem values uh
23:47 - on the left-hand side of this line so we
23:49 - might have 12
23:51 - 11 10 and maybe 13
23:54 - and 14 and on the on the right hand side
23:58 - of this line
23:59 - we would record the leaf values let's
24:01 - assume for a second just for simplicity
24:03 - that we didn't observe this part
24:05 - and all we saw was the three
24:09 - we might list one two three and this
24:12 - would
24:12 - be read off as a hundred and twenty
24:15 - three
24:16 - because according to part four we're
24:18 - somewhere in this display we're going to
24:19 - indicate the units of the stem
24:21 - so we might say in this case tens uh to
24:24 - indicate that the
24:25 - stem is uh tens place and beyond
24:29 - uh which means that everything to the
24:30 - right will be in the ones place
24:33 - or maybe we're doing a more advanced
24:34 - plot where maybe we actually did write
24:36 - three four
24:37 - five so this would be one two three
24:39 - point four
24:40 - five uh but we're not going to do that
24:42 - because
24:43 - uh i actually think that would be it
24:47 - kind of defeats the purpose of a stem
24:48 - and leaf plot
24:49 - because what we would instead do is list
24:52 - the leaf values for each observation our
24:54 - data set so maybe we saw 101
24:56 - 101 1 1 2 3
24:59 - four so we have an observation a hundred
25:02 - and fourteen and another observation
25:04 - that's a hundred and thirteen and an
25:05 - observer uh
25:06 - another observation that's 112. uh or
25:09 - we'll also even throw in a nine so
25:10 - there's an observation that's 119.
25:13 - we've got 123 130 144
25:17 - um and that and now we have
25:21 - a display and let's just throw in like
25:24 - one more number uh like right here
25:28 - uh now we have a display that
25:31 - that um actually later on we're going to
25:34 - see a histogram and say
25:36 - oh this actually looks a lot like a
25:37 - histogram but we do have this
25:40 - display that's indicating that that's
25:43 - telling us a lot about this data set
25:44 - for one thing it actually tells us the
25:46 - actual values in the data set that's one
25:48 - advantage
25:49 - of a stem-and-leaf plot when you have a
25:51 - stem-and-leaf plot you get to see the
25:52 - actual data values
25:54 - and that's one thing that's nice another
25:56 - thing that's nice about them is that
25:57 - they're very quick to create
25:59 - like i imagine the stem and leaf plot
26:01 - being created in the field
26:03 - like you are out there you are watching
26:06 - uh i don't know birds and tracking how
26:08 - many birds you're seeing or
26:10 - characteristics of birds and you
26:12 - actually get to write down
26:14 - uh as as you are recording your data
26:16 - you're also visualizing it so you can
26:18 - learn
26:19 - uh things about it because you can see
26:20 - like for example this data set uh
26:24 - seems to cluster in this 110s area
26:28 - uh we might be able to guess that the
26:30 - mean is somewhere around here that this
26:33 - is like where the center of the data set
26:34 - is
26:35 - and we get some sense of its spread all
26:37 - right so um
26:39 - here is another data set for us to
26:41 - practice with
26:42 - uh this is a subset of mcdonald's
26:46 - data on hint on heightened finger
26:48 - lengths and
26:50 - this right here is actu this part right
26:52 - here is actually our code
26:53 - so we could go into r and type this part
26:56 - in
26:57 - and uh use r to do some of this analysis
27:02 - so um and i'll actually show you a
27:05 - little later
27:06 - how this can get turned into like what r
27:09 - would do with this part
27:10 - uh but um we're just gonna take this and
27:13 - treat it as our data set
27:15 - and construct a standard leaf plot by
27:17 - hand
27:18 - so what are we going to do uh we first
27:21 - need to decide
27:23 - um where we're going to make the cutoff
27:26 - we're
27:26 - not going to make it in the ones place
27:29 - because if we put it in the ones place
27:30 - then there's going to be just
27:32 - five for our stem value and what you'll
27:36 - end up with
27:37 - is a plot uh that looks like this
27:40 - and it will be just like a giant
27:42 - skyscraper and you won't
27:43 - you won't learn anything so we don't
27:46 - want that
27:47 - so the next place that is available to
27:49 - us is the what is the tenths
27:51 - place and the tens place seems pretty
27:53 - good so
27:55 - i'm now going to make the line for the
27:57 - stem
27:58 - and i'm going to write let's see we've
28:00 - got five
28:01 - three five five we have a five two
28:04 - oh we also have a five oh so we'll put
28:07 - five
28:07 - zero five one
28:11 - five two five
28:16 - three five four
28:19 - uh and we go all the way up to five nine
28:22 - so five
28:23 - five five six
28:26 - five seven five
28:30 - eight five nine
28:35 - okay there's there's some dead spots on
28:38 - this
28:38 - super super cheap computer i bought it
28:40 - for 200
28:42 - um at target across the street the
28:46 - which was the most convenient place to
28:47 - buy a laptop super cheap so
28:51 - you can't what you pay for and i'm
28:52 - especially learning that right now uh
28:54 - anyway um
28:57 - so uh we now have the stem values and
29:00 - now we can start recording our data set
29:01 - if you were to do this in a computer a
29:03 - computer would actually
29:04 - order the uh leaf values here they
29:08 - actually are ordered because nine comes
29:10 - after four which comes after three
29:12 - uh so a computer would actually order it
29:14 - i don't really see the point because the
29:16 - point
29:17 - is to understand the distribution as
29:21 - in in a visual way and ordering believes
29:24 - does not aid in that so we can go ahead
29:27 - and just start
29:28 - reading across in this data set to
29:32 - make our stem and leaf plot so we've got
29:35 - 5.55 so we'll put a 5 right here
29:37 - [Music]
29:38 - point three zero so we'll put zero there
29:42 - five six three five three
29:46 - zero five one three
29:50 - five zero five five
29:53 - three eight five nine
29:56 - six five two one
30:00 - and five three eight so in this case it
30:04 - actually turned out to be ordered and
30:05 - that's pure coincidence i wasn't
30:06 - planning on that
30:08 - but now we have a stem-and-leaf plot and
30:11 - if we scroll down
30:12 - and or if you're actually if you
30:14 - actually have these notes printed out oh
30:16 - one last thing we need to do is indicate
30:19 - what this line means
30:21 - and this line is the tenths place
30:31 - okay all right so if we were to continue
30:34 - on to the next page
30:36 - we would actually see some r co that
30:37 - would do this so this is the high data
30:39 - set from before
30:40 - this part right here the r function
30:44 - oh okay so this is the act
30:47 - the same height data set we had before
30:49 - the the r function stem creates stem and
30:52 - leaf plots
30:53 - and this is basically the same as we had
30:54 - before okay before i continue on i'm
30:57 - just going to double check to make sure
30:58 - i'm still streaming
31:00 - yeah we're still streaming okay all
31:02 - right um
31:05 - okay so next up for our visualizations
31:09 - we have a dot plot a dot plot
31:12 - represents each data point as a dot
31:15 - along a real number line
31:17 - uh so we can we would then be looking
31:20 - for clusters in dots
31:21 - to note patterns in our data set
31:25 - so in this example we're going to use
31:27 - the
31:28 - data from the previous example which has
31:30 - already been very nicely
31:32 - organized for us in a stem and leaf plot
31:35 - to create a dot plot let's see is there
31:38 - a nice
31:38 - easy way to zoom out oh very nice
31:43 - oh that's not what i wanted that's
31:44 - delete i need i'm new to this program
31:47 - uh and we're at the very bottom too
31:51 - that's not what i want either okay so
31:54 - all right so this this program is new to
31:57 - me my apologies
31:58 - um all right so all i want to do is zoom
32:03 - out a little bit okay that's good
32:06 - that's good um
32:09 - oh cool all right i just want to be able
32:13 - to see
32:14 - this uh nice stem and leaf plot that we
32:16 - created before
32:18 - when creating our dot plot
32:21 - so for our dot plot
32:25 - uh let's see we've got let's see
32:29 - so for the left point on the range we
32:32 - should pick
32:33 - 5.0 and
32:35 - uh for the right point of the range we
32:39 - should probably pick
32:39 - 6.0 because we get really close to 6.
32:43 - so in between we've got 5.5
32:47 - and then we can just mark off
32:50 - in in a fifths so
32:56 - two three four five okay that's that's
32:58 - that's close enough
33:00 - uh if you want perfection ask a computer
33:02 - to do it
33:03 - all right so um so we've got an
33:06 - observation 55.05 so that's about right
33:11 - there
33:13 - 5.13 is about there
33:18 - 5.21
33:20 - is about there and then we've got 5.30
33:24 - [Music]
33:25 - um that is about there and that's also
33:28 - got
33:29 - another one right with it uh there's
33:31 - another
33:32 - observation uh oh that's actually could
33:35 - be a little bit to the left but we could
33:37 - also call that 5.38
33:39 - so here's 5.30 for realsies this time
33:43 - we've got 5.55
33:47 - 5.63
33:49 - and then 5.96 so then we end up with the
33:53 - stop plot
33:54 - that's also giving us a sense of where
33:55 - the data tends to lie
33:57 - but this time along a real number line
33:59 - so
34:00 - and and i i'm sure you can guess so far
34:02 - that the methods that we have been
34:03 - working with
34:04 - uh our visualization methods are pretty
34:06 - much devoted exclusively
34:08 - to uh quantitative data we'll see
34:10 - categorical in a second
34:12 - i'm pretty much going to say draw bar
34:14 - plot and that's it so
34:16 - here is our code for uh what i just did
34:19 - the r function for making dot plots is
34:22 - a strip chart
34:26 - so strip chart is what makes these
34:28 - things and i'm just passing some
34:30 - additional parameters to strip chart
34:32 - to uh make the function to make the plot
34:35 - how i like like for example this
34:37 - parameter right here controls
34:39 - how it's like what exactly we're drawing
34:40 - here are we drawing filled in circles or
34:42 - empty circles and so on
34:44 - so this you can think of this as point
34:46 - character because you're choosing
34:48 - the character of the point and 19
34:50 - corresponds to a filled in circle
34:52 - uh we have a little bit of an offset and
34:56 - uh some other parameters that just help
34:58 - make the
34:59 - visualization look nice i would
35:01 - recommend go ahead going ahead firing up
35:04 - r playing around with these parameters
35:06 - seeing what happens
35:07 - uh stack for example uh means that when
35:10 - two points are about to collide with one
35:12 - another
35:12 - stack them on top of each other because
35:15 - it's not the only way to do things there
35:16 - are often very many ways to solve the
35:18 - same problem
35:20 - so all right uh continuing on
35:25 - so a quantitative variable we we need a
35:28 - further
35:29 - a refinement of quantitative variables
35:32 - and and we say that a quantitative
35:34 - variable is discrete
35:35 - if it's possible values are countable uh
35:38 - countable that could be for example one
35:41 - two three four five six
35:43 - or that could be one two three four five
35:45 - six seven eight nine blah blah blah up
35:46 - into the future
35:48 - uh it could be the integers so including
35:50 - negative numbers as well
35:52 - or including zero as well um
35:55 - and then so we would call any of those
35:57 - things countable
35:58 - there's also continuous variables
36:02 - where the possible values could come
36:04 - from a continuum
36:05 - so that includes 0 1 and anything in
36:08 - between so
36:10 - 0 one zero point five
36:13 - uh zero point zero one one zero one two
36:16 - five uh one divided by pi
36:19 - any one of those things are possible uh
36:22 - there's actually a nice little rule of
36:24 - thumb
36:24 - which is that uh discrete random
36:27 - variable
36:28 - discrete variables emerge when you count
36:31 - things like for example i count sheep so
36:34 - what am i going to get i'm going to get
36:35 - an integer in the end so if that's going
36:38 - to be the case
36:39 - then it's comfortable whereas continuous
36:41 - emerges from measurements
36:43 - so like with a ruler so you measure how
36:45 - long something is with a ruler and that
36:47 - will probably produce
36:48 - a continuous variable
36:51 - so continuing on uh some further notions
36:55 - when describing a distribution
36:57 - there is the frequency of a of a value
37:01 - and there's also the possible values it
37:03 - could take so frequency
37:05 - is how frequently we see
37:08 - a variable take a particular value and
37:11 - for discrete variables it is in fact
37:13 - possible and quite likely that
37:18 - we will see the same numbers repeatedly
37:21 - whereas if we're dealing with continuous
37:23 - you cannot necessarily assume that so
37:25 - the frequency for each value for a
37:28 - continuous variable would be
37:30 - rather uninteresting since you're
37:32 - probably going to see
37:33 - like going back to the data set we were
37:35 - just looking at
37:36 - admittedly uh for this uh height
37:39 - data set we did have some collisions
37:43 - but you can't really count on that you
37:44 - can imagine that
37:46 - if you were to increase the decimal
37:47 - position precision of the numbers that
37:49 - you're looking at
37:50 - you would um
37:54 - you would actually not have a collision
37:56 - you'd have two distinct numbers the only
37:58 - reason why
37:59 - we get 5.30 twice is because that's
38:02 - where we decided to round so
38:06 - in the case of continuous variables well
38:08 - with frequency
38:09 - well with discrete variables it's okay
38:11 - to just kind of consider the values in
38:13 - and of themselves and just say
38:14 - how many times did one show up how many
38:16 - times did two show up and so on
38:18 - for continuous variables we probably
38:21 - should do a procedure known as binning
38:24 - so binning is where we define a range
38:28 - in which a data point could possibly be
38:30 - and instead of tracking how frequently
38:32 - a certain value is taken we're going to
38:34 - track how frequently
38:36 - um a variable will fall into a bin
38:39 - right so you can imagine for instance
38:43 - that this is a this this is a continuous
38:46 - line between zero and one
38:48 - and we saw uh numbers
38:51 - uh in this region and they're just
38:53 - random numbers
38:55 - and we're not going to actually list out
38:57 - in a table
38:58 - how often we saw those individual
39:00 - numbers because most of the time the
39:01 - frequency for that number will be one
39:04 - instead what we'll do is we'll divide up
39:06 - this region and then count
39:08 - how many times uh numbers fell into
39:11 - those individual bins like for example
39:12 - we get we got twice here
39:14 - three times a year uh one two three four
39:17 - five
39:17 - six times here one two three
39:20 - four five six times here and then four
39:22 - times here and then we'd have like a
39:25 - more useful table
39:26 - um for understanding our distribution
39:29 - very closely related to the frequency is
39:31 - the relative frequency relative
39:32 - frequency
39:33 - is where you take the frequency and
39:35 - divide it by the sample size
39:36 - that's it so if you really want to see
39:40 - a formula for the relative frequency
39:44 - the relative frequency
39:50 - the relative frequency is equal to
39:54 - frequency
39:57 - divided by n
40:00 - a frequency distribution is a tabulation
40:03 - of the frequencies or the relative
40:05 - frequencies or
40:06 - in the case of continuous variables we'd
40:08 - probably also include the frequency of
40:10 - bins um something that i haven't really
40:12 - discussed so far
40:14 - is that actually how should we choose
40:16 - the bins
40:18 - like like like bidding is a choice
40:21 - i chose these bins but alternatively
40:25 - i could have chosen bins like this like
40:27 - here's one bin and here's another bin
40:29 - why didn't i do that so
40:32 - the actual bidding decision is it is in
40:35 - fact a decision
40:37 - you need to decide how many bins they
40:40 - are and what regions they're going to
40:41 - cover
40:42 - so in terms of the regions i think it's
40:45 - pretty safe to say
40:46 - the bin should be equal length if the
40:48 - bins are not equal length then what
40:50 - you're going to end up with
40:51 - is a difficult to understand chart or a
40:54 - difficult to understand table
40:55 - because people then have to pay much
40:57 - more attention to the bin length
41:00 - um and also when you're trying to
41:02 - visualize
41:03 - what's like this is leading up to
41:04 - histograms and we're going
41:06 - and the binge bin size is uh
41:10 - something you have to make when talking
41:11 - about histograms and when you
41:14 - um uh when you end up with bins of
41:17 - unequal length then the histogram
41:20 - like you there is ac there is a precise
41:22 - way to understand
41:23 - what's being visualized in the histogram
41:25 - but unfortunately
41:26 - people's eyes are not going to perceive
41:27 - it that way um
41:29 - and it's that you are making the
41:31 - histogram more difficult to understand
41:34 - so so just always make your bins equal
41:38 - length
41:39 - but the number of bins that you have and
41:41 - also what exact regions they cover
41:43 - because you can still for example take
41:46 - all of these bins shown here
41:50 - take all these bins and then shift them
41:52 - slightly to the left
41:55 - and then you have then you have a
41:56 - different visualization
41:59 - or you then have a very different
42:01 - description of your
42:02 - of your data set how exactly should you
42:06 - decide where the boundaries of your bins
42:08 - are
42:09 - and how many bins should you have these
42:12 - are all very important decisions that
42:14 - can have uh drastic implications for
42:17 - visualizations
42:18 - but i i really should move on to
42:21 - actually creating a frequency
42:23 - distribution which is a tabulation of
42:24 - frequencies or relative frequencies
42:26 - i should really move on before i start
42:29 - digging into that
42:30 - topic so let's make
42:34 - a frequency distribution for this
42:37 - soccer data set where we have a supposed
42:40 - statistically minded parent
42:41 - maybe that parent is you if you have a
42:43 - daughter or a son and they play soccer
42:46 - and you've and you've decided that
42:48 - you're going to
42:50 - uh track their little league soccer team
42:53 - scores during a regular season
42:55 - so here i have a data set i've and by
42:58 - the way what i've done here
43:00 - uh in terms of our code is creating an r
43:02 - vector
43:03 - so i have this list of values
43:07 - um in what's known as an r vector i'm
43:09 - not really going to talk about
43:10 - r in these videos i'm going to
43:14 - pretend and assume that
43:17 - you are watching my video series on how
43:20 - to use
43:20 - r uh but enough of that for now um
43:25 - i want to construct a frequency
43:26 - distribution for this data set so what
43:28 - are possible values
43:29 - in this data set uh well uh
43:33 - i should let's see uh we've got
43:36 - we've got a so on the left hand side of
43:40 - this line for this table
43:41 - so we're going to have um a value
43:46 - and we're going to have on the right
43:48 - hand side the frequency
43:52 - so what are possible values well we've
43:54 - got one and we've got nine and we've got
43:56 - numbers in between so we'll just go uh
43:59 - one two
44:00 - three four five
44:03 - six seven eight nine
44:07 - all right and then we need frequency so
44:09 - let's see how many times did 9 appear
44:12 - looks like 9 just appeared once all
44:14 - right so we got that covered
44:16 - uh six appeared uh
44:19 - so one two yeah that's it so six
44:23 - appeared twice
44:24 - uh five appeared three times
44:30 - and uh eight appeared twice
44:36 - uh and each each of the remaining
44:38 - numbers appears only once
44:40 - so one appeared once two appeared once
44:43 - uh three appeared once and four appeared
44:46 - once
44:47 - okay uh seven never appeared
44:52 - all right
44:56 - one second i catch up in my notes
44:59 - um so the sample size for this data set
45:02 - you can count
45:03 - there are 1 2 3 4 5 6 7
45:07 - 8 9 10 11 12 observations or
45:10 - alternatively what you could have done
45:12 - is um and in fact i'll go ahead and just
45:14 - kind of
45:15 - scroll down a little bit and down here
45:17 - i'm going to track
45:19 - the sum and yeah it adds up to 12.
45:22 - another thing i want to do is in this
45:25 - table
45:26 - track the relative frequency
45:31 - so we'll call this relative frequency
45:38 - and in the relative frequency i'm going
45:41 - to take the frequency and then divide by
45:42 - the sample size
45:44 - so 1 divided by 12. uh that
45:47 - is going to be a 0.05
45:55 - or is that an 8 no that's an 8. 0.08
45:59 - three
46:00 - and the three is a continuing uh digit
46:04 - all right so for the next one it's the
46:05 - same thing zero
46:07 - eight three uh and
46:10 - again for three zero point zero eight
46:14 - three okay and then four
46:17 - five uh no wait so let's see uh
46:20 - four also appeared that amount so zero
46:23 - point
46:24 - zero eight three uh
46:27 - four appeared three times so that's
46:30 - going to be
46:31 - zero point uh
46:34 - two five because three divided by
46:38 - uh 12 will be 0.25 3 is a quarter of 12.
46:43 - 6 will be 0.16 where the 6 is a
46:49 - continuing digit
46:51 - 0 0.16
46:55 - and 0.083
47:00 - okay and if you add these numbers up
47:02 - this is a way for you
47:04 - to check your sanity this should add up
47:08 - to one
47:11 - okay
47:16 - so moving on if you were interested in
47:19 - how to make
47:20 - a table like this in r the function
47:24 - for r in r for making such a table is
47:27 - well table so you would have given it
47:29 - the soccer data set and ask it to make a
47:31 - table
47:32 - and it will tabulate uh which
47:34 - observations
47:35 - were taken and how frequently they were
47:36 - taken notice that it did skip
47:38 - seven um which i don't know is that a
47:41 - feature you want
47:42 - i don't know uh i actually personally
47:45 - would
47:46 - rather not skip seven and you'll
47:47 - probably see why in a second
47:50 - um when working with continuous data as
47:52 - i was mentioning before there's this
47:54 - issue of
47:54 - binning uh deciding on the number of
47:57 - bins uh deciding um
47:59 - uh where the boundaries of the bins are
48:02 - and so forth
48:03 - but um so
48:08 - but uh once you've decided on the number
48:12 - of bins and there are some rules of
48:13 - thumb
48:14 - that you can use uh for how many bins
48:18 - that you should use
48:19 - like for example if the if the sample
48:21 - size is n you could perhaps choose
48:23 - the number of bins to be the square root
48:25 - of n actually there is some
48:26 - statistical arguments that the correct
48:29 - number of bins should be on the order of
48:31 - n to the power one fifth or the fifth
48:33 - root of n
48:34 - um there are some arguments for that but
48:38 - uh square root of n is also fine for now
48:41 - um
48:42 - at the end of the day whatever it is
48:44 - that r is going to use
48:47 - for its uh binning decisions is better
48:49 - than what you're going to do
48:51 - and it's going to have some uh nice
48:54 - theory backing it up
48:55 - so you probably should just trust r
48:57 - whatever r is doing
48:59 - is probably better than what than than
49:01 - the square root of n roll
49:02 - um so after you've decided on the number
49:06 - of bins
49:07 - you segment your number line so that you
49:10 - have that many equal length bins
49:13 - uh and uh depending on where in each
49:16 - data point falls assign it to a bin
49:18 - if it falls on a border between the bins
49:20 - assign it to the bin
49:21 - on the right so in other words bins are
49:25 - right inclusive that is a parameter some
49:27 - people prefer left inclusive bins does
49:30 - it really matter no it doesn't
49:31 - it doesn't really matter nobody really
49:33 - cares uh just just pick one
49:35 - right and be consistent either put it on
49:37 - the left band or the right bin just
49:38 - don't
49:39 - put it in both please and then construct
49:42 - a frequency distribution for the bims
49:44 - uh very quickly before i keep going okay
49:47 - we're still good
49:48 - all right um i i i'm very nervous about
49:52 - losing this video
49:53 - i'll just put it this way today i've
49:55 - recorded a number of videos that
49:56 - suddenly just went up into thin air
49:58 - because things went bad and i don't want
50:00 - that to happen again
50:02 - all right um so uh example four so using
50:05 - the data from example one
50:07 - uh you can scroll you can go back in
50:09 - time to see what that is
50:11 - this is the heights data set uh
50:13 - construct a frequency distribution
50:16 - and for what it's worth there were 10
50:18 - observations in that data set
50:21 - the function length when given a vector
50:24 - will tell you the number of elements in
50:26 - that vector
50:26 - in r so since the number of elements is
50:30 - the number of data points we want to use
50:31 - length to determine the sample size
50:35 - okay continuing on i have made some
50:39 - decisions
50:40 - i'm going to decide that the number of
50:43 - bins i'm going to use
50:44 - is going to be uh about the square root
50:48 - of 10. the square root of 10
50:52 - is between three and four
50:55 - so uh uh so if you wanted to use the
50:59 - square root of ten rule
51:00 - uh that would be um about uh
51:05 - we'll say for rounding up
51:08 - although apparently in my notes i
51:10 - decided that i was going to go with
51:11 - five and i don't want to change it so
51:15 - i'm gonna go with five why five why not
51:18 - five
51:19 - um it's it's it's
51:22 - it's whatever you want i i think the
51:24 - reason why i went with five
51:26 - uh i don't know i just did
51:30 - so um let's go ahead and
51:33 - zoom oops is that what i want
51:38 - yeah that's fine okay so
51:41 - uh i'm going to make a table so i
51:45 - on uh in this table i'm going to list
51:48 - the bins
51:49 - so i've got 5 to
51:52 - 5.2 and to indicate this is going to be
51:56 - right inclusive i'm going to put a
51:58 - little uh
52:00 - less than sign to say 5 2 less than 5.2
52:04 - not including 5.2 and then 5.2
52:08 - to uh 5.4
52:12 - uh 5.4 hold on
52:17 - okay five point
52:21 - four two five
52:25 - point six five
52:28 - yeah i need to kind of get away from
52:31 - there
52:32 - okay uh 5.6
52:36 - to 5.8
52:39 - and then 5.82
52:43 - uh less than six and we can just leave
52:46 - it at that
52:47 - all right and then we have the frequency
52:49 - for each of these bins
52:53 - so the frequency for these is going to
52:55 - be well there were two numbers between
52:57 - five and five point two
52:59 - uh five numbers between 5.2 and 5.4
53:02 - and then one number in each of the
53:04 - remaining bins
53:06 - so uh if we were to do the relative
53:12 - frequency
53:15 - we're going to take each of those
53:16 - frequencies and divide it by 10. in
53:17 - which case it's you're just going to
53:18 - move the decimal point over one place
53:20 - so 0.2 0.5 0.1.1.1
53:26 - okay all right once we have a frequency
53:30 - distribution
53:31 - such as this we can now construct what's
53:33 - known as a histogram
53:34 - which is a plot for visualizing the
53:36 - distribution of quantitative data
53:38 - so how do we construct a histogram first
53:41 - draw a number line
53:43 - and mark the location of the bins so for
53:46 - example we could do something like this
53:48 - and we're going to say that the bins are
53:49 - going to be about
53:51 - here um and then for just
53:54 - if you want to for discrete data you can
53:56 - center your bins on the corresponding
53:58 - values themselves
53:59 - um because you're not really doing any
54:02 - binning with the
54:03 - the discrete variables or you can
54:04 - imagine that your bins are exactly what
54:06 - they need to be
54:07 - uh to be touching each other and uh
54:10 - centered on the integer
54:12 - uh the corresponding integer and then
54:15 - for
54:15 - each of these classes or bins draw a bar
54:19 - extending from the number line so we
54:21 - have
54:22 - uh some y-axis that's tracking let's say
54:26 - the frequency
54:32 - so we have a y-axis that's checking the
54:33 - frequency we're going to draw
54:35 - a bar from the number line to the
54:39 - y value that corresponds to the
54:40 - frequency of that bin
54:43 - or the relative frequency if that is in
54:45 - fact what you're plotting on the y axis
54:47 - so it would end up with a plot maybe
54:49 - looking like this
54:52 - and that resulting plot is a histogram
54:59 - okay so uh going back to some of our
55:02 - examples
55:03 - let's draw a histogram for the data set
55:06 - in example three which was that soccer
55:08 - data set
55:13 - so for the soccer data set we had
55:15 - numbers between
55:17 - one through nine we'll go ahead and
55:18 - include zero as well
55:21 - for this data set so
55:24 - i'm just going to i'm going to start out
55:27 - by drawing the graph the
55:30 - x-axis corresponds to goals
55:34 - so goals in a game
55:38 - and the y-axis corresponds to the
55:40 - frequency
55:46 - so let's see the the frequencies never
55:48 - seem to go beyond three so we've got
55:51 - uh one two three
55:55 - okay and then possible integer values
55:58 - i'm going to go
55:59 - i'm going to go ahead and include zero
56:01 - because in principle this soccer team
56:03 - could go could score zero points so
56:06 - we'll go ahead and include zero
56:08 - but we'll also include at the very end
56:10 - we'll have
56:11 - nine and we've got one two
56:14 - three four five
56:17 - six seven eight nine okay
56:21 - so uh they there was one game where they
56:24 - scored one point
56:26 - uh one game where they scored two points
56:29 - one game where they scored three points
56:31 - uh and one game where they scored four
56:33 - points
56:34 - then they had three games where the team
56:37 - scored five points uh
56:40 - two games where they scored six points
56:44 - uh no games where they scored seven
56:46 - points
56:47 - uh two games where they scored eight
56:50 - points and
56:51 - one game where they scored nine points
56:56 - okay hmm new feature let's go ahead and
56:59 - see what happens
57:01 - uh when i no i don't think that works
57:04 - oh well um so there there be the
57:07 - histogram
57:08 - it's it's not a perfect picture but it's
57:10 - mine
57:11 - so if we wanted to down here is the r
57:15 - code
57:16 - for constructing a histogram so the r
57:19 - function is hist and
57:22 - uh i've given it some parameters to
57:25 - manually specify the breaks
57:27 - because i because otherwise
57:30 - um it would choose its own algorithm it
57:33 - would use its own procedures to come up
57:35 - with the breaks
57:36 - and i wanted to override that and this
57:38 - is one way to do it
57:39 - where i basically gave uh the
57:43 - um uh the function
57:46 - the uh break points so we have
57:50 - so the minimum of scott of soccer is
57:52 - going to be uh
57:54 - 1 so 1 minus 0.5 will be
57:58 - um why can't i do math it's 0.5
58:03 - um and then you have and then i went one
58:06 - above the maximum score
58:08 - so that would be this number right here
58:09 - so this right here corresponds to 0.5
58:12 - and this will be a 9.5 uh over
58:15 - at the right hand side and that's just
58:17 - telling it where i want those breaks
58:19 - and then it will infer that you have
58:23 - that everything in between is a bin uh
58:25 - but then you end up with a
58:26 - with a pot that is essentially what we
58:28 - came up with by hand
58:31 - okay uh next example for
58:34 - uh the data set in example one let's
58:37 - create a histogram
58:39 - okay so for that one i am going to use
58:42 - the relative frequencies instead
58:44 - which do you use at this point it
58:47 - doesn't really matter
58:48 - because the shape is going to be the
58:50 - same regardless of whether we divide by
58:52 - n or not
58:53 - and the interesting part when you're
58:55 - creating a histogram
58:56 - is looking at the shape of the resulting
58:58 - histogram so
58:59 - i actually really don't care although
59:02 - for what it's worth if you want to use
59:04 - this thing for more probabilistic
59:06 - inference
59:06 - you probably should pay more attention
59:08 - to whether you've got the frequency or
59:09 - the relative frequency
59:10 - um and if also the bins were not
59:15 - of um
59:18 - uh equal length then you would have to
59:22 - pay much more attention
59:23 - to the frequency versus relative
59:25 - frequency but also just take my advice
59:28 - and make your bins all equal length
59:30 - so uh we've got the relative frequency
59:34 - that is what we're going to draw this
59:35 - time
59:37 - and uh we've got the highest it will
59:40 - ever go
59:42 - is uh 0.5 in fact i don't think it ever
59:44 - reaches there
59:45 - so we've got right yeah it does it does
59:48 - reach
59:49 - 0.5 so and we'll just increment by
59:52 - 0.1 so 0.1.2 0.3.4.5
59:56 - so this is 0.1
60:00 - uh right here so possible bin values i
60:03 - said
60:04 - i decided that we were going to have
60:06 - five bins for some crazy reason i don't
60:08 - remember what it was
60:09 - uh and so let's see is that five
60:12 - it is now so we've got numbers between
60:15 - five
60:16 - and six and using the table that we came
60:18 - up with uh
60:19 - that time ago we're going to end up with
60:21 - a histogram
60:22 - uh looking something uh like like uh
60:26 - this so it goes up to here
60:30 - and then oh yeah so it goes to 0.2 and
60:33 - then
60:34 - 0.1 for these remaining bins
60:38 - and that's our histogram
60:44 - so if we were to continue along and look
60:46 - on the next page
60:47 - uh oh i think that's the reason why i
60:49 - chose 0.5 it's because
60:50 - it it's exactly what r thinks it should
60:53 - be and you should probably
60:54 - given the choice between what you think
60:56 - the number of fins should be
60:57 - and what are things it should be you
60:58 - should probably go with what r is good
61:00 - what r
61:00 - is doing um don't fully trust it
61:04 - but at this point you probably don't
61:06 - have the experience to
61:07 - have your own opinion so um
61:11 - yeah it came up with basically the same
61:14 - picture
61:15 - okay now we've been coming up with these
61:18 - nice plots and everything is great
61:20 - one second let's just uh satisfy my
61:23 - nervousness
61:24 - okay we're still good okay um
61:28 - i mean making these pictures is nice but
61:30 - why are we making these pictures well
61:32 - there are certain things that we are
61:34 - looking for when we're making
61:35 - visualizations like this
61:36 - for example is the data unimoda we're
61:39 - like
61:39 - one thing we're looking for is modality
61:41 - which is where does the
61:43 - data tend to cluster is the data
61:45 - unimodal
61:46 - where it only has one peak this would
61:48 - suggest it's clustering around
61:49 - one area or on the other hand is a
61:52 - bimodal or
61:53 - multimodal where you have multiple peaks
61:56 - so the unimodal case would be something
61:59 - that resembles this
62:01 - uh where so we'll call this unimodal
62:05 - bimodal would be a situation like this
62:08 - uh maybe think of it as a camel hump and
62:11 - multimodal
62:12 - uh is uh like all like you've got
62:16 - all sorts of different modes all sorts
62:18 - of different peaks
62:21 - so what would it mean if you had a unimo
62:24 - versus bimodal versus multimodal
62:27 - for starters the multimodal case the
62:30 - first thing
62:31 - that you should ask with multimodal is
62:34 - did i choose a bin that's too small
62:37 - because you can end up with situations
62:39 - if you choose your bin size
62:41 - to be too large when you're making a
62:44 - histogram
62:45 - uh if if at one extreme you can have a
62:48 - skyscraper where everything is in one
62:50 - bin
62:51 - and that is a chart that doesn't really
62:52 - tell you anything on the other hand you
62:54 - could have that
62:55 - basically a pancake where every single
62:57 - observation
62:58 - gets its own bin and that really doesn't
63:01 - tell you anything either that's
63:02 - basically a dot chart
63:03 - and you've kind of lost the point of the
63:05 - histogram
63:06 - so the first thing in the case of blue
63:09 - of this uh blue
63:10 - uh sort of histogram it's not really
63:13 - histogram because it's a smooth curve
63:15 - but
63:15 - whatever um in this in this case you
63:19 - should probably ask yourself whether
63:20 - you've
63:21 - chosen too few or too few bins
63:24 - uh it could happen that you have true
63:26 - genuinely multi-modal data
63:29 - but the number of modes should be
63:33 - should not be too many because modality
63:37 - and having more than one mode is
63:39 - indicative of there being more than one
63:42 - actual population in your data set so
63:44 - for example if
63:46 - i say that this is tracking height
63:49 - of people i could genuinely have
63:54 - a bimodal data set because actually
63:56 - there isn't
63:57 - one population in this data set of
63:59 - people there's actually two populations
64:02 - men and women because men and women will
64:03 - cluster around different
64:05 - average heights so that's that's that's
64:08 - that's features that you're looking for
64:10 - unimodal indicates that your data set it
64:12 - probably
64:12 - consists of one population um
64:16 - and another thing that we're looking for
64:19 - when looking at stuff like histograms
64:21 - and by the way
64:22 - a lot of this discussion also applies to
64:24 - the strip the
64:26 - stem leaf plot and the dot plot
64:29 - in particular the seven leaf plot
64:30 - because you can argue that the standard
64:32 - leaf plot is
64:33 - actually a histogram it's just a
64:35 - histogram with partic with a particular
64:37 - bin choice
64:38 - but there's nothing really different
64:40 - about it um
64:42 - but anyway is the data positively skewed
64:45 - or negatively skewed or symmetric
64:48 - so a positively skewed data set
64:51 - the way i like to think of it is well
64:53 - let's first draw it out
64:54 - we have the positively skewed data set
64:57 - and then we have
64:58 - the negatively skewed data set
65:02 - which looks more like this and then we
65:04 - have the symmetric data set
65:10 - let's do a little bit better than that
65:13 - okay that's a little bit more symmetric
65:16 - okay so if you're
65:17 - if you're at all bothered by the terms
65:19 - positively skewed negatively skewed
65:21 - symmetric and you're wondering
65:22 - how can i tell the difference between
65:24 - positively skewed and negatively skewed
65:26 - here's a little rule for you draw a
65:28 - dinosaur
65:33 - draw a dinosaur and then
65:36 - ask where is his tail pointing in the
65:39 - case of the black dinosaur
65:40 - it's pointing towards the positive end
65:42 - so it's positively skewed
65:44 - all right now let's talk about the green
65:46 - dinosaur well let's uh draw
65:48 - the green dinosaur give him some legs
65:51 - give him some stuff on his back because
65:53 - he's like a stegosaurus so he's got
65:55 - these plates on his back
65:56 - where is his tail pointing oh it's
65:58 - pointing in the negative direction
66:00 - so it's negatively skewed right and you
66:03 - don't draw
66:04 - you don't draw a dinosaur for symmetric
66:05 - because
66:07 - i know we're not we're just not going to
66:08 - do that we're already silly enough
66:10 - but um why does it matter whether a data
66:14 - set is positively skewed or negatively
66:15 - skewed
66:16 - um it matters when we're thinking about
66:18 - the relationship
66:19 - between uh important statistics such as
66:21 - the mean and the median
66:23 - so the mean and the he the median are
66:25 - going to behave a certain
66:26 - way and have certain relationships
66:28 - depending on whether the data is
66:29 - positively skewed or negatively skewed
66:31 - uh positively skewed data sets
66:33 - what that basically means is that
66:35 - outliers there are outliers in this data
66:37 - set
66:37 - and when observations tend to be large
66:39 - they tend to be very large
66:41 - um negatively skewed data sets are data
66:44 - sets where when an observation is small
66:46 - is small it tends to be very small and i
66:48 - can think of specific data sets
66:50 - that fall into these types of categories
66:52 - like for example
66:53 - income tends to be positively skewed
66:57 - so you have most people in a certain
67:00 - range of
67:01 - incomes and then you have a few people
67:03 - who make
67:04 - much much more than that right so
67:07 - most people are around here but when
67:09 - you're rich you tend to be very rich
67:11 - and on the other hand for negatively
67:14 - skewed data sets
67:15 - i do have something in mind i have test
67:18 - scores
67:19 - test scores for me like there's a lot of
67:22 - ways
67:24 - test scores could actually appear
67:25 - statistically but it seems like most
67:28 - students
67:29 - tend to fall within a certain range and
67:31 - the students who
67:32 - really didn't get it uh when they fail
67:35 - they fail hard so it tends to be
67:38 - negatively skewed
67:40 - and symmetric symmetric is kind of this
67:42 - ideal case
67:43 - where you're just as equally likely to
67:45 - be above or below i think
67:46 - heights could be possibly uh i haven't
67:49 - really looked at a height distribution a
67:50 - long time
67:51 - uh but i think that heights could
67:53 - probably be
67:55 - a symmetric uh where it's like you're
67:57 - just as likely to be
67:58 - uh really tall or really short um at
68:02 - some degree
68:04 - so you're also looking in these uh plots
68:06 - for outliers for example
68:07 - a histogram that has an outlier
68:11 - you might have a histogram that looks
68:13 - something like this and then you have a
68:15 - point that's way out here
68:17 - and you would say that this point right
68:19 - here is a candidate's being outlier
68:22 - and you're also interested in how spread
68:23 - out the data is
68:26 - and by spread uh when we're talking
68:28 - about spread we're talking about
68:31 - the less spread out black distribution
68:35 - as opposed to the more spread out
68:38 - green distribution which of these cases
68:41 - are you actually looking at
68:42 - or at the very least you're just
68:43 - interested in what the range of the data
68:45 - is
68:47 - okay so that's it for visualizing
68:51 - qualitative
68:52 - quantitative data now let's talk about
68:54 - qualitative or
68:55 - categorical data how do you visualize
68:58 - that we're only going to advocate
69:00 - one method here and that's a bar plot in
69:02 - fact in my visualization class
69:04 - i was told if you don't know what
69:06 - visualization method to use use a bar
69:07 - plot
69:08 - because bar plots are actually very good
69:10 - visualizations
69:11 - never ever for the love of god draw a
69:14 - pie chart
69:15 - i know i know especially in like public
69:18 - policy
69:19 - and economics people love their pie
69:21 - charts for the love of god do not create
69:23 - a pie chart
69:25 - do you have any idea how many memes of
69:27 - ugly and stupid pie charts there are
69:30 - please do not drop our chart okay um
69:32 - continuing on uh
69:34 - to construct a bar plot list
69:37 - each possible value of the variable and
69:39 - how frequently that value
69:40 - is taken uh a lot like what we were
69:43 - doing before
69:44 - it's just instead of having numbers on
69:45 - the left hand side
69:47 - for like these bins instead of that
69:49 - you're just going to have the categories
69:50 - that your dataset could be in
69:52 - uh and then you're going to draw a
69:54 - horizontal line
69:56 - so let's see a cartoon bar plot that
69:59 - we're making so we have like category a
70:01 - category b
70:02 - category c so draw a horizontal line it
70:06 - could be a vertical line it doesn't
70:07 - really matter
70:08 - and along the axis marks possible values
70:11 - of the variables
70:12 - and then draw a bar for each category
70:14 - extending to the categories
70:15 - observed frequency so we'd end up with
70:17 - something that looks like this um
70:21 - uh it is worth mentioning bar plots and
70:24 - histograms are two different things
70:27 - and the difference is this axis here
70:30 - this
70:30 - x-axis with a histogram
70:34 - that axis is a number line
70:37 - and there is a very specific order and
70:40 - if you were to plot a
70:41 - point on that line it would mean
70:44 - something
70:45 - whereas with a bar plot this doesn't
70:48 - mean anything
70:49 - you could rearrange it if you wanted to
70:52 - and the bar plot would say
70:53 - exactly the same thing even in the case
70:56 - of ordinal data you could space
70:58 - it out there's all sorts of
70:59 - transformations that you could do and
71:01 - the plot says the exact same thing
71:03 - that is not the case for histograms so
71:05 - bar plots and histograms
71:07 - despite looking somewhat similar are
71:09 - certainly not
71:10 - the same thing okay
71:14 - so uh for our next example
71:18 - uh there is a data set showing the
71:20 - frequency of the class of passengers
71:21 - aboard the titanic who survived
71:24 - her sinking uh i have the titanic data
71:27 - set in
71:27 - r uh contains this data set but they're
71:30 - actually tracking a lot of things
71:32 - they don't just track the class they
71:35 - track male and female they track age
71:37 - they track survival
71:39 - and so on here i have restricted
71:43 - to using the supply function which i'm
71:46 - not going to talk about right now
71:49 - i have restricted it to the case of
71:51 - survivors
71:52 - for individual for certain classes i
71:55 - want to create a bar plot for the
71:56 - frequency of each class's
71:58 - survival so um
72:01 - i've already got the frequency
72:02 - distribution r is already given it to be
72:04 - very nicely
72:05 - so i'm going to say we've got first
72:07 - class second class
72:09 - third crap class class and
72:12 - a crew okay and then we're going to
72:16 - extend up we're going to say
72:20 - up here let's say that this is a
72:25 - 220 and right here we've got
72:28 - 110. we're gonna just eyeball this
72:32 - uh so for first class there were about
72:35 - 203
72:37 - survivors so that's about here for crew
72:41 - there were 212 survivors so that's about
72:43 - here
72:44 - uh for second class there are about 118
72:47 - survivors that's about here
72:48 - for third class about 178 so our bar
72:51 - plot should look something like this
73:03 - okay and
73:06 - in fact we can look at what r does we
73:09 - give
73:10 - the bar plot function in r this bar uh
73:13 - this data set
73:14 - and it will in fact make a very nice bar
73:16 - plot for us
73:18 - okay there is actually a visualization
73:21 - method
73:21 - that i haven't really discussed here and
73:23 - you know i'm
73:24 - on i'm making a video everything's quite
73:28 - nice
73:28 - i really don't see why i shouldn't
73:32 - show you this if you're familiar with r
73:36 - so let's see first are we still
73:38 - streaming yes we are okay
73:39 - so um there is another type of
73:42 - visualization
73:43 - called a density plot you cannot make a
73:46 - density plot by hand
73:48 - so i'm going to go ahead and i'm going
73:49 - to make a density plot and does this
73:51 - command still work this this is a new
73:55 - installation oh good
73:56 - everything's working uh maybe we should
73:58 - go back
73:59 - to um
74:02 - well let's see what's a data set that we
74:05 - could work
74:06 - with um rivers is fine
74:10 - this is the length of some north
74:13 - american rivers
74:14 - and there is a plot called the density
74:16 - plot remember that i was drawing some
74:18 - smooth
74:18 - curves and histograms do not look like
74:21 - smooth curves
74:22 - what i kind of was drawing was a density
74:24 - plot so
74:25 - i could create such a plot by typing in
74:27 - plot
74:29 - density and then give it the name of the
74:32 - data set if it's stored in a vector so
74:34 - in this case
74:35 - rivers and this is the resulting density
74:38 - plot
74:39 - and it's actually plotting a smooth
74:40 - curve uh let's see
74:42 - let's what's uh one of the data sets
74:44 - that we were looking at before
74:46 - um page up page up
74:50 - uh what is it was this okay
74:56 - okay so we had this height data set so
74:59 - let's uh go ahead and combine those two
75:01 - things
75:03 - uh center on this okay so
75:06 - height uh will be
75:10 - a vector consisting of the numbers 5.55
75:14 - 5.30 uh
75:17 - 5.03
75:20 - 5.30
75:24 - 5.13
75:27 - 5.05 uh
75:30 - what was that number 5.36 no that's 5.38
75:35 - so we've got 5.38
75:41 - 5.96 and
75:44 - uh 5.21 and
75:47 - 5.38 okay so here's our data set
75:52 - and i'm just going to create a density
75:56 - plot
75:58 - for this data set
76:01 - um that's not it it's still rivers
76:04 - oh because i typed in reverse silly me
76:07 - um
76:08 - okay so what i want instead is height
76:14 - there that's better and it makes a
76:16 - smooth curve that
76:17 - kind of resembles actually it it yeah it
76:20 - certainly does resemble the histogram
76:22 - that we drew
76:22 - um except it's actually like in the
76:25 - histogram
76:26 - there is in fact when you look at that
76:27 - data set kind of a peak in this region
76:30 - um and that was completely masked by the
76:33 - histogram
76:34 - but the density plot was able to capture
76:36 - it interesting
76:38 - so but you have to make a plot like this
76:41 - an
76:42 - r you cannot make a plot like this by
76:44 - hand
76:45 - so just i'm just bringing it to your
76:47 - attention because these two these kind
76:49 - of plots do show up
76:52 - all right so uh that's it for section
76:55 - two of the book
76:56 - and uh i thank you for joining me and uh
76:59 - i will see
77:00 - and i hope that you watch the video for
77:02 - section three so have a nice day
77:12 - hey students all right so uh next
77:14 - section
77:15 - is on measures of location
77:18 - so up to this point we've talked about
77:20 - visual summaries and
77:22 - visual summaries are nice the thing is
77:24 - though we don't want to restrict
77:26 - ourselves
77:26 - just to visual summaries we would also
77:29 - like to be able to have numerical
77:31 - measures
77:32 - of data to understand
77:35 - distributions so we're going to start
77:39 - with measures of location measures of
77:42 - location tell us where
77:44 - a data set tends to be located along a
77:47 - number line
77:49 - so the first and most common measure you
77:51 - may have you probably have already seen
77:53 - a lot of these measures that we're going
77:54 - to talk about
77:55 - but the very first one we're going to
77:56 - talk about is the sample mean
77:59 - and for a data set consisting of
78:01 - observations x1
78:03 - to xn the sample mean is just is defined
78:07 - as x bar which
78:10 - equals 1 divided by
78:14 - n times the sum
78:18 - from i equals 1 to n
78:22 - x i which if you're not familiar with
78:25 - this notation what this means is we
78:26 - would take our data set
78:28 - add up everything in the data set and
78:30 - then divide
78:31 - the resulting sum by n
78:36 - now the there's the thing called the
78:38 - sample proportion and in fact
78:40 - relative frequencies are sample
78:42 - proportions they're counting
78:44 - the proportion of observations in a
78:46 - sample that take a certain value
78:49 - the sample proportion is also
78:52 - a measure of location it loosely is like
78:55 - the proportion of observations in the
78:57 - sample that have a certain
78:58 - characteristic
79:00 - so we divide the sample
79:03 - into successes and failures we like to
79:06 - use that vocabulary of success and
79:07 - failure
79:08 - a success and the sample proportion will
79:11 - count the number of successes
79:13 - so we'll have p hat
79:16 - equals and very loosely we're just going
79:20 - to say this is the
79:22 - number of whatever we consider to be a
79:26 - success
79:31 - and divide this by the sample size
79:34 - now this could also be written as
79:39 - x over n which could also
79:42 - where x is this uh number of
79:45 - successes and then we could say
79:48 - let's suppose that x i
79:53 - is equal to one
79:57 - if the ith observation counts
80:00 - as a success and zero otherwise
80:04 - what then does it mean to count the
80:06 - number of successes
80:07 - to count the number of success successes
80:10 - is to go through each
80:11 - observation and then add one to a
80:13 - running count
80:14 - if that observation counts as a success
80:16 - and otherwise do nothing which is the
80:18 - same as adding zero
80:20 - so we could then say that the number of
80:22 - successes
80:23 - is equal to the sum from i
80:26 - equals one
80:30 - to n of this new
80:33 - x i variable that is counting the number
80:36 - of successes effectively
80:37 - and saying whether an observation is a
80:40 - success
80:40 - so we should say that this sum
80:44 - is equal to the number of successes
80:47 - and then we take this sum and divide it
80:50 - by
80:50 - n
80:54 - which is also for what it's worth the
80:57 - same as taking the sum
80:59 - and multiplying it by 1 over n
81:04 - so notice what i just wrote down
81:07 - i just wrote down the sample mean again
81:11 - which means that the sample proportion
81:15 - is the same as the sample mean of a
81:17 - sample that consists of ones for
81:18 - successes and zeros
81:20 - otherwise so it
81:24 - recognizing this is actually very
81:26 - important because in probability theory
81:28 - the mean or more more directly
81:32 - sums of variables or sums of random
81:34 - variables
81:35 - are very important so recognizing
81:38 - something as a sample mean means that
81:41 - any
81:42 - theorems from probability theory that
81:44 - involve the sample mean
81:46 - can be applied to that variable
81:50 - so this is actually quite important to
81:52 - recognize
81:54 - but that means that after this point we
81:56 - really don't have to say much more about
81:58 - sample proportions because the sample
82:00 - mean
82:00 - whenever we're talking about the sample
82:02 - mean we're also talking about sample
82:03 - proportions
82:05 - so let's get started with an example
82:08 - what is the average number of points
82:10 - your daughter's soccer team scores here
82:13 - is
82:13 - as a reminder the data set this is
82:15 - actually proper r code right here just
82:18 - to write
82:18 - the just to write down the variable name
82:21 - soccer
82:22 - and what will happen is r will then
82:24 - print out
82:25 - that data set so uh but we're just going
82:28 - to take that for granted for now
82:30 - and uh compute first let's go ahead and
82:33 - compute the sum of these numbers
82:37 - so the sum from i equals 1 to n
82:40 - x i which is basically this right here
82:42 - just means take all the
82:44 - all the numbers in this data set and add
82:46 - them up
82:47 - so we've got 9 plus 6 is 15 plus 5 is 20
82:51 - plus 5 is 25 plus 5 is 30
82:54 - plus 6 is 36 plus 2 is 38
82:57 - plus 8 is 46 plus 3 is 49
83:01 - plus 4 is 53 plus 8 is 51.
83:06 - plus oh it looks like i'm all right so i
83:08 - actually have something written down i i
83:10 - have
83:11 - i i have 62. i think i might i might
83:13 - have uh
83:14 - missed something in that account but
83:15 - it's going to add up to 62. okay
83:18 - so just take my word for it uh so this
83:20 - adds up to 62
83:22 - uh n is equal to 12. so the sample size
83:25 - is equal to 12.
83:27 - and the sample mean then
83:30 - x bar will be 62
83:33 - divided by 12 which equals
83:36 - 31 over 6
83:40 - which is equal to 5.16 with a repeating
83:45 - six
83:47 - okay if we
83:50 - were to go to the next page
83:54 - in these notes we would see some r code
83:57 - that
83:57 - computes the sample mean for this data
83:59 - set the r
84:00 - function is mean so we ask
84:04 - so we say mean of the soccer data set
84:06 - and it will report to us the mean which
84:08 - is what we computed
84:10 - now let's suppose that uh
84:14 - r1 rn is the ordered version of this
84:16 - data set
84:17 - so x1 to xn uh is just
84:20 - so r1 to rn is x1 and xn but ordered
84:23 - remember from a previous video
84:25 - uh for section two on um
84:28 - that that for this notation uh x1 xn
84:32 - i do not necessarily imply
84:35 - any sort of ordering now i do for r1 to
84:39 - rn i'm going to
84:40 - imply an ordering uh
84:43 - the sample median is another measure
84:46 - for the location of the data set it is
84:50 - defined as the number that splits this
84:52 - data set
84:53 - in half so uh
84:56 - we can that is basically the definition
84:59 - um
85:00 - and from that we can come up with
85:01 - mathematical formula
85:03 - so we can say that x tilde that's the
85:06 - notation we will use for the sample
85:08 - median x tilde will be
85:10 - one of two possibilities first there is
85:13 - a case
85:14 - when there are an odd number of
85:16 - observations
85:19 - if there are an odd number of
85:21 - observations
85:25 - uh let's see let's zoom in so i can have
85:27 - a little bit more precise writing
85:30 - uh if there are odd number of
85:32 - observations
85:33 - after we order the data set the
85:35 - observation in the position
85:37 - n plus one divided by 2 will be the
85:40 - number
85:41 - that splits the data set in half
85:45 - so this will be what we're using if
85:48 - n is odd
85:51 - so as a so to think about this if we had
85:54 - 11 observations
85:56 - 11 plus 1 divided by 2 so that's 12
85:58 - divided by 2 that's equal to 6.
86:00 - the sixth observation after you order
86:02 - the data set
86:03 - will be the median okay uh
86:07 - now suppose that there are an even
86:09 - number of
86:10 - observations we could
86:13 - potentially choose our
86:16 - the r so the um
86:21 - observation in the n over tooth position
86:24 - so if uh the sample size were 12
86:27 - this would be the ordered observation
86:30 - the sixth ordered observation
86:33 - or we could potentially have the seventh
86:35 - ordered observation
86:37 - both of those are kind of dividing uh
86:40 - the center
86:41 - so what we'll do instead is we will take
86:44 - the midpoint between these two numbers
86:46 - which could potentially end up being the
86:47 - same number
86:48 - there's nothing that says that these two
86:50 - numbers are not the same but we're just
86:51 - going to average those two numbers take
86:53 - the midpoint in between them
86:55 - and admittedly if you had um
87:00 - a number line and you have some
87:04 - data over here and some data over here
87:07 - and you have these two
87:10 - uh observations as being potentially the
87:13 - median
87:14 - any number in between them could be
87:17 - defined as the median since i
87:19 - any of those numbers would divide the
87:22 - data set
87:22 - in half and in fact you may see
87:25 - alternative definitions
87:27 - of the median in r to take advantage of
87:29 - this fact um
87:32 - but for now it doesn't really matter if
87:35 - we had a lot of observations how
87:36 - exactly we define the median uh this is
87:40 - perfectly fine to just take the midpoint
87:41 - between
87:42 - uh the or the ordered observation
87:46 - in the position n over two and the order
87:48 - observation in the position n over two
87:50 - plus one
87:51 - so if we had uh twelve observations
87:54 - we would take the sixth and the seventh
87:56 - observations and average them to get the
87:58 - median
87:59 - okay let's see an example of computing
88:03 - the median find the median of the first
88:06 - 11 soccer games
88:08 - uh your daughter's team participated in
88:10 - i chose 11 just to have an even number
88:12 - to kind of
88:13 - no sorry odd number uh just
88:16 - for demonstration purposes um in this
88:19 - case
88:20 - uh i've our the the sort function
88:23 - in r uh this function
88:27 - will order your data set
88:30 - from smallest to largest uh given a
88:32 - vector it will order that vector
88:35 - from smallest to largest so
88:38 - we have sorted this data set we now have
88:40 - an order data set
88:42 - and i want to compute the median of that
88:45 - data set well first off
88:46 - there are n equals 11 observations
88:50 - in this data set that means that the
88:52 - observation in the middle will be
88:53 - m plus one over 2 which is 12 over 2
88:58 - which equals 6. therefore
89:01 - the median will be
89:04 - the sixth ordered observation
89:08 - which is let's see one two three four
89:12 - five six si five so it will be five
89:17 - that will be the medium
89:22 - if we were to look at some r code there
89:25 - is a function
89:25 - an r function called median and given
89:28 - that data set
89:29 - by the way i didn't mention this before
89:32 - this right here
89:33 - uh this square bracket stuff this is
89:36 - a subsetting notation uh it basically
89:40 - translates to
89:41 - get the observations uh get all
89:44 - observations between the first and the
89:46 - eleventh in this data set
89:48 - okay and remember that soccer itself is
89:51 - not ordered so we're just
89:52 - getting observations x1 through x11
89:56 - okay but i just feed that vector
90:00 - to the median function and it tells me
90:02 - that the median is 5.
90:04 - and by the way if you're wondering what
90:06 - this little one right here means
90:07 - uh that's just part of how r prints
90:10 - vectors
90:11 - if there were if this vector was quite
90:13 - long it would split over a number of
90:15 - lines
90:16 - and this would and this little one would
90:19 - just be tracking
90:20 - uh which observation you're looking at
90:23 - with each line so you wouldn't have
90:25 - so like if there was um uh 10
90:28 - down so if there was like more numbers
90:30 - after this
90:31 - and then we had in square brackets 10
90:33 - and saw the number six
90:34 - and numbers after that this would tell
90:36 - us that the sixth number and that
90:38 - that would tell us the tenth number and
90:40 - that back in that uh vector was six
90:42 - it's just something that's supposed to
90:44 - make reading
90:46 - uh what's in vectors um visually easier
90:50 - okay so uh i think i pressed something
90:55 - let's uh do that okay all right
90:58 - so continuing on and other measures of
91:01 - location are percentiles so we're going
91:04 - to say that the
91:05 - so this is the greek letter alpha and
91:07 - alpha is a number between 0 and 1 and
91:09 - we're going to include
91:10 - 0 and 1 as well the alpha times 100th
91:13 - percentile is the number such that
91:15 - roughly alpha times 100 percent
91:18 - of the data in the order data set lies
91:21 - to the left of that
91:22 - number uh so if we choose
91:26 - uh alpha equals 0.5 that would be uh
91:30 - the 50th percentile so roughly 50
91:33 - of the data set lies to the left of that
91:35 - number which means that 50
91:37 - lies to the right and what i just
91:40 - described
91:40 - is the median because the median is the
91:43 - observation that splits the data set in
91:44 - half
91:44 - which means that half of the data set is
91:46 - to the left or 50 and 50
91:48 - is to the right so um
91:51 - the median actually counts as a
91:54 - percentile percentiles are a
91:55 - generalization of the notion of a median
91:58 - in fact there are other percentiles that
92:00 - we care about such as quartiles
92:02 - quartiles divide the data set up into
92:04 - quarters
92:05 - so for the first quartile roughly 25
92:08 - percent
92:09 - of the data set rise to the left of that
92:11 - quartile and for the third quartile
92:13 - roughly 75 of the percent of the data
92:15 - set lies to the left of that quartile
92:17 - so um to visualize we would have
92:22 - um so we would have the first second and
92:25 - third
92:26 - quartiles and roughly 25 lies to the
92:29 - left
92:29 - of the first quartile and roughly 75
92:32 - percent
92:33 - of the of the data set lies to the left
92:36 - of the third quartile
92:37 - and the second quartile is the quartile
92:40 - where roughly 50 percent
92:42 - lies to the left of that four quartile
92:44 - in other words the median
92:45 - again um there's you can also say that
92:49 - there is a zeroth quartile
92:51 - which corresponds to alpha equals zero
92:53 - alpha equals zero
92:54 - means that there is nothing really to
92:56 - the left of this observation so
92:58 - that would correspond to the sample
93:00 - minimum whereas
93:01 - alpha equals one means that about 100
93:04 - percent of the data set lies to the left
93:05 - of that number
93:06 - that corresponds to the sample maximum
93:11 - there are actually a number of
93:13 - procedures
93:14 - for computing percentiles from data sets
93:19 - and i'm not going to repeat all of those
93:21 - procedures
93:23 - if anything i'm just going to list off
93:25 - the procedure that's easiest to do by
93:27 - hand because
93:28 - at the end of the day in the real world
93:30 - what you would do
93:31 - is ask r to get a percentile
93:35 - and r has its own algorithm for getting
93:38 - percentiles that's more complicated than
93:39 - what we're about to do
93:41 - and i really don't see the point in
93:44 - telling introductory students how to do
93:46 - that because you're it's it's more
93:48 - complicatedly intensive and
93:50 - like what's the point you get the if
93:51 - you're doing things by hand let's keep
93:53 - things simple
93:54 - um i should also probably mention um
93:57 - if you combine the zeroth first second
94:00 - third and fourth quartile fourth
94:01 - quartile is the maximum
94:03 - if you combine those together you end up
94:06 - with what's known
94:07 - as the five number summary of a data set
94:13 - so um here's a procedure for finding
94:16 - quartiles
94:19 - first find the median of the data set
94:23 - then split the data set into two data
94:26 - sets at the median
94:27 - and we're working with the order data
94:28 - set now so split it into do data sets
94:31 - at the medium if n is odd remove the
94:34 - data point that corresponds to the
94:36 - median
94:36 - and then to find the first quartile find
94:39 - the median of the lower data set
94:42 - and then to find the third quartile find
94:44 - the median of the
94:46 - upper data set so to visualize this
94:48 - procedure
94:49 - we have a data set find the median
94:52 - and then and this will split the data
94:55 - set into two
94:56 - then find the medians of the other two
94:58 - halves
95:00 - okay that will tell you what the first
95:03 - and third quartile
95:04 - are the minimum and the maximum are easy
95:06 - find the smallest and the largest
95:07 - numbers in the order data set
95:09 - okay so for our first example we're
95:12 - going to find the first and third
95:13 - quartiles
95:14 - of our of uh of this uh little girl's uh
95:18 - first 11 soccer games
95:21 - so uh let me get caught up in my notes
95:24 - for a second okay
95:28 - so to find these two quartiles
95:33 - how about we write down uh what that
95:36 - data set
95:36 - was just for our own purposes so we've
95:40 - got
95:40 - two three four uh
95:44 - five five five
95:47 - uh six six eight nine
95:51 - okay so the median was five and this
95:54 - is an odd number data set so we're going
95:56 - to
95:58 - delete that median
96:02 - six six oh oops there were two eights
96:06 - sorry about that so we're going to
96:08 - delete the median which is five
96:10 - and then we have split the data set in
96:12 - two
96:13 - so then we find the median of the first
96:15 - half there is an odd number of
96:17 - observations here
96:19 - so the median will be four thus the
96:21 - first quartile which i will call q1
96:24 - that will be four and the median
96:27 - of the upper data set that will be eight
96:30 - so that means that the third quartile
96:32 - will be eight and if we wanted to we can
96:35 - kind of fill out the five number summary
96:38 - saying the second quartile which is the
96:40 - median
96:42 - this that number is going to be five
96:47 - the zeroth quartile which is the minimum
96:53 - well that's going to be 2. 2 is the
96:56 - smallest number
96:57 - and the 4th quartile which is the
96:59 - maximum
97:03 - well the largest number in the data set
97:04 - is 9 so that will be 9.
97:08 - okay
97:11 - okay next example find the 10th and 90th
97:14 - percentiles of the height data i have
97:16 - listed the data
97:17 - for you below in order so the data set
97:21 - oh yeah this is actually what i was
97:22 - getting to um
97:24 - regarding those little numbers in square
97:26 - brackets this is the ninth observation
97:28 - right so that means that this number
97:30 - right here is the tenth observation in
97:32 - the data set
97:33 - so that means that this data set has
97:36 - n equals 10 observations
97:40 - uh so 10th percentile so roughly 10
97:44 - percent
97:44 - of the data set lies to the left of that
97:46 - number so
97:48 - uh 0.1 times 10
97:51 - is equal to one so that's about uh
97:54 - so about one number lies there so we're
97:56 - going to choose 5.05
98:00 - so uh 5.05
98:03 - will be the 10th percentile
98:06 - which we will call uh
98:09 - we'll call that q hat point uh
98:13 - 10 and uh for the 90th percentile
98:17 - so 0.9 times 10 that's going to be
98:21 - 9 so that's observation number 9. so 90
98:24 - of the data set lies including that
98:28 - number
98:28 - to the left of 5.63 so we'll say
98:32 - that 5.63 will be the 90th percentile
98:36 - which we'll call
98:37 - uh q q hat 90.
98:42 - okay and actually r
98:45 - has functions for computing
98:48 - uh quantiles another word for what we're
98:52 - talking about here are quantiles
98:54 - here i have asked r specifically to give
98:56 - me the 25th and 75th percentiles
98:59 - it is not using this procedure that i
99:01 - described it is using a different
99:03 - procedure
99:04 - for finding quantiles there's a number
99:06 - of different procedures for finding
99:07 - quantiles
99:08 - and percentiles and if your sample size
99:11 - is large nobody cares
99:12 - which one you use really um it really
99:15 - only matters at smaller sample sizes
99:17 - which procedure exactly you use
99:19 - and there are various different
99:20 - motivations um
99:22 - for different kinds of procedures
99:24 - there's various ways to solve the same
99:25 - problem
99:26 - and what i described is simple enough to
99:29 - do by hand
99:30 - so if you're going to do it by hand go
99:32 - ahead and use what i used this is
99:33 - probably
99:34 - uh whatever came up with this is
99:36 - probably much more complicated
99:37 - and if you're going to do if you need
99:39 - something more complicated then don't do
99:40 - it by hand that's hard to do it
99:42 - so you can read rs documentation to see
99:44 - what exactly is being done
99:46 - here it's using some sort of
99:47 - interpolation trick so
99:49 - here i've asked so the second parameter
99:52 - here this is a vector that contains the
99:54 - numbers 0.25 and 0.75 corresponding to
99:56 - the 25th and 75th percentiles
99:58 - or quantiles i'm not really sure what
100:00 - the difference between those two words
100:01 - is
100:02 - um here i ask for the 10th and 90th
100:05 - quantiles or percentiles
100:07 - and it gives me numbers these are all
100:09 - pretty close to what i came up with
100:10 - before
100:14 - okay uh next up uh let's kind of
100:17 - we we've come up with some measures of
100:20 - location
100:21 - and there's actually a number of
100:23 - different measures of location like for
100:25 - example
100:26 - i have seen a measure where you take the
100:29 - largest and the smallest observations
100:32 - so you could say that um
100:35 - so you could say that's r n m
100:38 - minus r one uh no plus r one
100:42 - so take the midpoint between the largest
100:45 - and the smallest observations that's
100:47 - actually another measure of location
100:48 - there's a number of different measures
100:50 - of location
100:51 - what i've shown here so far is fine but
100:54 - let's go ahead
100:55 - and start comparing these different
101:00 - methods for uh
101:04 - for measuring for describing the
101:06 - location
101:08 - of a data set so the sample mean x bar
101:12 - is known to be sensitive to outliers
101:15 - which
101:15 - means that outliers the data set have a
101:18 - profound effect
101:19 - on the sample mean so if you had
101:22 - for example a data set that consists of
101:24 - one one
101:25 - uh let's say one two three the average
101:28 - of that data set or the mean of that
101:30 - data set will be
101:31 - two and the median of that data set will
101:33 - also be two
101:34 - uh compare that to
101:38 - a data set that contains the numbers one
101:40 - to one thousand
101:42 - so uh the mean of that is going to be
101:46 - about 500 which is much larger than 2
101:50 - what it was before compare that to the
101:53 - median
101:54 - the median of that data set 1 to 1000
101:57 - is still 2 which means that the median
102:01 - is insensitive to outliers it basically
102:04 - doesn't care
102:04 - what they are all it cares about is the
102:07 - ordering of the observations
102:09 - so long as an outlet if you were to
102:10 - change the value of an outlier
102:12 - so long as it doesn't change the
102:14 - ordering of the data set
102:16 - the median will not change
102:19 - so as an example of this i'm going to
102:22 - compute
102:24 - the sample mean and the sample mean of
102:25 - the daughter's uh
102:28 - uh soccer games if i throw in a 12th
102:31 - soccer game and i'm going to consider a
102:34 - number of different scenarios
102:37 - where her 12th game was
102:40 - 1 point or one goal of four goals two
102:44 - goals and
102:44 - eighteen goals we're going to consider
102:47 - that
102:48 - and what i actually did here was um i
102:51 - created a vector that contains
102:55 - these uh that contains these so
102:59 - uh i'm not gonna talk too much more
103:01 - about this because
103:02 - i wanna focus on the math um
103:06 - so uh first off when i add
103:10 - up the 11 other games
103:17 - when i add those up i end up with a
103:19 - cumulative score
103:20 - of 61. so let's create a table
103:27 - for all of these potential outliers
103:30 - so in this table uh we're going to
103:32 - consider when the outlier
103:34 - when the 12th game is 1 when it's 4 when
103:38 - it's
103:38 - 2 and when it's 18
103:41 - and we're going to have a column for
103:45 - the sum of the observations we're going
103:48 - to have a column
103:49 - for the sample mean and a column for
103:52 - the sample median so in the case
103:56 - where the 12th game is one this will add
103:59 - up to
104:00 - 62. and by the way i'm using the word
104:02 - outlier for one but one in this case
104:03 - would not be an outlier
104:05 - same with 4 and 2 but 18 certainly would
104:07 - be considered an outlier
104:08 - so if her 12th game is 1 then it adds up
104:12 - to 62.
104:13 - if her 12th game is 4 it adds up to 65.
104:16 - if her 12th game is 2 the game's
104:20 - cumulative score is going to be not 62.
104:22 - uh 63
104:24 - and if it's 18 they all add up to 79.
104:29 - and then we're going to take these sums
104:31 - and divide them by 12.
104:32 - in the end we end up with uh
104:35 - in the first case we get a sample mean
104:37 - of 5.16
104:40 - uh in the second case we get a sample
104:42 - mean of 5.416
104:46 - uh repeating six uh in the third case we
104:50 - get
104:51 - 5.25 and in the fourth case
104:54 - we get 6.58 which is
104:57 - uh much different from what we had
105:00 - before
105:01 - uh
105:05 - hold on i think my pen might be okay my
105:07 - pen's back
105:08 - is it though might need to charge
105:11 - okay now the median in the first case
105:16 - uh so since uh
105:19 - so in the first case the median is still
105:22 - going to be five and in fact it's going
105:24 - to be
105:24 - five for the first three cases because
105:27 - the median
105:28 - was five and it didn't change the order
105:31 - of the data set if you go back and look
105:32 - at the original data set
105:33 - we're essentially in in these first
105:36 - three cases we shift all the numbers to
105:38 - uh the left which means that the
105:40 - median's gonna be
105:42 - uh the number to the left of what it was
105:44 - before which was
105:45 - five but if we chose or it's actually
105:49 - going to be the average of five and five
105:50 - which is still
105:51 - five but in the last case
105:54 - uh the median actually will be the
105:57 - average of five and six
105:59 - so it will be 5.5 so the median did
106:02 - change
106:02 - a little bit um in the last case but
106:05 - it's mostly because of
106:06 - where that outlier ended up it ended up
106:08 - on the right hand side of the data set
106:11 - or the right hand side of what the
106:12 - median was before
106:14 - and it could have been 273 and the
106:17 - median would be exactly the same
106:20 - but as we can see the median isn't
106:22 - changing
106:23 - really at all compare especially when
106:25 - you compare it to
106:27 - the mean and here is some r code
106:30 - that the idea of this code is i'm going
106:33 - through
106:33 - a loop adding this observation to
106:37 - a to a version of the data set and then
106:40 - uh computing the uh
106:44 - median and the mean of that data set the
106:46 - result will be an
106:47 - r matrix i take the transpose of that
106:50 - matrix because that's the version of the
106:52 - matrix that i prefer i'm giving the
106:54 - matrix some row names and column names
106:57 - do some rounding and this is a resulting
106:59 - matrix and it's pretty similar to what
107:01 - we had before
107:04 - in fact there is a relationship that we
107:06 - can say in general between the mean
107:09 - and the median depending on whether the
107:11 - data is negatively skewed
107:13 - positively skewed or symmetric if the
107:16 - data set
107:17 - is nic is a positively skewed
107:23 - so that would so the data set looks
107:25 - roughly like this
107:26 - then the me the median which is the
107:29 - point that devas
107:30 - divides the data set roughly in half
107:33 - will be less than the mean
107:39 - and that's because the mean is going to
107:40 - try to chase the outliers the outliers
107:42 - are going to be on the right hand side
107:44 - of the bulk of the data
107:46 - so in the case of
107:49 - negatively skewed data
107:53 - we're going to have the opposite
107:54 - relationship where the median
107:58 - since the outliers are going to be on
107:59 - the left hand side of the bulk of the
108:01 - data
108:02 - the mean is going to chase the smaller
108:03 - numbers so the
108:05 - me the median will tend to be greater
108:07 - than the mean
108:08 - and in the case of a symmetric data set
108:12 - the mean and the median should be
108:14 - approximately the same
108:16 - in real data i mean probabilistically
108:19 - when we talk about
108:20 - means and medians and probability uh
108:23 - they will be exactly the same when the
108:24 - date when the distribution of the data
108:26 - is symmetric but in real data that's
108:28 - never the case
108:29 - in real data they will just be close and
108:31 - what it means to be close
108:33 - is is like that's a judgment call
108:36 - right um so what would that some
108:38 - implications for that
108:40 - thinking back to some examples of
108:42 - positively skewed and negatively skewed
108:43 - data sets
108:44 - i said that a positively skewed data set
108:46 - is incomes
108:48 - this relationship means um
108:52 - this relationship means that the average
108:54 - income tends to be larger than the
108:56 - median income
108:57 - and economists generally prefer to use
108:59 - the median income
109:01 - for income distributions because it
109:03 - seems inappropriate
109:05 - to use the mean this kind of gets to the
109:08 - issue of
109:09 - i've given you these competing measures
109:13 - for means and for for me for measuring
109:16 - the location of a data set
109:18 - which one of these measures should you
109:20 - use
109:21 - i would say use the one that's
109:24 - appropriate
109:25 - which means um well here's one thing
109:27 - once you use the mean
109:29 - uh i would say you should use the mean
109:31 - when
109:33 - large observations are allowed to
109:35 - compensate for small ones
109:37 - let's say for example that you're
109:39 - gambling if you're gambling what you
109:41 - care about
109:42 - are your mean earnings and not your
109:44 - median earnings
109:45 - because it's okay for you to win nothing
109:50 - 99 times if you win a million
109:53 - or i guess it also depends on how how
109:56 - much this game is worth but let's say
109:57 - that
109:58 - you're playing a game that costs a
109:59 - dollar each time you play it's okay for
110:01 - you
110:02 - to win nothing 99 times if on the 100th
110:05 - time
110:06 - you win a million dollars that would be
110:08 - awesome for you
110:10 - whereas in that situation the median
110:12 - would be zero dollars
110:13 - and if you were judging by the median
110:15 - how well you were doing you would think
110:16 - you were actually doing poorly
110:19 - so if you're allowing large observations
110:22 - or small
110:23 - observations to to
110:27 - maybe uh replace or detract from
110:30 - the overall score then the mean is
110:33 - appropriate
110:34 - on the other hand the median in the case
110:37 - of
110:38 - uh social sciences we care more about
110:40 - what
110:41 - like what fifty percent of the obs of
110:44 - the population is experiencing
110:45 - and we don't necessarily want to allow
110:47 - uh like the
110:49 - very the the uh the um
110:52 - the great fortune of the wealthy to
110:56 - uh compensate for uh the great poverty
111:00 - of the very poor
111:01 - so in that situation the medium would be
111:03 - the preferred observation and of course
111:05 - if theoretically what you're trying to
111:07 - measure is the population median
111:09 - then you should use the sample median if
111:11 - you're trying to estimate the population
111:12 - mean
111:13 - you should use the sample mean and in
111:15 - which case you should not be using
111:17 - the opposite now there are some
111:19 - exceptions to this and we'll talk about
111:20 - this
111:21 - when we talk about probability and talk
111:23 - about um
111:24 - hypothesis testing there's notions such
111:26 - as most powerful tests like if
111:27 - your data set was symmetric and you knew
111:29 - it came from a normal distribution you
111:31 - should always use the mean even
111:32 - even if what you care about is the
111:33 - median uh but and the reason why that is
111:36 - is because for a normal distribution the
111:38 - two numbers are the same
111:39 - uh but um we're just going to leave that
111:42 - for now
111:43 - um
111:46 - so um and there so all of this was
111:50 - talking about uh sample results
111:52 - uh there are analogous population
111:54 - numbers too
111:55 - okay so um i just want to very quickly
112:00 - satisfy my nervousness okay we're still
112:02 - streaming okay
112:03 - um all right there is another number
112:08 - another measure for location called
112:12 - uh the trimmed mean so i'm going to comp
112:16 - i'm going to tell you about the trim
112:17 - bean and i'm even going to compute it
112:18 - for you but then i'm going to
112:20 - uh have some caveats about using it
112:23 - um you probably should not be using uh
112:26 - the trimmed mean
112:28 - and i'll explain that in a second but
112:30 - the idea of the trend mean
112:32 - is we have the stat we have the median
112:36 - and the median is not sensitive to
112:38 - outliers which is generally considered a
112:39 - blessing but it's not always a good
112:41 - thing because if
112:42 - it feels a little inappropriate to throw
112:44 - out so much data when you're computing
112:46 - the median
112:47 - because once you know the ordering of
112:50 - the data and you know
112:51 - which two which one or two numbers are
112:53 - in the middle then the other day
112:54 - the rest of the data doesn't matter and
112:56 - that feels a little extreme
112:58 - on the other hand you're a little
112:59 - bothered by the means
113:02 - sensitivity to outliers
113:05 - and furthermore on outliers you might
113:08 - think should we throw
113:09 - out outliers should we ignore outliers
113:12 - because that's actually kind of what the
113:14 - truth mean is suggesting that we should
113:15 - do
113:16 - with the trimmed mean what we're going
113:18 - to do is we're going to
113:21 - uh use only um uh
113:25 - we're going to throw out a certain
113:26 - percentage of the data like we're going
113:28 - to throw out 10 percent on the left-hand
113:30 - side and 10
113:31 - on the right-hand side of the data set
113:34 - so throw out
113:35 - the 10 10 percent of the smallest
113:37 - numbers
113:38 - and or the um
113:42 - so in this data set ten percent of the
113:44 - numbers that are the smallest numbers
113:45 - that the data set
113:46 - and ten percent of the numbers are the
113:47 - large larger numbers in the data set
113:49 - that would be the
113:50 - uh trend mean where alpha equals point
113:51 - one where you're trimming at ten 10 on
113:53 - each end
113:54 - um so the idea of the trend mean is
113:57 - throw out the outliers
113:59 - um so on this issue of whether you
114:03 - should throw out
114:04 - outliers you should actually think very
114:06 - carefully before you throw out outliers
114:08 - in general
114:10 - if you're competing a trimmed mean then
114:11 - that's kind of what you're doing
114:13 - but let's say you look at a data set and
114:15 - you see that
114:17 - your estimators are actually very
114:19 - influenced by
114:20 - a couple outliers and you're thinking
114:23 - maybe i should just throw those out i
114:25 - would first ask
114:26 - why are you throwing them out are you
114:28 - throwing it out
114:29 - because uh you think the number is
114:32 - erroneous because i
114:33 - in my own experience have seen numbers
114:36 - in data sets where it's like that's
114:38 - probably an error someone probably
114:40 - entered the wrong number so i'm going to
114:41 - throw it out
114:43 - um like when someone writes in
114:46 - the american community survey that
114:47 - someone's made a trillion dollars it's
114:49 - like that's probably not correct
114:51 - um you should just throw it out um if
114:53 - that's what's going on
114:54 - go ahead and throw it out because the
114:56 - reason why you're throwing it out is
114:57 - because of data contamination
115:00 - but if you're throwing it out just
115:02 - because
115:03 - it's causing bad behavior in your
115:07 - estimators
115:09 - that is probably inappropriate and you
115:12 - should
115:12 - instead try to model the outlier or just
115:16 - accept it
115:16 - rather than throw it out or think harder
115:20 - about why it is that you are using the
115:22 - mean rather than the median
115:25 - i would actually suggest that over
115:28 - throwing
115:29 - the outlier out but anyway um
115:32 - let's go ahead and compute uh the
115:35 - trimmed mean
115:36 - for the height data set so let's
115:40 - go ahead and rewrite that data set
115:43 - we've got uh i don't want blue
115:49 - okay so i've got for the height data set
115:53 - 5.05 you can go ahead and like
115:55 - skip ahead a little bit to skip me
115:57 - writing down numbers
115:59 - so 5.05 5.13
116:04 - 5.21 5.3
116:08 - uh 5.3 this
116:11 - splits the data set in half so the next
116:14 - number is
116:18 - 5.38
116:20 - and we've got 5.38
116:23 - again 5.55
116:30 - 5.63
116:32 - and 5.96
116:36 - okay so there are 10 numbers in this
116:40 - data set
116:41 - if we're going to trim so if we're going
116:43 - to say alpha
116:44 - equals 0.1 so we're going to trim 10 of
116:47 - the data off on each end
116:54 - so trim ten percent at each end
117:01 - that means that we're going to end up
117:02 - trimming
117:04 - uh 0.1 times
117:07 - 10 where 10 is the sample size
117:11 - which is 1. we're going to trim off the
117:14 - smallest and the largest number
117:16 - in this data set so that would be
117:21 - that would be 5.05 and 5.96
117:27 - and then take the average of the
117:28 - remaining data set so
117:30 - in that case x bar where we trim off
117:35 - uh 10 percent
117:39 - will be the average
117:43 - of the eight remaining numbers where
117:46 - we're sum
117:46 - from i equals two to nine uh
117:50 - the order data set
117:54 - um and that is going to end up being
117:58 - uh 42 so the sum is going to be 42.88
118:05 - uh and then we divide that by eight and
118:07 - the result
118:09 - will be uh 5.36
118:12 - so 5.36 feet because this data set is in
118:15 - feet we're we're talking about height
118:18 - okay and uh r can compute trimmed means
118:22 - in fact you can just use the mean
118:23 - function like we had before we're just
118:24 - going to pass it an additional parameter
118:27 - that tells the function to trim now i
118:30 - mentioned
118:31 - a little while back you actually
118:32 - probably should not
118:34 - be computing trimmed means and the
118:36 - reason why
118:37 - is because when we compute a median
118:40 - which i
118:40 - a a little side note i guess the median
118:43 - both the mean and the median count
118:45 - as particular trimmed means where the
118:48 - median
118:48 - is like the trend mean where you trim
118:50 - off 50 percent and
118:52 - the mean is where it is the trimmed beam
118:54 - where you turn off zero percent
118:56 - so the trend being kind of generalizes
118:58 - these other two statistics
119:00 - um but you probably should only use
119:02 - those other two statistics you should
119:03 - probably not use the trim mean
119:05 - i mean i guess if what you were doing is
119:07 - instead of taking off 10 on either end
119:09 - it's like
119:10 - always take off uh the two largest and
119:13 - two smallest numbers
119:14 - that could be appropriate um from
119:18 - a theoretical perspective
119:21 - uh so because basically as you increase
119:24 - the sample size the number of
119:26 - observations that you're trimming off
119:27 - becomes very small
119:29 - relative to the rest of the sample but
119:32 - trimming off ten percent
119:34 - at either end from a theoretical
119:36 - standing
119:37 - is a little odd and the reason why
119:40 - is that there is actually a population
119:42 - median
119:43 - that we are estimating when we are using
119:45 - a sample median
119:46 - and there is a population mean that we
119:49 - are estimating
119:50 - when we use a sample mean and both of
119:52 - those quantities are very well
119:54 - understood
119:55 - but when you're using a trend mean you
119:57 - are estimating neither of those things
119:59 - you're estimating some weird hybrid a
120:03 - monstrous monstrous population statistic
120:05 - that we don't necessarily understand
120:07 - you're
120:07 - estimating essentially the population
120:10 - version
120:10 - of a trimmed mean and it's questionable
120:13 - whether that's
120:14 - actually what you want it seems like the
120:17 - worst of both worlds in that case
120:19 - because no one can actually like why
120:20 - would we talk about
120:22 - the population except for the 10 largest
120:25 - and 10
120:26 - smallest numbers like that doesn't
120:28 - really make a whole lot of sense
120:30 - so for that reason
120:34 - unless you are actually i would actually
120:37 - more advocate for like a fixed trimming
120:39 - where you take off the two largest and
120:41 - two smallest observations
120:43 - um although at that point you probably
120:44 - should just use the mean
120:46 - or use the median i probably would not
120:48 - use the trimmed mean
120:51 - so okay so that's it for
120:54 - this section in the next section we will
120:56 - be discussing
120:57 - uh measures of variability so
121:00 - um i will cut it off here and have a
121:03 - good day
121:20 - okay so this section is about
121:24 - measures of variability so last
121:27 - section we discussed measures of
121:30 - location
121:31 - let's start by justifying why we need
121:34 - measures of variability
121:36 - consider these three data sets and i'm
121:38 - going to construct a dot plot
121:40 - for each of these data sets so i've got
121:44 - uh three lines for my three dot plots
121:46 - data set one data set two data set three
121:49 - uh and in the these dot plots i'm going
121:52 - to
121:53 - uh start uh with one and it's
121:56 - ending the twelve
121:59 - and uh in between i've got six
122:03 - so so let's see i'm going to have
122:07 - we're going to keep these all on the
122:10 - same scale
122:11 - so 1 12 6
122:15 - 1 12. uh
122:19 - six and uh we'll go one
122:23 - two three four
122:26 - five six seven eight nine
122:30 - ten 12 okay that's one two
122:33 - three four oops five six
122:36 - okay that's that's that's that's a
122:39 - little inexcusable we're gonna have to
122:40 - try a little harder on that one
122:41 - uh one two three
122:44 - four 5 6 7 8
122:48 - 9 10 11 12. okay
122:51 - 1 2 3 4 5
122:54 - 6 7 8 9
122:57 - 10 11 12. okay so i've now got these
123:01 - three number lines and let's start with
123:02 - data set one so we've got numbers
123:05 - at uh so four
123:09 - five six seven
123:12 - eight and then we've got one at
123:16 - uh two five six
123:20 - seven ten and then finally we have
123:24 - one three six
123:28 - 9 9
123:31 - and then 11. okay so
123:35 - look at these three dot plots now i want
123:38 - you to
123:40 - let's let's first start actually by uh
123:42 - computing the mean and the medium for
123:43 - each of these data sets
123:44 - so data set one four plus five plus six
123:46 - plus seven plus eight plus nine
123:48 - uh that is going to add up to 36
123:53 - the second data set well we subtract 2
123:55 - from 4 to get 2
123:56 - but then add 2 to 8 to get 10. so that
123:58 - second one is also going to add up to
124:00 - 36.
124:01 - and for the third one you kind of are
124:02 - going to do the same trick so they all
124:03 - add up to 36
124:05 - which then means that the sample mean is
124:07 - going to be
124:08 - 36 divided by there's six observations
124:11 - no actually there's
124:12 - five uh oh i'm sorry they don't add up
124:15 - to 36 they don't add up to 36.
124:19 - oh silly curtis silly curtis
124:22 - they added to 30
124:27 - so the sampling would be 30 divided by 5
124:30 - which equals 6
124:32 - which is also equal to the median
124:37 - because you look at them because these
124:39 - data sets are ordered they have five
124:40 - observations
124:41 - so the third row is going to correspond
124:43 - to the median so that means that the
124:45 - mean and the median for these data sets
124:47 - are all the same
124:48 - and yet let's suppose now that i were to
124:50 - ask tell you
124:51 - that this was the waiting time for the
124:52 - train uh which
124:54 - of these data sets would you prefer to
124:56 - be the observed waiting times for the
124:58 - train
124:59 - probably the first one at least if
125:02 - you're like me
125:03 - because for myself i actually did not
125:07 - really like inconsistent trains
125:10 - i mean it's kind of cool that this train
125:12 - will there might be
125:13 - a one-minute waiting time uh for this
125:16 - train
125:16 - but there also could be an 11 minute
125:18 - waiting time for this train
125:20 - and one way or the other i would just
125:22 - love it if trains always showed up
125:24 - exactly six minutes
125:26 - um between which okay admittedly around
125:29 - here they generally do do that
125:31 - but um you don't really like a lot of
125:34 - variability in the wait time for the
125:36 - train because that makes the train
125:37 - unreliable
125:39 - that said this aspect
125:42 - of the data set is not being captured by
125:46 - uh our measures of location the sample
125:48 - mean and the sample median
125:50 - unfortunately so and the reason why is
125:53 - because the attribute that we're talking
125:55 - about is an attribute that doesn't have
125:56 - anything to do with the location
125:58 - these data sets are located at
126:00 - essentially the same place
126:02 - they it has to do instead with spread
126:04 - and we now have a pictographic
126:06 - method for understanding spread we can
126:08 - see that these data sets have different
126:10 - spread
126:11 - but we would like to have some numerical
126:13 - measures
126:14 - so very quickly i'm just going to say
126:16 - that i would prefer one because it's
126:18 - more consistent
126:18 - or less spread
126:31 - okay what we need is a measure of
126:34 - variability to describe how spread out a
126:36 - data set
126:37 - is uh how could we possibly do that well
126:40 - we might
126:41 - start by examining deviations
126:45 - which where we look at x i and subtract
126:47 - out the sample mean
126:49 - and and if we were to add these up
126:51 - together this might give us a measure
126:53 - for um how spread out the data set is
126:57 - here's the thing though when we try that
127:00 - um
127:01 - we're going to sum up from
127:04 - i equals 1 to n
127:08 - uh no that should be a 1. so from i
127:12 - equals 1 to n
127:13 - x i minus x bar
127:18 - and this is a sum sums are linear
127:22 - which means that i can now break up this
127:24 - sum into two sums and say that this is
127:26 - going to be a sum
127:27 - from i equals one
127:30 - to n x i
127:34 - minus um the sum from i equals one
127:39 - to n x bar but
127:42 - here's the thing about that ladder sum
127:46 - see this is actually adding up a
127:50 - constant
127:51 - n times and you probably remember from
127:54 - second grade what it means to add up
127:56 - the same number and times you end up
127:58 - with multiplication
127:59 - so this number is going to actually end
128:01 - up being
128:03 - n times x bar
128:06 - okay and this number also can be
128:09 - interpreted as
128:10 - n times x bar because it is
128:13 - the sum of the observations divided by
128:16 - the sample size and then multiplied by
128:18 - the sample size again
128:20 - so we end up with
128:23 - n x bar minus
128:26 - n x bar and that equals zero
128:31 - so what that means is that this quantity
128:33 - is always
128:34 - equal to zero always equal to zero
128:42 - i think i found a dead spot on my screen
128:50 - okay so that always adds up to zero uh
128:53 - which means
128:54 - i mean the issue is that these
128:56 - deviations
128:58 - they always have the same sign
129:02 - well okay the they they all have um they
129:05 - all right what i just said was literally
129:07 - false um
129:08 - they don't always have the same sign in
129:11 - fact uh
129:12 - you have opposite signs you have some
129:14 - positive some negative deviations
129:16 - and it turns out that uh
129:19 - the positive and negative deviations
129:20 - cancel each other out
129:22 - so you end up with a zero um so that
129:25 - didn't quite work although there was an
129:26 - interesting idea there
129:28 - um looking at the distance between an
129:31 - observation and the sample mean
129:33 - and one might be tempted
129:37 - to try this replace the parentheses
129:41 - with absolute values so you end up
129:43 - adding up
129:44 - the absolute value of x i minus x bar
129:47 - and um
129:50 - that now you don't have that issue of
129:52 - negatives and positives canceling each
129:54 - other out because everything will be
129:56 - positive
129:56 - and you'll end up with a positive number
129:58 - and that makes sense
130:00 - um the thing though is
130:03 - this is actually more difficult for a
130:05 - mathematical perspective to work with
130:09 - and the reason why is because it's
130:10 - involving absolute values and
130:12 - absolute values are not differentiable
130:15 - absolute values if you remember from
130:17 - calculus 1
130:18 - they have a cusp of a sharp point
130:22 - and sharp points are not differentiable
130:25 - compare that instead
130:28 - to so this is like the absolute value of
130:30 - x compare that instead
130:32 - with the function x squared
130:35 - that should work i mean that that has
130:38 - the uh nice feature of being
130:40 - differentiable
130:41 - so what actually statisticians end up
130:43 - doing
130:44 - is they say we should add up the sum
130:48 - from i equals 1 to
130:51 - n x i
130:54 - minus x bar
130:57 - squared and this quantity is known
131:01 - as the sum of squared errors x i minus x
131:05 - bar
131:05 - a term that statisticians like to use
131:07 - for that is the error
131:09 - and um
131:12 - we're adding up the squared errors
131:15 - and there is in fact an interpretation
131:18 - for
131:19 - uh for the square part which is maybe
131:22 - you remember this is how i like to think
131:24 - of it
131:24 - i think that this formula kind of rhymes
131:27 - with x1 minus x2
131:31 - squared plus y1
131:35 - minus y2 do you remember that do you
131:38 - remember that from geometry class
131:41 - if you take the square root of this
131:42 - quantity you end up with the distance
131:45 - formula
131:46 - for euclidean distance for euclidean
131:48 - geometry
131:49 - and it actually kind of rhymes with
131:53 - that sum that i've drawn that i've uh
131:55 - shown
131:56 - uh up above so uh and in fact
132:00 - there is um a very deep
132:03 - connection between uh the sum of squared
132:07 - errors and euclidean geometry
132:09 - um but this quantity to me
132:12 - like it that seems like an appropriate
132:15 - way to think about distance
132:16 - and if we if we were to
132:20 - average this by by saying like this is
132:23 - one over n
132:24 - we would have an average squared
132:26 - distance
132:27 - now that's actually a good idea but um
132:30 - there's a better idea which is to divide
132:33 - instead of by
132:33 - n by n minus one now that
132:36 - might strike you as a little bit odd why
132:38 - is it that we're dividing by n minus one
132:41 - uh there's a few reasons for that some
132:43 - of which we'll talk about later in maybe
132:44 - chapter six
132:45 - but uh n minus 1 there's actually a term
132:48 - for this quantity and it's known in this
132:50 - in this context as the degrees of
132:52 - freedom
132:58 - and why are we dividing by the degrees
133:01 - of
133:02 - freedom rather than n
133:05 - i'm going to present to you a few
133:07 - arguments for why you'd want to do that
133:11 - for starters let's imagine that we had a
133:13 - data set of size
133:14 - 1. right so there's only one observation
133:17 - or data set
133:18 - what we're trying to measure right now
133:20 - is
133:21 - is a spread in the data set if we had a
133:24 - data set of
133:25 - size one is there really any way to
133:28 - estimate spread
133:29 - how can you determine the spread from a
133:31 - data set of one
133:33 - observation um that doesn't really make
133:36 - a whole lot of sense
133:38 - and it seems like something has
133:41 - fundamentally gone
133:42 - wrong in that situation
133:45 - and when you divide by n it's not going
133:48 - to reveal that something is wrong but
133:50 - when you divide by n minus 1 a sample
133:51 - size of 1 is explicitly forbidden
133:53 - because you cannot divide by 0. okay
133:56 - so that's one way to think that that's
133:59 - one way to think that maybe n
134:03 - minus one is more appropriate um
134:06 - and then uh uh secondly
134:09 - uh what we're actually doing with this
134:13 - number we actually call this
134:14 - we we've given this number a name as
134:16 - statisticians
134:18 - this is known as the variance the sample
134:24 - variance
134:28 - now maybe you recall from previous
134:31 - sections
134:32 - uh my saying that there is a sample
134:35 - meaning you can actually talk about a
134:36 - population mean
134:38 - and there's a sample median and you
134:39 - could talk about a population median
134:41 - and there is a sense in which the sample
134:43 - mean estimates the population mean
134:45 - and the media and the sample median
134:46 - estimates the population median
134:48 - so the sample variance should estimate
134:50 - the population variance
134:52 - there is in fact a population variance
134:55 - but here's the thing though about the
134:56 - population variance um
134:59 - our estimator if we were to divide by n
135:02 - would have a tendency to be too
135:04 - small i mean it would still be close to
135:07 - the population
135:08 - variance but you could be a little bit
135:10 - better
135:11 - by dividing by n minus 1 instead of n if
135:14 - you were to divide by n you'd actually
135:15 - be a little too small
135:17 - so we should divide by minus 1 instead
135:20 - there's a term
135:21 - called biasedness that we will discuss
135:24 - more in chapter six but
135:28 - long story short it turns out that when
135:30 - you divide by n minus one you have an
135:32 - unbiased estimator
135:33 - for the population variance whereas if
135:35 - you divide by n
135:36 - there is a very small bias now that said
135:40 - you can still divide by n and have a
135:42 - reasonable estimator
135:43 - it just will have that bias problem the
135:45 - bias gets really small as you increase
135:47 - the sample size but it is still there
135:50 - so why not just get rid of it um
135:54 - so these are some potential arguments
135:55 - for why you should be dividing
135:57 - by n minus one and later on in ch in
135:59 - that chapter we will actually
136:01 - uh i may actually show that if you
136:04 - compute the expected value and you
136:06 - divide by n minus one you get
136:08 - the sample variance but uh we're a long
136:10 - ways off from that
136:11 - so accept it that you pretty much have
136:14 - to divide by n minus one instead of and
136:15 - although it is still reasonable to think
136:18 - of this
136:20 - as um uh
136:23 - like an average squared distance oh yes
136:26 - another argument for why you should uh
136:28 - be dividing by n minus one
136:30 - uh when you have when you compute the
136:32 - variance
136:33 - there's something you have to do first
136:35 - you have to compute
136:37 - the sample mean you have to compute the
136:41 - sample mean first and there's a penalty
136:43 - that you have to pay for that
136:44 - the term degrees of freedom means
136:48 - that if you this is basically the number
136:51 - of
136:52 - observations in the data set that you
136:54 - are allowed to change
136:56 - um and where you can change those
137:00 - observations um freely
137:04 - and you could still end up with the same
137:06 - sample mean
137:07 - um it because it turns out for
137:10 - basically the reason that i showed up
137:12 - here
137:14 - uh this this this line of reasoning that
137:18 - um if you know n minus one of the
137:21 - observations and you know the sample
137:23 - mean
137:24 - then you know the nth observation that
137:26 - you didn't list out before
137:28 - so the sample mean contains information
137:33 - and the fact that you had to estimate a
137:34 - parameter
137:36 - before you could estimate the sample
137:38 - variance means that you need to divide
137:40 - that it means that there's in some sense
137:42 - a penalty
137:43 - to your sample size um so it's
137:46 - inappropriate to divide by n minus one
137:48 - you now need to do
137:49 - or to divide by n you now need to divide
137:52 - by n minus one
137:54 - all right now here's the thing about the
137:55 - sample variants that we don't like
137:57 - uh think about the units
138:00 - of these things let's say that we were
138:03 - talking about feet or going back to some
138:04 - examples or
138:05 - even for this uh soccer data set that
138:07 - i've seen in a few videos in the past
138:10 - where we're tracking the goals scored by
138:14 - a little league soccer team
138:17 - uh if we were talking about goals then
138:20 - this right here is a goal for a game so
138:22 - it's units or goals the sample mean
138:24 - is also in goals because you add up
138:26 - goals divided by something without units
138:28 - you end up with goals
138:30 - and you have goals minus goals so you
138:33 - still
138:33 - in that difference have goals but then
138:36 - you square
138:37 - and you end up with goals squared what
138:40 - the heck are skull
138:41 - are goals squared that's a unit that
138:44 - doesn't mean anything to us
138:47 - we don't like the fact that in the end
138:49 - the sample variance produces
138:51 - squared units we would rather have
138:54 - an s some measure of spread that is in
138:57 - the same units as the data set
138:59 - and there is such a measure called the
139:02 - sample standard deviation
139:04 - so the sample standard deviation is s
139:06 - which is equal to the
139:07 - square root of s squared so
139:10 - s squared is the sample variance s is
139:13 - the sampled standard deviation
139:17 - so we'll call that sd right
139:20 - i mean it's right there so uh the sample
139:24 - standard deviation
139:28 - so yes
139:32 - since you've taken the square root of
139:35 - the variance
139:37 - uh you now take the square root of gold
139:40 - squared
139:40 - and now end up the with the unit goals
139:44 - which is what you want so the sample
139:46 - standard deviation
139:48 - will be in the same units as the data
139:50 - set and we like that
139:51 - um furthermore it is still reasonable
139:55 - to think of the sample standard
139:57 - deviation as measuring
139:59 - the average distance of an observation
140:02 - from the mean or a typical distance
140:06 - all right so continue on uh there are in
140:09 - fact population analogs to these
140:11 - quantities
140:12 - uh such as the um
140:16 - such as uh the population variance and
140:19 - the population standard deviation
140:21 - and those will be discussions for a
140:24 - later chapter i believe that's chapter
140:26 - uh three so um now
140:31 - when you're computing the sample
140:32 - variance another way to write it
140:34 - if we write if we define s x x as the
140:38 - sum from y equals
140:39 - 1 to n of x i minus x bar squared we
140:41 - could say that the sample variance which
140:43 - is what you need to compute
140:44 - for the sample mean is equal to s x
140:47 - x divided by n minus 1. the thing though
140:50 - is
140:51 - a lot of people don't like to compute
140:53 - the deviations and then square them
140:56 - so compute the mean and then compute the
140:58 - deviations
140:59 - and by subtracting the mean from the
141:00 - observations and squaring them
141:02 - people don't seem to like to do that so
141:04 - when doing stuff by hand
141:05 - it's often easier to use this shortcut
141:08 - formula where you add up the
141:09 - observations
141:10 - squared and then subtract uh the mean
141:13 - squared
141:14 - multiplied with n and it is in fact
141:18 - possible to show
141:19 - and because this is the second time i'm
141:20 - recording this video i'm not going to
141:22 - show it because
141:23 - i'm tired um
141:26 - it is possible to show that these two
141:27 - quantities are the same and i would say
141:29 - i'm going to leave this as an exercise
141:30 - to you if you are curious if you're
141:33 - uh thinking you're probably going to
141:34 - take some more advanced stats classes
141:36 - why don't you take a second to show uh
141:39 - that
141:40 - these two quantities uh the
141:43 - uh some the sum of squared errors
141:46 - and the shortcut formula are the same
141:51 - um but i'm just going to leave it for
141:52 - now that these are in fact
141:54 - the same number so that gives
141:57 - that could help you potentially save
141:59 - some time when computing
142:01 - the sample variance by hand okay
142:05 - so uh let's start out let's now start
142:07 - looking at examples
142:08 - in example 14 we're going to compute the
142:10 - sample variance and sample standard
142:12 - deviation
142:13 - of the soccer game scores so here are
142:16 - the scores let's set again
142:17 - uh length is an r function
142:21 - uh length of so sock or in
142:24 - r is known as a vector and
142:27 - the length of the v of a vector will
142:30 - tell you how many objects are in that
142:32 - vector
142:33 - so um there's also an r function called
142:36 - summary
142:36 - which will give you uh some basic
142:39 - statistical summaries
142:41 - for a data set stored uh in a vector
142:44 - or actually summary is something we'll
142:47 - give you
142:48 - some basic statistical information about
142:50 - lots of things
142:51 - uh but that we're going to leave that
142:53 - for the r lab for now it's just giving
142:55 - us some basic
142:56 - statistics for an a vector and
143:01 - i'm now going to compute uh
143:04 - the sample variance
143:07 - of this uh data set
143:12 - okay so uh i like to
143:15 - create a table uh when computing this by
143:18 - hand if you don't want to watch me
143:20 - compute this by hand
143:21 - uh because it is kind of a tedious
143:24 - calculation
143:25 - if you don't want to watch it this is a
143:27 - part that you can skip over
143:29 - um all right anyway so um
143:32 - we have 12 observations in our data set
143:34 - so i'm going to start numbering off
143:37 - 1 2 3 4 5 6 just to track the
143:39 - observations
143:41 - 7 8 9 10
143:44 - 11 12. okay
143:48 - we have an observation and we have an
143:51 - observation
143:55 - uh hold on
143:58 - another dead spot
144:02 - we have an observation squared okay so
144:06 - uh observations in our data set
144:09 - we had uh nine six
144:13 - five five five
144:16 - uh six two
144:19 - eight uh
144:23 - three four eight one and then we're
144:28 - going to square each of these
144:29 - observations so we'll get 81
144:34 - 36 25
144:38 - three times
144:43 - uh 36
144:46 - for uh 64
144:50 - 9 16
144:54 - 64 and one
144:58 - okay um if you're out also
145:02 - like you kind of want to work on this by
145:04 - hand a little bit but you don't want to
145:06 - completely trivialize the problem by
145:07 - going to r
145:08 - and asking for the variance and standard
145:09 - deviation because it'll just give it to
145:11 - you
145:12 - uh maybe this would be something to work
145:14 - on in excel
145:16 - because i because what i'm basically
145:18 - doing is being
145:19 - a a human excel spreadsheet at the very
145:22 - bottom i'm going to sum up these two
145:24 - columns the first column sums up to 62
145:27 - and the second column consisting of
145:29 - squares sums up to 386.
145:34 - so now i want to compute the sum of
145:37 - squared
145:38 - errors and that's sxx and we have our
145:41 - shortcut formula for computing that
145:44 - that's going to be 386 which is the sum
145:47 - of the squares of the observations
145:49 - minus 12 which is the sample size times
145:53 - the mean
145:54 - all right we need to compute the mean so
145:56 - the mean
145:57 - is going to be the sum of the
145:58 - observations which is 62
146:00 - divided by 12. so in this
146:04 - parentheses i'm going to put 62 over 12
146:07 - and square it and you plug into
146:10 - calculator and the number that you get
146:12 - for the sum of squared errors is 197
146:15 - divided by 3
146:19 - which is as a decimal number uh 65.6
146:24 - where the six is repeating so the sample
146:27 - variance
146:28 - will be the sum of squared errors
146:31 - divided by
146:34 - as a reminder the sample size minus 1
146:36 - which is going to be
146:39 - 65.6
146:42 - divided by 12 minus 1 which is 11 which
146:45 - is equal to
146:46 - 5.99
146:50 - where the 9 6 itself is repeating now
146:54 - this is nice but the thing is the sample
146:56 - variance is in
146:57 - the units of the sample variance is gold
146:58 - squared we don't like that we want to
147:00 - compute the standard deviation too
147:02 - because that's a more interpretable
147:03 - number
147:04 - so the standard deviation is going to be
147:06 - the square root of the variance
147:08 - which is going to be about
147:11 - uh 2.443
147:15 - into three decimal places so you can
147:18 - think of this as
147:19 - your uh daughter's soccer team is
147:22 - varying
147:23 - around their average uh score of about
147:26 - five points
147:28 - but they're they're deviating from that
147:30 - by about two points so on average
147:32 - they'll be about two points away
147:35 - okay all right
147:38 - uh so in r the functions that are
147:41 - responsible
147:42 - for computing these quantities are var
147:44 - and sd r computes the variance and sd
147:46 - computes the standard deviation
147:48 - so var of the soccer data set is
147:50 - basically what i wrote down
147:52 - and the standard deviation it's
147:53 - basically the same thing too
147:55 - all right so um the sample mean so
147:58 - for the sample variance we actually have
148:00 - some nice properties that also translate
148:02 - into properties for uh the standard
148:05 - deviation actually before i continue on
148:07 - i'm going to double check because this
148:10 - scares me okay
148:13 - okay everything is good
148:16 - i'm scared okay um
148:20 - so uh some basic properties
148:24 - uh let's suppose for
148:27 - in this proposition uh let's suppose
148:30 - that
148:30 - we take our data set and then we shift
148:33 - everything by a constant
148:34 - to produce a new data set that's shifted
148:36 - by a constant
148:38 - it turns out that the set the uh
148:41 - sample variance for the new set data set
148:44 - will be the same as the sample variance
148:46 - for the old data set where you didn't
148:47 - shift
148:48 - uh that's a good thing what that means
148:51 - is basically this is in fact
148:52 - a measure of spread if it wasn't a
148:55 - measure of spread
148:57 - uh well basically if this was not the
148:59 - case
149:00 - if the sample variance changed
149:03 - uh by shifting the data then
149:07 - it doesn't seem really fair to
149:10 - uh call it a measure
149:13 - of of spread because it's also capturing
149:17 - location too
149:18 - but the fact that you don't have to uh
149:21 - the fact that it doesn't care about the
149:22 - location or in a way it doesn't actually
149:25 - care about the mean
149:26 - because and you can think of that as
149:27 - because it subtracts the means out the
149:29 - mean out for the data set
149:31 - uh the fact that it doesn't care means
149:32 - it is in fact a bona fide
149:34 - measure of spread and not measuring
149:36 - something else along with it
149:38 - the second proposition says that the
149:41 - variance
149:42 - uh if you were to rescale your data set
149:45 - by
149:46 - c uh the variance will scale by
149:50 - c squared and the standard deviation
149:53 - will scale by the absolute value of c
149:56 - so um the standard deviation is always
149:59 - going to be positive
150:01 - uh the variance will always be positive
150:03 - um
150:04 - so when you rescale the data set it's
150:06 - not going to
150:08 - the whether you multiply by a positive
150:11 - or a negative number doesn't actually
150:12 - matter
150:13 - and also it tells you that um if like
150:16 - you can think of this as saying
150:17 - something about
150:18 - uh unit conversions uh because remember
150:21 - that unit conversions
150:23 - uh generally and are multiplicative
150:25 - operations
150:27 - if you wanted to change the units of the
150:30 - standard deviation um
150:32 - you could do so by just
150:35 - multiplying the original standard
150:37 - deviation by whatever unit conversion
150:39 - formula you have
150:40 - and also this is basically telling us
150:41 - what i was saying before
150:43 - that the variance is in units squared
150:46 - but the standard deviation is in just
150:48 - the same units as the data set
150:52 - so these are good properties to be aware
150:54 - of
150:55 - um okay so
150:58 - the uh the sample variance and standard
151:01 - deviation
151:02 - uh these are one uh these are
151:05 - these are these are one class of uh
151:07 - estimator of spread they're not the only
151:09 - ones
151:10 - oh by the way you we did have this
151:12 - discussion
151:13 - when talking about measures of location
151:16 - about
151:17 - biasedness no no not biasedness um
151:21 - sensitivity to outliers
151:24 - it turns out that the sample mean
151:28 - and the sample standard the sample
151:29 - variance and sample standard deviation
151:31 - are also sensitive to outliers in fact
151:33 - they are more sensitive to outliers than
151:35 - the mean is
151:36 - so they care a great deal about outliers
151:38 - too um
151:40 - just throwing that out there and you can
151:42 - kind of tell by
151:43 - looking at those formulas since
151:46 - uh when you look at them they're
151:49 - basically
151:49 - means right they're averages or at least
151:52 - the variance
151:53 - looks like an average and the standard
151:55 - deviation is the square root of an
151:57 - average
151:58 - so uh if you end up having a very large
152:01 - error then that's going to make your
152:04 - variance very large
152:06 - so it's sensitive to outliers just
152:08 - mentioning that uh
152:10 - another measure for spread is known as
152:14 - the fourth spread or sometimes like in
152:16 - math 1070 we call the interquartile
152:18 - range
152:18 - it is the third quartile minus the first
152:21 - quartile
152:22 - and we're going to denote it in this
152:24 - class with fs
152:25 - this is another measure of dispersion so
152:27 - let's compute the fourth spread for the
152:29 - soccer game scores
152:30 - uh we already computed in uh
152:33 - in a previous uh video uh
152:37 - the third and first quartile for the
152:39 - soccer game
152:40 - for the soccer games so the fourth
152:41 - spread
152:45 - will be the third quartile minus the
152:48 - first quartile
152:49 - which actually turns out for this data
152:51 - set to be eight minus four
152:54 - which is equal to four
152:58 - okay so the fourth spread is
153:01 - in and of itself a measure of dispersion
153:04 - uh and one way statisticians might use
153:08 - the fourth spread is as a tool for
153:10 - outlier detection remember we care a
153:11 - great deal about outliers
153:13 - uh if there's an out we would like to
153:15 - have outlier detection tools
153:17 - because if there is an outlier in this
153:18 - data set we would like to investigate it
153:20 - further
153:21 - and decide how we should approach it and
153:23 - why the outlier is there
153:25 - outliers are very interesting aspects of
153:27 - data sets so we would like to be able to
153:29 - detect them
153:30 - so we might call an observation
153:33 - that is further than one and a half
153:35 - times the fourth spread from its nearest
153:37 - quartile a mild outlier
153:39 - so as an example uh let's suppose we
153:42 - have a data set our data set looks
153:43 - something like this here's kind of a
153:45 - a dot plot sketch of our data set and
153:48 - we've also got
153:49 - a couple observations over here so those
153:52 - two observations visually
153:54 - look like outliers what the for
153:57 - let's suppose that we have the
154:01 - uh oh
154:04 - i didn't realize that was the thing okay
154:06 - uh let's suppose
154:09 - that uh our first and third quartiles
154:12 - are here and here
154:14 - okay um if that is the case
154:18 - uh the fourth spread or the iqr
154:22 - is going to be the distance between
154:25 - uh those two quartiles so we'll call
154:27 - this q1
154:28 - q3 uh the distance between those will be
154:31 - the fourth spread
154:32 - so according to this rule how you detect
154:35 - an outlier is you take this quantity
154:37 - and then increase it by uh one so one
154:40 - and a half
154:42 - and go beyond and you're gonna
154:45 - like see compare an observation to its
154:48 - nearest quartile so these are going to
154:51 - be close to the third quartile because
154:52 - they're above the third quartile
154:54 - um and you um
154:58 - compare see if those observations
155:01 - are one and a half times the iqr away
155:05 - from their nearest quartile
155:07 - and if they are beyond that range then
155:09 - they are candidates to be
155:10 - outliers so these are now starting to
155:13 - look like at least
155:14 - mild outliers but in fact if we were to
155:18 - uh double that quantity so three times
155:22 - the fourth spread
155:23 - um so that would look like this
155:27 - turns out that they are beyond three
155:29 - times the fourth spread as well
155:31 - so now these are looking like extreme
155:33 - outliers
155:35 - we could also do the same thing on the
155:37 - left hand side of the data set
155:38 - look for outliers that are you know
155:41 - numbers that are really small there's
155:42 - nothing that says that outliers have to
155:44 - always be large numbers they can also be
155:46 - really small numbers too uh and there's
155:49 - no
155:49 - outliers on the left-hand side of this
155:51 - data set
155:53 - i should point out it's tempting for
155:55 - students to think that what i just gave
155:56 - you as a definition for outliers
155:59 - the word outlier is intentionally vague
156:01 - because there's many different ways you
156:03 - could define an
156:04 - outlier this is one such definition
156:08 - or one such a criterion for deciding if
156:11 - something is an outlier
156:12 - this criterion would not work if we were
156:16 - to go
156:16 - into two dimensions it is a
156:18 - one-dimensional
156:20 - one-dimensional approach not a
156:22 - two-dimensional approach
156:23 - and you can still have uh outliers
156:27 - in two dimensions or in bivariate data
156:30 - and
156:30 - uh um you can still have outliers there
156:34 - and they're going to behave
156:36 - like there's more possibilities the
156:37 - moment you've moved on to a plane
156:39 - as opposed to just a number line more
156:41 - ways for things to be outliers
156:44 - um so um
156:48 - i i'm i'm i'm hesitant to allow to just
156:52 - let students think that this is what an
156:54 - outlier is
156:55 - there's actually different ways to think
156:57 - about outliers we could come up with
156:58 - different definitions
157:00 - based off of our problem and how we want
157:02 - to approach it
157:03 - so so we could we could choose a
157:07 - procedure
157:08 - that is tailored to our problem to
157:10 - define outliers so that it's most useful
157:12 - to us
157:13 - so this is what we're using in this
157:14 - class but it is far from
157:16 - like what we'd always use okay and it's
157:20 - not really the definition of an outlier
157:21 - we would just say
157:22 - that an outlier is a point that that
157:25 - seems unusual to the other points that
157:28 - seems to be distant in some way
157:30 - from the other points or it doesn't seem
157:32 - to follow the same pattern as the rest
157:34 - of the data
157:35 - okay so moving on into example 16 use
157:38 - the fourth spread
157:39 - to detect outliers and soccer game
157:41 - scores what is the minimum score needed
157:43 - for a data point to be a mile outlier or
157:45 - an extreme outlier
157:47 - so the force spread as you may recall
157:49 - from above
157:50 - was four so 1.5
157:53 - and i don't want that green that color
157:56 - all right so 1.5
157:58 - times the fourth spread is going to be 4
158:01 - times 1.5 which is going to be 6
158:04 - and 3 times the fourth spread is going
158:07 - to be
158:08 - 12. so uh
158:11 - let's see what it would take for
158:12 - something to be considered at least a
158:14 - mild outlier
158:17 - to be mild you would have to possibly
158:20 - exceed the third quartile plus
158:24 - 1.5 times the fourth spread
158:27 - which is going to be eight plus six
158:31 - which is 14. there were no
158:34 - double digit scores in our soccer data
158:37 - set
158:38 - so that means that there were no
158:39 - outliers at least on the positive end
158:41 - and as for the negative end uh in order
158:44 - for something to be so small that it's
158:45 - an hour it would have to be less than q1
158:48 - minus uh 1.5 times the force spread
158:54 - which is going to be uh that's going to
158:57 - be 4 minus 6
158:59 - which is negative 2. well
159:02 - negative soccer scores are impossible so
159:04 - there's not going to be any mild
159:06 - outliers on the left hand side which
159:07 - means that there are no outliers in this
159:08 - data set
159:10 - now that's it let's let's go ahead and
159:11 - continue on just just for fun
159:13 - let's see what it would take for
159:14 - something to be an extreme outlier
159:20 - to be an extreme outlier you would have
159:22 - to exceed
159:23 - q3 plus 3fs
159:26 - which is 8 plus 12
159:31 - which is uh 20. so in other words the
159:34 - other team didn't show up
159:36 - to be on the left hand side you'd uh to
159:39 - be an outlier on the left-hand
159:40 - side you have to be less than q1 minus
159:42 - three fs
159:44 - which is um uh four minus 12
159:50 - uh which is negative which is negative
159:53 - eight
159:56 - and no way that's going to happen so
159:57 - there are no outliers
160:00 - in our data set
160:07 - okay so uh
160:13 - there's another visualization method
160:15 - that i would like to discuss
160:16 - that we weren't able to discuss in
160:18 - section two
160:19 - the reason why is because this is a box
160:21 - plot and box plots require
160:24 - uh the five number summary which is the
160:27 - minimum max so the five number summary
160:30 - is the minimum
160:31 - uh maximum median first and third
160:34 - quartiles
160:34 - so you need to have that in order to be
160:36 - able to compute a box plot
160:38 - and create a box plot that's why we
160:39 - didn't talk about before because we
160:41 - hadn't actually talked about those
160:42 - things
160:43 - uh so um so for
160:46 - a box plot we first compute those
160:49 - quantities
160:50 - uh on a number line which could
160:53 - this could be a vertical number line or
160:55 - it could be a horizontal number line if
160:57 - you want
160:58 - uh box plots oriented horizontally or
161:01 - oriented vertically
161:03 - either one is fine whatever whatever
161:06 - suits your needs do whatever you want
161:09 - okay but on a number line
161:13 - let's say something like this we're
161:14 - going to draw a box so we've got
161:17 - the first quartile and the third
161:20 - quartile
161:21 - we're going to draw a box whose ends are
161:25 - at those quartiles we're also going to
161:28 - draw a line
161:29 - in that box corresponding to the
161:31 - location of the median
161:34 - we will then draw what are known as
161:36 - whiskers that extend out
161:38 - to the maximum and to the minimum
161:44 - so the whiskers will stand out to extend
161:46 - out
161:47 - to the extrema of the data set
161:51 - um and that's that's a box plot
161:54 - now i should point out that r does
161:58 - not draw a box plus this way by default
162:01 - r does something different r will
162:03 - actually try to detect outliers and it
162:05 - will
162:06 - draw the whiskers out to the
162:07 - observations that are not
162:09 - outliers so the largest observation that
162:12 - is on outlier and the smallest
162:13 - observation that is not an
162:15 - outlier the outliers are treated
162:17 - differently
162:18 - they get their own points in the box
162:21 - plot
162:21 - kind of like with the dot plot
162:25 - uh that's more complicated to do and i'm
162:27 - not going to ask you to do that
162:28 - uh using just the five number summary
162:30 - and extending out to
162:32 - the maxima is fine for me and i feel
162:34 - like that if you were ever
162:35 - to draw a box plot by hand you should
162:37 - just keep things simple because you can
162:39 - it's not too hard to compute
162:41 - a five number summary if you especially
162:43 - if you had say
162:44 - um uh if you had like a stem-and-leaf
162:47 - plot
162:48 - but a box plot like competing
162:51 - the outliers is a bit much so
162:55 - a box plot on its own i mean it's okay
162:58 - but a box plot really shines
163:00 - when there are other box plots with it
163:02 - and when you have that you can now start
163:04 - drawing comparative box plots
163:06 - and when you have comparative box plots
163:08 - you can start to say things about the
163:10 - relationships
163:11 - amongst different groups that uh
163:15 - like if you want to compare different
163:16 - data sets you may have a data you may
163:18 - have two data sets for
163:19 - two similar but not the same populations
163:23 - uh like for example men and women men
163:25 - and women are both human
163:26 - uh but if you were to compare height you
163:29 - would probably want to differentiate
163:30 - between men and women
163:31 - so you could have a box plot for men's
163:34 - heights
163:37 - and you can have a box plot for women's
163:39 - heights
163:41 - and then you can make comparisons and
163:43 - you could compare in this case
163:45 - this looks like uh if if this were in
163:47 - fact talking about men's and women's
163:49 - heights
163:49 - this would be suggesting that men tend
163:51 - to be taller than women
163:53 - um so one thing that you could do when
163:56 - looking at a comparative box plot is
163:58 - compare look the location of the boxes
164:00 - you can also compare the spreads of the
164:02 - boxes so we for example
164:04 - if we saw one box plot that looks like
164:06 - this for one group and a box plot that
164:08 - looks like this for another group
164:10 - we might say that those two groups
164:12 - certainly have different spread
164:15 - okay so you can start making comparisons
164:17 - that are more easily made with with
164:20 - plots such as these and if you were to
164:21 - say
164:22 - try to overlap density plots or
164:24 - something
164:25 - um so comparative box plots are very
164:27 - nice because they allow you to
164:29 - at a glance compare two different groups
164:32 - and their distributions so let's go
164:35 - ahead
164:36 - and start creating some box plots
164:39 - uh in this example uh we're using a data
164:42 - set from
164:43 - r uh in this example we're studying
164:46 - we're studying the tooth growth of
164:48 - guinea pigs that were given a vitamin c
164:49 - supplement
164:50 - via orange juice at three different
164:52 - dosage levels
164:53 - uh here's a bunch of r code that takes
164:56 - the tooth growth data set
164:58 - and transforms it into a format that is
165:02 - um uh nice for this problem where
165:05 - uh it did not look like this before it
165:08 - go ahead and look at the tooth growth
165:09 - data
165:09 - it does not look like this at all for
165:11 - starters the
165:13 - tooth growth data set it also includes a
165:16 - group where the guinea pigs were given
165:18 - vitamin c
165:19 - like a vitamin c supplement directly
165:21 - rather than through via orange juice
165:24 - and we've completely excluded that group
165:26 - using this filter command
165:28 - these are known as pipes they are
165:31 - part of the dipler package um
165:35 - so we um filtered so that we were
165:39 - looking at only orange juice
165:40 - we selected the length and the dosage uh
165:44 - as the variables we were interested in
165:46 - and then did some other stuff
165:47 - so that the data came in a format that i
165:51 - liked which is where
165:54 - it's ordered from smallest to largest
165:57 - for each of these three groups and we
165:58 - have the half dosage group
166:00 - full dosage group and double dosage
166:02 - group
166:04 - okay so uh
166:07 - all right so and the data set is ordered
166:09 - which means it's going to be
166:10 - uh somewhat easy to compute a five
166:13 - number summary for each of these three
166:15 - groups
166:15 - so for example so for instance uh the
166:18 - last row is going to be the maximum for
166:20 - the three groups
166:20 - and the first row is going to be the
166:22 - minimum for the three groups
166:24 - as for the other quantities we also need
166:28 - the median
166:29 - so there are 10 observations for each
166:31 - group
166:33 - so the median is going to be the average
166:37 - of the fifth and the sixth rows so
166:40 - uh the medians after we compute those
166:43 - averages
166:46 - or or midpoints if you prefer the
166:48 - medians for
166:50 - the half dosage group uh its median
166:53 - is uh 12.25
166:56 - for the full dosage group it's going to
166:58 - be 23.45
167:01 - and for the double dosage group it's
167:04 - going to be
167:06 - 25.95
167:09 - okay so those are the medians
167:12 - now we need to compute the first
167:14 - quartile remember what we do is we split
167:17 - the data set in half and then look
167:18 - at the median for the smaller data set
167:21 - or the lower data set
167:23 - this will be the first quartiles and the
167:26 - median for
167:26 - the third quarter data set remember that
167:28 - both these data sets have five
167:30 - observations each
167:31 - after we do the splitting so the median
167:33 - for the upper data set that will be the
167:35 - third quartile
167:37 - okay so we now have everything we need
167:39 - to start constructing our box plots
167:43 - okay so i'm going to construct these
167:47 - this by hand i don't want that
167:51 - let's make it black isn't that a song
167:55 - okay oh yeah i want a painted black yeah
167:59 - that's right
168:00 - that is a song all right um i want to
168:03 - paint it black
168:04 - uh anyway uh so um
168:07 - i have the half dosage group the full
168:11 - dosage group
168:12 - and a the double dosage group
168:16 - i'm going to have my box plot end at
168:19 - 31 up here and we're gonna start down
168:22 - here
168:23 - at eight so we're gonna go eight uh
168:26 - nine ten eleven twelve 13.
168:31 - okay uh 14 15
168:34 - 16 17 18.
168:39 - and then we go 1920.
168:44 - uh 21 22 23 24
168:48 - 25
168:53 - uh 26 27 28
168:56 - 29 30 31 okay
169:00 - all right so we've got our scale uh so
169:03 - for our first
169:04 - group uh let's zoom out a little bit
169:11 - so for the first group the median
169:14 - of the minimum was 8.2 so
169:18 - minimum of 8.2 we'll put a dot right
169:20 - there
169:22 - uh then we go
169:26 - to the first quartile so that's going to
169:29 - be
169:31 - 9.7 that's about there
169:35 - um then we've got 12.25
169:40 - that's about there uh
169:43 - q3 is 16.5 that's about
169:47 - 13. that's about there
169:52 - and finally the maximum is 21.5
169:56 - so that is going to be about
170:00 - there okay
170:04 - so then uh we have a box
170:08 - the median and the whiskers
170:13 - all right so there's our first box plot
170:17 - all right so now for full dosage uh
170:20 - scrolling up a little bit
170:23 - so for full dosage the minimum is at
170:27 - uh 14.5 which is about there
170:32 - uh then we have a quartile at 20
170:35 - which is about there
170:38 - uh the median's at 23.445
170:42 - so that's about there uh
170:45 - q3 is at 25.8 so that's about
170:50 - there um and then
170:54 - uh the maximum is at 27.3
170:58 - so that's about there okay so draw the
171:02 - box
171:04 - the line for the median and draw out the
171:07 - whiskers
171:09 - okay and finally for the double dosage
171:12 - group the minimum is at 22.5 which is
171:15 - about there
171:16 - uh the first quartile is at 24.5 which
171:19 - is about there
171:20 - uh the median is at 25.95 which is about
171:24 - there
171:25 - uh the third quartile is at 27.3 which
171:28 - is about
171:29 - uh there and the maximum is at
171:33 - uh 30.9 which is about there so
171:36 - draw the box draw the line for the
171:39 - median
171:40 - extend out the whiskers and there we go
171:43 - all right
171:43 - and now we have a box plot a comparative
171:46 - box plot
171:47 - and what can we see we can see that in
171:50 - fact increasing the dosage does seem to
171:52 - increase the tooth growth length
171:54 - we also see that there's much more
171:56 - spread in the half and full dose than
171:57 - there is for the double dose which is an
172:00 - interesting fact
172:01 - uh all right so moving on uh there is um
172:04 - an r function called box plot
172:06 - that can construct these box plots for
172:08 - you this is largely in agreement with
172:10 - what we drew
172:11 - um and as a reminder
172:14 - r doesn't by default uh produce box
172:17 - plots in the way that i just described
172:19 - where it draws whiskers out
172:20 - to the minimum and the maximum it does
172:23 - something a little bit different
172:24 - where it will um uh draw it out to
172:28 - the largest and smallest observations
172:29 - that are not outliers and then draw the
172:31 - outliers as their own individual points
172:34 - okay uh there's actually one more
172:36 - visualization that's similar to a box
172:38 - plot that i would like to discuss
172:40 - um i didn't discuss it in the lecture
172:43 - notes because
172:44 - you can't really draw it by hand
172:48 - uh but i've got a computer in front of
172:50 - me okay very quickly
172:52 - okay everything seems to be fine okay um
172:56 - so you can't really draw it by hand uh
172:58 - because
173:00 - oh what was that
176:23 - hey students
176:26 - let's get started with the chapter on
176:29 - probability
176:30 - probability is the mathematical study of
176:32 - randomness and uncertain outcomes
176:35 - so the subject in fact may be about as
176:37 - old as calculus at some level
176:39 - humans have known about probability for
176:41 - a very long time
176:42 - it's just it wasn't until around the
176:45 - time of probability that we saw some of
176:46 - the first
176:47 - uh semi-rigorous treatments of
176:50 - probability
176:51 - and then probability really became a
176:54 - serious mathematical subject
176:56 - around uh the beginning of the 20th 20th
176:59 - century
177:00 - uh when a mathematician by the name of
177:03 - kolmogorov
177:04 - rooted probability theory in in the
177:08 - in some real analysis theory so he
177:10 - developed a set of axioms that made it a
177:12 - rigorous
177:13 - uh mathematical subject in its own right
177:15 - and here we are today
177:17 - and statistics relies very heavily on
177:20 - probability
177:21 - we've seen in the previous chapter
177:24 - quantities such as the mean and the
177:26 - median
177:27 - we saw all these sample statistics we
177:29 - discussed the ideas
177:30 - of a sample and a sample's relationship
177:33 - to
177:34 - a population but it's hard you can't
177:37 - really say much more than that and
177:39 - really can't have
177:41 - a rigorous discussion about different
177:45 - uh sample statistics without having
177:48 - a probability theory to back it up so
177:52 - we're gonna start with that right now
177:54 - we're gonna start with section one on
177:56 - sample spaces and events so we start out
177:58 - with the idea of an experiment
178:01 - and experiment is an activity
178:04 - or process with an uncertain outcome
178:07 - examples of experiments including
178:09 - flipping a coin or flipping a coin until
178:13 - the coin lands heads up or you could
178:16 - have
178:17 - rolling a die
178:20 - a six sided die
178:24 - or a rolling two six sided die
178:28 - or you could even have something a bit
178:30 - more abstract such as
178:32 - uh the time in the morning that you wake
178:35 - up that can also be understood
178:37 - via probability theory
178:40 - so when we have an experiment that we
178:43 - have described
178:44 - narratively in a sense so i say i'm
178:47 - going to flip a coin or i'm going to
178:49 - flip a coin until it lands heads up
178:51 - after we have an experiment we need to
178:52 - describe the sample space
178:55 - which we are going to denote in this
178:57 - class with the letter s
178:59 - although i should point out that at
179:01 - least in my experience
179:02 - omega the greek letter omega is
179:06 - more common um notation for the sample
179:10 - space
179:11 - um but this is fine uh s is fine
179:15 - um so this will be the sample space
179:19 - is the set of all possible outcomes
179:22 - of the experiment the sample space is
179:25 - defined
179:26 - by the person who's developing this
179:29 - probability model
179:31 - so it basically you say what the sample
179:33 - space is and you're going to pick a
179:35 - sample space that seems appropriate to
179:36 - the phenomena that you wish to describe
179:39 - a set is very loosely defined as a
179:41 - collection of
179:43 - of objects uh actually this definition
179:46 - of set
179:47 - is bad um because it's possible
179:50 - using just the idea of a collection of
179:53 - objects
179:54 - to construct impossible sets uh sets
179:57 - that are
179:57 - like it's impossible in the sense of
180:00 - being contradictory to itself
180:02 - so uh there's this uh area of
180:04 - mathematics called axiomatic set theory
180:07 - that actually develops a rigorous notion
180:09 - of sets that
180:10 - largely allows for sets that we'd like
180:12 - to think of but honestly
180:14 - uh for our purposes this is f this is
180:17 - definitely overkill
180:18 - uh just thinking of set as a collection
180:20 - of objects is uh
180:22 - fine for us events are subsets of the
180:25 - sample space
180:26 - defining possible outcomes of an
180:28 - experiment
180:31 - we automatically get an event
180:34 - called or a subset called the empty set
180:37 - or the null
180:38 - event uh which is noted with this
180:40 - notation
180:41 - this is a set with no members it can be
180:44 - thought of as an event
180:46 - of as the event where nothing happens
180:49 - and that is precisely how you should
180:51 - think about it i might uh
180:52 - create a separate video describing
180:57 - what precisely the empty set is and kind
180:59 - of try to dispel
181:00 - some inclinations of students to try to
181:03 - assign some
181:04 - deeper meaning to what to the empty set
181:06 - it's like no no no
181:08 - the empty set is a set with nothing in
181:10 - it and you really cannot call it
181:11 - anything else
181:12 - it's more uh a necessity of the
181:15 - mathematics than it is anything that you
181:16 - can
181:17 - honestly interpret so
181:20 - let's get started uh with an example
181:23 - we're going to define a sample space for
181:25 - the experiment of flipping a coin
181:27 - we're going to list all possible events
181:29 - for this
181:30 - experiment let me just get caught up in
181:33 - my notes
181:34 - uh that i have a side here and uh all
181:37 - right so
181:38 - i'm going to say uh that this sample
181:41 - space
181:42 - uh which i'm going to call s uh
181:46 - what are going to be the possible
181:48 - outcomes of flipping a coin
181:50 - well despite what might be physically
181:52 - possible like i actually have seen coins
181:55 - uh not not necessarily like mint coins
181:58 - uh but things very coin like that
182:00 - end up landing on their side but that is
182:02 - not going to be allowed here
182:03 - there's only two possible outcomes heads
182:07 - and tails and notice that
182:11 - notice the curly braces often sets what
182:14 - we are talking about
182:15 - is a set generally sets are going to be
182:18 - denoted with uh curly braces another
182:22 - important fact about sets is that the
182:25 - objects in sets only appear
182:28 - once generally if you were so like for
182:30 - example this set
182:32 - is the same as h
182:35 - h uh t so at at some level there's
182:38 - uniqueness in a set you get imposed
182:40 - uniqueness
182:40 - so if you list heads twice it's the same
182:43 - set
182:44 - okay um and additionally the ordering of
182:49 - how i write stuff down in a set does not
182:51 - matter
182:52 - so i could have written tails heads and
182:54 - it would have been the exact same
182:56 - set here so but yeah now we have the
183:00 - sample space and this is what it is by
183:02 - definition i decided this is the sample
183:04 - space for my experiment
183:06 - and i decided this because i believe
183:08 - that this sample space is going to be
183:10 - the appropriate sample space
183:12 - uh for my problem so i'm saying that
183:14 - there's two possible outcomes of this
183:16 - experiment you the coin either lands
183:17 - heads up or it lands tails up
183:20 - uh and i so next i'm going to list some
183:22 - possible events
183:24 - for this experiment so an event is a
183:27 - subset of the sample space
183:30 - okay so what is one possible subset
183:33 - uh well
183:37 - one possible subset is the sec that
183:40 - contains
183:41 - only h right so only heads so this is
183:45 - the event
183:47 - or the subset where when you flip the
183:49 - coin it lands
183:50 - heads up and similarly we have the event
183:54 - where it lands tails up and some authors
183:58 - like to call
184:00 - um sets like these sets with only one
184:04 - element
184:04 - simple events because they have uh only
184:07 - one outcome even komogorov in his book
184:09 - on probability theory uh denoted
184:13 - uh had the notion of simple events where
184:16 - it has only one
184:17 - outcome corresponding to something that
184:18 - you would actually observe
184:20 - um i i i personally don't really care
184:23 - for the distinction myself
184:25 - uh but students might like it uh
184:28 - we can also have the event heads or
184:31 - tails
184:32 - so what's a what is a possible outcome
184:34 - for this well we get get heads or tails
184:36 - basically um and notice right here that
184:40 - this
184:41 - is the same as the sample space so i
184:44 - could have said
184:45 - the sample space is an event
184:48 - and generally that's true because what
184:50 - does it mean
184:52 - for a set to be a
184:55 - subset of another set what does it mean
184:58 - for
184:59 - for something to be a subset it means
185:01 - that
185:02 - every element in a set
185:05 - is present in another set or
185:09 - equivalently there are no elements
185:12 - in the subset that are not present
185:16 - in let's call it the parent set right
185:19 - so equivalently you cannot find an
185:21 - element in the subset that isn't present
185:24 - in this uh containing set okay
185:28 - so by that definition the sample space
185:30 - is a subset of itself
185:32 - since every element in the sample space
185:35 - is also present in the sample space
185:38 - so it seems almost tautologically true
185:41 - and yet at the same time it matters it
185:44 - matters a great deal
185:45 - that one set that you automatically get
185:48 - when you de
185:49 - one event you automatically get we need
185:50 - to find a sample space is the sample
185:52 - space itself
185:54 - and there is one more event
185:57 - that we have the moment we define the
186:00 - sample space
186:01 - the empty set that is also
186:04 - a subset of the sample space now it
186:07 - seems really weird because you ask
186:09 - yourself
186:10 - how is it that a set with nothing in it
186:14 - another way
186:15 - to write the sample space is like this
186:17 - where you write two curly braces but
186:19 - with
186:19 - nothing in between them because the
186:21 - empty set has nothing in it
186:23 - okay um so you ask yourself
186:28 - how is it that the sample space every
186:30 - element of the sample space is also
186:32 - every element of the empty set is also
186:33 - in the sample space it has no elements
186:36 - well exactly because by this alternative
186:38 - way to think about
186:40 - what it takes to be a subset there is
186:42 - nothing in
186:43 - the empty set that isn't present in the
186:46 - sample space
186:47 - because there's nothing in the empty set
186:49 - therefore
186:50 - you automatically get that you
186:52 - automatically get
186:53 - that the empty set is a subset of the
186:56 - sample space
186:57 - and therefore the empty space is an
187:00 - event
187:02 - now i i i'm kind of implying here
187:05 - that what it takes for something to be
187:07 - an event
187:08 - is that this set
187:12 - needs to be a subset of the sample space
187:14 - so in other words what it takes to be an
187:15 - event is that you simply be a subs of
187:17 - the sample space
187:18 - technically that is not true but
187:22 - the reason why it's not true is going
187:24 - well beyond the scope of this class
187:27 - and uh you might see a little bit of it
187:30 - in probability theory
187:32 - and it would become much more important
187:34 - if you were to take
187:35 - graduate level probability theory
187:37 - measure theoretic probability theory
187:38 - technically it is not true that every
187:40 - subset
187:41 - um of the sample space is an event
187:44 - that said it's really hard to imagine a
187:48 - subset that is a one so basically if you
187:50 - imagine the subset and you didn't
187:51 - actually try
187:52 - to break the theory if you imagine a
187:55 - subset it's probably an event
187:57 - so uh it's for now
188:00 - it's probably fine although i'll
188:02 - probably add a little more
188:04 - rigor to the notion of what it takes to
188:05 - be an event um
188:07 - or do i do that only in like a class
188:10 - devoted or probability theory i'm not
188:11 - really sure if i talk about in this
188:12 - class
188:13 - um we'll see we'll see we'll have to see
188:16 - as we go through the notes
188:18 - all right uh so that's that um by the
188:21 - way i should
188:21 - probably uh mention something uh here
188:25 - we might give names to these events like
188:28 - we might call this first
188:29 - event eh we might call this
188:33 - second uh simple event e
188:36 - t uh to say that one sub
188:39 - one set is a subset of another we can
188:42 - use the notation say
188:44 - uh e h uh is a subset uh and i like to
188:48 - put a line underneath
188:49 - um so to say that it could possibly be
188:52 - the same as
188:53 - uh the sample space so we have this
188:56 - or ah i mean a lot of people also will
188:59 - just write it like this
189:00 - uh so we set have that the event e h is
189:04 - a subset of the sample space
189:06 - e t is a subset of the sample space
189:11 - um s is a subset of itself
189:15 - and the empty set is a subset of the
189:18 - sample space
189:22 - okay so uh
189:25 - continuing on next example define a
189:28 - sample space for the experiment of
189:29 - rolling
189:30 - a six-sided die list three events
189:33 - uh based on this sample space okay
189:36 - so i'm going to say because i think this
189:38 - is the best way to understand this
189:40 - problem
189:40 - uh that it's going to consist of
189:44 - outcomes uh one two three four five six
189:46 - but i'm not going to write the numbers
189:48 - one two three four five six and i have
189:50 - and i have reasons for not writing the
189:52 - numbers
189:52 - the reason is there is nothing in
189:54 - probability theory that requires that
189:56 - your sets contain numbers
189:58 - nothing says that it just says some
189:59 - object very loosely defined
190:02 - so instead of writing numbers i'm going
190:04 - to write
190:05 - little dice faces so i have a dice face
190:09 - that has one pip a dice face that has
190:12 - two pips
190:13 - a dice face that has three pips a dice
190:15 - face that has four pips
190:18 - a dice face that has five pips
190:22 - and finally a dice face that has six
190:25 - pips
190:31 - okay that that looks like six pips all
190:34 - right
190:35 - so that's my sample space and now i'm
190:37 - going to list three events based on this
190:38 - sample space
190:40 - so one event
190:44 - uh one event is the empty set i'm just
190:46 - gonna say it right now
190:47 - the empty set is this upset so here we
190:49 - go we get one we get one event for free
190:53 - i've promised myself i'm not gonna do
190:54 - that here that's my preferred notation
190:57 - i'm not sure if it's the notation used
190:58 - in the book and not everybody uses the
190:59 - same notation
191:00 - that's something that you need to get
191:01 - used to in mathematics you need to pay
191:03 - attention to what notation someone is
191:04 - using
191:05 - because despite the fact that the books
191:08 - and sometimes the instructors make it
191:10 - look like there's one set of notation
191:13 - uh for a subject that's just not true
191:16 - and that's and that's including
191:18 - probability theory and statistics it's
191:19 - just not true that there is one set of
191:21 - notation
191:22 - and you need to pay attention to what
191:24 - someone is actually using to
191:26 - mean their stuff um anyway um
191:31 - so the empty set is a subset of the
191:33 - sample space the sample space is a
191:35 - subset of the sample space
191:36 - both of these are uh valid events
191:40 - okay but they're almost trivial at this
191:42 - point because these are events that you
191:44 - automatically get
191:46 - so what's something that's a little bit
191:47 - more interesting uh well we could have
191:50 - the event where you roll a four
191:53 - uh this is one of those simple events
191:56 - that you may have heard of
191:57 - so four is an event and it's a subset of
191:59 - the sample space
192:01 - uh what's another one that we could have
192:03 - well we could
192:04 - have the event where uh
192:07 - you have an even number of pips
192:13 - that's a valid event uh because
192:16 - in fact even though i've written this
192:17 - down in english and often it's useful to
192:20 - write down
192:20 - sets in english sentences what this
192:23 - actually translates
192:24 - into is a set with three elements you
192:26 - have the
192:27 - set containing the dice with a with two
192:30 - pips
192:31 - on its face uh four pips on its face
192:34 - and six pips on its face
192:40 - these are all uh come on you come on i
192:43 - said six
192:44 - pips don't make me a fool you stupid
192:46 - laptop
192:52 - some people
192:55 - there we go so this is also
192:58 - a subset of the sample space this is
193:00 - also an event
193:02 - um okay
193:06 - so um there we go i've given you
193:09 - four events in fact uh so you got a
193:12 - little bit more than what you paid for
193:13 - anyway uh example three define a sample
193:17 - space
193:17 - describing the event the experiment of
193:19 - flipping a coin
193:20 - until it lands heads up list five events
193:23 - from this sample space ooh
193:25 - so what does this look like what does
193:28 - this look like
193:29 - well i'm going to say here's my sample
193:31 - space
193:32 - and uh well what's one possibility
193:36 - uh flip a coin until it lands heads up
193:39 - well it could land heads on the first
193:41 - flip so you flip the coin it lands heads
193:43 - and then you stop
193:45 - uh you could then fl you could another
193:47 - outcome is you flip the coin and it
193:49 - lands tails
193:50 - so you haven't gotten heavy yet so you
193:51 - need to flip it again and then it lands
193:52 - up
193:53 - heads the second time so tails heads is
193:56 - another possible
193:57 - possibility uh tails tails heads
194:01 - is a third possibility tails tails tails
194:04 - heads
194:05 - is yet another possibility and so on
194:09 - this set has an infinite number of
194:11 - elements because
194:12 - in principle if i have if i have an
194:16 - outcome where uh where you've got so
194:19 - many tails and the last one is ahead
194:21 - it's possible to also have an outcome
194:24 - where you
194:24 - where in order where before you got to
194:26 - that point you flipped the coin you got
194:28 - tails once
194:29 - so for every outcome you can find a next
194:31 - outcome
194:32 - in a sense so since there's always going
194:34 - to be an x outcome this set
194:35 - must have an infinite number of members
194:39 - now there is actually an interesting
194:41 - wrinkle that co that came up in
194:43 - one of my lectures on this is there an
194:45 - outcome
194:46 - in this sample space that corresponds to
194:49 - flipping the coin and it never comes up
194:51 - with heads
194:53 - the answer is no the answer is
194:57 - in this probability model it is
195:01 - impossible for the coin to land
195:04 - to never get heads because
195:08 - i never described an element in this
195:11 - sample space
195:13 - where the coin just where you just flip
195:15 - the coin forever because you never get
195:17 - heads it is explicitly forbidden
195:20 - in this probability model since you
195:22 - cannot find an
195:23 - outcome corresponding to it
195:26 - so therefore since there is no such
195:28 - outcome where the coin never
195:30 - where you never stop flipping the coin
195:32 - it is literally impossible
195:34 - in this probability model it's a very
195:36 - subtle point
195:38 - it seems like you should have that
195:40 - outcome in this but in fact you don't
195:42 - um it it's just and the reason why being
195:46 - i didn't define this sample space to
195:48 - allow for that possibility since there
195:50 - isn't a possibility that corresponds to
195:52 - it
195:52 - it simply doesn't exist right
195:56 - although it seems a little unfair
195:58 - because we can imagine a universe in
196:00 - which someone
196:01 - flips a coin and they never get tails
196:03 - and they never get heads forever
196:05 - it does seem like it's a possibility but
196:07 - it is explicitly forbidden in this
196:08 - probability model since it is not in the
196:10 - universe of possibilities
196:12 - um and by the way it is different from
196:14 - something being
196:16 - impossible and improbable improbable
196:19 - will probably mean
196:20 - uh when you define a probability model
196:22 - for this the probability of something
196:24 - happening is zero
196:25 - which seems realistic to say for
196:27 - flipping a coin
196:28 - until it where you flip a coin forever
196:32 - and uh you never get heads it seems like
196:35 - it's reasonable to say it's improbable
196:36 - but not impossible
196:37 - but right now it is literally impossible
196:40 - since there is no
196:41 - outcome that corresponds to that
196:45 - we would have to add a separate element
196:48 - which
196:48 - if we really wanted to we could
196:51 - add maybe the infinity element
196:55 - to our probability model to represent
196:57 - the
196:58 - outcome where you flip the coin forever
197:01 - um
197:01 - and by the way none of this says
197:03 - anything about what the probability of
197:04 - these
197:05 - events or these outcomes are i have no
197:08 - notion of probably at this point this is
197:10 - all set theory right so
197:15 - it seems like you would say that the
197:16 - probably that you flipped the coin
197:17 - forever is zero but i've i have said
197:20 - nothing about probability so far now
197:22 - that said that is a complication
197:23 - that we are going to leave out we are
197:25 - not going to consider it any further
197:28 - uh we are just going to stick with this
197:30 - probability model
197:31 - uh maybe i would revisit uh this notion
197:34 - of flipping the coin forever
197:36 - uh in a later lecture but that that's it
197:38 - for now
197:39 - uh let's list five events from this
197:41 - sample space
197:43 - well what's one event one event is the
197:46 - sam what is the empty set why didn't i
197:48 - listen that because i'm bored
197:49 - well no not because i'm bored because
197:51 - i'm lazy um another one
197:53 - is um let's say uh
197:56 - you've you can maybe flip the coin
197:59 - exactly three times
198:00 - that were that would correspond to tails
198:03 - tails heads
198:07 - okay this is a possible event um
198:10 - that seems almost like a triviality we
198:12 - could have
198:14 - the event where you flip the coin
198:18 - um at uh at most
198:22 - let's say three times because i don't
198:23 - want to write too much all right
198:25 - at most three times
198:29 - so at most three flips what would this
198:33 - correspond to like this is in words but
198:35 - in fact i can translate in
198:37 - that into a collection of outcomes
198:40 - well you could have flipped the coin
198:42 - only once that's at that's that's no
198:43 - more than three
198:44 - uh you could have flipped twice that's
198:47 - no more than three but if you flip twice
198:48 - and that means that the first flip was
198:50 - tails
198:51 - and uh if you flip three times exactly
198:54 - then you got two tails and a head
198:56 - so we could have tails tails heads and
198:58 - this would be the event
199:00 - uh what's another event we could say um
199:03 - [Music]
199:05 - well another possible event would be uh
199:08 - in words um at least
199:14 - uh three flips
199:22 - that seems reasonable what would that
199:24 - translate into
199:26 - that would be the event where you
199:29 - flip it three times because it's at
199:31 - least three flips
199:32 - so tails tails heads
199:36 - and tails tails tails heads has at least
199:39 - three flips since it has four flips
199:42 - and tails tails tails tails heads
199:46 - has five flips so that counts and in
199:49 - fact there's an infinite number of such
199:50 - flips
199:54 - okay uh and one final uh
199:57 - possibility is an even number of flips
200:05 - that's that's possible what would that
200:07 - look like if we were
200:08 - actually using the elements uh of the
200:11 - set to describe it that would be the
200:14 - event where you have tails heads that
200:17 - has an even number of flips since it has
200:18 - two
200:19 - tails tails tails heads has an even
200:22 - number of flips
200:23 - uh tails tails tails tails
200:26 - tails heads that also counts since it
200:29 - has six flips
200:30 - and so on this event also has an
200:33 - infinite number
200:34 - of outcomes so you can see here it's
200:36 - perfectly possible to talk about a
200:38 - probability model that has an infinite
200:39 - number of outcomes
200:41 - um in fact such models are quite common
200:44 - uh and in fact this particular model
200:47 - where i'm flipping a coin until i get
200:49 - heads as an as an instructor i really
200:51 - like this model because
200:53 - uh it's it's um it's not too difficult
200:56 - at least in my opinion to understand
200:58 - what is going on
200:59 - the idea of flipping a coin until you
201:01 - get heads it's a perfectly reasonable
201:03 - thing to think about
201:04 - and yet at the same time despite its
201:06 - apparent simplicity it
201:07 - is actually a quite rich probability
201:10 - model
201:10 - and makes a lot of points about
201:12 - probability theory
201:14 - so i'll probably be revisiting this one
201:15 - it like it's
201:18 - it's simple but it can very easily
201:23 - get out of hand in a way uh you you can
201:26 - start
201:26 - it can get quite complicated when you
201:29 - start
201:30 - analyzing it uh probabilistically and
201:32 - the mathematics themselves
201:33 - like uh a um like students at this level
201:37 - can't understand
201:38 - it but it's also starting to push their
201:40 - knowledge a little bit
201:42 - and it starts requiring some trickier uh
201:45 - calculations to do all right i'm just
201:46 - going to check something
201:49 - that's not what i wanted yeah we're
201:51 - still streaming okay
201:53 - all right uh example four define a
201:56 - sample space describing the experiment
201:58 - of rolling two
201:59 - six-sided die simultaneously list three
202:02 - events
202:03 - from this sample space uh all right two
202:06 - six sided die simultaneously what would
202:09 - that look like
202:11 - well it's tempting to say that this
202:13 - sample space consists of the numbers two
202:15 - through twelve but that's actually
202:16 - not what we should use the reason why is
202:20 - probably what you're thinking is i'm
202:21 - adding the two the pips on the two dice
202:23 - together
202:24 - but i never said that i never said that
202:26 - there was going to be
202:28 - um addition of two pips so
202:31 - we're not going to do that because you
202:33 - can define a number
202:34 - of uh potential outcomes like maybe
202:36 - instead
202:37 - of uh combining the two pipes together
202:39 - you're taking the larger of the two pips
202:42 - something like that um or the smaller of
202:45 - the two pips
202:46 - so we don't want to define our our
202:48 - sample space that way
202:50 - um what's another thing that we probably
202:53 - should do
202:54 - well when developing such a probability
202:57 - model it's generally better to imagine
203:00 - that you are actually rolling two
203:02 - distinct dice
203:03 - reason why is because when you think of
203:05 - it that way you end up with more
203:06 - appropriate mathematics
203:08 - so it's actually better to think of this
203:10 - sample space
203:12 - uh i keep doing that it's better to keep
203:15 - it's better think of this sample space
203:17 - as consisting of rolling a red dye and a
203:20 - blue dye
203:21 - so that's what we're going to do and
203:23 - just uh
203:24 - to just for my own sanity i try to draw
203:27 - things out as a table
203:28 - so as an example we have a blue dye
203:32 - or we'll call it the left eye and we
203:35 - have
203:35 - a red dye so we have an outcome where
203:39 - the blue dye comes up with a one and the
203:41 - red dye comes up with a two
203:43 - we could also have an outcome where the
203:44 - blue die comes up with a one
203:47 - and the red die comes up with a two
203:51 - i think i might have said the wrong
203:53 - thing a second ago but whatever
203:54 - and we can have an outcome where the
203:56 - blue die comes up with a one
203:58 - and the red die comes up with a three
204:03 - and we would continue on with this i'm
204:04 - not gonna i'm not going to list out
204:07 - everything because i got better ways to
204:08 - spend my day
204:10 - i'm going to say that in this first row
204:13 - uh the last element is where the blue
204:15 - die comes up with the one
204:16 - and the red die comes up uh with a six
204:24 - all right uh so for the next row
204:27 - in the next row we'll have the blue die
204:29 - comes up with a two
204:32 - and the red die comes up with a one
204:35 - uh so we'll just say that
204:39 - uh everything in the blue second row
204:42 - the blue die will be a two so
204:45 - we'll just start out uh listing some of
204:47 - those outcomes
204:49 - and then let's uh and in the third row
204:52 - the blue die
204:53 - will be uh three
205:03 - and we in our very last row we would
205:06 - have the blue die
205:07 - is a six
205:10 - so six six
205:20 - six
205:24 - and finally six all right so i probably
205:27 - should still write down my red die so
205:30 - i'm going to write down my red die
205:32 - uh so we got one and one
205:36 - two two
205:41 - uh two
205:44 - three
205:49 - excuse me i have somewhat of a cough
205:52 - three
205:53 - three and uh
205:57 - six
206:03 - ah you failure you failure of a computer
206:06 - so six
206:11 - uh six
206:17 - and finally six
206:22 - ah for goodness sakes like i said
206:26 - really cheap computer
206:30 - six all right there we go
206:33 - i'm satisfied uh we'll just
206:36 - kind of tidy stuff up put some ellipses
206:44 - um
206:46 - i like things to be rather organized
206:51 - and while we're at it we'll put some
206:52 - commas too although i at this point i'm
206:54 - not really sure
206:55 - if the commas matter all that much but
206:58 - this is a set
206:59 - this is containing a bunch of stuff this
207:01 - is what our set contains
207:02 - um so just uh so
207:06 - how many how many elements are in this
207:08 - set
207:09 - um well this set contains 36 elements
207:13 - since you have
207:14 - six possibilities for the blue dice and
207:15 - six possibility for the red dice so
207:17 - there's 36 things
207:19 - in this sample space which
207:22 - using it this way comes up with more
207:24 - appropriate probability models
207:26 - because this is would be a model where
207:28 - uh if you were thinking about adding up
207:30 - the dice
207:31 - uh a sum of two would be less likely
207:34 - than a sum of three because there's
207:36 - three ways
207:37 - to get the dice to add up to three but
207:39 - only no there's two ways to get the dice
207:41 - to add up to three but there's only one
207:42 - way for the dice to add up to two
207:44 - uh so and that's and that's more
207:47 - appropriate
207:48 - for our probability model
207:51 - and we can start listing some events
207:53 - from this sample space i'm going to list
207:55 - one event
207:56 - uh one of one of them from this sample
207:59 - space
208:00 - uh and it will be the empty set because
208:02 - i'm i'm i'm tired
208:04 - uh e2 another uh event would be the
208:07 - sample space itself so anything happens
208:09 - uh again i'm just really lazy right now
208:11 - all right now
208:12 - i actually have to start writing down
208:15 - some real events
208:16 - well i mean those were real events but
208:19 - uh
208:19 - something that's a bit more interesting
208:21 - um
208:22 - let's see e3 what can we do for e3 well
208:26 - we do have the outcome where we roll um
208:30 - six for the blue die
208:34 - and uh one for the red
208:38 - die why why why this event because why
208:40 - not this event it is an event
208:42 - uh what's a fourth event let's let's
208:44 - start to get a little bit more creative
208:46 - we'll say that the fourth event will be
208:49 - um an even number
208:53 - of total pips total suggesting that
208:56 - you're adding the pips together
208:58 - so
209:03 - all right so let's uh translate this
209:05 - into
209:06 - something a little bit more mathy so you
209:07 - can have let's see
209:09 - there's a whole bunch of outcomes for
209:11 - one where you could have uh
209:13 - uh let's say that the
209:17 - uh oops all right we could say that the
209:20 - blue dice is a one and what's
209:22 - one outcome we have the red dice also be
209:24 - one
209:25 - uh we could have the blue dice b1
209:29 - and the red dice b3 that has an even
209:32 - number of total pips we can just keep
209:34 - going on with this i'm not really sure
209:36 - off the top of my head i could compute
209:38 - it but i'm not really sure off the top
209:39 - of my head how many outcomes
209:41 - uh how many how many outcomes are
209:43 - actually in this uh sample space but it
209:45 - seems like it's going to be a number
209:47 - and i'm tired and i'm sure you don't
209:49 - want to watch me just list out stuff
209:51 - so uh we'll just end this with uh
209:54 - six and six so
209:59 - because at this point you probably get
210:00 - the idea
210:05 - plus i also don't honestly remember the
210:06 - last time anyone asked
210:08 - for this type of this type of event
210:11 - uh another of it so we could have
210:14 - for our fifth event um that
210:17 - the die add up to seven
210:27 - all right so what would be in this event
210:29 - well
210:30 - uh we could have that the blue dice is
210:33 - one
210:34 - and if the blue dice is one since they
210:36 - have to add up to seven that means that
210:37 - the red dice is going to be
210:38 - six
210:42 - uh what's another possible outcome well
210:44 - you could have that the blue dice is two
210:46 - and since the the
210:47 - oh no they don't add up to six they add
210:48 - up to seven yeah that's what i said
210:52 - so uh you could have that the blue dice
210:54 - is two in which case the red dice must
210:56 - be five
210:58 - uh you can have the blue dice be three
211:02 - in which case the red dice uh must be
211:05 - must be four and you keep going on like
211:09 - this
211:10 - uh until eventually you get to the very
211:14 - uh last element that if you were to
211:17 - continue writing down
211:19 - you would write down that element being
211:20 - when the blue dice is six
211:24 - and the red dice is one so
211:27 - how many elements are going to be in
211:29 - this uh event
211:30 - well it seems like for every blue die
211:32 - there's a corresponding
211:34 - and for every blue outcome between one
211:36 - and six there's a corresponding red
211:37 - outcome that would lead to the die
211:39 - setting of just
211:40 - adding up to seven so there must be six
211:43 - things
211:43 - in this event so yes
211:47 - um by the way for whatever
211:50 - for for what it's worth i'm not sure if
211:51 - i mentioned this later but i'll just
211:52 - mention it now uh
211:54 - we often use this notation uh like for
211:57 - example
211:58 - uh this we would put a set in between
212:01 - two what almost look like absolute value
212:02 - lines
212:03 - to mean the size of a set or more
212:06 - technically the cardinality of a set but
212:08 - for now
212:09 - when we're talking about um finite sets
212:12 - it's fine to talk about to say the size
212:15 - of a set
212:15 - meaning the number of elements in that
212:17 - set in which case
212:19 - the size of the sample space is 36 the
212:22 - size of the empty set the set with
212:25 - nothing in it since there's nothing in
212:26 - it its size is zero
212:28 - uh the size of um
212:31 - e5
212:34 - would be six there's six elements in e5
212:38 - and there's uh one element in e3
212:46 - okay continuing on uh our next example
212:51 - define a sample space describing the
212:53 - experiment
212:54 - of waking up in the morning at a
212:56 - particular time where the time you wake
212:58 - up at
212:59 - the thought of as a real number and
213:01 - that's a critical point
213:02 - is the outcome of interest list three
213:05 - events from this sample space
213:07 - um i'm going to say that the sample
213:10 - space
213:11 - so we can think of it in terms of hours
213:14 - in a day but we're going to allow hours
213:16 - to be
213:17 - decimal points so an hour and a half
213:19 - would be
213:20 - 1.5 so we would say that
213:24 - midnight corresponds to zero hours
213:28 - and we're just going to say that you
213:29 - cannot reach the midnight of the next
213:31 - day
213:32 - so maybe you've seen this notation
213:34 - before when regarding
213:35 - uh intervals of the real line where a
213:38 - square bracket
213:39 - means that that number on that side is
213:42 - being included in the set
213:44 - and an open bracket or a parentheses
213:47 - on a side means that that number is not
213:50 - being included
213:51 - in that event uh so some more notation
213:54 - we say we use uh
213:57 - what almost looks like an e or an
213:59 - epsilon to say that something is a
214:02 - member of a set
214:03 - so for example zero is an element
214:06 - of the sample space uh 12 is an element
214:10 - of the sample space
214:12 - um but 24
214:16 - uh well let's let's uh come back to 24
214:18 - in a second 50
214:19 - is certainly not an element of the
214:21 - sample space so we'll put a slash in
214:23 - that
214:24 - uh and 24 is not an element of the
214:27 - sample space
214:28 - either because i put an open what i call
214:30 - an open bracket or
214:31 - a parenthesis around the 24.
214:34 - uh so in this alternate so if i were
214:37 - thinking of 0
214:38 - 24 as a set this would be a set that
214:41 - doesn't include zero either
214:43 - alternatively uh just getting you more
214:45 - familiar with this notation
214:47 - uh we could describe a set that includes
214:50 - both of its endpoints
214:51 - 0 and 24 and by the way all of these are
214:54 - corresponding oh go away
214:57 - all of these by the way are
214:59 - corresponding to
215:00 - uh intervals of the real line so
215:04 - some some interval of the real line uh
215:07 - hopefully you're somewhat familiar with
215:08 - that notation
215:10 - uh yeah okay so
215:13 - uh continuing on this is our sample
215:14 - space let's uh erase all this stuff
215:16 - because this is someone evident aside
215:22 - um
215:25 - okay uh so uh three events from this
215:29 - sample space
215:30 - we could have an event where um
215:34 - uh we wake up at noon so that would
215:37 - probably
215:38 - correspond uh to the outcome where you
215:41 - wake up 12 hours from midnight
215:44 - um so this would be exactly 12 hours
215:48 - though
215:48 - exactly 12. not a second
215:52 - more not a millisecond more not a
215:54 - millisecond less and not a second less
215:56 - exactly
215:57 - 12 hours later which you think of
216:01 - that that matters it is exactly 12 and
216:04 - nothing
216:05 - it seems like like like our own
216:07 - intuition of the world is
216:09 - like if you wake up a millisecond after
216:11 - mid afternoon
216:12 - um you still woke up at noon i was like
216:15 - no that is not what i mean right here
216:18 - so it is a precise number and that
216:21 - precision
216:22 - should lead you to think well that's
216:25 - impossible it's
216:26 - it's or maybe not impossible but
216:29 - but really really really really hard to
216:32 - the point
216:32 - of being almost impossible and that is
216:35 - true
216:36 - when we develop for a probability model
216:39 - for this experiment we would probably
216:41 - assign a probability of zero
216:43 - to the event of waking up exactly 12
216:45 - hours from midnight which is kind of a
216:48 - strange idea but
216:49 - i'm going to talk about that later video
216:51 - just understand that
216:53 - just to understand what exactly i'm
216:55 - saying here uh let's uh come up with
216:57 - another event
216:59 - uh this event corresponds to waking up
217:01 - between
217:03 - let's say instead that you wake up
217:05 - sometime between 8 and 12 hours
217:07 - now my language is a little imprecise
217:11 - here
217:13 - so i need to specify do i mean
217:16 - uh including eight hours
217:19 - uh including exactly eight hours
217:22 - um or am i saying more than eight hours
217:25 - so eight hours in a millisecond is okay
217:27 - but eight hours
217:28 - on the nose is wrong uh so we'll just
217:31 - say
217:31 - that we're going to exclude those
217:33 - endpoints why because i said so
217:35 - um so
217:38 - this excuse me uh this uh this event
217:42 - uh would be an event where
217:46 - you do in fact uh where you're
217:49 - where you can wake up um eight hours in
217:51 - a millisecond eight hours in a second
217:53 - or 12 hours less a second but you cannot
217:56 - wake up at exactly eight hours or
217:58 - exactly 12 hours and
218:00 - have this event actually have occurred
218:02 - okay
218:03 - um let's say for a third event
218:07 - um we're going to wake up um let's say
218:12 - that
218:12 - that this is uh waking up after 3 a.m
218:15 - so or um
218:18 - let's see uh
218:22 - maybe no earlier than three event at 3am
218:24 - so no earlier than 3am
218:27 - uh that suggests that we should include
218:29 - 3am
218:30 - in this in this uh in this event
218:33 - so uh so we should include 3am or 3
218:37 - hours past midnight
218:38 - but all right so what should the end
218:40 - point on the other end be it seems like
218:42 - we can wake up any time
218:43 - after 3 a.m and that's fine so we're
218:46 - going to end at 24
218:48 - since that's the last um so
218:51 - since those points are the last ones in
218:53 - this sample space
218:58 - okay
219:02 - let me get caught up all right moving on
219:06 - uh events once we have some events
219:10 - we can start uh manipulating these
219:12 - events in ways
219:13 - that create new events uh
219:17 - so we can start giving some uh
219:19 - operations
219:20 - on our set theory so for example let's
219:23 - just say that a
219:24 - and b are events uh and events in this
219:27 - situation they're also synonymous with
219:29 - sets so anything you know about set
219:31 - theory
219:32 - uh you can import that here i am
219:35 - actually when i'm talking about events
219:36 - i'm really talking about sets there
219:38 - those two terms in this class are used
219:40 - almost interchangeably
219:42 - so the complement of a which in this
219:45 - class we're denoting with a
219:47 - oops uh in this class we're denoting
219:49 - with a prime but that's not the only
219:51 - notation there's a bar
219:53 - or a complement this is actually my
219:55 - preferred notation
219:56 - uh that's but anyway uh in this class
219:59 - we're going to use a prime
220:00 - this is the set of outcomes of s
220:04 - the sample space not in
220:08 - uh the event a
220:12 - uh so it's all outcomes that could
220:15 - possibly happens
220:16 - that are not in a which so
220:20 - and in fact we the the the
220:23 - english words that we use for this event
220:25 - is not
220:26 - a that's that's a perfectly way to
220:29 - describe it
220:30 - there's the union of two sets
220:33 - uh which we would say a union b but i
220:36 - also like to just say
220:37 - a or b since this corresponds to the
220:40 - logical notion of or
220:42 - where something in one event can happen
220:44 - or something in another
220:45 - event could happen and by the way when
220:47 - we're using or in this context we're
220:49 - using it in the logical sense where
220:51 - um we can have an outcome in is
220:54 - in a only would count as happening in a
220:57 - or b
220:58 - an outcome in b only would
221:02 - count as have having occurred in the
221:03 - event a or b and an
221:05 - outcome that is both in a and in b
221:09 - counts as happening in the event a or b
221:12 - which is a little bit different from how
221:13 - the word or is used in english because
221:15 - it's
221:16 - quite often the case in english that or
221:17 - means exclusively or
221:19 - exclusive or or you can say you can have
221:22 - her
221:22 - your cake or eat it and
221:26 - like in that so the phrase have your
221:28 - cake or eat it
221:29 - it suggests that you can either have
221:30 - your cake or you can eat it but you
221:32 - can't have both
221:33 - right if you there was actually a it was
221:36 - actually a really long time until that
221:38 - phrase made sense so uh this might
221:41 - actually be something for
221:42 - um people whose native language is not
221:44 - english
221:45 - uh what they mean by have your cake and
221:47 - eat it too
221:49 - um you can have your cake in your hand
221:51 - or you can eat your cake
221:53 - but if you eat your cake it is no longer
221:55 - in your hand so that's what it means all
221:56 - right so
221:57 - just in case i think that that might be
222:00 - something that
222:00 - for non-native speakers it's actually
222:03 - worth it to make that clarification but
222:04 - for a long time i
222:05 - also as someone who's spoken english all
222:07 - their life and was on the debate team
222:09 - and on the literary magazine
222:10 - did not get anyway um
222:14 - uh the intersection of two sets a
222:17 - and b or a intersect b so it's the set
222:21 - that
222:21 - contains only objects that appear in
222:23 - both a and b
222:25 - so in words we use a and b
222:29 - i'm going to go ahead and get started
222:30 - drawing some diagrams
222:32 - venn diagrams are a way to
222:36 - describe set theoretic relationships
222:38 - between
222:39 - uh different sets and venn diagrams you
222:42 - can construct venn diagrams for pretty
222:43 - much
222:44 - up to uh three events
222:47 - and venn diagrams make sense and very
222:49 - diagrams are very simple
222:51 - and the moment you try to go to four
222:52 - events event diagrams become impossible
222:55 - so we're just going to live in a world
222:56 - where there's only three events um
223:00 - so here's how you kind of draw a venn
223:01 - diagram at least for this class
223:04 - you draw circles and squares and stuff
223:06 - to denote sets
223:08 - often we draw a giant square
223:11 - and this square denotes the sample space
223:14 - so the square denotes
223:16 - the universe of possibilities and we
223:19 - denote
223:20 - a subset of this sample space otherwise
223:22 - known as an event
223:23 - with some other shape such as a circle
223:26 - or maybe
223:27 - i there are times where i will draw
223:30 - lines to try to divide it up but
223:31 - basically we're going to draw shapes and
223:33 - lines
223:34 - inside of this sample space that divide
223:37 - regions of the sample space from each
223:40 - other
223:41 - and those will denote subsets so often
223:43 - i'm going to draw
223:44 - circles to denote events so i'll often
223:48 - label one circle a
223:49 - and another circle b and
223:52 - how we draw these circles allows us to
223:55 - reason about
223:56 - relationships amongst events so here i
223:59 - have drawn
224:00 - a sample space with two events a and b
224:04 - and those events have some outcomes in
224:07 - in common
224:08 - since there is a region in which both
224:11 - set
224:11 - both of these circles intersect now
224:13 - there's also a situation where there are
224:16 - outcomes in a that are not in b so
224:19 - situations where a happens but b doesn't
224:21 - happen
224:21 - because you can kind of imagine if you
224:23 - really want a probability model
224:25 - that we're going to pick a random point
224:27 - from
224:28 - this set and we're going to ask where
224:30 - did this appear did this appear in
224:32 - in a did it appear in b did it appear in
224:35 - a or b
224:36 - um did it appear in a and b or did it
224:39 - not appear in either one
224:40 - um that would almost be a probability
224:42 - model so we can use
224:44 - diagrams such as this to start reasoning
224:47 - about
224:47 - uh set theoretic relationships um
224:51 - and in fact here's something to
224:55 - kind of think about well actually no i'm
224:58 - going to save that for the next section
225:00 - um but this is a perfect perfectly
225:02 - reasonable way
225:03 - to uh think about uh set theory at this
225:07 - level
225:08 - um okay so uh two
225:11 - so uh let's uh describe the situation
225:16 - uh a or b
225:20 - so a union b means an outcome that is
225:22 - either in a
225:23 - or in b or both so
225:26 - what i would do in a venn diagram is
225:29 - shade the region that corresponds to
225:31 - this event
225:32 - well maybe before i do that maybe before
225:34 - i go
225:35 - into that level of extra complication
225:38 - how about i first describe the event
225:41 - just a
225:42 - well visually using a venn diagram that
225:44 - would what i would do is i would shade
225:46 - the region a
225:47 - and nothing else and that would
225:50 - correspond
225:51 - to the event where a happens so i'm
225:53 - using
225:54 - shading up these circles uh to
225:57 - try and reason about what happens here
226:00 - uh i wonder if there's a
226:02 - let's use this highlighter tool i wonder
226:03 - what would happen if i use the
226:04 - highlighter
226:06 - uh it makes my computer lag a lot i'm
226:08 - not gonna use that
226:10 - um so um
226:13 - uh similarly we could describe another
226:16 - event where we just
226:18 - uh have b occur so we'll say the event b
226:22 - how would we shade this we're going to
226:24 - shade it like so just shade in the
226:25 - region that's enclosed by b
226:27 - and nothing else so this corresponds so
226:30 - this is the region that corresponds
226:32 - to the event b happening now let's go
226:34 - back to some of these other
226:36 - uh potential relationships uh such as
226:39 - a or b or a union b
226:42 - um hmm this seems like a fancy feature
226:46 - no no it doesn't work that way so i need
226:48 - to get rid of that
226:50 - red region
226:55 - all right so um
226:56 - [Music]
226:58 - here's a way to draw venn diagrams uh
227:01 - when you're trying to do
227:03 - um stuff so like
227:06 - let's take for example the union
227:08 - operation a
227:09 - union b to draw a union operation what i
227:13 - generally will do
227:14 - is i'll take a common color and i will
227:17 - shade
227:18 - in first uh a
227:21 - uh i don't want black i want blue
227:25 - i'll first shade in a the the set on the
227:27 - left side of the union relationship
227:30 - and then i will shade and be the element
227:33 - on the right hand side
227:34 - and a point that was shaded at all
227:37 - is a member of this set
227:42 - so um so
227:45 - this region that i've kind of enclosed
227:46 - in red
227:48 - uh corresponds that's that a or b since
227:50 - any point that got shaded by blue at all
227:53 - counts as being a member in this event
227:56 - okay uh let's draw another
228:00 - uh event let's uh illustrate for example
228:04 - a intersected with b so to draw
228:07 - intersection
228:08 - let's uh first draw our sets a and b
228:14 - to draw intersection you're going to
228:15 - subtract points uh you're going to
228:18 - subtract from uh
228:19 - uh or or actually what you would do
228:22 - is i like to think of it in terms of
228:24 - pieces of paper uh
228:26 - where you overlap two pace pieces of
228:28 - paper on top of each other and then
228:30 - uh cut the pieces of paper so that only
228:32 - the overlapping region is what's left
228:35 - so uh you could imagine here
228:38 - uh i draw uh something on a
228:42 - and i draw something on b and then
228:45 - i'm going to erase from the picture the
228:48 - region
228:49 - that what that was in b but not in a
228:54 - and i'm going to erase from the picture
228:56 - the region that is in a but not in b
229:03 - and what's left is going to be the
229:05 - region that corresponds to a
229:07 - intersected with b
229:11 - okay um so another
229:15 - important notion is disjointedness two
229:17 - sets are disjoint if they have no
229:19 - elements in common
229:20 - in that case a intersected with b is the
229:23 - empty set
229:24 - so to draw disjointedness this is what i
229:27 - would do
229:28 - here's my sample space i draw an event a
229:32 - and i draw an event b such that there is
229:34 - nothing in common between the two events
229:36 - all right i'm going to draw the
229:37 - intersection between these two events
229:39 - i'm done
229:40 - because there's nothing intersecting so
229:42 - the intersection between these two
229:44 - events
229:44 - is uh is the empty set
229:48 - to so this is one way if you really
229:52 - wanted to assign some sort of meaning to
229:56 - the empty set it would be
229:58 - that the empty set is a logical
230:01 - contradiction in a way since the empty
230:03 - set shows up
230:04 - and when you have events that have
230:06 - nothing in common
230:07 - which are almost logical contradictions
230:11 - uh another notion in probability theory
230:14 - no in a set theory is complementation
230:16 - how would i illustrate complementation
230:18 - so let's uh draw a sample space
230:21 - i have a i have b
230:25 - what is the so this is um
230:28 - a intersect with b uh equals the empty
230:31 - set that means disjoint
230:33 - uh what is a complement a complement is
230:37 - the region in the sample space that is
230:39 - an
230:39 - a that is in the sample space but it's
230:42 - not an a
230:43 - so that's going to correspond to shading
230:45 - everything that's
230:46 - outside of a
230:50 - so there are some parts of b that gets
230:52 - that get shaded but nothing that's in a
230:57 - okay so you shade everything except a
231:02 - uh some other important subset
231:04 - relationships
231:06 - um we have uh
231:10 - uh or so some other set relationships
231:13 - here is the relationship
231:14 - a is a subset of b
231:17 - what it means for one set to be a subset
231:20 - of another
231:21 - is that um all of the elements in a
231:24 - are also impres are also present in b or
231:27 - alternatively there is nothing that a
231:30 - that doesn't also exist in b
231:32 - so a subset relationship looks like so
231:34 - you have the set b
231:35 - and the set b contains the the set a so
231:38 - you would
231:39 - draw a as being in the interior of b
231:43 - another uh notion that i haven't really
231:46 - described above but let's
231:47 - go ahead and mention it is the notion of
231:50 - set subtraction
231:51 - we have a subtract the set b that
231:54 - corresponds to every element in a that
231:57 - is
231:57 - not in b so uh it
232:00 - this it is in fact possible to prove
232:03 - that
232:03 - a subtract b and you might actually be
232:06 - required to show this in your homework
232:08 - and the way you probably show it
232:09 - is by playing around with venn diagrams
232:12 - you can say this is a
232:16 - intersected with um the complement
232:19 - of b so a and
232:23 - not b so what does set subtraction look
232:27 - like
232:27 - as a venn diagram we have the set a
232:31 - we have the set b and we draw everything
232:34 - in a
232:35 - and we shade everything in a that is not
232:37 - in b
232:41 - so you kind of subtract out the set b
232:43 - it's as if
232:44 - you had these pieces of paper they put
232:46 - them you put uh
232:47 - the piece of paper the b piece of paper
232:49 - on top of the a piece of paper and then
232:51 - you cut out the part of the b
232:52 - of the a piece of paper that's in the b
232:54 - piece of paper
232:58 - excuse me okay
233:01 - uh so let's see an example use a venn
233:04 - diagram to illustrate
233:05 - uh a or b complement or a and b
233:09 - uh before i continue on let's just make
233:11 - sure i'm still streaming i'm still
233:12 - streaming
233:13 - um at some point i'll relax about that
233:17 - but
233:17 - today is not that day uh let's get
233:20 - started by just drawing uh
233:22 - the venn diagram oops i don't want red
233:25 - yet
233:30 - so here's my venn diagram for the sample
233:32 - space
233:33 - s i have a and i have
233:37 - b okay so i'm first going to
233:41 - illustrate uh the event a intersected
233:44 - with b
233:44 - i'm going to do so in blue so a
233:46 - intersective b that's the region that is
233:48 - in both a
233:49 - and b uh as for
233:52 - a or b complement you have the component
233:55 - you have the union of a or b which is
233:57 - kind of this uh
233:59 - figure eight looking region here
234:02 - and i want everything outside of that
234:05 - region because i want the complement of
234:07 - that region
234:08 - so i'm going to shade it everything that
234:10 - is outside
234:12 - of that region and this is what you end
234:16 - up with
234:23 - okay so that's one example next example
234:26 - consider three sets a b and c
234:29 - so how would i draw these in a venn
234:32 - diagram
234:33 - i would have my square denoting the
234:37 - universe of possibilities which is
234:38 - s and then i draw um
234:42 - three circles one for a one for b and
234:46 - one for c
234:47 - so what is the union of a b and c
234:50 - well it's going to be the set where uh
234:53 - if i shade a
234:55 - everything in a and then i shade
234:57 - everything that's in b
234:59 - and then i shade everything that's in c
235:02 - if the point ever got shaded
235:03 - it's going to be in that union
235:07 - all right next up we have a intersected
235:10 - with b
235:11 - intersected with c
235:14 - so let's draw our universe
235:18 - here's our sample space
235:22 - so we have a b and c
235:26 - so let's see uh let's take a point right
235:28 - here
235:29 - this point that i just drew is uh a
235:32 - point that is in b
235:33 - and it is in c but it is not an a so
235:35 - it's not going to be in the intersection
235:37 - uh this point right here is in b but
235:39 - it's in neither a or c
235:41 - but it it's also just not an a so it's
235:43 - not going to be in the intersection of
235:44 - all three
235:45 - you can just start reasoning about all
235:47 - of these points in a similar way
235:49 - and the conclusion that you're forced to
235:51 - reach is that this little sliver here
235:54 - is the only part that's going to be in
235:55 - all three sets so that corresponds to
235:57 - the intersection of all three
236:00 - all right uh next example this is a more
236:02 - complicated one
236:03 - we have a intersected with b or a
236:06 - intersective with c
236:07 - or b intersected with c what is that
236:09 - going to look like well let's get
236:11 - started
236:12 - i don't want red
236:17 - all right here's our sample space
236:20 - here is a here is b
236:25 - here is c all right let's start out by
236:29 - uh building this thing up with uh the
236:32 - intersection a and b
236:34 - right here is a and b here's a and c
236:38 - and here's b and c so this wind or
236:40 - pinwheel
236:41 - looking region uh is the only region
236:44 - that ever got shaded so that's going to
236:45 - correspond
236:46 - to the union of the three of the three
236:49 - intersections
236:50 - so this would in words this would be an
236:53 - event where at least
236:54 - two events happen this will correspond
236:58 - to
236:59 - all right uh so example eight describe
237:02 - the intersection complement and union of
237:04 - events described in examples one through
237:06 - five
237:11 - let me get caught up um
237:15 - so the point of this section is to go
237:20 - from these pictorial representation or
237:22 - not this section this example
237:24 - is to go from these pictorial
237:26 - representations
237:27 - of events to more algebraic
237:30 - representations
237:32 - so uh let's go with example one
237:39 - so for example one that was the example
237:43 - where we were flipping a coin we could
237:44 - get either heads or tails
237:46 - uh we could have uh the intersection of
237:49 - heads and tails
237:51 - uh of the two uh sets heads
237:54 - and tails these two sets have nothing in
237:57 - common
237:58 - they are therefore considered disjoint
238:00 - events
238:01 - so the intersection of these is the
238:02 - empty set since you would only
238:04 - restrict yourself to what is in common
238:06 - and they have nothing in common
238:08 - uh whereas the event
238:11 - heads or tails
238:16 - uh that event would correspond to uh the
238:20 - event heads with
238:21 - heads and tails uh which corresponds by
238:24 - the way to the sample space
238:28 - okay uh here's also some basic uh
238:31 - properties for you uh let's let's take
238:34 - an
238:34 - arbitrary uh set
238:38 - s no no no not s uh let's take an
238:41 - arbitrary event a
238:43 - a intersected with a sample space is
238:46 - equal to
238:47 - a since a is a subset of the sample
238:49 - space in fact
238:50 - in general if a is a subset
238:53 - of b then a intersected with b
238:58 - is going to be uh a
239:01 - and a union with b is going to be
239:05 - b and i'm just going to leave it at that
239:06 - i want you to think about why that's
239:08 - true
239:08 - uh go try and reason about it with a
239:11 - venn diagrams
239:12 - um so and in fact that's pretty much
239:15 - that pretty much says everything that i
239:16 - would want to say about uh the sample
239:19 - space
239:20 - and the empty set because uh since a is
239:22 - the subset of the sample space a
239:24 - intersected with s
239:25 - is going to be a and a unioned with
239:28 - s is going to be s likewise
239:32 - a intersect intersected with the empty
239:35 - set the empty set
239:36 - is a subset of a therefore the
239:40 - intersection of two is going to be
239:42 - the subset which is going to be the
239:44 - empty set
239:45 - on the other hand a union with the empty
239:48 - set
239:48 - is going to equal a since a contains the
239:51 - empty set
239:54 - all right uh continuing on with what i
239:56 - was saying before
239:58 - um the next example
240:03 - uh so example two uh so
240:06 - uh we have uh for example the sets
240:10 - um my notation in the notes is a little
240:13 - bit different from one i'm sure i wrote
240:15 - down
240:16 - earlier um so let's uh
240:20 - uh um
240:23 - i don't want to go all the way back
240:25 - there
240:28 - i'll just uh i'll just throw some stuff
240:31 - out
240:31 - uh we can have um for example uh
240:38 - so this was the one where we were
240:41 - rolling a die
240:42 - uh one we could say that the union of
240:45 - the set
240:47 - uh one with the die roll of one
240:50 - and the set with the die roll of uh
240:54 - so the intersection of these two sets
240:56 - this is also going to be the empty set
240:58 - since they have nothing in common
241:01 - but uh the intersection
241:05 - of the event with um
241:08 - no the union of these two sets
241:12 - is going to be
241:16 - uh is going to be um the
241:19 - event that contains both of these
241:21 - possible outcomes
241:27 - darn laptop
241:31 - so one and three
241:36 - and uh maybe less trivially um
241:41 - uh we could have
241:44 - uh we could have the um event
241:48 - uh let's say one three five
241:52 - so this would correspond to an odd
241:54 - number of pips
241:58 - and we're going to intersect that with
242:00 - the event where the number of pips
242:01 - doesn't exceed three
242:03 - um so that would be the event where you
242:06 - have
242:06 - one uh two
242:10 - and three oh
242:14 - okay so
242:19 - uh the intersection of these two events
242:22 - is going to be
242:23 - well let's see what outcomes do they
242:24 - have in common uh one amperes in both of
242:27 - them
242:28 - three appears in this set but it doesn't
242:30 - appear on the other one so let's see
242:31 - we've got
242:32 - one one appears in both
242:36 - uh three appears on only one
242:39 - no three actually appears in both of
242:40 - them so three appears in both of them
242:44 - five appears only and one and two only
242:46 - appears on one so whatever got
242:47 - underlined twice that is going to be in
242:50 - the intersection of the end
242:51 - of these two sets so we're going to end
242:55 - up with the
242:57 - set
243:00 - one and three
243:09 - now for a lot of these uh set
243:11 - relationships such as and and or you can
243:13 - kind of think
243:14 - logically using english like for example
243:17 - um
243:18 - uh maybe going to uh the fourth example
243:21 - where i had uh
243:24 - sets where um yeah i was rolling a two
243:28 - dice
243:29 - uh a red dye and a blue dye you could
243:32 - imagine a set
243:33 - where uh they summed to seven
243:37 - and the blue dice is at least three
243:40 - so maybe going to that so in the context
243:44 - of example four
243:49 - so in the context of oops
243:52 - in the context of example four
243:55 - so they sum to seven
244:03 - and they don't
244:07 - or no no no no uh the blue dice
244:15 - uh so blue dice is
244:21 - um uh let's say five
244:25 - so that would correspond to this to the
244:27 - event
244:29 - where uh you have um
244:33 - so let's see the blue dice is at least
244:35 - five so we could have
244:36 - the blue dice b5
244:41 - stupid pen so the blue dice could be
244:44 - five
244:46 - or the blue dice could be six and
244:49 - in these two situations we know what the
244:51 - red dice is going to be
244:53 - in the first the red dice is going to be
244:54 - two and in the second the red dice is
244:57 - going to be one
244:58 - so this would be uh the corresponding uh
245:01 - resulting event after we do that
245:02 - intersection
245:04 - okay um i'm going to leave it at that
245:07 - there's
245:08 - because there's a lot of examples so for
245:09 - the rest of these maybe
245:11 - uh try going through uh some of the set
245:14 - the events that i listed down and
245:17 - figuring out if you
245:18 - intersect or complement these events so
245:20 - intersection complementation
245:21 - union uh all these things try going
245:24 - through those sets and
245:26 - uh seeing what you get okay
245:29 - but i've given you some examples to
245:30 - start working off of for now
245:32 - all right so i'm gonna leave that for
245:34 - this section and uh
245:36 - i will see you in section two
245:39 - where we go beyond just uh talking about
245:42 - some basic set theory and actually
245:44 - start saying what it takes to build a
245:46 - probability model
245:47 - all right so i will see you later
246:15 - hey students all right so let's move on
246:19 - now to the next section
246:21 - uh on starting probability theory
246:24 - so uh in order for us to uh
246:28 - we would like to be able to assign
246:30 - numbers
246:31 - to events from uh sample spaces
246:35 - to describe how likely those events are
246:38 - and in order to do so we need to develop
246:41 - a notion of probability
246:42 - so we'll start by defining what a
246:44 - probability measure is
246:45 - so we have so we have what's known as a
246:48 - probability measure
246:49 - p and this is a function it's a function
246:53 - that takes
246:53 - events as inputs and returns numbers
246:56 - between
246:57 - zero and one and satisfies the following
247:00 - three
247:00 - axioms and axioms are things that are
247:02 - true basically because we say so they're
247:05 - they're almost similar to assumptions
247:10 - maybe you remember axioms from geometry
247:12 - but the idea is that
247:14 - we have some starting things that we
247:15 - simply say they are true because they
247:16 - are almost self-evident
247:18 - first we say that every probability must
247:21 - be at least zero
247:23 - uh you cannot have negative
247:24 - probabilities second
247:26 - we say that the probability of the
247:28 - sample space is equal to one so the
247:30 - probability that anything happens
247:32 - is equal to one finally uh this is the
247:35 - weirdest
247:36 - looking axiom if we have a sequence of
247:40 - disjoint events
247:42 - uh so in other words for any uh i that's
247:45 - equal to j the intersection of uh any
247:48 - two such events
247:49 - is uh equal to the empty set and
247:52 - furthermore
247:53 - this is for a potentially infinite
247:55 - sequence of disjoint events
247:57 - we say that the probability of the union
247:59 - of those events is equal to
248:01 - the sum of their individual
248:02 - probabilities this is very wordy
248:05 - this is very technical there is a much
248:08 - easier way
248:09 - to understand what this axiom is saying
248:11 - if we
248:12 - decide well okay there's usually only
248:15 - one situation
248:16 - where you're going to understand where
248:18 - you're actually going to use this
248:20 - axiom most of the time which is that if
248:22 - a intersected with b
248:24 - is equal to the empty set so if a and b
248:26 - are two disjoint events
248:28 - then the probability of a or b is equal
248:31 - to the probability of a
248:32 - plus the probability of b so
248:35 - the probability of uh two events that
248:38 - have nothing in common
248:40 - uh if the probability that either one
248:41 - happens will be the sum of their
248:43 - individual probabilities
248:46 - okay so these are true because we say so
248:50 - and from this we get uh
248:54 - pretty much everything else that we
248:55 - believe should be true about
248:57 - uh probability or about how
248:59 - probabilities
249:00 - work so uh for starters uh we have
249:05 - uh that the probability of the empty set
249:08 - is equal to zero
249:10 - so this is not something that we said
249:12 - this is not something that we said must
249:14 - be true
249:15 - this is simply something that
249:18 - um this is actually a consequence of the
249:21 - assumptions that we have made
249:23 - up to this point so this is kind of a
249:26 - weird
249:27 - so we're going to show that this is in
249:30 - fact
249:30 - true using just
249:34 - these uh three um
249:37 - just these three assumptions not just
249:39 - these three axioms
249:41 - so only these
249:44 - three things all right um
249:48 - now that said we're going to have to use
249:51 - the fact that probably the sample space
249:52 - is equal to one
249:53 - what we could say is this the sample
249:55 - space
249:58 - the sample space is equal to
250:01 - the sample space unioned
250:04 - with the empty set and in order to use
250:08 - axiom three in the way we have written
250:12 - axiom three down
250:14 - we're going to have to say that the
250:15 - empty set is equal to
250:18 - uh the union from i equals one
250:22 - uh to well i equals one to infinity
250:25 - the empty set so in other words we need
250:27 - to create an infinite union
250:29 - of empty sets and it's certainly true
250:31 - that if you take an infinite
250:33 - uh collection of empty sets and you
250:35 - union them all together there's still
250:36 - not going to be anything
250:38 - in that union so you're still going to
250:39 - have the empty set furthermore
250:41 - the intersection of the empty set with
250:43 - the empty set is also going to be the
250:45 - empty set so technically the empty set
250:47 - is disjoint with itself so that means
250:50 - that this collection
250:52 - of empty sets this giant uh repeating of
250:54 - the empty set
250:55 - technically satisfies the condition of
250:58 - the conditions of the third axiom
251:01 - so then and and also it's true that the
251:04 - set
251:04 - the sample space uh intersected with the
251:07 - empty set is going to be
251:09 - the empty set because the empty set is a
251:11 - subset of the sample space
251:13 - therefore we can now apply this
251:17 - uh this uh this uh
251:20 - third axiom to prove the prob uh
251:22 - proposition
251:23 - because we know that one is equal to the
251:26 - probability
251:27 - of the sample space and the probability
251:30 - of the sample space is equal to the
251:31 - probability
251:32 - of the sample spaced uh unioned
251:36 - with the empty set which is equal to
251:40 - uh the probability of the sample space
251:43 - unioned with this infinite collection
251:48 - of uh empties of uh empty sets
251:54 - and then we apply that third axiom to
251:56 - say
251:57 - that uh this is going to equal to the
252:00 - probability
252:01 - of the sample space uh plus
252:05 - uh this uh plus um the uh
252:09 - the sum from uh i equals one to infinity
252:13 - uh the probability of the empty set
252:17 - and what that forces us to then say is
252:20 - that since
252:21 - uh the probability of the sample space
252:22 - is equal to one we're forced then to say
252:25 - that the sum from i equals one
252:28 - to infinity of the probability of the
252:30 - empty set
252:32 - is equal to 0 but that's only going to
252:35 - be possible
252:37 - if the probability of the empty set
252:40 - is itself equal to 0.
252:44 - because since probabilities uh
252:47 - cannot um be uh
252:51 - negative and oh and even just because of
252:53 - the fact
252:54 - that uh we're adding up something an
252:57 - infinite number of times
252:58 - and that's and that's the exact same
252:59 - thing and it adds up to zero
253:01 - so that means that each one of those
253:03 - must be equal to zero
253:05 - therefore this uh axiom this uh
253:08 - proposition is proven
253:10 - this one this is actually rather weird
253:13 - it's
253:13 - it's so surprising like there is a way
253:16 - in which you
253:17 - you you kind of just want to say uh you
253:20 - have
253:21 - one equals to the probability of the
253:24 - sample space
253:25 - which is equal to uh the probability of
253:27 - the sample space
253:29 - plus the sample space unioned with the
253:31 - empty set and then it turns into a sum
253:33 - of probabilities
253:34 - and uh that means that probably the
253:37 - empty set is equal to zero
253:38 - but we have to in order to be
253:40 - mathematically rigorous
253:41 - use these axioms in the way they were
253:43 - written down
253:44 - so we would actually have written the
253:46 - axiom in a way in which it wasn't
253:47 - written and therefore wouldn't be able
253:49 - to
253:50 - directly apply it so we had to be really
253:52 - tricky but the good news
253:54 - is that we only had to be tricky really
253:56 - once
253:57 - with this uh proposition just with this
254:00 - one because now that we have
254:02 - this proposition the others will not
254:04 - nearly uh
254:05 - will not be nearly as uh strange for
254:08 - example proposition three
254:09 - like this one this proposition if we had
254:12 - this already
254:14 - then that proposition that we just
254:15 - proved would actually be rather easy
254:17 - because
254:18 - uh the complement of the sample space
254:21 - is equal to the empty set uh the
254:24 - probability of the
254:25 - the sample space uh is equal to one
254:28 - and since the uh uh the the empty set is
254:31 - the complement of the sample space we
254:32 - can say
254:33 - if this proposition 3 were in fact true
254:38 - the probability of the empty set is
254:40 - equal to
254:41 - 1 minus the probability of the sample
254:44 - space
254:45 - which is 1 minus 1 which equals 0.
254:49 - right if we had proposition three then
254:51 - it would actually be rather easy
254:53 - to prove that the probably the empty set
254:56 - is zero
254:56 - but the thing though is in order to be
254:59 - able to prove
255:00 - uh proposition three we're going to have
255:03 - to
255:04 - uh probably use uh some of those uh
255:07 - other propositions so
255:09 - unfortunately uh it's it's um
255:13 - it's not quite that it's we we
255:15 - technically need to
255:17 - take a very long circuitous route
255:20 - in our proof before we can actually
255:22 - invoke this one
255:24 - so what we're going to say is that the
255:27 - probability
255:28 - of the sample space we can say that the
255:30 - sample space can be divided
255:32 - into uh into the set a
255:36 - unioned with the complement of a
255:40 - and uh a union a complement
255:44 - um well okay a intersected with a
255:46 - complement
255:48 - um is equal to the empty set
255:52 - so since a intersected with uh its
255:55 - complement is
255:56 - is the empty set that means that these
255:57 - two sets are going to be disjoint
256:00 - which means that we can then invoke uh
256:03 - that um that a second that third axiom
256:07 - and say that the probability of the
256:09 - sample space is equal to
256:11 - the probability of a union a complement
256:16 - which is equal to by that that other
256:18 - axiom the probability of a
256:20 - plus the probability of a complement
256:24 - and uh by the first axiom this is all
256:26 - equal to one
256:28 - therefore after you do a little bit of
256:30 - algebra where you subtract
256:33 - uh you subtract both from both sides the
256:36 - probability of
256:37 - a
256:41 - uh after you do that algebra you can now
256:44 - say
256:45 - that the probability of a complement
256:48 - is equal to one minus the probability of
256:52 - a and we're done that proposition has
256:55 - been proven
256:57 - uh next proposition
257:00 - we have that the probability of a is
257:02 - less than or equal to 1
257:04 - for any event a let me get caught up in
257:07 - my
257:08 - notes over here
257:12 - all right uh so
257:15 - the probability of a complement
257:19 - since a complement is in fact an event
257:22 - by the first axiom uh we get to say that
257:25 - the probability of a complement
257:27 - is greater than or equal to zero
257:30 - which means that one minus the
257:33 - probability
257:34 - of a is going to be greater than or
257:36 - equal to zero
257:38 - and therefore it follows after oops
257:41 - after we add the probability
257:46 - of a to both sides
257:50 - after we do that uh we get to say
257:54 - that uh uh
257:57 - so since these two things end up
257:58 - cancelling out uh the probability
258:01 - of a is less than or equal to one
258:05 - so that proposition has been proven uh
258:09 - next one or is that it no that's not it
258:13 - uh the probability of a union b is equal
258:16 - to the probability of a plus the
258:17 - probability of b
258:18 - minus the probability of a intersected
258:19 - with b for any events
258:22 - a and b so
258:25 - uh this by the way is generalizing
258:28 - more or less that third axiom since that
258:31 - third axiom required disjoint events
258:34 - whereas here in this proposition we do
258:37 - not require
258:38 - disjoint events um we do not require
258:40 - just disjoint events and the penalty
258:42 - that we pay for that is that we have to
258:43 - subtract out
258:44 - the probability of the intersection
258:46 - here's the thing though uh this
258:48 - this makes perfect sense what's going on
258:50 - if we draw
258:52 - a venn diagram um
258:55 - it turns out that probability
258:59 - is part of this general class of
259:01 - mathematical objects
259:03 - or probability measures are part of this
259:05 - general class of mathematical objects
259:06 - known as measures
259:08 - and amongst that family of mathematical
259:10 - objects we include
259:12 - things such as measures of area or
259:14 - measures of length
259:15 - which means that it's actually rather
259:18 - appropriate
259:19 - and convenient to reason about
259:22 - probabilities the same way we reason
259:23 - about
259:25 - areas or lengths as we do in the real
259:27 - world
259:28 - so let's think instead about
259:31 - we have um a couple uh sets
259:35 - a and b we want to figure out what the
259:39 - area
259:39 - is in a and
259:42 - b like the that's enclosed within both
259:45 - of those we want to figure out that area
259:47 - um and how we're going to do so is uh
259:50 - we have a couple sheets we have an a
259:53 - sheet and a b
259:54 - sheet and we lay them down and we have
259:56 - some scissors
259:58 - so um we can measure the areas
260:02 - of these sheets so uh given that
260:05 - um we're able to measure the areas of
260:07 - these sheets how can we possibly compute
260:08 - the area that's enclosed
260:10 - in both a and b well what we would do
260:13 - is we would lay down the a sheet onto
260:17 - this diagram and we would have its area
260:20 - then we would lay down the b sheet on
260:23 - this diagram and we would have its area
260:25 - but the thing though is once we've done
260:27 - that we now have an
260:28 - overlap we have an overlapping area
260:31 - um so we've actually over counted
260:35 - the area um in a and b we the number is
260:38 - too large
260:40 - so what we need to do is subtract out uh
260:43 - the part the overlapping part of the
260:47 - a and b subtract out its area because
260:49 - otherwise we would have double counted
260:51 - it
260:52 - so we're going to uh subtract out that
260:55 - area um and uh or at least we'll like
261:00 - cut out
261:00 - a little slice of it um uh maybe just
261:04 - like the red slice part
261:06 - and leaving only the blue part left and
261:08 - and we're allowed to measure the area of
261:10 - the part that we cut
261:11 - out so after we uh subtract that
261:15 - the area that we removed we now have the
261:17 - area that's enclosed between
261:19 - uh that's closed within these two
261:21 - circles and
261:22 - that's an aerial way to understand
261:26 - why this formula here is in fact true
261:30 - because this is base because how i
261:31 - described it is
261:33 - basically what we're doing we have the
261:35 - area of the region a and the area of the
261:37 - region
261:38 - b and we subtract out once the area
261:40 - that's in between them since we
261:42 - accidentally well not accidentally since
261:45 - uh without doing so
261:47 - we would have double counted that part
261:50 - so we need to remove the double counting
261:54 - so how are now that's like a reasoning
261:58 - for what we're doing
261:59 - and now let's turn that into
262:03 - a series of mathematical statements and
262:05 - a proof
262:07 - here's a way to think about this region
262:10 - this venn diagram
262:11 - we can divide up in this venn diagram
262:17 - uh we can divide up uh this uh region so
262:20 - that we have
262:22 - uh the light blue region
262:25 - we have the green region
262:28 - and we have the red region
262:34 - we will call the blue region
262:37 - a and not b
262:42 - we will call the green region
262:45 - a and b and we will call the red region
262:50 - uh not a and b
262:54 - and it's clear that these three regions
262:57 - are um
263:00 - these three regions are disjoint you can
263:03 - see so visually but if you wanted
263:05 - a non-visual way to reason why that is
263:07 - the case let's suppose an element
263:10 - is let's suppose that we pick a point in
263:12 - that is in the set a
263:14 - and not b can that set be an a and b
263:17 - well it must not because in order b
263:19 - in the part a and b it must be b b end b
263:22 - uh but it is not in b since it's an a
263:25 - and
263:25 - not b so it cannot be in there and uh
263:28 - similarly for
263:30 - uh not a and b and for the same reason
263:32 - if you're in a and not b
263:34 - you cannot also be in not a and b since
263:36 - you're in a and therefore you're not in
263:38 - not
263:39 - a i know this is the verbiage is getting
263:41 - really complicated but
263:43 - that is that is really the logic for why
263:46 - these things
263:46 - must be destroyed or you can just look
263:48 - at the picture and be satisfied
263:50 - um for that reason we can say
263:54 - that the i mean you can also look at
263:56 - this picture and see that when you take
263:57 - the union of these three
263:59 - uh events uh you have in fact the region
264:02 - a or b
264:03 - and there's also no double counting here
264:05 - since you counted each little part at
264:07 - this each division once
264:09 - so now we can say i'm going to zoom in
264:12 - uh some more we can say that the
264:15 - probability
264:16 - of a or b
264:20 - is equal to invoking that third axiom
264:24 - the probability of a and
264:27 - not b plus
264:30 - the probability of a and
264:34 - b plus the probability
264:39 - of um uh
264:42 - not a and b
264:46 - and uh let's see let's
264:49 - see what how should i proceed
264:52 - next well i do notice for starters that
264:56 - this part right here is equal to the
264:58 - probability of a
265:01 - which is uh clear from the picture
265:04 - because if you add the probability of a
265:06 - and not b
265:06 - and the probability of a and b then
265:08 - you've basically counted the probability
265:10 - of a
265:11 - so this must be the probability of a uh
265:13 - but the thing is though
265:15 - uh we need to somehow account for
265:18 - uh i mean we end up in the end with a
265:20 - subtraction of the probability of a
265:22 - and b so how are we going to do that
265:25 - huh well if we look at the probability
265:27 - of b
265:28 - we could say as an aside that the
265:30 - probability of
265:31 - b is equal to the probability
265:34 - for basically uh the reasons that i just
265:37 - wrote here
265:39 - that this is the probability
265:42 - of um a
265:45 - and b plus the probability
265:50 - of uh uh not a
265:54 - and b and then if you do
265:58 - some algebra you can then say
266:01 - that the probability of not a
266:04 - and b is equal to the probability
266:08 - of b minus uh the probability
266:12 - of uh a and b
266:16 - okay oh well look at that we now
266:18 - basically have what we need because
266:20 - we've identified a part
266:21 - as the probability of a and we can also
266:24 - identify
266:25 - the latter part in this sum as the
266:27 - probability of b
266:29 - uh minus the probability of
266:32 - a and b which is exactly the statement
266:35 - that we wanted to prove
266:37 - so therefore we're done
266:40 - and we have in fact proven this
266:42 - statement and notice that this statement
266:45 - does in fact include that third axiom
266:47 - since
266:47 - if since that third axiom was about
266:50 - disjoint events
266:51 - so um if we have a disjoint event we've
266:56 - proven in uh in
266:59 - our the first proposition that we proved
267:01 - in this section that the probability
267:03 - of the empty set is equal to zero and
267:06 - the subtraction of the probability of a
267:09 - and b
267:10 - is going to give you um
267:13 - so so that's going to uh so the
267:16 - intersection of a and b
267:18 - if and since in this imaginary scenario
267:19 - they're uh disjoint that's going to be
267:22 - the probably the empty set which is
267:23 - equal to zero and thus you get
267:26 - uh that uh that other axiom
267:32 - all right so uh oh oops i am
267:35 - uh i am doing something that i don't
267:38 - want to do
267:39 - what i want to do is zoom out
267:46 - okay so the next proposition i'm not
267:49 - going to bother to prove
267:51 - just because it's a lot of work although
267:53 - i may give you an
267:54 - argument for why it's true so
267:56 - proposition 6 now we have
267:58 - three events a b and c and the
268:01 - probability of the union of those three
268:03 - events is going to be
268:04 - the sum of the probabilities of the
268:06 - events minus the probabilities of
268:08 - intersections of two
268:10 - plus the probability of the intersection
268:11 - of all three all right so
268:14 - i'm not going to prove this but i'm
268:15 - going to give you an argument for why
268:16 - this must be true
268:18 - so here we have our sample space um we
268:21 - have the set
268:23 - a we have the set b
268:26 - and we have the set c it's basically the
268:29 - same reasoning as we used before
268:32 - um let's uh uh let's
268:35 - see so uh so what does it mean to add
268:39 - the probability of a probably be
268:40 - probably c so if we
268:42 - add those three probabilities so we're
268:44 - gonna add the probability of a
268:47 - because what we're trying to do is
268:48 - figure out in the area that is enclosed
268:51 - in all three circles uh but those
268:54 - circles are overlapping with each other
268:55 - so we need to figure out how to handle
268:56 - the overlaps so we're gonna color in the
268:58 - probability of a that's what we're told
268:59 - to do first
269:00 - uh then we're gonna color in the region
269:03 - and close by b
269:04 - that's what we need to do second and
269:06 - then we're going to enclose
269:07 - the region enclosed then we're going to
269:09 - color in the region and close by c
269:13 - because that's what we've been told to
269:15 - do
269:16 - yeah you can almost read this as a set
269:18 - of instructions on how to calculate an
269:20 - uh how to calculate um
269:23 - uh um uh an area
269:27 - so we see that we have done some double
269:30 - counting
269:31 - we've double counted here we've double
269:33 - counted here and we double counted here
269:35 - all right so we need to remove those
269:39 - double counts
269:40 - uh let's start with the um let's uh
269:43 - let's start with the double counting
269:45 - here we're going to subtract
269:48 - out uh the probability of a and b which
269:52 - will i will under
269:53 - i will understand that as subtracting
269:54 - out a little green sliver
269:57 - so how does this i'm wondering if this
270:00 - is doing something oh
270:01 - no it just was being laggy
270:05 - all right so um
270:09 - so i have subtracted out the green area
270:11 - from that sliver
270:13 - uh leaving a blue left
270:17 - and some red red left
270:20 - okay uh and then i need to
270:24 - i'm gonna cut out the uh
270:27 - red area that is intersecting and that
270:30 - is at the intersection of a
270:31 - and c okay so i'm going to cut that
270:35 - out uh leaving only blue
270:38 - uh um leaving only blue there
270:43 - uh let's see so we're going to have
270:46 - uh right here
270:50 - so and then we need to uh
270:53 - cut out the area that is in the
270:55 - intersection of
270:56 - b uh and so we now need to cut out the
271:00 - uh
271:01 - region that's in uh both b and c
271:04 - so we're going to subtract out uh the
271:06 - red area but the thing is that doesn't
271:08 - that doesn't uh that isn't enough we
271:10 - cannot just take out the red arrow
271:12 - because we need to take out
271:13 - everything that's in here so we also
271:14 - have to take out what that little blue
271:16 - sliver that's left as well
271:18 - so uh we're going to be left with green
271:22 - in that in that spot uh but the thing is
271:25 - in that little sliver
271:29 - uh that that is in the intersection of
271:32 - all three
271:33 - we have now cut out oops uh we have now
271:36 - cut out too much and
271:38 - that area is not being accounted for at
271:41 - all there's nothing left in there
271:43 - so we need to add that area
271:47 - back in
271:50 - in order to be able to have an accurate
271:51 - computation of the area
271:54 - and there we go so after we add it back
271:57 - in uh we now have the area so everything
271:59 - has been colored
272:00 - uh exactly once and uh we're able to
272:04 - compute the area of
272:06 - how much area we've colored uh in a
272:08 - sense so we
272:09 - have what we need so not exactly a proof
272:11 - because we need to
272:12 - uh do some do some of that a tricky
272:14 - algebra business but
272:16 - uh we're not gonna bother with that this
272:18 - should give you an idea of why it's true
272:21 - okay uh so the next example that all of
272:24 - that stuff was uh
272:25 - uh rather theoretical uh let's let's um
272:29 - start seeing how uh probability theory
272:32 - is actually going to be used so for the
272:34 - most part of the remainder of the
272:36 - section i'm going to be going through a
272:37 - number
272:38 - of illustrative examples uh we do talk a
272:41 - little bit more
272:42 - about uh theory mostly about what
272:44 - probability means
272:46 - uh but uh for the most part we can just
272:48 - talk about
272:49 - um examples so example nine reconsider
272:53 - the experiment of flipping a coin
272:55 - and assume that the coin is equally
272:57 - likely to land
272:58 - with each face facing up assign
273:00 - probabilities to all
273:01 - outcomes in the sample space so recall
273:04 - that the sample space
273:06 - for this experiment consists of the
273:08 - outcome heads and the outcome tails
273:11 - and we're assuming that all of the
273:12 - outcomes in this sample space
273:14 - are equally likely so that means that
273:16 - the probability
273:18 - of uh getting heads uh and
273:21 - i'm writing this in terms of a simple
273:23 - event but honestly
273:25 - this writing it this way often gets
273:28 - rather tedious so
273:29 - i'm often going to admit uh to omit the
273:32 - curly braces that are usually used
273:34 - to denote sets and just write whatever
273:36 - is in the set
273:37 - so we have the probability of h uh this
273:40 - is going to equal the probability
273:42 - of t because all outcomes are equally
273:45 - likely
273:46 - so the thing though is the probability
273:48 - the sample space
273:51 - is equal to the probability
273:54 - of the set containing only h unioned
273:58 - with the set containing only t since
274:00 - this
274:00 - uh so since the union of those two sets
274:03 - is in fact equal to the sample space
274:05 - and furthermore those two sets have
274:06 - nothing in common one has h one has t
274:08 - and they don't share anything so that
274:10 - means that they are disjoint events
274:12 - and therefore we can write this
274:13 - probability as a sum
274:15 - as the probability of h
274:19 - plus the probability of t and both of
274:22 - these are the same so we're going to say
274:23 - that this is equal to p
274:25 - so this is equal to uh p plus
274:29 - p which is two p and furthermore we know
274:33 - from
274:34 - uh axiom two that the probability of the
274:36 - sample space
274:37 - is equal to one so this is equal to one
274:40 - and that implies after you do some
274:42 - division by two
274:44 - that p is equal to one half
274:49 - which makes perfect sense right if
274:51 - you're saying that heads is just as
274:52 - likely as tails then the probability of
274:54 - getting heads is one
274:55 - one half perfectly intuitive right well
274:58 - you know
274:58 - uh probability is intuitive up until the
275:01 - point it isn't
275:02 - and when it stops being intuitive it
275:04 - really stops being intuitive
275:06 - and it becomes suddenly very very
275:08 - strange so it's simultaneously intuitive
275:11 - and
275:11 - unintuitive in fact i've heard someone
275:13 - say that
275:14 - there are fewer subjects with as many
275:17 - paradoxes they're not literally
275:19 - paradoxes but they contradict
275:21 - how humans think of the world very few
275:24 - subjects have as many paradoxes as
275:26 - probability
275:27 - because probability can get really weird
275:29 - really fast we have not reached that
275:31 - point yet but we probably will
275:32 - eventually
275:33 - example 10 do the same as example 9 but
275:37 - when rolling a single dice that is
275:39 - um we are trying to
275:43 - so we we're trying to assign
275:45 - probabilities to outcomes in a sample
275:46 - space when we say that those outcomes
275:48 - are equally likely
275:49 - so the sample space in this case is
275:52 - going to consist of die rolls
275:54 - um so we have uh an outcome one
275:58 - a two uh three
276:04 - four
276:08 - five
276:12 - and six
276:19 - okay so those are our six outcomes and
276:22 - we say that each one of them is equally
276:24 - likely so that means that the
276:25 - probability
276:26 - of rolling a one is the same as
276:30 - is uh the same as the probability
276:33 - of rolling a two and that's going to be
276:37 - the same as
276:38 - dot dot dot as this and the same as
276:41 - uh the probability of rolling a six
276:51 - all right so uh it's a six so
276:54 - all right there we go six it's a it's an
276:57 - oddly painted die
276:58 - all right so all of those are going to
277:00 - be the same and we're just going to say
277:01 - for
277:02 - convenience that this is equal to p all
277:04 - right
277:05 - so then we say that the probability
277:08 - of the sample space is equal to the
277:12 - probability of
277:14 - the result one uh unioned with the
277:18 - result
277:20 - uh two union with
277:23 - dot dot dot unioned with uh
277:28 - the uh outcome uh containing
277:32 - uh six
277:37 - there we go
277:41 - because that's equal to the sample space
277:43 - you basically written the sample space
277:44 - as a union of each of its elements
277:46 - and furthermore each of these sets again
277:48 - are disjoint
277:50 - so uh we can then write them as the sum
277:53 - of probabilities so say that this is
277:55 - equal to the probability
277:56 - of rolling a 1 plus the probability
278:00 - of rolling a 2
278:04 - plus dot dot dot plus the probability
278:08 - of rolling a six
278:15 - and then we use the fact that we said
278:17 - from the beginning that all these
278:18 - probabilities are the same
278:20 - so this is p plus p plus p plus p plus p
278:23 - plus p so this is equal to six p but
278:26 - it's also equal to one
278:27 - because of axiom two that says that
278:29 - probably the sample space is equal to
278:30 - one
278:31 - therefore p must be equal to one over
278:34 - six
278:36 - and you can see where this is going in
278:39 - general if you have a sample space with
278:41 - a finite number of elements
278:43 - uh and you say each of those elements
278:45 - are equally likely
278:46 - then the probability of a single one of
278:49 - those elements
278:50 - in that sample space probably of drawing
278:52 - that element is going to be 1 divided by
278:54 - the size of the sample space or the
278:56 - number of elements number of unique
278:57 - elements in that sample space
279:00 - um so in fact what we're seeing here
279:03 - can be very easily generalized
279:07 - okay uh so example 11 we're now
279:11 - going to try and move away
279:14 - from equally likely outcomes and say
279:17 - that the dice from example 10 has been
279:19 - altered with weights
279:21 - now the probability of the dice rolling
279:22 - a 6 is twice as likely as rolling a 1
279:24 - while all the other sides have the same
279:26 - probability of appearing as before
279:29 - therefore we want to know what the new
279:31 - probability model is
279:33 - so we're going to say that the
279:34 - probability
279:36 - of rolling a six
279:41 - so one two three
279:44 - four five six
279:47 - so the probability of this of rolling a
279:49 - six is equal to
279:50 - two times the probability of rolling
279:54 - a one and we're just going to say that's
279:58 - equal to two
279:58 - times p or uh
280:02 - we'll just we'll try to keep things a
280:04 - little bit different we'll call it q
280:05 - this time
280:06 - all right so we need to figure out q
280:08 - because q gives us the probability of
280:10 - rolling a one
280:11 - and if we figure out the probability of
280:12 - rolling a one we then instantly know
280:14 - rolling a six
280:15 - now remember these are the only two
280:17 - faces that have been altered
280:19 - all the other dice faces have the exact
280:21 - same probability as before so the
280:23 - probability of rolling a two
280:24 - is equal to the probability of rolling a
280:26 - three which is the probability of
280:27 - rolling a four which is probably ruling
280:28 - a five and all
280:29 - all of those are equal to one over six
280:32 - so that means um that the probability
280:37 - of rolling a one uh plus the probability
280:41 - of rolling a two
280:44 - plus dot dot dot plus the probability
280:48 - of rolling a 5
280:52 - plus the probability of rolling a 6.
281:01 - come on cooperate you stupid screen
281:06 - i hate drawing the 6 because my screen
281:08 - is not very cooperative
281:10 - all right uh this is equal to one
281:14 - but uh what's different now is that all
281:17 - of these are equal to one over six
281:19 - so you end up adding one over six four
281:22 - times
281:22 - since it's two three four five there's
281:24 - four things there um
281:26 - the probability of rolling a six is
281:28 - equal to two q
281:30 - and the probability of rolling a one is
281:32 - equal to q
281:34 - okay so um collecting all of that
281:37 - information
281:39 - we now say um that we have a
281:44 - q plus 2q
281:48 - plus 4 over 6 which is 2 over 3
281:53 - is equal to 1 which implies
281:57 - after you do some algebra that 3q
282:00 - is equal to 1 3 which then means
282:04 - that q is equal to ah no is equal to
282:08 - one over nine not nine because that's
282:10 - not even a probability
282:12 - uh that's that's that's one thing to
282:13 - keep in mind if you get a probability
282:15 - that's above one you have made a mistake
282:17 - so nine is not possible the problem so
282:20 - that means that the probability of
282:21 - rolling a one
282:23 - so yeah let's let's uh write this down
282:25 - now the probability of rolling a one
282:27 - in this new probability model is equal
282:29 - to one-ninth
282:30 - and the probability of rolling a six in
282:32 - this new probability model
282:38 - is equal to two over nine
282:42 - and in fact this isn't this is
282:43 - consistent because
282:45 - one over nine plus two over nine is
282:47 - equal to three over nine which is one
282:49 - third
282:50 - there are the uh other dice faces when
282:53 - you add up their probabilities you have
282:55 - one over six one over six one over six
282:56 - one over six which is two-thirds
282:59 - and so you have one-third plus
283:00 - two-thirds which equals one
283:02 - so the probabilities still add up to one
283:05 - which means that we're fine
283:06 - this is a way for you to check uh
283:08 - whether this is one way for you to check
283:10 - that you're that when you do your
283:12 - probability calculations you have done
283:13 - so correctly
283:14 - make sure that all the probabilities add
283:16 - up to one if they don't add up to one
283:19 - you've made a mistake i remember once a
283:21 - student was
283:22 - working in a i had a student of mine and
283:25 - i gave her a quiz
283:26 - and she had to do some calculations with
283:28 - probabilities and
283:30 - she added up the probably sample space
283:32 - and it added up to
283:33 - something that was to some fraction it
283:35 - might have been
283:37 - uh maybe uh eight over nine or eight
283:40 - like 80 or 81 who knows
283:42 - but but i said this is very wrong and
283:45 - she's like well
283:46 - it needs to add up to one and she said
283:48 - well it's close to one
283:49 - and i said close to one is not one
283:53 - so if it doesn't add up to one it's just
283:56 - wrong
283:57 - it like close to one no caboose it's
284:00 - either one or it's not one
284:02 - if it doesn't add up to one it's wrong
284:04 - it's very wrong
284:06 - it always adds up to one so that's a way
284:08 - for you to check
284:09 - that you've done things correctly make
284:10 - sure that your probabilities add up to
284:13 - okay moving on
284:17 - example 12 reconsider the experiment of
284:19 - rolling two six sided die
284:21 - it is reasonable to assume that each
284:22 - outcome in s is equally likely this is
284:24 - the reason why
284:25 - instead of writing the numbers 2 through
284:28 - 12 or maybe just
284:30 - listing out without really caring about
284:32 - the ordering of the die
284:34 - or not really thinking about there being
284:35 - a red dye and a blue dye
284:37 - we we decided that we were actually
284:39 - going to assign an ordering to the dice
284:41 - it's so that we could basically work
284:42 - with problem
284:43 - example 12 and get a reasonable looking
284:46 - probability model
284:47 - because now if we assume that the two
284:49 - dice are distinct we can now say that
284:51 - everything in the sample space
284:53 - in that sample space with a red and blue
284:55 - die is equally likely
284:56 - and then get accurate probability
284:59 - calculations
285:00 - so uh getting back to this it is
285:03 - reasonable to assume that each outcome
285:05 - in the sample space s is equally likely
285:08 - what then
285:08 - is the probability of each outcome in s
285:12 - um well basically i already argued it uh
285:14 - to you before we'll say that omega
285:16 - is an element of s so this
285:20 - i'm saying this in general right um
285:23 - omega is the element is an element in in
285:26 - s
285:26 - and every element in s is equally likely
285:29 - i argued before without writing it down
285:31 - that the probability of drawing omega
285:34 - then
285:35 - will be 1 divided by the size of
285:39 - s so in this case for example 12
285:43 - where you have two six-sided dice um
285:46 - the size of the sample space was 36 so
285:49 - the probability of a sink
285:50 - of a of a particular outcome of dice is
285:53 - going to be 1 over 36.
285:55 - now we can use this uh this model
285:59 - uh to find the probability of an event e
286:03 - where so i'll i'll just go ahead and
286:05 - write down in this case
286:06 - that um the size of the sample space s
286:09 - is equal to 36
286:11 - so for this particular problem this is
286:13 - going to be 1 over 36
286:18 - but in fact what i just wrote down here
286:19 - true what i just wrote down here is
286:22 - pretty much drew in general
286:24 - like um i'm i
286:27 - i'm thinking in the context of uh
286:29 - rolling a couple die but
286:31 - actually this is true in general when
286:33 - you say
286:34 - that s which is a finite sample space it
286:36 - has a finite number of elements
286:38 - um when everything in that sample space
286:40 - is equally likely
286:42 - um then the probability of an individual
286:44 - individual element is going to be one
286:45 - over the samples
286:46 - over the size of the sample space and
286:48 - it's not too hard why go ahead
286:50 - uh show that the uh probably the sample
286:52 - space when you do this is equal to one
286:55 - uh so use this model to find the
286:57 - probability of an event e
286:59 - where first e is the event where at
287:01 - least one die
287:02 - is a six so let's actually write down
287:06 - um let's write down
287:10 - uh what's in e so that would be we have
287:13 - a red dye
287:14 - and a blue dye at least one dye is six
287:17 - so
287:18 - that includes when the red dye is
287:21 - one and the blue dye uh is
287:24 - six
287:28 - we have uh the case when
287:32 - the red die is two
287:35 - and the blue die is six
287:44 - we have and we can keep going on like
287:46 - this
287:47 - until we eventually reach the case where
287:51 - uh the the red die
287:54 - is a five
288:02 - the red die is five and the blue die
288:06 - is six
288:13 - yeah damn it you cooperate
288:17 - all right that's six um
288:20 - so uh that's one set of outcomes and
288:23 - notice that there's
288:24 - uh that i've just listed down five
288:27 - elements
288:29 - okay uh so next
288:32 - up we will now make the red dye six
288:35 - so we've got one two three
288:39 - four five six
288:42 - it never wants to do the last one never
288:45 - ever wants to do the last one
288:47 - all right so one two three four five six
288:50 - and the blue die
288:51 - is one and
288:55 - uh just just repeat just keep uh
288:58 - carrying on with this logic until we get
289:03 - where the red dice is six
289:08 - and the blue dice is five
289:14 - and finally we have one last outcome
289:17 - where both the red dye and the blue dye
289:21 - are six so one two
289:24 - three four five six and
289:28 - uh one two three four five
289:31 - six all right
289:34 - so
289:37 - uh there are
289:41 - um five elements where
289:45 - the uh um
289:48 - where the red die is fixed at six and
289:50 - then there's
289:51 - one extra element where
289:54 - uh both of them are six so that means
289:57 - that the size
289:58 - of this event is going to be um
290:02 - 11. which then means when we're
290:05 - computing the probability of this event
290:08 - we could add up the probability of each
290:10 - one of these outcomes
290:12 - and each one of these outcomes has an
290:13 - equal likelihood and
290:16 - they all have an equal probability all
290:17 - those probabilities are 36 so you're
290:19 - gonna add up 36
290:20 - and one over 36 11 times so this is
290:23 - going to add up to 11 over 36.
290:28 - and in fact it's once you have this uh
290:32 - this um assumption that
290:35 - all outcomes in this finite sample space
290:38 - are equally likely in general the
290:41 - probability of any event
290:42 - e is going to be the number of elements
290:45 - in e
290:46 - or the size of e divided by the size of
290:48 - the sample space
290:50 - so at this point all we need to do is
290:52 - decide how to determine how many
290:53 - elements are in our event
290:55 - in order to figure out that events
290:56 - probability all right so
290:58 - for let's let's use that now for uh
291:02 - this uh next problem where e is the set
291:06 - where is the event where the sum of the
291:07 - pips showing on the two die is uh
291:09 - five how many outcomes are there where
291:12 - the sum
291:14 - is going to be five let's start listing
291:16 - out
291:17 - uh possible things in this sample space
291:24 - so uh we've got i guess i've been
291:26 - writing the
291:27 - red dice first
291:33 - okay so
291:37 - okay so this is all right so i've been
291:40 - writing out the red dye first
291:41 - so let's suppose let's see the the two
291:44 - dice is five so can the red dice be one
291:45 - yeah sure why not
291:47 - uh the red dice can be one and if it's
291:49 - one well it must add up to five so that
291:51 - means that the blue dice must be four
291:54 - okay that's one possibility uh next
291:57 - possibility is when the
291:59 - blue dice is two and which is when the
292:01 - red dice is two sorry in which case the
292:03 - blue dice must be three
292:05 - uh we could have the case where the red
292:07 - dice is three
292:08 - in which case the blue dice must be two
292:12 - and finally we have the case
292:15 - where the red dice is four
292:20 - in which case the blue dice must be one
292:22 - and we can't go to five
292:24 - because it's not possible for the blue
292:26 - blue dice to
292:27 - roll a zero um so we're just gonna have
292:30 - to end it there
292:32 - uh we now have uh a sample
292:35 - an event and this event has four
292:37 - outcomes in it
292:39 - so that means that the size of the event
292:43 - is going to be a four in which case
292:47 - the probability of the event e
292:50 - is going to be uh 4
292:53 - over 36 which is going to be
292:57 - uh 1 over 9.
293:03 - okay next part uh
293:07 - e is the event where the maximum
293:10 - of the two numbers showing on the dice
293:12 - is greater than two
293:14 - okay now here's the thing though um
293:17 - there's actually a lot of outcomes in
293:18 - this event so
293:20 - we could attempt to try and figure out
293:23 - how many
293:24 - outcomes there are where
293:27 - the larger of the two numbers is greater
293:30 - than 2.
293:32 - well let's see we could even have like
293:33 - three one three two three three three
293:35 - four three or five three six
293:36 - all the all the cases where the red dice
293:38 - is three and that's already six
293:40 - such outcomes and that's only for three
293:42 - so
293:43 - uh this could actually be quite
293:46 - difficult to compute
293:48 - except for the fact that it's actually
293:50 - much easier to compute
293:51 - the comp the probability of the
293:53 - complement of this event
293:54 - if we were to look at the complement of
293:56 - the event e
293:58 - that's going to be the event where the
294:00 - maximum um
294:02 - of the two dice is
294:06 - um not greater than two
294:09 - it so it's at most two okay
294:16 - uh where what are what are combinations
294:19 - of dice facing
294:20 - faces where the maximum is at most two
294:23 - well we have one situation where
294:25 - uh we have um oh yeah i said that the
294:28 - red dice is first
294:30 - so we have the situation where the red
294:32 - dice is one
294:33 - and the blue dice is one
294:37 - all right in that case the maximum of
294:38 - the two dice is going to be one
294:40 - and that is uh not greater than two so
294:43 - this is in fact
294:44 - in our event uh then we have the outcome
294:48 - where the red dice is two and the blue
294:51 - dice is one
294:53 - in that case the maximum would be two
294:55 - and that doesn't exceed two
294:57 - so this is in our event we also have
295:01 - uh the case where the red dice is one
295:05 - and the blue dice is two
295:08 - let's see all right there we go so the
295:10 - blue dice is a two
295:13 - and we also have the outcome
295:16 - where the red dice is two and the blue
295:20 - dice is two
295:22 - and the maximum of those in this case
295:24 - would be two and again that's uh at most
295:26 - two
295:26 - and we have to stop there because then
295:28 - because the the next thing we would do
295:30 - is make one of the dice
295:31 - at least three and in which case the
295:34 - maximum would be
295:35 - at least three so it's not long going to
295:37 - be
295:38 - in this event so therefore we have an
295:41 - event with four outcomes in it and the
295:42 - probability
295:43 - of the complement of the event e
295:47 - is going to be the number of things in
295:49 - the complement of e
295:50 - uh which is four divided by 36 which is
295:53 - equal to
295:54 - one over nine all right we're cooking
295:56 - because now we can use one of those
295:58 - propositions that said that the
295:59 - probability of the complement of an
296:01 - event
296:01 - is one minus the probability of the
296:02 - event so that means that the probability
296:06 - of e itself which is what we actually
296:08 - want to compute a probability of
296:11 - that's going to be 1 minus the
296:13 - probability
296:14 - of e complement because e is the
296:17 - complement of
296:18 - e complement so and that's going to be
296:21 - 1 minus 1 over 9 which is
296:24 - 8 over 9. there we go
296:28 - we've we've solved it and that's much
296:31 - easier
296:32 - than if we tried to do it directly and
296:34 - this is one reason why you
296:35 - care about uh these uh complementation
296:37 - rules because sometimes
296:39 - it's easier work to work with the
296:40 - complement of an event rather than the
296:41 - event itself
296:43 - if we had tried to work with that event
296:44 - uh we would have ended up with a sample
296:46 - space with uh
296:47 - or an event with um
296:50 - 32 elements or or a size of 32 so we
296:54 - would have ended up having to count 32
296:56 - things
296:57 - i mean it's not like impossible to count
296:58 - 32 things but it's also a lot more work
297:01 - so so this is a very good trick to have
297:05 - and something that you should be looking
297:07 - out for when you're doing your own work
297:09 - you should be looking out for situations
297:11 - where the compliment is actually easier
297:12 - to work with
297:13 - than the actual event itself okay
297:17 - uh just real quick uh satisfy my
297:19 - nervousness
297:21 - all right we're still streaming all
297:23 - right so we shall continue
297:26 - example 13 reconsider the experiment of
297:29 - flipping a coin until heads is seen ooh
297:31 - this one this one's going to get rather
297:34 - involved
297:35 - what is one way to assign probabilities
297:37 - to all
297:38 - outcomes of this experiment so that we
297:41 - have a legal probability model
297:43 - justify your answer so
297:46 - how could we possibly do this because
297:50 - we no longer can say that each
297:54 - outcome in the sample space is equally
297:56 - likely because that only works
297:58 - when the sample space is finite but as i
298:01 - mentioned before
298:02 - this sample space is infinite um there's
298:05 - an infinite number
298:06 - of outcomes in the sample space since
298:08 - it's basically the integers in a way
298:10 - is you can map the sample space to the
298:12 - integers and kind of identify it with
298:13 - the integers
298:14 - or not the integers but the natural
298:16 - numbers
298:17 - and in fact i think that's what we
298:19 - should do we
298:21 - should uh try to view this sample space
298:25 - in terms of the natural numbers i'm
298:28 - going to want to zoom in for this one
298:31 - zooming in for me is a way to get to
298:34 - almost get more room
298:35 - on this piece of paper so
298:39 - we're going to say um
298:42 - we're going to define n of omega
298:46 - so omega is an element of this sample
298:50 - space
298:51 - so this is going to be one of those
298:53 - strings heads tails heads tails tails
298:55 - heads
298:56 - tails tails tails heads and so on so
298:59 - we'll say that um
299:02 - n of omega is
299:06 - going to equal the length
299:11 - of the string
299:17 - omega so for example
299:21 - uh n of the string t t
299:24 - h this is a string of length
299:27 - three
299:31 - it then follows that um using abusing
299:34 - notation a little bit because
299:36 - n is a function that doesn't take sets
299:39 - as inputs
299:40 - but let's suppose for a second that we
299:42 - were to put a set as the input this is
299:44 - actually pretty commonly done
299:45 - uh often when you have a function and
299:47 - you plug in
299:48 - uh its domain what you're talking what
299:51 - you're
299:52 - actually referring to is a set
299:53 - representing the range of that function
299:56 - so in this case uh the range of the
300:00 - function
300:01 - n is going to be the natural numbers
300:04 - it's going to be one
300:06 - two three four and
300:10 - so on these are all the possible
300:12 - outcomes it's going to be the natural
300:14 - numbers
300:15 - okay so um
300:19 - i'm going to suggest that what we should
300:22 - do
300:22 - for our probability assignment is say
300:25 - that the probability of an outcome omega
300:28 - is equal to uh
300:32 - one-half to the power
300:35 - of uh n of omega so in other words it's
300:39 - going to be one half to the power of the
300:40 - length of the string omega
300:43 - so for example in the in this earlier
300:45 - case where we had t
300:46 - t h the probability of that outcome
300:49 - tth would be uh one half to the power of
300:53 - the length of the string tth or one half
300:54 - to the power of three
300:55 - which is one over eight okay
301:01 - all right then so this is a suggestion
301:03 - for what the probability should be
301:06 - but now we need to make sure that this
301:08 - is a valid probability measure
301:10 - so what is what needs to be true in
301:12 - order for this to be the case
301:14 - first off is the probability of um
301:17 - are all probabilities greater than or
301:18 - equal to zero under this method yes
301:20 - that is certainly true because there's
301:22 - no way that this function will produce
301:23 - negative numbers
301:24 - uh secondly uh is the probability of the
301:27 - sample space
301:28 - equal to one uh well that's actually
301:31 - something that we're probably going to
301:32 - have to check
301:33 - and then there's that third axiom about
301:35 - um the probability of a
301:38 - excuse me uh about the probability of
301:40 - unions of uh
301:41 - disjoint events and i'm not going to
301:43 - check that one because that one's
301:45 - actually getting rather complicated
301:46 - that's getting even more theoretical a
301:49 - bit too theoretical for this class
301:51 - but i personally think it's perfectly
301:53 - appropriate to check
301:55 - that under this probability model the
301:57 - probability of the sample space is equal
301:58 - to one
302:00 - um and in fact in my classes
302:04 - this is actually something that i love
302:06 - to ask
302:07 - questions about on quizzes i love to ask
302:11 - students
302:12 - that the probability of the sample space
302:14 - is equal to one
302:15 - under some probability model i love
302:18 - asking that
302:20 - so if you are in my class you should
302:23 - expect a question like that
302:24 - you should expect me to ask you to show
302:28 - that the probability of the sample space
302:29 - is one under some
302:32 - potential probability measure or
302:34 - alternatively
302:35 - very very similarly i might ask you
302:39 - to i might give you a probably measure
302:41 - but it depends on some
302:42 - unknown constant and i might ask you
302:45 - to compute what the constant is that
302:48 - causes this
302:49 - the probability measure to be a valid
302:50 - probability measure that is the
302:52 - probability of the sample space would be
302:53 - one
302:54 - under that measure all right so so this
302:56 - is something to look out for if you're
302:58 - actually taking classes from me
302:59 - uh but let's uh get back to the issue at
303:02 - hand i need to show
303:03 - that the probability of the sample space
303:06 - under this probability model
303:08 - is in fact equal to one so the
303:10 - probability of the sample space here
303:12 - is going to be the pr is going to end up
303:14 - being the sum
303:18 - um i i can basically view the sample
303:21 - space
303:22 - as consisting of you of
303:25 - as being the union of all of these
303:28 - of events uh con where each of these
303:31 - events contains
303:32 - uh one of these uh strings h t t
303:36 - h t t t t h something like that uh and
303:39 - this
303:39 - is actually a situation where that third
303:42 - form of
303:43 - where the way i wrote down axiom three
303:46 - uh before you really actually need it
303:49 - the way
303:50 - i wrote it down originally in like the
303:52 - body of the lecture notes rather than
303:54 - that
303:54 - footnote because we actually do in fact
303:57 - here
303:58 - have an infinite collection of events so
304:01 - i'm going to write this is going to be
304:03 - the sum over all the
304:05 - all of the omega that's in the sample
304:07 - space of the probability
304:09 - of that omega right so remember what
304:13 - this is actually doing is summing up the
304:14 - probability of
304:15 - h the probability of th the probability
304:18 - of t
304:18 - t h the probability of t t t h and so on
304:22 - all right uh continuing on uh we can
304:25 - then say
304:26 - that this is uh equal to
304:30 - um the sum over all
304:33 - omega in the sample space
304:36 - uh we have our probability assignment
304:39 - this is one half
304:41 - to the power of uh n
304:44 - of omega and at this point i'm just
304:47 - going to say that n of omega
304:49 - is equal to the length that sample is
304:50 - equal to length that string
304:52 - i can simplify what i'm writing down
304:55 - here a little bit
304:56 - by basically writing down uh what the
304:59 - image
305:00 - of n is uh under
305:03 - uh uh over the set s so i can now write
305:07 - equivalently
305:09 - that this is going to be the sum when uh
305:12 - n equals one to infinity
305:16 - of uh one half
305:19 - to the power n and now you should
305:22 - recognize from i think they talk about
305:24 - this in math 1010 at the university of
305:26 - utah
305:27 - intermediate algebra that this is a
305:30 - geometric cell
305:31 - this is a geometric sum and there is a
305:34 - formula
305:35 - for finding uh the value
305:38 - of of a geometric sum so recall
305:42 - this formula from your previous classes
305:46 - you have a a number r such that
305:50 - uh the the magnitude of r does not
305:53 - exceed one
305:54 - then the sum from uh
305:58 - n equals uh we'll say 1
306:01 - to infinity of r to the power n
306:04 - is equal to r over 1 minus
306:08 - r remember that well we're going to use
306:11 - that
306:12 - here right now and we're going to say
306:14 - that this sum
306:15 - is equal to one-half over one minus
306:18 - one-half
306:19 - which is equal to one-half divided by
306:22 - one-half
306:23 - which is equal to one which is what we
306:26 - wanted to show
306:27 - we have now shown that the probability
306:30 - of the sample space
306:31 - under this probability measure is equal
306:33 - to one all right hopefully you have
306:35 - written down this formula if you don't
306:37 - remember it
306:39 - because i need to reclaim that space now
306:42 - that we have this
306:43 - we can now start answering some
306:44 - questions now that we know that this is
306:46 - valid probability model we can start
306:47 - using it
306:48 - uh so under this model what is the
306:50 - probability that the number of flips
306:52 - needed to see the first head uh exceeds
306:55 - four
306:56 - so what i'm asking for is the
306:57 - probability
306:59 - of um i'm gonna have to write this
307:03 - in a somewhat funny looking way go away
307:06 - stop
307:06 - stop bothering me i have to write this
307:08 - in a somewhat funny looking way
307:10 - i'm going to say that this is the
307:11 - probability of drawing an omega from the
307:14 - sample space
307:15 - uh or or drawing a sequence of flips
307:18 - such that um the length of that sequence
307:23 - is greater than four
307:26 - and this is another one of those
307:27 - situations where it's actually easier to
307:29 - work with the complement
307:30 - and say that this is equal to one minus
307:33 - the probability
307:34 - of drawing a string of flips
307:37 - such that the length of that string
307:41 - is less than or equal to four okay
307:44 - this is actually a finite set because we
307:47 - can actually list out the strings of
307:48 - flips
307:49 - in which uh
307:50 - [Music]
307:52 - in which the length of string is less
307:54 - than or equal to four we have heads
307:56 - we have tails heads we have tails tails
307:58 - heads and tails tails tails heads
308:01 - so this is one minus the probability no
308:03 - one minus
308:05 - uh the probability that you get heads on
308:07 - the first flip
308:08 - plus the probability that you get tails
308:11 - heads
308:12 - uh first tails then heads uh plus the
308:14 - probability
308:15 - they get tails and tails and heads and
308:18 - then you have
308:19 - the probability of three tails
308:24 - and a head
308:27 - and we can in fact figure out uh what
308:30 - each of those probabilities are
308:31 - the first probability is going to be one
308:34 - half
308:34 - the second is going to be one half to
308:37 - the power two or one fourth
308:38 - the third is going to be one half to the
308:40 - power of three or one eighth
308:42 - uh 1 8 and the last one is going to be
308:45 - one half to the power of 4
308:46 - or 1 16 and long story short
308:51 - uh this is going to be equal to
308:54 - 1 minus 15 over
308:57 - 16 which is equal to
309:01 - 1 over 16.
309:05 - which is kind of funny it's a little
309:08 - funny to think about that
309:09 - why is it that it is equal to 1 over 16.
309:12 - hmm
309:13 - well here's kind of another way you
309:15 - could think about it
309:17 - um well it'll make more sense when you
309:21 - get
309:21 - when we start talking about uh
309:24 - independence
309:25 - but when thinking about independence
309:28 - what you end up doing is
309:29 - like we know that if a coin is equally
309:32 - likely to get heads and tails
309:34 - then we know that uh the probability of
309:37 - getting
309:38 - heads is one half so it's so what would
309:41 - then be the probability
309:43 - of getting four tails in a row
309:47 - um because that we must get at least
309:50 - four tails in a row
309:52 - in order for us to have to have at least
309:55 - four
309:55 - flips in order for us to get ahead so
309:58 - what we're actually asking for is the
310:00 - probability of getting four tails in a
310:02 - row
310:02 - so what i notice is well let's see it's
310:05 - if we were to think about
310:06 - um getting two tails in a row it seems
310:09 - like the probability of getting two
310:10 - tails in a row
310:11 - is one fourth and three tails in a row
310:14 - is one eighth
310:15 - um and then four tails of a row that's
310:18 - one over sixteenth
310:19 - hmm so that's an alternative way to
310:21 - think about what's going on here
310:23 - uh but there's an algebraic way to get
310:26 - it
310:27 - to an algebraic way to get the answer
310:29 - all right uh
310:30 - the second part what is the probability
310:32 - the number of the number of flips until
310:34 - the experiment
310:35 - ends in other words the last flip will
310:37 - be heads is between
310:39 - 3 and 20. this one is a little bit more
310:43 - painful
310:45 - but actually maybe we could use that
310:48 - trick
310:51 - well let's see uh we what we could
310:54 - potentially do
310:56 - is say um
311:00 - that the probability of observing a
311:03 - sequence of flips
311:05 - such that the length of that sequence um
311:08 - is exactly less than three
311:13 - well that's going to be uh so that's
311:16 - going to be the sequences where the
311:18 - length of the sequence is one or two
311:20 - so that's going to be heads or tails
311:22 - heads and that's going to be one-half
311:23 - plus one-fourth
311:24 - which is uh three-fourths
311:27 - um now let's compute the probability
311:32 - of observing a sequence
311:36 - of flips where the length of that
311:38 - sequence
311:40 - is greater than 20.
311:47 - now this is not so far computing the
311:49 - probability
311:51 - of uh being between inclusively 3 and
311:54 - 20.
311:55 - but the reason why i'm computing these
311:57 - numbers is because i'm actually thinking
311:59 - i might want to use
312:01 - that complement trick again so i want to
312:03 - compute this probability
312:05 - where the length of the sequence is
312:08 - at least no is strictly greater than 20
312:12 - and i could actually use the arguments
312:14 - that i was using above to argue that
312:16 - this is going to be a sum
312:18 - from n equals uh 21
312:21 - to infinity of one-half
312:24 - to the power n all right it's basically
312:28 - the same arguments that i was using
312:29 - before
312:30 - uh to compute the size of the sample
312:32 - space because this is going to be
312:34 - uh computing so you have the sum of the
312:36 - probably when you have 21 flips and then
312:38 - probably when you have 22 flips and 23
312:41 - flips and so on
312:42 - and i could say
312:45 - [Music]
312:46 - what should i do next the thing is
312:48 - unfortunately i don't have a formula
312:51 - for when we're adding up starting at 21.
312:55 - i do have a formula when we add up
312:56 - starting at 1.
312:58 - so maybe what we could do is try to find
313:01 - a sneaky way
313:03 - to uh try adding up at 1 again
313:07 - what i could potentially do is say
313:11 - that n minus 20 plus 20
313:15 - i could say that this is equal to
313:18 - the sum
313:22 - n minus 20 is equal to
313:26 - uh one
313:30 - because that's equivalent to saying that
313:33 - n is equal to 21 right
313:36 - certainly and then i could say this is
313:39 - one half
313:40 - uh to the power uh n minus 20
313:44 - uh plus 20. hmm
313:46 - [Music]
313:48 - well i know i remember uh now
313:52 - that uh we can actually write this
313:55 - as uh we can write this as uh
314:00 - or the region the part enclosed in red
314:04 - as uh one-half to the power
314:07 - n minus 20 multiplied
314:10 - with one-half to the power 20. that's
314:13 - equivalent
314:14 - okay um so what can i
314:17 - use with that well i can now
314:20 - say that this is equal
314:23 - oops that this is equal to
314:27 - uh one half to the power oh i don't want
314:30 - red
314:31 - one half to the power 20 since that is
314:34 - effectively a constant that doesn't
314:35 - depend on
314:36 - n and i have a sum from
314:39 - n minus 20 equals one
314:43 - up to infinity of one-half
314:46 - to the power n minus 20. uh
314:50 - but what i could do is say well
314:53 - n minus 20 uh you know what i'm just
314:56 - going to call that i
314:57 - i'm just going to call n minus 20 i'm
314:59 - just going to say that's i so replace
315:02 - all the n minus 20's
315:04 - with i so i equals 1 to infinity
315:08 - 1 half to the power i oh i know how to
315:10 - compute this
315:11 - i know how to compute this this is going
315:14 - to be
315:15 - uh one half times uh
315:18 - one half divided by one minus one half
315:20 - so actually this part right here
315:22 - i actually in fact this right here is
315:24 - pretty much a probability model
315:26 - uh this is summing over the sample space
315:28 - so this
315:29 - this part this blue part is equal to one
315:34 - so that means that this probability is
315:37 - equal to um
315:41 - is uh equal to one half to the power 20.
315:46 - think about that uh once again
315:49 - going back to this probability uh
315:52 - the probability of getting a sample of
315:54 - getting a string of at least 20 means
315:55 - that you must have
315:56 - flipped at least 20 tails and i've
315:59 - competed effectively the probability of
316:01 - flipping of getting 20 tails in a row
316:04 - oh all right now uh i'm still not done
316:08 - though
316:08 - uh let's uh start collecting this
316:10 - information
316:12 - um i now know
316:16 - uh let's see that uh so i'm gonna
316:19 - start erasing all this because i need to
316:20 - start reclaiming some space
316:24 - uh
316:29 - all right so uh this is equal to
316:33 - um
316:36 - uh one half to the power 20.
316:40 - all right so we can now say that the
316:43 - probability
316:45 - that uh of uh observing a sequence of
316:48 - flips
316:49 - such that three is less than or equal to
316:53 - the length of that sequence
316:55 - which is less than or equal to 20. i
316:57 - could work
316:58 - instead with the complement of this and
317:00 - say this is one minus the probability
317:02 - of drawing an omega such that either
317:06 - n of omega is less than three
317:09 - or n of omega
317:12 - is greater than 20. and here's the thing
317:14 - though i've basically written down
317:17 - two disjoint sets one set where n of
317:19 - omega is less than three
317:21 - or one set that is great where n of
317:23 - omega is greater than 20.
317:25 - um so so either a sequence ha is less
317:28 - than three flips or more than twenty
317:29 - twenty flips
317:30 - but it's not both so this is actually
317:32 - the union of two disjoint events
317:34 - and therefore i can say this is 1 minus
317:37 - the probability
317:39 - of drawing an omega such that
317:42 - n of omega is less than 3
317:46 - plus the probability of
317:50 - drawing an omega such that uh
317:54 - the length of the string omega is
317:56 - greater than 20.
317:57 - and i have computed both of those
317:58 - probabilities so this is equal to
318:01 - uh 1 minus three-fourths
318:07 - uh plus one-half to the power
318:10 - uh 20 and i'm just going to leave it at
318:12 - that i'm not going to bother
318:14 - uh trying to simplify this because this
318:16 - is the correct answer
318:17 - so we're done at this point i'm not
318:19 - gonna go any further
318:20 - um i'm going to that said reclaim all
318:23 - this space
318:25 - and say um
318:28 - so hopefully you can rewind if you
318:31 - missed some of that
318:32 - and catch up on what you missed because
318:34 - i'm erasing this right now
318:36 - um and we're going to say
318:40 - that the probability of this event e
318:43 - is equal to 1 minus three fourths
318:47 - plus um uh
318:50 - one half to the power twenty
318:54 - all right uh next part what is oh
318:58 - yeah i don't want this this business all
319:00 - right uh what is the probability that an
319:02 - even number of flips is seen before the
319:05 - experiment ends
319:06 - hmm uh an even number of flips how could
319:09 - i possibly think about that
319:11 - uh so i want the probability
319:14 - of an e
319:18 - probability of an even number
319:22 - of uh flips
319:27 - and in short using some of the reasoning
319:29 - that i was using above
319:31 - i could say that this is the sum from
319:34 - of a one-half how would i represent an
319:37 - even number
319:38 - i could say that an even number is 2
319:41 - times a natural number that guarantees
319:43 - that the number the number is even so
319:45 - just take any natural number you want
319:47 - and multiply it by 2 and that's going to
319:49 - be an even number of fact that's going
319:50 - to cover all the even numbers
319:51 - and i'm going to sum this up from m
319:53 - equals 1
319:55 - to infinity
319:58 - now what i do next well what i could do
320:01 - is
320:01 - recognize that i can basically take the
320:04 - m out and say that this is one half
320:06 - squared
320:06 - raised to the power m so that this is
320:09 - equal to
320:10 - the sum uh when m equals 1
320:13 - to infinity of 1 4
320:17 - to the power m oh i know how to sum that
320:21 - that's going to be since this is a
320:23 - geometric sum
320:25 - 1 4 over 1 minus 1 4
320:29 - which is equal to 1 4 over
320:32 - 3 4 and those
320:36 - 1 4 parts cancel out so this ends up
320:38 - being
320:40 - one-third so the probability of an
320:43 - even number of flips is one-third
320:47 - which is kind of funny to think about if
320:48 - you think about it it seems like it like
320:50 - an even number of flips and an odd
320:52 - number of flips is equal
320:53 - equally likely but actually no they're
320:55 - not equally likely at all
320:56 - it's much more likely to have an odd
320:57 - number of flips since it's much more
320:59 - likely to get one
321:01 - one flip exactly
321:04 - okay um in fact getting one flip is
321:07 - twice as likely as getting two flips
321:09 - so by that reasoning it actually makes
321:11 - perfect sense
321:13 - okay uh moving on to the next part that
321:16 - was all a lot of work
321:18 - that was a lot of work okay uh
321:21 - example 14. in a small town
321:25 - uh 20 of the population is considered
321:28 - wealthy
321:28 - 30 of the population identifies as black
321:31 - and 5
321:32 - of the population is wealthy and black
321:34 - select a random individual from this
321:36 - population everyone equally likely to be
321:38 - selected but i don't think that actually
321:39 - matters to this problem so much because
321:41 - of how i've laid it out what is the
321:43 - probability that an individual
321:45 - is wealthy and not black
321:48 - so let's see let me catch up in my notes
321:53 - here's how i like to think about this
321:55 - one with problems like this where you
321:56 - have
321:57 - uh let's let's start out by saying that
321:59 - we've got uh
322:02 - we've got a we've we've got a sample
322:04 - space but i'm not going to think too
322:05 - hard about what the sample space is for
322:07 - this one
322:08 - i'm just going to say we have an event b
322:10 - which
322:11 - uh corresponds to an individual being
322:14 - black
322:16 - or uh racially identifying as black
322:19 - uh w will be the event that a person
322:23 - identifies as no not identifies uh
322:26 - probably this is probably like some sort
322:28 - of a census business so
322:30 - the census bureau has maybe may have
322:32 - some uh
322:33 - more technical definition of wealthy
322:35 - like they make over ninety thousand
322:37 - is making over ninety thousand rendering
322:39 - you wealthy well you're certainly well
322:41 - off but um anyway that stuff aside
322:44 - we'll just say w that denotes the event
322:47 - that you are wealthy
322:48 - by some uh criterion and uh the
322:51 - probability of b
322:53 - that an individual is a is a black
322:57 - uh will be 0.3 and the probability
323:00 - of w that this individual is wealthy is
323:03 - going to be
323:03 - 0.2 okay and that's just true because
323:07 - the problem said so
323:09 - so now what we want to well actually
323:11 - we're also given another thing that the
323:13 - probability of an individual being both
323:14 - wealthy
323:15 - and black uh is going to be
323:19 - uh 0.05
323:23 - oh by the way here's the thing to keep
323:25 - in mind
323:26 - um the probability of a
323:30 - and b is always going to be less than or
323:33 - equal to the probability of a
323:35 - or if you want you can replace the
323:36 - probability of a with probability of b
323:38 - it's just as true
323:39 - and that's because a and b is certainly
323:42 - a subset
323:43 - of a and if you think about
323:47 - um if you think about what about
323:51 - probabilities in terms of areas the area
323:53 - of a subset is certainly less than the
323:55 - less than or equal to the area of the
323:57 - thing it is it is uh
323:58 - nested within so you're always going to
324:01 - have a decrease in area it's not
324:02 - possible
324:03 - despite some human cognitive biases
324:07 - for the probability of a and b
324:10 - or something being true and something
324:12 - else also being true
324:14 - it is not possible for that to exceed
324:16 - the probability of the original thing is
324:17 - true
324:18 - i think people sometimes confuse uh
324:21 - intersection with conditional
324:23 - probability um that's probably what is
324:25 - going on
324:26 - when p when you start seeing those uh
324:28 - studies talk about stuff like that
324:29 - anyway uh we need to figure out uh what
324:32 - is the probability that this indiv
324:34 - that an individual selected from this uh
324:36 - from this uh village
324:38 - is wealthy and not black
324:42 - and here's how i like to solve problems
324:44 - like this and let's
324:46 - make it clearer all right
324:49 - here's how i like to solve problems like
324:51 - this i like to create a venn diagram
324:54 - and in that venn diagram i will start
324:58 - filling out regions so we'll have the
325:00 - wealthy circle and the black circle
325:03 - and we'll have this and we have the
325:05 - sample space
325:06 - uh and uh you start thinking about this
325:08 - like a puzzle
325:10 - like we know that the wealthy and black
325:12 - part is 0.05 and we're going to need to
325:14 - zoom in some more the wealthy and black
325:18 - part
325:19 - that is going to be 0.05
325:23 - okay and then we have the wealthy part
325:25 - which is 0.2 but we can't point
325:27 - put 0.2 here because that 0.2 is also
325:30 - including that 0.05
325:32 - so we need to put here is 0.15 so that
325:35 - when we add
325:36 - uh the this blue region and this red
325:40 - region those need to add up to
325:42 - 0.2 okay now for the black part
325:46 - um that needs to add up to 0.3 so that
325:49 - means that the part in
325:51 - that is not in the intersection is set
325:53 - to be 0.25
325:54 - and i didn't write that very clearly um
325:57 - that needs to be point
325:59 - uh 0.25
326:04 - okay which for what it's worth we now
326:06 - know the probability of w
326:08 - or b which would be 0.45
326:11 - which means that the probability of
326:12 - being neither wealthy nor black is going
326:15 - to be 0.55
326:19 - so that's something that i recommend
326:20 - when you encounter problems like these
326:22 - create a venn diagram and then fill out
326:24 - the diagram
326:25 - figure out the probability of every
326:27 - single little sliver
326:28 - in that venn diagram and then you can
326:30 - get any probability you want
326:33 - all right um and it will it will be
326:37 - like a genuine a generally useful
326:40 - chart for you not just for that
326:42 - individual problem because often these
326:44 - uh
326:45 - problems come in bunches
326:48 - all right so uh continuing on the
326:50 - probability of being wealthy and not
326:52 - black
326:53 - well actually that corresponds in our
326:56 - little chart up here to the blue region
326:59 - which is going to be 0.15
327:01 - so this is going to be 0.15
327:08 - okay what is the probability that the
327:10 - individual is neither wealthy nor black
327:12 - actually i already computed that i set
327:13 - it in words it's going to be 0.55
327:20 - all right uh next example a ball
327:23 - contains
327:24 - a bag contains balls and blocks thirty
327:27 - percent of the bag's contents are balls
327:29 - an object is either red or blue and
327:31 - forty percent of the objects are red
327:32 - an object is made of either wood or
327:34 - plastic and sixty five percent of the
327:35 - objects are wooden
327:37 - ten percent of the objects are wooden
327:38 - balls five percent of the objects are
327:39 - red balls and twenty percent of the
327:41 - objects are red and plastic
327:42 - two percent of the objects are red
327:45 - plastic
327:46 - uh uh there's a typo here uh
327:49 - we're going to need to change blocks
327:53 - to balls
327:57 - okay reach into the bag and pick out an
328:00 - object at random
328:02 - each object equally likely to be
328:03 - selected what is the probability that
328:06 - the object selected is a ball
328:08 - red or wooden and this is inclusive by
328:11 - the way
328:12 - well this is another one of those
328:13 - problems where what i suggest you do
328:15 - is you create a venn diagram to
328:18 - represent the situation
328:20 - and fill out all of the little parts of
328:22 - that venn diagram
328:24 - so what does that look like here um
328:28 - all right so i've got my venn diagram
328:31 - this is my sample space
328:34 - uh right so i've got giant circles but i
328:38 - should probably be a bit more precise
328:40 - about uh let's start before i start
328:43 - filling out this chart
328:44 - uh let's create some notation and fill
328:47 - out what we already know
328:48 - in mathematical notation so we've got
328:51 - balls
328:52 - so b will be the event that we grab a
328:55 - ball
328:57 - and we'll say so objects are either red
328:59 - or blue
329:00 - so we'll say r is the event that an
329:04 - object pulled out of here is red so
329:07 - b complement is going to be a block and
329:09 - our complement is going to be
329:11 - a blue object and
329:14 - we say that so we can either have
329:17 - wood or plastic so we'll have a w
329:21 - correspond uh to the event that you get
329:24 - a wooden object
329:28 - all right uh let's start collecting some
329:30 - more information
329:32 - uh we have that the probability of uh
329:34 - drawing a
329:36 - stop it uh we have the probability of
329:38 - drawing a ball so the probability of the
329:40 - event b
329:41 - is equal to 0.3
329:46 - the probability of getting a red object
329:49 - is a 0.4
329:53 - the probability of a wooden object
329:59 - probably of a wooden object is going to
330:01 - be 0.65
330:05 - uh the probability that an
330:08 - object is a wooden ball well that's w
330:11 - intersected with b
330:13 - since the object is both wooden and a
330:15 - ball that
330:16 - is going to be point one uh
330:19 - the probability of getting a red ball
330:23 - uh a red ball
330:27 - is going to be 0.05
330:31 - the probability of getting
330:34 - uh something that's red and plastic
330:37 - so that's r and w complement
330:43 - that's going to be 0.2
330:46 - and finally we have
330:51 - the probability of drawing
330:55 - a red
330:58 - uh plastic so w complement
331:03 - a block
331:07 - no no not a block it's no block it's a
331:08 - ball this is equal to 0.02
331:14 - okay so so so so
331:18 - uh we need to start filling out this
331:21 - chart we're going to have uh we're going
331:24 - to have red here
331:25 - we're going to have balls here and we're
331:27 - going to have
331:28 - wooden objects over here uh i always
331:31 - recommend
331:31 - starting out with the smallest region
331:34 - first
331:35 - so the greatest intersection start out
331:37 - with that and then fill outwards
331:39 - so in this case the smallest region is
331:42 - red plastic balls uh where is that
331:46 - on our venn diagram uh you are
331:50 - in r you're in b but you're not in w so
331:53 - that actually corresponds to the blue
331:55 - region
331:56 - okay so the probability of the blue
331:58 - region according to the problem is 0.02
332:02 - all right uh next up we could
332:05 - probably ask for uh what is the
332:08 - probability
332:09 - of being red and plastic uh red and
332:11 - plastic
332:13 - uh that corresponds to being
332:16 - um let's see so you're in the red
332:19 - region uh but you're not in the w
332:23 - region so that's going to correspond to
332:25 - this blue area
332:27 - and we know that the probability of
332:31 - being red and plastic is 0.2 but we've
332:34 - already got a 0.02
332:36 - so that means that that in the other
332:39 - part we're going to be left with 0.18
332:44 - so the probability of being a red
332:46 - plastic block
332:47 - is going to be 0.18 all right uh
332:51 - how about next uh we've got
332:54 - uh the probability of being
332:57 - a red ball so a problem the probability
333:01 - of being
333:03 - a red ball is going to be .05 what
333:06 - corresponds to
333:07 - red balls well this is the ball so we
333:10 - have the balls region and we have
333:11 - red stuff so that's going to be
333:14 - corresponding to the red region
333:16 - okay and that is 0.05 which means
333:20 - we've already got a 0.02 here it needs
333:21 - to add at 0.05 so that means that this
333:23 - little sliver in the middle
333:25 - is going to be 0.03 okay
333:28 - uh let's see we need next uh
333:32 - wooden balls so wooden balls corresponds
333:35 - to this green region
333:36 - uh the probability of being a wooden
333:38 - ball is point
333:39 - one so that we've already got a 0.03 so
333:43 - that means that
333:44 - we've got 0.07 in this other little
333:46 - sliver
333:49 - all right we're getting close to done uh
333:51 - what about
333:55 - uh let's see so we've got uh wooden
333:58 - balls
333:59 - red balls red plastic things
334:03 - uh what else have we got well we can
334:06 - we've got the probability of balls the
334:07 - probably balls is gonna be
334:08 - point uh 0.3 and
334:12 - that so balls correspond to this blue
334:14 - region
334:15 - and we've already got um
334:18 - 0.12 of the area accounted for
334:22 - the total area needs to be three point
334:24 - three so that means that
334:25 - for this uh remaining sliver uh we have
334:29 - point one eight for its area
334:32 - okay uh next
334:36 - up next up
334:40 - well we've got um the probability of red
334:43 - stuff
334:44 - and we know that the probability of red
334:45 - stuff is point four
334:47 - so point two so red stuff is going to be
334:51 - uh the region i've just encircled in red
334:54 - and
334:54 - so far 0.23 of that area is accounted
334:58 - for
334:59 - but point its total area is going to be
335:01 - 0.4 so that means that
335:03 - only so that means that when we're
335:06 - filling out
335:07 - uh this remaining blue sliver it must be
335:10 - 0.17
335:14 - okay and for wooden stuff we know that
335:16 - the total area in the wooden region
335:18 - it's going to be 0.65 and
335:22 - 0.27 of that area is already accounted
335:26 - for so that means that the remaining
335:29 - area is going to be 0.38
335:33 - all right and we are almost done we
335:35 - actually in fact know the probability of
335:37 - everything else
335:38 - if you were to add up all of those
335:39 - probabilities they would add up to 0.98
335:41 - so that means that the remaining area
335:43 - that is outside of this region is 0.02
335:46 - all right we have filled out that
335:48 - diagram and we are ready to answer some
335:50 - questions
335:51 - what is the probability that an object
335:52 - selected is a ball
335:54 - red or wooden so that's the probability
335:58 - of r or w
336:01 - or b and we have already figured that
336:04 - out
336:05 - that corresponds uh to the 0.02
336:10 - so that is equal to
336:15 - 0.02
336:19 - okay uh next up what is the probability
336:21 - that the object
336:22 - is a red wooden ball uh so the
336:25 - probability of being
336:28 - red
336:31 - of being red and wooden and a ball
336:35 - what is that going to be well we're
336:37 - going to go back to our diagram that we
336:39 - created
336:40 - we look for red wooden balls so
336:44 - you need to be in the red circle you
336:47 - need to be
336:49 - uh wooden so you need to be in the blue
336:50 - circle and you need to be a ball so you
336:52 - need to be in the green circle
336:54 - uh oh i guess that's a darker blue so
336:56 - that leaves us 0.03
337:02 - so that leaves us 0.03 for that for uh
337:05 - that area so this is going to be 0.03
337:12 - all right uh what is the probability
337:14 - let's go ahead and do some zooming out
337:16 - we no longer
337:17 - need such fine control when drawing uh
337:20 - what is the probability that an object
337:21 - is a blue plastic block
337:23 - well the probability of being a
337:26 - blue which is the complement of red
337:30 - uh not in a color sense not like
337:33 - when artists talk about complements
337:34 - that's a red is not a complement of blue
337:37 - but
337:37 - whatever in a probabilistic sense in our
337:40 - probability model
337:41 - uh so that's the reason that we've drawn
337:43 - here and it's actually a problem
337:45 - i'm gonna leave this to yourself but
337:47 - look up de morgan's laws
337:49 - uh learn about de morgan's laws uh for
337:52 - my students this is an assignment
337:54 - um i believe if i remember right but
337:57 - basically
337:57 - this is r union w
338:01 - union b complement
338:05 - which then means it's 1 minus the
338:08 - probability
338:09 - of r union w union b
338:13 - and that is a probability you've already
338:14 - figured out that's 1 minus 0.98
338:19 - which is equal to 0.02
338:24 - okay
338:30 - okay so that's it for the examples those
338:33 - are a lot of examples
338:34 - but i'm just gonna we're now wrapping up
338:38 - this section
338:38 - and i'm going to wrap up this section
338:41 - with a
338:42 - short discussion on the interpretation
338:45 - of
338:45 - probability now it turns out this is
338:48 - actually a rather large topic
338:50 - philosophers actually are debating uh
338:54 - what do probabilities actually mean
338:58 - [Music]
338:59 - and what is an appropriate definition
339:03 - of uh of uh of what a probability is
339:06 - because we have a mathematical notion
339:09 - of probability but that doesn't
339:10 - necessarily translate into a real-world
339:12 - notion of probability
339:13 - now i'm going to leave that discussion
339:15 - aside i might make an aside video
339:18 - about the interpretation of probability
339:21 - but in this class despite the
339:23 - limitations of this interpretation
339:26 - we're going to adopt the frequentest
339:27 - interpretation of probability
339:29 - which is interpreting probabilities as
339:31 - long run frequencies of events
339:33 - so in other words if we were to repeat
339:35 - an event an experiment many many many
339:36 - many times
339:37 - and each of those repetitions were
339:39 - independent of the past
339:41 - uh we would the
339:44 - sample proportion of times uh we would
339:47 - see that
339:48 - experiment occur would approach that
339:50 - probability
339:52 - so here's kind of and i here is a chart
339:56 - that kind of illustrates this idea
339:59 - we're going to have along the y-axis
340:04 - the y-axis ranges from 0 to 1
340:07 - and somewhere along here we have the
340:09 - probability of some event
340:11 - uh e uh
340:15 - stop being uncooperative tablet
340:20 - all right so we have the probability of
340:21 - an event e
340:23 - okay and uh along the x-axis well it's
340:27 - not really the x-axis this time it will
340:29 - be
340:29 - the n-axis and we're going to mark this
340:32 - axis
340:32 - off at integer values
340:37 - 1 2 3 4
340:41 - and so on and um
340:45 - uh we are going to track uh p
340:48 - hat n which is
340:51 - the sample proportion
340:58 - of times
341:01 - the event e occurs
341:05 - and you remember how to compute sample
341:07 - proportions uh but here's kind of the
341:09 - thing
341:10 - about what we're doing here uh we are
341:12 - remembering pat the past so you could
341:14 - imagine that we
341:15 - uh are flipping a coin and we're
341:17 - tracking how many times we see heads
341:20 - in this uh experiment and uh
341:24 - uh so the first time you flip it you're
341:26 - gonna have either one or zero heads
341:28 - and uh the second time you're going to
341:30 - keep the results of that first flip but
341:32 - then recompute your proportion for a
341:34 - sample size of two
341:35 - uh for the third time you then flip the
341:38 - coin a third time and you keep the
341:39 - previous two results
341:41 - uh and then recompute the proportion
341:43 - using those previous two results and
341:44 - also the third flip you just did
341:46 - uh and so on keep doing this in a
341:49 - sequence
341:50 - so uh what could possibly happen so i'm
341:53 - going to
341:54 - put a little dashed line at the
341:57 - probability of
341:58 - e according to the frequentist
342:00 - interpretation of probability
342:02 - uh the probability is basically this
342:03 - long-run frequency where
342:05 - if you're tracking p hat n you'll start
342:08 - out
342:08 - at um like maybe zero or one depending
342:12 - on whether the event happened on the
342:13 - first trial or not
342:14 - and then you might track that it happens
342:16 - on the second trial
342:18 - and the third trial and the fourth trial
342:20 - and so on and
342:21 - what will happen is this line
342:24 - will get very very close to the
342:26 - probability of e
342:28 - um and that's how frequencies think
342:31 - about probabilities as a long run
342:33 - proportion
342:33 - as a limiting proportion um
342:36 - now the there is an unfortunate thing
342:38 - about this uh interpretation which that
342:40 - actually
342:41 - in mathematical probability theory
342:44 - that is not an assumption that's a
342:46 - theorem so this is not something that we
342:48 - assume is true this is something we
342:50 - prove is true
342:51 - so why are we assuming that something is
342:53 - true that we then prove is true
342:55 - that that seems like a circular
342:57 - reasoning and it's unfortunate
342:59 - but i'm going to leave those
343:01 - philosophical issues aside
343:03 - uh because at some level my own personal
343:06 - belief is that
343:08 - all of those technical philosophical
343:09 - issues aside at some level it's almost
343:11 - a limitation of human language uh and
343:14 - probability theory despite
343:16 - our difficulty in coming up with a
343:17 - definition a proper definition for it
343:19 - uh is a very real phenomenon so whatever
343:23 - the definition is it matches
343:25 - this frequentist notion uh i've actually
343:27 - got some r code here that you can look
343:29 - at
343:30 - uh this functions r is able of capable
343:33 - of generating random numbers and set
343:35 - seed is what's known as sets what's
343:36 - known as a random seed
343:38 - and make sure that when you're
343:39 - generating random numbers they actually
343:41 - are going to come up with the exact same
343:42 - results uh
343:44 - the same time so if you run this code it
343:46 - will produce the exact same results
343:47 - uh at least in principle uh different
343:50 - versions of software may cause different
343:52 - results but
343:53 - uh whatever so i set the sample size for
343:56 - my experiments
343:58 - uh i conduct a number of flips
344:03 - i track uh i accumulatively track the
344:06 - number of heads in these flips
344:09 - and then i plot these uh sample
344:12 - proportions
344:13 - and also draw a line at the theoretical
344:16 - probability of that event
344:18 - so this is the case when you were to if
344:20 - you were to flip a coin
344:22 - flip 15 times and keep a running
344:23 - proportion of how many times you saw
344:25 - heads
344:26 - this is what it looks like um and you
344:28 - can kind of see it seems to be
344:29 - converging around 0.5
344:31 - if we repeat this experiment but this
344:33 - time when
344:35 - n is equal to 50 what we'll see is
344:38 - it kind of looks like it's getting even
344:40 - closer to something constant
344:42 - and then if we were to repeat this again
344:44 - but now uh
344:46 - the sample size is 500 it almost
344:49 - converges
344:51 - it almost converges to the truth now
344:54 - that said with probabilities it never
344:56 - means that
344:57 - you are guaranteed to see that many in
345:00 - any single sample you we did not
345:02 - say that uh which means that at the very
345:05 - end here you're not actually at one half
345:07 - you're pretty close to one half but
345:08 - you're not
345:09 - at one half but if you were to just keep
345:12 - going
345:12 - on like this and you were in fact
345:14 - possible for you to continue forever
345:16 - and god suddenly grants you the ability
345:19 - to live forever so long as you continue
345:21 - to flip this coin
345:23 - at the end of eternity you will have one
345:25 - half a flip uh
345:26 - half of your flips will be one half
345:28 - that's basically what we're saying
345:32 - all right so that's it for this video
345:34 - quite intense
345:35 - quite intense it's probably giving you
345:37 - some idea that probability can get
345:39 - rather out of hand um at least in terms
345:42 - of uh computational complexity
345:44 - uh but uh i've given you a number of
345:47 - useful examples and a number of useful
345:48 - techniques
345:49 - the thing though is those techniques
345:53 - like they're useful now and it's good
345:54 - for you to see them but you always kind
345:56 - of have to adapt to individual problems
345:59 - the next section is going back to the
346:01 - situation where
346:02 - you have a finite sample space we're
346:05 - going to talk about how you count
346:07 - elements from that sample space
346:09 - this part is both fun and frustrating
346:11 - because
346:12 - it's fun because we get to talk about
346:14 - things like poker problems and gambling
346:16 - and stuff like that
346:17 - but it's frustrating because counting is
346:19 - hard counting is really hard
346:21 - and also like i would love it to give
346:23 - you a magic formula
346:24 - that solves every and all counting
346:26 - problems and i will give you some
346:27 - formulas but those are kind of
346:29 - those are just tools that you kind of
346:31 - need to combine and the
346:32 - combination of those tools that's the
346:34 - hard part and i can't really give you a
346:37 - general principle for that
346:39 - every counting problem is kind of its
346:41 - own thing but enough of that we're
346:43 - we're calling it for now uh i will see
346:45 - you
346:46 - later in the future video on counting
346:54 - let's move on now to the section on
346:57 - counting techniques
346:58 - to start things off let's suppose that
347:02 - we have a burger shop that we're going
347:03 - to call
347:04 - and just off the top of our head bob's
347:06 - burgers
347:08 - and uh they're offering three types of
347:11 - bread
347:12 - we have white bread rye bread and
347:14 - sourdough
347:16 - a burger can come with or without cheese
347:18 - how many burgers
347:19 - are possible well let's see
347:23 - uh we have uh let's let's come up with
347:27 - some encoding for our possibilities
347:29 - uh we have white bread we have rye bread
347:34 - and we have a sourdough
347:37 - and a burger can come with or without
347:40 - cheese
347:42 - so we'll say we can have with cheese or
347:44 - we can have no
347:45 - cheese all right so given this encoding
347:49 - uh what are the possible burgers that we
347:51 - could have
347:53 - so we've got our possible burgers
347:57 - and we could have a burger with white
348:01 - bread and
348:02 - cheese a burger with rye bread and
348:05 - cheese and a burger with sourdough
348:06 - and cheese or a burger with white bread
348:09 - and
348:10 - no cheese a burger with rye bread and no
348:13 - cheese and a burger with sourdough and
348:15 - no cheat and
348:17 - no cheese so let's see how many burgers
348:20 - is that that's going to be
348:23 - six burgers
348:26 - uh so that's the way to do it by hand
348:29 - where you basically enumerate all the
348:31 - possibilities for
348:33 - uh how many burgers are going to be but
348:36 - the thing though is i mean that's only
348:39 - going to work for so long i mean
348:41 - you can you can enumerate stuff when
348:43 - it's possible to do and then literally
348:46 - count uh how many possibilities there
348:48 - are but there's also other ways to
348:50 - possibly represent what's going on here
348:52 - for example we could use what's known as
348:54 - a tree diagram
348:55 - with the t tree diagram we're going to
348:57 - create a tree
348:59 - that visualizes um
349:03 - the branching possibilities of our
349:05 - choices
349:06 - so for example at the first node we're
349:09 - going to choose the type of bread so we
349:10 - have white
349:11 - rye and sourdough and then
349:14 - at the second level of node we decide
349:16 - whether we're going to have
349:18 - cheese or no cheese so we have
349:22 - um for the upper branch
349:25 - cheese and for the lower branch no
349:27 - cheese and we're going to do this
349:29 - a few more times for the different types
349:31 - of bread that we could have chosen and
349:34 - we've got cheese no cheese
349:36 - cheese no cheese and then we're going to
349:40 - count how many no's there are on the end
349:43 - points
349:44 - and the number of nodes on the endpoints
349:46 - will correspond to the number
349:48 - of possible choices in this case there
349:50 - are six nodes
349:52 - so that means six possibilities
349:56 - and of course one thing that's nice
349:58 - about these types of tree diagrams
350:01 - is that we could allow for
350:05 - more flexibility in our possibilities
350:07 - such as
350:08 - perhaps for some reason bob has decided
350:11 - that he is not going to permit
350:13 - a sourdough bread without cheese in
350:16 - which case you just remove that
350:18 - possibility from
350:19 - from the from the tree diagram you
350:21 - remove that branch
350:22 - in which case there would now be five
350:24 - nodes in the branch
350:26 - so it's pretty general uh
350:30 - there's also something that we can use
350:32 - uh called the product rule
350:34 - uh proposition seven if there are n1
350:36 - possibilities for choice one and two
350:38 - possibilities for choice two
350:40 - all the way to nk choice of
350:42 - possibilities for choice k
350:43 - then the total number of possible
350:45 - combinations is going to be the product
350:47 - of the number of possible choices we can
350:49 - make at each point
350:50 - so uh let's use the product rule to
350:53 - answer this question
350:55 - according to the product rule
350:58 - we could have three possible choices for
351:02 - the first node
351:04 - and two possible choices for
351:07 - the second node and thus the total
351:10 - number of possibilities will be
351:12 - three times two which is equal to
351:15 - six all right so we've seen so far
351:18 - uh six ways to basically answer this
351:21 - question
351:22 - uh now let's uh move on some more
351:26 - uh the sandwich shop deluxe deli offers
351:29 - four bread
351:30 - options where we have white sourdough
351:32 - whole wheat
351:33 - and rye five meat options turkey ham
351:36 - beet
351:36 - beef chicken or no meat six cheese
351:40 - options cheddar white cheddar swiss
351:41 - american pepper jack and no cheese
351:43 - with or without lettuce with or without
351:45 - tomatoes whether without bacon with
351:46 - without mayonnaise and with without
351:47 - mustard
351:48 - how many sandwiches are possible let's
351:51 - see
351:51 - uh how many decisions do we have to make
351:54 - first we need to decide
351:56 - on our bread option so we'll say that
351:58 - that's uh decision one
352:00 - so we'll say that n1 there are uh
352:03 - four possible options so four
352:06 - then we can decide uh our meat options
352:10 - and there's uh five meat options uh
352:14 - we have six cheese options so n3 will be
352:18 - six uh there are
352:21 - and then for the remaining options it's
352:23 - all binary
352:25 - so we've got with or without lettuce so
352:27 - that means that n4
352:28 - equals two with or without tomatoes so
352:32 - n5 equals two with or without
352:35 - bacon so n6 equals 2
352:39 - with or without mayonnaise so and 7
352:42 - equals 2 and with or without
352:45 - mustard so n 8 equals
352:49 - two okay so based off this how many
352:53 - sandwiches are possible
352:54 - well according to the product rule we're
352:55 - just going to multiply all of those
352:57 - numbers together
352:58 - so we've got four times five times
353:02 - six times uh two five times so we'll say
353:06 - two to the fifth power
353:07 - so this when you multiply all this out
353:10 - you're going to end up with
353:11 - 3 840 possibilities
353:15 - for this sandwich shop
353:20 - okay so uh moving on here's
353:24 - here's the thing we are now uh
353:28 - very fully into the realm of
353:30 - combinatorics combinatorics
353:33 - is generally figuring out how many ways
353:35 - there are to do things
353:37 - and uh combi or how many combinations
353:41 - there are of things
353:43 - how large a finite size sets are
353:46 - when they're generated with certain
353:48 - rules
353:50 - and honestly combinatorics is pretty
353:52 - hard like for example i find
353:54 - combinatorics rather challenging
353:56 - um i remember once when i was in
354:00 - a probability class and i was in
354:04 - uh excuse me
354:07 - i was in office hours with a professor
354:10 - and
354:10 - a fellow student of mine just we were
354:13 - discussing
354:14 - combinatorics and uh my a fellow student
354:17 - of mine was just like is this what
354:18 - probability is
354:19 - or statistics is because if it is i i
354:22 - don't know if i want to do this and he
354:23 - was like no
354:24 - it's not uh you just kind of have to do
354:26 - this
354:27 - you have to learn this at some point but
354:30 - it's
354:30 - not what the bulk of probability is and
354:32 - i can understand why she said that
354:34 - because
354:35 - it can get pretty painful here's the
354:36 - thing about combinatorics
354:38 - uh what i'm about to do is give you some
354:42 - tools some counting tools for
354:45 - problem for counting type
354:48 - problems that are often reappearing
354:52 - but the thing is every single
354:54 - combinatorics problem
354:56 - is its own thing it's really hard to
355:00 - come up with general tools like what i'm
355:01 - coming up with here
355:03 - um there are certainly tools in the
355:04 - toolbox uh things that you kind of look
355:07 - out for
355:08 - when you're solving combinatorics type
355:10 - problems but you still need to think of
355:12 - it
355:12 - of each problem as its own thing
355:16 - and for this section i kind of have to
355:18 - come up with a number of examples and
355:20 - run through those examples to give you
355:22 - an idea of the thought process
355:24 - of combinatorics but it's really hard to
355:27 - teach that thought process
355:29 - uh without example since i can't just
355:31 - give you an
355:32 - algorithm that will solve every single
355:34 - covenantoric's problem
355:36 - every problem is its own beast and
355:39 - you're
355:40 - pretty much forced to think very
355:42 - carefully
355:43 - about uh the process by which
355:47 - a single combination has been formed uh
355:50 - also i should probably just mention uh
355:52 - if we have
355:53 - uh there's there's something that i
355:55 - didn't mention in these uh
355:57 - typed up notes but i call this the sum
356:00 - rule
356:04 - uh so uh if you're choosing
356:11 - uh from k
356:16 - uh k disjoint sets
356:25 - so sets each with
356:30 - uh each with um
356:34 - we'll say n sub i
356:38 - uh possibilities uh possible choices
356:50 - uh at one stage so you're gonna pick uh
356:53 - one of these sets and an item from that
356:56 - set
356:57 - uh then in this case
357:04 - uh there's going to be the sum from i
357:07 - equals 1 uh to k
357:11 - uh n sub i uh possible
357:14 - so possibilities
357:20 - so in other words if you need to choose
357:22 - an item from bin a
357:24 - or an item from bin b exclusively
357:27 - then the number of ways you can make
357:29 - that decision is going to be the sum of
357:31 - the items in those two bins
357:33 - not a particularly difficult concept but
357:35 - i'm going to just lay it out
357:37 - for you right now just so that you are
357:40 - aware of that and it's a
357:41 - it's a valid approach to solving
357:42 - cognitoric's problems
357:44 - and i would say that the the uh
357:47 - techniques that i'm about to show
357:49 - like these will these will get you
357:53 - through
357:54 - these are things that you look out for
357:55 - when solving combinatorics problems uh
357:58 - but uh they probably won't take you all
358:00 - the way
358:01 - all right so uh when we're solving some
358:05 - problems
358:06 - uh suppose that out of n possibilities
358:08 - we will be choosing k
358:10 - we have two essential questions to
358:12 - answer
358:13 - um are we choosing with or without
358:16 - replacement
358:17 - and does order matter depending on our
358:20 - answer to those questions we're going to
358:21 - have different solutions that are
358:22 - summarized below
358:24 - uh here uh if if the choice
358:27 - was where uh
358:30 - there's order and we're doing so with
358:32 - replacement then the number of possible
358:34 - outcomes is end of the power k
358:36 - uh ordered without replacement that's
358:39 - known as a permutation
358:41 - and here is a formula for a permutation
358:44 - uh n factorial by the way so this
358:47 - so uh n with an exclamation point means
358:50 - n factorial
358:51 - and that is n times n minus 1
358:56 - times n minus 2
359:01 - times dot dot dot times
359:04 - 3 times 2 times one
359:09 - that is n factorial and by convention
359:13 - zero factorial equals one
359:16 - huh that's kind of strange uh
359:19 - at least for me when i first saw that
359:22 - zero factorial equals one i found it
359:24 - rather surprising
359:25 - but actually it makes perfect sense for
359:27 - zero factorial to equal one
359:29 - and i might may explain why zero
359:31 - factorial is one in a separate video
359:34 - but we're just going to leave it at that
359:35 - so that is what n factorial is
359:38 - um and one way to think of what n
359:41 - factorial is
359:42 - it's it's the number of way to order
359:44 - number of ways
359:45 - to order n things so it's the number of
359:49 - permutations of
359:50 - n things um
359:54 - uh now in this case for uh ordered with
359:57 - without replacement uh we we all we're
360:00 - also calling that a permutation
360:02 - uh it's just a permutation of k things
360:04 - out of n
360:05 - so uh suppose that
360:09 - uh we are that we don't have replacement
360:13 - so remember what we're so think about
360:14 - what replacement means it means that
360:17 - uh so replacement means that if you
360:19 - choose
360:21 - uh an option you can choose it again
360:24 - so you're allowed to choose it again an
360:26 - example of
360:28 - of ordered with replacement is a
360:30 - sequence of heads and tails
360:32 - in coin flips where you may consider
360:36 - possibly you may not but if you're
360:39 - looking at a string there are
360:42 - different ways that heads and tails can
360:44 - manifest themselves in the string
360:46 - and furthermore if you get heads on the
360:49 - first flip
360:50 - you're still allowed to get hats on the
360:51 - second flip whereas if you don't have
360:54 - replacement you can get heads in the
360:55 - first flip but you can't get heads on
360:56 - the second flip
360:57 - so that would be without replacement in
361:00 - order matters
361:01 - that means uh you're that means that the
361:03 - order in which you see
361:04 - an item in a sequence matter so you're
361:06 - tracking that
361:08 - so headsets tails is different from
361:10 - head's tails heads
361:12 - whereas if order doesn't matter then
361:14 - headset's tails is essentially the same
361:15 - as heads tails heads
361:17 - because you don't care about the
361:18 - ordering of the items
361:20 - so that's what we mean by whether
361:21 - without replacement or with or without
361:23 - order
361:25 - okay so suppose that we're choosing
361:27 - items where order doesn't matter
361:29 - and uh we don't have replacement
361:33 - in that situation uh we have we're going
361:36 - to use the formula
361:37 - n choose k which is n factorial divided
361:39 - by k factorial
361:41 - uh and or the product of k factorial n
361:44 - minus k factorial
361:45 - i'm going to prove each one of these
361:47 - formulas
361:48 - in a second because i do believe that
361:50 - the proofs
361:52 - are enlightening on the combinatoric
361:54 - thought process
361:56 - and the combinatoric thought process is
361:57 - something where you just kind of have to
361:58 - get exposed to it a lot in order to be
362:00 - able to
362:01 - uh think that way yourself um finally we
362:05 - have the situation where
362:07 - uh you are you're choosing with
362:10 - replacement
362:10 - but order doesn't matter in which case
362:14 - you're going to have
362:15 - k plus n minus one choose n minus one
362:19 - okay so next up is uh
362:22 - the uh the proofs or the justifications
362:25 - for each of these formulas we're going
362:29 - and i'm going to uh prove this
362:33 - in um a sneaking fashion
362:37 - or a clockwise matter because
362:41 - often the formula after the formula in
362:43 - this
362:44 - clockwise order from before is going to
362:46 - be used in the next proof
362:48 - okay so uh let's get started
362:53 - uh we're going to start by
362:56 - showing by proving the formula and we in
362:59 - which case we have
363:00 - ordering and
363:04 - so and we also have replacement
363:12 - all right so ordered with replacement
363:14 - i'm going to zoom in because i'm going
363:15 - to need some uh
363:17 - i'm going to need more space all right
363:20 - so we want to come up with the formula
363:22 - um n to the power k well all right so we
363:25 - have k
363:25 - choices to make and since we have
363:28 - replacement
363:30 - we're going to use the product rule the
363:32 - product rule is kind of the underlying
363:35 - uh rule that we can use for all of these
363:37 - so starting out with the product rule
363:42 - we have k decisions to make i like to
363:45 - think of confidence of a
363:47 - combinatoric problems where you are
363:49 - describing any combinatoric problem
363:51 - how to construct an instance of
363:55 - an element of this set and
363:59 - when you are thinking about how to
364:01 - construct an element you construct a
364:03 - narrative for how to uh construct
364:06 - one of these elements and you're
364:08 - counting how many ways there are
364:09 - to make the decisions along the way and
364:12 - you're tracking how
364:13 - and you're tracking how many decisions
364:14 - you need to make and
364:17 - using the product rule the entire time
364:19 - so
364:20 - uh we can think of if we're doing
364:23 - ordering with replacement we might have
364:25 - for example
364:27 - we might have for example three spaces
364:29 - and we need to fill up those spaces with
364:31 - elements so
364:32 - we might have two possibilities for each
364:34 - space so
364:36 - at the first space we would have two
364:39 - possibilities and at the second space
364:40 - we'd have two possibilities
364:42 - and at the third space we have two
364:43 - possibilities and we care about the
364:45 - ordering
364:46 - and um
364:50 - we care about the ordering of what we
364:52 - put in in these spaces
364:54 - so the number of possibilities for
364:56 - filling up these three spaces would be 2
364:58 - to the power 3
365:00 - because we're going to multiply and we
365:02 - have to make a decision at each space
365:04 - and when you have to make a decision at
365:05 - each space
365:06 - you multiply the possible number of
365:08 - decisions that you needed to make
365:10 - so uh generalizing this idea
365:14 - uh we have uh we need to make a decision
365:17 - in the first slot the second thought the
365:20 - third slot all the way to the kth slot
365:22 - and for each of these
365:24 - and then and for each of these slots
365:29 - there were n possible things to choose
365:31 - one from because
365:33 - we're not taking we're not removing
365:35 - options
365:36 - as we go through our sequence of case
365:38 - slots
365:40 - so then by using the product rule
365:43 - according to the product rule we end up
365:44 - having to multiply
365:46 - um we have we end up having to multiply
365:51 - uh each of our possibilities
365:54 - or each of the possible options we can
365:56 - make at each slot
365:58 - but since all of those are the same
366:05 - since all of those are the same number
366:08 - you end up with multiplying n
366:12 - k times which is n to the power k
366:17 - so that gives us the first formula the
366:19 - next formula we need to come up with is
366:22 - ordering without replacement which gives
366:23 - us the number of permutations
366:26 - okay so we now next have
366:29 - oops that is definitely not what i
366:31 - wanted
366:32 - all right so next up we have ordered
366:37 - without replacement
366:47 - i'm going to go back for a second and
366:50 - i have in my head prototype problems
366:54 - for each of these four scenarios
366:57 - so ordering with replacement is like
367:00 - determining how many strings of heads
367:02 - and tails there are in a string of flips
367:05 - because the ordering in that situation
367:06 - would matter because your track
367:08 - because heads tails it's not the same as
367:10 - tails heads in a string
367:12 - and you have replacement because if you
367:14 - get heads on the first flip you can get
367:15 - heads again on the second flip
367:18 - um ordering without replacement to me
367:21 - my prototype problem for that is forming
367:23 - a list
367:25 - because the item that you put at the top
367:27 - of the list cannot be chosen for
367:28 - the sec for the next item in the list so
367:31 - you are removing items as you go through
367:33 - this list
367:34 - and maybe it's like a times top top 100
367:38 - people or 100 people for the year list
367:41 - where the ordering matters and there's
367:43 - certainly more people than 100
367:45 - so they might have this larger
367:48 - this larger master list of people they
367:51 - would consider to be candidates on their
367:53 - top 100 list
367:54 - so you're going to choose people to be
367:56 - in certain slots on this list and when
367:58 - you choose a person to be on the list
367:59 - you can't pick them again
368:01 - so that's kind of my prototype problem
368:04 - for
368:06 - ordered with without replacement for
368:09 - um not
368:12 - ordered and with replacement this is
368:14 - like uh poker problems
368:16 - because when you draw cards from
368:20 - a deck of cards you
368:23 - are allowed
368:26 - to reorder the cards in your hand so
368:29 - there is no ordering
368:31 - but once you draw a card you cannot draw
368:35 - it again
368:36 - so there isn't replacement and for the
368:39 - final situation my prototype problem
368:41 - here
368:42 - is choosing a dozen donuts when you have
368:45 - uh so many flavors of donuts because you
368:48 - are allowed to reorder the donuts in the
368:50 - box
368:51 - and in principle the donut shop has
368:54 - this almost infinite list for the
368:58 - number of or an almost infinite supply
369:01 - of donuts in this imaginary donut shop
369:03 - so there is replacement when you choose
369:06 - a flavor
369:07 - like when you choose a flavor you're
369:09 - allowed to choose as many donuts of that
369:11 - flavor
369:11 - as you want so a replacement is not an
369:14 - issue
369:16 - all right so you do have in fact
369:17 - replacement those are my prototype
369:19 - problems
369:20 - and those are good to keep in mind when
369:22 - we're going through and trying to think
369:24 - about
369:25 - what formulas work we should have and
369:27 - how we're going to
369:29 - prove these formulas and additionally uh
369:32 - it's kind of good to have these
369:33 - prototype problems in your mind when
369:35 - you're trying to solve common torque
369:37 - problems yourself
369:38 - all right so next up uh getting back to
369:40 - these proofs
369:41 - uh we need to show the formula for
369:43 - ordering without replacement
369:45 - uh so i had that formula for n factorial
369:49 - um so all right so
369:52 - how many ways are there to uh
369:55 - form a list of k things
369:59 - uh when you have n possibilities
370:02 - so we would need to pick the first item
370:04 - in the list
370:05 - and the number of ways that we could
370:07 - pick the first item in the list is
370:08 - n then we need to pick the the so there
370:11 - are
370:12 - n ways to
370:15 - pick the first item in the list
370:24 - then we need to pick the second item in
370:26 - the list
370:27 - so there are going to be n minus one
370:32 - ways
370:34 - to pick the second
370:42 - the reason why is that whatever we chose
370:44 - to be our
370:45 - our first element in the list cannot be
370:47 - chosen again to be the second element
370:50 - so we have n minus one ways to fill the
370:52 - second item
370:53 - then we have n minus two ways to pick
370:55 - the third
370:56 - the third item because now now in
370:58 - addition to not being able to pick
371:00 - what we chose for the first item in the
371:02 - list we also can't pick what we chose
371:04 - for the second item of the list so we're
371:06 - going to have n minus two way
371:07 - uh mis two choices now so mi it's two
371:10 - ways
371:11 - uh for the second item
371:17 - no no not the second the third
371:25 - and we just continue on with this uh
371:28 - with this process until we reach the kth
371:31 - slot
371:32 - uh and there will be uh for the
371:36 - kth item in the list there will be
371:40 - n minus k minus 1
371:43 - ways to pick that item which by the way
371:45 - is equal to n minus k
371:48 - uh plus one right so
371:51 - so n minus k plus one ways
371:55 - to pick
371:58 - the cave item
372:05 - so then we're going to use the product
372:10 - rule
372:15 - and by the product rule we multiply all
372:18 - of these numbers together so we're going
372:19 - to get
372:20 - so the number of possibilities is going
372:22 - to be n times n minus 1
372:24 - times dot dot times
372:27 - n minus k minus 1.
372:33 - and if we wanted to we could stop there
372:35 - this is in fact the formula but
372:37 - thing though is it's sometimes uh better
372:40 - to
372:42 - we we might prefer a more compact uh
372:46 - formula so we might say instead that
372:50 - um we have n n minus 1
372:54 - n minus 2 uh dot dot dot
372:58 - and then we have n minus k plus 1
373:03 - and then we could keep going multiplying
373:08 - and say we're going to multiply by n
373:09 - minus k
373:11 - and still you know still decreasing down
373:13 - but but we've now multiplied by n minus
373:15 - k
373:16 - and if we're going to multiply by n
373:17 - minus k we now
373:19 - need to divide by n minus k in order to
373:22 - keep things balanced
373:23 - and then we're going to multiply by n
373:24 - minus k minus 1.
373:27 - okay so we're going to need to divide by
373:28 - n minus k minus 1.
373:33 - and we're going to keep going with this
373:34 - process
373:36 - on both the top and the bottom until
373:38 - we're multi we multiply
373:40 - three two and one
373:47 - and what i've actually written down in
373:49 - doing this the top can be recognized
373:52 - uh so the top uh part of this uh
373:56 - fraction you you may recognize that as
373:59 - being uh i don't want that color
374:03 - you may recognize the top part as being
374:06 - n factorial
374:10 - and the bottom part of the fraction the
374:13 - denominator
374:14 - as being uh as being
374:18 - n minus k
374:22 - factorial
374:25 - hence uh producing the formula
374:31 - this will produce the formula uh n
374:34 - factorial
374:35 - over n minus k factorial
374:40 - which we may also just call p and k
374:49 - and that gives us our second formula
374:53 - okay uh next one very
374:57 - now what we now need to do is figure out
375:00 - how many ways there
375:01 - are to choose items that are not
375:04 - ordered and there is no replacement
375:08 - this gives us the formula that is often
375:10 - referred to in english as
375:11 - and choose k so
375:15 - the proof for this one is actually quite
375:17 - tricky where you start out by assuming
375:19 - that you know how to do it
375:21 - and then you get recover the formula in
375:23 - the end
375:24 - where you you pretend that you know the
375:26 - formula and after you pretend that you
375:28 - know the formula you
375:29 - then figure out something that you
375:31 - already know which in this case is the
375:33 - number of permutations of a list
375:36 - and once you have the number of
375:39 - permutations in the list you're able to
375:41 - recover
375:42 - uh the formula that you pretended that
375:43 - you knew but you actually actually
375:45 - didn't
375:46 - all right so that's that's rather
375:47 - convoluted let's get started
375:49 - with uh uh showing how we can get this
375:52 - formula so the proof again is kind of
375:54 - weird
375:54 - it's a really weird one but all of a
375:56 - sudden at the end
375:58 - we're going to have the result that we
376:00 - wanted so
376:01 - we have no order
376:06 - and we don't have replacement
376:17 - i keep touching that all right i keep
376:20 - touching it
376:22 - alright so not ordered without
376:24 - replacement so
376:25 - suppose
376:30 - uh n choose k
376:35 - is the number of ways
376:40 - to is a number of ways to do this
376:47 - so we know how so we know
376:51 - the number of ways to choose items when
376:53 - we don't care about order and we don't
376:54 - care about replacement and the number of
376:56 - ways to do so is n choose k
376:59 - okay bear with me i now
377:02 - want to know how many ways
377:10 - are there going to be to pick
377:17 - to pick
377:22 - uh k items
377:26 - out of n when replacement doesn't matter
377:41 - so as opposed to um doing so in order
377:46 - doesn't matter which we're assuming that
377:48 - we know
377:49 - uh that is what i want to do
377:54 - i want it what i want to do is calculate
378:02 - uh uh p and k or the number
378:05 - of of k length permutations of n objects
378:10 - okay so uh i want to compute that which
378:13 - by the way
378:14 - we have already computed that's that
378:17 - formula is actually already known to us
378:18 - it's up here
378:19 - but we're going to suggest that there is
378:21 - an alternative way
378:23 - to calculate this if somehow you knew
378:26 - the number of ways to pick k items out
378:30 - of
378:30 - n uh without without order
378:33 - so how would we do this how would we
378:36 - let's think about how we would construct
378:39 - a single permutation if what we had to
378:42 - do
378:43 - was pick items without order first
378:47 - so our first step
378:50 - if we were to attempt to form
378:53 - a single list of k items out of n
378:57 - when we can pick items without ordering
379:00 - them the first thing we would do is pick
379:02 - the number of
379:02 - items or pick what items will appear
379:06 - on the list without ordering them first
379:08 - so our first step
379:10 - in our process is to pick objects
379:19 - when order doesn't matter
379:24 - so you've decided basically what's going
379:26 - to appear on the list you just don't
379:27 - know
379:28 - in what slots it will appear
379:37 - and supposedly we know how to do so
379:41 - there's and choose k ways to do so so
379:54 - there are n choose k ways to pick the
379:56 - items that will appear on our list
379:57 - without
379:58 - ordering the items so first we pick
380:01 - a set of items that will appear on the
380:02 - list the next step then
380:04 - is to order those items
380:13 - so the second step is to order the k
380:20 - objects
380:23 - so how do we order the k objects well uh
380:26 - we picked the first
380:27 - so we pick uh one of the objects that we
380:30 - have selected
380:31 - to be the first item of the list uh
380:33 - there's k ways to do
380:34 - that then we pick another one of the
380:38 - objects that we haven't picked yet to be
380:39 - the second item on the list
380:40 - there's k minus ways to k minus one ways
380:43 - to do that keep doing so until you run
380:46 - out of items
380:47 - so there are uh
380:52 - k factorial ways to order the items
380:57 - oops oh darn it so there's k minus one
381:01 - ways to order the items going back to
381:04 - where i was
381:11 - or k factorial ways to order
381:22 - so now we're going to use the product
381:23 - rule and say
381:25 - that the number of ways to pick
381:28 - uh items to appear on our list is going
381:31 - to be the number of
381:32 - is going to be the number of decisions
381:33 - we have to make in step one which is the
381:35 - number of ways to pick the items to
381:36 - appear on the list
381:38 - uh there's n choose k ways to do that
381:41 - uh and then multiply that with the
381:43 - number of ways to order the items
381:45 - so there will be k factorial so that's
381:48 - the number of ways to form permutations
381:50 - but we also know that the number of ways
381:52 - to get permutations
381:53 - from what we did before is n factorial
381:56 - divided by
381:57 - n minus k factorial
382:02 - okay well what we actually were
382:04 - interested in this whole time
382:06 - was calculating this number that i just
382:08 - highlighted in red
382:10 - that's the number we actually want
382:13 - well how can we get that with
382:16 - division
382:21 - because now we have an algebraic
382:23 - relationship
382:24 - uh with which we can so that we can
382:27 - solve
382:28 - to get and choose k and it follows
382:32 - that n choose k is equal to
382:36 - n factorial divided by k factorial
382:41 - times n minus k factorial
382:48 - and we're done we computed what we
382:50 - actually
382:51 - wanted to compute
382:54 - so that was a little odd that was a
382:56 - little strange um
382:57 - here's some more ways to kind of justify
383:01 - uh this formula that we ended up with uh
383:04 - so the number of permutations is larger
383:08 - than the number of combinations
383:10 - because the because when you are
383:12 - sensitive to order
383:14 - you're going to end up with many more
383:15 - possibilities than if you're not
383:16 - sensitive to order
383:18 - so as a result you need to divide out
383:22 - uh by a factor that effectively removes
383:26 - uh all of the orderings that are that
383:28 - contain the same items that just in
383:30 - different
383:30 - and just in a different arrangement or
383:32 - to get the combinations
383:34 - and when it comes to computation like
383:36 - let's say for example three choose
383:38 - no uh five choose three um
383:41 - this formula for me at least when i was
383:44 - taking this class
383:45 - the way i think of it is i have five
383:47 - factorial on the bottom and on the
383:48 - bottom
383:49 - i'm going no i have five factorial in
383:50 - the top and on the bottom i'm going to
383:52 - have 3 factorial and whatever it takes
383:54 - to get
383:54 - 2 and the other number such that 3
383:58 - and the other number adds up to 5. so
383:59 - i'm going to have 5 factorial divided by
384:02 - 3 factorial times 2 factorial okay
384:05 - so that's a way so the formula is
384:07 - actually not that hard
384:08 - to remember because it's like okay uh
384:12 - 10 choose 7. that's going to be 10
384:14 - factorial divided by
384:15 - well we've got 7 factorial down there
384:17 - and we've also got something that adds
384:18 - up to ten
384:19 - okay three factorial so ten factorial
384:21 - divided by seven factorial times three
384:23 - factorial so the formula itself is not
384:25 - that hard to remember
384:26 - uh but you now have it
384:30 - okay uh continuing on
384:33 - so that was our third formula that i
384:37 - promised that we were going to prove
384:39 - uh so now we have one last formula
384:43 - to prove uh and that formula
384:47 - is let's see how much space we got
384:50 - uh hopefully we've got enough so the
384:53 - last formula we have
384:55 - uh not ordered
385:03 - and we have replacement so this is the
385:06 - donut shop situation
385:14 - okay so the donut shop uh situation
385:20 - uh here's like this this proof is again
385:23 - also rather tricky combinatorics often
385:26 - requires
385:28 - tricks honestly that that's that's one
385:31 - that's one reason why combinatorics is
385:32 - rather painful
385:34 - uh it feels like there are rather few
385:37 - unifying
385:38 - uh principles and combinatorics i know
385:40 - that there are some
385:42 - but it doesn't really seem that there's
385:44 - all that many
385:46 - so here's what we're going to do
385:49 - to um to solve this one
385:53 - uh what let's let's suppose that we're
385:56 - in a donut shop
385:57 - and we're going to choose say five
386:01 - donuts
386:02 - of three flavors uh how could we
386:06 - possibly do that
386:07 - uh we don't care about the ordering of
386:09 - the donuts because you put in the box
386:11 - and i mean no one's going to ask in what
386:13 - order the donuts
386:14 - were when you come home so
386:18 - what i like to do is imagine okay
386:19 - whatever we're going to do we're always
386:21 - going to arrange the donuts in the box
386:23 - right so so whatever arrangement they
386:25 - originally were we're going to put them
386:27 - into a fixed arrangement where we have
386:30 - one flavor first one flavor second and
386:32 - one flavor for
386:33 - flavor third so what we could end up
386:36 - doing to form
386:37 - a single box after we say that we're
386:39 - going to arrange to rearrange them at
386:41 - the very end
386:41 - is we're going to have
386:45 - uh we're going to uh put down all the
386:49 - donuts of a single flavor
386:51 - and we know that the donut that comes
386:53 - first
386:54 - is going to be of a certain flavor but
386:57 - we're also going to
386:59 - put dividers in our box to separate
387:02 - flavors
387:04 - so so the second flavor will come after
387:07 - the first
387:08 - divider and the third flavor will come
387:09 - after the second divider
387:12 - so and it is also possible to uh
387:15 - not have a donut of uh
387:18 - certain flavors so for example if we
387:20 - wanted a box of only the third flavor
387:22 - we put a divider in the first position
387:24 - of divider in the second position
387:27 - and then put donuts in all the remaining
387:29 - positions
387:30 - and with this encoding i have now
387:32 - encoded one of the one of the boxes of
387:34 - five donuts
387:35 - this is a box where you have only the
387:37 - third flavor and you can imagine what a
387:39 - box
387:40 - consisting of only the second flavor
387:42 - would look like so you'd have a divider
387:44 - at the beginning and a divider at the
387:45 - end
387:46 - and then in between you have your donuts
387:50 - so this is a box
387:51 - that has only donuts of the second
387:53 - flavor
387:56 - and maybe play around with this encoding
387:58 - of donut boxes
388:00 - but once we have this encoding it is now
388:03 - possible to calculate how many how many
388:05 - boxes we can achieve
388:07 - because what we can do is say how many
388:09 - ways are there
388:11 - to pick positions for dividers
388:14 - and positions for donuts
388:20 - how many ways are there to do that well
388:22 - we're going to have if
388:23 - in the if we have uh three flavors we're
388:26 - going to have two dividers
388:28 - and if we have five donuts we're gonna
388:30 - have positions four or five donuts
388:33 - so we're gonna end up with seven
388:35 - positions that we need to fill up
388:37 - uh we could just pick two of the seven
388:40 - positions
388:41 - to contain dividers and the remaining
388:44 - positions will contain
388:46 - donuts so let's see we could
388:49 - potentially pick numbers we could assign
388:52 - a number to each of the positions
388:54 - so we have positions one two three four
388:58 - five six seven and we pick
389:01 - numbers that represent positions that
389:04 - will
389:05 - contain dividers so in this case uh
389:08 - uh numbers four and six represent
389:10 - positions four and six
389:12 - so those will be uh so
389:15 - if we pick four and six which by the way
389:17 - is the same as picking six and four
389:19 - to contain dividers so in other words we
389:20 - don't care on the ordering of the
389:22 - numbers that we end up picking
389:24 - uh once we pick that we now know what
389:26 - our box of donuts is going to be
389:28 - because we've picked the positions for
389:29 - the dividers and therefore every other
389:31 - position will contain
389:32 - donuts okay
389:35 - all right then um so
389:38 - if that is the case how many boxes
389:42 - of donuts are there going to be
389:45 - um
389:53 - so in this case the number of
389:55 - possibilities
389:56 - which is n in this scenario is equal to
390:00 - three
390:01 - and out of that n we're going to be
390:03 - choosing
390:04 - uh k possibilities uh or we're going to
390:07 - be choosing k
390:08 - items or k donuts so for this problem k
390:10 - is equal to five
390:13 - okay so um
390:16 - how many ways were there to make this
390:18 - decision well it turns out
390:21 - uh there were um
390:24 - there were five plus three
390:28 - minus one choose three minus one ways
390:31 - to pick donuts in this fashion
390:35 - because we end up picking the or
390:39 - in other words this is uh uh this is
390:42 - seven
390:42 - choose two because we have seven slots
390:46 - and we pick two of those slots to
390:48 - contain dividers and the rest of the
390:49 - slots contain donuts hence we get the
390:52 - number of donuts
390:53 - all right so let's generalize this idea
390:55 - so if we have
390:56 - n possibilities we're going to have n
390:59 - minus 1 div
391:00 - n minus 1 plus k slots because we're
391:03 - going to have
391:04 - n minus 1 dividers and then also the
391:06 - case slots for the k
391:07 - things that we're going to end up
391:08 - picking so in general
391:10 - um uh so
391:13 - let's see uh
391:16 - so we're so we have um
391:23 - so we have uh
391:27 - n that no uh
391:31 - k plus n minus slots
391:35 - n minus one slots
391:38 - uh to fill
391:43 - with uh n minus one
391:47 - dividers
391:51 - which we call uh which we're going to
391:54 - call the
391:55 - the bar uh remaining slots
392:06 - so the remaining slots
392:09 - contain items
392:17 - uh so we need to pick uh so pick the
392:20 - positions
392:23 - let's see yeah so pick positions
392:29 - for dividers
392:37 - and order doesn't matter
392:48 - so since order doesn't matter
392:52 - there are
392:55 - going to be uh n plus
392:59 - uh no uh i like a different ordering uh
393:02 - there are going to be k plus n minus one
393:05 - choose
393:05 - n minus one ways to do so
393:13 - and we're done
393:17 - you end up with the formula
393:26 - and we're done so that was exhausting
393:29 - uh this is rather tricky
393:33 - types of mathematics that honestly it's
393:35 - it's frustrating
393:37 - because it often just requires knowing
393:39 - special tricks
393:41 - in order to solve a certain problem it
393:43 - just feels like all you're doing is
393:44 - coming up with
393:45 - a longer and longer list of tricks and
393:47 - it doesn't really feel like there's much
393:48 - of a unifying principle to them
393:50 - uh at least to me at least to me
393:52 - personally
393:53 - it's it seems also to me like there are
393:55 - some people out there
393:57 - some really smart mathematicians for
393:58 - which this type of thinking
394:00 - just for some reason just clicks and
394:03 - they and they
394:03 - are able to see an underlying principle
394:06 - i don't see it
394:07 - neces uh often uh but i often know
394:10 - enough
394:11 - combinatorics like this co this amount
394:12 - of combinatorics will get you very far
394:14 - in life
394:15 - uh in your statistics life that is uh
394:17 - you don't really need to know
394:19 - that much more than this
394:23 - all right uh so those were complicated
394:26 - proofs
394:27 - uh now we're gonna go through a series
394:30 - of examples to show
394:32 - uh how these techniques can be applied
394:35 - so uh suppose we're going to roll two
394:37 - six-sided dice and we assume
394:40 - each outcome is equally likely and
394:43 - we're going to say that the dice are
394:44 - different colors so uh
394:46 - if the red dice has a six and the blue
394:48 - dice has a one that's different from the
394:49 - red dice having a one a blue dice having
394:51 - a six
394:52 - those are two different pop those are
394:53 - two different outcomes
394:55 - how many possible outcomes are there and
394:58 - what about
394:59 - the situation when there's three six
395:00 - sided die uh
395:02 - so we're to first answer problem that
395:05 - i'm going to call
395:06 - one and the problem that i'm going to
395:08 - call two
395:09 - all right uh so the answer to one
395:12 - uh order matters and we're doing so with
395:15 - replacement because if we roll
395:17 - um a one for the red dice we're still
395:19 - allowed to roll a one for the blue dice
395:21 - so we end up with uh there are six
395:24 - possibilities and we're choosing two of
395:25 - them so we get 36.
395:29 - all right for the next option uh well
395:32 - it's just the same things now there's
395:33 - we're just choosing three instead of two
395:35 - so that's going to be
395:36 - 216. so this is a situation where order
395:40 - matters
395:42 - because the dices the dice are different
395:44 - colors
395:46 - and there is replacement
395:52 - because dice don't care what the other
395:53 - dice rolled
395:59 - all right uh next example a high school
396:02 - has 27 boys playing men's basketball
396:05 - in basketball there are five positions
396:06 - point guard shooting guard small forward
396:08 - power forward and center
396:10 - each assignment of player two position
396:12 - is unique
396:13 - how many teams can then be formed this
396:16 - is
396:17 - a this is a permutation type problem so
396:20 - here order matters because there's
396:21 - different positions
396:24 - and each of those positions are distinct
396:28 - but there is a replacement because a
396:30 - person playing point guard cannot also
396:32 - be center
396:33 - so position doesn't matter
396:37 - oh no position matters or order matters
396:41 - no sorry sorry no replacement no
396:43 - replacement i get it eventually
396:45 - all right so no replacement
396:55 - so that means we're going to be using
396:57 - that a second uh
396:59 - that second formula in the clock so
397:01 - we've got
397:02 - uh p uh there's a 27 possibilities we're
397:06 - going to order five of them
397:09 - so that's going to be 27 factorial
397:12 - divided by 27 minus 5
397:16 - factorial which is 27 factorial
397:22 - over 22 factorial which also can be
397:26 - written
397:26 - as uh maybe more simply into the point
397:29 - 27 times 26 times 25
397:35 - times 24.
397:38 - times 23 that's almost easier than
397:41 - remembering the formula just remembering
397:43 - that you
397:44 - decrement that many times and this
397:47 - multiplies
397:48 - out to uh 9 million six hundred and
397:52 - eighty seven
397:54 - thousand six hundred potential teams
397:59 - okay uh example nineteen when playing
398:02 - poker
398:03 - players draw five cards from a 52 card
398:05 - deck
398:06 - every card is distinct but the order of
398:09 - the draw
398:10 - does not matter you are allowed to
398:12 - reorder the cards in your hand
398:14 - how many hands are possible in this
398:16 - situation
398:17 - because you're allowed to reorder order
398:19 - doesn't matter
398:28 - and since order and also an additional
398:30 - ordering not mattering there is no
398:32 - replacement
398:33 - because if you draw an ace you can't if
398:35 - you draw an ace of spades you're not
398:37 - allowed to draw an ace of spades again
398:38 - so no replacement
398:45 - okay uh so let's see if that's the case
398:48 - then we're going to use that fourth
398:49 - third formula in the clock uh we have uh
398:52 - 52 possibilities for 52 cards we're
398:55 - going to choose five of them when we
398:56 - draw cards
398:57 - and that's going to be 52 factorial
399:00 - divided by 5 factorial
399:04 - times 47 factorial
399:08 - which is equal to 52
399:12 - times 51 times 50
399:16 - times 49 times 48
399:23 - divided by 5 times 4
399:27 - times 3 times two times one
399:31 - okay uh and we and we don't care about
399:33 - the one because it's times one that that
399:35 - doesn't really do anything
399:36 - um we can do some cancellation like for
399:39 - example the 50 and the five cancel down
399:41 - to a ten
399:43 - uh the 4 3 and 2
399:47 - those met multiply the 24 so that
399:50 - reduces with the 48 rendering it a 2
399:53 - so this is equal to 52 times
399:57 - 51 times
400:00 - 10 times 49
400:03 - times 2 which you then go to your
400:06 - calculator
400:07 - and it will give you 2 million uh
400:10 - 500 and thousand
400:14 - uh 960
400:17 - poker hands
400:22 - now r can do a lot of these calculations
400:25 - so
400:26 - for example 16 you can just say what is
400:30 - 6 to the power 2 and i'll tell you that
400:31 - the 36
400:32 - and 4 6 to the power 3 that's 216.
400:35 - r does have a factorial function
400:40 - now be careful with some of these
400:42 - functions because you might end up
400:45 - with integer overflow you might end up
400:47 - with
400:48 - numbers that are so large that uh the
400:51 - computer cannot handle them so i would
400:54 - not just blindly use these functions
400:56 - because it is possible
400:58 - for these numbers to explode very
401:00 - rapidly in which case
401:02 - your calculations will end up being
401:03 - wrong but we do have a factorial
401:05 - function and we do have a choose
401:07 - function
401:08 - so here's example 17 and here's example
401:10 - 18.
401:11 - um uh
401:14 - so or is that that number might be wrong
401:19 - yeah that that that numbering's wrong my
401:21 - apologies
401:23 - uh so i guess at some point when i wrote
401:25 - this
401:26 - r code uh i uh
401:30 - or when i wrote these notes i must have
401:31 - deleted an example a long time ago but i
401:33 - did not change the comments in the r
401:35 - code this is why i need to be very
401:36 - careful with comments
401:38 - comments expire eventually they turn bad
401:42 - all right and you know what's worse than
401:45 - a note then no comment a misleading
401:48 - comment
401:49 - that's even worse um all right so
401:52 - example 20 you want to choose a dozen
401:55 - donuts from a donut shop
401:56 - there are eight different kinds of
401:58 - donuts how many boxes of a dozen donuts
402:00 - are possible
402:01 - well okay so in this situation uh we are
402:04 - choosing
402:05 - 12 donuts and there are eight
402:08 - possibilities so using that uh
402:11 - fourth formula in the clock
402:14 - we have uh 12 plus
402:18 - 8 minus 1 choose
402:21 - 8 minus 1 possibilities which is going
402:24 - to be
402:26 - 19 choose 7 which is equal to 19
402:29 - factorial
402:31 - divided by 7 factorial
402:35 - times 12 factorial which is equal to
402:41 - 19 times 18
402:45 - times 17 times
402:48 - 16 times 15
402:51 - times 14
402:54 - times 13 divided by 7
402:57 - times 6 times five times
403:01 - four times three times
403:04 - two times one all right and uh let's do
403:08 - some
403:09 - cancellation to help make our life a
403:11 - little bit easier
403:12 - uh let's see the 7 and the 2
403:16 - are going to cancel with the 14
403:19 - we've got what else 4 and 16 will reduce
403:23 - the 16 down to 4
403:26 - the 5 and the 3 will cancel out the 15
403:29 - and the 6 cancels out with the 18
403:31 - reducing it to 3
403:33 - so in the end we're going to have this
403:36 - is 19
403:37 - times 17 times 13
403:41 - times three times four
403:44 - and then you go to your calculator and
403:45 - ask what that is and you'll get
403:48 - fifty thousand uh 388 potential boxes of
403:53 - donuts from this donut shop
403:57 - okay and here is some arco that will
404:01 - also
404:02 - compute that quantity
404:06 - okay uh so let's do some classic poker
404:10 - problems
404:10 - once people uh introduce combinatorics
404:14 - it's like the next thing you have to
404:15 - talk about are poker problems because
404:17 - poker problems are fun
404:18 - because poker is fun it's fun to talk
404:20 - about poker
404:21 - um so um
404:25 - now that said there is uh one potential
404:28 - was talking about poker problems is that
404:30 - unfortunately not everyone is familiar
404:32 - with the poker deck
404:34 - or the standard playing card deck as
404:36 - it's known in the english-speaking world
404:38 - which for what it's worth the standard
404:40 - deck is technically the french deck
404:43 - and different european countries have
404:45 - different traditional playing card
404:47 - decks so like for example my advisor leo
404:50 - horvath the
404:52 - traditional deck that's used in hungary
404:54 - is not the french deck
404:56 - so he doesn't like personally poker
404:58 - problems because there's something that
404:59 - like they talk about a deck and he's not
405:01 - very familiar with that deck and it just
405:03 - goes against his intuition whereas i
405:05 - myself
405:06 - i grew up with this deck i grew up in
405:08 - america
405:09 - so i'm very familiar with uh with what
405:12 - is inside
405:13 - of a playing card deck that said if
405:16 - you're like an international student or
405:17 - something like that
405:18 - here is a description of what is inside
405:23 - of a playing card of a standard playing
405:25 - card deck or
405:27 - a a french deck the deck that's used in
405:29 - the english speaking world
405:31 - and often used in these probability
405:33 - textbooks because
405:34 - most of these probably authors are most
405:37 - most of these
405:38 - most of the authors of these uh probably
405:39 - books they may be in america but they're
405:41 - certainly
405:42 - speaking english they're probably using
405:43 - uh this deck
405:45 - so and here's some additional notation
405:48 - for
405:49 - uh this is a poker notation to describe
405:52 - what goes inside
405:54 - uh what cards are inside of a deck all
405:56 - right so but basically you've got four
405:58 - sweets and 13 possible faces uh
406:02 - all right for a total of 52 possible
406:05 - cards you should know how to do that by
406:06 - now because
406:07 - uh how do you form a single card you
406:09 - first pick
406:10 - a suit there's four suits possible then
406:14 - pick a face value there's 13 faces so
406:17 - 13 times 4 will be 52. all right so
406:20 - you've learned something today uh
406:22 - example 21 a poker hand
406:24 - is four of a kind if four cards
406:27 - have the same face value how many four
406:29 - of a kind hands
406:30 - exist how are we going to solve this
406:33 - problem
406:34 - well uh the trick that we're going to
406:38 - use
406:38 - again with these poker with these not
406:40 - just poker problems but
406:41 - most combinatoric problems what i
406:44 - suggest that you do
406:45 - is come up with a narrative like i just
406:48 - did right now
406:48 - for figuring out how many cards there
406:50 - are in a deck come up with a narrative
406:53 - for forming a single combination
406:56 - and then once you have that narrative
406:58 - figure out how many choices there were
406:59 - to make
407:00 - at each at each junction and
407:04 - multiply them together with the pop with
407:05 - the uh power rule
407:07 - and you'll get what you need um so
407:10 - first thing we're going to do to form a
407:12 - four of a kind hand
407:15 - is we're going to decide what card
407:18 - face value will be the four of a kind
407:21 - card so first we're going to pick
407:24 - uh the four of a kind card
407:33 - so we're going to say for example
407:35 - they're going to be four kings
407:36 - in this hand or uh four twos or four
407:40 - tens something like that so we need to
407:42 - pick a face value
407:44 - and there are 13 face values to be the
407:47 - four of a kind
407:48 - part right so there are 13 ways
407:51 - to pick the face value then
407:55 - we need to pick the remaining card
407:57 - because a poker hand has five cards
407:59 - we have picked four of those cards if we
408:02 - decided
408:03 - that we were going to use the ace we
408:05 - were going to use ace
408:07 - for the four of a kind card then we've
408:09 - automatically got
408:10 - the ace of spades the ace of hearts the
408:13 - ace of diamonds and the ace of clubs
408:15 - that's four cards now we need to pick
408:18 - the remaining card if we picked ace then
408:22 - we cannot pick the ace
408:23 - face value again so
408:26 - uh we've uh and in fact in that 52 card
408:30 - deck we have taken out
408:32 - four of the cards and put them in our
408:34 - hand leaving 48 cards remaining
408:36 - in our deck so that means that we have
408:39 - 48
408:40 - cards to be the potential fifth card
408:43 - so pick the fifth card
408:52 - so 13 times 48 you multiply those two
408:55 - numbers together to get 624 poker hands
408:59 - with four of a kind
409:05 - uh next up a poker hand is considered
409:08 - full house if two cards have the same
409:11 - face value
409:12 - and three different cards have another
409:14 - common face value
409:16 - how many full house hands exist
409:19 - let me just list out for you an example
409:21 - of a full house hand
409:22 - you could have let's say the four
409:26 - of diamonds the four
409:29 - of spades
409:32 - so the four of spades and
409:36 - uh the four of uh hearts
409:41 - and uh so that's three cards with a
409:43 - common face value and then we need two
409:45 - cards with another
409:46 - face value we can't pick four for this
409:48 - so we're going to pick
409:50 - uh let's say king so we'll have the king
409:53 - of
409:53 - uh clubs
409:57 - and the king of uh diamonds
410:01 - so this would be a full house hand
410:04 - so let's see how can we form a full
410:07 - house hand in general
410:09 - well we're going to have two different
410:11 - face values
410:12 - in our full house hand but one face
410:16 - value will be the
410:17 - more numerous face value the three of a
410:19 - kind and the other face value will be
410:21 - the two of a kind
410:22 - so what i suggest we do is first
410:27 - pick the card that will be the three of
410:29 - a kind card
410:31 - so pick the card that will be the three
410:32 - of a kind card
410:35 - so pick the three of a kind
410:42 - face value
410:48 - and there's 13 face values for this
410:50 - choice
410:51 - so there's 13 ways to make that choice
410:54 - then we need to pick the suits because
410:58 - we picked for example four
411:00 - but there are four suits from which we
411:02 - can choose and we need to pick
411:03 - three of them so now we need to pick the
411:08 - suits
411:11 - and order doesn't matter in this
411:13 - situation we don't care about the
411:15 - ordering of the suits we just need to
411:16 - pick them
411:17 - so so we need to pick exactly three
411:20 - suits
411:21 - for all those three cards and since
411:23 - order doesn't matter for this choice
411:26 - we have four choose three ways to pick
411:30 - the suits and four choose three
411:33 - evaluates to four
411:37 - which is not surprising you know like
411:39 - alternatively and by the way this this
411:41 - thought process is quite useful
411:43 - instead of picking the three faces we're
411:45 - going to include we could have picked
411:46 - the one face
411:48 - no no no no not face i am so sorry
411:51 - i for the life of me i can't keep my
411:53 - word straight i'm always doing this
411:56 - uh so not face but suits uh instead of
411:58 - picking
411:59 - uh the three suits we're going to
412:02 - include
412:03 - we could pick the one suit we're going
412:05 - to exclude
412:07 - and there's four ways to pick the suit
412:09 - we're going to skip
412:11 - all right so there's an useful trick to
412:12 - keep in mind uh
412:14 - next up we need to pick the two of a
412:17 - kind
412:18 - card
412:24 - we need to pick the two of a kind face
412:26 - value
412:30 - so we can't pick what we chose before we
412:33 - like for example we couldn't choose 4
412:34 - again
412:35 - so we need to pick one of the 12
412:37 - remaining face values so there's 12 ways
412:39 - to make this choice
412:41 - and then after we pick the face of the
412:44 - two of a kind card we now need to pick
412:45 - the suits
412:47 - so we need to pick two
412:50 - suits and again
412:53 - uh we need to do so where we don't have
412:56 - replacement but order doesn't matter
412:58 - you can't have replacement because there
412:59 - are not two uh king of hearts in the
413:02 - deck there's only one king of heart
413:04 - so you pick so yeah you can't
413:07 - pick the same suit twice uh so um
413:11 - of the four suits we need to choose two
413:14 - of them to include
413:16 - so that's going to be 4 choose 2 4
413:18 - choose 2 evaluates to 6.
413:21 - so in the end this is going to equal
413:27 - 13 times 12
413:30 - times 6 times 4 which is equal to
413:34 - uh 3744
413:38 - uh full house hands
413:44 - all right uh and here's some r code
413:47 - that's computing those quantities
413:49 - uh notice by the way this is a useful
413:52 - trick to
413:52 - to know to be aware of uh when doing uh
413:56 - when when using r notice that i
413:59 - wrapped the entire expression in
414:02 - parentheses
414:04 - if i didn't put the parentheses there
414:06 - nothing would have printed but when i
414:07 - wrap an entire
414:08 - expression in parentheses uh when i'm
414:11 - doing some variable assignment
414:13 - the variable the value of the variable
414:15 - that i just assigned gets printed
414:17 - which is really nice so if you remove
414:20 - these parentheses
414:21 - these parentheses the 624 would not have
414:24 - been printed
414:25 - but since i put the parentheses there it
414:28 - in addition to
414:29 - doing the assignment done here it prints
414:31 - the value of the variable
414:35 - that's a that's a nice trick all right
414:38 - so
414:38 - example 23 a flush is
414:42 - a poker hand where all cards belong to
414:45 - the same suit
414:46 - how many flush hands exist including
414:49 - what's called a straight flush
414:51 - a straight flush is a flush
414:55 - where the cards are also where you can
414:58 - order the car so that they're in
414:59 - sequence
415:00 - uh poker actually has a number of
415:02 - sequences but for example
415:04 - a hand where all the cards are spades
415:07 - and
415:07 - the cards are five six seven eight nine
415:10 - that is a straight flush since they're
415:11 - also in or
415:12 - since you can order them right and the
415:15 - straight flush is considered
415:17 - a different kind of hand in poker that's
415:20 - a straight flush is in fact
415:22 - the best possible poker hand so
415:26 - we actually should be accounting for
415:27 - straight flushes because generally when
415:29 - people say flush they are not
415:30 - including straight flush but we're just
415:33 - going to
415:34 - include straight flushes for now um
415:40 - we're going to allow that possibility so
415:43 - uh
415:43 - for a flush hand we're going to
415:47 - so in a flush hand all the cards have
415:49 - the same suit
415:51 - there are four possible suits so the
415:53 - first thing we need to do
415:54 - is pick the suit
416:01 - and there are four ways to pick the suit
416:04 - after we pick the suit we need to pick
416:07 - the face values
416:17 - so there are 13 possible face values we
416:20 - need to pick
416:20 - five of them to be in our hand so uh
416:24 - once you pick one face value you cannot
416:25 - pick it again
416:27 - because there are no two king of spades
416:29 - for example
416:30 - so there are 13 face values and we're
416:33 - going to choose five of them we don't
416:34 - care about the ordering and we're going
416:35 - to do so without replacement
416:37 - so you multiply those two numbers
416:38 - together uh this is going to be
416:41 - four times uh 1287 that's what that
416:46 - uh 13 choose 5 evaluates to and this is
416:49 - going to equal
416:53 - 5148
416:56 - but this is also including the straight
416:58 - flush hands
417:00 - in the next problem i'm going to show
417:02 - you how we could potentially
417:04 - remove the straight flush hands because
417:07 - i'm actually going to ask that we
417:08 - that we do in fact remove them so a
417:11 - straight
417:11 - in poker is where cards can be arranged
417:15 - in sequence
417:16 - so for example we have five of uh five
417:19 - of spades six of clubs seven of clubs
417:22 - eight of hearts nine of hearts so this
417:24 - is a straight
417:25 - and the suit doesn't matter for a
417:28 - straight
417:29 - unless it's a straight flush if it's a
417:31 - straight flush the suit must be
417:33 - the same because it's also a flush
417:37 - so a straight flush is both a straight
417:39 - and a flush so it has a flush with all
417:41 - cards
417:42 - no it is uh so it is um
417:46 - uh i should change the that wording
417:49 - uh it is a straight
417:52 - it is a straight with all cars belong to
417:54 - the same suit
417:58 - and it is also the best possible poker
418:01 - hand
418:01 - uh hold on okay good
418:07 - all right uh how many straight flush
418:09 - hands exist we're going to compute that
418:11 - number now
418:12 - how many straight flushes exist
418:16 - so to get a straight flush so let's
418:19 - uh let's uh start some separations
418:23 - so first thing we're going to do is
418:24 - compute the straight flush
418:31 - all right so excuse me uh
418:34 - here by the way um is uh the
418:38 - uh the ordering of poker hands because
418:40 - poker
418:41 - poker face values have a ring uh have a
418:44 - ranking
418:45 - uh we have ace two three four
418:48 - five six seven eight
418:52 - nine and i'm gonna write x for ten
418:55 - the roman numeral x then we have
418:59 - uh uh jack
419:02 - queen uh king
419:06 - and also ace again because ace can be
419:09 - both low and high um and in fact the
419:13 - best
419:14 - literal possible poker hand is
419:17 - 10 jack queen king ice all of the same
419:20 - suit
419:21 - that is the best straight flush
419:24 - okay um
419:28 - so uh to have so to
419:31 - so for the flush part we we need to
419:33 - figure out how many ways there are for
419:35 - the flush part
419:36 - and how many ways there are to get the
419:37 - straight part so there are four ways to
419:39 - pick the flush part
419:40 - the part where you have the same suit so
419:42 - we're going to pick
419:43 - the suit for the flush part and then we
419:45 - have the straight part and the straight
419:47 - part
419:48 - for that what i would recommend you do
419:50 - is
419:51 - think about how many ways there are to
419:53 - pick the first card
419:56 - so pick the first card of the straight
420:06 - because if you know that the first card
420:09 - or the lowest card
420:10 - of the straight is ace since you there
420:14 - are five cards in hand that means that
420:15 - the other
420:16 - four cards are two three four five right
420:19 - so how many ways are there to pick the
420:21 - lowest card
420:22 - in the straight so let's see we got uh
420:26 - one two three four five six uh
420:30 - five six seven eight nine
420:33 - oh ten what do you know because the
420:36 - moment you get to ten
420:37 - the the uh the straight hand would be
420:40 - ten
420:40 - jack queen king ace and there is no
420:44 - straight that starts the jack because
420:46 - there is no
420:47 - you would not be able to get the fifth
420:49 - card since you can't circle around back
420:51 - to two
420:52 - so um so that means that there's ten
420:56 - ways to pick the lowest card
420:58 - in the straight so that means that the
421:00 - number of straight flush hands is going
421:02 - to be 4 times 10 which equals 40.
421:05 - all right and i asked also how many
421:09 - straights are possible
421:10 - but this time i'm not including straight
421:12 - flush because straight flush is
421:14 - considered different
421:19 - well uh the first thing we're going to
421:23 - do
421:23 - is pick the lowest card
421:31 - like we did above and then we're going
421:34 - to pick the suits
421:37 - so pick the suits of the cards
421:42 - so we decided there are 10 ways to pick
421:44 - the first card
421:47 - of to pick the first card of the
421:50 - straight
421:50 - and now to pick the suits well if i
421:54 - decide that the first card
421:55 - let's say the ace is going to be clubs
421:58 - and then i pick the next card which is
422:00 - going to be a 2
422:02 - i can still pick clubs again because
422:05 - i took out the the ace of clubs but i
422:07 - didn't take out the two of clubs so i
422:08 - can still pick clubs
422:10 - for that second card so and i can still
422:13 - pick clubs for the third card and so on
422:15 - so basically i do have replacement
422:17 - but order does matter because uh picking
422:20 - the first car there to be the ace of
422:22 - clubs and the second car to be
422:24 - the two of hearts is different from the
422:25 - first car being the ace of hearts and
422:27 - the second card
422:28 - being the two of clubs those are two
422:29 - different hands so order does matter
422:32 - and also um
422:36 - uh you're doing so with replacement
422:37 - there are four possibilities we need to
422:39 - pick
422:39 - five of them so this will be four of the
422:42 - power five
422:43 - the thing though is once i've done this
422:46 - i am including
422:48 - picking the first card to be hearts the
422:49 - second card would be hard the third card
422:51 - to be hard the fourth card would be
422:52 - hearts and the fifth card to be hearts
422:54 - that's a straight flush i don't want to
422:56 - include straight flushes
422:58 - so how am i gonna remove the straight
422:59 - flushes i'm gonna subtract them out
423:04 - so this is straight flushes
423:10 - so just remove them remove them from the
423:12 - calculations subtract them out
423:14 - and you no longer need to worry about
423:16 - them anymore
423:18 - this is basically remember that sum rule
423:20 - that i very briefly mentioned this is
423:22 - basically that sum rule
423:23 - right where you say the total number of
423:26 - like straight flush
423:28 - number of straights including flushes is
423:29 - going to be the number of straight flush
423:31 - and straight non-flush
423:33 - hands so you add those together which
423:34 - means you can do some algebra to get
423:36 - subtraction too
423:37 - anyway uh in the end you calculate this
423:39 - and you get
423:40 - 10 200 uh straight hands
423:44 - excluding the straight flush
423:54 - okay uh we are almost done with this
423:57 - section
423:58 - we've had a long discussion about
424:00 - counting
424:02 - and i haven't really said anything about
424:03 - how this test of what this has to do
424:05 - with probability
424:06 - well this entire section was devoted to
424:09 - the case that you may have thought was
424:10 - the easy case
424:12 - where you have a set
424:15 - where your sample space has finite size
424:18 - your sample space has finite size
424:20 - and you decide that every element in
424:22 - that sample space is equally likely and
424:24 - you may have thought this is the easy
424:26 - case
424:26 - because in that situation it's actually
424:28 - rather easy
424:29 - to compute probabilities you can assign
424:32 - a very natural probability measure
424:34 - the probability of any event a which is
424:37 - a subset of this sample space
424:38 - will be the number of elements in that
424:41 - set a
424:42 - or in that event a divided by the size
424:44 - of the sample space
424:48 - which is a very nice natural probability
424:50 - measure
424:51 - but here's the thing though you need to
424:53 - compute then
424:54 - using these counting techniques the size
424:57 - of the sample space and the size of your
424:59 - set a
425:00 - and that's actually tricky because now
425:02 - you need to use these uh
425:04 - counting techniques and i personally
425:06 - don't think the counting techniques are
425:07 - all that easy
425:08 - all right um so based off of this
425:12 - once we have this natural probability
425:13 - measure we can use counting techniques
425:15 - if we need to
425:16 - to compute probabilities so we're going
425:19 - to use this to compute the probability
425:21 - of the poker hands that we were
425:22 - considering above
425:23 - so the size of pos of the sample space
425:26 - the sample space consists of possible
425:28 - poker hands
425:29 - we say that each of those poker hands
425:31 - are equally likely we do not care about
425:33 - the ordering of poker hands
425:34 - so there's going to be 52 choose five
425:37 - such poker hands
425:39 - which is going to be uh two million five
425:42 - hundred ninety eight
425:44 - uh thousand uh 960 possible poker hands
425:48 - all right so i want to compute the
425:50 - probability of
425:52 - each of the poker hands that we saw
425:55 - in the previous examples so
425:58 - for example 21
426:02 - this was the four of a kind so the
426:05 - probability of a four of a kind hand
426:11 - is going to be the number of four of a
426:13 - kind hands which we computed to be
426:15 - to be 624 divided by the size of the
426:19 - sample space which is 2 million 198
426:22 - thousand
426:23 - 960 poker hands which is going to be
426:27 - approximately 0.0002
426:33 - uh for example 22
426:41 - the probability of getting a full house
426:49 - is going to be uh let's see
426:52 - so probability of full house is going to
426:55 - be 3744
427:00 - divided by two million five hundred
427:02 - ninety eight thousand
427:04 - nine hundred and sixty the number of
427:05 - full house hands divided by the total
427:06 - number of possible poker hands
427:08 - this is approximately zero point
427:13 - zero zero one four
427:16 - uh next up so
427:20 - uh for example 23.
427:27 - so for example 23 we're going to compute
427:29 - the probability of our
427:30 - so-called flush hand a flush that isn't
427:33 - actually of
427:34 - that includes straight flush so just
427:37 - the suit is the same uh how many
427:40 - so there were 5148
427:44 - such hands possible we're going to
427:46 - divide that by the size of the sample
427:47 - space which is that 2 million ish number
427:50 - um and that's going to be approximately
427:53 - 0.002
427:55 - and now for example 24
428:02 - the probability of a straight flush
428:12 - uh that's going to be there were 40
428:14 - straight flush hands
428:16 - divided by the size of the sample space
428:18 - this is going to be
428:19 - approximately 0.00002 really really
428:26 - really small
428:27 - and the probability of a straight
428:32 - so that's excluding the straight flush
428:36 - is going to be
428:39 - 10 200 divided by the size of the sample
428:43 - space
428:44 - which is approximately 0.004
428:50 - there's always kind of this question of
428:51 - which is more likely a flush or a
428:52 - straight
428:53 - turns out the straight is twice as
428:55 - likely as the flush
428:56 - which is which to me is a little
428:58 - surprising it feels it feels
429:00 - intuitively like the straight is harder
429:03 - but it's actually not
429:04 - um and that's one that's a common trait
429:07 - of probability
429:08 - it it defies what you feel like should
429:10 - be true
429:11 - all right so that's it for this section
429:15 - and uh in the next success section we're
429:18 - going to talk about
429:19 - a conditional probability so
429:23 - uh i will see you then
429:31 - since somewhere using the idea
429:34 - of independence you can almost find it
429:36 - anywhere if you look
429:38 - hard enough so independence is an
429:40 - extremely
429:41 - important idea in probability i've heard
429:44 - some people say
429:45 - that independence is what keeps
429:48 - probability as a mathematical theory
429:49 - from just
429:50 - being a subset of analysis so we're
429:53 - going to talk about
429:54 - uh independence if you come away from
429:56 - this lecture
429:58 - not or come away from this class maybe
429:59 - more appropriately
430:01 - not knowing what independence is then
430:04 - that's
430:05 - that's bad that's really bad you must
430:08 - understand independence because it is
430:10 - used everywhere it's one of the most
430:12 - important ideas in probability theory
430:15 - so two events a and b are said to be
430:19 - independent
430:20 - if the probability of a given b is the
430:22 - probability of a
430:24 - so in some sense the information about
430:26 - the event b
430:27 - gives no information about whether a has
430:28 - happened
430:30 - since uh the probability of a knowing
430:33 - that b happened does not change
430:36 - so first off i've said only that the
430:39 - probability of a given b is the
430:40 - probability of a
430:41 - uh what about the probability of b given
430:45 - a
430:45 - and before i proceed i should probably
430:47 - uh make a quick
430:48 - note saying i'm going to assume
430:52 - that a and b are events that have
430:54 - probabilities on their own that are not
430:56 - necessarily zero just because
430:59 - it's easier mathematically to assume
431:01 - that their probabilities are not zero
431:03 - and you kind of have to
431:05 - uh treat the case where they are events
431:08 - with
431:08 - probability zero as a little as a
431:12 - as a separate thing but honestly
431:15 - for for the most part you get the
431:17 - information that you need with
431:18 - just pretending that just ignoring the
431:21 - the chance that
431:22 - uh they have probability zero okay so
431:26 - uh all right then let's uh compute the
431:29 - probability of b
431:30 - given a uh it seems
431:33 - like what should be the case is that if
431:37 - a is independent of b
431:38 - then b should be independent of a as
431:41 - well
431:42 - and in fact that turns out to be the
431:44 - case because we compute the probability
431:46 - of b
431:47 - given a and that's going to be the
431:50 - probability
431:51 - of a and b divided by the probability
431:55 - of a and we then
431:58 - say that the numerator is equal to the
432:00 - probability of
432:01 - a given b uh
432:04 - times the probability of b
432:08 - uh divided by the probability of a by
432:10 - the way
432:11 - uh if you're not recognizing this part
432:13 - uh recalling back to the previous
432:15 - lecture this is
432:16 - a bayes theorem prototype
432:23 - that all right that's some bad bad
432:25 - handwriting up there i cannot
432:27 - i cannot let that go i know that in some
432:28 - of these videos my handwriting isn't
432:30 - great but i cannot let that go
432:35 - so this is a bayes theorem prototype in
432:38 - that you're
432:38 - pretty much one step away from getting
432:40 - base
432:42 - theorem all you would need to do at this
432:44 - point
432:46 - is apply uh the law of total probability
432:49 - to the denominator
432:50 - uh down here in order to get bayes
432:52 - theorem but anyway
432:54 - that aside all right that was a a
432:56 - distraction
432:57 - um we now say that this is the
432:59 - probability of a given b
433:01 - times the probability of b uh divided by
433:03 - the probability of a
433:04 - but we now know that the probability of
433:07 - a given b
433:08 - because a is independent of b that's
433:10 - going to be the probability of
433:11 - a so this is the probability of a times
433:14 - the probability of b
433:17 - uh divided by the probability of b you
433:19 - can a probability of a
433:20 - sorry and you can probably see why we're
433:23 - assuming that these events don't have
433:24 - zero probability because we could end up
433:26 - with division by zero
433:28 - so uh we're going to cancel those
433:31 - out because they appear in the numerator
433:33 - and denominator and we get the
433:35 - probability of
433:36 - b in the end okay
433:39 - all right so that shows what we wanted
433:41 - to want to show so
433:43 - um this implies that
433:47 - if a is uh if a is independent
433:50 - of b
433:54 - so if a is independent of b uh
433:57 - that up that's going to imply that b is
434:00 - independent of a
434:04 - and vice versa all right uh it seems
434:07 - that
434:08 - it would seem to be the case that if a
434:11 - is independent of b
434:12 - and so um heuristically that knowing
434:16 - whether b
434:16 - happened gives you no information about
434:18 - whether a happened it should also be the
434:20 - true
434:21 - for the complement of a if you if
434:24 - um so knowing that b happened should not
434:27 - tell you whether
434:28 - a didn't happen either so
434:31 - this does in fact turn out to be the
434:33 - case because we compute the probability
434:35 - of
434:35 - a complement given b and that's going to
434:37 - be the probability
434:38 - of a complement given b uh
434:42 - we know from uh the previous section
434:45 - this is 1 minus the probability
434:47 - of a given b and the probability of a
434:50 - given b
434:52 - is going to be the probability of a so
434:54 - this is 1 minus the probability of a
434:56 - which we know from section 2 is equal to
434:59 - the probability
435:00 - of a complement so
435:04 - that means that the complement of a is
435:06 - also independent of b
435:10 - so there is an immediate consequence
435:14 - of this definition which is that the
435:16 - probability
435:18 - of a intersected with b is equal to
435:22 - the probability of a times the
435:25 - probability
435:26 - of b you can see that because you could
435:30 - uh potentially say that uh
435:34 - like you would start out before saying
435:35 - the probability of a that this is the
435:36 - probability of a given b
435:39 - but since a is independent of b that's
435:41 - just going to be the probability of a
435:43 - so that means that the probability of
435:45 - intersections turns into the product of
435:47 - probabilities
435:48 - and not only is this a consequence of
435:52 - how i've defined
435:54 - the definition oh i misspelled
435:56 - definition
435:58 - not only is this a consequence of how
436:01 - i uh defined independence
436:04 - independence of events this is
436:08 - actually taken in later courses in
436:10 - higher level probability courses
436:13 - or courses devoted to probability as the
436:15 - definition
436:16 - of independence because the two are
436:19 - essentially equivalent and furthermore
436:24 - what i've highlighted in blue here it's
436:27 - used
436:27 - even more frequently and
436:33 - and also
436:36 - uh it it's uh you have this issue of
436:40 - zero probability events
436:41 - and we don't have that issue here
436:43 - because there's no
436:44 - division taking place okay uh
436:48 - students often want to
436:51 - like i've been encouraging students to
436:52 - think about uh
436:54 - probabilities and and and uh
436:57 - sets and events in terms of venn
437:00 - diagrams
437:02 - and uh is it possible to get a venn
437:07 - diagram
437:08 - uh graphically representing independence
437:11 - it is although it's kind of hard and
437:14 - honestly not
437:16 - that enlightening i'm going to show it
437:17 - to you anyway though
437:19 - first off this is not what independence
437:22 - is
437:23 - this is not a picture of independence we
437:26 - have
437:26 - a we have b and we have our sample space
437:28 - that is not independence
437:30 - this is disjointedness and two events
437:34 - that are disjoint
437:35 - and have nonzero probability are
437:38 - certainly not
437:39 - independent of each other because if you
437:41 - knew that one event
437:42 - one of those two events happened you
437:43 - know that the other one didn't
437:45 - so disjointedness is not the same thing
437:48 - in fact it's almost the opposite thing
437:50 - or implies non-independence
437:54 - so disjointness is not independence it's
437:56 - almost the opposite
437:58 - okay so uh i'm promising you to
438:01 - make a to show what independence kind of
438:03 - looks like
438:04 - as a venn diagram i can kind of torture
438:07 - a venn diagram
438:09 - so what i do is um i'm gonna need to
438:12 - zoom in a lot for this
438:16 - so to attempt
438:20 - to draw uh two sets that are independent
438:24 - of each other
438:25 - i uh draw the sample space as a square
438:30 - i divide it up into fourths
438:35 - and then i'm going to give the set
438:39 - a the upper left corner nothing
438:42 - special about that particular corner it
438:44 - just needs to be a corner
438:47 - and then i give the event b
438:51 - the middle quadrant it should be the
438:54 - same
438:54 - area roughly as
438:57 - a it's just taking up the middle area
439:01 - and
439:04 - the area inside of a
439:08 - and b relative to b is same is the same
439:11 - as a's
439:12 - area relative to the entire sample space
439:16 - so or or that is uh a
439:19 - is take a's area is a quarter of the
439:21 - sample space
439:22 - and a's area and b or a's intersection
439:25 - and b
439:26 - is a is a quarter of b
439:29 - so knowing that you fell into
439:32 - that so basically a corresponds to the
439:35 - probability of falling into the upper
439:36 - left hand corner
439:38 - quadrant and knowing that you are in
439:41 - this middle square does not really tell
439:43 - you whether you fell
439:44 - into that quadrant or not um that gives
439:47 - you basically
439:48 - no information on that so this is a
439:50 - sketch of what
439:51 - independent events might look like if
439:53 - you want to draw them as a venn diagram
439:55 - if this sketch does not make sense to
439:57 - you then don't worry about it just leave
439:59 - it alone
440:01 - because you know it's it's very much a
440:04 - tortured example
440:05 - you have to kind of work hard to get
440:06 - something that looks like this
440:08 - and i'm not super convinced that it's
440:10 - necessarily all that enlightening
440:12 - as to what independence means so if it
440:15 - doesn't make sense to you then just move
440:16 - on don't worry about it
440:18 - uh okay so the next example
440:21 - we're going to consider rolling a
440:22 - six-sided die and i'm going to show that
440:24 - the event a
440:25 - which is that the number uh of pips does
440:28 - not exceed
440:29 - four and the event b that the number of
440:31 - pips is even
440:32 - are independent events so let's uh go
440:35 - ahead and
440:36 - enumerate what falls into uh
440:39 - these two uh separate events we have
440:43 - the event a well not separate just
440:46 - different
440:47 - so the event a uh the number does not
440:51 - exceed
440:52 - four so that includes uh
440:55 - one pip two pips
440:58 - three pips and four pips
441:04 - all right uh the event b which is that
441:08 - you get an even number of pips is going
441:10 - to be
441:11 - two pips four pips
441:16 - uh come on i want the fourth pip
441:23 - i tried and six pips
441:31 - ugh not always cooperative all right
441:34 - um so let's compute the probability of a
441:38 - given b to show that two events are
441:40 - independent it is sufficient
441:41 - to just compute the probability of
441:45 - a given b and show that that equals the
441:47 - probability of a
441:49 - so the probability of a given b is going
441:50 - to be the probability of a
441:52 - and b divided by the probability of b
441:58 - all of these outcomes are equally likely
442:01 - so the intersection of the two
442:05 - sets is going to consist of two
442:08 - and four those are the two things the
442:11 - two sets
442:12 - have in common so this is going to be
442:16 - the dice with face two and the dice
442:20 - with face four
442:28 - why is it so uncooperative
442:32 - okay there we go uh probably two
442:35 - probably four
442:36 - and um divided by uh
442:40 - the probability of b okay and at this
442:44 - point we can basically just count how
442:45 - many elements are in these sets
442:46 - so uh for the intersection there's two
442:49 - outcomes in that set uh and uh for the
442:53 - probability of b
442:55 - there's three so that's going to be
442:56 - three over six so
442:58 - we end up with uh the six is cancelling
443:01 - out and the probability
443:03 - of a given b is equal to two-thirds
443:06 - as a result but the probability of a
443:10 - there are four outcomes in a uh divided
443:14 - by
443:15 - the size of the sample space which is
443:16 - six which is also two-thirds
443:19 - those two numbers are the same therefore
443:22 - these two events
443:23 - are independent of each other
443:27 - okay next up we suppose we have events
443:30 - a1 through a n
443:32 - these sets are said to be mutually
443:35 - independent if
443:36 - and i know that this definition is
443:38 - rather complicated
443:39 - for k less than or equal to n the
443:41 - probability of any
443:42 - subset of that collection of events will
443:45 - become the product of their individual
443:46 - probabilities
443:48 - so it is not sufficient to just
443:51 - check that for all of these events a1
443:53 - through a n
443:54 - uh the probability of the intersection
443:56 - is the probability of probabilities it
443:57 - has to be true
443:58 - for not just the entire collection of
444:01 - events
444:01 - but any subcollection of that collection
444:04 - of events
444:05 - it they all must turn into product of
444:08 - probabilities
444:09 - if this is not true then they are not
444:11 - said to be mutually independent of each
444:12 - other
444:13 - um and here's kind of the reason why um
444:16 - when we were saying mutually independent
444:18 - it what we really kind of want to say
444:20 - is that uh none of these events give
444:24 - any information about any of the other
444:25 - events
444:27 - and so that means it you would want to
444:31 - say that you take any two
444:33 - events in this collection of events
444:36 - and they will be independent of each
444:38 - other you want to be able to say that
444:41 - but it is not sufficient to just check
444:44 - that the entire intersection the
444:46 - probably the entire
444:47 - intersection turns into the product of
444:48 - the respective probabilities
444:50 - and here is an example that shows uh
444:53 - why that is not in fact true um
444:58 - so uh this is an example
445:02 - from uh from actually
445:05 - uh this paper that i've highlighted in
445:07 - red
445:09 - use the diagram below for finding
445:12 - probabilities compute the probability of
445:14 - a and b and c that actually is just
445:16 - going to be
445:17 - that tiny little sliver so probably of a
445:20 - and b and c
445:22 - is equal to 0.04
445:26 - and then we're going to find the product
445:30 - of the of the probabilities of the
445:31 - events a b excuse me
445:33 - and c uh so the probability of
445:36 - a so that's going to be the probability
445:40 - of the blue circle which is going to be
445:43 - 0.1 plus 0.06 plus 0.04 which is 0.2
445:48 - so this will be 0.2
445:51 - the probability of b
445:54 - is going to be the green circle
445:58 - so that's going to be 0.06 plus 0.04
446:01 - that's 0.1
446:02 - plus another 0.1 is 0.2 plus another 0.2
446:06 - is 0.4 so this will be
446:08 - 0.4 and
446:12 - finally we've got the probability
446:15 - of c which is going to be
446:19 - this blue circle here so we've got
446:23 - uh 0.16 plus 0.04 is 0.2 plus another
446:26 - 0.2 is 0.4 plus 0.1 will be 0.5
446:30 - so uh this probability the probability
446:34 - of c is equal to 0.5
446:38 - so uh the probability of a
446:41 - times the probability of b times the
446:45 - probability
446:45 - of c is going to be
446:49 - 0.2 times 0.5 which is 0.1
446:53 - times 0.4 which will be 0.04
446:56 - and those two numbers are equal to each
446:59 - other
447:00 - so it's tempting to say uh that
447:03 - these events are independent of each
447:05 - other but then
447:07 - i ask you to compute the probability of
447:10 - a uh
447:14 - intersected with b
447:18 - so the probability of a intersected with
447:20 - b corresponds to this blue region that
447:22 - i've highlighted
447:23 - which is going to be 0.1 so that equals
447:26 - 0.1
447:28 - but that does not equal
447:32 - the probability of a times the
447:35 - probability of b
447:38 - which is equal to 0.08 and you might
447:41 - and before you say well those numbers
447:43 - are close close doesn't mean a thing
447:45 - i don't care about close they need to be
447:47 - the same
447:48 - so uh yeah they are they are not
447:52 - independent events
447:54 - and as a result these events a b and c
447:57 - are uh not mutually independent
448:00 - could you say that there is some other
448:03 - notion of independence that you could
448:04 - apply
448:06 - i don't really know and i don't really
448:07 - care because i have seen
448:09 - notions maybe like pairwise independence
448:13 - as an alternative notion of independence
448:15 - and i've never
448:16 - seen it ever used in my own
448:20 - life or work it doesn't really seem like
448:23 - it leads to any sort of useful uh
448:26 - useful concepts so yeah
448:30 - that's that's that's that
448:36 - all right uh next example
448:44 - this example is supposed to motivate uh
448:47 - maybe you remember when i was uh talking
448:49 - about flipping a coin take it has this
448:51 - example supposed to motivate uh
448:54 - our assignment of probabilities in that
448:57 - situation
448:59 - so we're going to flip uh remove the
449:01 - eight part that
449:03 - i don't know why that was there i don't
449:05 - know why i have uh
449:06 - eight fair coins so flip fair coins
449:09 - until we get heads
449:20 - and furthermore each flip is independent
449:32 - okay so what is the probability of heads
449:34 - tails heads tail sales heads
449:36 - tails those stills heads uh what in
449:38 - general what would be the probability of
449:40 - a sequence of flips
449:41 - a sequence of n flips to have n minus
449:43 - one tails
449:44 - and a heads at the end so the
449:47 - probability
449:48 - of heads if this is a fair coin is going
449:50 - to be
449:51 - uh one half uh the probability of tails
449:55 - heads
449:57 - and this is by the way getting a little
449:59 - abusive with notation
450:01 - but the but the probability of tails
450:03 - heads is gonna be the probability of
450:05 - tails
450:06 - times the probability of heads since
450:09 - those two flips were independent of each
450:11 - other
450:18 - we are now on to chapter three
450:21 - on discrete random variables and
450:23 - probability distributions
450:25 - this chapter serves both as
450:28 - an introduction to
450:31 - random variables and also an
450:34 - introduction to discrete random
450:36 - variables in particular
450:37 - but many of the concepts that we see
450:40 - here
450:41 - are going to transform is are going to
450:43 - transfer over
450:45 - to the continuous context when we're
450:47 - dealing with continuous random variables
450:50 - so let's get started
450:54 - so in this section we're going to talk
450:57 - about
450:58 - some random variables a random variable
451:01 - which is sometimes abbreviated with the
451:02 - letters rb is a function
451:05 - taking values from the sample space s
451:07 - and associating numbers with them
451:09 - conventional notation for random
451:11 - variables uses capital letters
451:13 - from the end of the english alphabet
451:14 - with lowercase letters
451:16 - while lowercase letters are usually used
451:19 - to denote
451:20 - a non-random value or outcome so
451:23 - up to this point we have been using
451:25 - lowercase letters
451:26 - for uh data and when talking about
451:30 - random variables we're going to switch
451:31 - to uppercase
451:33 - variables or uppercase letters
451:36 - and the distinction does somewhat matter
451:39 - um
451:40 - honestly this is a rule that is very
451:41 - frequently broken although at the same
451:44 - while i say that in this class i'm not
451:47 - planning on breaking that rule all that
451:49 - much uh
451:51 - yeah i mean i i break it all the time in
451:53 - in my
451:54 - research work but this is a situation
451:57 - where
451:58 - it's probably going i'm probably going
452:01 - to stay rather true
452:02 - to it so there's a difference between a
452:05 - random variable whose outcome is unknown
452:08 - and a possible value that random
452:10 - variable could take
452:12 - so uh using the fact that random
452:15 - variables are actually functions like
452:17 - the term random variable is somewhat
452:19 - there's somewhat of a joke
452:21 - that random variables are neither random
452:22 - nor variables because they are
452:25 - uh because random variables are not
452:28 - really variables they're
452:29 - treated as functions and they're not
452:32 - random because if you know what outcome
452:34 - from the sample space you got you know
452:35 - the value of the random variable
452:37 - because the randomness comes not from
452:40 - the random variable x itself but rather
452:42 - from omega which is an
452:43 - outcome from the sample space so
452:47 - when uh so if omega is an outcome from
452:50 - the sample space the notation x of omega
452:52 - equals little x can be used to say
452:54 - the value of the random variable x when
452:56 - the when the outcome
452:57 - little omega occurs is little x
453:01 - so little omega that is a random outcome
453:05 - uh little x is uh non-random
453:09 - and x will equal little x when the
453:12 - random outcome omega occurs
453:21 - all right so the set of omega
453:25 - such that x of omega is equal to x is
453:28 - the event
453:29 - that an element of s is drawn that
453:31 - causes the random variable variable
453:33 - x to equal little x
453:38 - and the set of omega such that x of
453:40 - omega is
453:41 - in some other set a where that other set
453:44 - is often some such a subset of the real
453:47 - numbers
453:48 - is the event that an element of s is
453:50 - drawn that causes the random variable x
453:52 - to assume a value that is in the set
453:55 - a so technically when we want to talk
453:59 - about
454:00 - whether a random variable is in a it
454:03 - takes a value inside of a set or not
454:06 - this is the notation this is what we
454:09 - should be using
454:10 - for that notation we are asking for the
454:13 - probability
454:14 - that we draw an omega that causes x of
454:16 - omega to be in the set a
454:19 - but that is often rather tedious we
454:22 - would rather just say
454:23 - what is the probability that x is in a
454:26 - that's much simpler
454:29 - random variables are commonly classified
454:31 - as being either discrete or continuous
454:33 - discrete random variables or discrete
454:35 - real valued random variables take values
454:37 - in a finite or countably infinite or
454:39 - innumerable if you prefer
454:41 - a set with positive probability so
454:44 - examples of sets that random discrete
454:47 - random variables
454:48 - uh take their values in could be a
454:51 - finite set of numbers
454:53 - such as zero or one it could be
454:56 - the
455:00 - the integers it could be uh the natural
455:02 - numbers such as 0 1 2 3
455:04 - 4 5 6 and so on if it's innumerable
455:08 - then it could potentially be a discrete
455:10 - random variable
455:12 - if the set that it falls into is all any
455:15 - number between zero and one including
455:17 - fractions and including
455:19 - um uh rational numbers and irrational
455:21 - numbers any of those numbers then it's
455:23 - no longer discrete it's going to be
455:24 - considered continuous
455:26 - uh on the other hand continuous real
455:28 - value variables satisfy the following
455:31 - two properties
455:32 - first the random variables take values
455:34 - and intervals which are possibly
455:36 - infinite in length
455:37 - or disjoint unions of intervals of the
455:39 - real line with positive probability
455:42 - that's the first property that
455:45 - continuous random variable satisfied the
455:46 - second is that
455:48 - for any constant c and r the probability
455:50 - that that random variable is equal to c
455:52 - is equal to zero which is kind of a
455:54 - strange assumption you're saying
455:56 - that you know that this random variable
455:59 - this continuous random variable is going
456:01 - to equal
456:01 - some real number but you're saying the
456:03 - probability that it equals any real
456:04 - number is equal to zero
456:06 - well for starters that must be the case
456:10 - because uh this would be this is an
456:13 - infinite set so you need to have
456:15 - some way to assign probabilities
456:18 - and there's too many numbers in the real
456:20 - number system
456:21 - for uh numbers in general to have
456:23 - positive probability
456:25 - and secondly uh
456:28 - there's a way that my probability
456:31 - instructor put it
456:32 - he said if you
456:36 - like you know that someone's going to
456:38 - win the lottery
456:39 - you just know it's not going to be you
456:41 - so
456:43 - by that same logic you know that these
456:46 - random variables will fall inside of an
456:47 - interval you just don't know what number
456:49 - it will be you will never know what
456:50 - number it will be
456:51 - and uh it's highly highly highly
456:53 - unlikely that it will be
456:55 - that particular number you chose so
456:57 - unlikely
456:58 - that that probability is effectively
457:00 - zero uh perhaps the simplest non-trivial
457:03 - random variable
457:04 - is the bernoulli random variable if x is
457:07 - a bernoulli random variable then the
457:08 - probability that x equals one is equal
457:09 - to one minus probably that x equals zero
457:12 - which is p and we say uh
457:15 - that x follows a bernoulli distribution
457:18 - with parameter p
457:19 - that's that's what this we're saying so
457:23 - we have the random variable x and this
457:26 - notation means that it comes from
457:28 - some distribution with some parameter
457:31 - uh we're gonna leave that um alone
457:34 - that that uh verbiage about
457:37 - distributions alone for a minute that's
457:38 - going to be the subject
457:40 - of the next video
457:43 - uh so but basically we just say that x
457:45 - is a bernoulli random variable because
457:47 - it has these properties
457:49 - all right so the set s on which x of
457:51 - omega is defined could be
457:53 - really just about anything uh
457:56 - it's natural to think of bernoulli
457:59 - random variables
458:00 - as being equivalent to coin flips or
458:03 - possibly biased coin flips
458:05 - the thing though is you do not have to
458:09 - necessarily have coin flips for example
458:11 - you could have a probability space
458:13 - where there is a coin flip where if you
458:16 - draw
458:16 - heads this random variable will evaluate
458:19 - to the number one
458:21 - and if you draw tails or you flip and it
458:24 - lands tails up
458:25 - it will evaluate to zero
458:29 - that's one possibility to get a
458:32 - fair or balanced bernoulli random
458:35 - variable where the probability of
458:36 - getting one is 0.5
458:39 - alternatively you could roll a fair die
458:42 - track whether the result of the die was
458:45 - even or odd and in the event of an even
458:48 - number of pips
458:49 - this random variable evaluates to one
458:52 - and the event of an
458:53 - odd number of pips this random variable
458:54 - evaluates to zero
458:57 - either one of those situations could be
458:59 - the case
459:01 - and it is statistically impossible to
459:03 - differentiate
459:04 - between these two possible setups
459:08 - for of random variables and
459:11 - the and and sample spaces
459:15 - so in that case it's almost
459:18 - as if once you know the distribution of
459:21 - the random variable
459:22 - you can pretty much forget whatever the
459:26 - original sample space was and any of the
459:29 - properties of that sample space
459:31 - you can now work in some you can work as
459:35 - if this
459:36 - random variable were the identity
459:37 - function and its sample space
459:40 - is going to be the real numbers or
459:42 - something uh maybe the number zero
459:44 - one so uh in that sense
459:47 - you almost forget all of that stuff that
459:49 - we talked about before
459:51 - um i mean i okay i guess you don't
459:53 - forget about it but you no longer care
459:54 - about the specifics of the sample space
459:57 - and the elements that you're drawing
459:58 - from the sample space the specifics no
460:00 - longer matter
460:02 - so we we get to talk about these things
460:05 - in a very general way all right so for
460:08 - the first example which of the following
460:09 - random variables are likely to be
460:11 - continu
460:11 - considered discrete and which are likely
460:13 - to be considered considered continuous
460:15 - describe the space of outcomes the
460:17 - random variable takes with positive
460:19 - probability
460:20 - so for first case flip a coin record one
460:22 - for heads and zero for tails
460:24 - if this is a situation the sample space
460:28 - for experiment would probably be the
460:30 - sample space consisting of heads and
460:32 - tails
460:34 - and the random variable x when given the
460:38 - outcome
460:38 - h is equal to is equal to one
460:42 - and the random variable x when taken
460:45 - the value tails is going to equal
460:48 - zero so the space of outcomes which i
460:52 - will
460:53 - denote by x of s where you almost feed
460:57 - the entire sample space into this
460:58 - function
460:59 - x is going to be the set
461:02 - uh 0 to 1. by the way the term for
461:06 - uh this is this is the image
461:10 - of the sample space s under the under
461:13 - the random variable x or the function x
461:18 - all right uh this random variable since
461:22 - the space in which its outcomes falls is
461:25 - generally
461:26 - like that's a finite space there's only
461:27 - two numbers in it so this random
461:29 - variable would be considered discrete
461:37 - okay uh next example roll a die
461:41 - record the number of pips showing so the
461:44 - sample space in this situation would be
461:48 - uh die rolls so we got one
461:52 - uh two
461:55 - three
462:00 - four
462:03 - five
462:08 - and six all right
462:16 - so that's the sample space uh x
462:20 - of one would be equal to
462:24 - one x of
462:28 - uh two or two pips would be equal to two
462:33 - and you'd kind of keep going on like
462:35 - this for uh
462:36 - other possibilities so you could say x
462:38 - when you have
462:40 - six pips on your face
462:43 - would be equal to six
462:48 - so this is the reason why we didn't want
462:51 - to write down the numbers one through
462:52 - six
462:53 - when talking about die rolls it makes it
462:56 - easier then to talk about random
462:58 - variables as being a translation
463:00 - from the number of pips showing on a
463:02 - dice face
463:03 - to numbers actual numbers
463:07 - and we would like to be able to make
463:08 - that translation random variables do not
463:10 - need to be defined
463:12 - on numeric spaces it doesn't have it
463:14 - they basically say
463:15 - nothing about whatever's in that
463:18 - original space
463:19 - they don't care at all what's in that
463:21 - original space
463:22 - and once you have random variable you
463:23 - get to work in the real numbers and
463:26 - that's really nice
463:27 - so the image of the sample space under x
463:30 - is going to be the numbers 1 2
463:33 - 3 4 5 6.
463:40 - and that suggests since since this is a
463:43 - finite set
463:44 - that this is a discrete random variable
463:50 - all right next example roll a die record
463:54 - one for an even number of pips and
463:55 - negative one for uh
463:57 - for an odd number of pips okay
464:00 - so the sample space is the same as
464:02 - before so we're gonna copy copy that
464:04 - sample space down
464:05 - so x of one
464:09 - is equal to x of three
464:15 - which is equal to x of five
464:22 - in all of these cases x is going to
464:25 - come out as negative ones and to have an
464:27 - odd number of pips
464:29 - uh in the case of two uh
464:36 - uh that's so x evaluated for the dice
464:39 - with two pips
464:41 - is going to be the same as when there's
464:43 - four pips which is going to be the same
464:45 - as when there's uh six pips
464:53 - in all of these situations the random
464:56 - variable x comes out as
464:57 - one so the image of the sample space
465:01 - under this random variable will be
465:04 - negative one and one and again this is a
465:07 - discrete random variable
465:16 - okay uh the time in minutes needed to
465:20 - complete a race uh
465:23 - it seems appropriate to say here that
465:25 - the sample space is going to be
465:29 - some uh positive real number which we'll
465:31 - put with the
465:32 - r plus which will be which is equal to
465:36 - the set
465:36 - uh zero to infinity including zero
465:40 - and in this case so
465:43 - it seems like the the the random
465:46 - variable is going to be the identity
465:47 - function where it takes one of the
465:48 - numbers from the space and just spits
465:49 - out that exact same number
465:51 - so in this case x of
465:55 - uh omega is equal to omega which is in
466:00 - uh the which is in the uh
466:05 - posit or non-negative real numbers so
466:08 - that means that
466:09 - the image of the sample space
466:12 - under x is going to be the positive real
466:14 - numbers
466:15 - or the non-negative real numbers and
466:17 - this is a continuous space
466:20 - so this is going to be a continuous
466:22 - random variable
466:24 - and basically what we're saying is uh
466:27 - the time it takes to
466:28 - complete the race can be any number from
466:32 - 0 to infinity
466:33 - uh not just integers but also including
466:36 - fractions and algebraic numbers and
466:39 - transcendental numbers every single
466:41 - possible
466:42 - real number and since the real numbers
466:43 - are an uncountable space
466:45 - that means that this random variable is
466:47 - going to be continuous
466:49 - uh for similar reasons number five the
466:52 - length in centimeters of a hair plucked
466:53 - from a person's head
466:54 - uh we could say um that the sample space
466:59 - is going to be uh hairs
467:02 - no no no no not x sorry
467:05 - the sample space will consist of hairs
467:15 - and the a random variable x
467:18 - from that sample space which i'm going
467:21 - to draw
467:22 - a hair okay that's the thing
467:25 - all right uh so x when given a hair
467:28 - uh gives you uh a number
467:33 - so the length of the hair
467:41 - and so it suggests that the
467:44 - image of the sample space under x is
467:47 - going to be the positive real numbers
467:49 - or the non-negative real numbers again
467:52 - since hairs can be in principle of any
467:53 - length and it's an uncountable length
467:56 - so i mean idea all right it's it's going
467:59 - to be true
467:59 - that there's a finite number of hairs in
468:02 - the world and therefore a finite number
468:03 - of hair lengths
468:05 - so if we were being super super duper
468:09 - technical
468:09 - we would say actually this is a discrete
468:11 - random variable because there's only a
468:12 - finite number of possible hair lengths
468:14 - since there's only a finite number of
468:16 - pairs but that seems ridiculous that
468:17 - seems like a ridiculous model that seems
468:19 - like way too much complication
468:21 - continuous random variables are
468:22 - continuous because it's easier to work
468:24 - with continuous things than discrete
468:25 - things
468:26 - and you're probably going to agree with
468:27 - that when we start
468:29 - doing all the work for the discrete
468:31 - random variables and all the work for
468:32 - the continuous random variables and see
468:34 - oh it's actually not
468:35 - it's actually somewhat easier to work
468:37 - with continuous stuff
468:38 - so uh we're going to say that it's the
468:40 - real numbers in which case this is
468:42 - a continuous random variable
468:50 - okay uh next up roll two dice record the
468:54 - sum of the number of pep showing
468:56 - uh i'm not gonna bother writing out the
468:57 - sample space but i'm going to say that
468:59 - uh x could be like say if you gave x the
469:03 - numbers
469:03 - uh one and four it's going to add up the
469:07 - pips
469:08 - showing on those two dice and give you
469:10 - the result five
469:11 - so the image of x under the sample space
469:14 - is going to be the numbers
469:16 - from 2 to 12.
469:20 - and since this is a finite set this is
469:23 - going to be
469:28 - discrete okay next up
469:31 - flip a coin until h is seen and count
469:32 - the number of flips so the sample space
469:35 - consists of heads tails heads
469:39 - tails tails heads uh and so on
469:44 - so the si so the random variable x when
469:46 - given one of these uh strings let's say
469:48 - tails tails heads
469:50 - uh is going to evaluate to this
469:53 - to the length of the string which means
469:56 - that x when fed this sample space its
470:00 - image is going to be
470:01 - the natural numbers which are the
470:03 - numbers uh
470:05 - 1 2 3 and so on
470:08 - and this is a discrete space because the
470:12 - because there's an innumerable amount
470:15 - of numbers in this space and since it's
470:18 - innumerable
470:20 - that means it's going to be discrete so
470:22 - this is a discrete sample this is a
470:23 - discrete random variable
470:30 - by the way if you're not familiar with
470:31 - the word innumerable that means
470:33 - listable as then you can list it out and
470:35 - even though it will take you
470:37 - an infinite number of years to list out
470:41 - everything in this sample space or in
470:43 - this uh
470:44 - set there will you will hit everything
470:48 - in that set a in finite time
470:52 - so every possibility the same cannot be
470:55 - said for the real numbers by the way
470:57 - if even if given infinite number of
470:59 - years you will not hit every value
471:01 - if you started trying to list them out
471:04 - so
471:04 - that's actually a very deep result in
471:07 - set theory
471:08 - or i don't know about i don't know if
471:10 - deep is quite the right word for it but
471:11 - it's certainly an important result
471:13 - so uh we're just going to take it for
471:16 - granted here that that is the case
471:18 - example two consider an experiment of
471:20 - rolling two six-sided die
471:22 - define two random variables for this
471:24 - experiment are they continuous or
471:25 - discrete
471:26 - we can define multiple random variables
471:28 - for the same sample space
471:29 - and there are advantages to doing so
471:31 - because when doing so we can talk about
471:33 - notions such as correlation
471:35 - or study of the behavior different
471:36 - random variables in the same space
471:40 - consider different possibilities i'll
471:42 - talk about their joint distribution
471:44 - together
471:45 - stuff like that so as a reminder the
471:48 - sample space consists
471:50 - of dice faces so we could have for
471:52 - example one one
471:54 - uh one two and we would keep going on
471:58 - like this
471:59 - you've seen in previous videos how to do
472:01 - this until eventually we listed out
472:04 - 6 6.
472:10 - okay so this is our sample space what's
472:13 - one random variable we could define we
472:14 - could say that
472:15 - x the random variable x when given some
472:18 - outcome omega in the sample space
472:20 - is equal to the sum of the pips
472:28 - whereas if we gave uh no let's not call
472:31 - the other random variable x we'll call
472:32 - it y
472:34 - that y will be another random variable
472:35 - defined on this space and it will be the
472:37 - max
472:39 - of the pips
472:43 - so for example uh x
472:46 - of the dice face
472:49 - of the combination one and two
472:52 - will equal three because that's the sum
472:55 - whereas y
472:57 - of one and two
473:00 - will equal two because the maximum of
473:03 - the two
473:04 - of the number of pips is going to be two
473:07 - okay so that's it for this section this
473:09 - was a basic introduction
473:11 - into what
473:31 - one uh we are on to the next section in
473:33 - our chapter on
473:35 - uh discrete random variables
473:38 - and at this point you're probably
473:39 - thinking okay we got these things called
473:41 - discrete random variables
473:43 - uh all right what's the point why why do
473:46 - we have these things
473:47 - random variables what why do why does
473:49 - anyone care about this
473:50 - this seems like an extra complication
473:54 - on uh this uh on these probability
473:57 - spaces we we were still
473:59 - able to talk about probability we
474:00 - haven't really
474:02 - added really anything so far well
474:05 - random variables truly are and
474:09 - and an addition that makes things much
474:11 - better and allows you to say
474:13 - additional things about uh
474:16 - randomness uh once you have random
474:19 - variables you are now
474:20 - allowing for um
474:24 - concepts such as um
474:28 - uh you're allowing for concepts such as
474:32 - we have uh we we have a
474:36 - phenomena that is essentially uh
474:40 - like phenomena that is essentially the
474:42 - same the same except for a couple
474:44 - parameters
474:45 - like there's a few parameters that we
474:48 - need to figure out and once we know
474:50 - those parameters we basically know
474:52 - everything there is to know about this
474:54 - phenomena or
474:55 - there's concepts such as uh expectation
474:58 - or
474:58 - mean uh you in order to be able to talk
475:02 - about expected values you need to have
475:03 - random variables
475:04 - so once we've introduced random
475:07 - variables
475:09 - uh these things that take inc
475:12 - that take uh
475:15 - things in our sample space and turn it
475:17 - into numbers you now gain a lot of
475:19 - additional structure
475:21 - the first thing that we get that is an
475:23 - essential property
475:25 - of uh random variables is a probability
475:28 - distribution so a probability
475:29 - distribution
475:30 - for a random variable is a function that
475:33 - describes the probability that a random
475:35 - variable takes on certain values
475:37 - discrete random variables are determined
475:38 - completely by
475:40 - the probability mass function which is
475:42 - abbreviated
475:43 - pmf and the probably mass function
475:47 - for a random variable is p of x
475:52 - which is equal to the probability that
475:54 - that the random variable x
475:56 - capital x so remember that capital x is
475:58 - referring to a random variable
476:00 - this is a thing that hasn't taken a
476:01 - value at yet or we don't
476:04 - view it as being equal to anything per
476:07 - se
476:07 - at this moment uh but we're asking for
476:10 - the probability that this random
476:12 - variable
476:12 - when we actually evaluate it and get a
476:14 - number out of it is going
476:16 - to equal x
476:19 - i like to think of random variables as
476:21 - uh they will have a future value
476:24 - but they don't really have a value right
476:27 - now when we're asking for their
476:28 - probability and stuff
476:31 - so we're asking what could this thing be
476:33 - in the future
476:35 - so uh a coin flip
476:38 - is random before you make the flip
476:41 - if that makes sense after you've made
476:45 - the flip
476:45 - then it's no longer random because you
476:47 - get to see whether it was heads or tails
476:49 - which actually is um
476:53 - getting more again into the issue of
476:56 - what does probability actually mean like
476:58 - for example
476:59 - uh let's suppose that we flip a coin
477:03 - and it lands heads up and then we cover
477:06 - it up we don't get to look at it
477:07 - we never saw what the coin did we
477:09 - immediately the moment it lands
477:11 - on the ground uh covered up in a box
477:15 - in principle uh we would say that that
477:18 - under this uh frequentest notion of
477:20 - probability
477:22 - we should say that this random variable
477:24 - has a value and is no longer
477:26 - random we just don't know what it is
477:28 - whereas
477:29 - if you're adopting maybe the gambler's
477:31 - notion of random
477:32 - of randomness and probability you might
477:35 - still
477:36 - view it as being random where you can
477:39 - start placing a bet on whether it's
477:40 - heads or tails when you take the
477:42 - lid off of the uh when you when you take
477:45 - the box off of the of the coin and then
477:46 - actually observe it
477:48 - uh i'm not gonna talk anymore about that
477:49 - i've already recorded a half hour video
477:52 - about the interpretation of probability
477:53 - probability
477:54 - and you can watch that if you want to
477:56 - learn more uh but
477:59 - uh all right so i'm just saying this
478:02 - because i feel like
478:03 - students especially with this notation
478:06 - this notation especially when you're
478:08 - starting out can
478:10 - bother students and they're wondering
478:12 - what capital x is and what little x is
478:14 - and what's the difference between them
478:15 - and the difference is this is random and
478:16 - we don't actually know what it is
478:18 - whereas little x is something that is
478:21 - fixed
478:22 - and we know what little x is so um
478:24 - really when i'm writing this down little
478:26 - x is going to be substituted for a
478:28 - number like for example there's going to
478:29 - be p
478:29 - of 1 which is going to be the
478:31 - probability that capital x is equal to
478:34 - 1. like at some point we're going to
478:37 - make that substitution
478:38 - um so basically the little x right here
478:43 - is going to get substituted with a
478:44 - number but the
478:46 - capital x is never going to get
478:48 - substituted and that's always going to
478:50 - be viewed right now as being random
478:52 - and we don't really know what its value
478:54 - is going to be and we're just
478:56 - studying uh what its value could
478:58 - possibly end up being and how likely it
479:00 - will end up
479:01 - taking certain values uh sorry if i'm
479:04 - going on too long about this
479:06 - this is just something that i know that
479:08 - students at some level struggle with
479:10 - and i try over and over again to try and
479:13 - explain it
479:14 - and i'm never fully satisfied with my
479:16 - own explanation
479:18 - okay um the probably mass function one
479:22 - way to visualize
479:23 - a probably mass function is using a line
479:25 - graph
479:27 - where a line is placed on each point x
479:29 - of r that x takes a positive probability
479:31 - extends to the height
479:32 - representing p of x um okay so before i
479:35 - draw a visualization i'm going to say
479:38 - that we are totally allowed
479:40 - to say all right we've got inputs x and
479:43 - outcomes p of x
479:45 - where p of x is a function that gives us
479:47 - the probability this random variable
479:48 - will equal x
479:49 - and we can construct a table if we
479:50 - wanted like for example 0
479:53 - 1 2 3 4. we could construct a table
479:56 - of possible inputs to this function and
479:59 - for possible outputs we could say
480:02 - uh let's see uh what says something we
480:04 - could do
480:05 - uh we'll just say that all of these are
480:07 - equally likely so all of these are
480:08 - one-fifths so this function puts out
480:10 - one-fifth all the
480:10 - all the time well yeah
480:14 - should we always do that uh we might say
480:17 - this is two-fifths
480:19 - and this is one-tenth and this is
480:21 - one-tenth there
480:23 - i think that's okay does this still add
480:25 - up to one this must add up to one by the
480:27 - way
480:27 - actually that's the thing we're going to
480:28 - talk about in the future let's see two
480:30 - one-fifth plus one-fifth is two-fifths
480:32 - plus two-fifths is four-fifths
480:33 - plus two-tenths is another fifth so that
480:36 - does in fact add up to one
480:39 - which probably mass functions if you add
480:41 - them up
480:42 - if you add up all of their non-zero
480:44 - values
480:45 - then they must always add up to one
480:47 - always
480:48 - probably mass functions always add up to
480:50 - one if they don't add up to
480:52 - one then they're wrong they're not
480:54 - probably mass functions
480:56 - if you ever compute a probably mass
480:58 - function and it doesn't add up to one
481:00 - then it's not a probably mass function
481:01 - and i don't care if it's close
481:04 - it closes close as nothing because it's
481:06 - not one
481:07 - one is one all right um
481:11 - okay and and i suppose we're allowed to
481:13 - say all right let's suppose i threw in
481:15 - a fifth value then the probably mass
481:17 - function will be zero
481:18 - and presumably anything that's on this
481:20 - table if i didn't list it out
481:22 - then the s function is zero right so
481:25 - anything not written down is zero all
481:28 - right
481:28 - uh but then we can visualize a
481:30 - probability mass function using a line
481:32 - graph
481:34 - so we've got possible x values that this
481:37 - thing could take
481:38 - and we've got it's probably mass
481:39 - function uh it's it's probability at
481:41 - those points
481:42 - so we could have x equals zero one two
481:46 - three four uh and then for those let's
481:50 - let's see i've i've already got
481:52 - a lot of what i need so uh we'll do
481:55 - one tenth two tenths three tenths
481:59 - four tenths so zero is going to be two
482:02 - tenths so that's about here
482:04 - one is going to be two tenths again and
482:06 - then we got four tenths
482:08 - and then one tenth and one tenth this is
482:10 - a visualization
482:12 - of the probably mass function that i
482:14 - wrote on the right hand side
482:16 - of the page so i generally am like
482:20 - drawing a dot
482:21 - at the probability and then drawing a
482:22 - line up
482:24 - uh to the probability of the random
482:26 - variable equaling that value
482:28 - or something like that
482:31 - so yeah uh that's uh how you can
482:34 - visualize it
482:35 - there's also probably histograms which
482:38 - are very similar to line
482:39 - to the line graphs and very similar to
482:40 - the histograms that i was discussing
482:43 - uh several lecture videos ago where you
482:46 - could
482:47 - instead of having these uh lines
482:51 - uh draw i like these the the
482:54 - the table that i have is a lot like the
482:57 - the relative frequencies
482:58 - that i was discussing when discussing
483:00 - histograms so what you would do is draw
483:02 - something similar to the relative
483:03 - frequency of those uh
483:05 - of uh those um uh possible
483:08 - values so we got 0 1 2
483:12 - 3 4 and we draw
483:16 - a bar that is
483:20 - uh centered on that point so we got uh
483:24 - uh so we've got something going up to
483:26 - two tenths
483:28 - and two tenths again and then
483:31 - four-tenths and then two one-tenths
483:37 - yeah so there we go uh so uh there's
483:40 - that
483:42 - uh this is another way to visualize the
483:43 - problem distribution if you
483:45 - if you prefer it um there's really no
483:48 - difference
483:49 - um in fact i would say that edward tufte
483:51 - would probably say that they are the
483:52 - same graph essentially it's just
483:54 - uh the one up here it should be
483:56 - preferred because it uses less ink
483:58 - okay uh continuing on uh let's see some
484:01 - examples
484:02 - uh a fair coin is flipped we define a
484:05 - random variable x
484:07 - uh when h occurs x evaluates to one and
484:10 - when t
484:11 - occurs x evaluates to 0 find the problem
484:14 - as function of
484:14 - x which is p of x visualize p of x
484:18 - through with a line graph and you're
484:20 - thinking
484:21 - how do we know what the probabilities
484:23 - are
484:24 - well we know because i said this is a
484:27 - fair coin
484:28 - and since i know that it's a fair coin i
484:30 - know that the probability of heads is
484:31 - one and probably a tails of zero
484:33 - okay so uh i'm going to start out
484:36 - actually with that tabular form
484:37 - so we've got x and p of x
484:41 - so we've got so possible values that x
484:44 - that the random variable x could take
484:48 - are one and zero so we're going to put
484:50 - zero
484:51 - and one as potential values for this
484:53 - random variable
484:54 - uh this random variable will equal zero
484:57 - if the coin comes tails up
484:58 - and since this is a fair coin the
485:00 - probability of getting tails so
485:02 - actually maybe i should be more verbose
485:03 - about this and say that the probability
485:06 - that x equals zero is
485:10 - equal to the probability of drawing an m
485:13 - and omega from the sample space an
485:15 - outcome from the sample space
485:17 - such that x when evaluated at that
485:20 - outcome equals zero i'm being very
485:23 - verbose about this
485:24 - uh what are such outcomes that causes us
485:27 - to evaluate to zero well we know from
485:28 - the definition of the problem
485:30 - that such outcomes are only tails
485:33 - so this is the probability that you get
485:35 - tails and the probability of tails
485:36 - because there's a fair coin is one half
485:40 - so that means that um x is
485:43 - so at z at zero the probably mass
485:46 - function will be one half
485:47 - and at one the probability mass function
485:49 - will also be one half because
485:51 - well a uh this random variable
485:55 - um well for starters this random
485:57 - variable is going to be one when the
485:58 - coin lands heads up and the probability
486:00 - of heads is
486:01 - one half that's one way to think about
486:04 - it
486:04 - and another thing to think about it is
486:07 - there's only two
486:08 - numbers that this random variable could
486:10 - take with positive probability
486:12 - zero and one zero
486:15 - it's probably mass function is one half
486:16 - so at one it must be whatever it takes
486:19 - to cause the probably mass function to
486:21 - add up to one so one minus one half
486:24 - meter is going to be one half
486:25 - and thus the other value is going to be
486:28 - one half
486:29 - uh so uh we we visualize this with a
486:31 - line graph
486:33 - uh we'll go ahead and make this one half
486:35 - uh what we end up having for
486:37 - our visualization of the probability
486:39 - mass function is we have lines
486:41 - uh extending up to uh
486:44 - one half all right and that's our
486:47 - visualization for it
486:51 - okay uh this by the way is a complete
486:54 - description
486:56 - of the problems function if we are
486:58 - willing to say that anywhere isn't
487:00 - anywhere we don't list uh the probably
487:02 - mass function
487:03 - evaluates to zero because like like if
487:07 - that should make complete sense to you
487:09 - because let's say what is going to be p
487:12 - at one half well that is the probability
487:16 - that x equals one half
487:20 - which equals the probability of drawing
487:23 - an outcome from the sample space
487:25 - that causes the random variable to
487:28 - evaluate
487:29 - to one half okay so we know
487:32 - that there are two outcomes in the
487:34 - sample space
487:35 - which are heads and tails
487:38 - and x is a random variable and therefore
487:41 - it is a function so
487:42 - you know that functions uh when given
487:46 - one input give you only one output only
487:48 - one output will come out
487:50 - so what does that mean here uh well
487:54 - we know what this function will be at
487:57 - heads which is one outcome in the sample
487:58 - space
487:59 - we know how this function will be at
488:00 - tails which is the other outcome in the
488:02 - sample space
488:03 - so what outcome causes this random
488:06 - variable
488:07 - to equal one-half because neither of
488:09 - those caused the function to evaluate at
488:11 - one-half
488:12 - so that means that the probability of
488:16 - drawing an outcome from the
488:18 - sample space that causes this to
488:19 - evaluate to one-half
488:21 - is the probability of the empty set
488:24 - because the the set of all numbers
488:28 - or not numbers the set of all outcomes
488:30 - in our experiment that causes this
488:32 - random variable to evaluate to one half
488:35 - is the empty set because there is no
488:37 - such outcome
488:39 - so you end up computing the probability
488:40 - of the empty set the probability then of
488:42 - the empty set is zero
488:45 - so that would mean that anything that
488:46 - isn't listed here
488:48 - uh it's natural to say that the problem
488:50 - s function is zero
488:52 - okay okay okay
488:56 - uh moving along
489:00 - moving along there is an
489:03 - r package called discrete rv
489:07 - and this package allows for
489:11 - users to define random variables and
489:13 - work with them
489:15 - and i think this package is
489:16 - pedagogically useful
489:18 - but for serious work with these random
489:22 - variables i wouldn't recommend using it
489:24 - um i uh i've uh i actually just made a
489:28 - few minutes ago
489:30 - well it wasn't a few minutes ago it was
489:32 - more like a few hours ago i just made a
489:34 - few hours ago
489:35 - uh lecture videos for the lab for r uh
489:38 - are on my our introduction on
489:40 - introductory videos um
489:42 - functions for dealing with probability
489:44 - in r and dealing with a lot of random
489:46 - variables and classes of random
489:47 - variables families random variables and
489:49 - i never use this package
489:51 - because it's more for allowing students
489:54 - a laboratory to work with
489:56 - random variables in the notation that
489:59 - we're using
489:59 - in the lecture class or a notation very
490:01 - similar to it
490:02 - and not actually meant for serious work
490:05 - trying to compute
490:07 - trying to work with the cdfs and the
490:08 - pdfs and expectability and all that
490:10 - stuff
490:10 - of these random variables but it's kind
490:13 - of nice
490:14 - so for example in this situation i could
490:17 - define the random variable x
490:19 - and say that this is a random variable
490:21 - with the rv function
490:22 - its possible values are zero and one
490:26 - and the probability of getting those
490:28 - outcomes are each one half
490:30 - and it will print and make a nice uh
490:33 - output
490:34 - a printout for that basically
490:35 - summarizing what i just said
490:37 - and in addition to that when i tell r to
490:39 - plot this random variable
490:41 - it creates the plot of the probability
490:44 - mass function
490:46 - so that was all very nice uh next
490:48 - example
490:49 - let s be the sum of the number of pips
490:53 - rolled onto dice find p of s
490:56 - and plot it okay okay
491:00 - so let's uh come so let's uh form our
491:03 - table again
491:04 - so we've got s and we've got
491:07 - p of s
491:11 - okay so s are possible sums of the dice
491:15 - so what are some possible sums of the
491:17 - dice well
491:19 - uh the smallest it could be is two
491:20 - because that's what happens when you
491:21 - roll snake eyes or one and one
491:23 - so the poss the smallest possible sum is
491:26 - two and the largest one happens when you
491:28 - roll box cars which is
491:30 - both of them are six so that will be
491:32 - twelve so it's going to be everything in
491:33 - between so two
491:34 - three four five six
491:38 - 7 8 9
491:42 - 10 11 12. uh
491:45 - hold on hold on uh
491:49 - i didn't write that quite right 10
491:53 - 11 12. okay
491:57 - and now we need to figure out
492:00 - uh the probably mass function okay
492:04 - so we're saying we we could imagine that
492:06 - there is a sample space
492:08 - that's consisting of a
492:11 - dice of combinations of two dice like we
492:15 - got one one
492:16 - uh uh one
492:19 - [Music]
492:20 - two
492:23 - and two and going on
492:27 - and uh we got like six six we've already
492:30 - worked with this
492:31 - uh type of uh sample space in previous
492:34 - videos
492:36 - and i don't want to go into too much
492:37 - more detail into it because it can get
492:39 - kind of tedious so um
492:43 - we know that there are 36 outcomes
492:46 - in this uh sample space so
492:50 - and we're saying that everything is
492:51 - equally likely so therefore
492:53 - uh the probability of the event the dice
492:55 - add up to two is going to be the number
492:56 - of outcomes
492:58 - uh where the two dice add up to two
493:00 - divided by the size of the sample space
493:02 - so how many outcomes are there with the
493:04 - die set up to two well
493:06 - you can you can get uh snake eyes and
493:08 - that's it
493:09 - uh so there's only one outcome that
493:12 - corresponds to that and then we divide
493:13 - it by 36.
493:15 - uh how about three we could either roll
493:18 - one the left dice and two on the right
493:19 - dice or i guess let's uh
493:21 - uh re-adopt that uh blue dice red dice
493:26 - uh verbiage and we could say
493:29 - uh the blue dice rolls a one and the red
493:32 - dice rolls a two
493:34 - or we could have the blue dice roll a
493:36 - two and a red dice to rule a one
493:38 - and that's it otherwise it will not be
493:40 - it will not add up to three
493:42 - so that's two outcomes
493:45 - that correspond with this so we're gonna
493:47 - have 2 over 36
493:49 - and for 4 we could either have 1 and 3 2
493:52 - and 2 or 3 and 1 so that would be
493:54 - 3 over 36 and that's going to keep going
493:59 - so we would have 4 over 36 4 5
494:03 - 5 over 36 for 6 and
494:06 - 6 over 36 for 7. uh all right
494:09 - 12. there's only one outcome where you
494:12 - can get 12 and that's box cars 6 and 6.
494:14 - so this will be 1 over 36 for 11. you
494:17 - could get 5 and 6 or 6 and 5. so we'll
494:19 - have 2 over 36
494:20 - and you can see the pattern it's going
494:22 - to become 3 over 36 for 10
494:25 - uh 4 over 36 for nine
494:29 - and uh five over 36 for eight
494:32 - okay and now we're ready to create our
494:35 - little visualization
494:37 - so we got two three uh
494:40 - four five six
494:43 - seven eight nine ten
494:47 - eleven uh twelve
494:51 - uh that is an awful looking eleven i
494:54 - really did try harder
494:56 - but sometimes the screen doesn't want to
494:58 - cooperate
494:59 - all right and uh let's see
495:02 - for the y-axis we could go
495:05 - uh the highest you ever go is six over
495:07 - 36 so we could go one
495:08 - two three four five six
495:12 - there we go so six over 36 is at the top
495:16 - so for our probability so we'll go one
495:20 - two three
495:24 - four five
495:28 - six five
495:31 - four three
495:36 - two one
495:40 - okay and that's it that's our probably
495:42 - mass function
495:44 - so um using this discrete rv package
495:48 - uh we've got what we could do is create
495:52 - a random variable representing a single
495:54 - die
495:55 - so that's created here and then we could
495:58 - say all right
495:59 - s is the sum of independent and
496:01 - identically distributed copies
496:03 - of the random variable d that is getting
496:06 - into
496:07 - verbage that uh and terms and ideas that
496:12 - i haven't really talked about yet
496:13 - but basically you add up two independent
496:15 - dice which is kind of what's going on
496:17 - in this experiment and then i tell it to
496:20 - plot it
496:20 - and it makes a plot and that's a very
496:22 - good plot all right
496:24 - uh next up consider flipping a fair coin
496:27 - until heads is seen let n be the number
496:30 - of flips
496:30 - find a probably mass function describing
496:32 - the distribution of n and plot the first
496:34 - few values
496:36 - of the pmf and um we've actually talked
496:40 - about
496:41 - uh this type of random variable before
496:45 - a number of times in the previous
496:46 - chapter uh as i mentioned there
496:49 - it's one of my favorite random variables
496:52 - to refer to since
496:54 - on the one hand it's not like the setup
496:56 - is quite simple to understand you flip a
496:58 - coin until you get heads that doesn't
496:59 - that's not that doesn't take a great
497:01 - deal of imagination
497:02 - and yet you can still get a lot of
497:05 - richness
497:06 - out of it and the mathematics can get
497:09 - kind of involved
497:10 - so um
497:14 - okay so in this case what we ended up
497:16 - coming up with before
497:18 - and suggest what we should have now in
497:20 - fact maybe if you go back to that video
497:23 - and look at how it was uh showing that
497:25 - the probably the sample space under some
497:27 - probably measure
497:28 - um will in fact be one i actually kind
497:31 - of defined a prototype random variable
497:33 - n of omega uh that was uh
497:37 - measuring the length of the string and
497:39 - that was
497:40 - that's basically a random variable right
497:42 - there
497:43 - um i defined a random variable on that
497:45 - space so that i could compute
497:46 - show that the probability um of that
497:49 - space added up to one
497:51 - so yeah they're very useful things but
497:53 - basically we could just say that this is
497:55 - one half to the power
497:56 - n for when
498:00 - n is a natural number
498:04 - uh otherwise you would just assume that
498:06 - this thing is going to be zero
498:08 - so this is a natural probability mass
498:10 - function for this space
498:12 - we actually showed in that section that
498:14 - it adds up to one
498:16 - and uh yeah so okay so uh let's um
498:20 - uh plot this uh probably mass function
498:23 - we got
498:24 - possible values a half a fourth an
498:26 - eighth a sixteenth
498:28 - a thirty second so uh one
498:32 - two three four
498:36 - five so for one we go up to a half
498:41 - for two we go up to a fourth
498:44 - for three we go up to an eighth for four
498:47 - we go
498:47 - up to a sixteenth and for five we go up
498:50 - to thirty second
498:51 - and in principle this this graph goes on
498:54 - forever
498:55 - but we're just going to stop at 5.
499:09 - section is on expected values let's
499:12 - start by deciding
499:13 - defining what an expected value is the
499:16 - expected value of a discrete random
499:18 - variable
499:18 - uh which we are often dealing with e x
499:22 - is given below so the expected value
499:26 - of x is equal to
499:29 - the sum of x times the probability mass
499:34 - function of x wherever the problem mass
499:38 - function
499:39 - of x is non-zero
499:45 - this is just a more general way to write
499:47 - down what expected values are
499:50 - in principle most of the time the x over
499:53 - what you're summing
499:54 - are the integers so if you really wanted
499:56 - to most of the time you'll be okay
499:58 - thinking of expected values as this like
500:00 - um
500:02 - x equals let's say zero to infinity
500:05 - or another way we could do it is
500:08 - this is actually allowed uh x equals
500:11 - negative infinity to infinity
500:14 - um or if you know what that um
500:18 - your little x ranges from a to b you
500:20 - could say
500:21 - uh we could say
500:25 - x equals a to b there's all sorts of
500:28 - ways we could possibly rearrange this
500:30 - but really what matters is that you're
500:32 - summing up
500:33 - uh this expression x times the
500:36 - probability mass function
500:37 - of x wherever that probability mass
500:40 - function
500:42 - is non-zero so i'm just going to leave
500:45 - this as
500:46 - x such that probably mass function of x
500:48 - is greater than zero
500:50 - actually i'm just going to not even
500:51 - really say all that much over what
500:53 - x we're summing over i'm just going to
500:55 - say that you sum
500:57 - over x okay
501:00 - next uh expected value of x is viewed as
501:02 - the population mean
501:04 - which we're often denoting with the
501:05 - greek letter mu described in previous
501:07 - chapters
501:08 - we can always compute the expected
501:09 - values of functions of x oh no not
501:11 - always why did i say always like at the
501:13 - very
501:14 - end of this section i'm going to give
501:16 - you an example of an expected value that
501:17 - can't be computed
501:18 - but we can also compute i meant also
501:22 - also compute the expected value of
501:24 - functions of x
501:26 - functions of x that is um so that would
501:29 - be the expected value of
501:30 - h of x in a natural way by saying
501:34 - that the expected value of
501:37 - h of x is equal to
501:41 - the sum over x
501:45 - of h of x times the probability mass
501:49 - function
501:50 - at x and this formula should make some
501:53 - sense to you because you've actually
501:54 - seen it before
501:56 - or something very similar to it before
501:59 - remember when we remember when we were
502:00 - computing the sample mean
502:02 - the sample mean was 1 over n times the
502:04 - sum
502:05 - from i equals 1 to
502:09 - n x i which is also equal to the sum
502:13 - from i equals one to n
502:18 - uh x i times one over n
502:22 - and the one over n i mean if you add up
502:25 - one over n
502:26 - n times what number do you get you get
502:27 - one and also one over n
502:30 - is a number that's uh greater than zero
502:33 - so actually you could see this
502:36 - as a probably mass function
502:40 - for x i so you can imagine that you have
502:44 - your sample of observations
502:46 - and you're going to pull an observation
502:49 - at random with equal probability from
502:51 - your sample if it's pull from with equal
502:52 - probability
502:53 - then you're going to pull it with
502:54 - probability 1 over n so
502:57 - actually you've seen this expected value
503:00 - formula before it's just that this
503:02 - formula down here is a more general
503:04 - notion
503:05 - of of a sample mean or not really a
503:07 - sample mean but of mean
503:10 - so it allows for more situations it
503:11 - allows for
503:15 - infinite uh possibilities for x
503:18 - and so on the expected value is in some
503:20 - sense a best prediction
503:22 - for the value of x and uh this
503:25 - form uh no did i say sample mean i don't
503:28 - think i did
503:29 - expected value you can see this footnote
503:31 - for what sense in which it's the best
503:33 - prediction
503:34 - but it's hence the term expectation
503:37 - it's like if you had to guess what value
503:40 - this random variable is going to be
503:42 - uh you can use the expected value to do
503:44 - that and it will be
503:45 - uh correct in some sense so example
503:49 - 11 complete the expected value for some
503:51 - of the random variables that we've seen
503:52 - in previous sections
503:55 - so bernoulli random variables discrete
503:57 - uniform this
503:58 - discrete uniform random variable that
503:59 - represents a die roll
504:01 - and the geometric random variable with
504:04 - parameter p
504:06 - okay so let's get started
504:09 - um the first situation for a bernoulli
504:13 - random variable
504:15 - the expected value of x
504:20 - is equal to the sum
504:23 - of x p of x and
504:26 - the values of x for which p of x
504:30 - is possibly non-zero will be
504:33 - x equals zero to one so this will be
504:37 - um zero times p of zero
504:41 - which is one minus p
504:45 - this corresponds to the poly mass
504:47 - function at 0
504:49 - plus 1 times p
504:54 - where p is the value of the problem as
504:57 - function at one
504:58 - i guess this uh notation is actually a
505:00 - little unfortunate
505:01 - because i've got a couple different p's
505:03 - so maybe
505:04 - i should switch out the notation
505:08 - for the probably mass function in this
505:11 - example just switch it with f
505:15 - that could uh probably make things a bit
505:18 - more clear
505:19 - so this still nevertheless in any
505:21 - situation
505:23 - is the probably mass function for x
505:30 - okay and well let's see that just goes
505:33 - to zero
505:33 - and that's going to equal p so that
505:35 - means that the expected value of x is
505:36 - equal to p
505:38 - which is nice and actually
505:41 - rather insightful it's saying that
505:45 - the expected value of a bernoulli random
505:47 - variable with parameter p
505:48 - is equal to the probability that that
505:50 - random variable is equal to one
505:52 - and that's actually a very useful fact
505:55 - that's
505:56 - rather useful it allows us
505:59 - to relate bernoulli random variables
506:03 - probabilities for events and expected
506:05 - values
506:06 - it gives us a way to what we could do
506:09 - if we wanted to relate expected values
506:12 - to probability of an event
506:13 - is create a bernoulli random variable
506:15 - that's equal to one when that event
506:17 - occurs and zero
506:18 - otherwise and then the expected value of
506:20 - that random variable would be
506:22 - the probability of that event occurring
506:24 - but i'm just going to leave that
506:25 - issue for now let's work with the
506:29 - next example so the expected value of s
506:34 - is equal to the sum of
506:38 - let's say s p of s again we're talking
506:41 - about
506:42 - uh a probability mass function when
506:45 - we're talking about p
506:47 - uh down here and
506:50 - let's uh think about what are things
506:52 - that this random variable could take
506:53 - with positive probability
506:55 - we'll end up summing from s equals one
506:58 - to
506:58 - six so that's going to be
507:02 - uh let's see p of s is always
507:05 - 1 over 6. so this is equal to 1 over 6
507:09 - times the sum from s equals
507:12 - 1 to 6 of s and actually there's a
507:15 - formula for that
507:16 - hopefully you remember that from from
507:18 - your algebra classes
507:19 - in general you have the sum
507:24 - of uh s equals 1 to n
507:28 - of s this is a sum
507:31 - of an arithmetic series
507:34 - that's going to be
507:37 - n times n plus one
507:41 - divided by two okay
507:44 - which means that this sum is going to
507:47 - equal
507:48 - one-sixth times six
507:51 - times seven divided by two
507:55 - those two sixes cancel out so this will
507:58 - equal
507:59 - 7 over 2 or 3.5
508:06 - okay finally
508:09 - we have the expected value of what am i
508:12 - calling this geometric random variable
508:14 - i'm calling it
508:14 - n the expected value of
508:18 - n which is going to be the sum
508:23 - of n times p of n
508:27 - uh where n ranges from one to
508:30 - infinity all right this is gonna get
508:32 - much more complicated
508:34 - uh so because this is going to involve
508:36 - an infinite sum
508:37 - p of n is going to be
508:40 - we've got np 1 minus p to the power
508:44 - n minus 1. and we're uh summing from
508:47 - n equals 1 to infinity
508:50 - let's see the p here is a constant so we
508:54 - can bring that out
508:55 - we are not summing over p that means
508:58 - that we can say
508:59 - that this is equal to p
509:02 - times the sum from n equals
509:05 - 1 to infinity
509:09 - n times 1 minus p to the power n minus
509:12 - 1.
509:13 - you probably did not see a formula for
509:15 - this
509:17 - so what are we going to do
509:20 - we're going to get tricky we're going to
509:23 - get really tricky
509:25 - you've taken calculus presumably you've
509:27 - taken calculus 1 which includes
509:29 - differential calculus
509:31 - so you have seen this formula before
509:36 - the derivative with respect to x
509:39 - of x to the power n is equal to
509:43 - n x n minus one
509:46 - with some assumptions on n like for
509:48 - example that it's
509:49 - uh let's say at least one in
509:52 - which case that would hold
509:56 - in this situation if you're looking at
509:58 - one minus p
510:00 - as your thing as the thing you're
510:01 - differentiating
510:03 - hmm those two things look rather similar
510:07 - which means that actually
510:11 - we could be invoking some sort of
510:15 - differential
510:16 - or derivative in our
510:19 - sum and say instead
510:22 - that this is equal to p
510:26 - times the sum from n
510:30 - equals one to infinity
510:34 - the derivative with respect to p
510:38 - 1 minus p to the power n
510:42 - now you would notice that right now if
510:43 - you were to in fact take that derivative
510:46 - you would almost get what i wrote on the
510:49 - left
510:49 - over here that i've underlined in green
510:51 - you would almost get that except you'd
510:53 - be off by a sign
510:54 - as in you'd get negative something
510:56 - because you have to invoke
510:58 - the chain rule so what are we gonna do
511:01 - about that we're just gonna
511:02 - throw a negative out here and
511:06 - uh then it's certainly true
511:09 - although i'm going to mention that
511:12 - well okay it's actually true as written
511:15 - down there's
511:16 - no there's no caveats yet
511:20 - but there is going to be one in just a
511:22 - second because
511:23 - i'm going to say that this
511:28 - is equal to negative p
511:32 - times the derivative with respect to
511:35 - p the sum from n equals 1
511:40 - to infinity 1 minus p to the power
511:44 - n okay you can't just bring derivatives
511:47 - out of sums like that
511:50 - or you can but there
511:53 - are conditions like you can't just look
511:56 - at any
511:57 - sum in the world and see derivatives
511:59 - inside the sum and say
512:01 - okay i could just slip the derivative
512:03 - out it's not that simple
512:05 - there are details there are conditions
512:07 - under which you can do this
512:09 - those conditions are satisfied here i
512:11 - actually don't remember them off the top
512:13 - my head
512:13 - um i just know you can you can go
512:17 - look at some calculus book or some
512:19 - analysis book it's probably going to be
512:20 - an analysis book
512:22 - uh or wikipedia wikipedia has pretty
512:24 - good math articles
512:26 - and it would tell you when you can
512:29 - switch
512:31 - because that's what you're doing you're
512:33 - switching
512:34 - a sum and a derivative
512:37 - it will tell you when you're allowed to
512:38 - do that and i'm just
512:40 - completely sweeping that under the rug
512:41 - and i'm fine with that because
512:43 - i don't care i got other stuff to do all
512:45 - right um
512:46 - the thing though is we know how to
512:49 - compute this sum
512:52 - you know that this sum is going to
512:54 - become
512:57 - uh 1 minus p divided by p
513:03 - so that means that this will become
513:06 - negative p
513:08 - oops wrong color negative p
513:13 - and then we've got the derivative with
513:14 - respect to p
513:16 - of um 1 minus p
513:20 - divided by p
513:24 - which we should probably uh simplify
513:27 - somewhat
513:28 - and say that this is equal to negative
513:32 - p and the derivative with respect to p
513:35 - of
513:37 - let's see one over p minus
513:40 - one yeah that's right and the derivat so
513:44 - then we take that derivative and say
513:46 - we've got
513:46 - negative p and
513:50 - on the inside after we take the
513:51 - derivative we'll get negative one over p
513:52 - squared
513:56 - those two negatives turn into positives
513:58 - we cancel out one of ps and this is
514:00 - equal to one over p
514:04 - there we go this actually has a very
514:07 - nice
514:07 - intuitive um interpretation
514:11 - which is that let's say that you're
514:12 - flipping a coin until you get heads
514:14 - how many times do you what is the
514:16 - expected number of flip first ones tails
514:18 - second one's heads
514:20 - makes pretty good sense to me or let's
514:22 - say
514:23 - that you have a bias coin and
514:26 - you flip this coin until you get heads
514:28 - and the probably that you get heads
514:31 - is uh 0.1 how many flips do you need 10
514:34 - flips
514:36 - seems to make a pretty good sense at
514:38 - least to me
514:41 - it allows at least in my mind
514:45 - way for me to relate probability
514:48 - of an event happening with time
514:52 - and say that if that if you were to have
514:55 - an uh
514:56 - a sequence of independent re
514:58 - replications of this event
514:59 - this is about how long you have to wait
515:01 - until you see that event happen
515:03 - which is another way to think of or
515:05 - another way to reason about how rare
515:06 - that event would be
515:07 - like for example if there's like a 10
515:10 - chance of an earthquake every year how
515:12 - many years
515:12 - is it going to take for you to see an
515:14 - earthquake 10 years on average
515:16 - uh something like that that's so i
515:19 - really like
515:20 - uh that formula interpretations like
515:22 - that okay moving on
515:24 - this is the same body of lecture notes
515:25 - that we've seen before so we have loaded
515:28 - up
515:28 - the discrete rv library in r
515:31 - when you whenever you see the r
515:35 - sections and there is an
515:38 - there's a discrete rv function called e
515:41 - that is for computing expected values
515:44 - so x was a random variable that we
515:46 - defined at some point that corresponds
515:47 - to
515:48 - the bernoulli random variable that we
515:49 - were talking about s
515:51 - corresponds to actually the sum of two
515:54 - dice so in this case it we should be
515:55 - doing
515:56 - seven over two or something like that um
516:01 - uh if we were actually talking about the
516:03 - same s but this is a different
516:05 - s uh this is a uh sum of two dice
516:11 - two independent dice by the way uh
516:15 - not just any two dice although i don't
516:17 - know how you make
516:18 - two non-independent dice that would be
516:20 - really hard and is the what we're
516:22 - talking about and notice that the answer
516:23 - is approximate
516:25 - uh we know that in in for this end the p
516:28 - parameter was 2
516:30 - so the number that results should be 2
516:32 - but it's not 2.
516:33 - it's 1.9999999 so
516:36 - be aware of that it's giving us an
516:38 - approximate answer because it's only
516:39 - fine
516:40 - summing over a finite number of uh
516:43 - places but you get the idea you can tell
516:46 - that that is
516:47 - that it's essentially doing the right
516:48 - thing uh okay
516:50 - uh continuing on expectations are linear
516:54 - functions
516:55 - and being a linear function is an
516:58 - extremely
516:59 - important property um
517:02 - and it's for that reason that we can say
517:04 - expectations are integrals
517:06 - but that is a 60 40 idea right there
517:09 - instead
517:10 - we're going to talk about how the
517:12 - expected value of ax
517:13 - plus b is equal to a times the expected
517:15 - value of x plus b
517:17 - this is something that we can show uh
517:19 - watch the expected value
517:22 - of a x plus b
517:25 - uh is equal to this this by the way is
517:29 - uh
517:29 - we this is basically h of x where h
517:33 - of let's say s is equal to a
517:36 - s plus b
517:39 - so we can use some of those that uh
517:42 - expectation formula that we mentioned
517:44 - above uh towards the beginning of this
517:46 - lecture
517:47 - and say that this expectation
517:50 - is equal to the sum over
517:53 - x a
517:57 - x plus b times the probably mass
518:00 - function of
518:01 - x which is equal to the sum
518:05 - over x and then we'll factor all that
518:07 - stuff together we've got
518:08 - a x p of x
518:12 - plus b p of x
518:16 - and then we'll break up the sum and say
518:18 - that this is the sum
518:20 - over x a x p of x
518:24 - plus uh the sum over
518:28 - x scroll down
518:31 - scroll down uh the sum over x
518:35 - b p of x factor out the constants to say
518:39 - that this
518:40 - is a times the sum over
518:43 - x x p of x
518:47 - plus b times the sum over
518:50 - x p of x and
518:53 - we can recognize what some of those sums
518:56 - are
518:56 - for instance this sum is the expected
518:59 - value
519:00 - of x and this sum is the sum of the
519:03 - probability mass function
519:04 - um over everything where it's positive
519:07 - so this is going to sum to 1
519:09 - and hence you get uh the result
519:12 - uh a times the expected value
519:16 - of x plus b
519:21 - hence it's a linear the variance
519:25 - of a random variable is given by
519:28 - we'll call it var of x
519:33 - which is equal to the expected value
519:36 - of x minus mu
519:42 - where mu is just the expected value of x
519:47 - we just don't want to write that again
519:48 - in there because it's it feels confusing
519:51 - so we just put a mu in there but
519:53 - basically it's the
519:54 - mean squared distance of x from its uh
519:57 - from its expected value so
520:00 - this actually does correspond very
520:03 - closely
520:04 - to the sample variance as well if you
520:07 - could think of the sample variance as
520:08 - divided by one over n
520:10 - instead of one over n minus one uh you
520:12 - could say
520:13 - we could argue as we did for how the
520:16 - sample mean is very similar to
520:19 - the population mean in terms of expected
520:20 - values
520:22 - and say that this is a sample variance
520:24 - too so
520:26 - yeah they so this is uh something to
520:29 - notice
520:30 - the greek letter that's used to
520:33 - represent the sample variance
520:35 - is sigma squared and like
520:38 - with the sample standard deviation you
520:40 - can get the population standard
520:42 - deviation by taking the square root of
520:44 - the variance it's just not as common to
520:47 - do so
520:48 - um all right so uh there is actually a
520:51 - handy formula for computing the variance
520:53 - that is often easier than computing it
520:55 - directly
520:56 - and in this formula you may recognize
520:58 - this from when we were working with the
521:00 - uh the uh did i say sample variance a
521:03 - second ago
521:04 - uh i meant the variance or the
521:06 - population variance but this formula
521:08 - resembles the formula for the sum of
521:10 - squared sum of squared errors
521:13 - uh that we had that we saw in chapter
521:15 - one
521:16 - where you have the mean of x squared
521:19 - minus the mean of x
521:21 - squared
521:25 - where hopefully you can tell from my
521:27 - inflection
521:28 - what's being squared
521:33 - okay so the variance of x is thought of
521:36 - as the population variance
521:38 - and is denoted by var x which is sigma
521:39 - squared and the population standard
521:41 - deviation is sigma which is the square
521:42 - root of sigma squared
521:44 - sometimes i'll write though the standard
521:46 - deviation of x
521:47 - because it's it's sometimes nice to do
521:50 - uh
521:50 - all right so our next example compute
521:53 - the variance and standard deviation of
521:54 - the random variables
521:56 - listed in example 11.
521:59 - so if that's the case let's start out
522:02 - with
522:03 - x we already have the expected value of
522:08 - x which was p
522:13 - the expected value of s
522:17 - which is uh seven halves
522:21 - and the expected value of n
522:25 - which is equal to one over p so as a
522:27 - reminder of what we have already
522:29 - so now let's compute the expected value
522:32 - of
522:32 - x squared
522:36 - which is equal to the sum
522:39 - from little x equals 0 to 1
522:43 - um x squared uh f of x
522:47 - because that's what i'm calling the
522:48 - probably mass function which is equal to
522:51 - 0 squared times 1 minus p
522:54 - plus 1 squared times p
522:57 - which is equal to p again hence you get
523:00 - to say
523:01 - that the variance of x
523:05 - is going to be the expected value
523:08 - of x squared minus
523:12 - the mean of x
523:16 - squared which is equal to p squared
523:19 - minus p which we could be done right
523:21 - there but people often like to factor
523:23 - this
523:24 - into p times one minus p
523:31 - all right next example
523:35 - uh in the case of s so
523:38 - the expected value of s squared
523:43 - is going to be the sum from
523:46 - s equals 1 to 6
523:50 - of s square times probably mass function
523:53 - at
523:54 - s which is 1 over
523:57 - 6 times the sum from
524:00 - s equals 1 to 6 s squared
524:05 - this is something that we have actually
524:08 - all right you might not have seen this a
524:11 - formula for
524:12 - the sum of squares uh
524:15 - in your previous algebra classes maybe
524:18 - you saw that maybe you didn't
524:20 - but there is in fact a formula for that
524:24 - which i'm gonna have to look up okay so
524:27 - you have that the sum
524:30 - yeah let's use a different color for
524:32 - this
524:34 - the sum from s
524:38 - equals 1 to n
524:42 - s squared is equal to
524:45 - n times n plus one
524:48 - times two n plus one
524:52 - divided by six
525:00 - okay so using that here
525:03 - we get to say that this is equal to
525:06 - 1 6 times 6
525:10 - times 7 times
525:15 - 13
525:18 - divided by six
525:21 - so those cancel and that's pretty much
525:24 - all we can cancel
525:26 - so we get to say that this is equal to
525:30 - 91 divided by 6. therefore the variance
525:35 - of s is going to be
525:39 - the expected value of s squared
525:42 - minus the mean of s squared
525:50 - which is equal to
525:53 - 91 over 6 minus
525:57 - 49 over four
526:01 - what is that number uh
526:05 - uh 35 over 12.
526:15 - and you should have that the variance is
526:18 - always
526:19 - a non-negative number in fact the only
526:21 - time that the variance is ever equal to
526:23 - zero as if the random variable is
526:24 - degenerate that is if it's effectively a
526:27 - constant
526:28 - so it's unlikely that your variance is
526:30 - zero
526:32 - and uh and
526:35 - in the more general case it's
526:39 - impossible for your r for your variance
526:42 - to be negative
526:44 - so if you ever ended up with a negative
526:46 - variance then you've done something
526:47 - wrong
526:50 - all right for the final one and this one
526:54 - is where things get weird all right i'm
526:57 - i'm going to zoom in for this one
527:01 - because this one is where things get
527:03 - really tricky because
527:04 - we're now working with the geometric
527:06 - case and we need to compute the expected
527:08 - value
527:09 - of n squared okay and that is equal to
527:14 - uh the sum from n equals
527:18 - 1 to infinity n squared times the
527:21 - probability mass function
527:22 - at n which is equal to
527:25 - uh the sum from
527:29 - n equals 1 to infinity i'm going to go
527:31 - ahead and already do some simplification
527:32 - we get p
527:35 - n squared 1 minus p to the power
527:38 - n minus 1. now
527:42 - how on earth are we going to compute
527:44 - that
527:45 - well we're going to get we're going to
527:48 - get really tricky is what we're going to
527:50 - do
527:51 - so we're going to say that
527:54 - let's let's zoom in even more
527:58 - we're going to say and you're not all
528:00 - right this this is just so weird what's
528:02 - about to happen
528:05 - um according to my notes it's actually
528:08 - advantageous
528:09 - to keep the p inside uh
528:12 - so let's let's uh put the p
528:17 - back inside of this sum rather than
528:20 - factor it out
528:22 - uh so we got a sum from n equals one to
528:25 - infinity
528:28 - n squared p all right
528:33 - what's going to end up happening is
528:34 - we're going to end up um
528:38 - adding 1 no hold on
528:41 - subtracting 1 and then adding 1 again
528:44 - inside of that square
528:49 - and leave everything else the same
528:54 - and we're going to go do some
528:55 - calculations
528:57 - and at the very end of them
529:01 - the expected value go away
529:05 - the expected value of n squared is going
529:08 - to appear on both the left hand and the
529:10 - right hand side
529:11 - uh left hand and right hand side of an
529:14 - equal sign
529:16 - so after that happens the thing is
529:18 - though on the right hand side of that
529:20 - equal sign
529:21 - it's not going to be just the expected
529:22 - value of n squared it's going to be the
529:24 - expected value of n squared plus
529:26 - something times something
529:28 - and when you have a situation like that
529:30 - you're going to be able to
529:32 - solve for the expected value of n
529:34 - squared because you just have an
529:35 - algebraic relationship
529:38 - and you're just going to have to see it
529:40 - and and watch it happen
529:42 - in order to kind of understand it's just
529:44 - at the very end
529:45 - all of a sudden what you're going to
529:46 - need pops out
529:48 - this is one of those situations where
529:50 - it's a trick and
529:51 - you're going to see the trick and you
529:53 - might not understand the motivation for
529:55 - the trick
529:56 - but someone did that trick once and it
529:58 - seems to work
530:00 - all right so uh here we go
530:04 - this part right here is a perfect is a
530:07 - it's a square
530:08 - a perfect square so uh we can
530:12 - we can now write that part i i'm not
530:16 - going to write
530:16 - n equals 1 to infinity all the time
530:18 - that's going to get annoying
530:20 - so i'm just going to write a sum over n
530:22 - down here
530:23 - and say we've got n plus one no no no no
530:27 - not n plus one n minus one
530:31 - we've got n minus one squared
530:35 - plus two times
530:38 - n minus one plus one
530:42 - and then we've got p one minus p
530:46 - to the power n minus one
530:50 - yeah okay and then this is equal to
530:56 - this is equal to uh breaking up
531:00 - this part and breaking up the resulting
531:02 - sum we get the sum
531:05 - over n
531:08 - and we have n minus 1
531:11 - squared p 1 minus p
531:15 - power n minus 1.
531:19 - [Music]
531:20 - plus 2
531:24 - plus 2 times the sum over
531:27 - n uh n minus 1
531:31 - p 1 minus p to the power n minus 1
531:36 - plus the sum over n
531:39 - of p 1 minus p
531:43 - to the power n minus 1. okay now we can
531:46 - start
531:46 - uh recognizing some stuff uh the term
531:50 - on the very left hand side no not left
531:53 - hand right hand side
531:55 - this term this is equal to one because
531:57 - this is just the sum of the probably
531:58 - mass function
532:00 - this term is the expected value of
532:03 - n minus one
532:08 - okay and then we get to say that
532:12 - so far collecting our stuff um
532:14 - recognizing those substitutions is the
532:16 - sum
532:16 - over n uh n minus 1
532:20 - squared p 1 minus p to the power n minus
532:24 - 1
532:25 - plus 2 times the expected value
532:28 - of i'm going to write this as the
532:30 - expected value of
532:31 - n minus 1 because we have that linearity
532:35 - property that i proved
532:36 - a few mo a few minutes ago and then we
532:39 - have plus one
532:40 - all right and uh doing some even further
532:43 - simplification
532:44 - we're able to recognize that the
532:46 - expected value of n
532:47 - is equal to one over p so that means
532:50 - that this
532:50 - term uh that we're adding is going to be
532:54 - uh one over p minus one so we got
532:58 - plus 2 over p minus 2 plus 1 so this
533:01 - will be
533:02 - uh so that means that this is going to
533:04 - simplify
533:06 - into
533:09 - plus 2 over p minus 1.
533:15 - and then we're going to take
533:18 - this n minus 1 and say oh well that's a
533:20 - perfect square too so this will be n
533:22 - squared minus 2n
533:24 - plus 1. all right so
533:27 - we then get um oh wait actually we don't
533:31 - want to do that
533:32 - no we don't want to do that we do not
533:34 - want to do that
533:36 - uh we don't want to do that we want to
533:39 - do something even
533:40 - trickier what we're going to do instead
533:44 - all right let's write in again what i
533:46 - have what what i have been omitting this
533:47 - whole time
533:48 - that this is the sum from n equals 1 to
533:50 - infinity
533:52 - but we're going to re-index this
533:59 - we're going to re-index this and say
534:02 - well
534:03 - actually this is the same as saying uh
534:06 - n minus one equals zero to infinity
534:10 - all right and then replace
534:14 - all of those n minus ones
534:18 - with let's say j
534:22 - and say this is j equals 0 to infinity
534:24 - so we get j
534:26 - j squared and the thing is though we're
534:29 - uh
534:32 - the first term in this sum though is
534:34 - going to end up being zero because j
534:36 - because all right plug in j equals zero
534:38 - that means the first term is going to be
534:39 - zero because zero squared is zero
534:41 - zero times whatever is zero so that
534:44 - means that the first term
534:45 - is actually zero so we get to
534:48 - um replace j equals zero
534:51 - with j equals one because
534:55 - well when you start at zero you just add
534:58 - a zero term you're just adding zero to
535:00 - the sum so we get to start at one
535:02 - and this is now looking almost like
535:06 - almost like uh the expected value
535:09 - except for uh the power
535:13 - up here is wrong it should be j minus
535:16 - one
535:16 - to have the problem mass function but
535:17 - we've got j instead okay
535:20 - so we'll replace that with j minus one
535:22 - plus one
535:24 - okay and to account for the plus one
535:27 - that means that what we need to do
535:29 - is factor out a 1 minus p
535:33 - so all told we will have
535:39 - 1 minus p times the sum
535:43 - from j equals 1 to infinity
535:47 - j squared p
535:50 - 1 minus p to the power j minus 1
535:56 - plus 2 over p minus 1.
535:59 - and that sum
536:03 - is what we started with
536:06 - this term right here is the expected
536:08 - value
536:10 - of n squared
536:13 - oh look at that so we get to say
536:18 - that this is going to be
536:23 - uh 1 minus p times the expected value
536:28 - of n squared
536:31 - plus two over p
536:36 - minus one
536:39 - and as a reminder at the very beginning
536:42 - of this
536:43 - long statement of equalities is the
536:45 - expected value
536:48 - of n squared oh look at that
536:52 - we can do some algebra now for instance
536:56 - we can subtract over uh we can subtract
537:00 - from both sides
537:01 - the expected value of n squared
537:11 - and say that this is going to suggest
537:15 - that uh what is uh 1 minus p
537:18 - minus 1 uh that's going to be negative p
537:22 - so we've got negative p times the
537:25 - expected value
537:27 - of n squared uh we will subtract
537:32 - two over p and add one to both sides
537:36 - minus two over p plus one
537:40 - to get uh that this is
537:43 - equal to negative two over p plus one
537:49 - and then divide both sides
537:52 - by negative p
537:56 - and now we get to say that this is equal
537:59 - to
538:02 - that uh the expected value of n squared
538:09 - is equal to 2 over p squared
538:15 - minus 1 over p
538:18 - and there it is that by the way is the
538:20 - expected value of n squared
538:24 - i'm just writing it down again because
538:25 - it's on my screen and because it was put
538:27 - on a separate line
538:28 - that's what we need to compute
538:32 - so it then follows
538:36 - that the variance of n
538:41 - is equal to 2 over p squared
538:45 - minus 1 over p minus 1 over p squared
538:49 - which is equal to
538:50 - [Music]
538:52 - 1 over p squared minus 1 over p
538:56 - which is equal to p minus 1
538:59 - over p squared wait a minute that's
539:03 - hold it hold it hold it hold it hold it
539:06 - that's
539:07 - oh no not p minus one uh one minus p
539:10 - okay
539:11 - otherwise that would have been bad
539:12 - because i would have i would have just
539:13 - computed a negative variance so one
539:15 - minus p over p squared
539:16 - uh did i ask to
539:20 - uh did i did i did i say that we should
539:22 - compute the standard deviation too
539:26 - it does it's not it's not super hard to
539:28 - do you just take the square root
539:29 - but yeah i did ask for standard
539:31 - deviations so
539:34 - okay computing the standard deviations
539:35 - too like that's super easy
539:37 - you just take the square root of the
539:39 - variance so the standard deviation of
539:41 - x is equal to the square root of p times
539:44 - one minus p
539:45 - uh the expected the standard deviation
539:49 - of
539:49 - s
539:53 - is equal to uh the square root uh let's
539:56 - see
539:57 - uh one half times the square root
540:01 - of uh 35
540:04 - over three and for
540:08 - n uh let's see the variance no the
540:11 - standard
540:12 - deviation of
540:15 - n is going to be the square root of 1
540:18 - minus p
540:19 - divided by p
540:23 - that's an ugly looking p
540:28 - all right there we go and in fact
540:31 - there are functions for
540:34 - in this uh discrete rv library for
540:37 - computing variance and standard
540:38 - deviation
540:39 - they're going to be v oops
540:43 - v and sd respectively and you can see
540:45 - this function computing the variance and
540:47 - standard deviation remember that this is
540:48 - not the same as we were talking about
540:49 - before
540:51 - so we get to compute those things and
540:55 - we get pretty much what we had so
540:58 - all right uh proposition 10 the variance
541:02 - of ax plus b
541:03 - is equal to a squared times the variance
541:04 - of x and
541:06 - the standard deviation of ax plus b is a
541:09 - times the standard deviation of x or the
541:11 - absolute value of a times the standard
541:12 - deviation of x
541:14 - uh actually this would probably be
541:15 - better to write in terms of that sd
541:17 - notation
541:17 - to say that sd of a x
541:21 - plus b is equal to the absolute value of
541:24 - a
541:24 - times the standard deviation of
541:28 - x
541:31 - so uh let's go ahead let's see this uh
541:35 - let's let's go ahead and prove this so
541:36 - the variance
541:39 - of a x plus b
541:42 - so the variance of ax plus b is the
541:44 - expected value
541:47 - of a x plus b
541:51 - minus we should have the expected value
541:54 - of
541:55 - a x plus b here
541:59 - here's the thing though uh the b's
542:03 - are going to cancel because expectations
542:06 - are linear
542:08 - and the a's can that a can be factored
542:10 - out in front of that expectation
542:13 - so that means that a is going to be a
542:15 - common factor and therefore the a can be
542:17 - factored out
542:18 - completely if we just square it so we
542:20 - get to say that this is the expected
542:22 - value
542:23 - of a squared and then we have on the
542:27 - inside
542:27 - x minus the expected value of x
542:34 - which is equal to a squared expected
542:37 - value
542:38 - of x minus mu squared just remember that
542:42 - mu is the expected value of x
542:44 - and that's the variance so this is equal
542:45 - to a squared times the variance
542:48 - of x all right what do we then
542:52 - say for the standard deviation we say
542:54 - that the standard deviation
542:56 - of a x plus b is the square root
543:01 - is equal to the square root of the
543:02 - variance of
543:04 - a x plus b which is equal to
543:08 - after you do that uh algebra the
543:11 - absolute value of
543:12 - a times the standard deviation of x
543:14 - because you're just going to take
543:16 - the square root of what i've highlighted
543:18 - in blue just take the square root of
543:20 - that and you're good
543:21 - all right so there's that formula
543:25 - uh one final note
543:28 - there is nothing that says that
543:30 - expectations need to be finite or even
543:32 - exist
543:33 - there are random variables out there
543:35 - that do not have
543:37 - finite uh uh standard
543:40 - uh finite expectations and they may not
543:43 - even have like
543:45 - like an infinite expectation they might
543:48 - not have an expectation at all like
543:49 - there's just no way to define it like
543:51 - i guess technically an infinite
543:53 - expectation is considered undefined
543:56 - but it's like you can't but there's a
543:58 - sense in which it is defined like
544:00 - infinite just means arbitrarily large
544:02 - but even then
544:04 - even then you might not be able to say
544:07 - that it's even infinite
544:08 - it could be anything there are random
544:10 - variables out there that don't have
544:11 - expectations
544:15 - so uh let's actually see an example of
544:19 - this this one's a fun one
544:21 - uh this is what's known as the saint
544:24 - petersburg game
544:26 - consider a game where a fair coin is
544:28 - flipped until it lands heads up
544:30 - a player would earn a dollar if the game
544:33 - ends with one flip
544:34 - two dollars if it ends two flips four
544:36 - dollars if it ends with three flips
544:37 - eight dollars if it ends with four flips
544:38 - and so on
544:39 - so basically your winnings are doubling
544:41 - every time
544:42 - uh this game goes on the fair price of a
544:45 - game
544:46 - corresponds to the game's expected
544:48 - payout what then is the fair price to
544:50 - play this game
544:51 - and before i continue on i would like
544:53 - for you to think about
544:54 - how much you think this game is worth
544:56 - and how much you would be willing to pay
544:57 - for it
544:58 - how much would you be willing to play at
545:00 - pay to play this gambling game
545:02 - what do you think is a fair price you
545:05 - might be surprised
545:09 - so let's calculate it we're going to say
545:14 - that n is following
545:19 - a geometric distribution
545:22 - with parameter one-half because that's
545:24 - how you should
545:25 - we're flipping a coin until we get heads
545:28 - and
545:29 - this geometric random variable it's a
545:31 - fair coin is what will
545:33 - model such an experiment so then
545:37 - what would be our winnings it would be
545:39 - two to the power
545:41 - um n minus one
545:45 - because if you get one flip that would
545:47 - be
545:48 - you should get one dollar so
545:51 - n minus one so that'll be one minus one
545:53 - so to the power zero
545:55 - which is one uh if we get if it took two
545:57 - flips that's
545:58 - two minus one in the power so that'll be
546:00 - two minus one
546:02 - uh so the power will be one so we get to
546:04 - the one which is equal to two
546:05 - and if we have three flips that's going
546:07 - to be two squared
546:08 - so we'll get four so this is in fact
546:10 - corresponding to what we think it should
546:12 - all right then what we were computing is
546:14 - the expected value
546:16 - of two to the power and minus one
546:21 - which we can make our lives a little bit
546:23 - easier by saying that this is one half
546:25 - times the expected value of two to the
546:28 - power
546:29 - n and we know how to compute the
546:32 - expected value of 2 to the power n
546:34 - so we'll say this is one half and we
546:36 - have the sum
546:37 - from n equals 1 to infinity
546:41 - to the power n and then we write down
546:44 - the probability mass function
546:45 - for the geometric random variable which
546:48 - is 2 to the power negative n
546:50 - or which is the same as that's the same
546:52 - thing as 1 half to the power n
546:54 - so n minus n that's going to be one so
546:58 - this is one half
547:00 - times the sum from n equals one to
547:03 - infinity
547:05 - one which is equal to infinity
547:09 - this game has infinite value
547:12 - you should pay a dollar to play this
547:14 - game you should pay ten dollars to play
547:16 - this game
547:17 - you should pay a hundred dollars to play
547:19 - this game you should go to the bank
547:21 - and take out a loan for a billion
547:23 - dollars and play this game
547:27 - because this game has infinite expected
547:29 - value any finite price is a bargain
547:33 - and yet no one in their right mind would
547:36 - ever do that
547:38 - no one thinks that this game is really
547:40 - worth anything people think this is a
547:42 - terrible game
547:44 - and why that is is somewhat remarkable
547:48 - it gets to the point that once you've
547:51 - earned a million dollars
547:54 - another million dollars doesn't seem
547:55 - that great i mean it's pretty good
547:58 - to go from one million dollars to two
547:59 - billion dollars that's nice
548:03 - same thing with one trillion dollars and
548:04 - two trillion dollars
548:06 - like you're gaining a trillion dollars
548:08 - when you go from one trillion dollars to
548:09 - two trillion dollars but it doesn't
548:10 - really feel like it
548:12 - like you already got everything you want
548:14 - at one trillion dollars
548:16 - the other trillion dollars is just gravy
548:20 - so basically the point is with this game
548:23 - the way to rationalize the paradox of
548:26 - this game
548:27 - the fact that it's expected value is
548:28 - infinite but no one wants to pay that
548:32 - is that people are not actually thinking
548:35 - about
548:37 - the winnings in terms of literal money
548:39 - they're thinking about in terms of the
548:40 - utility they get from that money
548:42 - and people know that the net the next
548:45 - trillion dollars
548:46 - is not as good as the first trillion
548:47 - dollars
548:49 - so in that case this game actually
548:52 - doesn't look very good
548:53 - when once you actually account for
548:55 - decreasing utility
548:57 - from your winnings that's how you
548:59 - resolve the paradox you resolve it with
549:01 - economics
549:02 - that's it for this video uh we have been
549:05 - talking about
549:07 - general ideas and random variable theory
549:11 - let's call it random variable theory
549:12 - that seems like a good word
549:14 - uh probably mass functions accumulate
549:17 - distribution functions all that stuff
549:18 - these are general things that
549:20 - all discrete random variables have and
549:22 - expectations
549:24 - uh excluding the cases where they don't
549:27 - exist they
549:28 - they are they're generally around two
549:31 - so now
549:34 - we're going to start looking at specific
549:36 - examples of common families of random
549:38 - variables that probabilists care about
549:40 - the first one being the binomial
549:42 - probability distribution
549:44 - a lot of the ideas also that we talked
549:46 - about here
549:47 - actually transfer over to the continuous
549:50 - case when we're talking about continuous
549:51 - random variables
549:53 - uh there they have analogs that are
549:55 - pretty similar what you do is you
549:57 - replace probably mass functions with
549:59 - probably density functions
550:02 - and you replace sums with integrals
550:05 - so that's what that's how you go from
550:08 - the discrete to the continuous case
550:10 - and but everything else applies
550:12 - everything else is the same
550:14 - variances expectations uh
550:17 - cdfs probably mass functions become
550:19 - probably density functions which are
550:21 - pretty similar
550:22 - so these are all basic ideas that you're
550:24 - going to see over and over again when
550:25 - talking about probability
550:27 - and from this point on we're going to
550:29 - for the remainder of this chapter we're
550:31 - going to be looking
550:32 - at specific examples
550:36 - because the advantage of talking about a
550:38 - family of distributions
550:40 - is once you have a family distributions
550:42 - you get to talk about
550:44 - you get to talk about it once and then
550:45 - you get to generalize to
550:47 - lots of different cases and it's like
550:49 - the expected value if you're able to
550:51 - recognize a random variable as being a
550:54 - particular case of a binomial
550:56 - then there's an expected value formula
550:58 - available to you and you don't need to
551:00 - compute it by hand
551:01 - you shouldn't compute it by hand because
551:02 - it's going to be a pain you could just
551:04 - use that formula
551:06 - and it's really easy same thing with a
551:08 - lot of these other
551:09 - random variables hypergeometric negative
551:11 - binomial
551:12 - there they would be a pain for you to do
551:14 - over and over again but because we've
551:15 - identified a family of random variables
551:18 - with common characteristics someone
551:19 - computed a formula and now you get to
551:21 - use that formula
551:22 - and that's really nice that's really
551:24 - nice
551:25 - okay so i'll just uh leave it at that
551:29 - uh we will end this uh the study of this
551:32 - section
551:33 - and i will see you later i will see you
551:36 - when we start talking about binomial
551:38 - random variables
551:51 - for our next section we are now
551:54 - discussing
551:55 - the binomial probability distribution
551:58 - so a binomial experiment is an
552:00 - experiment that
552:02 - satisfies the following requirements the
552:04 - experiment consists of n
552:06 - bernoulli trials that end in either
552:08 - success which we will denote
552:10 - s or failure with which we will denote
552:13 - f the trials are independent and
552:16 - for each trial the probability of s is 1
552:20 - minus the probability of failure
552:22 - which is some number p between 0 and 1.
552:25 - in the case of p being 0 or 1 this
552:28 - random variable is degenerate
552:30 - or the resulting random variable i guess
552:32 - i haven't mentioned a random variable
552:33 - yet
552:34 - but that random variable would be
552:35 - degenerate because you'd either always
552:37 - get success or always get failure
552:39 - so we don't consider that situation uh
552:42 - so we can think of the outcome of an
552:44 - experience as
552:46 - a sequence of s and f such as
552:49 - s f s f which
552:52 - in that case the
552:56 - uh the duration of the experiment would
552:59 - be
553:00 - n equals five so the binomial random
553:03 - variable is the associated random
553:05 - variable with binomial experiments and
553:07 - what a binomial random variable does
553:09 - is it will count the number of successes
553:12 - in the experiment
553:14 - so x of omega is equal to the number of
553:17 - s in omega uh we should probably
553:20 - not be uh um
553:23 - i mean why did i write that as a set
553:26 - that doesn't make any sense it's not a
553:28 - set
553:28 - right it's just a number
553:32 - uh we will then say that x follows a
553:35 - binomial distribution
553:36 - with parameters n and p so for example
553:40 - given that sequence of s's and f's
553:44 - that we saw before the binomial random
553:46 - variable would evaluate to three
553:49 - so we denote the problem mass function
553:51 - of x with
553:52 - lowercase b although that's more
553:54 - notation for this class
553:56 - i don't really see notations that we use
553:58 - in this class elsewhere because
554:01 - people know what they're talking about
554:03 - when you're reading papers
554:04 - and stuff so they don't bother to come
554:07 - up with some special notation for it
554:10 - anyway
554:12 - here we have the uh probably mass
554:16 - function for binomial random variables
554:19 - this is zero for x that's not an integer
554:21 - from zero to n
554:22 - and for x between zero and n it can
554:26 - in fact be computed that the
554:30 - probability mass function for a binomial
554:32 - random variable
554:33 - with parameters n and p is equal to
554:38 - n choose x
554:41 - p to the power x
554:44 - times 1 minus p to the power
554:47 - n minus x
554:55 - so here's some further explanation of
554:59 - this formula
555:01 - in this situation there are x
555:04 - successes out of n trials
555:08 - okay the probability of each of those
555:10 - successes is p
555:11 - they are independent trials so you
555:13 - multiply
555:15 - p x times and you would multiply
555:19 - y minus p minus 6 times this is
555:21 - accounting for the probability of each
555:22 - of those failures
555:24 - that occurred and here's the thing
555:27 - that will get you the probability of
555:29 - let's say the sequence
555:32 - ssf
555:35 - that would get you the probability of
555:37 - getting that particular sequence
555:40 - but the thing is there's a number of
555:42 - sequences
555:43 - where you could have three successes
555:45 - until two failures
555:47 - for example you could have sss ff
555:51 - or the other way around like ff sss and
555:54 - so on
555:55 - so we need to pick the positions
555:58 - in which successes occur and failures
556:01 - occur
556:02 - or we'll just simply pick the position
556:04 - of successes
556:05 - and if we pick the position of successes
556:07 - we then know
556:08 - where all the failures occurred in the
556:10 - sample
556:11 - or in this string so we end up with n
556:15 - choose x
556:16 - meaning out of x position out of n
556:19 - positions
556:20 - choose the x positions where
556:23 - successes occur
556:30 - the cdf of this random variable x is
556:32 - given
556:33 - next the probability
556:37 - no i don't want black blue
556:44 - the probability that x is less than or
556:48 - equal to
556:48 - little x is equal to
556:52 - the cdf of
556:56 - the binomial random variable
557:00 - which is equal to the sum
557:03 - from i equals zero because
557:07 - you could potentially have zero
557:08 - successes in your sample
557:10 - so from i equals zero to x rounded down
557:15 - the probability mass function at i
557:19 - n p which is equal to
557:23 - the sum from i equals zero
557:28 - to x rounded down
557:32 - and choose i
557:36 - p to the power i 1 minus p
557:42 - to the power n minus i
557:46 - and it looks like all i did was write
557:48 - down sum over the probability mass
557:50 - function then that that is what i'm
557:51 - writing down
557:52 - i didn't simplify this any further
557:54 - there's not really a whole lot more
557:57 - that you can say with this formula
557:59 - there's no
558:00 - fun little algebraic simplifications
558:03 - that you get
558:04 - all you're just going to say is sum up
558:07 - over the probably mass function
558:09 - and for that reason you're
558:13 - with the exception of um
558:16 - uh cases like some strange
558:20 - n or p
558:23 - historically in this class i would have
558:25 - students use the tables that were
558:27 - provided in the back
558:28 - of the textbook to
558:32 - work with the probably mass function or
558:34 - no not the problem mass function
558:36 - the cumulative distribution function
558:39 - now seeing as i am teaching this class
558:42 - online
558:43 - at the moment i don't necessarily see
558:47 - why i shouldn't like i'm telling my
558:50 - students
558:52 - that they can use r for pretty much
558:55 - anything
558:56 - even on quizzes and even on tests so
559:00 - for that reason i'm just not going to
559:02 - bother
559:04 - with working with the textbook and using
559:06 - the tables in the back of the book
559:08 - in these videos instead i'm just going
559:12 - to use
559:12 - r to get the
559:16 - cdf for binomial random variables
559:18 - although there may be some situations
559:20 - where like we might have um the input x
559:25 - we might replace the input x with say
559:27 - one okay if it's one
559:29 - you don't necessarily have to use r
559:31 - maybe i'll tell you not to look up the
559:32 - number and not to use r
559:34 - and ask you to compute the cdf just
559:37 - because there's only two things you're
559:38 - going to end up
559:39 - having to compute only two things are
559:41 - going to get plugged in
559:42 - so but that's kind of where
559:46 - what we're working with right now i
559:48 - might recycle these videos
559:50 - in the future and if i do be aware
559:55 - all right so one thing that's nice
559:59 - in these uh upcoming sections is
560:02 - i'm not going to go through the trouble
560:05 - of computing
560:06 - expected values using that
560:09 - summation formula using like x times p
560:13 - of x the sum over all x with p of x is
560:15 - not zero
560:16 - i'm not gonna bother with that anymore
560:18 - i'm just going to tell you what the
560:19 - expected value for this random variable
560:20 - is
560:21 - it's np no it's just np
560:24 - it's np i was jumping ahead of my head
560:27 - to the variance
560:28 - the variance of this random variable x
560:30 - is equal to n
560:31 - times p times 1 minus p and
560:35 - the standard deviation of x
560:38 - is just the square root of the variance
560:44 - going to draw your attention to
560:45 - something one way you can view
560:48 - binomial random variables is as the sum
560:52 - of bernoulli random variables in fact
560:55 - you could probably play around oops i
560:58 - didn't want to race
561:01 - have a look at this probably mass
561:02 - function formula
561:05 - and show for me
561:09 - that if you choose an n equal to one
561:14 - the resulting probability mass function
561:16 - is the is the probably mass function of
561:18 - the bernoulli random variable
561:20 - that is in fact the case um so
561:23 - uh i so that's that's something to look
561:25 - into
561:27 - uh but
561:30 - okay i'm saying that binomial random
561:33 - variables
561:34 - are the sum of n bernoulli random
561:36 - variables the expected value of us
561:38 - oh hold on and independent bernoulli
561:41 - random variables
561:42 - that's critical
561:46 - if that's the case remember
561:49 - that the expected value of a bernoulli
561:51 - random variable was p
561:53 - and the expected value of a binomial
561:55 - random variable
561:56 - is n times p
562:00 - so you're saying in a sense that we add
562:03 - up p
562:04 - n times to get the expected value
562:07 - hmm intriguing and
562:10 - actually remember that the variance of a
562:12 - bernoulli random variable
562:14 - was p times 1 minus p well now we're
562:16 - adding up n of those and we get
562:18 - np1 minus p for the variance
562:23 - intriguing so
562:26 - that is something to notice and also
562:29 - these
562:29 - expected value well i don't know
562:31 - necessarily about the variance being
562:33 - something very easily interpreted but
562:34 - the expected value certainly is
562:36 - it's saying that if there's a
562:37 - probability of a success happening let's
562:40 - say that it's a
562:41 - let's say that the probably success is
562:43 - 0.1
562:44 - and you do this experiment 10 times then
562:46 - you expect to see one success in your
562:48 - sample
562:49 - or if you do this experiment 100 times
562:50 - you expect to see 10 successes in your
562:52 - in your sample so it's actually a rather
562:57 - easily interpreted quantity this uh n
562:59 - times p
563:00 - quantity okay
563:03 - and i mention here that select values
563:08 - of bx and p are given in table 8.1 of
563:10 - the textbook but
563:12 - in this video i'm just going to use r
563:17 - okay uh moving on you flip
563:20 - a fair coin ten times all right
563:24 - so we should start filling out with
563:26 - numbers you flip a fair coin ten times
563:29 - there's going to be a binomial random
563:31 - variable showing up
563:32 - so fair coin suggests that the p
563:35 - parameter of this binomial random
563:36 - variable
563:37 - is going to be one half presumably what
563:39 - we're doing is counting the number of
563:41 - heads
563:41 - and if we're counting the number of
563:43 - heads then the resulting
563:44 - random variable is binomial and the end
563:48 - parameter of that binomial random
563:49 - variable will be 10.
563:51 - all right what is the probability you
563:53 - see exactly four heads
563:54 - do so without using a table the
563:58 - probability
563:59 - that this random variable x which is
564:02 - following a binomial
564:04 - binomial distribution with parameters
564:07 - n is 10 and p is one half
564:12 - so the probability that x is equal to 4
564:18 - is going to be 10 choose 4
564:24 - one-half to the power 4 and
564:28 - one half to the power ten minus four
564:33 - okay you're probably noticing well okay
564:36 - we got one half to the power
564:37 - four one half to the power ten minus
564:39 - four so that's the same as
564:41 - 10 choose four one half
564:44 - to the power ten and that's just
564:47 - basically because
564:49 - one half is equal to one minus one half
564:51 - so maybe i should write one minus one
564:52 - half to be a little bit more clear
564:55 - uh like that that that's the reason why
564:58 - but if he had instead
564:59 - instead of one half we said the
565:01 - probability of getting heads is point
565:02 - one
565:04 - then this would be the then
565:07 - thinking about the problems function
565:09 - this way would have been more correct
565:12 - or would have been correct it's not more
565:14 - correct because the other one is
565:15 - incorrect
565:16 - all right so uh
565:20 - actually we're going to compute this
565:21 - thing by hand so we're
565:24 - going to say that 10 choose 4
565:28 - is 10 factorial divided by
565:32 - 4 factorial times 6 factorial
565:36 - and we have 1 over 2 to the power
565:40 - 10 that's one half raised to the power
565:42 - 10
565:43 - which is equal to 10 factorial
565:46 - divided by 4 factorial times 6 factorial
565:48 - we've got 10
565:50 - times nine times
565:53 - eight times seven
565:57 - over four times three
566:00 - times two times one and then this is all
566:04 - multiplied by one half to the power
566:05 - 10 and the 4
566:09 - and the 2 will cancel out with the 8 and
566:11 - the 3 will cancel out with a 9
566:14 - leaving us a 3 so that gets us
566:18 - uh 210
566:23 - over 1024 that's the tenth power of two
566:29 - uh which is equal to since there is a
566:31 - two in common
566:32 - uh 105 over 512
566:37 - which is approximately equal to 0.2
566:43 - okay so that's the answer to that one if
566:46 - x follows a binomial distribution with
566:48 - parameters 10 and 0.5 compute the
566:49 - probability that 4 is less than x which
566:51 - is less than or equal to 6.
566:54 - so we've actually got a couple ways we
566:56 - could do this
566:58 - let's do this without the table the
567:01 - probability
567:03 - that 4 is less than
567:07 - x which is less than or equal to 6
567:10 - is equal to
567:14 - the probably mass function at five
567:16 - because you don't include four
567:21 - plus the probably mass function at six
567:29 - which is equal to uh
567:33 - uh it's going to be 10 choose
567:36 - five and we know we're just going to end
567:38 - up with one half to the power 10
567:41 - in the end if in general if
567:45 - uh our parameter were not one half we
567:47 - should probably
567:49 - we should probably reason this way
567:54 - so we've got 10 choose five plus
567:58 - ten choose six i guess we switch to
568:01 - green
568:02 - uh one half to the power ten
568:06 - and that means what we need to compute
568:07 - now is
568:10 - ten choose five and ten two six we
568:12 - already know that one half to the power
568:13 - of the
568:14 - ten is uh uh one over a thousand twenty
568:17 - four
568:20 - so ten choose five that's going to be uh
568:23 - ten times nine times 8
568:27 - times 7 times 6
568:31 - divided by 5
568:34 - times 4 times 3
568:37 - times two times one
568:42 - and ten to six
568:46 - is equal to two hundred and ten
568:51 - and that's because 10 choose 6 is equal
568:54 - to 10 choose 4.
568:56 - i'm going to leave it up to you to
568:57 - figure that out i believe that was a
568:59 - problem in the exercise set but
569:02 - yeah that's the thing so now what we
569:04 - need to figure out is 10 choose 5.
569:06 - so we've got the 5 and the 2 canceling
569:09 - out with the 10 the 3 canceling out with
569:11 - the 9 reducing it to 3
569:13 - uh and the 4 canceling out with the 8
569:15 - reducing it to 2.
569:18 - so we've got in the numerator
569:21 - 3 times 2 times 7 times 6 and 3 times 2
569:24 - times 7 times 6
569:26 - is uh 252
569:30 - i believe yeah so it's going to be
569:34 - yeah that's 252. so this quantity
569:38 - evaluates to 252.
569:44 - so we will get for their
569:48 - our probability uh 252 plus 210
569:53 - which is uh
569:56 - which is going to be
569:59 - 462 divided by 1024.
570:04 - which that is
570:08 - that's around point uh five
570:13 - after you do some rounding uh we can
570:15 - also do some reducing of that fraction
570:17 - too
570:19 - now that said there was an alternative
570:21 - way we could have computed this quantity
570:23 - which was to say that this is equal to
570:27 - the cdf at six
570:33 - minus the cdf at
570:37 - 4.
570:41 - and then what's left what's left to do
570:44 - is get the cdf
570:46 - at six and four all right
570:51 - well let's get that
570:54 - i'm gonna have to boot up an r session
571:02 - all right so we've got p binom
571:05 - that's the function that is responsible
571:08 - for working with binomial random
571:09 - variables
571:10 - and we're going to give p bino what are
571:14 - we going to give it
571:16 - uh right so we're going to give it
571:19 - six our other parameters are size
571:22 - that's 10 and prob is equal to 0.5
571:28 - minus p by nom which is going
571:32 - at 4 size equals
571:35 - 10 prob equals 0.5
571:39 - yeah about 0.45 which rounds to about
571:42 - 0.5
571:43 - which for what it's worth i said it was
571:46 - approximately 0.5
571:47 - when we were doing stuff by hand and
571:50 - that was because we were rounding
571:54 - i knew i was rounding when i was uh when
571:56 - i was
571:57 - when i was saying that's about 0.5
572:00 - so
572:04 - uh well let's go ahead and write down
572:06 - that more exam
572:07 - exact answer say that this is
572:11 - approximately 0.45
572:17 - okay next example
572:21 - compute the probability that 2 is less
572:22 - than or equal to x which is less than or
572:24 - equal to four
572:25 - we could do this by summing up over the
572:27 - probability mass function but now i
572:28 - really don't want to do that
572:30 - i'm just i mean i've got better
572:33 - ways to spend my time so i'm going to
572:37 - instead say that this is equal to
572:41 - the cdf at
572:44 - 4
572:51 - minus the cdf at
572:54 - uh two minus one
572:59 - remember we have to do the two minus one
573:01 - because we need to include the two in
573:02 - our region
573:03 - and the only way to do that is if we do
573:05 - two minus one
573:07 - okay and the other parameters are ten
573:10 - and one half
573:12 - and and you guys know what two minus one
573:15 - is this is equal to one
573:16 - which is going to be well
573:23 - let's compute this so
573:26 - p binom at
573:30 - 1 minus p by nom
573:34 - or other way around actually so we got
573:36 - four
573:39 - all right so 0.366
573:44 - so approximately equal to 0.36
573:48 - six what is the probability that you see
573:53 - more than seven heads strictly more so
573:55 - this is the probability
573:57 - that x is greater than
574:01 - seven that is
574:04 - well the converse event or the uh
574:08 - um the complementary event to the x
574:11 - being greater than seven
574:13 - is x is less than or equal to seven
574:16 - so this is going to be one minus the
574:19 - probability
574:21 - that x is less than or equal to seven
574:26 - since remember the probability of a
574:28 - complement is 1 minus the probability of
574:30 - a
574:31 - the complement of the set x is smaller
574:33 - than 7 is x is less than or equal to 7.
574:37 - so then we get that formula uh so this
574:40 - is going to be
574:42 - uh oops
574:46 - this is equal to one minus a cdf
574:49 - at seven with parameters ten
574:52 - and one half
574:56 - and now we need to compute that and
574:58 - we're going to
574:59 - turn to r for that so this is 1
575:03 - minus the cdf
575:07 - at 7 size equals 10
575:11 - prob equals 0.5
575:14 - so the probability is about 0.05
575:17 - let's say 0.055
575:20 - so this is approximately equal to
575:28 - .055
575:29 - now i should probably mention something
575:31 - else about how the software was working
575:33 - we could have done
575:34 - instead p by nom
575:37 - 7 size equals 10
575:40 - prob equals 0.5
575:43 - and there's an additional parameter that
575:45 - all of these uh
575:47 - p functions have which is lower dot tail
575:51 - lower dot tail let's set that equal to
575:53 - false
575:56 - that got us the same thing uh
576:01 - the p so basically these p bottom
576:03 - functions by default they're giving you
576:05 - the cdf but they can also give you one
576:06 - minus the cdf if you
576:07 - set lower tail equals false
576:11 - if you were to ask the developers for
576:13 - these functions they would say
576:14 - that rather than doing one minus the
576:18 - the cdf or one minus p binom or whatever
576:22 - you should use the lower tail equals
576:24 - false parameter the reason being that
576:27 - you're going to get less numerical error
576:29 - if you're using the lower tail equals
576:30 - false parameter
576:33 - because numerical error is very much a
576:35 - thing like we care
576:36 - whenever whenever we're using software
576:38 - to compute numbers we care about
576:39 - numerical error
576:41 - and it turns out that setting lower
576:42 - table tail equals false
576:44 - that results in less numerical error uh
576:47 - i
576:47 - i don't really know why i'm guessing
576:49 - it's because they can do some more
576:50 - optimizations or some other fancy
576:52 - uh numerical tricks thing is
576:56 - as an instructor i like to make sure
576:59 - that people are thinking like
577:01 - this expression or this relationship
577:05 - i really want students to understand
577:07 - that and it's very easy to just
577:10 - lose that under the easiness of this
577:13 - of this um of this uh of this function
577:17 - in this notation
577:19 - so i don't know how frequently i'm going
577:20 - to do that um
577:22 - and also for for whatever it's worth
577:24 - sometimes when i'm
577:25 - writing my own uh functions for
577:28 - probability
577:29 - i don't always include that parameter
577:30 - myself just because i can't i don't have
577:33 - it i know i don't really know what the
577:34 - developers are doing
577:35 - to make sure that lower tail goes false
577:38 - gives you more accurate answers
577:41 - uh so also by the way if
577:44 - if you're watching this in the future
577:46 - hello future person
577:47 - if you're watching this in the future
577:49 - and you're using a table because
577:51 - maybe because i told you to uh you're
577:54 - using a table for these calculations
577:56 - you don't have access to the probably
577:58 - that x is greater than seven you only
577:59 - have
578:00 - access to the probability that x is less
578:01 - than or equal to seven
578:03 - so being aware of um
578:07 - how this stuff is working uh or at least
578:10 - being aware of this
578:11 - relationship that i've underlined in
578:12 - blue still matters a great deal
578:16 - all right so continuing
578:20 - on compute the expected value of
578:24 - x the variance of x and the standard
578:26 - deviation
578:28 - of x
578:32 - all right so the expected value
578:36 - of x
578:40 - let's see all right the expected value
578:43 - of
578:43 - x is n times p
578:47 - which is 10 times one half
578:53 - which is equal to five all right simple
578:55 - enough
578:56 - the variance of x
579:00 - is n times p times 1 minus p
579:07 - which is 5 times one half
579:12 - which is five halves or two point five
579:17 - the standard deviation of x is going to
579:20 - be the square root
579:22 - of five halves and i don't know what
579:24 - that is off the top of my head
579:26 - so we can just leave it like that that's
579:28 - fine
579:29 - all right uh our functions that are
579:32 - doing this stuff well i think we just
579:34 - did it so some of this is completely
579:35 - redundant
579:37 - um
579:40 - here i actually created a random
579:42 - variable
579:44 - using discrete rv which presumably is
579:47 - loaded up
579:48 - uh so i create a random variable x to
579:50 - represent the x that we were talking
579:51 - about before
579:52 - and computing's expected value variance
579:54 - and standard deviation
579:55 - these are basically the same as what i
579:57 - had before i also plotted its
579:59 - probability mass function this is what
580:00 - it's probably a mass function looks like
580:04 - okay next example your
580:07 - manufacturer of widgets sends
580:10 - batches of witches and giant bins your
580:13 - company will accept a shipment of widget
580:15 - of widgets if no more than seven percent
580:18 - of widgets are
580:19 - defective the procedure for deciding
580:22 - whether a shipment is defective is to
580:23 - choose
580:23 - four widgets from the batch at random
580:25 - without replacement
580:26 - if more than one widget is defective the
580:28 - batch is rejected
580:30 - what's the probability of rejecting the
580:31 - batch if seven percent of widgets are
580:32 - defective
580:33 - model the process using a binomial
580:35 - random variable
580:37 - so we have so we're going to assume that
580:41 - there are in fact seven percent of
580:42 - widgets and actually
580:44 - the argument being used in this problem
580:47 - uh this problem is kind of suggesting
580:49 - the possibility
580:51 - well actually the procedure being
580:54 - described in this problem is basically a
580:56 - hypothesis test
580:58 - and uh we're gonna talk more about
581:02 - hypothesis testing later
581:04 - in a later chapter uh but basically what
581:08 - you do
581:08 - when you're working on the mathematics
581:10 - of a hypothesis test
581:12 - for computing p-values and all that
581:13 - stuff you assume that the null
581:15 - hypothesis is true
581:17 - so the null hypothesis is that seven
581:19 - percent of the widgets are defective
581:21 - so um uh so in this case we assume that
581:25 - seven percent of widgets are defective
581:27 - in which case the distribution
581:29 - of the random variable x is going to be
581:32 - let's see how many widgets did they pull
581:34 - out for
581:36 - uh so n is equal to
581:40 - four and p is equal to 0.07
581:48 - okay so also there's another wrinkle
581:52 - here
581:54 - the bin of widgets has a finite number
581:58 - of bims
582:00 - uh no no no not not fighting overpins
582:02 - has a finite number of widgets
582:05 - but this binomial random variable
582:09 - is not supposed to work in that
582:10 - situation
582:13 - see if there's a finite number
582:16 - of successes in this possible
582:19 - in this finite population then an
582:23 - implication of that is that
582:26 - you don't have independent successes and
582:29 - failures
582:30 - because if you pull a success out of the
582:33 - population
582:34 - you cannot pull that success again and
582:36 - presumably in this example when a
582:38 - when this uh when you're checking the
582:41 - widgets
582:42 - uh you pull out a widget you check it
582:44 - but you don't put it back in the bin
582:46 - it's for you to draw again no one ever
582:48 - does that
582:50 - so actually we don't have independent
582:55 - bernoulli trials
582:58 - so we don't have a sum of independent
583:00 - bernoullis we don't have independent
583:02 - trials
583:03 - which means that technically this random
583:05 - variable should not be
583:06 - a binomial random variable actually the
583:09 - random variable
583:10 - that is more accurate for this context
583:14 - is what's known as a hypergeometric
583:16 - random variable which we'll talk about
583:18 - in a later section
583:19 - thing though is if the sample if the
583:21 - population is large enough
583:24 - relative to how many successes there are
583:27 - in the sample like if there's a million
583:29 - widgets and seven percent of those
583:31 - widgets
583:32 - are defective then that means that about
583:36 - 70 000 defective widgets exist in the
583:39 - sample
583:40 - or in the population my my apologies uh
583:43 - in which case it the the numbers are so
583:46 - large that basically you can treat this
583:48 - as
583:49 - a binomial experiment anyway
583:53 - because the difference between the
583:55 - binomial
583:57 - random variable and this
584:00 - more accurate random variable the
584:02 - hypergeometric random variable those
584:04 - differences are negligible so
584:07 - you so you you get to you get to cheat
584:10 - you get to use the
584:11 - simpler binomial random variable as
584:14 - opposed to the
584:15 - more complicated more complicated
584:17 - hypergeometric
584:18 - okay
584:21 - so let's carry on then
584:24 - they want to know what is the
584:26 - probability of rejecting the batch if
584:28 - some percent are defective
584:30 - uh if more than one widget is defective
584:32 - okay
584:33 - with this problem we actually need to
584:35 - translate out because this is a word
584:37 - problem and by the way in stats classes
584:39 - i
584:39 - absolutely love to ask word problems
584:42 - so many word problems because statistics
584:46 - is so
584:46 - applied that it just feels inappropriate
584:49 - to
584:50 - not be asking word problems um it's
584:53 - it's such an applied subject that you
584:55 - have to be asking them
584:56 - okay so what corresponds to rejecting
584:59 - the batch you reject
585:00 - the batch if there's more than one
585:03 - defective widget
585:05 - in your sample so that's it so more than
585:08 - one that means greater than one
585:10 - and our random variable for tracking the
585:12 - number of defective widgets that we
585:14 - found
585:14 - is x and i guess all right a student
585:17 - might find it wait a minute wait a
585:19 - minute you're calling it defective
585:20 - widget
585:20 - a success yes it's mostly for the
585:24 - language
585:25 - all right so this is the probability we
585:27 - want to compute we want to compute the
585:29 - probability that x is greater than one
585:31 - which is equal to one minus the
585:33 - probability
585:34 - that x is less than or equal to one and
585:37 - at this point you could say all right
585:38 - let's go to r
585:39 - and compute this and i'm gonna say no
585:41 - we're not going to do that
585:42 - we're going to instead compute this
585:44 - thing by hand
585:46 - and say that this is going to be the pmf
585:50 - at uh 0 for
585:53 - parameters 4 and 0.07 plus the
585:56 - probability mass function
585:58 - at 1.
586:01 - oh i'm sorry i'm sorry uh i need to say
586:06 - that this is equal to 1 minus
586:08 - parentheses
586:09 - all that stuff okay
586:15 - all right there we go that's correct
586:19 - and now we need to compute each of those
586:20 - probability mass functions
586:23 - okay so the probably mass function at
586:26 - zero
586:31 - is going to be we've got
586:34 - four choose zero and four choose zero
586:38 - is one there's only one way to choose
586:42 - uh none of the four things
586:45 - in your in your little group and that's
586:47 - to choose all the one
586:48 - all the other ones there's only one way
586:50 - to do it so four to zero is one
586:52 - uh next up we've got uh what what have
586:56 - we got
586:56 - um oh yeah uh so .07
587:01 - to the zeroth power and .93
587:05 - to the fourth power b
587:09 - one for .07
587:14 - is equal to four choose one which
587:17 - hopefully you know
587:18 - is four you can at least reason about i
587:20 - was like okay how many ways are there to
587:22 - pick one thing out of four
587:23 - we'll pick one of those four things all
587:25 - right there we go um
587:27 - so there we go uh so then we got .07
587:31 - to the first power .93
587:34 - to the third power and
587:37 - after this you go to a calculator to
587:39 - compute those numbers
587:41 - i think i actually computed them
587:43 - [Music]
587:44 - rather accurately in my notes
587:50 - so let's see what did
587:55 - i have
588:00 - okay so what i'm seeing here
588:06 - is that this is equal to
588:11 - so the so the top one is equal to point
588:13 - seven
588:14 - four
588:18 - that is an exact number i got
588:21 - a little carried away with the accuracy
588:23 - and the second one is point two two
588:26 - five two two
588:31 - and that means that we're going to have
588:35 - that this quantity here
588:39 - that ultimately is what we're trying to
588:40 - compute
588:42 - is equal to 1 minus
588:50 - 0.748052
588:52 - plus 0.22522
588:56 - which is equal to 1 minus 0.973272
589:04 - 3272 which is equal to
589:10 - 0.026 seven
589:12 - two eight
589:15 - all right
589:18 - uh next up oh yeah there it is
589:22 - excuse me
589:30 - my apologies i had to sneeze okay uh
589:33 - next example uh this one's fun
589:37 - i claim that i can make 80 of my free
589:39 - throw shots when playing basketball
589:41 - you plan to test me by having me shoot
589:43 - 20 baskets if i make fewer baskets in a
589:45 - specified amount
589:47 - you will call me a liar the threshold
589:49 - amount of baskets is chosen so that the
589:51 - probably make less than this amount
589:53 - given that i am in fact an 80 free throw
589:55 - shooter does not exceed five percent
589:57 - what is the threshold amount all right
590:00 - uh oh yeah additionally compute the mean
590:01 - and standard deviation of the number of
590:03 - shots i would make if my claim is true
590:04 - okay uh let's do this let's do the
590:06 - second part first because that's easier
590:08 - uh the first part is going to require a
590:10 - conversation
590:12 - so the expected value will say um
590:15 - s is following a binomial distribution
590:18 - uh with uh parameters i'm going to shoot
590:22 - 20 baskets yeah so n is 20 and
590:25 - p is 0.8
590:29 - because all right this is again a
590:32 - hypothesis testing type
590:33 - problem in which case you're assuming
590:36 - that i am in fact an 80 free throw
590:39 - shooter
590:41 - so the expected value of not x because i
590:44 - decided not to do x
590:45 - um of s is going to be
590:48 - 0.8 times 20
590:52 - which is equal to 16.
590:56 - so you expect me to make 16 of my
590:58 - baskets
590:59 - uh the did i ask for standard deviation
591:02 - yes i did so
591:03 - the variance of this random variable
591:08 - is going to be uh 20 times
591:11 - 0.8 times 0.2
591:16 - which is equal to 3.2
591:23 - and the standard deviation
591:27 - of s is equal to the square root of 3.2
591:32 - which i don't know off the top of my
591:33 - head and i i'm i'm just
591:35 - i'm just not gonna bother okay
591:39 - all right so that was the easy part now
591:42 - for the hard part
591:44 - uh i have asked
591:47 - that you pick a threshold amount
591:51 - of baskets so that the probability i
591:54 - make
591:54 - less than this amount given that i am in
591:56 - fact an 80 free throw shooter
591:58 - does not exceed five percent all right
592:02 - so we will call this threshold amount we
592:04 - will give it a name
592:06 - uh the threshold amount we will call
592:09 - this
592:09 - we will call this quantity k all right
592:13 - i'm asking that i i'm asking you to find
592:18 - a k
592:20 - such that the probability that s is
592:23 - let's see
592:23 - so if i make fewer baskets so s
592:26 - is less than k
592:31 - this number needs to be
592:35 - at most 5 so this needs to be
592:38 - at least 0.05
592:42 - well at most 0.05 and
592:45 - actually there's going to be a number of
592:47 - possible k's
592:49 - such that it's less than 0.05 but we're
592:50 - going to say that this is the largest
592:52 - possible k
592:54 - uh such that such that this probability
592:56 - is less than
592:58 - or equal to 0.05 uh k is a constant here
593:01 - we just don't know what it is
593:03 - uh s is a random variable uh let's go
593:06 - ahead and play around with this
593:08 - expression some more
593:10 - before continuing on this is saying that
593:13 - the probability
593:15 - that s is less than or equal to k minus
593:18 - 1
593:23 - uh less than or equal to k minus one
593:29 - that's going to be less than or equal to
593:30 - 0.05
593:33 - all right so we actually so by doing
593:35 - that i have
593:37 - the cdf on the left hand side of the
593:39 - inequality
593:41 - and 0.05 on the right hand side so we're
593:43 - go what we need to do now
593:44 - is basically a reverse lookup we're
593:48 - looking up
593:48 - a number such that the cdf
593:53 - is um less than or equal to 0.05
593:57 - the largest number possible such that
593:59 - the cdf is less or equal 0.05
594:02 - this is similar to the notion of
594:04 - quantile
594:06 - because a quo so you have a probability
594:08 - that a random variable
594:10 - is less than or equal to uh let's say
594:13 - 10 percent then the 10 quantile is that
594:16 - number
594:18 - so this is actually related to the
594:20 - notion of quantiles the unfortunate
594:21 - thing though is that what we're talking
594:22 - about
594:22 - are discrete random variables which
594:26 - adds this complication in that it's
594:28 - possible that the cdf in fact not just
594:30 - possible
594:31 - it's quite likely that the cdf never
594:34 - actually equals 0.05
594:36 - if it were in fact equal for the cdf to
594:38 - equal 0.05
594:39 - then we would say all right pick a k
594:42 - minus 1
594:42 - such that cdf is equal to 0.05 the only
594:46 - thing though is that's not actually
594:47 - likely to be the case
594:49 - um it's likely that the cdf never
594:52 - actually reaches 0.05
594:53 - in fact let's now go to r
594:58 - so let's look at so we got uh the cdf
595:04 - the cdf is
595:09 - so possible values for the standard
595:11 - variable are from 0 to 20.
595:13 - so we're going to ask for the cdf's
595:15 - values from
595:16 - for all numbers between 0 and 20. size
595:19 - is equal to 20
595:20 - prob is equal to 0.8
595:24 - ah that's it's not very helpful
595:27 - um all right i think that if we were
595:30 - looking at
595:31 - the textbook we would be
595:34 - rounding to uh three decimal places
595:38 - so uh so digits
595:42 - equals three
595:46 - all right yeah this is yeah we're gonna
595:48 - do this just because
595:51 - like what we have up here is scientific
595:52 - notation which may be more accurate but
595:56 - uh also it's hard to read so we're gonna
595:58 - round to three decimal places and work
596:00 - with this
596:01 - so this is the cdf uh and what we're
596:05 - looking for
596:06 - is uh we would go let's actually
596:09 - let's actually call give this vector
596:12 - a name so we'll call it cdf
596:16 - and we'll say names cdf
596:20 - will be 0 to 20.
596:23 - so now let's print out cdf okay
596:27 - all right so so so so so
596:30 - so um
596:31 - [Music]
596:34 - what we're looking for in this is a qua
596:37 - is a quantity
596:38 - where the cdf uh
596:42 - does so that so notice that the cdf is
596:44 - increasing as we
596:45 - increase the input to the cdf right
596:49 - our cdf is increasing but and we want to
596:52 - find the largest number
596:54 - that we can put into the cdf such that
596:56 - it doesn't exceed 0.05
596:58 - and that number is 12 because at 12 the
597:02 - cdf is going to be 0.032
597:04 - and at 13 the cdf is going to be 0.087
597:07 - so it crosses that threshold at 12
597:09 - suggesting
597:11 - the that k minus 1
597:16 - suggesting that k minus 1
597:20 - equals 12. and then we add 1 to both
597:24 - sides
597:25 - to suggest that k is equal to 13 that is
597:30 - if i score less than if i make less than
597:32 - 13 baskets you're going to call me a
597:34 - liar
597:35 - uh what is uh 13 divided by 20
597:38 - that is uh 0.6 so
597:41 - that's 0.65 so if i make uh 65
597:45 - or less than if i make less than 65 of
597:48 - my basket you're going to call me a liar
597:49 - the principle being
597:51 - the the logic being that that is such a
597:53 - rare amount
597:55 - it's so unlikely for you to score less
597:58 - than 60
597:59 - 65 of your baskets if you were in fact
598:01 - an eighty percent three
598:02 - free throw shooter that we're actually
598:05 - gonna say it's more likely that you're
598:06 - lying
598:07 - then you're actually telling the truth
598:10 - or at least it seems
598:12 - it seems unreasonable to continue to
598:14 - believe
598:15 - that you are still telling the truth so
598:18 - this is this so so that's how you would
598:20 - use like the cdf if we were
598:23 - if we if you were using the textbook you
598:24 - would use the cdf this way
598:26 - you would scan the cdf uh
598:29 - for some reason my mouse stopped working
598:31 - i don't know why this is a super cheap
598:33 - computer
598:34 - you would scan the cdf until eventually
598:38 - you crossed over that 0.05
598:40 - threshold and then take whatever this
598:43 - what
598:44 - take whatever number got the cdf2 uh
598:46 - just below that
598:47 - threshold so if i had switched this to
598:49 - uh say all right it needs to be less
598:51 - than or equal to 0.10
598:54 - then we would go to 13 we haven't
598:56 - crossed point 10 yet
598:57 - we would go to 14 but then we then we
598:59 - would cross so we would say all right
599:01 - the threshold amount is 13. uh let's say
599:04 - that i said
599:04 - instead 0.01 uh actually 11 is probably
599:08 - not what you would choose because
599:10 - you know that we're rounding here so you
599:12 - would go with 10.
599:14 - so because you know that actually well i
599:17 - don't know
599:18 - what what was a 11 so if we look back at
599:21 - the
599:22 - original vector that's a bit more
599:24 - accurate that doesn't have any rounding
599:26 - so one two three four five
599:29 - six seven eight
599:33 - nine ten eleven
599:40 - uh
599:42 - next one oh actually it's rounding up
599:45 - so you could choose 11. yeah 11 would be
599:48 - fine
599:49 - so yeah it doesn't always round down
599:53 - uh so that's what you would do you would
599:55 - just kind of reverse look
599:57 - from the uh the uh from the cdf
600:01 - especially if you're using the book or
600:04 - you could use q binom uh
600:07 - we'll put in 0.05 size equals 20
600:12 - prob equals 0.8 you could have used uh
600:16 - okay what exactly is cubinom doing
600:20 - uh it might be doing something else it
600:22 - might be saying okay you exceed it
600:24 - because so we actually might need to do
600:26 - q by
600:27 - minus one uh let's look at the
600:29 - documentation for q
600:30 - by nom
600:35 - oh i think you just yeah you would just
600:38 - have to
600:39 - recognize all right so uh it's going to
600:42 - give you some details
600:43 - somewhere okay so it says right here
600:47 - the quantile is defined as the smallest
600:49 - value x such that f of x is greater than
600:51 - or equal to p
600:52 - which is actually different from how i
600:55 - just defined it so r's definition of
600:56 - what a quantile is for discrete random
600:58 - variables
600:59 - is uh different than what i just said so
601:03 - being aware of that you would actually
601:05 - have to take
601:07 - uh whatever q binong gave you and then
601:09 - do
601:10 - minus one
601:13 - to get the right answer
601:16 - okay but all right
601:20 - or i mean i
601:23 - i don't know i think there's so many
601:26 - different ways
601:27 - to possibly think about it and say okay
601:29 - the q binom that
601:30 - that r actually has is uh
601:34 - effectively uh giving you this quantity
601:37 - right away
601:38 - so you don't have to work with the cdf
601:40 - you that's another way you could
601:42 - possibly think about it
601:45 - all right so uh by the way what we're
601:47 - basically saying is that if you ended up
601:49 - shooting a number of baskets
601:52 - such that you ended up in uh 11 12
601:55 - such that you ended up in this region
601:58 - it's so unlikely
601:59 - if you were in fact an 80 free throw
602:01 - sure to end up in this region
602:03 - i'm justified in saying you're a liar
602:06 - how unlikely is it well it's actually uh
602:09 - 3.2 percent
602:11 - uh it's so so so unlikely that we would
602:14 - just call you a liar
602:15 - because if you were in fact a three
602:17 - throw shooter you should probably be
602:18 - ending up in the other region
602:20 - and 80 free throw shooter my apologies
602:22 - i'm always confusing my words
602:24 - okay so that's it for uh the binomial
602:28 - random variable and
602:30 - in the next section we will be talking
602:31 - about the hypergeometric and negative
602:33 - binomial distributions
602:35 - all right so i will see you there
602:44 - uh we are now on to the last section
602:48 - of this chapter discussing the poisson
602:52 - process and the poisson probability
602:54 - distribution
602:55 - so x is said to follow a poisson poisson
602:59 - distribution with
603:00 - parameter mu or if
603:03 - the probability mass function of x is
603:06 - given by
603:07 - p of x parameterized by
603:10 - mu this is equal to
603:15 - e to the power negative mu mu to the
603:18 - power
603:19 - x over x factorial
603:23 - for uh x being
603:27 - a member of the set 0 1
603:30 - 2 and so on so in other words if if x is
603:34 - a whole number
603:34 - if x is a whole number then this is the
603:36 - probably mass function otherwise it's
603:37 - zero
603:39 - so the first question you may ask is
603:41 - this a valid pmf
603:42 - the answer is yes and why is that well
603:45 - let's sum
603:46 - the probably mass function from x uh
603:48 - sorry
603:49 - from x uh equals
603:53 - zero to infinity we got p
603:56 - of x parameterized by mu
604:00 - this is equal to the sum from x
604:04 - equals zero to infinity
604:08 - e negative mu mu
604:11 - to the x over x
604:14 - factorial the e to the negative mu part
604:18 - that is effectively a constant so that
604:19 - can be pulled out
604:21 - so we could say that this is e to the
604:22 - negative mu and then we have the sum
604:25 - from x equals zero to infinity
604:29 - mu to the x over x factorial
604:34 - and that part which i have highlighted
604:39 - in red that is e to the power mu
604:43 - y calculus 2. this is the
604:46 - if i remember right the taylor expansion
604:48 - of the function e to the power
604:50 - x so this is coming from calculus 2
604:54 - but yeah that evaluates to e to the mu
604:57 - hence we get e negative mu
605:02 - e to the power of mu and those are that
605:05 - equals one
605:07 - and furthermore this probably mass
605:09 - function is positive everywhere
605:11 - so we get this is a valid pmf
605:14 - so uh poison random variables if x is
605:17 - following a poisson
605:18 - distribution with prion or mu then the
605:19 - expected value of x is equal to mu
605:21 - and also the variance of x is equal to
605:23 - mu
605:25 - so that means that
605:28 - we are parameterizing poisson random
605:30 - variables by their mean
605:33 - so in uh
605:37 - in your book if you're using the uh
605:40 - the dvor book table 8.2 contains the cdf
605:44 - of select poisson random variables
605:46 - or select poisson distributions uh
605:50 - in r the functions that are responsible
605:53 - for
605:54 - handling points on random variables
605:55 - random variables sorry
605:57 - our deploys
606:10 - handling polymath function cdf
606:14 - quantiles and random and random variants
606:17 - so in other words creating
606:18 - random instances of poisson random
606:21 - variables
606:22 - and they will be parameterized by their
606:23 - mean
606:25 - so the poisson distribution describes uh
606:28 - random variables that follow the poisson
606:30 - process
606:31 - so here is the intuition of poisson
606:33 - random variables
606:35 - they are
606:39 - tracking how many times within some how
606:42 - often
606:43 - a quote-unquote rare event occurs within
606:46 - a finite span of time
606:49 - so and which isn't at all clear from
606:52 - looking at this probably mass function
606:54 - that would in fact be the interpretation
606:57 - uh this is more of a limiting result
606:59 - this
607:01 - footnote that i have right here actually
607:04 - gives a little bit more justification as
607:06 - to why this is the case
607:08 - but you can think of it as all right we
607:09 - have some rare event
607:11 - uh me like i have a favorite example
607:15 - that was
607:15 - used by my uh 3070 instructor maybe i'll
607:20 - just pull up his webpage for you uh like
607:23 - i
607:24 - he he's got some he's got some
607:26 - interesting stuff
607:28 - on his on his web page that even now
607:32 - i kind of look back to it and like i
607:34 - think about it and i just kind of
607:36 - i kind of want to update it i think i
607:39 - think it could be updated but there's
607:40 - some interesting stuff there genuinely
607:42 - so math.utah.edu
607:46 - tilde treyberg
607:51 - uh his name was uh andres trebergs
607:54 - a professor at the u not actually a
607:56 - probability
607:57 - uh professor that's not his uh area of
607:59 - expertise although he's taught
608:01 - math 70 a few times and math the
608:04 - no matt 3070 sorry and matt 3080 a few
608:07 - times and
608:08 - like he he's interested in statistics
608:10 - but he is a
608:12 - uh what what is that area of study of
608:16 - his
608:17 - i think it's like analytic geometry i
608:19 - think that's what he studies
608:21 - so i wonder what happens if i just go
608:23 - straight here we could probably find
608:26 - from here his uh 3070 page
608:32 - yes i think this is
608:35 - i think this is it
608:39 - math 3070 fall 2013.
608:43 - math 3070 fall 2012
608:46 - is the last that was the class that i
608:48 - took this was my
608:49 - web page this is where i went to find
608:52 - his stuff
608:53 - so yeah i i took matt 3070
608:56 - in this class i think i'm pretty sure
608:59 - that's true
609:00 - all right if i scroll down yeah the
609:03 - supplementary materials
609:06 - oh yeah so here's a simple r uh
609:10 - which by john farzani this is the
609:12 - uh
609:13 - lab textbook for the r lab
609:16 - and uh several example problems he's got
609:19 - this
609:20 - one let's find it
609:23 - let's see prussian no no press
609:28 - poise
609:30 - ah let's see
609:33 - is this it i think this is it
609:38 - i think this is it
609:42 - uh are there horse kicks
609:47 - are people getting kicked by horses
609:50 - that's what i want to know
609:52 - i think this is i think this is it um
609:59 - i don't think so
610:04 - i don't think this is the horse kick
610:05 - example but the horse kick example is
610:07 - fun
610:10 - let's see oh why i want to close that
610:12 - webpage
610:14 - uh
610:18 - if we try horse
610:21 - come on
610:26 - there it is there it is the horse kick
610:28 - example
610:30 - ah yeah i love this one
610:34 - yeah so so have a look at this but
610:35 - basically it turned out that in the
610:37 - prussian army
610:38 - um you can model the number of people
610:40 - who died from horse kicks
610:43 - with poison random variables that
610:46 - poisson random variables actually do a
610:48 - good job of modeling stuff like that
610:49 - but i also had as
610:53 - potential examples i'm not entirely sure
610:54 - how the how accurate this is
610:56 - but you could maybe like maybe think of
610:59 - uh
611:00 - number of calls that a call center is
611:02 - getting in a day or the number of points
611:04 - that scored by a team in a game
611:07 - uh poison random variables
611:11 - so and also as being a poisson process
611:15 - a poisson process uh
611:18 - is a bit more general than that so a
611:21 - poisson process
611:23 - is a stochastic process stochastic
611:26 - processes
611:27 - are not the subject of this class
611:31 - beyond this little description i think
611:34 - another thing that could be considered a
611:36 - poisson process
611:37 - or that could be modeled by a poisson
611:39 - random variable is let's say you have
611:40 - some radioactive matter
611:42 - and the number of uh
611:46 - is it atoms i'm not sure i'm not very
611:48 - good at physics but
611:50 - some some particles are leaving the
611:53 - radioactive mass and the amount of
611:54 - particles that are leaving the mass
611:56 - uh over some period of time can be
611:58 - understood as a poisson
612:01 - random variable and also as a poisson
612:03 - process
612:05 - so i have this r code that's meant to
612:09 - simulate a poisson process and with a
612:11 - poisson process
612:12 - the time at which a particle or
612:16 - the time at which uh the pro the process
612:18 - jumps is random
612:19 - so it will so there are going to be
612:22 - random times at which the process
612:24 - increases its value by one so
612:27 - uh it will jump so you'll wait for a
612:30 - period of time and then suddenly the
612:31 - process will jump and
612:32 - basically something has left and then
612:34 - you'll wait a little longer and
612:35 - something else has happened maybe you
612:36 - could imagine this as being
612:38 - uh points in a game maybe a basketball
612:40 - game although i
612:41 - you could probably see why it's a little
612:43 - bit inaccurate to view points in
612:45 - basketball
612:46 - game as a poisson process because
612:48 - there's
612:49 - no way that you could well i guess this
612:52 - is possible
612:53 - if maybe if maybe you said both teams
612:57 - uh if you're counting like total points
612:59 - scored by either team
613:00 - i don't know it seems unreasonable
613:02 - though but you could
613:04 - but basically at random times this
613:05 - process is going to jump
613:07 - uh by one and the value of the process
613:10 - is going to increase and basically
613:13 - any fixed time the value of the process
613:17 - at a fixed time
613:18 - uh so you fix it at two at two
613:22 - or something and the value of the
613:23 - process at that time
613:25 - can be modeled by a poisson random
613:27 - variable
613:29 - so yeah that's uh hopefully that gives
613:32 - you some idea of what they're what
613:33 - they're describing
613:35 - how often some event occurs over a fixed
613:38 - period of time
613:40 - which in principle that event could
613:43 - be unbounded there's an infinite number
613:46 - of pos
613:46 - it could happen like there's no upper
613:49 - bound on how many times this event could
613:51 - happen it can
613:51 - happen an infinite number of times
613:53 - there's well okay maybe not literally
613:55 - infinite but
613:56 - any large number of times it's highly
613:58 - unlikely that it will be very large but
614:00 - there is no upper bound
614:02 - right so you're not gonna restrict it
614:03 - like the number of points that you could
614:05 - score in a basketball game
614:07 - like there's no there's no limit on that
614:10 - no one will just end the game well i
614:12 - don't know maybe but
614:13 - that's that seems that seems rather
614:16 - academic but
614:17 - yeah that's so this is going to be a
614:19 - random variable
614:21 - that is
614:24 - defined
614:31 - hello everyone we are now on the next
614:34 - chapter
614:35 - on continuous random variables and
614:37 - probability distributions
614:39 - continuous probability models are
614:41 - another major class of probability
614:43 - models
614:44 - in the previous chapter we saw models
614:47 - for
614:47 - discrete random variables with discrete
614:50 - random variables
614:52 - there was either a finite number of
614:54 - possible numbers this random variable
614:56 - could take
614:56 - or it was countably infinite for example
614:59 - the whole numbers
615:01 - were were possible in this situation
615:05 - not only are the number of possibilities
615:08 - infinite
615:08 - they are uncountably infinite so
615:13 - this these uh probably models allow
615:16 - for any real number within some range it
615:20 - could be within a range a to b
615:22 - or z like a to infinity or zero to
615:25 - infinity
615:26 - or negative infinity to infinity so any
615:29 - real number
615:30 - any of those could be possibly taken by
615:33 - the
615:34 - random variable so
615:37 - as a consequence of this we're going to
615:40 - change some of the notions that we had
615:41 - with discrete random variables but
615:45 - not by much for example the probability
615:47 - mass function
615:49 - is going to be replaced with the
615:50 - probability density function which is
615:51 - what we're going to be talking about
615:52 - right now
615:54 - and the cdf is still defined
615:58 - the same way but it's going to be
616:01 - computed a little bit differently
616:02 - instead of it being
616:03 - involving a sum it's going to involve
616:07 - an integral and expectations are also
616:09 - going to involve integrals basically
616:11 - whenever you would add numbers with
616:13 - discrete random variables
616:15 - you integrate with continuous random
616:17 - variables
616:18 - so this is where your calculus knowledge
616:20 - is going to be
616:22 - tested one nice thing though
616:25 - about continuous random variables though
616:29 - is that when you're working with
616:31 - continuous random variables
616:32 - uh the probability that the that that
616:34 - random variable
616:36 - is equal to any particular number is
616:38 - always zero
616:40 - which is the reason why we have to have
616:42 - uh integration and density functions
616:45 - so the probability that x is less than x
616:47 - is the probability that x is less than
616:49 - or equal to x
616:50 - and admittedly it is a little strange
616:53 - that the probability that this random
616:54 - variable
616:54 - is equal to a particular number is zero
616:57 - it's
616:58 - it's somewhat strange because when you
617:01 - go to your random number generator and
617:03 - ask this thing to produce a number it
617:04 - gives you a number
617:05 - but the probability that you got that
617:06 - number was zero so
617:09 - events that are happening with probably
617:10 - zero are happening all the time whenever
617:13 - you're working with
617:14 - continuous random variables
617:17 - but the way ferrous rasulaga
617:21 - one of my probability instructors
617:24 - researcher at the university of utah
617:26 - very good mathematician uh one way he
617:29 - put it
617:30 - was uh
617:33 - the you know at some level that
617:36 - someone's going to win the lottery
617:38 - you just know it's not going to be you
617:41 - so you get the probably that you win the
617:42 - lottery is zero
617:44 - and the probably that anyone wins the
617:46 - lottery is not zero
617:48 - that's the way he put it so uh
617:51 - let's uh move on to discussing
617:53 - probability density functions
617:56 - so these are the analog to the
617:58 - probability mass function for discrete
617:59 - random variables
618:00 - the pdf is a non-net negative function
618:03 - which i'm calling f
618:04 - x such that for any two numbers a and b
618:08 - with a less than or equal to b the
618:09 - probability
618:11 - that a is less than or equal to x
618:14 - which is our random variable which is
618:16 - less than or equal to b
618:19 - is equal to the integral from
618:23 - a to b f of x
618:27 - dx
618:30 - naturally in order for f to be a valid
618:33 - pdf we must also have that the integral
618:36 - from no not a uh the integral from
618:40 - negative infinity
618:41 - to infinity of f of x
618:45 - dx well that's the probability
618:49 - that this random variable
618:52 - is between negative infinity and
618:54 - infinity
618:58 - and that's basically asking for what is
619:00 - the probability that this random
619:01 - variable
619:03 - is any real number we're basically
619:06 - asking what is the probability that x
619:08 - is a real number
619:12 - so naturally this must equal 1 because
619:15 - we know that
619:16 - x will be a random a real number
619:19 - and it will be finite
619:23 - so this is another relation we have to
619:24 - have now regarding this
619:26 - f itself might not be
619:31 - continuous everywhere and it's also
619:34 - possible that
619:35 - actually integrating from negative
619:37 - infinity to infinity is
619:40 - a bit much because actually this random
619:43 - variable is only
619:44 - positive on some on some
619:48 - interval of finite length and everywhere
619:51 - else it's zero
619:52 - so you're integrating for the most part
619:54 - zero and in fact we'll see
619:56 - one example of this
620:04 - one we are now discussing the
620:07 - ever famous maybe two famous sometimes
620:09 - infamous
620:10 - infamous not infamous that's not a word
620:13 - sometimes
620:14 - infamous normal distribution we
620:17 - say that a random variable x follows the
620:20 - normal distribution sometimes denoted
620:22 - like so
620:23 - x uh x follows a distribution n
620:27 - mu sigma so we say x follows a normal
620:30 - distribution with mean mu and standard
620:32 - deviation sigma
620:33 - if it has the pdf
620:37 - v of x parameterized by uh
620:42 - mu and sigma
620:45 - is equal to all right we should probably
620:48 - zoom in for this
620:50 - um it's a fairly
620:54 - a little complicated we have 1
620:57 - over the square root of 2
621:01 - pi sigma squared
621:06 - so this fraction multiplying with e
621:10 - to the power negative x
621:13 - minus mu squared divided by
621:17 - all in the power uh two
621:20 - sigma squared it's worth mentioning
621:24 - that often the normal distribution is
621:27 - parameterized not by its standard
621:29 - deviation but rather by
621:31 - uh its variance and you can kind of see
621:33 - why
621:34 - when you look when i've written this
621:36 - formula down yeah it's possible to take
621:38 - this sigma squared
621:40 - and pull it out in front of the square
621:42 - root
621:43 - uh but we can put it inside of the
621:46 - square root and then
621:47 - you have uh and then everything you've
621:51 - got sigma squares everywhere so you
621:52 - could just specify sigma squared
621:54 - directly
621:54 - and also it feels to statisticians to be
621:58 - or and probabilists my apologies to be
622:01 - more appropriate to parameterize by the
622:04 - variance rather than the standard
622:05 - deviation
622:06 - since it's often easier to work with the
622:07 - variance uh
622:09 - directly rather than the standard
622:10 - deviation
622:12 - and in addition to this
622:15 - you could say that parameterizing with
622:17 - the variance generalizes better when you
622:19 - start talking about
622:20 - multivariate versions of the normal
622:22 - distribution but this is fine for now
622:24 - like admittedly at at an
622:28 - introductory stats level it feels
622:31 - somewhat
622:31 - like since the standard deviation is the
622:33 - more uh
622:35 - natural measure of spread and the
622:36 - variance a little bit more alien
622:39 - you could argue that to these students
622:42 - it seems
622:44 - somewhat better to use the standard
622:45 - deviation rather than the variance but
622:47 - it's fine
622:48 - so this is the curve here is a sketch of
622:51 - the density curve
622:52 - for the normal distribution
622:56 - we have uh oops
622:59 - okay so here's kind of what it looks
623:02 - like
623:03 - uh we would have basically here is the
623:06 - mean
623:08 - here is the mean plus one standard
623:10 - deviation
623:12 - and here's the mean minus one standard
623:15 - deviation
623:17 - okay so we would have
623:20 - basically a curve that goes up there
623:25 - within one standard deviation is an
623:27 - inflection point
623:29 - so it will go from convex to concave
623:33 - at the inflection point and then
623:39 - it's going to be a symmetric curve
623:42 - and then go from concave back to convex
623:50 - all right and this is a
623:54 - simple sketch of what it may look like
623:57 - its peak occurs
624:02 - around the mean
624:06 - and we have our inflection points being
624:09 - within
624:10 - one standard deviation of the mean
624:15 - okay so that's a this is the
624:19 - whenever you hear the words the bell
624:20 - curve they are
624:22 - probably referring to the normal
624:24 - distribution
624:25 - be aware that the normal distribution is
624:28 - not
624:28 - the only bell-shaped curve that is used
624:32 - in probability and statistics
624:34 - there are other bell-shaped curves for
624:37 - it for instance there's the t
624:38 - distribution there's the koshi
624:40 - distribution
624:42 - i think i encountered a distribution
624:44 - recently
624:46 - that is meant to model uh
624:49 - when well okay it's not really a
624:51 - probability distribution though but
624:53 - it's kind of like one uh when someone
624:56 - gets the coronavir
624:57 - or no not cordovice when someone gets a
624:59 - virus
625:01 - during a pandemic type situation there's
625:04 - a curve for that that looks like the
625:05 - normal but isn't the normal
625:07 - i can't remember what it is though it's
625:08 - like the logistic curve i
625:10 - i don't know but yeah um
625:15 - uh this is usually when people are
625:17 - talking about the bell curve
625:19 - all in caps like capitalize and all that
625:22 - they're talking about the normal
625:23 - distribution
625:24 - here's an r plot of the density function
625:27 - of a standard normal curve
625:29 - and it's got that bell shape so
625:34 - the expected value of a random variable
625:38 - x following this distribution the
625:40 - variance and the standard deviation will
625:42 - be good given next
625:43 - uh these should not be shocking at all
625:46 - the expected value of x is equal to mu
625:50 - the variance of x is equal to
625:54 - sigma squared and the standard deviation
625:58 - of x is equal to sigma
626:01 - so normal random variables
626:04 - are specified by their mean and their
626:07 - variance
626:10 - okay you set the mean and the variance
626:12 - directly with normal random variables
626:15 - okay one property of the normal
626:17 - distribution is the 68 95 99.7 rule
626:21 - which i ain't going to sketch out for
626:23 - you this is basically a rule of thumb
626:25 - for
626:26 - how much of the distribution is within
626:29 - uh let's say uh within one standard
626:31 - deviation of the mean
626:33 - so we got mu plus sigma mu minus sigma
626:38 - within two standard deviations of the
626:40 - mean
626:42 - so mu plus two sigma mu minus two sigma
626:47 - and within three standard deviations of
626:50 - the mean
626:51 - so mu plus three sigma
626:55 - and uh mu minus
626:58 - three sigma okay
627:02 - so the peak of the curve happens at mu
627:06 - and we'll and let's see uh just kind of
627:09 - getting
627:10 - a sketch so uh the inflection points
627:14 - are going to happen with within uh
627:18 - one standard deviation
627:22 - all right so we could sketch out the
627:25 - curve
627:26 - to look something
627:30 - like this
627:40 - okay so
627:44 - uh what this rule says is
627:49 - let's uh start out within uh one
627:52 - standard deviation the area
627:54 - underneath the curve within
627:58 - one standard deviation
627:59 - [Music]
628:04 - is going to be 0.68
628:10 - so within one standard deviation
628:14 - so mu plus or minus sigma all right
628:17 - uh let's go to within two standard
628:20 - deviations
628:22 - okay so within
628:26 - two standard deviations so the area
628:29 - underneath the curve
628:32 - within two standard deviations
628:37 - so the area underneath the curve within
628:39 - two standard deviations
628:40 - is going to be 0.95
628:44 - so this will be mu
628:47 - plus or minus 2 sigma
628:51 - and then if we go to
628:55 - three standard deviations
628:59 - so let's uh have this going
629:02 - now let's have this going up and down
629:05 - horizontally
629:06 - is rough to draw
629:18 - okay
629:20 - on the other hand that's kind of a
629:21 - conflicting picture but you get the idea
629:24 - um within three standard deviations
629:28 - the area underneath the curve within
629:29 - three standard deviations is going to be
629:32 - 0.997
629:35 - so this will be mu
629:38 - plus or minus three sigma
629:43 - okay the unfortunate thing about
629:46 - that is i just drew over uh the text but
629:49 - i can still read
629:50 - so uh let z follow a
629:54 - normal distribution with mean zero and
629:56 - standard deviation one
629:58 - we then say that the random variable z
630:00 - follows the standard
630:02 - normal distribution and this
630:05 - distribution is useful
630:07 - since we can relate an arbitrary normal
630:09 - random variable to the standard normal
630:11 - distribution and vice versa
630:12 - and we can do so like so
630:16 - uh as a so as a reminder z is following
630:18 - a standard normal
630:22 - uh mu is an arbitrary normal random
630:24 - variable
630:25 - no no no not mu sorry mu is not a random
630:27 - variable
630:30 - so x is an arbitrary
630:34 - normal random variable so any mean and
630:36 - sigma
630:38 - in this case what you get
630:42 - is that if you take
630:46 - a normal random variable subtract out
630:48 - its mean
630:50 - and then divide by its standard
630:52 - deviation
630:53 - the distribution of the resulting
630:55 - variable will be equal to the
630:56 - distribution
630:58 - of the random variable z
631:01 - that is it's following a standard normal
631:03 - distribution
631:05 - which should make sense what this
631:08 - operation does
631:09 - is shift the mean to zero
631:22 - and then what you do is you
631:26 - uh scale by sigma or scale but let's say
631:29 - scale by one over sigma
631:34 - so you both uh take your curve
631:37 - along the number line oops
631:40 - i didn't want to erase stuff all right
631:47 - so what you're doing is you're taking
631:50 - your standard normal curve
631:54 - which is located at mu you are shifting
631:57 - it to the left
631:59 - uh by by mu and then
632:03 - you are compressing the curve
632:06 - so let's let's draw like a new curve
632:08 - that's oops
632:10 - let's draw a new curve that's centered
632:12 - at zero
632:14 - and then you compress your curve
632:18 - so it has a standard deviation of one
632:23 - all right that's what that operation is
632:25 - doing
632:26 - and similarly we could say
632:31 - that if we take a our standard normal
632:33 - random variable z
632:35 - scale it by sigma and then add mu
632:40 - the resulting random variable will be
632:41 - equal in distribution
632:47 - to the random variable x which is
632:48 - following a normal distribution with
632:50 - mean mu and standard dev
632:51 - standard deviation sigma all right what
632:54 - this is doing it's
632:55 - basically doing the opposite we
632:58 - are scaling by sigma
633:06 - and then we are and then we are shifting
633:09 - the mean to mu
633:18 - so to sketch out what this is doing
633:21 - we start out with our standard normal
633:23 - random variable which is centered at
633:25 - zero
633:26 - we then take that random variable
633:30 - shift its mean by mu
633:34 - so we end up with something over here
633:38 - and then we scale out its uh standard
633:42 - deviation
633:43 - so we get a curve with possibly a
633:46 - different
633:46 - standard deviation it doesn't have to
633:48 - get bigger it could get smaller too but
633:51 - you get the idea of scaling okay
633:55 - so uh let
633:58 - capital phi of little z be the
634:01 - probability that a standard normal
634:02 - random variable is less than or equal to
634:04 - z
634:04 - this is the cdf of the standard normal
634:06 - distribution
634:08 - then if x is the probability of an
634:09 - arbitrary normal distribution
634:11 - or normally just of an arbitrary
634:13 - normally distributed random variable
634:15 - we have excuse me
634:18 - we have the falling relationship between
634:20 - f and phi
634:22 - f of x is equal to
634:27 - phi of x minus mu
634:30 - over sigma where phi
634:33 - is the cdf of a standard normal
634:35 - distribution
634:37 - which means since we can relate
634:40 - any normal distributions cdf
634:43 - to the cdf of the standard normal random
634:46 - variable
634:47 - this means that we only need to worry
634:50 - about tabulating values
634:51 - for phi of z for the standard normal
634:55 - distribution
634:56 - in order to work with any normal
634:57 - distribution which is what's done
635:00 - in table 8.3 of dvor's book and often
635:03 - many of these statistics and probability
635:05 - books will have
635:07 - tables of the cdf of the standard
635:11 - random variable and i even remember
635:13 - buying
635:14 - a study card for math 3070 for my set my
635:18 - stats class
635:19 - and that table came with a
635:23 - no and that card came with a very small
635:27 - table for c for working with the cdf
635:31 - of a standard normal random variable and
635:33 - the reason why you can do
635:34 - why they're doing that is because you
635:36 - can get
635:38 - all of the information you need for any
635:39 - normal random variable from
635:41 - that table which is very nice because
635:44 - you can have just
635:45 - you can have any real number mean and
635:47 - any positive real number standard
635:49 - deviation
635:50 - and have an infinite number of normal
635:53 - random variables
635:54 - but you only need to print one table
635:56 - because once you have the table for the
635:58 - standard normal
636:00 - then everything works out great
636:03 - and additionally let's consider
636:06 - for a second the pdf of a standard or
636:10 - of a normal random variable uh let's
636:13 - let's consider actually first
636:15 - the pdf of a standard normal uh fee
636:19 - which will we will call just fee of x
636:21 - that's going to be
636:22 - uh 1 over the square root of 2 pi
636:26 - um actually let's let's call this v of z
636:32 - it's going to be 1 over the square root
636:33 - of 2 pi e negative
636:36 - z squared over 2.
636:40 - and this is kind of a mess so let's
636:42 - clean that up
636:47 - okay you
636:50 - you know by now that cdfs are computed
636:54 - via integration because all
636:58 - probabilities are computed v integration
636:59 - when you're working with
637:00 - normal random variables what then
637:04 - is the antiderivative
637:09 - of this
637:14 - the answer is basically what you see
637:17 - the thing is uh you there is no
637:20 - closed form or elementary solution
637:25 - for the anti-derivative of a normal
637:27 - random variable
637:29 - it doesn't exist it simply doesn't exist
637:32 - it's and i think in fact it's provable
637:35 - that you can't come up
637:37 - with an antiderivative
637:41 - for a normal random variable
637:44 - for a number of random variables pdf um
637:47 - in a in a in in any element in any
637:51 - elementary form
637:53 - so at the end of the day you're just
637:54 - kind of left with saying all right this
637:56 - is ranging from
637:57 - from uh negative infinity to
638:01 - uh i don't know x negative infinity
638:05 - to x you're just left with saying that
638:07 - this is equal to
638:08 - phi of x like that you're kind of left
638:12 - with this uh
638:13 - unsatisfying uh unsatisfying answer
638:17 - where you just say all right phi is this
638:19 - der is this integral
638:21 - and yet at the same time this is not a
638:25 - problem
638:26 - in fact nobody really cares nobody
638:28 - really needs
638:30 - to have an anti-derivative for
638:33 - what i've written down in black nobody
638:36 - really needs it because
638:38 - we have numerical routines that can
638:41 - compute these integrals
638:44 - mathematically we can still work with it
638:47 - there's nothing that says that we can't
638:48 - work with this we have the fundamental
638:50 - theorem of calculus that is able that
638:51 - tells us how to take derivatives of this
638:53 - thing
638:54 - so we can take derivatives we can still
638:55 - study its properties we can
638:57 - study how how quickly it grows and
638:59 - decays and
639:00 - and stuff like that there's really
639:04 - no reason we need a simpler expression
639:08 - for the antiderivative of a normal
639:11 - random variable
639:12 - so we just say this is true by d
639:16 - this is true by definition and go about
639:20 - our business
639:21 - because whenever we need to actually
639:23 - compute
639:25 - what the cdf is we have techniques for
639:27 - doing it
639:29 - we can use all these numerical routines
639:32 - uh numerical routines maybe some monte
639:35 - carlo simulation something like that
639:36 - there's all sorts of things that we can
639:38 - do
639:40 - to compute the cdf and you only need to
639:43 - do it once
639:44 - for the standard normal curve in fact i
639:47 - think
639:48 - r actually internally when when working
639:51 - with the cdf of a
639:53 - uh stan of um of normal random variables
639:57 - is work
639:57 - it's working with an internal table uh
640:01 - that computes probabilities
640:05 - so um admittedly
640:08 - i'm teaching this right now in an online
640:10 - format
640:12 - in a context where students are not
640:15 - going to ever enter a testing center
640:18 - so they don't need to use the table
640:21 - and if i were teaching this in a regular
640:23 - semester i would teach
640:24 - students to
640:28 - use the table and it just feels like
640:31 - right now that's silly because they have
640:33 - access to r
640:34 - and they're not going to want to use the
640:35 - table and i've all never really had a
640:37 - problem with
640:38 - teaching the table from a pedagogical
640:41 - perspective because i feel like using
640:42 - the table
640:44 - was good practice for
640:49 - working with the basic properties of the
640:52 - normal distribution
640:55 - and now i don't really see a reason to
640:59 - use it
641:00 - i mean there is still that pedagogical
641:02 - reason but it's just
641:04 - completely swamped
641:07 - by the convenience of having r around
641:12 - so we're going to lose that i'm actually
641:15 - rather sad that this time
641:17 - i'm not really going to use the table
641:19 - that i'm just going to
641:21 - compute probabilities using r but
641:23 - hopefully you can still
641:25 - get the message and understand some of
641:28 - the properties
641:30 - that would be learned by working with
641:32 - the table
641:33 - such as the symmetry of the normal
641:36 - distribution or
641:38 - working with one minus and stuff like
641:40 - that
641:41 - okay if i ever teach this class again in
641:45 - person
641:46 - uh maybe what a maybe i would and if i
641:50 - were to use these lecture videos again
641:51 - maybe
641:52 - i would create a separate video for uh
641:56 - working with the table
641:57 - but i don't think i will do that now all
641:59 - right
642:01 - so um anyway
642:05 - let's compute the following
642:08 - we're now working with a standard normal
642:10 - random variable so remember that this is
642:12 - a random variable
642:14 - uh with uh a mean
642:17 - i can do better than that
642:21 - hold on okay so this is a random
642:25 - variable
642:28 - good grief
642:31 - good grief come on just
642:36 - some people just don't want to press
642:39 - the undo button good grief
642:44 - ugh this screen how i hate it
642:48 - okay um
642:51 - okay so remember that we
642:55 - are working with the standard normal
642:56 - distribution so it's a distribution
642:58 - centered at zero standard deviation one
643:00 - i'll just tell you that standard
643:01 - deviation is one
643:02 - i want to compute the probability that z
643:04 - is less than equal to zero
643:06 - so this is what the normal distribution
643:09 - looks like this is the area that i want
643:10 - to compute
643:14 - what is that area well i know the area
643:16 - under the entire curve is one
643:18 - because it's a pdf and we know
643:22 - that we're shading the area to the left
643:25 - of zero and the zero is the point of
643:27 - symmetry
643:28 - so that means half the area so if you
643:30 - were to fold
643:31 - the curve over on itself around zero it
643:33 - would have equal area to the left and to
643:35 - the right of zero which means that this
643:36 - must be
643:37 - half of the area underneath the curve so
643:41 - that means that this is going to be
643:42 - equal to 0.5 because
643:45 - 0 is the median of a
643:48 - standard normal distribution all right
643:52 - uh next what is the probability that z
643:53 - is less than or equal to
643:56 - 1.23 okay
643:58 - so what where she so what we're
644:00 - computing here
644:02 - here's a standard normal curve here's
644:05 - 1.23 we are computing the area
644:09 - underneath the curve and to the left of
644:12 - 1.23
644:14 - okay and at this point
644:17 - i'm going to ask r what that area
644:20 - is so
644:28 - okay so i want
644:31 - a p norm by default p norm is working
644:35 - with a standard normal curve
644:37 - so we've got p norm of
644:46 - right uh 1.23 so that's going to be
644:50 - 0.8906 uh or we'll say 0.8907
644:53 - so this will be this is equal to
644:56 - 0.89
645:02 - okay uh let's see next up oh look at
645:06 - that some r code
645:07 - and uh it's basically confirming what we
645:09 - got via
645:10 - r so not shocking all right next up the
645:14 - probability that uh the center number
645:15 - and a variable is between negative 1.97
645:17 - and
645:18 - 2.1 so this will be
645:23 - we've got the standard normal curve
645:26 - okay here's zero
645:30 - here's uh 2.1 here's negative 1.97
645:37 - and we want the area in this region the
645:40 - area in between
645:42 - 2.1 and negative 1.97 so how are we
645:45 - going to do that
645:46 - well one thing we could do is say
645:49 - remembering
645:50 - that we are working with a cdf
645:54 - that this is going to be the area
645:59 - underneath the curve to the left of 2.1
646:05 - minus the area underneath the curve to
646:09 - the right of negative 1.97
646:14 - so you just subtract out the area from
646:16 - the left of negative 1.97
646:18 - from the air that's to the left of 2.1
646:21 - you can think of that as
646:22 - you have a piece of construction paper
646:24 - and this piece of construction
646:26 - paper can is the normal curve including
646:29 - the region
646:30 - underneath the normal curve that's a
646:31 - really long piece of construction paper
646:33 - since
646:34 - uh the normal curve extends from
646:36 - negative infinity to infinity
646:38 - right i never put any sort of bounds on
646:40 - this curve
646:42 - so i hope you notice that that this is a
646:44 - random variable that can take any number
646:46 - any real number between negative
646:48 - infinity and infinity
646:51 - um so we uh uh but you know we imagine
646:54 - that we have this uh maybe we clipped it
646:56 - off after a certain point
646:58 - and uh which is fine because after after
647:01 - a while the
647:02 - normal curve becomes minuscule so so
647:05 - minuscule
647:07 - because it it approaches zero very very
647:09 - quickly one way to interpret the 30
647:12 - 68 95 99.7 rule is saying that
647:16 - almost all of the curve is within three
647:18 - standard deviations and there's almost
647:19 - nothing outside of it
647:21 - so and that goes even more so for
647:24 - four standard deviations five standard
647:26 - deviations and so on there's almost
647:27 - nothing there
647:28 - um anyway we have we imagine that we
647:31 - have this piece of construction paper
647:34 - and we we clip off the area
647:37 - at 2.1 and have the area underneath the
647:40 - curve and to the left 2.1 and then we
647:42 - clip off
647:43 - the area underneath the curve to the
647:44 - right to the left of negative one point
647:46 - nine seven
647:46 - and what we end up with is the area that
647:48 - we want so we met or
647:50 - and if what we could end up doing is we
647:53 - start out by measuring the area
647:55 - underneath the curve uh the the area of
647:58 - our construction paper
647:59 - when we did our first clip when we
648:00 - clipped at 2.1
648:02 - and we measured that area and then we
648:05 - clip
648:06 - again at negative 1.97 and measure the
648:09 - area of the part that we clipped off
648:11 - and then subtract that from our earlier
648:13 - calculation
648:15 - to get the area that's remaining uh
648:19 - uh for our construction paper so this
648:22 - will be
648:23 - uh fee uh
648:26 - remember this remember that capital fee
648:28 - is a cdf of a standard normal random
648:30 - variable
648:31 - so fiat 2.1 minus
648:34 - fee at negative
648:40 - 1.97
648:43 - okay and then we need to compute this
648:48 - so p norm
648:51 - uh 2.1 minus p norm
648:57 - negative 1.97
649:00 - and this is what we get so we get 0.9577
649:04 - so we get oops so we get that this is
649:07 - equal to
649:14 - 0.9577
649:18 - okay uh next example the probability
649:21 - that z is greater than or equal to 1.8
649:24 - so this is the area underneath
649:27 - the normal curve that's
649:30 - to the right of 1.8
649:37 - okay and we say that this is going to be
649:43 - well we could say that this is uh the
649:44 - area underneath the entire curve
649:49 - so here i've shaded the whole thing
649:53 - minus the area underneath the curve
649:58 - uh to the left of 1.8
650:03 - going back to our construction paper
650:04 - analogy
650:06 - you have this piece of construction
650:07 - paper that has that's the the area
650:09 - underneath the entire curve
650:12 - uh and you clip off
650:16 - the uh the part at 1.8 and you're
650:20 - left with uh the part
650:23 - from negative infinity up to 1.8 so you
650:26 - lost the other part and you want to
650:27 - figure out the area of the other part
650:29 - well you knew the entire area was one so
650:30 - you measure the area of the part that's
650:32 - that you um the part to the left of
650:36 - uh to the left of 1.8 and subtract that
650:40 - from one
650:40 - to get uh to get the area that's
650:43 - underneath the curve and to the right of
650:45 - 1.8
650:46 - so this would be
650:51 - 1 minus the cdf
650:54 - of the standard normal at 1.8
650:58 - and we can go to r and compute this
651:03 - so 1 minus p norm
651:08 - at 1.8 and this is going to be 0.0359
651:14 - so this is 0.0359
651:20 - okay excellent uh the probability
651:23 - that it's greater than 5.2
651:26 - so this is going to be approximately
651:29 - zero
651:31 - but i'll go ahead and
651:35 - compute this in r and say
651:38 - one minus p norm
651:42 - uh what was the number we're plugging in
651:44 - 5.2 okay 5.2
651:47 - all right very very very small number
651:50 - uh not quite numerically zero because we
651:54 - can go to 16 decimal places
651:56 - but very very close
651:59 - um very small number
652:03 - and actually if you were using the table
652:06 - and if you look at most tables most
652:07 - tables don't go beyond
652:09 - four standard deviations they might even
652:11 - go beyond three
652:12 - or three and a half but you'll almost
652:15 - never see a
652:16 - table go beyond four standard deviations
652:18 - and then here we're asking for
652:20 - the area underneath the curve to the
652:22 - left of five standard deviations
652:25 - so uh if we were actually into the table
652:29 - which is
652:29 - uh the context in which these notes were
652:31 - written uh we would say
652:34 - what i would basically tell students is
652:35 - say this is approximately zero
652:37 - so don't even bother to look at the
652:40 - table just
652:40 - say this is approximately zero
652:44 - okay here's some arco that's doing all
652:47 - these calculations
652:48 - uh we also we also could have done some
652:50 - of that one minus stuff using the lower
652:52 - tail
652:52 - uh parameter let's see is that different
652:55 - from what we had
652:57 - no it's not different so that looks to
653:00 - be about the same
653:01 - um all right uh next example
653:05 - so iq scores are said to be normally
653:06 - distributed with mean 100 and standard
653:08 - deviation 15 like q be randomly selected
653:10 - be a randomly selected individual's iq
653:12 - score uh compute the probability that q
653:15 - is between 85 and 115.
653:18 - so in this case
653:25 - this was here's another thing um this is
653:28 - there's a lot of reasons why i really
653:30 - liked working with the table and one of
653:31 - the things that was great about working
653:32 - with the table was that it forced you to
653:35 - translate
653:37 - from a normal distribution to a standard
653:38 - normal distribution
653:40 - or any normal distribution to standard
653:42 - normal
653:44 - and thus i felt like it would force
653:47 - students to learn
653:48 - the relationship between any normal
653:50 - random variable and a standard normal
653:52 - random variable
653:53 - so what we could say here is that this
653:56 - is going to be
653:58 - the probability that 85
654:02 - minus the so q
654:06 - is going to be according to this problem
654:09 - a normal random variable with being 100
654:11 - and standard deviation 15. so this will
654:15 - be
654:15 - 85 minus 100 over
654:19 - 15 that's less than or equal to q
654:23 - minus 100 over 15
654:29 - which is less than or equal to 115
654:33 - minus 100 over 15.
654:38 - and this part right here is equal in
654:41 - distribution
654:43 - to a standard normal random variable z
654:51 - so we could say that this is equal to
654:54 - after you compute that lower and upper
654:56 - bound you'll find that this is
654:58 - the probability that negative one is
655:00 - less than or equal to z
655:03 - which is less than or equal to one which
655:06 - is going to be about 0.68
655:10 - because of the 68 95 99.7 roll
655:16 - so you really wouldn't even have to go
655:17 - the oh okay
655:19 - so 0.68 is very much an approximation
655:22 - it's not exactly true
655:24 - uh but it's kind of close it's i think
655:26 - it's true if you round to two decimal
655:28 - places
655:29 - so um well i'm actually not really sure
655:34 - uh but yeah you do have this um and this
655:37 - is basically forcing us to
655:40 - convert to the standard normal case
655:44 - um the unfortunate thing though is that
655:46 - now i could just do this you could do
655:48 - p norm um
655:51 - uh 115 uh
655:54 - mean equals 100
655:58 - and the other parameter is sd is equal
656:01 - to
656:02 - 15 minus p norm
656:08 - uh 85
656:12 - mean equals 100
656:16 - sd equals 15.
656:20 - and we get 0.6826 or
656:23 - 2 7. uh if we wanted to
656:26 - uh we could instead have written p norm
656:31 - 1 minus p norm
656:35 - negative 1 and they get the same number
656:39 - uh using basically that alternate form
656:42 - because you might not be converting you
656:45 - might not be
656:46 - converting to a standard random variable
656:47 - but r i'm pretty sure is
656:50 - so all right there's competing that
656:54 - uh this so the sad thing is that you can
656:56 - just do that and now i can't force you
656:59 - to uh use a table
657:02 - and uh and convert
657:06 - uh and uh convert to a standard normal
657:08 - random variable that's unfortunate
657:11 - okay uh so the probability that q is
657:13 - greater than 90.
657:15 - you know what i can't force to do it but
657:17 - i can still do it because i still have a
657:19 - point to make
657:20 - right so this is going to equal
657:23 - the probability that
657:26 - q minus 100
657:29 - over 15 is greater than
657:33 - 90 minus 100
657:37 - over 15
657:40 - which is equal to uh
657:45 - 1 minus the cdf of a standard normal
657:47 - curve a standard normal random variable
657:50 - at uh at a 90 minus 100 over 15
657:53 - this part becomes uh 10 over 15 or
657:57 - negative 10 over 15
658:01 - which is negative two-thirds
658:04 - so i would say uh this is going to be
658:08 - fee at i'll even round it i'll se
658:11 - i'll even convert it to a decimal number
658:13 - so negative
658:14 - zero 0.67
658:18 - which is approximate but this is
658:19 - supposed to be
658:21 - negative two-thirds
658:24 - and then i go to r and compute this
658:29 - and say this is one minus p norm
658:34 - uh negative two thirds
658:40 - which is point seven four seven five
658:44 - so this is approximately equal to 0.7475
658:51 - all right next up the international
658:54 - society for philosophical inquiry
658:56 - requires potential members to have an iq
658:58 - of at least 135
659:00 - in order to join the society this is one
659:01 - of those so-called genius societies
659:04 - based on this what proportion of the
659:07 - population
659:08 - is eligible for membership
659:11 - so this is the probability
659:14 - that an individual's iq is at least
659:19 - 135 which is
659:22 - equal to uh the probability
659:28 - that q minus 100
659:32 - over 15 is greater than or equal to
659:35 - 135 uh hold on
659:40 - uh let's move this
659:43 - so this is equal to
659:44 - [Music]
659:46 - probability that q minus 100
659:51 - over 15. is greater than or equal to
659:55 - 135 minus 100
659:59 - over 15 which is equal to
660:06 - uh the probability that a well okay this
660:09 - is going to be
660:11 - uh one minus
660:15 - the cdf of
660:19 - a normal variable or one minus fee at so
660:22 - 135
660:23 - minus 100 over 15 is 35 over 15
660:26 - uh which is seven thirds
660:32 - which is about 2.33
660:40 - okay so about 2.33 so
660:44 - 1 minus p norm
660:47 - 2.33 actually we'll just do seven
660:50 - divided by
660:51 - three so point zero zero nine eight
660:57 - so this is approximately point zero zero
661:00 - nine
661:00 - eight okay
661:05 - and here is some r code where i'm
661:08 - actually
661:09 - calling these mean functions sd function
661:13 - so mean sd parameters also using lower
661:15 - tail
661:17 - rather than doing one minus the cdf and
661:20 - so on
661:21 - all right uh so that was all
661:25 - trying to compute probabilities but
661:27 - sometimes we want to compute quantiles
661:29 - so we've got the notation z alpha which
661:32 - means that
661:33 - the cdf of at z alpha
661:36 - is equal to one minus alpha we can
661:39 - relate this
661:39 - back to general uh percentiles to find
661:42 - for arbitrary normal
661:44 - normally distributed random variables
661:46 - because these are the
661:47 - percentiles of um
661:52 - standard normal random variables okay
661:55 - so this is going to so we have in
661:58 - general
661:59 - a to p is going to be
662:04 - uh sigma z
662:07 - 1 minus p
662:11 - plus mu that's our percentile formula
662:16 - and z1 minus alpha can be found using
662:19 - table
662:19 - 8.3 using a reverse lookup but since you
662:22 - now have r
662:23 - there's really no point about discussing
662:25 - reverse lookups like this
662:28 - okay
662:34 - i mean i felt like they were good
662:35 - practice but
662:37 - we're now not going to be doing that
662:38 - anymore all right so uh
662:40 - what is z 0.5 that's the median of a
662:44 - standard normal
662:45 - z 0.5 is the area where the curve is
662:49 - split
662:49 - in two equal parts and that's going to
662:52 - be zero
662:55 - uh what is z 0.05
662:58 - well uh what we could do is say all
663:01 - right let's go to p
663:02 - norm no no no no we're not using p norm
663:04 - anymore we're using cute arm
663:07 - so q norm uh 0.05 so
663:11 - uh when you could put in 0.99
663:14 - no put put in 0.95 instead that would
663:17 - give us the same thing
663:19 - this is going to be 1 minus so this is 1
663:22 - minus .05
663:25 - or alternatively we could do q norm
663:29 - .05 lower dot tail
663:32 - equals false and that also works
663:36 - those all get us the same number so
663:39 - in the end though this is about 1.64 so
663:43 - this
663:43 - is so
663:46 - z 0.05
663:54 - is about 1.64
663:59 - okay what are the first and third
664:01 - quartiles of the standard normal
664:03 - distribution
664:04 - so what we're looking for is uh z
664:08 - so the first quartile will be z point
664:11 - seven
664:12 - five because remember we're doing up so
664:15 - the uh so here the uh
664:18 - subscript of the z is the upper tail
664:22 - area
664:23 - so this is the first quartile
664:27 - and the third quartile is 0.25
664:32 - this is q3
664:35 - all right so what are those going to be
664:38 - when we go to
664:39 - r and say q norm
664:46 - 0.75
664:48 - dot tail
664:51 - equals false so we get negative zero
664:55 - point six
664:55 - seven so negative
664:58 - zero point six
665:02 - seven and this one well actually i'm not
665:04 - even going to bother computer because i
665:06 - know what it is it's 0.67
665:08 - because we're working with a symmetric
665:11 - curve
665:12 - since we're working with a symmetric
665:14 - curve if we know one of those things
665:15 - that we know the other one
665:17 - because if the area underneath the curve
665:19 - to the left
665:20 - of z 7.75 is .25 the area underneath the
665:25 - curve to the right of point
665:27 - z 0.25 is also 0.25 so all we ever did
665:31 - was just flip over the flip over the y
665:35 - axis where x is equal to zero oh
665:38 - no let's not put a y there that's just
665:40 - confusing
665:42 - but my stupid undo button isn't working
665:45 - [Music]
665:46 - gosh why does stuff have to be so
665:52 - moody anyway um
665:56 - well since we're working with a
665:59 - symmetric distribution we actually have
666:01 - a property that
666:02 - i think gets written down uh later
666:06 - uh where did i write it down did i was i
666:08 - supposed to write it down
666:09 - up here where did i write it
666:14 - i know i wrote it down i know that i
666:17 - plan on talking about it at some point
666:20 - in these lecture notes
666:23 - oh i brought about on page 21 but
666:25 - basically i'll i'll i'll just cut to the
666:27 - chase and write it right now
666:28 - uh so z
666:32 - alpha is equal to negative z
666:35 - one minus alpha so basically
666:39 - uh you can just flip over the flip over
666:42 - the y
666:42 - uh the uh y axis so change the sign
666:46 - and work with one minus that area and
666:48 - you can get the same quantiles
666:50 - so but if it makes you feel better i'll
666:53 - go ahead and compute it
666:56 - and this is 0.25 yeah so what i did
667:00 - basically was exploit
667:01 - the symmetry of the standard normal
667:04 - distribution it's symmetry around zero
667:08 - our description of the random variable q
667:09 - from example 11.
667:11 - uh so using that answer the following
667:13 - questions mensa international requires
667:15 - individuals have an iq score
667:17 - that would place them in the top two
667:18 - percent of the population was the
667:20 - minimum iq score needed to be a member
667:22 - of mensa well that would be um
667:25 - [Music]
667:27 - so the standard deviation is 15.
667:30 - we've got z so top two percent so that's
667:33 - going to be 0.02
667:34 - the upper tail area is 0.02 plus 100
667:38 - and this is going to be let's see
667:43 - we've got 15
667:46 - times q norm 0.02
667:50 - lower dot tail equals
667:54 - false plus 100
667:58 - 130 or i guess he'd ran into 131
668:01 - uh so we'll say 131. uh
668:04 - after rounding so 131 you need to have
668:07 - an iq of 131
668:09 - in order to be a member of mensa uh
668:12 - there's an alternative way to do that
668:13 - though we could have instead
668:15 - we could have instead done q norm
668:19 - .02 mean equals 100
668:22 - sd equals 15 lower dot tail
668:26 - equals false that would have also worked
668:30 - okay uh the part of the population with
668:33 - the lowest five percent of iq scores is
668:35 - considered to be intellectually disabled
668:37 - what is the highest iq score needed to
668:39 - be in this group
668:42 - okay so uh that means that
668:46 - we are looking at uh z
668:50 - 0.95 or actually we're asking for eta
668:54 - of 0.05
668:58 - up here in this problem we were looking
669:00 - for
669:02 - eta of 0.98
669:06 - okay so
669:09 - this is going to be 15z 0.95 plus 100
669:15 - and then we go and compute that
669:18 - so in this case we got
669:21 - 0.95 now this is going to be 75.32 so
669:26 - we'll say
669:28 - about after rounding 75.
669:32 - so if you have a so if you have an iq
669:34 - score of 75 or lower you're considered
669:36 - intellectually disabled
669:40 - okay so
669:44 - right there's some r code that's doing
669:46 - the same thing so do the symmetry at the
669:48 - normal distribution we have the
669:49 - following useful identities for fee
669:52 - uh one second
669:58 - okay we have that the cdf
670:04 - at z is equal to
670:08 - one minus the cdf
670:12 - at negative z
670:16 - which what this is saying is
670:20 - uh if you were looking at
670:24 - so this is a standard normal
670:25 - distribution
670:27 - if you were looking at the area
670:30 - underneath the curve
670:31 - and to the left of z
670:36 - another way you could compute that
670:37 - quantity
670:39 - is look at the area underneath the curve
670:42 - and to the right of negative z
670:44 - which is what hap which is what you get
670:45 - when you flip over
670:47 - the uh y axis and then subtract that
670:50 - from 1
670:51 - to get the to get the red area
670:54 - underneath the curve so the so the area
670:56 - above negative z um
670:59 - is going to be equal to the area below z
671:02 - or the area to the right of negative z
671:04 - is equal to the area to the left of z
671:07 - because
671:07 - of the symmetry of the curve and
671:10 - equivalently
671:12 - well as a consequence of this we have z
671:16 - alpha is equal to negative z
671:19 - 1 minus alpha that's our immediate
671:22 - consequence
671:24 - all right so as mentioned before fee can
671:26 - be used to approximate the cdf of other
671:28 - random variables
671:30 - so it turns out that the normal
671:31 - distribution can
671:33 - be used to approximate the uh
671:36 - cdf of other random variables or
671:40 - approximately other random variables
671:41 - basically
671:44 - which of course mattered more
671:45 - historically
671:47 - when we didn't have uh
671:50 - when we had to basically physically
671:52 - print out tables for random variables
671:55 - but you still want to be able to get
671:56 - probabilities for
671:58 - binomials with large parameters large n
672:01 - or
672:02 - uh poisson random variables with large
672:05 - mu but in this situation
672:09 - um like in the in the world in which we
672:11 - currently live
672:12 - that's less of an issue because software
672:14 - doesn't ask you how big
672:16 - n is it just works so
672:19 - um all right so well i guess it does
672:21 - literally ask you
672:23 - but uh it's not like you put in the
672:25 - wrong number and it will just not work
672:28 - um unless of course of course you put in
672:30 - something that's
672:31 - really big to the point that software
672:32 - can handle it but
672:34 - that's highly unlikely uh anyway
672:38 - uh still the fact that certain random
672:41 - variables can be approximated by normal
672:42 - random variables
672:43 - is not only important
672:46 - it's getting to fundamental theorems of
672:50 - statistics and probability
672:51 - uh that will be discussed in chapter
672:53 - five
672:54 - so let's say for example let's work it
672:58 - let's work with the binomial random
672:59 - variable
673:00 - when n the the sample size is large
673:07 - the cdf of a binomial random variable
673:12 - at x with parameters n and p
673:15 - and here we're assuming that n is
673:17 - somewhat large
673:18 - uh you should probably say well okay so
673:21 - a rule of thumb is that n times p
673:22 - is greater than or equal to 10 and n
673:24 - times 1 minus p is greater than equal to
673:26 - 10
673:26 - that's one rule of thumb that they're
673:28 - using um
673:30 - we could probably say that if your p is
673:31 - between uh 0.1 and 0.9
673:34 - then and a sample size of 40
673:37 - is probably fine so this is going to be
673:40 - approximately equal to
673:43 - the cdf of a standard normal random
673:46 - variable evaluated at
673:49 - x minus the mean of the random variable
673:52 - which is np because that's the mean of a
673:54 - binomial
673:55 - divided by the standard deviation of a
673:57 - binomial which is n times p
673:59 - times 1 minus p and then
674:02 - in addition to this we do plus 0.5
674:07 - the plus point five is what's known as a
674:09 - continuity correction
674:11 - uh it accounts for the fact that we are
674:14 - using a continuous random variable
674:16 - to uh approximate a discrete random
674:20 - variable
674:22 - if you didn't do this then you often end
674:24 - up with some numerical inaccuracy with
674:26 - the
674:27 - approximation the approximation is still
674:29 - at some level true
674:31 - but it's just off and you get better
674:35 - you get better approximate computations
674:38 - for
674:38 - say the cdf when you include
674:42 - this uh continuity correction i think
674:45 - the justification
674:47 - for why you add plus 0.5 is you could
674:51 - imagine that
674:52 - you have this uh probably mass histogram
674:56 - and your
674:59 - and your uh and the uh
675:03 - or cdf of the normal curve would be
675:06 - going through it
675:08 - basically at the left endpoints so you'd
675:10 - have
675:11 - something that's looking like
675:14 - this and if you shift everything over
675:19 - to the right yeah i think it's to the
675:22 - right when you add do plus or no it's
675:24 - to the left but when you shift the curve
675:27 - oops
675:30 - the undo button is not working when you
675:32 - shift the curve
675:33 - over a little bit you get
675:37 - like a better pass through
675:40 - of these uh histograms so it's like
675:43 - re-centering so that
675:44 - it's centered evenly um
675:48 - on the uh on the uh probability
675:51 - histogram
675:52 - or this uh probably mass function
675:54 - understood as a histogram
675:55 - anyway uh let's let's work let's do an
675:58 - example
675:59 - a manufacturer will reject a batch of
676:02 - widgets if in a sample of 100 randomly
676:04 - selected widgets of the batch
676:06 - uh 15 or more are defective if 12 of the
676:08 - widgets in the batch are defective
676:10 - was the probability of rejecting the
676:11 - batch so the random variable in question
676:14 - is uh we'll call it s
676:18 - and it's following a binomial
676:20 - distribution
676:22 - uh the the parameter n is 100
676:26 - and the parameter p for the probability
676:29 - of getting a defective widget is 0.12
676:33 - okay so the approximating normal random
676:36 - variable
676:37 - follows a normal distribution with what
676:40 - is going to be in the mean well it's
676:41 - going to be the sample size times p
676:43 - so that's 12. and then we've got
676:47 - a standard deviation which is going to
676:51 - be
676:53 - the square root of a hundred
676:56 - times 0.12 times 0.88
677:01 - okay so the standard deviation is about
677:04 - 3.25
677:06 - so 3.25 this is the distribution of the
677:10 - approximating normal random variable so
677:13 - s's distribution is approximate so s is
677:17 - approximately equal in distribution to x
677:22 - so then when we want so okay reject the
677:24 - batch
677:25 - when is the batch rejected the batch is
677:27 - rejected when
677:29 - uh 15 or more widgets are defective so
677:33 - we're looking at the probability that s
677:36 - is greater than or equal to 15
677:41 - and this is going to be
677:44 - uh 1 minus the cdf
677:47 - of this random variable at um uh
677:53 - hold on uh yeah at 14
677:57 - uh and it's got parameters 100 and 0.2
678:01 - and point 12.
678:06 - okay and
678:09 - this is according to our normal
678:11 - approximation approximately equal to
678:16 - uh one minus phi
678:20 - and we've got 14
678:24 - minus 12 divided by
678:28 - 3.25 and then we add in
678:32 - the continuity correction 0.5
678:37 - and what is this going to be equal to
678:40 - well
678:40 - we've got 1 minus p norm
678:46 - so we've got uh
678:50 - so 14 minus 12
678:53 - plus 0.5 divided by
678:59 - 3.25 so
679:03 - 0.2209
679:10 - and just for reference we could have
679:13 - alternatively computed
679:18 - p by nom and
679:22 - we would have used
679:26 - 14
679:29 - size equals 100 prob
679:32 - equals 0.12 and we would have said
679:36 - lower dot tail equals
679:41 - false yeah so that's
679:45 - pretty close to what we uh got using the
679:48 - normal approximation
679:51 - okay all right so
679:55 - uh scrolling down
679:58 - oh did i
680:02 - oh something's different uh
680:07 - on the other hand is the r code wrong
680:21 - okay it looks like i might have made a
680:22 - mistake here
680:24 - so the probably that s is greater than
680:25 - or equal to 15 is probably that
680:28 - is one minus probably that s is less
680:29 - than or is strictly less than 15
680:32 - which is one minus the probability that
680:35 - s is less than or equal to
680:37 - 14. so i think that my
680:40 - r code in these in this
680:43 - yeah i think that i did not
680:47 - put in
680:50 - yeah or
680:54 - [Music]
680:57 - hmm curious
681:02 - i don't know i i'm thinking actually
681:04 - that this might be wrong
681:14 - on a second look okay
681:18 - but you get the point at the very least
681:21 - and by the way i would suggest using the
681:24 - normal pro
681:24 - approximation at the very last step so
681:28 - right when you're about to compute
681:29 - something and that you don't know how to
681:30 - compute so
681:31 - like for example um i guess
681:34 - i should probably write down what steps
681:37 - i've kind of been emitting i can say
681:39 - this is the probability that s is no
681:42 - this is 1 minus the probability that s
681:44 - is less than 15
681:46 - which is 1 minus the probability that
681:50 - s is less than or equal to 14
681:54 - so here i um
681:57 - basically was still treating s as if it
681:59 - were a discrete random variable
682:02 - you should still do that uh you should
682:05 - like if i was
682:06 - treating this as a continuous random
682:08 - variable i would not be caring so much
682:10 - about whether i was working with less
682:11 - than or less than or equal to
682:13 - but you should you should still treat
682:16 - your random variable that you're
682:18 - approximating with a normal random
682:19 - variable
682:20 - as if it's discrete up until the final
682:22 - point
682:23 - when you need to compute something like
682:25 - the cdf
682:27 - okay so that's what i recommend all
682:30 - right
682:30 - uh the approximation works for poisson
682:33 - random variables as well
682:34 - when uh this lambda parameter is large
682:38 - or i think it was i don't know why i
682:40 - wrote lambda here
682:41 - i think that might be because the book's
682:42 - using lambda i'm not really sure why
682:44 - because of what i remember is that in
682:46 - chapter 3 i was using mu
682:47 - to write down poisson random variables
682:50 - okay but whatever
682:51 - um so
682:55 - in this case the approximating
682:57 - distribution for a poisson random
682:59 - variable
683:00 - uh let's suppose that
683:05 - um x follows a poisson distribution
683:11 - uh with mean parameter
683:15 - mu then we could approximate it with
683:18 - y which is following a normal
683:20 - distribution with mean mu
683:22 - and standard deviation square square
683:24 - root of mu
683:25 - because the standard deviation of a
683:26 - poisson random variable is square root
683:28 - is the square root of mu okay
683:31 - uh so suppose that x follows a poisson
683:35 - distribution with parameter 100
683:36 - let's estimate the probability that x is
683:38 - less than or equal to 110
683:40 - so uh the probability
683:43 - that x is less than or equal to 110
683:47 - is approximately equal to fee at
683:52 - 110 minus 100
683:56 - plus 0.05 that's the continuity
683:58 - correction
684:00 - divided by uh the square root
684:03 - of 100 which is 10. so
684:08 - this is going to be this is going to be
684:11 - the cdf
684:15 - at 10.5 divided by
684:19 - 10 which
684:22 - is
684:25 - 10.5 minus 10. i don't know divided by
684:28 - 10.
684:31 - oh yeah that's a 1.05 so that's going to
684:33 - be
684:36 - 0.8531
684:43 - okay uh let's compare that
684:46 - to uh
684:49 - to what we would have had uh if we used
684:52 - the poisson distribution directly
684:54 - so 110 and uh
684:58 - lambda is equal to 100.
685:01 - oh very close very very close so a good
685:04 - approximation
685:06 - all right okay that's
685:10 - that concludes this section this is a
685:12 - very important section
685:13 - very very important because the normal
685:14 - distribution is a distribution that is
685:16 - appearing all over the place
685:19 - so and not just in this chapter but
685:22 - in later chapters too uh chapter five
685:26 - chapter seven chapter eight you're going
685:27 - to be using the normal distribution all
685:29 - the time
685:30 - it's going to be assumed that random
685:31 - variables are normally distributed so
685:34 - you need to get comfortable
685:35 - with this distribution all right so work
685:38 - on problems for this
685:39 - make sure you understand it if you don't
685:41 - understand it fix that
685:43 - and yeah it's it's your responsibility
685:45 - to fix it part of the way you fix it is
685:47 - by asking me what you don't understand
685:49 - right so um like
685:53 - part of how you fix not understanding
685:55 - something is getting help when you need
685:56 - it
685:56 - right from from whoever could possibly
685:58 - help you but you need to fix what you
686:01 - don't understand
686:02 - please please learn the normal
686:04 - distribution inside and out
686:06 - and you will be rewarded for it all
686:08 - right so that's it for
686:10 - uh this uh this section and i will see
686:13 - you
686:13 - in the next section when we talk about
686:15 - exponential random variables which
686:17 - we've already talked about quite a bit
686:19 - and also the gamma distribution which is
686:22 - an interesting distribution
686:23 - often shows up in applications and also
686:26 - in a more
686:27 - theoretical setting okay uh right so see
686:30 - you then