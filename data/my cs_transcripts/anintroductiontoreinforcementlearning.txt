00:07 - the enforcement learning is somewhat
00:10 - special in that it receives some
00:13 - teaching signal but not as much as in
00:16 - supervised learning so it's it's
00:17 - somewhere between supervised and
00:19 - unsupervised learning the teaching
00:21 - signal that it gets is either whether it
00:24 - has done something good or bad so it's a
00:26 - scaler teaching signal but what's so
00:28 - special about the reinforcement learning
00:30 - is that you have an agent
00:34 - that has to make decisions right I that
00:39 - can go left or it can go right for
00:43 - instance
00:45 - once it has done that it gets either
00:49 - reward or punishment depending on
00:59 - whether it was good or bad and typically
01:00 - just that's just called reward can be
01:03 - positive reward or negative reward so I
01:07 - recommend to to pause here for a moment
01:09 - and read this very nice short
01:12 - description of the essence of
01:15 - reinforcement learning however I will go
01:21 - straight to the more formal description
01:26 - and use this didactic example for that
01:32 - so in reinforcement learning we have
01:35 - states and states are these little boxes
01:40 - and the agent can be in that state so X
01:43 - is the agent and the blue square here is
01:46 - the state in which the agent currently
01:49 - is the agent can take actions in this
01:52 - case and can for example go down it
01:54 - could also go left or could go up if
01:57 - it's in the middle somewhere it has four
01:59 - directions where it can go
02:03 - in this case in this particular scenario
02:06 - that's just one example the agent gets a
02:09 - negative reward of minus one for each
02:12 - step that it takes but it does not have
02:15 - the option not to move unless it is what
02:18 - in one of these two red squares which
02:20 - other absorbing states so once it
02:23 - arrives in this state it is allowed not
02:26 - to do any step anymore so and since it
02:29 - gets a reward of minus one for each step
02:31 - that it takes of course it shouldn't
02:33 - move as quickly as possible into one of
02:37 - these two absorbing states and then just
02:38 - rest there so it can thereby minimize
02:42 - its negative reward
02:46 - the states have a description in this
02:49 - case it's the coordinate of the box and
02:51 - they have a value minus four or minus
02:55 - three or whatever
02:58 - these values are unknown to the agent so
03:01 - the agent has to learn these values and
03:04 - it has to learn so-called policy policy
03:08 - services strategy how to move in that
03:11 - environment and these two are closely
03:13 - related as we will see in a moment
03:25 - ideally the value of the states
03:28 - corresponds to the expected reward given
03:31 - an optimal policy so each time the agent
03:37 - moves from one state to another each
03:47 - time my little agent here moves from one
03:50 - step from one side to the other it gets
03:57 - a reward and in this case the reward is
04:03 - R of T is minus 1 for each step that the
04:08 - agent takes and that is this reward and
04:14 - the expected reward is simply the sum of
04:17 - all the rewards that the agent gathers
04:20 - from now which is T zero to the infinite
04:25 - future and that of course depends on the
04:28 - state in which the agent currently is so
04:32 - this is s of T zero which is called it
04:34 - referred to as s0 so right now the state
04:38 - of the agent is in state s0 and then it
04:42 - takes a series of actions and for each
04:44 - action it receives a reward and if you
04:49 - sum all that up that would be the reward
04:52 - that it gets and if it's sort of a
04:54 - prediction about the future that would
04:55 - be the expected reward
05:00 - so in the example above if we assume
05:04 - that the agent has this policy so for
05:08 - each state there's an arrow that leads
05:12 - into another state right except for the
05:16 - absorbing States and that is a policy
05:18 - right so the agent can behave according
05:21 - to these arrows and if it does these are
05:27 - the true values of the states ie these
05:32 - are the expectation values for the
05:36 - reward for example if the agent is in
05:39 - this state then according to this policy
05:42 - it would first move to the right and
05:44 - then down and if you do that we get an
05:47 - expect we get a reward of minus 1 for
05:49 - this step minus 1 for this step etc so
05:53 - we add 1 2 3 4 5 steps which add up to a
06:01 - expected reward of minus 5 that's why
06:05 - this state here has the value of minus 5
06:07 - so this is a true value of all these
06:10 - states right I mean these states are
06:12 - nearer to this absorbing state so these
06:16 - are the true values of the states given
06:19 - this policy
06:21 - now but obviously I mean we already see
06:23 - that this is not a optimal policy it
06:26 - would be much better to move directly to
06:28 - the left if the agent starts in this
06:31 - state these are the two values given
06:36 - this policy but these values are not
06:39 - known to the agents so the agent has to
06:40 - learn that so how does that work
06:47 - so let's look sort of a better
07:01 - maybe lower right corner so let's assume
07:07 - and this is the absorbing state right
07:31 - okay so the agent I mean has a map or a
07:38 - function that expresses the value of the
07:41 - different states and initially of course
07:42 - there are wrong values in this so let's
07:46 - say there's three here there's four here
07:49 - there's -1 here 5 1 2-3 yeah and ok and
08:04 - the agent I mean let's tell the agent
08:07 - that this has a value 0 because once it
08:10 - is there it doesn't have to move anymore
08:11 - and then it doesn't get any punishment
08:14 - anymore so let's assume the agent we put
08:23 - the agent somewhere right we put it here
08:27 - then the question is okay what should
08:29 - the agent who the agent can actually not
08:33 - plan ahead but it can see the values of
08:38 - the states directly around it right so
08:43 - put it know let's put a minus three here
08:46 - and a minus four so that there's very
08:48 - little incentive to go there and minus 5
08:52 - here so let's consider this situation
08:54 - and sort of the with this policy how the
08:59 - values that are initialized to these
09:01 - wrong values how they can be corrected
09:03 - and learned to these correct values and
09:07 - we follow strictly this policy we will
09:11 - consider how to optimize a policy in a
09:13 - moment but right now we fix this policy
09:15 - and we just try to learn the value
09:16 - function and we do that by placing the
09:20 - agent randomly in different locations
09:26 - in different states of this environment
09:28 - and then see what it can do in order to
09:32 - adapt learn the values to correct values
09:38 - okay so let's put the agent in this
09:40 - state and then it moves by the policy it
09:47 - moves to the right yeah so now it sees
09:50 - here it has a value of five and it sees
09:52 - value of minus one here and it it feels
09:56 - that it gets a punishment of minus one
09:59 - or a reward of minus one on the way to
10:02 - this to this state now it's quite
10:06 - obvious that this state that the value
10:08 - of this state cannot be five if you move
10:11 - into a state with a value of minus one
10:13 - and you receive a punishment of minus
10:15 - one on the way yeah so this value needs
10:21 - to be corrected to a value of minus two
10:23 - minus two is the expectation value or
10:28 - expectation expected reward in this
10:31 - state plus a reward on the way to this
10:34 - state so what we do we correct this five
10:39 - to minus 2
10:45 - now the agent is in this state it will
10:47 - move according to this policy right it
10:50 - will move down it sees a expected reward
10:52 - of four and receives a reward of minus
10:57 - one on the way and that means minus one
11:01 - is not a correct value right it should
11:03 - be three so we correct this to 3 because
11:10 - that's four minus one right if the
11:14 - expected reward is 4 in this state then
11:16 - it is 3 in this state if it moves down
11:20 - ok finally it moves down to this
11:23 - absorbing state and with the same
11:25 - argument the value 4 is incorrect it
11:29 - should be minus 1
11:35 - and now we see at least this value is
11:38 - correct right that's the correct value
11:41 - but the others are still wrong so now we
11:43 - place the agent again into someone one
11:46 - of the random states let's say we put it
11:48 - here then same argument the value should
11:51 - not be one it should be
11:55 - - right 3-1 then it moves to the right
12:00 - so it should not be three it should be
12:02 - minus two
12:05 - right and then we go down and that's
12:07 - fine the value is correct and by
12:10 - repeating many of these processes so we
12:12 - place it here let's say this gets this
12:14 - gets corrected 2-3 right then we go from
12:20 - here to there this gets corrected 2-1
12:22 - then we start here again this gets
12:26 - corrected do two then from here to there
12:30 - this gets corrected 2-2 this is correct
12:34 - etc then we place it here again let's
12:36 - say then this gets corrected 2-3 and so
12:41 - on now if we're here gets corrected 2-3
12:44 - if you're here and so on right so by by
12:50 - going through these different states and
12:52 - always updating the state where it comes
12:56 - from by the value of state where's where
12:58 - it goes into plus the reward that
13:00 - receives on the way the valleys become
13:03 - more and more correct yeah until it
13:05 - converges to this situation
13:10 - so this is how the correct value
13:16 - function can be learned if you have
13:18 - fixed policy
13:20 - and that's written in a question here so
13:26 - the value in the next time step of state
13:32 - s equals the value of the state that it
13:38 - goes into and that's given here by this
13:40 - capital s function so capital S is the
13:42 - Fung is the state where you get into if
13:46 - you are currently in state s and you
13:48 - take an action a that's possible in
13:52 - state s so a state and a particular
13:56 - action brings you into a new state for
14:01 - example if you're in this state and you
14:02 - take the action going to the right then
14:05 - you're in this state so this is the
14:07 - capital S state and this is a lowercase
14:09 - s state now and you take that value so
14:13 - you take this value and you add the
14:17 - reward that you receive if you add state
14:19 - s and you take the action a s all right
14:22 - so this formalizes exactly what I've
14:24 - just shown
14:27 - okay so now we know how to get the
14:30 - correct value function
14:31 - give me policy however we have already
14:35 - Mari marked that this is not an optimal
14:38 - policy right from this state it would be
14:41 - much better to move to the left rather
14:43 - than to the right so how can the agent
14:45 - learn an optimal policy give me a value
14:49 - function well there's not much learning
14:51 - actually it should simply look into the
14:57 - neighboring states and then move towards
15:02 - that state that gives a maximum reward
15:04 - at least in this case where the reward
15:07 - on the way into that state would be
15:09 - equal in all cases yeah so that if the
15:12 - agent knows what the reward is it gets
15:15 - it if it does step into another state
15:19 - and it sees the value in that state it
15:21 - can figure out what's sort of the
15:23 - optimal direction to go and that sense
15:26 - would be optimal to go from here to
15:27 - there rather than from here to there
15:31 - and this is expressed by this equation
15:34 - so the action that the agent takes in
15:37 - state s should be the one that gives the
15:40 - maximum expected reward and the maximum
15:42 - maximal expected reward so this is the
15:45 - maximum operation here over the
15:48 - different actions that are possible to
15:50 - take right a dash are the different
15:52 - options that the agent has and this
15:58 - expected reward is a combination of the
16:02 - value of the state where the agent ends
16:05 - up in after having done this action and
16:08 - the reward that it receives on the way
16:10 - into this state and that's the sum and
16:13 - then the agent simply figures out what's
16:17 - the optimal action to take based on this
16:20 - so there's no real learning here here
16:21 - involved that is something that can be
16:25 - done right away so here we see what
16:28 - happens if we iterate these two
16:30 - processes so we iterate deciding on a
16:34 - policy then learning the optimal the
16:36 - correct value function then based on
16:39 - that decide on the optimal policy
16:41 - because you have seen it is not optimal
16:44 - to go from this state to that state
16:46 - would be rather optimal to go to the
16:48 - left state
16:51 - so that would be the optimal policy
16:53 - given this value function then based on
16:56 - this policy the system can learn the new
16:59 - value function because now it realizes
17:00 - that going from this state to that state
17:04 - makes this we expect a reward minus one
17:07 - rather than minus five and now it turns
17:10 - out that going from this state to that
17:12 - state is better than going to the right
17:14 - so we adapt the policy again and then
17:18 - based on that we can learn the true
17:20 - value function and this now is the true
17:22 - value function and this is actually an
17:26 - optimal policy I mean if the agent is in
17:30 - this state it could also go to go to the
17:34 - left so it's not a unique policy optimal
17:36 - policy but is it is one of the optimal
17:38 - policies now it's not very efficient to
17:45 - 1st converge completely on the two value
17:48 - function and only then decide on a new
17:51 - policy it's better to do both
17:53 - simultaneously actually
17:57 - and this is this can be done by
18:01 - combining the two equations that we had
18:04 - above so the action that we take is
18:06 - always the optimal action in that
18:09 - current situation right given the value
18:12 - function that we currently have we take
18:14 - the optimal action
18:18 - this is how the agent decides where to
18:21 - go and then we use the equation from
18:24 - above to update the value function
18:32 - so this would be the most primitive
18:35 - simplest form of reinforcement learning
18:38 - so that is known as temporal difference
18:41 - learning although there are more refined
18:44 - versions of that and a lot of ways to
18:47 - improve on that in particular there are
18:49 - also ways to to deal with uncertainty
18:54 - right now we have argued we have a
18:57 - deterministic system deterministic
19:00 - environment but you have to use some
19:04 - other strategies if you have the
19:06 - probabilistic environment but this is
19:07 - sort of the the most basic version of
19:11 - reinforcement learning and gives you a
19:13 - sort of a feeling for what the
19:14 - principles are
19:23 - you
20:30 - standard reinforcement learning or
20:32 - temporal difference learning as we have
20:33 - learned it in the first section is
20:35 - rather inefficient one problem is the
20:43 - very slow update of the states so if we
20:48 - consider a very simple variant of the
20:53 - system that we've considered so far
20:56 - where we have these states
21:02 - and we have one absorbing state so the
21:07 - box in the this state in the very right
21:11 - is the absorbing state
21:19 - let's assume the boxes are all
21:21 - initialized with some values okay so
21:27 - let's put it in 3 4 1 1 - 3 - 5 - 5 - 3
21:44 - - 1 3 and we know this has a value of 0
21:49 - right now as and we place the agent in
21:54 - some of these states and then has the
21:55 - choice of going left or right so if this
21:59 - one is about for some time and then in
22:02 - the end moves from this state with a 3
22:06 - in there to the last state then it
22:09 - learns that the that the value here
22:12 - should actually be minus 1
22:18 - yeah but his learn nothing about the
22:23 - other states if then it one is bowed
22:26 - again and again then it updates
22:28 - eventually updates this stay 2-2
22:31 - correctly at least this at minus 1 yeah
22:34 - so it has learned as the value of a
22:37 - second state and so it needs to run
22:40 - through this over and over again until
22:44 - the true value sort of gradually
22:46 - propagates back into the through the
22:49 - states now one can improve on this by
23:01 - not only only updating the the value of
23:09 - the state where which I just leave but
23:12 - also the previous states so let's assume
23:14 - we put the agent here in this box and it
23:19 - moves right straight to the right yeah
23:24 - then it learns from here to there that
23:28 - the value 2 is wrong it should be 4
23:33 - yeah and now at that moment we don't
23:36 - update any other values because we have
23:39 - just started now if then the agent moves
23:44 - from here to there it learns that the
23:46 - value here should be minus four rather
23:49 - than five
24:05 - now the value here has changed from 5 to
24:08 - minus 4 so it has changed by 9 now and
24:13 - now that can help us to update also the
24:18 - value of the previous state to correct
24:21 - that because that used to be the correct
24:23 - value for the 5 that was in here
24:27 - but now it should be reduced by nine
24:34 - like this state so it is corrected along
24:36 - with this one
24:39 - so we put a it was a 4 minus 9 would be
24:45 - -5 now so now the system has learned in
24:49 - one go the two states
24:57 - now that has that would in principle
25:00 - work in such a one-dimensional thing
25:06 - No so you could actually start here on
25:09 - this side and let it run once and then
25:12 - it would learn all the different values
25:15 - in one go yeah there are problems if you
25:20 - if you consider a two dimensional
25:25 - environment and for example you imagine
25:31 - for some reason the agent is running in
25:34 - a circle right then if you would really
25:38 - do this strict updating then it would
25:41 - update this point twice and then would
25:44 - get a wrong value so it's maybe not a
25:47 - good value get not a good strategy to
25:49 - update it by strictly the same amount by
25:52 - which you update the current state sort
25:54 - of so it might not be a good idea to
25:56 - update the previous and the values of
25:59 - the previous states by as much as you
26:02 - update the value of the current state so
26:05 - there should be maybe a decay sir
26:06 - the past should be corrected less in
26:08 - order to avoid these kinds of problems
26:14 - and this is what is what is being done
26:18 - in what's called TD lamda learning so
26:22 - what you do here is so this is a
26:26 - standard learning rule that we had above
26:28 - so we update the value of the current
26:31 - state s of T we update it so we look at
26:35 - the value of T plus 1 by the value that
26:40 - the next state has that is VT of s T
26:46 - plus 1 the value of the future state
26:49 - plus the reward
26:53 - and now what come what is added to this
26:56 - is that you also update the values of
26:59 - the state that have been previously
27:01 - visited so as T - t - so these because
27:06 - of the - T these are the states that
27:08 - have been previously visited and you
27:11 - update these right so the going from VT
27:14 - to the T plus one by the difference
27:19 - of the value of the current stage this
27:23 - is their update of the current state
27:27 - and this is the value of the current
27:33 - value of previous states right so you
27:35 - simply correct the previous states by
27:38 - the adaptation of the current state
27:40 - if this prefactor would be 1 then we had
27:44 - the situation that we have that have
27:46 - just described that would allow this one
27:47 - here to update all states from left to
27:50 - right but typically one chooses a value
27:54 - somewhere between 0 and 1 to avoid these
27:57 - problems and the one takes the exponent
28:00 - adesh so the previous the value of the
28:04 - previous state is updated most and then
28:08 - as you go into the past they will be
28:10 - updated less and less
28:22 - if you have a system if you have an
28:25 - environment with no absorbing States so
28:27 - the agent will never finish the job then
28:32 - you might run into trouble because the
28:34 - agent moves all the time that reward
28:39 - will add up to infinity right if you
28:41 - have let's say a reward of minus one for
28:45 - each step that it takes it will just go
28:47 - to minus infinity or if you're more
28:49 - positive right you give the agent a
28:51 - reward of plus one for each successful
28:54 - step that it does then the expected
28:58 - reward will end up plus infinity so this
29:01 - would for instance be the case if you
29:04 - have a situation like
29:07 - Paul balancing writes imagine you have a
29:11 - little card here that can move left and
29:18 - right and you have a pole that's being
29:26 - balanced right so in this case right now
29:29 - and what the cop can do it can move left
29:32 - and right and it's limited right it it's
29:37 - limited here and the left and right so
29:39 - it has to balance the pole within that
29:42 - one meter or whatever right now it
29:46 - shouldn't problem move to the right so
29:50 - that it
29:52 - balances the pole the state in this
29:55 - situation might be described by an angle
30:04 - this angle and maybe the position of the
30:07 - cart would be two dimensional state
30:10 - space maybe also velocity that can vary
30:13 - now the job for this agent is to balance
30:17 - this pole and so you would give it
30:20 - positive reward for every second that it
30:23 - manages to balance the pole and if you
30:25 - would just add that up and if the agent
30:27 - is successful then this reward would add
30:31 - up to infinity so you have an ill-posed
30:34 - problem what you can do about this is
30:37 - you can discount the reward right so the
30:46 - expected reward would not be just the
30:48 - sum of all the reward that it
30:50 - accumulates but as you go further into
30:53 - the future
30:57 - the reward is discounted by a gamma
31:00 - factor now so T goes to infinity so the
31:05 - exponent becomes larger and larger t
31:07 - zero is the current time step and you
31:11 - can imagine that if this is a factor
31:13 - between zero and one then with this
31:16 - exponent through what in the very future
31:18 - would not count anymore basically right
31:21 - so this is a way to define to well
31:24 - define expected reward if you have a an
31:27 - environment with no absorbing States
31:33 - now in order to reflect this expected
31:35 - reward in the value function you need to
31:38 - discount you need to discount factor
31:42 - there as well so the update rule that we
31:45 - had above would be corrected by just
31:49 - adding this gamma factor here so the new
31:52 - value off state s is the reward reward
31:57 - that you get by taking action a in this
31:59 - state s plus the value in the future
32:03 - states or s of s and a is the state
32:06 - where the agent goes into but discounted
32:09 - by gamma and if you do that you end up
32:14 - exactly with this equation of expected
32:16 - reward because of course this value
32:19 - contains also information about the
32:23 - value one step ahead but that would get
32:27 - two gammas right gamma times gamma
32:30 - because it's two steps ahead and that in
32:32 - the end leads to this gamma exponent
32:36 - expression that you have up here
32:47 - I've mentioned already that it's also
32:53 - possible to deal with non determined ik
32:56 - deterministic transitions
33:00 - but you have to adapt the questions a
33:03 - bit by later for that so let's assume we
33:09 - have a transition we have transition
33:10 - probabilities so if the system is in
33:12 - state s and takes action a then it will
33:16 - not deterministically go into an new
33:19 - state s dash but there will be probable
33:20 - probabilities that are expressed here as
33:22 - P of s dash given s and a
33:26 - now if you had these probabilities you
33:30 - could update the value of the state by
33:34 - adding
33:37 - the reward that you get for taking that
33:39 - action in that state and a weighted
33:44 - average over the values of the state you
33:48 - might end up in or you might transition
33:52 - to and the probabilities are these P of
33:56 - s - given s and a problem is these
33:59 - probabilities are not known so they have
34:01 - to be learned along with the values so
34:06 - one possibility would be to really just
34:09 - count the numbers you end up in a
34:12 - particular state if you are to count how
34:17 - often you end up in state s - if you end
34:20 - state s and take action a so we can
34:24 - assume these probabilities however
34:27 - that's expensive in terms of of memory
34:30 - capacity at least
34:34 - so what you actually do is you learn the
34:40 - so called cue function that's expressed
34:42 - down here the Q function assigns a value
34:47 - to particular state action combination
34:51 - all right so the values are not own now
34:55 - not the assigned to a state but to a
34:58 - state action pair
35:02 - if you have that it's easy to calculate
35:05 - the value of a state by simply taking
35:08 - the maximum over Q over all actions
35:12 - right obviously the agent and would take
35:15 - the action that promises the highest Q
35:19 - value now and that's described down here
35:22 - so the action taken would be the one
35:25 - that gives the highest Q value if you
35:28 - run over all a - possible actions
35:38 - now how do we update queue now if we had
35:45 - a deterministic situation we would
35:48 - update our queue for state s and action
35:51 - a by the reward of SNA and the value of
35:57 - the state we transition to write as -
36:00 - would be the deterministically
36:02 - determined new state given we are in
36:05 - state s and take action a
36:09 - however in the probabilistic case where
36:13 - we have non-deterministic transitions
36:16 - this s - is not clearly determined right
36:19 - sometimes maybe with 20% it would be you
36:22 - would go left and with 50% he would go
36:25 - down over 30% he would go right or
36:27 - something so you would need to average
36:29 - over the different states
36:33 - I mean if you had the probabilities up
36:40 - there we could actually do a similar
36:43 - trick like here yeah but we don't have
36:45 - that so what we simply do is we have a
36:50 - we use a so called leaky integrator so
36:54 - we don't replace the old value which is
36:58 - QT of s an a completely by this new
37:01 - value but if we simply replace a
37:05 - fraction of it by the new value which is
37:08 - this one here and alpha is the fraction
37:11 - that determines how much we replaced by
37:13 - the new value and we keep partly the old
37:16 - value
37:22 - so let me illustrate that so
37:46 - so let this be the Q value and the
37:50 - initial Q value should be 1
38:03 - yeah and and now let's not worry about
38:07 - the different states etc let's just say
38:10 - there comes sort of two different values
38:13 - namely 1 and 0 and they come with 50%
38:17 - chance so let's just look what happens
38:20 - with Q if we updated according to such a
38:24 - rule where we just assumed the value
38:25 - that's that that we just get or if we
38:29 - update it to according this rule where
38:31 - we just replace 20% of the Q value by
38:33 - the incoming value and leave 80% at the
38:38 - old value so let's first do the
38:41 - deterministic thing so we get the values
38:45 - 0 and 1 with 50% probability and we
38:51 - start with a value of 1 now let's assume
38:56 - the next value is also 1 I mean then
39:00 - obvious obviously we take that value the
39:03 - next value would be 0 so we will just
39:06 - take the 0 value then maybe a 0 again
39:09 - and then 1 get 1 again 0 a 1 a 1 etc
39:17 - yeah so in that case so if we use this
39:21 - version here the value would just jump
39:23 - back and forth between the two
39:24 - possibilities and the two possibilities
39:27 - correspond to the 2s - possible values
39:33 - that we here assume now if we use this
39:36 - rule down here it's different so we add
39:40 - one and we get another one so that would
39:42 - be of course again a 1
39:49 - so we have the same value do a little
39:53 - cross here now if the next value is 0
39:57 - and we replace 80% of this value we
40:00 - don't set the value exactly to 0 but we
40:02 - just replace 80% 20% of that by 0 then
40:06 - we get an open date now the next value
40:10 - is a 0 again so we go down another 20%
40:14 - the next value is a 1 so we go up 20% on
40:18 - the wave from this value to this value
40:20 - so we have a value here miss let's say
40:24 - then we go down again a little bit you
40:27 - go up a left a little bit and go up a
40:30 - little bit further but and what happens
40:32 - is that depending on whether you have
40:35 - zeros or 1 it's sort of zigzags back and
40:38 - forth always just moving 20% and it will
40:42 - average
40:46 - in the end around this 0.5 which is the
40:50 - theoretical prediction now so that's
40:54 - what you get but that's what you can
40:57 - achieve if you use this leaky integrator
41:00 - or Lucas leaky summation as it would be
41:03 - probably better called if alpha is large
41:07 - you jump back and forth more if alpha is
41:10 - small you jump back and forth less but
41:13 - you also converge slower to the true
41:15 - value so this is a parameter you have to
41:18 - think about
41:25 - okay I said that using this Q value is
41:31 - more efficient than estimating these
41:35 - probabilities because if you have 1,000
41:39 - states and ten actions in each state
41:41 - then this would be ten million thousand
41:45 - times thousand times ten ten million
41:47 - probabilities you have to estimate while
41:49 - Q only has a variables s and a so that
41:52 - would be ten thousand rather than ten
41:54 - million and that's a big difference not
41:56 - so therefore this is more efficient than
41:59 - estimating these Peas
42:01 - these probabilities and then going from
42:04 - there
42:22 - now if you have a probabilistic system
42:24 - so if the transitions are not
42:26 - deterministic you have a trade-off
42:28 - between exploration and exploitation now
42:32 - exploration means you try different
42:34 - things right so if you're in a state and
42:36 - you have three possible actions you try
42:39 - all of them out in order to figure out
42:41 - okay what will be the expected reward if
42:43 - you take that action
42:45 - exploitation means if you've learned
42:48 - already something about the the payoffs
42:51 - off the actions in that state then you
42:55 - should take the action that gives you
42:57 - the that promises the largest payoff
42:59 - right so that would be exploiting what
43:01 - you have learned already and this is a
43:04 - trade-off because if you to quickly
43:12 - today go into the exploitation mode so
43:15 - you might not have learnt the the
43:19 - expected rewards so that your Q values
43:21 - might just not be right you know and I
43:24 - want to illustrate this with this
43:25 - example here so let's assume the agent
43:29 - always starts in state s1 and then it
43:34 - can assume can take the action a 1 or a
43:38 - 0 and from then on it just runs through
43:42 - this whole thing so there's just one
43:43 - action to take
43:46 - so if it takes action one it gets a
43:49 - reward of one here transitions too as
43:50 - for it gets another reward of one and
43:53 - transitions to as five right so quite
43:57 - obviously the expected reward along this
43:59 - line along this path would be two
44:04 - yeah so
44:18 - so Q of s 1 and a equals 1 would be 2
44:36 - now let's assume it has and this is
44:44 - something that the agent can figure out
44:46 - with I mean going through there ones
44:48 - right but let's assume it has sort of
44:51 - tried four times the two actions a
44:54 - equals zero and a equals one and with
44:57 - equal probability so n is choosing this
44:59 - twice and this twice now if it chooses
45:01 - this twice and it once goes over the top
45:05 - path here and once it goes to take this
45:09 - lower path Yeah right then the system if
45:14 - it's if you just average over this and
45:16 - don't don't worry about the details of
45:20 - this this leaky summation right then we
45:26 - would get a Q of s 1 and a equals 0 off
45:33 - well if you go this way
45:37 - we have a reward of minus 2 if you go
45:39 - this way we have a reward of plus 4 so
45:44 - the reward on average
45:47 - averaging just over these two trials
45:50 - would be one now because the minus 3 and
45:54 - the plus 3 cancel out however if you
45:58 - look at this one here we see that the
46:00 - probability of getting a minus 2 is 0.1
46:05 - while the probability of going down here
46:08 - is 0.9 now so this would be the
46:12 - estimated Q value after the two trials
46:16 - which happen to be one up here and one
46:19 - down here but the true QL value would be
46:25 - so maybe so this is estimated and this
46:33 - would be two Q of s 1 and a equals 0 so
46:44 - that would be 0.1 x minus 2
46:51 - Plus 0.9 times 4 and that would be I
47:03 - mean if this were a 1 then this would be
47:05 - 4 but we have to reduce this by 10% so
47:08 - that would be 3 point 6 and then we
47:10 - reduce it by another 0.2 so that would
47:13 - be 3 point 4
47:24 - so here we see that by just averaging
47:31 - over the two trials that we had we get a
47:34 - to low value by chance and if you would
47:38 - now decide to always choose a equals one
47:43 - because there we get reliably a value of
47:45 - 2 which is greater than this one then we
47:48 - would just have not done the right thing
47:51 - right because the true probability or
47:53 - the true expected reward here for equal
47:56 - zero would be three point four now so
48:00 - you have to try often enough the
48:04 - different options before you decide on
48:08 - one particular strategy or policy if you
48:16 - ain't a determined if here in a
48:17 - non-deterministic system
48:34 - reinforcement learning has successfully
48:36 - been used also for game playing for
48:39 - training and game playing agent now in
48:43 - that case you have two players so how do
48:47 - you deal with the situation that you
48:48 - have two players and you want to train
48:49 - reinforcement learning
48:50 - well that's relatively simple an
48:53 - illustrated in this picture you simply
48:55 - treat the other agent as part of
48:58 - environment and then it becomes
49:00 - obviously it becomes a probabilistic
49:02 - environment because the other agent
49:04 - might change its policy on the way
49:10 - now but otherwise it goes through
49:13 - straightforward right so let's assume we
49:15 - have such an environment and we haven't
49:21 - we have him
49:23 - sort of the this is a state in which the
49:27 - game is right now and we have a player a
49:29 - and a player B that alternate in their
49:34 - decision right so in this case player B
49:36 - starts first and the goal of knowledge
49:42 - is check
49:47 - yeah so the goal of and the goal of
49:51 - player of player a is to reach this
49:55 - absorbing state while the goal of player
49:58 - B is to reach this absorbing state so
50:02 - let's assume B starts it moves down
50:08 - then it's the turn of player a it moves
50:11 - left then it's a turn of player B turns
50:15 - down again then it's turn the turn of
50:20 - player a moves left again etc so that
50:22 - way you have this path now if you only
50:25 - look at the role of player a for play a
50:29 - it looks like it takes a decision now
50:33 - this here takes a decision
50:36 - namely left and it ends up down so this
50:40 - is a probabilistic decision
50:42 - probabilistic transition to this
50:45 - downfield
50:46 - to the lower left field due to the
50:50 - action left
50:55 - then it takes the action left again and
50:58 - then adds ends up here but it could also
51:01 - have ended up here if player beef had
51:03 - decided to move to the left right so the
51:06 - other player adds some randomness to the
51:10 - transitions to the transitions here for
51:14 - player a and in this case player has one
51:17 - right because player B has dum dum moves
51:23 - okay so the last extension I want to
51:25 - talk about is large is an extension for
51:28 - large state spaces in particular games
51:31 - because you have many many positions
51:33 - like backgammon for example that's a
51:36 - game that has been played by
51:38 - successfully trained with reinforcement
51:41 - learning these games have a huge state
51:46 - space right so there are many different
51:47 - ways the stones can be on the on the
51:51 - board much too large for the standard
51:55 - discrete reinforcement learning to work
51:58 - that we have seen so far so what you can
52:01 - do in these cases you can approximate
52:03 - the value function by a neural network
52:06 - let's say so you need some fun function
52:08 - approximator
52:09 - that works in this state space but has
52:11 - much fewer parameters than you would
52:13 - need to represent all the values of the
52:16 - different states
52:23 - so these were a few simple extensions to
52:29 - address some problems that you might run
52:31 - into in reinforcement learning as of
52:35 - course much more to it but I will stop
52:38 - here
52:46 - you