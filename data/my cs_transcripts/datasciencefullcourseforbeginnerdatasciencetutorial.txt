00:00 - Welcome to Data Science: An Introduction.
I'm Barton Poulson and what we are going to
00:05 - do in this course is We are going to have
a brief, accessible and non-technical overview
00:11 - of the field of Data Science. Now, some people
when they hear Data Science, they start thinking
00:16 - things like: Data and think about piles of
equations and numbers and then throw on top
00:22 - of that Science and think about people working
in their lab and they start to say eh, that's
00:28 - not for me. I'm not really a technical person
and that just seems much too techy. Well,
00:35 - here's the important thing to know. While
a lot of people get really fired up about
00:38 - the technical aspects of Data Science the
important thing is that Data Science is not
00:43 - so much a technical discipline, but creative.
And, really, that's true. The reason I say
00:50 - that is because in Data Science you use tools
that come from coding and statistics and from
00:57 - math But you use those to work creatively
with data. The idea is there's always more
01:04 - than one way to solve a problem or answer
a question But most importantly to get insight
01:10 - Because the goal, no matter how you go about
it, is to get insight from your data. and
01:15 - what makes Data Science unique, compared to
so many other things is that you try to listen
01:20 - to all of your data, even when it doesn't
fit in easily with your standard approaches
01:27 - and paradigms you're trying to be much more
inclusive in your analysis and the reason
01:33 - you want to do that is because everything
signifies. everything carries meaning and
01:39 - everything can give you additional understanding
and insight into what's going on around you
01:44 - and so in this course what we are trying to
do is give you a map to the field of Data
01:49 - Science and how you can use it and so now
you have the map in your hands and you can
01:54 - get ready to get going with Data Science.
Welcome back to Data Science: An Introduction.
02:01 - And we're going to begin this course by defining
data science. That makes sense. But, we are
02:06 - going to be doing it in kind of a funny way.
The first thing I am going to talk about is
02:10 - the demand for data science. So, let's take
a quick look. Now, data science can be defined
02:17 - in a few ways. I am going to give you some
short definitions. Take one on my definition
02:23 - is that data science is coding, math, and
statistics in applied settings. That’s a
02:29 - reasonable working definition. But, if you
want to be a little more concise, I've got
02:33 - take two on a definition. That data science
is the analysis of diverse data, or data that
02:39 - you didn't think would fit into standard analytic
approaches. A third way to think about it
02:45 - is that data science is inclusive analysis.
It includes all of the data, all of the information
02:51 - that you have, in order to get the most insightful
and compelling answer to your research questions.
02:59 - Now, you may say to yourself, “Wait... that's
it?” Well, if you're not impressed, let
03:07 - me show you a few things. First off, let's
take a look at this article. It says, “Data
03:13 - Scientist: the Sexiest Job of the 21st Century.”
And please note, this is coming from Harvard
03:18 - Business Review. So, this is an authoritative
source and it is the official source of this
03:24 - saying: that data science is sexy! Now, again,
you may be saying to yourself, “Sexy? I
03:32 - hardly think so.” Oh yeah, it's sexy. And
the reason data science is sexy is because
03:39 - first, it has rare qualities, and second it
has high demand. Let me say a little more
03:45 - about those. The rare qualities are that data
science takes unstructured data, then finds
03:51 - order, meaning, and value in the data. Those
are important, but they're not easy to come
03:56 - across. Second, high demand. Well, the reason
it's in high demand is because data science
04:02 - provides insight into what’s going on around
you and critically, it provides competitive
04:08 - advantage, which is a huge thing in business
settings. Now, let me go back and say a little
04:15 - more about demand. Let's take a look at a
few other sources. So, for instance the McKinsey
04:20 - Global Institute published a very well-known
paper, and you can get it with this URL. And
04:26 - if you go to that webpage, this is what's
going to come up. And we're going to take
04:30 - a quick look at this one, the executive summary.
It's a PDF that you can download. And if you
04:35 - open that up, you will find this page. And
let's take a look at the bottom right corner.
04:41 - Two numbers here, I'm going to zoom in on
those. The first one, is they are projecting
04:46 - a need in the next few years for somewhere
between 140 and 190,000 deep analytical talent
04:54 - positions. So, this means actual practicing
data scientists. That’s a huge number; but
05:00 - almost ten times as high is 1.5 million more
data-savvy managers will be needed to take
05:08 - full advantage of big data in the United States.
Now, that's people who aren't necessarily
05:14 - doing the analysis but have to understand
it, who have to speak data. And that’s one
05:18 - of the main purposes of this particular course,
is to help people who may or may not be the
05:24 - practicing data scientists learn to understand
what they can get out of data, and some of
05:29 - the methods used to get there. Let's take
a look at another article from LinkedIn. Here
05:33 - is a shortcut URL for it and that will bring
you to this webpage: “The 25 hottest job
05:38 - skills that got people hired in 2014.” And
take a look at number one here: statistical
05:44 - analysis and data mining, very closely related
to data science. And just to be clear, this
05:49 - was number one in Australia, and Brazil, and
Canada, and France, and India, and the Netherlands,
05:56 - and South Africa, and the United Arab Emirates,
and the United Kingdom. Everywhere. And if
06:04 - you need a little more, let's take a look
at Glassdoor, which published an article this
06:09 - year, 2016, and it's about the “25 Best
Jobs in America.” And look at number one
06:18 - right here, it's data scientist. And we can
zoom on this information. It says there is
06:25 - going to be 1,700 job openings, with a median
base salary of over $116,000, and fabulous
06:32 - career opportunities and job scores. So, if
you want to take all of this together, the
06:37 - conclusion you can reach is that data science
pays. And I can show you a little more about
06:41 - that. So for instance, here's a list of the
top ten highest paying salaries that I got
06:47 - from US News. We have physicians (or doctors),
dentists, and lawyers, and so on. Now, if
06:53 - we add data scientist to this list, using
data from O'Reilly.com, we have to push things
06:59 - around. And goes in third with an average
total salary (not the base we had in the other
07:06 - one, but the total compensation) of about
$144,000 a year. That's extraordinary. So
07:14 - in sum, what do we get from all this? First
off, we learn that there is a very high demand
07:18 - for data science. Second, we learn that there
is a critical need for both specialists; those
07:24 - are the sort of practicing data scientists;
and for Generalists, the people who speak
07:29 - the language and know what can be done. And
of course, excellent pay. And all together,
07:35 - this makes Data Science a compelling career
alternative and a way of making you better
07:40 - at whatever you are doing. Back here in data
science, we're going to continue our attempt
07:47 - to define data science by looking at something
that's really well known in the field; the
07:51 - Data Science Venn Diagram. Now if you want
to, you can think of this in terms of, “What
07:57 - are the ingredients of data science?” Well,
we're going to first say thanks to Drew Conway,
08:04 - the guy who came up with this. And if you
want to see the original article, you can
08:08 - go to this address. But, what Drew said is
that data science is made of three things.
08:15 - And we can put them as overlapping circles
because it is the intersection that’s important.
08:19 - Here on the top left is coding or computer
programming, or as he calls it: hacking. On
08:25 - the top right is stats or, stats or mathematics,
or quantitative abilities in general. And
08:30 - on the bottom is domain expertise, or intimate
familiarity with a particular field of practice:
08:37 - business, or health, or education, or science,
or something like that. And the intersection
08:42 - here in the middle, that is data science.
So it's the combination of coding and statistics
08:48 - and math and domain knowledge. Now, let's
say a little more about coding. The reason
08:54 - coding is important is because it helps you
gather and prepare the data. Because a lot
08:59 - of the data comes from novel sources and is
not necessarily ready for you to gather and
09:04 - it can be in very unusual formats. And so
coding is important because it can require
09:09 - some real creativity to get the data from
the sources to put it into your analysis.
09:16 - Now, a few kinds of coding that are important;
for instance, there is statistical coding.
09:23 - A couple of major languages in this are R
and Python. Two open-source free programming
09:30 - languages. R, specifically for data. Python
is general-purpose, but well adapted to data.
09:35 - The ability to work with databases is important
too. The most common language there is SQL,
09:41 - usually pronounced “Sequel,” which stands
for Structured Query Language, because that's
09:46 - where the data is. Also, there is the command
line interface, or if you are on a Mac, people
09:52 - just call it “the terminal.” Most common
language there is Bash, which actually stands
09:57 - for Bourne-again shell. And then searching
is important and regex, or regular expressions.
10:04 - While there is not a huge amount to learn
there (it’s a small little field), it’s
10:09 - sort of like super-powered wildcard searching
that makes it possible for you to both find
10:14 - the data and reformat it in ways that are
going to be helpful for your analysis. Now,
10:18 - let's say a few things about the math. You’re
going to need things like a little bit of
10:25 - probability, some algebra, of course, regression
(very common statistical procedure). Those
10:29 - things are important. And the reason you need
the math is: because that is going to help
10:33 - you choose the appropriate procedures to answer
the question with the data that you have.
10:39 - And probably even more importantly; it is
going to help you diagnose problems when things
10:42 - don’t go as expected. And given that you
are trying to do new things with new data
10:46 - in new ways, you are probably going to come
across problems. So the ability to understand
10:51 - the mechanics of what is going on is going
to give you a big advantage. And the third
10:56 - element of the data science Venn Diagram is
some sort of domain expertise. Think of it
11:00 - as expertise in the field that you're in.
Business settings are common. You need to
11:05 - know about the goals of that field, the methods
that are used, and the constraints that people
11:10 - come across. And it’s important because
whatever your results are, you need to be
11:14 - able to implement them well. Data science
is very practical and is designed to accomplish
11:19 - something. And your familiarity with a particular
field of practice is going to make it that
11:24 - much easier and more impactful when you implement
the results of your analysis. Now, let's go
11:32 - back to our Venn Diagram here just for a moment.
Because this is a Venn, we also have these
11:36 - intersections of two circles at a time. At
the top is machine learning. At the bottom
11:42 - right is traditional research. And on the
bottom left hand is what Drew Conway called,
11:46 - “the danger zone.” Let me talk about each
of these. First off, machine learning, or
11:51 - ML. Now, you think about machine learning
and the idea here is that it represents coding,
11:57 - or statistical programming and mathematics,
without any real domain expertise. Sometimes
12:03 - these are referred to as "black box" models.
They kind of throw data in and you don’t
12:07 - even necessarily have to know what it means
or what language it is in, and it will just
12:11 - kind of crunch through it all and it will
give you some regularities. That can be very
12:15 - helpful, but machine learning is considered
slightly different from data science because
12:21 - it doesn’t involve the particular applications
in a specific domain. Also, there's traditional
12:28 - research. This is where you have math or statistics
and you have domain knowledge; often very
12:32 - intensive domain knowledge but without the
coding or programming. Now, you can get away
12:37 - with that because the data that you use in
traditional research is highly structured.
12:41 - It comes in rows and columns, and is typically
complete and is typically ready for analysis.
12:46 - Doesn’t mean your life is easy, because
now you have to expand an enormous amount
12:51 - of effort in the methods and the designing
of the project and the interpretation of the
12:57 - data. So, still very heavy intellectual cognitive
work, but it comes from a different place.
13:04 - And then finally, there is what Conway called,
“the danger zone.” And that's the intersection
13:09 - of coding and domain knowledge, but without
math or statistics. Now he says it is unlikely
13:14 - to happen, and that is probably true. On the
other hand, I can think of some common examples,
13:19 - what are called “word counts,” where you
take a large document or a series of documents,
13:23 - and you count how many times a word appears
in there. That can actually tell you some
13:27 - very important things. And also, drawing maps
and showing how things change across place
13:31 - and maybe even across time. You don’t necessarily
have to have the math, but it can be very
13:36 - insightful and helpful. So, let's think about
a couple of backgrounds where people come
13:43 - from here. First, is coding. You can have
people who are coders, who can do math, stats,
13:48 - and business. So, you get the three things
(and this is probably the most common), most
13:53 - the people come from a programming background.
On the other hand, there is also stats, or
13:58 - statistics. And you can get statisticians
who can code and who also can do business.
14:03 - That's less common, but it does happen. And
finally, there is people who come into data
14:07 - science from a particular domain. And these
are, for instance, business people who can
14:11 - code and do numbers. And they are the least
common. But, all of these are important to
14:17 - data science. And so in sum, here is what
we can take away. First, several fields make
14:24 - up Data Science. Second, diverse skills and
backgrounds are important and they are needed
14:31 - in data science. And third, there are many
roles involved because there are a lot of
14:35 - different things that need to happen. We’ll
say more about that in our next movie. The
14:43 - next step in our data science introduction
and our definition of data science is to talk
14:48 - about the Data Science Pathway. So I like
to think of this as, when you are working
14:54 - on a major project, you have got to do one
step at a time to get it from here to there.
14:59 - In data science, you can take the various
steps and you can put them into a couple of
15:02 - general categories. First, there are the steps
that involve planning. Second, there's the
15:09 - data prep. Third, there's the actual modeling
of the data. And fourth, there's the follow-up.
15:15 - And there are several steps within each of
these; I'll explain each of them briefly.
15:20 - First, let's talk about planning. The first
thing that you need to do, is you need to
15:25 - define the goals of your project so you know
how to use your resources well, and also so
15:30 - you know when you are done. Second, you need
to organize your resources. So you might have
15:35 - data from several different sources; you might
have different software packages, you might
15:40 - have different people. Which gets us to the
third one: you need to coordinate the people
15:44 - so they can work together productively. If
you are doing a hand-off, it needs to be clear
15:49 - who is going to do what and how their work
is going to go together. And then, really
15:54 - to state the obvious, you need to schedule
the project so things can move along smoothly
15:58 - and you can finish in a reasonable amount
of time. Next is the data prep, where you
16:03 - are taking like food prep and getting the
raw ingredients ready. First of course, is
16:08 - you need to get the data. And it can from
many different sources and be in many different
16:13 - formats. You need to clean the data and, the
sad thing is, this tends to be a very large
16:20 - part of any data science project. And that
is because you are bringing in unusual data
16:24 - from a lot of different places. You also want
to explore the data; that is, really see what
16:31 - it looks like, how many people are in each
group, what the shape of the distributions
16:35 - are like, what is associated with what. And
you may need to refine the data. And that
16:40 - means choosing variables to include, choosing
cases to include or exclude, making any transformations
16:46 - to the data you need to do. And of course
these steps kind of can bounce back and forth
16:51 - from one to the other. The third group is
modeling or statistical modeling. This is
16:57 - where you actually want to create the statistical
model. So for instance, you might do a regression
17:02 - analysis or you might do a neural network.
But, whatever you do, once you create your
17:07 - model, you have to validate the model. You
might do that with a holdout validation. You
17:13 - might do it really with a very small replication
if you can. You also need to evaluate the
17:19 - model. So, once you know that the model is
accurate, what does it actually mean and how
17:25 - much does it tell you? And then finally, you
need to refine the model. So, for instance,
17:31 - there may be variables you want to throw out;
maybe additional ones you want to include.
17:35 - You may want to, again, transform some of
the data. You may want to get it so it is
17:41 - easier to interpret and apply. And that gets
us to the last part of the data science pathway.
17:47 - And that's follow up. And once you have created
your model, you need to present the model.
17:52 - Because it is usually work that is being done
for a client, could be in house, could be
17:57 - a third party. But you need to take the insights
that you got and share them in a meaningful
18:02 - way with other people. You also need to deploy
the model; it is usually being done in order
18:08 - to accomplish something. So, for instance,
if you are working with an e-commerce site,
18:12 - you may be developing a recommendation engine
that says, “people who bought this and this
18:17 - might buy this.” You need to actually stick
it on the website and see if it works the
18:20 - way that you expected it to. Then you need
to revisit the model because a lot of the
18:25 - times, the data that you worked on is not
necessarily all of the data, and things can
18:31 - change when you get out in the real world
or things just change over time. So, you have
18:36 - to see how well your model is working. And
then, just to be thorough, you need to archive
18:43 - the assets, document what you have, and make
it possible for you or for others to repeat
18:48 - the analysis or develop off of it in the future.
So, those are the general steps of what I
18:54 - consider the data science pathway. And in
sum, what we get from this is three things.
18:59 - First, data science isn’t just a technical
field, it is not just coding. Things like,
19:04 - planning and presenting and implementing are
just as important. Also, contextual skills,
19:10 - knowing how it works in a particular field,
knowing how it will be implemented, those
19:16 - skills matter as well. And then, as you got
from this whole thing, there are a lot of
19:21 - things to do. And if you go one step at a
time, there will be less backtracking and
19:26 - you will ultimately be more productive in
your data science projects. We'll continue
19:32 - our definition of data science by looking
at the roles that are involved in data science.
19:37 - The way that different people can contribute
to it. That's because it tends to be a collaborative
19:43 - thing, and it's nice to be able to say that
we are all together, working together towards
19:46 - a single goal. So, let’s talk about some
of the roles involved in data science and
19:51 - how they contribute to the projects. First
off, let's take a look at engineers. These
19:56 - are people who focus on the back end hardware.
For instance, the servers and the software
20:01 - that runs them. This is what makes data science
possible, and it includes people like developers,
20:08 - software developers, or database administrators.
And they provide the foundation for the rest
20:14 - of the work. Next, you can also have people
who are Big Data specialists. These are people
20:20 - who focus on computer science and mathematics,
and they may do machine learning algorithms
20:27 - as a way of processing very large amounts
of data. And they often create what are called
20:32 - data products. So, a thing that tells you
what restaurant to go to, or that says, “you
20:37 - might know these friends,” or provides ways
of linking up photos. Those are data products,
20:42 - and those often involve a huge amount of very
technical work behind them. There are also
20:49 - researchers; these are people who focus on
domain-specific research. So, for instance,
20:54 - physics, or genetics, or whatever. And these
people tend to have very strong statistics,
21:01 - and they can use some of the procedures and
some of the data that comes from the other
21:05 - people like the big data researchers, but
they focus on the specific questions. Also
21:12 - in the data science realm, you will find analysts.
These are people who focus on the day-to-day
21:16 - tasks of running a business. So for instance,
they might do web analytics (like Google analytics),
21:21 - or they might pull data from a SQL database.
And this information is very important and
21:29 - good for business. So, analysts are key to
the day-to-day function of business, but they
21:35 - may not be, exactly be Data Science proper,
because most of the data they are working
21:41 - with is going to be pretty structured. Nevertheless,
they play a critical role in business in general.
21:47 - And then, speaking of business. You have the
actual business people; the men and women
21:52 - who organize and run businesses. These people
need to be able to frame business-relevant
21:58 - questions that can be answered with the data.
Also, the business person manages the project
22:04 - and the efforts and the resources of others.
And while they may not actually be doing the
22:09 - coding, they must speak data; they must know
how the data works, what it can answer, and
22:16 - how to implement it. You can also have entrepreneurs.
So, you might have a data startup; they are
22:22 - starting their own little social network,
their own little web search platform. An entrepreneur
22:29 - needs data and business skills. And truthfully,
they have to be creative at every step along
22:34 - the way. Usually because they are doing it
all themselves at a smaller scale. Then we
22:42 - have in data science something known as “the
full stack unicorn.” And this is a person
22:47 - who can do everything at an expert level.
They are called a unicorn because truthfully,
22:51 - they may not actually exist. I will have more
to say about that later. But for right now,
22:56 - we can sum up what we got out of this video
by three things. Number one, data science
23:02 - is diverse. There's a lot of different people
who go into it, and they have different goals
23:07 - for their work, and they bring in different
skills and different experiences and different
23:12 - approaches. Also, they tend to work in very
different contexts. An entrepreneur works
23:17 - in a very different place from a business
manager, who works in a very different place
23:20 - from an academic researcher. But, all of them
are connected in some way to data science
23:26 - and make it a richer field. The last thing
I want to say in “Data Science: An Introduction”
23:34 - where I am trying to define data science,
is to talk about teams in data science. The
23:40 - idea here is that data science has many different
tools, and different people are going to be
23:46 - experts in each one of them. Now, you have,
for instance, coding and you have statistics.
23:53 - Also, you have what feels like design, or
business and management that are involved.
23:58 - And the question, of course, is: “who can
do all of it? Who's able to do all of these
24:02 - things at the level that we need?” Well,
that's where we get this saying (I have mentioned
24:07 - it before), it's the unicorn. And just like
in ancient history, the unicorn is a mythical
24:15 - creature with magical abilities. In data science,
it works a little differently. It is a mythical
24:21 - Data Scientist with universal abilities. The
trouble is, as we know from the real world,
24:26 - there are really no unicorns (animals), and
there are really not very many unicorns in
24:32 - data science. Really, there are just people.
And so we have to find out how we can do the
24:37 - projects even though we don’t have this
one person who can do everything for everybody.
24:42 - So let’s take a hypothetical case, just
for a moment. I am going to give you some
24:44 - fictional people. Here is my fictional person
Otto, who has strong visualization skills,
24:52 - who has good coding, but has limited analytic
or statistical ability. And if we graph his
24:58 - stuff out, his abilities... So, here we have
five things that we need to have happen. And
25:04 - for the project to work, they all have to
happen at least, a level of eight on the zero-to-ten.
25:10 - If we take his coding ability, he is almost
there. Statistics, not quite halfway. Graphics,
25:17 - yes he can do that. And then, business, eh,
alright. And project, pretty good. So, what
25:23 - you can see here is, in only one of these
five areas is Otto sufficient on his own.
25:29 - On the other hand, let's pair him up with
somebody else. Let’s take a look at Lucy.
25:34 - And Lucy has strong business training, has
good tech skills, but has limited graphics.
25:39 - And if we get her profile on the same thing
that we saw, there is coding, pretty good.
25:45 - Statistics, pretty good. Graphics, not so
much. Business, good. And projects, OK. Now,
25:52 - the important thing here is that we can make
a team. So let’s take our two fictional
25:58 - people, Otto and Lucy, and we can put together
their abilities. Now, I actually have to change
26:02 - the scale here a little bit to accommodate
the both of them. But our criterion still
26:07 - is at eight; we need a level of eight in order
to do the project competently. And if we combine
26:13 - them: oh look, coding is now past eight. Statistics
is past eight. Graphics is way past. Business
26:21 - way past. And then the projects, they are
too. So when we combine their skills, we are
26:27 - able to get the level that we need for everything.
Or to put it another way, we have now created
26:33 - a unicorn by team, and that makes it possible
to do the data science project. So, in sum:
26:41 - you usually can’t do data science on your
own. That's a very rare individual. Or more
26:46 - specifically: people need people, and in data
science you have the opportunity to take several
26:53 - people and make collective unicorns, so you
can get the insight that you need in your
26:58 - project and you can get the things done that
you want. In order to get a better understanding
27:05 - of data science, it can be helpful to look
at contrasts between data science and other
27:10 - fields. Probably the most informative is with
Big Data because these two terms are actually
27:15 - often confused. It makes me think of situations
where you have two things that are very similar,
27:21 - but not the same. Like we have here in the
Piazza San Carlo here in Italy. Part of the
27:26 - problem stems from the fact that data science
and big data both have Venn Diagrams associated
27:32 - with them. So, for instance, Venn number one
for data science is something we have seen
27:37 - already. We have three circles and we have
coding and we have math and we have some domain
27:44 - expertise, that put together get data science.
On the other hand, Venn Diagram number two
27:51 - is for Big Data. It also has three circles.
And we have the high volume of data, the rapid
27:58 - velocity of data, and the extreme variety
of data. Take those three v's together and
28:03 - you get Big Data. Now, we can also combine
these two if we want in a third Venn Diagram,
28:10 - we call Big Data and Data Science. This time
it is just two circles. With Big Data on the
28:15 - left and Data Science on the right. And the
intersection in the middle, there is Big Data
28:20 - Science, which actually is a real term. But,
if you want to do a compare and contrast,
28:26 - it kind of helps to look at how you can have
one without the other. So, let's start by
28:30 - looking at Big Data without Data Science.
So, these are situations where you may have
28:37 - the volume or velocity or variety of data
but don’t need all the tools of data science.
28:42 - So, we are just looking at the left side of
the equation right now. Now, truthfully, this
28:49 - only works if you have Big Data without all
three V’s. Some say you have to have the
28:53 - volume, velocity, and variety for it to count
as Big Data. I basically say anything that
28:58 - doesn’t fit into a standard machine is probably
Big Data. I can think of a couple of examples
29:04 - here of things that might count as Big Data,
but maybe don't count as Data Science. Machine
29:10 - learning, where you can have very large data
sets and probably very complex, doesn’t
29:16 - require very much domain expertise, so that
may not be data science. Word counts, where
29:21 - you have an enormous amount of data and it's
actually a pretty simple analysis, again doesn’t
29:28 - require much sophistication in terms of quantitative
skills or even domain expertise. So, maybe/maybe
29:35 - not data science. On the other hand, to do
any of these you are going to need to have
29:39 - at least two skills. You are going to need
to have the coding and you will probably have
29:43 - to have some sort of quantitative skills as
well. So, how about data science without Big
29:51 - Data? That’s the right side of this diagram.
Well, to make that happen you are probably
29:58 - talking about data with just one of the three
V's from Big Data. So, either volume or velocity
30:05 - or variety, but singly. So for instance, genetics
data. You have a huge amount of data and it
30:13 - comes in very set structure and it tends to
come in at once. So, you have got a lot of
30:18 - volume and it is a very challenging thing
to work with. You have to use data science,
30:23 - but it may or may not count as Big Data. Similarly,
streaming sensor data, where you have data
30:29 - coming in very quickly, but you are not necessarily
saving it; you are just looking at these windows
30:34 - in it. That is a lot of velocity, and it is
difficult to deal with, and it takes Data
30:39 - Science, the full skill set, but it may not
require Big Data, per se. Or facial recognition,
30:46 - where you have enormous variety in the data
because you are getting photos or videos that
30:51 - are coming in. Again, very difficult to deal
with, requires a lot of ingenuity and creativity
30:57 - may or may not count as Big Data, depending
on how much of a stickler you are about definitions.
31:03 - Now, if you want to combine the two, we can
talk about Big Data Science. In that case,
31:10 - we are looking right here at the middle. This
is a situation where you have volume, and
31:15 - velocity, and variety in your data and truthfully,
if you have the three of those, you are going
31:20 - to need the full Data Science skill set. You
are going to need coding, and statistics,
31:26 - and math, and you are going to have to have
domain expertise. Primarily because of the
31:30 - variety you are dealing with, but taken all
together you do have to have all of it. So
31:36 - in sum, here is what we get. Big Data is not
equal to, it is not identical to data science.
31:42 - Now, there is common ground, and a lot of
people who are good at Big Data are good at
31:46 - data science and vice versa, but they are
conceptually distinct. On the other hand,
31:51 - there is the shared middle ground of Big Data
Science that unifies the two separate fields.
32:00 - Another important contrast you can make in
trying to understand data science is to compare
32:04 - it with coding or computer programming. Now,
this is where you are trying to work with
32:09 - machines and you are trying to talk to that
machine, to get it to do things. In one sense
32:14 - you can think of coding as just giving task
instructions; how to do something. It is a
32:19 - lot like a recipe when you're cooking. You
get some sort of user input or other input,
32:24 - and then maybe you have if/then logic, and
you get output from it. To take an extremely
32:29 - simple example, if you are programming in
Python version 2, you write: print, and then
32:34 - in quotes, “Hello, world!” will put the
words “Hello, world!” on the screen. So,
32:40 - you gave it some instructions and it gave
you some output. Very simple programming.
32:46 - Now, coding and data gets a little more complicated.
So, for instance, there is word counts, where
32:54 - you take a book or a whole collection of books,
you take the words and you count how many
32:58 - there are in there. Now, this is a conceptually
simple task, and domain expertise and really
33:06 - math and statistics are not vital. But to
make valid inferences and generalizations
33:14 - in the face of variability and uncertainty
in the data you need statistics, and by extension,
33:22 - you need data science. It might help to compare
the two by looking at the tools of the respective
33:28 - trades. So for instance, there are tools for
coding or generic computer programming, and
33:36 - there are tools that are specific for data
science. So, what I have right here is a list
33:41 - from the IEEE of the top ten programming languages
of 2015. And it starts at Java and C and goes
33:49 - down to Shell. And some of these are also
used for data science. So for instance, Python
33:55 - and R and SQL are used for data science, but
the other ones aren’t major ones in data
34:03 - science. So, let's, in fact, take a look at
a different list of most popular tools for
34:08 - data science and you see that things move
around a little bit. Now, R is at the top,
34:13 - SQL is there, Python is there, but for me
what is the most interesting on the list is
34:17 - that Excel is number five, which would never
be considered programming, per se, but it
34:23 - is, in fact, a very important tool for data
science. And that is one of the ways that
34:28 - we can compare and contrast computer programming
with data science. In sum, we can say this:
34:35 - data science is not equal to coding. They
are different things. On the other hand, they
34:40 - share some of the tools and they share some
practices specifically when coding for data.
34:46 - On the other hand, there is one very big difference
in that statistics, statistical ability is
34:52 - one of the major separators between general
purpose programming and data science programming.
35:01 - When we talk about data science and we are
contrasting with some fields, another field
35:06 - that a lot of people get confused and think
they are the same thing is data science and
35:09 - statistics. Now, I will tell you there is
a lot in common, but we can talk a little
35:15 - bit about the different focuses of each. And
we also get into the issue of definitionalism
35:21 - that data science is different because we
define it differently, even when there is
35:25 - an awful lot in common between the two. It
helps to take a look at some of the things
35:31 - that go on in each field. So, let's start
here about statistics. Put a little circle
35:35 - here and we will put data science. And, to
borrow a term from Steven J. Gould, we can
35:41 - call these non-overlapping magisteria; NOMA.
So, you think of them as separate fields that
35:48 - are sovereign unto themselves with nothing
to do with each other. But, you know, that
35:54 - doesn’t seem right; and part of that is
that if we go back to the Data Science Venn
35:59 - Diagram, statistics is one part of it. There
it is in the top corner. So, now what do we
36:06 - do? What's the relationship? So, it doesn’t
make sense to say these are totally separate
36:11 - areas, maybe data science and statistics because
they share procedures, maybe data science
36:18 - is a subset or specialty of statistics, more
like this. But, if data science were just
36:26 - a subset or specialty within statistics then
it would follow that all data scientists would
36:33 - first be statisticians. And interestingly
that’s just not so. Say, for instance, we
36:41 - take a look at the data science stars, the
superstars in the field. We go to a rather
36:47 - intimidating article; it's called “The World's
7 Most Powerful Data Scientists” from Forbes.com.
36:54 - You can see the article if you go to this
URL. There's actually more than seven people,
37:00 - because sometimes he brings them up in pairs.
Let’s check their degrees, see what their
37:04 - academic training is in. If we take all the
people on this list, we have five degrees
37:09 - in computer science, three in math, two in
engineering, and one each in biology, economics,
37:18 - law, speech pathology, and one in statistics.
And so that tells us, of course, these major
37:26 - people in data science are not trained as
statisticians. Only one of them has formal
37:31 - training in that. So, that gets us to the
next question. Where do these two fields,
37:38 - statistics and data science, diverge? Because
they seem like they should have a lot in common,
37:41 - but they don’t have a lot in training. Specifically,
we can look at the training. Most data scientists
37:49 - are not trained, formally, as statisticians.
Also, in practice, things like machine learning
37:57 - and big data, which are central to data science,
are not shared, generally, with most of statistics.
38:05 - So, they have separate domains there. And
then there is the important issue of context.
38:11 - Data scientists tend to work in different
settings than statisticians. Specifically,
38:17 - data scientists very often work in commercial
settings where they are trying to get recommendation
38:21 - engines or ways of developing a product that
will make them money. So, maybe instead of
38:28 - having data science a subset of statistics,
we can think of it more as these two fields
38:34 - have different niches. They both analyze data,
but they do different things in different
38:40 - ways. So, maybe it is fair to say they share,
they overlap, they have analysis in common
38:47 - of data, but otherwise, they are ecologically
distinct. So, in sum: what we can say here
38:56 - is that data science and statistics both use
data and they analyze it. But the people in
39:02 - each tend to come from different backgrounds,
and they tend to function with different goals
39:09 - and contexts. And in that way, render them
to be conceptually distinct fields despite
39:14 - the apparent overlap. As we work to get a
grasp on data science, there is one more contrast
39:21 - I want to make explicitly, and that is between
data science and business intelligence, or
39:27 - BI. The idea here is that business intelligence
is data in real life; it's very, very applied
39:35 - stuff. The purpose of BI is to get data on
internal operations, on market competitors,
39:43 - and so on, and make justifiable decisions
as opposed to just sitting in the bar and
39:48 - doing whatever comes to your mind. Now, data
science is involved with this, except, you
39:56 - know, really there is no coding in BI. There's
using apps that already exist. And the statistics
40:04 - in business intelligence tend to be very simple,
they tend to be counts and percentages and
40:09 - ratios. And so, it's simple, the light bulb
is simple; it just does its one job there
40:15 - is nothing super sophisticated there. Instead
the focus in business intelligence is on domain
40:21 - expertise and on really useful direct utility.
It’s simple, it’s effective and it provides
40:30 - insight. Now, one of the main associations
with business intelligence is what are called
40:36 - dashboards, or data dashboards. They look
like this; it is a collection of charts and
40:42 - tables that go together to give you a very
quick overview of what is going on in your
40:46 - business. And while a lot of data scientists
may, let's say, look down their nose upon
40:51 - dashboards, I'll say this, most of them are
very well designed and you can learn a huge
40:56 - amount about user interaction and the accessibility
information from dashboards. So really, where
41:05 - does data science come into this? What is
the connection between data science and business
41:09 - intelligence? Well, data science can be useful
to BI in terms of setting it up. Identifying
41:16 - data sources and creating or setting up the
framework for something like a dashboard or
41:21 - a business intelligence system. Also, data
science can be used to extend it. Data science
41:29 - can be used to get past the easy questions
and the easy data, to get the questions that
41:33 - are actually most useful to you; even if they
require really sometimes data that is hard
41:37 - to wrangle and work with. And also, there
is an interesting interaction here that goes
41:41 - the other way. Data science practitioners
can learn a lot about design from good business
41:48 - intelligence applications. So, I strongly
encourage anybody in data science to look
41:54 - at them carefully and see what they can learn.
In sum: business intelligence, or BI, is very
42:01 - goal oriented. Data science perhaps prepares
the data and sets up the form for business
42:08 - intelligence, but also data science can learn
a lot about usability and accessibility from
42:16 - business intelligence. And so, it is always
worth taking a close look. Data science has
42:24 - a lot of real wonderful things about it, but
it is important to consider some ethical issues,
42:29 - and I will specifically call this "do no harm"
in your data science projects. And for that
42:36 - we can say thanks to Hippocrates, the guy
who gave us the Hippocratic Oath of Do No
42:40 - Harm. Let's specifically talk about some of
the important ethical issues, very briefly,
42:46 - that come up in data science. Number one is
privacy. That data tells you a lot about people
42:54 - and you need to be concerned about the confidentiality.
If you have private information about people,
43:00 - their names, their social security numbers,
their addresses, their credit scores, their
43:04 - health, that’s private, that's confidential,
and you shouldn’t share that information
43:10 - unless they specifically gave you permission.
Now, one of the reasons this presents a special
43:15 - challenge in data science because, we will
see later, a lot of the sources that are used
43:20 - in data science were not intended for sharing.
If you scrape data from a website or from
43:25 - PDFs, you need to make sure that it is ok
to do that. But it was originally created
43:30 - without the intention of sharing, so privacy
is something that really falls upon the analyst
43:35 - to make sure they are doing it properly. Next,
is anonymity. One of the interesting things
43:41 - we find is that it is really not hard to identify
people in data. If you have a little bit of
43:47 - GPS data and you know where a person was at
four different points in time, you have about
43:52 - a 95% chance of knowing exactly who they are.
You look at things like HIPAA, that’s the
43:57 - Health Insurance Portability and Accountability
Act. Before HIPAA, it was really easy to identify
44:02 - people from medical records. Since then, it
has become much more difficult to identify
44:07 - people uniquely. That’s an important thing
for really people's well-being. And then also,
44:15 - proprietary data; if you are working for a
client, a company, and they give you their
44:18 - own data, that data may have identifiers.
You may know who the people are, they are
44:23 - not anonymous anymore. So, anonymity may or
may not be there, but major efforts to make
44:28 - data anonymous. But really, the primary thing
is even if you do know who they are, that
44:34 - you still maintain the privacy and confidentiality
of the data. Next, there is the issue about
44:40 - copyright, where people try to lock down information.
Now, just because something is on the web,
44:47 - doesn’t mean that you are allowed to use
it. Scraping data from websites is a very
44:52 - common and useful way of getting data for
projects. You can get data from web pages,
44:58 - from PDFs, from images, from audio, from really
a huge number of things. But, again the assumption
45:06 - that because it is on the web, it's ok to
use it is not true. You always need to check
45:12 - copyright and make sure that it is acceptable
for you to access that particular data. Next,
45:18 - and our very ominous picture, is data security
and the idea here is that when you go through
45:24 - all the effort to gather data, to clean up
and prepare for analysis, you have created
45:29 - something that is very valuable to a lot of
people and you have to be concerned about
45:33 - hackers trying to come in and steal the data,
especially if the data is not anonymous and
45:38 - it has identifiers in it. And so, there is
an additional burden to place on the analyst
45:43 - to ensure to the best of their ability that
the data is safe and cannot be broken into
45:49 - and stolen. And that can include very simple
things like a person who is on their project
45:53 - but is no longer, but took the data on a flash
drive. You have to find ways to make sure
45:57 - that that can’t happen as well. There’s
a lot of possibilities, it’s tricky, but
46:00 - it is something that you have to consider
thoroughly. Now, two other things that come
46:06 - up in terms of ethics, but usually don’t
get addressed in these conversations. Number
46:11 - one is potential bias. The idea here is that
the algorithms or the formulas that are used
46:18 - in data science are only as neutral or bias-free
as the rules and the data that they get. And
46:27 - so, the idea here is that if you have rules
that address something that is associated
46:33 - with, for instance, gender or age or race
or economic standing, you might unintentionally
46:40 - be building in those factors. Which, say for
instance, say for title nine, you are not
46:45 - supposed to. You might be building those into
the system without being aware of it, and
46:49 - an algorithm has this sheen of objectivity,
and people say they can place confidence in
46:55 - it without realizing that it is replicating
some of the prejudices that may happen in
46:59 - real life. Another issue is overconfidence.
And the idea here is that analyses are limited
47:07 - simplifications. They have to be, that is
just what they are. And because of this, you
47:13 - still need humans in the loop to help interpret
and apply this. The problem is when people
47:19 - run an algorithm to get a number, say to ten
decimal places, and they say, “this must
47:24 - be true,” and treat it as written-in-stone
absolutely unshakeable truth, when in fact,
47:30 - if the data were biased going in; if the algorithms
were incomplete, if the sampling was not representative,
47:38 - you can have enormous problems and go down
the wrong path with too much confidence in
47:46 - your own analyses. So, once again humility
is in order when doing data science work.
47:52 - In sum: data science has enormous potential,
but it also has significant risks involved
47:59 - in the projects. Part of the problem is that
analyses can’t be neutral, that you have
48:05 - to look at how the algorithms are associated
with the preferences, prejudices, and biases
48:12 - of the people who made them. And what that
means is that no matter what, good judgment
48:18 - is always vital to quality and success of
a data science project. Data Science is a
48:26 - field that is strongly associated with its
methods or procedures. In this section of
48:32 - videos, we're going to provide a brief overview
of the methods that are used in data science.
48:38 - Now, just as a quick warning, in this section
things can get kind of technical and that
48:43 - can cause some people to sort of freak out.
But, this course is a non-technical overview.
48:50 - The technical hands on stuff is in the other
courses. And it is really important to remember
48:56 - that tech is simply the means to doing data
science. Insight or the ability to find meaning
49:05 - in your data, that's the goal. Tech only helps
you get there. And so, we want to focus primarily
49:11 - on insight and the tools and the tech as they
serve to further that goal. Now, there’s
49:18 - a few general categories we are going to talk
about, again, with an overview for each of
49:23 - these. The first one is sourcing or data sourcing.
That is how to get the data that goes into
49:29 - data science, the raw materials that you need.
Second is coding. That again is computer programming
49:35 - that can be used to obtain and manipulate
and analyze the data. After that, a tiny bit
49:42 - of math and that is the mathematics behind
data science methods that really form the
49:47 - foundations of the procedures. And then stats,
the statistical methods that are frequently
49:53 - used to summarize and analyze data, especially
as applied to data science. And then there
50:00 - is machine learning, ML, this is a collection
of methods for finding clusters in the data,
50:05 - for predicting categories or scores on interesting
outcomes. And even across these five things,
50:13 - even then, the presentations aren’t too
techie-crunchy, they are basically still friendly.
50:19 - Really, that's the way it is. So, that is
the overview of the overviews. In sum: we
50:26 - need to remember that data science includes
tech, but data science is greater than tech,
50:32 - it is more than those procedures. And above
all, that tech while important to data science
50:38 - is still simply a means to insight in data.
The first step in discussing data science
50:48 - methods is to look at the methods of sourcing,
or getting data that is used in data science.
50:54 - You can think of this as getting the raw materials
that go into your analyses. Now, you have
50:59 - got a few different choices when it comes
to this in data science. You can use existing
51:04 - data, you can use something called data APIs,
you can scrape web data, or you can make data.
51:11 - We'll talk about each of those very briefly
in a non-technical manner. For right now,
51:17 - let me say something about existing data.
This is data that already is at hand and it
51:22 - might be in-house data. So if you work for
a company, it might be your company records.
51:27 - Or, you might have open data; for instance,
many governments and many scientific organizations
51:33 - make their data available to the public. And
then there is also third party data. This
51:39 - is usually data that you buy from a vendor,
but it exists and it is very easy to plug
51:44 - it in and go. You can also use APIs. Now,
that stands for Application Programming Interface,
51:52 - and this is something that allows various
computer applications to communicate directly
51:57 - with each other. It’s like phones for your
computer programs. It is the most common way
52:02 - of getting web data, and the beautiful thing
about it is it allows you to import that data
52:06 - directly into whatever program or application
you are using to analyze the data. Next is
52:13 - scraping data. And this is where you want
to use data that is on the web, but they don’t
52:18 - have an existing API. And what that means,
is usually data that's in HTML web tables
52:26 - and pages, maybe PDFs. And you can do this
either with using specialized applications
52:33 - for scraping data or you can do it in a programming
language, like R or Python, and write the
52:38 - code to do the data scraping. Or another option
is to make data. And this lets you get exactly
52:46 - what you need; you can be very specific and
you can get what you need. You can do something
52:51 - like interviews, or you can do surveys, or
you can do experiments. There is a lot of
52:56 - approaches, most of them require some specialized
training in terms of how to gather quality
53:01 - data. And that is actually important to remember,
because no matter what method you use for
53:05 - getting or making new data, you need to remember
this one little aphorism you may have heard
53:10 - from computer science. It goes by the name
of GIGO: that actually stands for “Garbage
53:15 - In, Garbage Out,” and it means if you have
bad data that you are feeding into your system,
53:20 - you are not going to get anything worthwhile,
any real insights out of it. Consequently,
53:24 - it is important to pay attention to metrics
or methods for measuring and the meaning - exactly
53:31 - what it is that they tell you. There's a few
ways you can do this. For instance, you can
53:36 - talk about business metrics, you can talk
about KPIs, which means Key Performance Indicators,
53:42 - also used in business settings. Or SMART goals,
which is a way of describing the goals that
53:48 - are actionable and timely and so on. You can
also talk about, in a measurement sense, classification
53:54 - accuracy. And I will discuss each of those
in a little more detail in a later movie.
54:01 - But for right now, in sum, we can say this:
data sourcing is important because you need
54:05 - to get the raw materials for your analysis.
The nice thing is there's many possible methods,
54:11 - many ways that you can use to get the data
for data science. But no matter what you do,
54:16 - it is important to check the quality and the
meaning of the data so you can get the most
54:22 - insight possible out of your project. The
next step we need to talk about in data science
54:29 - methods is coding, and I am going to give
you a very brief non-technical overview of
54:35 - coding in data science. The idea here is that
you are going to get in there and you are
54:38 - going to King of the Jungle/master of your
domain and make the data jump when you need
54:44 - it to jump. Now, if you remember when we talked
about the Data Science Venn Diagram at the
54:48 - beginning, coding is up here on the top left.
And while we often think about sort of people
54:53 - typing lines of code (which is very frequent),
it is more important to remember when we talk
54:58 - about coding (or just computers in general),
what we are really talking about here is any
55:03 - technology that lets you manipulate the data
in the ways that you need to perform the procedures
55:10 - you need to get the insight that you want
out of your data. Now, there are three very
55:15 - general categories that we will be discussing
here on datalab. The first is apps; these
55:20 - are specialized applications or programs for
working with data. The second is data; or
55:26 - specifically, data formats. There's special
formats for web data, I will mention those
55:30 - in a moment. And then, code; there are programming
languages that give you full control over
55:36 - what the computer does and how you interact
with the data. Let's take a look at each one
55:40 - very briefly. In terms of apps, there are
spreadsheets, like Excel or Google Sheets.
55:45 - These are the fundamental data tools of probably
a majority of the world. There are specialized
55:51 - applications, like Tableau for data visualization,
or SPSS, it is a very common statistical package
55:58 - in the social sciences and in businesses,
and one of my favorites, JASP, which is a
56:04 - free open source analog of SPSS, which actually
I think is a lot easier to use and replicate
56:08 - research with. And, there are tons of other
choices. Now, in terms of web data, it is
56:15 - helpful to be familiar with things like HTML,
and XML, and JSON, and other formats that
56:22 - are used to encapsulate data on the web, because
those are the things that you are going to
56:27 - have to be programming about to interact with
when you get your data. And then there are
56:33 - actual coding languages. R is probably the
most common, along with Python; general purpose
56:39 - language, but it has been well adapted for
data use. There's SQL, the structured query
56:44 - language for databases, and very basic languages
like C, C++, and Java, which are used more
56:50 - in the back-end of data science. And then
there is Bash, the most common command line
56:56 - interface, and regular expressions. And we
will talk about all of these in other courses
57:02 - here at datalab. But, remember this: tools
are just tools. They are only one part of
57:10 - the data science process. They are a means
to the end, and the end, the goal is insight.
57:19 - You need to know where you are trying to go
and then simply choose the tools that help
57:23 - you reach that particular goal. That’s the
most important thing. So, in sum, here’s
57:29 - a few things: number one, use your tools wisely.
Remember your questions need to drive the
57:35 - process, not the tools themselves. Also, I
will just mention that a few tools is usually
57:41 - enough. You can do an awful lot with Excel
and R. And then, the most important thing
57:46 - is: focus on your goal and choose your tools
and even your data to match the goal, so you
57:53 - can get the most useful insights from your
data. The next step in our discussion of data
58:00 - science methods is mathematics, and I am going
to give a very brief overview of the math
58:05 - involved in data science. Now, the important
thing to remember is that math really forms
58:10 - the foundation of what we're going to do.
If you go back to the Data Science Venn Diagram,
58:15 - we've got stats up here in the right corner,
but really it's math and stats, or quantitative
58:21 - ability in general, but we'll focus on the
math part right here. And probably the most
58:26 - important question is how much math is enough
to do what you need to do? Or to put it another
58:34 - way, why do you need math at all, because
you have got a computer to do it? Well, I
58:40 - can think of three reasons you don’t want
to rely on just the computer, but it is helpful
58:45 - to have some sound mathematical understanding.
Here they are: number one, you need to know
58:51 - which procedures to use and why. So you have
your question, you have your data and you
58:57 - need to have enough of an understanding to
make an informed choice. That's not terribly
59:01 - difficult. Two, you need to know what to do
when things don’t work right. Sometimes
59:08 - you get impossible results. I know that statistics
you can get a negative adjusted R2; that's
59:13 - not supposed to happen. And it is good to
know the mathematics that go into calculating
59:18 - that so you can understand how something apparently
impossible can work. Or, you are trying to
59:22 - do a factor analysis or principal component
and you get a rotation that won’t convert.
59:27 - It helps to understand what it is about the
algorithm that's happening, and why that won’t
59:33 - work in that situation. And number three,
interestingly, some procedures, some math
59:39 - is easier and quicker to do by hand than by
firing up the computer. And I'll show you
59:44 - a couple of examples in later videos, where
that can be the case. Now, fundamentally there
59:50 - is a nice sort of analogy here. Math is to
data science as, for instance, chemistry is
59:57 - to cooking, kinesiology is to dancing, and
grammar is to writing. The idea here is that
60:03 - you can be a wonderful cook without knowing
any chemistry, but if you know some chemistry
60:07 - it is going to help. You can be a wonderful
dancer without know kinesiology, but it is
60:11 - going to help. And you can probably be a good
writer without having an explicit knowledge
60:16 - of grammar, but it is going to make a big
difference. The same thing is true of data
60:21 - science; you will do it better if you have
some of the foundational information. So,
60:26 - the next question is: what kinds of math do
you need for data science? Well, there's a
60:31 - few answers to that. Number one is algebra;
you need some elementary algebra. That is,
60:36 - the basically simple stuff. You can have to
do some linear or matrix algebra because that
60:41 - is the foundation of a lot of the calculations.
And you can also have systems of linear equations
60:46 - where you are trying to solve several equations
all at once. It is a tricky thing to do, in
60:51 - theory, but this is one of the things that
is actually easier to do by hand sometimes.
60:56 - Now, there's more math. You can get some Calculus.
You can get some big O, which has to do with
61:03 - the order of a function, which has to do with
sort of how fast it works. Probability theory
61:09 - can be important, and Bayes' theorem, which
is a way of getting what is called a posterior
61:14 - probability can also be a really helpful tool
for answering some fundamental questions in
61:20 - data science. So in sum: a little bit of math
can help you make informed choices when planning
61:29 - your analyses. Very significantly, it can
help you find the problems and fix them when
61:36 - things aren’t going right. It is the ability
to look under the hood that makes a difference.
61:41 - And then truthfully, some mathematical procedures,
like systems of linear equations, that can
61:46 - even be done by hand, sometimes faster than
you can do with a computer. So, you can save
61:51 - yourself some time and some effort and move
ahead more quickly toward your goal of insight.
61:58 - Now, data science wouldn’t be data science
and its methods without a little bit of statistics.
62:04 - So, I am going to give you a brief statistics
overview here of how things work in data science.
62:09 - Now, you can think of statistics as really
an attempt to find order in chaos, find patterns
62:16 - in an overwhelming mess. Sort of like trying
to see the forest and the trees. Now, let's
62:23 - go back to our little Venn Diagram here. We
recently had math and stats here in the top
62:28 - corner. We are going to go back to talking
about stats, in particular. What you are trying
62:32 - to do here; one thing is to explore your data.
You can have exploratory graphics, because
62:39 - we are visual people and it is usually easiest
to see things. You can have exploratory statistics,
62:44 - a numerical exploration of the data. And you
can have descriptive statistics, which are
62:49 - the things that most people would have talked
about when they took a statistics class in
62:52 - college (if they did that). Next, there is
inference. I've got smoke here because you
62:58 - can infer things about the wind and the air
movement by looking at patterns in smoke.
63:04 - The idea here is that you are trying to take
information from samples and infer something
63:11 - about a population. You are trying to go from
one source to another. One common version
63:16 - of this is hypothesis testing. Another common
version is estimations, sometimes called Confidence
63:21 - Intervals. There are other ways to do it,
but all of these let you go beyond the data
63:26 - at hand to making larger conclusions. Now,
one interesting thing about statistics is
63:33 - you're going to have to be concerned with
some of the details and arranging things just
63:36 - so. For instance, you get to do something
like feature selection and that’s picking
63:40 - variables that should be included or combinations
and there are problems that can come up that
63:46 - are frequent problems and I will address some
of those in later videos. There’s also the
63:51 - matter of validation. When you create a statistical
model you have to see if it is actually accurate.
63:56 - Hopefully, you have enough data that you can
have a holdout sample and do that, or you
64:01 - can replicate the study. Then, there is the
choice of estimators that you use; how you
64:05 - actually get the coefficients or the combinations
in your model. And then there's ways of assessing
64:11 - how well your model fits the data. All of
these are issues that I'll address briefly
64:17 - when we talk about statistical analysis at
greater length. Now, I do want to mention
64:23 - one thing in particular here, and I just call
this "beware the trolls.” There are people
64:29 - out there who will tell you that if you don’t
do things exactly the way they say to do it,
64:34 - that your analysis is meaningless, that your
data is junk and you've lost all your time.
64:40 - You know what? They’re trolls. So, the idea
here is don’t listen to that. You can make
64:47 - enough of an informed decision on your own
to go ahead and do an analysis that is still
64:52 - useful. Probably one of the most important
things to think about in this is this wonderful
64:57 - quote from a very famous statistician and
it says, "All models or all statistical models
65:01 - are wrong, but some are useful." And so the
question isn’t whether you’re technically
65:07 - right, or you have some sort of level of intellectual
purity, but whether you have something that
65:14 - is useful. That, by the way, comes from George
Box. And I like to think of it basically as
65:20 - this: as wave your flag, wave your “do it
yourself” flag, and just take pride in what
65:26 - you're able to accomplish even when there
are people who may be criticizing it. Go ahead,
65:31 - you’re doing something, go ahead and do
it. So, in sum: statistics allow you to explore
65:38 - and describe your data. It allows you to infer
things about the population. There is a lot
65:43 - of choices available, a lot of procedures.
But no matter what you do, the goal is useful
65:50 - insight. Keep your eyes on that goal and you
will find something meaningful and useful
65:56 - in your data to help you in your own research
and projects. Let's finish our data science
66:03 - methods overview by getting a brief overview
of Machine Learning. Now, I've got to admit
66:09 - when you say the term “machine learning,”
people start thinking something like, “the
66:13 - robot overlords are going to take over the
world.” That's not what it is. Instead,
66:18 - let's go back to our Venn Diagram one more
time, and in the intersection at the top between
66:23 - coding and stats is Machine Learning or as
it's commonly called, just ML. The goal of
66:30 - Machine Learning is to go and work in data
space so you can, for instance, you can take
66:36 - a whole lot of data (we’ve got tons of books
here), and then you can reduce the dimensionality.
66:42 - That is, take a very large, scattered, data
set and try to find the most essential parts
66:48 - of that data. Then you can use these methods
to find clusters within the data; like goes
66:53 - with like. You can use methods like k-means.
You can also look for anomalies or unusual
67:00 - cases that show up in the data space. Or,
if we go back to categories again, I talked
67:06 - about like for like. You can use things like
logistic regression or k-nearest neighbors,
67:12 - KNN. You can use Naive Bayes for classification
or Decision Trees or SVM, which is Support
67:19 - Vector Machines, or artificial neural nets.
Any of those will help you find the patterns
67:25 - and the clumping in the data so you can get
similar cases next to each other, and get
67:30 - the cohesion that you need to make conclusions
about these groups. Also, a major element
67:36 - of machine learning is predictions. You're
going to point your way down the road. The
67:41 - most common approach here; the most basic
is linear regression, multiple regression.
67:46 - There is also Poisson regression, which is
used for modeling count or frequency data.
67:51 - And then there is the issue of Ensemble models,
where you create several models and you take
67:55 - the predictions from each of those and you
put them together to get an overall more reliable
68:00 - prediction. Now, I will talk about each of
these in a little more detail in later courses,
68:06 - but for right now I mostly just want you to
know that these things exist, and that’s
68:09 - what we mean when we refer to Machine Learning.
So, in sum: machine learning can be used to
68:15 - categorize cases and to predict scores on
outcomes. And there's a lot of choices, many
68:21 - choices and procedures available. But, again,
as I said with statistics, and I’ll also
68:26 - say again many times after this, no matter
what, the goal is not that “I'm going to
68:31 - do an artificial neural network or a SVM,”
the goal is to get useful insight into your
68:36 - data. Machine learning is a tool, and use
it to the extent that it helps you get that
68:42 - insight that you need. In the last several
videos I've talked about the role in data
68:48 - science of technical things. On the other
hand, communicating is essential to the practice,
68:55 - and the first thing I want to talk about there
is interpretability. The idea here is that
69:01 - you want to be able to lead people through
a path on your data. You want to tell a data-driven
69:08 - story, and that's the entire goal of what
you are doing with data science. Now, another
69:13 - way to think about this is: when you are doing
your analysis, what you're trying to do is
69:17 - solve for value. You're making an equation.
You take the data, you're trying to solve
69:22 - for value. The trouble is this: a lot of people
get hung up on analysis, but they need to
69:28 - remember that analysis is not the same thing
as value. Instead, I like to think of it this
69:33 - way: that analysis times story is equal to
value. Now, please note that's multiplicative,
69:43 - not additive, and so one consequence of that
is when you go back to, analysis times story
69:49 - equals value. Well, if you have zero story
you're going to have zero value because, as
69:55 - you recall, anything times zero is zero. So,
instead of that let's go back to this and
70:00 - say what we really want to do is, we want
to maximize the story so that we can maximize
70:06 - the value that results from our analysis.
Again, maximum value is the overall goal here.
70:12 - The analysis, the tools, the tech, are simply
methods for getting to that goal. So, let's
70:19 - talk about goals. For instance, an analysis
is goal-driven. You are trying to accomplish
70:25 - something that's specific, so the story, or
the narrative, or the explanation you give
70:30 - about your project should match those goals.
If you are working for a client that has a
70:35 - specific question that they want you to answer,
then you have a professional responsibility
70:40 - to answer those questions clearly and unambiguously,
so they know whether you said yes or no and
70:46 - they know why you said yes or no. Now, part
of the problem here is the fact the client
70:53 - isn't you and they don’t see what you do.
And as I show here, simply covering your face
70:57 - doesn’t make things disappear. You have
to worry about a few psychological abstractions.
71:02 - You have to worry about egocentrism. And I'm
not talking about being vain, I'm talking
71:07 - about the idea that you think other people
see and know and understand what you know.
71:12 - That’s not true; otherwise, they wouldn't
have hired you in the first place. And so
71:16 - you have to put it in terms that the client
works with, and that they understand, and
71:21 - you're going to have to get out of your own
center in order to do that. Also, there's
71:27 - the idea of false consensus; the idea that,
“well everybody knows that.” And again,
71:32 - that's not true, otherwise, they wouldn’t
have hired you. You need to understand that
71:36 - they are going to come from a different background
with a different range of experience and interpretation.
71:40 - You're going to have to compensate for that.
A funny little thing is the idea about anchoring.
71:46 - When you give somebody an initial impression,
they use that as an anchor, and then they
71:51 - adjust away from it. So if you are going to
try to flip things over on their heads, watch
71:55 - out for giving a false impression at the beginning
unless you absolutely need to. But most importantly,
72:02 - in order to bridge the gap between the client
and you, you need to have clarity and explain
72:08 - yourself at each step. You can also think
about the answers. When you are explaining
72:15 - the project to the client, you might want
to start in a very simple procedure: state
72:19 - the question that you are answering. Give
your answer to that question, and if you need
72:25 - to, qualify as needed. And then, go in order
top to bottom, so you're trying to make it
72:31 - as clear as possible what you're saying, what
the answer is, and make it really easy to
72:36 - follow. Now, in terms of discussing your process,
how you did this all. Most of the time it
72:42 - is probably the case of they don’t care,
they just want to know what the answer is
72:46 - and that you used a good method to get that.
So, in terms of discussing processes or the
72:51 - technical details, only when absolutely necessary.
That's something to keep in mind. The process
72:57 - here is to remember that analysis, which means
breaking something apart. This, by the way,
73:02 - is a mechanical typewriter broken into its
individual component. Analysis means to take
73:07 - something apart, and analysis of data is an
exercise in simplification. You're taking
73:14 - the overall complexity, sort of the overwhelmingness
of the data, and you're boiling it down and
73:20 - finding the patterns that make sense and serve
the needs of your client. Now, let's go to
73:25 - a wonderful quote from our friend Albert Einstein
here, who said, “Everything should be made
73:30 - as simple as possible, but not simpler.”
That's true in presenting your analysis. Or,
73:37 - if you want to go see the architect and designer
Ludwig Mies van der Rohe, who said, “Less
73:43 - is more.” It is actually Robert Browning
who originally said that, but Mies van der
73:47 - Rohe popularized it. Or, if you want another
way of putting a principle that comes from
73:53 - my field, I'm actually a psychological researcher;
they talk about being minimally sufficient.
74:00 - Just enough to adequately answer the question.
If you're in commerce you know about a minimal
74:05 - viable product, it is sort of the same idea
within analysis here, the minimal viable analysis.
74:12 - So, here's a few tips: when you’re giving
a presentation, more charts, less text, great.
74:19 - And then, simplify the charts; remove everything
that doesn’t need to be in there. Generally,
74:23 - you want to avoid tables of data because those
are hard to read. And then, one more time
74:29 - because I want to emphasize it, less text
again. Charts, tables can usually carry the
74:35 - message. And so, let me give you an example
here. I'm going to give a very famous dataset
74:40 - at Berkeley admissions. Now, these are not
stairs at Berkeley, but it gives the idea
74:43 - of trying to get into something that is far
off and distant. Here's the data; this is
74:49 - graduate school admissions in 1973, so it's
over 40 years ago. The idea is that men and
74:55 - women were both applying for graduate school
at the University of California Berkeley.
75:00 - And what we found is that 44 percent of the
men who applied were admitted, that’s their
75:05 - part in green. And of the women, only 35 percent
of women were admitted when they applied.
75:12 - So, really at first glance this is bias, and
it actually led to a lawsuit, it was a major
75:18 - issue. So, what Berkeley then tried to do
was find out, “well which programs are responsible
75:24 - for this bias?” And they got a very curious
set of results. If you break the applications
75:29 - down by program (and here we are calling them
A through F), six different programs. What
75:35 - you find, actually, is that in each of these
male applicants on the left female applicants
75:39 - are on the right. If you look at program A,
women actually got accepted at a higher rate,
75:46 - and the same is true for B, and the same is
true for D, and the same is true for F. And
75:54 - so, this is a very curious set of responses
and it is something that requires explanation.
75:59 - Now in statistics, this is something that
is known as Simpson's Paradox. But here is
76:05 - the paradox: bias may be negligible at the
department level. And in fact, as we saw in
76:12 - four of the departments, there was a possible
bias in favor of women. And the problem is
76:18 - that women applied to more selective programs,
programs with lower acceptance rates. Now,
76:25 - some people stop right here and say therefore,
“nothing is going on, nothing to complain
76:29 - about.” But you know, that's still ending
the story a little bit early. There are other
76:33 - questions that you can ask, and as producing
a data-driven story, this is stuff that you
76:39 - would want to do. So, for instance, you may
want to ask, “why do the programs vary in
76:44 - overall class size? Why do the acceptance
rates differ from one program to the other?
76:49 - Why do men and women apply to different programs?”
And you might want to look at things like
76:55 - the admissions criteria for each of the programs,
the promotional strategies, how they advertise
77:00 - themselves to students. You might want to
look at the kinds of prior education the students
77:04 - have in the programs, and you really want
to look at funding level for each of the programs.
77:09 - And so, really, you get one answer, at least
more questions, maybe some more answers, and
77:15 - more questions, and you need to address enough
of this to provide a comprehensive overview
77:20 - and solution to your client. In sum, let’s
say this: stories give value to data analysis.
77:30 - And when you tell the story, you need to make
sure that you are addressing your client's'
77:34 - goals in a clear, unambiguous way. The overall
principle here is be minimally sufficient.
77:43 - Get to the point, make it clear. Say what
you need to, but otherwise be concise and
77:49 - make your message clear. The next step in
discussing data science and communicating
77:56 - is to talk about actionable insights, or information
that can be used productively to accomplish
78:02 - something. Now, to give sort of a bizarre
segue here, you look at a game controller.
78:08 - It may be a pretty thing, it may be a nice
object, but remember: game controllers exist
78:13 - to do something. They exist to help you play
the game and to do it as effectively as possible.
78:20 - They have a function, they have a purpose.
Same way data is for doing. Now, that's a
78:27 - paraphrase for one of my favorite historical
figures. This is William James, the father
78:32 - of American Psychology, and pragmatism is
philosophy. And he has this wonderful quote,
78:38 - he said, “My thinking is first and last
and always for the sake of my doing.” And
78:45 - the idea applies to analysis. Your analysis
and your data is for the sake of your doing.
78:50 - So, you’re trying to get some sort of specific
insight in how you should proceed. What you
78:57 - want to avoid is the opposite of this from
one of my other favorite cultural heroes,
79:01 - the famous Yankees catcher Yogi Berra, who
said, “We're lost, but we're making good
79:06 - time.” The idea here is that frantic activity
does not make up for lack of direction. You
79:12 - need to understand what you are doing so you
can reach the particular goal. And your analysis
79:16 - is supposed to do that. So, when you're giving
your analysis, you're going to try to point
79:22 - the way. Remember, why was the project conducted?
The goal is usually to direct some kind of
79:29 - action, reach some kind of goal for your client.
And that the analysis should be able to guide
79:35 - that action in an informed way. One thing
you want to do is, you want to be able to
79:41 - give the next steps to your client. Give the
next steps; tell them what they need to do
79:46 - now. You want to be able to justify each of
those recommendations with the data and your
79:53 - analysis. As much as possible be specific,
tell them exactly what they need to do. Make
79:58 - sure it's doable by the client, that it's
within their range of capability. And that
80:03 - each step should build on the previous step.
Now, that being said, there is one really
80:09 - fundamental sort of philosophical problem
here, and that's the difference between correlation
80:16 - and causation. Basically, it goes this way:
your data gives you correlation; you know
80:22 - that this is associated with that. But your
client doesn’t simply want to know what's
80:28 - associated; they want to know what causes
something. Because if they are going to do
80:32 - something, that's an intervention designed
to produce a particular result. So, really,
80:36 - how do you get from the correlation, which
is what you have in the data, to the causation,
80:41 - which is what your client wants? Well, there's
a few ways to do that. One is experimental
80:47 - studies; these are randomized, controlled
trials. Now, that's theoretically the simplest
80:52 - path to causality, but it can be really tricky
in the real world. There are quasi-experiments,
80:57 - and these are methods, a whole collection
of methods. They use non-randomized data,
81:03 - usually observational data, adjusted in particular
ways to get an estimate of causal inference.
81:10 - Or, there's the theory and experience. And
this is research-based theory and domain-specific
81:16 - experience. And this is where you actually
get to rely on your client’s information.
81:20 - They can help you interpret the information,
especially if they have greater domain expertise
81:25 - than you do. Another thing to think about
are the social factors that affect your data.
81:33 - Now, you remember the data science Venn Diagram.
We've looked at it lots of times. It has got
81:37 - these three elements. Some proposed adding
a fourth circle to this Venn diagram, and
81:42 - we'll kind of put that in there and say that
social understanding is also important, critical
81:49 - really, to valid data science. Now, I love
that idea, and I do think that it's important
81:56 - to understand how things are going to play
out. There are a few kinds of social understanding.
82:00 - You want to be aware of your client's mission.
You want to make sure that your recommendations
82:04 - are consistent with your client's mission.
Also, that your recommendations are consistent
82:09 - with your client's identity; not just, “This
is what we do,” but, “This is really who
82:13 - we are.” You need to be aware of the business
context, sort of the competitive environment
82:18 - and the regulatory environment that they're
working in. As well as the social context;
82:23 - and that can be outside of the organization,
but even more often within the organization.
82:28 - Your recommendations will affect relationships
within the client's organization. And you
82:33 - are going to try to be aware of those as much
as you can to make it so that your recommendations
82:37 - can be realized the way they need to be. So,
in sum: data science is goal focused, and
82:45 - when you're focusing on that goal for your
client you need to give specific next steps
82:50 - that are based on your analysis and justifiable
from the data. And in doing so, be aware of
82:56 - the social, political, and economic context
that gives you the best opportunity of getting
83:03 - something really useful out of your analysis.
When you're working in data science and trying
83:09 - to communicate your results, presentation
graphics can be an enormously helpful tool.
83:14 - Think of it this way: you are trying to paint
a picture for the benefit of your client.
83:20 - Now, when you're working with graphics there
can be a couple of different goals; it depends
83:24 - on what kind of graphics you're working with.
There's the general category of exploratory
83:30 - graphics. These are ones that you are using
as the analyst. And for exploratory graphics,
83:35 - you need speed and responsiveness, and so
you get very simple graphics. This is a base
83:41 - histogram in R. And they can get a little
more sophisticated and this is done in ggplot2.
83:48 - And you can break it down into a couple other
histograms, or you can make it a different
83:51 - way, or make it see-through, or split them
apart into small multiples. But in each case,
83:56 - this is done for the benefit of you as the
analyst understanding the data. These are
84:01 - quick, they're effective. Now, they are not
very well-labeled, and they are usually for
84:06 - your insight, and then you do other things
as a result of that. On the other hand, presentation
84:13 - graphics which are for the benefit of your
client, those need clarity and they need a
84:19 - narrative flow. Now, let me talk about each
of those characteristics very briefly. Clarity
84:24 - versus distraction. There are things that
can go wrong in graphics. Number one is color.
84:30 - Colors can actually be a problem. Also, three-dimensional
or false dimensions are nearly always a distraction.
84:38 - One that gets a little touchy for some people
is interaction. We think of interactive graphics
84:43 - as really cool, great things to have, but
you run the risk of people getting distracted
84:48 - by the interaction and start playing around
with it. Going, like, “Ooh, I press here
84:50 - it does that.” And that distracts from the
message. So actually, it may be important
84:55 - to not have interaction. And then the same
thing is true of animation. Flat, static graphics
85:02 - can often be more informative because they
have fewer distractions in them. Let me give
85:08 - you a quick example of how not to do things.
Now, this is a chart that I made. I made it
85:14 - in Excel, and I did it based on some of the
mistakes I've seen in graphics submitted to
85:19 - me when I teach. And I guarantee you, everything
in here I have seen in real life, just not
85:24 - necessarily combined all at once. Let's zoom
in on this a little bit, so we can see the
85:28 - full badness of this graphic. And let's see
what's going on here. We've got a scale here
85:33 - that starts at 8 goes to 28% and is tiny;
doesn’t even cover the range of the data.
85:39 - We've got this bizarre picture on the wall.
We've got no access lines on the walls. We
85:43 - come down here; the labels for educational
levels are in alphabetical order, instead
85:47 - of the more logical higher levels of education.
Then we've got the data represented as cones,
85:54 - which are difficult to read and compare, and
it's only made worse by the colors and the
85:58 - textures. You know, if you want to take an
extreme, this one for grad degrees doesn’t
86:03 - even make it to the floor value of 8% and
this one for high school grad is cut off at
86:08 - the top at 28%. This, by the way, is a picture
of a sheep, and people do this kind of stuff
86:15 - and it drives me crazy. If you want to see
a better chart with the exact same data, this
86:22 - is it right here. It is a straight bar chart.
It's flat, it's simple, it's as clean as possible.
86:28 - And this is better in many ways. Most effective
here is that it communicates clearly. There's
86:34 - no distractions, it's a logical flow. This
is going to get the point across so much faster.
86:41 - And I can give you another example of it;
here's a chart previously about salaries for
86:46 - incomes. I have a list here, I've got data
scientist in it. If I want to draw attention
86:51 - to it, I have the option of putting a circle
around it and I can put a number next to it
86:55 - to explain it. That’s one way to make it
easy to see what's going on. We don’t even
87:00 - have to get fancy. You know, I just got out
a pen and a post-it note and I drew a bar
87:05 - chart of some real data about life expectancy.
This tells the story as well, that there is
87:12 - something terribly amiss in Sierra Leone.
But, now let's talk about creating narrative
87:17 - flow in your presentation graphics. To do
this, I am going to pull some charts from
87:22 - my most cited academic paper, which is called,
A Third Choice: A Review of Empirical Research
87:27 - on the Psychological Outcomes of Restorative
Justice. Think of it as mediation for juvenile
87:33 - crimes, mostly juvenile. And this paper is
interesting because really it's about fourteen
87:38 - bar charts with just enough text to hold them
together. And you can see there's a flow.
87:43 - The charts are very simple; this is judgments
about whether the criminal justice system
87:48 - was fair. The two bars on the left are victims;
the two bars on the right are offenders. And
87:54 - for each group on the left are people who
participated in restorative justice, so more
87:59 - victim-offender mediation for crimes. And
for each set on the right are people who went
88:04 - through standard criminal procedures. It says
court, but it usually means plea bargaining.
88:09 - Anyhow, it’s really easy to see that in
both cases the restorative justice bar is
88:14 - higher; people were more likely to say it
was fair. They also felt that they had an
88:19 - opportunity to tell their story; that’s
one reason why they might think it’s fair.
88:23 - They also felt the offender was held accountable
more often. In fact, if you go to court on
88:28 - the offenders, that one's below fifty percent
and that's the offenders themselves making
88:32 - the judgment. Then you can go to forgiveness
and apologies. And again, this is actually
88:38 - a simple thing to code and you can see there's
an enormous difference. In fact, one of the
88:43 - reasons there is such a big difference is
because instead of court preceding, the offender
88:46 - very rarely meets the victim. It also turns
out I need to qualify this a little bit because
88:52 - a bunch of the studies included drunk driving
with no injuries or accidents. Well, when
88:58 - we take them out, we see a huge change. And
then we can go to whether a person is satisfied
89:03 - with the outcome. Again, we see an advantage
for restorative justice. Whether the victim
89:07 - is still upset about the crime, now the bars
are a little bit different. And whether they
89:10 - are afraid of revictimization and that is
over a two to one difference. And then finally
89:15 - recidivism for offenders or reoffending; and
you see a big difference there. And so what
89:20 - I have here is a bunch of charts that are
very very simple to read, and they kind of
89:25 - flow in how they're giving the overall impression
and then detailing it a little bit more. There's
89:30 - nothing fancy here, there's nothing interactive,
there's nothing animated, there's nothing
89:35 - kind of flowing in seventeen different directions.
It's easy, but it follows a story and it tells
89:41 - a narrative about the data and that should
be your major goal with the presentation graphics.
89:47 - In sum: presenting, or the graphics you use
for presenting, are not the same as the graphics
89:52 - you use for exploring. They have different
needs and they have different goals. But no
89:56 - matter what you are doing, be clear in your
graphics and be focused in what you're trying
90:01 - to tell. And above all create a strong narrative
that gives different level of perspective
90:07 - and answers questions as you go to anticipate
a client's questions and to give them the
90:13 - most reliable solid information and the greatest
confidence in your analysis. The final element
90:20 - of data science and communicating that I wanted
to talk about is reproducible research. And
90:26 - you can think of it as this idea; you want
to be able to play that song again. And the
90:31 - reason for that is data science projects are
rarely “one and done;” rather they tend
90:35 - to be incremental, they tend to be cumulative,
and they tend to adapt to these circumstances
90:42 - that they're working in. So, one of the important
things here, probably, if you want to summarize
90:46 - it very briefly, is this: show your work.
There's a few reasons for this. You may have
90:52 - to revise your research at a later date, your
own analyses. You may be doing another project
90:57 - and you want to borrow something from previous
studies. More likely you'll have to hand it
91:01 - off to somebody else at a future point and
they're going to have to be able to understand
91:04 - what you did. And then there’s the very
significant issue in both scientific and economic
91:10 - research of accountability. You have to be
able to show that you did things in a responsible
91:16 - way and that your conclusions are justified;
that's for clients funding agencies, regulators,
91:22 - academic reviewers, any number of people.
Now, you may be familiar with the concept
91:27 - of open data, but you may be less familiar
with the concept of open data science; that's
91:33 - more than open data. So, for instance, I’ll
just let you know there is something called
91:37 - the Open Data Science Conference and ODSC.com.
And it meets three times a year in different
91:43 - places. And this is entirely, of course, devoted
to open data science using both open data,
91:50 - but making the methods transparent to people
around them. One thing that can make this
91:55 - really simple is something called the Open
Science Framework, which is at OSF.io. It's
92:01 - a way of sharing your data and your research
with an annotation on how you got through
92:05 - the whole thing with other people. It makes
the research transparent, which is what we
92:10 - need. One of my professional organizations,
the Association for Psychological Science
92:16 - has a major initiative on this called open
practices, where they are strongly encouraging
92:21 - people to share their data as much as is ethically
permissible and to absolutely share their
92:27 - methods before they even conduct a study as
a way of getting rigorous intellectual honesty
92:32 - and accountability. Now, another step in all
of this is to archive your data, make that
92:39 - information available, put it on the shelf.
And what you want to do here is, you want
92:42 - to archive all of your datasets; both the
totally raw before you did anything with it
92:48 - dataset, and every step in the process until
your final clean dataset. Along with that,
92:53 - you want to archive all of the code that you
used in the process and analyzed the data.
92:58 - If you used a programming language like R
or Python, that's really simple. If you used
93:02 - a program like SPSS you need to save the syntax
files, and then that can be done that way.
93:08 - And again, no matter what, make sure to comment
liberally and explain yourself. Now, part
93:14 - of that is you have to explain the process,
because you are not just this lone person
93:18 - sitting on the sofa working by yourself, you’re
with other people and you need to explain
93:23 - why you did it the way that you did. You need
to explain the choices, the consequences of
93:29 - those choices, the times that you had to backtrack
and try it over again. This also works into
93:34 - the principle of future-proofing your work.
You want to do a few things here. Number one;
93:39 - the data. You want to store the data in non-proprietary
formats like a CSV or Comma Separated Values
93:45 - file because anything can read CSV files.
If you stored it in the proprietary SPSS.sav
93:52 - format, you might be in a lot of trouble when
somebody tries to use it later and they can’t
93:56 - open it. Also, there's storage; you want to
place all of your files in a secure, accessible
94:02 - location like GitHub is probably one of the
best choices. And then the code, you may want
94:08 - to use something like a dependency management
package like Packrat for R or Virtual Environment
94:13 - for Python as a way of making sure that the
packages that you use; that there are always
94:19 - versions that work because sometimes things
get updated and it gets broken. This is a
94:24 - way of making sure that the system that you
have will always work. Overall, you can think
94:30 - of this too: you want to explain yourself
and a neat way to do that is to put your narrative
94:35 - in a notebook. Now, you can have a physical
lab book or you can also do digital books.
94:41 - A really common one, especially if you're
using Python, is Jupyter with a “y” there
94:46 - in the middle. Jupyter notebooks are interactive
notebooks. So, here's a screenshot of one
94:52 - that I made in Python, and you have titles,
you have text, you have the graphics. If you
94:58 - are working in R, you can do this through
something called RMarkdown. Which works in
95:03 - the same way you do it in RStudio, you use
Markdown and you can annotate it. You can
95:07 - get more information about that at rmarkdown.rstudio.com.
And so for instance, here's an R analysis
95:14 - I did, and as you can see the code on the
left and you see the markdown version on the
95:20 - right. What's neat about this is that this
little bit of code here, this title and this
95:25 - text and this little bit of R code, then is
displayed as this formatted heading, as this
95:32 - formatted text, and this turns into the entire
R output right there. It's a great way to
95:36 - do things. And if you do RMarkdown, you actually
have the option of uploading the document
95:42 - into something called RPubs; and that's an
online document that can be made accessible
95:48 - to anybody. Here's a sample document. And
if you want to go see it, you can go to this
95:53 - address. It's kind of long, so I am going
to let you write that one down yourself. But,
95:59 - in sum: here's what we have. You want to do
your work and archive the information in a
96:05 - way that supports collaboration. Explain your
choices, say what you did, show how you did
96:11 - it. This allows you to future-proof your work,
so it will work in other situations for other
96:16 - people. And as much as possible, no matter
how you do it, make sure you share your narrative
96:21 - so people understand your process and they
can see that your conclusions are justifiable,
96:27 - strong and reliable. Now, something I’ve
mentioned several times when talking about
96:33 - data science, and I’ll do it again in this
conclusion, is that it's important to give
96:36 - people next steps. And I'm going to do that
for you right now. If you're wondering what
96:41 - to do after having watched this very general
overview course, I can give you a few ideas.
96:46 - Number one, maybe you want to start trying
to do some coding in R or Python; we have
96:51 - courses for those. You might want to try doing
some data visualization, one of the most important
96:56 - things that you can do. You may want to brush
up on statistics and maybe some math that
97:01 - goes along with it. And you may want to try
your hand at machine learning. All of these
97:07 - will get you up and rolling in the practice
of data science. You can also try looking
97:12 - at data sourcing, finding information that
you are going to do. But, no matter what happens
97:17 - try to keep it in context. So, for instance,
data science can be applied to marketing,
97:24 - and sports, and health, and education, and
the arts, and really a huge number of other
97:31 - things. And we will have courses here at datalab.cc
that talk about all of those. You may also
97:39 - want to start getting involved in the community
of data science. One of the best conferences
97:43 - that you can go to is O'Reilly Strata, which
meets several times a year around the globe.
97:48 - There's also Predictive Analytics World, again
several times a year around the world. Then
97:54 - there's much smaller conferences, I love Tapestry
or tapestryconference.com, which is about
97:59 - storytelling in data science. And Extract,
a one-day conference about data stories that
98:07 - is put on by import.io, one of the great data
sourcing applications that's available for
98:13 - scraping web data. If you want to start working
with actual data, a great choice is to go
98:18 - to Kaggle.com and they sponsor data science
competitions, which actually have cash rewards.
98:26 - There's also wonderful data sets you can work
with there to find out how they work and compare
98:31 - your results to those of other people. And
once you are feeling comfortable with that,
98:36 - you may actually try turning around and doing
some service; datakind.org is the premier
98:41 - organization for data science as humanitarian
service. They do major projects around the
98:47 - world. I love their examples. There are other
things you can do; there's an annual event
98:52 - called Do Good Data, and then datalab.cc will
be sponsoring twice-a-year data charrettes,
98:59 - which are opportunities for people in the
Utah area to work with the local nonprofits
99:04 - on their data. But above all of this, I want
you to remember this one thing: data science
99:10 - is fundamentally democratic. It's something
that everybody needs to learn to do in some
99:16 - way, shape or form. The ability to work with
data is a fundamental ability and everybody
99:23 - would be better off by learning to work with
data intelligently and sensitively. Or, to
99:28 - put it another way: data science needs you.
Thanks so much for joining me in this introductory
99:34 - course and I hope it has been good and I look
forward to seeing you in the other courses
99:38 - here at datalab.cc. Welcome to “Data Sourcing”.
I'm Barton Poulson and in this course, we're
99:46 - going to talk about Data Opus or that's Latin
for Data Needed. The idea here is that no
99:54 - data, no data science; and that is a sad thing.
So, instead of leaving it at that we're going
100:00 - to use this course to talk about methods for
measuring and evaluating data and methods
100:06 - for accessing existing data and even methods
for creating new, custom data. Take those
100:12 - together and it's a happy situation. At the
same time, we'll do all of this still at an
100:18 - accessible, conceptual and non-technical level
because the technical hands-on stuff will
100:24 - happen in later other courses. But for now,
let's talk data. For data sourcing, the first
100:34 - thing we want to talk about is measurement.
And within that category, we're going to talk
100:38 - about metrics. The idea here is that you actually
need to know what your target is if you want
100:44 - to have a chance to hit it. There’s a few
particular reasons for this. First off, data
100:51 - science is action-oriented; the goal is to
do something as opposed to simply understand
100:57 - something, which is something I say as an
academic practitioner. Also, your goal needs
101:02 - to be explicit and that's important because
the goals can guide your effort. So, you want
101:07 - to say exactly what you are trying to accomplish,
so you know when you get there. Also, goals
101:13 - exist for the benefit of the client, and they
can prevent frustration; they know what you’re
101:17 - working on, they know what you have to do
to get there. And finally, the goals and the
101:22 - metrics exist for the benefit of the analyst
because they help you use your time well.
101:28 - You know when you're done, you know when you
can move ahead with something, and that makes
101:32 - everything a little more efficient and a little
more productive. And when we talk about this
101:39 - the first thing you want to do is try to define
success in your particular project or domain.
101:46 - Depending on where you are, in commerce that
can include things like sales, or click-through
101:50 - rates, or new customers. In education it can
include scores on tests; it can include graduation
101:57 - rates or retention. In government, it can
include things like housing and jobs. In research,
102:03 - it can include the ability to serve the people
that you're to better understand. So, whatever
102:10 - domain you're in there will be different standards
for success and you’re going to need to
102:14 - know what applies in your domain. Next, are
specific metrics or ways of measuring. Now
102:23 - again, there are a few different categories
here. There are business metrics, there are
102:27 - key performance indicators or KPIs, there
are SMART goals (that's an acronym), and there's
102:33 - also the issue of having multiple goals. I'll
talk about each of those for just a second
102:37 - now. First off, let's talk about business
metrics. If you're in the commercial world
102:44 - there are some common ways of measuring success.
A very obvious one is sales revenue; are you
102:51 - making more money, are you moving the merchandise,
are you getting sales. Also, there's the issue
102:57 - of leads generated, new customers, or new
potential customers because that, then, in
103:01 - turn, is associated with future sales. There's
also the issue of customer value or lifetime
103:08 - customer value, so you may have a small number
of customers, but they all have a lot of revenue
103:13 - and you can use that to really predict the
overall profitability of your current system.
103:20 - And then there's churn rate, which has to
do with, you know, losing and gaining new
103:24 - customers and having a lot of turnover. So,
any of these are potential ways for defining
103:29 - success and measuring it. These are potential
metrics, there are others, but these are some
103:34 - really common ones. Now, I mentioned earlier
something called a key performance indicator
103:40 - or KPI. KPIs come from David Parmenter and
he's got a few ways of describing them, he
103:47 - says a key performance indicator for business.
Number one should be nonfinancial, not just
103:51 - the bottom line, but something else that might
be associated with it or that measures the
103:55 - overall productivity of the association. They
should be timely, for instance, weekly, daily,
104:02 - or even constantly gathered information. They
should have a CEO focus, so the senior management
104:08 - teams are the ones who generally make the
decisions that affect how the organization
104:12 - acts on the KPIs. They should be simple, so
everybody in the organization, everybody knows
104:18 - what they are and knows what to do about them.
They should be team-based, so teams can take
104:24 - joint responsibility for meeting each one
of the KPIs. They should have significant
104:30 - impact, what that really means is that they
should affect more than one important outcome,
104:36 - so you can do profitably and market reach
or improved manufacturing time and fewer defects.
104:43 - And finally, an ideal KPI has a limited dark
side, that means there's fewer possibilities
104:49 - for reinforcing the wrong behaviors and rewarding
people for sort of exploiting the system.
104:56 - Next, there are SMART goals, where SMART stands
for Specific, Measurable, Assignable to a
105:05 - particular person, Realistic (meaning you
can actually do it with the resources you
105:09 - have at hand), and Time-bound, (so you know
when it can get done). So, whenever you form
105:14 - a goal you should try to assess it on each
of these criteria and that's a way of saying
105:19 - that this is a good goal to be used as a metric
for the success of our organization. Now,
105:26 - the trick, however, is when you have multiple
goals, multiple possible endpoints. And the
105:33 - reason that's difficult is because, well,
it's easy to focus on one goal if you're just
105:37 - trying to maximize revenue or if you're just
trying to maximize graduation rate. There's
105:43 - a lot of things you can do. It becomes more
difficult when you have to focus on many things
105:48 - simultaneously, especially because some of
these goals may conflict. The things that
105:52 - you do to maximize one may impair the other.
And so when that happens, you actually need
105:58 - to start engaging in a deliberate process
of optimization, you need to optimize. And
106:04 - there are ways that you can do this if you
have enough data; you can do mathematical
106:08 - optimization to find the ideal balance of
efforts to pursue one goal and the other goal
106:14 - at the same time. Now, this is a very general
summary and let me finish with this. In sum,
106:21 - metrics or methods for measuring can help
awareness of how well your organization is
106:27 - functioning and how well you’re reaching
your goals. There are many different methods
106:31 - available for defining success and measuring
progress towards those things. The trick,
106:37 - however, comes when you have to balance efforts
to reach multiple goals simultaneously, which
106:42 - can bring in the need for things like optimization.
When talking about data sourcing and measurement,
106:50 - one very important issue has to do with the
accuracy of your measurements. The idea here
106:55 - is that you don’t want to have to throw
away all your ideas; you don’t want to waste
106:58 - effort. One way of doing this in a very quantitative
fashion is to make a classification table.
107:07 - So, what that looks like is this, you talk
about, for instance, positive results, negative
107:14 - results... and in fact let's start by looking
at the top here. The middle two columns here
107:18 - talk about whether an event is present, whether
your house is on fire, or whether a sale occurs,
107:22 - or whether you have got a tax evader, whatever.
So, that's whether a particular thing is actually
107:29 - happening or not. On the left here, is whether
the test or the indicator suggests that the
107:35 - thing is or is not happening. And then you
have these combinations of true positives;
107:41 - where the test says it's happening and it
really is, and false positives; where the
107:45 - test says it happening, but it is not, and
then below that true negatives, where the
107:49 - test says it isn’t happening and that's
correct and then false negatives, where the
107:54 - test says there's nothing going on, but there
is in fact the event occurring. And then you
107:58 - start to get the column totals, the total
number of events present or absent, then the
108:04 - row totals about the test results. Now, from
this table what you get is four kinds of accuracy,
108:12 - or really four different ways of quantifying
accuracy using different standards. And they
108:17 - go by these names: sensitivity, specificity,
positive predictive value, and negative predictive
108:26 - value. I'll show you very briefly how each
of them works. Sensitivity can be expressed
108:31 - this way, if there's a fire does the alarm
ring? You want that to happen. And so, that's
108:37 - a matter of looking at the true positives
and dividing that by the total number of alarms.
108:43 - So, the test positive means there's an alarm
and the event present means there's a fire;
108:49 - you want it to always have an alarm when there's
a fire. Specificity, on the other hand, is
108:55 - sort of the flip side of this. If there isn’t
a fire, does the alarm stay quiet? This is
109:00 - where you're looking at the ratio of true
negatives to total absent events, where there’s
109:07 - no fire, and the alarms aren’t ringing,
and that's what you want. Now, those are looking
109:12 - at columns; you can also go sideways across
rows. So, the first one there is positive
109:18 - predictive value, often abbreviated as PPV,
and we flip around the order a little bit.
109:25 - This one says, if the alarm rings, was there
a fire? So, now you're looking at the true
109:31 - positives and dividing it by the total number
of positives. Total number of positives is
109:36 - any time the alarm rings. True positives are
because there was a fire. And negative predictive
109:42 - value, or NPV, says of the alarm doesn’t
ring, does that in fact mean that there is
109:48 - no fire? Well, here you are looking at true
negatives and dividing it by total negatives,
109:53 - the time that it doesn’t ring. And again,
you want to maximize that so the true negatives
109:58 - account for all of the negatives, the same
way you want the true positives to account
110:01 - for all of the positives and so on. Now, you
can put numbers on all of these going from
110:06 - zero percent to a 100% and the idea is to
maximize each one as much as you can. So,
110:13 - in sum, from these tables we get four kinds
of accuracy and there's a different focus
110:19 - for each one. But, the same overall goal,
you want to identify the true positives and
110:25 - true negatives and avoid the false positives
and the false negatives. And this is one of
110:30 - way of putting numbers on, an index really,
on the accuracy of your measurement. Now data
110:39 - sourcing may seem like a very quantitative
topic, especially when we're talking about
110:43 - measurement. But, I want measure one important
thing here, and that is the social context
110:48 - of measurement. The idea here really, is that
people are people, and they all have their
110:54 - own goals, and they're going their own ways.
And we all have our own thoughts and feelings
110:58 - that don’t always coincide with each other,
and this can affect measurement. And so, for
111:03 - instance, when you’re trying to define your
goals and you're trying to maximize them you
111:07 - want to look at things like, for instance,
the business model. An organization's business
111:12 - model, the way they conduct their business,
the way they make their money, is tied to
111:16 - its identity and its reason to be. And if
you make a recommendation and it’scontrary
111:22 - to their business model, that can actually
be perceived as a threat to their core identity,
111:26 - and people tend to get freaked out in that
situation. Also, restrictions, so for instance,
111:32 - there may be laws, policies, and common practices,
both organizationally and culturally, that
111:39 - may limit the ways the goals can be met. Now,
most of these make a lot of sense, so the
111:45 - idea is you can’tjust do anything you want,
you need to have these constraints. And when
111:50 - you make your recommendations, maybe you'll
work creatively in them as long as you're
111:54 - still behaving legally and ethically, but
you do need to be aware of these constraints.
112:00 - Next, is the environment. And the idea here
is that competition occurs both between organizations,
112:07 - that company here is trying to reach a goal,
but they're competing with company B over
112:11 - there, but probably even more significantly
there is competition within the organization.
112:16 - This is really a recognition of office politics.
And when you, as a consultant, make a recommendation
112:21 - based on your analysis, you need to understand
you're kind of dropping a little football
112:25 - into the office and things are going to further
one person's career, maybe to the detriment
112:30 - of another. And in order for your recommendations
to have maximum effectiveness they need to
112:35 - play out well in the office. That’s something
that you need to be aware of as you're making
112:39 - your recommendations. Finally, there's the
issue of manipulation. And a sad truism about
112:47 - people is that any reward system, any reward
system at all, will be exploited and people
112:53 - will generally game the system. This happens
especially when you have a strong cut off;
112:59 - you need to get at least 80 percent, or you
get fired and people will do anything to make
113:05 - their numbers appear to be eighty percent.
This happens an awful lot when you look at
113:10 - executive compensation systems, it looks a
lot when you have very high stake school testing,
113:16 - it happens in an enormous number of situations,
and so, you need to be aware of the risk of
113:21 - exploitation and gaming. Now, don’t think,
then, that all is lost. Don’t give up, you
113:27 - can still do really wonderful assessment,
you can get good metrics, just be aware of
113:33 - these particular issues and be sensitive to
them as you both conduct your research and
113:37 - as you make your recommendations. So, in sum,
social factors affect goals and they affect
113:44 - the way you meet those goals. There are limits
and consequences, both on how you reach the
113:49 - goals and how, really, what the goal should
be and that when you’re making advice on
113:55 - how to reach those goals please be sensitive
to how things play out with metrics and how
114:01 - people will adapt their behavior to meet the
goals. That way you can make something that's
114:06 - more likely to be implemented the way you
meant and more likely to predict accurately
114:11 - what can happen with your goals. When it comes
to data sourcing, obviously the most important
114:18 - thing is to get data. But the easiest way
to do that, at least in theory, is to use
114:24 - existing data. Think of it as going to the
bookshelf and getting the data that you have
114:29 - right there at hand. Now, there's a few different
ways to do this: you can get in-house data,
114:35 - you can get open data, and you can get third-party
data. Another nice way to think of that is
114:41 - proprietary, public, and purchased data; the
three Ps I've heard it called. Let's talk
114:48 - about each of these a little bit more. So,
in-house data, that's stuff that's already
114:53 - in your organization. What’s nice about
that, it can be really fast and easy, it's
114:57 - right there and the format may be appropriate
for the kind of software in the computer that
115:03 - you are using. If you're fortunate, there's
good documentation, although sometimes when
115:08 - it's in-house people just kind of throw it
together, so you have to watch out for that.
115:12 - And there's the issue of quality control.
Now, this is true with any kind of data, but
115:16 - you need to pay attention with in-house, because
you don’t know the circumstances necessarily
115:20 - under which people gathered the data and how
much attention they were paying to something.
115:24 - There’s also an issue of restrictions; there
may be some data that, while it is in-house,
115:29 - you may not be allowed to use, or you may
not be able to publish the results or share
115:34 - the results with other people. So, these are
things that you need to think about when you're
115:39 - going to use in-house data, in terms of how
can you use it to facilitate your data science
115:46 - projects. Specifically, there are a few pros
and cons. In-house data is potentially quick,
115:52 - easy, and free. Hopefully it's standardized;
maybe even the original team that conducted
115:57 - this study is still there. And you might have
identifiers in the data which make it easier
116:02 - for you to do an individual level analysis.
On the con side however, the in-house data
116:08 - simply may not exist, maybe it's just not
there. Or the documentation may be inadequate
116:13 - and of course, the quality may be uncertain.
Always true, but may be something you have
116:19 - to pay more attention to when you’re using
in-house data. Now, another choice is open
116:25 - data like going to the library and getting
something. This is prepared data that's freely
116:30 - available, consists of things like government
data and corporate data and scientific data
116:36 - from a number of sources. Let me show you
some of my favorite open data sources just
116:41 - so you know where they are and that they exist.
Probably, the best one is data.gov here in
116:47 - the US. That is the, as it says right here,
the home of the US government's open data.
116:53 - Or, you may have a state level one. For instance,
I'm in Utah and we have data.utah.gov, also
116:59 - a great source of more regional information.
If you're in Europe, you have open-data.europa.eu,
117:05 - the European Union open data portal. And then
there are major non-profit organizations,
117:13 - so the UN has unicef.org/statistics for their
statistical and monitoring data. The World
117:20 - Health Organization has the global health
observatory at who.int/gho. And then there
117:29 - are private organizations that work in the
public interest, such as the Pew Research
117:35 - Center, which shares a lot of its data sets
and the New York Times, which makes it possible
117:40 - to use APIs to access a huge amount of the
data of things they've published over a huge
117:47 - time span. And then two of the mother loads,
there’s Google, which at google.com has
117:53 - public data which is a wonderful thing. And
then Amazon at aws.amazon.com/datasets has
118:01 - gargantuan datasets. So, if you needed a data
set that was like five terabytes in size,
118:05 - this is the place that you would go to get
it. Now, there's some pros and cons to using
118:11 - this kind of open data. First, is that you
can get very valuable datasets that maybe
118:15 - cost millions of dollars to gather and to
process. And you can get a very wide range
118:20 - of topics and times and groups of people and
so on. And often, the data is very well formatted
118:27 - and well documented. There are, however, a
few cons. Sometimes there's biased samples.
118:33 - Say, for instance, you only get people who
have internet access, and that can mean, not
118:39 - everybody. Sometimes the meaning of the data
is not clear or it may not mean exactly what
118:43 - you want it to. A potential problem is that
sometimes you may need to share your analyses
118:49 - and if you are doing proprietary research,
well, it’s going to have to be open instead,
118:54 - so that can create a crimp with some of your
clients. And then finally there are issues
118:59 - with privacy and confidentiality and in public
data that usually means that the identifiers
119:04 - are not there and you are going to have to
work at a larger aggregate level of measurement.
119:10 - Another option is to use data from a third-party,
these go by the name Data as a Service or
119:16 - DaaS. You can also call them data brokers.
And the thing about data brokers is they can
119:20 - give you an enormous amount of data on many
different topics, plus they can save you some
119:25 - time and effort, by actually doing some of
the processing for you. And that can include
119:29 - things like consumer behaviors and preferences,
they can get contact information, they can
119:35 - do marketing identity and finances, there’s
a lot of things. There’s a number of data
119:40 - brokers around, here's a few of them. Acxiom
is probably the biggest one in terms of marketing
119:47 - data. There’s also Nielsen which provides
data primarily for media consumption. And
119:53 - there's another organization Datasift, that’s
a smaller newer one. And there’s a pretty
119:59 - wide range of choices, but these are some
of the big ones. Now, the thing about using
120:03 - data brokers, there's some pros and there's
some cons. The pros are first, that it can
120:08 - save you a lot of time and effort. It can
also give you individual level data which
120:14 - can be hard to get from open data. Open data
is usually at the community level; they can
120:19 - give you information about specific consumers.
They can even give you summaries and inferences
120:25 - about things like credit scores and marital
status. Possibly even whether a person gambles
120:29 - or smokes. Now, the con is this, number 1
it can be really expensive, I mean this is
120:35 - a huge service; it provides a lot of benefit
and is priced accordingly. Also, you still
120:41 - need to validate it, you still need to double
check that it means what you think it means
120:45 - and that it works in with what you want. And
probably the real sticking point here is the
120:50 - use of third-party data is distasteful to
many people, and so you have to be aware that
120:57 - as you're making your choices. So, in sum,
as far as data sourcing existing data goes
121:03 - obviously data science needs data and there’s
the three Ps of data sources, Proprietary
121:10 - and Public and Purchased. But no matter what
source you use, you need to pay attention
121:15 - to quality and to the meaning and the usability
of the data to help you along in your own
121:21 - projects. When it comes to data sourcing,
a really good way of getting data is to use
121:28 - what are called APIs. Now, I like to think
of these as the digital version of Prufrock's
121:35 - mermaids. If you're familiar with the love
song on J. Alfred Prufrock by TS Eliot, he
121:40 - says, “I have heard the mermaids singing,
each to each,” that's TS Eliot. And I like
121:46 - to adapt that to say, “APIs have heard apps
singing, each to each,” and that's by me.
121:52 - Now, more specifically when we talk about
an API, what we're talking about is something
121:57 - called Application Programming Interface,
and this is something that allows programs
122:02 - to talk to each other. Its most important
use, in terms of data science, is it allows
122:07 - you to get web data. It allows your program
to directly go to the web, on its own, grab
122:12 - the data, bring it back in almost as though
it were local data, and that's a really wonderful
122:17 - thing. Now, the most common version of APIs
for data science are called REST APIs; that
122:25 - stands for Representational State Transfer.
That's the software architectural style of
122:30 - the world wide web and it allows you to access
data on web pages via HTTP, that's the hypertext
122:37 - transfer protocol. They, you know, run the
web as we know it. And when you download the
122:43 - data that you usually get its in JSON format,
that stands for Javascript Object Notation.
122:49 - The nice thing about that is that's human
readable, but it’s even better for machines.
122:54 - Then you can take that information and you
can send it directly to other programs. And
122:59 - the nice thing about REST APIs is that they're
what is called language agnostic, meaning
123:05 - any programming language can call a REST API,
can get data from the web, and can do whatever
123:10 - it needs to with it. Now, there are a few
kinds of APIs that are really common. The
123:16 - first is what are called Social APIs; these
are ways of interfacing with social networks.
123:21 - So, for instance, the most common one is Facebook;
there's also Twitter. Google Talk has been
123:26 - a big one and FourSquare as well and then
SoundCloud. These are on lists of the most
123:30 - popular ones. And then there are also what
are called Visual APIs, which are for getting
123:35 - visual data, so for instance, Google Maps
is the most common, but YouTube is something
123:40 - that accesses YouTube on a particular website
or AccuWeather which is for getting weather
123:46 - information. Pinterest for photos, and Flickr
for photos as well. So, these are some really
123:51 - common APIs and you can program your computer
to pull in data from any of these services
123:57 - and sites and integrate it into your own website
or here into your own data analysis. Now,
124:05 - there's a few different ways you can do this.
You can program it in R, the statistical programming
124:11 - language, you can do it in Python, also you
can even use it in the very basic BASH command
124:17 - line interface, and there's a ton of other
applications. Basically, anything can access
124:22 - an API one way or another. Now, I'd like to
show you how this works in R. So, I'm going
124:28 - to open up a script in RStudio and then I’m
going to use it to get some very basic information
124:33 - from a webpage. Let me go to RStudio and show
you how this works. Let me open up a script
124:39 - in RStudio that allows me to do some data
sourcing here. Now, I’m just going to use
124:45 - a package called JSON Lite, I’m going to
load that one up, and then I’m going to
124:50 - go to a couple of websites. I'm going to getting
historical data from Formula One car races
124:57 - and I’m going to be getting it from Ergast.com.
Now, if we go to this page right here, I can
125:02 - go straight to my browser right now. And this
is what it looks like; it gives you the API
125:09 - documentation, so what you're doing for an
API, is you're just entering a web address
125:13 - and in that web address it includes the information
you want. I'll go back to R here just for
125:18 - a second. And if I want to get information
about 1957 races in JSON format, I go to this
125:26 - address. I can skip over to that for a second,
and what you see is it's kind of a big long
125:35 - mess here, but it is all labeled and it is
clear to the computer what’s going on here.
125:40 - Let’s go back to R. And so what I'm going
to do is, I am going to save that URL into
125:49 - an object here, in R, and then I'm going to
use the command from JSON to read that URL
125:56 - and save it into R. And which it has now done.
And I’m going to zoom in on that so you
126:01 - can see what's happened. I've got this sort
of mess of text, this is actually a list object
126:07 - in R. And then I'm going to get just the structure
of that object, so I'm going to do this one
126:14 - right here and you can see that it's a list
and it gives you the names of all the variables
126:18 - within each one of the lists. And what I’m
going to do is, I'm going to convert that
126:24 - list to a data frame. I went through the list
and found where the information I wanted was
126:30 - located, you have to use this big long statement
here, that will give me the names of the drivers.
126:36 - Let me zoom in on that again. There they are.
And then I'm going to get just the column
126:44 - names for that bit of the data frame. So,
what I have here is six different variables.
126:50 - And then what I’m going to do is, I'm going
to pick just the first five cases and I’m
126:53 - going to select some variables and put them
in a different order. And when I do that,
127:00 - this is what I get. I will zoom in on that
again. And the first five people listed in
127:05 - this data set that I pulled from 1957, are
Juan Fangio, makes sense one of the greatest
127:11 - drivers ever, and other people who competed
in that year. And so what I've done is by
127:16 - using this API call in R, a very simple thing
to do, I was able to pull data off that webpage
127:23 - in a structured format, and do a very simple
analysis with it. And let's sum up what we’ve
127:29 - learned from all this. First off, APIs make
it really easy to work with web data, they
127:35 - structure, they call it for you, and then
they feed it straight into the program for
127:39 - you to analyze. And they are one of the best
ways of getting data and getting started in
127:44 - data science. When you're looking for data,
another great way of getting data is through
127:52 - scraping. And what that means is pulling information
from webpages. I like to think of it as when
127:57 - data is hiding in the open; it's there, you
can see it, but there's not an easy, immediate
128:04 - way to get that data. Now, when you’re dealing
with scraping, you can get data in several
128:09 - different formats. You can get HTML text from
webpages, you can get HTML tables from the
128:15 - rows and columns that appear on webpages.
You can scrape data from PDFs, and you can
128:19 - scrape data from all sorts of data from images
and video and audio. Now, we will make one
128:25 - very important qualification before we say
anything else: pay attention to copyright
128:30 - and privacy. Just because something is on
the web, doesn’t mean you're allowed to
128:35 - pull it out. Information gets copyrighted,
and so when I use examples here, I make sure
128:40 - that this is stuff that's publicly available,
and you should do the same when you are doing
128:45 - your own analyses. Now, if you want to scrape
data there's a couple of ways to do it. Number
128:50 - one, is to use apps that are developed for
this. So, for instance, import.io is one of
128:55 - my favorites. It is both a webpage, that’s
its address, and it's a downloadable app.
128:59 - There's also ScraperWiki. There's an application
called Tabula, and you can even do scraping
129:04 - in Google Sheets, which I will demonstrate
in a second, and Excel. Or, if you don’t
129:09 - want to use an app or if you want to do something
that apps don’t really let you do, you can
129:13 - code your scraper. You can do it directly
in R, or Python, or Bash, or even Java or
129:21 - PHP. Now, what you're going to do is you’re
going to be looking for information on the
129:26 - webpage. If you're looking for HTML text,
what you're going to do is pull structured
129:32 - text from webpages, similar to how a reader
view works in a browser. It uses HTML tags
129:39 - on the webpage to identify what's the important
information. So, there's things like body,
129:44 - and h1 for header one, and p for paragraph,
and the angle brackets. You can also get information
129:50 - from HTML tables, although this is a physical
table of rows and columns I am showing you.
129:55 - This also uses HTML table tags, that is like
table, and tr for table row, and td for table
130:01 - data, that's the cell. The trick is when you're
doing this, you need the table number and
130:05 - sometimes you just have to find that through
trial and error. Let me give you an example
130:09 - of how this works. Let's take a look at this
Wikipedia page on the Iron Chef America Competition.
130:16 - I'm going to go to the web right now and show
you that one. So, here we are in Wikipedia,
130:22 - Iron Chef America. And if you scroll down
a little bit, you see we have got a whole
130:26 - bunch of text here, we have got our table
of contents, and then we come down here, we
130:30 - have a table that lists the winners, the statistics
for the winners. And let's say we want to
130:35 - pull that from this webpage into another program
for us to analyze. Well, there is an extremely
130:42 - easy way to do this with Google Sheets. All
we need to do is open up the Google Sheet
130:47 - and in cell A1 of that Google Sheet, we paste
in this formula. It's IMPORTHTML, then you
130:55 - give the webpage and then you say that you
are importing a table, you have to put that
130:59 - stuff in quotes, and the index number for
the table. I had to poke around a little bit
131:03 - to figure out this was table number 2. So,
let me go to Google Sheets and show you how
131:08 - this works. Here I have a Google Sheet and
right now it's got nothing in it. But watch
131:13 - this; if I come here to this cell, and I simply
paste in that information, all the stuff just
131:19 - sort of magically propagates into the sheet,
makes it extremely easy to deal with, and
131:25 - now I can, for instance, save this as a CSV
file, put it in another program. Lots of options.
131:30 - And so this is one way that I'm scraping the
data from a webpage because I didn’t use
131:34 - an API, but I just used a very simple, one-link
command to get the information. Now, that
131:42 - was a HTML table. You can also scrape data
from PDFs. You have to be aware of if it's
131:48 - a native PDF, I call that a text PDF, or a
scanned or imaged PDF. And what it does with
131:53 - native PDFs, it looks for text elements; again
those are like code that indicates this is
131:58 - text. And you can deal with Raster images,
that's pixel images, or vector, which draws
132:03 - the lines, and that's what makes them infinitely
scalable in many situations. And then in PDFs,
132:09 - you can deal with tabular data, but you probably
have to use a specialized program like Scraper,
132:13 - Wiki, or Tabula in order to get that. And
then finally media, like images and video
132:20 - and audio. Getting images is easy; you can
download them in a lot of different ways.
132:25 - And then if you want to read data from them,
say for instance, you have a heat map of a
132:29 - country, you can go through it, but you will
probably have to write a program that loops
132:32 - through the image pixel-by-pixel to read the
data and them encode it numerically into your
132:38 - statistical program. Now, that’s my very
brief summary and let's summarize that. First
132:44 - off, if the data you are trying to get at
doesn’t have an existing API, you can try
132:49 - scraping and you can write code in a language
like R or Python. But, no matter what you
132:56 - do, be sensitive to issues of copyright and
privacy, so you don’t get yourself in hot
133:00 - water, but instead, you make an analysis that
can be of use to you or to your client. The
133:07 - next step in data sourcing is making data.
And specifically, we're talking about getting
133:12 - new data. I like to think of this as, you're
getting your hands on and you're getting "data
133:17 - de novo," new data. So, can't find the data
that you need for your analysis? Well, one
133:24 - simple solution is, do it yourself. And we're
going to talk about a few general strategies
133:29 - used for doing that. Now, these strategies
vary on a few dimensions. First off is the
133:34 - role. Are you passive and simply observing
stuff that's happening already, or are you
133:39 - active where you play a role in creating the
situation to get the data? And then there's
133:44 - the "Q/Q question," and that is, are you going
to get quantitative, or numerical, data, or
133:51 - are you going to get qualitative data, which
usually means text, paragraphs, sentences
133:56 - as well as things like photos and videos and
audio? And also, how are you going to get
134:00 - the data? Do you want to get it online, or
do you want to get it in person? Now, there's
134:05 - other choices than these, but these are some
of the big delineators of the methods. When
134:11 - you look at those, you get a few possible
options. Number one is interviews, and I'll
134:15 - say more about those. Another one is surveys.
A third one is card sorting. And a fourth
134:21 - one is experiments, although I actually want
to split experiments into two kinds of categories.
134:27 - The first one is laboratory experiments, and
that's in-person projects where you shape
134:32 - the information or an experience for the participants
as a way of seeing how that involvement changes
134:39 - their reactions. It doesn't necessarily mean
that you're a participant, but you create
134:43 - the situation. And then there's also A/B testing.
This is automated, online testing of two or
134:50 - more variations on a webpage. It's a very,
very simple kind of experimentation that's
134:56 - actually very useful for optimizing websites.
So, in sum, from this very short introduction
135:04 - make sure you can get exactly what you need.
Get the data you need to answer your question.
135:09 - And if you can't find it somewhere, then make
it. And, as always, you have many possible
135:15 - methods, each of which have their own strengths
and their own compromises. And we'll talk
135:19 - about each of those in the following sections.
The first method of data sourcing where you're
135:26 - making new data that I want to talk about
is interviews. And that's not because it's
135:30 - the most common, but because it's the one
you would do for the most basic problem. Now,
135:36 - basically an interview is nothing more than
a conversation with another person or a group
135:41 - of people. And, the fundamental question is,
why do interviews as opposed to doing a survey
135:47 - or something else? Well, there's a few good
reasons to do that. Number one: you're working
135:53 - with a new topic and you don't know what people's
responses will be, how they'll react. And
135:59 - so you need something very open-ended. Number
two: you're working with a new audience and
136:04 - you don't know how they will react in particular
to what it is you're trying to do. And number
136:09 - three: something's going on with the current
situation, it's not working anymore, and you
136:13 - need to find what's going on, and you need
to find ways to improve. The open-ended information
136:19 - where you get past you're existing categories
and boundaries can be one of the most useful
136:23 - methods for getting that data. If you want
to put it another way, you want to do interviews
136:28 - when you don't want to constrain responses.
Now, when it comes to interviews, you have
136:35 - one very basic choice, and that's whether
you do a structured interview. And with a
136:39 - structured interview, you have a predetermined
set of questions, and everyone gets the same
136:44 - questions in the same order. It gives a lot
of consistency even though the responses are
136:49 - open-ended. And then you can also have what's
called an unstructured interview. And this
136:54 - is a whole lot more like a conversation where
you as the interviewer and the person you're
136:59 - talking to - your questions arise in response
to their answers. Consequently, an unstructured
137:06 - interview can be different for each person
that you talk to. Also, interviews are usually
137:12 - done in person, but not surprisingly, they
can be done over the phone, or often online.
137:19 - Now, a couple of things to keep in mind about
interviews. Number one is time. Interviews
137:25 - can range from just a few minutes to several
hours per person. Second is training. Interviewing's
137:32 - a special skill that usually requires specific
training. Now, asking the questions is not
137:38 - necessarily the hard part. The really tricky
part is the analysis. The hardest part of
137:43 - interviews by far is analyzing the answers
for themes, and way of extracting the new
137:48 - categories and the dimensions that you need
for your further research. The beautiful thing
137:54 - about interviews is that they allow you to
learn things that you never expected. So,
138:00 - in sum: interviews are best for new situations
or new audiences. On the other hand, they
138:06 - can be time-consuming, and they also require
special training; both to conduct the interview,
138:10 - but also to analyze the highly qualitative
data that you get from them. The next logical
138:17 - step in data sourcing and making data is surveys.
Now, think of this: if you want to know something
138:23 - just ask. That’s the easy way. And you want
to do a survey under certain situations. The
138:29 - real question is, do you know your topic and
your audience well enough to anticipate their
138:34 - answers? To know what the range of their answers
and the dimensions and the categories that
138:45 - are going to be important. If you do, then
a survey might be a good approach. Now, just
138:50 - as there were a few dimensions for interviews,
there are a few dimensions for surveys. You
138:55 - can do what is called a closed-ended survey;
that is also called a forced choice. It is
139:02 - where you give people just particular options,
like a multiple choice. You can have an open-ended
139:04 - survey, where you have the same questions
for everybody, but you allow them to write
139:08 - in a free-form response. You can so surveys
in person and you can also do them online
139:12 - or over the mail or phone or however. And
now, it is very common to use software when
139:15 - doing surveys. Some really common applications
for online surveys are SurveyMonkey, and Qualtrics,
139:19 - or at the very simple end there is Google
Forms, and the simple and pretty end there
139:26 - is Typeform. There is a lot more choices,
but these are some of the major players and
139:32 - how you can get data from online participants
in survey format. Now, the nice thing about
139:39 - surveys is, they are really easy to do, they
are very easy to set up and they are really
139:45 - easy to send out to large groups of people.
You can get tons of data really fast. On the
139:51 - other hand, the same way that they are easy
to do, they are also really easy to do badly.
139:56 - The problem is that the questions you ask,
they can be ambiguous, they can be double-barreled,
140:00 - they can be loaded and the response scales
can be confusing. So, if you say, “I never
140:07 - think this particular way” and the person
puts strongly disagree, they may not know
140:11 - exactly what you are trying to get at. So,
you have to take special effort to make sure
140:14 - that the meaning is clear, unambiguous, and
that the rating scale, the way that people
140:24 - respond, is very clear and they know where
their answer falls. Which gets us into one
140:31 - of the things about people behaving badly
and that is beware the push poll. Now, especially
140:37 - during election time; like we are in right
now, a push poll is something that sounds
140:42 - like a survey, but really what it is is a
very biased attempt to get data, just fodder
140:47 - for social media campaigns or I am going to
make a chart that says that 98% of people
140:51 - agree with me. A push poll is one that is
so biased, there is really only one way to
140:57 - answer to the questions. This is considered
extremely irresponsible and unethical from
141:00 - a research point of view. Just hang up on
them. Now, aside from that egregious violation
141:06 - of research ethics, you do need to do other
things like watch out for bias in the question
141:12 - wording, in the response options, and also
in the sample selection because any one of
141:16 - those can push your responses off one way
or another without you really being aware
141:25 - that it is happening. So, in sum, let's say
this about surveys. You can get lots of data
141:32 - quickly, on the other hand, it requires familiarity
with the possible answers in your audience.
141:39 - So, you know, sort of, what to expect. And
no matter what you do, you need to watch for
141:43 - bias to make sure that your answers are going
to be representative of the group that you
141:52 - are really concerned about understanding.
An interesting topic in Data Sourcing when
141:57 - you are making data is Card Sorting. Now,
this isn’t something that comes up very
142:00 - often in academic research, but in web research,
this can be a really important method. Think
142:06 - of it as what you are trying to do is like
building a model of a molecule here, you are
142:10 - trying to build a mental model of people's
mental structures. Put more specifically,
142:14 - how do people organize information intuitively?
And also, how does that relate to the things
142:18 - that you are doing online? Now, the basic
procedure goes like this: you take a bunch
142:23 - of little topics and you write each one on
a separate card. And you can do this physically,
142:27 - with like three by five cards, or there are
a lot of programs that allow you to do a digital
142:30 - version of it. Then what you do is you give
this information to a group of respondents
142:35 - and the people sort those cards. So, they
put similar topics with each other, different
142:43 - topics over here and so on. And then you take
that information and from that you are able
142:50 - to calculate what is called, dissimilarity
data. Think of it as like the distance or
142:54 - the difference between various topics. And
that gives you the raw data to analyze how
142:58 - things are structured. Now, there are two
very general kinds of card sorting tasks.
143:01 - There are generative and there's evaluative.
A generative card sorting task is one in which
143:05 - respondents create their own sets, their own
piles of cards using any number of groupings
143:09 - they like. And this might be used, for instance,
to design a website. If people are going to
143:15 - be looking for one kind of information next
to another one, then you are going to want
143:23 - to put that together on the website, so they
know where to expect it. On the other hand,
143:29 - if you've already created a website, then
you can do an evaluative card sorting. This
143:32 - is where you have a fixed number or fixed
names of categories. Like for instance, the
143:39 - way you have set up your menus already. And
then what you do is you see if people actually
143:44 - put the cards into these various categories
that you have created. That’s a way of verifying
143:51 - that your hierarchical structure makes sense
to people. Now, whichever method you do, generative
143:56 - or evaluative, what you end up with when you
do a card structure is an interesting kind
144:02 - of visualization called a Dendrogram. That
actually means branches. And what we have
144:07 - here is actually a hundred and fifty data
points; if you are familiar with the Fisher’s
144:13 - Iris data, that's what's going on here. And
it groups it from one giant group on the left
144:15 - and then splits it in pieces and pieces and
pieces until you end up with lots of different
144:18 - observations, well actually, individual-level
observations at the end. But you can cut things
144:23 - off into two or three groups or whatever is
most useful for you here, as a way of visualizing
144:30 - the entire collection of similarity or dissimilarity
between the individual pieces of information
144:36 - that you had people sort. Now, I will just
mention very quickly if you want to do a digital
144:43 - card sorting, which makes your life infinitely
easier because keeping track of physical cards
144:49 - is really hard. You can use something like
Optimal Workshop, or UserZoom or UX Suite.
144:56 - These are some of the most common choices.
Now, let's just sum up what we've learned
145:01 - about card sorting in this extremely brief
overview. Number one, card sorting allows
145:05 - you to see intuitive organization of information
in a hierarchical format. You can do it with
145:10 - physical cards or you can also have digital
choices for doing the same thing. And when
145:14 - you are done, you actually get this hierarchical
or branched visualization of how the information
145:22 - is structured and related to each other. When
you are doing your Data Sourcing and you are
145:28 - making data, sometimes you can't get what
you want through the easy ways, and you’ve
145:32 - got to take the hard way. And you can do what
I am calling laboratory experiments. Now of
145:40 - course, when I mention laboratory experiments
people start to think of stuff like, you know,
145:43 - doctor Frankenstein in his lab, but lab experiments
are less like this and in fact they are a
145:46 - little more like this. Nearly every experiment
I have done in my career has been a paper
145:47 - and pencil one with people in a well-lighted
room and it's not been the threatening kind.
145:53 - Now, the reason you do a lab experiment is
because you want to determine cause and effect.
146:00 - And this is the single most theoretically
viable way of getting that information. Now,
146:03 - what makes an experiment an experiment is
the fact that researchers play active roles
146:07 - in experiments with manipulations. Now, people
get a little freaked out when they hear manipulations,
146:12 - think that you are coercing people and messing
with their mind. All that means is that you
146:16 - are manipulating the situation; you are causing
something to be different for one group of
146:24 - people or for one situation than another.
It's a benign thing, but it allows you to
146:27 - see how people react to those different variations.
Now, you are going to want to do an experiment,
146:28 - you are going to want to have focused research,
it is usually done to test one thing or one
146:32 - variation at a time. And it is usually hypothesis-driven;
usually you don’t do an experiment until
146:37 - you have done enough background research to
say, “I expect people to react this way
146:42 - to this situation and this way to the other.”
A key component to all of this is that experiments
146:48 - almost always have random assignment regardless
of how you got your sample, when they are
146:51 - in your study, you randomly assign them to
one condition or another. And what they does
146:54 - is it balances out the pre-existing differences
between groups and that's a great way of taking
146:57 - care of confounds and artifacts. The things
that are unintentionally associated with differences
147:00 - between groups that provide alternate explanations
for your data. If you have done good random
147:03 - assignment and you have a large enough group
of people than those confounds and artifacts
147:06 - are basically minimized. Now, some places
where you are likely to see laboratory experiments
147:09 - in this version are for instance are eye tracking
and web design. This is where you have to
147:15 - bring people in front of a computer and you
stick a thing there that sees where they are
147:19 - looking. That’s how we know for instance
that people don’t really look at ads on
147:24 - the side of web pages. Another very common
place is research in medicine and education
147:28 - and in my field, psychology. And in all of
these, what you find is that experimental
147:33 - research is considered the gold standard for
reliable valid information about cause and
147:37 - effect. On the other hand, while it is a wonderful
thing to have, it does come at a cost. Here's
147:45 - how that works. Number 1, experimentation
requires extensive, specialized training.
147:48 - It is not a simple thing to pick up. Two,
experiments are often very time consuming
147:55 - and labor intensive. I have known some that
take hours per person. And number three, experiments
148:00 - can be very expensive. So, what that all means
is that you want to make sure that you have
148:07 - done enough background research and you need
to have a situation where it is sufficiently
148:12 - important to get really reliable cause and
effect information to justify these costs
148:17 - for experimentation. In sum, laboratory experimentation
is generally considered the best method for
148:21 - causality or assessing causality. That's because
it allows you to control for confounds through
148:26 - randomization. On the other hand, it can be
difficult to do. So, be careful and thoughtful
148:32 - when considering whether you need to do an
experiment and how to actually go about doing
148:38 - it. There's one final procedure I want to
talk about in terms of Data Sourcing and Making
148:44 - New Data. It’s a form of experimentation
and it is simply called A/B testing and it's
148:51 - extremely common in the web world. So, for
instance, I just barely grabbed a screenshot
148:55 - of Amazon.com's homepage and you’re got
these various elements on the homepage and
149:04 - I just noticed, by the way, when I did this
that this woman is actually an animated gif,
149:12 - so she moves around. That was kind of weird;
I have never seen that before. But the thing
149:21 - about this, is this entire layout, how things
are organized and how they are on there, will
149:27 - have been determined by variations on A/B
testing by Amazon. Here's how it works. For
149:31 - your webpage, you pick one element like what’s
the headline or what are the colors or what's
149:36 - the organization or how do you word something
and you create multiple versions, maybe just
149:44 - two version A and version B, why you call
it A/B testing. Then when people visit your
149:51 - webpage you randomly assign these visitors
to one version or another, you have software
149:55 - that does that for you automatically. And
then you compare the response rates on some
150:04 - response. I will show you those in a second.
And then, once you have enough data, you implement
150:10 - the best version, you sort of set that one
solid and then you go on to something else.
150:16 - Now, in terms of response rates, there are
a lot of different outcomes you can look at.
150:22 - You can look at how long a person is on a
page, you can actually do mouse tracking if
150:29 - you want to. You can look at click-throughs,
you can also look at shopping cart value or
150:33 - abandonment. A lot of possible outcomes. All
of these contribute through A/B testing to
150:36 - the general concept of website optimization;
to make your website as effective as it can
150:40 - possibly be. Now, the idea also is that this
is something that you are going to do a lot.
150:46 - You can perform A/B tests continually. In
fact, I have seen one person say that what
150:50 - A/B testing really stands for is always be
testing. Kind of cute, but it does give you
150:57 - the idea that improvement is a constant process.
Now, if you want some software to do A/B testing,
151:03 - two of the most common choices are Optimizely
and VWO, which stands for Visual Web Optimizer.
151:07 - Now, many others are available, but these
are especially common and when you get the
151:11 - data you are going to use statistical hypothesis
testing to compare the differences or really
151:15 - the software does it for you automatically.
But you may want to adjust the parameters
151:27 - because most software packages cut off testing
a little too soon and the information is not
151:33 - quite as reliable as it should be. But, in
sum, here is what we can say about A/B testing.
151:39 - It is a version of website experimentation;
it is done online, which makes it really easy
151:44 - to get a lot of data very quickly. It allows
you to optimize the design of your website
151:51 - for whatever outcome is important to you.
And it can be done as a series of continual
151:56 - assessments, testing, and development to make
sure that you're accomplishing what you want
151:58 - to as effectively as possible for as many
people as possible. The very last thing I
152:05 - want to talk about in terms of data sourcing
is to talk about the next steps. And probably
152:10 - the most important thing is, you know, don’t
just sit there. I want you to go and see what
152:16 - you already have. Try to explore some open
data sources. And if it helps, check with
152:22 - a few data vendors. And if those don't give
you what you need to do your project, then
152:27 - consider making new data. Again, the idea
here is get what you need and get going. Thanks
152:34 - for joining me and good luck on your own projects.
Welcome to “Coding in Data Science”. I'm
152:43 - Bart Poulson and what we are going to do in
this series of videos is we're going to take
152:47 - a little look at the tools of Data Science.
So, I am inviting you to know your tools,
152:52 - but probably even more important than that
is to know their proper place. Now, I mention
152:59 - that because a lot of the times when people
talk about data tools, they talk about it
153:04 - as though that were the same thing as data
science, as though they were the same set.
153:10 - But, I think if you look at it for just a
second that is not really the case. Data tools
153:15 - are simply one element of data science because
data science is made up of a lot more than
153:20 - the tools that you use. It includes things
like, business knowledge, it includes the
153:25 - meaning making and interpretation, it includes
social factors and so there’s much more
153:30 - than just the tools involved. That being said,
you will need at least a few tools and so
153:37 - we’re going to talk about some of the things
that you can use in data science if it works
153:41 - well for you. In terms of getting started,
the basic things. #1 is spreadsheets, it is
153:47 - the universal data tool and I'll talk about
how they play an important role in data science.
153:51 - #2 is a visualization program called Tableau,
there is Tableau public, which is free, and
153:58 - there's Tableau desktop and there is also
something called Tableau server. Tableau is
154:01 - a fabulous program for data visualization
and I’m convinced for most people provides
154:07 - the great majority of what they need. And
though while it is not a tool, I do need to
154:12 - talk about the formats used in web data because,
you have to be able to navigate that when
154:17 - doing a lot of data science work. Then we
can talk about some of the essential tools
154:22 - for data science. Those include the programming
language R, which is specifically for data,
154:28 - there's the general purpose programming language
Python, which has been well adapted to data.
154:33 - And there's the database language sequel or
SQL for structured query language. Then if
154:40 - you want to go beyond that, there are some
other things that you can do. There are the
154:45 - general purpose programming languages C, C++,
and Java, which are very frequently used to
154:50 - form the foundation of data science and sort
of high level production code is going to
154:56 - rely on those as well. There’s the command
line interface language Bash, which is very
155:03 - common, a very quick tool for manipulating
data. And then there's the, sort of wild card
155:10 - supercharged regular expressions or Regex.
We'll talk about all of these in separate
155:16 - courses. But, as you consider all the tools
that you can use, don't forget the 80/20 rule.
155:24 - Also known as the Pareto Principle. And the
idea here is that you are going to get a lot
155:29 - of bang for your buck out of small number
of things. And I'm going to show you a little
155:33 - sample graph here. Imagine that you have ten
different tools and we'll call them A through
155:39 - B. A does a lot for you, B does a little bit
less and it kind of tapers down to, you have
155:45 - got a bunch of tools that do just a little
of stuff that you need. Now, instead of looking
155:50 - at the individual effectiveness, look at the
cumulative effectiveness. How much are you
155:54 - able to accomplish with a combination of tools?
Well, the first ones right here at 60% where
155:59 - the tools started and then you add on the
20% from B and it goes up and then you add
156:04 - on C and D and you add up little smaller,
smaller pieces and by the time you get to
156:09 - the end, you have got 100% of effectiveness
from your ten tools combined. The important
156:15 - thing about this is, you only have to go to
the 2nd tool, that is two out of ten, that's
156:20 - B, that's 20% of your tools and in this made
up example, you have got 80% of your output.
156:28 - So, 80% of the output from 20% of the tools,
that's a fictional example of the Pareto Principle,
156:35 - but I find in real life it tends to work something
approximately like that. And so, you don't
156:41 - necessarily have to learn everything and you
don’t have to learn how to do everything
156:44 - in everything. Instead you want to focus on
the tools that will be most productive and
156:50 - specifically most productive for you. So,
in sum, let's say these three things. Number
156:57 - 1, coding or simply the ability to manipulate
data with programs and computers. Coding is
157:04 - important, but data science is much greater
than the collection of tools that's used in
157:11 - it. And then finally, as you're trying to
decide what tools to use and what you need
157:15 - to learn and how to work, remember the 80/20,
you are going to get a lot of bang from a
157:20 - small set of tools. So, focus on the things
that are going to be most useful for you in
157:23 - conducting your own data science projects.
As we begin our discussion of Coding and Data
157:31 - Science, I actually want to begin with something
that's not coding. I want to talk about applications
157:35 - or programs that are already created that
allow you to manipulate data. And we are going
157:39 - to begin with the most basic of these, spreadsheets.
We're going to do the rows and columns and
157:45 - cells of Excel. And the reason for this is
you need spreadsheets. Now, you may be saying
157:51 - to yourself, “no no no not me, because you
know what I'm fancy, I’m working in my big
157:58 - set of servers, I’ve got fancy things going
on.” But, you know what, you too fancy people,
158:04 - you need spreadsheets as well. There's a few
reasons for this. Most importantly, spreadsheets
158:10 - can be the right tool for data science in
a lot of circumstances; there are a few reasons
158:15 - for that. Number one, spreadsheets, they're
everywhere, they're ubiquitous, they're installed
158:19 - on a billion machines around the world and
everybody uses them. They probably have more
158:26 - data sets in spreadsheets than anything else,
and so it's a very common format. Importantly,
158:33 - it's probably your client's format; a lot
of your clients are going to be using spreadsheets
158:38 - for their own data. I’ve worked with billion
dollar companies that keep all of their data
158:42 - in spreadsheets. So, when you're working with
them, you need to know how to manipulate that
158:46 - and how to work with it. Also, regardless
of what you’re doing, spreadsheets are specifically
158:51 - csv - comma separated value files - are sort
of the lingua franca or the universal interchange
158:58 - format for data transfer, to allow you to
take it from one program to another. And then,
159:03 - truthfully, in a lot of situations they're
really easy to use. And if you want a second
159:08 - opinion on this, let's take a look at this
ranking. There's a survey of data mining experts,
159:14 - it’s the KDnuggets data mining poll, and
these are the tools they most use in their
159:20 - own work. And look at this: lowly Excel is
fifth on the list, and in fact, what's interesting
159:26 - about it is it's above Hadoop and Spark, two
of the major big data fancy tools. And so,
159:34 - Excel really does have place of pride in a
toolkit for data analyst. Now, since we’re
159:42 - going to sort of the low tech end of things,
let's talk about some of the things you can
159:45 - do with a spreadsheet. Number one, they are
really good for data browsing. You really
159:49 - get to see all of the data in front of you,
which isn't true if you are doing something
159:53 - like R or Python. They're really good for
sorting data, sort by this column then this
159:58 - column then this column. They're really good
for rearranging columns and cells and moving
160:03 - things around. They’re good for finding
and replacing and seeing what happens so you
160:07 - know that it worked right. Some more uses
they’re really good for formatting, especially
160:13 - conditional formatting. They're good for transposing
data, switching the rows and the columns,
160:18 - they make that really easy. They're good for
tracking changes. Now it's true if you're
160:22 - a big fancy data scientist you're probably
using GitHub, but for everybody else in the
160:28 - world spreadsheets and the tracking changes
is a wonderful way to do it. You can make
160:33 - pivot tables, that allows you to explore the
data in a very hands-on way, in a very intuitive
160:39 - way. And they're also really good for arranging
the output for consumption. Now, when you're
160:46 - working with spreadsheets, however, there's
one thing you need to be aware of: they are
160:49 - really flexible, but that flexibility can
be a problem in that when you are working
160:54 - in data science, you specifically want to
be concerned about something called Tidy Data.
160:58 - That's a term I borrowed from Hadley Wickham,
a very well-known developer in the R world.
161:03 - Tidy Data is for transferring data and making
it work well. There’s a few rules here that
161:09 - undo some of the flexibility inherent in spreadsheets.
Number one, what you want to do is have a
161:14 - column be equivalent to the same thing as
a variable; columns, variables, they are the
161:20 - same thing. And then, rows are equal - exactly
the same thing as cases. That you have one
161:27 - sheet per file, and that you have one level
of measurement, say, individual, then organization,
161:33 - then state per file. Again, this is undoing
some of the flexibility that's inherent in
161:39 - spreadsheets, but it makes it really easy
to move the data from one program to another.
161:45 - Let me show you how all this works. You can
try this in Excel. If you have downloaded
161:49 - the files for this course, we simply want
to open up this spreadsheet. Let me go to
161:54 - Excel and show you how it works. So, when
you open up this spreadsheet, what you get
162:00 - is totally fictional data here that I made
up, but it is showing sales over time of several
162:07 - products at two locations, like if you're
selling stuff at a baseball field. And this
162:13 - is the way spreadsheets often appear; we’ve
got blank rows and columns, we’ve got stuff
162:16 - arranged in a way that makes it easy for the
person to process it. And we have got totals
162:23 - here, with formulas putting them all together.
And that's fine, that works well for the person
162:28 - who made it. And then, that's for one month
and then we have another month right here
162:32 - and then we have another month right here
and then we combine them all for first quarter
162:36 - of 2014. We have got some headers here, we’ve
got some conditional formatting and changes
162:42 - and if we come to the bottom, we have got
a very busy line graphic that eventually loads;
162:47 - it's not a good graphic, by the way. But,
similar to what you will often find. So, this
162:54 - is the stuff that, while it may be useful
for the client's own personal use, you can’t
162:59 - feed this into R or Python, it will just choke
and it won’t know what to do with it. And
163:04 - so, you need to go through a process of tidying
up the data. And what this involves is undoing
163:10 - some of the stuff. So, for instance, here's
data that is almost tidy. Here we have a single
163:17 - column for date, a single column for the day,
a column for the site, so we have two locations
163:23 - A and B, and then we have six columns for
the six different things that are sold and
163:28 - how many were sold on each day. Now, in certain
situations, you would want the data laid out
163:33 - exactly like this if you are doing, for instance,
a time series, you will do something vaguely
163:36 - similar to this. But, for true tidy stuff,
we are going to collapse it even further.
163:42 - Let me come here to the tidy data. And now
what I have done is, I have created a new
163:46 - column that says what is the item being sold.
And so, by the way, what this means is that
163:51 - we have got a really long data set now, it
has got over a thousand rows. Come back up
163:56 - to the top here. But, what that shows you
is that now it's in a format that's really
164:03 - easy to import from one program to another,
that makes it tidy and you can re-manipulate
164:08 - it however you want once you get to each of
those. So, let's sum up our little presentation
164:14 - here, in a few lines. Number one, no matter
who you are, no matter what you are doing
164:18 - in data science you need spreadsheets. And
the reason for that is that spreadsheets are
164:24 - often the right tool for data science. Keep
one thing in mind though, that is as you are
164:30 - moving back and forth from one language to
another, tidy data or well-formatted data
164:34 - is going to be important for exporting data
into your analytical programmer language of
164:41 - choice. As we move through “Coding and Data
Science,” and specifically the applications
164:46 - that can be used, there's one that stands
out for me more than almost anything else,
164:50 - and that's Tableau and Tableau Public. Now,
if you are not familiar with these, these
164:56 - are visualization programs. The idea here
is that when you have data, the most important
165:01 - thing you can do is to first look and see
what you have and work with it from there.
165:07 - And in fact, I'm convinced that for many organizations
Tableau might be all that they really need.
165:15 - It will give them the level of insight that
they need to work constructively with data.
165:20 - So, let’s take a quick look by going to
tableau.com. Now, there are a few different
165:28 - versions of Tableau. Right here we have Tableau
Desktop and Tableau Server, and these are
165:35 - the paid versions of Tableau. They actually
cost a lot of money, unless you work for a
165:41 - nonprofit organization, in which case you
can get them for free. Which is a beautiful
165:46 - thing. What we're usually looking for, however,
is not the paid version, but we are looking
165:52 - for something called Tableau Public. And if
you come in here and go to products and we
165:58 - have got these three paid ones, over here
to Tableau Public. We click on that, it brings
166:06 - us to this page. It is public.tableau.com.
And this is the one that has what we want,
166:11 - it's the free version of Tableau with one
major caveat: you don’t save files locally
166:17 - to your computer, which is why I didn’t
give you a file to open. Instead, it saves
166:22 - them to the web in a public form. So, if you
are willing to trade privacy, you can get
166:29 - an immensely powerful application for data
visualization. That’s a catch for a lot
166:35 - of people, which is why people are willing
to pay a lot of money for the desktop version.
166:39 - And again, if you work for a nonprofit you
can get the desktop version for free. But,
166:43 - I am going to show you how things work in
Tableau Public. So, that’s something that
166:48 - you can work with personally. The first thing
you want to do is, you want to download it.
166:53 - And so, you put in your email address, you
download; it is going to know what you are
166:57 - on. It is a pretty big download. And once
it is downloaded, you can install and open
167:01 - up the application. And here I am in Tableau
Public, right here, this is the blank version.
167:06 - By the way, you also need to create an account
with Tableau in order to save your stuff online
167:12 - to see it. I will show you what that looks
like. But, you are presented with a blank
167:16 - thing right here and the first thing you need
to do is, you need to bring in some data.
167:20 - I'm going to bring in an Excel file. Now,
if you downloaded the files for the course,
167:26 - you will see that there is this one right
here, DS03_2_2_TableauPublic.excel.xlsx. In
167:31 - fact, it is the one that I used in talking
about spreadsheets in the first video in this
167:38 - course. I'm going to select that one and I’m
going to open it. And a lot of programs don’t
167:45 - like bringing in Excel because it's got all
the worksheets and all the weirdness in it.
167:49 - This one works better with it, but what I'm
going to do is, I am going to take the tidy
167:52 - data. By the way, you see that it put them
in alphabetical order here. I'm going to take
167:58 - tidy data and I’m going to drag it over
to let it know that it's the one that I want.
168:03 - And now what it does is it shows me a version
of the data set along with things that you
168:10 - can do here. You can rename it, I like that
you can create bin groups, there's a lot of
168:15 - things that you can do here. I'm going to
do something very, very quick with this particular
168:20 - one. Now, I've got the data set right here,
what I'm going to do now is I'm going to go
168:26 - to a worksheet. That’s where you actually
create stuff. Cancel that and go to worksheet
168:31 - one. Okay. This is a drag and drop interface.
And so what we are going to do is, we are
168:42 - going to pull the bits and pieces of information
we want to make graphics. There's immense
168:47 - flexibility here. I’m going to show you
two very basic ones. I'm going to look at
168:52 - the sales of my fictional ballpark items.
So, I'm going to grab sales right here and
168:58 - I'm going to put that as the field that we
are going to measure. Okay. And you see, put
169:03 - it down right here and this is our total sales.
We're going to break it down by item and by
169:10 - time. So, let me take item right here, and
you can drag it over here, or I can put it
169:15 - right up here into rows. Those will be my
rows and that will be how many we have sold
169:20 - total of each of the items. Fine, that’s
really easy. And then, let's take date and
169:26 - we will put that here in columns to spread
it across. Now, by default it is doing it
169:31 - by year, I don’t want to do that, I want
to have three months of data. So, what I can
169:35 - do is, I can click right here and I can choose
a different time frame. I can go to quarter,
169:40 - but that's not going to help because I only
have one quarter's worth of data, that's three
169:44 - months. I'm going to come down to week. Actually,
let me go to day. If I do day, you see it
169:50 - gets enormously complicated, so that’s no
good. So, I'm going to back up to week. And
169:55 - I’ve got a lot of numbers there, but what
I want is a graph. And so, to get that, I'm
170:01 - going to come over here and click on this
and tell it that I want a graph. And so, we're
170:06 - seeing the information, except it lost items.
So, I'm going to bring item and put it back
170:13 - up into this graph to say this is a row for
the data. And now I've got rows for sales
170:19 - by week for each of my items. That's great.
I want to break it down one more by putting
170:24 - in the site, the place that it sold. So, I'm
going to grab that and I'm going to put it
170:29 - right over here. And now you see I’ve got
it broken down by the item that is sold and
170:39 - the different sites. I'm going to color the
sites, and all I've got to do to do that is,
170:42 - I'm going to grab site and drag it onto color.
Now, I've got two different colors for my
170:49 - sites. And this makes it a lot easier to tell
what is going on. And in fact, there is some
170:54 - other cool stuff you can do. One of the things
I'm going to do is come over here to analytics
170:58 - and I can tell it to put an average line through
everything, so I'll just drag this over here.
171:06 - Now we have the average for each line. That's
good. And I can even do forecasting. Let me
171:14 - get a little bit of a forecast right here.
I will drag this on and if you can go over
171:20 - here. I will get this out of the way for a
second. Now, I have a forecast for the next
171:26 - few weeks, and that’s a really convenient,
quick, and easy thing. And again, for some
171:30 - organizations that might be all that they
really need. And so, what I’m showing you
171:36 - here is the absolute basic operation of Tableau,
which allows you to do an incredible range
171:42 - of visualizations and manipulate the data
and create interactive dashboards. There's
171:47 - so much to it and we'll show that in another
course, but for right now I want to show you
171:51 - one last thing about Tableau Public, and that
is saving the files. So now, when I come here
171:57 - and save it, it’s going to ask me to sign
into Tableau Public. Now, I sign in and it
172:06 - asks me how I want to save this, same name
as the video. There we go, and I'm going to
172:20 - hit save. And then that opens up a web browser,
and since I’m already logged into my account,
172:29 - see here's my account and my profile. Here's
the page that I created. And it's got everything
172:37 - that I need there; I'm going to edit just
a few details. I'm going to say, for instance,
172:43 - I'm going to leave its name just like that.
I can put more of a description in there if
172:46 - I wanted. I can allow people to download the
workbook and its data; I'm going to leave
172:51 - that there so you can download it if you need
to. If I had more than one tab, I would do
172:55 - this thing that says show the different sheets
as tabs. Hit save. And there’s my data set
173:07 - and also it’s published online and people
can now find it. And so what you have here
173:13 - is an incredible tool for creating interactive
visualizations; you can create them with drop-down
173:17 - menus, and you can rearrange things, and you
can make an entire dashboard. It's a fabulous
173:22 - way of presenting information, and as I said
before, I think that for some organizations
173:26 - this may be as much as they need to get really
good, useful information out of their data.
173:31 - And so I strongly recommend that you take
some time to explore with Tableau, either
173:37 - the paid desktop version or the public version
and see what you can do to get some really
173:42 - compelling and insightful visualizations out
of your work in data science. For many people,
173:49 - their first experience of “Coding and Data
Science” is with the application SPSS. Now,
173:57 - I think of SPSS and the first thing that comes
to my mind is sort of life in the Ivory tower,
174:02 - though this looks more like Harry Potter.
But, if you think about it the package name
174:08 - SPSS comes from Statistical Package for the
Social Sciences. Although, if you ask IBM
174:15 - about it now, they act like it doesn’t stand
for anything. But, it has its background in
174:19 - social science research which is generally
academic. And truthfully, I'm a social psychologist
174:24 - and that's where I first learned how to use
SPSS. But, let's take a quick look at their
174:29 - webpage ibm.com/spss. If you type that in,
that will just be an alias that will take
174:37 - you to IBM's main webpage. Now, IBM didn’t
create SPSS, but they bought it around version
174:42 - 16, and it was very briefly known as PASW
predictive analytic software, that only lasted
174:48 - briefly and now it's back to SPSS, which is
where it's been for a long time. SPSS is a
174:54 - desktop program; it's pretty big, it does
a lot of things, it's very powerful, and is
174:58 - used in a lot of academic research. It's also
used in a lot of business consulting, management,
175:04 - even some medical research. And the thing
about SPSS, is it looks like a spreadsheet
175:11 - but has drop-down menus to make your life
a little bit easier compared to some of the
175:14 - programming languages that you can use. Now,
you can get a free temporary version, if you're
175:21 - a student you can get a cheap version, otherwise
SPSS costs a lot of money. But, if you have
175:27 - it one way or another, when you open it up
this is what it is going to look like. I'm
175:33 - showing SPSS version 22, now it's currently
on 24. And the thing about SPSS versioning
175:42 - is, in anything other than software packaging,
these would be point updates, so I sort of
175:46 - feel like we should be on 17.3, as opposed
to 23 or 24. Because the variations are so
175:53 - small that anything you learn from the early
ones, is going to work on the later ones and
175:57 - there is a lot of backwards and forwards compatibility,
so I'd almost say that this one, the version
176:03 - I have practically doesn’t matter. You get
this little welcome splash screen, and if
176:08 - you don’t want to see it anymore you can
get rid of it. I'm just going to hit cancel
176:11 - here. And this is our main interface. It looks
a lot like a spreadsheet, the difference is,
176:16 - you have a separate pane for looking at variable
information and then you have separate windows
176:21 - for output and then an optional one for something
called Syntax. But, let me show you how this
176:25 - works by first opening up a data set. SPSS
has a lot of sample data sets in them, but
176:31 - they are not easy to get to and they are really
well hidden. On my Mac, for instance, let
176:36 - me go to where they are. In my mac I go to
the finder, I have to go to Mac, to applications,
176:44 - to the folder IBM, to SPSS, to statistics,
to 22 the version number, to samples, then
176:49 - I have to say I want the ones that are in
English, and then it brings them up. The .sav
176:56 - files are the actual data files, there are
different kinds in here, so .sav is a different
177:01 - kind of file and then we have a different
one about planning analyses. So, there are
177:05 - versions of it. I'm going to open up a file
here called “market values .sav,” a small
177:12 - data set in SPSS format. And if you don’t
have that, you can open up something else;
177:16 - it really doesn’t matter for now. By the
way, in case you haven’t noticed, SPSS tends
177:23 - to be really really slow when it opens. It
also, despite being version 24, it tends to
177:29 - be kind of buggy and crashes. So, when you
work with SPSS, you want to get in the habit
177:34 - of saving your work constantly. And also,
being patient when it is time to open the
177:39 - program. So, here is a data set that just
shows addresses and house values, and square
177:44 - feet for information. This, I don’t even
know if this is real information, it looks
177:48 - artificial to me. But, SPSS lets you do point
and click analyses, which is unusual for a
177:57 - lot of things. So, I am going to come up here
and I am going to say, for instance, make
178:01 - a graph. I'm going to make a- I’m going
to use what is called a legacy dialogue to
178:06 - get a histogram of house prices. So, I simply
click values. Put that right there and I will
178:13 - put a normal curve in top of it and click
ok. This is going to open up a new window,
178:19 - and it opened up a microscopic version of
it, so I’m going to make that bigger. This
178:24 - is the output window, this is a separate window
and it has a navigation pane here on the side.
178:31 - It tells me where the data came from, and
it saves the command here, and then, you know,
178:37 - there's my default histogram. So, we see most
of the houses were right around $125,000,
178:43 - and then they went up to at least $400,000.
I have a mean of $256,000, a standard deviation
178:53 - of about $80,000, and then there is 94 houses
in the data set. Fine, that's great. The other
178:58 - thing I can do is, if I want to do some analyses,
let me go back to the data just for a moment.
179:04 - For instance, I can come here to analyze and
I can do descriptive and I'm actually going
179:10 - to do one here called Explore. And I'll take
the purchase price and I'll put it right here
179:19 - and I’m going to get a whole bunch just
by default. I'm going to hit ok. And it goes
179:23 - back to the output window. Once again made
it tiny. And so, now you see beneath my chart
179:31 - I now have a table and I’ve got a bunch
of information. A stem and leaf plot, and
179:36 - a box plot too, a great way of checking for
outliers. And so this is a really convenient
179:42 - way to save things. You can export this information
as images, you can export the entire file
179:48 - as an HTML, you can do it as a pdf or a PowerPoint.
There’s a lot of options here and you can
179:54 - customize everything that's on here. Now,
I just want to show you one more thing that
179:59 - makes your life so much easier in SPSS. You
see right here that it's putting down these
180:05 - commands, it’s actually saying graph, and
then histogram, and normal equals value. And
180:09 - then down here, we've got this little command
right here. Most people don’t know how to
180:16 - save their work in SPSS, and that's something
you kind of just have to do it over again
180:20 - every time, but there's a very simple way
to do this. What I’m going to do is, I’m
180:25 - going to open up something called a Syntax
file. I'm going to go to new, Syntax. And
180:32 - this is just a blank window that's a programming
window, it's for saving code. And let me go
180:40 - back to my analysis I did a moment ago. I'll
go back to analyze and I can still get at
180:45 - it right here. Descriptives and explore, my
information is still there. And what happens
180:53 - here is, even though I set it up with drop-down
menus and point and click, if I do this thing,
180:58 - paste, then what it does is, it takes the
code that creates that command and it saves
181:03 - it to this syntax window. And this is just
a text file. It saves it as .spss, but it
181:08 - is a text file that can be opened in anything.
And what’s beautiful about this is, it is
181:12 - really easy to copy and paste, and you can
even take this into Word and do a find and
181:18 - replace on it, and it's really easy to replicate
the analyses. And so for me, SPSS is a good
181:26 - program. But, until you use Syntax you don’t
know the true power of it and it makes your
181:31 - life so much easier as a way of operating
it. Anyhow, this is my extremely brief introduction
181:37 - to SPSS. All I want to say is that it is a
very common program, kind of looks like a
181:42 - spreadsheet, but it gives you a lot more power
and options and you can use both drop-down
181:47 - menus and text-based Syntax commands as well
to automate your work and make it easier to
181:53 - replicate it in the future. I want to take
a look at one more application for “Coding
181:59 - and Data Science”, that's called JASP. This
is a new application, not very familiar to
182:04 - a lot of people and still in beta, but with
an amazing promise. You can basically think
182:08 - of it as a free version of SPSS and you know
what, we love free. But, JASP is not just
182:16 - free, it's also open source, and it's intuitive,
and it makes analyses replicable, and it even
182:25 - includes Bayesian approaches. So, take that
all together, you know, we're pretty happy
182:30 - and we're jumping for joy. So, before we move
on, you just may be asking yourself, JASP,
182:37 - what is that? Well, the creator has emphatically
denied that it stands for Just Another Statistics
182:44 - Program, but be that as it may, we will just
go ahead and call it JASP and use it very
182:49 - happily. You can get to it by going to jasp-stats.org.
And let's take a look at that right now. JASP
182:56 - is a new program, they say a low fat alternative
to SPSS, but it is a really wonderful great
183:03 - way of doing statistics. You're going to want
to download it, by supplying your platform;
183:09 - it even comes in Linux format, which is beautiful.
And again, it's beta so stay posted, things
183:15 - are updating regularly. If you're on Mac,
you're going to need to use Xquartz, that's
183:21 - an easy thing to install and it makes a lot
of things work better. And it's the wonderful
183:24 - way to do analyses. When you open up JASP,
it's going to look like this. It's a pretty
183:31 - blank interface, but it's really easy to get
going with it. So for instance, you can come
183:36 - over here to file and you can even choose
some example data sets. So for instance, here's
183:42 - one called Big 5 that's personality factors.
And you've got data here that's really easy
183:49 - to work with. Let me scroll this over here
for a moment. So, there's our five variables
183:53 - and let's do some quick analyses with these.
Say for instance, we want to get descriptives;
183:59 - we can pick a few variables. Now, if you're
familiar with SPSS, the layout feels very
184:06 - much the same and the output looks a lot the
same. You know, all I have to do is select
184:12 - what I want and it immediately pops up over
here. Then I can choose additional statistics,
184:17 - I can get core tiles, I can get the median.
And you can choose plots; let's get some plots,
184:24 - all you have to do is click on it and they
show up. And that's a really beautiful thing
184:28 - and you can modify these things a little bit,
so for instance, I can take the plot points.
184:33 - Let's see if I can drag that down and if I
make it small enough I can see the five plots,
184:39 - I went a little too far on that one. Anyhow,
you can do a lot of things here. And I can
184:49 - hide this, I can collapse that and I can go
on and do other analyses. Now, what's really
184:55 - neat though is when I navigate away, so I
just clicked in a blank area of the results
185:00 - page, we are back to the data here. But if
I click on one of these tables, like this
185:05 - one right here, it immediately brings up the
commands that produced it and I can just modify
185:10 - it some more if I want. Say I want skewness
and kurtosis, boom they are in there. It is
185:16 - an amazing thing and then I can come back
out here, I can click away from that and I
185:21 - can come down to the plots expand those and
if I click on that it brings up the commands
185:27 - that made them. It's an amazingly easy and
intuitive way to do things. Now, there’s
185:32 - another really nice thing about JASP and that
is that you can share the information online
185:38 - really well through a program called osf.io.
That stands for the open science foundation,
185:45 - that’s its web address osf.io. So, let's
take a quick look at what that's like. Here's
185:51 - the open science framework website and it's
a wonderful service, it's free and it's designed
185:56 - to support open, transparent, accessible,
accountable, collaborative research and I
186:04 - really can’t say enough nice things about
it. What’s neat about this is once you sign
186:09 - up for OSF you can create your own area and
I've got one of my own, I will go to that
186:17 - now. So, for instance, here's the datalab
page in open science framework. And what I've
186:22 - done is i created a version of this JASP analysis
and I've saved it here, in fact, let's open
186:31 - up my JASP analysis in JASP and I'll show
you what it looks like in osf. So, let's first
186:37 - go back to JASP. When we're here we can come
over to file and click computer and I just
186:45 - saved this file to the desktop. Click on desktop,
and you should have been able to download
186:54 - this with all the other files, DS03_2_4_JASP,
double click on that to open it and now it's
187:02 - going to open up a new window and you see
I was working with the same data set, but
187:07 - I did a lot more analyses. I've got these
graphs; I have correlations and scatter plots.
187:13 - Come down here, I did a linear regression.
And we just click on that and you can see
187:18 - the commands that produce it as well as the
options. I didn’t do anything special for
187:23 - that, but I did do some confidence intervals
and specified that and it's really a great
187:28 - way to work with all this. I'll click back
in an empty area and you see the commands
187:32 - go away and so I've got my output here in
JASP, but when I saved it though, I had the
187:39 - option of saving it to OSF, in fact if you
go to this webpage osf.io/3t2jg you'll actually
187:52 - be able to go to the page where you can see
and download the analyses that I conducted,
187:56 - let's take a look. This is that page, there's
the address I just barely gave you and what
188:02 - you see here is the same analysis that I conducted,
it's all right here, so if you're collaborating
188:08 - with people or if you want to show things
to people, this is a wonderful way to do it.
188:12 - Everything is right there, this is a static
image, but up at the top people have the option
188:18 - of downloading the original file and working
with it on their own. In case you can't tell,
188:24 - I'm really enthusiastic about JASP and about
its potential, still in beta, still growing
188:29 - rapidly. I see it really as an open source
free and collaborative replacement to SPSS
188:37 - and I think it is going to make data science
work so much easier for so many people. I
188:41 - strongly recommend you give JASP a close look.
Let's finish up our discussion of “Coding
188:48 - and Data Science” the applications part
of it by just briefly looking at some other
188:53 - software choices. And I'll have to admit it
gets kind of overwhelming because there are
188:58 - just so many choices. Now, in addition to
the spreadsheets, and Tableau, and SPSS, and
189:06 - JASP, that we have already talked about, there's
so much more than that. I'm going to give
189:10 - you a range of things that I'm aware of and
I'm sure I've left out some important ones
189:15 - or things that other people like really well,
but these are some common choices and some
189:20 - less common, but interesting ones. Number
one, in terms of ones that I haven't mentioned
189:24 - is SAS. SAS is an extremely common analytical
program, very powerful, used for a lot of
189:30 - things. It's actually the first program that
I learned and on the other hand it can be
189:36 - kind of hard to use and it can be expensive,
but there's a couple of interesting alternatives.
189:41 - SAS also has something called the SAS University
Edition, if you're a student this is free
189:47 - and it's slightly reduced in what it does,
but the fact that it's free. And also it runs
189:54 - in a virtual machine which makes it an enormous
download, but it's a good way to learn SAS
190:00 - if it's something that you want to do. SAS
also makes a program that I really love were
190:04 - it not so extraordinarily expensive and that
is called JMP and its visualization software.
190:12 - Think a little bit of Tableau, how we saw
it, you work with it visually and this one
190:16 - you can drag things around, it's really wonderful
program. I personally find it prohibitively
190:21 - expensive. Another very common choice among
working analysts is Stata and some people
190:28 - use Minitab. Now, for mathematical people,
there's MATLAB and then of course there's
190:34 - Mathematica itself, but it is really more
of a language than a program. On the other
190:38 - hand, Wolfram; who makes Mathematica, is also
the people who give us Wolfram Alpha, most
190:45 - people don't think of this a stats application
because you can run it on your iPhone. But,
190:49 - Wolfram Alpha is an incredibly capable and
especially if you pay for the pro account,
190:56 - you can do amazing things in this, including
analyses, regression models, visualizations
191:02 - and so it's worth taking a little closer look
at that. Also, because it provides a lot of
191:07 - the data that you need so Wolfram Alpha is
an interesting one. Now, several applications
191:13 - that are more specifically geared towards
data mining, so you don't want to do your
191:18 - regular, you know, little t tests and stuff
on these. But, there's RapidMiner and there's
191:23 - KNIME and Orange and those are all really
nice to use because they are control languages
191:31 - where you drag notes onto a screen and you
connect them with lines and you can see how
191:35 - things run through. All three of them are
free or have free versions and all three of
191:41 - them work in pretty similar manners. There's
also BigML, which is for machine learning
191:46 - and this is unusual because it's browser based,
it runs on their servers. There's a free version,
191:52 - though you can’t download a whole lot, it
doesn't cost a lot to use BigML and it's a
191:56 - very friendly, very accessible program. Then
in terms of programs you can actually install
192:02 - for free on your own computer, there's one
call SOFA Statistics, it means statistics
192:06 - open for all, it's kind of a cheesy title,
but it's a good program. And then one with
192:11 - a web page straight out of 1990 is Past 3,
this is paleontological software, on the other
192:19 - hand does do very general stuff, it runs on
many platforms and it’s a really powerful
192:25 - thing and it's free, but it is relatively
unknown. And then speaking of relatively unknown,
192:30 - one that's near and dear to my heart is a
web application called Statcrunch, it costs,
192:35 - but it costs like $6 or $12 a year, it's really
cheap and it's very good, especially if for
192:42 - basic statistics and for learning, I used
in some of the classes that I was teaching.
192:46 - And then if you're deeply wedded to Excel
and you can't stand to leave that environment,
192:51 - you can purchase add-ons like XLSTAT, which
give you a lot of statistical functions within
192:58 - the Excel environment itself. That's a lot
of choices and the most important thing here
193:04 - is don't get overwhelmed. There's a lot of
choices, but you don't even have to try all
193:08 - of them. Really the important question is
what works best for you and the project that
193:14 - you're working on? Here's a few things you
want to consider in that regard. First off
193:19 - is functionality, does it actually do what
you want or does it even run on your machine?
193:23 - You don't need everything that a program can
do. When you think about the stuff Excel can
193:28 - do, people probably use five percent of what's
available. Second is ease of use. Some of
193:34 - these programs are a lot easier to use than
the others and I personally find that the
193:40 - ones that are easier to use, I like them,
so you might say, “No, I need to program
193:44 - because I need custom stuff”. But I'm willing
to bet that 95% of what people do does not
193:48 - require anything custom. Also, the existence
of a community. Constantly when you're working
193:56 - you come across problems and don't know how
to solve it and being able to get online and
194:01 - do a search for an answer and have enough
of a community that there are people there
194:05 - who have put answers up and discuss these
things. Those are wonderful. Some of these
194:09 - programs are very substantial communities
and some of them it is practically nonexistent
194:13 - and it is to you to decide how important it
is to you. And then finally of course there
194:16 - is the issue of cost. Many of these programs
I mentioned are free, some of them are very
194:22 - cheap, some of them run some sort of premium
model and some of them are extremely expensive.
194:26 - So, you don't buy them unless somebody else
is paying for it. So, these are some of the
194:30 - things that you want to keep in mind when
you're trying to look at various programs.
194:34 - Also, let's mention this; don't forget the
80/20 rule. You're going to be able to do
194:40 - most of the stuff that you need to do with
only a small number of tools, one or two,
194:45 - maybe three, will probably be all that you
ever need. So, you don't need to explore the
194:50 - range of every possible tool. Find something
that you need, find something you're comfortable
194:56 - with and really try to extract as much value
as you can out of that. So, in sum, in our
195:02 - discussion of available applications for coding
and data science. First remember applications
195:08 - are tools, they don't drive you, you use them.
And that your goals are what drive the choice
195:14 - of your applications and the way that you
do it. And the single most important thing
195:19 - is to remember, what works for you, may work
well for somebody else, if you're not comfortable
195:24 - with it, if it's not the questions you address,
then it's more important to think about what
195:30 - works for you and the projects that you're
working on as you make your own choices for
195:35 - tools, for working in data science. When you're
“Coding in Data Science,” one of the most
195:41 - important things you can do is be able to
work with web data. And if you work with web
195:45 - data you're going to be working with HTML.
And in case you're not familiar with it, HTML
195:51 - is what makes the World Wide Web go ‘round.
What it stands for is HyperText Markup Language
195:58 - - and if you've never dealt with web pages
before, here's a little secret: web pages
196:05 - are just text. It is just a text document,
but it uses tags to define the structure of
196:13 - the document and a web browser knows what
those tags are and it displays them the right
196:18 - way. So, for instance, some of the tags, they
look like this. They are in angle brackets,
196:24 - and you have an angle bracket and then the
beginning tag, so body, and then you have
196:28 - the body, the main part of your text, and
then you have in angle brackets with backslash
196:33 - body to let the computer know that you are
done with that part. You also have p and backslash
196:39 - p for paragraphs. H1 is for header one and
you put it in between that text. TD is for
196:48 - table data or the cell in a table and you
mark it off that way. If you want to see what
196:53 - it looks like just go to this document: DS03_3_1_HTML.txt.
I'm going to go to that one right now. Now,
197:03 - depending on what text editor you open this
up, it may actually give you the web preview.
197:08 - I've opened it up in TextMate and so it actually
is showing the text the way I typed it. I
197:15 - typed this manually; I just typed it all in
there. And I have HTML to see what a document
197:21 - is, I have an empty header, but that sort
of needs to be there. This, I say what the
197:25 - body is, and then I have some text. li is
for list items, I have headers, this is for
197:31 - a link to a webpage, then I have a small table.
And if you want to see what this looks like
197:38 - when displayed as a web page, just go up here
to window and show web preview. This is the
197:45 - same document, but now it is in a browser
and that's how you make a web page. Now, I
197:51 - know this is very fundamental stuff, but the
reason this is important is because if you're
197:55 - going to be extracting data from the web,
you have to understand how that information
198:00 - is encoded in the web, and it is going to
be in HTML most of the time for a regular
198:05 - web page. Now, I will mention something that,
there’s another thing called CSS. Web pages
198:11 - use CSS to define the appearance of a document.
HTML is theoretically there to give the content
198:18 - and CSS gives the appearance. And that stands
for Cascading Style Sheets. I'm not going
198:22 - to worry about that right now because we're
really interested in the content. And now
198:27 - you have the key to being able to read web
pages and pull data from web pages for your
198:34 - data science project. So, in sum; first, the
web runs on HTML and that's what makes the
198:40 - web pages that are there. HTML defines the
page structure and the content that is on
198:46 - the page. And you need to learn how to navigate
the tags and the structure in order to get
198:51 - data from the web pages for your data science
projects. The next step in “Coding and Data
198:58 - Science” when you’re working with web
data is to understand a little bit about XML.
199:03 - I like to think of this as the part of web
data that follows the imperative, “Data,
199:09 - define thyself”. XML stands for eXtensible
Markup Language, and what it is XML is semi-structured
199:18 - data. What that means is that tags define
data so a computer knows what a particular
199:22 - piece of information is. But, unlike HTML,
the tags are free to be defined any way you
199:29 - want. And so you have this enormous flexibility
in there, but you’re still able to specify
199:34 - it so the computer can read it. Now, there’s
a couple of places where you’re going to
199:38 - see XML files. Number one is in web data.
HTML defines the structure of a web page,
199:44 - but if they’re feeding data into it, then
that will often come in the form of an XML
199:49 - file. Interestingly, Microsoft Office files,
if you have .docx or .xlsx, the X-part at
199:57 - the end stands for a version of XML that’s
used to create these documents. If you use
200:03 - iTunes, the library information that has all
of your artists, and your genre’s, and your
200:08 - ratings and stuff, that’s all stored in
an XML file. And then finally, data files
200:14 - that often go with particular programs can
be saved as XML as a way of representing the
200:20 - structure of the data to the program. And
for XML, tags use opening and closing angle
200:27 - brackets just like HTML did. Again, the major
difference is that you’re free to define
200:32 - the tags however you want. So for instance,
thinking about iTunes, you can define a tag
200:38 - that's genre, and you have the angle brackets
in genre to begin that information, and then
200:43 - you have the angle brackets with the backslash
to let it know you're done with that piece
200:47 - of information. Or, you can do it for composer,
or you can do it for rating, or you can do
200:52 - it for comments, and you can create any tags
you want and you put the information in between
200:57 - those two things. Now, let's take an example
of how this works. I’m going to show you
201:03 - a quick dataset that comes from the web. It's
at ergast.com and API, and this is a website
201:11 - that stores information about automobile Formula
One racing. Let's go to this webpage and take
201:17 - a quick look at what it’s like. So, here
we are at Ergast.com, and it’s the API for
201:23 - Formula One. And what I’m bringing up is
the results of the 1957 season in Formula
201:28 - One racing. And here you can see who the competitors
were in each race, and how they finished and
201:33 - so on. So, this is a dataset that is being
displayed in a web page. If you want to see
201:39 - what it looks like in XML, all you have to
do is type XML onto the end of this: .XML.
201:45 - I’ve done that already, so I’m just going
to go to that one. And as you see, it's only
201:49 - this bit that I’ve added: .XML. Now, it
looks exactly the same because the web page
201:53 - is structuring XML data by default but if
you want to see what it looks like in its
201:57 - raw format, just do an option, click on the
web page, and go to view page source. At least
202:04 - that’s how it works in Chrome, and this
is the structured XML page. And you can see
202:11 - we have tags here. It says Race Name, Circuit
Name, Location, and obviously, these are not
202:18 - standard HTML tags. They are defined for the
purposes of this particular dataset. But we
202:23 - begin with one. We have Circuit Name right
there, and then we close it using the backslash
202:29 - right there. And so this is structured data;
the computer knows how to read it, which is
202:35 - exactly, this is how it displays it by default.
So, it's a really good way of displaying data
202:40 - and its a good way to know how to pull data
from the web. You can actually use what is
202:45 - called an API, an Application Programming
interface to access this XML data and it pulls
202:51 - it in along with its structure which makes
working with it really easy. What’s even
202:56 - more interesting is how easy it is to take
XML data and convert it between different
203:02 - formats, because it’s structured and the
computer knows what you’re dealing with.
203:05 - So for example, one it’s really easy to
convert XML to CSV or comma separated value
203:12 - files (that’s the spreadsheet format) because
it knows exactly what the headings are; what
203:16 - piece of information goes in each column.
Example two: it’s really easy to convert
203:22 - HTML documents to XML because you can think
of HTML with its restricted set of tags as
203:30 - sort of a subset of the much freer XML. And
three, you can convert CSV, or your spreadsheet
203:37 - comma separated value, to XML and vice versa.
You can bounce them all back and forth because
203:41 - the structure is made clear to the programs
you’re working with. So in sum, here’s
203:48 - what we can say. Number one, XML is semi-structured
data. What that means is that it has tags
203:54 - to tell the computer what the piece of information
is, but you can make the tags whatever you
203:59 - want them to be. And, XML is very common for
web data and it’s really easy to translate
204:07 - the format XML/HTML/CSV so on and so forth.
It’s really easy to translate them back
204:13 - and forth which gives you a lot of flexibility
in manipulating data so can get into the format
204:18 - you need for your own analysis. The last thing
I want to mention about “Coding and Data
204:26 - Science” and web data is something called
JSON. And I like to think of it as a version
204:32 - of smaller is better. Now, what JSON stands
for is JavaScript Object Notation, although
204:40 - JavaScript is supposed to be one word. And
what it is, is that like XML, JSON is semi-structured
204:47 - data. That is, you have tags that define the
data, so the computer knows what each piece
204:52 - of information is, but like XML the tags can
vary freely. And so there’s a lot in common
204:58 - between XML and JSON. So XML is a Markup Language
(that’s what the ML stands for), and that
205:06 - gives meaning to the text; it lets the computer
know what each piece of information is. Also,
205:11 - XML allows you to make comments in the document,
and it allows you to put metadata in the tags
205:16 - so you can actually put information there
in the angle brackets to provide additional
205:20 - context. JSON, on the other hand, is specifically
designed for data interchange and so it’s
205:26 - got that special focus. And the structure;
JSON corresponds with data structures, you
205:32 - know it directly represents objects and arrays
and numbers and strings and booleans, and
205:37 - that works really well with the programs that
are used to analyze data. Also, JSON is typically
205:43 - shorter than XML because it does not require
the closing tags. Now, there are ways to do
205:49 - that with XML, but that's not typically how
it’s done. As a result of these differences,
205:55 - JSON is basically taking XML’s place in
web data. XML still exists, it’s still used
206:02 - for a lot of things, but JSON is slowly replacing
it. And we’ll take a look at the comparison
206:07 - between the three by going back to the example
we used in XML. This is data about Formula
206:13 - One car races in 1957 from ergast.com. You
can just go to the first web page here, then
206:20 - we will navigate to the others from that.
So this is the general page. This is if you
206:26 - just type in without the .XML or .JSON or
anything. So it’s a table of information
206:32 - about races in 1957. And we saw earlier that
if you add just add .XML to the end of this,
206:39 - it looks exactly the same. That’s because
this browser is displaying XML properly by
206:44 - default. But, if you were to right click on
it, and go to view page source, you would
206:51 - get this instead, and you can see the structure.
This is still XML, and so everything has an
206:56 - opening tag and a closing tag and some extra
information in there. But, if you type in
207:04 - .JSON what you really get is this jumbled
mess. Now that’s unfortunate because there
207:11 - is a lot of structure to this. So, what I
am going to do is, I am actually going to
207:15 - copy all of this data, then I’m going to
go to a little web page; there’s a lot of
207:20 - things you can do here, and it’s a cute
phrase. It's called JSON Pretty Print. And
207:25 - that is, make it look structured so it’s
easier to read. I just paste that in there
207:30 - and hit Pretty Print JSON, and now you can
see hierarchical structure of the data. The
207:38 - interesting thing is that the JSON tags only
have tags at the beginning. It says series
207:45 - in quotes, then a colon, then it gives the
piece of information in quotes, and a comma
207:50 - and it moves on to the next one. And this
is a lot more similar to the way data would
207:55 - be represented in something like R or Python.
It is also more compact. Again, there are
208:03 - things you can do with XML but this is one
of the reasons that JSON is becoming preferred
208:08 - as a data carrier for websites. And as you
may have guessed, it’s really easy to convert
208:14 - between the formats. It’s easy to convert
between XML, JSON, CSV, etc. You can get a
208:23 - web page where you can paste a version in
and you get the other version out. There are
208:27 - some differences, but for the vast majority
of situations, they are just interchangeable.
208:33 - In Sum: what did we get from this? Like XML,
JSON is semi-structured data, where there
208:39 - are tags that say what the information is,
but you define the tags however you want.
208:43 - JSON is specifically designed for data interchange
and because it reflects the structure of the
208:50 - data in the programs, that makes it really
easy. Also, because it’s relatively compact
208:56 - JSON is replacing gradually XML on the web,
as the container for data on web pages. If
209:04 - we are going to talk about “Coding and Data
Science” and the languages that are used,
209:09 - then first and foremost is R. The reason for
that is, according to many standards, R is
209:16 - the language of data and data science. For
example, take a look at this chart. This is
209:21 - a ranking based on a survey of data mining
experts of the software they use in doing
209:27 - their work, and R is right there at the top.
R is first, and in fact that’s important
209:33 - because there’s Python which is usually
taken hand in hand with R for Data Science.
209:40 - But R sees 50% more use than Python does,
at least in this particular list. Now there’s
209:45 - a few reasons for that popularity. Number
one, R is free and it’s open source, both
209:51 - of which make things very easy. Second, R
is specially developed for vector operations.
209:56 - That means it’s able to go through an entire
list of data without having to write ‘for’
210:00 - loops to go through. If you’ve ever had
to write ‘for’ loops, you know that would
210:03 - be kind of disastrous having to do that with
data analysis. Next, R has a fabulous community
210:09 - behind it. It’s very easy to get help on
things with R, you Google it, you're going
210:13 - to end up in a place where you're going to
be able to find good examples of what you
210:16 - need. And probably most importantly, R is
very capable. R has 7,000 packages that add
210:27 - capabilities to R. Essentially, it can do
anything. Now, when you are working with R,
210:33 - you actually have a choice of interfaces.
That is, how you actually do the coding and
210:37 - how you get your results. R comes with it’s
own IDE or Interactive Development Environment.
210:43 - You can do that, or if you are on a Mac or
a Linux you can actually do R through the
210:48 - Terminal through the command line. If you’ve
installed R, you just type R and it starts
210:52 - up. There is also a very popular development
environment called RStudio.com, and that’s
210:57 - actually the one I use and the one I will
be using for all my examples. But another
211:01 - new competitor is Jupyter, which is very commonly
used for Python; that’s what I use for examples
211:07 - there. It works in a browser window, even
though its locally installed. And RStudio
211:11 - and Jupyter there's pluses and minus to each
one of them and I’ll mention them as we
211:15 - get to each one of them. But no matter which
interface you use, R’s command line, you’re
211:19 - typing lines of code in order to get the commands.
Some people get really scared about that but
211:25 - really there are some advantages to that in
terms of the replicability and really the
211:30 - accessibility, the transparency of your commands.
So for instance, here’s a short example
211:37 - of some of the commands in R. You can enter
them into what is called a console, and that’s
211:41 - just one line at a time and that’s called
an interactive way. Or you can save scripts
211:46 - and run bits and pieces selectively and that
makes your life a lot easier. No matter how
211:51 - you do it, if you are familiar with programming
other languages then you’re going to find
211:55 - that R’s a little weird. It has an idiosyncratic
model. It makes sense once you get used to
212:01 - it, but it is a different approach, and so
it takes some adaptation if you are accustomed
212:06 - to programming in different languages. Now,
once you do your programming to get your output,
212:11 - what you’re going to get is graphs in a
separate window. You’re going to get text
212:16 - and numbers, numerical output in the console,
and no matter what you get, you can save the
212:21 - output to files. So that makes it portable,
you can do it in other environments. But most
212:27 - importantly, I like to think of this: here’s
our box of chocolates where you never know
212:32 - what you’re going to get. The beauty of
R is in the packages that are available to
212:36 - expand its capabilities. Now there are two
sources of packages for R. One goes by the
212:43 - name of CRAN, and that stands for the Comprehensive
R Archive Network, and that’s at cran.rstudio.com.
212:51 - And what that does is takes the 7,000 different
packages that are available and organizes
212:57 - them into topics that they call task views.
And for each one if they have done their homework,
213:02 - they have datasets that come along with the
package. You have a manual in .pdf format,
213:08 - and you can even have vignettes where they
run through examples of how to do it. Another
213:13 - interface is called Crantastic! And the exclamation
point is part of the title. And that is at
213:18 - crantastic.org. And what this is, is an alternative
interface that links to CRAN. So if you find
213:25 - something you like in Crantastic! and you
click on the link, it’s going to open in
213:28 - CRAN. But the nice thing about Crantastic!
is it shows the popularity of packages, and
213:34 - it also shows how recently they were updated,
and that can be a nice way of knowing you’re
213:39 - getting sort of the latest and greatest. Now
from this very abstract presentation, we can
213:44 - say a few things about R: Number one, according
to many, R is the language of data science
213:51 - and it’s a command line interface. You’re
typing lines of code, so that gives it both
213:56 - a strength and a challenge for some people.
But the beautiful thing is that for the thousands
214:01 - and thousands of packages of additional code
and capability that are available for R, that
214:06 - make it possible to do nearly anything in
this statistical programming language. When,
214:13 - talking about “Coding and Data Science”
and the languages, along with R, we need to
214:17 - talk about Python. Now, Python the snakes
is a general-purpose program that can do it
214:25 - all, and that’s its beauty. If we go back
to the survey of the software used by data
214:32 - mining experts, you see that Python’s there
and it’s number three on the list. What’s
214:37 - significant about that, is that on this list,
Python is the only general purpose programming
214:43 - language. It’s the only one that can be
theoretically used to develop any kind of
214:46 - application that you want. That gives it some
special powers compared to all the others,
214:52 - most of which are very specific to data science
work. The nice things about Python are: number
214:59 - one, it’s general purpose. It’s also really
easy to use, and if you have a Macintosh or
215:05 - Linux computer, Python is built into it. Also,
Python has a fabulous community around it
215:12 - with hundreds of thousands of people involved,
and also python has thousands of packages.
215:18 - Now, it actually has 70 or 80,000 packages,
but in terms of ones that are for data, there
215:24 - are still thousands available that give it
some incredible capabilities. A couple of
215:30 - things to know about Python. First, is about
versions. There are two versions of Python
215:35 - that are in wide circulation: there’s 2.x;
so that means like 2.5 or 2.6, and 3.x; so
215:43 - 3.1, 3.2. Version 2 and version 3 are similar,
but they are not identical. In fact, the problem
215:51 - is this: there are some compatibility issues
where code that runs in one does not run in
215:57 - the other. And consequently, most people have
to choose between one and the other. And what
216:04 - this leads to is that many people still use
2.x. I have to admit, in the examples that
216:09 - I use, I’m using 2.x because so many of
the data science packages that are developed
216:15 - with that in mind. Now let me say a few things
about the interfaces for Python. First, Python
216:22 - does come with its own Interactive Development
Learning Environment and they call it IDLE.
216:27 - You can also run it from the Terminal, or
command line interface, or any IDE that you
216:31 - have. A very common and a very good choice
is Jupyter. Jupyter is a browser-based framework
216:40 - for programming and it was originally called
IPython. That served as its initial, so a
216:46 - lot of the time when people are talking about
IPython, what they are really talking about
216:49 - is this Python in Jupyter and the two are
sometimes used interchangeably. One of the
216:55 - neat things you can do, there are two companies:
Continuum and Enthought. Both of which have
217:00 - made special distributions of Python with
hundreds and hundreds of packages preconfigured
217:07 - to make it very easy to work with data. I
personally prefer Continuum Anaconda, it’s
217:13 - the one that I use, a lot of other people
use it, but either one is going to work and
217:17 - it’s going to get you up and running. And
like I said with R, no matter what interface
217:21 - you use, all of them are command line. You’re
typing lines of code. Again, there is tremendous
217:26 - strength to that but, it can be intimidating
to some people at first. In terms of the actual
217:32 - commands of Python, we have some examples
here on the side, and the important thing
217:36 - to remember is that it’s a text interface.
On the other hand, Python is familiar to millions
217:42 - of people because it is very often a first
programming language people learn to do general
217:47 - purpose programming. And there are a lot of
very simple adaptations for data that make
217:53 - it very powerful for data science work. So,
let me say something else again: data science
217:59 - loves Jupyter, and Jupyter is the browser-based
framework. It’s a local installation, but
218:06 - you access it through a web browser that makes
it possible to really do some excellent work
218:12 - in data science. There’s a few reasons for
this. When you’re working in Jupyter you
218:16 - get text output and you can use what’s called
Markdown as a way of formatting documents.
218:22 - You can get inline graphics for the graphics
to show up directly beneath the code that
218:25 - you did it. It’s also really easy to organize,
present, and to share analyses that are done
218:32 - in Jupyter. Which makes it a strong contender
for your choices in how you do data science
218:39 - programming. Another one of the beautiful
things about Python, like R, is there are
218:43 - thousands of packages available. In Python,
there is one main repository; it goes by the
218:49 - name PyPI. Which is for the Python Package
Index. Right here it says there are over 80,000
218:55 - packages and 7 or 8,000 of those are for data-specific
purposes. Some of the packages that you will
219:02 - get to be very familiar with are NumPy and
SciPy, which are for scientific computing
219:07 - in general; Matplotlib and a development of
it called Seaborn are for data visualization
219:16 - and graphics. Pandas is the main package for
the doing statistical analysis. And for machine
219:23 - learning, almost nothing beats scikit-learn.
And when I go through hands-on examples in
219:30 - Python, I will be using all of these as a
way of demonstrating the power of the program
219:35 - for working with data. In sum we can say a
few things: Python is a very popular program
219:42 - very familiar to millions of people and that
makes it a good choice. Second, of all the
219:47 - languages we use for data science on a frequent
basis, this is the only one that’s general
219:52 - purpose. Which means it can be used for a
lot of things other than processing data.
219:57 - And it gets its power, like R does, from having
thousands of contributed packages which greatly
220:03 - expand its capabilities especially in terms
of doing data science work. A choice for “Coding
220:10 - in Data Science,” one of the languages that
may not come immediately to mind when they
220:13 - think data science, is Sequel or SQL. SQL
is the language of databases and we think,
220:21 - “why do we want to work in SQL?” Well,
to paraphrase the famous bank robber Willie
220:26 - Sudden who apparently explained why he robbed
banks and said: “Because that’s where
220:30 - the money is.” The reason we would with
SQL in data science is because that’s where
220:35 - the data is. Let’s take another look at
our ranking of software among data mining
220:41 - professionals, and there’s SQL. Third on
the list, and also of this list, its also
220:47 - the first database tool. Other tools, for
instance, get much fancier, and much new and
220:53 - shinier, but SQL has been around for a while
as very very capable. There’s a few things
220:58 - to know about SQL. You will notice that I
am saying Sequel even though it stands for
221:05 - Structured Query Language. SQL is a language,
not an application. There’s not a program
221:12 - SQL, it’s a language that can be used in
different applications. Primarily, SQL is
221:17 - designed for what are called relational databases.
And those are special ways of storing structured
221:24 - data that you can pull in. You can put things
together, you can join them in special ways,
221:28 - you can get summary statistics, and then what
you usually do is then export that data into
221:35 - your analytical application of choice. The
big word here is RDBMS - Relational Database
221:46 - Management System; that is where you will
usually see SQL as a query language being
221:51 - used. In terms of Relational Database Management
System, there are a few very common choices.
221:59 - In the industrial world where people have
some money to spend, there’s Oracle database
222:02 - is a very common one and Microsoft SQL Server.
In the open source world, two very common
222:10 - choices are MySQL, even though we generally
say Sequel, when it’s here you generally
222:16 - say MySQL. Another one is PostgreSQL. These
are both open source, free versions of the
222:24 - language; sort of dialects of each, that make
it possible for you to working with your databases
222:29 - and for you to get your information out. The
neat thing about them, no matter what you
222:32 - do, databases minimize data redundancy by
using connected tables. Each table has rows
222:39 - and columns and they store different levels
or different of abstraction or measurement,
222:44 - which means you only have to put the information
one place and then it can refer to lots of
222:48 - other tables. Makes it very easy to keep things
organized and up to date. When you are looking
222:54 - into a way of working with a Relational Database
Management System, you get to choose in part
222:59 - between using a graphical user interface or
GUI. Some of those include SQL Developer and
223:05 - SQL Server Management Studio, two very common
choices. And there are a lot of other choices
223:11 - such as Toad and some other choices that are
graphical interfaces for working with these
223:16 - databases. There are also text-based interfaces.
So really, any command line interface, and
223:24 - any interactive development environment or
programming tool is going to be able to do
223:28 - that. Now, you can think of yourself on the
command deck of your ship and think of a few
223:34 - basic commands that are very important for
working with SQL. There are just a handful
223:40 - of commands that can get you where you need
to go. There is the Select command, where
223:44 - you’re choosing the cases that you want
to include. From: says what tables are you
223:51 - going to be extracting them from. Where: is
a way of specifying conditions, and then Order
223:58 - By: obviously is just a way of putting it
all together. This works because usually when
224:03 - you are in a SQL database you’re just pulling
out the information. You want to select it,
224:08 - you want to organize it, and then what you
are going to do is you are going to send the
224:13 - data to your program of choice for further
analysis, like R or Python or whatever. In
224:19 - sum here’s what we can say about SQL: Number
one, as a language it’s generally associated
224:25 - with relational databases, which are very
efficient and well-structured ways of storing
224:31 - data. Just a handful of basic commands can
be very useful when working with databases.
224:37 - You don’t have have to be a super ninja
expert, really a handful. Five, 10 commands
224:42 - will probably get you everything you need
out of a SQL database. Then once the data
224:46 - is organized, the data is typically exported
to some other program for analysis. When you
224:53 - talk about coding in any field, one of the
languages or one of the groups of languages
224:59 - that come up most often are C, C++, and Java.
These are extremely powerful applications
225:06 - and very frequently used for professional,
production level coding. In data science,
225:12 - the place where you will see these languages
most often is in the bedrock. The absolute
225:18 - fundamental layer that makes the rest of data
science possible. For instance, C and C++.
225:26 - C is from the ‘60s, C++ is from the ‘80s,
and they have extraordinary wide usage, and
225:31 - their major advantage is that they're really
really fast. In fact, C is usually used as
225:37 - the benchmark for how fast is a language.
They are also very, very stable, which makes
225:42 - them really well suited to production-level
code and, for instance, server use. What’s
225:48 - really neat is that in certain situations,
if time is really important, if speeds important,
225:53 - then you can actually use C code in R or other
statistical languages. Next is Java. Java
226:01 - is based on C++, it’s major contribution
was the WORA or the Write Once Run Anywhere.
226:08 - The idea that you were going to be able to
develop code that is portable to different
226:12 - machines and different environments. Because
of that, Java is the most popular computer
226:19 - programming language overall against all tech
situations. The place you would use these
226:26 - in data science, like I said, when time is
of the essence, when something has to be fast,
226:32 - it has to get the job accomplished quickly,
and it has to not break. Then these are the
226:36 - ones you’re probably going to use. The people
who are going to use it are primarily going
226:40 - to be engineers. The engineers and the software
developers who deal with the inner workings
226:48 - of the algorithms in data science or the back
end of data science. The servers and the mainframes
226:54 - and the entire structure that makes analysis
possible. In terms of analysts, people who
227:02 - are actually analyzing the data, typically
don’t do hands-on work with the foundational
227:06 - elements. They don’t usually touch C or
C++, more of the work is on the front end
227:11 - or closer to the high-level languages like
R or Python. In sum: C, C++ and Java form
227:21 - a foundational bedrock in the back end of
data and data science. They do this because
227:26 - they are very fast and they are very reliable.
On the other hand, given their nature that
227:33 - work is typically reserved for the engineers
who are working with the equipment that runs
227:38 - in the back that makes the rest of the analysis
possible. I want to finish our extremely brief
227:45 - discussion of “Coding in Data Sciences”
and the languages that can be used, by mentioning
227:50 - one other that's called Bash. Bash really
is a great example of old tools that have
227:58 - survived and are still being used actively
and productively with new data. You can think
228:04 - of it this way, it’s almost like typing
on your typewriter. You’re working at the
228:08 - command line, you’re typing out code through
a command line interface or a CLI. This method
228:14 - of interacting with computers practically
goes back to the typewriter phase, because
228:18 - it predates monitors. So, before you even
had a monitor, you would type out the code
228:23 - and it would print it out on a piece of paper.
The important thing to know about the command
228:27 - line is it’s simply a method of interacting.
It’s not a language, because lots of languages
228:33 - can run at the command line. For instance,
it is important to talk about the concept
228:38 - of a shell. In computer science, a shell is
a language or something that wraps around
228:44 - the computer. It’s a shell around the language,
that is the interaction level for the user
228:49 - to get things done at the lower level that
aren’t really human-friendly. On Mac computers
228:56 - and Linux, the most common is Bash, which
is short for Bourne Again Shell. On Windows
229:08 - computers, the most common is PowerShell.
But whatever you do there actually are a lot
229:13 - of choices, there’s the Bourne Shell, the
C shell; which is why I have a seashell right
229:17 - here, the Z shell, there’s fish for Friendly
Interactive Shell, and a whole bunch of other
229:22 - choices. Bash is the most common on Mac and
Linux and PowerShell is the most common on
229:28 - Windows as a method of interacting with the
computer at the command line level. There’s
229:34 - a few things you need to know about this.
You have a prompt of some kind, in Bash, it’s
229:38 - a dollar sign, and that just means type your
command here. Then, the other thing is you
229:45 - type one line at a time. It's actually amazing
how much you can get done with a one-liner
229:51 - program, by sort of piping things together,
so one feeds into the other. You can run more
229:56 - complex commands if you use a script. So,
you call a text document that has a bunch
230:03 - of things in it and you can get much more
elaborate analyses done. Now, we have our
230:11 - tools here. In Bash we talk about utilities
and what these are, are specific programs
230:17 - that accomplish specific tools. Bash really
thrives on "Do one thing, and do it very well."
230:24 - There are two general categories of utilities
for Bash. Number one, is the Built-ins. These
230:29 - are the ones that come installed with it,
and so you're able to use it anytime by simply
230:34 - calling in their name. Some more common ones
are: cat, which is for catenate; that's to
230:40 - put information together. There's awk, which
is it's own interpreted language, but it's
230:47 - often used for text processing from the command
line. By the way, the name 'Awk' comes from
230:52 - the initials of the people who created it.
Then there's grep, which is for Global search
230:59 - with a Regular Expression and Print. It's
a way of searching for information. And then
231:05 - there's sed, which stands for Stream Editor
and its main use is to transform text. You
231:11 - can do an enormous amount with just these
4 utilities. A few more are head & tail, display
231:18 - the first or last 10 lines of a document.
Sort & uniq, which sort and count the number
231:25 - of unique answers in a document. Wc, which
is for word count, and printf which formats
231:32 - the output that you get in your console. And
while you can get a huge amount of work done
231:38 - with just this small number of built-in utilities,
there are also a wide range of installable.
231:44 - Or, other command line utilities that you
can add to Bash, or whatever programming language
231:51 - you're using. So, since some really good ones
that have been recently developed are jq:
231:56 - which is for pulling in JSON or JavaScript,
object notation data from the web. And then
232:02 - there's json2csv, which is a way of converting
JSON to csv format, which is what a lot of
232:09 - statistical programs are going to be happy
with. There's Rio which allows you to run
232:13 - a wide range of commands from the statistical
programming language R in the command line
232:18 - as part of Bash. And then there's BigMLer.
This is a command line tool that allows you
232:24 - to access BigML's machine learning servers
through the command line. Normally, you do
232:31 - it through a web browser and it accesses their
servers remote. It's an amazingly useful program
232:36 - but to be able to just pull it up when you're
in the command line is an enormous benefit.
232:41 - What's interesting is that even though you
have all these opportunities, all these different
232:45 - utilities, you can do all amazing things.
And there's still an active element of utilities
232:51 - for the command line. So, in sum: despite
being in one sense as old as the dinosaurs,
232:58 - the command line survives because it is extremely
well evolved and well suited to its purpose
233:05 - of working with data. The utilities; both
the built-in and the installable are fast
233:10 - and they are easy. In general, they do one
thing and they do it very, very well. And
233:15 - then surprisingly, there is an enormous amount
of very active development of command line
233:21 - utilities for these purposes, especially with
data science. One critical task when you are
233:29 - Coding in Data Science is to be able to find
the things that you are looking for, and Regex
233:34 - (which is short of Regular Expressions) is
a wonderful way to do that. You can think
233:39 - of it as the supercharged method for finding
needles in haystacks. Now, Regex tends to
233:45 - look a little cryptic so, for instance, here’s
an example. As something that’s designed
233:50 - to determine if something is a valid email
address, and it specifies what can go in the
233:55 - beginning, you have the at sign in the middle,
then you’ve got a certain number of letters
234:00 - and numbers, then you have to have a dot something
at the end. And so, this is a special kind
234:04 - of code for indicating what can go where.
Now regular expressions, or regex, are really
234:11 - a form of pattern matching in text. And it’s
a way of specifying what needs to be where,
234:17 - what can vary, and how much it can vary. And
you can write both specific patterns; say
234:22 - I only want a one letter variation here, or
a very general like the email validator that
234:27 - I showed you. And the idea here is that you
can write this search pattern, your little
234:32 - wild card thing, you can find the data and
then once you identify those cases, then you
234:37 - export them into another program for analysis.
So here’s a short example of how it can
234:42 - work. What I’ve done is taken some text
documents, they’re actually the texts to
234:47 - Emma and to Pygmalion, two books I got off
of Project Gutenberg, and this is the command.
234:54 - Grep ^l.ve *.txt - so what I’m looking for
in either of these books are lines that start
235:03 - with ‘l’, then they can have one character;
can be whatever, then that’s followed by
235:08 - ‘ve’, and then the .txt means search for
all the text files in the particular folder.
235:16 - And what it found were lines that began with
love, and lived, and lovely, and so on. Now
235:24 - in terms of the actual nuts and bolts of regular
expressions, there are some certain elements.
235:29 - There are literals, and those are things that
are exactly what they mean. You type the letter
235:33 - ‘l’, you’re looking for the letter ‘l’.
There are also metacharacters, which specify,
235:38 - for instance, things need to go here; they’re
characters but are really code that give representations.
235:45 - Now, there are also escape sequences, which
is normally this character is used as a variable,
235:52 - but I want to really look for a period as
opposed to a placeholder. Then you have the
235:58 - entire search expression that you create and
you have the target string, the thing that
236:02 - it is searching through. So let me give you
a few very short examples. ^ this is the caret.
236:08 - This is the sometimes called a hat or in French,
a circonflexe. What that means, you’re looking
236:14 - for something at the beginning of the search
you are searching. For example, you can have
236:19 - ^ and capital M, that means you need something
that begins with capital M. For instance the
236:26 - word “Mac,” true, it will find that. But
if you have iMac, it’s a capital M, but
236:31 - it’s not the first letter and so that would
be false, it won’t find that. The $ means
236:36 - you are looking for something at the end of
the string. So for example: ing$ that will
236:43 - find the word ‘fling’ because it ends
in ‘ing’, but it won’t find the word
236:48 - ‘flings’ because it actually ends with
an ‘s’. And then the dot, the period,
236:54 - simply means that we are looking for one letter
and it can be anything. So, for example, you
236:58 - can write ‘at.’. And that will find ‘data’
because it has an ‘a’, a ‘t’, and
237:05 - then one letter after it. But it won’t find
‘flat’, because ‘flat’ doesn’t have
237:10 - anything after the ‘at’. And so these
are extremely simple examples of how it can
237:15 - work. Obviously, it gets more complicated
and the real power comes when you start combining
237:19 - these bits and elements. Now, one interesting
thing about this is you can actually treat
237:24 - this as a game. I love this website, it’s
called Regex golf and it’s at regex.alf.nu.
237:31 - And what it does is brings up lists of words;
two columns, and your job is to write a regular
237:38 - expression in the top, that matches all the
words on the left column and none of the words
237:44 - on the right. And uses the fewest characters
possible, and you get a score! And it’s
237:49 - a great way of learning how to do regular
expressions and learning how to search in
237:55 - a way that is going to get you the data you
need for your projects. So, in sum: Regex,
238:01 - or regular expressions, help you find the
right data for your project, they’re very
238:06 - powerful and they’re very flexible. Now,
on the other hand, they are cryptic, at least
238:10 - when you first look at them but at the same
time, it’s like a puzzle and it can be a
238:14 - lot of fun if you practice it and you see
how you can find what you need. I want to
238:21 - thank you for joining me in “Coding in Data
Science” and we’ll wrap up this course
238:27 - by talking about some of the specific next
steps you can take for working in data science.
238:32 - The idea here, is that you want to get some
tools and you want to start working with those
238:37 - tools. Now, please keep in mind something
that I’ve said at another time. Data tools
238:42 - and data science are related, they’re important
but don’t make the mistake of thinking that
238:47 - if you know the tools that you have done the
same thing as actually conducted data science.
238:52 - That’s not true, people sometimes get a
little enthusiastic and they get a little
238:56 - carried away. What you need to remember is
the relationship really is this: Data Tools
239:01 - are an important part of data science, but
data science itself is much bigger than just
239:06 - the tools. Now, speaking of tools remember
there’s a few kinds that you can use, and
239:12 - that you might want to get some experience
with these. #1, in terms of just Apps, specific
239:16 - built applications Excel & Tableau are really
fundamental for both getting the data from
239:24 - clients or doing some basic data browsing
and Tableau is really wonderful for interactive
239:30 - data visualization. I strongly recommend you
get very comfortable with both of those. In
239:35 - terms of code, it’s a good idea to learn
either ‘R’ or ‘Python’ or ideally
239:41 - to learn both. Ideally because you can use
them hand in hand. In terms of utilities,
239:47 - it’s a great idea to work with Bash, the
command line utility and to use regular expression
239:53 - or regex. You can actually use those in lots
and lots of programs; regular expressions.
239:58 - So they can have a very wide application.
And then finally, data science requires some
240:04 - sort of domain expertise. You’re going to
need some sort of field experience or intimate
240:09 - understanding of a particular domain and the
challenges that come up and what constitutes
240:14 - workable answers and the kind of data that’s
available. Now, as you go through all of this,
240:19 - you don’t need to build this monstrous list
of things. Remember, you don’t need everything.
240:25 - You don’t need every tool, you don’t need
every function, you don’t need every approach.
240:30 - Instead remember, get what’s best for your
needs, and for your style. But no matter what
240:37 - you do, remember that tools are tools, they
are a means to an end. Instead, you want to
240:42 - focus on the goal of your data science project
whatever it is. And I can tell you really,
240:48 - the goal is in the meaning, extracting meaning
out of your data to make informed choices.
240:53 - In fact, I’ll say a little more. The goal
is always meaning. And so with that, I strongly
240:59 - encourage you to get some tools, get started
in data science and start finding meaning
241:03 - in the data that’s around you. Welcome to
“Mathematics in Data Science”. I’m Barton
241:10 - Poulson and we’re going to talk about how
Mathematics matters for data science. Now,
241:16 - you maybe saying to yourself, “Why math?”,
and “Computers can do it, I don’t need
241:22 - to do it”. And really fundamentally, “I
don’t need math I am just here to do my
241:27 - work”. Well, I am here to tell you, No.
You need math. That is if you want to be a
241:36 - data scientist, and I assume that you do.
So we are going to talk about some of the
241:40 - basic elements of Mathematics, really at a
conceptual level and how they apply to data
241:44 - science. There are few ways that math really
matters to data science. #1, it allows you
241:51 - to know which procedures to use and why. So
you can answer your questions in a way that
241:58 - is the most informative and the most useful.
#2, if you have a good understanding of math,
242:04 - then you know what to do when things don’t
work right. That you get impossible values
242:09 - or things won’t compute, and that makes
a huge difference. And then #3, an interesting
242:15 - thing is that some mathematical procedures
are easier and quicker to do by hand then
242:21 - by actually firing up the computer. And so
for all 3 of these reasons, it’s really
242:25 - helpful to have at least a grounding in Mathematics
if you're going to do work in data science.
242:31 - Now probably the most important thing to start
with in Algebra. And there are 3 kinds of
242:36 - algebra I want to mention. The first is elementary
algebra, that’s the regular x+y. Then there
242:42 - is Linear or matrix algebra which looks more
complex, but is conceptually it is used by
242:47 - computers to actually do the calculations.
And then finally I am going to mention Systems
242:52 - of Linear Equations where you have multiple
equations simultaneously that you’re trying
242:56 - to solve. Now there’s more math than just
algebra. A few other things I’m going to
243:01 - cover in this course. Calculus, a little bit
of Big O or order which has to do with the
243:09 - speed and complexity of operations. A little
bit of probability theory and a little bit
243:15 - of Bayes or Bayes theorem which is used for
getting posterior probabilities and changes
243:19 - the way you interpret the results of an analysis.
And for the purposes of this course, I’m
243:25 - going to demonstrate the procedures by hand,
of course you would use software to do this
243:29 - in the real world, but we are dealing with
simple problems at conceptual levels. And
243:34 - really, the most important thing to remember
is that even though a lot of people get put
243:38 - off by math, really You can do it! And so,
in sum: let’s say these three things about
243:44 - math. First off, you do need some math to
do good data science. It helps you diagnose
243:49 - problems, it helps you choose the right procedures,
and interestingly you can do a lot of it by
243:54 - hand, or you can use software computers to
do the calculations as well. As we begin our
244:01 - discussion of the role of “Mathematics and
Data Science”, we’ll of course begin with
244:05 - the foundational elements. And in data science
nothing is more foundational than Elementary
244:09 - Algebra. Now, I’d like to begin this with
really just a bit of history. In case you’re
244:17 - not aware, the first book on algebra was written
in 820 by Muhammad ibn Musa al-Khwarizmi.
244:24 - And it was called “The Compendious Book
on Calculation by Completion and Balancing”.
244:28 - Actually, it was called this, which if you
transliterate that comes out to this, but
244:34 - look at this word right here. That’s the
algebra, which means Restoration. In any case,
244:40 - that’s where it comes from and for our concerns,
there are several kinds of algebra that we’re
244:45 - going to talk about. There’s Elementary
Algebra, there’s Linear Algebra and there
244:50 - are systems of linear equations. We’ll talk
about each of those in different videos. But
244:56 - to put it into context, let's take an example
here of salaries. Now, this is based on real
245:02 - data from a survey of the salary of people
employed in data science and to give a simple
245:07 - version of it. The salary was equal to a constant,
that’s sort of an average value that everybody
245:12 - started with and to that you added years,
then some measure of bargaining skills and
245:19 - how many hours they worked per week. And that
gave you your prediction, but that wasn’t
245:23 - exact there’s also some error to throw into
it to get to the precise value that each person
245:27 - has. Now, if you want to abbreviate this,
you can write it kind of like this: S + C
245:33 - + Y + B + H + E, although it’s more common
to write it symbolically like this, and let’s
245:42 - go through this equation very quickly. The
first thing we have is outcome,; we call that
245:48 - y the variable y for person i, “i” stands
for each case in our observations. So, here’s
245:56 - outcome y for person i. This letter here,
is a Greek Beta and it represents the intercept
246:04 - or the average, that’s why it has a zero,
because we don’t multiply it times anything.
246:08 - But right next to it we have a coefficient
for variable 1. So Beta, which means a coefficient,
246:16 - sub 1 for the first variable and then we have
variable 1 then x 1, means variable 1, then
246:24 - i means its the score on that variable for
person i, whoever we are talking about. Then
246:28 - we do the same thing for variables 2 and 3,
and at the end, we have a little epsilon here
246:34 - with an i for the error term for person i,
which says how far off from the prediction
246:38 - was their actual score. Now, I’m going to
run through some of these procedures and we’ll
246:43 - see how they can be applied to data science.
But for right now let’s just say this in
246:48 - sum. First off, Algebra is vital to data science.
It allows you to combine multiple scores,
246:54 - get a single outcome, do a lot of other manipulations.
And really, the calculations, their easy for
246:59 - one case at at time. Especially when you’re
doing it by hand. The next step for “Mathematics
247:05 - for Data Science” foundations is to look
at Linear algebra or an extension of elementary
247:11 - algebra. And depending on your background,
you may know this by another name and I like
247:15 - to think welcome to the Matrix. Because it’s
also known as matrix algebra because we are
247:20 - dealing with matrices . Now, let's go back
to an example I gave in the last video about
247:25 - salary. Where salary is equal to a constant
plus years, plus bargaining, plus hours plus
247:30 - error, okay that’s a way to write it out
in words and if you want to put it in symbolic
247:36 - form, it’s going to look like this. Now
before we get started with matrix algebra,
247:41 - we need to talk about a few new words, maybe
you're familiar with them already. The first
247:45 - is Scalar, and this means a single number.
And then a vector is a single row or a single
247:54 - column of numbers that can be treated as a
collection. That usually means a variable.
248:02 - And then finally, a matrix consists of many
rows and columns. Sort of a big rectangle
248:08 - of numbers, the plural of that by the way
is matrices and the thing to remember is that
248:13 - Machines love Matrices. Now let’s take a
look at a very simple example of this. Here
248:20 - is a very basic representation of matrix algebra
or Linear Algebra. Where we are showing data
248:28 - on two people, on four variables. So over
here on the left, we have the outcomes for
248:35 - cases 1 and 2, our people 1 and 2. And we
put it into the square brackets to indicate
248:40 - that it’s a vector or a matrix. Here on
the far left, it’s a vector because it’s
248:45 - a single column of values. Next to that is
a matrix, that has here on the top, the scores
248:54 - for case 1, which I’ve written as x’s.
X1 is for variable 1, X2 is for variable 2
249:00 - and the second subscript is indicated that
it’s for person 1. Below that, are the scores
249:07 - for case 2, the second person. And then over
here, in another vertical column are the regression
249:12 - coefficients, that’s a beta there that we
are using. And then finally, we've got a tiny
249:18 - little vector here which contains the error
terms for cases 1 and 2. Now, even though
249:24 - you would not do this by hand, it’s helpful
to run through the procedure, so I’m going
249:29 - to show it to you by hand. And we are going
to take two fictional people. This will be
249:34 - fictional person #1, we’ll call her Sophie.
We’ll say that she’s 28 years old and
249:38 - we’ll say that she’s has good bargaining
skills, a 4 on a scale of 5, and that she
249:42 - works 50 hours a week and that her salary
is $118,000.00. Our second fictional person,
249:50 - we’ll call him Lars and we’ll say that
he’s 34 years old and he has moderate bargaining
249:54 - skills 3 out of 5, works 35 hours per week
and has a salary of $84,000.00. And so if
250:02 - we are trying to look at salaries, we can
look at our matrix representation that we
250:08 - had here, with our variables indicated with
their Latin and sometimes Greek symbols. And
250:14 - we will replace those variables with actual
numbers. We have the salary for Sophie, our
250:19 - first person. So why don’t we plug in the
numbers here and let’s start with the result
250:24 - here. Sophie’s salary is $118,000.00 and
here’s how all these numbers all add up
250:29 - to get that. The first thing here is the intercept.
And we just multiply that times 1, so that’s
250:34 - sort of the starting point, and then we get
this number 10, which actually has to do with
250:40 - years over 18. She’s 28 so that’s 10 years
over 18, we multiply each year by 1395. Next
250:47 - is bargaining skills. She’s got a 4 out
of 5 and for each step up you get $5,900.00.
250:52 - By the way, these are real coefficients from
study of survey of salary of data scientists.
250:59 - And then finally hours per week. For each
hour, you get $382.00. Now you can add these
251:06 - up, and get a predicted value for her but
it’s a little low. It’s $30,00.00 low.
251:12 - Which you may be saying that’s pretty messed
up, well that’s because there’s like 40
251:15 - variables in the equation including she might
be the owner and if she’s the owner then
251:19 - yes she’s going to make a lot more. And
then we do a similar thing for the second
251:24 - case, but what’s neat about matrix algebra
or Linear Algebra is this means the same stuff
251:33 - and what we have here are these bolded variables.
That stand in for entire vectors or matrices.
251:41 - So for instance; this Y, a bold Y stands for
the vector of outcome scores. This bolded
251:49 - X is the entire matrix of values that each
person has on each variable. This bolded beta
251:58 - is all of the regression coefficients and
then this bolded epsilon is the entire vector
252:04 - of error terms. And so it’s a really super
compact way of representing the entire collection
252:12 - of data and coefficients that you use in predicting
values. So in sum, let’s say this. First
252:19 - off, computers use matrices. They like to
do linear algebra to solve problems and is
252:24 - conceptually simpler because you can put it
all in there in this type formation. In fact,
252:29 - it’s a very compact notation and it allows
you to manipulate entire collections of numbers
252:34 - pretty easily. And that’s that major benefit
of learning a little bit about linear or matrix
252:39 - algebra. Our next step in “Mathematics for
Data Science Foundations” is systems of
252:47 - linear equations. And maybe you are familiar
with this, but maybe you’re not. And the
252:53 - idea here is that there are times, when you
actually have many unknowns and you're trying
252:57 - to solve for them all simultaneously. And
what makes this really tricky is that a lot
253:03 - of these are interlocked. Specifically that
means X depends on Y, but at the same time
253:09 - Y depends on X. What’s funny about this,
is it’s actually pretty easy to solve these
253:15 - by hand and you can also use linear matrix
algebra to do it. So let’s take a little
253:21 - example here of Sales. Let’s imagine that
you have a company and that you’ve sold
253:26 - 1,000 iPhone cases, so that they are not running
around naked like they are in this picture
253:30 - here. Some of them sold for $20 and others
sold for $5. You made a total of $5,900.00
253:38 - and so the question is “How many were sold
at each price?” Now, if you were keeping
253:44 - our records, but you can also calculate it
from this little bit of information. And to
253:50 - show you I’m going to do it by hand. Now,
we’re going to start with this. We know
253:55 - that sales the two price points x + y add
up to 1,000 total cases sold. And for revenue,
254:04 - we know that if you multiply a certain number
times $20 and another number times $5, that
254:09 - it all adds up to $5,900.00. Between the two
of those we can figure out the rest. Let’s
254:15 - start with sales. Now, what I’m going to
do is try to isolate the values. I am going
254:21 - to do that by putting in this minus y on both
sides and then I can take that and I can subtract
254:29 - it, so I’m left with x is equal to 1,000
- y. Normally I solve for x, but I solve for
254:35 - y, you’ll see why in just a second. Then
we go to revenue. We know from earlier that
254:41 - our sales at these two prices points, add
up to $5,900.00 total. Now what we are going
254:45 - to do is take the x that’s right here and
we are going to replace it with the equation
254:48 - we just got, which is 1,000 - y. Then we multiply
that through and we get $20,000.00 minus $20y
254:56 - plus $5 y equals $5,900.00. Well, we can subtract
these two because they are on the same thing.
255:02 - So, $20y then we get $15y, and then we subtract
$20,000.00 from both sides. So there it is,
255:13 - right there on the left, and that disappears,
then I get it over on the right side. And
255:18 - then I do the math there, and I get minus
$14, 100.00. Well, then I divide both sides
255:24 - by negative $15.00 and when we do that we
get y equals 940. Okay, so that’s one of
255:32 - our values for sales. Let’s go back to sales.
We have x plus y equals 1,000. We take the
255:38 - value we just got, 940, we stick that into
the equation, then we can solve for x. Just
255:43 - subtract 940 from each side, there we go.
We get x is equal to 60. So, let’s put it
255:50 - all together, just to recap what happened.
What this tells us is that 60 cases were sold
255:57 - at $20.00 each. And that 940 cases were sold
at $5 each. Now, what’s interesting about
256:05 - this is you can also do this graphically.
We’re going to draw it. So, I’m going
256:10 - to graph the two equations. Here are the original
ones we had. This one predicts sales, this
256:14 - one gives price. The problem is, these aren’t
in the economical form for creating graphs.
256:19 - That needs to be y equals something else,
so we’re going to solve both of these for
256:24 - y. We subtract x from both sides, there it
is on the left, we subtract that. Then we
256:29 - have y is equals to minus x plus 1,000. That’s
something we can graph. Then we do the same
256:34 - thing for price. Let’s divide by 5 all the
way through, that gets rid of that and then
256:40 - we’ve got this 4x, then let's subtract 4x
from each side. And what we are left with
256:47 - is minus 4x plus 1,180, which is also something
we can graph. So this first line, this indicates
256:55 - cases sold. It originally said x plus y equals
1000, but we rearranged it to y is equal to
257:02 - minus x plus 1000. And so that’s the line
we have here. And then we have another line,
257:08 - which indicates earnings. And this one was
originally written as $20.00 times x plus
257:13 - $5.00 times y equals $5,900.00 total. We rearranged
that to y equals minus 4x plus 1,180. That’s
257:22 - the equation for the line and then the solution
is right here at the intersection. There’s
257:29 - our intersection and it’s at 60 on the number
of cases sold at $20.00 and 940 as the number
257:35 - of cases sold at $5.00 and that also represents
the solution of the joint equations. It’s
257:42 - a graphical way of solving a system of linear
equations. So in sum, systems of linear equations
257:50 - allow us to balance several unknowns and find
unique solutions. And in many cases, it’s
257:57 - easy to solve by hand, and it’s really easy
with linear algebra when you use software
258:03 - to do it at the same time. As we continue
our discussion of “Mathematics for Data
258:09 - Science” and the foundational principles
the next thing we want to talk about is Calculus.
258:14 - And I’m going to give a little more history
right here. The reason I’m showing you pictures
258:19 - of stones, is because the word Calculus is
Latin for stone, as in a stone used for tallying.
258:25 - Where when people would actually have a bag
of stones and they would use it to count sheep
258:30 - or whatever. And the system of Calculus was
formalized in the 1,600s simultaneously, independently
258:36 - by Isaac Newton and Gottfried Wilhelm Leibniz.
And there are 3 reasons why Calculus is important
258:44 - for data science. #1, it’s the basis for
most of the procedures we do. Things like
258:49 - least squares regression and probability distributions,
they use Calculus in getting those answers.
258:56 - Second one is if you are studying anything
that changes over time. If you are measuring
259:00 - quantities or rates that change over time
then you have to use Calculus. Calculus is
259:06 - used in finding the maxima and minima of functions
especially when you’re optimizing. Which
259:11 - is something I’m going to show you separately.
Also, it is important to keep in mind, there
259:15 - are two kinds of Calculus. The first is differential
Calculus, which talks about rates of change
259:22 - at a specific time. It’s also known as the
Calculus of change. The second kind of Calculus
259:28 - is Integral Calculus and this is where you
are trying to calculate the quantity of something
259:33 - at a specific time, given the rate of change.
It’s also known as the Calculus of Accumulation.
259:39 - So, let’s take a look at how this works
and we’re going to focus on differential
259:44 - Calculus. So I’m going to graph an equation
here, I’m going to do y equals x2 a very
259:50 - simple one but it’s a curve which makes
it harder to calculate things like the slope.
259:56 - Let’s take a point here that’s at minus
2, that’s the middle of the red dot. X is
260:01 - equal to minus 2. And because y is equal to
x2 , if we want to get the y value, all we
260:08 - got to do is take that negative 2 and square
it and that gives us 4. So that’s pretty
260:12 - easy. So the coordinates for that red point
are minus 2 on x, and plus 4 on the y. Here’s
260:18 - a harder question. “What is the slope of
the curve at that exact point?” Well, it’s
260:24 - actually a little tricky because the curve
is always curving there’s no flat part on
260:28 - it. But we can get the answer by getting the
derivative of the function. Now, there are
260:34 - several different ways of writing this, I
am using the one that’s easiest to type.
260:38 - And let’s start by this, what we are going
to do is the n here and that is the squared
260:44 - part, so that we have x2 . And you see that
same n turns into the squared, and then we
260:50 - come over here and we put that same value
2 in right there, and we put the two in right
260:57 - here. And then we can do a little bit of subtraction.
2 minus 1 is 1 and truthfully you can just
261:03 - ignore that then then you get 2x. That is
the derivative, so what we have here is the
261:08 - derivative of x2 is 2x. That means, the slope
at any given point in the curve is 2x. So,
261:16 - let’s go back to the curve we had a moment
ago. Here’s our curve, here’s our point
261:21 - at x minus 2, and so the slope is equal to
2x, well we put in the minus 2, and we multiply
261:28 - it and we get minus 4. So that is the slope
at this exact point in the curve. Okay, what
261:35 - if we choose a different point? Let’s say
we came over here to x is equal to 3? Well,
261:39 - the slope is equal to 2x so that’s 2 times
3, is equal to 6. Great! And on the other
261:47 - hand, you might be saying to yourself “And
why do I care about this?” There’s a reason
261:52 - that this is important and what it is, is
that you can use these procedures to optimize
261:58 - the decisions. And if that seems a little
to abstract to you, that means you can use
262:03 - them to make more money. And I’m going to
demonstrate that in the next video. But for
262:07 - right now in sum, let’s say this. Calculus
is vital to practical data science, it's the
262:15 - foundation of statistics and it forms the
core that’s needed for doing optimization.
262:25 - In our discussion about Mathematics and data
science foundations, the last thing I want
262:28 - to talk about right here is calculus and how
it relates to optimization. I like to think
262:34 - of this, in other words, as the place where
math meets reality, or it meets Manhattan
262:38 - or something. Now if you remember this graph
I made in the last video, y is equal to x2,
262:46 - that shows this curve here and we have the
derivative that the slope can be given by
262:51 - 2x. And so when x is equal to 3, the slope
is equal to 6, fine. And this is where this
262:58 - comes into play. Calculus makes it possible
to find values that maximize or minimize outcomes.
263:06 - And if you want to think of something a little
more concrete here, let’s think of an example,
263:10 - by the way that’s Cupid and Psyche. Let’s
talk about pricing for online dating. Let’s
263:17 - assume you’ve created a dating service and
you want to figure out how much can you charge
263:21 - for it that will maximize your revenue. So,
let’s get a few hypothetical parameters
263:28 - involved. First off, let’s say that subscriptions,
annual subscriptions cost $500.00 each year
263:34 - and you can charge that for a dating service.
And let’s say you sell 180 new subscriptions
263:40 - every week. On the other hand, based on your
previous experience manipulating prices around,
263:46 - you have some data that suggests that for
each $5 you discount from the price of $500.00
263:53 - you will get 3 more sales. Also, because its
an online service, lets make our life a little
263:59 - more easier right now and assume there is
no increase in overhead. It’s not really
264:03 - how it works, but we’ll do it for now. And
I’m actually going to show you how to do
264:07 - all this by hand. Now, let’s go back to
price first. We have this. $500.00 is the
264:14 - current annual subscription price and you’re
going to subtract $5.00 for each unit of discount,
264:20 - that’s why I’m giving D. So, one discount
is $5.00, two discounts is $10.00 and so on.
264:27 - And then we have a little bit of data about
sales, that you're currently selling 180 subscriptions
264:31 - per week and that you will add 3 more for
each unit of discount that you give. So, what
264:40 - we’re going to do here is we are going to
find sales as a function of price. Now, to
264:46 - do that the first thing we have to do is get
the y intercept. So we have price here, is
264:51 - $500.00, is the current annual subscription
price minus $5 times d. And what we are going
264:57 - to do is, is we are going to get the y intercept
by solving when does this equal zero? Okay,
265:04 - well we take the $500 we subtract that from
both sides and then we end up with minus $5d
265:10 - is equal to minus $500.00. Divide both sides
by minus $5 and we are left with d is equal
265:17 - to 100. That is, when d is equal to 100, x
is 0. And that tells us how we can get the
265:24 - y intercept, but to get that we have to substitute
this value into sales. So we take d is equal
265:29 - to 100, and the intercept is equal to 180
plus 3; 180 is the number of new subscriptions
265:36 - per week and then we take the three and we
multiply that times our 100. So, 180 times
265:43 - 3 times 100,[1] is equal to 300 add those
together and you get 480. And that is the
265:50 - y intercept in our equation, so when we’ve
discounted sort of price to zero then the
265:57 - expected sale is 480. Of course that’s not
going to happen in reality, but it’s necessary
266:02 - for finding the slope of the line. So now
let’s get the slope. The slope is equal
266:07 - to the change in y on the y axis divided by
the change in x. One way we can get this is
266:15 - by looking at sales; we get our 180 new subscriptions
per week plus 3 for each unit of discount
266:23 - and we take our information on price. $500.00
a year minus $5.00 for each unit of discount
266:29 - and then we take the 3d and the $5d and those
will give us the slope. So it’s plus 3 divided
266:37 - by minus 5, and that’s just minus 0.6. So
that is the slope of the line. Slope is equal
266:45 - to minus 0.6. And so what we have from this
is sales as a function of price where sales
266:52 - is equal to 480 because that is the y intercept
when price is equal to zero minus 0.6 times
267:02 - price. So, this isn’t the final thing. Now
what we have to do, we turn this into revenue,
267:07 - there’s another stage to this. Revenue is
equal to sales times price, how many things
267:14 - did you sell and how much did it cost. Well,
we can substitute some information in here.
267:20 - If we take sales and we put it in as a function
of price, because we just calculated that
267:24 - a moment ago, then we do a little bit of multiplication
and then we get that revenue is equal to 480
267:32 - times the price minus 0.6 times the price.
Okay, that’s a lot of stuff going on there.
267:40 - What we’re going to do now is we’re going
to get the derivative, that’s the calculus
267:43 - that we talked about. Well, the derivative
of 480 and the price, where price is sort
267:49 - of the x, the derivative is simply 480 and
the minus 0.6 times price? Well, that’s
267:58 - similar to what we did with the curve. And
what we end up with is 0.6 times 2 is equal
268:04 - to 1.2 times the price. This is the derivative
of the original equation. We can solve that
268:11 - for zero now, and just in case you are wondering.
Why do we solve it for zero? Because that
268:19 - is going to give us the place when y is at
a maximum. Now we had a minus squared so we
268:27 - have to invert the shape. When we are trying
to look for this value right here when it’s
268:33 - at the very tippy top of the curve, because
that will indicate maximum revenue. Okay,
268:39 - so what we’re going to do is solve for zero.
Let’s go back to our equation here. We want
268:46 - to find out when is that equal to zero? Well,
we subtract 480 from each side, there we go
268:54 - and we divide by minus 1.2 on each side. And
this is our price for maximum revenue. So
269:02 - we’ve been charging $500.00 a week, but
this says we’ll have more total income if
269:06 - we charge $400.00 instead. And if you want
to find out how many sales we can get, currently
269:13 - we have 480 and if you want to know what the
sales volume is going to be for that. Well,
269:20 - you take the 480 which is the hypothetical
y intercept when the price is zero, but then
269:26 - we put in our actual price of $400.00, multiply
that, we get 240, do the subtraction and we
269:33 - get 240 total. So, that would be 240 new subscriptions
per week. So let’s compare this. Current
269:41 - revenue, is 180 new subscriptions per week
at $500.00 per year. And that means our current
269:49 - revenue is $90,000.00 per year, I know it
sounds really good, but we can do better than
269:55 - that. Because the formula for maximum value
is 240 times $400.00, when you multiply those
270:02 - you get $96,000.00. And so the improvement
is just a ratio of those two. $96,000.00 divided
270:08 - by $90,000.00 is equal to 1.07. And what that
means is a 7% increase and anybody would be
270:16 - thrilled to get a 7% increase in their business
simply by changing the price and increasing
270:22 - the overall revenue. So, let’s summarize
what we found here. If you lower the cost
270:28 - by 20%, go from $500.00 year to $400.00 per
year, assuming all of our other information
270:34 - is correct, then you can increase sales by
33%; that’s more than the 20 that you had
270:40 - and that increases total revenue by 7%. And
so we can optimize the price to get the maximum
270:47 - total revenue and it has to do with that little
bit of calculus and the derivative of the
270:52 - function. So in sum, calculus can be used
to find the minima and maxima of functions
270:58 - including prices. It allows for optimization
and that in turn allows you to make better
271:04 - business decisions. Our next topic in “Mathematics
and Data Principals”, is something called
271:12 - Big O. And if you are wondering what Big O
is all about, it is about time. Or, you can
271:19 - think of it as how long does it take to do
a particular operation. It’s the speed of
271:25 - the operation. If you want to be really precise,
the growth rate of a function; how much more
271:31 - it requires as you add elements is called
its Order. That’s why it’s called Big
271:36 - O, that’s for Order. And Big O gives the
rate of how things grow as the number of elements
271:43 - grows, and what’s funny is there can be
really surprising differences. Let me show
271:47 - you how it works with a few different kinds
of growth rates or Big O. First off, there’s
271:53 - the ones that I say are sort of one the spot,
you can get stuff done right away. The simplest
271:59 - one is O1, and that is a constant order. That’s
something that takes the same amount of time,
272:04 - no matter what. You can send an email out
to 10,000 people just hit one button; it’s
272:10 - done. The number of elements, the number of
people, the number of operations, it just
272:13 - takes the same amount of time. Up from that
is Logarithmic, where you take the number
272:19 - of operations, you get the logarithm of that
and you can see it’s increased, but really
272:23 - it’s only a small increase, it tapers off
really quickly. So an example is finding an
272:29 - item in a sorted rate. Not a big deal. Next,
one up from that, now this looks like a big
272:35 - change, but in the grand scheme, it’s not
a big change. This is a linear function, where
272:41 - each operation takes the same unit of time.
So if you have 50 operations, you have 50
272:46 - units of time. If you’re storing 50 objects
it takes 50 units of space. So, find an item
272:52 - in an unsorted list it’s usually going to
be linear time. Then we have the functions
272:57 - where I say you know, you’d better just
pack a lunch because it’s going to take
273:00 - a while. The best example of this is called
Log Linear. You take the number of items and
273:06 - you multiply that number times the log of
the items. An example of this is called a
273:11 - fast Fourier transform, which is used for
dealing for instance with sound or anything
273:16 - that sort of is over time. You can see it
takes a lot longer, if you have 30 elements
273:20 - your way up there at the top of this particular
chart at 100 units of time, or 100 units of
273:25 - space or whatever you want to put it. And
it looks like a lot. But really, that’s
273:29 - nothing compared to the next set where I say,
you know you’re just going to be camping
273:33 - out you may as well go home. That includes
something like the Quadratic. You square the
273:38 - number of elements, you see how that kind
of just shoots straight up. That’s Quadratic
273:42 - growth. And so multiplying two n-digit numbers,
if you’re multiplying two numbers that have
273:49 - 10 digit numbers it’s going to take you
that long, it’s going to take a long time.
273:54 - Even more extreme is this one, this is the
exponential, two raised to the power to the
273:59 - number of items you have. You’ll see, by
the way, the red line does not even go all
274:03 - the way to the top. That’s because the graphing
software that I’m using, doesn’t draw
274:07 - it when it goes above my upper limit there,
so it kind of cuts it off. But this is a really
274:13 - demanding kind of thing, it’s for instance
finding an exact solution for what’s called
274:17 - the Travelling Salesman Problem, using dynamic
programming. That’s an example of exponential
274:22 - rate of growth. And then one more I want to
mention which is sort of catastrophic is Factorial.
274:28 - You take the number of elements and you raise
that to the exclamation point Factorial, and
274:32 - you see that one cuts off very soon because
it basically goes straight up. You have any
274:37 - number of elements of any size, it’s going
to be hugely demanding. And for instance if
274:43 - you're familiar with the Travelling Salesman
Problem, that’s trying to find the solution
274:46 - through the brute force search, it takes a
huge amount of time. And you know before something
274:51 - like that is done, you’re probably going
to turn to stone and wish you’d never even
274:56 - started. The other thing to know about this,
is that not only do something’s take longer
275:00 - than others, some of these methods and some
functions are more variable than others. So
275:05 - for instance, if you’re working with data
that you want to sort, there are different
275:09 - kinds of sort or sorting methods. So for instance,
there is something called an insertion sort.
275:15 - And when you find this on its best day, it’s
linear. It’s O of n, that’s not bad. On
275:23 - the other hand the average is Quadratic and
that’s a huge difference between the two.
275:30 - Selection sorts on the other hand, the best
is quadratic and the average is quadratic.
275:35 - It’s always consistent, so it’s kind of
funny, it takes a long time, but at least
275:40 - you know how long it’s going to take versus
the variability of something like an insertion
275:43 - sort. So in sum, let me say a few things about
Big O. #1, You need to know that certain functions
275:51 - or procedures vary in speed, and the same
thing applies to making demands on a computer’s
275:57 - memory or storage space or whatever. They
vary in their demands. Also, some are inconsistent.
276:03 - Some are really efficient sometimes and really
slow or difficult the others. Probably the
276:08 - most important thing here is to be aware of
the demands of what you are doing. That you
276:13 - can’t, for instance, run through every single
possible solution or you know, your company
276:18 - will be dead before you get an answer. So
be mindful of that so you can use your time
276:22 - well and get the insight you need, in the
time that you need it. A really important
276:29 - element of the “Mathematics and Data Science”
and one of its foundational principles is
276:33 - Probability. Now, one of the things that Probability
comes in intuitively for a lot of people is
276:39 - something like rolling dice or looking at
sports outcomes. And really the fundamental
276:44 - question of what are the odds of something.
That gets at the heart of Probability. Now
276:49 - let's take a look at some of the basic principles.
We’ve got our friend, Albert Einstein here
276:52 - to explain things. The Principles of Probability
work this way. Probabilities range from zero
276:59 - to 1, that’s like zero percent to one hundred
percent chance. When you put P, then in parenthesis
277:07 - here A, that means the Probability of whatever
is in parenthesis. So P(A), means the Probability
277:13 - of A. and then P(B) is the Probability of
B. When you take all of the probabilities
277:19 - together, you get what is called the probability
Space. And that’s why we have S and that
277:24 - all adds up to 1, because you’ve now covered
100 % of the possibilities. Also you can talk
277:30 - about the compliment. The tilde here is used
to say the probability of not A is equal to
277:37 - 1 minus the probability of A, because those
have to add up. So, let’s take a look at
277:42 - something also that conditional probabilities,
which is really important in statistics. A
277:47 - conditional probability is the probability
that something if something else is true.
277:52 - You write it this way: the probability of,
and that vertical line is called a Pipe and
277:56 - it’s read as assuming that or given that.
So you can read this as the probability of
278:01 - A given B, is the probability of A occurring
if B is true. So you can say for instance,
278:09 - what’s the probability if something’s
orange, what’s the probability that it’s
278:13 - a caret given this picture. Now, the place
that this comes in really important for a
278:17 - lot of people is the probability of type one
and type two errors in hypothesis testing,
278:22 - which we’ll mention at some other point.
But I do want to say something about arithmetic
278:26 - with probabilities because it does not always
work out the way people think it will. Let’s
278:30 - start by talking about adding probabilities.
Let’s say you have two events A and B, and
278:37 - let’s say you want to find the probabilities
of either one of those events. So that’s
278:40 - like adding the probabilities of the two events.
Well, it’s kind of easy. You take the probability
278:46 - of event A and you add the probability of
event B, however you may have to subtract
278:53 - something, you may have to subtract this little
piece because maybe there are some overlap
278:58 - between the two of them. On the other hand
if A and B are disjoined, meaning they never
279:02 - occur together, then that’s equal to zero.
And then you can subtract zero which is just,
279:08 - you get back to the original probabilities.
Let’s take a really easy example of this.
279:13 - I’ve created my super simple sample space
I have 10 shapes. I have 5 squares on top,
279:20 - 5 circles on the bottom and I’ve got a couple
of red shapes on the right side. Let’s say
279:24 - we want to find the probability of a square
or a red shape. So we are adding the probabilities
279:32 - but we have to adjust for the overlap between
the two. Well here’s our squares on top.
279:36 - 5 out of the 10 are squares and over here
on the right we have two red shapes, two out
279:42 - of 10. Let’s go back to our formula here
and let’s change a little bit. Change the
279:47 - A and the B to S and R for square and red.
Now we can start this way, let’s get the
279:52 - probability that something is a square. Well,
we go back to our probability space and you
279:57 - see we have 5 squares out of 10 shapes total.
So we do 5 over 10, that reduces to .5. Okay,
280:06 - next up the probability of something red in
our sample space. Well, we have 10 shapes
280:12 - total, two of them on the far right are red.
That’s two over 10, and you do the division
280:18 - get.2. Now, the trick is the overlap between
these two categories, do we have anything
280:23 - that is both square and red, because we don’t
want to count that twice we have to subtract
280:28 - it. Let’s go back to our sample space and
we are looking for something that is square,
280:33 - there’s the squares on top and there’s
the things that are red on the side. And you
280:36 - see they overlap and this is our little overlapping
square. So there’s one shape that meets
280:42 - both of those, one out of 10. So we come back
here, one out of 10, that reduces to .1 and
280:49 - then we just do the addition and subtraction
here. .5 plus .2 minus .1, gets us .6. And
280:56 - so what that means is, there is a 60% chance
of an object being square or red. And you
281:05 - can look at it right here. We have 6 shapes
outlined now and so that’s the visual interpretation
281:11 - that lines up with the mathematical one we
just did. Now let’s talk about multiplication
281:17 - for Probabilities. Now the idea here is you
want to get joint probabilities, so the probability
281:23 - of two things occurring together, simultaneously.
And what you need to do here, is you need
281:28 - to multiply the probabilities. And we can
say the probability of A and B, because we
281:33 - are asking about A and B occurring together,
a joint occurrence. And that’s equal to
281:38 - the probability of A times the probability
of B, that’s easy. But you do have to expand
281:45 - it just a little bit because you can have
the problem of things overlapping a little
281:49 - bit, and so you actually need to expand it
to a conditional probability, the probability
281:56 - of B given A. Again, that’s that vertical
pipe there. On the other hand, if A and B
282:01 - are independent and they never co-occur, or
B is no more likely to occur if A happens,
282:08 - then it just reduces to the probability of
B, then you get your slightly simpler equation.
282:12 - But let’s go and take a look at our sample
space here. So we’ve got our 10 shapes,
282:17 - 5 of each kind, and then two that are red.
And we are going to look at originally, the
282:23 - probability of something being square or red,
now we are going to look at the probability
282:26 - of it being square and red. Now, I know we
can eyeball this one real easy, but let’s
282:31 - run through the math. The first thing we need
to do, is get the ones that are square. There’s
282:36 - those 5 on the top and the ones that are red,
and there’s those two on the right. In terms
282:42 - of the ones that are both square and red,
yes obviously there’s just this one red
282:47 - square at the top right. But let’s do the
numbers here. We change our formula to be
282:51 - S and R for square and red, we get the probability
of square. Again that’s those 5 out of 10,
282:58 - so we do 5/10, reduce this to .5. And then
we need the probability of red given that
283:04 - it’s a square. So, we only need to look
at the squares here. There’s the squares,
283:09 - 5 of them, and one of them is red. So that’s
1 over 5 . That reduces to .2. You multiply
283:17 - those two numbers; .5 times .2, and what you
get is .10 or 10% chance or 10 percent of
283:23 - our total sample space is red squares. And
you come back and you look at it and you say
283:28 - yeah there’s one out of 10. So, that just
confirms what we are able to do intuitively.
283:32 - So, that’s our short presentation on probabilities
and in sum what did we get out of that? #1,
283:39 - Probability is not always intuitive. And also
the idea that conditional values can help
283:44 - in a lot of situations, but they may not work
the way you expect them to. And really the
283:49 - arithmetic of Probability can surprise people
so pay attention when you are working with
283:54 - it so you can get a more accurate conclusion
in your own calculations. Let’s finish our
284:00 - discussion of “Mathematics and Data Science”
and the basic principles by looking at something
284:05 - called Bayes’ theorem. And if you're familiar
with regular probability and influential testing,
284:08 - you can think of Bayes’ theorem as the flip
side of the coin. You can also think of it
284:14 - in terms of intersections. So for instance,
standard inferential tests and calculations
284:17 - give you the probability of the data; that’s
our d, given the hypothesis. So, if you assume
284:23 - a known hypothesis is true, this will give
you the probability of the data arising by
284:29 - chance. The trick is, most people actually
want the opposite of that. They want the probability
284:36 - of the hypothesis given the data. And unfortunately,
those two things can be very different in
284:42 - many circumstances. On the other hand, there’s
a way of dealing with it, Bayes does it and
284:50 - this is our guy right here. Reverend Thomas
Bayes, 18th Century English minister and statistician.
284:55 - He developed a method for getting what he
called posterior probabilities that use as
285:03 - prior probabilities. And test information
or something like base rates, how common something
285:06 - overall to get the posterior or after the
fact Probability. Here’s the general recipe
285:13 - to how this works: You start with the probability
of the data given the hypothesis which is
285:20 - what you get from the likelihood of the data.
You also get that from a standard inferential
285:25 - test. To that, you need to add the probability
to the hypothesis or the cause of being true.
285:30 - That’s called the prior or the prior probability.
To that you add the D; the probability of
285:39 - the data, that’s called the marginal probability.
And then you combine those and in a special
285:45 - way to get the probability of the hypothesis
given the data or the posterior probability.
285:50 - Now, if you want to write it as an equation,
you can write it in words like this; posterior
285:55 - is equal to likelihood times prior divided
by marginal. You can also write it in symbols
286:01 - like this; the probability of H given D, the
probability of the hypothesis given the data,
286:07 - that’s the posterior probability. Is equal
to the probability of the data given the hypothesis,
286:11 - that the likelihood, multiplied by the probability
of the hypothesis and divided by probability
286:14 - of the data overall. But this is a lot easier
if we look at a visual version of it. So,
286:22 - let’s go this example here. Let's say we
have a square here that represents 100% of
286:28 - all people and we are looking at a medical
condition. And what we are going to say here
286:34 - is that we got this group up here that represents
people who have a disease, so that’s a portion
286:39 - of all people. And that what we say, is we
have a test and people with the disease, 90%
286:43 - of them will test positive, so they’re marked
in red. Now it does mean over here on the
286:50 - far left people with the disease who test
negative that’s 10%. Those are our false
286:55 - negatives. And so if the test catches 90%
of the people who have the disease, that’s
287:01 - good right? Well, let’s look at it this
way. Let me ask y0u a basic question. “If
287:05 - a person tests positive for a disease, then
what is the probability they really have the
287:12 - disease?” And if you want a hint, I’m
going to give you one. It’s not 90%,. Here’s
287:18 - how it goes. So this is the information I
gave you before and we’ve got 90% of the
287:26 - people who have the disease; that’s a conditional
probability, they test positive. But what
287:30 - about the other people, the people in the
big white area below, ‘of all people’.
287:35 - We need to look at them and if any of them
ever test positive, do we ever get false positives
287:41 - and with any test you are going to get false
positives. And so let’s say our people without
287:47 - the disease, 90% of them test negative, the
way they should. But of the people who don’t
287:51 - have the disease, 10% of them test positive,
those are false positives. And so if you really
287:55 - want to answer the question, “If you test
positive do you have the disease?”, here’s
288:02 - what you need. What you need is the number
of people with the disease who test positive
288:08 - divided by all people who test positive. Let’s
look at it this way. So here’s our information.
288:13 - We’ve got 29.7% of all people are in this
darker red box, those are the people who have
288:19 - the disease and test positive, alright that’s
good. Then we have 6.7% of the entire group,
288:24 - that’s the people without the disease who
test positive. So we want to do, we want the
288:30 - probability of the disease what percentage
have the disease and test positive and then
288:33 - divide that by all the people that test positive.
And that bottom part is made up of two things.
288:40 - That’s made up of the people who have the
disease and test positive, and the people
288:48 - who don’t have the disease and test positive.
Now we can take our numbers and start plugging
288:55 - them in. Those who have the disease and test
positive that’s 29.7% of the total population
288:59 - of everybody. We can also put that number
right here. That’s fine, but we also need
289:05 - to look at the percentage that do not have
the disease and test positive; of the total
289:11 - population, that’s 6.7%. So, we just need
to rearrange, we add those two numbers on
289:16 - the bottom, we get 36.4% and we do a little
bit of division. And the number we get is
289:21 - 81.6%, here’s what that means. A positive
test result still only means a probability
289:26 - of 81.6% of having the disease. So, the test
is advertised at having 90% accuracy, well
289:31 - if you test positive there’s really only
a 82% chance you have the disease. Now that’s
289:36 - not really a big difference. But consider
this: what if the numbers change? For instance,
289:41 - what if the probability of the disease changes?
Here’s what we originally had. Let’s move
289:46 - it around a little bit. Let’s make the disease
much less common. And so now what we do, we
289:52 - are going to have 4.5% of all people are people
who have the disease and test positive. And
289:57 - then because there is a larger number of people
who don’t have the disease, we are going
290:02 - to have a relatively larger proportion of
false positives. Again, compared to the entire
290:05 - population it’s going to be 9.5% of everybody.
So we are going to go back to our formula
290:10 - here in words and start plugging in the numbers.
We get 4.5% right there, and right there.
290:14 - And then we add in our other number, the false
positives that’s 9.5%. Well, we rearrange
290:21 - and we start adding things up, that’s 14%
and when we divide that, we get 32.1%. Here’s
290:30 - what that number means. That means a positive
test result; you get a positive test result,
290:36 - now means you only have a probability of 32.1%
of having the disease. That’s ? less than
290:41 - the accuracy of 90%, and in case you can’t
tell, that’s a really big difference. And
290:45 - that’s why Bayes theorem matters, because
it answers the questions that people want
290:48 - and the answer can be dramatically different
depending on the base rate of the thing you
290:51 - are talking about. And so in sum, we can say
this. Bayes theorem allows you to answer the
290:55 - right question, people really want to know;
what’s the probability that I have the disease.
290:58 - What’s the probability of getting a positive
if I have the disease. They want to know whether
291:04 - they have the disease. And to do this, you
need to have prior probabilities, you need
291:10 - to know how common the disease is, you need
to know how many people get positive test
291:17 - results overall. But, if you can get that
information and run them through it can change
291:23 - your answers and really the emotional significance
of what you're dealing with dramatically.
291:28 - Let’s wrap up some of our discussion of
“Mathematics and Data Science” and the
291:33 - data principles and talk about some of the
next steps. Things you can do afterwards.
291:37 - Probably the most important thing is, you
may have learned about math a long time ago
291:43 - but now it's a good time to dig out some of
those books and go over some of the principles
291:49 - you’ve used before. The idea here is that
a little math can go a long way in data science.
291:55 - So, things like Algebra and things like Calculus
and things like Big O and Probability. All
292:00 - of those are important in data science and
its helpful to have at least a working understanding
292:03 - of each. You don’t have to know everything,
but you do need to understand the principles
292:07 - of your procedures that you select when you
do your projects. There are two reasons for
292:12 - that very generally speaking. First, you need
to know if a procedure will actually answer
292:15 - your question. Does it give you the outcome
that you need? Will it give you the insight
292:21 - that you need? Second; really critical, you
need to know what to do when things go wrong.
292:28 - Things don’t always work out, numbers don’t
always add up, you got impossible results
292:32 - or things just aren’t responding. You need
to know enough about the procedure and enough
292:36 - about the mathematics behind it, so you can
diagnose the problem, and respond appropriately.
292:40 - And to repeat myself once again, no matter
what you're working on in data science, no
292:44 - matter what tool you're using, what procedure
you’re doing, focus on your goal. And in
292:48 - case you can’t remember that, your goal
is meaning. Your goal is always meaning. Welcome
292:52 - to “Statistics in Data Science”. I’m
Barton Poulson and what we are going to be
292:58 - doing in this course is talking about some
of the ways you can use statistics to see
293:04 - the unseen. To infer what’s there, even
when most of it’s hidden. Now this shouldn’t
293:09 - be surprised. If you remember the data science
Venn Diagram we talked about a while ago,
293:13 - we have math up here at the top right corner,
but if you were to go to the original description
293:20 - of this Venn Diagram, it’s full name was
math and stats. And let me just mention something
293:25 - in case it’s not completely obvious about
why statistics matters to data science. And
293:28 - the idea is this; counting is easy. It’s
easy to say how many times a word appears
293:34 - in a document, it’s easy to say how many
people voted for a particular candidate in
293:37 - one part of the country. Counting is easy,
but summarizing and generalizing those things
293:42 - hard. And part of the problem is there’s
no such thing as a definitive analysis. All
293:47 - analyses really, depend on the purposes that
you're dealing with. So as an example, let
293:54 - me give you a couple of pairs of words and
try to summarize the difference between them
294:00 - in just two or three words. In a word or two,
how is a souffle different from a quiche,
294:05 - or how is an Aspen different from a Pine tree?
Or how is Baseball different from Cricket?
294:10 - And how are musicals different from opera?
It really depends on who you are talking to,
294:16 - it depends on your goals and it depends on
the shared knowledge. And so, there’s not
294:24 - a single definitive answer, and then there’s
the matter of generalization. Think about
294:28 - it again, take music. Listen to three concerti
by Antonio Vivaldi, and do you think you can
294:32 - safely and accurately describe all of his
music? Now, I actually chose Vivaldi on purpose
294:39 - because even Igor Stravinsky said you could,
he said he didn’t write 500 concertos he
294:47 - wrote the same concerto 500 times. But, take
something more real world like politics. If
294:52 - you talk to 400 registered voters in the US,
can you then accurately predict the behavior
294:57 - of all of the voters? There’s about 100
million voters in the US, and that’s a matter
295:05 - of generalization. That’s the sort of thing
we try to take care of with inferential statistics.
295:08 - Now there are different methods that you can
use in statistics and all of them are described
295:11 - to give you a map; a description of the data
you're working on. There are descriptive statistics,
295:17 - there are inferential statistics, there’s
the inferential procedure Hypothesis testing
295:18 - and there’s also estimation and I’ll talk
about each of those in more depth. There are
295:22 - a lot of choices that have to be made and
some of the things I’m going to discuss
295:33 - in detail are for instance the choice of Estimators,
that’s different from estimation. Different
295:41 - measures of fit. Feature selection, for knowing
which variables are the most important in
295:46 - predicting your outcome. Also common problems
that arise when trying to model data and the
295:51 - principles of model validation. But through
this all, the most important thing to remember
295:56 - is that analysis is functional. It’s designed
to serve a particular purpose. And there’s
295:59 - a very wonderful quote within the statistics
world that says all models are wrong. All
296:02 - statistical descriptions of reality are wrong,
because they are not exact depictions, they
296:07 - are summaries but some are useful and that’s
from George Box. And so the question is, you're
296:12 - not trying to be totally, completely accurate,
because in that case you just wouldn’t do
296:16 - an analysis. The real question is, are you
better off not doing your analysis than not
296:26 - doing it? And truthfully, I bet you are. So
in sum, we can say three things: #1, you want
296:33 - to use statistics to both summarize your data
and to generalize from one group to another
296:40 - if you can. On the other hand, there is no
“one true answer” with data, you got to
296:43 - be flexible in terms of what your goals are
and the shared knowledge. And no matter what
296:48 - your doing, the utility of your analysis should
guide you in your decisions. The first thing
296:53 - we want to cover in “Statistics in Data
Science” is the principles of exploring
296:57 - data and this video is just designed to give
an exploration overview. So we like to think
297:01 - of it like this, the intrepid explorers, they’re
out there exploring and seeing what’s in
297:05 - the world. You can see what’s in your data,
more specifically you want to see what your
297:09 - dataset is like. You want to see if your assumptions
are right so you can do a valid analysis with
297:15 - your procedure. Something that may sound very
weird, but you want to listen to your data.
297:19 - Something's not work out, if it’s not going
the way you want, then you’re going to have
297:25 - to pay attention and exploratory data analysis
is going to help you do that. Now, there are
297:30 - two general approaches to this. First off,
there’s a graphical exploration, so you
297:33 - use graphs and pictures and visualizations
to explore your data. The reason you want
297:35 - to do this is that graphics are very dense
in information. They're also really good,
297:43 - in fact the best to get the overall impression
of your data. Second to that, there is numerical
297:48 - exploration. I make it very clear, this is
the second step. Do the visualization first,
297:52 - then do the numerical part. Now you want to
do this, because this can give greater precision,
297:57 - this is also an opportunity to try variations
on the data. You can actually do some transformations,
298:01 - move things around a little bit and try different
methods and see how that effects the results,
298:06 - see how it looks. So, let’s go first to
the graphical part. They are very quick and
298:10 - simple plots that you can do. Those include
things like bar charts, histograms and scatterplots,
298:15 - very easy to make and a very quick way to
getting to understand the variables in your
298:18 - dataset. In terms of numerical analysis; again
after the graphical method, you can do things
298:21 - like transform the data, that is take like
the logarithm of your numbers. You can do
298:27 - Empirical estimates of population numbers,
and you can use robust methods. And I’ll
298:33 - talk about all of those at length in later
videos. But for right now, I can sum it up
298:38 - this way. The purpose of exploration is to
help you get to know your data. And also you
298:44 - want to explore your data thoroughly before
you start modelling, before you build statistical
298:48 - models. And all the way through you want to
make sure you listen carefully so that you
298:55 - can find hidden or unassumed details and leads
in your data. As we move in our discussion
299:02 - of “Statistics and Exploring Data”, the
single most important thing we can do is Exploratory
299:07 - Graphics. In the words of the late great Yankees
catcher Yogi Berra, “You can see a lot by
299:15 - just looking”. And that applies to data
as much as it applies to baseball. Now, there’s
299:22 - a few reasons you want to start with graphics.
#1, is to actually get a feel for the data.
299:30 - I mean, what’s it distributed like, what’s
the shape, are there strange things going
299:33 - on. Also it allows you to check the assumptions
and see how well your data match the requirements
299:37 - of the analytical procedures you hope to use.
You can check for anomalies like outliers
299:44 - and unusual distributions and errors and also
you can get suggestions. If something unusual
299:48 - is happening in the data, that might be a
clue that you need to pursue a different angle
299:52 - or do a deeper analysis. Now we want to do
graphics first for a couple of reasons. #1,
299:57 - is they are very information dense, and fundamentally
humans are visual. It’s our single, highest
300:00 - bandwidth way of getting information. It’s
also the best way to check for shape and gaps
300:04 - and outliers. There’s a few ways that you
can do this if you want to and the first is
300:08 - with programs that rely on code. So you can
use the statistical programming language R,
300:11 - the general purpose language Python. You can
actually do a huge amount in JavaScript, especially
300:12 - D3JS. Or you can use Apps, that are specifically
designed for exploratory analysis, that includes
300:18 - Tableau both the desktop and public versions,
Qlik and even Excel is a good way to do this.
300:22 - And finally you can do this by hand. John
Tukey who’s the father of Exploratory Data
300:28 - Analysis, wrote his seminal book, a wonderful
book where it’s all hand graphics and actually
300:32 - it’s a wonderful way to do it. But let’s
start the process for doing these graphics.
300:35 - We start with one variable. That is univariate
distributions. And so you’ll get something
300:38 - like this, the fundamental chart is the bar
chart. This is when you are dealing with categories
300:41 - and you are simply counting however many cases
there are in each category. The nice thing
300:45 - about bar charts is they are really easy to
read. Put them in descending order and may
300:50 - be have them vertical, maybe have them horizontal.
Horizontal could be nice to make the labels
300:55 - a little easier to read. This is about psychological
profiles of the United States, this is real
301:03 - data. We have most states in the friendly
and conventional, a smaller amount in the
301:09 - temperamental and uninhibited and the least
common of the United States is relaxed and
301:16 - creative. Next you can do a Box plot, or sometimes
called a box and whiskers plot. This is when
301:22 - you have a quantitative variable, something
that’s measured and you can say how far
301:26 - apart scores are. A box plot shows quartile
values, it also shows outliers. So for instance
301:32 - this is google searches for modern dance.
That’s Utah at 5 standard deviations above
301:37 - the national average. That’s where I’m
from and I’m glad to see that there. Also,
301:41 - it’s a nice way to show many variables side
by side, if they are on proximately similar
301:47 - scales. Next, if you have quantitative variables,
you are going to want to do a histogram. Again,
301:53 - quantitative so interval or ratio level, or
measured variables. And these let you see
301:57 - the shape of a distribution and potentially
compare many. So, here are three histograms
302:01 - of google searches on Data Science, and Entrepreneur
and Modern Dance. And you can see, mostly
302:07 - for the part normally distributed with a couple
of outliers. Once you’ve done one variable,
302:11 - or the univariate analyses, you’re going
to want to do two variables at a time. That
302:18 - is bivariate distributions or joint distributions.
Now, one easy way to do this is with grouped
302:24 - plots. You can do grouped bar charts and box
plots. What I have here is grouped box plots.
302:31 - I have my three regions, Psychological Regions
of the United States and I’m showing how
302:37 - they rank on openness that's a psychological
characteristic. As you can see, the relaxed
302:40 - and creative are high and the friendly conventional
tend to go to the lowest and that’s kind
302:45 - of how that works. It’s also a good way
of seeing the association between a categorical
302:50 - variable like region of the United States
psychologically, and a quantitative outcome,
302:53 - which is what we have here with openness.
Next, you can also do a Scatterplot. That’s
302:58 - where you have quantitative variables and
what you’re looking for here is, is it a
303:03 - straight line? Is it linear? Do we have outliers?
And also the strength of association. How
303:07 - closely do the dots all come to the regression
line that we have here in the middle. And
303:12 - this is an interesting one for me because
we have openness across the bottom, so more
303:19 - open as you go to the right and agreeableness.
And what you can see is there is a strong
303:26 - downhill association. The states and the states
that are the most open are also the least
303:33 - agreeable, so we’re going to have to do
something about that. And then finally, you’re
303:39 - going to want to go to many variables, that
is multivariate distributions. Now, one big
303:44 - question here is 3D or not 3D? Let me make
an argument for not 3D. So, what I have here
303:50 - is a 3D Scatterplot about 3 variables from
Google searches. Up the left, I have FIFA
303:55 - which is for professional soccer. Down there
on the bottom left, I have searches for the
303:59 - NFL and on the right I have searches for NBA.
Now, I did this in R and what’s neat about
304:07 - this is you can click and drag and move it
around. And you know that’s kind of fun,
304:14 - you kind of spin around and it gets kind of
nauseating as you look at it. And this particular
304:20 - version, I’m using plotly in R, allows you
to actually click on a point and see, let
304:24 - me see if I can get the floor in the right
place. You can click on a point and see where
304:32 - it ranks on each of these characteristics.
You can see however, this thing is hard to
304:41 - control and once it stops moving, it’s not
much fun and truthfully most 3D plots I’ve
304:49 - worked with are just kind of nightmares. They
seem like they’re a good idea, but not really.
304:56 - So, here’s the deal. 3D graphics, like the
one I just showed you, because they are actually
305:01 - being shown in 2D, they have to be in motion
for you to tell what is going on at all. And
305:07 - fundamentally they are hard to read and confusing.
Now it’s true, they might be useful for
305:11 - finding clusters in 3 dimensions, we didn’t
see that in the data we had, but generally
305:16 - I just avoid them like the plague. What you
do want to do however, is see the connection
305:21 - between the variables, you might want to use
a matrix of plots. This is where you have
305:28 - for instance many quantitative variables,
you can use markers for group membership if
305:31 - you want, and I find it to be much clearer
than 3D. So here, I have the relationship
305:38 - between 4 search terms: NBA, NFL, MLB for
Major League Baseball and FIFA. You can see
305:43 - the individual distributions, you can see
the scatterplots, you can get the correlation.
305:47 - Truthfully for me this is a much easier chart
to read and you can get the richness that
305:51 - we need, from a multidimensional display.
So the questions you're trying to answer overall
305:54 - are: Number 1, Do you have what you need?
Do you have the variables that you need, do
306:00 - you have the ability that you need? Are there
clumps or gaps in the distributions? Are there
306:07 - exceptional cases/anomalies that are really
far out from everybody else, spikes in the
306:11 - scores? And of course are there errors in
the data? Are there mistakes in coding, did
306:15 - people forget to answer questions? Are there
impossible combinations? And these kinds of
306:19 - things are easiest to see with a visualization
that really kind of puts it there in front
306:24 - of you. And so in sum, I can say this about
graphical exploration of data. It’s a critical
306:29 - first step, it’s basically where you always
want to start. And you want to use the quick
306:33 - and easy methods, again. Bar charts, scatter
plots are really easy to make and they're
306:37 - very easy to understand. And once you're done
with the graphical exploration, then you can
306:40 - go to the second step, which is exploring
the data through numbers. The next step in
306:45 - “Statistics and Exploring Data” is exploratory
statistics or numerical exploration of data.
306:48 - I like to think of this, as go in order. First,
you do visualization, then you do the numerical
306:55 - part. And a couple of things to remember here.
#1, you are still exploring the data. You're
307:02 - not modeling yet, but you are doing a quantitative
exploration. This might be an opportunity
307:06 - to get empirical estimates, that is of population
parameters as opposed to theoretically based
307:11 - ones. It’s a good time to manipulate the
data and explore the effect of manipulating
307:14 - the data, looking at subgroups, looking at
transforming variables. Also, it’s an opportunity
307:18 - to check the sensitivity of your results.
Do you get the same general results if you
307:22 - test under different circumstances. So we
are going to talk about things like Robust
307:26 - Statistics, resampling data and transforming
data. So, we’ll start with Robust Statistics.
307:30 - This by the way is Hercules, a Robust mythical
character. And the idea with robust statistics
307:36 - is that they are stable, is that even when
the data varies in unpredictable ways you
307:39 - still get the same general impression. This
is a class of statistics, it’s an entire
307:43 - category, that’s less affected by outliers,
and skewness, kurtosis and other abnormalities
307:46 - in the data. So let’s take a quick look.
This is a very skewed distribution that I
307:48 - created. The median, which is the dark line
in the box, is right around one. And I am
307:53 - going to look at two different kinds of robust
statistics, The Trimmed Mean and the Winsorized
307:58 - Mean. With the Trimmed mean, you take a certain
percentage of data from the top and the bottom
308:03 - and you just throw it away and compute for
the rest. With the Winsorized, you take those
308:07 - and you move those scores into the highest
non-outlier score. Now the 0% is exactly the
308:11 - same as the regular mean and here it’s 1.24,
but as we trim off or move in 5%, the mean
308:15 - shifts a little bit. Then 10 % it comes in
a little bit more to 25%, now we are throwing
308:18 - away 50% of our data. 25% on the top and 25%
on the bottom. And we get a trimmed mean of
308:24 - 1.03 and a winsorized of 1.07. When we throw
away 50% or we trim 50%, that actually means
308:31 - we are leaving just the median, only the middle
scores left. Then we get 1.01. What’s interesting
308:37 - is how close we get to that, even when we
have 50% of the data left, and so that’s
308:44 - an interesting example of how you can use
robust statistics to explore data, even when
308:49 - you have things like strong skewness. Next
is the principle of resampling. And that’s
308:52 - like pulling marbles repeatedly from the jar,
counting the colors, putting them back in
308:57 - and trying again. That’s an empirical estimate
of sampling variability. So, sometimes you
309:00 - get 20% red marbles, sometimes you get 30,
sometimes you get 22 and so on. There are
309:03 - several versions for this, they go by the
name jackknife, the bootstrap the permutation.
309:07 - And the basic principle of resampling is also
key to the process of cross-validation, I’ll
309:18 - have more to say about validation later. And
then finally there’s transforming variables.
309:22 - Here’s our caterpillars in the process of
transforming into butterflies. But the idea
309:26 - here, is that you take a difficult data set
and then you do what’s called a smooth function.
309:31 - There’s no jumps in it, and something that
allows you to preserve the order and work
309:38 - on the full dataset. So you can fix skewed
data, and in a scatter plot you might have
309:43 - a curved line, you can fix that. And probably
the best way to look at this is probably with
309:50 - something called Tukey’s ladder of powers.
I mentioned before John Tukey, the father
309:54 - of exploratory data analysis. He talked a
lot about data transformations. This is his
309:59 - ladder, starting at the bottom with the -1,
over x2, up to the top with x3. Here’s how
310:04 - it works, this distribution over here is a
symmetrical normally distributed variable,
310:08 - and as you start to move in one direction
and you apply the transformation, take the
310:12 - square root you see how it moves the distribution
over to one end. Then the logarithm, then
310:17 - you get to the end then you get to this minus
1 over the square of the score. And that pushes
310:25 - it way way, way over. If you go the other
direction, for instance you square the score,
310:29 - it pushes it down in the one direction and
then you cube it and then you see how it can
310:34 - move it around in ways that allow you to,
you can actually undo the skewness to get
310:40 - back to a more centrally distributed distribution.
And so these are some of the approaches that
310:44 - you can use in the numerical distribution
of data. In sum, let’s say this: statistical
310:48 - or numerical exploration allows you to get
multiple perspectives on your data. It also
310:51 - allows you to check the stability, see how
it works with outliers, and skewness and mixed
310:56 - distributions and so on. And perhaps most
important it sets the stage for the statistical
311:02 - modelling of your data. As a final step of
“Statistics and Exploring Data”, I’m
311:06 - going to talk about something that’s not
usually exploring data but it is basic descriptive
311:11 - statistics. I like to think of it this way.
You’ve got some data, and you are trying
311:19 - to tell a story. More specifically, you’re
trying to tell your data’s story. And with
311:24 - descriptive statistics, you can think of it
as trying to use a little data to stand in
311:31 - for a lot of data. Using a few numbers to
stand in for a large collection of numbers.
311:37 - And this is consistent with the advice we
get from good ole Henry David Thoreau, who
311:43 - told us Simplify, Simplify. If you can tell
your story with more carefully chosen and
311:49 - more informative data, go for it. So there’s
a few different procedures for doing this.
311:53 - #1, you’ll want to describe the center of
your distribution of data, that is if you’re
311:56 - going to choose a single number, use that.
# 2, if you can give a second number give
312:02 - something about the spread or the dispersion
of the variability. And #3, give something
312:07 - about the shape of the distribution. Let me
say more about each of these in turn. First,
312:13 - let’s talk about center. We have the center
of our rings here. Now there are a few very
312:17 - common measure of center or location or central
tendency of a distribution. There’s the
312:22 - mode, the median and there’s the mean. Now,
there are many, many others but those are
312:28 - the ones that are going to get you most of
the way. Let’s talk about the mode first.
312:33 - Now, I’m going to create a little dataset
here on a scale from 1 to 11, and I’m going
312:36 - to put individual scores. There’s a one,
and another one, and another one and another
312:39 - one. Then we have a two, two, then we have
a score way over at 9 and another score over
312:45 - at 11. So we have 8 scores, and this is the
distribution. This is actually a histogram
312:50 - of the dataset. The mode is the most commonly
occurring score or the most frequent score.
312:53 - Well, if you look at how tall each of these
go, we have more ones than anything else,
313:00 - and so one is the mode. Because it occurs
4 times and nothing else comes close to that.
313:05 - The median is a little different. The median
is looking for the score that is at the center
313:10 - if you split it into two equal groups. We
have 8 scores, so we have to get one group
313:11 - of 4, that’s down here, and the other group
of four, this really big one because it’s
313:14 - way out and the median is going to be the
place on the number line that splits those
313:19 - into two groups. That’s going to be right
here at one and a half. Now the mean is going
313:25 - to be a little more complicated, even though
people understand means in general. It’s
313:29 - the first one here that actually has a formula,
where M for the mean is equal to the sum of
313:34 - X (that’s our scores on the variable), divided
by N (the number of scores). You can also
313:39 - write it out with Greek notation if you want,
like this where that’s sigma - a capital
313:46 - sigma is the summation sign, sum of X divided
by N. And with our little dataset, that works
313:52 - out to this: one plus one plus one plus one
plus two plus two plus nine plus eleven. Add
313:58 - those all up and divide by 8, because that’s
how many scores there are. Well that reduces
314:04 - to 28 divided by 8, which is equal to 3.5.
If you go back to our little chart here, 3.5
314:12 - is right over here. You’ll notice there
aren’t any scores really exactly right there.
314:19 - That’s because the mean tends to get very
distorted by its outliers, it follows the
314:28 - extreme scores. But a really nice, I say it’s
more than just a visual analogy, is that if
314:35 - this number were a sea saw, then the mean
is exactly where the balance point or the
314:43 - fulcrum would be for these to be equal. People
understand that. If somebody weighs more they
314:46 - got to sit in closer to balance someone who
less, who has to sit further out, and that’s
314:49 - how the mean works. Now, let me give a bit
of the pros and cons of each of these. Mode
314:53 - is easy to do, you just count how common it
is. On the other hand, it may not be close
314:59 - to what appears to be the center of the data.
The Median it splits the data into two same
315:04 - size groups, the same number of scores in
each and that’s pretty easy to deal with
315:11 - but unfortunately, it’s pretty hard to use
that information in any statistics after that.
315:15 - And finally the mean, of these three it’s
the least intuitive, it’s the most effective
315:20 - by outliers and skewness and that really may
strike against it, but it is the most useful
315:24 - statistically and so it’s the one that gets
used most often. Next, there’s the issue
315:28 - of spread, spread your tail feathers. And
we have a few measures here that are pretty
315:33 - common also. There’s the range, there are
percentiles and interquartile range and there’s
315:37 - variance and standard deviation. I’ll talk
about each of those. First the Range. The
315:43 - Range is simply the maximum score minus the
minimum score, and in our case that’s 11
315:48 - minus 1, which is equal to 10, so we have
a range of 10. I can show you that on our
315:51 - chart. It’s just that line on the bottom
from the 11 down to the one. That’s a range
315:56 - of 10. The interquartile range which is actually
usually referred to simply as the IQR is the
316:02 - distance between the Q3; which is the third
quartile score and Q1; which is the first
316:10 - quartile score. If you're not familiar with
quartiles, it’s the same the 75th percentile
316:15 - score and the 25th percentile score. Really
what it is, is you’re going to throw away
316:21 - some of the some of the data. So let’s go
to our distribution here. First thing we are
316:24 - going to do, we are going to throw away the
two highest scores, there they are, they’re
316:27 - greyed out now, and then we are going to throw
away two of the lowest scores, they’re out
316:31 - there. Then we are going to get the range
for the remaining ones. Now, this is complicated
316:35 - by the fact that I have this big gap between
2 and 9, and different methods of calculating
316:39 - quartiles do something with that gap. So if
you use a spreadsheet it’s actually going
316:44 - to do an interpolation process and it will
give you a value of 3.75, I believe. And then
316:49 - down to one for the first quartile, so not
so intuitive with this graph but that it is
317:00 - how it works usually. If you want to write
it out, you can do it like this. The interquartile
317:05 - range is equal to Q3 minus Q1, and in our
particular case that’s 3.75 minus 1. And
317:08 - that of course is equal to just 2.75 and there
you have it. Now our final measure of spread
317:12 - or variability or dispersion, is two related
measures, the variance and the standard deviation.
317:14 - These are little harder to explain and a little
harder to show. But the variance, which is
317:19 - at least the easiest formula, is this: the
variance is equal to that’s the sum, the
317:24 - capital sigma that’s the sum, X minus M;
that’s how far each score is from the mean
317:26 - and then you take that deviation there and
you square it, you add up all the deviations,
317:29 - and then you divide by the number. So the
variance is, the average square deviation
317:32 - from the mean. I’ll try to show you that
graphically. So here’s our dataset and there’s
317:36 - our mean right there at 3 and a half. Let’s
go to one of these twos. We have a deviation
317:42 - there of 1.5 and if we make a square, that’s
1.5 points on each side, well there it is.
317:48 - We can do a similar square for the other score
too. If we are going down to one, then it’s
317:54 - going to be 2.5 squared and it’s going to
be that much bigger, and we can draw one of
317:59 - these squares for each one of our 8 points.
The squares for the scores at 9 and 11 are
318:06 - going to be huge and go off the page, so I’m
not going to show them. But once you have
318:11 - all those squares you add up the area and
you get the variance. So, this is the formula
318:17 - for the variance, but now let me show the
standard deviation which is also a very common
318:24 - measure. It’s closely related to this, specifically
it’s just the square root of the variance.
318:30 - Now, there’s a catch here. The formulas
for the variance and the standard deviation
318:35 - are slightly different for populations and
samples in that they use different denominators.
318:39 - But they give similar answers, not identical
but similar if the sample is reasonably large,
318:44 - say over 30 or 50, then it’s really going
to be just a negligible difference. So let’s
318:49 - do a little pro and con of these three things.
First, the Range. It’s very easy to do,
318:53 - it only uses two numbers the high and the
low, but it’s determined entirely by those
318:58 - two numbers. And if they’re outliers, then
you’ve got really a bad situation. The Interquartile
319:02 - Range the IQR, is really good for skewed data
and that’s because it ignores extremes on
319:07 - either end, so that’s nice. And the variance
and the standard deviation while they are
319:11 - the least intuitive and they are the most
affected by outliers, they are also generally
319:14 - the most useful because they feed into so
many other procedures that are used in data
319:18 - science. Finally, let’s talk a little bit
about the shape of the distribution. You can
319:24 - have symmetrical or skew distribution, unimodal,
uniform or u-shaped. You can have outliers,
319:33 - there’s a lot of variations. Let me show
you a few of them. First off is a symmetrical
319:36 - distribution, pretty easy. They’re the same
on the left and on the right. And this little
319:39 - pyramid shape is an example of a symmetrical
distribution. There are also skewed distributions,
319:43 - where most of the scores are on one end and
they taper off. This here is a positively
319:48 - skewed distribution where most of the scores
are at the low end and the outliers are on
319:57 - the high end. This is unimodal, our same pyramid
shape. Unimodal means it has one mode, really
320:04 - kind of one hump in the data. That’s contrasted
for instance to bimodal where you have two
320:10 - modes, and that usually happens when you have
two distributions that got mixed together.
320:14 - There is also uniform distribution where every
response is equally common, there’s u-shaped
320:17 - distributions where people tend to pile up
at one end or the other and a big dip in the
320:27 - middle. And so there’s a lot of different
variations, and you want to get those, the
320:32 - shape of the distribution to help you understand
and put the numerical summaries like the mean
320:41 - and like the standard deviation and put those
into context. In sum, we can say this: when
320:44 - you use this script of statistics that allows
you to be concise with your data, tell the
320:50 - story and tell it succinctly. You want to
focus on things like the center of the data,
320:57 - the spread of the data, the shape of the data.
And above all, watch out for anomalies, because
321:03 - they can exercise really undue influence on
your interpretations but this will help you
321:09 - better understand your data and prepare you
for the steps to follow. As we discuss “Statistics
321:14 - in Data Science”, one of the really big
topics is going to be Inference. And I’ll
321:20 - begin that with just a general discussion
of inferential statistics. But, I’d like
321:23 - to begin unusually with a joke, you may have
seen this before it says “There are two
321:28 - kinds of people in the world. 1) Those you
can extrapolate from incomplete data and,
321:32 - the end”. Of course, because the other group
is the people who can’t. But let’s talk
321:36 - about extrapolating from incomplete data or
inferring from incomplete data. First thing
321:40 - you need to know is the difference between
populations and samples. A population represents
321:47 - all of the data, or every possible case in
your group of interest. It might be everybody
321:54 - who’s a commercial pilot, it might be whatever.
But it represents everybody in that or every
322:01 - case in that group that you're interested
in. And the thing with the population is,
322:06 - it just is what it is. It has its values,
it has it’s mean and standard deviation
322:12 - and you are trying to figure out what those
are, because you generally use those in doing
322:23 - your analyses. On the other hand, samples
instead of being all of the data are just
322:27 - some of the data. And the trick is they are
sampled with error. You sample one group and
322:30 - you calculate the mean. It’s not going to
be the same if you do it the second time,
322:36 - and it's that variability that’s in sampling
that makes Inference a little tricky. Now,
322:44 - also in inference there are two very general
approaches. There’s testing which is short
322:50 - for hypothesis testing and maybe you’ve
had some experience with this. This is where
322:54 - you assume a null hypothesis of no effect
is true. You get your data and you calculate
323:01 - the probability of getting the sample data
that you have if the null hypothesis is true.
323:08 - And if that value is small, usually less than
5%, then you reject the null hypothesis which
323:14 - says really nothings happen and you infer
that there is a difference in the population.
323:19 - The other most common version is Estimation.
Which for instance is characterizing confidence
323:23 - intervals. That’s not the only version of
Estimation but it’s the most common. And
323:34 - this is where you sample data to estimate
a population parameter value directly, so
323:37 - you use the sample mean to try to infer what
the population mean is. You have to choose
323:45 - a confidence level, you have to calculate
your values and you get high and low bounds
323:54 - for you estimate that work with a certain
level of confidence. Now, what makes both
323:56 - of these tricky is the basic concept of sampling
error. I have a colleague who demonstrates
324:00 - this with colored M&M's, what percentage are
red, and you get them out of the bags and
324:04 - you count. Now, let’s talk about this, a
population of numbers. I’m going to give
324:09 - you just a hypothetical population of the
numbers 1 through 10. And what I am going
324:13 - to do, is I am going to sample from those
numbers randomly, with replacement. That means
324:20 - I pull a number out, it might be a one and
I put it back, I might get the one again.
324:25 - So I’m going to sample with replacement,
which actually may sound a little bit weird,
324:27 - but it’s really helpful for the mathematics
behind inference. And here are the samples
324:35 - that I got, I actually did this with software.
I got a 3, 1, 5, and 7. Interestingly, that
324:39 - is almost all odd numbers, almost. My second
sample is 4, 4, 3, 6 and 10. So you can see
324:49 - I got the 4 twice. And I didn’t get the
1, the 2, the 5, 7, or 8 or 9. The third sample
325:00 - I got three 1’s! And a 10 and a 9, so we
are way at the ends there. And then my fourth
325:10 - sample, I got a 3, 9, 2, 6, 5. All of these
were drawn at random from the exact same population,
325:17 - but you see that the samples are very different.
That’s the sampling variability or the sampling
325:21 - error. And that’s what makes inference a
little trickier. And let’s just say again,
325:23 - why the sampling variability, why it matters.
It’s because inferential methods like testing
325:27 - and like estimation try to see past the random
sampling variation to get a clear picture
325:29 - on the underlying population. So in sum, let’s
say this about Inferential Statistics. You
325:34 - sample your data from the larger populations,
and as you try to interpret it, you have to
325:38 - adjust for error and there’s a few different
ways of doing that. And the most common approaches
325:42 - are testing or hypothesis testing and estimation
of parameter values. The next step in our
325:48 - discussion of “Statistics and Inference”
is Hypothesis Testing. A very common procedure
325:52 - in some fields of research. I like to think
of it as put your money where your mouth is
325:58 - and test your theory. Here’s the Wright
brothers out testing their plane. Now the
326:01 - basic idea behind hypothesis testing is this,
and you start out with a question. You start
326:08 - out with something like this: What is the
probability of X occurring by chance, if randomness
326:11 - or meaningless sampling variation is the only
explanation? Well, the response is this, if
326:12 - the probability of that data arising by chance
when nothing’s happening is low, then you
326:19 - reject randomness as a likely explanation.
Okay, there’s a few things I can say about
326:23 - this. #1, it’s really common in scientific
research, say for instance in the social sciences,
326:29 - it’s used all the time. #2, this kind of
approach can be really helpful in medical
326:34 - diagnostics, where you're trying to make a
yes/no decision; does a person have a particular
326:38 - disease. And 3, really anytime you're trying
to make a go/no go decision, which might be
326:43 - made for instance with a purchasing decision
for a school district or implementing a particular
326:48 - law, You base it on the data and you have
to make a yes/no. Hypothesis testing might
326:53 - be helpful in those situations. Now, you have
to have hypotheses to do hypothesis testing.
326:57 - You start with H0, which is shorthand for
the null hypothesis. And what that is in larger,
327:02 - what that is in lengthier terms is that there
is no systematic effect between groups, there’s
327:07 - no effect between variables and random sampling
error is the only explanation for any observed
327:11 - differences you see. And then contrast that
with HA, which is the alternative hypothesis.
327:16 - And this really just says there is a systematic
effect, that there is in fact a correlation
327:20 - between variables, that there is in fact a
difference between two groups, that this variable
327:24 - does in fact predict the other one. Let’s
take a look at the simplest version of this
327:30 - statistically speaking. Now, what I have here
is a null distribution. This is a bell curve,
327:37 - it’s actually the standard normal distribution.
Which shows z-scores in relative frequency,
327:41 - and what you do with this is you mark off
regions of rejection. And so I’ve actually
327:45 - shaded off the highest 2.5% of the distribution
and the lowest 2.5%. What’s funny about
327:50 - this is, is that even though I draw it +/-
3, it looks like 0. It’s actually infinite
327:57 - and asymptotic. But, that’s the highest
and lowest 2.5% collectively leaves 95% in
328:01 - the middle. Now, the idea is then that you
gather your data, you calculate a score for
328:04 - you data and you see where it falls in this
distribution. And I like to think of that
328:09 - as you have to go down one path to the other,
you have to make a decision. And you have
328:17 - to decide to whether to retain your null hypothesis;
maybe it is random, or reject it and decide
328:23 - no I don’t think it's random. The trick
is, things can go wrong. You can get a false
328:28 - positive, and this is when the sample shows
some kind of statistical effect, but it’s
328:33 - really randomness. And so for instance, this
scatterplot I have here, you can see a little
328:40 - down hill association here but this is in
fact drawn from data that has a true correlation
328:44 - of zero. And I just kind of randomly sampled
from it, it took about 20 rounds, but it looks
328:51 - negative but really there’s nothing happening.
The trick about false positives is; that's
328:56 - conditional on rejecting the null. The only
way to get a false positive is if you actually
329:04 - conclude that there’s a positive result.
It goes by the highly descriptive name of
329:10 - a Type I error, but you get to pick a value
for it, and .05 or a 5% risk if you reject
329:17 - the null hypothesis, that’s the most common
value. Then there’s a false negative. This
329:23 - is when the data looks random, but in fact,
it’s systematic or there’s a relationship.
329:29 - So for instance, this scatterplot it looks
like there’s pretty much a zero relationship,
329:34 - but in fact this came from two variables that
were correlated at .25, that’s a pretty
329:41 - strong association. Again, I randomly sampled
from the data until I got a set that happened
329:48 - to look pretty flat. And a false negative
is conditional on not rejecting the null.
329:53 - You can only get a false negative if you get
a negative, you say there’s nothing there.
329:55 - It’s also called a Type II error and this
is a value that you have to calculate based
330:00 - on several elements of your testing framework,
so it’s something to be thoughtful of. Now,
330:06 - I do have to mention one thing, big security
notice, but wait. The problem with Hypothesis
330:10 - Testing; there’s a few. #1, it’s really
easy to misinterpret it. A lot of people say,
330:15 - well if you get a statistically significant
result, it means that it’s something big
330:19 - and meaningful. And that’s not true because
it’s confounded with sample size and a lot
330:26 - of other things that don’t really matter.
Also, a lot of other people take exception
330:30 - with the assumption of a null effect or even
a nil effect, that there’s zero difference
330:34 - at all. And that can be, in certain situations
can be an absurd claim, so you’ve got to
330:39 - watch out for that. There’s also bias from
the use of cutoff. Anytime you have a cut
330:46 - off, you’re going to have problems where
you have cases that would have been slightly
330:51 - higher, slightly lower. It would have switched
on the dichotomous outcome, so that is a problem.
330:55 - And then a lot of people say, it just answers
the wrong question, because “What it’s
330:59 - telling you is what’s the probability of
getting this data at random?” That’s not
331:05 - what most people care about. They want it
the other way, which is why I mentioned previously
331:11 - Bayes theorem and I’ll say more about that
later. That being said, Hypothesis Testing
331:13 - is still very deeply ingrained, very useful
in a lot of questions and has gotten us really
331:22 - far in a lot of domains. So in sum, let me
say this. Hypothesis Testing is very common
331:31 - for yes/no outcomes and is the default in
many fields. And I argue it is still useful
331:38 - and information despite many of the well substantiated
critiques. We’ll continue in “Statistics
331:41 - and Inference” by discussing Estimation.
Now as opposed to Hypothesis Testing, Estimation
331:48 - is designed to actually give you a number,
give you a value. Not just a yes/no, go/no
331:54 - go, but give you an estimate for a parameter
that you’re trying to get. I like to think
332:01 - of it sort of as a new angle, looking at something
from a different way. And the most common,
332:09 - approach to this is Confidence Intervals.
Now, the important thing to remember is that
332:13 - this is still an Inferential procedure. You're
still using sample data and trying to make
332:20 - conclusions about a larger group or population.
The difference here, is instead of coming
332:24 - up with a yes/no, you’d instead focus on
likely values for the population value. Most
332:30 - versions of Estimation are closely related
to Hypothesis Testing, sometimes seen as the
332:35 - flip side of the coin. And we’ll see how
that works in later videos. Now, I like to
332:41 - think of this as an ability to estimate any
sample statistic and there’s a few different
332:45 - versions. We have Parametric versions of Estimation
and Bootstrap versions, that’s why I got
332:51 - the boots here. And that’s where you just
kind of randomly sample from the data, in
332:59 - an effort to get an idea of the variability.
You can also have central versus noncentral
333:04 - Confidence Intervals in the Estimation, but
we are not going to deal with those. Now,
333:09 - there are three general steps to this. First,
you need to choose a confidence level. Anywhere
333:14 - from say, well you can’t have a zero, it
has to be more than zero and it can’t be
333:20 - 100%. Choose something in between, 95% is
the most common. And what it does, is it gives
333:24 - you a range a high and a low. And the higher
your level of confidence the more confident
333:33 - you want to be, the wider the range is going
to be between your high and your low estimates.
333:42 - Now, there’s a fundamental trade off in
what’ happening here and the trade off between
333:45 - accuracy; which means you're on target or
more specifically that your interval contains
333:48 - the true population value. And the idea is
that leads you to the correct Inference. There’s
333:54 - a tradeoff between accuracy and what’s called
Precision in this context. And precision means
334:00 - a narrow interval, as a small range of likely
values. And what’s important to emphasize
334:07 - is this is independent of accuracy, you can
have one without the other! Or neither or
334:12 - both. In fact, let me show you how this works.
What I have here is a little hypothetical
334:20 - situation, I’ve got a variable that goes
from 10 to 90, and I’ve drawn a thick black
334:24 - line at 50. If you think of this in terms
of percentages and political polls, it makes
334:29 - a very big difference if you're on the left
or the right of 50%. And then I’ve drawn
334:36 - a dotted vertical line at 55 to say that that’s
our theoretical true population value. And
334:41 - what I have here is a distribution that shows
possible values based on our sample data.
334:47 - And what you get here is it’s not accurate,
because it’s centered on the wrong thing.
334:53 - It’s actually centered on 45 as opposed
to 55. And it’s not precise, because it’s
334:58 - spread way out from may be 10 to almost 80.
So, this situation the data is no help really
335:06 - at all. Now, here’s another one. This is
accurate because it’s centered on the true
335:13 - value. That’s nice, but it’s still really
spread out and you see that about 40% of the
335:19 - values are going to be on the other side of
50%; might lead you to reach the wrong conclusion.
335:27 - That’s a problem! Now, here’s the nightmare
situation. This is when you have a very very
335:33 - precise estimate, but it’s not accurate;
it’s wrong. And this leads you to a very
335:37 - false sense of security and understanding
of what’s going on and you’re going to
335:42 - totally blow it all the time. The ideal situation
is this: you have an accurate estimate where
335:49 - the distribution of sample values is really
close to the true population value and it’s
335:56 - precise, it’s really tightly knit and you
can see that about 95% of it is on the correct
336:03 - side of 50 and that’s good. If you want
to see all four of them here at once, we have
336:15 - the precise two on the bottom, the imprecise
ones on the top, the accurate ones on the
336:20 - right, the inaccurate ones on the left. And
so that’s a way of comparing it. But, no
336:26 - matter what you do, you have to interpret
confidence interval. Now, the statistically
336:28 - accurate way that has very little interpretation
is this: you would say the 95% confidence
336:34 - interval for the mean is 5.8 to 7.2. Okay,
so that’s just kind of taking the output
336:39 - from your computer and sticking it to sentence
form. The Colloquial Interpretation of this
336:43 - goes like this: there is a 95% chance that
the population mean is between 5.8 and 7.2.
336:49 - Well, in most statistical procedures, specifically
frequentist as opposed to bayesian you can’t
336:52 - do that. That implies the population mean
shifts, that’s not usually how people see
336:58 - it. Instead, a better interpretation is this;
95% of confidence intervals for randomly selected
337:03 - samples will contain the population mean.
Now, I can show you this really easily, with
337:08 - a little demonstration. This is where I randomly
generated data from a population with a mean
337:12 - of 55 and I got 20 different samples. And
I got the Confidence Interval from each sample
337:17 - and I charted the high and the low. And the
question is, did it include the true population
337:23 - value. And you can see of these 20, 19 included
it, some of them barely made it. If you look
337:31 - at sample #1 on the far left; barely made
it. Sample #8, it doesn’t look like it made
337:39 - it, sample 20 on the far right, barely made
it on the other end. Only one missed it completely,
337:46 - that sample #2, which is shown in red on the
left. Now, it’s not always just one out
337:49 - of twenty, I actually had to run this simulation
about 8 times, because it gave me either zero
337:54 - or 3, or 1 or two, and I had to run it until
I got exactly what I was looking for here,.
338:01 - But this is what you would expect on average.
So, let’s say a few things about this. There
338:06 - are somethings that affect the width of a
Confidence Interval. The first is the confidence
338:10 - level, or CL. Higher confidence levels create
wider intervals. The more certain you have
338:15 - to be, you're going to give a bigger range
to cover your basis. Second, the Standard
338:21 - Deviation or larger standard deviations create
wider intervals. If the thing that you are
338:28 - studying is inherently really variable, then
of course you’re estimate of the range is
338:32 - going to be more variable as well. And then
finally there is the n or the sample size.
338:42 - This one goes the other way. Larger sample
sizes create narrower intervals. The more
338:48 - observations you have, the more precise and
the more reliable things tend to be. I can
338:54 - show you each of these things graphically.
Here we have a bunch of Confidence Intervals,
339:00 - where I am simply changing the confidence
level from .50 at the low left side to .999
339:09 - and as you can see, it gets much bigger as
we increase. Next one is Standard Deviation.
339:13 - As the sample standard deviation increases
from 1 to 16, you can see that the interval
339:17 - gets a lot bigger. And then we have sample
size going from just 2 up to 512; I’m doubling
339:23 - it at each point. And you can see how the
interval gets more and more and more precise
339:28 - as we go through. And so, let’s say this
to sum up our discussion of estimation. Confidence
339:33 - Intervals which are the most common version
of Estimation focus on the population parameter.
339:38 - And the variation in the data is explicitly
included in that Estimation. Also, you can
339:43 - argue that they are more informative, because
not only do they tell you whether the population
339:48 - value is likely, but they give you a sense
of the variability of the data itself, and
339:55 - that’s one reason why people will argue
that confidence levels should always be included
340:00 - in any statistical analysis. As we continue
our discussion on “Statistics and Data Science”,
340:06 - we need to talk about some of the choices
you have to make, some of the tradeoffs and
340:13 - some of the effects that these things have.
We’ll begin by talking about Estimators,
340:17 - that is different methods for estimating parameters.
I like to think of it as this, “What kind
340:21 - of measuring stick or standard are you going
to be using?” Now, we’ll begin with the
340:28 - most common. This is called OLS, which is
actually short for Ordinary Least Squares.
340:32 - This is a very common approach, it’s used
in a lot of statistics and is based on what
340:39 - is called the sum of squared errors, and it’s
characterized by an acronym called BLUE, which
340:44 - stands for Best Linear Unbiased Estimator.
Let me show you how that works. Let’s take
340:47 - a scatterplot here of an association between
two variables. This is actually the speed
340:53 - of a car and the distance to stop from about
the ‘20’s I think. We have a scatterplot
341:00 - and we can draw a straight regression line
right through it. Now, the line I’ve used
341:12 - is in fact the Best Linear Unbiased Estimate,
but the way that you can tell that is by getting
341:17 - what are called the Residuals. If you take
each data point and draw a perfectly vertical
341:22 - line up or down to the regression line, because
the regression line predicts what the value
341:27 - would be for that value on the X axis. Those
are the residuals. Each of those individual,
341:32 - vertical lines is Residual. You square those
and you add them up and this regression line,
341:36 - the gray angled line here will have the smallest
sum of the squared residuals of any possible
341:39 - straight line you can run through it. Now,
another approach is ML, which stands for Maximum
341:46 - Likelihood. And this is when you choose parameters
that make the observed data most likely. It
341:51 - sounds kind of weird, but I can demonstrate
it, and it’s based on a kind of local search.
341:57 - It doesn’t always find the best, I like
to think of it here like the person here with
342:04 - a pair of binoculars, looking around them,
trying hard to find something, but you could
342:10 - theoretically miss something. Let me give
a very simple example of how this works. Let’s
342:15 - assume that we’re trying to find parameters
that maximize the likelihood of this dotted
342:20 - vertical line here at 55, and I’ve got three
possibilities. I’ve got my red distribution
342:25 - which is off to the left, blue which is a
little more centered and green which is far
342:34 - to the right. And these are all identical,
except they have different means, and by changing
342:45 - the means, you see there the one that is highest
where the dotted line is the blue one. And
342:53 - so, if the only thing we are doing is changing
the mean, and we are looking at these three
343:00 - distributions, then the blue one is the one
that has the maximum likelihood for this particular
343:06 - parameter. On the other hand, we could give
them all the same meaning right around 50,
343:14 - and vary their standard deviations instead
and so they spread out different amounts.
343:21 - In this case, the red distribution is highest
at the dotted vertical line and so it has
343:27 - the maximum value. Or if you want to, you
can vary both the mean and the standard deviations
343:36 - simultaneously. And here green gets the slight
advantage. Now this is really a caricature
343:43 - of the process because obviously you would
just want to center it on the 55 and be done
343:52 - with it. The question is when you have many
variables in your dataset. Then it’s a very
343:56 - complex process of choosing values that can
maximize the association between all of them.
344:01 - But you get a feel for how it works with this.
The third approach which is pretty common
344:07 - is MAP or map for Maximum A Posteriori. This
is a Bayesian approach to parameter estimation,
344:13 - and what it does it adds the prior distribution
and then it goes through sort of an anchoring
344:21 - and adjusting process. What happens, by the
way is stronger prior estimates exert more
344:26 - influence on the estimate and that might mean
for example larger sample or more extreme
344:32 - values. And those have a greater influence
on the posterior estimate of the parameters.
344:36 - Now, what’s interesting is that all three
of these methods all connect with each other.
344:42 - Let me show you exactly how they connect.
The ordinary least squares, OLS, this is equivalent
344:48 - to maximum likelihood, when it has normally
distributed error terms. And maximum likelihood,
344:53 - ML is equivalent to Maximum A Posteriori or
MAP, with a uniform prior distribution. You
345:01 - want to put it another way, ordinary least
squares or OLS is a special case of Maximum
345:05 - Likelihood. And then maximum likelihood or
ML, is a special case of Maximum A Posteriori,
345:11 - and just in case you like it, we can put it
into set notation. OLS is a subset of ML is
345:21 - a subset of MAP, and so there are connections
between these three methods of estimating
345:27 - population parameters. Let me just sum it
up briefly this way. The standards that you
345:29 - use OLS, ML, MAP they affect your choices
and they determine which parameters best estimate
345:34 - what’s happening in your data. Several methods
exist and there’s obviously more than what
345:38 - I showed you right here, but many are closely
related and under certain circumstances they’re
345:44 - all identical. And so it comes down to exactly
what are your purposes and what do you think
345:50 - is going to work best with the data that you
have to give you the insight that you need
345:56 - in your own project. The next step we want
to consider in our “Statistics and Data
345:59 - Science”, are choices that we have to make.
Has to do with Measures of fit or the correspondence
346:02 - between the data that we have and the model
that you create. Now, turns out there are
346:06 - a lot of different ways to measure this and
one big question is how close is close enough
346:14 - or how can you see the difference between
the model and reality. Well, there’s a few
346:20 - really common approaches to this. The first
one has what’s called R2. That’s kind
346:24 - of the longer name, that’s the coefficient
of determination. There’s a variation; adjusted
346:27 - R2, which takes into consideration the number
of variables. Then there’s minus 2LL, which
346:30 - is based on the likelihood ratio and a couple
of variations. The Akaike Information Criterion
346:32 - or AIC and the Bayesian Information Criterion
or BIC. Then there’s also Chi-Squared, it’s
346:34 - actually a Greek c, it looks like a x, but
it’s actually c and it’s chi-squared.
346:38 - And so let’s talk about each of these in
turn. First off is R2, this is the squared
346:45 - multiple correlation or the coefficient of
determination. And what it does is it compares
346:49 - the variance of Y, so if you have an outcome
variable, it looks like the total variance
346:53 - of that and compares it to the residuals on
Y after you’ve made your prediction. The
347:01 - scores on squared range from 0 to 1 and higher
is better. The next is -2 Log-likelihood that’s
347:08 - the likelihood ratio or like I just said the
-2 log likelihood. And what this does is compares
347:10 - the fit of nested models, we have a subset
then a larger set, than the larger set overall.
347:16 - This approach is used a lot in logistic regression
when you have a binary outcome. And in general,
347:23 - smaller values are considered better fit.
Now, as I mentioned there are some variations
347:28 - of this. I like to think of variations of
chocolate. The -2 log likelihood there’s
347:34 - the Akaike Information Criterion (AIC) and
the Bayesian Information Criterion (BIC) and
347:38 - what both of these do, they adjust for the
number of predictors. Because obviously you're
347:41 - going to have a huge number of predictors,
you’re going to get a really good fit. But
347:46 - you're probably going to have what is called
overfitting, where your model is tailored
347:51 - to specifically to the data you currently
have and that doesn’t generalize well. These
347:56 - both attempt to reduce the effect of overfitting.
Then there’s chi-squared again. It’s actually
348:00 - a lower case Greek c, looks like an x and
chi-squared is used for examining the deviations
348:05 - between two datasets. Specifically between
the observed dataset and the expected values
348:11 - or the model you create, we expect this many
frequencies in each category. Now, I’ll
348:17 - just mention when I go into the store there’s
a lot of other choices, but these are some
348:22 - of the most common standards, particularly
the R2. And I just want to say, in sum, there
348:27 - are many different ways to assess the fit
that corresponds between a model and your
348:30 - data. And the choices effect the model, you
know especially are you getting penalized
348:35 - for throwing in too many variables relative
to your number of cases? Are you dealing with
348:41 - a quantitative or binary outcome? Those things
all matter, and so the most important thing
348:48 - as always, my standing advice is keep your
goals in mind and choose a method that seems
348:54 - to fit best with your analytical strategy
and the insight you're trying to get from
349:01 - your data. The “Statistics and Data Science”
offers a lot of different choices. One of
349:09 - the most important is going to be feature
selection, or the choice of variables to include
349:16 - in your model. It’s sort of like confronting
this enormous range of information and trying
349:19 - to choose what matters most. Trying to get
the needle out of the haystack. The goal of
349:25 - feature selection is to select the best features
or variables and get rid of uninformative/noisy
349:31 - variables and simplify the statistical model
that you are creating because that helps avoid
349:36 - overfitting or getting a model that works
too well with the current data and works less
349:41 - well with other data. The major problem here
is Multicollinearity, a very long word. That
349:46 - has to do with the relationship between the
predictors and the model. I’m going to show
349:51 - it to you graphically here. Imagine here for
instance, we’ve got a big circle here to
349:59 - represent the variability in our outcome variable;
we’re trying to predict it. And we’ve
350:04 - got a few predictors. So we’ve got Predictor
# 1 over here and you see it’s got a lot
350:09 - of overlap, that’s nice. Then we’ve got
predictor #2 here, it also has some overlap
350:13 - with the outcome, but it’s also overlaps
with Predictor 1. And then finally down here,
350:20 - we’ve got Predictor 3, which overlaps with
both of them. And the problem rises the overlap
350:26 - between the predictors and the outcome variable.
Now, there’s a few ways of dealing with
350:30 - this, some of these are pretty common. So
for instance, there’s the practice of looking
350:34 - at probability values and regression equations,
there’s standardized coefficients and there’s
350:38 - variations on sequential regression. There
are also, there’s newer procedures for dealing
350:42 - with the disentanglement of the association
between the predictors. There’s something
350:46 - called Commonality analysis, there’s Dominance
Analysis, and there are Relative Importance
350:51 - Weights. Of course there are many other choices
in both the common and the newer, but these
350:59 - are just a few that are worth taking a special
look at. First, is P values or probability
351:05 - values. This is the simplest method, because
most statistical packages will calculate probability
351:12 - values for each predictor and they will put
little asterisks next to it. And so what you’re
351:22 - doing is you’re looking at the p-values;
the probabilities for each predictor or more
351:26 - often the asterisks next to it, which sometimes
give it the name of Star Search. You're just
351:32 - kind of cruising through a large output of
data, just looking for the stars or asterisks.
351:37 - This is fundamentally a problematic approach
for a lot of reasons. The problem here, is
351:43 - your looking individually and it inflates
false positives. Say you have 20 variables.
351:48 - Each is entered and tested with an alpha or
a false positive of 5%. You end up with nearly
351:56 - a 65% chance of a least one false positive
in there. That’s distorted by sample size,
352:00 - because with a large enough sample anything
can become statistically significant. And
352:03 - so, relying on p-values can be a seriously
problematic approach. Slightly better approach
352:08 - is to use Betas or Standardized regression
coefficients and this is where you put all
352:17 - the variables on the same scale. So, usually
standardized from zero and then to either
352:22 - minus 1/plus 1 or with a standardized deviation
of 1. The trick is though, they’re still
352:29 - in the context of each other and you can’t
really separate them because those coefficients
352:34 - are only valid when you take that group of
predictors as a whole. So, one way to try
352:40 - and get around that is to do what they call
stepwise procedures. Where you look at the
352:46 - variables in sequence, there’s several versions
of sequential regression that’ll allow you
352:51 - to do that. You can put the variables into
groups or blocks and enter them in blocks
352:57 - and look at how the equation changes overall.
You can examine the change in fit in each
353:03 - step. The problem with a stepwise procedure
like this, is it dramatically increases the
353:07 - risk of overfitting which again is a bad thing
if you want to generalize your data. And so,
353:13 - to deal with this, there is a whole collection
of newer methods, a few of them include commonality
353:17 - analysis, which provides separate estimates
for the unique and shared contributions of
353:21 - each variable. Well, that’s a neat statistical
trick but the problem is, it just moves the
353:29 - problem of disentanglement to the analyst,
so you’re really not better off then you
353:32 - were as far as I can tell. There’s dominance
analysis, which compares every possible subset
353:36 - of Predictors. Again, sounds really good,
but you have the problem known as the combinatorial
353:40 - explosion. If you have 50 variables that you
could use, and there are some that have millions
353:46 - of variables, with 50 variables, you have
over 1 quadrillion possible combinations,
353:49 - you're not going to finish that in your lifetime.
And it’s also really hard to get things
353:53 - like standard errors and perform inferential
statistics with this kind of model. Then there’s
353:54 - also something that’s even more recent than
these others and that’s called relative
353:55 - importance weights. And what that does is
creates a set of orthogonal predictors or
353:56 - uncorrelated with each other, basing them
off of the originals and then it predicts
353:57 - the scores and then it can predict the outcome
without the multicollinear because these new
353:58 - predictors are uncorrelated. It then rescales
the coefficients back to the original variables,
353:59 - that’s the back-transform. Then from that
it assigns relative importance or a percentage
354:00 - of explanatory power to each predictor variable.
Now, despite this very different approach,
354:01 - it tends to have results that resemble dominance
analysis. It’s actually really easy to do
354:02 - with a website, you just plug in your information
and it does it for you. And so that is yet
354:03 - another way of dealing with a problem multicollinearity
and trying to disentangle the contribution
354:04 - of different variables. In sum, let’s say
this. What you’re trying to do here, is
354:05 - trying to choose the most useful variables
to include into your model. Make it simpler,
354:06 - be parsimonious. Also, reduce the noise and
distractions in your data. And in doing so,
354:07 - you're always going to have to confront the
ever present problem of multicollinearity,
354:08 - or the association between the predictors
in your model with several different ways
354:09 - of dealing with that. The next step in our
discussion of "Statistics and the Choices
354:10 - you have to Make", concerns common problems
in modeling. And I like to think of this is
354:11 - the situation where you're up against the
rock and the hard place and this is where
354:12 - the going gets very hard. Common problems
include things like Non-Normality, Non-Linearity,
354:13 - Multicollinearity and Missing Data. And I’ll
talk about each of these. Let’s begin with
354:14 - Non-Normality. Most statistical procedures
like to deal with nice symmetrical, unimodal
354:15 - bell curves, they make life really easy. But
sometimes you get really skewed distribution
354:16 - or you get outliers. Skews and outliers, while
they happen pretty often, they’re a problem
354:17 - because they distort measures like the mean
gets thrown off tremendously when they have
354:18 - outliers. And they throw off models because
they assume the symmetry and the unimodal
354:19 - nature of a normal distribution. Now, one
way of dealing with this as I’ve mentioned
354:20 - before is to try transforming the data, taking
the logarithm, try something else. But another
354:21 - problem may be that you have mixed distributions,
if you have a bimodal distribution, maybe
354:22 - what you really have here is two distributions
that got mixed together and you may need to
354:23 - disentangle them through exploring your data
a little bit more. Next is Non-Linearity.
354:24 - The gray line here is the regression line,
we like to put straight lines through things
354:25 - because it makes the description a lot easier.
But sometimes the data is curved and this
354:26 - is you have a perfect curved relationship
here, but a straight line doesn’t work with
354:27 - that. Linearity is a very common assumption
of many procedures especially regression.
354:28 - To deal with this, you can try transforming
one or both of the variables in the equation
354:29 - and sometimes that manages to straighten out
the relationship between the two of them.
354:30 - Also, using Polynomials. Things that specifically
include curvature like squares and cubed values,
354:31 - that can help as well. Then there’s the
issues of multicollinearity, which I’ve
354:32 - mentioned previously. This is when you have
correlated predictors, or rather the predictors
354:33 - themselves are associated to each other. The
problem is, this can distort the coefficients
354:34 - you get in the overall model. Some procedures,
it turns out are less affected by this than
354:35 - others, but one overall way of using this
might be to simply try and use fewer variables.
354:36 - If they’re really correlated maybe you don’t
need all of them. And there are empirical
354:37 - ways to deal with this, but truthfully, it’s
perfectly legitimate to use your own domain
354:39 - expertise and your own insight to the problem.
To use your theory to choose among the variables
354:44 - that would be the most informative. Part of
the problem we have here, is something called
354:49 - the Combinatorial Explosion. This is where
combinations of variables or categories grow
354:55 - too fast for analysis. Now, I’ve mentioned
something about this before. If you have 4
355:01 - variables and each variable has two categories,
then you have 16 combinations, fine you can
355:06 - try things 16 different ways. That’s perfectly
doable. If you have 20 variables with five
355:13 - categories; again that’s not to unlikely,
you have 95 trillion combinations, that’s
355:19 - a whole other ball game, even with your fast
computer. A couple of ways of dealing with
355:25 - this, #1 is with theory. Use your theory and
your own understanding of the domain to choose
355:32 - the variables or categories with the greatest
potential to inform. You know what you’re
355:37 - dealing with, rely on that information. Second
is, there are data driven approaches. You
355:42 - can use something called a Markov chain Monte
Carlo model to explore the range of possibilities
355:48 - without having to explore the range of possibilities
of each and every single one of your 95 trillion
355:52 - combinations. Closely related to the combinatorial
explosion is the curse of dimensionality.
355:57 - This is when you have phenomena, you’re
got things that may only occur in higher dimensions
356:02 - or variable sets. Things that don’t show
up until you have these unusual combinations.
356:09 - That may be true of a lot of how reality works,
but the project of analysis is simplification.
356:16 - And so you’ve got to try to do one or two
different things. You can try to reduce. Mostly
356:22 - that means reducing the dimensionality of
your data. Reduce the number of dimensions
356:27 - or variables before you analyze. You're actually
trying to project the data onto a lower dimensional
356:33 - space, the same way you try to get a shadow
of a 3D object. There’s a lot of different
356:37 - ways to do that. There’s also data driven
methods. And the same method here, a Markov
356:43 - chain Monte Carlo model, can be used to explore
a wide range of possibilities. Finally, there
356:50 - is the problem of Missing Data and this is
a big problem. Missing data tends to distort
356:56 - analysis and creates bias if it’s a particular
group that’s missing. And so when you're
357:04 - dealing with this, what you have to do is
actually check for patterns and missingness,
357:08 - you create new variables that indicates whether
or not a variable is missing and then you
357:12 - see if that is associated with any of your
other variables. If there’s not strong patterns,
357:17 - then you can impute missing values. You can
put in the mean or the median, you can do
357:22 - Regression Imputation, something called Multiple
Imputation, a lot of different choices. And
357:27 - those are all technical topics, which we will
have to talk about in a more technically oriented
357:31 - series. But for right now, in terms of the
problems that can come up during modeling,
357:36 - I can summarize it this way. #1, check your
assumptions at every step. Make sure that
357:42 - the data have the distribution that you need,
check for the effects of outliers, check for
357:47 - ambiguity and bias. See if you can interpret
what you have and use your analysis, use data
357:52 - driven methods but also your knowledge of
the theory and the meaning of things in your
357:58 - domain to inform your analysis and find ways
of dealing with these problems. As we continue
358:02 - our discussion of “Statistics and the Choices
that are Made”, one important consideration
358:03 - is Model Validation. And the idea here is
that as you are doing your analysis, are you
358:04 - on target? More specifically, the model that
you create through regression or whatever
358:05 - you do, your model fits the sample beautifully,
you’ve optimized it there. But, will it
358:06 - work well with other data? Fundamentally,
this is the question of Generalizability,
358:07 - also sometimes called Scalability. Because
you are trying to apply in other situations,
358:08 - and you don’t want to get too specific or
it won’t work in other situations. Now,
358:09 - there are a few general ways of dealing with
this and trying to get some sort of generalizability.
358:10 - #1 is Bayes; a Bayesian approach. Then there’s
Replication. Then there’s something called
358:11 - Holdout Validation, then there is Cross-Validation.
I’ll discuss each one of these very briefly
358:12 - in conceptual terms. The first one is Bayes
and the idea here is you want to get what
358:13 - are called Posterior Probabilities. Most analyses
give you the probability value for the data
358:14 - given; the hypothesis, so you have to start
with an assumption about the hypothesis. But
358:15 - instead, it’s possible to flip that around
by combining it with special kind of data
358:16 - to get the probability of the hypothesis given
the data. And that is the purpose of Bayes
358:17 - theorem; which I’ve talked about elsewhere.
Another way of finding out how well things
358:18 - are going to work is through Replication.
That is, do the study again. It’s considered
358:19 - the gold standard in many different fields.
The question is whether you need an exact
358:20 - replication or if a conceptual one that is
similar in certain respects. You can argue
358:21 - for both ways, but one thing you do want to
do is when you do a replication then you actually
358:22 - want to combine the results. And what’s
interesting is the first study can serve as
358:23 - the Bayesian prior probability for the second
study. So you can actually use meta-analysis
358:24 - or Bayesian methods for combining the data
from the two of them. Then there’s hold
358:25 - out validation. This is where you build your
statistical model on one part of the data
358:26 - and you test it on the other. I like to think
of it as the eggs in separate baskets. The
358:27 - trick is that you need a large sample in order
to have enough to do these two steps separately.
358:28 - On the other hand, it’s also used very often
in data science competitions, as a way of
358:29 - having a sort of gold standard for assessing
the validity of a model. Finally, I’ll mention
358:30 - just one more and that’s Cross-Validation.
Where you use the same data for training and
358:31 - for testing or validating. There’s several
different versions of it, and the idea is
358:32 - that you’re not using all the data at once,
but you’re kind of cycling through and weaving
358:33 - the results together. There’s Leave-one-out,
where you leave out one case at a time, also
358:34 - called LOO. There’s Leave-p-out, where you
leave out a certain number at each point.
358:35 - There’s k-fold where you split the data
into say for instance 10 groups and you leave
358:36 - out one and you develop it on the other nine,
then you cycle through. And there’s repeated
358:37 - random subsampling, where you use a random
process at each point. Any of those can be
358:38 - used to develop the model on one part of the
data and tested on another and then cycle
358:39 - through to see how well it holds up on different
circumstances. And so in sum, I can say this
358:40 - about validation. You want to make your analysis
count by testing how well your model holds
358:41 - up from the data you developed it on, to other
situations. Because that is what you are really
358:42 - trying to accomplish. This allows you to check
the validity of your analysis and your reasoning
358:43 - and it allows you to build confidence in the
utility of your results. To finish up our
358:44 - discussion of “Statistics and Data Science”
and the choices that are involved, I want
358:45 - to mention something that really isn’t a
choice, but more an attitude. And that’s
358:46 - DIY, that’s Do it yourself. The idea here
is, you know really you just need to get started.
358:47 - Remember data is democratic. It’s there
for everyone, everybody has data. Everybody
358:48 - works with data either explicitly or implicitly.
Data is democratic, so is Data Science. And
358:49 - really, my overall message is You can do it!
You know, a lot of people think you have to
358:50 - be this cutting edge, virtual reality sort
of thing. And it’s true, there’s a lot
358:51 - of active development going on in data science,
there’s always new stuff. The trick however
358:52 - is, the software you can use to implement
those things often lags. It’ll show up first
358:53 - in programs like R and Python, but as far
as it showing up in a point click program
358:54 - that could be years. What’s funny though,
is often these cutting edge developments don’t
358:55 - really make much of a difference in the results
of the interpretation. They may in certain
358:56 - edge cases, but usually not a huge difference.
So I’m just going to say analyst beware.
358:57 - You don’t have to necessarily do it, it’s
pretty easy to do them wrong and so you don’t
358:58 - have to wait for the cutting edge. Now, that
being said, I do want you to pay attention
358:59 - to what you are doing. A couple of things
I have said repeatedly is “Know your goal”.
359:00 - Why are you doing this study? Why are you
analyzing the data, what are you hoping to
359:01 - get out of it? Try to match your methods to
your goal, be goal directed. Focus on the
359:02 - usability; will you get something out of this
that people can actually do something with.
359:03 - Then, as I’ve mentioned with that Bayesian
thing, don’t get confused with probabilities.
359:04 - Remember that priors and posteriors are different
things just so you can interpret things accurately.
359:05 - Now, I want to mention something that’s
really important to me personally. And that
359:06 - is, beware the trolls. You will encounter
critics, people who are very vocal and who
359:07 - can be harsh and grumpy and really just intimidating.
And they can really make you feel like you
359:08 - shouldn’t do stuff because you’re going
to do it wrong. But the important thing to
359:09 - remember is that the critics can be wrong.
Yes, you’ll make mistakes, everybody does.
359:10 - You know, I can’t tell you how many times
I have to write my code more than once to
359:11 - get it to do what I want it to do. But in
analysis, nothing is completely wasted if
359:12 - you pay close attention. I’ve mentioned
this before, everything signifies. Or in other
359:13 - words, everything has meaning. The trick is
that meaning might not be what you expected
359:14 - it to be. So you’re going to have to listen
carefully and I just want to reemphasize,
359:15 - all data has value. So make sure your listening
carefully. In sum, let’s say this: no analysis
359:16 - is perfect. The real questions is not is your
analysis perfect, but can you add value? And
359:17 - I’m sure that you can. And fundamentally,
data is democratic. So, I’m going to finish
359:18 - with one more picture here and that is just
jump write in and get started. You’ll be
359:19 - glad you did. To wrap up our course “Statistics
and Data Science”, I want to give you a
359:20 - short conclusion and some next steps. Mostly
I want to give a little piece of advice I
359:21 - learned from a professional saxophonist, Kirk
Whalum. And he says there’s “There’s
359:22 - Always Something To Work On”, there’s
always something you can do to try things
359:23 - differently to get better. It works when practicing
music, it also works when you're dealing with
359:24 - data. Now, there are additional courses, here
at datalabb.cc that you might want to look
359:25 - at. They are conceptual courses, additional
high-level overviews on things like machine
359:26 - learning, data visualization and other topics.
And I encourage you to take a look at those
359:27 - as well, to round out your general understanding
of the field. There are also however, many
359:28 - practical courses. These are hands on tutorials
on these statistical procedures I’ve covered
359:29 - and you learn how to do them in R, Python
and SPSS and other programs. But whatever
359:30 - you’re doing, keep this other little piece
of advice from writers in mind, and that is
359:31 - “Write what you know”. And I’m going
to say it this way. Explore and analyze and
359:32 - delve into what you know. Remember when we
talked about data science and the Venn Diagram,
359:33 - we’ve talked about the coding and the stats.
But don’t forget this part on the bottom.
359:34 - Domain expertise is just as important to good
data science as the ability to work with computer
359:35 - coding and the ability to work with the numbers
and quantitative skills. But also, remember
359:36 - this. You don’t have to know everything,
your work doesn’t have to be perfect. The
359:37 - most important thing is just get started,
you’ll be glad you did. Thanks for joining
359:38 - me and good luck!