00:00 - [Music]
00:03 - this video will cover introductory
00:06 - information about business analytics
00:07 - including a definition of business
00:09 - analytics the wisdom hierarchy data
00:13 - sources and business analytics terms
00:16 - applications history and uses business
00:21 - analytics is the process of transforming
00:23 - data into insights that support improve
00:26 - and/or automate business decisions the
00:29 - data can be of many types and from a
00:31 - variety of sources and there are many
00:33 - techniques and software packages that
00:35 - can be used for an analysis a simple way
00:38 - to think of business analytics is that
00:39 - it's the tools and processes used to
00:42 - find value in data to transform the raw
00:44 - data into information that can be acted
00:46 - upon a common method of understanding
00:49 - the relationship between data
00:51 - information knowledge and wisdom is by
00:53 - using a pyramid this graphic is called
00:55 - the di kW pyramid or the wisdom
00:58 - hierarchy and it illustrates that data
01:00 - is the foundation upon which decisions
01:02 - can be made information is defined in
01:04 - terms of data knowledge is defined in
01:06 - terms of information and wisdom is
01:08 - defined in terms of knowledge data are
01:11 - numbers or text without any context
01:13 - information provides meaning from data
01:15 - often combining multiple data points to
01:18 - produce a tangible idea knowledge
01:20 - provides context from the information
01:22 - making it directly applicable to a
01:24 - situation wisdom applies the knowledge
01:26 - to make a decision the original data has
01:29 - become useful enabling an action to be
01:31 - taken so where does data come from data
01:34 - can come from a variety of sources both
01:36 - internal and external internal data is
01:38 - collected by businesses and stored
01:40 - within their own servers this data can
01:42 - be generated in a number of ways either
01:44 - by physical objects such as sensors or
01:46 - barcodes or by using computer software
01:48 - such as websites while collecting and
01:51 - storing proprietary data sources is
01:53 - still very common there are now many
01:55 - external data sources that businesses
01:57 - can use public domain data sources such
01:59 - as government surveys or social media
02:01 - posts can be accessed and there are also
02:03 - many services that offer paid data
02:05 - sources like stock market or weather
02:07 - data these external data sources can be
02:10 - combined with a company's proprietary
02:11 - data to build a more complete
02:13 - picture of reality the amount of data
02:15 - that is being generated and collected is
02:17 - growing exponentially this growth is
02:19 - occurring due to the similar exponential
02:21 - trend in computing power along with a
02:23 - decrease in costs for digital storage
02:25 - because so much new data is being
02:27 - created and captured every year there's
02:29 - a corresponding growth of demand for
02:31 - business analysts there are many terms
02:34 - that are synonymous or semi synonymous
02:36 - to business analytics in the past few
02:39 - years
02:39 - data science has become the most common
02:41 - term to be used to describe this field
02:43 - other terms are often used
02:45 - interchangeably such as business
02:47 - intelligence big data data mining
02:50 - knowledge discovery and machine learning
02:52 - while there are many discussions about
02:54 - which term to use in which scenario all
02:56 - of the terms refer to the overall
02:58 - concept of using data to make better
03:00 - decisions or better products business
03:03 - analytics is an interdisciplinary
03:04 - subject based heavily on math and
03:06 - statistics it uses computer science
03:09 - principles and algorithms the math and
03:12 - computer science concepts are applied to
03:14 - a specific subject
03:17 - Business Analytics has applications in
03:19 - every field data on all our clicks on
03:23 - the Internet are used by retailers to
03:24 - figure out our preferences and provide
03:26 - targeted advertising analyzing data on
03:29 - past marketing campaigns provides a
03:31 - useful base to determine who to target
03:33 - for the next round supply chains in
03:36 - almost all industries have become far
03:38 - more efficient due to analytics
03:40 - organizations use historical data to
03:42 - figure out optimal numbers for staffing
03:44 - and hiring companies use demand data to
03:47 - determine optimal prices and
03:49 - professional sports teams use analytics
03:52 - such as when they look at player
03:53 - performance to determine who would be
03:55 - the best draft pick no matter which
03:57 - industry you work in there's room for
03:59 - analytics to add value although the
04:02 - popularity of analytics has recently
04:03 - peaked the field is nowhere as new as
04:06 - certain companies or people make it out
04:07 - to be since the computer was invented it
04:10 - was being used to process data to solve
04:12 - problems from decoding messages in World
04:14 - War 2 to generating weather forecasts in
04:17 - 1950 to modeling credit risk in 1958
04:21 - of course these tasks involved enormous
04:23 - computational costs and only
04:25 - organizations with the most resources
04:27 - could attempt them toward the end of the
04:29 - 20th century as computing power became
04:32 - more affordable more organizations began
04:34 - collecting and storing data the types of
04:37 - analytical projects transitioned from
04:38 - being historical in nature to real-time
04:40 - in 1992 the first real-time credit card
04:44 - fraud system was introduced then the
04:46 - first analytically centric companies
04:48 - emerged companies such as Google used
04:51 - data to build their core product while
04:54 - other companies such as Amazon use
04:56 - analytical techniques to earn market
04:57 - share from competitors the rapid
05:00 - ascension of these tech companies has
05:01 - led to an arms race where all businesses
05:04 - have become committed to analytics
05:06 - businesses use analytics to gain an edge
05:08 - on their competitors and increase
05:10 - profits the three main areas in which
05:12 - they do this are competition to increase
05:14 - revenue from products or services sold
05:17 - efficiency to reduce the costs of
05:19 - resources or internal processes and
05:21 - customer satisfaction to improve the
05:24 - customer experience and encourage
05:25 - customer loyalty here's a case study as
05:28 - an example loyalty cards are
05:31 - used by grocery stores to uniquely
05:32 - identify their customers by requiring a
05:35 - loyalty card to obtain special discounts
05:37 - in the store the grocer can isolate
05:38 - habits of each customer and then provide
05:41 - customers with customized promotions to
05:43 - increase spending
05:44 - when a customer stops frequenting the
05:46 - store the grocer can mail coupons with
05:48 - aggressive offers the layout of a
05:50 - grocery store is constantly being
05:52 - changed to maximize customer spending
05:54 - this is why the milk section is always
05:56 - on the opposite side of the produce
05:57 - section so customers will have to
05:59 - traverse past every aisle to get to the
06:01 - two most commonly bought items each
06:04 - shelf is also analyzed to find the ideal
06:06 - arrangement more expensive items are
06:08 - typically placed at or around eye level
06:10 - while the cheaper products will be on
06:12 - the top or bottom shelves optimizing
06:15 - prices is another analytical technique
06:17 - used to maximize customer spending many
06:20 - grocery stores will have what are called
06:21 - loss leaders products that are very
06:23 - cheap to draw our customers into the
06:25 - store where they will inevitably spend
06:26 - more on other overpriced items grocers
06:29 - will also find the ideal times and
06:31 - prices to markdown expiring products
06:33 - preventing the product from being thrown
06:35 - away at a complete loss this concludes
06:38 - our introductory video about business
06:40 - analytics today we covered a definition
06:42 - of business analytics the wisdom
06:44 - hierarchy data sources and business
06:48 - analytics terms applications history and
06:51 - uses
06:54 - [Music]
07:13 - this video will cover introductory
07:16 - information about business analytics
07:18 - including the role of the business
07:20 - analyst what makes a business analyst
07:22 - successful and an overview of business
07:25 - analytics tools project outcomes and the
07:28 - analytical process there are three main
07:32 - reasons business analytics as an
07:33 - enticing career choice the first is that
07:36 - there's a high demand for business
07:37 - analysts and the relatively low supply
07:39 - of skilled workers means that salaries
07:41 - are higher in this field
07:42 - another reason is for the challenge of
07:44 - solving interesting problems
07:46 - analysts are typically people who are
07:48 - interested in solving complex puzzles
07:50 - finally those who are curious about how
07:52 - things work can use their skills in
07:54 - analyzing data to uncover previously
07:56 - unknown truths a business analyst can
07:59 - take many roles depending on the data
08:00 - and the type of project the most common
08:03 - roles are that of an interpreter in
08:04 - which the analyst uses descriptive
08:06 - analytics to tell the story of what
08:08 - happened an oracle and which analysts
08:11 - use predictive analytics to predict
08:13 - future events and a console in which the
08:15 - analyst uses prescriptive analytics to
08:18 - provide advice on the best course of
08:19 - action an analyst becomes successful due
08:22 - to a combination of hard and soft skills
08:24 - the hard skills are more tangible and
08:27 - refer to what the analyst can do with
08:29 - what tools while the soft skills are
08:31 - less flashy on a resume but equally or
08:33 - even more important than the hard skills
08:34 - analytical tools can be separated into
08:38 - two categories software that requires
08:40 - coding and software in a graphical user
08:42 - interface or GUI that is based on
08:45 - point-and-click interaction the main
08:47 - benefit of writing code is that it
08:48 - allows for more flexibility there are
08:50 - more features and it allows for more
08:52 - possibilities the drawback of coding is
08:54 - the extended learning curve within the
08:56 - last decade there have been many new GUI
08:58 - programs that make analytics easier to
09:00 - implement without the need for writing
09:01 - code programs such as tableau Alteryx
09:04 - and rapid miner have started gaining
09:06 - market share going along with older
09:08 - tools such as SAS Enterprise guide but
09:10 - none of these tools have yet to replace
09:12 - the overwhelming popularity of code
09:14 - based software such as SAS r Python or
09:17 - SQL there are many different goals that
09:19 - an analytics project can strive for
09:21 - typically these goals fit into one of
09:23 - two categories the first is providing
09:25 - information about a business
09:26 - such as reports and dashboards for
09:28 - business stakeholders reports or
09:30 - presentations provide one-time insights
09:32 - to explain events that have occurred and
09:34 - predict future events and dashboards are
09:36 - used by stakeholders for ongoing
09:38 - monitoring of key aspects of the
09:40 - business the second category is the
09:42 - production of analytical products in
09:44 - these types of projects the business's
09:46 - data becomes the input for a complex
09:47 - process that automatically produces in
09:49 - action this can take the form of
09:51 - features that offer a better experience
09:53 - for consumers for instance Amazon has an
09:55 - automated algorithm that determines
09:57 - products you might like to buy
09:58 - analytical products can also be built to
10:00 - make internal business processes more
10:02 - efficient an example of this is how
10:04 - credit card companies test every
10:06 - transaction for the probability of fraud
10:07 - an analytical project should start with
10:10 - the goals well-defined
10:11 - very rarely our project started to
10:13 - simply explore the data or find hidden
10:15 - truths collecting an inventory of all
10:17 - the relevant data sources is also an
10:19 - important step in the beginning of a
10:21 - project finally every analysis should
10:23 - begin with an effort to better
10:24 - understand the data this should be done
10:26 - before any analytical techniques are
10:28 - used simply observe the data files and
10:30 - write down a list of observations
10:32 - questions and any other ideas you have
10:34 - this process called creating disfluency
10:36 - enhances the data dictionary and helps
10:39 - the analyst internalize elements of the
10:40 - data cognitive disability nabel's
10:44 - students who take lecture notes by
10:45 - writing to retain more of the material
10:47 - than students who type notes even though
10:49 - those who type can take notes more
10:50 - efficiently sometimes the more work we
10:53 - have to do to process the information
10:54 - the better we can understand the
10:56 - information using this principle at the
10:58 - beginning of a project before using any
11:00 - advanced analytical techniques enhances
11:02 - the analysts capability to understand
11:04 - the data after the initial stages of a
11:06 - project there are a few more steps in
11:08 - the analytical process the majority of
11:10 - time is spent exploring and preparing
11:12 - data and the exploration stage the
11:14 - analyst learns more about the variables
11:16 - including distributions and frequency of
11:18 - values and also identifies variables
11:20 - with missing or null values in the
11:22 - preparation stage the data becomes more
11:24 - useful as variables are transformed and
11:26 - anomalous values are rectified this is
11:28 - referred to as data cleaning another
11:30 - common task during this stage is to join
11:32 - data into as few sources as possible
11:34 - perhaps one of the most valuable
11:36 - techniques to use during the preparation
11:38 - stage is called feature engineering
11:40 - in which the analysts transform certain
11:42 - variables into new variables containing
11:43 - slightly different data this allows
11:45 - hidden aspects of variables to be
11:47 - analyzed for example a data set with a
11:49 - date variable can have a new variable
11:51 - added to determine whether that date is
11:53 - on a weekday or the weekend and this new
11:55 - variable could add important information
11:57 - that was not readily available in the
11:58 - original data source this is one of the
12:00 - areas of the analytical process where
12:02 - creativity is needed the last two steps
12:04 - in the analytical process are to build
12:06 - models and then to put these models into
12:08 - production a model is a type of
12:10 - mathematical equation that describes
12:11 - relationships among variables in a
12:13 - dataset often for the purpose of
12:15 - predicting an outcome by putting a model
12:17 - in production an automated decision can
12:19 - be made when new data is observed in
12:21 - some cases models are not the goal of a
12:23 - project rather the goal is to analyze
12:25 - data and communicate the findings in an
12:27 - analytical report presentation or
12:28 - dashboard visualization of the data is
12:31 - also a key component throughout the
12:32 - entire analytical process as the analyst
12:34 - attempts to learn more about the data
12:36 - this concludes our introductory video
12:38 - about business analytics today we
12:40 - covered the role of the business analyst
12:42 - what makes a business analyst successful
12:45 - and an overview of tools project
12:47 - outcomes and the analytical process
12:52 - [Music]
13:10 - this video will cover some of the
13:12 - foundations of Business Analytics
13:13 - selecting filtering and sorting
13:16 - there are many synonymous terms to
13:18 - describe the aspects of a typical data
13:20 - file which can be referred to as a table
13:22 - spreadsheet data set or data source
13:24 - along the horizontal axis are the
13:26 - variables which can also be called
13:28 - fields attributes or columns along the
13:31 - vertical axis are the observations also
13:33 - referred to as records tuples or rows
13:35 - fields that are sparsely populated
13:37 - meaning that a high percentage of
13:39 - records is missing should not be
13:40 - selected
13:44 - fields with redundant values meaning
13:46 - that a high percentage of records have
13:48 - the same value should also not be
13:49 - selected techniques to identify these
13:51 - fields vary for now a visual inspection
13:54 - of the first few records can be used
13:56 - from this table we can remove the state
13:58 - column because all of its values are the
13:59 - same we can remove the religion column
14:02 - because it is sparsely populated our
14:03 - table now depicts only the variables of
14:05 - interest once the variables of interest
14:07 - have been selected they can be renamed
14:09 - if needed and assigned the proper data
14:10 - type there are two common variable types
14:12 - numeric and categorical numeric
14:14 - variables include discrete variables
14:16 - which can include whole numbers used for
14:18 - counting and IDs that represent a unique
14:20 - entity and continuous variables which
14:22 - are numbers with decimals used for
14:24 - measuring categorical variables include
14:25 - text which is any combination of letters
14:27 - numbers and symbols and strings or
14:29 - characters and boolean or binary
14:31 - variables which contain only one of two
14:33 - possible values variables that can be
14:35 - treated as either numeric or categorical
14:37 - include dates such as date time date or
14:40 - time and spatial objects which show
14:42 - location such as latitude and longitude
14:45 - here are the variable types for our
14:46 - table person ID is a discrete numeric
14:49 - variable gender and city are string
14:51 - variables weight is a fixed decimal
14:53 - variable date of birth is a date and
14:55 - student is boolean software packages can
14:57 - automatically assign fields with data
14:59 - types and sometimes these can be
15:01 - assigned incorrectly examination of
15:03 - these data types is necessary to ensure
15:04 - proper utilization further downstream
15:06 - sometimes data formats require advanced
15:09 - data transformation and extraction
15:11 - techniques for example dates can have
15:13 - varying formats that can be interpreted
15:14 - as strings and parts of these strings
15:16 - need to be separated before the software
15:18 - can identify as a date field for now
15:20 - we'll assume that you can change a
15:21 - fields datatype without these advanced
15:23 - techniques the final aspect of the
15:25 - Select step is to assign the proper size
15:27 - for each variable many software packages
15:29 - can do this automatically but it's still
15:31 - a good idea to review the sizes because
15:32 - the size of each field has a direct
15:34 - impact on the amount of storage required
15:36 - for a data set specifying these can save
15:38 - system resources and processing time the
15:40 - common issue is for one record that
15:41 - contains many more characters than the
15:43 - typical record to cause the size to be
15:45 - much larger than need be a visual
15:46 - inspection of the table can highlight
15:48 - these instances but beware that changing
15:50 - a variable size can truncate or cut off
15:52 - some of the values while selecting
15:54 - limits a data set size by omitting
15:56 - certain columns a filter limits
15:58 - sighs by omitting certain rows a filter
16:00 - is also known as a condition subset or
16:03 - in SQL a where clause and it's commonly
16:05 - used for investigative purposes it's
16:07 - important to be aware of the information
16:09 - contained within the row of a data table
16:11 - also commonly referred to as a record or
16:13 - observation a row defines the level of
16:15 - detail that is contained in the data set
16:17 - for this example each row represents one
16:19 - person an ID fields such as person ID in
16:21 - which no rows contain the same ID can be
16:24 - used to determine that the tables level
16:25 - of detail is a person if there are
16:27 - multiple rows for the same ID we know
16:29 - that the rows reflect data from the same
16:31 - person in this example the level of
16:32 - detail is more granular it shows a
16:34 - record of one person's weight by day
16:36 - finding and understanding the level of
16:37 - detail for a table is necessary before
16:40 - analyzing the data after determining the
16:42 - tables level of detail you should check
16:43 - for duplicate observations these are
16:45 - rows of data in which all values are
16:47 - exactly the same as another row in some
16:49 - cases duplicate observations may be
16:51 - legitimate but usually these are
16:53 - erroneous and need to be removed here's
16:55 - our table with the erroneous observation
16:56 - removed this table is filtered to show
16:58 - only records of people who live in
17:00 - Raleigh let's try another filter here's
17:02 - our original data set again this table
17:04 - is filtered to show only records of
17:06 - people who weigh more than 180 pounds
17:08 - filters are different variables can be
17:11 - applied together combining above
17:12 - examples we can filter the original
17:14 - table for people who live in Raleigh and
17:16 - weigh more than 180 this results in only
17:18 - one observation the next step is sorting
17:20 - when we sort we rearrange a table by
17:22 - ordering the rows according to the
17:24 - values of one or more fields and either
17:25 - ascending or descending order here's our
17:27 - original data set it's sorted by date of
17:29 - birth in ascending order here we've
17:31 - sorted by city in ascending order and
17:33 - then by weight in descending order this
17:35 - concludes our video on selecting
17:37 - filtering and sorting
17:41 - [Music]
18:02 - this video will cover two types of
18:05 - formulas same row and multi row a
18:08 - commonly used method of data preparation
18:10 - is using existing fields to create new
18:13 - variables
18:13 - examples include adding subtracting
18:16 - multiplying dividing or applying another
18:19 - mathematical function to numeric field
18:21 - extracting and transforming substrings
18:23 - from a text field extracting truncating
18:26 - and parsing date parts from a date field
18:28 - conditional statements using if-then and
18:31 - binning to create new variables and
18:33 - comparing values of two different fields
18:36 - to create a boolean variable formulas
18:39 - can be used for adding subtracting
18:40 - multiplying or dividing two numeric
18:43 - fields in this table we apply a formula
18:46 - to create the price column which is
18:48 - calculated by dividing units sold by
18:50 - total amount formulas can also be used
18:54 - when applying a mathematical function to
18:56 - a numeric field examples of mathematical
18:58 - functions are average floor finding the
19:01 - smallest value ceiling finding the
19:03 - largest value square square root
19:05 - absolute value trigonometric functions
19:08 - and logarithmic functions in this table
19:11 - we will create an observation average
19:13 - column using the average formula which
19:15 - averaged values from the observation 1
19:17 - and observation two columns formulas can
19:21 - also be used to extract and transform
19:23 - strings from a text field there are a
19:25 - few common functions for transforming
19:27 - text and they are typically named
19:28 - differently depending on the software
19:30 - package being used the most common of
19:32 - these are upper lower use to change the
19:34 - field to all upper or lowercase
19:36 - characters concatenate which combines
19:38 - two or more strings into one field
19:40 - substring used to extract a portion of a
19:43 - string trim which can be used to remove
19:45 - certain characters usually spaces from a
19:47 - field index used to find the location of
19:50 - a certain string within a field and
19:52 - length which finds how many characters
19:53 - are in a field these are the basic
19:55 - string functions that almost every
19:57 - software package will include and can be
19:59 - combined to complete almost any string
20:01 - transformation for the following example
20:03 - we'll use two functions to transform a
20:05 - field that holds a city and state into
20:07 - two fields to extract city and state
20:10 - from the airport location field we would
20:11 - need to extract all the text before the
20:13 - comma as the city
20:14 - field and everything after the comma and
20:16 - space as the state field to do this we
20:19 - need two functions index and substring
20:21 - the index function will help us identify
20:23 - the location of the comma in the text
20:25 - field counting from the left including
20:27 - spaces this is done because the comma is
20:29 - not always in the same location then we
20:31 - use the substring function to extract
20:33 - everything before the comma as the
20:35 - formula for the city field and extract
20:37 - everything after the comma and space for
20:39 - the state field to get the city and
20:41 - state columns back into the format of
20:43 - the original field we would use a
20:44 - concatenate function we can also use
20:46 - formulas for extracting truncating and
20:48 - parsing date parts from a date field a
20:51 - date or date/time field carries more
20:53 - information than a typical variable the
20:55 - time of day the day of week the day of
20:57 - month the month of year the year number
20:59 - and the difference in the date from
21:01 - other dates such as today extracting
21:03 - this and other information out of a date
21:05 - field is a transforming technique that
21:07 - can add to the value of a data set
21:09 - during analysis and reporting the most
21:11 - heavily used date functions are today
21:13 - used to get today's date date part used
21:16 - to get a part of a date for example
21:17 - getting the month of date will return
21:19 - the numeric value of the month date
21:21 - truncate which will return a date value
21:23 - at the beginning of a period specified
21:24 - for example truncating a date at the
21:27 - month level will return the first day of
21:28 - the month for the date date ad which
21:31 - will add or subtract a certain number of
21:33 - periods to a date and date difference
21:35 - used to calculate the number of periods
21:37 - between two dates this table contains
21:39 - the major holidays for 2016 for each
21:42 - date value we can extract other features
21:44 - we can add a column with a numeric value
21:46 - to describe the day of the week
21:48 - we can subtract today's date from the
21:50 - date column to find the number of days
21:52 - remaining until the holiday occurs with
21:54 - formulas we can also compare values of
21:56 - two different fields to create a boolean
21:58 - variable a boolean variable is typically
22:00 - used to capture true/false values to
22:03 - create a boolean variable a logical
22:05 - statement or condition can be used in a
22:07 - formula this logical statement can
22:09 - compare two values of the same type
22:11 - numeric values strings and dates this
22:14 - table depicts the population of States
22:16 - we can use a formula to create a column
22:19 - that displays whether the state is on
22:20 - the East Coast we can also create a
22:22 - column to display whether the state has
22:24 - a high population formulas can also be
22:26 - used with conditional statements that
22:28 - if then and when binning to create new
22:30 - variables to create a variable with
22:32 - multiple possible outcomes conditional
22:35 - statements are needed these usually take
22:37 - the form of if condition1 is met then
22:39 - outcome 1 else if condition 2 is met
22:43 - then outcome 2 else outcome 3 while the
22:47 - conditions within these statements are
22:49 - the same as boolean formulas these
22:51 - formulas offer more flexibility as the
22:53 - analyst can assign any value when the
22:55 - condition is met and can use as many
22:57 - conditions and outcomes as needed while
22:59 - there are many applications of
23:01 - conditional statements one of the most
23:02 - common is tube in or tile numeric
23:04 - variables a process which transforms the
23:07 - continuous numeric variables into
23:09 - categorical variables in this table we
23:11 - compute sales volume with if-then logic
23:13 - if sales is greater than 40000 then high
23:17 - else if sales is greater than 20,000
23:20 - then medium else low this formula begins
23:23 - with testing the first condition if
23:25 - sales is greater than 40,000 and if it
23:28 - is true the first outcome high will be
23:30 - assigned the second condition else if
23:33 - less than 20,000 is evaluated only if
23:36 - the first condition is false if the
23:38 - second condition is true the second
23:40 - outcome medium will be assigned if
23:42 - neither the first nor second conditions
23:45 - are true then the final outcome low will
23:47 - be assigned
23:50 - sometimes data from a separate row or
23:52 - rows needs to be used when creating
23:54 - variables for this a multi row formula
23:56 - is needed a running total is the most
23:58 - basic multirow formula it adds the value
24:01 - of field of a current row to the value
24:03 - of a previous row this can be done
24:05 - across an entire data set for a
24:07 - particular variable or it can be grouped
24:09 - by certain dimensions a lag value looks
24:12 - at the data in preceding rows while lead
24:14 - values look at data and subsequent rows
24:17 - and is the opposite of lag window
24:19 - functions provide the ability to perform
24:21 - calculations like sum average and rank
24:24 - across sets of rows that are related to
24:26 - the current query row this is equivalent
24:28 - to aggregating the data set across one
24:30 - or more dimensions then joining the
24:32 - resulting data set back to the original
24:34 - this should be used carefully as values
24:36 - will be repeated for all levels of the
24:38 - dimension an example of this is in the
24:40 - transportation industry where a row
24:42 - typically represents one leg of a trip
24:44 - after sorting the data set properly the
24:46 - lag function can be used to combine rows
24:48 - so that the data describing the round
24:50 - trip is displayed total price is a
24:52 - running total of ticket price grouped by
24:54 - passenger ID
24:55 - notice how upon reaching the first row
24:57 - for a passenger ID the sum starts over
24:59 - and total price equals ticket price
25:01 - total flight time is a running total of
25:04 - flight time grouped by passenger ID lag
25:06 - of destination uses the lag function
25:08 - which doesn't consider any variables
25:10 - other than destination this means it's
25:12 - dependent on the sort order of the data
25:14 - set sum of ticket price is a window
25:16 - function which is the sum of the ticket
25:18 - price partitioned or grouped by
25:19 - passenger ID this concludes our video on
25:23 - single and multi row formulas
25:27 - [Music]
25:45 - this video will cover unions and joins
25:49 - union is also referred to as appending
25:52 - concatenating or combining two tables
25:54 - vertically or simply adding rows the
25:57 - table should have the same variable
25:59 - names types and sizes variables that are
26:01 - in only one table will receive null or
26:04 - missing values for the rows from the
26:05 - tables that do not contain them in this
26:08 - union are two tables one with three rows
26:10 - and one with one row will combine to
26:12 - create one larger table which has 4 rows
26:14 - notice the null values in the storage
26:17 - column storage was not contained in both
26:19 - original tables so the rows that do not
26:21 - contain data for storage will have null
26:23 - or missing values
26:24 - joining is also referred to as merging
26:27 - very rarely as data collected and stored
26:29 - in a format that's ideal for analysis
26:31 - typically data is stored across multiple
26:33 - tables in a relational database schema
26:35 - like you see here schemas are designed
26:38 - to optimize for storage so values that
26:40 - repeat often and take up more storage
26:42 - such as names are reduced to
26:44 - representative IDs that take up much
26:46 - less storage in a schema the tables that
26:48 - contain identifying information are
26:50 - called dimension tables and can
26:51 - sometimes be referred to as look-up
26:53 - tables our diagram has three so for
26:55 - example the product dimension table
26:57 - contains the name manufacturer and type
27:00 - of product each dimension table has a
27:02 - primary key that is used to identify a
27:04 - unique record the fact table is a record
27:06 - of events that happen for a combination
27:08 - of dimensions it's comprised of two
27:10 - things foreign keys and measures foreign
27:12 - keys map to primary keys of dimension
27:14 - tables these are the fields that will be
27:16 - used to join the tables together for
27:18 - example in this diagram the foreign keys
27:20 - are person ID store ID and Product ID
27:23 - which each connect to their own
27:24 - dimension table measures can be any type
27:27 - of variable we've already discussed in
27:29 - this diagram we have two measures units
27:31 - sold and total amount to join these
27:33 - tables together we will use an inner
27:34 - join this will return only rows that are
27:37 - contained in both tables when a fact
27:39 - table has no lore missing values for a
27:41 - dimension using an inner join will
27:43 - remove the entire row since the row may
27:45 - contain useful data and other fields
27:47 - it's better to use a left outer join to
27:49 - keep all values from the left table in
27:51 - this case the fact table and the values
27:53 - from the dimension table that match on
27:55 - the key while there are other types of
27:56 - joins as shown here in
27:58 - and left outer joins are the most
27:59 - commonly used first we will join
28:01 - transaction fact with person dimension
28:06 - next we will join the results from the
28:08 - previous join with store dimension
28:12 - finally we will join our results from
28:14 - the previous join with product dimension
28:19 - notice how each step creates a
28:21 - progressively larger table this
28:24 - concludes our video on unions and joins
28:29 - [Music]
28:43 - this video will cover a definition of
28:46 - aggregation as well as examples of
28:48 - aggregation
28:50 - aggregation is also referred to as a
28:53 - PivotTable group by statement or
28:55 - summarize aggregation transforms data
28:58 - into lower dimensions using summing
29:00 - averaging and counting the benefits of
29:03 - this are to answer basic questions of
29:05 - data sets with many different dimensions
29:07 - the most basic aggregation can be done
29:09 - on the level of detail for the table in
29:11 - this case each rows a transaction and
29:14 - the table as a whole represents sales
29:16 - for phones for quarter 1 of 2015 the
29:19 - answers can be calculated by applying a
29:21 - formula to a single field using all the
29:23 - rows in the table how many units were
29:26 - sold 9 how much total revenue 3500 $90
29:38 - how many transactions six
29:43 - how many distinct products were sold
29:47 - three
29:53 - how many customers
29:57 - three
30:02 - how many stores had sales
30:09 - for
30:13 - what was the average price
30:19 - 398 dollars and 89 cents what was the
30:22 - average number of units sold per
30:24 - transaction
30:26 - one point five what was the average
30:30 - amount spent per transaction
30:35 - 598 dollars and 33 cents
30:40 - what was the average amount spent per
30:43 - customer
30:47 - 1196 dollars and 67 cents
30:52 - pay special attention to how the average
30:54 - function works it uses the sum of the
30:57 - field divided by the number of rows so
30:59 - it can't be used in averages such as
31:01 - this where the denominator is not the
31:03 - number of rows the next questions are
31:05 - related to aggregating one dimension at
31:07 - a time and can be calculated in one step
31:09 - for each of the questions the original
31:12 - table is grouped by the dimension of
31:13 - interest either the person ID store ID
31:17 - product ID or date for each value of the
31:21 - dimension chosen calculations are
31:23 - performed using the measures the number
31:25 - of rows and the number of distinct IDs a
31:28 - new data table is the result of these
31:30 - calculations
31:38 - you
31:41 - notice that when grouping by a dimension
31:43 - the dimension is sorted in the output
31:45 - for this example we'll aggregate at the
31:47 - person level of detail to answer the
31:49 - following questions how many units to
31:52 - each person by how much money did each
31:54 - person spend for each person ID we need
31:57 - to calculate the sum of units sold and
31:59 - the sum of the total amount
32:01 - starting with person ID 1 we see that
32:04 - there were 1 plus 1 equals 2 units sold
32:08 - and 365 plus 425 equals seven hundred
32:13 - and ninety dollars spent these values
32:15 - are populated in the resulting table
32:17 - similar calculations are done for every
32:20 - person ID in the original table
32:30 - you
32:33 - aggregating at the store level of detail
32:36 - can provide basic information about
32:37 - store performance to find the number of
32:40 - transactions each store had each
32:42 - occurrence of a store ID needs to be
32:44 - counted store ID 101 has two rows in the
32:47 - original table indicating that there
32:48 - were two total transactions each store
32:51 - ID has its number of rows counted and
32:53 - the final result is the num transactions
32:55 - column in the newly created table
32:58 - aggregating at the product level of
33:00 - detail can answer basic questions about
33:01 - each product such as what was the
33:04 - average price of each product how many
33:06 - distinct customers bought each product
33:09 - starting with product ID one zero zero
33:11 - one average price is calculated as the
33:14 - sum of the total amount
33:15 - 16:25 divided by the sum of units sold
33:18 - for the final value of 406 twenty five
33:22 - is populated into the resulting table
33:24 - again be aware that using an average
33:26 - function in this case would not give us
33:28 - the expected result distinct customers
33:31 - is calculated by counting the distinct
33:33 - number of person IDs for Product ID one
33:35 - zero zero one there are two person ID 1
33:38 - and person ID 2 the calculations are
33:41 - then completed for every product ID
33:49 - the final example uses the month and
33:51 - year components of the date field to
33:53 - calculate all possible levels of
33:54 - aggregation which were shown on the
33:56 - previous slides how many units were sold
33:59 - each month how much revenue each month
34:01 - how many transactions each month what
34:04 - was the average price how many distinct
34:07 - products were sold each month
34:16 - you
34:37 - the final type of questions that can be
34:39 - asked involve combinations of multiple
34:41 - dimensions one of the most common
34:43 - combinations is to aggregate by a date
34:45 - dimension along with another dimension
34:47 - for this calculation each combination of
34:49 - the dimensions chosen for grouping are
34:51 - performed in the transaction table there
34:54 - are four such combinations transactions
34:57 - for person ID one are only in March of
34:59 - 2015
35:03 - there's only one transaction for person
35:05 - ID to occurring in February 2015
35:10 - and there are three transactions for
35:12 - person ID for two occurring in January
35:15 - and one occurring in February of 2015
35:19 - within each of these combinations each
35:22 - of the calculations are performed here
35:24 - will show you the first row as an
35:26 - example
35:33 - you
35:41 - note that the more unique dimensions a
35:44 - data table contains the more the number
35:46 - of possible levels of aggregations
35:48 - increases in this example table with
35:51 - four unique dimensions there are
35:53 - fourteen different possible levels of
35:54 - aggregation each different level of
35:56 - aggregation can offer a unique insight
35:58 - but it's best to start with two or fewer
36:01 - levels example questions that can be
36:03 - answered using the example transaction
36:05 - data set include which customers tend to
36:08 - buy which products which stores two
36:10 - customers tend to frequent what products
36:12 - sell well in what stores and during what
36:14 - months do they sell more on average
36:17 - here's one final example using multiple
36:20 - concepts that have been covered using
36:22 - the transaction fact table we want to
36:23 - answer the question which stores had
36:25 - products that were on sale answering
36:27 - this question is a multi-step process
36:28 - the first step is to find the average
36:31 - price for each product for this we
36:33 - aggregate by product ID and for each
36:35 - Product ID we calculate the average
36:37 - price for example product one zero zero
36:39 - one sold for four hundred and twenty
36:41 - five dollars and four hundred dollars
36:43 - the average of these two values is
36:45 - calculated and then entered into the
36:47 - resulting table values for subsequent
36:49 - IDs are calculated in a similar manner
36:59 - you
37:04 - we also need to aggregate at the store
37:06 - and product level this removes the
37:08 - customer and date level information
37:09 - giving us the totals for each store and
37:11 - product combination due to the
37:15 - limitations of this example the
37:16 - aggregation does not result in fewer
37:18 - rows as it normally would the resulting
37:21 - table is however sorted by store ID and
37:24 - Product ID the next step is to join the
37:27 - product aggregate table to the store
37:29 - product aggregate table using product ID
37:31 - as the common field between the two
37:33 - tables this results in a table with a
37:36 - similar structure as the store product
37:38 - aggregate table with the only difference
37:40 - being the new column average product
37:43 - price we filter the resulting table to
37:46 - only include rows where price is less
37:48 - than average product price there are
37:50 - only two rows where this is true finally
37:53 - we join this table with the store
37:54 - dimension and product dimension tables
37:56 - adding store name and product name to
37:59 - the table is necessary to fully answer
38:01 - the original question the resulting
38:03 - table makes it clear that the g2 went on
38:06 - sale at Best Buy and the s6 went on sale
38:08 - at h/h Greg this example is a simplified
38:12 - an illustration of how data preparation
38:13 - techniques are often used together to
38:16 - answer typical questions of the data
38:17 - this concludes our video on aggregation
38:20 - today we covered a definition of
38:23 - aggregation and examples of aggregation
38:27 - [Music]
38:43 - this video will cover cross tabs and
38:46 - transposing cross tabs are also called
38:49 - pivot tables they are used to compare
38:52 - one measure across two or more
38:53 - dimensions one dimension will have its
38:56 - values transformed into columns and the
38:58 - other dimensions will be aggregated the
39:01 - visual result will be a table with fewer
39:03 - rows but more columns for example this
39:05 - table shows sales by region and month
39:08 - the data is stored in a form similar to
39:10 - how it would be captured with one row
39:12 - for every combination of region and
39:14 - month a crosstab function is performed
39:16 - with region defined as the grouping
39:18 - field month defined as the header field
39:20 - and sales defined as the data field
39:22 - there can be multiple grouping fields
39:24 - but only one header field and one data
39:26 - field are allowed cross tabs are an
39:28 - effective way to summarize and present
39:30 - data as trends within the data are
39:31 - easier to identify and data
39:33 - visualization color is added to build a
39:36 - heatmap cross tabs are also used in
39:38 - statistics to build contingency tables
39:41 - transposing can be thought of as the
39:43 - opposite of cross tabs it transforms the
39:45 - data from a wide format into a narrow
39:47 - format typically transposing is required
39:50 - after receiving financial data from a
39:52 - spreadsheet as columns can be used to
39:54 - capture dates locations or categories
39:56 - once the columns known as data fields
39:58 - are transposed into one variable the
40:00 - data is ready for a more sophisticated
40:02 - analysis any fields that are not to be
40:04 - transposed are called key fields for
40:07 - this example we can use the result of
40:09 - the crosstab function from the previous
40:10 - section region is the only key field
40:13 - while January February and March are the
40:15 - data fields to be transposed transposing
40:18 - results in the original data set notice
40:21 - how the column names are transformed
40:22 - into values for the newly created month
40:24 - column
40:26 - this concludes our video on cross tabs
40:28 - and transposing
40:31 - [Music]
40:47 - this video will cover contingency tables
40:50 - first we will discuss the definition of
40:53 - a contingency table and then the steps
40:55 - for creating one finally we will discuss
40:57 - chi-square distributions in statistics a
41:01 - contingency table is a type of table in
41:04 - a matrix format that displays the
41:05 - multivariate frequency distribution of
41:08 - variables contingency tables are heavily
41:10 - used in Survey Research Business
41:13 - Intelligence engineering and scientific
41:16 - research they provide a basic picture of
41:19 - the inter relation between two or more
41:21 - variables and can help find interactions
41:23 - a contingency table is also referred to
41:26 - as a two-way frequency table here's an
41:28 - example given this table can you
41:31 - calculate the following metrics the
41:33 - number of males who are right-handed the
41:36 - percent of males who are left-handed
41:38 - whether more males are left-handed than
41:40 - females or the percent of left-handed
41:43 - people who are females in a table such
41:46 - as this notice that there are no
41:48 - numerical values the person ID is a
41:50 - numerical identifier but the numbers are
41:52 - arbitrary so there are no obvious
41:54 - calculations to perform however because
41:57 - each row represents one person we can
41:59 - count the number of rows along each
42:01 - dimension gender and dominant hand to
42:03 - analyze how the dimensions are
42:05 - distributed to create a contingency
42:07 - table the first step is to aggregate the
42:09 - original data set along two dimensions
42:11 - gender and handedness and count the
42:13 - number of rows for each combination of
42:15 - values since there are two possible
42:18 - values for gender male and female and
42:20 - two possible values for dominant hand
42:22 - left and right there are four total
42:24 - possible combinations male and right
42:27 - male and left female and right and
42:29 - female and left for each of these
42:32 - combinations we count the number of rows
42:34 - that contain both values at this point
42:37 - we can extract information regarding the
42:39 - quantity of each combination allowing us
42:42 - to answer basic questions such as how
42:44 - many females are left-handed and are
42:46 - there more males or females who are
42:48 - left-handed next we perform a crosstab
42:50 - on the result moving the dominant hand
42:52 - to the horizontal axis this will make
42:55 - interpretation of the results much
42:57 - easier and allow us to easily calculate
42:59 - totals
43:00 - each dimension there are other questions
43:01 - however that are harder to answer and
43:04 - these deal with the proportions of each
43:05 - combination what proportion are percent
43:08 - of males are left-handed and are a
43:10 - greater proportion of males left-handed
43:12 - than the proportion of females for these
43:15 - questions the proportion of gender we
43:17 - need to divide the values in the cells
43:20 - by the totals along the gender axis
43:22 - which in this case is the vertical axis
43:24 - the resulting table called a row
43:26 - conditional frequency table will have
43:28 - 100% values in each cell in the total
43:31 - column the cells for each row will
43:34 - account for all the people within the
43:35 - category male and female of the gender
43:37 - dimension the table can be easily read
43:41 - eighty-three percent of males are
43:42 - right-handed and only 8 percent of
43:45 - females are left-handed but what if we
43:49 - want to know what percent of left-handed
43:51 - people are female this is a different
43:53 - type of question one that can be
43:54 - answered by dividing the original values
43:56 - of the cells by the totals along the
43:59 - dominant hand or horizontal axis of the
44:01 - contingency table these are called
44:03 - column relative frequencies now the
44:06 - results can be interpreted in two
44:08 - sentences by focusing on the columns in
44:10 - the contingency table 49% of
44:12 - right-handed people are males 31% of
44:15 - left-handed people are females etc but
44:19 - what if we want to answer questions
44:20 - about how prevalent each separate
44:22 - combination is among all people in this
44:24 - data set for instance what percent of
44:26 - the people are left-handed males for
44:29 - that we divide the original values by
44:31 - the total number of rows in the data set
44:33 - for our example this makes for an easy
44:35 - calculation since there were 100 rows
44:38 - with this table we can answer many basic
44:41 - questions about the data set including
44:42 - what percent of the people in the data
44:44 - set are right-handed what percent of the
44:47 - people in the data set are female and
44:48 - what percent of the people on the data
44:50 - set are left-handed females and so forth
44:54 - note that the language used what percent
44:57 - of people in the data set is different
44:58 - from the easier to say what percent of
45:00 - people this is because the data set
45:03 - being used is a sample of the population
45:05 - and to infer the trends about the
45:07 - population will need to use a
45:09 - statistical technique for a contingency
45:12 - table a chi
45:13 - square distribution can be used to make
45:15 - such an inference to do so however
45:17 - assumes that the sample you are using
45:19 - was acquired from the population
45:21 - randomly
45:23 - the chi-square distribution compares the
45:25 - actual values to the expected values to
45:28 - determine if the actual numbers that
45:30 - were observed and recorded in the data
45:31 - set are due to chance or if there's a
45:34 - difference between the two variables
45:36 - that cannot be explained by chance in
45:37 - this example we want to determine if the
45:40 - observed difference in the proportion of
45:42 - females who are left-handed is really
45:44 - smaller than the proportion of males who
45:46 - are left-handed or if that observation
45:48 - could be due to chance in other words we
45:51 - want to know if the dominant hand is
45:53 - dependent on gender to calculate the
45:55 - expected values for each cell multiply
45:57 - the relative horizontal and vertical
45:59 - dimension totals and divide by the
46:01 - number of total observations in this
46:04 - example to calculate the expected value
46:06 - for right-handed males multiply 52 the
46:09 - total number of males by 87 the total
46:12 - number of right-handed people and divide
46:15 - by 100 the total number of people
46:17 - similar calculations are done for each
46:19 - cell once the expected values have been
46:23 - computed the chi-square test can be run
46:25 - in Excel the chi-square test function
46:28 - uses the actual table and expected table
46:31 - as inputs to calculate the p-value this
46:34 - p-value is the probability that these
46:36 - results did not occur due to chance a
46:39 - common way to evaluate the p-value is to
46:42 - compare it to 0.05 if the p-value is
46:46 - less than 0.05 we say there is an
46:49 - association between the two variables
46:51 - that is statistically significant in
46:53 - this example our p-value is 0.18 which
46:57 - is greater than 0.05 so we conclude that
47:00 - the dominant hand variable is
47:02 - independent of gender if there is an
47:05 - association between two variables
47:06 - completing the calculation for the
47:08 - chi-square test statistic can be used to
47:11 - find which values contribute the most to
47:14 - the Association this can be calculated
47:16 - using this formula for each cell
47:20 - subtract the expected value from the
47:22 - observed and square the result then
47:25 - divide the answer by the expected value
47:27 - each of these results is summed
47:29 - indicated by the Greek letter Sigma this
47:32 - value is compared to the chi-square
47:34 - distribution
47:36 - within each cell the calculation
47:37 - describes how far the actual value is
47:40 - from the expected value in this example
47:42 - the calculations for left-handed people
47:44 - are much greater than those for
47:46 - right-handed people these cells have the
47:49 - greatest impact on the potential
47:50 - association between the two variables
47:53 - summing those values results in
47:55 - chi-square test statistic of 1.78 using
47:59 - this value in a chi-square distribution
48:01 - along with the degrees of freedom based
48:03 - on the number of values for each
48:05 - variable and a significance level such
48:07 - as 0.05 is how the p-value is obtained
48:11 - this concludes our video on contingency
48:14 - tables today we defined contingency
48:16 - tables and then we discussed the steps
48:18 - for creating one and finally we covered
48:21 - chi-square distributions
48:24 - [Music]
48:42 - this video will first define
48:45 - distribution then we'll cover measures
48:47 - of distribution the mean median outliers
48:52 - mode minimum and maximum values and
48:56 - quantiles the most common method of
48:59 - analyzing a numeric variable is by
49:01 - exploring how the values are distributed
49:03 - the distribution of a numeric variable
49:05 - shows all the possible values and how
49:07 - often they occur a distribution provides
49:10 - methods in which many records of data
49:12 - can be summarized to provide basic
49:14 - information about the variable these
49:16 - methods can either be numerical measures
49:18 - or visualizations we'll explore the most
49:20 - popular methods using this data set of
49:22 - 10 rows of bank teller salary data
49:24 - because the sample data set is so small
49:27 - we can make some quick observations
49:28 - about the salary variable the lowest
49:31 - salary is twenty eight thousand six
49:33 - hundred and sixty-five dollars this is
49:34 - referred to as the minimum value the
49:36 - highest salary is forty four thousand
49:39 - and twenty dollars the maximum value
49:41 - finally many of the salaries are in the
49:43 - low $30,000 range we'll begin by
49:46 - defining and calculating the salary
49:48 - variables measures of distribution mean
49:51 - median outliers mode minimum and maximum
49:56 - values in quantiles to find the mean or
49:59 - average add up all the numbers and
50:01 - divide by the number of rows 340 one
50:04 - thousand eight hundred and sixty dollars
50:06 - divided by 10 equals a mean of thirty
50:08 - four thousand 186 dollars to find the
50:12 - median sort the numbers and find the
50:14 - middle value if there are an odd number
50:16 - of rows there is one middle value if
50:18 - there are an even number of rows there
50:20 - are two middle values and the median
50:22 - will be the average of these two values
50:23 - here we sort the table by salary then
50:26 - find the average of the two middle
50:27 - values by adding thirty three thousand
50:30 - nine hundred eighty dollars to thirty
50:31 - four thousand eight hundred fifty
50:33 - dollars and dividing by 2 to get thirty
50:35 - four thousand one hundred and forty-five
50:37 - dollars values that fall outside of the
50:40 - normal range of the rest of the
50:41 - observations are called outliers in our
50:44 - example the value of forty four thousand
50:46 - twenty dollars is an outlier from the
50:47 - rest of the values
50:49 - the mode is the most commonly occurring
50:51 - value in our sample data set no values
50:53 - occur more than once
50:54 - so there is no mode as we've already
50:56 - noticed the minimum and maximum values
50:58 - are twenty thousand six hundred and
51:00 - sixty five and forty four thousand and
51:03 - twenty dollars a quantile is a set of
51:05 - values that divide a frequency
51:07 - distribution in two equal groups each
51:08 - containing the same fraction of the
51:10 - total population to find quantiles
51:12 - divide the distribution into groups of
51:14 - equal size with each group containing
51:16 - about the same number of rows the most
51:18 - simple of quantiles has already been
51:19 - calculated the median divides the
51:21 - records into two groups of five the
51:24 - following quantiles are most often used
51:26 - tersh aisles three groups quartiles four
51:29 - groups quintiles five groups deciles ten
51:32 - groups and percentiles 100 groups let's
51:35 - look at quartiles for our example
51:38 - the median is commonly referred to as q2
51:41 - since it is the second quartile q1 and
51:44 - q3 are also used often to calculate
51:47 - quartile 1 we can look at the middle
51:48 - value of the first 5 records sorted in
51:50 - order which is 30 $1,300 and then look
51:54 - at the middle value of the last 5
51:56 - records for quartile 3 which is 30
51:59 - $5,100 these values along with our
52:01 - median are our quartile values this
52:04 - concludes our video on measures of
52:06 - distribution today we define
52:08 - distribution and then we discuss the
52:10 - measures of distribution including mean
52:12 - median outliers mode minimum and maximum
52:17 - values and quantiles
52:21 - [Music]
52:34 - this video will cover variation
52:36 - including a definition of variation and
52:39 - the measures of variation range inter
52:42 - quartile range variance standard
52:45 - deviation and standardization as well as
52:48 - testing differences between means
52:50 - variation refers to how spread out the
52:53 - values are for a variable interpreting
52:55 - variation that is explaining a variables
52:57 - variation in reference to other
52:59 - variables is a foundational task in
53:01 - Business Analytics variables that have
53:04 - values that are spread out have higher
53:06 - variation while variables with values
53:08 - very close to the mean have lower
53:09 - variation the following measures help us
53:12 - understand how the values for a variable
53:14 - are spread out or on the mean range
53:17 - interquartile range
53:19 - variance standard deviation and
53:22 - standardization using an example of
53:25 - salaries by gender a question arises do
53:28 - men make more than women certainly some
53:30 - men make more than women but is this
53:32 - true for the group as a whole using the
53:34 - principles of variation will help us to
53:36 - answer this question the range is the
53:39 - largest number minus the smallest number
53:41 - to find the range we first sort from
53:43 - smallest to largest then we subtract the
53:46 - smallest value from the largest value
53:53 - to find the interquartile range first
53:56 - separate the data into quartiles and
53:58 - then subtract q1 from q3 to find the
54:01 - variance we first calculate the mean or
54:04 - average
54:10 - then we subtract the mean from each
54:12 - value and square the result by squaring
54:15 - the differences we remove the
54:16 - possibility of negative values
54:18 - cancelling out the positive values next
54:20 - find the average of the squared
54:22 - differences
54:28 - variance is not a very useful measure
54:30 - the value we got for variance is much
54:32 - different from the range of salaries
54:33 - that are variable captures to get it
54:36 - back into the correct scale we take the
54:37 - square root of the variance this results
54:39 - in the standard deviation this value is
54:42 - used relative to the mean to determine
54:44 - which values are within the normal
54:45 - variation of the salary variable
54:47 - subtract the standard deviation from the
54:50 - mean to find the lower threshold add the
54:52 - standard deviation to the mean to find
54:54 - the upper threshold using the standard
54:57 - deviation gives us a way to determine
54:59 - which values are within the normal
55:01 - variation and which values are either
55:02 - less than normal or greater than normal
55:04 - in other words it gives us the ability
55:06 - to identify outliers more easily in our
55:09 - example we can identify that the
55:11 - smallest two salaries twenty-eight
55:13 - thousand six hundred and sixty-five
55:14 - dollars and twenty-nine thousand five
55:16 - hundred dollars and the largest salary
55:19 - forty-four thousand twenty dollars are
55:21 - outside one standard deviation from the
55:23 - mean
55:25 - mean and standard deviation are measures
55:28 - that describe the distribution of a set
55:30 - of numbers but what if we want to
55:31 - describe the numbers within the set to
55:33 - compare two numbers within an entirely
55:35 - different set we can do this by
55:37 - standardizing the values in statistics
55:39 - this measure is called the z-score to
55:42 - standardize a value subtract the mean
55:43 - and divide by the standard deviation for
55:46 - example in the first row of our data set
55:48 - since we've already calculated this
55:50 - salary - mean column all we need to do
55:52 - is divide by 4103
55:54 - to get the result of negative 1.1 for
55:57 - this value can be interpreted as the
56:00 - value of $29,500 is 1.1 for standard
56:04 - deviations lower than the mean looking
56:07 - at the rest of the values we see that
56:09 - the largest salary forty four thousand
56:10 - and twenty dollars is actually very far
56:12 - away from the mean this must be a high
56:14 - performing bank teller we can use this
56:16 - standardized value to compare the high
56:18 - performing bank teller to the high
56:20 - performer of another profession
56:22 - let's return to the question post at the
56:24 - beginning do men make more than women to
56:27 - answer this question we can use a
56:28 - statistical test called a t-test this
56:31 - test requires that we make three
56:33 - assumptions about our data which for now
56:34 - we will assume to be true the two
56:37 - populations have the same variance the
56:39 - populations are normally distributed
56:41 - each value is sampled independently from
56:44 - each other value by considering the
56:47 - values of salary separately we can see
56:49 - that females have the lowest value and
56:51 - the highest value so the variation must
56:52 - be higher calculating the mean and
56:55 - standard deviation for each group we see
56:57 - that the mean is almost the same yet the
56:59 - standard deviation for females is more
57:01 - than twice that for males using these
57:03 - values we can calculate the t-statistic
57:05 - and corresponding p-value but in Excel
57:08 - we only need the original values in
57:10 - different columns using the t-test
57:13 - function we get a p-value of 0.44 since
57:16 - this value is much higher than point
57:18 - zero five we conclude that the two
57:20 - distributions are not statistically
57:21 - significant this is mainly due to the
57:24 - fact that our sample size for this
57:25 - example is very small and also because
57:28 - the means are almost equivalent
57:30 - this concludes our video on measures of
57:32 - variation today we defined variation and
57:35 - then discussed range enter quartile
57:37 - range variance standard deviation
57:40 - standardization and testing differences
57:42 - between the means
57:44 - [Music]
57:56 - this video will cover distribution
57:59 - visualizations including buckets
58:02 - histograms and an introduction to area
58:05 - line graphs we begin with a data set
58:08 - that contains gender dominant hand and
58:10 - salary we've calculated a number of
58:12 - different measures including the mean
58:14 - median minimum maximum range
58:18 - interquartile range variance standard
58:21 - deviation and a list of z-scores next
58:24 - we'll build a histogram which is the
58:26 - most common way to visualize a numeric
58:27 - distribution to build a distribution
58:29 - will group the data into buckets or bins
58:31 - of equal size this will effectively
58:33 - transform the variable into a
58:35 - categorical variable allowing us to
58:37 - count the occurrences of each bucket
58:39 - next we determine the size of each
58:41 - bucket first we locate our minimum and
58:43 - maximum values
58:46 - then we subtract the minimum from the
58:48 - maximum and divide by the number of
58:50 - buckets this gives us a value of one
58:52 - thousand five hundred and thirty six
58:53 - dollars next we need to figure out the
58:56 - starting value for each bucket starting
58:58 - with the lowest salary we add the size
59:00 - of the bucket to determine the starting
59:01 - value of the second bucket we do this
59:03 - until we obtain the starting values for
59:05 - all ten buckets
59:07 - we'll add a new column to the original
59:09 - table to assign each person into a
59:11 - salary bucket summarize the new variable
59:14 - starting value of salary bucket counting
59:17 - how many occurrences of each bucket are
59:18 - observed note that although this
59:20 - variable is numeric by summarizing it
59:22 - we're treating it as a categorical
59:23 - variable finally we'll make a simple bar
59:26 - chart to visualize this table this bar
59:29 - chart is known as a histogram and it's
59:30 - one of the most common methods of
59:32 - visualizing a numeric distribution
59:33 - although many software packages can
59:35 - produce a histogram very quickly as
59:37 - we've seen creating one involves quite a
59:39 - few steps if we change the visualization
59:42 - to an area line graph and change the
59:44 - vertical axis to the percent of records
59:46 - by dividing the count by the total
59:48 - number of records the result is similar
59:50 - to what is known as a probability
59:51 - density function or PDF this is commonly
59:55 - used in statistics to estimate the
59:56 - probability of a new value for now we
59:59 - just need to know that the shaded area
60:00 - under the curve adds up to 1 or 100% and
60:03 - that the lines are typically much smooth
60:05 - and we see in this example which is
60:07 - because most PDFs visualize more than 10
60:09 - data points and use more than 10 buckets
60:12 - this concludes our video on distribution
60:14 - visualizations today we covered buckets
60:17 - histograms as well as a brief
60:19 - introduction to the use of area line
60:21 - graphs
60:23 - [Music]
60:36 - this video will cover normal
60:38 - distributions continuous distributions
60:42 - density functions cumulative
60:44 - distribution functions and the 6895 99.7
60:49 - rule the single most important
60:51 - distribution in statistics is the normal
60:53 - distribution it's a continuous
60:55 - distribution and it's the basis of the
60:57 - familiar symmetric bell-shaped curve the
60:59 - mean of the normal distribution is in
61:01 - the center the standard deviations are
61:03 - marked at equal distances from the mean
61:05 - any particular normal distribution is
61:08 - specified by its mean and standard
61:09 - deviation by changing the mean the
61:11 - normal curve shifts to the left or right
61:19 - by changing how spread out the standard
61:21 - deviations are the curve also changes
61:23 - standard deviations can be spread out
61:25 - wider or closer together
61:28 - therefore there are really many normal
61:30 - distributions not just a single one the
61:32 - normal distribution is a two parameter
61:34 - family where the two parameters are the
61:36 - mean and standard deviation here's a
61:38 - tool you can play with online that
61:40 - illustrates a normal distribution in
61:42 - real life it looks like a triangular
61:43 - shaped pegboard into which balls are
61:45 - dropped when there's an equal
61:47 - probability that the balls will drop
61:48 - either left or right their final
61:49 - placement forms a normal distribution
61:51 - however when the probability of the
61:53 - balls dropping left or right is unequal
61:55 - which is something you can experiment
61:56 - with using this tool the distribution
61:58 - changes the formulas for mean and
62:01 - standard deviation are very complex but
62:03 - you will not have to compute them
62:04 - because the software will with
62:06 - continuous variables there is a
62:08 - continuum of possible values such as all
62:10 - values between 0 and 100 or all values
62:13 - greater than 0 instead of assigning
62:15 - probabilities to each individual value
62:17 - in the continuum the total probability
62:19 - of one is spread over the continuum thus
62:21 - the shaded area within the bell curve
62:23 - will always have an area of 1 the key to
62:26 - the spreading is called a density
62:27 - function which acts like a histogram the
62:30 - higher the value of the density function
62:31 - the more likely this region of the
62:33 - continuum is a density function usually
62:37 - denoted by FX specifies the probability
62:40 - distribution of a continuous random
62:41 - variable X the higher FX is the more
62:44 - likely X is probabilities are found from
62:47 - a density function as areas under the
62:49 - curve so for example the shaded portion
62:51 - under the spell curve represents the
62:53 - probability of X being between 65 and 75
62:56 - the cumulative distribution function or
62:59 - CDF is the probability that the variable
63:01 - takes a value less than or equal to X
63:03 - it's the total area under the normal
63:05 - curve up to X the beauty of the normal
63:08 - curve is that no matter what its mean
63:09 - and standard deviation are the area
63:11 - between the mean minus 1 standard
63:13 - deviation and the mean plus 1 standard
63:15 - deviation is always about 68% the area
63:18 - between the mean minus 2 standard
63:20 - deviations and the mean plus two
63:22 - standard deviations is always about 95%
63:26 - the area between the mean minus three
63:28 - standard deviations and the mean plus
63:30 - three standard deviations is always
63:31 - about ninety-nine point seven percent
63:33 - that means almost all values fall within
63:35 - three standard deviations on either side
63:37 - of the mean that's true for all normal
63:39 - curves no matter their shape but how
63:41 - good is this rule for real data let's go
63:44 - ahead and check out an example here's
63:45 - our data the mean of the weight of one
63:47 - hundred and twenty women runners in a
63:49 - sample is one twenty seven point eight
63:50 - pounds the standard deviation is fifteen
63:53 - point five here's what our distribution
63:55 - would look like let's look a little more
63:56 - closely at that distribution 68% of our
63:59 - 120 runners is about eighty three
64:02 - runners according to the 68 95 99 point
64:05 - seven rule those runners should fall
64:07 - fall within one standard deviation of
64:08 - the mean weight of one hundred and
64:10 - twenty seven point eight pounds that is
64:13 - eighty-three of our runners should fall
64:14 - between one hundred and twelve point
64:16 - three and one forty three point three
64:17 - pounds when we check our data we see
64:19 - that seventy-nine runners fall within
64:21 - one standard deviation of the mean
64:23 - furthermore ninety-five percent of our
64:25 - group or about 114 runners should fall
64:28 - within two standard deviations of the
64:29 - mean or between ninety six point eight
64:31 - and one hundred and fifty eight point
64:33 - eight pounds the data shows that 115
64:35 - runners fall within two standard
64:37 - deviations of the mean
64:39 - finally according to the rule
64:41 - ninety-nine point seven percent of our
64:43 - runners or one hundred nineteen point
64:45 - six runners should fall within three
64:46 - standard deviations of the mean or
64:48 - within a range of eighty one point three
64:50 - pounds to one hundred and seventy four
64:52 - point three pounds according to our data
64:54 - all 120 runners fall within this range
64:56 - so it seems as if the rule is pretty
64:58 - accurate in this case this concludes our
65:01 - video on normal distributions continuous
65:04 - distributions density functions and
65:06 - cumulative distribution functions as
65:08 - well as the 6895 99.7 rule
65:13 - [Music]
65:24 - this video will cover kurtosis including
65:27 - a definition as well as positive and
65:29 - negative kurtosis and asymmetrical
65:31 - distributions including those of
65:33 - positive and negative skew kurtosis is
65:36 - the measure that describes the size of
65:38 - the tails in a distribution a
65:39 - distribution with positive kurtosis
65:41 - contains fewer values in the tails than
65:44 - a normal distribution a distribution
65:46 - with more values in the tails has
65:48 - negative kurtosis the normal
65:50 - distribution is a type of symmetrical
65:51 - distribution in which the mean is equal
65:53 - to the median and there is an equal
65:55 - probability of a value falling on either
65:57 - side of the mean although normal
65:59 - distributions and other types of
66:01 - symmetrical distributions are very
66:02 - common there are often distributions
66:04 - that are asymmetrical we call these
66:06 - types of distributions skewed for a
66:08 - negatively skewed distribution the left
66:10 - tail is longer and the mean is less than
66:12 - the median this is due to the occurrence
66:14 - of outliers at the lower end of the
66:15 - distribution away from the most
66:17 - frequently occurring values a good
66:19 - example of this is the height of NBA
66:21 - players since taller basketball players
66:23 - have an advantage the majority of NBA
66:25 - players are very tall for a positively
66:27 - skewed distribution the right tail is
66:29 - longer as there are outliers with larger
66:31 - numbers these outliers cause the mean to
66:33 - be greater than the median an example of
66:35 - a positively skewed distribution is in
66:37 - the salary of baseball players there are
66:39 - a few star players who make much more
66:41 - than the majority of players these high
66:43 - salaries are outliers that make the mean
66:45 - of the distribution increase for
66:47 - distributions that are skewed both
66:49 - positively and negatively the meeting is
66:51 - a better representation of central
66:52 - tendency than the mean as the outliers
66:54 - will impact the calculation of the mean
66:56 - the median is not affected by outliers
66:58 - and typically is a much better
67:00 - approximation for the middle of a
67:01 - distribution this concludes our video on
67:04 - kurtosis today we defined kurtosis and
67:06 - discussed positive and negative kurtosis
67:08 - we also covered asymmetrical
67:10 - distributions including those of
67:12 - positive and negative skew
67:15 - [Music]
67:31 - this video will cover sampling basics
67:33 - including populations and inferences
67:35 - selecting a sample random sampling
67:39 - stratified sampling and cluster sampling
67:42 - a population is a set of all members
67:44 - about which is study intends to make
67:46 - inferences here's a population of people
67:49 - we'd like to study their television
67:50 - watching behavior and infer how many
67:52 - watch a particular shows so we can
67:53 - decide whether to purchase advertising
67:55 - spots during this period of time but our
67:57 - population is much too large to study
67:59 - effectively to study their television
68:01 - viewing habits we'll have to survey them
68:03 - and it's not feasible to survey every
68:05 - single individual
68:06 - so we'll take a sample of the population
68:07 - to study the sample will represent the
68:09 - population as a whole and so will it
68:11 - survey results if we choose our sample
68:13 - correctly we will focus on selecting a
68:15 - sample using probability a probability
68:17 - sample is chosen from a population using
68:20 - a random mechanism there are two types
68:22 - of probability sampling stratified and
68:24 - cluster a random sample is only random
68:27 - if each individual has the same chance
68:28 - of being chosen from the population so
68:31 - back to our television viewing research
68:32 - we want to figure out how many
68:33 - television viewers there might be during
68:35 - a particular so let's say there are
68:37 - 30,000 viewers in our population each
68:39 - viewer is known as a unit in order to
68:43 - select a sample n of viewers from this
68:45 - population of 30,000 we could choose to
68:47 - use a simple random sample this means
68:49 - that there is an equal probability that
68:51 - each viewer could be selected for
68:53 - inclusion in the sample if our desired
68:55 - sample size was 200.final echt 200
68:58 - viewers randomly and then we could send
68:59 - each of those viewers a questionnaire in
69:01 - the mail about their viewing habits but
69:03 - suppose various subpopulations within
69:05 - the total population can be identified
69:07 - these populations are called strata
69:09 - instead of taking a random sample from
69:11 - the entire population we might get
69:13 - better information by selecting a simple
69:15 - random sample from each stratum
69:16 - separately this is called stratified
69:18 - sampling examples of subpopulations and
69:21 - television viewers might include age or
69:23 - gender there are several advantages to
69:25 - stratified sampling one obvious
69:27 - advantage is that separate estimates can
69:29 - be obtained within each stratum which
69:31 - would not be obtained with a single
69:32 - random sample from the entire population
69:34 - for example let's say we're looking at
69:36 - our television viewers by age group we
69:38 - have three strata 18 to 24 25 to 39 and
69:42 - 40 plus we find that their peak
69:44 - times vary based on age thus we can make
69:47 - better decisions about which product to
69:48 - advertise during which time period a
69:50 - more important advantage of stratified
69:52 - sampling is that the accuracy of the
69:54 - resulting population estimates can be
69:55 - increased by using appropriately defined
69:58 - strata in cluster sampling the
70:00 - population is separated into clusters
70:02 - such as regions of the country and then
70:04 - a random sample of the clusters is
70:05 - selected the primary advantage of
70:07 - cluster sampling is sampling convenience
70:09 - and possibly lower cost selecting a
70:11 - cluster sample is straightforward the
70:13 - key is to define the sampling units as
70:15 - the clusters such as the regions of the
70:17 - continental US shown here this concludes
70:19 - our video on sampling basics today we
70:21 - covered populations and inferences
70:23 - selecting a sample random sampling
70:25 - stratified sampling and cluster sampling
70:30 - [Music]
70:43 - this video will cover bivariate data
70:47 - scatter plots and null values measures
70:51 - of central tendency variability and
70:53 - spread summarize a single variable by
70:55 - providing important information about
70:56 - its distribution often more than one
70:58 - variable is collected on each individual
71:00 - for example in large health studies of
71:02 - populations it's common to obtain
71:04 - variables such as age sex height weight
71:07 - blood pressure and total cholesterol in
71:09 - each individual economic studies may be
71:11 - interested in among other things
71:13 - personal income and years of education
71:15 - as a third example most university
71:17 - admissions committees ask for an
71:19 - applicant's high school grade point
71:20 - average in standardized admission test
71:22 - scores like the SAT bivariate data
71:24 - consists of two quantitative variables
71:26 - for each individual in contrast with
71:28 - univariate or single variable data our
71:31 - first interest is in summarizing such
71:33 - data in a way that's analogous to
71:35 - summarizing univariate data by way of
71:37 - illustration let's consider something
71:39 - with which we're all familiar age let's
71:41 - begin by asking if people tend to marry
71:43 - other people of about the same age our
71:45 - experience tells us yes but how good is
71:47 - the correspondence one way to address
71:49 - the question is to look at pairs of Ages
71:51 - for a sample of married couples table
71:54 - one shows the ages of 10 married couples
71:56 - going across the columns we see that yes
71:58 - husbands and wives tend to be of about
72:00 - the same age with men having a tendency
72:02 - to be slightly older than their wives
72:04 - this is no big surprise but at least the
72:06 - data bear out our experiences which is
72:08 - not always the case the pairs of Ages in
72:10 - table 1 are from a data set consisting
72:12 - of two hundred and eighty two pairs of
72:14 - spousal ages too many to make sense of
72:17 - from a table what we need is a way to
72:18 - summarize the two hundred and eighty two
72:20 - pairs of ages we know that each variable
72:23 - can be summarized by a histogram which
72:25 - is a graphical representation of a
72:26 - distribution a histogram partitions the
72:28 - variable on the x axis into various
72:31 - contiguous class intervals of usually
72:33 - equal widths the heights of the bars
72:35 - represent the class frequencies here we
72:37 - can see that each distribution is fairly
72:39 - skewed with a long right tail we can
72:42 - also summarize the variables with a mean
72:44 - and standard deviation
72:46 - from table 1 we can see that not all
72:48 - husband's are older than their wives and
72:50 - it's important to see that this fact is
72:51 - lost when we separate the variables that
72:54 - is even though we provide summary
72:55 - statistics on each variable the pairing
72:57 - within the couple is lost by separating
72:59 - the variables we cannot say for example
73:02 - based on means alone what percentage of
73:04 - couples has younger husbands and wives
73:06 - we have to count across the pair's to
73:08 - find this out only by maintaining the
73:10 - pairing can meaningful answers be found
73:12 - about the couples another example of
73:15 - information not available from the
73:16 - separate descriptions of husbands and
73:18 - wives ages is the mean age of husbands
73:21 - with wives of a certain age
73:22 - for instance what is the average age of
73:24 - husbands with 45 year old wives finally
73:27 - we don't know the relationship between
73:29 - the husband's age and the wife's age we
73:32 - can learn much more by displaying the
73:33 - bivariate data in a graphical form that
73:36 - maintains the pairing figure to shows a
73:38 - scatter plot of the paired ages the
73:40 - x-axis represents the age of the husband
73:42 - and the y-axis the age of the wife there
73:45 - are two important characteristics of the
73:46 - data revealed by figure two first it's
73:49 - clear that there's a strong relationship
73:50 - between the husband's age and the wife's
73:52 - age the older the husband the older the
73:55 - wife when one variable Y increases with
73:58 - the second variable X we say that X and
74:00 - y have a positive association conversely
74:03 - when Y decreases as x increases we say
74:06 - that they have a negative association
74:08 - second the points cluster along a
74:11 - straight line when this occurs the
74:12 - relationship is called a linear
74:14 - relationship figure 3 shows a
74:16 - scatterplot of arm strength and grip
74:18 - strength from 149 individuals working in
74:21 - physically demanding jobs including
74:23 - electricians construction maintenance
74:25 - workers and auto mechanics not
74:27 - surprisingly the stronger someone's grip
74:28 - the stronger their arm tends to be there
74:30 - is therefore a positive association
74:32 - between these variables
74:33 - although the points cluster along a line
74:36 - they're not clustered quite as closely
74:37 - as they are for the scatterplot of
74:39 - spousal age a common problem when
74:41 - working with real-world data is the
74:43 - presence of missing or null values
74:45 - within a data set there are three
74:46 - strategies to deal with the issue the
74:48 - first one is to omit the rows if the
74:51 - variable is very important to the
74:52 - analysis and there are not many
74:54 - observations with missing values it can
74:56 - be acceptable to filter or delete those
74:58 - rows
74:59 - the second is to treat missing as a
75:01 - separate category if the variable is
75:03 - categorical this is easy if the variable
75:06 - is numeric then the variable will need
75:07 - to be binned and a category created for
75:09 - the missing rows the third is to impute
75:12 - a value using distribution measures such
75:15 - as the mean or the median or other
75:16 - variables if values of other fields have
75:19 - differing distributions for the variable
75:21 - with missing values we can calculate
75:23 - separate distribution measures using
75:25 - these categories this concludes our
75:27 - video on bivariate data scatter plots
75:30 - and null values
75:34 - [Music]
75:47 - this video will cover uncertainty
75:50 - entropy and analyzing data the result of
75:54 - data analysis is information information
75:57 - resolves uncertainty the uncertainty of
76:00 - an event is measured by its probability
76:02 - of occurrence the more uncertain an
76:04 - event the more information is required
76:06 - to resolve the uncertainty of that event
76:09 - entropy refers to the fact that you
76:11 - cannot stir things apart it's a measure
76:13 - of information content and
76:15 - unpredictability here's a concrete
76:18 - example of entropy if you have cold
76:20 - water and hot water and mix them
76:22 - together you will have warm water you
76:25 - can't separate the cold and the hot
76:26 - after they're mixed this is what is
76:28 - meant by you cannot stir things apart to
76:30 - get an informal intuitive understanding
76:32 - of the connection between these terms
76:33 - consider the example of a pole on some
76:35 - political issue the outcome of the pole
76:38 - is relatively unpredictable and actually
76:40 - performing the pole and learning the
76:41 - results gives some new information these
76:43 - are just different ways of saying that
76:45 - the entropy of the poles results is
76:47 - large now let's say a second poll is
76:49 - performed shortly after the first poll
76:51 - since the result of the first poll is
76:53 - already known the outcome of the second
76:55 - poll can be predicted well and the
76:57 - results should not contain much new
76:58 - information in this case the entropy of
77:00 - the second poll result is small relative
77:02 - to the first now consider the example of
77:06 - a coin toss when the coin is fair that
77:08 - is when the probability of heads is the
77:10 - same as the probability of tails then
77:12 - the entropy of the coin toss is as high
77:14 - as it can be that's because there's no
77:16 - way to predict the outcome of the coin
77:18 - toss ahead of time such a coin toss has
77:20 - one bit of entropy since there are two
77:22 - possible outcomes that occur with equal
77:24 - probability and learning the actual
77:26 - outcome contains one bit of information
77:27 - on the contrary a coin toss with a coin
77:31 - that has two heads and no tails has zero
77:33 - entropy since the coin will always come
77:35 - up heads and the outcome can be
77:37 - predicted perfectly in this graph
77:39 - entropy is maximized when the
77:41 - probability is 50%
77:43 - when the probability is zero or 100%
77:46 - there is zero entropy by adding
77:48 - information we can reduce entropy and
77:50 - gain certainty how does entropy apply to
77:52 - analyzing data we can use the principles
77:55 - of entropy to decide what results are
77:57 - important and should be included in a
77:59 - report and what results are trivial and
78:01 - should not be included if there is no
78:04 - uncertainty for a variable then there's
78:05 - no information if we obtained a new
78:08 - observation we would already know the
78:09 - value of that variable typically
78:11 - variables with only one value are
78:13 - excluded from an analysis at the very
78:15 - beginning on the other hand if a
78:17 - variable has a maximum uncertainty
78:19 - because it contains values that are
78:21 - equally likely it will be difficult to
78:23 - guess the value for a new observation
78:24 - these variables are not excluded from
78:27 - analysis applying this concept to data
78:29 - analysis our goal is to reduce entropy
78:31 - by explaining outcomes using other
78:34 - variables we are reducing uncertainty
78:36 - and these results should be the focus of
78:37 - a report this concludes our video today
78:40 - we covered uncertainty entropy and
78:42 - analyzing data
78:45 - [Music]
78:58 - this video will cover the parts of an
79:01 - analytical report including the
79:03 - introduction data analysis and results
79:07 - in conclusion the introduction should
79:10 - provide a concise summary of the project
79:12 - including the problem faced the type of
79:15 - data gathered and the highlights of the
79:18 - solution the data section should go into
79:20 - detail about the data used to complete
79:22 - the project variables and other
79:24 - technical terms should be defined well
79:26 - an example value should be listed and
79:28 - interpreted also this section should
79:31 - mention any abnormalities in the data
79:32 - such as missing values and discuss the
79:34 - steps that were taken to clean and
79:36 - prepare the data for analysis in the
79:38 - analysis section the report should cover
79:40 - the thought process behind the analysis
79:42 - including any output and data
79:44 - visualizations that are pertinent
79:46 - methods that are used should be
79:47 - introduced along with a brief
79:49 - description for the reasons they were
79:50 - used and possibly including references
79:52 - to external sources for further study
79:55 - care should be taken to not include
79:57 - every possible analysis as this can
79:59 - provide information overload to the
80:01 - audience insignificant or less
80:03 - significant findings can be briefly
80:05 - summarized leaving the majority of
80:07 - content in this section to focus on the
80:09 - most important findings the results in
80:11 - conclusion section should summarize the
80:13 - results of the analysis and if
80:15 - applicable provide specific
80:16 - recommendations on a course of action
80:18 - because the analysis section covered
80:20 - most of the information gleaned from the
80:22 - data the results in conclusion should
80:24 - mostly just apply that information
80:25 - towards a goal or a further course of
80:27 - study combined the introduction data
80:29 - analysis and results in conclusion
80:31 - sections of an analytical report
80:33 - delivered succinctly and clearly by an
80:35 - effective business analyst can provide
80:37 - useful and pertinent information to
80:39 - drive business improvement this
80:41 - concludes our video on writing
80:42 - analytical reports today we covered the
80:44 - parts of a report including the
80:46 - introduction data analysis and results
80:49 - in conclusion
80:52 - [Music]
81:02 - this video will cover automation
81:05 - including a brief introduction to
81:06 - automation macros and stored procedures
81:10 - a report is not the only results of a
81:13 - business analytics project typically
81:15 - automation will often be a separate goal
81:17 - especially when a data source is often
81:19 - updated with new records the main
81:22 - benefit of automation is that it frees
81:23 - up the analyst time to work on different
81:25 - problems this is very valuable from a
81:27 - business perspective as analysts that
81:29 - are skilled in automating tasks can save
81:31 - the company money in terms of labor
81:33 - expenses and overtime can uncover more
81:35 - and more patterns in the data leading to
81:37 - greater profitability there are two main
81:39 - types of automation used in business
81:41 - analytics macros and stored procedures
81:43 - macros are also referred to as functions
81:46 - the purpose of macros are to easily
81:48 - replicate certain steps without having
81:50 - to write out those steps individually
81:51 - macros can make an analysis quicker and
81:53 - more concise for instance with a certain
81:56 - data set you may want to filter sort and
81:58 - then take the average of a field if this
82:00 - set of steps will be used multiple times
82:02 - or for different data sets you may want
82:04 - to transform these steps into a macro to
82:06 - save time
82:09 - the parameters of a macro are the input
82:12 - this can be datasets fields or values
82:14 - the parameters are a component that will
82:17 - change with each call to a macro after
82:19 - the steps of the macro are complete the
82:21 - output will be returned and like the
82:23 - parameters can also be of different
82:24 - types
82:25 - macros are the foundation of many
82:27 - analytics and software packages as many
82:29 - of the complex algorithms used in data
82:31 - science are mostly just the building of
82:33 - certain simpler functions by utilizing
82:36 - macros that have already been built an
82:37 - analyst can become more efficient by
82:39 - building customized macros and analysts
82:41 - can easily pass on their efforts to
82:42 - other analysts most companies with a
82:45 - data science team will build out a
82:47 - repository of customized macros called a
82:49 - code base it makes it easier to manage
82:51 - certain tasks such as committing changes
82:53 - in version control these concepts are
82:55 - taken from computer science best
82:57 - practices of developing software another
83:00 - concept that comes from computer science
83:01 - is the use of object-oriented
83:03 - programming this principle guides the
83:05 - development of macros and is based on
83:07 - the idea that the components of code
83:09 - should be compartmentalized the main
83:11 - benefit of this technique is that it
83:12 - makes large projects easier to develop
83:14 - and maintain instead of an analyst
83:16 - having to replicate an analysis manually
83:18 - every time new data is captured a stored
83:21 - procedure will execute the stored
83:23 - procedure is a type of algorithm that
83:25 - runs according to a schedule and
83:26 - executes a series of steps that the
83:28 - analysts sets up in advance
83:30 - usually these steps will be in the form
83:33 - of code but new software programs allow
83:35 - analysts to build stored procedures
83:37 - without having to write code once code
83:40 - has been production alized only
83:41 - monitoring the execution of the stored
83:43 - procedure is necessary when the
83:45 - procedure completes with an error the
83:47 - analyst will need to troubleshoot the
83:48 - code the most common reason for errors
83:50 - to arise are due to unforeseen data
83:52 - values that is variables will contain
83:54 - value types that were not present in the
83:56 - original data set for example a numeric
83:58 - field will contain an alphabetic
84:00 - character the best practice is to test
84:02 - for these values within the stored
84:04 - procedure and transform them or remove
84:06 - records altogether and add a warning
84:08 - message this will prevent the stored
84:10 - procedure from failing altogether logs
84:12 - are output from stored procedures to
84:14 - describe the processes that ran
84:16 - information that logs produce can vary
84:18 - but usually include the amount of time
84:20 - each process took the number of Records
84:22 - were input and any error or warning
84:24 - messages that were triggered analysts
84:26 - should add code to stored procedures to
84:28 - make logs more descriptive and thus
84:30 - easier to troubleshoot in problems arise
84:32 - depending on the tools being used and
84:34 - the amount of data being processed
84:35 - production Eliza an algorithm can be
84:38 - either a simple task for one analyst or
84:40 - a multi-year project involving many
84:42 - analysts project managers database
84:44 - administrators documentation writers and
84:46 - quality assurance specialists in these
84:50 - cases management methodologies such as
84:52 - agile software development are used to
84:54 - coordinate team members and progress
84:56 - through the project lifecycle this
84:58 - concludes our video on automation today
85:00 - we covered a brief introduction to
85:02 - automation macros and stored procedures
85:06 - [Music]
85:16 - this video on regression will cover
85:19 - simple linear regression analysis
85:22 - regression line fitting observed in
85:25 - predicted values the least-squares
85:28 - coefficient estimation goodness of fit
85:31 - explained an unexplained variation root
85:34 - mean square error and the coefficient of
85:36 - determination significance testing and
85:39 - regression assumptions regression
85:43 - analysis is used to predict the value of
85:45 - one variable the dependent variable Y on
85:47 - the basis of other variables the
85:49 - independent variable X in other words if
85:52 - you know something about X you can use
85:54 - it to predict something about why we
85:57 - provide the independent variable X and
85:59 - we observe the dependent variable Y the
86:02 - linear regression equation is shown here
86:05 - the variable X is considered the
86:07 - independent or predictor variable the
86:10 - variable Y is the dependent or outcome
86:12 - variable we have data on both x and y we
86:16 - use this information to estimate the
86:17 - value of the intercept beta 0 and the
86:20 - slope beta 1 that relate to x and y
86:24 - since the linear relationship is not
86:26 - exact we include an error term in the
86:29 - model Epsilon why is this useful because
86:32 - once we have estimates of beta0 and
86:34 - beta1 from our regression we can use
86:36 - this for any value of x to predict what
86:39 - the value of y would be so if we have
86:41 - data on NBA players weight and height we
86:43 - can estimate how much a typical NBA
86:45 - player weighs based on his height then
86:47 - if someone wants to join the NBA and we
86:49 - know what is height is we can estimate
86:51 - what weight he should be to make it to
86:53 - the NBA in linear regression analysis
86:56 - our goal is to estimate a pattern in
86:57 - this case a line the best fits the data
87:00 - the best fit for our data will go
87:02 - through the core of our data and
87:03 - minimize error the linear relationship
87:06 - in algebra is when a line is represented
87:08 - by its slope and intercept but instead
87:10 - of an exact relationship regression
87:12 - analysis estimates the line from data
87:14 - since the line does not fit data points
87:17 - precisely there is an error term AI
87:19 - measuring the deviation of actual Y from
87:22 - estimated y using the least squares koi
87:25 - fish
87:25 - estimation we can obtain the line that
87:27 - best fits the data by minimizing the sum
87:29 - of squared errors
87:35 - the total variance in Y is divided into
87:37 - two parts
87:38 - that which can be explained by X using
87:40 - regression and that which cannot no line
87:43 - is perfect there's always some error in
87:44 - the estimation unless there's a
87:46 - comprehensive dependency between the
87:48 - predictor and response there's always
87:50 - some part of the response Y that can't
87:52 - be explained by the predictor X using
87:55 - the mean value of y as our reference
87:57 - point we can decompose the total error
87:59 - in measurement between the part that is
88:01 - explained by the regression line and the
88:03 - part that remains unexplained
88:08 - here's a look at the decomposition the
88:12 - sum of squares regression or SSR is the
88:15 - explained variation attributable to the
88:17 - linear relationship between X and y the
88:20 - sum of squares error or SSE measures the
88:23 - variation attributable to factors other
88:25 - than the linear relationship between x
88:27 - and y the SST is the total sum of
88:31 - squares the SSR and SSE together make up
88:34 - the SST
88:41 - so given that total variation SST is the
88:44 - sum of explained and unexplained
88:46 - variation
88:48 - we can divide through by the total sum
88:51 - of squares SST
88:52 - to get the ratios equal to one the ratio
88:55 - of error sum of squares / total sum of
88:57 - squares plus the ratio of regression sum
89:00 - of squares / total sum of squares equals
89:02 - one
89:02 - either of these two ratios can be used
89:05 - to measure our model fit error sum of
89:07 - squares show is called the mean square
89:09 - error we want to choose models with the
89:12 - lowest mean square error regression sum
89:14 - of square ratio is called R squared or
89:16 - the coefficient of determination we
89:19 - choose a model with the highest R
89:20 - squared because R squared + mean square
89:23 - error equals 1 it has to be true that R
89:26 - square our coefficient of determination
89:27 - has to lie between 0 & 1
89:32 - likewise we can look at the coefficients
89:34 - of the intercept and slope to see if
89:36 - they're significantly different from
89:37 - zero this is done by examining the T
89:39 - statistic and the corresponding p-values
89:41 - p-value is less than 0.05 imply that the
89:45 - coefficient is significantly different
89:47 - from zero some key assumptions before
89:49 - you apply regression techniques first
89:52 - the variables have to have a linear
89:53 - relationship in this example the
89:55 - relationship between x and y is not
89:57 - linear so we cannot fit a linear
89:58 - regression line the variables also have
90:02 - to be approximately normally distributed
90:03 - in this example Y is not normally
90:05 - distributed at each value of x so it
90:08 - doesn't make sense to fit a linear
90:09 - regression line
90:12 - additionally the variance of y at each
90:14 - value of x should be the same or in
90:16 - other words we should have a homogeneity
90:17 - of variances the fourth and final
90:21 - assumption is that the observations are
90:23 - independent here one trendline is not
90:25 - sufficient in other words if sales today
90:27 - depend on sales yesterday then linear
90:30 - regression models will not work this
90:33 - concludes our video on regression
90:34 - today we covered simple linear
90:37 - regression analysis regression line
90:40 - fitting observed and predicted value
90:50 - you
91:10 - [Music]
91:15 - this lesson covers the t-distribution
91:17 - and how it compares to the normal
91:19 - distribution as well as a brief look at
91:21 - the student's t-distribution for large
91:24 - samples the normal distribution applies
91:26 - for small samples the standard deviation
91:29 - is measured in precisely and the data
91:31 - followed the T distribution a T
91:33 - distribution will approach a normal
91:35 - distribution for a larger n greater than
91:37 - or equal to 100 but it has fatter tails
91:40 - for a smaller n less than 100 the T
91:44 - distribution is very similar to the
91:45 - standard normal distribution it also has
91:47 - a bell curve but the standard deviations
91:49 - are computed from the sample data
91:51 - instead of the population suppose a
91:54 - simple random sample of size n is drawn
91:56 - from a population whose distribution can
91:58 - be approximated by a normal Mu Sigma
92:00 - model when the standard deviation is
92:03 - known then the sampling model for the
92:04 - mean X is distributed as a normal
92:07 - distribution with mean x-bar and
92:09 - standard deviation Sigma divided by the
92:11 - square root of n when the standard
92:13 - deviation is estimated from the sample
92:15 - standard deviation s the sampling model
92:17 - follows a T distribution with degrees of
92:19 - freedom and minus 1 this is the one
92:22 - sample T statistic in this figure both
92:27 - distributions have 0 means but the
92:29 - variances are a bit different the T
92:31 - distribution has a lower peak and fatter
92:33 - tails
92:35 - this concludes our video on t
92:37 - distributions today we covered the T
92:39 - distribution and showed how it is very
92:40 - similar to the normal distribution we
92:42 - also briefly showed the students T
92:44 - distribution
92:46 - [Music]
93:02 - in this video we will cover logistic
93:05 - regression including the need for
93:07 - logistic regression the logistic
93:09 - regression model and odds ratios and
93:11 - prediction in many instances when you're
93:15 - testing hypotheses and making
93:17 - predictions you will have dichotomous
93:18 - outcomes for example in a game you can
93:21 - either win or lose on a website a user
93:24 - either clicks or does not click in an
93:27 - election a person votes for a candidate
93:29 - or does not when the outcome variable is
93:32 - categorical such as our game example it
93:35 - does not follow a normal distribution
93:36 - the outcome variable is a probability
93:39 - measured between 0 & 1 the estimates you
93:42 - make should be numbers in that range a
93:44 - linear model cannot be applied we need a
93:46 - nonlinear function there are many
93:48 - nonlinear models we can choose from to
93:50 - fit our data some nonlinear functions
93:53 - are shown here the last one shown is the
93:55 - logistic function that will fit the data
93:57 - the best because it has an upper and
93:59 - lower bound here's a logistic regression
94:03 - model in orange versus a linear
94:04 - regression model in blue isn't it a
94:07 - better fit for the data to best suit our
94:09 - data we want a model that predicts
94:11 - probabilities between 0 and 1 so it will
94:13 - be s-shaped there are lots of s-shaped
94:16 - curves but the logistic regression model
94:18 - is what we'll use in this instance the
94:20 - logistic function is a nonlinear
94:22 - function of independent variables
94:24 - however we can convert this nonlinear
94:26 - function into a linear relationship
94:27 - using the log of the odds ratio note
94:31 - that instead of modeling just zeros and
94:32 - ones we're modeling the probability of
94:34 - an event occurring with a logistic
94:37 - regression model instead of winning or
94:39 - losing we build a model for log odds of
94:41 - winning or losing it's a natural
94:43 - logarithm of the odds of the outcome P
94:45 - stands for the probability of the
94:47 - outcome while 1 minus P stands for the
94:49 - probability of not getting an outcome
94:51 - however having log of P over 1 minus P
94:54 - on the y axis is not very helpful we
94:57 - have to compute the actual odds to do
94:59 - that we have to use the exponential
95:01 - functions let's look at a very simple
95:03 - example of a log it function does
95:05 - alcohol drinking predict political party
95:08 - political party is the outcome variable
95:10 - and it is binary therefore we need a
95:13 - logistic regression a typical log in
95:15 - equation contains
95:16 - the log of the odds ratio is the outcome
95:18 - which is a linear function of the
95:19 - predictors X the log it model to measure
95:23 - the impact of drinking on voter choice
95:25 - is going to be set up as follows the log
95:28 - of the odds ratio will be measured from
95:30 - data on X where X here is the number of
95:32 - drinks per week it's really important to
95:35 - understand that negative 1 point 4 is
95:37 - measuring the log of the odds ratio in
95:39 - other words it is the log of the
95:40 - probability of being Republican divided
95:43 - by the probability of not being a
95:44 - Republican to get the actual odds ratio
95:47 - you have to compute the exponent which
95:49 - is equal to 0.25 since the odds are less
95:53 - than 1 it tells us that the more you
95:55 - drink the lower your odds of being
95:56 - Republican all these calculations can be
95:59 - done automatically in SAS but it's
96:01 - important to understand the math behind
96:03 - what SAS is doing the same model can be
96:06 - extended to more than one variable we
96:08 - just add more predictors to the equation
96:11 - the coefficient beta measures the impact
96:14 - of x on the log of the odds ratio for
96:16 - example in linear regression if y equals
96:19 - 2 plus 3x a one unit increase in X will
96:22 - increase Y by three units in a logistic
96:25 - regression log P over 1 minus P minus 2
96:29 - plus 3x shows us that if x increases by
96:32 - one unit then the log odds of P y equals
96:35 - 1 increases by three units the impact on
96:39 - the odds ratio is represented by E
96:41 - exponent beta we can also compute
96:44 - probabilities directly to compute odds
96:46 - we have to use the exponent if we don't
96:48 - want to look at odds but the actual
96:50 - probabilities we apply the entire
96:52 - logistic function formula as shown in
96:54 - this probability function these are the
96:58 - odds ratios and log of odds ratios for
97:00 - various probabilities an important point
97:03 - to understand is how the odds ratios are
97:05 - tied to the probabilities note the
97:07 - mathematical equivalencies a 50%
97:10 - probability or probability of 0.5 is the
97:13 - same as 1 to 1 odds the log of the odds
97:16 - ratio at that point is equal to 0 as
97:19 - probability increases odds ratio
97:22 - increases from 0 to infinity while the
97:24 - log of the odds ratio can become any
97:26 - value
97:28 - this concludes our video on logistic
97:30 - regression today we covered the need for
97:32 - logistic regression logistic regression
97:35 - model and odds ratios and prediction
97:39 - [Music]
97:56 - this video will cover two types of
97:58 - statistical error type 1 or alpha and
98:02 - type 2 or beta all statistics derived
98:05 - from samples are subject to error a type
98:08 - 1 error rejects the null hypothesis when
98:10 - it is actually true a type 2 error
98:12 - accepts the null hypothesis when it is
98:15 - not true remember that a different
98:17 - sample can give a completely different
98:18 - result a sample mean is likely to fall
98:21 - in the confidence interval only 95% of
98:23 - the time so the inferences drawn from
98:25 - the sample may be wrong
98:27 - let's talk in a little more detail about
98:29 - the type 1 error the type 1 error occurs
98:31 - when a researcher thinks he or she has
98:33 - found a significant result but really
98:35 - that result is due to chance it's
98:37 - similar to a false positive on a drug
98:39 - test the type 1 error or the mistake of
98:42 - rejecting the true null hypothesis will
98:44 - happen with a frequency of alpha thus if
98:47 - alpha our critical value is 0.05 then a
98:50 - type 1 error will occur 5% of the time
98:53 - on the other hand a type 2 error occurs
98:55 - when results seem insignificant but in
98:58 - fact there was something significant
98:59 - going on type 2 errors are like a false
99:02 - negative on a drug test they occur when
99:04 - the alternative hypothesis is true but
99:06 - there's not enough evidence in the
99:08 - sample to reject the null hypothesis
99:10 - this type of error is traditionally
99:12 - considered less important than a type 1
99:14 - error but it can lead to serious
99:15 - consequences in real situations the
99:19 - power of a test is 1 minus the
99:21 - probability of a type 2 error it is the
99:24 - probability of rejecting the null
99:25 - hypothesis when the alternative
99:27 - hypothesis is true in these competing
99:29 - sampling distributions alpha is set to
99:32 - point zero 5 the bottom curve assumes H
99:34 - a is true the top curve assumes that the
99:37 - null hypothesis H naught is true its
99:40 - right tail shows that we will reject H
99:42 - naught when a sample mean exceeds one
99:44 - eighty nine point six the probability of
99:47 - getting a value greater than one eighty
99:49 - nine point six on the bottom curve is
99:51 - 0.5 one six zero corresponding to the
99:54 - power of the test here's a table that
99:56 - summarizes the types of errors here's an
100:00 - example using a fire alarm if a fire
100:02 - alarm is silent and there is no fire our
100:04 - null hypothesis that it is working is
100:07 - correct but what if the assumption
100:09 - wrong then we've accepted the null
100:11 - hypothesis but we actually have a fire
100:13 - that's our type 1 error the opposite
100:16 - case may also happen if the alarm goes
100:18 - off and there's actually a fire there's
100:20 - no error but if there's no fire and the
100:23 - alarm goes off it's a false alarm that's
100:25 - the type 2 error here it is the less
100:28 - serious problem this concludes our video
100:31 - on statistical error today we discuss
100:34 - type 1 or alpha error and type 2 or beta
100:37 - error
100:39 - [Music]
100:52 - this video will cover hypothesis testing
100:55 - which is also called significance
100:57 - testing and occurs when we test a claim
100:59 - about a population parameter using
101:01 - sample evidence that confirms or rejects
101:04 - the claim there are four steps in the
101:06 - hypothesis testing process all of which
101:08 - will be covered in this video here's a
101:10 - summary of the four steps in hypothesis
101:12 - testing after this we'll discuss each
101:14 - step in detail the first step is stating
101:16 - the null and alternative hypotheses we
101:19 - have to establish what we are testing to
101:21 - be true once we do that we have to
101:23 - decide how close to true our sample
101:25 - statistic has to be for us to accept the
101:27 - truth for example we might want our
101:29 - estimate to be accurate with a 5% margin
101:31 - of error this is called locating the
101:33 - critical region once we know that we
101:35 - have to compute the test statistic the Z
101:38 - value or the T value finally based on
101:40 - our results we draw conclusions from the
101:42 - study the first step in the procedure is
101:45 - to convert the research question to a
101:46 - statement of the hypotheses null and
101:48 - alternative forms our study will be to
101:51 - collect and seek evidence against the
101:53 - null hypothesis as a way of deductively
101:55 - bolstering the alternative hypothesis
101:57 - the null hypothesis abbreviated h naught
102:00 - is a statement of no difference in other
102:03 - words the null hypothesis argues that
102:04 - there is no significant difference
102:06 - between our specified populations and
102:08 - that any observed difference is due to
102:10 - sampling or experimental error the
102:12 - alternative hypothesis or H a is the
102:15 - opposite of the null hypothesis it
102:17 - provides a statement of difference in
102:19 - our study we will seek evidence against
102:21 - the claim of H naught as a way of
102:23 - proving h a here's an example of setting
102:26 - up the null and alternative hypotheses
102:28 - in the late 1970s the weight of US men
102:31 - between 20 and 29 years of age had a log
102:33 - normal distribution with a mean of 170
102:36 - pounds and a standard deviation of 40
102:38 - pounds to illustrate the hypothesis
102:40 - testing procedure we asked if body
102:42 - weight in this group has changed since
102:44 - 1970 this is called our research
102:46 - question and it can be answered in one
102:48 - of two ways under the null hypothesis
102:50 - there is no difference in the mean body
102:52 - weight between then and now in which
102:53 - case Mew would still equal 170 pounds
102:56 - under the alternative hypothesis we
102:59 - assert that the mean weight has changed
103:01 - EMU's not equal to 170 pounds this is
103:04 - called a two-sided test
103:05 - the most common form of hypothesis
103:07 - testing we can also do a one-sided test
103:10 - in which we ask if weight has increased
103:12 - over time so the alternative hypothesis
103:14 - would be mu is greater than 170 pounds
103:17 - in step 2 we will locate the critical
103:20 - region once we've established the
103:22 - research question we have to define the
103:24 - level of accuracy with which we want to
103:26 - measure our test statistic any estimate
103:29 - from a sample will not be exactly the
103:31 - same as the population parameter so we
103:33 - have to decide what we think is likely
103:35 - versus unlikely this is called locating
103:38 - the critical region the critical region
103:40 - consists of outcomes that are very
103:41 - unlikely to occur if the null hypothesis
103:44 - is true or in other words the sample
103:46 - means that are almost impossible to
103:48 - obtain when we're estimating population
103:50 - parameters using a sample we have to
103:52 - determine the cutoff values these cutoff
103:55 - values are called alpha if we decide
103:57 - that we want to measure the mean with a
103:59 - 90 percent precision level then the
104:01 - shaded area on the left and right will
104:03 - be larger if we want to measure with a 1
104:05 - percent precision then the area will be
104:07 - smaller and the range will be larger
104:09 - these are the locations of the critical
104:12 - region boundaries for three different
104:13 - levels of significance alpha equals 0.05
104:17 - alpha equals 0.01 and alpha equals 0.01
104:22 - note that boundaries get wider as the
104:24 - critical value Falls in most cases
104:27 - researchers choose an alpha of 0.05 or
104:30 - point zero one our rejection region
104:32 - should have a probability of alpha if
104:34 - the null hypothesis is true but some
104:37 - bigger probability if the alternative
104:38 - hypothesis is true so if the mean lies
104:41 - inside the cutoff value for alpha then
104:43 - the null hypothesis is true otherwise we
104:45 - fail to accept the null hypothesis the
104:48 - result is significant beyond the alpha
104:50 - level for example if alpha is 0.05 our
104:53 - result is significant if it's less than
104:55 - point zero five once we decide whether
104:57 - we want to measure accuracy at the 10
104:59 - percent 5% or 1% level we can compute
105:03 - the test statistic here we will use the
105:05 - z-score which is a ratio comparing the
105:07 - obtained difference between the sample
105:09 - mean and the hypothesized population
105:11 - mean this is an example of a one sample
105:14 - test of a mean when the standard
105:16 - deviation Sigma is known in our male way
105:19 - example we're going to use the Z
105:20 - statistic because we know the population
105:22 - mean and the population standard
105:24 - deviation to compute the Z statistic we
105:27 - simply insert values derived from our
105:29 - sample into the formula if in one sample
105:32 - we found that the sample mean was 173
105:34 - then the Z statistic would be 0.6 0 this
105:37 - value on the x-axis under a standard
105:40 - normal curve let's say we found the
105:42 - sample mean to be 185 putting these
105:44 - values into the Z stat formula we find
105:46 - the z stat is 3.0 this is much higher at
105:49 - the tail end of the x-axis on a normal
105:52 - distribution the final step is drawing
105:54 - conclusions
105:55 - once we've computed the Z value of our
105:57 - test statistic we have to look at the
105:59 - corresponding probability values to find
106:01 - out if it's reasonably close to the
106:02 - population mean a large value shows that
106:05 - the obtained mean difference is large
106:07 - and in the critical region the
106:09 - difference is significant which means we
106:11 - have to reject the null hypothesis that
106:12 - the weights have not changed over time
106:14 - if the mean difference is relatively
106:16 - small then the test statistic will have
106:19 - a low value in this case we conclude
106:21 - that the evidence from the sample is not
106:23 - sufficient and the decision is to fail
106:25 - to reject the null hypothesis the
106:27 - p-value is the area under the normal
106:28 - curve in the tails beyond the z stat it
106:31 - answers the question what is the
106:33 - probability of the observed test
106:35 - statistic or one more extreme when H
106:37 - naught is true to convert Z statistics
106:39 - to p-value we will use software in one
106:42 - sample with a sample mean of 173 the z
106:45 - statistic was 0.6 0 if we had this
106:48 - sample we would fail to reject the null
106:49 - hypothesis that the mean weights have
106:51 - increased over time likewise if we
106:53 - computed the p-values for Z equals 3.0
106:57 - we would get point zero zero 1 which
107:00 - means we have to reject the null
107:01 - hypothesis that the mean weight has
107:03 - remained the same over time note that
107:06 - when we're looking at weight change
107:07 - instead of weight increase all we have
107:09 - to do is multiply the one-sided p-value
107:11 - by 2 to do a two-tailed test since we
107:15 - will be using p-values in all our
107:17 - subsequent analysis it's worth
107:19 - emphasizing what that means p-values ask
107:22 - the question what is the probability of
107:24 - the observed test statistic when H
107:27 - naught is true remember the smaller the
107:29 - p-value the more likely that your null
107:31 - hypothesis
107:32 - this is not true this graphic depicts
107:34 - the significance of p-values at less
107:36 - than one percent between one and five
107:38 - percent between five and ten percent
107:41 - and greater than ten percent these are
107:43 - common significance levels five percent
107:46 - is the most common cutoff however a note
107:48 - that is unwise to draw firm borders for
107:51 - significance as an example a p-value of
107:54 - 0.27 would not be significant against H
107:57 - naught a p-value of 0.01 on the other
107:59 - hand would be highly significant against
108:01 - H naught this concludes our video on
108:04 - hypothesis testing also called
108:06 - significance testing which occurs when
108:08 - we test a claim about a population
108:10 - parameter using evidence that confirms
108:12 - or rejects that claim today we covered
108:15 - the four steps in hypothesis testing
108:16 - state the null and alternative
108:18 - hypotheses locate the critical region
108:21 - compute the test statistic and draw
108:24 - conclusions
108:26 - [Music]
108:44 - this presentation will cover correlation
108:47 - including a definition of correlation a
108:50 - discussion of the need for correlation
108:53 - details on computing correlation
108:55 - including variance covariance and the
108:59 - correlation coefficient strength of
109:01 - Association linear and curvilinear
109:04 - relationships
109:06 - properties of correlation R squared the
109:09 - coefficient of determination and a
109:12 - discussion of correlation versus
109:14 - causation correlation is one of the most
109:17 - common and useful statistics it's a
109:19 - measure of Association a single number
109:22 - that describes the degree of
109:23 - relationship between two variables we
109:26 - can examine correlations between two
109:27 - variables heuristic ly by looking at a
109:29 - scatter chart in this chart our
109:32 - observations are very tightly centered
109:33 - around the line in this case we would
109:35 - say that the relationship between x and
109:37 - y is more correlated we call this a
109:40 - strong correlation by contrast if the
109:43 - observations are scattered further out
109:44 - we might say the relationship between x
109:47 - and y is less correlated or that there
109:49 - is a weak correlation here are some
109:51 - examples of questions that ask about
109:53 - correlation is there any association
109:56 - between hours of study and grades
109:58 - is there any association between the
110:00 - number of churches in a city and the
110:02 - murder rate when the weather gets hot
110:04 - what happens to sweater sales what is
110:08 - the strength of association between them
110:09 - what about the sale of ice-cream versus
110:12 - temperature what is the strength of
110:14 - association between them furthermore how
110:17 - do we quantify the association while we
110:19 - can guess the relationship there's a
110:21 - better way to do this using statistical
110:23 - measures the measure we use is the
110:25 - Pearson correlation coefficient to
110:28 - compute correlation we'll need
110:29 - information on standard deviation and
110:31 - covariance we know that the variance is
110:34 - the dispersion within a variable X or Y
110:36 - or the squared average deviation from
110:38 - the mean as shown here the covariance is
110:41 - the dispersion of X multiplied by the
110:43 - dispersion in Y it is calculated as the
110:46 - average of the product of deviations in
110:48 - individual means using the information
110:50 - on variance and covariance we can
110:52 - compute the correlation coefficient as
110:54 - the covariance of x and y divided by the
110:57 - state
110:57 - deviation of X multiplied by the
110:59 - standard deviation in Y this measure of
111:02 - correlation ranges from negative one to
111:04 - positive one a higher number is a
111:07 - stronger correlation and the lower
111:09 - number is a weaker correlation
111:11 - correlation coefficient are measures the
111:13 - strength of linear Association it
111:16 - measures the extent to which two
111:17 - variables are proportional to each other
111:19 - it's unit free so for example a measure
111:22 - of correlation between player height
111:24 - measured in inches and player weight
111:26 - measured in pounds will be meaningful
111:28 - even if they're measured in different
111:29 - units here are some examples no linear
111:33 - Association negligible negative
111:35 - Association weak positive Association
111:38 - moderate negative Association very
111:40 - strong positive Association very strong
111:43 - negative association in these scatter
111:46 - plots what is happening to Y as X is
111:48 - increasing an important point to
111:50 - remember is that correlation is a
111:52 - measure of linear Association if the
111:55 - relationship is curvilinear using the
111:57 - correlation measure is not appropriate
111:59 - if X changes and Y stays the same then
112:02 - the correlation is zero since the
112:05 - correlation measure is a measure of
112:06 - linear Association we cannot use
112:09 - correlations on categorical data it's
112:12 - related to sample size and it's also
112:13 - very sensitive to outliers the
112:16 - correlation measure R measures the
112:17 - strength of linear Association squaring
112:20 - the correlation coefficient gives us R
112:22 - squared which is the coefficient of
112:24 - determination it is the proportion of
112:27 - common variation in two variables this
112:29 - measures the strength or the magnitude
112:31 - of the relationship while we cannot use
112:33 - percentage to interpret R we can do so
112:36 - for R squared for example if R squared
112:39 - equals 67% then we can say that 67% of
112:43 - variation in X is related to variation
112:46 - in Y correlation does not imply
112:48 - causation it's easy to see that in this
112:50 - chart the Internet Explorer market share
112:52 - correlates with the murder rate in the
112:54 - US but that doesn't mean that one caused
112:56 - the other causal relationships are
112:58 - determined based on facts and business
113:01 - models we cannot determine causality
113:03 - from data correlation is a mathematical
113:06 - formula you will get a number no matter
113:08 - what data you feed first you need to
113:10 - establish a logic
113:11 - relation and then find the correlation
113:13 - variables may be correlated if they have
113:16 - a causal relationship for example water
113:18 - causes plants to grow correlation can
113:21 - also occur when one variable is both the
113:23 - cause and the effect
113:25 - for example coffee consumption can cause
113:27 - nervousness but it's possible that
113:29 - nervous people also drink more coffee
113:32 - correlation can also be high because
113:34 - both variables move together due to a
113:36 - missing third variable for example this
113:39 - comparison of deaths due to drowning and
113:41 - soft drink consumption during summer
113:43 - both variables are related to heat and
113:45 - humidity a third variable not shown here
113:47 - emitting such variables can be dangerous
113:51 - here's a look at some additional
113:52 - measures of correlation using scatter
113:54 - charts
113:57 - this concludes our video on correlation
114:00 - today we discuss the definition of
114:02 - correlation the need for correlation
114:05 - details on computing correlation
114:07 - including variance covariance and the
114:10 - correlation coefficient strength of
114:12 - Association linear and curva linear
114:16 - relationships properties of correlation
114:18 - R squared the coefficient of
114:21 - determination and correlation versus
114:23 - causation
114:25 - [Music]
114:43 - this video will cover binomial
114:45 - distributions which are a type of
114:47 - discrete distribution we will first
114:49 - compare discrete and continuous
114:50 - distributions of a single random
114:52 - variable and then we'll look at the
114:54 - binomial distribution specifically there
114:57 - are two types of random variables
114:59 - discrete and continuous a discrete
115:02 - random variable has only a finite number
115:04 - of possible values whereas a continuous
115:06 - random variable has a continuum of
115:08 - possible values usually a discrete
115:11 - distribution results from account
115:13 - whereas a continuous distribution
115:14 - results from a measurement the
115:17 - distinction between counts and
115:18 - measurements is not always clear-cut a
115:20 - probability distribution is simply a
115:22 - mapping of all distinct events for a
115:23 - variable and their probability of
115:25 - occurrence such as the distribution of a
115:27 - coin flip experiment the form of the
115:29 - distribution depends on whether the
115:31 - variables are discrete or continuous
115:33 - here are some examples of discrete
115:35 - variables outcomes of dice rolls whether
115:38 - a customer likes or dislikes a product
115:40 - or the number of hits on a website some
115:43 - examples of continuous variables include
115:45 - the weekly change in the Dow Jones
115:47 - industrial average daily temperature or
115:50 - the time between machine failures to
115:53 - specify the probability distribution of
115:55 - event X we need to specify all of its
115:57 - possible values and their probabilities
115:59 - we assume that there are K possible
116:01 - values and write out our list of
116:03 - possible values like this a typical
116:05 - value is denoted like this and the
116:09 - probability of a typical value is
116:10 - denoted like this next we will discuss
116:13 - distributions of both discrete and
116:15 - continuous variables for each type of
116:17 - variable distributions can be
116:18 - characterized by three measures mean
116:20 - variance and standard deviation these
116:23 - are formulas for working with discrete
116:25 - distributions well we won't be computing
116:27 - these measures by hand you do need to be
116:29 - aware of the formulas the mean also
116:32 - called the expected value is calculated
116:34 - with this formula the mean is a weighted
116:36 - sum of all possible values weighted by
116:38 - their probabilities mean is denoted by
116:40 - the Greek letter mu the variance has a
116:43 - weighted sum of the squared deviations
116:45 - of the possible values from the mean
116:46 - where the weights are again the
116:48 - probabilities the standard deviation is
116:51 - simply the square root of the variance
116:52 - standard deviation is denoted by the
116:55 - Greek letter Sigma
116:57 - these are the formulas for working with
116:59 - continuous distributions
117:02 - a probability distribution visually
117:05 - summarizes the probabilities associated
117:07 - with all possible events for a variable
117:09 - we will focus on three probability
117:11 - distributions that are commonly used in
117:13 - explaining real-world events binomial
117:16 - and exponential distributions are used
117:18 - with discrete data while normal
117:20 - distributions are used with continuous
117:21 - data a binomial distribution is a
117:24 - discrete distribution that represents
117:26 - the number of successes in n independent
117:28 - trials each of which has the probability
117:30 - of success P each trial has a binary
117:34 - outcome for example a coin toss yields
117:36 - either heads or tails the probability of
117:41 - either observation heads or tails is the
117:43 - same each time we toss the coin these
117:46 - outcomes are generally called success
117:47 - and failure the probability of success
117:49 - is P and the probability of failure is 1
117:52 - minus P the distribution Maps the
117:55 - outcome of all the trials each trial has
117:57 - to be independent and the probability of
117:59 - success has to be the same for each
118:00 - trial this is the probability mass
118:03 - function formula for a binomial
118:04 - distribution if we toss a coin 100 times
118:07 - what is the probability that we will get
118:09 - 40 heads what is the probability of
118:11 - getting 90 heads that probability can be
118:14 - computed by applying this formula we
118:16 - have only two possible outcomes 1 0 or
118:19 - success/failure in n independent trials
118:21 - this formula depicts the probability of
118:24 - exactly X successes n is the number of
118:28 - trials x is the number of successes out
118:30 - of n trials P is the probability of
118:33 - success and 1 minus P is the probability
118:36 - of failure all probability distributions
118:39 - are characterized by an expected value
118:41 - and variance if we toss a coin 100 times
118:43 - what would be the average number of
118:45 - heads we would get what about the
118:47 - variance these are computed using these
118:49 - formulas you'll often see the
118:51 - assumptions of normal distribution being
118:53 - applied to discrete outcomes this is
118:55 - because the binomial distribution
118:56 - approximates to a normal distribution
118:58 - for large samples so for large enough
119:01 - samples we can calculate probabilities
119:02 - using normal probability rules this
119:06 - concludes our video on binomial
119:08 - distributions today we covered discrete
119:10 - and continuous distributions of a single
119:12 - random variable and the binomial
119:14 - distribution
119:17 - [Music]
119:35 - this video will cover normal
119:37 - distributions the probability density
119:40 - function cumulative distribution
119:43 - functions the 6895 99.7 rule and
119:48 - standardizing z values the single most
119:51 - important distribution in statistics is
119:53 - the normal distribution it is a
119:55 - continuous distribution and it's the
119:57 - basis of the familiar symmetric
119:59 - bell-shaped curve the mean of the normal
120:01 - distribution is in the center the
120:04 - standard deviations are marked at equal
120:05 - distances from the mean
120:07 - any particular normal distribution is
120:09 - specified by its mean and standard
120:10 - deviation by changing the mean the
120:13 - normal curve shifts to the left or the
120:14 - right by changing how spread out the
120:17 - standard deviations are the curve also
120:19 - changes standard deviations can be
120:21 - spread out wider or closer together
120:24 - therefore there are really many normal
120:27 - distributions not just a single one the
120:29 - normal distribution is a two parameter
120:30 - family where the two parameters are the
120:33 - mean and the standard deviation here's a
120:35 - tool you can play with online that
120:37 - illustrates a normal distribution in
120:39 - real life it looks like a triangular
120:41 - shaped pegboard into which balls are
120:42 - dropped when there's an equal
120:44 - probability that the balls will drop
120:46 - either left or right their final
120:48 - placement forms a normal distribution
120:50 - however when the probability of the
120:52 - balls dropping left or right is unequal
120:54 - which is something you can experiment
120:55 - with using this tool the distribution
120:58 - changes
121:02 - the formulas for mean and standard
121:04 - deviation are very complex but you will
121:06 - not have to compute them because the
121:08 - software will with continuous variables
121:11 - there is a continuum of possible values
121:12 - such as all values between 0 and 100 or
121:16 - all values greater than zero instead of
121:18 - assigning probabilities each individual
121:20 - value in the continuum the total
121:22 - probability of one is spread over this
121:24 - continuum thus the shaded area within
121:27 - the bell curve will always have an area
121:29 - of one the key to this spreading is
121:31 - called a density function which acts
121:33 - like a histogram the higher the value of
121:36 - the density function the more likely
121:37 - this region of the continuum is a
121:40 - density function usually denoted by FX
121:43 - specifies the probability distribution
121:45 - of a continuous random variable X the
121:48 - higher FX is the more likely X is
121:51 - probabilities are found from a density
121:54 - function as areas under the curve so for
121:57 - example the shaded portion under this
121:59 - bell curve represents the probability of
122:00 - X being between 65 and 75 the cumulative
122:04 - distribution function or CDF is the
122:07 - probability that the variable takes a
122:08 - value less than or equal to X it is the
122:11 - total area under the normal curve up to
122:13 - X here's an example
122:17 - the beauty of the normal curve is that
122:19 - no matter what its mean and standard
122:20 - deviation are the area between the mean
122:22 - minus one standard deviation and the
122:24 - mean plus one standard deviation is
122:26 - always about 68% the area between the
122:29 - mean minus two standard deviations and
122:31 - the mean plus two standard deviations is
122:33 - always about 95% and the area between
122:36 - the mean minus three standard deviations
122:37 - and the mean plus three standard
122:40 - deviations is always about ninety-nine
122:42 - point seven percent that means almost
122:44 - all values fall within three standard
122:45 - deviations on either side of the mean
122:47 - this is true for all normal curves no
122:50 - matter their shape but how good is this
122:52 - rule for real data let's go ahead and
122:54 - check out an example here's our data the
122:56 - mean of the weight of one hundred and
122:58 - twenty women runners in the sample is
123:00 - one hundred and twenty seven point eight
123:02 - pounds the standard deviation is fifteen
123:04 - point five here's what our distribution
123:06 - would look like let's look a little more
123:08 - closely at that distribution 68% of our
123:11 - 120 runners is about 83 runners
123:14 - according to the 68 95 99.7 rule those
123:18 - runners should all fall within one
123:20 - standard deviation of the mean weight of
123:22 - one hundred and twenty seven point eight
123:24 - that is eighty-three of our runners
123:25 - should fall between one hundred and
123:27 - twelve point three and one hundred and
123:29 - forty three point three pounds when we
123:31 - check our data we see that seventy-nine
123:33 - runners fall within one standard
123:34 - deviation of the mean
123:35 - furthermore ninety-five percent of our
123:37 - group or about 114 runners should fall
123:40 - within two standard deviations of the
123:41 - mean or between ninety six point eight
123:44 - and one hundred and fifty eight point
123:45 - eight pounds the data shows that 115
123:48 - runners fall within two standard
123:49 - deviations of the mean finally according
123:52 - to the rule ninety-nine point seven
123:53 - percent of our runners or one hundred
123:55 - and nineteen point six runners should
123:57 - fall within three standard deviations of
123:59 - the mean or within a range of eighty one
124:01 - point three pounds to one hundred and
124:03 - seventy four point three pounds
124:05 - according to our data all 120 runners
124:08 - fall within this range so it seems as if
124:10 - the rule is pretty accurate in this case
124:12 - there are indefinitely many normal
124:14 - distributions one for each pair of
124:16 - standard deviation and mean one
124:18 - particular combination of standard
124:19 - deviation and mean deserves special
124:21 - attention and that is the standard
124:23 - normal distribution all normal
124:25 - distributions can be converted into the
124:27 - standard normal curve by subtracting the
124:29 - mean
124:30 - dividing by the standard deviation but
124:32 - all of the integrals for the standard
124:33 - normal distribution have been calculated
124:35 - and put into a table for us and we also
124:37 - have software to help us out so we never
124:39 - have to integrate the long way this
124:41 - diagram illustrates the conversion of X
124:43 - values into Z values when we convert a
124:45 - normal distribution to a standard normal
124:47 - distribution
124:49 - here's a practice problem if birth
124:51 - weights in a population are normally
124:53 - distributed with a mean of 100 and 9
124:55 - ounces and a standard deviation of 13
124:57 - ounces
124:58 - what is the chance of obtaining a birth
125:00 - weight of 141 ounces or heavier when
125:03 - sampling birth records at random here's
125:05 - how we solve this problem we subtract
125:07 - 109 our mean from 141 and then divide by
125:11 - our standard deviation 13 so Z equals
125:14 - two point four six then we will use the
125:17 - normdist function in excel to discover
125:19 - that our value for Z two point four six
125:21 - equals zero point nine nine three the
125:24 - chance of a baby being born heavier
125:26 - corresponds to the right tail of the
125:28 - distribution so the probability that we
125:30 - will get a value for Z that's greater
125:32 - than or equal to two point four six can
125:35 - be discovered by subtracting our value
125:36 - for Z point nine nine three from the
125:39 - total area of the standard distribution
125:40 - one
125:43 - this concludes our video on normal
125:46 - distributions continuous distributions
125:48 - density functions cumulative
125:50 - distribution functions the 6895 99.7
125:54 - rule and standardizing Z values
125:58 - [Music]
126:12 - this video will cover populations and
126:15 - inferences sampling error and the
126:17 - central limit theorem a population is
126:20 - the set of all members about witches
126:22 - study intends to make inferences here's
126:24 - a population of people
126:25 - we'd like to study there television
126:27 - watching behavior to determine how many
126:28 - watch a particular show so we can decide
126:30 - whether to purchase advertising spots
126:32 - during this period of time but our
126:34 - population is much too large for a
126:36 - feasible study to study their television
126:38 - viewing habits we will have to survey
126:39 - them and it's not feasible to survey
126:41 - every single individual so instead we'll
126:43 - take a sample of the population to study
126:45 - choosing a representative sample we can
126:47 - make some inferences about the
126:48 - population behavior but it's unlikely
126:50 - that one sample can provide accurate
126:52 - measures of behavior for the entire
126:53 - population an estimate of the population
126:55 - parameter or the proportion watching a
126:57 - television show is likely to be
126:59 - different for different samples of the
127:01 - same size and is likely to be different
127:02 - from the population parameter this is
127:04 - called sampling error the sampling error
127:06 - is unknown but we can estimate the
127:08 - extent of this error by applying the
127:09 - central limit theorem the central limit
127:11 - theorem tells us that what we know about
127:13 - our sample can tell us about the larger
127:14 - population the sample came from for any
127:16 - results that are generated from samples
127:18 - we get a range of estimates of a
127:19 - population parameter which includes mean
127:21 - and standard deviation from each sample
127:23 - in our example it would be an estimate
127:25 - of the proportion watching a particular
127:27 - TV show these estimates have their own
127:28 - distribution and the central limit
127:30 - theorem tells us that the distribution
127:32 - looks like a bell curve the central
127:33 - limit theorem makes predicting outcomes
127:34 - a lot easier if the sample size is large
127:37 - enough then the sampling distribution of
127:39 - the mean is approximately normally
127:40 - distributed regardless of the
127:42 - distribution of the population if all
127:43 - possible random samples each of size n
127:45 - are taken from any population with the
127:48 - mean mu and a standard deviation Sigma
127:50 - the sampling distribution of the sample
127:52 - means or averages will have a mean have
127:54 - a standard deviation and be
127:55 - approximately normally distributed
127:57 - regardless of the shape of the parent
127:59 - population
127:59 - remember that normality improves with a
128:01 - larger n and it all comes back to Z note
128:04 - the symbols here the mean of the sample
128:06 - means is noted as mu of x bar the
128:08 - standard deviation of the sample means
128:10 - is written as Sigma of X bar and is also
128:12 - called the standard error of the sample
128:14 - mean that concludes our video today we
128:16 - covered populations and inferences
128:18 - sampling error and the central limit
128:20 - theorem
128:20 - [Music]
128:31 - in this video we will first define
128:33 - probability then we will cover the rule
128:36 - of complements the addition rule
128:38 - probabilistic independence conditional
128:41 - probability and the Bayes theorem a
128:43 - probability is a number between 0 and 1
128:45 - that measures the likelihood that some
128:47 - event will occur for a random variable
128:48 - an event with probability 0 cannot occur
128:51 - whereas an event with probability 1 is
128:54 - certain to occur an event with
128:55 - probability greater than 0 and less than
128:57 - 1 involves uncertainty here are some
129:00 - examples the odds of winning a lottery
129:02 - the likelihood of a particular candidate
129:04 - winning an election or the chance of
129:06 - rolling a 4 on a fair die in the case of
129:09 - the die there are 6 sides so the odds of
129:12 - rolling of four are one out of six the
129:14 - complementary rule in probability is
129:16 - simply the probability of an event not
129:18 - occurring if a is any event the
129:20 - probability of a is P of a the
129:23 - complement of a is the event that a does
129:26 - not occur the probability of the
129:28 - complement of a is shown by this
129:29 - equation one minus the probability of
129:31 - the event occurring in our dice example
129:33 - the probability of getting a four was
129:35 - one in six so the probability of not
129:37 - getting a four is one minus one and six
129:40 - which equals five and six the addition
129:43 - rule of probability involves the
129:44 - probability that at least one of the
129:46 - events will occur events are exhaustive
129:48 - if they exhaust all possibilities one of
129:50 - the events must occur for example when
129:52 - we roll a 6-sided die we will always end
129:54 - up with a number between 1 & 6 we say
129:57 - that events are mutually exclusive if at
129:59 - most one of them can occur for example
130:00 - you can't roll a 3 & a 6 on one die at
130:03 - the same time if you have two mutually
130:05 - exclusive events like our 3 & 6 then the
130:07 - probability of either one occurring is
130:09 - the sum of the two separate
130:10 - probabilities if two events are
130:13 - independent or their outcomes aren't
130:15 - affected by each other then the
130:16 - probability of both a and B occurring is
130:18 - simply the product of the two
130:20 - probabilities in the case of our die the
130:22 - probability of getting a six on the
130:23 - first roll and getting a three on the
130:25 - second roll is one in six times one and
130:28 - six which equals a 1 in 36 chance
130:30 - sometimes the probability of one event
130:32 - will affect another these are called
130:34 - dependent events and their probabilities
130:36 - are called conditional this is the
130:38 - formula for conditional probability the
130:40 - conditional probability of a conditional
130:42 - that B has already occurred
130:44 - is equal to the joint probability of
130:46 - both of the events occurring together
130:47 - divided by the probability of B
130:49 - occurring without regard to whether a
130:51 - has occurred or not
130:53 - the bayes theorem allows us to estimate
130:55 - posterior probabilities once we obtain
130:58 - new data with it we can measure the
130:59 - likelihood of event H occurring once we
131:02 - obtain particular pieces of evidence
131:04 - from data D the parts of the theorem
131:06 - include the independent probability of H
131:08 - or prior probability the independent
131:11 - probability of D the conditional
131:13 - probability of D given H or likelihood
131:15 - and conditional probability of H given D
131:18 - or posterior probability this concludes
131:21 - our video on basic probability today we
131:24 - defined probability and covered the rule
131:26 - of complements the addition rule
131:27 - probabilistic independence conditional
131:30 - probability and the Bayes theorem
131:34 - [Music]
131:47 - in this video we will cover variable
131:50 - roles including explanatory and outcome
131:52 - variables and variable classification
131:54 - including qualitative variables nominal
131:57 - ordinal and binary and quantitative
131:59 - variables discrete continuous interval
132:02 - and ratio any analytics project first
132:05 - begins with a question what is the
132:07 - problem you're trying to solve to
132:09 - address that question we need to collect
132:10 - data the next step in the process is to
132:13 - understand the data collected and only
132:15 - then can we move to further steps of
132:17 - data cleaning data analysis and solving
132:19 - the problem a key step in understanding
132:21 - the information collected is to identify
132:23 - all the variables in the data set we
132:25 - need to know what variable types we have
132:28 - in order to make them amenable to
132:29 - further analysis variables have two
132:32 - possible roles
132:33 - the first is explanatory explanatory
132:36 - variables are also called features or
132:38 - independent variables these are
132:40 - variables that are used as inputs to
132:42 - explain the variation in the outcome
132:44 - variable
132:44 - the second role a variable can take on
132:46 - is outcome an outcome variable is also
132:49 - known as a target or dependent variable
132:51 - these are variables that measure the
132:53 - output or impact that's being studied
132:55 - most studies have many independent
132:57 - variables and one dependent variable for
132:59 - example a person's weight could be a
133:01 - function of age gender and calories
133:03 - consumed fuel efficiency is a function
133:05 - of features such as car size weight and
133:08 - number of cylinders restaurant ratings
133:10 - are a function of food quality ambiance
133:12 - and service variables can be qualitative
133:15 - or quantitative qualitative data can be
133:17 - nominal ordinal or binary quantitative
133:21 - data can be discrete or continuous with
133:23 - either an interval or ratio level of
133:25 - measurement we'll start by discussing
133:27 - how to determine whether a variable is
133:29 - qualitative or quantitative the best way
133:31 - to decide whether a variable is
133:32 - qualitative or quantitative is to use
133:34 - the subtraction test if two experimental
133:37 - units such as people have different
133:39 - values for a particular measure then you
133:41 - should subtract the two values and ask
133:42 - yourself about the meaning of the
133:44 - difference for example when hair color
133:46 - is coded as 1 equals blonde 2 equals red
133:49 - 3 equals brown and 4 equals black the
133:52 - difference between the variables has no
133:53 - meaning so it fails the subtraction test
133:56 - which means hair color is a categorical
133:58 - or qualitative variable however if the
134:00 - difference
134:00 - is meaningful then it is a quantitative
134:02 - variable for example age in years the
134:05 - differences between these numbers have a
134:07 - meaning so the variable is quantitative
134:08 - we will now discuss qualitative
134:11 - variables in detail categorical
134:13 - variables are those that have only a few
134:15 - possible values
134:16 - thus assigning each value to a
134:17 - particular group or category for example
134:20 - oceans are categorical variable nominal
134:23 - and ordinal variables are often called
134:25 - labels a nominal variable has levels
134:28 - with arbitrary names for example car
134:30 - colors ordinal variables have a logical
134:33 - order for example exam grades a
134:35 - dichotomous or binary variable is a
134:38 - categorical variable that has only two
134:40 - levels or categories often the answer to
134:42 - a yes or no question but a variable
134:44 - doesn't have to be a yes/no variable to
134:46 - be binary it just has to have only two
134:48 - categories such as gender we will now
134:50 - discuss quantitative variables in detail
134:52 - quantitative variables are those for
134:54 - which the recorded numbers encode
134:55 - magnitude information based on a true
134:57 - quantitative scale they can be discrete
134:59 - or continuous a discrete variable has
135:01 - only whole number counts a continuous
135:04 - variable can take on any value on the
135:05 - number scale to determine whether a
135:07 - variable is discrete or continuous use
135:09 - the midway test if for every pair of
135:12 - values of a quantitative variable the
135:14 - value midway between them is a
135:16 - meaningful value then the variable is
135:18 - continuous otherwise it's discrete for
135:21 - example age is continuous because the
135:23 - difference between ages 20 and 30 is
135:25 - meaningful
135:27 - an example of a discrete variable is the
135:30 - number of children in a family you can
135:32 - see here that 2.5 does not make sense
135:34 - the interval level of measurement ranks
135:36 - data it can be either discrete or
135:38 - continuous with interval variables
135:40 - precise differences between units of
135:42 - measure exist but there's no meaningful
135:44 - 0 for example take IQ scores make sense
135:47 - to talk about someone having an IQ 50
135:49 - points higher than another person but an
135:51 - IQ of 0 has no meaning ratio variables
135:54 - are interval variables but with the
135:56 - added condition that 0 of the
135:57 - measurement indicates that there is none
135:59 - of that variable true ratios exist when
136:01 - the same variable is measured on two
136:03 - different members of the population for
136:05 - example consider the weight of an
136:06 - individual it makes sense to say that a
136:08 - hundred and fifty pound adult weighs
136:10 - twice as much as a 75 pound child
136:12 - however it doesn't make sense to say
136:14 - that 70 degrees Fahrenheit is twice as
136:16 - hot as 35 degrees Fahrenheit so
136:18 - temperature is not a ratio variable this
136:21 - concludes our video on variables in this
136:23 - video we covered variable roles
136:24 - including explanatory and outcome
136:27 - variables and we also covered variable
136:29 - classification including qualitative
136:31 - variables nominal ordinal and binary and
136:34 - quantitative variables discrete
136:36 - continuous interval and ratio
136:40 - [Music]
136:50 - this video will cover basic information
136:53 - about coding coding systems and types of
136:56 - variables in coding including binary
136:58 - ordinal nominal and continuous coding is
137:02 - the process of translating the
137:03 - information gathered from questionnaires
137:05 - and other investigations into something
137:07 - that can be analyzed usually using a
137:09 - computer program coding involves
137:11 - assigning a value to the information
137:13 - given in a questionnaire and often that
137:15 - value is given a label coding can make
137:17 - the data more consistent for example if
137:19 - you ask the question what gender you
137:21 - might end up with the answers male
137:22 - female M F etc coding will avoid such
137:26 - inconsistencies a common coding system
137:29 - for binary variables is the following
137:30 - zero equals no and one equals yes where
137:34 - the number is the value assigned and the
137:36 - yes or no is the label of that value
137:38 - some like to use a system of ones and
137:40 - twos where one equals no and two equals
137:43 - yes this brings out an important point
137:45 - in coding when you assign a value to a
137:47 - piece of information you must also make
137:48 - it clear what the value means in the
137:51 - first example one equals yes but in the
137:52 - second example one equals no either way
137:55 - is fine as long as it's clear how the
137:56 - data are coded you can make it clear by
137:58 - creating a data dictionary as a separate
138:00 - file to accompany the data set a binary
138:03 - variable is any variable that is coded
138:05 - to have two levels like this example in
138:07 - SAS data representing gender coded as MF
138:11 - would be converted into a binary
138:12 - variable here's an example
138:15 - if we're asking about the number of
138:17 - years of education a person has with a
138:19 - value of 1 for each year of education
138:22 - that would mean anyone with more than 12
138:24 - years of education has been to college
138:25 - and anyone with less than 12 years of
138:28 - education has not been to college we can
138:30 - recode into a binary yes/no variable by
138:33 - saying that if education is greater than
138:35 - 12 that implies that college equals one
138:38 - otherwise college equals zero this type
138:41 - of coding is useful in descriptive and
138:43 - predictive analytics
138:45 - the coding process is similar with other
138:48 - categorical variables for the variable
138:50 - education we might code as follows zero
138:53 - equals did not graduate from high school
138:54 - one equals high school graduate two
138:57 - equals some college or post high school
138:58 - education and three equals college
139:01 - graduate note that for this ordinal
139:03 - categorical variable we need to be
139:05 - consistent with the numbering because
139:07 - the value of the code assigned has
139:08 - significance the higher the code the
139:10 - more educated the respondent is in SAS
139:13 - we would convert years of education to
139:15 - education categories like this
139:21 - here's an example of what not to do zero
139:23 - equals some college or post high school
139:25 - education one equals high school
139:27 - graduate two equals college graduate and
139:30 - three equals did not graduate from high
139:32 - school can you tell what's wrong with
139:33 - this example the data we're trying to
139:36 - code has an inherent order but the
139:38 - coding in this example does not follow
139:39 - that order here's the correct way to do
139:41 - it
139:45 - for nominal categorical variables
139:47 - however the order makes no difference
139:48 - here's an example for the variable
139:51 - reside 1 equals northeast 2 equals South
139:55 - 3 equals northwest 4 equals Midwest and
139:58 - 5 equals southwest it doesn't matter
140:01 - what order we use for these categories
140:02 - Midwest can be coded as 4 2 or 5 because
140:06 - there's not an ordered value associated
140:07 - with each response continuous variables
140:10 - are usually left in the same format as
140:12 - they are in the original data set
140:13 - however be careful about missing values
140:15 - in Muscoda data you may also need to
140:17 - code responses from fill in the blank
140:19 - and open-ended questions with an
140:21 - open-ended question such as why did you
140:23 - choose not to see a doctor about this
140:25 - illness respondents will all answer
140:27 - differently also you may give response
140:30 - choices for a particular question but
140:31 - offer an other specify option as well
140:34 - where respondents can write whatever
140:35 - response they choose these types of
140:38 - open-ended questions can be a lot of
140:39 - work to analyze one way to analyze the
140:42 - information is to group together
140:43 - responses with similar themes for the
140:45 - question why did you choose not to see a
140:47 - doctor about this illness responses such
140:50 - as didn't feel sick enough to see a
140:51 - doctor
140:52 - symptoms stopped and the illness didn't
140:54 - last very long could all be grouped
140:56 - together as the illness was not severe
140:58 - you will also need to code don't know
141:01 - responses typically don't know is coded
141:03 - as 9 that concludes our video on coding
141:06 - with variables today we covered some
141:08 - basic information about coding coding
141:10 - systems and types of variables in coding
141:12 - including binary ordinal nominal and
141:16 - continuous
141:19 - [Music]
141:36 - this video will cover using graphs to
141:39 - understand data we will cover three
141:41 - types of graphs bar charts box plots and
141:44 - histograms it's important to know which
141:47 - graph to use if the variable is
141:50 - categorical look at it using a bar chart
141:52 - if it is continuous you should examine
141:54 - it using either a box and whisker plot
141:56 - or a histogram a bar chart translates
142:00 - the data from frequency tables into a
142:02 - pictorial representation it depicts
142:04 - categorical variables and shows
142:06 - frequency or proportion in each category
142:08 - this bar chart looks at the frequency
142:11 - distribution of patients with pulmonary
142:12 - embolism which occurs when one or more
142:15 - arteries and the lungs gets blocked by a
142:17 - blood clot notice that this is a binary
142:18 - variable with only two possible
142:20 - responses yes and no yes is coded as one
142:24 - and no is coded as zero the frequency
142:26 - distribution of a binary variable shows
142:29 - the number of patients in each group
142:30 - it's much easier to extract information
142:32 - from a bar chart than from a table this
142:35 - chart depicts shock index which is the
142:36 - ratio of heart rate to blood pressure
142:38 - and should lie between 0.5 and 0.8 the
142:41 - higher it is the greater the risk shock
142:43 - index is an ordinal categorical variable
142:46 - the vertical bars here represent the
142:48 - number of patients in each category the
142:50 - shape of a distribution describes how
142:52 - the data are distributed measures of
142:54 - shape include symmetric and skewed the
142:56 - first thing we're looking for in any
142:57 - continuous variable is the shape of
142:59 - distribution what are the boundaries of
143:01 - the data points and how are they
143:03 - clustered if a few small values are
143:05 - mixed in with a majority of values being
143:07 - much higher the data will have a left or
143:09 - negative skew likewise if we have some
143:11 - large values mixed in with a majority of
143:13 - small values the distribution will have
143:15 - a right skew or we say it is positively
143:17 - skewed if the distribution is balanced
143:19 - it is symmetric we can also observe
143:21 - skewness by inspecting the values for
143:24 - instance consider the numeric sequence
143:26 - 49 50 51 whose values are evenly
143:29 - distributed around a central value of 50
143:32 - this produces a symmetric shape we can
143:34 - transform the sequence into a negatively
143:36 - skewed distribution by adding a value
143:38 - far below the mean 40 49 50 51 this
143:42 - produces a left skew similarly we can
143:45 - make this sequence positively skewed by
143:46 - adding a value far above the mean 49
143:49 - 50:51 60 this produces a right skew a
143:53 - box-and-whisker plot provides an easy
143:55 - way to examine the entire distribution
143:57 - of a variable and it's also very useful
143:59 - when we want to examine relationships
144:01 - between two variables where one is
144:02 - categorical and another is continuous
144:04 - let's look at the Box first the bottom
144:07 - of the box represents the 25th
144:09 - percentile while the top of the box
144:11 - represents the 75th percentile the line
144:13 - in the middle represents the median the
144:15 - bigger the box the greater the spread of
144:17 - the data the whiskers in the box plot do
144:19 - not necessarily represent the minimum
144:21 - and maximum values they show the minimum
144:23 - and maximum only if these values are
144:25 - less than one and a half times the
144:27 - interquartile range if the values are
144:29 - bigger than that the whiskers represent
144:31 - one point five times the interquartile
144:33 - range or IQR values outside that range
144:36 - represented as dots in the example here
144:38 - note that there are many dots above the
144:40 - top whisker this is a quick and easy way
144:42 - to check for outliers we can look at the
144:45 - same shock index data with a histogram
144:46 - the dots in the box plot showed us that
144:49 - there were several large values greater
144:51 - than one point five times the
144:52 - interquartile range the same is
144:54 - represented in this histogram with the
144:56 - right skew there's no single rule of
144:58 - thumb for choosing bin sizes the bin
145:00 - sizes you choose will depend on the
145:02 - research question you're asking here
145:04 - using 100 bins shows too much detail and
145:06 - it's not useful
145:07 - likewise too few bins tells us little
145:09 - about the underlying shape of the
145:11 - distribution this example uses two bins
145:13 - and provides too little detail note that
145:15 - the box plot we looked at earlier shows
145:17 - the same positive or right skew that we
145:19 - observe in a histogram for shock index
145:21 - the median inside the box plot also
145:23 - provides information on the skewness of
145:25 - the distribution if the median is at the
145:27 - center of the box the distribution is
145:29 - symmetric if the data have a left skew
145:31 - then the median will be pulled to the
145:33 - right inside the box if the data have a
145:35 - right skew
145:35 - then the median will be pulled to the
145:37 - left in the box this concludes our video
145:38 - on using graphs to understand data today
145:41 - we covered three types of graphs bar
145:42 - charts box plots and histograms
145:47 - [Music]
146:00 - this video provides a quick review of
146:03 - the measures of central tendency
146:04 - including mean median and mode variation
146:07 - and dispersion range for tiles and
146:09 - interquartile range sample variance
146:11 - standard deviation and the normal curve
146:13 - as well as the 6895 99.7 rule before we
146:18 - begin here's a quick review of symbols
146:20 - we'll use in this video
146:25 - we will also use the abbreviation IQR
146:28 - for interquartile range the mean is the
146:31 - average or balancing point to find the
146:32 - mean find the sum of all the values
146:34 - divided by the sample size here's a
146:37 - simple example of calculating the mean
146:38 - of the age of several participants in a
146:40 - study the Sigma is the sum and the x-bar
146:43 - is the sample mean
146:43 - after adding the values together and
146:45 - dividing by the number of values 8 we
146:47 - arrived at our mean 23 point 2 5 we can
146:49 - construct means of binary variables the
146:52 - mean of a binary variable represents the
146:54 - percentage of one's the mean is affected
146:56 - by extreme values which is why we often
146:58 - look at means in conjunction with
147:00 - medians to understand how the data are
147:02 - distributed in this example the mean of
147:04 - the values 1 2 3 4 and 5 is calculated
147:08 - by adding the values together to make 15
147:10 - then dividing the values by 5 the mean
147:12 - of this group is 3 however if the values
147:16 - are 0 1 2 3 4 and 10 the mean shifts to
147:20 - 4 the median is the middle value of the
147:23 - data in this example we have 7 different
147:25 - ages to find the mean we first order
147:28 - them from smallest to largest and then
147:29 - locate the value in the center
147:32 - however if we have an even number of
147:34 - observations median is computed as the
147:36 - average of the two middle values
147:41 - the median is not impacted by outliers
147:44 - here our median of the five values is
147:46 - three if we add the value ten to the set
147:50 - of values our median is still three
147:54 - the mode is the value that occurs most
147:56 - frequently it is only useful when we
147:58 - have some values clustering together in
148:00 - this example the mode is 9 there may be
148:03 - no mode or there may be several modes
148:08 - there is no single measure of center
148:10 - that is best if the data are normally
148:12 - distributed then mean is used
148:14 - however if data are not normally
148:16 - distributed the median is a better
148:17 - measure often we use both to understand
148:20 - the underlying structure of the
148:21 - distribution
148:22 - there are several measures to examine
148:24 - the spread of the data they include
148:25 - range percentiles interquartile range
148:28 - and variance or standard deviation their
148:30 - range is the difference between the
148:31 - largest and the smallest value this
148:34 - histogram shows a minimum value of 15
148:36 - and a maximum value of 94 the ranges 94
148:40 - minus 15 equals 79 another measure of
148:43 - spread is the value of each quartile we
148:45 - take the total number of data points we
148:47 - have and divide them into four parts
148:48 - the value corresponding to the end point
148:51 - of each part is the quartile value the
148:53 - interquartile range is the difference
148:54 - between the value at the third quartile
148:56 - minus the value at the first quartile
148:58 - the first quartile q1 is the value for
149:01 - which 25% of the observations are
149:03 - smaller and 75% are larger the second
149:07 - quartile q2 is the same as the median
149:09 - 50% are smaller and 50% are larger only
149:13 - 25% of the observations are greater than
149:15 - the third quartile let's take the age
149:18 - values 1535 49 65 and 94 the first
149:23 - quartile is at 35 this means that 25% of
149:26 - the participants are below age 35
149:30 - like why is 25% are above 65 years old
149:33 - the interquartile range is 65 minus 35
149:36 - equals 30 years sample variance is
149:39 - calculated as the average of squared
149:41 - deviations of values from the mean as
149:43 - shown here we square the differences
149:44 - from the mean to provide equal weight to
149:46 - observations below the mean versus those
149:48 - above the mean because we square the
149:50 - difference values that are further away
149:51 - from the mean get higher weight than
149:53 - those close to the mean standard
149:55 - deviation is the most commonly used
149:56 - measure of variation it shows the
149:58 - variation around the mean and has the
150:00 - same units as the original data it is
150:02 - calculated by finding the square root of
150:03 - the variance here's an example of the
150:06 - standard deviation using age data note
150:08 - that sample standard deviation is
150:10 - represented by the symbol s X bar
150:12 - represents the sample mean the standard
150:15 - deviation is an extremely useful measure
150:17 - it tells us how close or far apart data
150:19 - are from the mean the higher the
150:21 - standard deviation the greater the
150:23 - spread of the data here in red is an
150:25 - example of a moderate standard deviation
150:27 - you can see that the data is spread
150:29 - pretty evenly the purple shows a low
150:31 - standard deviation in which the data is
150:33 - concentrated near the middle the blue
150:35 - example shows a high standard deviation
150:37 - where the data is concentrated on the
150:39 - outside these formulas are important to
150:42 - know well while software can compute
150:44 - these for you it's important to know how
150:46 - it's done using simple numbers whenever
150:48 - you work with data you'll have variables
150:50 - that have a center and spread a very
150:52 - useful rule to know is that no matter
150:54 - what the shape of the distribution 75%
150:57 - of values will lie within two standard
150:58 - deviations of the mean while 89 percent
151:01 - will lie three standard deviations from
151:03 - the mean so if someone gives you just
151:05 - these two pieces of information you can
151:07 - make some predictions on where a new
151:09 - data point will lie however what's even
151:11 - better in statistics is knowing that for
151:13 - large samples data are distributed
151:15 - symmetrically and follow the bell curve
151:17 - the 6895 99.7 rule states that 68% of
151:23 - the area of a normal curve lies within
151:25 - one standard deviation of the mean 95%
151:28 - of the area lies within two standard
151:29 - deviations of the mean and ninety-nine
151:32 - point seven percent of the area lies
151:33 - within three standard deviations of the
151:35 - mean
151:35 - this rule works for all normal curves no
151:39 - matter their shape that concludes our
151:41 - video on measures of central tendency
151:43 - including mean median and mode variation
151:46 - and dispersion range quartiles and
151:49 - interquartile range
151:50 - sample variance standard deviation the
151:53 - normal curve and the 6895 99.7 rule
151:59 - [Music]
152:07 - you