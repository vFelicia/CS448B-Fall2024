00:05 - this video
00:06 - is an introduction to one-way anova and
00:08 - two-way anova so this is like the basic
00:10 - idea
00:10 - and you really need to understand this
00:12 - video before you move on to other videos
00:14 - otherwise
00:15 - the equations just won't make any sense
00:17 - all right so first one-way anova
00:19 - the basic idea here is that you have at
00:21 - least three populations
00:23 - so three or more populations and you're
00:25 - trying to compare
00:27 - the mean of each of those populations to
00:30 - see if any of those differ
00:31 - so our null hypothesis is all the means
00:34 - from
00:35 - all of the populations all m populations
00:37 - are equal
00:39 - and then the alternative hypothesis is
00:40 - at least one of these means
00:42 - differs all right so what we're going to
00:45 - do
00:46 - to do this test we're going to compare
00:50 - the
00:51 - variability within each group
00:54 - against the variability between
00:56 - different groups so if we have
00:58 - m populations we'll take a sample from
00:59 - each of those m populations
01:02 - and then we're going to maybe make a
01:04 - plot to visually see the variability
01:06 - within each group
01:07 - and the variability between groups
01:09 - alright so imagine
01:10 - we take a sample here's our data set
01:14 - here's our box plots here we can see
01:17 - that
01:18 - yes there is a little bit of difference
01:20 - in these sample means
01:22 - but there's a lot of variability within
01:25 - each group so here we see a lot of
01:26 - variability in this group that's why the
01:28 - box plot is so long
01:29 - a lot of variability in this group
01:30 - that's why the box plot is so long and
01:32 - so
01:33 - even though there is variability between
01:36 - groups
01:37 - it's not very much compared to the
01:38 - variability within each group
01:40 - so here if we saw box plots like this it
01:42 - could just mean
01:44 - that the difference in sample means is
01:45 - simply due to random chance
01:48 - all right in the second data set here
01:51 - we see very little variability within
01:55 - each group
01:56 - so a very tight box plot here another
01:59 - very tight box another very tight box
02:02 - and so we see
02:02 - very little variability within each
02:05 - group
02:06 - compared to the variability between
02:08 - groups so if we saw a box plot like this
02:11 - assuming the sample size is large enough
02:12 - we'd probably be inclined to reject the
02:14 - null hypothesis
02:16 - and say that at least one of these means
02:17 - differs
02:19 - all right so again the null hypothesis
02:22 - is all the means are equal
02:23 - alternative hypothesis is at least one
02:25 - of these means differs
02:26 - you'll notice that if we end up
02:27 - rejecting the alternative hypothesis we
02:29 - don't yet know which
02:31 - mean is different all we know is that at
02:32 - least one of the means
02:34 - differs all right so that's the
02:37 - um big idea for one way anova for
02:39 - two-way anova
02:41 - we're changing a little bit so in
02:42 - one-way anova we just had one
02:43 - categorical variable
02:44 - now in two-way anova we're going to have
02:46 - two categorical variables
02:48 - so we still have a quantitative response
02:51 - in both of these
02:52 - here in two-way nova we have two
02:54 - categorical variables
02:55 - okay so it's easiest if we think about
02:57 - an example so imagine that we're doing
02:59 - an experiment
03:00 - to look at the temperature of water used
03:03 - when you're washing our clothes
03:05 - and the type of detergent you're using
03:07 - when you're washing your clothes
03:08 - so imagine you're using two detergents
03:10 - tide versus kirkland
03:12 - and then your water temperature on your
03:15 - washing machine is like cold warm or hot
03:19 - okay so you want to see if you make the
03:22 - clothes
03:23 - equally dirty in each of these
03:25 - experiments is one of these
03:28 - things going to release more dirt
03:32 - get rid of more dirt from your clothing
03:33 - so we're trying to measure as our
03:35 - response
03:36 - the amount of dirt that's removed from
03:37 - the clothing in one wash cycle
03:40 - all right so there's our response the
03:41 - amount of dirt removed from one wash
03:43 - cycle
03:44 - uh predictors like we said the type of
03:46 - detergent used and then
03:47 - the water temperature okay so we can use
03:50 - two-way anova to figure out a whole
03:52 - bunch of things
03:53 - so one of the things we can figure out
03:55 - is whether
03:56 - one of the detergents removes more dirt
03:59 - so in other words
04:00 - if we like look at the mean for tide
04:02 - versus the mean for kirkland
04:05 - do these differ all right next question
04:09 - we can answer is whether one of the
04:10 - water temperatures more effectively
04:12 - removes
04:13 - dirt so our null hypothesis is going to
04:16 - be the mean for cold water the mean for
04:18 - warm water and the mean for hot water
04:20 - those are all equal and then the
04:21 - alternative is one of those means
04:24 - is not equal okay
04:27 - and then finally the thing that we can
04:29 - figure out is
04:30 - whether some combination of
04:33 - detergent and water temperature is more
04:36 - or less effective
04:37 - at removing dirt so in other words we're
04:40 - trying to see is there some good
04:41 - combination
04:43 - that will remove more dirt or in other
04:45 - words we could say
04:46 - do some detergents work better or worse
04:48 - in certain water temperatures
04:50 - so it might be for example that like
04:52 - kirkland works the best in hot water but
04:54 - for tide it doesn't really matter
04:56 - or something like that all right you'll
04:58 - notice that
04:59 - there's question one and this question
05:01 - two here those are things that we could
05:03 - answer
05:04 - by doing a one-way anova for each one of
05:08 - those
05:09 - but the advantage of two-way anova is
05:11 - this
05:12 - last question that we get to answer we
05:14 - get to figure out whether some
05:15 - combination is working so in other words
05:17 - we're trying to see is the interaction
05:19 - term
05:22 - significant
05:27 - [Music]
05:30 - in this video we started digging into
05:32 - the math of the one-way anova
05:34 - all right so first thing we need to do
05:36 - is inform you how this all is working
05:39 - and set up the notation all right so
05:41 - what we can imagine is that we have m
05:43 - groups so that means we have
05:45 - m different means mu 1 mu 2 up to mu m
05:49 - so what we do is we take a popular
05:52 - take a sample of size ni from the ith
05:56 - population
05:57 - all right so like you go to population
05:59 - one you take a sample of size
06:00 - n1 then you go to population two take a
06:03 - sample of size n2
06:04 - and so on in other words all these
06:08 - sample sizes don't have to be the same
06:09 - across the different populations
06:12 - all right so once you have that sample
06:14 - from each one of those populations we
06:15 - can get our total sample size
06:17 - n by just adding up the sample sizes of
06:20 - each of those
06:22 - uh samples right so n
06:25 - equals just the sum of the ni all right
06:28 - so then we need to define our random
06:32 - sample so
06:33 - the sample that we take is denoted by x
06:35 - i n i
06:36 - so this is a random sample of size n i
06:39 - from the ice population
06:41 - and we're going to assume that these
06:43 - populations have a normal distribution
06:45 - with mean mu i for the population and
06:48 - then a shared variance
06:49 - sigma squared so this is an assumption
06:51 - we're making that the variance is
06:53 - shared across the different populations
06:56 - all right so we have our samples x i and
06:59 - i from
07:00 - each of those i populations we can think
07:03 - about
07:04 - the mean for each of those populations
07:06 - so like
07:07 - what's the mean for population one
07:09 - what's the mean for population two and
07:10 - so on
07:11 - all right so our sample mean for the
07:13 - eighth population we just add up
07:16 - our sample
07:20 - and then divide by the sample size so we
07:22 - took
07:23 - a sample of size ni from the eyes
07:25 - population so we're dividing by
07:27 - that i populations that ice groups
07:29 - sample size
07:30 - and then here this is just saying we're
07:32 - adding up all of the
07:34 - measurements that we took from that ice
07:36 - population
07:38 - all right so we denote this by x bar and
07:40 - then in the subscript we have
07:42 - i and then a dot so that dot just means
07:44 - that we're
07:45 - um averaging across that second
07:48 - subscript there that
07:49 - which which is shown as j here all right
07:52 - so this is how we're going to denote the
07:53 - sample mean taken from population i
07:56 - all right so then how are we going to
07:58 - denote our overall mean we're going to
08:00 - denote it with
08:01 - x bar with two dots in the subscript
08:04 - because we're
08:04 - averaging over both of those subscripts
08:07 - all right so we divide by our overall
08:09 - sample size
08:10 - and we need to add up all of the
08:13 - observations
08:14 - we need to add up all of our
08:15 - measurements so the way that we can
08:17 - think about doing this is
08:18 - go to each one of these different groups
08:21 - so like let's imagine right now we're at
08:23 - population one so then i would be equal
08:26 - to one
08:27 - and then we're going to add up all of
08:29 - the measurements we took from population
08:31 - i
08:31 - so we're going to add up xi1 xi2
08:34 - xi3 and so on so we add up all of those
08:37 - measurements from the
08:39 - um population i equals one
08:42 - and then we add up all the measurements
08:44 - from population i equals two
08:45 - and so on until we have added up all of
08:48 - these different measurements
08:49 - so we've added up all the different
08:50 - measurements and just divided by the
08:52 - sample size that's our overall mean
08:55 - all right so we have our group means and
08:56 - our overall means now we can
08:59 - start actually talking about variability
09:01 - so remember in one way anova we're
09:03 - interested in measuring
09:04 - the variability within groups and the
09:06 - variability
09:07 - between groups because we want to
09:09 - compare those to see if the variability
09:12 - between groups is really big compared to
09:14 - the variability within groups
09:16 - then that probably means that we can
09:17 - reject our null hypothesis in favor of
09:19 - our alternative all right so how do we
09:22 - measure the variability between groups
09:24 - well let's denote this by sst
09:27 - so this t is like if we're thinking
09:30 - about doing an experiment we have
09:31 - different treatments
09:32 - so the t here means the treatment for
09:35 - the
09:37 - um the treatment here and ss indicates
09:40 - like sum of squares
09:41 - so that'll make sense because we're
09:43 - adding up a bunch of things that are
09:44 - squared
09:46 - all right so what is the sum of squares
09:48 - for the treatment well
09:51 - let's think about this intuitively for a
09:53 - second we're measuring the variability
09:54 - between
09:55 - groups so in other words we want to
09:58 - look at all these different sample means
09:59 - and see how much variability there is so
10:01 - the way that we're going to do that
10:02 - is we're going to compare each sample
10:04 - mean against the overall mean
10:07 - so we take each one of these sample
10:08 - means
10:10 - and subtract off the overall mean so
10:12 - that says how far apart those two things
10:14 - are
10:15 - and then we square it and multiply by
10:18 - the sample size for that ice population
10:21 - okay so this is going to give us our
10:22 - treatment sum of squares
10:25 - so again this is measuring the
10:27 - variability between
10:29 - the different sample means all right and
10:32 - then later we'll use this notation
10:34 - mst is the sum of squares for t
10:37 - divided by m minus 1. so remember m is
10:40 - our
10:41 - number of populations our number of
10:42 - groups so what we're doing here is we're
10:45 - taking our total
10:46 - sum of our treatment sum of squares and
10:48 - then dividing by
10:49 - the number of groups minus one so we can
10:52 - think of this kind of like as
10:53 - the mean sum of squares for
10:57 - the treatment
11:01 - all right next thing we need to look at
11:03 - is measuring the variability
11:04 - within groups okay so what does it mean
11:07 - to have variability within groups
11:08 - well we could look at each group's mean
11:12 - and then see how much the sample varies
11:16 - for that ice population so we're say
11:18 - we're talking about population one
11:20 - we look at the sample mean for
11:22 - population one
11:24 - and then we see how much variability is
11:25 - there in the measurements from
11:27 - population one
11:28 - how much variability is there around
11:30 - that sample mean from population one
11:33 - okay so say let's
11:36 - work with i equals one for now um
11:40 - we go and we take each one of those
11:42 - measurements
11:43 - from sample one
11:46 - and we compare it to the sample mean for
11:50 - sample one
11:51 - so we're finding all of these
11:52 - differences squaring them and adding
11:54 - them
11:54 - up all right so now we've done this part
11:58 - for i equals 1. now we need to do it for
12:01 - i equals 2
12:02 - i equal 3 all the way to i equals m so
12:05 - we're adding up
12:06 - all of those squares and that will be
12:09 - our sum of squared
12:11 - sum of squares for the error okay so we
12:15 - can think about it kind of like as error
12:16 - because
12:17 - um we're looking at the variability
12:19 - around some sample mean
12:22 - and then the mean squared error is going
12:24 - to be the sum of squares for the error
12:26 - and then we're going to divide by the
12:28 - sample size minus the number of groups
12:31 - so we'll talk a little bit more about
12:32 - why that's the denominator
12:34 - later all right so we have the
12:37 - variability between
12:38 - groups the variability within groups now
12:39 - we need to talk about what is the total
12:41 - variability
12:42 - and very conveniently the total
12:44 - variability is just
12:46 - the variability within groups plus the
12:49 - variability between groups okay so we're
12:52 - going to denote the total variability by
12:54 - ssto
12:56 - and what we're going to do is just
12:58 - compare each
13:01 - observation against that overall mean
13:05 - all right so this differs from like this
13:07 - one because here we are measuring
13:09 - the variability compared to each group's
13:13 - sample mean here we're comparing it to
13:15 - the overall mean
13:16 - so take every observation find the
13:18 - distance between that
13:19 - and the overall mean square it
13:22 - add all of those up so that's the total
13:26 - sum of squares
13:28 - okay so intuitively if we are looking at
13:32 - the total variability it can really only
13:34 - come from two sources it can either
13:35 - arise because of error
13:38 - in other words variability within groups
13:41 - or it can arise because the treatments
13:45 - are different so in other words the
13:48 - variability between groups
13:50 - so that means that the total sum of
13:52 - squares is equal to the treatment sum of
13:54 - squares plus
13:55 - sse and then mathematically if you want
13:58 - to think about it that way
13:59 - if you write this out and then expand it
14:04 - you'll see that the cross term is
14:05 - actually zero and they go through that
14:06 - in the book if you would like to check
14:08 - that out
14:16 - [Music]
14:19 - in the last anova video we set the
14:21 - notation and now we can actually get
14:22 - into
14:23 - the theory behind the one-way anova
14:26 - all right so um remember that we just
14:29 - left off saying that
14:30 - the total sum of squares equals the
14:32 - treatment sum of squares plus sse
14:35 - so if we want to we can divide by a
14:36 - constant all the way across and that's
14:38 - just fine
14:39 - so we divide across by sigma squared
14:42 - all right so if you look at your book
14:44 - theorem 931 then
14:46 - it says that the total
14:49 - sorry the treatment sum of squares and
14:51 - sse
14:52 - are independent okay so
14:55 - if we look at this first piece and break
14:58 - that down
15:00 - we're going to see that it's a
15:01 - chi-squared random variable with n
15:03 - minus 1 degrees of freedom similarly if
15:06 - we look at
15:06 - sse that that's also sorry sse divided
15:10 - by sigma squared that's also a
15:11 - chi-squared random variable
15:13 - with m minus 1 degrees of freedom
15:17 - all right so this is a chi-squared
15:20 - random variable
15:21 - this is a chi-squared random variable
15:24 - and
15:25 - this and this are independent of each
15:28 - other
15:28 - so if we remember back in our mgf days
15:31 - from last semester
15:33 - if we sorry that's moment generating
15:35 - function
15:36 - if we have a chi-squared random variable
15:39 - equals
15:40 - something plus a chi-squared random
15:41 - variable then that's going to mean that
15:44 - this is also a chi-squared chi-squared
15:46 - random variable
15:47 - and remember the way that these work is
15:50 - that if we have chi squared with
15:51 - however many degrees of freedom plus chi
15:53 - squared with
15:54 - m minus 1 degrees of freedom that equals
15:57 - chi squared with n
15:58 - minus 1 degrees of freedom so in other
15:59 - words the degrees of freedom just add
16:01 - so if this has n minus 1 degrees of
16:03 - freedom and this has m
16:04 - minus 1 degrees of freedom and this is
16:06 - chi squared and it's not a mystery how
16:08 - many degrees of freedom it has
16:10 - we know that it's going to be n minus
16:13 - m degrees of freedom because we need n
16:16 - minus
16:16 - m plus m minus 1 to equal n minus 1.
16:22 - all right so under the null hypothesis
16:26 - then
16:26 - this must be a chi-squared random
16:29 - variable with n minus m
16:30 - degrees of freedom all right
16:34 - so remember that this whole one-way
16:37 - anova stuff
16:38 - we're looking at comparing the
16:39 - variability within groups to the
16:41 - variability
16:41 - between groups so let's start thinking
16:44 - about that variability within groups and
16:46 - the variability between
16:47 - groups all right so under the null
16:48 - hypothesis the treatment sum of squares
16:50 - divided by sigma squared we just said
16:52 - is chi squared distributed with minus
16:54 - one degrees of freedom
16:55 - remember if we have a chi-squared random
16:57 - variable and we want to
16:58 - get its mean that's just going to be
17:00 - equal to the number
17:01 - of degrees of freedom that that chi
17:03 - squared has
17:05 - okay so we have n minus 1 degrees of
17:07 - freedom so that means this
17:09 - random variable sst divided by sigma
17:11 - squared that means that this
17:12 - has expectation m minus 1.
17:16 - all right n minus 1 that's a constant
17:18 - sigma squared that's a constant so
17:20 - there's no harm done in moving these
17:23 - around so we could have the expectation
17:25 - of
17:27 - sst divided by m minus 1 and that will
17:29 - equal
17:30 - sigma squared under the null hypothesis
17:33 - so you might be tempted to say okay if
17:34 - we want to estimate sigma squared
17:36 - let's just take the treatment sum of
17:38 - squares and divide by m minus 1.
17:40 - but this is not a good way to estimate
17:43 - our sigma squared because
17:45 - this all relies on the null hypothesis
17:47 - being true
17:48 - so if it's not true we're actually going
17:51 - to be overestimating
17:52 - sigma squared all right
17:55 - so now we know something about how this
17:57 - is distributed
18:00 - let's think now about the null
18:01 - hypothesis whether it's true or not
18:04 - doesn't matter sse divided by sigma
18:06 - squared is going to have a chi-squared
18:08 - random variable
18:09 - um is going to be a chi-squared random
18:12 - variable with n minus m
18:13 - degrees of freedom so same sort of thing
18:15 - if we take the expectation of this
18:17 - it's going to have n minus m as
18:20 - its expectation
18:23 - all right so since this does not rely on
18:26 - the null hypothesis being true we can
18:28 - use this
18:29 - mse to
18:33 - estimate sigma squared and it will be an
18:34 - unbiased estimator
18:37 - all right so we know we have a chi
18:39 - squared and a chi squared
18:41 - and that they're independent so it says
18:43 - our theorem in the book
18:44 - and so if we take a chi-squared random
18:47 - variable
18:48 - and divide by another chi-squared random
18:50 - variable as long as they're independent
18:52 - then we're going to end up with an f
18:55 - random variable
18:57 - all right so that's exactly what we're
18:58 - going to do
19:00 - so we said that mst
19:03 - has a chi-squared random variable mse
19:07 - has a chi-squared random variable
19:08 - they're independent so this
19:12 - which will be our test statistic will
19:14 - actually have an f distribution
19:15 - with m minus 1 degrees of freedom and n
19:18 - minus m
19:19 - degrees of freedom under the null
19:20 - hypothesis so this all relies
19:22 - on being under the null hypothesis
19:24 - because we need
19:26 - sst over sigma squared to have a
19:28 - chi-squared distribution and the only
19:29 - way that's going to happen is if the
19:30 - null hypothesis is true
19:32 - so under the null hypothesis here's our
19:34 - test statistic it's going to have this
19:35 - distribution
19:37 - f with m minus 1 degrees of freedom and
19:39 - n minus
19:40 - m degrees of freedom all right so let's
19:43 - think about this logically
19:45 - we said that if the variability
19:49 - between groups is much larger than the
19:52 - variability within groups
19:53 - then we're going to want to reject the
19:55 - null hypothesis so what's the
19:57 - variability between groups
19:58 - that's that mst and then the variability
20:01 - within groups that's that mse
20:03 - so here we're looking at the variability
20:05 - between groups divided by
20:07 - the variability within groups so if
20:10 - this quantity is a lot bigger than one
20:13 - and a lot will be defined by our p-value
20:16 - if
20:16 - that quantity that test statistic is a
20:18 - lot bigger than 1 then we're going to
20:19 - want to reject the null hypothesis
20:23 - all right so in other words the bigger
20:25 - that test statistic gets
20:26 - the more we're going to want to reject
20:28 - the null hypothesis so our p-value is
20:30 - going to be
20:31 - the right tail all right so here's our f
20:34 - distribution with m
20:35 - minus 1 degrees of freedom and n minus m
20:37 - degrees of freedom
20:38 - here's our test statistic mst over mse
20:42 - and then the p-value is going to be the
20:43 - area to the right of that test statistic
20:46 - because like we said as our test
20:48 - statistic gets larger and larger
20:50 - we're going to want to reject the null
20:51 - hypothesis more and more
20:53 - and so as we move further to the right
20:56 - with our test statistic our p-value is
20:57 - going to be getting smaller and smaller
20:59 - thus letting us have more and more
21:01 - evidence against the null hypothesis
21:08 - [Music]
21:11 - so in the previous video we went through
21:12 - all the math of one way anova
21:14 - and now we can talk about how we
21:15 - summarize it so lots of times we'll
21:17 - summarize it in what's called an anova
21:18 - table here
21:20 - and the way it works is we have a column
21:22 - for the source of variability
21:24 - a column for the sum of squares a column
21:27 - for the degrees of freedom
21:29 - a column for the mean squares and then
21:31 - finally our test statistic
21:33 - all right so remember that we have the
21:36 - treatment
21:37 - sum of squares and the sse because we
21:40 - have two sources of error
21:41 - two sources of variability the treatment
21:44 - and the error
21:45 - all right so then we in the next column
21:48 - have our degrees of freedom
21:50 - so this is the number of populations or
21:52 - number of groups minus one
21:53 - and then this is the overall sample size
21:56 - minus the number of groups
21:59 - all right so when we look at the total
22:02 - this total will add
22:04 - so remember the total sum of squares is
22:06 - equal to sst plus sse
22:09 - similarly the total degrees of freedom
22:12 - equals n minus 1 which is m minus 1
22:16 - plus and minus m all right so f
22:19 - sum of squares and degrees of freedom
22:21 - add and once we get to the next column
22:23 - these no longer add
22:24 - so here in this next column we have the
22:26 - mean squares so mst
22:27 - is just sst divided by n minus 1. so we
22:31 - just
22:31 - take this column divide by that column
22:34 - similarly for mse we take this column
22:36 - divide by that column so it's pretty
22:37 - easy as long as we organize it this way
22:39 - it's super easy to
22:41 - remember what how to calculate mean
22:43 - squares
22:44 - and then finally when we get to our test
22:46 - statistic remember that's just mst
22:48 - divided by mse
22:50 - so again that's pretty easy we just look
22:51 - at this column
22:53 - take this value divide by that value and
22:55 - that's our test statistic
22:58 - okay so remember under the null
22:59 - hypothesis that test statistic has
23:01 - an f distribution with m minus 1 degrees
23:04 - of freedom and
23:05 - and minus m degrees of freedom and
23:07 - remember the larger the test that is the
23:09 - more evidence we have against the null
23:10 - hypothesis
23:11 - so here i've drawn our f distribution
23:13 - with m minus one and n minus n degrees
23:15 - of freedom
23:16 - and then here marked is our test
23:18 - statistic mst over mse
23:20 - and then the p-value is just the area to
23:22 - the right of that
23:31 - in a previous video we very briefly
23:33 - introduced two-way anova and now we're
23:35 - going to get into
23:36 - more of the details more of the notation
23:38 - all right so remember that in two-way
23:39 - anova we have two
23:41 - categorical variables so every
23:43 - measurement has
23:44 - two categorical variables associated
23:46 - with it so like if our
23:48 - categorical variables are the water
23:50 - temperature and the brand of detergent
23:52 - then every measurement that we take
23:54 - every experiment that we run we will
23:56 - have some detergent that we're using
23:58 - whatever brand kirkland tied whatever
24:00 - and then the water temperature so cold
24:03 - warm or hot
24:05 - all right so each measurement has two
24:06 - attributes all right so if we want
24:08 - we could look at a bunch of different
24:10 - things we could look at the average
24:12 - dirt removed by each detergent by
24:14 - averaging over the different water
24:15 - temperatures
24:17 - or we could average over the different
24:19 - detergents to look at the average
24:21 - amount of dirt removed by each water
24:23 - temperature
24:24 - and then finally if we have more than
24:26 - one observation for
24:28 - each one of those water temp detergent
24:30 - brand combos
24:32 - then we can take the average for each
24:33 - one of those combos
24:35 - so that we can say like okay the average
24:37 - amount of dirt removed by
24:39 - tide in warm water is this much
24:42 - but we can only take an average rate if
24:44 - we have more than one observation for
24:46 - each one of those combos
24:48 - and it'll make it a little bit easier on
24:50 - the notation at first we look at
24:52 - one observation per combo so we actually
24:55 - won't be able to answer
24:57 - this third question here quite yet
25:00 - we'll get to that in a little bit though
25:02 - so for now we're going to set things up
25:04 - so that we can look at
25:05 - the average dirt removed by each
25:07 - detergent and the average
25:09 - dirt removed by each water temperature
25:12 - okay so let's generalize this more
25:14 - so let's say that we have two factors
25:17 - not just water temp
25:18 - and the brand of detergent let's say
25:20 - that we have factor a and factor b
25:22 - factor a has eight categories or levels
25:25 - factor b
25:26 - has b categories or levels
25:29 - all right so we're going to denote each
25:30 - observation
25:32 - by x i j so x i j we're going to assume
25:36 - has a normal distribution
25:37 - with mean mu i j and variance sigma
25:40 - squared and again just like in the
25:41 - one-way anova we're going to assume that
25:43 - sigma squared is a shared variance all
25:46 - right
25:47 - so we're going to have
25:50 - one observation per combo so that means
25:52 - we'll have i equals 1 through a
25:54 - and j equals one through b so in other
25:56 - words this first subscript the i
25:58 - that's for factor a the second subscript
26:01 - b
26:02 - that's for factor the second subscript
26:05 - is j and that's for factor b
26:07 - okay so since we have one observation
26:09 - per combo
26:10 - then if we take the number of levels in
26:13 - a
26:14 - and the number of levels in b that's
26:16 - going to be our overall sample size
26:20 - all right so we said that we could look
26:21 - at the average dirt removed
26:23 - by each detergent and the average dirt
26:25 - removed by each water temperature so in
26:27 - other words let's find the
26:28 - mean for each level of factor a and the
26:31 - mean for each level of factor b
26:34 - all right so if we're looking at the ice
26:35 - level of factor a
26:38 - then we're looking for the mean for the
26:41 - i level and we're going to
26:43 - average over all of the different
26:47 - j's so this would be like we're finding
26:50 - the
26:51 - average dirt removed by each detergent
26:53 - and we're averaging over all the
26:55 - different water temperatures
26:56 - okay so this x bar i that is
27:00 - 1 divided by b which is the number of
27:03 - levels in factor b and we're going to
27:05 - add up all the temperatures
27:08 - sorry all the observations x i j where
27:12 - this i and that i are the same so we're
27:14 - adding up over all the j's
27:16 - okay similar thing for our factor b
27:20 - if we're going to look for the mean for
27:22 - the j level of
27:23 - b we'll denote that by x bar
27:27 - dot j so we'll add up all the
27:31 - observations
27:33 - for that jth level
27:38 - and all the different levels for
27:41 - the first factor so we add all those up
27:44 - divide by the sample size which is
27:46 - a which is the number of levels in
27:48 - factor a
27:49 - and then our overall mean we're just
27:51 - going to add up all of our observations
27:53 - for all of the all of the levels in b
27:57 - and all of the levels in a and then
27:58 - we're going to divide by our sample size
28:00 - which is 1 over
28:02 - a times b all right so we've got
28:06 - most of our notation down so now we can
28:08 - define like the total sum of squares and
28:10 - all that stuff
28:11 - so the total sum of squares remember is
28:13 - going to be
28:15 - we're going to take each one of these
28:16 - observations and find the distance
28:19 - between it and the mean that overall
28:21 - grand mean
28:22 - so we find that distance square it add
28:25 - all of those up
28:26 - and that's our total sum of squares so
28:28 - this is pretty much
28:30 - a very direct extension of the one-way
28:34 - anova
28:35 - all right so if we manipulate this a
28:38 - little bit
28:39 - we'll end up getting that this total sum
28:41 - of squares is equal to
28:42 - b times this sum
28:46 - which is saying let's look at the
28:49 - variability
28:50 - for the
28:54 - different levels in a so we'll look at
28:57 - okay the first level in a and take that
29:00 - mean
29:01 - and subtract off the grand mean find
29:02 - that distance there square it
29:04 - add all those up so we're going to find
29:08 - we're going to be looking at the
29:10 - variability among the
29:12 - means for factor a
29:16 - here we're going to be looking at the
29:18 - variability for the means for factor b
29:20 - so that's a similar thing and then here
29:23 - we're going to be looking at
29:25 - x i j minus the mean
29:28 - for the i level of a minus the mean for
29:31 - the
29:32 - j-th level of b minus that overall grand
29:35 - mean
29:37 - okay so this is um
29:42 - this part here is like what we would
29:44 - predict
29:45 - x i j to be and this is what it actually
29:48 - is so it's again
29:49 - same thing as usual like observed minus
29:51 - predicted
29:52 - so we take that observed subtract off
29:55 - the group mean
29:56 - square it and then add all of those up
30:00 - all right so we're going to call this
30:03 - piece
30:05 - ss a this piece
30:08 - is ssb then that final piece is
30:12 - sse so this is the sum of squares for
30:15 - factor a this is the sum of squares for
30:17 - factor b
30:18 - and then this is the sum of squares due
30:20 - to error
30:22 - all right so we're going to use this
30:23 - notation to move forward and actually
30:26 - like
30:26 - get our test statistics and
30:27 - distributions and all that sort of stuff
30:29 - in the next video
30:36 - now that we've set up some of the
30:37 - notation for this two-way novel we can
30:39 - actually get into some of the
30:40 - mathematical properties statistical
30:42 - properties
30:43 - of these different things that we've set
30:45 - up all right so we just left off by
30:46 - saying that
30:47 - the total sum of squares equals the sum
30:50 - of squares from a
30:51 - plus the sum of squares from b plus the
30:54 - sum of squares due to error
30:56 - all right so that's what we set up and
30:57 - then now if we think back to how we did
30:59 - things in one way anova
31:00 - we set up the o sum of squares and then
31:03 - we figured out
31:04 - okay what is the distribution of each of
31:05 - these components so that's exactly what
31:07 - we're going to do here
31:09 - and just like in one way anova we're
31:10 - going to see that each of these
31:11 - components
31:12 - is chi-squared distributed all right so
31:14 - we want to show that
31:16 - sum of squares for a over sigma squared
31:18 - sum of squares for b
31:19 - over sigma squared and sum of squares
31:21 - due to error over sigma squared are
31:23 - each independent chi-squared random
31:25 - variables
31:27 - as long as some hypotheses are true so
31:30 - this is just like we did for
31:32 - the one-way anova we said we're going to
31:34 - show that these are chi-squared
31:35 - distributed as long as the null is true
31:37 - and one-way anova so in two-way anova
31:39 - we're testing the means for a and the
31:42 - means for b so we need to say
31:44 - two sets of hypotheses so the null for a
31:47 - is that the means among a's different
31:50 - levels are equal
31:51 - and the null hypothesis for b is the
31:53 - means among these different levels
31:56 - are equal so if we want to think of an
31:57 - example then for the null hypothesis for
32:00 - a that might be saying like
32:02 - the different detergents remove an equal
32:05 - amount of dirt on
32:06 - average and then the null hypothesis for
32:09 - b would be saying
32:10 - that the different water temperatures
32:11 - remove an equal amount of dirt on
32:13 - average
32:15 - all right so those are the hypotheses
32:16 - we're going to work under to show that
32:18 - these different components are
32:19 - chi-squared distributed
32:21 - all right so let's go ahead and start
32:22 - from there we'll assume that h
32:24 - a and h b are true so if you remember
32:28 - from one way anova this total sum of
32:30 - squares is chi-squared distributed
32:32 - and its degrees of freedom is just the
32:34 - sample size minus one so remember we're
32:36 - working in the
32:37 - situation here where we have one
32:40 - observation per combo of detergent and
32:43 - water or in other words one observation
32:45 - per combo
32:46 - of each of these levels and each of
32:48 - these levels
32:49 - so the number of levels
32:52 - is a times b that's the total number of
32:57 - different combos we could have so that's
32:58 - the sample size n
33:00 - is a times b and so our degrees of
33:03 - freedom for
33:04 - ss total over sigma squared is a b minus
33:08 - one
33:10 - all right so if we use that theorem that
33:12 - we talked about back
33:13 - in one way anova then we know that ssa
33:16 - over sigma squared
33:17 - is going to be chi squared distributed
33:18 - with a minus 1 degrees of freedom so
33:20 - remember the degrees of freedom is the
33:22 - number of levels in a
33:23 - minus one and then same story for
33:27 - ssb over sigma squared again degrees of
33:29 - freedom is number of levels in b
33:31 - minus one all right so that means that
33:36 - um
33:39 - this piece over sigma squared is
33:42 - chi-squared distributed
33:43 - this piece over sigma squared is
33:45 - chi-squared distributed
33:46 - and this piece over sigma squared is
33:49 - chi-squared distributed
33:51 - also from the theorem we used back in
33:54 - one way nova
33:54 - we know that ssa over sigma squared and
33:57 - s
33:58 - b over sigma squared are independent so
34:00 - what that tells us
34:01 - is that since this is chi squared
34:05 - and this over sigma squared is chi
34:07 - squared this over sigma squared is chi
34:08 - squared therefore
34:10 - this plus this over sigma squared is
34:12 - also chi squared distributed
34:14 - all right so where that puts us is that
34:16 - this over sigma squared is chi squared
34:18 - distributed and
34:23 - this piece is also chi-square
34:24 - distributed so therefore if we have
34:27 - chi squared plus something
34:32 - is a chi-squared that must mean that
34:34 - then this is also chi-squared
34:36 - distributed so if we think back to mgfs
34:38 - that can help you
34:39 - understand that all right so
34:42 - sse over sigma squared must be chi
34:44 - squared distributed
34:45 - but what about the degrees of freedom
34:47 - well we know that the degrees of freedom
34:48 - should add right
34:50 - so if we know the degrees of freedom for
34:52 - ss total over sigma squared
34:54 - and we know the degrees of freedom for
34:56 - this chunk over sigma squared
34:58 - then we know that we can just add these
35:00 - to get there
35:01 - so the degrees of freedom for
35:05 - ss total over sigma squared is a b minus
35:07 - one
35:08 - and then we figured out that the degrees
35:10 - of freedom for
35:13 - this piece over sigma squared is
35:17 - a minus one plus b minus one so then
35:19 - that leaves the degrees of freedom for
35:21 - ss
35:22 - due to error over sigma squared and so
35:25 - it must be then the degrees of freedom
35:28 - here
35:28 - is equal to a minus 1 times b minus 1.
35:33 - all right so now we figured out that ss
35:36 - total over sigma squared is chi-square
35:38 - distributed
35:39 - and ssa over sigma squared's chi-squared
35:42 - distributed ssb
35:43 - over sigma squared is chi-squared
35:44 - distributed these two components are
35:46 - independent of each other and then
35:47 - finally
35:48 - sse over sigma squared is also
35:50 - chi-squared distributed and we have all
35:52 - the degrees of freedom
35:53 - here so that will help us set up the
35:55 - test statistics in the next video
36:03 - in the previous video we figured out a
36:05 - few things so if
36:07 - the means among a's different levels and
36:09 - if the means among these different
36:11 - levels
36:12 - are equal then we said that ss total
36:15 - over sigma squared is chi-squared
36:16 - distributed
36:17 - with a b minus one degrees of freedom
36:19 - ssa over sigma squared is also
36:21 - chi-squared distributed with a minus one
36:23 - degrees of freedom
36:25 - similar story for ssb over sigma squared
36:28 - and then ss e over sigma squared is chi
36:31 - squared distributed with a minus 1 times
36:33 - b minus 1 degrees of freedom
36:35 - and finally ssa over sigma squared ssb
36:38 - over sigma squared and sse
36:40 - over sigma squared are all independent
36:42 - random variables
36:45 - all right so now we can work with these
36:46 - facts
36:48 - work with these findings to derive our
36:50 - test statistics
36:52 - to test whether the means among a's
36:54 - different levels are truly equal
36:56 - all right so say we want to do just that
36:59 - so let's work first with the
37:01 - null hypothesis for a and then we could
37:03 - do the exact same thing for the null
37:04 - hypothesis for b
37:06 - so we want to test whether the means
37:07 - among a's
37:09 - different levels are equal so remember
37:11 - from one-way anova what we did
37:13 - is we compared the variability between
37:16 - groups
37:17 - the variability within groups so that's
37:19 - exactly what we're going to do here
37:20 - again
37:21 - so we're going to test set up the test
37:22 - at the exact same way
37:24 - so we have the variability
37:27 - between groups up in the numerator and
37:29 - the variability
37:30 - within groups in the denominator so this
37:32 - is the test at
37:34 - for testing whether a's different means
37:37 - are equal all right so
37:40 - if the null hypothesis actually were
37:42 - true then
37:43 - this should be pretty small or
37:46 - maybe about one but if the means are
37:50 - actually different then we're going to
37:51 - see
37:52 - sse sorry ssa is going to be
37:55 - bigger than expected so we're going to
37:57 - compare this to a statistic
37:59 - against its sampling distribution so
38:02 - what's its sampling distribution
38:03 - well we know that ssa over sigma squared
38:06 - is chi squared distributed with a minus
38:08 - 1 degrees of freedom
38:09 - we know sse over sigma squared is chi
38:11 - squared distributive of a minus 1 times
38:13 - b minus 1 degrees of freedom
38:15 - therefore this divided by this
38:18 - is going to have an f distribution so
38:21 - we're going to compare this test
38:22 - statistic
38:23 - against an f distribution df 1 is going
38:26 - to be a minus 1
38:27 - df 2 is a minus 1 times b minus 1.
38:30 - all right so when our test statistic is
38:32 - large compared to this distribution
38:34 - then we're going to reject the null
38:36 - hypothesis that the means amongst
38:38 - a's different levels are equal so here's
38:41 - our little picture
38:42 - we have our f distribution with a minus
38:45 - 1
38:45 - and a minus 1 times b minus 1 degrees of
38:47 - freedom we've marked off our test at
38:50 - and we've shaded the area to the right
38:51 - because that represents larger
38:54 - more extreme test statistics so that
38:57 - area to the right that's our p value so
38:59 - our p value is the probability
39:00 - that this f distribution is
39:04 - greater than our test statistic
39:07 - all right one more thing we probably
39:09 - want to have a good unbiased estimator
39:11 - for sigma squared
39:13 - and so we can do just that so if
39:16 - the null hypothesis for a were true we
39:18 - could do something with
39:20 - ssa and if the null hypothesis for b
39:22 - were true we could
39:23 - do something with ssb to figure out a
39:26 - good estimator for sigma squared
39:28 - but we can't rely on those being true
39:30 - right we can't rely on the means amongst
39:32 - a's different groups being true we can't
39:33 - rely on the means
39:34 - amongst b's different groups being true
39:36 - so what we're going to rely on is
39:39 - sse over sigma squared having a
39:40 - chi-squared distribution
39:42 - with a minus 1 times b minus 1 degrees
39:44 - of freedom because
39:45 - whether the null for a
39:48 - and the null for b whether those are
39:50 - true or false it doesn't matter
39:52 - this finding number 5 here that's still
39:55 - going to be true
39:56 - so sse over sigma squared has a
39:59 - chi-square distribution with a minus 1
40:01 - times b minus 1 degrees of freedom
40:03 - therefore a good
40:04 - unbiased estimator for sigma squared is
40:06 - going to be sse
40:08 - divided by a minus one times b minus one
40:13 - [Music]
40:19 - in our previous work with two factor
40:22 - anova we had two factors
40:24 - but we only had one observation for each
40:27 - combo of levels in a and levels in b so
40:30 - in other words we only had like
40:32 - one observation for tied with cold water
40:36 - but now we want to move from just having
40:37 - one observation for tied with cold water
40:40 - to having
40:40 - more than one observation all right so
40:44 - when we only had like one observation
40:46 - for each combo
40:47 - of detergent and water temperature
40:50 - we were restricted to testing just two
40:52 - hypotheses so
40:54 - one of the hypotheses was are the means
40:56 - among
40:57 - a's different levels equal and are the
41:00 - means among b's different levels equal
41:02 - but we were not able to test when we
41:04 - only had one observation per combo
41:07 - we were not able to test is there some
41:09 - combo of a
41:10 - and b that has a different mean from the
41:12 - other
41:14 - means from the other combos of a and b
41:17 - all right so we want to test for this
41:19 - interaction
41:20 - between a and b so what we're going to
41:23 - do is
41:24 - increase our number of observations from
41:26 - 1 to some
41:27 - larger number c all right
41:30 - so we're going to denote each
41:32 - observation now that we have
41:34 - multiple observations for each combo
41:36 - we're going to need three
41:38 - subscripts all right so each observation
41:41 - is going to be called
41:42 - x i j k so i is for the level of a
41:46 - j is for the level of b and then k is
41:50 - for
41:51 - um which like
41:54 - iteration we've done so like if we're on
41:56 - the second trial for
41:57 - this combo of a and b then k is going to
42:00 - be equal to two
42:01 - so if we're having c
42:05 - different observations for each combo
42:07 - then k is going to go from 1 to c
42:10 - all right so i goes from 1 to a because
42:12 - i is for the different levels of a
42:14 - j goes from 1 to b because j is for the
42:17 - different levels of b
42:18 - and then k tells us which number
42:20 - experiment we are on for that exact
42:22 - combo of a and b
42:24 - all right so since we have a different
42:26 - levels
42:27 - of a b different levels of b and c
42:29 - different
42:30 - runs for each combo then that means that
42:32 - our sample size is
42:33 - a times b times c all right so that's
42:36 - our overall sample size
42:38 - all right so just like we did before we
42:40 - need to define some notation so let's go
42:42 - ahead and do that
42:44 - so again when we have a mean and then we
42:45 - have a dot that means we're going to be
42:47 - averaging over that subscript
42:49 - so if we have x bar i j dot that means
42:51 - we're going to be averaging over
42:53 - the k subscript which means
42:56 - all right for this combo of
43:00 - level of a and level of b let's take the
43:02 - mean so this is like
43:04 - the mean amount of dirt removed if we
43:07 - use
43:07 - this type of detergent and whatever
43:10 - temperature of water
43:12 - and then x bar i dot dot so we're
43:15 - averaging over the last two subscripts
43:18 - so that means that
43:21 - this might be like the mean amount of
43:22 - dirt removed by one of the detergents
43:25 - like
43:25 - the ice level might be tied
43:29 - and then x bar dot j dot
43:32 - that means that we're averaging to find
43:35 - the mean
43:36 - amount of dirt removed by
43:40 - whatever water temperature we're looking
43:42 - at whatever water temperature j
43:43 - is and then finally that overall grand
43:46 - mean is x bar dot dot dot
43:49 - okay so if we're trying to find um the
43:51 - mean
43:53 - for the ife level of a then that means
43:55 - we have to add up
43:56 - all the different water temperatures and
43:59 - all the different
44:02 - trials that we've run so we add over j
44:04 - and k and then divide by the sample size
44:06 - which is b
44:06 - times c similar story for x bar dot j
44:09 - dot
44:10 - our sample size our number of thingies
44:13 - that we're adding up is one over a times
44:14 - c
44:16 - and what are the thingies that we're
44:17 - adding up well we're adding over the i
44:19 - subscript and the k subscript so that's
44:21 - what we're doing here we're adding from
44:23 - i
44:23 - equals 1 to a and from k equals 1 to c
44:27 - and then finally for this last one we're
44:29 - adding everything up because we're
44:30 - looking for that grand mean
44:32 - so we add them all up we're adding over
44:34 - all three of these indices
44:36 - and dividing by the overall sample size
44:40 - all right so that's our notation now we
44:43 - need to
44:43 - go ahead and write down our total sum of
44:46 - squares
44:47 - so as you can notice this is kind of
44:49 - following exactly
44:50 - what we did for one-way anova and
44:51 - two-way anova we're just making it more
44:53 - complicated
44:54 - so remember our total sum of squares
44:57 - that's going to be
44:58 - just our variability between each
45:01 - observation and the grand mean
45:04 - so we have one observation minus the
45:06 - grand mean square it
45:07 - and do that for all of our data points
45:10 - so we're adding over
45:11 - i equals one to a j equals one to b and
45:14 - k equals one to c
45:16 - all right so if we maneuver this just
45:18 - like we did in the one-way anova stuff
45:20 - we can rewrite this as
45:22 - this total sum of squares as b times c
45:25 - times
45:28 - the variability amongst
45:32 - a's different levels plus
45:35 - a times c times the variability amongst
45:38 - b these different levels
45:40 - and then here's the variability amongst
45:45 - the different combos of a and b
45:53 - and then finally we have our error term
45:55 - which is describing how much variability
45:57 - is there
45:58 - within one combo of
46:02 - level of a and level of b so
46:06 - remember x bar i j that's the mean for
46:09 - some combo and then
46:10 - x i j k is
46:13 - one observation so we're saying okay
46:16 - let's compare each observation within
46:18 - some combo of ambi to what its mean is
46:23 - square that add them all up all right
46:26 - so what that tells us is that
46:31 - this is our sum of squares for a
46:38 - this piece is our sum of squares for b
46:46 - the middle piece here is our sum of
46:47 - squares for the interaction
46:58 - and then finally this last piece here is
47:00 - the
47:01 - sum of squares due to error
47:06 - all right so in the next video we're
47:07 - going to talk about the distributions of
47:10 - each of these and then find our test
47:12 - stats
47:20 - now that we have set up the notation for
47:22 - our two-way anova with
47:24 - multiple observations for each combo of
47:26 - a and b we can actually get into some of
47:28 - the
47:28 - statistical properties all right so just
47:30 - like in all our previous anova
47:32 - situations
47:33 - the total sum of squares divided by
47:34 - sigma squared has a chi-squared
47:36 - distribution with n minus 1 degrees of
47:38 - freedom but what is
47:39 - n n is a times b times c so ss total
47:42 - over sigma squared has a chi-squared
47:44 - distribution with
47:45 - a times b times c minus 1 degrees of
47:48 - freedom
47:49 - all right so if we use our same line of
47:52 - reasoning as in our previous anovas
47:54 - then we know that s a over sigma squared
47:58 - ssb over sigma squared assets a b over
48:01 - sigma squared
48:02 - and sse over sigma squared are all
48:04 - independent and we're going to see that
48:06 - each one of these has a chi-squared
48:07 - distribution
48:09 - all right so ssa over sigma squared has
48:11 - a chi-squared distribution
48:13 - with a minus 1 degrees of freedom as
48:15 - long as
48:16 - our null hypothesis for a is true in
48:18 - other words as long as the means among
48:21 - a's different levels are all
48:22 - equal then ssa over sigma squared has a
48:25 - chi-square distribution
48:26 - with a minus 1 degrees of freedom
48:29 - similarly when the means amongst b's
48:33 - different levels are all equal then ssb
48:35 - over sigma squared
48:36 - has a chi-square distribution with b
48:37 - minus 1 degrees of freedom
48:41 - similarly when the means amongst
48:44 - all combos of a and b are equal in other
48:47 - words when there's no interaction term
48:48 - then ss a b over sigma squared has a
48:51 - chi-squared distribution with
48:52 - a minus 1 times b minus 1 degrees of
48:55 - freedom
48:56 - all right so if we have chi squared
48:59 - equals chi squared plus chi squared plus
49:02 - pi squared plus something
49:04 - then we know that that last thing e has
49:06 - to be a chi-squared random variable as
49:08 - well
49:09 - so that's exactly what we have going on
49:10 - just like we've been seeing all along
49:12 - so s e over sigma squared must be a
49:15 - chi-squared random variable
49:17 - and to find this degrees of freedom we
49:19 - would just use the fact that degrees of
49:21 - freedom add
49:22 - so we know that
49:25 - a minus 1 plus b minus 1 plus
49:29 - a minus 1 times b minus 1 plus
49:32 - whatever the degrees of freedom are for
49:34 - sse over sigma squared
49:35 - must equal n minus 1 which is a times b
49:39 - times c minus 1. so if we
49:43 - just do a little bit of arithmetic we
49:45 - can find that the degrees of freedom for
49:47 - sse over a sigma squared is
49:49 - a times b times c minus one
49:53 - all right so now we have all those
49:55 - chi-squared random variables set up we
49:57 - can
49:58 - now do some of our test stats so say
50:01 - that we want to test whether there is an
50:02 - interaction term
50:04 - so we're trying to figure out is there
50:06 - some combo
50:07 - of detergent and water temperature that
50:09 - will more effectively or less
50:11 - effectively
50:12 - remove dirt from clothing so we're going
50:15 - to use this test statistic here we're
50:17 - going to look at the
50:18 - variability among the different
50:20 - interaction terms so we're looking at
50:21 - ssab
50:23 - and we're comparing it to the
50:25 - variability
50:26 - within the interactions in other words
50:29 - we're looking at the variability due to
50:31 - error
50:32 - so the variability amongst
50:35 - all our different runs of the same type
50:37 - of detergent and the same
50:39 - temperature of water so our test stat is
50:42 - we can call it fab the sum of squares
50:45 - for a b
50:46 - divided by the degrees of freedom which
50:49 - is
50:50 - a minus 1 times b minus 1. and then in
50:53 - the
50:54 - denominator we have sse divided by
50:57 - a times b times c minus 1.
51:02 - all right so that's our test statistic
51:04 - we know that the numerator is a
51:06 - chi-square
51:06 - the denominator is a chi-squared and
51:08 - they're independent therefore
51:10 - this test statistic must be
51:14 - an f random variable and its degrees of
51:17 - freedom
51:17 - are this is the first degree of degrees
51:20 - of freedom
51:21 - and this is the second degrees of
51:22 - freedom so it's an f
51:24 - with a minus 1 times b minus 1 degrees
51:26 - of freedom for the first degrees of
51:28 - freedom and the second degrees of
51:29 - freedom is
51:30 - a times b times c minus 1.
51:33 - all right so this test at
51:36 - has this f distribution under the null
51:38 - hypothesis
51:39 - so as long as in reality there's no
51:43 - interaction term then this will be true
51:46 - so in other words as long as actually
51:49 - there's no combo
51:50 - of water temperature and detergent that
51:53 - does
51:53 - better or worse than the other ones then
51:56 - this distribution
51:58 - this uh test at has this sampling
52:00 - distribution here
52:02 - all right so then we can find our p
52:04 - value
52:06 - by looking at as always
52:11 - our right tail so here's our f
52:12 - distribution with a minus one times b
52:14 - minus one
52:16 - degrees of freedom for the first degrees
52:17 - of freedom and then a times b times c
52:19 - minus one for the second degrees of
52:21 - freedom
52:22 - we mark off our test stat here f a b and
52:25 - we look at the area to the right and
52:26 - that is our p value
52:28 - so if we have a small p value then we
52:30 - can say actually there must be
52:33 - some interaction that has a different
52:36 - mean than the others in other words
52:38 - there's some combo of detergent and
52:41 - water temperature
52:42 - that more effectively or less
52:44 - effectively removes
52:46 - dirt from clothing and if our p-value is
52:49 - too big
52:49 - then we would say there is no combo of
52:53 - water temperature and detergent that
52:56 - more or less effectively removes dirt
52:58 - from clothing
53:00 - all right so that's how we would test
53:01 - our interaction term now let's talk
53:03 - about these main effects
53:04 - so i'll show you just for a but b is the
53:08 - exact same story
53:09 - so to test whether any of a's levels
53:11 - have a different mean we're going to use
53:13 - this test statistic
53:14 - here so again we're going to look at the
53:16 - variability
53:18 - amongst a's different levels and
53:20 - compared to
53:21 - the variability within groups
53:25 - okay so we have ss a divided by
53:29 - number of levels in a minus 1
53:32 - in the numerator and then our
53:34 - denominator is sse divided by
53:36 - a times b times c minus 1.
53:43 - all right so that's our test statistic
53:47 - if all the means for the levels of a
53:50 - are actually equal then that test
53:52 - statistic will have an f distribution
53:54 - with a minus 1
53:55 - times a times b times c minus 1 degrees
53:58 - of freedom
54:01 - and our p value again we're going to
54:03 - draw out our f distribution
54:05 - mark off our test at
54:09 - and the p-value is the area to the right
54:11 - so our p-value is the probability that
54:13 - an f-distribution
54:14 - with a minus one and
54:17 - a times b times c-minus one degrees of
54:19 - freedom is greater than
54:21 - our test statistic finally a good thing
54:24 - to know
54:25 - again would be an unbiased estimator for
54:27 - sigma squared
54:28 - so that's going to be similar to all the
54:31 - previous anovas that we've looked at
54:33 - sse divided by a
54:36 - times b times quantity c minus 1.
54:40 - all right so that wraps up all the anova
54:43 - stuff
54:43 - it would be very useful to look at the
54:47 - anova tables in the book just to help
54:48 - you kind of organize everything
54:51 - as you are preparing to work on this in
54:58 - class