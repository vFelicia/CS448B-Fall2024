00:00 - conditional and joint probabilities are
00:03 - concepts in probability theory that are
00:06 - that are quite useful when multiple
00:09 - events are occurring a joint probability
00:16 - is quite easy to describe it's the
00:19 - probability that an event a and an event
00:23 - B occur at the same time
00:27 - so let's take for example that event a
00:31 - is the probability of landing let's say
00:38 - that a is defined as we have to define
00:41 - the event first a is the that a coin
00:44 - flip lands as heads and let's say that B
00:54 - is the event that a die rolls as a 2 we
01:12 - can easily calculate the probability of
01:14 - a this is simply equal to 1/2 we can
01:20 - also equally calculate the probability
01:22 - of B this is equal to 1 and 6 right fair
01:29 - die now let's ask the question what's
01:31 - the probability of a and B occurring
01:33 - together this is the joint probability
01:36 - well the probability that you both land
01:41 - as ahead and roll it to 1 these are
01:46 - independent events one doesn't influence
01:48 - the other so you have to have both coin
01:51 - and landing as ahead an or a rolling a
01:53 - die these are just these two multiplied
01:55 - together right it's 1/2 times 1/6 which
01:58 - is equal to 1/12
02:08 - that doesn't seem particularly
02:10 - insightful but things get a little more
02:14 - interesting
02:15 - when we talk about events that are not
02:22 - independent let's define another event
02:27 - event C as the die roll being even so
02:41 - now what's the probability of B and C
02:47 - occurring at the same time well if it
02:52 - turns out that the Dyer comes out as a 2
02:54 - then we know that the roll is also even
02:59 - so this is just 1 over 6 because it
03:05 - doesn't constrain it anymore similarly
03:12 - the probability that probability of C
03:13 - all by itself right is equal to 1/2 but
03:19 - here we did not multiply 1/6 by 1/2
03:22 - because we know that there's an
03:24 - interaction effect these are not
03:26 - independent events
03:38 - now let's ask a different question let's
03:41 - ask the likelihood let's ask for the
03:44 - probability of two events occurring but
03:48 - knowing that one of the events already
03:51 - occurred so let's ask what the
03:57 - probability of B and C occurring is
04:04 - right this event of rolling a dice a two
04:08 - and the diving even but we know that
04:13 - this actually occurred we know that C
04:16 - occurred let's then divide it by the
04:18 - probability that the die roll was even
04:27 - we know that I'm telling you that this
04:29 - occurred we're asking for the
04:31 - probability that these two events
04:32 - occurred at the same time but we know
04:34 - that I'm just we're just speculating I'm
04:37 - speculating but we're just defining
04:38 - we're saying that C occurred the
04:41 - probability of the die roll B even
04:42 - happened we know that it is even so if
04:46 - we know that it's even what is the
04:49 - probability that it lands as a - well
04:54 - that's this expression is 1 over 3
05:01 - because if we know it's even then it's
05:04 - either two or four or six and it's one
05:08 - of those three so it's 1 and 3 this is
05:10 - the same write as 1 over 6 which was the
05:19 - joint the joint probability divided by
05:22 - probability of C which was 1 over 2
05:25 - which is equal to 2 over 6 which is
05:29 - equal to 1 over 3 right it's the same
05:33 - that's quite cool and we define this
05:36 - expression
05:39 - as the joint as the conditional
05:42 - probability what's the probability that
05:46 - B occurs
05:47 - knowing that C occurs and this is equal
05:50 - to
05:52 - he joins C over P see I'm just rewriting
05:58 - this again so in so we have it in green
06:01 - in fact what I'm gonna do is I'm gonna
06:03 - write it with a and B the probability
06:09 - that some event a occurred knowing that
06:12 - some event B occurred is equal to the
06:15 - joint probability of a and B divided by
06:22 - the probability of B now it gets a
06:33 - little
06:33 - more interesting because this idea of
06:38 - two events occurring together is not by
06:44 - any means selective for a coming in
06:48 - front of B two events occurring
06:50 - simultaneously there's two events
06:52 - occurring simultaneously so there's
06:53 - actually two ways of writing this I mean
06:57 - that this let's look at this if we have
07:00 - the probability of an A intersecting
07:06 - with probability to be that these two
07:07 - occur together in fact what I'm gonna do
07:11 - is I'm going to write this in the middle
07:14 - and show it interpret it in two ways
07:21 - then on one hand write this is equal to
07:26 - we can divide this by by B as we did
07:30 - before and over here show our
07:35 - conditional probability so this is the
07:37 - conditional probability of a conditioned
07:42 - on B and normally I would have this
07:46 - right in the previous in the previous
07:47 - slide I had this divided by B but we'll
07:49 - just multiply that over here times the
07:51 - probability of B same thing I haven't
07:53 - changed the equation but I needed this
07:56 - is the same like the probability of a
07:58 - and B happening together is also the
08:00 - same probability of B and a happening
08:02 - together so there's another way that we
08:03 - can write this on this side such that
08:07 - this is also the probability of B
08:08 - conditioned on a time's the probability
08:11 - of a
08:19 - and if this is true if this is equal on
08:23 - both sides that means this and this are
08:25 - equal and if that's the case then let's
08:30 - throw this B over out here and rewrite
08:33 - this a B by itself and we'll do that one
08:37 - in yellow
08:38 - so here I'm gonna write the probability
08:41 - of a conditioned on B times the
08:45 - probability divided and I'm dividing
08:48 - this on the other end that's let me set
08:49 - this equal to this guy here probability
08:53 - of B conditioned on a time's the
08:56 - probability of a and I'm gonna pull this
08:58 - guy and just divided over here divided
09:00 - by probability of B and this equation is
09:08 - bayes's theorem
09:16 - and it is the basis for all statistical
09:20 - inference the conditional probability of
09:23 - a conditioned on B is equal to the
09:30 - probability of B conditioned on a times
09:32 - the probability of a divided by the
09:34 - probability of B these ideas actually
09:41 - have very common names that are often
09:46 - thrown around this value here is the
09:49 - posterior this value here is the prior
09:56 - this value here is the likelihood this
10:04 - value here this term over here is the
10:06 - evidence you'll also see these two terms
10:12 - called the marginal probabilities
10:23 - the marginal probability is just the
10:25 - probability of an event by itself so
10:28 - these are both marginal probabilities
10:30 - but if you have an event that you're
10:34 - trying to figure out the probability of
10:35 - and you know some other event occurred
10:38 - already right you have some point of
10:40 - evidence about it that you know these
10:42 - two are related then you can condition
10:45 - the likelihood of a occurring based on
10:48 - the evidence you've seen in B and
10:49 - rewrite it this way because here what
10:54 - you're asking you're saying is well how
10:56 - it's the probability that the evidence
10:59 - that I'm seeing is what it is if I
11:01 - assume that a is the event of interest
11:07 - that I'm looking at you multiply that by
11:08 - how likely a event is a by itself
11:10 - divided by the probability that you see
11:14 - the evidence B that you got and you
11:16 - actually will get your posterior and it
11:18 - all comes out of this relationship that
11:20 - it that the that the joint probability
11:23 - can be written in two different ways of
11:25 - conditional probability the naive Bayes
11:33 - classifier is a very simple statistical
11:37 - learning technique that leverage is the
11:41 - relationship of bayes's theorem to
11:44 - perform classification task that is
11:47 - otherwise difficult to simply estimate
11:51 - when there is lots of evidence available
11:54 - from different different features of
11:57 - data then you can make an estimate here
12:01 - of what the likelihood of the event is
12:04 - interested you know of interest is again
12:07 - this is our posterior this is our
12:09 - likelihood this is our prior and this is
12:12 - our evidence the naive Bayes classifier
12:17 - was one of the very first successful
12:21 - classifiers that could detect for
12:24 - example whether or not a piece of mail
12:27 - with spam versus legitimate so it was
12:30 - actually one of the very original spam
12:32 - busting algorithms that was deployed in
12:35 - the net
12:36 - and the Internet in the early days now
12:38 - we have much more sophisticated
12:39 - algorithms but the naive Bayes
12:41 - classifier still does a decent job and
12:45 - the way it works in general is that you
12:50 - want to for example a would be write the
12:53 - probability if we're talking about the
12:55 - spam example the probability that a
12:58 - piece of mail is spam and B here would
13:11 - be the evidence that we obtained right
13:16 - and the evidence could be all sorts of
13:17 - things it could be you know the text of
13:21 - the email it could be where it comes
13:25 - from it could be the time of day it came
13:40 - from it could be the length
13:48 - oops
13:52 - of the email it could be the presence or
14:03 - absence or the presence of certain words
14:07 - in the email or it could just be all of
14:21 - the different words the most classic
14:23 - version of this is simply the collection
14:25 - of all the words that are present in the
14:28 - eye so this is text all words the most
14:34 - the most classic version of this simply
14:37 - looked at just that I didn't do any of
14:40 - this stuff just took all the words that
14:44 - were in there and treated every word
14:47 - totally independently one of the key
14:49 - features of the naive Bayes classifier
14:51 - is that it assumes every piece of
14:52 - evidence that you're using is fully
14:54 - independent if your if your evidence is
14:57 - not independent then that can cause
14:59 - problems for the accuracy of the
15:01 - classifier so that's called a model
15:02 - mismatch but the naive Bayes classifier
15:04 - fundamentally assumes that all of the
15:07 - different pieces of evidence the
15:10 - different features the dimensions and
15:12 - the features in this case every word
15:13 - that's present is independent that may
15:16 - or may not be true but that's what the
15:18 - assumption of this D of this classifier
15:20 - does so that's our evidence B and so now
15:25 - let's take a look at what we're doing
15:27 - here what we're doing is we're saying
15:29 - well what is the probability we're gonna
15:31 - dam we're gonna we're gonna look at this
15:32 - what's the probability that we got that
15:34 - evidence all right so the probability of
15:37 - this piece of mail is BAM given the
15:38 - evidence we got right the text the words
15:41 - in the text is equal to the probability
15:45 - that we got this piece of evidence
15:47 - conditioned on this being a piece of
15:52 - spam would we get these words if what's
15:55 - the likelihood of it what's the
15:56 - probability of us getting those those
15:58 - words together seeing those words in
16:01 - this email can
16:02 - on the fact that this we're assuming
16:04 - this is either a piece of spam or
16:06 - condition on the fact that it's not a
16:08 - piece of spam that would be the
16:11 - probability this piece of mail is not
16:12 - spam
16:13 - all right so if a is the probability
16:14 - that it's not a spam then if this is
16:18 - this is just a binary classification
16:19 - then not a write the inverse of a
16:22 - opposite of a would be the probability
16:25 - of this piece of mail is not spam and
16:27 - thus that's equal to the probability
16:28 - that we see this piece of evidence
16:30 - conditioned on the assumption that it is
16:31 - not right not a not a piece of spam then
16:36 - we multiply this by the prior which is a
16:38 - probability that any piece of arbitrary
16:41 - mail that comes in is spam and so your
16:44 - prior might be very very high because in
16:46 - most mail systems the bulk of mail that
16:49 - goes through is just spam so this number
16:51 - and the probability that your mail is
16:54 - spam could be very high initially for a
16:56 - and the probability that it's not a it
16:58 - could be extremely low what does this
17:00 - mean this means that it helps push the
17:02 - system to make I things as spam because
17:05 - if for a thousand pieces of email that
17:08 - come in if 998 of them are spam and only
17:13 - two of them are real ajith omit then
17:16 - your prior right knowing nothing else
17:18 - other than a piece of email has come in
17:21 - you want to just assume that you know
17:24 - you your chances of that being spam is
17:27 - extremely high and so your prior right
17:29 - your your non evidence driven
17:32 - probability of this thing being spam is
17:34 - already very high and so you need to
17:36 - have overwhelming evidence in order to
17:40 - convince you that it is not spam right
17:43 - this number the probability of seeing
17:45 - those were conditioned on conditioned on
17:51 - this being spam needs to be extremely
17:54 - small
17:55 - to counteract something with the lard
17:57 - bias and that's that's exactly what
18:00 - these distributions are designed to do
18:02 - and then you divide everything by the
18:03 - probability of seeing that slice of
18:05 - evidence now for classification tasks
18:08 - this is not something that you tend to
18:10 - worry about very much because even
18:13 - though technically that will give you
18:14 - the actual probability here
18:16 - what you are really doing a naive Bayes
18:18 - classifier and this is in this two
18:20 - version example is pairing this
18:22 - probability with the opposite
18:25 - probability the probability of the other
18:28 - term all right so let's draw a line here
18:31 - you're comparing it to the probability
18:34 - that it's not spam given that we saw
18:39 - this piece of evidence and that's equal
18:41 - to the probability of B conditioned on
18:45 - not a times the probability of not a
18:50 - divided by the probability of B well
18:55 - this is great because if this is the
18:57 - probability that a piece of mail is not
18:59 - spam given our evidence and this is the
19:01 - probability that is spam given our
19:03 - evidence we're dividing by the
19:05 - probability of B in both cases let's not
19:07 - even bother because we don't want I
19:09 - think about that that's just the term
19:12 - that is the same in both so we can just
19:14 - get rid of it wonderful now we just have
19:18 - these two terms the probability of B
19:21 - conditioned on a and the probability of
19:23 - a or the probability of a not now in the
19:26 - case that the probability of the events
19:28 - is the same guess what then this term
19:32 - and this term would be the same and you
19:34 - could cancel these out as well when
19:36 - would this occur this would occur in the
19:38 - case where if you in a thousand emails
19:40 - half of them are legit and half of them
19:43 - are spam the probability or priors right
19:45 - the probability that any arbitrary piece
19:47 - of mail is spam is half and the
19:50 - probability that any arbitrary piece of
19:52 - mail is not spam is also half and thus
19:55 - these two probabilities these priors
19:57 - would be equal and you can cancel them
19:58 - out of the calculation for the spam
20:01 - example you cannot do that because we
20:05 - just said in a thousand emails 998 of
20:08 - them are going to be spam thus these
20:10 - probabilities need to be kept so that
20:13 - you can you can properly account for
20:16 - that other examples may have an equal
20:18 - distribution of events for a and not a
20:22 - or all the different different type of
20:23 - events that it could be and thus in
20:25 - those cases you can cancel this out
20:29 - but it's not a given you have to be very
20:31 - careful about what-what this prior is
20:34 - telling you let's again label our terms
20:38 - just so we know this is posterior and
20:40 - I'll do this in some green here so this
20:43 - is the posterior this is our evidence
20:51 - this is our prior and this is our
20:55 - likelihood and so now we're task right
21:04 - so it's obvious that we can calculate
21:06 - these these probabilities for our priors
21:08 - they're easy right because you just look
21:10 - at all of the different males you get
21:11 - and you know for some training set
21:13 - whether or not they're spam or not you
21:15 - just count those probabilities and thus
21:16 - you have your priors no problem now we
21:18 - have to think about how do they get this
21:20 - likelihood this is now the challenge to
21:24 - model what is the probability of seeing
21:26 - a particular evidence distribution that
21:28 - we just saw conditioned on the fact that
21:30 - we are assuming that this is spam or
21:33 - what is the probability of seeing this
21:35 - probability distribution conditioned on
21:36 - the fact that it is not spam that is not
21:40 - something that you can calculate
21:42 - explicitly however it is something that
21:45 - can be modeled and we can model it
21:49 - because we have prior data telling us so
21:53 - in order to build and execute an EIN
21:55 - based classifier you need a training set
21:57 - you need prior data and examples with
22:01 - which to build this likelihood estimate
22:04 - you need to be able to take some
22:07 - training set label some of them as spam
22:10 - some of them as not spam manually
22:13 - someone has to do this and then if you
22:16 - are looking for evidence and your
22:18 - evidence is going to be this text and
22:20 - all of the words in the text then you
22:23 - need to go in and find the probabilities
22:26 - that any given words showed up in spam
22:32 - versus not spam emails and you need to
22:34 - have a dictionary a table of all the
22:37 - different probabilities for all the
22:39 - different words
22:42 - when they are spam versus not spam and
22:47 - so if you had for example you were
22:54 - building this model if you had the word
22:56 - for example offer right or money then
23:04 - these terms Knight or opportunity or
23:12 - virus or hack right these are often
23:20 - terms that are usually in if they're in
23:24 - an email then you know or no other term
23:27 - Pro terms like MoneyGram right right
23:36 - these terms are more often associated
23:42 - with spam emails than not this the
23:49 - probability of seeing a word like this
23:52 - conditioned on it being spam is higher
23:56 - for virus hack and offer than it would
23:59 - be for not spam just as examples further
24:03 - because the naive based classifier
24:06 - treats everything at all the features
24:08 - all the words in this case since these
24:10 - are features as independent you can
24:12 - separately multiply the probabilities
24:15 - for every single one these words
24:16 - independently because you're not
24:17 - modeling any correlation between them
24:19 - that is often a limitation because if an
24:22 - email has the word hack in it probably
24:24 - also has the word virus in it or if it
24:26 - has the word offer and it probably also
24:27 - has the word money in it and if these
24:29 - terms are present together that could
24:32 - probably even strengthen right your
24:34 - ability to determine whether or not this
24:36 - is a piece of spam or not but that's not
24:38 - within the scope of this classifier
24:39 - classifier doesn't do this then I based
24:41 - classifier simply assumes that each of
24:43 - the features are independent and thus
24:44 - will independently look at the
24:46 - probability of the whatever words are in
24:49 - the the email and look up in its table
24:53 - look up in its model what the
24:55 - probability of see
24:56 - that word is conditioned on looking up
24:59 - in the in the spam table versus a non
25:00 - spam table and give you that probability
25:03 - as an output and then you do that for
25:05 - all different words that are in the
25:06 - email and you have a string of
25:08 - multiplications one for every single
25:10 - word and that if you look it up from the
25:13 - probability of the of the spam table of
25:17 - the spam spam train table gives you some
25:20 - probability and if you look it up for
25:21 - the table of words that are in the non
25:23 - spam category it gives you a different
25:25 - probability of multiplicative property
25:27 - and then you multiply those by the prior
25:29 - and whichever of these two are higher
25:31 - you then classify that piece of email as
25:36 - spam or not spam accordingly it's
25:39 - actually a very clever and simple idea
25:40 - but it requires being able to build
25:43 - these likelihood estimates and that
25:45 - requires previous training data but
25:48 - that's really all that's going on that's
25:50 - the process of a naive Bayes classifier
25:53 - it simply looks at prior examples builds
25:56 - up some likelihood build some prior and
25:59 - for all the different conditions that a
26:01 - can take on in this case it's only two
26:03 - because it's spam versus not spam it
26:06 - will estimate what the probability of
26:09 - that event occurring given the evidence
26:11 - that it is seen the same evidence in all
26:14 - cases right we're using the same
26:16 - evidence in it to evaluate whether or
26:18 - not a piece of email a piece of email
26:19 - because that's what we're evaluating
26:20 - right the email is there it is spam or
26:24 - not spam and so we're smashing it
26:25 - through the different likelihood models
26:27 - for some given evidence to see whether
26:29 - or not this conditional probability or
26:32 - this conditional probability is going to
26:33 - be higher and if you have three
26:35 - different possible outcomes of a right
26:38 - if it's if it's a decision you're making
26:40 - whether you have to go forward left or
26:42 - right then you have three different
26:44 - likelihood tables you have to look up or
26:46 - three different likelihood models you
26:47 - have to consult then you end up with
26:48 - three different probabilities and you
26:50 - take the highest of those and that's
26:51 - what the classifier would classify
26:54 - that's all that's going on in our Bayes
26:56 - classifier very simple but also
26:58 - extremely powerful is if you can get
27:00 - this likelihood model correct if you
27:03 - model your evidence correctly against
27:05 - the different conditions then it can be
27:07 - extremely accurate assuming the
27:10 - there is a difference of abilities in
27:13 - across the event conditions what does
27:18 - this mean in you when would about naive
27:21 - Bayes classifier fail for this example
27:23 - if a night anion based classifier would
27:25 - feel if spam and real emails used the
27:28 - same words right if the same words were
27:32 - used for both spam and non-spam emails
27:35 - in exactly the same frequencies and all
27:37 - that kind of stuff then this decoder
27:39 - this classifier wouldn't work at all it
27:40 - would not be able to tell the difference
27:42 - between spam and not spam because that's
27:45 - all it's using to come up with these
27:47 - these likelihood estimates but
27:51 - mercifully right what can what is very
27:55 - salient what's very important in
27:57 - determining whether a piece of email is
27:59 - spam or not is the actual words that are
28:01 - used and thus the likelihood of seeing a
28:05 - particular set of words when something
28:07 - is spam are going to be very different
28:08 - and the likelihood of seeing those same
28:10 - words if that if that email is not spam
28:12 - and this is how a naive Bayes classifier
28:15 - works this is how the earliest spam
28:17 - detectors worked by simply pulling out
28:19 - these models with all of the different
28:21 - emails that had previously been banked
28:23 - labeling them manually and building
28:25 - these banks of likely residents