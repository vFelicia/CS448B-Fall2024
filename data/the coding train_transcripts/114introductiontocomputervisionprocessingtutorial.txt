00:00 - okay so this is the last video in this
00:02 - section of videos about images and
00:04 - pixels and
00:05 - what i want to talk about in this video
00:07 - is computer vision
00:09 - creating using using an image as
00:11 - something other than a thing we draw on
00:13 - the screen or a thing we look up colors
00:15 - from to draw other things on the screen
00:16 - what does it mean to try to have our
00:19 - program our thing we're making and
00:21 - processing
00:21 - see the world in some capacity maybe to
00:24 - determine if there's a user present
00:26 - maybe to determine if the user is waving
00:28 - his or her hand maybe to determine what
00:30 - color clothes the user is wearing
00:32 - there are lots of things we might be
00:34 - able to figure out from a scene
00:36 - if we have a camera looking at that
00:37 - scene or an image that was taken
00:40 - from somewhere okay so this is a huge
00:42 - topic and
00:44 - somebody who actually knows what they're
00:45 - talking about could probably make a
00:47 - long series of videos going quite in
00:49 - depth about
00:51 - topics and examples in computer vision
00:54 - what i'm going to do in this short 10 to
00:56 - 20 minutes here is simply uh kind of
00:59 - look at some of the basics what are some
01:01 - of the key fundamental pieces that we
01:03 - need to know about or
01:04 - figure out in order to build our own
01:06 - computer vision application
01:07 - and then what are some pathways to doing
01:10 - more stuff with that so let's
01:12 - let's let's kind of like start with a
01:14 - basic kind of classic scenario
01:16 - let's say we have a scene there is a
01:19 - camera from our computer pointed into a
01:23 - room
01:23 - and in that room there is a very bright
01:26 - light
01:29 - you know as a this sort of weird-looking
01:30 - light bulb thing that i drew here and
01:32 - maybe there is
01:33 - a person like i can't draw at all
01:36 - uh holding on to that light bulb and
01:38 - they're moving it around
01:40 - could we have a camera over here looking
01:42 - at this scene
01:43 - track that light bulb how would we do
01:45 - that well
01:46 - uh conceptually we might say aha let's
01:49 - find the brightest pixel in this image
01:52 - how could we find the brightest pixel in
01:54 - this image well we might start with this
01:56 - pixel
01:56 - and see that this pixel is very dark
01:59 - it's
01:59 - uh has a color a brightness value of you
02:02 - know zero
02:02 - and then we look at the next one and see
02:04 - it has a brightness value of zero and
02:05 - the next one has a brightness value of
02:06 - three and the next one's a brightness
02:07 - value of four and then zero again and
02:09 - eventually you might get around and be
02:10 - like oh look there's a pixel over here
02:11 - with a brightness value of 255.
02:13 - that's really bright let's remember
02:16 - where this xy location is and as we get
02:19 - to the end of looking at all the pixels
02:21 - could which one had the the highest
02:24 - brightest value
02:25 - this is searching through the pixels to
02:28 - find to try to see where the brightest
02:30 - uh pixel is so let's look at a kind of
02:32 - variation on this for a second
02:34 - over here i have an example which i'm
02:36 - going to run for you
02:38 - that's hopefully going to pull up an
02:39 - image i have a camera right here looking
02:41 - at me
02:42 - um and you can see there i am above over
02:45 - here and i'm going to
02:46 - take this blue marker and i'm going to
02:48 - click on it and you can see oh i'm kind
02:49 - of drawing a dot
02:51 - following this blue marker i'm tracking
02:53 - the blue color
02:55 - in this image that's coming from a
02:56 - camera how is this done
02:59 - now you can see there's a lot of
03:01 - problems there it wasn't so perfectly
03:02 - accurate and this is not
03:04 - there's a lot of issues here that i that
03:06 - i want to take a minute to discuss too
03:08 - but let's just say for the sake of
03:09 - argument
03:10 - that is the be-all end-all of computer
03:12 - vision how do we write a program that
03:14 - does that
03:14 - that recognizes the color in an image
03:17 - and continuously follows that color as
03:19 - it moves over time
03:21 - very similar we talked about here the
03:22 - difference is
03:24 - uh maybe we're looking for the color the
03:26 - the pixel that's the most
03:27 - blue not the pixel that's the most
03:29 - bright so one
03:30 - key thing we need to figure out here is
03:33 - how do we determine
03:34 - if a color is similar to another color
03:37 - so let's think about these two colors
03:39 - here zero comma 100 comma 255
03:43 - r g b
03:46 - and now let's take another this is some
03:49 - color it's kind of
03:50 - bluish greenish now let's take another
03:52 - color 250
03:55 - you know 255. so how
03:58 - similar are these colors well one thing
04:00 - we could say is like uh
04:01 - the red values are 200 units apart
04:05 - the green values are 50 units apart and
04:07 - the blue values are 0 units apart so i
04:10 - could kind of give it a similarity score
04:12 - of 250
04:13 - where zero would be the most similar
04:16 - right if all of these values were equal
04:18 - and uh 255 times three
04:22 - would be the largest uh difference if
04:25 - all of these if
04:26 - you know values were only 0-255 so we
04:28 - can see here like taking the difference
04:30 - subtracting one color from another and
04:32 - you'll notice
04:33 - i'm using the absolute value meaning 200
04:36 - minus 0 is 200
04:37 - 50 minus 100 is actually negative 50 but
04:40 - i don't care
04:41 - whether this one's greater this one's
04:43 - greater i just want to know how far
04:45 - apart are they
04:45 - so this is kind of a way conceptually we
04:47 - could see how different are these colors
04:50 - it turns out that uh uh an accurate
04:53 - way that we can get this all into one
04:55 - line of code
04:56 - let's say i have these two colors as in
04:58 - variables r1 g1 b1
05:01 - and r2 g2 b2 i can say
05:06 - float the difference between two colors
05:10 - is the distance between those colors
05:13 - r1 g1 b1 and r2
05:17 - g2 b2 this line of code right here
05:22 - using processing's distance function
05:24 - will actually give me a
05:26 - rating a value a numeric value that
05:29 - indicates
05:30 - how similar these two colors are how
05:33 - does that work
05:34 - well distance probably makes sense to
05:36 - you if i have a three-dimensional
05:38 - space which is where i am right now my
05:41 - hands
05:42 - are a certain distance apart now they're
05:43 - getting closer and closer and closer
05:45 - well what are there's there's three
05:47 - dimensional space there's like an
05:48 - x-axis a y-axis and a z axis
05:52 - and things in this room that are closer
05:54 - to each other
05:55 - have a lower distance well
05:58 - we could think of these axes instead of
06:00 - being x y and z
06:02 - as being the red axis the green axis the
06:04 - blue axis what if we
06:05 - filled this three-dimensional space with
06:07 - every possible color colors that are
06:09 - nearer to each other
06:10 - would be more similar than colors that
06:12 - are further apart from each other and
06:14 - this
06:14 - use of euclidean distance the distance
06:16 - formula even though
06:18 - conceptually we think of it as something
06:20 - that has to do with physical space or
06:21 - even
06:22 - two-dimensional space in the case of
06:24 - two-dimensional distance
06:25 - we can use that with color as well now
06:28 - why am i spending all this time
06:29 - talking about this just this distance
06:31 - formula this is what we need to do
06:33 - if we're trying to find the color that
06:36 - is the most
06:37 - blue what if i look at this pixel and
06:41 - find its distance from blue
06:43 - then the next pixel and find its
06:44 - distance from blue then the next pixel
06:46 - and find its distance to blue one of
06:48 - those pixels
06:49 - will hold the world record for smallest
06:52 - distance to blue
06:53 - and if i keep track of that record does
06:56 - this pixel
06:56 - break the record no throw it away does
06:58 - this pixel break the record no throw it
07:00 - away
07:00 - does this pixel break the record yes it
07:02 - does keep its x
07:03 - y now keep going and if i ever find
07:06 - anything else that's greater than that
07:07 - then keep those x y when i after i
07:09 - finish looping through
07:10 - all the pixels i'm going to find the x y
07:13 - location
07:14 - of the pixel that is most blue so the
07:17 - two things that we need are one we all
07:19 - the things we already have is we know
07:20 - how to loop through all the pixels
07:22 - the things that we don't have
07:23 - necessarily from before is how to find
07:25 - the similarity between two pixels this
07:28 - distance function is a great way of
07:29 - doing it
07:30 - and also how to keep track of which
07:33 - pixel is the one i want to remember
07:36 - later after i finish looping through
07:37 - everything so let's go back
07:39 - and look at this example again and i
07:42 - need to run it
07:44 - so um there's a few things i guess i'll
07:46 - point out in the code
07:48 - but let's just make this example run
07:49 - again i'm going to hold up this blue
07:51 - marker and click on it you can see yeah
07:54 - you know it's tracking it it's
07:55 - continually finding this blue marker now
07:58 - a couple things that are going to fail
07:59 - here let's see if i want to track the
08:00 - green
08:01 - green you know i'm clicking on it and
08:03 - you know
08:05 - there's this whole background is green
08:06 - so tracking something green is going to
08:07 - do very good
08:08 - what if i click on my nose to try to
08:10 - track the color of my nose
08:11 - you know the color of my nose similar to
08:13 - my forehead is similar to my hands
08:15 - so there's a lot of flaws in this
08:17 - scenario of just looking for a single
08:20 - pixel
08:20 - you know how much this is jumping around
08:22 - we could add a little easing or
08:23 - interpolation that might help that
08:25 - there's a lot of flaws in this and
08:27 - honestly if we really want to track a
08:29 - color an object that has a specific
08:31 - color in a
08:32 - in a image it probably makes more sense
08:35 - to look for an
08:36 - area of pixels that are very similar
08:38 - we're just looking for a single pixel so
08:40 - um but this you know while this might
08:42 - not be the most useful
08:44 - uh application in a kind of real
08:46 - interactive scenario
08:47 - this demonstrates a lot of the basic
08:49 - principles of computer vision so let's
08:51 - go
08:51 - let's look at a couple things one is in
08:53 - the in the code here
08:55 - one thing i want to point out is this is
08:57 - exactly
08:58 - what i'm talking about um we have
09:01 - a current color from the video and i
09:04 - need to get its red green and blue
09:05 - values
09:06 - and then i have a color that i'm
09:07 - tracking that's the color i'm searching
09:08 - for
09:09 - so i want to know how what's the
09:11 - distance between the
09:12 - current pixel i'm looking at and the
09:14 - color that i'm tracking so this
09:15 - here's the distance function being used
09:18 - inside of this nested pixel loop
09:20 - the other thing that i'll point out is
09:22 - we're starting before we go through the
09:24 - loop
09:25 - we keep track of there's what's the
09:26 - world record the world record
09:29 - some big number that that's obviously
09:31 - the first pixel is going to beat
09:32 - and then we need to keep track of that x
09:34 - and y point so at any moment
09:37 - if we find a pixel with a distance
09:40 - that's less than the world record
09:42 - then the new world record is that pixel
09:44 - and that x and y we should save
09:46 - so this is that second piece the first
09:48 - piece of something new here is that we
09:49 - need to figure out how our two
09:51 - images how are two colors different or
09:53 - similar using the distance function
09:55 - and another piece is how do we while
09:57 - we're looping through how do we keep
09:59 - track of which is our
10:00 - favorite pixel essentially is this our
10:02 - new favorite it is save it
10:03 - and we'll save it again if we find so it
10:05 - doesn't actually matter how what order
10:07 - we look through the pixels we're going
10:08 - to hold on to that
10:10 - quote unquote best pixel all the way
10:12 - through that loop
10:13 - okay so this is giving you some of the
10:16 - basics again you can see there's a lot
10:18 - of flaws here
10:19 - if i were to give you an exercise i
10:21 - might add try to add some easing
10:23 - so that uh let's look at so even if i'm
10:25 - oh this is a black pen let's go back to
10:27 - the blue
10:28 - uh so even if it's you we could probably
10:30 - get this to be at least
10:32 - somewhat smooth the other thing i should
10:33 - point out here is there's no need to
10:35 - display the video
10:36 - look at this woo i'm magically
10:37 - controlling the circle above me
10:39 - um so so one thing where they recognize
10:42 - here is the camera is really acting as a
10:44 - sensor there's nothing about it that
10:45 - we're using for display purposes we're
10:48 - just reading the image
10:49 - and getting some um kind of analyzing
10:51 - for some piece of information
10:53 - so uh uh just recently at the
10:56 - winter show here at itp students built a
10:59 - spray
11:00 - a paint app an interactive paint
11:02 - application where you would hold a spray
11:03 - can up to the wall
11:04 - and inside that spray can was a very
11:06 - bright green led
11:08 - and there was a camera on the wall so
11:10 - the camera was able to track
11:11 - where that spray cam was simply by
11:13 - tracking a very bright led
11:15 - so there's a lot of cases where you
11:16 - might want to track something
11:18 - find it with simply with color
11:22 - i should point out that another um gonna
11:28 - just
11:28 - jump to this another
11:31 - uh scenario where you might like to look
11:33 - at the difference between pixels
11:35 - is for looking for motion so i'm gonna
11:37 - stand here and try to stand very very
11:39 - still
11:39 - and you can see when i stand very very
11:41 - still except for my mouth moving
11:43 - um there you're not seeing any black
11:46 - pixels
11:47 - but if i move around a lot you're seeing
11:49 - a lot of black pixels
11:50 - so one thing that you can do is
11:53 - you can find motion by saying i have one
11:56 - frame of video and i have the next frame
11:58 - of video
11:59 - let me compare each pixel a pixel that
12:01 - changes
12:02 - is likely where there's motion right
12:04 - because
12:05 - if my hand is never moving the pixel
12:08 - color here is not changing
12:09 - but as soon as i move my hand the pixel
12:11 - colors are changing
12:13 - now notice that even just with subtle
12:15 - movements we're getting
12:16 - it's in a way we're only we're finding
12:17 - the edges of my hand because if you
12:19 - think about it
12:20 - the pixel even as i move my hand the
12:22 - pixel colors in the center of my hand
12:23 - aren't really changing
12:25 - it's only changing along the edge where
12:27 - this white part
12:28 - where this with this part of my skin
12:30 - meets the green part of this green
12:31 - screen that you can't see because you
12:32 - see a computer screen
12:33 - so this is another scenario and all
12:35 - these examples are in the
12:37 - learning processing github repository
12:39 - connect this one is
12:41 - example 16.13. so
12:44 - um this is a little bit about a computer
12:47 - vision here
12:48 - i guess i'll just show you one more here
12:49 - there's another example which
12:51 - we can look at just how many pixels have
12:54 - changed per frame
12:55 - so if very few pixels have changed per
12:57 - frame you see the small dot in the
12:59 - center
12:59 - and as i move around a lot a lot of
13:02 - pixels are changing the dot is getting
13:03 - bigger
13:04 - so this is a scenario where we can use
13:07 - and maybe an exercise might be could you
13:09 - find where the motion is
13:11 - so you could create an application where
13:13 - as i'm waving my hand
13:15 - uh i may something's able to follow the
13:17 - thing that is moving
13:18 - so there's a lot of possibilities here
13:21 - in how you use
13:22 - what the computer can see can you find
13:24 - the edges we looked at
13:27 - edge detection and image processing
13:30 - finding a specific color finding the
13:31 - brightest pixel finding the darkest
13:33 - pixel finding pixels that are moving
13:35 - these are the types of things you can do
13:37 - writing your own code in processing
13:40 - however this is not a new idea
13:44 - i didn't invent any of this in fact i
13:46 - know very little about this compared
13:48 - to a lot of people in this world and in
13:50 - fact if you want to work on a computer
13:52 - vision application it's quite
13:53 - likely that am i recording yes and how
13:56 - long uh 13 minutes we're fine so it's
13:58 - quite likely that what you want to end
13:59 - up doing is using a library uh
14:02 - that has a lot of computer vision
14:03 - functionality built into it so one
14:05 - library that i would recommend you
14:07 - look at is uh
14:10 - i'm going to have to just sort of google
14:11 - search this here really quickly
14:13 - github is opencv for processing so this
14:17 - is a library by greg borenstein
14:19 - um opencv is an open source computer
14:22 - vision library originally developed by
14:24 - intel
14:24 - now maintained by open source
14:27 - collectives
14:28 - this is me and you can see here that
14:30 - there's a lot of functionality
14:32 - it can find your face it can do all
14:34 - sorts of image processing it can look
14:36 - for blobs and contours
14:38 - so a lot of the stuff that we might
14:40 - spend hours or weeks or days or months
14:42 - trying to program from scratch
14:44 - are features that are built into a
14:46 - library and you can even see as i get
14:48 - down here there's lots of
14:49 - kind of really interesting features like
14:52 - um like i don't know recognizing a
14:56 - card for example okay so or yeah
14:59 - recognizing uh different markers in an
15:00 - image so anyway okay
15:02 - so well let me show you one one thing
15:04 - that this is kind of classically used
15:05 - for
15:06 - uh is for uh face detection
15:09 - so let me run this one we close these
15:13 - out
15:14 - and back to processing and run this so
15:17 - one of the things that uh opencv
15:19 - will do for you is it will find faces in
15:22 - an image
15:23 - so you can see here as soon as i turn to
15:25 - the side by the way it does not it's not
15:27 - recognizing my profile
15:28 - only if i'm looking dead on so
15:31 - what opencv will do for you is it will
15:33 - give you a rectangle now this is quite
15:35 - different than
15:36 - face recognition right it's this is just
15:38 - face detection oh there's a face
15:40 - there it is that's how big it is but you
15:42 - can imagine some applications you might
15:43 - do here is some
15:44 - how many people are there are somebody
15:46 - looking straight versus looking to the
15:47 - side
15:48 - uh there was a project just here in the
15:50 - itp winter show where
15:51 - two people are standing in scene and
15:54 - their faces are swapped
15:56 - so there's a lot of creative
15:57 - possibilities here you know if you if
15:59 - you use google hangout you can see that
16:01 - you know you can add uh there's all
16:02 - these features you can add a hat or a
16:03 - mustache on somebody you could do this
16:05 - but with the opencv library as well so
16:08 - this is something that i might encourage
16:09 - you to take a look
16:10 - look into as a as a possibility and one
16:13 - thing that opencv has also is blob
16:15 - detection so finding
16:16 - areas of brightness or areas of darkness
16:18 - which is very useful
16:19 - in a tracking sense as well so the last
16:22 - piece that i want to just kind of demo
16:24 - for you here and i'm going to have to uh
16:26 - plug it in is that there is a thing
16:30 - called the microsoft kinect i'm afraid
16:32 - to pick this up
16:33 - can you see this this is the microsoft
16:36 - kinect
16:37 - this thing has a camera in it and a
16:40 - regular
16:41 - rgb camera it has an infrared uh camera
16:44 - and has an infrared projector
16:46 - so what what this sensor is going to do
16:49 - this camera this
16:50 - depth sensor and let me see if i can
16:54 - open this example up
16:59 - and
17:03 - oops hello this is what this is what
17:05 - happened oh there we go okay
17:07 - so you can see that this is showing us a
17:09 - couple different things
17:10 - one is here's another image of me in
17:12 - this room you can see that i have a
17:13 - little tv over here where i can watch
17:15 - myself this is all very strange
17:16 - um and there's just a camera coming out
17:18 - of this connect but then there's also
17:20 - this image over here
17:21 - this image is a depth map now it's very
17:23 - the way that the screen captures work
17:24 - it's very hard to see what's going on
17:26 - here
17:26 - but you can see that my hand is a little
17:28 - bit brighter
17:29 - and now it's quite a bit darker and now
17:32 - and you can see as i put it in here we
17:33 - can't see it anymore
17:34 - so what the the microsoft connect and
17:36 - this is the old connect the 144 this is
17:39 - the
17:39 - original model from a few years ago 1414
17:41 - is the model number
17:42 - um what it does is it's able to
17:46 - not just say like here's the pixel and
17:48 - here's its red green and blue value
17:50 - what it says is here's a pixel and
17:51 - here's how far it is
17:53 - from the connect from the camera from
17:54 - the sensor in millimeters
17:56 - so you can know what's close what's far
17:58 - away and another
18:12 - it's a little hard to see probably where
18:14 - we are based on this like orientation
18:16 - i'm like looking at 12 different places
18:18 - but you can see that this is essentially
18:20 - the data visualized in three dimensions
18:22 - so some really basic
18:23 - 3d scanning of terms of a scene
18:27 - what i'm not using here is there's a
18:28 - library called open ni
18:30 - uh simple benight which is actually now
18:33 - open and i was purchased by apple okay
18:35 - here's the thing
18:37 - i i i don't want to like ramble for like
18:40 - the next 20 minutes about all the
18:41 - different versions of connects and which
18:42 - ones work with which operating system
18:44 - and which library
18:45 - but what you have right now at this very
18:46 - moment there's the original kinect that
18:48 - came out a few years ago that you could
18:50 - use there's a library for
18:51 - pc and processing that you can use
18:54 - there's two libraries for the mac
18:57 - and processing one which is open connect
18:59 - which is a library that i built
19:01 - on top of some open source uh drivers
19:03 - for the kinect and there's also called
19:05 - something called simple open ni
19:06 - which uses open and i which has since
19:08 - been purchased by apple and has been
19:10 - shut down
19:10 - so while all of this works if you're
19:13 - interested in the connect you probably
19:14 - want to go and look for connect version
19:16 - 2.
19:16 - that's the newer connect it's a bit
19:18 - higher resolution a lot of the
19:20 - skeleton tracking which i haven't really
19:21 - mentioned yet is a bit more accurate i
19:24 - think
19:24 - um however uh there aren't open source
19:28 - drivers as easily available for the new
19:30 - connect
19:31 - and so there is an official microsoft
19:33 - developers kit
19:34 - and you can use that with processing
19:37 - however it's only
19:38 - for windows and at the moment there are
19:40 - a lot of people here
19:41 - at itp and probably other places in the
19:43 - world working on ways to
19:45 - pass the information from the kinect and
19:47 - a pc over the network what you have to
19:49 - realize is
19:49 - the connect itself is just sending this
19:52 - raw data
19:53 - what is the distance for each pixel from
19:54 - the connect what you can do with that
19:56 - information is incredibly powerful and
19:58 - if we looked at any demo of the
19:59 - version 2 connect or the old connect you
20:01 - would see it can recognize
20:03 - the form of my body very quickly and and
20:05 - track where my arms are my hand is my
20:07 - head is my knees are and
20:08 - all sorts of stuff you can do with that
20:10 - we don't have time
20:11 - or i don't really uh to get into all of
20:13 - that here and perhaps someday there'll
20:15 - be
20:16 - more videos or more examples that i can
20:18 - do and help prepare in that direction
20:20 - however
20:22 - let's just i just want to jump back for
20:23 - a second and just show you what you can
20:25 - at least do
20:26 - uh something that the kinect makes
20:28 - possible um
20:29 - with just even just the raw depth
20:32 - information
20:32 - so uh one thing i want to do is do this
20:35 - okay
20:36 - so one thing you can see here is uh this
20:38 - is where i'm standing as i walk
20:40 - closer to the connect i start to turn
20:42 - red as i walk further away i
20:44 - i stop being red as i put my hand here
20:46 - my hand is red
20:47 - so with depth one thing that you could
20:49 - do rather easily is say
20:51 - where are the where is the thing that's
20:53 - closest to the camera and in the sense
20:54 - of a hand
20:55 - pointing straight out that's pretty easy
20:57 - to track now notice i put my other hand
20:59 - out
20:59 - now that dot is in between them take
21:01 - this one away
21:03 - switch so this isn't doing any
21:04 - sophisticated hand tracking i could like
21:06 - put my head here
21:07 - it's just looking for like a blob of
21:09 - stuff that's close to the camera
21:11 - i could take this marker and point it
21:13 - out
21:14 - like this uh and you can see it's sort
21:16 - of tracking that i think this marker is
21:18 - actually a little bit reflective
21:19 - so there's a lot more to this uh
21:23 - how it works and infrared light and the
21:25 - depth versus the skeleton
21:27 - and blah blah blah and i'm just kind of
21:28 - doing a terrible job here
21:30 - but what i would hopefully you got a
21:31 - sense in this video if you're still
21:33 - watching
21:34 - is that that for loop right of looking
21:37 - through all the pixels
21:38 - you need that you need that for for the
21:40 - image processing stuff you're doing you
21:41 - need that for some computers and stuff
21:43 - and in fact you need it for this
21:44 - because instead of looking for the color
21:47 - that's the most closest to whatever i'm
21:48 - looking for the pixel that's the closest
21:50 - to the um
21:52 - to the connect itself so that for loop
21:54 - is pretty crucial and there's a lot of
21:56 - things you can do with computer vision
21:58 - however there are also you could do from
22:01 - scratch on your own but there are also a
22:02 - tremendous set of libraries and
22:03 - resources and other devices
22:05 - like a depth sensor that you might
22:07 - consider as well
22:08 - very last thing is i'll i'll try to
22:10 - include a link also there are uh
22:12 - third-party applications open source
22:13 - ones uh like compute
22:15 - community computing community core
22:18 - vision
22:18 - open tsps these are applications that
22:21 - you can run
22:22 - behind the scenes on your computer and
22:24 - have them pass
22:25 - messages about what they're tracking to
22:26 - processing and that that's also a way
22:28 - that you might think about in developing
22:30 - uh computer vision an application that
22:32 - involves computer vision okay
22:34 - so someday i will um kind of revisit
22:37 - some of this stuff in a hopefully more
22:39 - organized or useful way but for now i'm
22:41 - just gonna
22:41 - say goodbye and i have no idea this is
22:44 - probably way too long
22:45 - okay uh but i did manage to get the
22:47 - kinect working in this video which is
22:49 - kind of interesting okay
22:50 - see you later