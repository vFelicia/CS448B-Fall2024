00:00 - all right so in the previous video I
00:02 - went over how to calculate the error in
00:06 - a supervised learning system right the
00:08 - error is just be known answer - the
00:10 - system's guess then what I did is talk
00:13 - about well how can I take that error and
00:15 - move it backwards through the system
00:17 - feedback words instead of feeding the
00:19 - data forward to the network feed the
00:21 - error backwards and the way that I can
00:22 - do that is by looking at like JIT each
00:25 - of these contribute to that error let's
00:26 - kind of like make this error like a
00:28 - portion of that error and these were the
00:30 - formulas that I went through right look
00:32 - at this this neuron hidden neuron is
00:35 - connected to this output this is the
00:36 - weight how much is this way to
00:38 - percentage of all the other neurons that
00:41 - are connected it's just two in this case
00:42 - so it's just the sum of these two but
00:44 - there could be many more so what I want
00:46 - to do in the code now is actually add
00:49 - this training function where I send in
00:52 - input data to the network with a known
00:55 - answer and have the training function
00:57 - calculate the error so calculate this
01:01 - basically I want to calculate this
01:03 - vector e1 e2 and I sent vector but this
01:09 - matrix one column matrix that's what I
01:11 - want to calculate that's going to be a
01:13 - really easy part so let's do that first
01:16 - alright so actually I say in a previous
01:18 - video I sort of set up this function I
01:20 - think right this is this is the
01:22 - feed-forward function where I just take
01:23 - the inputs and I feed them forwards and
01:26 - return the output here is a function
01:28 - where I take the inputs and get an
01:30 - answer now ultimately I should just be
01:34 - able to use the feed-forward function
01:36 - right
01:36 - I might have to tweak this but on the
01:40 - one hand I should just be able to say I
01:44 - should just be I'll say let output equal
01:47 - this dot feed-forward inputs no reason
01:54 - why I should be able to do that and then
01:56 - now what I need to do is calculate the
01:58 - error so the error is Error error equals
02:03 - the target so answer maybe I should call
02:06 - this target targets I'll call a target
02:08 - it might be plural right there
02:10 - these are the target output that I want
02:12 - targets - outputs so let's call this
02:16 - outputs again the first example that I'm
02:20 - gonna build is gonna have just one
02:21 - output but I want the system to work for
02:23 - as many outputs as there are so in
02:26 - theory I should be able say let error
02:29 - equal math dot subtract targets comma
02:35 - outputs now this is where we get a
02:37 - little bit into the weeds here if I'm
02:39 - not math matrix I didn't even know if
02:43 - I've implemented this function in my
02:44 - matrix library so that's why I need to
02:46 - go and check but this is where we're
02:47 - kind of like oh my goodness if we're
02:49 - only we're using numpy we just use - and
02:51 - it just works or maybe there's some
02:53 - other highly optimized javascript matrix
02:55 - math library that will do this for us
02:57 - I'm gonna get to all that yes deep
02:59 - learning is is coming but right now I
03:02 - just want to do this in my code now here
03:05 - there's a little bit of awkwardness here
03:08 - this is an array so this comes out as an
03:13 - array
03:15 - I actually want that to be a matrix so
03:18 - it's a little bit awkward but I'm gonna
03:19 - say outputs equal outputs dot Oh matrix
03:24 - from array outputs so this is me convert
03:32 - convert array to matrix object and then
03:38 - the targets I probably the user is going
03:41 - to send that in as an array so I'm also
03:43 - going to say targets equals matrix from
03:45 - array targets so this is a bit of
03:50 - awkwardness because of the way that I
03:52 - developed my library that probably some
03:53 - day in the future I want to like
03:54 - refactor or rethink but I need to have
03:57 - those things be matrix objects for me it
03:59 - will do this matrix subtraction now
04:02 - let's check the let's check the matrix
04:07 - library I believe there's no subtract
04:11 - function there is an add function which
04:16 - adds to the alright so this is so silly
04:20 - the way that I'm doing this because I
04:21 - probably in a real world if I were
04:24 - really big thoughtful I would make a
04:25 - comprehensive major
04:26 - library that has every possibility that
04:28 - I'm just gonna put it in there what I
04:29 - need so what I need is a static subtract
04:36 - function and I'm gonna have another I'm
04:43 - gonna call it other for the other matrix
04:45 - oh no no matrix a and matrix B so this
04:48 - should return a new matrix a minus B so
04:54 - a couple things one is if the rows and
04:59 - columns aren't the same I mean I want to
05:01 - do this element wise right basically if
05:04 - you've forgotten what's going on here
05:05 - basically I have I have a matrix like
05:07 - this which has these guesses 0.7 0.4 and
05:11 - I want to and I have the known answers 1
05:16 - 0 and I want to take this matrix minus
05:18 - this matrix to give me one that has 0.3
05:21 - negative point 4 so let me add a
05:25 - function that does that it's too bad I
05:27 - didn't have it for before and I should
05:30 - do some error checking here like check
05:32 - if the rows and columns are the same
05:33 - yeah I'll put that in later I'll put
05:35 - that in later gotta kind of move along
05:36 - here so that'll be in there maybe when
05:38 - you look at the code someday but I'm
05:40 - just gonna say I'm gonna do a first I
05:42 - need to make a result which is a new
05:47 - matrix which is a new matrix that has
05:51 - the same number of rows and columns as
05:53 - either of them I think it called rows
05:57 - and columns right so I need to make a
05:58 - new matrix as the same amount and then
06:00 - I'm gonna use my very subtle silly loop
06:03 - function that I use over and over again
06:04 - to loop over all the elements and just
06:08 - say the result dot data index I which is
06:14 - the row index J equals a dot data index
06:19 - i j- b dot data index i index j right so
06:28 - this is we just going through and
06:30 - subtracting everything from one to the
06:32 - other and then i can return this result
06:37 - all right so I had this
06:39 - static subtract function is it silly to
06:42 - have a subtract function when I use have
06:44 - an odd function that then you know add I
06:46 - could be used for subtract by just
06:47 - saying like multiply that whole matrix
06:49 - by negative one but whatever I'm just
06:50 - doing I'm doing it as I'm doing it
06:52 - we'll refactor later all right so now
06:56 - let's think about this I'm going to
07:01 - comment this out and I'm going to say
07:04 - these are my inputs now I'm gonna have
07:07 - my targets and in this case my neural
07:11 - network is two to one so I'm gonna keep
07:14 - that my target is just one I want to get
07:16 - one now what I'm going to do is I want
07:20 - to say neural network instead of
07:22 - feed-forward I want to say train with
07:26 - these inputs and these targets so let's
07:30 - just run the code and see if I get any
07:31 - errors then I'll debug the actual output
07:35 - so let's go to the browser where I've
07:38 - kind of got this coach runs output is
07:41 - not defined sketch is line 11 oh there
07:47 - is no output now hey no errors
07:51 - everything thank you I'm done I forget
07:54 - about the thrill network stuff I get it
07:55 - yeah I have more to do I'm afraid to
07:57 - keep going but I'm gonna keep going
07:58 - let's just look in the training function
08:00 - and console.log the inputs the targets
08:12 - look let's just console.log the air
08:16 - actually I just do arrow dot print sorry
08:20 - so I can say outputs dot print because I
08:25 - have this print function and targets
08:29 - targets dot print and error dot print so
08:33 - I just want to look I just want to
08:34 - examine all these things or I got an
08:39 - error oh error dot print all right so ok
08:44 - something went wrong so this is what did
08:49 - I this is the output this is the target
08:52 - this should be the difference so what
08:54 - went wrong here within my subtract
08:55 - matrix dot subtract target's
08:58 - common outputs let's look at this
09:01 - function something must be wrong here
09:04 - all right here's the mistake this dot
09:06 - rows this columns that makes no sense I
09:09 - make I'm writing a static function
09:11 - that's not called on an instance of a
09:13 - matrix object I want to look through
09:14 - everything I've got to look through
09:15 - everything in result or a and B they
09:17 - should all have the same number of rows
09:18 - and columns thank you to Reuben in the
09:22 - chat who pointed that out to me okay
09:24 - result dot rows result columns all right
09:27 - now let's run this again great this
09:30 - looks right this is what the neural
09:31 - network produced this is what it this is
09:35 - the known output and this is the this is
09:39 - the error and just to just to take this
09:41 - one step further if I were to have two
09:45 - outputs I don't know I'm not and and
09:47 - have a second target this is matching
09:50 - what I've drawn over here we could see
09:54 - these are the guest outputs this is the
09:57 - target and these are the errors so we
09:59 - have now written into our code all of
10:02 - the pieces we need to get these two
10:06 - errors get the air the output errors the
10:09 - next step we need to do is calculate the
10:13 - hidden errors I need to calculate the
10:15 - hidden errors so looking at the code I
10:19 - need another step here I need to now say
10:23 - let hidden errors equal and figure that
10:28 - out so what goes here what goes here
10:32 - this is the and I suppose if I'm being
10:34 - consistent I should say errors there
10:36 - right errors are the out pen and and if
10:39 - I'm being really consistent I should say
10:40 - output errors now what I need to do is
10:44 - calculate the hidden errors okay so
10:49 - let's go back to here and I want to do
10:51 - this with matrix math so this looks like
10:54 - hey that could be some matrix math going
10:56 - on here right this looks like a weighted
10:58 - sum or something dot product e like
11:00 - looking thing
11:01 - here's the trick this looks good
11:06 - right but what's all this like fractured
11:08 - stuff well if you look at this this is
11:12 - the same as this this is the same as
11:14 - this these are really normalizing
11:17 - dividing these weights by the sum of all
11:20 - the weights is a way of normalizing
11:21 - everything so they all add up to 100%
11:23 - but in the end we're gonna kind of like
11:26 - multiply everything by this learning
11:28 - rate constant anyway so we could say
11:29 - like make a big step or make a small
11:31 - step so that normalizing of it kind of
11:33 - doesn't matter
11:33 - I mean it's imported important but we
11:35 - can also ignore it and that's kind of a
11:37 - trickier we're gonna just take out the
11:39 - we know we want the amount of the air to
11:41 - be proportional but the fact that we're
11:44 - multiplying by its weight it's going to
11:46 - be proportional we don't have to divide
11:48 - it by the Sun so we can actually take
11:50 - the bottom out and I'm gonna say wait
11:52 - one one here I'm gonna say wait one two
11:57 - here I'm gonna say wait to two here and
12:02 - I'm gonna say wait to one here and look
12:07 - at this
12:07 - by golly doesn't that look like some
12:10 - matrix math right that's got to be the
12:12 - result of some matrix product now I need
12:15 - more space on the whiteboard so how can
12:18 - I do is kind of condense this a little
12:20 - bit
12:20 - well one one times e 1 plus weight of -
12:24 - 1 times e to make this take up less
12:27 - space and then weight 1/2 times e 1 plus
12:34 - weight what was that - 2 times e to now
12:40 - conveniently I have this matrix here and
12:44 - what matrix would I put here to get this
12:47 - right if I want the matrix product of
12:50 - these two matrices I need to put a row
12:52 - in here that's right the row this row W
12:58 - 1 1 W 2 1 the weighted sum the dot
13:02 - product of this row and this column is
13:04 - exactly this now let's take this 1 1 2
13:07 - and wait - - right
13:10 - take this dot product with here boom
13:13 - we've got this
13:14 - this matrix product the matrix product
13:18 - between these two matrices is that
13:20 - hidden errors coming out of the hidden
13:22 - layer this is really exciting but
13:27 - there's something really strange here
13:28 - it's like stare at this now all along I
13:31 - keep getting my indices wrong right I've
13:33 - been getting my indices wrong in these
13:34 - tutorials I had to do the tories over
13:36 - again and this might look wrong right
13:38 - because shouldn't it be wrong column row
13:41 - column row column row column this is Row
13:43 - one how eyes are - there looks like I
13:46 - got it backwards well the fact is I did
13:48 - get the backwards I got that backwards
13:49 - on purpose this is actually this weight
13:52 - matrix transposed so these weights are
13:54 - stored in a matrix already in my code
13:57 - that matrix looks like this weight one
14:00 - one weight one - wait - one weight - to
14:06 - transpose this matrix - miss and take
14:12 - the dot product with the heirs and boom
14:15 - I've got the hidden errors coming out
14:17 - the good news is I believe in the matrix
14:20 - library I already wrote a function
14:22 - called transpose here's I should be
14:25 - consistent this function transpose
14:27 - returns a new matrix that's the
14:29 - transposition of the previous one and so
14:32 - I should actually make this static and
14:34 - it should require to receive a it should
14:39 - receive some matrix object and so it
14:45 - should be this so I just changed this to
14:47 - be a static function so that what I can
14:49 - do is I can say here back in the library
14:53 - what do I need to turn suppose I need to
14:56 - transpose the weights that are going
14:58 - from hidden to output that's the weights
15:04 - H of so I have this dot weights H oh so
15:16 - let hidden um weights
15:21 - H o transpose T for transpose hmm I
15:29 - could also just change the let Hootie
15:33 - these are the way naming I could use
15:36 - some work on the naming but these are
15:37 - the weights from hidden to output
15:39 - transposed equals matrix dot transpose
15:43 - this dot weights hit an output so the
15:48 - hidden errors now should be matrix
15:53 - multiply output way what did I say we
16:02 - look over here matrix multiply that
16:04 - transposed matrix and those errors that
16:11 - transposed matrix and the output errors
16:17 - so this is calculate the hidden layer
16:22 - errors now if I were writing a proper
16:25 - library that could support multiple
16:27 - layers there'd be some kind of loop
16:28 - going on here because I keep doing this
16:30 - from layer to layer to layer but since I
16:32 - just have this two layer Network I'm
16:34 - just gonna do the output layers
16:36 - I'll put errors and the hidden errors so
16:39 - that I can sort of get this back
16:40 - propagation thing going in just one step
16:43 - I'm getting a nice comment from the chat
16:46 - I was worrying about the dimensions of
16:48 - my matrices and Caitlin writes the
16:51 - dimensions work out when going backwards
16:53 - the transpose right of course because I
16:56 - was worried about the dimensions not
16:58 - working because you know the rows and
17:00 - columns have to match properly when
17:03 - you're doing matrix multiplication but
17:05 - since I'm now going backwards it
17:07 - actually makes sense that the matrices
17:08 - have to be transposed you can pause and
17:10 - think about that for a second but that
17:11 - does make sense now ok so I think I've
17:15 - come oh and I was also but you know if
17:18 - you're looking at this notation in like
17:20 - a textbook or something you'll often see
17:21 - if I have like if you have a weight
17:23 - matrix that's W and maybe it
17:25 - like wi j for rows and columns you'll
17:28 - often see T in the superscript as and
17:33 - you can't see that write it over already
17:37 - over here right you know if this is the
17:39 - weight matrix W and you have columns and
17:42 - rows then you'll often see a superscript
17:45 - of T and that refers to this this matrix
17:49 - trans transposed and so maybe this would
17:53 - be you know weight h.o.t or something so
17:59 - my notation is as you all know is kind
18:01 - of poor but I try to do my best to
18:02 - explain it hopefully it will match up
18:04 - with other other notation that you see
18:06 - in that sort of thing okay so this is
18:08 - now done in terms of back propagation
18:11 - I've computed the error I've computed
18:15 - the I've propagated that error backwards
18:17 - to compute the hidden layers error and
18:20 - now I just need to add the part where I
18:22 - adjust all of the weights these weights
18:25 - based on this error and these weights
18:27 - based on this error and then we just
18:29 - move on and then we're done so I'm gonna
18:33 - do that in the next video I should warn
18:34 - you that the math for doing that is this
18:37 - which I'll discuss in generalities the
18:40 - math of gradient descent for finding
18:45 - these Delta weights is pretty
18:47 - complicated I have two videos that I've
18:50 - made where I kind of do something
18:51 - similar which I will reference as well
18:53 - as all of those three blue one Brown
18:55 - videos that go through the math in
18:56 - detail I'll probably start the next
18:58 - video just by presenting the formula and
19:00 - then implementing kind of talking
19:02 - through the pieces of the formula and
19:03 - then implementing that in code okay
19:13 - you