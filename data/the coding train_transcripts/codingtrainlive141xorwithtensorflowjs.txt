00:08 - hello good evening this is not the
00:11 - evening at all this is the afternoon
00:12 - though
00:13 - welcome to the coding train on a Friday
00:15 - which is my usual day for the coding
00:17 - train but this summer it has not been my
00:19 - usual day I was just having a lot of
00:22 - trouble getting the start streaming
00:24 - button to start
00:26 - I had restricted mode enabled on this
00:28 - laptop for a variety of reasons that I'm
00:31 - not entirely sure of and interestingly
00:34 - enough I cannot start streaming I cannot
00:37 - actually I have to bat it's a long story
00:39 - but I couldn't get that to start
00:40 - streaming the button to work while those
00:41 - in restricted mode I fixed that so here
00:43 - I am how are you what's going on what's
00:46 - what's up what's what's happening got
00:54 - totally weird how you can't speak to me
00:56 - all right there's a chat so you sort of
00:58 - can you could type to me in the chat
01:00 - it's night in India writes Melvin in
01:03 - Israel it is 20 o'clock which i think is
01:07 - 8 p.m. all right so I have to admit
01:12 - something although this is nothing new
01:14 - you know I feel like normally I'm so
01:17 - well prepared and I spent all week big
01:20 - making notes and scheduling things out
01:22 - and knowing exactly what I'm gonna do
01:23 - and then I like I've got all this energy
01:25 - and I turn on the streaming and I'm go
01:27 - go go and I do a tutorial and I turn off
01:29 - the streaming and the day is over right
01:32 - now I feel like it was that I had to
01:34 - make a heroic effort just to make it
01:35 - here and press the start streaming
01:37 - button so I have to admit that I'm a
01:40 - little bit out of sorts but and I but I
01:43 - I have two hours today good news is I am
01:47 - planning for two live streams next week
01:49 - so I am planning to livestream both
01:53 - Wednesday and Thursday it looks like
01:55 - next week and I will publish Tynes to
01:57 - the homepage of this YouTube channel
01:59 - thing soon enough and so I'm hoping to
02:04 - get some get get into I but I really
02:07 - want to do I really want to do something
02:10 - practical a practical is the wrong word
02:12 - I don't do anything Pratt
02:13 - whatsoever I want to do some machine
02:17 - learning demonstrations in the browser
02:20 - with data and data that might interest
02:27 - you or inspire you to use your own data
02:29 - and do something else with it
02:30 - but I'm not there yet let me try to
02:32 - figure out where I am I'm gonna go to
02:38 - YouTube the coding train and I'm going
02:43 - to go to neural networks and machine
02:46 - learning I'm gonna go to session 6 hello
02:50 - and welcome I'm not gonna watch that
02:52 - alright so this is what I this is what I
02:55 - have so far I have to have to do this as
02:56 - beginning of every live stream to sort
02:58 - of recap and refrain recenter myself
03:04 - hi I'm gonna put this over here so it
03:06 - doesn't block the view you may or may
03:10 - not be aware there is something out in
03:12 - the world called tensor flow J yes this
03:16 - is a tensor flow a implementation of the
03:21 - tensor flow API in JavaScript that runs
03:24 - in the browser with no other
03:26 - dependencies all of the math is done and
03:29 - computed using WebGL and shaders and all
03:31 - sorts of amazing gymnastics and ways
03:33 - that I might never understand but enable
03:36 - opens the door and enables possibilities
03:38 - for us the people who like program like
03:42 - this it's a high program to try and
03:47 - experiment and learn a bit about machine
03:49 - learning and get our get our hands in
03:52 - there and and ask the right questions
03:55 - would be critical about the role of AI
03:57 - and machine learning in our world today
03:59 - so that's this I have started doing a
04:03 - series of tutorials these are not super
04:05 - beginner friendly you know there's some
04:09 - advanced JavaScript advanced aspects of
04:13 - the JavaScript language that I'm using
04:14 - that are confusing like promises and
04:16 - weight and they sink you have to do
04:18 - lower level memory management yourself
04:20 - when you make these arrays of data you
04:22 - have to allocate the memory and
04:24 - deallocate the memory
04:27 - and then there's all these like scary
04:29 - weird terms likes get the stochastic
04:31 - gradient descent and that's that's one
04:35 - of them
04:36 - you know optimizer root mean squared so
04:40 - but I'm doing this series for an
04:45 - audience who perhaps has already learned
04:47 - a bit about JavaScript programming maybe
04:48 - watch some of my other basic intro to
04:51 - neural network videos and just kind of
04:54 - like follow along and see how a larger
04:58 - machine learning library works in the
05:00 - browser and can be used so the tutorials
05:02 - that I have so far are sort of an
05:05 - introduction to what tensorflow DOJ's is
05:07 - talking about what it is to be a tensor
05:09 - what is it to be a tensor I want to be
05:16 - more relaxed I want to be a variable
05:17 - instead of a tensor oh look that tents
05:20 - are very bad whatever okay then
05:22 - variables and operations talking about
05:23 - memory management I implemented a
05:26 - version of linear regression which is a
05:30 - kind of like classic machine learning
05:32 - algorithm where you try to fit a line to
05:34 - a bunch of data points kind of serves as
05:36 - the foundation for a lot of machine
05:38 - learning research I also looked at
05:41 - polynomial regression where instead of a
05:42 - line we could fit a polynomial function
05:44 - which curves around and then ah then
05:48 - then then then then I finally finally
05:51 - finally started looking at the layers
05:52 - API and the layers API is a higher level
05:58 - API inside of tension flow j/s which
06:01 - allows you to create machine learning
06:04 - models as sequences of layers and that
06:07 - has you know how that works has to do
06:09 - with how neural networks are architected
06:12 - with inputs and outputs and layers that
06:14 - are in between and there's different
06:15 - kinds of layers and different kinds of
06:17 - math functions that happen with those
06:19 - layers all that sort of stuff so this is
06:21 - where I am at the moment this is where I
06:25 - am where am I going to where am I going
06:30 - to don't ask any more I forgot my
06:34 - ukulele I learned to play the ukulele a
06:36 - couple weeks ago thinking of routing
06:38 - this YouTube
06:39 - with me playing ukulele where am I going
06:42 - - don't ask anymore okay ah so what's
06:50 - next
06:51 - let me make a list really for me but
06:53 - you're watching so you can watch X or
06:59 - with T F layers I want to do a
07:06 - classification example I'm thinking of
07:11 - do what I'm thinking of right I want I
07:13 - so I'm gonna go through these one at a
07:14 - time classification and I'll come back
07:16 - to the details of these I want to do
07:18 - image then I want to do image
07:21 - classification then I want to do image
07:27 - classification again with convolutional
07:34 - layer so now I want to talk about what
07:38 - that is do I want to do some type of
07:43 - regression example maybe maybe so
07:48 - classification and then maybe sort of
07:51 - like as inside a Part B of like a basic
07:54 - like regression so this is kind of what
07:59 - I want to do in terms of the basic
08:01 - building blocks of machine learning with
08:03 - neural networks and I want to build all
08:05 - of these with the tensor flow das
08:09 - library so just just just to lower your
08:15 - expectations for a minute if I were able
08:18 - to get even just this done today I will
08:22 - be very happy about that okay so this
08:24 - this is kind of my goal that's my goal
08:27 - for today now I am doing things
08:29 - backwards in one in one sense I'm doing
08:33 - that these these are not super beginner
08:35 - friendly I mean they're as beginner
08:37 - friendly as I can make them I want them
08:39 - to be friendly but you know if I was my
08:43 - first day watching a coding video I
08:45 - might not want to jump
08:46 - into the image classification with the
08:48 - tensorflow layers api video which
08:51 - doesn't exist yet but but I once I get
08:54 - through this or actually once once June
08:56 - 15th hits or maybe even a little bit
08:58 - before June 15 that's next week
09:00 - I'm gonna start doing some
09:02 - beginner-friendly and hopefully gonna
09:08 - guests come and do these with me maybe
09:10 - somebody was watching this right now
09:11 - who's worked on this ml5 project would
09:13 - like to come beginner-friendly
09:14 - ml with some hearts and some stars like
09:22 - a little like rainbow and then like
09:27 - there's a train train going by I can't
09:31 - draw a train okay and with a library
09:45 - called ml v dot Jas so I'm gonna come
09:51 - back to this a second so just briefly
09:52 - let me just show you you catch a guess
09:57 - that's a great idea for a so if you go
10:00 - right now on the Internet to a URL ml v
10:05 - JSTOR G you will find this website and
10:09 - this is gonna be fun let's do something
10:11 - fun here we'll see here right here on
10:12 - the homepage is this interactive
10:15 - demonstration this is a picture of a
10:17 - Robin which the mobile net model labeled
10:20 - this as a Robin American Robin turdus
10:23 - migratorius with the confidence of 98.7
10:27 - so I'm going to upload an image do I
10:30 - have any images here this is good hold
10:32 - on let's go get let's go get a rainbow
10:39 - image is this looks good
10:44 - download we should done a train let's
10:48 - try a rainbow dot jpg where am I going
10:55 - desktop that looks good
10:57 - let's go for a train this one looks good
11:03 - Oh save image as train let's go back to
11:10 - the ml5 webpage and what I'm gonna do I
11:13 - could drag and drop it but I'm just
11:14 - gonna do this let's looks let's try the
11:20 - train trailer truck tractor trailer
11:26 - trucking rig rig articulated lorry semi
11:30 - with a confidence of forty two point
11:32 - ninety four percent let's try doing the
11:34 - dragging and dropping thing with the
11:37 - rainbow bring it over here and now we
11:39 - have a parachute shoot with a confidence
11:42 - of forty nine point twenty nine percent
11:44 - so how does this work well I will show
11:47 - you if you scroll down here you will see
11:49 - here is the code for such a thing and so
11:54 - the ml5 library is a machine learning
11:57 - library built on top of tensorflow das
12:00 - this library would not at all be
12:02 - possible without tension flow chess
12:04 - running behind the scenes to try to
12:06 - create some simple code examples to work
12:09 - with at the moment mostly pre-trained
12:11 - models in the browser and there's lots
12:13 - more coming here and I'm gonna do a
12:14 - bunch of tutorials with this but this
12:16 - project is I'm mentioning it now because
12:18 - Hannah Davis at the IO Festival I'm
12:21 - totally not in the slack channel here at
12:28 - the IO festival made talked about ml5 in
12:31 - her presentation when hopefully the IO
12:33 - Festival which is just finished up in
12:35 - Minneapolis the videos that all the
12:37 - talks that are amazing go find their
12:39 - Vimeo channel and all the ones from last
12:41 - year you can watch them the new ones
12:42 - from this year be out soon and we're
12:45 - looking at trying to launch this more
12:47 - officially on June 15th and I will
12:49 - mention that if you're interested in
12:50 - kind of poking around if you go to all
12:52 - of our github repos first let me just go
12:54 - over here oh we can see here's all sorts
12:56 - of wonderful people who have been
12:58 - working on this project more than just
13:01 - these ten people but here are 10 people
13:02 - and what I want to show you here is
13:07 - under projects this is my first
13:10 - or a into using the project management
13:13 - tool that's part of not to okay so but
13:31 - this project management tools the first
13:33 - time I've used it if anyone wants to
13:35 - jump on in and get involved and get
13:36 - bored kind of have a little sprint here
13:38 - from now until June 15th there's a lot
13:39 - if you look at that website there's a
13:41 - lot of stuff that's missing there's a
13:42 - lot of stuff that's broken and so you
13:45 - know reach out to me on Twitter at
13:47 - Schiffman type in a comment somewhere on
13:49 - github if you want to get involved and
13:51 - help us kind of push forward some of
13:52 - this code stuff for the release and okay
13:55 - so that's what I wanted to mention there
14:02 - too looking in the chat looking in the
14:05 - chat looking in the chat okay so now
14:07 - coming back over here so this is what's
14:10 - coming this is what I hope to eventually
14:12 - have on this YouTube channel is a
14:14 - playlist which is machine learning for
14:17 - beginners in the browser and what's to
14:18 - call it exactly and I'm gonna be using
14:20 - ml 5 NP 5gs for that together by the way
14:23 - the 5 in ml 5 is in omage 2 P 5 and
14:27 - processing because the ml 5 library
14:32 - aspires to be friendly and accessible in
14:36 - the same ways that processing in P 5
14:38 - have been over the years and there we go
14:40 - my camera's still shut off now back to
14:44 - today it's 1 o'clock already so what now
14:49 - I have a pretty clear picture what I
14:53 - want to do with my image classification
14:55 - examples that I'm going to build I want
14:59 - to use the quick-draw data set so one of
15:04 - my goals with making my machine learning
15:06 - tutorials is to use non traditional data
15:09 - sets and but maybe non-traditional is
15:11 - wrong word but data sets that are
15:13 - outside of what you would typically find
15:16 - in machine learning and data science
15:19 - curriculum want them I want to use
15:21 - different ones that are kind of in
15:23 - creative space to get people thinking
15:25 - more creatively about what kinds of data
15:27 - they encounter in their life that they
15:29 - could maybe use I want to use data sets
15:32 - that are really simple and kind of like
15:34 - easy to understand and look at and it
15:36 - also want to use data sets that are
15:38 - representative of the world that we live
15:42 - in and all of the all of the cultures
15:44 - and people that we share this great
15:46 - earth with so you know things like I'm
15:49 - trying to avoid things like M mist which
15:51 - is the classic handwritten digits data
15:54 - set things like the iris data set which
15:56 - is wonderful I have loved flowers
15:57 - nothing could possibly be wrong with the
15:59 - flowers data set but but things that you
16:01 - wouldn't that that kind of made me feel
16:03 - a bit more approachable so I really
16:05 - asked this question a bunch of places a
16:08 - bunch of times don't get any responses
16:12 - because I don't maybe it's hard to find
16:13 - these kind of data sets I feel like the
16:15 - Google quick-draw dataset is a great one
16:17 - for learning about image classification
16:19 - and then what I'm thinking of doing an X
16:23 - or is like it's not really a dataset
16:25 - it's just made up oh wow something else
16:27 - someone add in here okay hold on hold on
16:28 - and then classification what I'm right
16:33 - now the only thing I'm so tensorflow dot
16:36 - yes there's a node version tensorflow -
16:37 - yes that used this MLB Major League
16:41 - Baseball dataset to classify pitches and
16:43 - I love that football kind of a little
16:45 - bit of a baseball nerd you know any word
16:49 - so so I so that's kind of interests me
16:52 - but I don't know that baseball is
16:54 - perfect for what I want to do that's
16:56 - going to be you know a lot of people
17:03 - don't know about baseball and ranged in
17:04 - baseball and it's maybe not reaching the
17:05 - the more general audience that I'm
17:07 - imagining for this channel so but
17:09 - something like that that's really simple
17:11 - so all I could think of right now
17:12 - because I from gibreel go check out a
17:15 - CFD science that's the assess of science
17:19 - I can't ever say his channel name it's
17:21 - probably like there's like a really easy
17:23 - way to say that channel name and I just
17:24 - can't do it I don't know why I'm almost
17:26 - falling over for no reason I was talking
17:29 - about something datasets gibreel had
17:34 - this demonstration of a color predictor
17:35 - and what am I
17:37 - it's actually this semester created a
17:39 - variation on that which was kind of
17:41 - predicting more about a color than just
17:43 - a or B and so I was thinking of kind of
17:47 - using that as an inspiration and so like
17:51 - what if what if I made a color
17:54 - classifier that classified colors into
17:57 - like bluish grayish or like like the C I
18:01 - don't know some kind of set of arbitrary
18:02 - labels like five to ten labels that and
18:06 - what I need to so maybe I would
18:07 - crowdsource that Dave said I'm not sure
18:08 - yet so that's what I'm thinking about
18:09 - for classification I kind of hope to do
18:12 - that today but I talk too much and
18:14 - there's a little bit of time but that's
18:16 - that's coming next week another thing I
18:18 - forgot in here I wanted to make like
18:21 - make your own TF playground so just
18:29 - briefly one last thing that I'll mention
18:31 - here on this to-do list if you go to I
18:33 - believe it's playground tensorflow dot J
18:38 - s what's that well hold on J or whatever
18:43 - tensorflow playgrounds this is a project
18:48 - from the big picture group the research
18:50 - group from Google that created that
18:52 - where 10th floor address itself came out
18:54 - of and you can kind of create this
18:56 - little playground in the browser where
18:58 - you can configure a neural network you
18:59 - can have this kind of 2d data set you
19:02 - can actually there's like a play button
19:03 - so you can run it and you can watch it
19:05 - try to either classify and there we go
19:08 - sort of classify or I don't know it's
19:10 - doing classification regression it looks
19:11 - like classification to me they're sort
19:12 - of blue and orange so I have no interest
19:15 - in building out something that has this
19:18 - level of sophistication and design
19:20 - visual design but I would like to show
19:23 - you well could you similarly to how I
19:26 - made the linear regression the
19:28 - polynomial regression examples maybe
19:30 - I'll just do like a basic 2d
19:31 - classification problem with drawing
19:35 - stuff okay so that's that's my
19:40 - introductory talk I love this go you
19:47 - little neurons
19:48 - that data whoo all right oh oh I can
19:58 - break it I'm very keen for a fix
20:00 - all right so that's where I am so I
20:02 - think when all is said and done I think
20:05 - right now I'm just going to tackle today
20:08 - from now until about 2:30 which is an
20:11 - hour and a half X or and I really am
20:14 - torn like I don't want to do it because
20:16 - is it it's sort of but it's good for me
20:19 - so I'm gonna do all right but before I
20:23 - do that let me see if I can get some
20:25 - questions and get myself organized
20:30 - anybody have any questions about ml5
20:33 - tensorflow J s life the universe how to
20:44 - play the ukulele all right so what do I
20:48 - need to do here it's a couple things
20:52 - that I need number one is if anyone who
21:10 - is a sponsor and patron and there's an
21:16 - interesting question in the YouTube chat
21:18 - that's the might that I might like to
21:20 - answer you can paste it into the sock
21:24 - channel
21:27 - this is not right
21:30 - no no oh
21:36 - [Music]
21:38 - totally
21:39 - Papa Delhi doobie doobie wahh
21:42 - there we go alright there's two bits of
22:01 - code that I need to get started with
22:03 - this one is the actual previous XOR
22:12 - [Music]
22:17 - there we go coding challenge 92 and then
22:21 - I also want to get under maybe it's
22:31 - under courses intelligence and learning
22:33 - session wait
22:36 - [Music]
22:39 - no how come it's not there is this a
22:42 - different go past in a different place I
22:46 - [Music]
22:49 - have this in too many places courses
22:53 - intelligence and learning session 6 the
22:57 - layers API I need that and then p5
23:02 - tensorflow let's put these in there and
23:08 - I don't leave these anymore
23:09 - and let's go over here I don't know
23:20 - which would be better to start from
23:22 - let's start from this
23:25 - [Music]
23:29 - this'll be coding challenge what what
23:32 - coding challenge number isn't this the
23:35 - third time we do X or is it really oh
23:41 - you're really torching me because I
23:43 - really feel like I have a thing I have a
23:47 - little bit of a thing like it's kind of
23:49 - a little like you know like I can't if
23:51 - I'm listening to podcasts like I I have
23:54 - to listen to every all of it every
23:56 - minute I can't like not listen to one
23:58 - episode and I have to like so somehow in
24:01 - my stuff like guys like I just have to
24:03 - do I have to do the X or now with ten to
24:04 - flow Jay s because I have to but maybe I
24:07 - should skip it what coding challenge
24:12 - number am I on did somebody tell me no
24:16 - but I can find that out by going here
24:24 - and oops
24:27 - 105 was polynomial regression so this
24:33 - would be 106 okay I totally didn't do
24:38 - this right yeah how come there we go
24:42 - now do I have the atom editor open
25:04 - see here's the thing what's interesting
25:06 - about doing this YouTube channel is if I
25:08 - were teaching a course like I do
25:11 - supposedly he's at NYU I just would not
25:18 - I would just skip a lot of stuff because
25:20 - there's like limited amounts of time and
25:22 - I do to some extent to do that here but
25:23 - I I have this like false it's like this
25:26 - false sense of infinite time and I must
25:28 - do every single step which I need to
25:30 - move away from
25:41 - Cain wheat bun writes it's a good
25:44 - example comma but dot dot and I get it I
25:47 - get it but I'm waiting for what's coming
25:50 - next
25:50 - yeah I think I could I could fill it in
25:52 - but but but I think it's weird that's
25:59 - what I'm doing today
26:00 - unless I'm into that
26:06 - no no okay that's what I do all right
26:10 - all right all right so let's see here we
26:18 - are this is my list huh thank you thank
26:36 - you XOR Shipman see I think I only
26:46 - actually made one even though it's like
26:47 - probably the third or fourth time I'm
26:48 - doing it on this channel I didn't I
26:50 - don't only have one video it appears
26:55 - yeah look here's here's Simon talking
27:00 - about X or some other videos okay all
27:06 - right and now Joe server yes let me open
27:14 - up the browser I would love to do like
27:16 - some kind of little just fun algorithmic
27:18 - thing the equivalent of like phyllotaxis
27:21 - today before I go if there's time but I
27:23 - doubt there is okay
27:31 - all right you had got CGI rights I
27:35 - missed the non machine learning coding
27:37 - challenges totally agree yeah I don't
27:43 - know burger Bob asked could you please
27:46 - read the chat more often I totally get
27:51 - the sentiment I appreciate the question
27:52 - I will sir I can certainly try it is
27:55 - very hard to follow the chat and do the
27:59 - livestream at the same time and maybe
28:03 - someday I will have a better system for
28:05 - doing that I have some ideas for how to
28:07 - do that but it just I need time to get
28:09 - some more screens and maybe have some
28:11 - help with that and the sort of thing all
28:13 - right ADA she writes did he say filing
28:24 - taxes yes and now coding challenge
28:27 - number 327 filing your taxes let's go
28:32 - see IRS tax filing API ooh
28:41 - IRS gov a file providers software
28:44 - developer
28:49 - [Music]
28:55 - [Music]
29:12 - okay I guess I guess the XOR isn't so
29:14 - bad
29:16 - phyllotaxis the spiral beautiful
29:19 - Fibonacci spiral pattern of a sunflower
29:21 - that's what I was saying
29:24 - alright see what happens
29:31 - burgerbob that's exactly burgerbob Oh
29:33 - what about a giant screen which shows
29:35 - the chat behind the camera that is
29:37 - exactly what I would like and I will
29:40 - snap my fingers and giant screen will be
29:42 - mounted there behind the camera somehow
29:45 - that didn't happen I'm not sure why so I
29:48 - will I I do like that suggestion looks
29:55 - whoops yes Chris writes might be worth
29:58 - having a mod pull interesting questions
30:00 - out of both chats and give them to Dan
30:03 - at Q&A breaks I'm absolutely game for
30:05 - trying that and wants to sort of
30:07 - volunteer at this point I think I would
30:08 - need a volunteer to help facilitate that
30:11 - and that would be great
30:12 - all right let's let's let's let me get
30:22 - over all of my anxiety and hang-ups
30:23 - about you doing X or again and talk
30:26 - about them when I start and then and
30:34 - then and then and then I will begin I
30:43 - see all these people typing and Ada and
30:46 - Kate week Mon and Eric but I gotta move
30:49 - on I think I got to start because time
30:51 - time is a-wastin
30:59 - yes thank you okay Eric in the chat
31:02 - rightz sometimes it's important when
31:04 - learning something new to base your
31:06 - exploration around an example which is
31:08 - fairly trivial and you understand
31:10 - intimately well true or were better I
31:13 - could not have put it better myself
31:15 - thank you I'm gonna read that sentence
31:18 - at the beginning of this coding
31:20 - challenge if you don't mind
31:23 - hello welcome to a coding challenge yeah
31:26 - I know what you're thinking I mean I
31:28 - don't know what you're thinking I know
31:29 - what I'm thinking that looks like coding
31:31 - challenge number 92 XOR which is
31:35 - probably one of the less interesting
31:37 - creative like sort of just technical
31:40 - coding challenge demonstrations that
31:42 - you've done why why why are you doing it
31:46 - again
31:46 - well Eric from the coding train
31:49 - community writes
31:50 - thanks explanation because I was just
31:52 - before I started this having a real
31:53 - hang-up about this sometimes it's
31:55 - important when learning something new to
31:57 - base your exploration around an example
31:59 - which is fairly trivial and you
32:01 - understand intimately well so here's the
32:03 - thing
32:04 - I I'm learning something new I'll come
32:06 - back here and the thing that I am
32:08 - learning something new is this tension
32:10 - flow a thing and wouldn't it be fun to
32:12 - make like play pac-man with it or the
32:15 - emoji scavenger hunt project or
32:17 - teachable machine or play a piano with
32:19 - it all these things all pose that oh my
32:20 - god we got it we're gonna get this I
32:21 - could just sort of go there right now I
32:23 - will get there eventually but I'm trying
32:26 - to learn the basics of how the library
32:27 - works and I'm trying to step through
32:30 - this slowly so I will say that we're if
32:33 - you're watching this video right now
32:34 - where you are is not necessarily in the
32:35 - most beginner or friendly place because
32:38 - I'm working with tensorflow tas natively
32:40 - to implement basically like a weird math
32:43 - problem it's not that weird of a problem
32:44 - actually but a very basic trivial math
32:46 - problem just to see how tensorflow dachi
32:48 - has works that's what I'm trying to do
32:50 - with this coding challenge and about 20
32:52 - or 30 minutes he'll be coding this
32:54 - coding challenges just look it's right
32:55 - like four hours and 72 minutes long
32:57 - which is why i say 72 minutes cuz that's
32:58 - five hours and twelve minutes I don't
32:59 - know but the trajectory that I'm on is
33:03 - I'm gonna start doing some stuff inching
33:05 - my way towards hell let's actually use
33:06 - some data let's use some more data and
33:08 - maybe some images and so I've got a
33:10 - bunch of things that I'm stepping
33:11 - and I'm trying to get to the point where
33:13 - I'm going to use this other machine
33:14 - learning library called ml5 which at the
33:16 - time of this recording hasn't really
33:18 - officially been released yet but builds
33:20 - on top of tensorflow das thank you
33:22 - everyone who votes wherever you are to
33:26 - try to create some more accessible
33:27 - interfaces to some of the algorithms and
33:29 - models that you things that you can do
33:31 - with tensorflow digest without having to
33:33 - do the lower-level memory management and
33:35 - math operations stuff so all that is
33:38 - coming and I just took a lot of time in
33:40 - this coding job to say that to you so
33:43 - but as much as I kind of don't I haven't
33:47 - I'm not so sure but well but it's a why
33:49 - is XOR so here's the thing this is why I
33:51 - want I need an example this is the first
33:52 - time I'm going to ever in any of my
33:55 - videos except for the other one that I
33:57 - made but this is the first time that I'm
33:59 - actually going to use the TF layers API
34:04 - to Train Oh to train a model with a data
34:09 - set to produce a certain output okay I
34:12 - did two tutorials about what the TF
34:14 - layers API is you could pause down and
34:16 - go and watch those and then come back
34:18 - here but in those videos I didn't
34:20 - actually do anything with TF layers just
34:22 - sort of talk through and typed out some
34:23 - code so the problem that I want to solve
34:25 - and apologies for explaining this
34:27 - probably for like the fifteenth time on
34:29 - this YouTube channel is very well known
34:32 - from machine learning X or because when
34:35 - the original perceptron was invented the
34:38 - single perceptron the model of an
34:40 - individual neuron that could receive
34:42 - inputs and generate an output it could
34:46 - not solve X or it just couldn't it's not
34:50 - a linearly separable problem and I've
34:52 - talked about that in other videos about
34:54 - why we need multi-layer perceptrons so
34:56 - the nice thing about XOR is I can
34:58 - diagram for you hold on a second
35:07 - look back I can diagram for you the the
35:13 - architecture of the model that we need
35:14 - to create there are two inputs there is
35:19 - one output so that the inputs to the XOR
35:24 - problem are true and false values so
35:27 - unlike a so if I made a little truth
35:30 - table and or XOR right I can have true
35:36 - and true true true false false true
35:40 - false false and and operation would only
35:43 - ever give me true when both are true
35:45 - false false false and or operation would
35:50 - only ever give me would gives me true if
35:53 - just one of them is true true false can
36:02 - you even see that I can't see on my
36:04 - monitor but hopefully you can now XOR
36:06 - the X for exclusive gives me true only
36:11 - if one is true they can't both be true
36:13 - only one so in that case I get false
36:16 - true true false and the idea if linearly
36:21 - separable comes up here because I can
36:24 - draw a line here to separate true from
36:26 - false I can draw a line here to separate
36:28 - true from false but here I could do this
36:34 - but I can't draw a single line to
36:36 - separate true from false we need a more
36:39 - sophisticated model with a hidden layer
36:41 - so the inputs are things like a 1 and a
36:44 - 0 feed forward into the hidden layer
36:51 - activate feed to the output and the
36:55 - output should be a 0 or a 1 it's really
36:57 - in some ways a classification problem
36:59 - but I'm gonna do this as a regression
37:01 - essentially where I'm just gonna get
37:03 - some number between 0 & 1 if you watch
37:05 - the previous coding challenge the reason
37:07 - why that is is because thank you very
37:11 - much good night this video is now
37:12 - I hid that my sound effect by accident
37:16 - because what I'm trying to do is
37:18 - visualize the true-false space alright
37:24 - pause for a second
37:33 - just taking a pause for a second
37:49 - how long's 10 minutes at least right but
37:53 - I you know it's important it's important
37:55 - for me to talk about what I'm doing all
38:00 - right
38:04 - so I'm just thinking here where am I
38:08 - going next looking at the chat no one's
38:10 - complaining too terribly and I think I'm
38:13 - going to move on I guess I could
38:21 - transition back over here case I'm not -
38:24 - you want to edit out the weird sound
38:25 - effect thing
38:35 - oh they're good sounding okay that's
38:37 - good
38:38 - Hugo asks will you ever do non
38:41 - JavaScript videos well I do I do some
38:44 - processing and Java videos those aren't
38:46 - JavaScript but you know at this point
38:50 - I'm kind of I'm kind of I'm kind of
38:55 - doing the JavaScript thing okay let me
39:02 - let me transition back and that to me
39:04 - may be the sound effect little thing was
39:06 - like a funny little bit but I think I'm
39:08 - gonna just transition back because
39:12 - ultimately what I'm going to do is
39:14 - visualize the output of the model and
39:18 - I'm gonna send in numbers all the way
39:21 - between zero and one I know I don't even
39:24 - know what I'm saying it's fine because
39:32 - ultimately I'm gonna visualize the
39:33 - output as grayscale values and I want to
39:35 - see number I want to see grayscale
39:36 - values all the way between zero and once
39:38 - the same thing I did in the previous
39:39 - coding challenge if you if you're
39:41 - happened to have watched that one
39:42 - alright so now I actually I'm gonna also
39:44 - do something where I start from the code
39:48 - from the previous coding challenge and
39:50 - so we can see there's this idea of
39:52 - training data the inputs to the X or
39:55 - problem are zero zero gives me a 0 0 1
39:59 - gives me a 1 1 0 gives me a 1 at 1 1
40:01 - gives me a 0 this is the training data
40:03 - and in my previous version of this I
40:07 - used my own neural network library so in
40:09 - theory I'm gonna get rid of the idea of
40:10 - the learning rate slider just before we
40:13 - can add that back in later but let me
40:16 - get rid of the learning rate slider
40:19 - basically I want to do exactly the same
40:22 - thing the difference is I'm going to say
40:25 - neural network equals
40:26 - TF layers sequential and maybe I'll call
40:32 - this the model instead of neural network
40:36 - so the I do this is a neural net here so
40:38 - the idea here is that I want to replace
40:44 - my neural network library with
40:48 - tensorflow yes and so this for me what
40:51 - what the usefulness of this video is a
40:53 - me learn I spent all this time trying to
40:55 - build my own rather sort of terrible
40:57 - neural network javascript library and
40:59 - going through that was sort of helpful
41:01 - in thinking about how the stuff works
41:02 - now if I can translate that into
41:05 - attention flow dot yes I'm gonna things
41:07 - are gonna hopefully start to sell and
41:08 - make more sense into my brain Bruno is
41:17 - asking something in the chat about the
41:18 - true/false table yeah usually you draw
41:20 - it as a I might come back to this later
41:23 - usually you draw it as a matrix and I
41:28 - sort of did something weird there but I
41:30 - think it's fine sorry I'm looking see
41:38 - this is what happens wait look at the
41:38 - chat too much all right okay
41:57 - alright okay so now we need to what this
42:05 - constructor here said and let's just put
42:06 - this back to this this constructor here
42:08 - said would make a neural network with
42:12 - two inputs two hidden nodes and one
42:14 - output so I need to duplicate that idea
42:17 - here with PF layers so let's go to the
42:20 - 10th floor Jas API reference and we're
42:24 - gonna go all scroll down to TF layers
42:28 - and what I want to make is a dense layer
42:32 - TF layers dint a dense layer is a fully
42:36 - connected layer so what I'm going to do
42:38 - is I am going to say let hidden equal TF
42:45 - layers dense and then I can put inside
42:51 - there an object that has the parameters
42:54 - of how I want to configure that layer
42:56 - and so how do I want to configure it the
42:58 - two things that I want need really need
43:00 - to do is this is the hidden layer right
43:02 - I need to give it an input shape right
43:06 - he just say what's coming in what's
43:08 - coming in that's what this is here I
43:10 - need to say how many nodes it has that's
43:12 - the number of units and then I probably
43:15 - has a default one but I can specify an
43:17 - activation function and again I'm just
43:19 - going to use sigmoid as this historical
43:21 - activation function that I've been using
43:23 - in all my videos to date I'm gonna soon
43:26 - talk about softmax what that is as well
43:28 - as some other activation functions like
43:30 - lazy which is maybe more commonly used
43:34 - okay but like nobody pronounces that way
43:37 - foot B so don't get confused
43:39 - alright so I want to say input shape I
43:43 - believe is just there's just two inputs
43:46 - I also want to have two units two nodes
43:52 - and activation is going to be sigmoid so
43:57 - now I have created the hidden layer yay
44:01 - the other layer that I need to create is
44:04 - the output layer and so what am I know
44:07 - the app and layer I don't need to
44:08 - provide an input shape because the input
44:11 - shape can be inferred if I add them
44:13 - sequentially the inputs are not a layer
44:16 - so for this first layer the hidden layer
44:17 - I've got to say how many there are but
44:19 - now once I'm creating this next layer it
44:21 - can just the input shape is gonna be
44:23 - defined by what was before it so now I'm
44:26 - going to say really have to stop it at
44:29 - the sound effects by excellent and now
44:32 - I'm going to say let output equal TF
44:35 - layers dense and all I need to say is
44:41 - units one activation sigmoid okay then
44:47 - ought to do is say Model Model dot add
44:50 - hidden model dot add output okay so this
44:55 - is the model now one thing I need to do
44:59 - is I definitely need to import the
45:03 - tensor flow J's library which I happen
45:05 - to have from one of my previous examples
45:07 - so I'm going right now I only have I
45:09 - have the p5 libraries in my index.html
45:11 - plus my crazy neural network thing and
45:14 - my actual code and sketch J yes someday
45:18 - maybe I'll use the fancy new import
45:19 - syntax stuff let me just just have
45:21 - everything kind of line up let me add
45:23 - this in here so now TFS should be there
45:25 - I should be able to go back and run this
45:27 - and not see any errors aha
45:32 - TF dot layers dot sequential is not a
45:37 - function so I'm seeing things in the
45:41 - chat chats really off the rails with
45:44 - this XOR thing
45:48 - [Music]
45:52 - all right so I probably just so I
45:56 - probably just didn't even see half cut
45:58 - layers dot sequential the right thing
46:00 - you know I could go look I by the way
46:02 - made an example oh it's just TF dot
46:05 - sequential okay so all I all I want to
46:07 - say is I just got that wrong it's TF dot
46:10 - sequential so you know I could go look
46:13 - you know hopefully I would find this
46:15 - here at EF dot sequential yeah models
46:18 - creation there it is TF not sequential
46:20 - so I just had that wrong
46:21 - okay let's try refreshing this yet again
46:26 - slider is not defined hold on sorry much
46:31 - ya have to turn the notifications off on
46:35 - my watch I'm getting like phone calls
46:36 - and buzzing things okay
46:39 - take a minute here somebody said that I
46:41 - look I'm good at drinking drinks in
46:44 - profile that I could be like a coating
46:46 - train brought to you I'm buzz marketing
46:48 - Klean Kanteen that's not an official
46:52 - sponsor where was I
47:08 - right alright let me fix this learning
47:14 - rate issue 0.1 I just want the thing to
47:17 - run okay so it's going it's still
47:20 - working with the my neural network
47:25 - library not the new ten so Jeff's one
47:28 - but let's keep stepping through so ah so
47:32 - what am I missing here
47:33 - so when I make a model this is now I've
47:36 - architected the model
47:38 - I've architected this particular
47:40 - architecture but I need to do another
47:42 - step I need to compile the model and I
47:46 - need to define the loss function and the
47:50 - optimizer basically I need to say like
47:54 - okay well this is how I'm going to
47:56 - determine how well the model is
47:59 - currently performing with the training
48:01 - data and testing data potentially but
48:04 - I'm not getting testing data will come
48:06 - in my next video about classification
48:08 - but here I'm not making a distinction
48:10 - between training and testing date I'm
48:11 - conflating those two concepts which is a
48:13 - big mistake and a problem but we're
48:15 - stepping through this stuff later by
48:16 - little by little like a butterfly
48:18 - flapping its wings it's not at all a
48:21 - butterfly but I felt like I was being
48:23 - like a butterfly and then an optimizer
48:25 - is what sort of function what sort of
48:30 - algorithm am I using to adjust all of
48:33 - the weights of all these connections
48:35 - according to the loss function itself so
48:38 - I need to define those things so let me
48:45 - try to type it out how I think it is
48:47 - and then we'll go check so I know I need
48:51 - to create an optimizer GF optimizer like
48:58 - this and with a learning rate something
49:01 - like this like I'm gonna I want to have
49:04 - used to cast a crate to set with some
49:06 - learning rate that's not correct
49:07 - this is me like trying to remember what
49:10 - what the code is and then I need to say
49:13 - like model dot compile and then I think
49:16 - when I compile it I'll say things like
49:19 - this I'm going to compile it with
49:21 - optimizer and this loss function like
49:25 - like root mean squared or something like
49:29 - that
49:29 - so this is what I'm remembering from
49:31 - when I looked at this at one time and I
49:33 - probably got this wrong so let's
49:34 - actually go look at the API Docs
49:36 - well first what's the chance that any of
49:38 - this actually makes sense okay TF
49:40 - optimizers not a function so let's see
49:41 - how do we create the optimizer optimizer
49:48 - yes so it's this is what I want I want a
49:52 - TF train SGD this is how I create the
49:55 - other optimizer is not a keyword in the
49:58 - API just I imagine that for myself so I
50:01 - need to say TF train SGD and then give
50:04 - it a learning rate so TF train SGD and
50:08 - there are other kinds of optimizers that
50:10 - will will that I think I've even shown
50:12 - you and what we'll use more and give it
50:13 - a learning rate like 0.1 then I want to
50:18 - look at model dot compile so look for
50:24 - compile well we can see in some examples
50:27 - here what I'm looking for is where the
50:29 - actual compile there it is compile
50:32 - so the compile function compiles it and
50:35 - give an optimizer a loss and I can also
50:37 - do some metric stuff I'm not going to
50:39 - worry about the metrics too much
50:40 - although maybe I'll try to come back
50:41 - towards the end of this video okay
50:43 - model dot compile optimizer loss I think
50:45 - this might actually be fine is it root
50:47 - mean squared so let's look for the loss
50:49 - functions loss root means mean squared
50:59 - Oh
51:00 - Weiss ago I keep saying root because I
51:14 - have it in my head from some thing that
51:16 - I did a very long time ago where I was
51:19 - always taking the square root of the
51:21 - mean squared error so I always say root
51:23 - mean squared there's no root here
51:25 - involved I guess I have to get back up
51:30 - and continue this tutorial
51:32 - how long was I saying root for and hell
51:35 - annoying will that be for the people who
51:36 - watch this later okay well don't you
51:39 - know I get up slowly others I get kind
51:41 - of lightheaded alright apologies I've
51:45 - been saying root mean squared error for
51:48 - because I'm stuck in this world where
51:50 - you have to take the square root which
51:51 - you don't need to do here so just mean
51:53 - squared error that's all I need this is
51:55 - my loss function mean squared error now
52:01 - let us now go back here hit refresh all
52:07 - right things are happening things are
52:09 - going so the model is built the model is
52:13 - compiled and the next thing that I am
52:16 - ready to do is now actually start
52:18 - putting data in the model time out for a
52:23 - second why did I lose all right just a
52:36 - second here I really have to watch the
52:42 - time I've got an hour I have to be a
52:45 - little league practice I am NOT a
52:49 - strange forty four-year-old person who
52:51 - plays in the league but my son is
52:53 - playing Little League for the first time
52:54 - this year to be at the practice cannot
52:57 - be late so I have another hour though
53:00 - okay hmm I'm not the coach don't worry
53:04 - I'm just stand by the side a cheer do my
53:09 - little debt hold my little signs that's
53:11 - it I don't have any sign should have
53:13 - signs I was actually thinking of
53:14 - sponsoring a little league team like the
53:15 - coding train sponsored little league
53:17 - team I think get it get together this
53:18 - year maybe next year
53:19 - alright
53:27 - all right
53:35 - there too - these are two next steps
53:38 - whoops oh why is this completely died I
53:53 - just I know I'm saying 44 a lot because
53:56 - about to be 45 so I feel like it'll say
53:59 - 44 I like number 44 much better than
54:01 - number 45 for a variety of reasons that
54:03 - it will not get into can figure out what
54:06 - I'm talking about
54:07 - all right um it's pretty obvious
54:09 - probably all right so the two things
54:13 - that we need to do now what are the two
54:15 - main steps I don't know why I came over
54:16 - here but since I'm over here first of
54:20 - all I drew this truth table thing a
54:21 - little bit weirdly and so you might
54:24 - recall just to be clear about what's
54:25 - going on this is my little drawing of
54:27 - the canvas right now and the idea of the
54:29 - canvas is that I want to see what the
54:32 - neural network thinks false false is at
54:35 - 0-0
54:36 - I want to see what it thinks true false
54:39 - is at this right hands top right hand
54:41 - side the bottom left hand side I wanted
54:44 - to see it 0 1 and then I want to see
54:46 - here 1 1 so false is black 4 0 and true
54:52 - is white for one that's the way I'm
54:54 - gonna map the color so I should see some
54:57 - kind of bands of like I should be
55:00 - getting like something like this
55:01 - so darker here and like this so let's go
55:04 - look does that match yeah that's exactly
55:07 - what I'm seeing here so the reason why I
55:10 - came over here is what I need what I
55:13 - think there's two things that I need to
55:14 - do number one is I need to train the
55:16 - model to produce this output my desired
55:19 - output that I think it should do and
55:20 - then I also need to ask the model to
55:23 - predict so I can draw what it thinks its
55:27 - output is so the two and the and so the
55:30 - two steps here I don't run out of space
55:32 - but in the attention photo chess library
55:35 - I wanted you I need to look at the
55:37 - predict function and the fit function
55:40 - predict for just saying here's the
55:43 - inputs what is your output the fit
55:45 - function for saying here's labeled
55:47 - inputs inputs with no
55:49 - outputs adjust optimize yourself
55:51 - according to that so I'm gonna do things
55:53 - backwards I'm gonna do just the predict
55:55 - step first I just want to see when you
55:58 - starts up with no training
55:59 - what visual output - again so coming
56:05 - back to the code let's look here so this
56:09 - this is what I need to replace I need to
56:12 - say now I need to say let whoops y equal
56:21 - model dot predict
56:34 - model dot predict now let's go look at
56:37 - the documentation right I need to send
56:40 - in the inputs so let's go back to the
56:44 - documentation model dot predict I get a
56:50 - better way of browsing this
56:51 - documentation here it is so I need to
56:56 - sorry
56:57 - model dot predict I need to give it the
56:59 - X's what are the X's this are the X's
57:04 - but remember I'm using tensorflow de s
57:08 - now tensorflow - s oi oi oi vague volts
57:13 - I have to make them a tensor I can't use
57:15 - regular arrays so I could say let X is
57:18 - equal tensor 1d oh no it's CI sorry I
57:27 - got confused
57:28 - TF tensor one D inputs and then model
57:33 - X's now here's the thing so this is the
57:37 - ID there's many problems what I've done
57:39 - so far ok many problems which I will
57:42 - solve slowly this could be a very long
57:44 - video I apologize in advance you can
57:45 - take a break now pause take a break go
57:47 - do something else
57:47 - go back so what about what's the what's
57:51 - problem number one problem number one is
57:53 - predict happens asynchronously
57:56 - hoo-boy pause for a second here how did
58:02 - i do this and so i made an example i
58:06 - want i just need to think about this for
58:08 - a second wait my glasses are steaming up
58:11 - why is it getting all warm in here hold
58:13 - on oh and Eric thank you for that pull
58:16 - request
58:17 - I probably shouldn't that and then look
58:19 - at because he probably fixed a bunch of
58:20 - things I just want to see something can
58:22 - we do this as a batch
58:24 - oh no predict happens synchronously it's
58:28 - fit that's asynchronous oh good that's
58:30 - why I'm doing this right predict happens
58:36 - synchrony can happen synchronously let
58:40 - me look at should I should show you what
58:42 - I'm looking at I know why I'm looking on
58:44 - this other computer
58:48 - oops I'm just looking up some stuff for
58:51 - how this stuff works
58:56 - oh wait wait no hold on let me show you
59:12 - what I'm looking at because I don't know
59:14 - why I'm looking at this so I have a repo
59:21 - called 1:45 it's not called what for I
59:26 - this the time tensorflow jazz examples x
59:29 - or sketch right so I know I have to do
59:39 - as a batch neural network predict Oh or
59:44 - did I do it and so I was putting it in a
59:46 - class which I'm not going to do here
59:48 - tidy return oh I know predict happens
59:57 - synchrony synchronously but then pulling
60:00 - the data off happens asynchronously but
60:03 - I can use data sync even though I
60:05 - probably should be using TF next frame I
60:08 - don't know how to use that yet so I will
60:14 - deal with that later okay sorry okay
60:29 - okay oh here I am back all right sorry
60:34 - we can just back up a bit
60:38 - not you you can splice things however
60:41 - you so feel so inclined whoops
61:04 - so I need to now I need to ask the
61:07 - tensorflow layer sequential model thingy
61:10 - to give me the Y neuro a model dot
61:14 - predict but what does it expect its
61:18 - predict function unlike my predict
61:20 - function cannot get a regular array it
61:22 - expects a tensor so I need to make the
61:26 - X's into TF tensor 1d with those inputs
61:32 - and pass those through predicts now
61:36 - here's the thing there there's a lot of
61:38 - issues with this that I need to resolve
61:42 - and this is gonna run really slow I need
61:43 - to actually do this as a batch process
61:44 - I'm gonna get to all that but just
61:46 - looking at what I've got so far
61:48 - model dot predict there's there's a
61:51 - question of like is this happen
61:52 - synchronously or asynchronously this
61:54 - actually is happening synchronously but
61:57 - the problem is I need to say fill with
62:01 - the result like I need to look get that
62:03 - number out and to get the number out I
62:07 - actually want to call dot data and that
62:09 - happens asynchronously so because I'm
62:12 - working with some teeny bits of data
62:14 - right now
62:14 - I think I'm gonna use data sync and
62:17 - there could be issues with that and as I
62:19 - move more forward we're gonna see when I
62:21 - really need to be more thoughtful about
62:22 - callbacks and promises but I'm gonna use
62:25 - data sync right now so I should be able
62:27 - to predict the output with this input
62:30 - get that data and then let me just say
62:33 - console.log why and I'm gonna make the
62:39 - resolution here of the oh yeah the
62:46 - resolution really big like 50 because I
62:48 - just want to like look at very very
62:49 - little data to start with and let's look
62:53 - I'm not gonna draw anything let's just
62:54 - look and see what's coming out what's
62:56 - coming out here why and then let me just
62:57 - say no loop so let's look in the console
63:00 - and see if we get anything error
63:04 - expected when checking dense dense one
63:07 - input to have two dimensions but it got
63:09 - array with shape too
63:11 - I have the same problem I've had every
63:13 - single time I've done this with 10:00 to
63:15 - 4:00 guess so
63:16 - the good news is I want I don't want to
63:22 - just give this one D tensor so even
63:26 - though my data is just two values 0 1 1
63:29 - 0 1 1 and it's a one-dimensional array
63:32 - with two numbers in it I actually want
63:34 - to be able to do something like hey take
63:36 - these 15 data points and give me the
63:40 - results the predictions for all 15 of
63:41 - those and so what I really want to be
63:43 - doing is I always need to send in kind
63:46 - of like one order higher one degree one
63:50 - rank higher so this actually I'm just
63:54 - sending in one data it piece of data
63:56 - endpoint in point input frame stopped
64:00 - work and this now I also have to say
64:07 - tensor 2d now because it's a 2d tensor
64:10 - there we go ah so we could see look at
64:13 - this the results came out for all those
64:15 - little spots you can see in little
64:16 - numbers between 0 and 1 in an array so
64:18 - now I can instead of console logging Y
64:22 - and I just want that it comes back into
64:26 - the Ray but there's only one number I
64:28 - care about I can put this back in here I
64:32 - can take out no loop and I can run it
64:36 - let me see look there is my current
64:39 - visualization of X or
64:45 - I'm not really done I've so much left to
64:47 - do in this video that is recording for
64:50 - the last three or four days alright one
64:54 - thing I want to do is I just want to say
64:56 - stroke 255 I just want to sort of see a
65:00 - little bit more okay that's actually
65:01 - what I'm looking at here I actually want
65:03 - to make the resolution for debugging
65:05 - debugging wise on I also want to make
65:08 - the resolution a little bit bigger so
65:11 - let's see now one thing I'm curious
65:12 - about
65:12 - let's look at the frame rate here oh
65:15 - that's lighting at 30 frames per second
65:17 - so that's fine let me now actually make
65:20 - the resolution much much higher like
65:24 - this oh my goodness oh it's not even
65:29 - getting to the first frame oh well there
65:32 - we go look at the frame rail can't even
65:35 - give me a frame rate it's so stuck you
65:37 - can't even get one frame per second so
65:39 - here's the thing I have done something
65:41 - very very very bad and I needed to stop
65:46 - it no loop
65:48 - stop you don't have to do any more work
65:50 - and let's put the resolution back at 100
65:53 - and let's think about this what's going
65:55 - on here look at this look at this
65:57 - predict function and look at this data
65:58 - sync function what am i doing I am
66:01 - calling that function multiple times
66:04 - every single for every single spot on
66:07 - that grid when I'm working with
66:10 - something like tensorflow J s whenever I
66:12 - create a tensor or feed data into a
66:15 - model the data has to go from my code
66:19 - onto the GPU and then when it's done
66:22 - that data sync is pulling it off of the
66:24 - GPU so I can use it again in my code
66:26 - that graphics processing unit where all
66:28 - the math is happening behind the scenes
66:30 - I want to do that as few times as
66:33 - possible
66:34 - look how this is I'm creating this
66:37 - two-dimensional array with one thing in
66:39 - it you know ten hundred times I could
66:42 - just create one array with a hundred
66:45 - things in it and call predict once
66:47 - that's what I want to do so I what I
66:50 - need is for this nested loop to happen
66:52 - twice once to actually wants to setup
66:56 - the data and another to draw all the
66:58 - results
66:59 - so I'm gonna copy paste this just put it
67:01 - right below so this now what we need to
67:04 - do is create the input data so I'm gonna
67:13 - say let inputs be a blank array then I'm
67:19 - going to say inputs dot push and I'm
67:27 - going to just push in x1 x2 so I'm going
67:31 - to put every single x1 x2 all the way
67:35 - along I don't want to create the tensor
67:38 - or do this here I don't want to do the
67:40 - drawing stuff here I just want to create
67:42 - I just want to have a loop that creates
67:43 - all the data now I can get the X's is
67:48 - all of those inputs into a 2d tensor and
67:51 - the Y's this is now the Y's is and now
67:57 - here's the thing I don't just let's so
67:59 - hold on I got a look at what that's
68:00 - gonna look like let's comment this out
68:02 - for a second let's look at the Y's and
68:08 - see what that looks like oh okay
68:14 - Sketchup 78 error OOP oh no let there
68:21 - just inputs push okay oh I want to say
68:25 - no loop let me leave that no loop in put
68:28 - it back
68:29 - I just won't look at it once so you can
68:32 - see what did I get I got a big array of
68:34 - 16 numbers I got all the results so now
68:39 - what I want to do is back here now I
68:44 - just need to do the drawing and I don't
68:48 - need to the input data I don't need the
68:49 - model all I need to do is draw and I
68:51 - need to say fill wise index what I plus
68:56 - J times the number of columns maybe
68:59 - right because this is a one dimensional
69:02 - array to describe all each spot in that
69:06 - grid I could do something like let me
69:08 - just do this let index equal zero I'm
69:11 - going to say fill based on this
69:13 - particular one and I don't need this
69:15 - even sorry and I just need to say then
69:19 - index plus plus right so what are the
69:22 - steps here create the data get the
69:26 - predictions draw the results okay there
69:38 - we go so now we can see this is working
69:41 - I mean it's not doing anything but now
69:42 - let's check this framerate question we
69:45 - don't need to console.log the Y's I'm
69:47 - going to get rid of the no loop let's
69:51 - let's refresh this let's look at the
69:54 - framerate 30 frames per second let's
69:58 - let's pump it up a little but pump you
70:02 - up a little and where is the resolution
70:07 - there let's make this 20 I don't want to
70:09 - go crazy and look at the framerate there
70:13 - we go 30 frames per second no problem
70:15 - because I'm only one time through draw
70:17 - trying to copy data on to the GPU and
70:21 - get it I'm only calling predict once and
70:23 - we can just to check we can go to 10 and
70:29 - we can look at the framerate yeah you
70:31 - could see it's like kind of running a
70:33 - little bit slow but this is because I'm
70:34 - not being too thoughtful about the
70:36 - asynchronous nature of this stuff I
70:37 - could do other things to optimize it but
70:39 - I'm just going to ignore that and leave
70:42 - it at let me make it 25 hey timeout for
70:47 - a sec this should definitely be multiple
70:51 - parts
70:52 - oh yeah create inputs at the start they
70:56 - are constant oh that's such a good point
70:58 - okay I'm gonna do that right now that's
71:00 - a very good point who said that in the
71:02 - chat probably lots of people have
71:07 - precalculate them set up a bunch of
71:09 - people have yeah okay
71:15 - oh this alright
71:22 - all right everybody's saying that all
71:23 - right the chat is giving me some even
71:32 - further optimization which is why am i
71:36 - bothering to do this in draw this is
71:38 - something that the these inputs is never
71:41 - change I could just do them once at the
71:43 - beginning because they're and and I can
71:45 - I can ask for ask them many times in
71:48 - draw so let's actually fix that so I'm
71:50 - actually gonna I'm gonna take this and
71:52 - say let I'm gonna make this globe these
71:54 - global variables I don't know if you
71:58 - guys can hear the music that's coming
71:59 - from the room next to me but it's there
72:04 - alright then oh but the width and height
72:08 - does not exist until after create canvas
72:14 - so let me do this
72:19 - and let me do this okay so now that's
72:24 - there now I should be able to take this
72:28 - the input data and put this right here
72:33 - in the beginning and then I'm going to
72:37 - make a variable called X's and X is and
72:44 - where did I do that here and then create
72:50 - those X's so I'm now doing this in setup
72:53 - and then in draw the only thing I need
72:57 - to do in draw is run the predict this is
72:59 - going to make things run a lot faster
73:00 - let's make sure it still works here we
73:08 - go
73:09 - okay so you notice we getting like a
73:11 - different color each time i refresh
73:12 - because the neural network model the
73:14 - sequential model is initializing
73:16 - everything randomly but now I get to
73:17 - train it now I think we're ready to
73:22 - train it so here is what I did when I
73:29 - had my previous my own JavaScript neural
73:31 - network library I called neural network
73:33 - trained data inputs data outputs sorry
73:42 - I'm reading the chat you guys couldn't
73:45 - hear the music well alright so if I only
73:52 - I could remember exactly what I wrote
73:55 - when I made that TF layers tutorial but
73:58 - I know that what I need to do here and
74:00 - is I need to do something like this
74:03 - model dot fit some X's and some wise
74:07 - that's the training that's the
74:09 - equivalent and the learning rate is
74:10 - irrelevant and I don't necessarily need
74:14 - to do it this is basically what I want
74:15 - to do every time through draw I want to
74:17 - try to fit the model with some training
74:19 - data so let's first make the training
74:21 - data this is not exactly right I need to
74:24 - figure out and I need to use a weight
74:25 - that need to think asynchronously but
74:26 - this is the idea so if I go back to the
74:30 - top here this is my training data now
74:32 - one thing I definitely need to change is
74:39 - I'm going to keep the X's and Y's
74:41 - separate in training so I'm going to do
74:45 - this is I'm just going to do this kind
74:48 - of manually because I what's the big
74:51 - deal
74:52 - so let me make the training set and then
74:58 - one one those are the the X's now let me
75:04 - look at the Y's and the Y's would be 0 1
75:10 - 1 0 then I need those to be tensors so I
75:20 - need to say Const
75:22 - trait TF exes ah so I've got to think of
75:32 - good naming for this I kind of want them
75:33 - to call actually you know what I'm just
75:35 - gonna call it do I have a global X yeah
75:37 - I have a global X's already hmm hmm hmm
75:40 - tray TF X's equals 10 sir 2 D tensor 2 D
75:48 - OTF tensor 2 D you know what I'm gonna
75:53 - do I don't need these I don't need two
75:56 - separate sets of variables I'm just
75:57 - gonna create it I'm gonna call this ah
75:59 - everything is so much more complicated
76:01 - than I make it so if we're simple then I
76:03 - make it I'm just gonna make these
76:05 - tensors directly by saying TF tensor to
76:10 - D and then I'll put the parentheses
76:13 - around this and there now I made it a
76:16 - tensor then F is a TF tensor to D and
76:20 - now I made this a tensor ok now I've got
76:24 - the training data and I'm gonna get rid
76:27 - of this this is the old way that I had
76:29 - the training data which is totally
76:30 - unnecessary
76:30 - so this the training X's and the
76:32 - training wise are you with me if you're
76:36 - still watching I don't know dude get up
76:39 - and do it some jumping jacks
76:43 - let's see now I need to do model F it
76:47 - now model dot fit happens asynchronously
76:51 - so let's put it in its own async
76:55 - function called train model now if you
77:02 - don't know what it means to write a
77:04 - function that is tagged with the keyword
77:07 - a think this is part of es8
77:10 - a very newish version of JavaScript and
77:12 - I made a bunch of videos about what that
77:14 - is that you can go back and watch but
77:16 - this is basically a way for me to now
77:18 - say wait model dot fit and then let's
77:24 - look at actually let's look at the fit
77:26 - function model dot evaluate compile
77:31 - predict fit so what I need is
77:35 - is to give it the X's and the Y's
77:38 - there's batch size I'm not going to
77:40 - worry about there's epochs I'm not going
77:42 - to worry about or epochs and so H will
77:47 - give me back the history so let's just
77:49 - see here I'm now gonna say train model
77:53 - dot then H console dot log H dot loss
78:01 - index 0 let's say no loop again so
78:06 - basically what I'm doing here is I want
78:11 - to call this function train model and
78:13 - I'm using this idea of promises it's
78:15 - going to await the model on I need to
78:18 - return do I say a weight return or
78:22 - return a wait no I must say return oh
78:25 - wait return oh wait model dot fit so I'm
78:28 - going to return a promise which will
78:31 - have the result of the fit function and
78:33 - I don't know if this is right I want to
78:35 - just look I want to do that I want to
78:37 - call to train model every time and draw
78:39 - I might need to do this somewhere else
78:40 - just right now and then see what the
78:42 - loss is ok onyx 73 async function async
78:54 - function I've got a save function
78:55 - function it's an async function not an
78:58 - async there we go
79:00 - wise is not defined where a train model
79:04 - oh right this is I forgot to call it
79:07 - train X's and train wise so my training
79:10 - data train X's and train wise isn't that
79:14 - nice other word train just appears over
79:16 - out when you're doing machine learning
79:19 - you drink your glass of milk or whatever
79:21 - it is you're having while you're
79:22 - watching this coding training stuff
79:26 - cannot read property 0 of undefined a
79:29 - train model than H all right let's look
79:34 - just console.log H note that is what
79:37 - I've done okay
79:41 - history loss 0 okay oh no by the way
79:46 - didn't give it any testing data so
79:48 - whatsit computing the loss from history
79:52 - so this is I'm gonna call this result
79:55 - result history dot loss index 0 all
80:01 - right there we go there we go now let's
80:06 - let it do that over and over again in
80:09 - draw alright uh so this is a bit of a
80:26 - fail here
80:30 - no very returning the promise that's a
80:33 - good point
80:40 - all right
80:49 - so I let this run a little bit and
80:51 - unfortunately see it's getting nowhere
80:53 - this loss which I'm not sure exactly how
80:56 - it's country things I don't think about
80:57 - that come back to it is not going down
81:00 - anymore so what could be some problems
81:03 - here remember one is maybe my learning
81:05 - rate is no good not that it's no good
81:07 - maybe it's too low so where did I set up
81:09 - that learning rate again let me get rid
81:12 - of by the way I just want to now delete
81:14 - I want to make sure I'm not using any of
81:17 - my old neural network code so I'm
81:19 - deleting all references to that so this
81:24 - is now purely tensorflow Jess and and
81:27 - let me refresh and run this again and
81:30 - let me look into sketch dot yes and find
81:34 - where did I set the learning rate right
81:36 - here let's set it to 0.5 and see what we
81:41 - get yes getting better I'm gonna let
81:45 - this run for a little bit and I'll be
81:46 - back let me look at so one thing I want
81:58 - to do also is I want to write it looks
82:00 - nice if I write in the numbers actually
82:02 - of what the output is where it is you
82:05 - can see that it's actually getting there
82:06 - just very slowly I just want to see what
82:09 - settings I used in my other example try
82:16 - different optimizers and activation
82:17 - functions yeah I'm going to do that in a
82:19 - second I want to get it to work with
82:20 - this first I just want to see what
82:23 - settings I used
82:32 - [Music]
82:33 - oh did I forget to put shuffle in but
82:39 -  shuffle it by default I wonder if I
82:41 - ah you know what I forgot to put shuffle
82:45 - in it's actually working alright I'm
82:51 - back and you can see the losses now kind
82:53 - of much lower and you can start to see
82:55 - the visual that I'm expecting which has
82:56 - a true value in this corner a true value
82:58 - in the top corner and sort of darker
82:59 - false values in those corners but it's
83:01 - still kind of performing rather poorly
83:03 - one thing that I forgot to do is when I
83:06 - call the fit function there are a set of
83:13 - options that I can pass in for the
83:16 - number of epochs and all sort of thing
83:18 - but one of the ones that I really want
83:19 - to pass in here is called shuffle
83:21 - shuffle takes the training data and it
83:24 - shuffles the order of it each time right
83:26 - now I'm trading it with the same four
83:29 - data points in the same order every time
83:31 - which could be a bit of a problem and
83:33 - okay
83:34 - so let me now let me hit refresh here
83:37 - and run this again
83:47 - [Music]
83:49 - oops I can't see the frame rate because
83:54 - time out a sec 0.5 optimizer Oh
84:02 - optimizer yep this was all the same
84:05 - sigmoid sigmoid did I am I giving it you
84:15 - know what I probably did
84:28 - No I thought maybe I was giving it more
84:30 - epochs or something
84:32 - shuffle Forex or is not a big help
84:34 - apparently not
84:44 - I'm curious I could do just out of
84:48 - curiosity
84:58 - because if I go to that thing I was
85:01 - showing you everybody
85:17 - where this is this should be hosted
85:24 - hosted via github pages
85:42 - for loop and epochs will really help
85:44 - yeah I just wanted to see like I'm
85:47 - pretty sure I know why this isn't
85:49 - loading
85:59 - I mean the other thing is I probably
86:02 - should create a separate I mean there no
86:03 - such thing as a thread but I probably
86:06 - should have right I should just run the
86:11 - training like this async function I
86:14 - should just like I don't need to like I
86:24 - don't be calling this in draw but I just
86:27 - wanted to start doing that I was gonna
86:28 - fix that later what time is it - I have
86:35 - a half an hour and get through this
86:41 - [Music]
86:46 - whoops
86:47 - why is this not loading oh is it because
86:52 - oh it's probably cuz this is running
87:10 - I don't know why my example I wanted to
87:15 - see the performance of my example hold
87:17 - on I think I need to restart Chrome
87:35 - oops
87:40 - why is this not working here we go
87:50 - yeah I know I should do like I was gonna
87:52 - do this set interval thing but actually
87:55 - wasn't gonna do set interval I was gonna
87:57 - do set timeout and then have it like
87:59 - recursively call itself when it's done
88:02 - I'm just kidding I'm a got lost and this
88:04 - is not important I just thought I could
88:06 - run the thing that I made before just to
88:07 - see how it performed but I don't know
88:10 - what it's what it's stuck is anybody
88:12 - else anybody else try running this
88:13 - example maybe I'll merge Eric's pull
88:18 - request it's like stuck weird okay yeah
88:28 - that's what I that's what I wanted to do
88:30 - next frame so me I am so me let me come
88:36 - back to that I didn't want to do that
88:39 - first but yeah but thank you for that oh
88:48 - my goodness I forgot to tidy everything
88:53 - oh well but yeah the wise I need to
88:57 - dispose and I also need to tidy this oh
89:02 - yeah well let me at least just go back
89:06 - to where I was
89:23 - I'm gonna let this go for a little bit I
89:25 - just want to
89:36 - did anybody see what that was there it
89:41 - is where is it
89:44 - yeah look at that that's a mess okay
89:58 - okay
90:04 - all right so things are still working
90:06 - but it's still running kind of slow we
90:08 - can see I'm getting the lost down hold
90:11 - on
90:12 - okay things are working it's getting the
90:15 - kind of getting close to the right
90:16 - results you can see the losses going
90:18 - down I've realized thanks to the chat of
90:20 - course that I forgot something really
90:22 - important is I want to try to make it
90:24 - learn faster which I will kind of get to
90:26 - and I want to think about the sort of
90:27 - asynchronous nature of using p5 s draw
90:29 - loop and using the model dot fit at the
90:32 - same time but before I do any of that I
90:34 - just realized I haven't thought about
90:36 - memory management at all and there's a
90:38 - big problems if I just take out for a
90:40 - second this and let me let me do this
90:44 - here take out this console lot log and I
90:50 - run this again and I look at say if I
90:54 - say in the hooks yeah I'm like killing
90:57 - my computer it can barely run it again
90:59 - because watch what TF memory num tensors
91:06 - 3455 4042 79 i'm just generating tensors
91:09 - I'm filling up all the memory it's gonna
91:11 - perform so this is going to be a
91:12 - disaster so let me stop it before it
91:15 - gets too bad and let's see where do I
91:17 - need to do some cleanup so these X's
91:20 - these training tensors I never need to
91:23 - clean those up those I can keep forever
91:27 - but I in train model I should probably
91:32 - TF tidy this whole thing let me think
91:36 - about that let's first actually the Y's
91:39 - after I do this I can just dispose those
91:42 - so the Y's I definitely need to dispose
91:45 - let's at least just do that first and
91:47 - let's see what um let's see where that
91:49 - gets oh oh look at that look at this ah
91:55 - because I use data think I could use TF
91:57 - tidy but what I'm gonna do now is wise
92:00 - and then I'm gonna say let Y underscore
92:05 - values equals ma wise dot data sink I
92:09 - can't clean it up once it's also been
92:11 - data synced so now I do to why it's not
92:13 - disposed
92:18 - oops do I have no loop on nope and still
92:27 - going up that's weird
92:32 - oh this huh this has to be y-values also
92:35 - I have to change that to Y values so
92:37 - that helped now what I want to do is I
92:44 - need to uh let's just do this TF tidy
92:47 - let's just put the TF tidy here so like
92:51 - whatever happens in this function just
92:55 - tidy it all up I could put it properly
92:58 - up here it's that model that fit the
92:59 - need to tidy but I'm just going to do
93:01 - this let's do this five hundred forty
93:05 - nine hundred missiles being created over
93:09 - and over again that I didn't tidy
93:33 - it's to 2003 to a disaster what did I
93:39 - not tidy I guess maybe I need to put
93:43 - tidy in here like
94:05 - or I can just say like this right
94:20 - to learn
94:25 - Oh
94:33 - mmm
94:49 - I'm so confused like this no
94:59 - [Music]
95:08 - I'm so confused hold on I'll have to
95:12 - make the arrow function async yeah I
95:15 - dislike I'm like lost here let me back
95:18 - up for a second this is why I wanted to
95:20 - tidy outside of it you get rid of TF
95:23 - tidy for a second what did I have to
95:25 - start can I just put this here
95:44 - what am I missing
95:52 - I can't use promises in a tidy uh right
95:56 - so I was doing this right
96:11 - hold on a sec what what's wrong with
96:14 - what I had here what was wrong with this
96:31 - like why is this this should be fine
96:33 - right
96:43 - which version of
96:51 - well let me also go back to making the
96:59 - resolution much bigger oh uh I had it
97:09 - right before I just didn't save I'm
97:20 - gonna do a backflip ready what - no no
97:22 - I'm not dating back foot I had it right
97:26 - the whole time no that's so weird
97:38 - when the resolution is higher I have a
97:41 - memory leak
97:58 - that's weird what am I missing
98:07 - why do I have three in there that was
98:09 - weird
98:31 - [Music]
98:36 - I'm like going out of my mind here
98:45 - so what is it
98:50 - well I have some like weird bug here
98:52 - that I'm not thinking of exes
99:15 - let me see if tidy this whole thing hmm
99:24 - ah so why with a different resolution
99:35 - that's so weird I feel like that's is a
99:42 - bug he had to slower the resolution to
99:48 - less early I sort of feel like there's
99:52 - some bug that's not my code I can't
99:55 - figure it out but tidy here kind of
99:56 - fixes it
100:08 - I don't think that this needs to be this
100:11 - is just an array so this doesn't need to
100:13 - be cleaned up I mean the garbage
100:15 - collector should that's not a tensor but
100:18 - did I make some other tensor in here
100:19 - that I like it's modeled I predict maybe
100:22 - it makes some other tensors behind the
100:25 - scenes okay you know what it is I'm just
100:30 - being silly here I think I'm like
100:32 - forgetting that model that predicts like
100:33 - model dot fit does okay
100:35 - that's yeah I'm going back and talk and
100:48 - I'm gonna do my memory
101:07 - when did I make that three I'm gonna do
101:11 - my
101:17 - and go all the way back and memory leak
101:20 - it again and fix it
101:45 - yeah the epochs will definitely help
101:49 - [Music]
101:52 - whoo
101:53 - modeling evolution with tensorflow das
101:56 - from suraj rebel new video all right
102:08 - want to get this down a bit and then I'm
102:09 - going to come back
102:23 - all right here we go here we go
102:34 - I can't look at the chat right now jet
102:43 - all right so you can see it's kind of
102:46 - working I back it's sort of taking a
102:48 - while but I want to get this to train a
102:50 - little faster I want to make this I want
102:52 - to get a little further it's up first of
102:53 - all I was reminded by the chat that I've
102:54 - forgotten something really crucial
102:56 - important which is memory management and
102:58 - I really I really should stop this from
103:00 - running right now because there's a huge
103:02 - memory leak happening which I haven't
103:04 - cleaned up any of my tensors at all
103:06 - whoops look at that well not sure out
103:10 - that it was there this is there
103:30 - just do this again sorry everybody one
103:34 - more time shift ya memory leak really
104:08 - need that ukulele Wow
104:12 - [Music]
104:23 - all right wow look at this
104:27 - alright so you know it actually it
104:30 - actually is working it got the correct
104:32 - training result it's a little gray scaly
104:34 - in a way that I would like to be able to
104:36 - like emphasize visually what it's do but
104:38 - you could see the loss has gone way down
104:39 - but it took a while to get there but I
104:41 - want to add a few things to this and try
104:44 - to fix it up a little bit before I do
104:46 - anything I was reminded by the chat
104:48 - being over here that I haven't thought
104:50 - about memory management at all so I'm
104:52 - gonna say like no Luke for a second to
104:54 - just sort of turn this off then I'm
104:57 - going to say memory numb tensors oops no
105:00 - no way TF memory dot numb tensor I know
105:06 - I'm trying to use it I'm gonna say numb
105:10 - tensors TF dot memory numb numb there it
105:15 - is there it is I could get it there it
105:17 - is thirty two thousand two hundred five
105:20 - tensors that's crazy so I need to deal
105:22 - with that I'm just making tensors and
105:24 - letting them leak everywhere so I can
105:27 - manually run dispose but I've got kind
105:29 - of an issue whereas predict is gonna
105:31 - like make a lot of tensors behind the
105:32 - scenes as well as a model dot fit so I
105:36 - can use the TF tidy function so I'm
105:38 - gonna say TF dot tidy and then I just
105:43 - need to I'm gonna use the es6 arrow
105:45 - notation which you can watch my videos
105:48 - about what that is but and I've kind of
105:50 - gone through what tidy is tidy says
105:52 - anything inside of this code clean up
105:55 - the memory afterwards basically and then
105:57 - I'm gonna I probably could put this
105:58 - around everything but I just want to and
106:00 - I don't need this stuff anymore I just
106:02 - want to keep these two areas separate
106:04 - because I think I'm gonna at some point
106:07 - I really should change the way doing the
106:08 - fitting of the model excuse me in draw
106:11 - is somewhat problematic so now I'm gonna
106:14 - just tidy all of this I think that's
106:19 - right
106:21 - I tried an extra parenthesis there yes
106:24 - okay so now let's run this again
106:29 - comment out this console.log I don't
106:34 - want to see that right now oh all right
106:42 - now let's look at the number of tensors
106:44 - 15 15 15 so now I've gotten rid of the
106:47 - memory leak let's check out the frame
106:49 - rate 30 frames per second so this is
106:53 - running for up now let's I just want to
106:55 - be able to look at what's happening a
106:56 - little bit better so I'm actually gonna
106:57 - draw the number of the output inside
107:00 - each one of these things so let's do
107:03 - that so where am i drawing the
107:04 - rectangles here I'm going to say let the
107:08 - brightness value equal this and I'm
107:13 - going to fill the rectangle with that
107:14 - brightness and then I'm going to say
107:16 - fill 255 mind it like the inverse color
107:19 - I'm gonna say text number format the
107:24 - y-values index with just two decimal
107:31 - places and I'm gonna put that at boy
107:35 - this is awkward I the exercise this is
107:37 - gonna be high x resolution plus
107:39 - resolution / - I'm gonna say
107:41 - text-align:center text-align:center
107:46 - comma Center and then I'm gonna put the
107:49 - I'm just going to draw in the center of
107:52 - the rectangle and this should be J the
107:56 - text so let's see let's do this now so
108:00 - we can see you look there's lots of
108:02 - numbers there I think my number format
108:03 - thing didn't work 1 comma 2 and let's
108:10 - use a lower resolution just so I can see
108:12 - it better there we go now interestingly
108:17 - I can't see the numbers but there they
108:19 - go right you can see this is what it's
108:22 - getting the output for each one of these
108:24 - and I want to look at the law so you can
108:27 - see because it's just going so slow it's
108:30 - getting over time it's getting a little
108:35 - better but I really want to see a train
108:37 - much faster so let me see I have one
108:39 - idea one last thing I can add to this
108:42 - even though I and
108:42 - I have some suggestions for what I might
108:44 - do next but you can see all the numbers
108:45 - are starting to appear lovely I was
108:47 - because I forgot when it's gray when
108:49 - it's at point 5 2 or 55 minus 0.5 it's
108:52 - gonna be the same card I kind of like
108:53 - that effect so so what I want to do is
108:57 - what happens here if I actually give it
109:00 - tell it don't just do it once like do it
109:05 - 10 times do 10 a pox per cycle of
109:09 - fitting let me run this again and let me
109:15 - look at the let's actually have the loss
109:18 - continue to print out and there we go
109:24 - still running pretty fast you can see
109:26 - the losses going down and relatively
109:28 - quickly I am getting myself to the point
109:32 - where I'm starting to see you know this
109:34 - is definitely all the way got getting
109:35 - all the way down to zero there this is
109:37 - getting way up to 1 there it's getting a
109:39 - little bit stuck it's having trouble
109:40 - with this size side I imagine it'll get
109:42 - there eventually we could do some fun
109:44 - stuff for example I'm going to just let
109:46 - it have it's totally unnecessary but I'm
109:49 - gonna give it 4 hidden nodes and I'm
109:53 - also going to put the resin the
109:54 - resolution back to 25 and let's run this
109:58 - again and let's see how this goes so
110:03 - I'll give it a minute I'll come back oh
110:04 - the font is too big but you can see it's
110:12 - learning pretty quickly right now hold
110:13 - on
110:19 - well let me let me let me go back do
110:21 - that again
110:28 - so I'm gonna give it for no real reason
110:30 - all but just for fun four hidden nodes
110:33 - and I'm also gonna let me change the
110:35 - resolution to twenty five let me make
110:37 - the text size something like eight point
110:43 - and let me
110:45 - oops refresh it
111:09 - yeah one alright I let this run for a
111:15 - bit and you can kind of see here now you
111:17 - can see all the X or values here's a
111:19 - nice beautiful little map grayscale map
111:21 - of all X sorts getting all the way up to
111:23 - true and all the way down to zero at the
111:26 - corners this is pretty good
111:28 - so this is running at if I take out this
111:32 - console.log I can now take a look at the
111:40 - frame rate it's running kind of slow so
111:43 - here's the thing I have done something
111:45 - that I don't like which is and which is
111:50 - this train model is being called inside
111:53 - draw and I really shouldn't be doing
111:57 - that
111:58 - all right because I don't want I'm you
112:02 - know I don't want to call train model
112:03 - over I can call train model only thing
112:06 - about what I'm saying here yeah I how
112:16 - did I finish this up I have to go in
112:22 - just like a minute I'm saying of what I
112:25 - want to finish up here so I don't think
112:29 - I have time to do the TF frame thing but
112:33 - should I do
112:34 - would it make sense for me to at least
112:37 - do something like set time out train
112:43 - model let's just say hey do that and
112:47 - then
112:55 - like what if I did this function train
113:02 - set timeout train model I want to do
113:11 - like a then but well or what if I just
113:18 - did this
113:37 - like something like this right this is
113:42 - what I'm thinking
113:43 - and then this wood right does this make
113:56 - sense so I gonna explain this in a
113:58 - second like this will basically be
114:01 - happening elsewhere
114:13 - but I think I need the set timeout to
114:16 - like yeah this is never gonna release
114:18 - anything oh this is like superfluous
114:25 - because it's just returning a promise
114:28 - yeah I'll do that
114:43 - like what if I did this this is weird
114:47 - though
114:55 - kill this completely killed the browser
115:18 - yeah it's not really that much faster
115:21 - but this this is kind of like what I
115:23 - want to do right
115:40 - yeah now I can get a much faster frame
115:42 - rate and I could actually give it more
115:47 - epochs right
115:57 - yeah the training is happening more
115:59 - slowly but the framerate is happening
116:02 - faster perhaps result in a video about
116:06 - workers what tidy does nothing look look
116:14 - at this crazy way that it learned
116:21 - where's a tidy that does nothing
116:24 - me I am semi right is this an
116:27 - improvement over what I had before
117:00 - you know I I need this TF tidy what
117:06 - because this has to get tidied all right
117:17 - this is what's doing right now if I take
117:21 - that out oh you're right
117:28 - oh I don't I don't need a TF tidy for
117:35 - model that fit okay okay
117:40 - is this an improvement like to have
117:44 - pulled this out at least pulled this out
117:46 - of draw and a wit model that fit is
117:53 - tidied internally okay oh and tidy
117:57 - doesn't work with promises got it got it
117:59 - got it got it got it got it all right is
118:02 - this an improvement or should I just
118:04 - stay where I was and let it be like
118:10 - leave it and draw and talk about how
118:11 - that probably should do TF frame instead
118:13 - webworkers yadda-yadda-yadda
118:18 - answer fast I gotta go a sink while loop
118:24 - is a definitely good suggestion
118:35 - technically this is a much better way to
118:37 - do it all right thank you
118:39 - that's all I needed to know okay
119:11 - okay so I'm gonna go back to all right
119:24 - so the chat has given me some really
119:26 - helpful tips I've made quite a few
119:27 - little like weird little errors and
119:29 - mistakes here and I want to just fix
119:31 - this up a bit I think it's good actually
119:32 - to be easier to look at and watch if I
119:36 - just go back to a lower resolution so
119:40 - let's make this 40 and let me refresh
119:43 - this okay so here we go so this is now
119:48 - working training itself for X or you can
119:51 - see it's kind of moving along here now
119:53 - what what the real thing that's problem
119:56 - problematic here is the draw loop is
119:58 - happening over and over again and then
120:00 - I'm triggering something asynchronous in
120:02 - draw and I could be asking to train the
120:04 - model before it's even done with the
120:06 - previous training cycle so this really
120:08 - should not be happening in draw now
120:11 - tensorflow digest has a function called
120:13 - TF next frame I want you to explore it
120:17 - and make a version of this with Tia next
120:19 - frame as I can exercise after this video
120:22 - is over but I'm gonna do it a different
120:23 - way without that cuz I gotta come back
120:25 - then in a different video but first of
120:26 - all this I also learned this is totally
120:29 - unnecessary the via wait so a couple
120:34 - things number one is because there's
120:36 - just one thing happening in here I could
120:38 - just return the promise this doesn't
120:40 - actually have to be an async function
120:42 - and then I do not need the TF tidy
120:45 - because monal dot fit kind of will clean
120:47 - itself up automatically for you so this
120:50 - this should still work just fine and I
120:54 - should be able to see the number of
120:55 - tensors is still 15 so that was
120:57 - something that I didn't need that I've
120:59 - now fixed now what I really want to do
121:01 - is I want to get this out of draw so
121:03 - let's comment this out here and what I'm
121:06 - actually going to do is I'm going to
121:08 - write a separate function called train
121:10 - and in that function I'm going to say
121:16 - set time okay wait wait
121:20 - I'm gonna call train model
121:21 - wait hold on I'm gonna call trade model
121:25 - yes yes in that function I'm going to do
121:28 - this so I'm a separate function that
121:35 - does this piece of it that console logs
121:38 - the history and I could use and what I
121:41 - want to do is I want to say set timeout
121:46 - call the train function in 100
121:48 - milliseconds so I want to just let the
121:50 - program start 200 full effects late
121:51 - later call this train function train the
121:54 - model which does the fitting when that's
121:56 - done log the history and now say set
122:00 - timeout train 100 so I'm good this is
122:05 - sort of like workers like like don't you
122:07 - set interval here because I only want to
122:09 - call train again once it's finished
122:11 - with training the model itself so this
122:13 - is kind of like hey train and and by the
122:15 - way I could just increase the number of
122:16 - epochs or maybe do some kind of loop but
122:19 - I think this would be a sort of nice way
122:20 - to demonstrate it and if I just called
122:22 - train directly without a set timeout I'm
122:27 - never going to be giving back control
122:29 - for a second I couldn't end up with sort
122:30 - of like blocking so I might even be able
122:32 - to get this down to like 10 milliseconds
122:34 - just something really really low so
122:35 - let's run this and sort of see same
122:39 - result we can see there we go
122:41 - things are working but at least now I
122:43 - have gotten that out of draw so draw is
122:46 - happening on its own and in fact what I
122:48 - could really do is I could say hey try
122:50 - doing this with like a hundred epochs
122:52 - each time epochs what is it and you can
122:57 - see the lost function is coming out much
122:59 - more slowly but whoops but the frame
123:02 - rate let me just clear this for a second
123:06 - yeah the frame rate is quite fast hold
123:13 - on this is too confusing
123:21 - I probably need to give give it back
123:23 - more time let's do like another little
123:25 - break let me let me I just want to like
123:28 - take out the console.log thing so I can
123:30 - look at the frame rate I should just put
123:32 - the frame rate in the Dom would that be
123:34 - smart but you can see now I'm getting 60
123:37 - frames 30 frames of getting like a
123:39 - really pretty high frame rate even
123:41 - though the training is happening it's
123:43 - almost it's kind of like there is no
123:45 - threading in JavaScript so these things
123:47 - are just like passing off and really
123:49 - this might be a place where like web
123:51 - workers or something could do the
123:52 - training behind the scenes in some fancy
123:54 - way which maybe I will get to at some
123:55 - point oh my goodness
123:58 - so Alka is suggesting it might be better
124:03 - to use the draw loop and a boolean to
124:05 - know when it's safe to call it again
124:07 - that would also be a good idea so you
124:09 - can see though you can see what kind of
124:10 - like employing my hair out what kind of
124:12 - sort of like hassle situation we've
124:14 - gotten in but really let's just put this
124:16 - back to like two epochs
124:18 - let's put this to like a little 10
124:21 - milliseconds and we can sort of feel
124:23 - like there we go and I can look at the
124:25 - frame rate it's running nice 30 frames
124:27 - per second and even though and it's at
124:30 - some point it's going to get there
124:31 - what's that loss I forgot to console.log
124:33 - loss come back to me
124:37 - come on oh but let's see I have an idea
124:40 - what if just before I go just before I
124:43 - go what if we try using a different
124:48 - optimizer what if we try using for
124:51 - example a different loss function hold
124:59 - on
125:01 - this video was already 18 hours long
125:03 - what if oh no no no a different
125:06 - optimizer sorry what if I tried using
125:08 - the the atom optimizer
125:14 - so let's just try that just for fun
125:17 - times
125:18 - let's give it a lower learning rate that
125:38 - was pretty exciting let's go let's go
125:41 - let's uh let's make the resolution back
125:45 - to like 20 let me make the font size
125:48 - like nice and tiny for us we give myself
125:51 - some more space here hit refresh and
125:54 - then let's look at this look at it
125:57 - learning there wow look at that that is
125:59 - beautiful look at it learning XOR so
126:01 - nice and fast I'm just gonna hit refresh
126:03 - again
126:07 - so we could say we really should look at
126:10 - what this atom optimizer is and I will
126:13 - link to a paper and some more
126:16 - information about the some of the what
126:19 - the atom optimizer is you can find out
126:22 - actually we should just go yeah I'm
126:26 - sorry like hold on we should really I
126:41 - should really talk at some point about
126:42 - what some of these other optimizer
126:43 - functions are for now what I would
126:45 - suggest that you do is if I again if I
126:47 - go back to the API reference and I go
126:50 - all the way down and I find let me just
126:53 - look this way the OP if I go here and I
126:56 - go training out Adam we can see here
126:58 - this is what you're gonna want to click
126:59 - on this is the paper that describes the
127:00 - Adam algorithm of its a different but
127:04 - but it's optimizing the lost function in
127:06 - a slightly different way than stochastic
127:08 - gradient descent does you know we could
127:10 - also try starting to like we you know we
127:12 - could we could just never stop and I
127:14 - could start
127:15 - like I think using the rail ooh
127:19 - activation function instead whoops
127:22 - is that not what it's called where are
127:30 - the activation functions oh it's it's
127:32 - there's no it's just all lowercase so
127:36 - there's so many things you can play
127:37 - around with with these things how was a
127:45 - failure I think because I have the all
127:55 - right let's I'm not gonna add I'm gonna
127:57 - go back back to where I was explaining
128:01 - the atom optimizer oh it made this such
128:05 - a mess to edit and I really have to go
128:20 - I really should explain what these
128:22 - optimizers are but if I go back and look
128:25 - under here we can see what some of these
128:27 - are Adam the a da coming from the word
128:29 - adaptive and you could always click here
128:31 - and look at this paper which describes
128:33 - this particular method for optimization
128:36 - which is a little bit different than
128:38 - stochastic gradient descent and
128:40 - apparently things work a lot faster with
128:42 - this XOR problem so as I go forward into
128:44 - more of these videos hopefully we can
128:46 - dig into what some of these different
128:47 - optimizers do and kind of understand why
128:50 - I might pick one over the other in
128:51 - certain situations all right but I'm
128:53 - just going to just leave this B I'm
128:56 - going to hit refresh I'm going to watch
128:58 - it learn and train train train train XOR
129:02 - oh I don't know some things you could do
129:05 - investigate TF dot frame give me a
129:07 - little slider try different
129:09 - architectures different optimizers try
129:10 - some different activation functions I
129:12 - don't know if you actually made it all
129:15 - the way to the end of this video I don't
129:19 - know ashtag something I should Eric is
129:23 - telling me to watch one of those videos
129:24 - reviewing the JavaScript event loop
129:26 - which I definitely need to do so I need
129:28 - I'm gonna be back with more someday well
129:31 - good bye good bye good bye thank you
129:43 - momentum is add a knock for adaptive did
129:53 - I just make that up
129:57 - adaptive estimates so maybe the M is for
130:00 - estimates moments low-order moments and
130:03 - is for moments yeah
130:12 - all right everybody well you know today
130:15 - was one of those days I should really be
130:18 - doing my old style coding challenges
130:19 - again readout of the random numbers book
130:21 - okay okay I really I'm running late I
130:26 - gotta go everyone lie down go to sleep
130:31 - we're gonna let's let's artificially
130:36 - make this happen much slower let's train
130:46 - let's go caddy
130:48 - [Music]
130:56 - thirty two thousand nine hundred eighty
130:59 - five twenty six thousand eight hundred
131:05 - and fourteen fifty one thousand eight
131:08 - hundred thirty three fifty seven
131:10 - thousand three hundred and sixty three
131:12 - four thousand and sixty seven eighty
131:15 - four thousand six hundred forty eight
131:18 - eighty-five thousand five hundred five
131:21 - forty one thousand four hundred
131:23 - sixty-five seventy one thousand seven
131:26 - hundred sixty nine ninety nine thousand
131:28 - five hundred fifty fifty five thousand
131:31 - nine hundred four will there be a second
131:37 - live stream today no unfortunately why
131:39 - life's do on the weekend no
131:40 - unfortunately
131:41 - so apologies I wish I could live stream
131:43 - more often all the time make more stuff
131:45 - yeah blah blah the next live stream will
131:48 - likely be next Wednesday and next
131:49 - Thursday and I'm gonna be working on a
131:53 - lot more of this stuff in-between
131:56 - hopefully we will the things that I
132:01 - really need to look at or I need to look
132:03 - at that event loop article that I just
132:05 - to get a better sense of how to use this
132:07 - stuff together with the
132:10 - requestanimationframe better and TF dot
132:13 - frame alright so any last if I my real
132:20 - time that I had to leave was three
132:22 - o'clock I got two minutes to answer
132:27 - questions
132:32 - let's try like
132:34 - [Music]
132:39 - I like doing it with like a high
132:42 - learning rate you can see like it gets
132:44 - too good sort of see it bouncing
132:52 - [Music]
132:59 - maybe this learning rate is too high
133:00 - it's kind of get stuck it kind of can't
133:02 - get there and then if we could do we
133:08 - could do weird stuff like like what if I
133:10 - gave the hidden layer 16 units for like
133:12 - no reason like look how it is there's no
133:20 - correct answer there's only training
133:22 - data at the corners that learn to
133:23 - rainbow people for asking me questions
133:31 - and I are you going to do this
133:32 - convolutional 2t soon that's why I I
133:36 - hope to plan to yes
133:38 - all right everybody so this is my list
133:43 - Oh Thank You Joshua Myers that's very
133:46 - kind of you if I had my Philip's light
133:48 - bulb it would have flashed by the way oh
133:52 - let me just mention if any of you are if
133:55 - any of you are sponsors you can now
133:56 - sponsor the channel through the through
133:58 - the YouTube interface itself make sure
134:00 - you go and check the community tab and
134:03 - look for a post that links to a Google
134:05 - Form to enter your email so I can send
134:07 - you an invitation to the slack group I
134:08 - need a better system for doing that I
134:10 - have a thing set up that it actually
134:12 - like I have a I get alerts in a
134:13 - spreadsheet and I can look but I don't
134:15 - get your email address so there's no
134:16 - automatic way to invite you to slack
134:18 - other than by doing it manually yeah so
134:23 - this is where I'm this is where I am
134:27 - going to next I am going to do a I want
134:35 - to do the TF playground idea and the
134:37 - classification idea so if anybody has
134:42 - any ideas for really simple goofy just
134:45 - like basic numeric data sets probably
134:49 - going to do something where I'm going to
134:50 - look for some color data set which takes
134:52 - any RGB set of RGB numbers and like
134:56 - categorizes it it's like a few with a
134:58 - few different like labels that's one so
135:00 - if you have some suggestions for that
135:02 - please let me know at Schiffman on
135:05 - twitter is probably the best way all
135:07 - right everyone
135:08 - I don't know I had more layers
135:13 - sorry everybody I hope today I hope you
135:15 - enjoyed today somehow uh somehow I I
135:20 - just I don't know maybe I shouldn't be
135:21 - covered next week let me through really
135:23 - maybe I'll try to at least do a coding
135:24 - challenge that's not machine learning
135:25 - related I want to get thrown mean
135:27 - learning content but it is kind of
135:29 - overwhelming and taking over everything
135:30 - weather prediction but I need I want to
135:33 - do classification so I don't want to do
135:35 - time series great all these these things
135:38 - will come but I want to do a really
135:39 - basic classification I don't use images
135:42 - I don't want to use text data that's
135:44 - like I don't use time series sequential
135:47 - data I just want like you know like it's
135:49 - like house prediction the iris data say
135:51 - these are the kind of thing house price
135:53 - prediction
135:53 - well--that's prediction that's not
135:54 - classification even but regression but
135:56 - something like that but I want it to
135:58 - feel goofy creative in the art world
136:01 - space that kind of thing
136:03 - alright tic-tac-toe game okay goodbye
136:06 - everybody I'm gonna hit stop streaming I
136:07 - guess I will play you out with my weird
136:09 - trailer since that's what I do now
136:13 - [Music]
136:21 - while the trailer is playing I will
136:24 - attempt to explain why doesn't it
136:28 - dispose of tensors automatically s asks
136:30 - ray Jackson asks Arnab I don't actually
136:35 - know why it doesn't dispose of tensors
136:37 - automatically but it's this is the thing
136:40 - that there is no way to clean up the
136:42 - memory memory on the GPU with a garbage
136:44 - collector in the same way and this is
136:45 - like a lower-level question that I would
136:47 - be curious we have to investigate more
136:49 - about how tend to flow J s works
137:02 - [Music]
137:08 - [Music]
137:11 - you
137:20 - you