00:00 - Hello welcome to Coding Challenge in
this Coding Challenge I'm going to
00:04 - attempt to make a Perceptron this is
part of a whole bunch of videos that I'm
00:08 - doing about neural-network-based
learning and, but in this video I'm not
00:12 - kind of getting into too much about the
whole broader landscape of everything
00:16 - history-wise and future wise about
what's happening with neural network
00:19 - based learning in this example I just
want to build with code the simplest
00:23 - model of an artificial neural network
possible known as a Perceptron.
00:27 - Perceptron, uh, the concept of perceptron was
invented by Frank Rosenblatt in 1957 at
00:35 - the Cornell Aeronautical
Laboratory, there's a link here to the
00:40 - original paper which I will include in
this video's description if you want to
00:44 - take a look at that, and of course you
can always find more on the perceptron
00:48 - Wikipedia page. What is a Perceptron? Why
am I even here talking about this?
00:53 - So ultimately I want to look at Artificial Intelligence, machine
00:59 - neural network based machine learning systems
that are inspired by and modeled loosely
01:04 - off of the idea of the actual brain. The
actual brain being this thing with like
01:08 - neurons and axons that connect other
neurons and dendrites that receive input
01:13 - Actually, I don't know what I'm talking about just
reading whatever is on this diagram
01:18 - Ultimately where I'm going
to start building examples and show you
01:22 - examples of lots of these components
that are all interconnected and inputs
01:28 - are coming in and and outputs are
flowing out but I want to start with
01:32 - this idea of the perceptron being a
model of a single neuron the simplest
01:38 - possible artificial neural network that
we could build and this will serve as
01:43 - the building block for future examples
that it will make in future videos. Okay,
01:49 - So let's come over to the white board
for a second so if the idea of a
01:53 - perceptron is that there is a single
neuron, call this a neuron. That neuron
02:00 - could have inputs let's say there are two
inputs I'm going to call them X[0]
02:10 - X[1] so inputs zero and one.
They come into the neuron some type
02:16 - of mathematical process happens in the
neuron. And then, there is an output which
02:24 - I'll call Y or output. So these are
inputs. And again this diagram it might
02:35 - looks familiar to you if you've watched
some of my other videos because I often
02:38 - talk about this idea of. There is some
amount of input so long list of inputs
02:44 - that go into some machine learning
recipe that processes all those inputs
02:49 - and performs a task maybe it tries to
classify or perform a regression but
02:55 - make some sort of output some sort of
prediction so this is exactly what this
02:59 - Perceptron is designed to do. Okay here's
the thing. In order for us to understand
03:08 - and look at all of the pieces of what's
happening in here we need some scenario
03:13 - so let's come up with a scenario that we
can use so let's say I have a
03:17 - two-dimensional space you could think of
this this will be my canvas my window
03:22 - and what I'm going to do is I'm going to
arbitrarily divide the space with some
03:30 - line. Some points will be on one side of
the line and other points will be on
03:39 - another side of the line. So essentially
what I want to do is use this perceptron
03:44 - for a classification problem. I want to
say that these belong to you know Class
03:50 - A and these belong to Class B so and I'm
going to use a supervised learning
03:58 - strategy. So for background not some of
this you could go and watch session
04:04 - three of my intelligence and learning
series where I do some other videos
04:07 - about classification and regression
using a linear regression model there's
04:11 - a lot of crossover here but anyway you
could stay here if you want I'm going to
04:14 - kind of talk through everything but the
idea is I want to classify these so I
04:18 - want these inputs to be so x0 or any
points right here like this X comma Y
04:26 - it's a little bit confusing the way I'm
using it I don't I think I don't love
04:31 - this because I should really think it'd
be the X is input zero the Y is input
04:37 - one right the X is input zero the Y is
input 1 the output is Class A or Class B
04:44 - the Y so I'm using x and y in two
different places and slightly different
04:49 - ways which is a bit confusing but
hopefully we'll make sense as we
04:52 - continue to go along here okay
so but I actually want to say instead of
04:57 - instead of A and B what I actually want
to say is plus one and negative one so
05:02 - the idea is that my perceptron is going
to output a plus one if the point
05:09 - belongs to group A and it's going to
output a negative one if the point
05:15 - belongs to group B. So how does – and we're
going to use the technique known as
05:21 - supervised learning and what supervised
learning involves is I am going to ask
05:31 - the perceptron to say here is some input
give me a guess but I know the correct
05:36 - answer I'm going to give the perceptron
a point that I know should be in A and
05:41 - it's going to guess either A or B if it
guesses A I'm going to say great job
05:47 - perceptron you keep on going! If it
guesses B I'm going to say perceptron you made a
05:52 - mistake let's tweak something about your
algorithm to try to get you to the
05:59 - correct answer and this tweaking is a
process known as gradient descent and
06:04 - it's something I've also covered in a
couple different videos that I will also
06:08 - link to down here where to go through it
as I get through here okay so that's the
06:11 - supervised learning process. Okay so what
is the actual algorithm here, what happens
06:17 - here inside the neuron? So here there's
here's the missing piece there's a few
06:23 - different missing pieces that I'm get to
over time I was stepping on some of
06:27 - these you can think of as connections
the inputs flow into this neuron but our
06:35 - weighted weighted weighted as they flow in
so each one of these connections has a
06:41 - weight let me say W0 W1 so
these inputs are weighted and what the
06:51 - perceptron does its algorithm is to
create a sum of all of the inputs
06:58 - multiplied by the weights that sum is X
input times sorry input 0 times weight 0
07:07 - plus input 1 times weight 1. Now in this
case the perceptron only has two inputs
07:16 - so this is a very easy formula to write
as you're going to see as I get into
07:20 - future videos you might realize like oh
there's a hundred inputs or a thousand
07:25 - inputs or a hundred thousand inputs but
this same formula is always going to
07:29 - apply. A weighted sum of all the inputs
input 0 times weight 0 input 1 times
07:35 - weight 1 add them all up together so
that step one is the sum. Step two before
07:43 - so you could say like okay we'll take
that and that's the output but this
07:47 - isn't the output step 2 is something
called an activation function. Activation
07:54 - function. And this is a key concept in
neural network based machine learning as
08:01 - I get into future videos we're going to
see there's a variety of different kinds
08:05 - of activation functions and why might you
use this one or this one and what do
08:09 - they do and why – but typically what an
activation function does is it allows
08:15 - you to conform the output to some
desired range and do thing and another
08:23 - way actually another way to think about
it is if you think about that idea of
08:26 - the brain like you can think it does the
does the neuron fire and continue to
08:30 - send its data along, or does it not? So
what happens as the data comes out of
08:35 - that neuron? We're going to use in this
particular example we're going to use a
08:38 - very very simple activation function you
could think ok well I only want two outputs I
08:43 - want a plus one or a negative one. How could I
take any number I can take any number
08:48 - and convert that number into plus one or
negative one?
08:51 - How would I do that? How about a function
called sign? Take the sign of any number
08:58 - n. If that number n is positive then I
get a plus one, if that number n is
09:07 - negative then I get a negative one, okay?
So that's the activation function this
09:14 - is the whole process, it's often
referred to as feed-forward. The inputs
09:21 - come in they get multiplied by the
weights they get added together and then
09:26 - that weighted sum gets passed through an
activation function and then that
09:30 - activation function gives us a plus one
or negative one should the neuron fire
09:34 - or not fire and that gets sent out and
that's the output ok I'm back from I was
09:40 - just checking a live chat that's going
on if you're watching this in archives I
09:42 - live chat doesn't exist but there are
two important questions before I move on
09:45 - number one is what do you do with 0? I
don't know we could just pick we could
09:51 - just arbitrarily right now let's just
say this is greater than or equal so 0
09:54 - will will consider +1 I mean in the case
this is just like a toy example just to
09:59 - demonstrate the idea it's a building
block you know I don't know how
10:02 - meaningful it is to be able to like
classify points into these state space
10:05 - or a line in your app but but so it just
wanted to make an arbitrary
10:09 - determination for that if it's on the
line you know is it above or below I'll
10:13 - pick one. The other question that was
asked is well how do you pick these
10:16 - weights? So this is the essential
question and this is where I have to get
10:20 - to. So the idea is that what we – through
the supervised learning process we want
10:27 - to search for – we're basically doing the
search to find the optimal weights the
10:31 - optimal weight values that will get the
best results, the results with the
10:35 - least amount of errors and so to start
we have to pick something to start. In
10:40 - this case we could pick random values we
could just start with the weight at zero
10:44 - that could be problematic so there's
different ways this is a big topic in
10:48 - the field of machine learning when you
start a neural network based system how
10:51 - do you how do you initialize the weights?
Randomly? What
10:56 - distribution of random numbers you do
some other kind of like learning process
10:59 - that gets to like a good starting point
for the weights? That's a big topic of
11:03 - discussion and research but for us I'm
going to pick random weights and start to tweak
11:07 - them okay so there's a lot more pieces
of this still but I think I'm going to
11:10 - go and start writing some code and we'll
come back to pieces that we're missing
11:13 - I'm going to do this in Processing which
is a Java based programming language and
11:21 - environment you can find out more about
it at processing.org I will also release
11:26 - a JavaScript version of this that you
can run in the browser so check this
11:30 - video description for links to both
source codes after it's over. Okay so
11:34 - what I want to do, I want to create a
perceptron class. So we can see here by
11:41 - the way I have this slide here this is
the same algorithm I just talked through
11:45 - let's just make sure we have it right
the algorithm is for every input
11:48 - multiply input by the corresponding
weight. Sum all the weighting – weighted
11:53 - inputs and then compute the output based
on that sum passed through an activation
11:57 - function, the sign of the sum. So we can
see we can think of like this could be
12:01 - the point at 12 comma 4 and these could
be the weights of the perceptron so what
12:05 - I'm going to do for this particular
perceptron is I am going to create an
12:10 - array to store all of those weights and
I'm going to say it's an array with two
12:16 - elements in it and in the perceptron
constructor I'm going to loop through
12:26 - all of the weights and give them a
random value between negative 1 and 1 so
12:35 - whoops you don't say void with a
constructor I don't remember how to
12:39 - program in Java based languages okay so
this is the constructor and what I want to
12:48 - do in the constructor is initialize the
weights randomly. Okay now what are some
12:56 - things the perceptron should do? Well I – one of the things it should do
13:00 - is it should be able to receive inputs
and then compute a guess, an output
13:06 - we'll call that a guess
okay? So let's write a function I'm going
13:12 - to call it guess and it should return
an integer plus one or negative one and
13:18 - it should receive inputs which could
also be in the form of an array. Now I
13:24 - could if I wanted to because the
simplicity of the example I could have
13:27 - done something like float w0 and float
w1 I could just sort of have individual
13:32 - variables for the weights instead of an
array but the nice thing about doing
13:35 - this way is this is more flexible that
this we could happen if we reuse this
13:41 - code later with we can adjust the number
of inputs and that sort of things within
13:44 - our array okay so first thing I need to
do is compute that weighted sum so I'm
13:50 - going to create a variable called sum
and initialize it at zero then I'm going
13:54 - to loop through all of the weights and
14:01 - I'm going to say sum plus equal – what do
I want to do? The sum of all the inputs
14:06 - multiplied by their corresponding
weights, so inputs index i times weights
14:13 - index i, so this is now that weighted sum
I say that second step start with a
14:25 - summit zero loop through and multiply
all the inputs by the weights. Then what
14:30 - I'm going to do is I am going to return
– ah! I need to get the – so then I'm
14:37 - going to say the output is sign of the
sum so it doesn't know what sign is
14:43 - there probably is like a Java based
function I could call automatically but
14:48 - let's just write our own up at the top
of this code here I'm going to write a
14:51 - function I'm going to say int sign and
it gets I guess I could say it gets any
14:56 - float I'm going to say if n is greater
than or equal to zero return a positive
15:02 - one otherwise return a negative one so
this is just that this is the so I could
15:09 - I could write here as a comment this is
the activation function I could call it
15:14 - activate or something instead the
activation function is a function that
15:17 - receives some value if it's greater than
zero positive one if it's
15:21 - zero negative one so no matter what
number goes in to the what other inputs
15:26 - come in whatever that's weighted sum is
no matter what the only thing this
15:30 - perceptron can ever output is a 1 or
negative 1 so and then I can say return
15:35 - output so now I have basically if I kind
of give myself – help! I really want, I
15:46 - guess this is it, I really want to see the
whole thing this is if I have all the
15:50 - code for the almost all the code for the
perceptron a perceptron has a bunch of
15:53 - weights it initializes the weights
randomly and it can perform a guess by
15:58 - receiving all the inputs doing a weighted sum
that's passing through the activation
16:01 - function. Ok so now if I were to just
create something arbitrary just to sort
16:06 - of test if this is working I'm going to
say float inputs equals I'm going to just
16:14 - create some random values like negative
1 0.5 and I'm going to say print line Oh
16:21 - first I'm going to say I'm going to have
a perceptron I'll just call it P for
16:28 - perceptron P equals a new a new
perceptron and then I can say P guest
16:35 - inputs and I can say output I can say
guess and I can say what's wrong here
16:49 - what doesn't it not like oh so this
should be sorry
16:54 - that should syntax wise that should
these are the input that's an array and
16:57 - I should say sorry a print line guess so
if I run this – how come I can't – here we go
17:09 - if I run this we should see, oh it outputted a 1, let's run it again I got a 1 run
17:18 - it again eventually I should be able to to run
it a bunch of times and I got a negative
17:25 - 1 negative 1 okay so this I believe is
working the system works I have a
17:32 - perceptron object I could
feed-in inputs and I can get – make a guess.
17:39 - Okay so we have the overall structure
now for the perceptron and it works but
17:46 - we need some we need to do more so
here's the thing we need to create an if
17:53 - I had an actual data set if I were to
try to classify flowers and these were
17:58 - sunflowers and these were days and this
x-axis was like petal length and this
18:03 - was sepal length or something I could
use a real data set here I'm going to do
18:06 - something really phony-baloney I think
I'm going to like kind of almost be
18:11 - really ridiculous about it which is that
I'm going to say I'm going to actually
18:17 - just say that anything do like do I
really want to do this I'm going to do
18:21 - it this way let's do anything that is
18:26 - let's just consider the line y equals x
anything that's above y equals x is a
18:33 - plus 1 anything that's below y equals x
should be a negative 1 so I want to
18:38 - create a known data set a known data set
that I could use to train the perceptron
18:44 - so let's do that really quickly what I'm
going to do is, let me think about this, I'm
18:54 - going to – there's so many different ways
I could do this
18:58 - whoops sorry it got stuck in my eye, uh, so many different ways I could do
19:03 - this I'm going to do I'm going to make a
tab called training and I'm going to
19:09 - make a class called point and the point
is actually just an input array that has
19:22 - three values in it no no no no let me
think about this
19:31 - let me actually have the point have an X
and a Y and also a I don't want to call
19:41 - it class a label we'll just call it a
label okay so if I make a new point when
19:50 - I make a new point I'm going to say x
equals a random with y equals random
19:57 - height – am I going to run into trouble
without just using the pixel
20:00 - coordinates? Let's try just using the pixel
coordinates, I don't know if that's going
20:03 - to be a problem. And then the label I
could say if X is greater than Y the
20:14 - label is one else the label is negative
one right that should give me that
20:26 - should give me everything about I don't
know what's above what's below so let's
20:29 - do that and then let's do let's write a
little function called like show where
20:35 - I'm going to say stroke zero and I'm
going to say if label equals one fill
20:46 - 255 else fill a zero and then I'm going
to draw a little ellipse at X comma Y a
20:57 - small ellipse okay so what I'm going to do
is I'm going to make an array of points
21:09 - I'm going to make a hundred of them and
I'm going to initialize them
21:23 - and I'm going to say points index i is a
new point and then I let's make the size
21:32 - a little bit bigger 500 comma 300 then
I'm going to do a background 255 and for
21:42 - every point in points this is an
enhanced loop in Java for every point in
21:48 - points I'm going to say p.show() so if I
run this we can see yeah I mean let's
22:02 - make this a square so it looks a little
less weird and I can also draw just to
22:09 - sort of like see correctly I'll draw a
line from 0 0 to width comma height so
22:18 - you can see I picked all these points
half of them are on this side and half of
22:24 - them are on this side alright so now that
I have all these points and I can see
22:29 - them correctly categorized this is my
known training data so what I need to do
22:36 - process wise is I need to take all of my
known training data one at a time I need
22:42 - to pass it in ask the perceptron to
give me a guess. Is it in one is it in
22:48 - one or is it in negative one? And then I
need to do something based on whether
22:53 - it's correct or incorrect. What is it
that I need to do? Okay let's establish
23:00 - something there are still some missing
pieces here I need to that I – that I
23:05 - need to add but I'm going to keep going
let's talk about training let's talk
23:08 - about supervised learning. Here's what I
need to do I need to take all of that
23:15 - known data. I'm going to take each and
every piece of known data I'm going to
23:23 - take the X&Y point and I'm going to pass it in, again it's
going to do the weighted sum it's going
23:27 - to it's going to pass through the
activation function and it's going to
23:29 - guess plus 1 or negative
fun – one. Plus fun or negative fun! Are you
23:34 - having we're having some negative fun
right now I'm pretty sure. So this is
23:39 - going to give me a guess but I also have
the answer, right?
23:46 - So I have both the
23:48 - perceptron's guess and I have the answer.
If I have both of those things I can
23:57 - compute something known as the error. The
error I can think of as let's let's say
24:03 - the answer, I always get this wrong
backwards one way or the other but it's
24:08 - the difference between the correct value
and the incorrect
24:13 - sorry – between the correct value
and the guess right because if it's if the
24:18 - guess is a plus one and the answer is a
plus one what's that error? 1 minus 1
24:23 - that error is 0 and I actually have a
little bit of a diagram here over here
24:28 - where you know you can sort of see these
are the only possible correct answers
24:34 - negative 1 or plus 1 so there's only
four possibilities if it's supposed to
24:38 - be a negative one I could guess a
negative one or a plus one if it's
24:41 - supposed to be a plus one I could guess
a negative one or a plus one so these
24:45 - are all the possible errors the error is
either zero or the error is negative 2
24:50 - or positive 2 or 0 so this is a start
this is a good starting point I need to
24:55 - have this error the idea here oh come
back let me come back to the whiteboard
24:59 - okay so remember what I'm trying to do
is find the optimal weights so
25:08 - ultimately what I want to do is I want
to figure out if I want to say well the
25:14 - weight should equal itself plus some
change in weight. I want to adjust the
25:22 - weight if there's a mistake I want to
make a tweak I want to like make the
25:26 - weight a little bit higher or a little
bit lower right because maybe my
25:29 - weighted sum got me below negative 1 but
it should be a plus 1 if I make that
25:33 - weight higher maybe that will push the
output up to
25:36 - positive so the issue becomes how do I
calculate that Delta weight? How should
25:42 - this weight change based on – how should
the weight change for weight zero for
25:49 - weight one and if there were many many
many more weights? So the way that this
25:54 - is calculated with it is with a process
called gradient descent and I have a
25:58 - couple videos where I go through this in
in pretty large detail one way of
26:04 - thinking about this which I'll kind of
duplicate here in this video is this
26:07 - really relates back to a lot of my
steering examples so I have all these
26:12 - steering examples where I have a vehicle
that has a given velocity and it's
26:16 - seeking a given target so this vehicle
has a velocity and it also has a desired
26:23 - velocity right because if it should be
going towards that target its desired
26:27 - velocity is to go towards that target so
you can think of just steering formula
26:34 - if you go back to that Craig Reynolds
steering formula the steering formula
26:39 - equals the difference between the
desired velocity the way that I want to
26:44 - go and the current velocity which is
kind of like my guess and if I get this
26:48 - a steering formula if I get the steering
function if I get this steering vector and I
26:54 - add it to the velocity it's going to
cause me to turn and go towards that
26:58 - target so essentially that's what I want
to do here this steering vector is the
27:03 - error the desired velocity is the answer
that's where I want to go the velocity
27:08 - is my guess that's where I'm going right
now
27:09 - I want to steer in the direction of the
error so Delta weight the Delta weight is
27:16 - actually equal to the error multiplied
by the input so it's filtered through
27:22 - the it but what's that error filtered
through the input that's how I change
27:26 - the weight itself so that's the process
that I'm going to do over and over again
27:30 - and I have a slide here that I think
will walk through that and it looks I'm
27:35 - in the wrong keyboard here so this is
the process this is the supervised
27:40 - learning algorithm. Provide the
perceptron with inputs for which there
27:44 - is a known answer and ask the perceptron
to guess. Okay the perceptron guessed
27:49 - what's the error? Is it right or wrong? Is the error zero is it, is it two, is it negative two?
27:54 - Adjust the weights according to the
error and go back and do it again and
27:58 - again and again and this is the formula
the weight is changed according to the
28:05 - air multiplied by the input and there
something called learning rate which
28:08 - I'll get to in a second so let's see now
I've kind of explained that in pieces
28:11 - let me see if I can now add that to the
code so I'm going to here whoops I'm
28:22 - going to now create a function in the
perceptron and I'm not going to call it
28:32 - guess I'm going to call it train. So
this is going to receive some inputs and
28:42 - a target. Right the difference between the guess is the guess something like oh I
28:48 - just want to receive these inputs and
provide a guess with training I want to
28:53 - receive the inputs and the known answer
so I can adjust the weights accordingly
28:57 - okay so the first thing actually that I
should do is just get the guess which is
29:03 - actually guess with those inputs so
since I already have a function that
29:08 - does the guess I can ask for the guess
from that guess then what I want to do
29:13 - is get the error the error equals the
error equals the target minus the guess
29:24 - that's the error so now that I've done
that what I can do is I can go through
29:31 - all of the weights and say each weight
should change according to that error
29:44 - multiplied by its corresponding input so
this is that particular algorithm this
29:50 - is tune all the weights. Okay so this is
like basically supervised learning says
30:01 - put the data in, get the result, if the
result is right just move on move along
30:07 - nothing to see here. If the result is
wrong twist some dials in here to try to
30:10 - get it closer to the correct answer and
do it again and again and over and over
30:14 - again here keep twisting dials
eventually to find that optimal result
30:17 - now there's something important here
though. If I go back to this steering
30:22 - example you could think about okay so
this is the vehicle it's going in this
30:25 - direction it should seek the target it
knows what the error is the error is the
30:31 - difference between the way it should go
and its current velocity how much should
30:35 - it steer? If I steer a lot I could
actually like overshoot and start going
30:40 - the wrong way in the other direction but
if I steer just a little bit maybe I'm
30:44 - going closer – but I'm, but you realize that we're going
to doing this with lots and lots of data
30:49 - so one thing that's actually optimal
here is not to steer the full amount all
30:54 - the way according to the error but just
some percentage of the way and that
30:58 - percentage is referred to it's a key
concept here and it's called where's a
31:02 - learning rate. So what I would actually
do here is say that Delta weight what's
31:08 - this plus 1 here is the error times the
input multiplied also by a learning
31:16 - rate. So that's a key concept here so
let's add that into our code if I come
31:23 - back over here I'm at the perceptron is
going to have a variable called LR for
31:28 - learning rate I'm just going to say 0.1 so now I'm going to say also
31:32 - multiplied by learning rate. So there we
go now this is going to adjust all of
31:38 - the weights so we should be able to if I
go back to here I should be able to now
31:46 - train the perceptron. So let's go here
and say for all of the points oh I did
32:00 - something terrible though I was doing
some awful stuff here which is not I
32:03 - used P for perceptron and then I'm using
a local variable P for point so let's
32:08 - call this PT for points let's actually
just call this I'm going to call
32:12 - perceptron I'm going to call this like
the brain
32:16 - probably like a bad variable name but at
least so it says something more than P
32:22 - so for every point what I want to do is
I want to say brain.train the point the
32:34 - inputs associated with that point, and
the target pass in the target associated
32:40 - with that point right? The point oh it
has an X and a Y and a label so uh-huh
32:49 - so the target is actually the label and
can I do this I want to make something
32:55 - called inputs which is an array which
just has point.x and point.y in it.
33:05 - Is Java going to let me write that? I
33:09 - think so so this is what I want to do I
want to train the perceptron I want to
33:14 - send in every point as input so X the x
and a y make the zero on the one input
33:20 - and then I want to send it into the
train function with the label which is
33:24 - the known answer so if I do that okay so in theory it's training and it's doing
33:31 - this I can't see anything so now I need
some sort of way of tracking how well
33:37 - it's doing. This is going to be tricky
but I have an idea for how to do this.
33:43 - Somebody the chat just pointed out that you know why am I doing the uh, why am I
33:49 - doing the same loop twice yeah that's
kind of unnecessary here but I'm
33:52 - really just trying to separate out
different parts of the code ultimately I
33:56 - probably don't need this original loop
but so there's a couple things I do one
34:01 - is I could actually calculate the
overall error like I could actually look
34:04 - and this wouldn't be such a bad idea and
just sort of print that out I could I
34:07 - could calculate the total squared error
and kind of evaluate how well it's doing
34:12 - but I want to just actually look at it
visually for a second so I think what I
34:17 - want to do is let me see here what's a
good way so I'm going to just actually
34:23 - say guess equals inputs
brain.guess(inputs) and then I'm going to
34:32 - say if guess equals point.label let's
just call this looks like target equals
34:41 - points points label just to have these
in separate variables if guess equals
34:50 - target what I'm going to do is I'm going
to draw um, I'm going to say I'm going to
34:59 - draw something that's green I'm going to
say no stroke I'm going to draw an
35:05 - ellipse at X – like I just keep typing
p5 – Y that's like a smaller size else
35:20 - fill red. Well everything became green
instantly that can't be right
35:37 - all right let's not train it. Okay so we
can see I guess it's just working better
35:49 - than I had imagined I'm trying to think
visually if there's a better this is
35:58 - like okay so let me let me make the
window bigger and let me make all of the
36:05 - points let me make all the points much
bigger so it's a little easier to see
36:12 - and let's run this again okay so you can
see without any training everything is
36:24 - wrong so I'm not if I add the Train
function back in everything is correct
36:34 - now I guess it just like this is such a
simple scenario it's like trained and
36:40 - worked so so quickly in like a matter of
like one or two iterations me I am so me
36:46 - in the chat had an excellent suggestion
which I could demonstrate the training
36:49 - process with a mouse click so I'm going
to quickly do that so what I'm actually
36:53 - going to do is I'm going to take this
training out here I'm going to take this
36:57 - whole loop and they'll just delete this
line of code that actually does the
37:03 - training and I'm going to run it so now
it's not training and you can see it's
37:07 - got every time it you know when I run
this it's all wrong
37:12 - right but what I can do now is I can say
void mousepressed and I can just run the
37:24 - training algorithm so now what I did is
only when I click the mouse is it going
37:28 - to run through all the data and actually
adjust the weights so now if I run this
37:32 - you can see you can see look at this
it's got most wrong but it's got some
37:37 - right, it's got most right but some wrong
and now if I click the mouse ah a few
37:41 - more correct but some more wrong click
the mouse again OOP click the mouse
37:45 - again click the mouse again click the
mouse again so you can see it's like
37:48 - changing and eventually now it's got
everything
37:50 - right and so you can see how that
learning process happens over time it
37:55 - only took five or six cycles. One other
thing that I could do is just train one
38:00 - point at a time so I would draw
everything but I could here I could say
38:05 - I could say int training index equals
zero so what I'm going to do in draw now
38:13 - is I'm going to say point training
equals points training index so I'm just
38:23 - going to take one point and train it off
at one point okay and then what I'm
38:32 - going to do is I'm going to say training
index plus plus and if training training
38:40 - index equals points.length like if I get
to the end of the array I will just
38:46 - reset training index back equal to zero.
So this now in the draw loop what did I
38:52 - get wrong here I'm just a spelling
training wrong Oh double equals sorry so
38:57 - now in the draw loop it should be
training one point at a time and you can
39:01 - see like it's kind of weird all sorts of
weird flashy stuff is happening but over
39:05 - time it should eventually settle into at
getting everything correct but as it
39:10 - makes these little adjustments it's
going to get some things wrong and some
39:13 - things right and you can see now so
again there's so many other ways you can
39:16 - think about visualizing the training and
you could actually visualize the one
39:19 - thing you might try is actually
visualize the perceptron itself like
39:22 - visualize the weights visualize the
connection visualize the data flowing
39:25 - through it so I'll leave that to you
creative people watching this video so
39:29 - I'm coming towards the end of this
particular video and I think I need to
39:34 - do I'm going to give you some exercises
and I think I'm going to do a follow-up
39:36 - one that kind of adds I'm gonna give you
some exercises and then I'm going to
39:41 - come back and do another video where I
add this these answers in but there's a
39:45 - couple things here one is I've got a
serious problem a very serious problem
39:50 - with everything that I've done here
let's just say I'm going to let's say
39:56 - that this space that I'm in
40:04 - is actually this. 0 0 forget about pixel
space or anything there's a coordinate
40:12 - space where zero zero is in the middle
right here and I want to categorize us
40:17 - data as above or below this line or
maybe you know I want to use the same
40:24 - system but I want to categorize data as
above or below this other line so I want
40:32 - to use a perceptron to categorize the
same exact code to categorize both of
40:36 - these two scenarios well in one case in
the orange case zero zero should be a
40:42 - plus one in the black case zero zero
should be a negative one but notice
40:48 - something here if I'm actually feeding
in 0 0 into this perceptron system no
40:56 - matter what the weights are the only
thing I can ever get out of here is a 0
41:02 - this is a problem because sometimes 0 0
is going to be above or below, sometimes it's
41:09 - going to be in class A sometimes it's going to be in
class B that can't possibly be right and
41:13 - this is where this idea of a third – I'm
going to go and get another color here
41:19 - this perceptron will actually not work
or perform correctly with this genetic
41:24 - generic scenario other than with having
something called a bias so I need
41:36 - another input, a third input into the
perceptron that is always going to be a
41:44 - 1 input 0 is X input 1 is y and the bias
is 1 and actually if you think about
41:52 - this this is really I'm really working
through the same problem that's from my
41:58 - linear regression with gradient descent
videos or what I'm trying to do is solve
42:03 - the formula I'm trying – neural
networks are designed to generalize a
42:10 - function to solve a function and in
this case, this very simplistic
42:16 - scenario it's actually just looking to
solve the formula for this line y
42:20 - equals MX plus B and M being the slope
is kind of like rise over run and B
42:27 - being the y-intercept so this weight the
biased weight is really there to solve
42:34 - that y-intercept and these weights are
really solving the slope the rise over
42:41 - run so that's what we're doing we're
essentially doing linear regression with
42:45 - gradient descent again but just through
this perceptron model so if you didn't
42:51 - watch those other videos you could go
back and watch them to some connect all
42:54 - these systems so what I want you to do
is I have an example here from the
43:02 - nature of codebook this is essentially
the same code that I've been writing but
43:08 - it does two different things one is it
visualizes what the brain thinks the
43:15 - current line is and also it adds that
bias so if you're watching this video
43:22 - I'm going to release the code for this
video this I get to part one of a
43:26 - perceptron coding challenge if you're
watching this video you could go right
43:30 - to the next one and I'm going to add the
bias in and maybe do a bit more with
43:34 - sort of visualizing what's going on here
and have a generic formula for a line
43:39 - but what I might suggest you do is see
if you can add those things yourself to
43:43 - this particular perceptron and then when
I get to the end of the next video I'm
43:47 - going to talk about why this is such a
limited system that can barely actually
43:50 - do anything meaningful in machine
learning but it could be a building
43:54 - block for a much more complex system
that can do a lot more interesting and
43:57 - powerful things so I hope you've got a
sense of what a perceptron is what the
44:02 - algorithm for how a perceptron works is
and how the feed-forward supervised
44:07 - learning training process of a model
perceptron works because this exact
44:13 - building block this scenario is what I'm
going to use in the future videos
44:19 - restart to build more complex neural
networks ok thanks for watching and I
44:22 - look forward to your feedback and
thoughts in the comments
44:31 - you