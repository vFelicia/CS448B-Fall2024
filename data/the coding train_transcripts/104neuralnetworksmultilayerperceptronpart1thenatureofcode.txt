00:00 - hi again so maybe you just watched my
00:02 - previous videos about coding of
00:04 - perceptron and now I want to ask the
00:08 - question why is not just stop here so
00:13 - okay so we have this like very simple
00:15 - scenario right where we have a canvas
00:19 - and it has a whole bunch of points in
00:21 - that canvas or a Cartesian plane
00:23 - whatever we want to call it and we threw
00:25 - a line in between and we were trying to
00:27 - classify some points that are on one
00:30 - side of the line and some other points
00:32 - that are only another side of the line
00:34 - so that was a scenario where we had to
00:37 - be single perceptron the sort of like
00:38 - processing unit we can call it the
00:41 - neuron or the processor and it received
00:43 - inputs it had like x0 and x1
00:48 - we're like the x and y coordinates of
00:50 - the point it also had this thing called
00:52 - a bias and then it generated an output
00:57 - each one of these inputs was connected
01:00 - to the processor with a wait no wait one
01:05 - way to or never wait wait wait and the
01:08 - processor creates a weighted sum of all
01:11 - the inputs multiplied by the weights
01:13 - that weighted sum is passed through an
01:15 - activation function to generate the
01:19 - output so why isn't this good now let's
01:26 - first think about what what's what's the
01:29 - limit here so the idea is that what if I
01:32 - want any number of inputs to generate
01:36 - any number of outputs that's the essence
01:40 - of what I want to do in a lot of
01:43 - different machine learning applications
01:45 - let's take a very classic classification
01:48 - algorithm which is to say okay well what
01:50 - if I have a handwritten digit like the
01:53 - number 8 and I have all of the pixels of
01:56 - this digit and I want those to be the
01:59 - inputs to this perceptron and I want the
02:02 - output to tell me a set of probabilities
02:06 - as to which digit it is so the output
02:11 - should look something like
02:12 - you know there's a point one chance it's
02:15 - zero there's a point two chances of one
02:17 - there's a point one chance' two two zero
02:20 - three four five six seven oh it it's
02:24 - like a point ninety nine chances at
02:25 - eight and 0.05
02:27 - chance it's a ten and I don't think I
02:30 - got those to add up to one but you get
02:32 - the idea so the idea here is that we
02:35 - want to be able to have some type of
02:37 - processing unit that can take it
02:39 - arbitrary amount of inputs like maybe
02:42 - this is a 28 by 28 pixel image so
02:46 - there's 784 grayscale values and instead
02:50 - those are coming into the processor
02:52 - which was wait is it sudden and all this
02:54 - stuff when we get an output that have
02:55 - some arbitrary amounts of probabilities
02:57 - to mitla help us guess eight not that
03:00 - this is an eight this model why couldn't
03:05 - I just have a whole bunch more inputs
03:07 - and then a whole bunch more outputs but
03:09 - still have one single processing unit
03:11 - and the reason why I can't is a stems
03:16 - from an article I don't know I'm sorry a
03:18 - book that was published in 1969 by
03:21 - Marvin Minsky and Seymour Papert paper
03:23 - call of perceptrons you know AI
03:26 - luminaries here in the book perceptron
03:29 - Marvin Minsky and Seymour Papert point
03:35 - out that a simple perceptron the thing
03:38 - that I built in the previous two videos
03:41 - can only solve linearly separable
03:44 - problems so what does that mean anyway
03:47 - and why should you care about that so
03:49 - let's think about this this over here is
03:54 - a linearly separable problem meaning I
03:57 - need to classify this stuff and if I
04:02 - were to visualize all that stuff I can
04:04 - draw a line in between this part of the
04:09 - day this this stuff is to this class and
04:12 - this stuff that's with this class the
04:14 - stuff itself is separable by a line in
04:16 - three dimensions
04:18 - I could put a plane and that would be
04:19 - literally separable because I can kind
04:22 - of divide the space in half and and
04:24 - and understand it that way the problem
04:28 - is most interesting problems are not
04:31 - linearly separable you know there might
04:34 - be some Dana which clusters all here in
04:38 - the center that is of one class but
04:40 - anything outside of it is of another
04:42 - class and I can't draw one line to
04:45 - separate that stuff and you might be
04:47 - even thinking but that's you know still
04:49 - so much you could do so much with
04:51 - linearly separable stuff well here I'm
04:56 - going to show you right now a particular
04:58 - problem I'm looking for an eraser
05:01 - logging around like a crazy person I'm
05:04 - going to show you a particular problem
05:05 - called X or I'm making the case for why
05:10 - we need to go a step further I just had
05:13 - an idea let's go back to the litter
05:15 - I'm thinking the case for why we need to
05:17 - go to a close go a step further and make
05:21 - something called a multi-layer
05:23 - perceptron and I'm going to lay out that
05:25 - case for you right now so you might be
05:27 - familiar you might remember me from my
05:30 - videos on conditional statements and
05:32 - boolean expressions well in those videos
05:34 - I talked about operations like and and
05:38 - or which in computer programming syntax
05:41 - are often written you know double
05:43 - ampersand or two pipes the idea being
05:46 - that if I were to make a truth table
05:51 - true true false false so what I'm doing
05:58 - now is I'm showing you a truth table I
06:00 - have two elements I'm saying what if I
06:07 - say a and the B so if a is true
06:13 - well this makes no sense what I've drawn
06:16 - here because I'm losing my brain cells
06:20 - slowly over time with every passing any
06:24 - first not true false true false true
06:29 - hand true yields true if I am hungry and
06:34 - I am thirsty I shall go and
06:38 - have budge right true and true yields
06:43 - true true and false is false false and
06:46 - true is false false itself is false
06:49 - right if I have a boolean expression a
06:51 - and B I need both of those things to be
06:53 - true in order for me to get through
06:56 - interestingly enough this is a linearly
06:59 - separable problem I can draw a line
07:02 - right here and true is on one side and
07:05 - false is on the other side this means if
07:08 - this is a linearly separable problem
07:10 - which means I could create a perceptron
07:12 - that perceptron is going to have two
07:14 - inputs there are going to be boolean
07:16 - values true or false for a false and I
07:19 - could train this perceptron to give me
07:21 - an output which if two truths come in I
07:24 - should get it true if one false to the
07:26 - true comes David I should get a false -
07:27 - false coming I should get a false great
07:30 - or I could do the same thing what is or
07:32 - change in two if I'm going to do or me
07:36 - erase this dotted line and or now all of
07:40 - these become true because with an or
07:42 - operation A or B I only need one of
07:46 - these to be true in order to get true
07:50 - but if both are false I get false and
07:52 - guess what still a linearly separable
07:54 - problem and is literally separable or is
07:59 - literally separable we could have a
08:01 - perceptron learn to do both of those
08:04 - things now hold on a second
08:08 - there is another boolean operator which
08:12 - you may you might not have heard up
08:14 - until this video which would be really
08:16 - kind of exciting for me it would make me
08:17 - very happy if somebody watching this
08:18 - never heard of this before it is called
08:21 - x4 can you see what I'm writing near X
08:24 - or the X stands for exclusive exclusive
08:30 - its exclusive or which means it's only
08:36 - true if one is true and what is false
08:38 - it's not true both are false this or
08:42 - that both those things are false I'm
08:44 - still false but if both are true it's
08:46 - also false so this is exclusive
08:49 - or let me erase all this exclusive or I
08:58 - mean if one if one is true and one is
09:02 - false it's true if one is true is one
09:04 - assault is true if both are true it's
09:06 - false if both are false it's false this
09:10 - is exclusive or a very simple boolean
09:13 - operation however I triple dog dare with
09:20 - the cherry on top you to draw a single
09:23 - line through here to divide the false in
09:27 - the truth I cannot I can draw if this is
09:29 - not a linearly separable problem this is
09:31 - the point of all this like rambling I
09:33 - could draw two lines one here and now I
09:39 - have all the truths in here and the
09:40 - false is outside of them this means a
09:42 - single perceptron the simplest cannot
09:46 - solve cannot solve the simple operation
09:50 - like this so this is what Minsky and
09:52 - Papert talked about in the book
09:54 - perceptrons well this is like an
09:56 - interesting idea conceptually it kind of
10:00 - seems very exciting but if it can't
10:01 - solve X or what are we supposed to do in
10:04 - this the answer to this is a new bike
10:08 - I've already thought of this yourself
10:09 - it's not two but I kind of missed a
10:12 - little piece of my diagram here right
10:13 - let's say this is a perceptron that
10:18 - knows how to solve and and this is a
10:23 - perceptron that knows how to solve for
10:26 - what if I took those same inputs and
10:29 - sent them into both and then I got the
10:33 - output here so this output would give me
10:38 - the result of and and this output would
10:41 - give me the result of or well what is
10:44 - XOR really XOR is actually or but not
10:50 - and right so if I could solve something
10:54 - and is linearly separable not and is
10:57 - also linearly separable so what I want
11:01 - then is for both of these out
11:02 - but actually to go into another
11:04 - perceptron that would then be and so
11:11 - this project run can solve not and and
11:13 - this perceptron can solve or and those
11:15 - output can come into here then this
11:17 - would be the results of both or is true
11:21 - and not and is true which is actually
11:23 - this these are the only two things where
11:25 - or is too but not in but not and and so
11:29 - the idea here is that more complex
11:31 - problems that are not linearly separable
11:34 - can be solved by linking multiple
11:37 - perceptron together and this is the idea
11:40 - of a multi layered perceptron we have
11:47 - multiple layers and this is still a very
11:49 - simple diagram you could think of this
11:51 - almost as like if you were designing a
11:53 - circuit right if you decide what
11:55 - electricity should flow and because
11:57 - we're like a these were switches you
12:00 - know how could you get a bunch of outfit
12:02 - you have an LED turn on with exclusive
12:06 - or you would actually water the circuit
12:08 - basically in exactly this way so this is
12:12 - the idea here so what I am would like to
12:14 - do in the next so at some point I would
12:17 - like to make a video where I actually
12:18 - just kind of build take that previous
12:20 - perceptron example and just take a few
12:21 - steps farther to do exactly that but
12:24 - what I'm going to do actually in the
12:26 - next videos is diagram out this
12:29 - structure of a multi-layer perceptron
12:32 - how the inputs how the outputs work how
12:35 - the feed-forward algorithm works where
12:37 - the inputs come in get x weights get
12:40 - some together and generate an output and
12:42 - build a simple javascript library and
12:45 - has all the pieces of that neural
12:47 - network system in it okay so I hope that
12:51 - this video kind of gives you a nice
12:53 - follow-up from the perceptron in a sense
12:55 - of why this is important and I'm not
12:58 - sure if I was done yet I'm going to go
12:59 - check the live chat then questions are
13:02 - important things like this
13:03 - this video will be over oh yeah back so
13:05 - there was one question which is
13:06 - important like Oh what I heard somebody
13:08 - in the chat asked what about the hidden
13:10 - layer and so this is jumping ahead a
13:12 - little bit because I'm going to get to
13:13 - this in more detail in the next video
13:14 - there's a way that I drew this diagram
13:16 - is pretty awkward let me try to fix this
13:18 - up for a second
13:21 - imagine there were two inputs and I
13:24 - actually drew those as if they are
13:26 - neuron and I know I'm out of the frame
13:28 - but I'm still here and these inputs were
13:31 - connected to each of these perceptrons
13:35 - each was connected and each was weighted
13:38 - so this is actually what's now known as
13:41 - a 3 layer Network there is the input
13:45 - layer this is the hidden layer in the
13:51 - reason why it will actually reduce the
13:52 - output layer right that's obvious right
13:55 - this is the input those are the inputs
13:57 - the trues and falses this is the output
14:00 - layer that should give us a result
14:02 - are we still true or we false and then
14:06 - the hidden layer are the neurons that
14:08 - sit in between the inputs and the
14:11 - outputs and they're called hidden
14:12 - because as a kind of user of the system
14:15 - we don't necessarily see them a user of
14:17 - the system is speeding in data and
14:18 - looking at the output the hidden layer
14:20 - in a sense is where the magic happens
14:22 - the hidden layer is what allows one to
14:25 - get around this sort of linearly
14:26 - separable question so the war in layers
14:31 - the more neurons the more amount of
14:34 - complexity in a way that the system the
14:35 - more waits the more parameters that need
14:37 - to be tweaked and we'll see that as they
14:39 - start to build a neural network library
14:41 - the way that I want that library to be
14:43 - set up I want to say I want to make a
14:44 - network with ten inputs three outputs
14:47 - one hidden layer with 15 like hidden
14:50 - neurons something like that but there
14:51 - could be multiple hidden layers and
14:52 - eventually as I get further and further
14:55 - down this road if I keep going we'll see
14:57 - there are all sorts of other styles of
14:59 - how the network can be configured and
15:01 - set up and whether it's the output feeds
15:03 - back to to the input that's something
15:04 - called recurrent networks convolutional
15:06 - Network and if some this kind of like
15:08 - said image processing operations almost
15:11 - happens early on before as part of the
15:13 - layers so there's a
15:14 - a lot of stuff in the grand scheme of
15:16 - things to get to but this is the
15:18 - fundamental building blocks so okay so
15:21 - I'm in the next video I'm going to start
15:23 - building the library and to be honest I
15:25 - think what I need to do
15:28 - yeah the next video I'm going to set up
15:32 - the basic skeleton of the neural network
15:34 - library and look at all the pieces that
15:36 - we need and then I'm going to have to
15:38 - keep going and look at some matrix math
15:40 - that's going to be fun okay see you soon
15:46 - [Music]