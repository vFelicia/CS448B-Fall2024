00:00 - hello welcome to part 2 of my neuro
00:06 - evolution series where I am attempting
00:08 - to look at how I can train a neural
00:11 - network or more accurately a population
00:14 - of neural networks with a genetic
00:16 - algorithm
00:17 - I talked about this in the previous
00:18 - video which you could go back and watch
00:20 - if you haven't but the main thing that I
00:22 - want to do in this video is look at okay
00:24 - well if I have a neural network if I
00:26 - have a neural network how can I apply
00:30 - crossover or a mutation to that neural
00:32 - network so that will be the focus of
00:33 - this video I do sort of remember though
00:35 - I did kind of remember that there's a
00:37 - lot of stuff that I haven't talked about
00:38 - yet because one of the reasons why I use
00:40 - the neural network is to be able to give
00:42 - it some inputs and to get some output so
00:44 - that makes sense in the context of the
00:46 - dodol classification example probably
00:49 - but may not make sense to you
00:50 - immediately in terms of this idea of a
00:52 - flappy bird game so I will get to all
00:54 - that but right now I'm gonna focus on
00:56 - saying new neural network and you know
00:59 - what I'm gonna stick with copy like is
01:01 - that I'm gonna get to crossover in a
01:02 - future video I'm gonna stick with copy
01:04 - for simplicity and mutation so let's go
01:06 - back let's go over to the code and let's
01:08 - take care of that now let's start by
01:16 - having I'm gonna create a variable
01:17 - called brain and I'm use I'm in the p5
01:21 - I'm using I mean using the p5 GS library
01:23 - though what I'm gonna do right now it's
01:24 - totally unnecessary for but because the
01:27 - flappy bird gained this program in p5
01:29 - that's gonna be helpful later also I use
01:30 - it all the time so I don't need a canvas
01:33 - I'm just gonna say you know canvas and
01:34 - I'm gonna say brain equals new neural
01:37 - network now when I create a new neural
01:41 - network object if you remember the three
01:44 - arguments I need to give it and this is
01:46 - just for this particular neural network
01:47 - implementation it'll work differently if
01:50 - you're using you know say like a
01:51 - different machine learning framework
01:53 - like Kerris or somebody else's neural
01:56 - network library maybe you might want to
01:57 - look at deep learned Dutch is I will
01:59 - come back to that in a future video so I
02:02 - need to give it a certain number of
02:04 - inputs well all of a sudden now we're
02:06 - back to that question so let's actually
02:09 - not worry about that right now
02:10 - let's not worry about that question I'm
02:12 - gonna go back to that question and
02:13 - I'm just gonna make something up so
02:14 - let's take the X or example sort of like
02:16 - a trivial example of okay it has two
02:18 - inputs they're either true true true
02:20 - false false true false false so there
02:22 - may be two inputs let's just have a
02:25 - hidden layer with four nodes and one
02:28 - output so we can create some simple
02:30 - neural network and what I want to be
02:32 - able to do is I want to say let a
02:36 - cult-like child equal brain copy like I
02:39 - want to be able to say let me make a
02:41 - copy of that neural network and I also
02:44 - want to say something like child mutate
02:46 - so let me take that copy and apply a
02:49 - mutation in it mutation which is
02:52 - something I described more in the
02:53 - genetic algorithm video series so what
02:55 - this means is this neural network class
02:58 - needs to have two new functions it needs
03:00 - to have a copy function and needs to
03:01 - have a mutate function so let's go into
03:04 - the neural network library code this is
03:06 - the class there's a lot of stuff in here
03:08 - I have a way to many videos going
03:10 - through all this code luckily we can
03:12 - kind of ignore all of this and I'm just
03:14 - going to go down to the bottom here and
03:16 - I'm gonna say something like adding
03:18 - functions for neuro evolution now the
03:24 - truth of the matter is what is it that I
03:27 - really want to copy well if you recall
03:31 - the neural network structure is such
03:34 - that if there are two inputs and four
03:37 - hidden nodes and one output the neural
03:40 - network looks like this it's connections
03:44 - between the inputs and the hidden layer
03:47 - and connections between the hidden layer
03:49 - and the output and these connections the
03:52 - the sort of dials of the neural network
03:55 - the data of the neural network what
03:57 - controls how the information flows from
03:59 - the inputs and out through the output
04:01 - are all of these weights and these are
04:05 - stored in matrices it's just a whole
04:07 - bunch of numbers so I have a weight
04:10 - matrix which goes from input to hidden I
04:13 - have a weight matrix that goes from
04:16 - hidden to output and with each of these
04:19 - I also have this thing called a bias and
04:21 - if you recall the bias is something you
04:24 - know really all I'm doing in the end is
04:26 - like all of this really boils down to
04:28 - like hey there's a whole bunch of points
04:29 - I just fit into those points and the
04:31 - plastic could be like move the line a
04:32 - little bit up move the line a little bit
04:34 - down so that's really even though this
04:35 - all seems like fancy magic ultimately
04:37 - that's what it just boils down to in the
04:39 - end so I also have the bias for the
04:42 - hidden and I have the bias for the
04:45 - output I don't know I don't know what
04:46 - the best way to write the notation for
04:48 - this is or yeah so all of these things
04:52 - when I want to copy the neural network
04:54 - what I'm really saying I want to do is
04:56 - copy all this stuff so let me go ahead
04:59 - and so what I need probably is a
05:01 - function inside of all these are all
05:03 - matrix objects I need a function
05:05 - probably in the matrix class to say copy
05:07 - so let's start here and say copy and you
05:12 - know clone could be used and I want to
05:14 - make a deep copy I think not a shallow
05:17 - copy these terms get thrown around a lot
05:19 - in computer science deep versus shallow
05:21 - but I don't want to like just point to
05:23 - the data I want just giving my own
05:25 - version of all of those numbers because
05:27 - I'm gonna mess around with them and I
05:28 - want you to keep your numbers I want to
05:30 - mess with your numbers so instead of
05:31 - just saying like I I'm a new neural
05:33 - network can I just point over to your
05:34 - numbers I really want a whole version of
05:36 - those so and if we go back to here so
05:40 - things that I need to do is I need to
05:42 - keep track of these properties
05:45 - input/output the total input output and
05:47 - hidden nodes so I want to say let let's
05:50 - just put this in here so I want to say
05:52 - let input nodes equal this dot input
05:55 - nodes let hidden nodes equal this dot
05:59 - hidden nodes and let output nodes equal
06:04 - this dot I'll put in you know what this
06:06 - this is very poorly named this is sort
06:10 - of silly what I'm doing but I'm going to
06:12 - do it anyway input nodes this is very
06:17 - this is me this is how I like to code I
06:19 - like to make things as long-winded and
06:21 - as possible so that I can really think
06:22 - it through and explain it like all I'm
06:24 - doing right now is taking the properties
06:26 - of the neural net we're going to copy
06:28 - and put them into local variables why
06:30 - because I want to say neural network
06:34 - copy equals new neural network with what
06:40 - I have a better idea how to do this I
06:43 - have a better idea how to do this so
06:47 - this could this could work but I have a
06:49 - better idea
06:50 - so actually what I want to do is I kind
06:54 - of just want to say this return new
06:56 - neural network this now you might be
07:02 - asking me like I mean this is this isn't
07:03 - gonna be mad this isn't just gonna work
07:05 - but what I'm sort of realizing here is
07:07 - maybe I don't want to copy everything
07:08 - here what I actually want to do is call
07:12 - the constructor but give it a reference
07:14 - to the existing neural network and then
07:17 - have that constructor instead of
07:19 - creating a new new wait new wait
07:23 - matrices that are random it'll create
07:25 - wait matrices that are copies of the
07:26 - existing one in other words let me go
07:29 - back up to the constructor and look at
07:33 - this so what if so the constructor gets
07:36 - three things right a B I could just like
07:41 - rename these the parameters of the
07:44 - constructor function for a second and
07:45 - just call them ABC right a being the
07:48 - input nodes B being the hidden nodes C
07:50 - being the output nodes but before I do
07:53 - that what I actually want to say is is a
07:55 - an instance of instance of a neural
07:59 - network else in other words so what I
08:06 - want to do is if if I'm being sent three
08:10 - integers then I want to make the neural
08:12 - network the way I always have however if
08:15 - I'm being sent the first argument and
08:18 - this is this is kind of this is known as
08:20 - overloading typically in a programming
08:22 - language like Java if I had to overload
08:24 - the constructor like there's two
08:25 - different ways I could call the
08:26 - constructor I could give you three
08:27 - numbers to make a brand-new neural
08:29 - network or could give you a neural
08:30 - network to make a copy of yourself
08:32 - I would write two versions of the
08:34 - constructor but in JavaScript you can
08:36 - only have one version of the constructor
08:38 - but you can kind of check what you're
08:39 - passing in and just to be clear about
08:41 - this let me just make sure this instance
08:42 - of thing is correct so if I were to say
08:45 - let a be a new new neural network 4 4 3
08:51 - 2 just arbitrarily
08:53 - and I can say a instance of its without
08:57 - a capital instance of a string I should
09:00 - get false instance of a neural network I
09:04 - should get true okay so that's right
09:05 - this this of should be lowercase though
09:07 - if a is an instance of a neural network
09:09 - then what am i doing
09:11 - then I am saying this dot input nodes
09:15 - equals a dot input nodes like I can
09:18 - start right here's where if it's not a
09:21 - neural network I'm actually assigning it
09:23 - the numbers that are coming in if it is
09:25 - I can just keep going oh actually what
09:27 - am i doing I can say this and this
09:31 - should be hidden and now I can say this
09:37 - maybe I should have somebody has a
09:39 - suggestion for how to name these in a
09:41 - better way I just I didn't want to name
09:42 - them hidden input hit in output anymore
09:45 - because a could be either of those
09:47 - things so you know this this may be like
09:51 - to do document what a be CR uh output
09:59 - nodes that's a little note to myself
10:01 - that I don't like what I've written here
10:02 - and then now let's look at these so I'm
10:06 - not gonna need to randomize the weight
10:07 - matrices because I'm just gonna say
10:09 - equals what am I gonna do here a dot
10:14 - weights dot dot copy write what I want
10:19 - to do is say hey my weights are your
10:20 - weights and now my input to hidden
10:25 - weights are your weights and my hidden
10:28 - to output weights are your weights now
10:29 - is this gonna run I don't think so
10:31 - because I probably have to add a copy
10:33 - method to the matrix object but I'm
10:35 - getting somewhere now what else do I
10:37 - need
10:37 - I need the biases so I need to set the
10:43 - biases so the same thing I'm just doing
10:49 - a lot of like copy/paste stuff here so I
10:52 - need to set the hidden bias values and
10:54 - the output bias values okay so this is
10:57 - me creating this new copy of a
10:59 - previously neural network and then you
11:01 - know right now it looks like learning
11:04 - rate and activation function or at the
11:07 - even though I have different activation
11:08 - functions I can kind of write is this
11:13 - default was getting set to sigmoid its
11:15 - default is getting set to 0.1 so I
11:17 - probably should copy those as well I'm
11:20 - just gonna just be simple about this
11:22 - right now and just assume that my
11:24 - program is never gonna change learning
11:26 - rate or activation function I should
11:28 - that should be a to do to do copy these
11:34 - as well at some point but I don't need
11:36 - to worry about that right now and to be
11:37 - honest the learning rate isn't gonna
11:40 - play a role anymore the learning rate is
11:41 - completely irrelevant the learning rate
11:43 - is specifically a tied to the tied to
11:44 - the gradient descent algorithm which I'm
11:46 - no longer really using with the with the
11:49 - with the genetic algorithm that's what
11:52 - I'm doing now okay we're getting
11:54 - somewhere
11:54 - alright so just out of curiosity
11:56 - remember this is the code I'm making a
11:59 - new neural network and I'm let I know I
12:01 - haven't done mutate yet but is this even
12:03 - working is that was thereby
12:08 - did I maybe in some other universe
12:10 - happened to write a copy function
12:14 - already into the matrix class I
12:15 - seriously doubt it but let's see yeah
12:17 - copy is not a function so what this
12:20 - means is I need to also go into the
12:22 - matrix library and I think this this I
12:25 - think is worth having in here that's not
12:27 - just this isn't exclusive to genetic
12:30 - algorithms are neural evolution like so
12:32 - I'm adding this stuff you know copy and
12:35 - mutate and crossover will be here
12:37 - specifically because of genetic
12:38 - algorithm than the matrix I can the
12:40 - matrix object I can be a little S form
12:42 - about this so what do I want to do I
12:43 - want to write a function copy and what
12:47 - do I do I'm going to say let m equal a
12:52 - new matrix with this dot rows this
12:56 - columns so I create a matrix object with
13:00 - the same number of rows and columns and
13:02 - somebody in the chat I know is going to
13:04 - tell me there's some very fancy way that
13:07 - I could just instantly use some
13:09 - higher-order array function to copy the
13:11 - whole thing over but because I am Who I
13:14 - am I'm gonna say I'm going to write
13:20 - nice little nested loop I can always
13:22 - refactor this later I just know this is
13:24 - gonna work and I'm going to say m dot
13:27 - data index I index J equals this this
13:32 - dot data index I index J so this is
13:35 - manually me looping through the entire
13:38 - matrix it's a grid of numbers it's all
13:40 - the weights of the connection of the
13:41 - neural network and just manually copying
13:43 - them over one by one and I think this
13:47 - will work all right so let's see let me
13:49 - hit refresh there we go
13:51 - so does this work I have two neural
13:55 - networks they both seem to have two for
13:58 - one two for one you know I could go let
14:02 - me look at one of these biases now look
14:04 - at these values so this is bias H these
14:06 - are the values can you memorize those
14:08 - can you remember them let's go down here
14:11 - okay nope nope something is wrong so
14:15 - this stuff did not get copied guess what
14:17 - guess what I forgot I forgot something
14:24 - quite important in this function I did
14:27 - not forget to return the thing that's
14:29 - new but in the matrix I forgot to say in
14:33 - the return n so this new matrix that I'm
14:35 - making I've got to actually return it I
14:37 - made the copy you can make the copy you
14:39 - can take the reservation but you can't
14:41 - hold the return of the copy and I'll
14:44 - sign the full Seinfeld reference for all
14:46 - of you I try should have said that it'd
14:47 - be more interesting if I didn't say that
14:48 - okay so now I can look here and I can
14:53 - see okay look at these numbers memorize
14:56 - these numbers this is the way I actually
14:58 - let me look at I'm just gonna look at
15:00 - one of the biased ones this will be a
15:01 - little bit simpler so here's the bias
15:03 - bias H has these values memorize them
15:06 - memorize them I could write a nice unit
15:08 - test to actually see if this function
15:10 - worked I'll be much better but this has
15:13 - been I ball it 0.20 eight four one five
15:16 - yep these look like the same numbers
15:17 - same numbers say members so I'm gonna
15:20 - just choose at the moment to believe
15:22 - that this worked this is probably a bad
15:26 - idea but this is the reality of what I'm
15:29 - going to do and I'm okay with that so
15:32 - let's move on so we have now
15:34 - implemented copy what I next need to do
15:37 - is implement mutation now I do need in
15:41 - order to do mutation I need to have
15:43 - something called a mutation rate and
15:46 - that mutation rate is essentially a
15:48 - probability of how likely it is for each
15:51 - element of every of sorry each element
15:55 - of every sort of DNA to alter itself
15:59 - randomly when that child DNA is made so
16:01 - what that means in this context is for
16:04 - every single number that's a weight in
16:06 - these matrices for every single number
16:08 - that's in the bias there is a say 1%
16:11 - chance point five percent chance 10%
16:13 - chance then I'll pick a new random
16:15 - number
16:15 - I could also with mutation like nudge of
16:17 - the values so sort of picking an
16:19 - entirely random new number I could like
16:20 - push it a little higher push a little
16:21 - lower but for simplicity right now let's
16:23 - just pick a new random number so what do
16:26 - I need to do I need to go back to neural
16:30 - network J s and I'm gonna add a function
16:33 - called mutate and what am I going to do
16:36 - here I'm going to say there are four
16:39 - things that I need to mutate so I'm
16:42 - gonna say let's think about this what
16:45 - are all those things called there's I
16:47 - just got to go back up to here there's
16:49 - weights ih weights H Oh bias H bias oh
16:52 - so I need to say what I need to do is
16:56 - say weights I'm gonna map a mutation
16:59 - function so remember there's the the V
17:07 - sorry I'm also not able to type the talk
17:10 - at the same time so if you recall the
17:13 - matrix the matrix library has a function
17:17 - called map and what it allows you to do
17:23 - it's a little unfortunate that there's
17:25 - also a JavaScript native function called
17:28 - map which I'm using everywhere it allows
17:29 - you to apply a function to every single
17:33 - element of the array so I can pass in a
17:37 - function and apply it to every single
17:38 - element of the right and the function
17:40 - that I'm gonna pass in is mutate so
17:44 - let's write this function now
17:47 - and it's going to get it receives a
17:50 - value but to be honest I don't care
17:53 - about I I do care about what the value
17:55 - is
17:56 - if I were planning to nudge it higher
17:57 - and nudge it lower but what I'm going to
17:59 - do right now is I'm just going to return
18:01 - a random number so let me actually I
18:05 - sort of for that should probably link
18:07 - these better in some way but when I have
18:09 - this function called randomize and this
18:13 - is the kind of random number that I'm
18:15 - asking for so I am going to just return
18:19 - this ah but am I always going oh I do
18:22 - need the Val guess what if I do this
18:24 - it'll completely randomized every single
18:27 - element so this mutation function needs
18:29 - a mutation rate and what I'm going to do
18:32 - is I'm gonna pick a random number if
18:35 - math dot random is greater than or less
18:38 - than the rate right so only so let's say
18:42 - math dot random will give me a number
18:44 - between zero and one so if the mutation
18:47 - rate is 0.1 that random number between
18:49 - zero and one will be less than point one
18:50 - ten percent of the time
18:52 - otherwise stick with the same value so
18:57 - this is now the function that I want to
18:58 - apply to every element of all of the
19:01 - weight matrices I want to say hey mutate
19:05 - these weights mutate these weights
19:07 - mutate these biases mutate these biases
19:09 - and perhaps there's a more elegant way
19:11 - to write this and I will consider that
19:12 - all in the future with your many
19:13 - comments and pull requests and
19:15 - complaints I look forward to them but
19:17 - this is what I'm gonna leave it at right
19:18 - now so let's see now if what I can do if
19:22 - I go back to this particular program and
19:25 - I say child now just out of curiosity
19:27 - I'm gonna say child mutate one so I'm
19:31 - giving it a mutation rate of one which
19:33 - means everything should be completely
19:34 - random and what I'm gonna do just just
19:37 - as a kind of like a way of testing
19:39 - actually is I'm going to change this to
19:41 - I'm gonna multiply it by like a thousand
19:44 - just cuz I want to see like totally
19:46 - different numbers to see that this is
19:48 - working so I'm gonna go back and I'm
19:51 - going to refresh the code weights i H is
19:53 - not defined oh right I forgot about that
19:55 - this dot that won't surprise any of you
19:57 - you've probably been saying this in the
19:59 - chat all along there we go
20:01 - so let's take a look so here we have
20:04 - once again the biases they're all
20:06 - reasonable numbers between negative 1 &
20:08 - 1 which is how I started the neural
20:11 - network and now if I look at the child
20:15 - neural network and I look at the biases
20:18 - we can see yep so that mutated now let's
20:20 - change the mutation rate let's change it
20:23 - to 0.5 so we have kind of I mean we're
20:25 - not going to get exactly 50% of them
20:27 - because it's supposed to be random but
20:28 - we'll least see some of them didn't you
20:30 - take some of them did so let's change
20:32 - now the mutation rate to 0.5 just to see
20:36 - that this is working this is instead of
20:38 - me writing unit tests manual unit
20:41 - testing let's look here we can see
20:43 - wonderful here's some original values
20:46 - and now let's go down to here in the
20:49 - child object and let's look at the bias
20:53 - ease again and we can see hey it
20:55 - actually worked out exactly as planned
20:58 - it mutated 2 of them and didn't you take
21:00 - two of them which is what a you know
21:02 - most commonly with a 50% probability
21:04 - we're gonna see and I suspect that if I
21:07 - go into the output bias
21:08 - oh the alpha boys just has one value it
21:11 - didn't get mutated right because this is
21:13 - such a simple little neural network if
21:14 - we go into these weights we can see you
21:17 - know 3 out of 4 got mutated I think it's
21:19 - working so we are in good shape here we
21:23 - now and so I want a probably the actual
21:26 - mutation rate I'm going to want to use
21:27 - something like 1% because I want to do
21:29 - it pretty rarely but we now have the
21:31 - ability to both copy a neural network
21:35 - again as the next if you're watching
21:37 - this and the future videos of this
21:39 - tutorial aren't released yet or if you
21:40 - don't feel like watching them just yet
21:42 - try as an exercise to yourself go and
21:44 - implement crossover how could instead of
21:46 - instead of copy could you create a new
21:48 - neural network that's a mixture of all
21:50 - these weight matrices that would be a
21:52 - really interesting thing to try I will
21:53 - do that hopefully in a future video so
21:55 - I've got copy I've got mutation I've got
21:58 - the flappy bird game I've got the neural
22:00 - network library I've added cross or
22:01 - mutation so we're ready now we're
22:03 - actually ready to implement the genetic
22:06 - algorithm I'm going to say this twice
22:07 - I'm someone in the chat pointed out that
22:10 - the neat algorithm
22:12 - evolution augmented topology things
22:14 - refers to a very specific implementation
22:18 - of neuro evolution in a specific paper
22:20 - and obviously being much more informal
22:22 - about this here so technically it's
22:23 - probably isn't the neat algorithm and
22:25 - maybe I'll mention that the beginning of
22:26 - the next video - just to emphasize it a
22:28 - bit more alright thanks for watching and
22:30 - I will see you when I continue
22:36 - [Music]
22:41 - you