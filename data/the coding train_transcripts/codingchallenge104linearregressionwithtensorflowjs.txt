00:00 - hello you are here watching another
00:03 - coding challenge and I you know this
00:05 - coding challenge maybe this should just
00:07 - fit in being one of my tutorial videos
00:10 - but I'm gonna make it a coding challenge
00:11 - because I'm gonna attempt to do it in
00:12 - one video and what I'm doing is
00:14 - recreating something that I've done
00:16 - before in some of my machine learning
00:17 - tutorials and it was it suggested the
00:21 - Arnoff what was suggested exactly but a
00:23 - twitter user a cow's tube old pod car
00:26 - apologies if i pronouncing the name
00:27 - incorrectly created this interactive
00:31 - simulation of linear regression using
00:33 - tensor flow j s and so this is very
00:37 - similar to something that i've done
00:38 - previously right i have this video
00:40 - linear regression with gradient descent
00:41 - where i just did this with plain
00:43 - JavaScript and then you could also look
00:46 - at this other video which i go through
00:47 - the mathematics of gradient descent a
00:49 - little bit but here's the thing going
00:51 - through the mathematics making this
00:54 - video where i implement the mathematics
00:55 - in javascript while useful and perhaps
00:58 - background for this video one of the
00:59 - exciting things about doing this with
01:01 - tensorflow das is tensor photo jess has
01:04 - a nice API for optimizing loss functions
01:09 - with the gradient descent algorithm
01:12 - built into it so I could just do things
01:15 - so let's make a light to come back here
01:16 - but let's make a list alright so first
01:19 - of all what is linear regression anyway
01:23 - so let's say we have a space and I drew
01:26 - this as like a canvas
01:27 - but really I should be talking just
01:29 - about a generic kind of two dimensional
01:32 - Cartesian plane in that plane there are
01:34 - a lot of they're a bunch of points the
01:37 - idea of linear regression is to figure
01:41 - out can we fit oh this is a time for
01:44 - another colored marker can we fit a line
01:49 - into this two dimensional space that
01:51 - approximates all of these points as best
01:53 - we can and I can visually just kind of
01:56 - make myself do this like this so I can
01:59 - eyeball and say this line kind of gets
02:01 - closed what we're trying to do is
02:03 - minimize all of these this is the most
02:09 - beautiful diagram I've ever made all of
02:11 - these distances of all of the points
02:13 - the line the idea here then is that we
02:15 - can make some predictions right if this
02:18 - data if this x-axis you know represents
02:22 - height we might predict on the y-axis
02:29 - weight right you can think of kinds of
02:34 - data sets simple 2d data sets where
02:37 - there's a linear relationship between
02:39 - the 1:1 field of data and another field
02:43 - of data so if we pick a new height we
02:46 - can kind of make a guess approximately
02:48 - what that weight is gonna be that's the
02:50 - idea of linear regression it's
02:51 - incredibly simple a lot of data isn't
02:53 - two-dimensional a lot of data doesn't
02:56 - fit a line you know maybe a curve fits
02:58 - it better and this is more complex
03:00 - scenarios will come as we move forward
03:02 - and make more scenarios with complex
03:06 - polynomial equations our neural network
03:07 - based learning and other types of
03:09 - machine learning algorithms but this is
03:11 - a good place for us to start so what do
03:13 - we need we need a data set so we need a
03:17 - set of X's and Y's this is the data set
03:20 - right we need X's and Y's and I'm gonna
03:25 - create that data set through interactive
03:28 - clicking Interactive clicking is the way
03:31 - I'm gonna create that data set every
03:32 - with the mouse I need to have something
03:36 - called a loss function the loss function
03:40 - is a way of computing the error and
03:43 - there are a bunch of different loss
03:45 - functions and we'll see these as we as I
03:47 - use tensorflow HS in more tutorials I
03:49 - can select different kinds of loss
03:51 - functions for different scenarios but in
03:53 - this scenario I'm going to use a simple
03:54 - basic one which I believe is called
03:56 - root mean squared error did I say that
04:02 - correctly is that the right name of it
04:04 - but the idea is that I want to look at
04:06 - all of those distances okay
04:09 - I'm back as I started talking about the
04:10 - loss function I realized I really didn't
04:12 - draw I'm not actually looking for the
04:15 - the distance from the point to that line
04:17 - which would be perpendicular I'm looking
04:19 - for this vertical distance which is so
04:23 - this is what I'm trying to minimize
04:25 - right I'm trying to minimize
04:27 - and get a line that has and this that is
04:30 - the least dilute the sum of all these
04:34 - distances is the smallest number
04:36 - minimizing the loss so I have a loss
04:39 - function I need that I also need in
04:41 - tensorflow digest something called an
04:44 - optimizer and the optimizer is the thing
04:48 - that allows me to minimize the loss
04:52 - function and in order to do that I also
04:57 - need to have a learning rate so these
05:01 - are all I've actually missed something
05:02 - and very important here but these are
05:04 - all the pieces I need the data I need to
05:06 - define a loss function I need to define
05:09 - an optimizer I say hey optimizer
05:11 - minimize the loss function with this
05:13 - learning rate so keep tweaking the
05:15 - parameters tweaking the parameters so
05:17 - that's the thing I forgot what are those
05:18 - parameters well the formula for a line
05:20 - is y equals MX plus B M is often
05:26 - referred to as the slope B as the back
05:30 - that looked it up M is the slope and B
05:34 - referred to as the y-intercept kind of
05:39 - like bias by the way if you've watched
05:41 - them on other neural network tutorials
05:42 - this is like the thing we're doing with
05:44 - all the neurons oh it's also connected
05:46 - but we're just living in this very
05:48 - simple place so I need these parameters
05:52 - I need these variables because that's
05:55 - what's going to allow me to create the
05:57 - predictions that are on the line to
06:00 - compare with the actual points and
06:02 - compute the loss minimize it tweaking
06:05 - these values so tweak these values
06:06 - minimizing the loss this is what we're
06:08 - doing and I've done this before in great
06:11 - detail this is gonna be in less detail
06:13 - because tension flow digest is gonna do
06:15 - a lot of this for us the thing that's a
06:17 - little extra complicated is we can't
06:19 - just work with arrays of numbers and
06:21 - variables in the way that we're used to
06:23 - in JavaScript and so this is what brings
06:25 - me to if you haven't looked at these
06:28 - particular videos that I've made already
06:30 - what's a tensor flow tensor what's a
06:34 - variable what's in operation how does
06:35 - the memory management stuff this is
06:37 - stuff we're gonna have to lean on while
06:39 - I build this exam
06:40 - and this should be by the way an actual
06:42 - practical example of where I need a TF
06:45 - variable so I kind of in this video
06:47 - explained what a TF variable is it just
06:48 - kind of moved on and didn't use it for
06:50 - anything so hopefully this will show us
06:51 - that all right how about we write some
06:53 - code now so actually I appeared over
06:56 - here for a second because instead of I
07:00 - did make a little mistake here I mean
07:01 - root mean squared is a perfectly
07:04 - legitimate loss function but most linear
07:08 - regression with gradient descent
07:09 - examples will not bother with the root
07:12 - and the root refers to square root we
07:14 - just want the mean squared error which
07:16 - means like if I say that this value is y
07:20 - and this value is like the guests right
07:24 - I the error is guess minus y and squared
07:30 - and if I do that for all of these that's
07:33 - the mean an average them if the mean so
07:35 - I can really sum them or mean them well
07:37 - it's gonna take care of that for us so
07:40 - we're just gonna use the mean squared
07:41 - error but that's the idea we take the
07:44 - differences the reason why we have to
07:45 - square it it will for a variety of
07:47 - reasons one it has to do with the
07:48 - derivative stuff that's in my other
07:50 - videos but also just because positive or
07:52 - negative with the it's the distance that
07:56 - the size of the error whether it's up or
07:58 - down which is key all right so here's
08:01 - the kind of code we're gonna start with
08:02 - I'm using p5 so that I can draw stuff
08:05 - I'm making a canvas and the background
08:07 - is zero which means it's black and this
08:09 - is what I have so far so let's look at
08:12 - our list over here and let's first add
08:14 - the data set the X's and Y's so this is
08:17 - going to be easy because I just want to
08:20 - have X's be an array wise also be an
08:25 - array so and then whenever I click the
08:29 - mouse I want to say oh you know what I
08:33 - could make those vectors let's make them
08:35 - separate arrays I think we're gonna
08:37 - actually I know we're gonna we're gonna
08:38 - want to do that for a variety of reasons
08:39 - we're gonna keep those at separate
08:40 - arrays so every time I click the mouse
08:42 - I'm going to say X's push Mouse X Y's
08:47 - push Mouse Y ah okay here's this here's
08:52 - a little thing
08:54 - so this is our canvas right this is my
08:57 - drawing of the canvas I know I'm having
08:59 - like a DejaVu thing I totally talked
09:01 - about this on the other video the width
09:02 - is like 400 the height is 400 but I
09:05 - really want to think of this as I really
09:09 - want to think of zero zero down here and
09:11 - maybe one being over here oh and
09:13 - normalize everything between zero and
09:15 - one
09:15 - everything's just gonna work better if
09:16 - we do that so with Y pointing up so I'm
09:19 - gonna do a mapping so every Y value
09:22 - that's pixel value between zero and
09:24 - height I'm gonna map between 1 and 0 and
09:26 - every x value that's between 0 and width
09:28 - I'm going to map between 0 and 1 so
09:30 - let's do that so I'm going to say let X
09:35 - equal map mouse X which goes between 0
09:39 - and width 2 between 0 and 1 let Y which
09:45 - is Mouse y between 0 and height and have
09:49 - that go between 1 and 0 and then push X
09:53 - and push Y so the other thing I want to
09:57 - do is I just want to UM you know I'm
10:00 - gonna add a draw loop and I want to draw
10:03 - all those points so I'm also now gonna
10:06 - say stroke 255 stroke wait for 4 let I
10:14 - equal what huh let a equal 0 I is less
10:18 - than X dot link I plus plus and those
10:23 - are actually called X's X's dot length
10:27 - and what I'm gonna do is I'm gonna say
10:30 - let pixel X equal map I really should
10:32 - make just like I probably do this a lot
10:34 - so I should probably make a function
10:35 - that just like normalize and
10:37 - unnormalized or denormalize pix px
10:41 - equals map X's index I which goes
10:43 - between 0 and 1 back to 0 and width so
10:46 - this is the reverse py which maps Y
10:49 - which goes between 0 & 1 to height comma
10:55 - 0 and then I want to say a point P X py
11:00 - so I haven't done any I'm even I'm not
11:02 - even using tensorflow j s yet
11:04 - I'm just kind of doing the stuff with p5
11:07 - to draw things so let's see if I'm
11:09 - getting the results that I want which is
11:11 - whenever I click I get the points
11:13 - they're perfect and I kind of want to
11:15 - see them a bit more that's like really
11:17 - make it bigger
11:17 - great that's like too big okay
11:21 - great so we can see those those are the
11:23 - points I'm clicking on okay so what's
11:26 - next
11:26 - I need a loss function I need an
11:29 - optimizer ah oh I need these let's make
11:33 - these so cuz I'm looking for somewhere
11:35 - where I need to get some tensor flow das
11:37 - stuff working so what I need is I need
11:40 - to have M and B so let's figure that out
11:43 - so I'm going to create an M and a B and
11:50 - I'm not going to initialize them and set
11:52 - up here and I probably should be using
11:55 - Const in various places here just
11:57 - protect myself from reassigning
11:58 - something by accident but I'm gonna be
12:00 - loosey-goosey and just use let you know
12:05 - these could be Const but anyway I'm not
12:07 - gonna get into the whole let verses
12:08 - Const think it makes me crazy I'm gonna
12:10 - say up here M equals TF scalar random 1
12:16 - so I'm gonna use the p5 random function
12:19 - to give me a random number between 0 1
12:20 - cuz I gotta start somewhere
12:22 - so this is kind of like initializing the
12:24 - weights of a neural network there is no
12:25 - neural network I'm just doing I'm just
12:27 - kind of optimizing this function y
12:29 - equals MX plus B but those are like
12:30 - weights and then B so I'm gonna
12:32 - initialize them randomly and scaler
12:34 - because it's a single number so go back
12:36 - to my tensorflow J ass intro videos and
12:38 - you'll see then B I'm gonna say the same
12:40 - thing ah but these are the things that
12:44 - have to change right the data never
12:47 - changes it's sort of fixed M and B
12:50 - change over time I need to tweak those
12:52 - which means they have to be variable
12:55 - they have to be able to change which
12:56 - means when I over here I think what I
12:59 - write is TF variable I wrap them in the
13:01 - TF variable so now I have M and B as TF
13:07 - variable right isn't it crazy like you
13:09 - see this kind of code you're like that
13:11 - looks like the craziest scariest thing
13:13 - but you realize like it's just like make
13:14 - a number and because we're
13:16 - this like kind of lower level working on
13:18 - the GPU land I've got to be very like
13:20 - specific like this is a single number
13:22 - and it's going to be variable but really
13:25 - it's just a random number okay now what
13:29 - do we need to do we need to write I
13:31 - don't think I actually said this but I
13:34 - need to write a function called predict
13:37 - maybe which takes in all of the X's just
13:43 - the X's and gives me the Y predictions
13:46 - based on where the line is right because
13:48 - I need to compare the Y predictions to
13:50 - the actual Y values to get the mean
13:52 - squared error so let's write that
13:55 - function so I'm just gonna I'm putting
13:57 - these in like arbitrary places but I'm
13:58 - gonna write a function called predict
14:00 - and they're what I need to do is I need
14:03 - to have some X's and I need to return
14:10 - some wise I think that's the idea right
14:14 - yes I'm gonna so I don't want to just
14:17 - predict one value I want to predict a
14:19 - bunch so the X's here's the thing so if
14:21 - I call this function the X is if they're
14:25 - just a plain array I need to make it
14:27 - into a tensor so I'm gonna call it Const
14:31 - TF X's that might be a bad is tensor and
14:36 - this is a one D tensor tensor 1d OTF
14:44 - tensor 1d I think this will do it X's
14:48 - right and you turn it into a tensor and
14:52 - then I need to have the formula for a
14:58 - line so I need to say what it which is y
15:02 - equals MX plus B so what I would be
15:06 - doing is I would be saying TF X's
15:11 - multiplied is it mu L or nu L T
15:15 - multiplied by M plus B right this is the
15:22 - idea if I'm getting just a plain array
15:25 - of numbers I turned them into a tensor
15:28 - then I apply the formula
15:30 - for a line and these are the predictions
15:32 - the wise I guess if I don't like my
15:34 - naming here I'm just gonna call this X
15:40 - and maybe I'll call this X's I don't
15:41 - know I have to think about I'll come
15:46 - back to this later okay so I have that
15:53 - now let's go back and look at the things
15:58 - that I need so I have this predict
16:00 - function I have a data set ah I need a
16:03 - loss function you need a loss function
16:06 - and I need a let's before we do the loss
16:08 - function let's create the optimizer and
16:11 - the learning rate so this is what's
16:12 - wonderful about working with tensorflow
16:15 - j/s when I say make the optimizer
16:17 - I just mean make it TF optimizer like it
16:21 - exists it'll do this math for us so
16:23 - let's go to the this is not something
16:25 - that I covered in my other videos so
16:26 - let's go look for optimizer and I want
16:30 - an optimizer now there's all these
16:32 - different kinds of optimizers SGD
16:36 - stochastic gradient descent
16:38 - this means the idea of slowly adjusting
16:43 - mnb to to minimize the loss function and
16:48 - I've covered this in more detail in the
16:50 - other videos so so I'm gonna click on
16:54 - that and I'm gonna look here and this is
16:55 - basically what I need to do I we got the
16:57 - code right here look there's even a look
16:59 - at this oh my goodness there's like some
17:01 - stuff here we could really use so I'm
17:03 - gonna grab this and I'm gonna put this
17:08 - up here so I want to learning rate and
17:11 - I'm gonna I'm gonna have a much bigger
17:12 - learning write to start with and I want
17:13 - an optimizer so now I have a learning
17:15 - rate and an optimizer and the optimizer
17:17 - is doing stochastic gradient descent so
17:20 - I have learning rate optimizer now I
17:22 - need that loss function
17:24 - I need the loss function okay the loss
17:29 - function is something I'm gonna write
17:31 - loss and actually let's go back to here
17:35 - so look at this so this is the fancy es6
17:38 - way of writing a function but I'm gonna
17:40 - write it in a
17:42 - a less fancy way and I'm gonna do this
17:46 - so what I want is I need the loss
17:49 - function I have some predictions and I
17:51 - have some labels so these are the
17:54 - predictions are the y-values I'm getting
17:57 - from the predict function the labels are
18:00 - the actual Y values that are part of
18:03 - this and by the way I'm gonna have to do
18:05 - memory management don't worry I'm if
18:06 - you're screaming at me that I haven't
18:07 - worked about memory management I'm gonna
18:09 - do that just gonna do that later so what
18:12 - I want to do is say return the
18:16 - predictions minus the labels that makes
18:18 - sense right because I said here when I
18:20 - said mean squared error is the
18:23 - predictions like the guess - the labels
18:26 - which is the actual Y squared and so
18:31 - predictions - the labels squared and
18:39 - then take the mean of them look at this
18:41 - all of these mathematical operations are
18:44 - inside of tensorflow - yes and you can
18:47 - chain them so predictions is a tensor
18:52 - labels is a tensor all of these remember
18:55 - they're just gonna keep producing new
18:57 - tensors and I'm gonna have to tidy and
18:59 - clean all this stuff up or memory
19:00 - management but again I'll worry about
19:01 - that later
19:02 - so now I have the loss function alright
19:04 - well what is it that I want to do every
19:07 - time so let's say I think I'm actually
19:09 - like I have everything I have the loss
19:12 - function I have the data I have the
19:15 - optimizer I have a predict function I
19:17 - have a learning rate I can minimize Y oh
19:19 - this I haven't done so the training the
19:21 - actual training what does it mean to
19:24 - train it to train it means minimize the
19:27 - loss function with the optimizer and
19:29 - adjusting m and B based on that all
19:32 - right so let's see if we can make that I
19:34 - have a feeling that was in that page
19:36 - that I went to so maybe I could just
19:38 - copy it from there I'm kind of this is
19:39 - like very totally is I'm gonna happily
19:43 - cheat here who was seeing that example
19:45 - thank you very much thank you to float
19:46 - ideas documentation and so I'm gonna
19:51 - just put this in draw like every time
19:52 - through draw I'm gonna minimize so
19:54 - let's look at this oh look at this okay
19:55 - so this is a little different so first
19:57 - of all this is using nice fancy es6
20:01 - arrow notation which I'm somewhat happy
20:03 - about but let me just write a function
20:05 - here called train and the idea of the
20:08 - Train function is to execute the loss
20:11 - with the predictions and the and the
20:18 - actual wise okay so here what I'm really
20:22 - doing is minimizing the Train
20:24 - um that's weird this isn't really no
20:27 - training would be doing this so this is
20:28 - a terrible name for this and actually
20:31 - this is silly for me to even name this
20:33 - it really makes sense for me to just
20:35 - make this an anonymous function and that
20:37 - what I'm minimizing is this right this
20:41 - is what I want to minimize the loss
20:44 - function but if I want to be nice and
20:46 - es6 like with my arrow notation which I
20:48 - think by the fact I'm using tensorflow
20:50 - j/s and you can watch my arrow notation
20:53 - function if this is if this is I can
20:55 - kind of get rid of a lot of the extra
20:57 - stuff here and this should be good so I
21:01 - just want to minimize the loss function
21:03 - now here's the thing these have to be
21:07 - tensors right the loss function requires
21:11 - predictions and labels they have to be
21:13 - tensors and if you remember my X's and
21:15 - Y's aren't tensors when I call to
21:18 - predict function with the X's it gives
21:20 - me back a tensor so that I can't believe
21:25 - I haven't run this code yet it's a
21:26 - terrible thing
21:27 - usually I try to run my code
21:28 - incrementally all the time I guess I
21:31 - forgotten to do that so probably people
21:32 - in the chat are telling me about
21:33 - mistakes I'm making so this is a tensor
21:36 - but this is still a plain array so what
21:39 - I need to do is say constant and I got
21:42 - to rethink the naming maybe something in
21:43 - the chat has an idea for me I think what
21:47 - I actually should do I have an idea
21:49 - permit me a moment of refactoring X
21:52 - valve Y valve so I think what it's not a
21:55 - tensor I'm just going to call it like X
21:57 - underscore valve because that's gonna
22:01 - help me remember so X valve Y valve and
22:07 - then whenever I say and this should be X
22:11 - whenever I say X sry s that's really a
22:14 - tensor I guess I could have done T X s
22:15 - so here what I'm doing is predicting
22:18 - from the X valves and then Y s is TF dot
22:22 - tensor one D Y valve so I need to create
22:26 - that tensor and now I can minimize the
22:29 - loss with predicting from the x-files
22:31 - and the Y valves
22:32 - okay so this is good let's just run this
22:38 - alright I'm gonna I mean I'm from the
22:40 - future different day different clothes
22:42 - I'm braking to this video to mention
22:44 - something really important that I didn't
22:45 - actually mention when I recorded the
22:48 - coding challenge originally what is that
22:50 - optimized function doing how does it
22:51 - actually work and we need to look at the
22:53 - tensor flow test documentation to see so
22:55 - let me I'm bringing my laptop back up
22:57 - here I'm gonna switch over here so I've
22:59 - got the code from the past me in the
23:01 - future this is the part that I'm talking
23:03 - about well how is this going to adjust
23:06 - and then be that those are the these are
23:11 - the parameters the weights that the
23:13 - variables that we need to adjust to
23:15 - minimize that loss function but I'm not
23:18 - anywhere in here saying you know those
23:20 - are the variables to work with well this
23:22 - is part of what tensorflow jeaious does
23:24 - natively the fact that I made these up
23:27 - here TF variables means those are
23:31 - variables that can be adjusted and if we
23:33 - look here at the tensorflow
23:35 - documentation you'll see what does
23:37 - minimize do it executes this function f
23:41 - that is sorry that is this function
23:46 - right this whole function here and by
23:48 - the way the return here is implicit
23:51 - because I'm using the arrow function so
23:52 - it minimizes the output of that function
23:55 - tries to get it lower by computing the
23:58 - gradients with respect to a list of all
24:00 - trainable variables provided by var list
24:03 - guess what
24:04 - I didn't provide a list of trainable
24:06 - variables if no list is provided it
24:08 - defaults to all trainable variables and
24:10 - that's what's going on that's what I did
24:12 - in this coding challenge these are all
24:15 - the trainable variables in my system if
24:17 - I wanted to only use em or only use B I
24:20 - put those in a list so that's all I have
24:22 - to say about that I'm going to fade now
24:25 - back into the other video lower this and
24:28 - back up and it's going to keep going
24:30 - where I D bug and have also two other
24:32 - problems goodbye okay predict dotsub dot
24:38 - squared is not a function at loss at
24:40 - optimizer minimize so what do I have
24:42 - wrong here in my loss function sub
24:47 - labels dot squared dot mean hmm oh you
24:54 - know what it is there's nothing in the
24:59 - arrays at the beginning they have zero
25:00 - things in them so a couple things one is
25:02 - I could put something in it but I think
25:04 - I probably should just say I'm not I
25:06 - shouldn't do only if X dot length is
25:10 - greater than zero so this is definitely
25:12 - do I want to bother with do I want to
25:15 - bother with doing any of this if there's
25:16 - no values in there like calling
25:18 - predicting stuff with an empty array of
25:20 - think it's gonna cause problems that
25:22 - makes sense
25:22 - alright let's try this X is not to find
25:26 - it x valve my naming okay
25:30 - sketch 45 ah this is X valve and this is
25:38 - X valve
25:40 - y valve okay that should be good all
25:44 - right let's try this
25:49 - mmm this is not square I'm being told in
25:52 - the chat breaking news due to deduce
25:54 - that this is actually dot square not dot
25:57 - squared is that right yeah oh it's
26:02 - square a dot square okay
26:09 - there we go okay so things are going and
26:11 - there's no I don't have any like I could
26:14 - look at that's M m m dot print right so
26:21 - you can see it's changing it's actually
26:23 - like training it like the value of M is
26:25 - changing so everything's going and
26:27 - working the problem is I'm not seeing
26:29 - the results let's just check B and I
26:36 - haven't done any memory cleanup so if I
26:39 - say memory dot num tensors is that what
26:42 - it is now what is it again so let's look
26:45 - under memory management memory Oh memory
26:50 - num - oh this TF so I know you can't see
26:53 - this but this is what I want I want to
26:54 - check how much I want to check and see
26:57 - like how if I have to have cleaned up
27:00 - stuff you can see I have 1147 tensor so
27:03 - I need to do the memory cleanup I don't
27:04 - know probably better practice would be
27:06 - for me to clean up as I'm going but I'm
27:08 - kind of gonna clean up at the end all
27:10 - right so let's
27:12 - I just want to click back here for a
27:13 - second oops no I'm just gonna click no
27:16 - loop to shut this off and let's go here
27:19 - so what do I need to do ah I need to
27:21 - visualize what's going on all right so
27:24 - how do I do that so I need to draw a
27:27 - line so the way that I would draw the
27:30 - line is first what I would do is all I
27:33 - really need is to give myself the x
27:35 - value of 0 and the x value of 1 get the
27:39 - two Y values and draw a line between
27:41 - those two points so if I were to say let
27:46 - X equal TF scalar this is silly for me
27:52 - to use the predict function why not why
27:55 - not let's use the predict function TF
27:58 - scales are zero so x 1.is TF scalar 0 y1
28:03 - equals T F equals predict x1 x2 equals T
28:11 - F scalar 1 Y 2 equals predict X 2 right
28:18 - so this should give me I mean it's a
28:20 - little bit silly for me to not just do
28:22 - this
28:22 - keep an extra copy of like M&B as
28:24 - regular numbers but let's keep going
28:26 - with this will this work is it gonna be
28:29 - able to take a scaler and make a 1d
28:32 - tensor I think so so let me just see
28:34 - here so let me do X 1 dot print Y 1 dot
28:40 - print so let's see that tensor 1 D
28:44 - requires values to be a flat typed array
28:46 - hmm all right so one thing I could do is
28:49 - instead of making it a instead of making
28:53 - it a scalar 10 I can make it a 1 D
28:55 - tensor that's what it wants and do the
28:58 - same thing here and I have to put it in
28:59 - as an array then but it's just one value
29:01 - oh this is so silly why am i doing X 1 I
29:05 - could just do this right X is once again
29:09 - can I use X's yeah yeah yeah so I could
29:13 - do I could just do it with zero and one
29:16 - constant X's and then constant y z--
29:20 - equals predict X's right so I could have
29:23 - both these points now then let's say X
29:28 - is dot print wise dot print let's look
29:32 - at that let's see if this works
29:35 - predict is not defined because my e key
29:38 - doesn't work I have to type it several
29:40 - times tensor one D requires values to be
29:44 - a flat typed array Oh
29:47 - silly silly me predict doesn't want a
29:51 - tensor oh it wants this so line x equal
30:02 - let me just this is a little bit silly
30:04 - but I'm gonna do this so I'm gonna make
30:07 - the oh oh but I don't need to know the
30:10 - X's I don't need to have the extras of
30:12 - the tensor because yeah sorry everybody
30:17 - there we go my predict function I
30:22 - totally forgot already see this is just
30:24 - there's so many different ways you could
30:25 - do this like I could have I could
30:27 - enforce you to convert to a tensor
30:29 - before you pass it in to predict but
30:31 - I've just a lot of these decisions are
30:32 - the arbitrary so you might have a better
30:35 - way of doing it but so I'm gonna do this
30:36 - so now I have the XS and the Y's and I
30:40 - don't even need to say XS print so we
30:43 - can see okay great so I'm getting these
30:45 - points k week mon in the chat makes an
30:50 - excellent point which is that i should
30:55 - think about actually mapping it between
30:57 - negative one and one with zero zero in
30:59 - the center that's not such a bad idea
31:03 - let me just keep going with this and
31:05 - then i'll maybe i'll change that after
31:06 - the fact because this should work anyway
31:07 - so now here's the thing here's the
31:09 - awkward thing in order for me all i need
31:13 - to do now is basically say this let x1
31:18 - equal map x is 0 which goes between 0 &
31:21 - 1 1 between 0 and width and this is kind
31:24 - of silly could just multiply it times
31:26 - width but I'm gonna just go with the
31:30 - normalizing well the full normalizing y1
31:34 - equals map X sorry X 2 which map X's
31:39 - index 1 so this gives me X 1 which is
31:42 - just 0 and with now y1 and y2
31:53 - I want to map YS the Y value is between
31:59 - and between height and 0 because I'm
32:04 - flipping it the problem is and then I
32:10 - just want to say line x1 y1 x2 y2 so
32:16 - this is really all I need to do right I
32:18 - just want to get this sort of two points
32:23 - on the line and then draw a line between
32:24 - them this is fine because my X's are not
32:28 - tensors and I can use plain numbers
32:30 - right here x1 x2 but my y's and and here
32:35 - but my y's are tensors so for me to be
32:38 - able to I really need to get the values
32:41 - back and a way to do that is is with a
32:44 - function called data
32:46 - so I'm gonna say let's line y equal wise
32:55 - dot data and I'm just gonna say Data
32:57 - Sync right now and let me comment all
33:00 - this out and let me console.log that and
33:04 - see if this comes so there's this is
33:07 - kind of a bad idea for a variety of
33:09 - reasons but I think it's gonna work okay
33:13 - so you can see I'm getting those numbers
33:15 - back as a float array so here's the
33:18 - thing
33:19 - this really requires not a callback but
33:21 - a promise and I'm so happy I just did a
33:23 - whole video series on promises I really
33:25 - should be saying data then and there's
33:28 - even something called TF next frame
33:30 - which allows me to sort of think about
33:32 - the asynchronous nature of pulling the
33:35 - data out of a tensor into a number that
33:37 - I can use it in animation these are key
33:39 - things I'm definitely gonna have to get
33:40 - to them but here's the thing this is
33:42 - just two numbers I think my animation
33:45 - can handle using data sync and maybe
33:48 - somebody from the tensorflow Jazz team
33:50 - is gonna want to say like actually this
33:52 - was not just a bad idea but like a
33:54 - really bad idea I'm not so sure but I
33:56 - think it's gonna let's just get it to
33:57 - work and see if this demonstrates the
33:58 - idea so now I'm gonna call this line why
34:01 - I should be able to say y1 y2 and I
34:07 - should get 0 & 1 from line Y and now we
34:15 - should see we should be done up line Y
34:20 - is not defined
34:21 - we're sketch that J is line 61 I think I
34:27 - just didn't hit save yeah
34:30 - oh I haven't clicked any points Hey look
34:35 - at that oh hey look it's working alright
34:42 - for a couple things number one is let's
34:44 - say strokeweight - and by the way we can
34:51 - now start to play with the learning rate
34:52 - III don't have to clean up the memory
34:54 - stuff I have to look and play with the
34:57 - learning rate like let's make this
34:59 - zero-one so you can see what happens
35:04 - with this lower learning rate I don't
35:07 - know if it's let's see is it really
35:13 - working well I shouldn't use such a low
35:14 - learning rate let's make it point five
35:18 - yeah it's definitely it's definitely
35:20 - happy okay
35:22 - so this is working linear regression
35:23 - created two said but I have a severe
35:26 - problem I am just filling the GPU with
35:30 - tensors and tensors and tensors and
35:32 - tensors and never cleaning them up so
35:35 - now it's my job to go through and find
35:38 - every place I'm creating a tensor and
35:39 - dispose of it so I can use TF tidy to do
35:42 - that automatically or it can just use
35:44 - the actual dispose function which I
35:46 - might be inclined to do it first all
35:47 - right so let's go through so here these
35:50 - I do not I always want to keep them and
35:52 - then be your variables that I need to
35:54 - keep throughout the course of this
35:56 - program loss do I just put tidy in here
36:01 - or should I let's predict so do I put
36:05 - tidy in here do I wrap tidy here what if
36:09 - I just put tidy here like what if I say
36:12 - TF dot tidy and put all of this will
36:20 - this do it and then here
36:23 - I also need to well I'm here maybe what
36:27 - I'm going to do is just dispose these
36:28 - there's no logic to what I'm doing but
36:30 - I'm just gonna dispose these manually oh
36:32 - and that's just the wise right line y is
36:38 - just YS is the only thing that's a
36:40 - tensor here so this should tidy
36:43 - everything but hopefully not the
36:45 - variables that I need to keep rather
36:48 - than individually figuring out what to
36:50 - dispose of and down here I kind of know
36:52 - that this is my only tensor this by the
36:56 - way I should call this like line X just
36:58 - to be consistent with my variable naming
37:03 - you know I'm only using wat that YS and
37:06 - XS notate variable name when I have
37:09 - something that's actually a tensor which
37:11 - helps me remember what
37:12 - to clean up and not let's see if this
37:13 - goes okay it's still running 221 no okay
37:21 - so I better there's fewer tensors but I
37:25 - haven't cleaned up everything so what
37:27 - could I be missing maybe the call to
37:39 - predict wouldn't tidy clean that up
37:42 - alright need to debug this somehow one
37:44 - thing I could do is trucks are
37:45 - commenting stuff out to see like where
37:47 - is the memory leak so one worry I have I
37:50 - really think Lawson predict those
37:52 - functions generate a lot of tensors I
37:54 - believe TF tidy should clean up anything
37:57 - but let's just for the sake of argument
38:00 - comment this out and now of course the
38:06 - learning is no longer happening and what
38:08 - I might as well do is console.log the
38:13 - amount of tensors not have to like ask
38:16 - for it looks um what did I do wrong
38:21 - TF memory numb tensors what is it how
38:24 - come I can't remember what it is numb
38:26 - tensors no yes it's not a function it's
38:31 - just numb tensors sorry everybody okay
38:33 - so and I need another parenthesis here a
38:36 - little digression there all right all
38:39 - right so we can see it's growing so
38:40 - let's keep commenting stuff out to see
38:42 - like what's causing the memory leak
38:45 - let's comment out this whole area down
38:48 - here ah good news everybody the memory
38:53 - leak is in that part let's put this back
38:57 - just to be sure okay ah so the memory
39:05 - leak is definitely down here and I've
39:08 - probably created oh my goodness oh my
39:14 - goodness
39:18 - no I'm not sure well let's put this back
39:20 - in I thought I saw it but then I didn't
39:23 - again so this is a tensor and I'm
39:26 - disposing it Oh predict aha
39:33 - the predict function makes other tensors
39:35 - and predict got cleaned up because it
39:38 - wasn't ID but I'm just manually
39:40 - disposing the Y's down there
39:42 - that's what it is so let me use tidy I
39:48 - guess so let me do this let me put this
39:53 - up here so this is really what I need to
39:58 - tidy so instead of disposing manually I
40:05 - kind of liked disposing things manually
40:07 - the tidy thing kind of freaks me out but
40:10 - the problem with this is I have a scope
40:12 - issue which is that line Y right no
40:17 - matter what I do if I take this out here
40:19 - like this is going to tidy everything so
40:21 - I guess it's not the biggest deal at the
40:24 - moment at least for me to just put
40:25 - everything inside the tidy well that's
40:31 - it let's put everything inside the tidy
40:33 - for right now there's probably a way I
40:36 - could simplify that but this should work
40:38 - let's give this a try there we go
40:43 - there's only ever five tensors all the
40:46 - time so there's no more memory leak five
40:49 - tensors linear regression with gradient
40:51 - descent tensor float is interactive here
40:55 - it is so what's what's left here so
40:57 - number one things that could be improved
40:59 - so I wanted to talk through some things
41:02 - that could be improved but already me I
41:04 - am something the chat made a very good
41:05 - suggestion this is very awkward how I
41:07 - put everything in tidy so unnecessary
41:10 - let me take that out because predict
41:13 - returns something I can actually just
41:16 - put the tidy right here I don't know why
41:18 - I didn't think of that
41:19 - like I can actually just it's only this
41:21 - predict function that so I can actually
41:25 - put the tidy right here and I can use my
41:30 - fan
41:32 - to es6 arrow syntax right and the return
41:34 - is now assumed and then I can just say
41:37 - why is not disposed so this should work
41:39 - right tidies not going to tidy up the
41:42 - this y-value but I can I can dispose
41:46 - that manually once I have the values so
41:48 - I think this will do the trick let me
41:50 - just take a look at this yeah so this I
41:53 - like better and there's probably other
41:55 - styles or ways you can do it the point
41:56 - is you've got to keep track of all the
41:58 - tensors you're making and dispose that
42:00 - okay so I think we're gonna wrap this
42:02 - video up I'm getting all these great
42:04 - suggestions from the chat right you know
42:06 - I could have tidied this the tensors
42:08 - here and predict you know so number one
42:12 - is I'm um this code is gonna get posted
42:14 - to the coding train website and coding
42:15 - challenges make your improvements and
42:17 - add them as community contributions some
42:19 - things that I would love to see
42:21 - visualized graph the lost value I think
42:24 - there's a way to get the loss of sure
42:25 - there's a way to get the lost value out
42:27 - of that function that's one idea K week
42:30 - Mon suggested maybe trying some of the
42:32 - other optimizers so what happens if I go
42:34 - to the tensorflow chess documentation
42:37 - and like just use some of these other
42:39 - optimizers what are they what will they
42:40 - do do you get better or worse results
42:43 - can you make the learning rate somehow
42:44 - interactive adjust the learning rate you
42:47 - know I don't know if you could come up
42:48 - with any really clever visual ideas with
42:54 - this but anyway so but I think I'm good
42:59 - I think I have the basic idea so if you
43:02 - really want to dive as deeply as you can
43:04 - into linear regression with gradient
43:05 - descent you can go back and watch my
43:07 - other videos where I did this with just
43:09 - JavaScript in p5.js now you've seen it
43:12 - with JavaScript p5 GS and tensorflow
43:15 - times yes so I look forward to your
43:16 - feedback and hearing more about it a
43:18 - more tensorflow digest videos to come
43:20 - and I want to get to some actual more
43:22 - practical things that you might want to
43:23 - do for interactive creative arts
43:25 - projects but I'm still in the weeds here
43:27 - of just trying to understand the nuts
43:28 - and bolts of how the library works so I
43:30 - hope you're enjoying that and I look
43:31 - forward to seeing you in future videos
43:34 - [Music]
43:41 - you