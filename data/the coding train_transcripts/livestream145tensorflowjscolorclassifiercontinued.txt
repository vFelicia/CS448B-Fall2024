00:34 - oh here what a failure what a failure I
01:12 - know you hear me now I know I know I
01:14 - know I know you know i know i know i
01:16 - know i know i know i know i know i know
01:18 - i know i know i know i know i know we
01:21 - can i have we start this over again i'm
01:26 - waiting till the chat catches up to
01:28 - realize that my audio has come back on i
01:30 - always i haven't done this in such a
01:31 - long time i've become slightly more
01:34 - professional in my live streaming but
01:36 - yeah I went to go ah oh now I hear
01:41 - myself even coming out of here this
01:43 - computer I went to I went to go use the
01:49 - facilities which is something I'd like
01:50 - to do right before I start live
01:52 - streaming and when I do that even though
01:54 - I'm not we're streaming I'm actually not
01:55 - even recording so I'm gonna hit start
01:57 - recording or recording to disk I I like
02:00 - to just mute my microphone just in case
02:02 - you know anything happened by accident
02:04 - you know I don't want to confess to some
02:06 - crime in the bathroom by accident so and
02:12 - then I came back and I started and I
02:13 - forgot to turn the microphone back on
02:16 - anyway I was saying go Mexico scoop
02:19 - please give me score updates go Mexico
02:21 - sorry Sweden sorry sweden i love you'
02:23 - Sweden but go Mexico for what I
02:27 - understand if Mexico wins today Mexico
02:29 - would be the first World Cup team ever
02:31 - to win three games the first three games
02:32 - in a row I don't know if that's somebody
02:34 - out of fact check me on that but I think
02:35 - might be correct yes hey Zeus in the
02:39 - chat says Mexico is playing I can't
02:41 - watch both please go and watch Mexico
02:44 - this first of all this will be not
02:45 - nearly as interesting or exciting and
02:48 - also this you could watch later and you
02:51 - can watch the Mexico game later but
02:52 - that's you're gonna already know what's
02:54 - gonna happen and you know I can tell you
02:56 - what's gonna happen here I'm gonna try
02:58 - to code some stuff I'm gonna make lots
02:59 - of mistakes I'm gonna get stuck a lot
03:01 - I'm gonna waste a lot of time and
03:03 - eventually maybe at some point there
03:05 - will be a small useful nugget of
03:07 - educational material that comes out
03:09 - that's what's gonna happen okay anyway
03:13 - what was I saying
03:14 - good morning it is me the coding train
03:18 - person Schiffman thing human being and I
03:26 - have one job today my one job today is
03:30 - to complete the color classifier example
03:33 - with tensorflow Jas so the two things I
03:37 - would like from the chat i am i Yura I'm
03:42 - going to answer your question in a
03:43 - second the two things I would like from
03:44 - the chat well are one score updates so
03:48 - spoiler alert I want score updates the
03:50 - BIC okay Pat the chat so if you're
03:52 - watching this and you don't want to be
03:54 - spoiled I don't know too bad I guess
03:57 - - if this if slack comes back online
03:59 - somebody let me know because I can then
04:01 - look at the slack channel for the for
04:04 - the slack chat and actually since I
04:06 - don't have I'm gonna so I'm gonna be
04:11 - looking more at the YouTube chat than I
04:14 - typically do okay so I have to get
04:18 - myself organized and figure out where I
04:21 - last left off so let me open up terminal
04:25 - also I want to say thank you to the
04:28 - YouTube family and Learning Team I was
04:31 - able to attend something called YouTube
04:34 - EDU Khan last week so that's a move to a
04:38 - new apartment and so out of sorts I live
04:41 - in a sea of drowning in boxes going away
04:45 - on vacation this tomorrow
04:48 - two weeks all right so Wow
04:53 - oh no no no no no no no Sweden scored
04:55 - are you now I can't tell if the chat is
04:57 - trolling me I need somebody reliable
05:01 - nothing you're not reliable the dark
05:06 - hound who gave me the score of why look
05:10 - so unfortunately this is the the World
05:12 - Cup is on a lot this is the let me talk
05:14 - about schedule this is the only time I
05:16 - have right now to do a live stream this
05:18 - week and I have some bad news I suppose
05:21 - if you enjoy watching these live streams
05:23 - there won't be another one until the
05:24 - week of July 15th I'm gonna be away for
05:26 - two weeks the good news it's true okay
05:29 - that's too bad
05:31 - the good news is for though which isn't
05:36 - really good news for you presumably as a
05:38 - live stream viewer but for the non live
05:40 - stream viewers out there I have a
05:41 - backlog of about 8 edited video
05:44 - tutorials that have not been published
05:46 - to the channel and I'm gonna make a
05:47 - couple more today mostly there's the
05:50 - first seven parts of this color
05:51 - classifier thing if you can believe that
05:52 - the first that's right seven parts and
05:55 - then that one edited challenge of doing
05:57 - a subscriber visualization so those will
06:01 - come out be published as public edited
06:05 - videos once every few days while I'm
06:07 - away and I should also let you know that
06:10 - most likely even when one comes out and
06:13 - is public you can look in the
06:14 - description if there's a link that says
06:16 - next video you can go to the next video
06:18 - you might see that it's unlisted but I
06:20 - don't know if this is good practice but
06:21 - this is my the way that I approach if I
06:24 - have a series of like a 10 part series
06:26 - instead of just putting them all 10 live
06:29 - online at once I space them out every
06:32 - few days but I leave the other ones
06:34 - there unlisted so the viewer who really
06:37 - wants to keep watch all of them at once
06:38 - could do that ok
06:41 - oh Germany scored I'm not paying
06:42 - attention to that game I guess that's
06:44 - good though
06:44 - Chico Germany whose German I didn't know
06:46 - he's playing for Mexico I don't know
06:48 - like a huge Mexico football fan
06:56 - all right Oh Italy scored all right I
07:03 - really also I would like to do something
07:05 - with major I'm a little bit of a
07:07 - baseball nerd we'd like to do something
07:09 - with machine learning and Major League
07:11 - Baseball data something that I'm
07:12 - thinking about if people have ideas or
07:13 - suggestions there is a entire example
07:17 - created by the tension flow Jas team
07:19 - that classifies pitches into ball
07:22 - strikes you know curveballs sliders no
07:25 - no no no not ball strikes that
07:26 - classifies what was I saying
07:29 - classifies pitches into fastball
07:30 - curveball slider etc based on the pitch
07:34 - data all right wow we are really got it
07:41 - we've got a good World Cup discussion I
07:42 - love the international quality of this
07:44 - audience I love that there's a great
07:46 - World Cup discussion going on the chat
07:47 - is a wonderful break from the usual its
07:50 - programming language or which IDE is
07:53 - better than the other so if it's one
07:56 - thing that the World Cup has done it's
07:57 - made us forget about all of our quirky
07:59 - preferences for coding editors and the
08:03 - like sorry C are brown can't get behind
08:07 - that
08:08 - Red Sox thing but look congratulations
08:13 - to you on all the recent success
08:15 - interesting tidbit I did go to college
08:18 - with Theo Epstein I didn't really know
08:20 - him though I did not get a haircut as
08:25 - Shubra is mentioning in the chat but I
08:27 - did I could I meant to get a haircut but
08:29 - I did just use my home grooming device
08:32 - to trim this beard which had gotten out
08:34 - of control
08:34 - so no wilderness Dan today okay so I
08:36 - need to figure out where I am and I'm
08:39 - gonna go to the desktop what am I in p5
08:44 - tensorflow probably and I was so one
08:49 - thing I really messed up I think is so
08:56 - something I could use help with is it's
08:59 - gonna not be easy for you to help with
09:01 - it because I haven't published the
09:02 - videos yet but I was trying to keep the
09:05 - code that is at the end of
09:07 - each video separate and I erased one set
09:11 - of codes I need to recreate that which I
09:13 - will do myself but if someone wants to
09:14 - volunteer to do that
09:16 - it's the one I think where I was
09:17 - visualizing the grid of the colors he'd
09:22 - have to go back to the live stream at
09:23 - this point okay so I'm gonna create
09:27 - color classifier - I'm gonna run a
09:30 - server I'm gonna open up the browser and
09:41 - open up the console and go to color
09:44 - classifier - there we go this is where I
09:48 - left left off then I need to open up I'm
09:53 - gonna use the atom text editor
10:05 - happy birthday um I can't by the way
10:07 - people often ask for like shout out or
10:09 - you look at this or way it's impossible
10:11 - I think for me to catch every message
10:12 - and but you know hey happy birthday I
10:15 - care about your birthday hold on let's
10:22 - get this going here I don't want this
10:24 - YouTube this thing I do want to open up
10:30 - on the desktop oh oh computers are such
10:34 - a silly thing so let's close this and
10:45 - let's go to color classifier - that's
10:49 - where I want to be and Here I am and
10:55 - let's go here let's make this a little
10:59 - bit bigger and I think I am I am ready
11:10 - to go so it's 11:15 a.m. my goal today
11:16 - is to finish this example I really want
11:18 - this example to be finished I think this
11:19 - example is interesting enough or has
11:24 - enough to it that I can both publish the
11:29 - kind of fixed version that is the code
11:33 - that I'll leave with at the end of this
11:34 - tutorial as part of the coding train com
11:36 - website and then I'll also make a
11:38 - separate github repository with this
11:41 - color classifier that people can submit
11:45 - pull requests to like just improve the
11:46 - design and interface and that sort of
11:48 - thing I do have an issue where I don't
11:51 - have a good system for maintaining the
11:54 - various github projects that are
11:56 - associated with the coding train and so
11:58 - a lot of this stuff really looks like
11:59 - lingers and languages and so I am really
12:03 - thinking about that and how to mean how
12:06 - I can like manage the community better
12:08 - and so hopefully things will improve
12:18 - all right we need to find a tissue to
12:21 - blow my nose oh and I need to deal with
12:25 - this other camera so if you if you
12:32 - remember I did something really silly
12:34 - and ridiculous on the previous
12:36 - livestream where I didn't want to erase
12:41 - all of this over here I just turned the
12:45 - camera so that I could write some notes
12:48 - about a different project here so let me
12:50 - erase this this is nothing to do with
12:53 - the color classifier I'm just realizing
13:00 - now that I don't even need this so me
13:08 - not erasing it was kind of pointless but
13:10 - I want to turn the camera back uh-oh
13:12 - loose HDMI cable sorry about that okay
13:16 - no green all the way to like they're up
13:18 - loose HDMI cable so this should be good
13:22 - does this seem like focus and you're
13:26 - seeing what you're seeing Oh actually I
13:28 - don't usually have it turned all the way
13:29 - that way because that little smiley face
13:31 - that I drew was my like testing okay
13:34 - there we go
13:35 - so I think this seems good and this is
13:37 - where I last left off I know you can't
13:41 - read this tf1 hot thing there all right
13:45 - so that's good we come back over here
13:48 - and let me open up the tensorflow api
13:54 - reference we're definitely gonna need
13:56 - that a little bit okay my eyesight is
14:01 - more supports I bigger computer screen
14:06 - alright so now I am here and so I
14:18 - actually one thing that I have to talk
14:20 - about which I will get to is I I'm
14:23 - actually going to use validation data
14:25 - and not testing data as
14:28 - this tutorial here's some here's some
14:29 - news actually I took about two or three
14:32 - hours last night kind of building out
14:35 - the example I'll just show it to you
14:36 - right now so as just to prepare for
14:40 - today because as you know I typically
14:43 - don't prepare but I felt like this was I
14:46 - needed to sort of see how this was gonna
14:48 - work so if I go to a shipment github dot
14:53 - you color classifier I think this is it
15:01 - so this here is actually whoops this
15:10 - here is running a version of what I want
15:12 - to build today so behind the scenes its
15:15 - training the model with the training
15:18 - data and you and I'm and this is the
15:20 - number of epochs for now I'm gonna say
15:25 - epoch a lot pronounced epoch look
15:31 - YouTube thank you YouTube epic okay okay
15:35 - what about the British pronunciation is
15:37 - a difference epic British pronunciation
15:42 - epic how to pronounce well let's look at
15:46 - oxford dictionaries
16:03 - I guess you could say it either way
16:11 - don't die okay and yes I could watch
16:20 - Peppa Pig live right now all right
16:24 - oh I logged in to because I was watching
16:28 - the World Cup Mexico game I've logged
16:31 - into my daughter's the account that I
16:34 - have set up with YouTube red for my
16:36 - daughter to watch videos and we can see
16:38 - what she likes to watch here okay let's
16:42 - move away from that epic epoch I want to
16:46 - say Epoque all right second one is the
16:52 - only correct one I so agree me I am soo
16:56 - me as well as the only way to say maths
17:01 - is matched alright so this is what I'm
17:06 - going to look at this so I've trained it
17:08 - with the data and we should see now
17:10 - reddish yellowish greenish bluish stick
17:17 - it with bluish purplish purplish-pinkish
17:21 -  ish purplish so this does this will
17:25 - work and I've gone now for nine
17:28 - a pox I feel confident now it's if the
17:32 - key is saying it with confidence all
17:34 - right does anyone remember where last
17:39 - left up like what was the last thing
17:40 - that I did oh no Sweden scored again how
17:45 - much time is left
17:46 - oh I'm bad luck for sweet and I have to
17:49 - stop live-streaming I mean bad luck for
17:50 - Mexico come on Mexico I mean Mexico's
17:56 - gonna I think Mexico will advance anyway
17:58 - is that correct better be nice for them
18:01 - to win all right let me see hold on for
18:07 - a second I'm going to go somewhere that
18:10 - I don't want to show by accident so here
18:13 - we are for a second
18:15 - I'm going to I just want to login to my
18:18 - YouTube dashboard to find a particular
18:22 - video and I have to log in now as myself
18:24 - switch account another coding train and
18:34 - I'm gonna go to my creator studio
18:42 - creator studio videos color classifier
18:49 - part seven seven is the last one so this
18:53 - is an edited version of part seven that
18:56 - has not come out yet but I can watch it
18:59 - because it is private and I'm going to
19:02 - do that right now
19:03 - Oh should I put on the hoodie so that
19:10 - the matches continuity I know you can't
19:16 - hear the audio
19:34 - okay so let me see where what happens at
19:37 - the end do you want to hear this audio
19:46 - wait wait don't leave just wait don't
19:50 - leave just yet I do want to think about
19:54 - memory management and I'm maybe I'm
19:55 - going to think about memory management
19:57 - later but and the X's and Y's I'm gonna
19:59 - want to use in the next video but I
20:02 - probably should after I make the one
20:04 - last little tidbit here and then I'll
20:06 - move on in the next video I'm gonna
20:07 - start creating the architecture of the
20:10 - neural network model itself and oh I'm
20:12 - gonna introduce some new concepts soft
20:14 - backs and cross-entropy oh no I have to
20:21 - talk about softmax and cross-entropy
20:23 - okay all right that's fine that's fine
20:27 - okay good thing I watch that alright
20:34 - alright I'm ready ready everybody ready
20:38 - Freddy Roby we think of maybe I need to
20:48 - get into the mood a little bit
20:52 - [Music]
20:59 - oh I'm to the wrong string like I
21:09 - [Music]
21:37 - actually have left this ukulele here and
21:40 - I haven't been here for a while so I
21:41 - haven't had a chance to play I need to
21:42 - get a second one so I can have one here
21:44 - one at home so I can actually practice
21:45 - and learn some new songs
21:46 - [Music]
21:56 - [Music]
22:11 - all right here we go
22:21 - stretch my stretch I'm gonna get up and
22:25 - stretch stretch your hamstring muscles
22:31 - okay there we go let's cycle those
22:33 - cameras yes
22:42 - Warner says stick decoding it totally
22:44 - totally okay hello I am back I have
22:54 - returned to finally build the
22:58 - architecture to take this training data
23:01 - these RGB values that are matched with
23:04 - these one hot encoded labels if you
23:07 - don't know what I'm talking about you
23:09 - should probably go back and watch the
23:10 - first seven parts of this series but I'm
23:12 - now going to create the neural network
23:14 - architecture to train a model with this
23:17 - data and I'm going to use continue to
23:22 - use tensorflow j s in particular the
23:25 - layers API to do this so let's review
23:27 - over here a bit
23:31 - let's review over here a bit where I am
23:35 - alright so we've talked about all these
23:38 - things you know it's really funny I just
23:45 - want to erase I want to erase all that I
23:49 - can't like I can't I can't
23:53 - I apologize here sorry everybody go go
23:55 - watch the go go route for Mexico for a
23:58 - little bit
24:02 - this is just - it doesn't help me to
24:05 - have this here it's like two out of my
24:07 - head and so I need to erase all of it
24:12 - what oh how ridiculous mi then I so
24:17 - desperately jumping through various
24:21 - hoops to not erase this it's like I
24:25 - needed it and now that I'm here yes
24:38 - you haven't switched to watching the
24:40 - World Cup once again you were watching a
24:43 - livestream of a person quietly are not
24:49 - so quietly
24:51 - erasing I really need a cloth so I don't
25:09 - kill the earth by the way we have a
25:28 - little family World Cup pool going and
25:33 - the person in the family who happens to
25:36 - be leading with the best picks is my
25:40 - six-year-old far and away running off
25:47 - with
25:51 - [Music]
26:07 - [Music]
26:17 - [Music]
26:19 - watch this one of those glass boards
26:42 - that I could put right here drawn this
26:50 - before it started but don't worry I'm
26:53 - gonna get the coating is it noon already
26:57 - if it's noon time to be a knight I
26:59 - couldn't my watch my watch battery died
27:02 - and my Fitbit why and I didn't know
27:04 - packing and moving a hip behind the
27:06 - charger
27:12 - [Music]
27:29 - don't get your sips of tea or coffee or
27:32 - do some more stretching I'm almost ready
27:35 - I think I should probably just start
27:37 - over all right oh you weren't even watch
27:53 - I didn't even have the camera on sorry
28:09 - everybody are you ready oh it's only
28:21 - 11:30 okay whoo Oh slack is still down
28:29 - is that correct
28:33 - all right Oh No
28:39 - Sweden scored again I mean I love Sweden
28:45 - and everything but got to go all right
29:01 - hello I have returned once again to make
29:06 - a color classifier machine learning well
29:11 - by my first intro is much better okay
29:16 - alright hold on I think my belt isn't
29:21 - tight enough my pants feel like they're
29:23 - falling down okay now everything's gonna
29:25 - be right my brain is gonna work
29:27 - correctly
29:29 - hello I am back in this video I am
29:32 - finally going to start to build the
29:35 - neural network architecture to make this
29:37 - color classifier I am going to take this
29:40 - data over here which is a long array of
29:43 - many many RGB values normalized to
29:46 - arrange there to one which matches with
29:48 - all of these one hot encoded labels and
29:50 - if you don't know what I'm talking about
29:52 - then you might want to go back and watch
29:54 - the first seven yes that's right seven
29:56 - parts of this tutorial series that's
30:00 - getting very very long but this I think
30:01 - is I'm really getting to the good stuff
30:03 - I don't know maybe it was good stuff
30:05 - before maybe this is bad stuff I don't
30:06 - really know but this I'm really excited
30:08 - I'm excited because now what I'm gonna
30:10 - do and I'm gonna use tension flow yes
30:12 - but I'm going to create the neural
30:13 - network architecture so let's just
30:14 - remind ourselves what we have we have a
30:19 - data set most of the first seven videos
30:21 - of the series was all just about
30:23 - collecting and cleaning that data set
30:25 - and that data set is many many RGB
30:29 - values I think I have like five thousand
30:33 - which is actually is kind of very very
30:35 - small for a data set but it's fine for
30:37 - this particular demonstration I have
30:39 - five thousand RGB values each one is
30:41 - labeled with something like blueish or
30:43 - reddish or purplish these were
30:46 - crowd-sourced but those got converted to
30:49 - one hot encoded vectors meaning if there
30:53 - are nine if there are nine labels well
31:00 - let's see then I have a vector that
31:03 - looks like this one two three four five
31:05 - six seven times ten
31:06 - nice night and maybe this one refers to
31:10 - purplish if this particular element of
31:14 - this array of numbers has a 1 in it it
31:17 - is that that and that one is for a
31:19 - particular label this one sort of label
31:21 - okay so that's what I have
31:22 - so what I I know that I need to have
31:24 - some kind of neural network and the
31:27 - inputs has have a shape of 3 there are 3
31:32 - inputs are G B the outputs have a shape
31:39 - of 9 1 2 3 4 5 6 7 8 9 this is the
31:45 - output layer this has a shape of 9
31:49 - inputs of a shape of three outputs have
31:52 - a shape of nine because the goal of this
31:55 - is by what once this whole thing is
31:57 - trained and finished if I send in some
31:59 - RGB values what I'm gonna get is a bunch
32:02 - of numbers all between 0 & 1 and I'm
32:04 - gonna find the one that's the highest
32:05 - and and those numbers are gonna be the
32:08 - probability of this particular data
32:10 - point being a particular label and I'm
32:12 - gonna find the one that's highest in
32:13 - front of sign at that label who
32:14 - classification we're doing
32:16 - classification so now what goes in
32:18 - between all this now this is a big
32:21 - question and many different scenarios
32:23 - might call for multiple layers different
32:26 - kinds of layers there's something called
32:28 - a convolutional layer which I'll get to
32:31 - but I'm gonna do something really simple
32:33 - I'm gonna have a basic dense layer which
32:36 - is kind of the standard building block
32:40 - of neural network systems and I'm gonna
32:42 - give it some number of nodes so for the
32:45 - sake of our even right now let's pretend
32:46 - that I just gave it 4 nodes and a dense
32:48 - layer this output is also going to be a
32:50 - dense layer dense layer means fully
32:53 - connected meaning that every input is
32:55 - connected to every node and then every
33:01 - node in the hidden layer this dense
33:04 - layer is connected to every output now
33:06 - I'm going to let your imagination draw
33:08 - the rest of all these connections but so
33:10 - this is what I want to architect so
33:12 - let's now go on
33:13 - detectives now I'm going to do this
33:15 - using tensorflow digest and the layers
33:17 - API if you don't know about the layers
33:19 - API you're going to watch my three or
33:22 - four part series about the layers API
33:23 - tutorial but I'm bookin I sort of talk
33:24 - you through it while we're doing it here
33:25 - so you don't necessarily have to watch
33:26 - that okay so if I come back again this
33:29 - is what I built so far I have all of the
33:32 - training data and tensors and you can
33:34 - see the shape of it I have 5643 RGB
33:38 - values and 5643 labels nine with nine
33:42 - possibilities okay so the first thing
33:44 - that I want to do is and I'm gonna do
33:46 - some goofy stuff with some global
33:48 - variables that I might not know that you
33:51 - know just to make my life kind of easier
33:53 - I'm going to create a variable called
33:55 - model and my model which I'm going to
33:58 - create in setup at the end after I've
34:00 - prepared all the data I'm going to say
34:02 - model equals t f dot sequential TF dot
34:10 - sequential so that now that's that's me
34:13 - creating a sequential neural network
34:15 - model it's sequential because it's a
34:17 - feed-forward the layers go in this order
34:20 - so now what I need to do is create some
34:22 - layers so the first thing I want to do
34:24 - is make the hidden let's make the output
34:26 - layer now let's make this we should do
34:28 - it in order we have to do it in order
34:29 - and to make the hidden layer hidden
34:32 - equals TF layers dense and then I put
34:40 - some configuration stuff so I make a
34:42 - layer by calling TF dot layers and then
34:45 - I specify the kind of layer this is
34:47 - gonna be a dense layer and then I can
34:49 - pass an object in as an argument and
34:51 - that's where I can configure things like
34:54 - input I don't remember any of this let's
34:58 - go look it up so let's go to the
35:02 - documentation let's go to TF TF layers
35:07 - and let's go to dense where do we see
35:12 - that hold on
35:19 - time out for a second
35:23 - let me look at my cheat sheet which I
35:25 - didn't want to do but I'm going to
35:27 - totally do
35:40 - TF layers dense I got that right so
35:46 - where did I miss it
35:48 - oh it's right there under basic sorry
35:53 - I'm looking around for it and it's right
35:54 - there in front of my face under basic so
35:56 - I'm gonna make a TF layers of dents I'm
35:58 - gonna click on that and now I'm gonna
36:00 - see these are all of the things that I
36:05 - can pass into the configuration so I
36:07 - need to specify the number of units the
36:10 - number of units is like the number of
36:11 - nodes and I made up four right here
36:13 - maybe let's try sixteen maybe we want to
36:15 - have some more than four whatever we can
36:17 - make up anything we want so I'm going to
36:21 - now say units sixteen one thing that I
36:24 - know I need there's an activation
36:26 - function again I can't cover everything
36:28 - in this video I have an other videos
36:30 - where I've talked about what an
36:31 - activation function is and how it works
36:32 - but the idea is the activation function
36:35 - is the function that takes all the sum
36:38 - of all of the things passing through the
36:41 - network being multiplied by the weights
36:43 - and kind of squashes them into some
36:45 - range and so there probably is a really
36:49 - useful interesting discussion about we
36:51 - could have about what would be the best
36:53 - activation function to use right here
36:55 - right now maybe later
36:57 - he'll try some different ones but just
36:58 - for simplicity I'm gonna use I'm gonna
37:01 - make a bad decision and just use sigmoid
37:04 - this sort of like historically original
37:06 - activation function of neural networks
37:09 - I'm gonna use the activation function
37:10 - sigmoid let's see what else do I want
37:12 - input dimensions so this is something
37:15 - that I definitely need to do here
37:17 - because remember this this this these
37:20 - inputs this is not actually a layer this
37:22 - is a two layer network it looks like
37:24 - there's three but I'm just drawing it
37:26 - with three things and the inputs being
37:27 - but that's not a layer but I do need to
37:29 - specify that three things are coming in
37:32 - so I need to come here and say the input
37:37 - dimensions input dimensions is three
37:41 - because I have an RGB value this should
37:43 - do me just fine for right now so then I
37:47 - want to also create the output layer
37:53 - output TF that's gonna be dense that's
37:56 - going to have nine units because they're
37:58 - nine labels again that's completely
37:59 - arbitrary that's just how I happen to
38:01 - prepare my dataset now I don't need the
38:05 - input dimensions because the input
38:07 - dimensions can be inferred by the
38:09 - previous one the input dimensions to the
38:11 - output or the number of units of the
38:13 - hidden so I don't need that but I do
38:14 - need to specify an activation function
38:15 - and guess what I am going to use a
38:20 - different activation function softmax so
38:24 - I'm just gonna type that in right now I
38:26 - will come back and explain what softmax
38:28 - is in a separate video which i think
38:30 - will be the next video of this series
38:32 - just gonna push this a little bit
38:33 - further now I'm gonna say model dot add
38:39 - the hidden and then model dot add the
38:43 - output so this is now me this is now the
38:48 - code for exactly what I diagrammed right
38:53 - here three inputs into a hidden layer
38:56 - with some number of units with some
38:58 - activation function into an output layer
39:01 - with some number of units and an
39:02 - activation function timeout for a second
39:07 - I just want to see do I want to do the
39:10 - optimizer yes I think that I do okay
39:13 - okay okay
39:19 - oh yes so we have now built the model
39:24 - here's the thing the next thing that I
39:26 - need to do and I'm gonna do this in the
39:28 - next video what I need to do is create
39:30 - an optimizer so let's just put this in
39:33 - comments create an optimizer and I need
39:36 - an optimization function which typically
39:38 - in the past I've used mean squared error
39:40 - but I'm gonna use something called
39:43 - categorical cross true
39:48 - I don't know why it sounds really scary
39:52 - but it's not and I can't its I also
39:54 - can't spell it so I'm gonna create the
39:57 - optimizer and then I'm going to compile
40:00 - the model and then I'm going to train
40:03 - the model these are the next step so
40:04 - they need to do this is the architecture
40:06 - for the model people telling me I have
40:09 - an error oh yeah I have something extra
40:10 - extra comma here but so this one do the
40:15 - next video and so what I need to do in
40:17 - the next video this is like just a few
40:18 - lines of code but I need to I mean I
40:21 - could just add them but I would like to
40:23 - try to understand a bit more about what
40:25 - why am i have softmax here instead of
40:28 - sigmoid or new or any of the other
40:30 - activation functions and why I might
40:33 - choose categorical cross-entropy
40:35 - instead of mean squared error which is
40:37 - if you have happened to watch my ex or
40:40 - tensorflow TAS coding challenge or some
40:42 - of my other layers tutorials
40:45 - I always just use mean squared error so
40:48 - that's what's coming the next video I'm
40:49 - going to create the optimizer I'm gonna
40:51 - compile the model and I'm going to talk
40:53 - about softmax and categorical cross
40:56 - entropy oh wait wait wait wait let's
41:01 - actually run with this and see if
41:02 - there's a syntax errors no okay and if I
41:07 - just say if I if I look in the console
41:10 - here at model we can see there it is
41:12 - this is the object and it's got all this
41:15 - stuff in it alright see you in the next
41:16 - video
41:24 - okay now is the moment where people can
41:29 - ask some questions or offer some
41:32 - comments before I move on to the next so
41:37 - I I so I need to open up a Wikipedia
41:43 - page for softmax
41:46 - and then why did why does everybody make
41:51 - everything look so insanely insane
42:00 - and then categorical cross-entropy one
42:10 - thing I want to know is what's the
42:11 - difference between categorical
42:12 - cross-entropy and because I think in if
42:16 - I go to loss functions here but it's
42:24 - actually not here so this is I don't
42:27 - know if this is a bit of a point of
42:29 - confusion for me there's the loss
42:31 - function here is written as softmax
42:33 - cross-entropy
42:34 - but I know for a fact because I was
42:36 - building this example last night I'm
42:42 - looking at the chat so hard to follow
42:43 - the chat without the /o Caddick so
42:48 - here's something I would love so this is
42:56 - what I'm building I have it in here
42:57 - already right so this is what I did I
43:02 - use categorical cross entropy because
43:05 - this I have found in this is what I
43:10 - found that examples of tension flow jas
43:13 - however as the loss function and then
43:18 - but however if I go here and look at the
43:21 - API Docs
43:22 - it says softmax cross entropy are those
43:25 - the same thing
43:27 - no metrics oh it's a mint interesting
43:38 - it's here under metrics
43:50 - [Music]
43:53 - I'm looking at the chat I guess I'm
43:57 - gonna just kind of gloss over this right
44:03 - now and this should be a lowercase e by
44:09 - the way I mean I'll leave that there
44:12 - since all right oh I
44:18 - I also messed up which is that the
44:26 - optimizer I totally I totally misspoke
44:30 - at the end of the last video oh yeah
44:32 - okay thank you about for the the battery
44:40 - the battery notes I only have one plug
44:43 - today so let me plug this in the first
44:54 - thing I totally messed up the
44:55 - optimization function is stochastic
44:57 - gradient descent or adaptive something
45:01 - the loss function the loss function is
45:05 - mean squared error or in this case I'm
45:08 - going to use categorical cross entropy
45:10 - instead okay so that I messed up okay
45:19 - all right so I got to just move on I
45:21 - guess
45:29 - cycle the camera
45:55 - the chat has more entropy than the
45:58 - program itself all right
46:02 - I am going to move on and let's look at
46:31 - why is the word entropy use that's what
46:34 - I would like to understand it's between
46:41 - two probability distributions P and Q
46:44 - over the same underlying set of events
46:46 - measures the average number of bits
46:47 - needed to identify an event drawn from
46:50 - the set coding scheme so basically let
46:56 - me see if I get this right before I put
46:58 - it in the tutorial where did I put my
47:01 - marker where's that marker here it is so
47:08 - this this output layer ultimately with
47:12 - softmax gives us a probability
47:13 - distribution and we could absolutely
47:16 - look at the correct distribution wipe
47:20 - the thing that we're training against
47:22 - the calculating an error is this right
47:25 - the one hot encoded vector so we could
47:28 - use mean squared error it would give us
47:30 - a loss but cross-entropy is a formula
47:34 - for looking at the difference between
47:37 - this output vector and this vector if
47:40 - we're considering these probabilities
47:42 - because it looks it does use a different
47:44 - formula to compute the loss but the
47:47 - error between two probability
47:49 - distributions it's optimized for that
47:52 - and in particular but not and and it's
47:57 - linked to using cross entropy with soft
48:00 - max because soft max is an activation
48:02 - function that generates a probability
48:04 - distribution from any just vector of
48:07 - numbers that seemed about right without
48:11 - going into the actual maths the formulas
48:13 - for these two things although I I might
48:15 - describe the formula for a soft Mac so I
48:17 - think that's worth doing because it's
48:18 - much simpler than it looks like on
48:19 - Wikipedia but does that seem like a
48:22 - fairly accurate explanation before I get
48:24 - into the next video
48:25 - Oh Korea scored that's exciting
48:34 - I'm gonna have to wait a minute for
48:36 - people to catch up to me I'm gonna look
48:38 - at the formula here so uh so it's just
48:41 - the sum of probability
48:43 - P Times log of probability Q that's
48:46 - pretty simple actually H of P is the
48:49 - entropy of P the cross entropy for
48:52 - distributions P and Q is this and for
48:57 - discrete P and Q that's what we have
48:59 - here it means this okay I'm not gonna
49:02 - get into the mass of cross entropy the
49:10 - slack still that's not everyone just
49:13 - talking about Mac versus Windows so I'm
49:15 - just gonna quit slack because it's
49:17 - causing my computer to freeze up here
49:19 - how am i doing timewise it's only noon
49:22 - okay I think we're pretty good
49:28 - that's a nice definition of entropy like
49:32 - you know lack of order or predictability
49:39 - all right I I'm not seeing anyone
49:42 - complaining about my explanation like no
49:45 - there's like a completely separate
49:48 - discussion going on in the chat that has
49:50 - nothing to do with what I'm talking
49:51 - about so I'm just gonna assume that I'm
49:53 - gonna move forward Oh
49:58 - how much time is left in the Mexica game
50:00 - okay thanks Dan I'll take that as a yes
50:05 - all right
50:11 - Oh Korea's beating Germany and realize
50:15 - Korea and Germany were playing entropy
50:17 - is a mathematical way to determine how
50:19 - think Thank You Jo two micron who writes
50:21 - entropy is a mathematical way to
50:23 - determine how chaotic something is in
50:25 - this case it's a way to measure how two
50:26 - different how different two tensors are
50:29 - to one another that is a great way of
50:30 - explaining it so let me let me say that
50:32 - again in my own words so in other words
50:34 - entropy as a measurement of how chaotic
50:39 - something is so cross entropy is
50:43 - basically looking at this probability
50:45 - distribution that the neural network
50:47 - generated and the sort of known correct
50:49 - one and what's the sort of chaos that
50:51 - exists in between coats what's the cross
50:54 - entropy the error between those two
50:56 - probability distributions how if we are
50:58 - using them as problems we're generating
51:00 - an outcome like how chaotic is that
51:02 - outcome going to be well with the 100
51:05 - vector it won't be chaotic at all it
51:06 - will always be just the one thing
51:08 - because it's a hundred percent this
51:10 - label but with this what's the cross
51:12 - entropy there I think that's a good way
51:13 - of describing it all right let's move on
51:21 - all right okay I can't look at the chat
51:29 - now
51:41 - okay okay I'm back hi I'm back to create
51:50 - an optimizer and a loss function and
51:52 - compile my model for this color class
51:54 - fired another thing at the end of the
51:56 - previous video I was talking about oh I
51:58 - want to use I gonna use the thing called
52:01 - softmax
52:02 - and I want to create an optimizer I want
52:04 - to use mean squared I will use
52:05 - categorical cross entropy instead of
52:06 - mean squared error boy did I really kind
52:08 - of box that the optimization function I
52:11 - want to use this is something I'm
52:13 - choosing from for example what I'm going
52:14 - to use is stochastic gradient descent
52:17 - this is the optimization function that
52:24 - whole let me let me start this over I
52:32 - need to get my head straight here
52:45 - okay all right here we go I'm actually
52:53 - going to I think while I'm recording I
52:55 - don't really need this laptop even open
52:59 - because I have to there's no slack
53:01 - channel and I have the YouTube chat over
53:02 - here although actually I just realized
53:05 - it could be useful because I do have my
53:09 - pre-made code which I rarely do in
53:12 - advance circuit I do have that over here
53:16 - so I think that would be useful to have
53:18 - available to me okay alright I'm back in
53:26 - part 471 of building a color classifier
53:30 - now what am I gonna do here in the
53:32 - previous video I created the
53:34 - architecture of my model a hidden layer
53:37 - and output layer a sixtieth 10 ths
53:41 - sequential model to dense layers
53:43 - activation functions units etc now at
53:46 - the end of the last video said only the
53:48 - next thing I need to do is define an
53:49 - optimization function and then compile
53:53 - the model well I really botched that is
53:55 - what there's three things I need to do
53:56 - optimization function loss function and
53:59 - compile the model and so I kind of
54:02 - conflated optimization and loss I'm
54:04 - optimizing against the loss but the
54:06 - optimizer that I want to make is I can
54:11 - use Const I guess here I get a very
54:14 - inconsistent about winning using
54:15 - converses let maybe I'll go back and
54:17 - clean up that code at some point I'm
54:18 - gonna say I can get it from TF train
54:23 - stochastic gradient descent and I can
54:26 - create a learning rate which I'm going
54:27 - to say is like 0.2 so this so one thing
54:31 - to do is create an optimization function
54:32 - right there are different options and we
54:34 - can try other options stochastic
54:36 - gradient descent is the one that I
54:37 - basically used in almost all of my
54:39 - examples and covered in detailed in my
54:41 - how to build a neural network from
54:42 - scratch series and the idea of created
54:46 - descent is walking along trying to go
54:48 - down the graph of the loss function to
54:51 - minimize that loss so what is the loss
54:54 - function that I want well if I'm just
54:57 - say model
54:58 - compile I believe this is a whoops this
55:03 - is a function that I'm going to write
55:04 - with a configuration option and one of
55:10 - the things when I compile the model I
55:12 - need to specify up to optimizer
55:15 - optimizer ena this is very awkward that
55:17 - I just called this up here but that's
55:22 - fine and then the other thing I just
55:24 - specify is a loss function mean squared
55:27 - error so this is typically what I have
55:31 - done in previous examples if you look at
55:34 - my X or coding challenge but this is now
55:38 - going to change and the reason is
55:40 - because I am using an activation
55:42 - function called softmax so let's talk
55:44 - about what softmax
55:46 - is softmax
55:50 - question mark okay so remember the
55:54 - output that we want from the neural
55:58 - network is a probability distribution
56:00 - right what's an example of what an
56:03 - output might look like it might look
56:05 - like this there's nine values 0.1 0.1
56:08 - 0.2 0 zero zero zero point seven zero
56:13 - zero right
56:15 - oh-ho my math is off zero point six
56:18 - right these all add up to a hundred
56:21 - percent this is the idea we're what this
56:25 - is saying is this particular RGB color
56:28 - has a 60% chance of being you know
56:31 - bluish if that's the particularly ball
56:33 - that matches with zero one two three
56:35 - four five index number six a 10% chance
56:38 - of being reddish a 10% chance of being
56:41 - purplish and a 2% chance of being
56:42 - greenish this is what we want now the
56:45 - training data is encoded like this and
56:48 - maybe we can actually look at it right
56:52 - next to it maybe this is what the
56:53 - training data looks like zero zero one
56:55 - zero zero zero zero zero zero a one hot
56:59 - encoded vector because actually the
57:02 - correct label for that color is greenish
57:05 - so I need a loss function sorry
57:11 - let's good cat across entropy and soft
57:15 - backs are linked together they're used
57:17 - together so that's why I just can't
57:19 - remember which one I'm explaining but I
57:21 - need a loss function to give me the
57:24 - error between this probability
57:26 - distribution and this probability
57:27 - distribution but I need my neural
57:29 - network to generate a probability
57:31 - distribution in the first place
57:33 - activation function as you might recall
57:35 - is something that squashes any number
57:38 - into some range it's one way of thinking
57:40 - about it the sigmoid function if we were
57:42 - to graph that sigmoid function it looks
57:45 - like a boy can never do this something
57:49 - like this oh boy that's a terrible graph
57:50 - of it you look it up on Wikipedia
57:52 - something more like this right and this
57:54 - the top is one the bottom zero so any
57:57 - number given to sigmoid results in a
57:59 - number between zero and one softmax is
58:01 - an activation function that not only
58:03 - squashes the values that are coming in
58:07 - to these outputs between zero and one
58:08 - but guarantees that they all add up to
58:11 - one
58:11 - now you might say to yourself that's
58:14 - easy that's very easy to do we do this
58:16 - all the time with normalizing data I
58:18 - could just find I could just take all of
58:21 - the outputs add them all up and then
58:23 - divide each one by the sum of the total
58:25 - right because let's say somewhere I have
58:28 - these numbers two two one five right I
58:35 - can add all these up and they're going
58:39 - to add up well look at that they added
58:40 - up to ten let me divide by ten I have
58:43 - 0.2 0.2 0.1 0.5 so this we could do this
58:49 - sort of like divided by the sum as our
58:52 - activation function in but that's but
58:53 - but this is not going to give us an AK
58:55 - an accurate probability distribution
58:57 - that we want for this scenario and
58:59 - softmax is another way of doing the same
59:01 - thing with more that that sort of
59:04 - expands the difference this one makes
59:06 - this one much more likely expands the
59:09 - difference between these different
59:10 - values so the way that softmax works is
59:13 - we actually do the following you know
59:16 - that
59:17 - I gotta find it Eraser know that natural
59:20 - number e for natural log to point
59:25 - seven something I think well what if I
59:27 - said and took E squared e squared e to
59:31 - the 1 power e to the 5th power what if I
59:34 - took all of these what if I took all of
59:39 - these and then added them all up and
59:44 - made that I'll call that the e sum and
59:47 - then just took each one of these values
59:49 - and divided by E song that is softmax in
59:53 - a nutshell
59:53 - you'd only think I'm gonna do I'm gonna
59:54 - have a like a tan tangent video that you
59:56 - can go and watch now where I'm actually
59:58 - gonna write the code for the softmax
59:59 - function we'll explain it better
60:01 - only it's worth doing that in this video
60:02 - but I'm gonna I'm going to do that in a
60:05 - separate video so look for that in the
60:06 - delete it look for a link to that in
60:08 - this video's description just to like
60:14 - just to be sure that I'm right about
60:17 - this we can now go here and this makes
60:19 - it look like oh my god this is like the
60:21 - craziest scariest thing in the world but
60:24 - you could see it right here the softmax
60:26 - function for a vector of values z means
60:30 - take every value e to that z index J
60:37 - power divided by the sum of all of those
60:40 - values and so that and you can see here
60:43 - the probability theory the output of the
60:45 - softmax function can be used to
60:47 - represent a categorical distribution a
60:49 - probably miss tribution over k different
60:51 - possible outcomes time out for a second
60:59 - so what i want to do is you know i'm
61:03 - going to do that in a different video
61:05 - okay yeah
61:15 - okay
61:32 - okay okay
61:41 - alright so again in a separate video I'm
61:43 - gonna write the code for softmax and
61:45 - actually it's right there intensive load
61:47 - is also as functions for doing it and
61:49 - I'm gonna compare what those outcomes
61:51 - look like versus just summing and
61:53 - dividing but I'm gonna move on and say
61:56 - so if I've established that softmax is
62:01 - what I'm using as the activation
62:03 - function for the last layer the output
62:05 - layer the question then becomes what
62:09 - loss function should I use how do I
62:11 - calculate the error between the node the
62:15 - target outputs with the training data
62:17 - and what the what the model generated
62:19 - during the training process
62:20 - so again mean squared error would work
62:22 - here but I am gonna change that to
62:29 - categorical cross-entropy why am i using
62:33 - that so first of all what is entropy
62:37 - entropy is a term that refers to like
62:39 - the chaos associated with the system so
62:42 - you can think of a probability
62:43 - distribution is like being very chaotic
62:45 - or more or less chaotic so what the
62:48 - cross entropy function is a loss
62:51 - function designed to compare to
62:53 - probability distributions and look at
62:55 - how much chaos there is in between that
62:57 - the cross entropy between them and the
63:00 - math of it is you know mean squared
63:03 - errors like subtract take this one -
63:05 - this one and then do like the square
63:08 - root square it then do the square root
63:10 - or make it don't do the square root then
63:11 - add them all together me and squared
63:13 - error I've talked about that you can
63:15 - look it up it's a pretty simple
63:16 - mathematical function cross entropy if
63:19 - we look at it we get we could build that
63:22 - I could build this in a separate video
63:23 - which might be worth doing as well is
63:25 - really just the if if I have two
63:27 - probability distributions P and Q I'm
63:30 - looking at the mine- the sum of one
63:33 - probability distribution times the log
63:35 - of the other probability distribution so
63:37 - again you can research what cross
63:39 - entropy how the math behind it works
63:42 - more in more detail and maybe I'll do a
63:44 - video about that for those who are
63:45 - interested but at the moment the
63:48 - important thing to do where am i over
63:50 - here
63:51 - thing to realize is that softmax is an
63:53 - activation function for generating a
63:55 - probability distribution and
63:56 - cross-entropy
63:58 - is a loss function that works well for
64:01 - comparing to probability distributions
64:03 - so for a classification problem those
64:06 - are the two things we want to use pause
64:12 - wait if it's a binary classifier it's
64:17 - better if Dan use binary cross-entropy
64:19 - yeah yeah yeah that's a good point
64:22 - okay um in the chat someone just
64:26 - mentioned that you know if it was a
64:28 - binary classifier there's only two
64:29 - possible outcomes hot dog or not hot dog
64:32 - I think is the classic example now then
64:34 - I would use binary cross-entropy
64:37 - so again there is no be all end all for
64:41 - the loss function you choose I'm just
64:43 - showing you one scenario with the idea
64:45 - of and locking your mind to think about
64:47 - well let me research all these other
64:48 - loss functions and why would he use one
64:50 - with its the other and what's available
64:51 - as part of that I get for free as part
64:53 - of tensorflow digest okay so we've done
64:56 - that oh I think I'm done with this video
64:59 - let me just uh let me just kind of like
65:02 - run this code oh wait we got it unknown
65:04 - loss ah okay I think this is lowercase e
65:08 - okay there we go
65:11 - so so now we're done what is the next
65:14 - step what am I gonna do in the next
65:15 - video it is now time for me to call
65:19 - model dot fit model dot fit is actually
65:24 - the function I will call with the X's
65:27 - and the Y's that I've prepared in a
65:29 - previous video to train the model right
65:32 - I really only got two steps left and I'm
65:33 - sure there's gonna be lots other stuff
65:34 - that are forgetting about right now I
65:36 - want to train the model then I want to
65:38 - use the model to give me a label for a
65:41 - new color that the user is going to
65:43 - specify okay so in the next video I'm
65:45 - going to actually add model dot fit see
65:49 - you then
65:53 - oh please talk about the difference
65:57 - between stochastic gradient descent and
65:59 - gradient descent that's a good question
66:07 - I should talk about that somewhere maybe
66:09 - cannot I think I need to do like little
66:11 - aside videos let me see if I can answer
66:14 - this down so we can tell me if I'm
66:16 - correct stochastic rate I always forget
66:18 - which ones which but stochastic gradient
66:20 - descent is the one where you run you
66:26 - some a whole bunch of like loss function
66:29 - values and then compute the gradients as
66:31 - opposed to computing the gradients for
66:33 - each data point one at a time
66:35 - it has to do with went to you when do
66:38 - you tune the mod when do you tune the
66:39 - weights like here's a data point what's
66:42 - the error here's a data point what's the
66:43 - error here's a data point what's there
66:44 - do I do that a hundred times
66:45 - then tune some weights or do I some
66:47 - weights in between each data point I can
66:49 - never finish
66:51 - doing a bit stochastic means doing it in
66:54 - batches okay there we go all right how
66:58 - am I on time twelve ten that's pretty
67:00 - good I kind of want to be done by one if
67:02 - I can so we're gonna do model dot fit in
67:05 - the next video all right
67:15 - all right
67:26 - whoa binary cross-entropy is for
67:28 - multi-label classification x' whereas
67:30 - categorical cross-entropy is for
67:32 - multi-class classification where each
67:34 - example begun belongs to a single class
67:36 - oh I messed up so I totally miss explain
67:44 - that match I wonder if you can cut out
67:46 - the thing where I mentioned binary
67:48 - cross-entropy no people are asking I
67:56 - guess about my this is a google Summer
67:58 - of Code t-shirt yes so I do I've for the
68:01 - lot I think since 2011 I missed one year
68:03 - I have done I'm the organ the org
68:08 - administrator and sometimes a mentor for
68:10 - the processing foundation google Summer
68:11 - of Code shoot so Matthew let's figure
68:19 - out what to do about that binary
68:21 - cross-entropy thing I guess it's not the
68:24 - worst thing that I mentioned in there
68:27 - but I got it slightly wrong by the way
68:31 - this is something that came up in that
68:32 - YouTube conference that I went to like
68:34 - there's no way to correct a video there
68:38 - used to be those annotations and you
68:39 - could add there's no way to correct a
68:41 - video without once it's been published
68:44 - without like just removing it and
68:45 - uploading a whole new video binary
68:48 - cross-entropy
68:48 - look at this
69:02 - anyway I'm not gonna worry about it too
69:03 - much okay so let me get to let me uh
69:13 - cycle the cameras
69:28 - okay slack come back I'm out of water I
69:35 - may need to go get some water
69:40 - all right
69:46 - okay
69:50 - good the chat all right it's time it's
69:56 - time to fit our model here we go so so
69:59 - far you know hopefully you've watched
70:01 - all the previous parts of this series if
70:03 - you haven't that's fine too but what
70:05 - what I have so far is I prepared my data
70:08 - set
70:08 - loaded it from a JSON file I've turned
70:10 - everything into tensors and then I
70:12 - created a Model T F using touchflo data
70:16 - has a TF sequential model which is
70:20 - designed to receive RGB inputs and
70:23 - output a probability distribution for
70:26 - color labels and you know again this is
70:28 - somewhat of a trivial scenario but I'm
70:30 - classifying data simple data with just
70:33 - three values all between zero one and
70:35 - nine possible categories or labels okay
70:38 - so that's what I've done so far so now
70:42 - that I have this this is actually like
70:44 - it's always the feeders gonna be over in
70:45 - like two seconds not really all I need
70:47 - to do is call model dot fit so modeled
70:51 - outfit now what do I need to pass to
70:53 - model dot fit well the idea of model dot
70:55 - fit is that I'm saying hey here's the
70:57 - training data here are all the inputs
70:59 - and their associated target outputs
71:02 - which I have called X's and Y's now I
71:05 - think I'm gonna get an error right now
71:06 - let me just actually run this and I'm
71:09 - going to up so let me run this and see
71:11 - if I get the error that I'm expecting
71:12 - yeah so look at this Oh okay so a couple
71:15 - things
71:18 - welcome to your life doing machine
71:21 - learning shaped mismatching I didn't
71:23 - even expect this error so I have to
71:24 - think about this one error when checking
71:26 - input expected dense input to have shake
71:29 - three but got array with shape 5643
71:34 - three so I guess right I'm sending in
71:37 - not just three inputs the shape of my
71:40 - inputs is many so I think if I just do
71:48 - let me look at what an example that I
71:50 - made previously that's weird I didn't
71:53 - have this issue what I'm thinking about
71:55 - this where is this error coming from
71:59 - we'll know to think about this for a
72:00 - second
72:04 - huh I know that this batching thing is
72:09 - always an issue I thought I took care of
72:13 - that I'm just looking at the code I
72:17 - wrote yesterday but that's so weird the
72:26 - code that I wrote yesterday did is there
72:30 - something I did to it
72:39 - X's colors yeah
72:48 - Oh input shape got it that's what I
72:56 - that's the difference between the code
72:57 - that I wrote yesterday all right
73:09 - aha so I made a mistake and I used input
73:13 - dimensions where what I really meant was
73:16 - input shape let's see if we can actually
73:18 - look in the documentation here input dim
73:24 - with it for the dense layer what's the
73:27 - difference
73:28 - Oh input shape isn't actually listed
73:31 - here ah if specified defines input shape
73:36 - as is has input dimensions inside of
73:39 - brackets so actually I think this would
73:42 - probably fix it let's try this right
73:45 - really what I want is this like I'm
73:47 - going to have an array of batches of
73:50 - data each one with three values in it so
73:53 - I think no but probably what I would
73:57 - need to do is say five because I know I
74:03 - have this much data maybe I need to do
74:06 - this this is actually not the way I want
74:09 - to do it let's try this
74:11 - No so weird I know what I want is in I
74:16 - know what's going to work is this okay
74:26 - let me let me go let me go redo that
74:27 - explanation I have to think about this
74:30 - more later I went down a little bit of a
74:32 - rabbit hole though that's unnecessary
74:40 - all right let's go look at the
74:42 - documentation and see what it says there
74:46 - and I actually I've got it pulled up
74:48 - already okay so you can see what I
74:51 - specified was input dimensions if
74:54 - specified defines input shape as bracket
74:58 - input dimensions oh so actually I don't
75:02 - even need those that those array
75:06 - brackets there and that should fix it
75:09 - there we go
75:10 - but if I wanted to use those array
75:12 - brackets because I'm sending in many
75:14 - data points I could actually just
75:15 - specify the input shape directly and
75:18 - this would then have the array brackets
75:20 - around it so it's a subtle distinction I
75:22 - think because only input dimensions is
75:25 - documented let's use that one and let's
75:28 - put a 3 here okay so no peeps we've got
75:32 - that I wonder why that didn't know
75:33 - because I didn't call fit before okay so
75:35 - now I'm fitting the model I don't see an
75:38 - error I expected an error let's so what
75:41 - happens when I fit the model well it
75:43 - returns a promise model dot fit returns
75:45 - a promise if you don't know what a
75:47 - promise is guess what I have a whole set
75:49 - of videos about what a promises and I'm
75:52 - also going to be using eventually a
75:53 - weight and a sync which I also have
75:54 - videos about but right now I can just
75:56 - write the dot then the prompt fit
76:00 - returns a promise which I can then call
76:02 - a function called then to where the
76:05 - results will be passed in and I'm just
76:08 - gonna say and I'm gonna use this arrow
76:11 - syntax this es6 arrow syntax console dot
76:14 - log results and eventually I might want
76:17 - to do more with this so I'm actually
76:18 - gonna make it a full function so this is
76:20 - what I'm saying is once you fit the
76:22 - model then log the results let's see
76:27 - what happens
76:30 - waitingwaiting
76:32 - ah ok great look at this history loss
76:36 - and there's my loss so it fit that model
76:38 - it did one epoch and gave me a loss
76:45 - great so done train the model here's the
76:48 - thing I want what I want to do
76:51 - ultimately so this is actually
76:52 - way done what I want to do is first of
76:54 - all I want to train the model for more
76:56 - than just one epoch so one thing that I
76:59 - need to do here is pass in some options
77:02 - so I'm gonna create a variable called
77:04 - options and one thing I can specify is
77:08 - like epochs I'm gonna say do it for 10
77:11 - and then I'm gonna say and let's
77:13 - actually let's just say 2 right now
77:16 - because it's gonna take a while so the
77:18 - third argument to model dot fit is
77:20 - options and if I go into tension flow
77:22 - yes and I look for a model dot fit oops
77:25 - I was right there already we can see now
77:28 - these are the various options and I'm
77:29 - gonna be using a bunch of these but
77:30 - epochs is one of them the number of
77:32 - times to iterate over the training data
77:35 - so let's rub this down and you don't
77:40 - have to do I'm gonna I don't think we
77:42 - need all of this printing stuff so I'm
77:44 - gonna get rid of some of the earlier
77:45 - printing things because I don't need to
77:47 - look at all of that so much so let's run
77:50 - this whoops
77:52 - options is not defined I spelled that
77:55 - wrong I guess I still have 44 and 45
78:00 - console logging stuff which I don't need
78:03 - I didn't get an error that I expected
78:06 - yet which is kind of interesting and oh
78:10 - you know why one thing that I want to do
78:14 - is I want to update you know at the time
78:16 - of this recording I think the most
78:18 - recent version of tension flow chess is
78:20 - zero point eleven point seven well and I
78:25 - when I was previously recording I was
78:27 - using 0.4 and I think some things have
78:29 - changed so it was alright so let's let
78:31 - this run it's it's running for to epochs
78:33 - right now it's finished and I can look
78:36 - at the history and I can see both lost
78:38 - so we can see the loss went down for the
78:40 - second poque that's great now let's run
78:45 - this over ten a pox and let's run this
78:51 - and let's just console log results dot
78:54 - lost by the way or what was it is it
78:56 - results dot history dot loss might be
78:59 - that now let's look at what it is a
79:01 - history history dot loss okay so let's
79:06 - do this
79:06 - whoops I don't need that let's go back
79:09 - here hit refresh and waiting I'm gonna
79:12 - edit out this waiting part or speed up
79:16 - we're doing for ten a pox I'm have a lot
79:19 - I gotta fix I can fix this here while
79:22 - I'm here this is awful
79:28 - cloaking device oh it's back okay great
79:32 - so look at this over ten a pox the loss
79:35 - is going down this is good this is what
79:37 - we want to see now here's the thing
79:39 - what's it using to calculate that law
79:42 - huh oh there's so much to discuss I
79:45 - gotta get myself organized my thoughts
79:47 - here I want to here's I think maybe
79:49 - maybe I've done this video I'm really
79:52 - really drugged breaking this into lots
79:54 - of small parts and really what I've done
79:56 - now is call model dot fit with one
79:59 - single option the two things I need to
80:01 - do that are next one is I need to figure
80:05 - out what's getting that law like what
80:07 - data is it using to calculate that loss
80:08 - is it the training data didn't I talk
80:11 - about testing data and validation data
80:12 - should I be thinking about that it's a
80:14 - point so I've got to deal with that
80:15 - number two is I would like to I the
80:19 - point of this is I'm in a p5 sketch and
80:22 - I could say function draw background
80:25 - zero and I can run this but look at this
80:28 - it just is loading up there all the
80:30 - while while it's training I'm locked I
80:33 - don't have any ability to run an
80:34 - animation I want once it finishes I see
80:37 - the canvas I want the canvas to animate
80:39 - while it's training and I want to see
80:41 - the loss over time I want to have that
80:43 - reported back to me so those are the two
80:46 - things that I need to do I think I can
80:48 - tackle the training the testing and
80:51 - validation data thing right now because
80:53 - let's do that in this video and I'm
80:55 - going to add the animation stuff in the
80:57 - next video so first of all okay so I
81:01 - have my data set my data set has I think
81:06 - it was five thousand six hundred and
81:08 - forty three elements data points in it I
81:12 - said at the very beginning of this
81:14 - series runs preparing the data set that
81:16 - a typical thing to do is divide the data
81:19 - and again this is
81:19 - really small for proper machine learning
81:23 - model robust I probably want to have a
81:25 - much larger data set but this will
81:27 - actually kind of work just fine as we'll
81:28 - see I want to use probably the 80/20
81:32 - rule saying that 80% is actually the
81:35 - training data so I want to just only use
81:41 - why does it it's because the keyboard is
81:43 - next to this it's going sound I want to
81:48 - I want these X's and Y's to only
81:51 - actually be 80% of that original data so
81:55 - I'm not doing that I'll maybe I'll add
81:56 - that in another point that can be an
81:58 - exercise for you of the view for you as
82:00 - the viewer to take out 20% or maybe
82:02 - because my data centers are small just
82:04 - take out 10% of the data so that's what
82:07 - would be used to test the model after I
82:09 - finish training it but while I'm
82:11 - training it while I'm actually training
82:13 - it figuring out well how many input
82:16 - notes do I want what learning rate do I
82:17 - want
82:19 - what are these sort of taper parameters
82:21 - what are the parameters of this system
82:23 - that I want to try different things how
82:25 - many pucks do I want to train the model
82:26 - for what batch size do I want to use all
82:30 - these things are known as hyper
82:31 - parameters the parameters of the during
82:33 - the training process if I want to be
82:35 - playing around with those I need a
82:37 - separate data set to compute a loss
82:39 - that's not part of the training data but
82:42 - also is not part of my testing data that
82:44 - if you use when I'm completely done
82:45 - training that's what the validation data
82:48 - is the validation data is basically a
82:50 - test dataset but it's not your test data
82:53 - set when you're done and you're ready to
82:55 - publish your model it's your test dates
82:57 - that while you're doing all the training
82:58 - intensive like that jazz has a
83:01 - configuration option for model dot fit
83:03 - that just says hey use this much as the
83:06 - validation data so let's go back over
83:08 - here let's go back to the documentation
83:11 - and we can see here now I could specify
83:14 - the validation data or I could just
83:16 - specify validation split which is a
83:19 - float between 0 & 1 it's the fraction of
83:21 - the training data to be used as the
83:23 - validation data so if I come back here
83:25 - and I just add an option validation data
83:28 - and I say 0.1 I want to use 10%
83:31 - of my training data as the validation
83:35 - data that's what's going to be used to
83:37 - calculate the loss but it's not part of
83:40 - the Train down now there might be an
83:41 - issue I also want to make sure I have
83:43 - shuffle on shuffle is a parameter that
83:48 - shuffles the training data at each epoch
83:50 - because you don't always want to train
83:52 - with the data in the same order as
83:54 - you're tweaking all the weights and
83:55 - stuff as it's doing its training if it's
83:57 - in a different order it's gonna help it
83:58 - out but the validation data I think I
84:02 - looked at this before is before selected
84:06 - before shuffling so it's selected from
84:08 - the last sample so I might have a slight
84:10 - issue or if for some reason the order my
84:13 - data is in there's something weird about
84:15 - the end of it is all one label or
84:17 - something I probably won't like shuffle
84:18 - it myself manually but let's not worry
84:19 - about that right now
84:20 - but that's something definitely to be
84:21 - cautious of well this is so much to
84:24 - think about
84:25 - all right now so now that we've added
84:27 - shuffle and we've added 10% as
84:32 - validation data
84:33 - let me now run this again it's danced
84:37 - around
84:44 - oh it's finished already ten tiny boxes
84:51 - not very long so here we go
84:54 - and we can see this is good we still
84:57 - have a loss that's going down over ten a
85:00 - box maybe we can get this lost even less
85:01 - maybe one train for more epochs maybe we
85:03 - want a different learning rate we need
85:05 - to tweak all that stuff but I want to be
85:07 - least see it and see an animation going
85:09 - while I'm doing that so that's what I'm
85:12 - going to look at in the next video I'm
85:13 - gonna look at how to do an animation by
85:16 - also adding callbacks to these options
85:20 - here okay so ello and async you know I
85:23 - need to make this happen in an async
85:25 - function that's actually I think that it
85:26 - really helped me out okay I'll come back
85:28 - to that in next video thanks for
85:29 - watching so far so we train the model
85:31 - I'll be back soon oh shoot shoot I wrote
85:40 - the wrong thing hold on
85:46 - not you let's go back to where this
85:50 - finished we just run this again
85:55 - unnecessarily oh shoot and I'll fix that
86:00 - okay so I'm gonna do redo the end of
86:03 - this trip okay uh so we finished it
86:07 - trained now with the validation splits
86:10 - and OH breaking news breaking news
86:12 - getting information from the chat that I
86:15 - wrote validation data here interesting
86:17 - give me an error so if I wanted to give
86:19 - it specific validation data that's what
86:21 - I would use but I want to use validation
86:22 - split thank you for to the chat for
86:24 - correcting me there let's try running
86:26 - this again let's give it just more
86:28 - epochs a little bit more time to wait
86:30 - let's give it 50 all right all right so
86:34 - this is gonna be the sped up portion
86:35 - where I could play the ukulele but
86:38 - nobody will hear the ukulele in the
86:39 - edited version of this video
86:42 - why isn't still printing out this one
86:44 - tensor actually where is that there must
86:48 - be a labels tensor dot print alright
86:54 - [Music]
87:07 - alright so it's back let's take a look
87:13 - it's ok it's back let's take a look at
87:17 - our loss function over 50 epochs and we
87:20 - can see it's going way down to 0.75 you
87:23 - can see it's kind of stopped actually we
87:24 - kind of accidentally might have you
87:27 - could see how it kind of goes up now we
87:28 - can see like it's not able to get any
87:30 - better so we might not even need 50
87:32 - epochs but we might want to tune various
87:34 - parameters to see but I'm not going to
87:35 - worry about all that right now the point
87:37 - is I have now trained the model using
87:40 - model dot fit shuffling the data with a
87:44 - certain validation saving 10% for
87:46 - validation I'm not doing proper testing
87:48 - data yet that would come later
87:50 - and 50 ybox okay so in the next video
87:55 - what I want to do is make it so that I
87:59 - can run an animation I can graph the
88:01 - loss function over time all that sort of
88:03 - stuff and not have it kind of like
88:05 - blocking right the way it's doing right
88:07 - now the animation thread and then of
88:11 - course I also need to allow the user to
88:13 - specify a color and get a label for that
88:15 - so those are the next two steps I need
88:16 - to do see you in those videos
88:20 - okay sorry I'm my nose is running and my
88:26 - tissue box is empty I'm out of water I
88:28 - might need a little break let me see if
88:31 - I can find oh I have oh you know what
88:32 - look at me here prepared prepared with
88:36 - the Kleenex uh alright okay how are we
88:52 - doing everything everybody wait wait
88:59 - crazy teenager rights lol I was watching
89:03 - roblox and this on my recommended so did
89:06 - you are you were you watching just like
89:07 - some gaming and this was like arbitrary
89:09 - recommended to you and you just pops
89:11 - here welcome now if you don't know about
89:13 - coding what I'm doing here is probably
89:17 - like the most advanced stuff that I do
89:19 - on my channel and you might want to go
89:21 - back and watch a lot of the beginner
89:22 - tutorials if you're interested certainly
89:24 - don't have to slack is back okay let me
89:28 - see if I can get the slack channel going
89:34 - so for those of you might be wondering
89:36 - the slack channel is for sponsors of the
89:39 - of the YouTube channel or patreon
89:41 - patrons we have patreon it's the same
89:43 - thing there should be a sponsor button
89:45 - you can see I mean nobody eats the
89:46 - sponsor only if you feel so inclined
89:49 - you'll be able to watch all the content
89:51 - but I use that to have a smaller
89:53 - community people asking questions and
89:54 - discussing the stuff I'm trying to
89:56 - connect right now okay looks like it's
90:04 - coming back to me what time is it 12:30
90:07 - I'm okay on time it's gonna be way worse
90:10 - Josh in the chat asked do tutorials on
90:13 - slack BOTS no I would love to though
90:16 - okay okay let me close this thread okay
90:26 - I am now got the slack channel in
90:29 - welcome new sponsor ex-miss
90:32 - drink X so I don't know if my kids are
90:34 - at home they don't of school today pay
90:37 - for a babysitter so I could be here to
90:39 - do this live stream by the way so but I
90:43 - set it up my Phillips hue lights at home
90:44 - they blink some sponsors the channel I
90:47 - can't do that here because I'm in a
90:49 - lockdown in the NYU Wi-Fi I new sponsor
90:52 - Kyle right because I should mention the
90:53 - whole sponsorship if you're there thank
90:57 - you thank you to those very kind of you
90:59 - to sponsor it does really help me
91:01 - allocate time to doing these videos and
91:07 - algas thank you for thanking people and
91:10 - I really one thing I really need to do I
91:12 - need to do some maintenance in terms of
91:14 - like getting all the emojis stuff set up
91:18 - on the channel okay you have World Cup
91:23 - winner with prediction with tens of Lodi
91:24 - Jess ok alright I got to move on
91:29 - let's cycle these cameras cuz I got to
91:33 - get out of here
91:33 - I'm leaving for as I mentioned I'm
91:35 - leaving for a trip tomorrow for two
91:38 - weeks do my kids know how to code so my
91:41 - kids are 6 and 9 so I would say my
91:45 - younger daughter I talk and I show her
91:48 - things sometimes but she's really too
91:50 - young to do text based coding certainly
91:53 - and my son who actually is not 9 he just
91:55 - turned 10 was his birthday over the
91:57 - weekend he has done he's taking scratch
91:59 - classes and he's attended actually some
92:02 - p5.js workshops that I taught for fourth
92:04 - and fifth graders so he doesn't know how
92:06 - to do it unfortunately he fortnight is a
92:11 - thing it's very distracting anyway so
92:16 - let me having to get our gaming channel
92:17 - set up all right sorry ok ok all right
92:24 - let's move on now to the next video ok
92:34 - all right okay this is really let me see
92:40 - if I can do some where is it it's over
92:45 - here that it's like if I fold it I think
92:52 - that's better now
92:54 - the Kubo robot that sounds interesting
93:07 - okay
93:13 - okay I don't know about this fortnight
93:16 - thing I don't know if I should be
93:19 - allowing it okay all right all right
93:31 - videos seven thousand two hundred and
93:34 - sixty three of my making your own color
93:37 - classifier with previous video
93:42 - previously on making your own color
93:44 - classifier intense photo yes I worked in
93:46 - the model dot fit function so I'm
93:48 - fitting the model according to my
93:49 - training data with these options now
93:52 - what I want to do is I want to be able
93:55 - to basically see an animation graphing
93:58 - the loss function while it's doing the
94:00 - training so right now I just get a
94:01 - report when it's done so there's a few
94:03 - steps that I want to take to do this the
94:05 - first step that I want to do is I
94:06 - actually want to move this into a
94:08 - separate function so I'm gonna just
94:10 - write a function I'm gonna just make it
94:11 - a global function called train nice
94:15 - train and I'm gonna put model dot fit
94:20 - there then I'm gonna call train here so
94:23 - that's after one and let's let's uh oh
94:25 - and I'm gonna put the options here in
94:27 - this function and I'm gonna just go back
94:29 - to to a pox and I'm gonna run this Oh X
94:32 - is not defined oh boy I did all sorts of
94:35 - goofy stuff here so let's let's make
94:37 - these global variables X's and Y's I'm
94:41 - gonna need to do again just could use
94:43 - some refactoring but now it's training
94:45 - and to epochs done well you can see the
94:49 - loss functions great but still I don't
94:51 - have an animation so what I want to do
94:53 - is I want this to be an asynchronous
94:56 - function I want this function to be an
95:00 - asynchronous function to happen and let
95:02 - things keep going and guess what I have
95:04 - a video series about I do that with the
95:06 - keyword async and then if I say viii if
95:14 - I make a function async I can use the
95:16 - keyword await meaning this function will
95:20 - wait for model that fit to finish before
95:23 - it's done in
95:24 - turns a promise by the way so I can
95:27 - actually take this now I could say
95:32 - return a weight and then I can put my
95:36 - then up here right because it's gonna
95:38 - return that same promise but it will
95:43 - happen asynchronously meaning it will
95:46 - the code up here will be allowed to move
95:48 - on while this is happening in the
95:50 - background in theory but I've got to do
95:53 - more here it's the same behavior hmm
95:59 - so why is it the same behavior well I've
96:01 - set myself up for success but I don't
96:03 - have success yet and the reason why is
96:06 - that tensorflow das is using something
96:11 - called WebGL to do all of the
96:13 - calculations and it's taking over
96:15 - basically your animation or drawing
96:17 - capabilities while you're fitting the
96:19 - model however tensorflow J s comes with
96:23 - a function called next frame which
96:28 - returns a promise that resolves when a
96:30 - request animation frame has been
96:33 - completed it's simply a sugar method so
96:36 - that users can do the following a weight
96:38 - TF next frame so what I can actually do
96:41 - is kind of trigger the animation letting
96:44 - drew the draw loop go the p5.js draw
96:46 - loop is just using requestanimationframe
96:48 - itself by adding a weight TF next frame
96:53 - somehow in this async function so where
96:57 - do I add it so I have an idea I'm going
97:01 - to add something to this called
97:03 - callbacks so and I got a spell callbacks
97:07 - correctly for this to work so let's go
97:09 - back to look at model dot fit model dot
97:14 - fit and we can see that oh look at this
97:19 - this is like a list I was looking in the
97:23 - wrong place but last night when I was
97:24 - looking this up it wasn't actually here
97:27 - these are optional callbacks that can be
97:31 - called during training for example on
97:33 - train begin on train end
97:35 - EPOC begin on epoch and on back begin on
97:39 - batch end so let's just let's add on
97:42 - train begin on train end just for real
97:45 - quick so I'm gonna say I'm gonna have a
97:47 - callback on train begin and this needs
97:51 - to be a function it's gonna my life's
97:52 - gonna be easier if I just use this yes a
97:56 - six arrow notation and then I'm gonna
98:02 - have another callback called on train
98:04 - end and I'm gonna say training complete
98:09 - so I'm gonna just add these two
98:12 - callbacks so these are functions that
98:14 - are going to be executed during the
98:16 - training process let's see if I did that
98:19 - right training start and try to complete
98:24 - and I see the results wonderful so what
98:27 - if I in train begin just make it a
98:31 - slightly longer function which also has
98:34 - a weight TF next frame in it what
98:38 - happens if I await the next frame in it
98:43 - no oh huh
98:47 - do I have to say a sink here wait where
98:52 - do I put that
99:02 - all right that's not what I wanted to do
99:12 - let me go back
99:18 - we're gonna look at the chat while I'm
99:20 - taking a little okay alright alright
99:26 - nobody's discussing anything about like
99:30 - does anybody actually watch the stream
99:31 - or you just come to talk about like
99:33 - different programming languages and why
99:34 - why why why this is making me crazy okay
99:44 - all right training start training
99:48 - complete okay now let's try a different
99:51 - callback let's try on epoch end and on
99:58 - epoch and well it takes two arguments
100:03 - I'm looking over here on this computer
100:04 - because I I have some notes there which
100:07 - I don't typically do but it's the
100:09 - documentation here doesn't actually if
100:12 - we look here it's not telling you what
100:15 - the arguments are for these callback
100:17 - functions but I looked them up and so
100:21 - the arguments are the number of epochs
100:23 - so I can say num and then a log which is
100:27 - like a report so I'm gonna say num and
100:29 - logs and then what I can do so what I'm
100:33 - gonna do is I'm gonna do console I'm
100:35 - gonna write a function here with
100:36 - multiple lines of code I'm gonna say
100:40 - console.log epoch num and then I'm gonna
100:46 - say console log loss a logs dot loss
100:52 - loss so there's a property of loss
100:56 - that's in that logs object so these are
100:58 - the arguments do every time it finishes
101:00 - an epoch so I'm gonna now give it 10
101:03 - epochs let's see what happens if I add
101:05 - that call back alright
101:10 - epoch zero epoch one epoch to epoch
101:13 - three epoch four look at that so I am
101:16 - now getting I'm getting a call back for
101:19 - every one of those individual epochs and
101:21 - we can see the loss going down and then
101:23 - of course we see all of the lost values
101:24 - when we're done but now if I want to let
101:28 - it draw something
101:29 - I believe I can say oh wait TF dot next
101:34 - frame hold on
101:35 - timeout did I talk about what I'm so
101:40 - lost in to what I'm so lost like did I
101:49 - explain what TF not next frame is
101:51 - already cuz I went back and forth and I
101:53 - like went down a route and then I didn't
101:55 - want to I think I did explain it
102:04 - shoot I'm confused I'm just gonna have
102:06 - to do my best
102:07 - Matthew I'm sorry hopefully this will
102:09 - come together I might end up explaining
102:11 - the same thing twice okay if I want to
102:17 - draw something at the end of each epoch
102:20 - I want to allow the animation to proceed
102:23 - I can go and use that function TF next
102:28 - frame its whoops TF next frame which
102:36 - allows me to which allows me to sort of
102:39 - unlock the drawing thread and and let
102:43 - draw update itself so I'm gonna go and
102:45 - I'm gonna say a wait TF next frame right
102:52 - here at the end of each epoch and then
102:55 - this is also an async function so this
102:58 - now has to be an async function as well
103:03 - is that gonna allow me to do that I
103:05 - think so yes okay let's try this
103:14 - oh yeah look it's drawing now let's
103:20 - actually add an animation so let's do
103:22 - something like stroke 255 stroke wait
103:26 - for line a frame count modulus with zero
103:31 - frame count modulus with height so I
103:34 - just want to draw a line that is that is
103:38 - moving across so for example if I don't
103:41 - bother calling this train function
103:42 - all we can see here I have an animation
103:48 - that's running okay so now let's call
103:52 - the train function and see if that
103:55 - animation runs waiting waiting waiting
103:57 - waiting waiting waiting let's get to
103:59 - epoch zero I'll look at it so the
104:02 - animation is going but it's only able to
104:04 - draw once at the end of each epoch so
104:08 - while it's training if I want to let it
104:11 - unlock that drawing more often maybe a
104:14 - different callback would work better and
104:16 - in fact one something that tension photo
104:18 - jess is doing behind the scenes and
104:23 - model dot fit sorry is in the callbacks
104:29 - right it's actually batching the data so
104:32 - I have five thousand six hundred data
104:34 - points it's actually running the
104:37 - gradient descent algorithm in batches
104:40 - that's what stochastic gradient descent
104:41 - means and there are also on batch begin
104:44 - on batch ends and I could sort of
104:46 - specify the batch size I'm letting it
104:47 - use a default so what I actually think
104:50 - that I want to use it's gonna be able to
104:51 - it does a batch pretty quickly a full
104:53 - epoch takes quite a bit of time so I can
104:55 - actually do on batch end what I'm going
104:58 - to do here is I'm gonna add one more
105:01 - callback on batch end and I'm gonna make
105:05 - this the async one so it also has a
105:09 - batch num number and a number of logs so
105:13 - it's the like epoch end but I'm gonna
105:16 - put the await next frame in there
105:20 - this one no long as it needs to be an
105:22 - async function so this should unlock the
105:24 - animation much more quickly because it
105:26 - lets it draw every at the end of every
105:29 - batch so let's go to this now and we
105:33 - should see yeah look at this so the
105:34 - animation is running just fine and we
105:37 - should see now it got a little glitch
105:41 - there when I got to the end of epoch 0
105:43 - let's see if it does that again no I
105:45 - don't know what so the first epoch they
105:47 - must have had to do some copying onto
105:49 - the GPU I'm not sure why but you can see
105:51 - the animation is no longer study
105:52 - stuttering from epoch to epoch
105:55 - okay so now we have a trading the model
106:00 - with an animation going let's at least
106:03 - so what I really should do is graph the
106:06 - loss function and by the way I can look
106:08 - at the loss function at the end of each
106:09 - batch so I can get a much more quickly
106:11 - updated loss function so I'm gonna leave
106:13 - that as an exercise to the viewer but
106:15 - I'm gonna just what I'm gonna do is I'm
106:17 - gonna say let loss P and I'm gonna
106:24 - create a paragraph element again I'm not
106:25 - really being thoughtful about design and
106:28 - interface here loss so what I'm going to
106:31 - do here is it's just going to have a
106:33 - paragraph element that says loss in it
106:35 - and what I'm going to do is instead of
106:40 - logging the loss to the console I'm
106:42 - gonna say loss P dot HTML and this is
106:46 - using the p5.js Dom library I give use
106:48 - native JavaScript or jQuery I'm going to
106:50 - put this loss information into that
106:53 - paragraph element so now I have an
106:58 - animation going and then as soon as I
107:00 - get to the end of the first epoch I have
107:02 - to talk for a bit here I see the loss
107:05 - function so now I'm training and getting
107:07 - a report of the loss function so for you
107:09 - I'm in the next video what I'm gonna add
107:11 - is inference or prediction I'm gonna
107:14 - allow the user with sliders to specify a
107:16 - color and have the label returned to me
107:18 - and what I would say to you as an
107:20 - exercise is see what happens if you can
107:22 - query the loss function with the batches
107:25 - and graph it over time and so you see
107:27 - that would be an exercise to you as the
107:29 - viewer and I'm gonna publish a github
107:32 - repo with this finished project so you
107:35 - can look for the code this is very
107:36 - confusing but you can look for the code
107:37 - it'll be linked in the description in
107:39 - two different places there'll be the
107:40 - code that matches exactly this video and
107:43 - then there will be the code that's in a
107:45 - separate github repo that and someone in
107:47 - the future people will be contributing
107:48 - to that will have maybe the graph and
107:50 - other kind of designing things that
107:51 - people have from the community have
107:52 - added okay great so one more video to go
107:56 - I think and and then some other
107:58 - ancillary ones that I forgotten about
107:59 - but one more core video to this tutorial
108:02 - series which is adding the
108:04 - and I will see you if you're really
108:06 - gonna watch all of these I will see you
108:08 - in the next video you can check the
108:10 - video's description for the next video
108:11 - link okay
108:32 - okay so that's interesting I should have
108:34 - been looking at the chat maybe I'll
108:35 - cover this in the next video let me try
108:37 - some things so you're saying if I just
108:41 - say return TF next frame I don't need
108:46 - the await an async thing I like this
108:57 - better and then in fact I can just say
109:01 - TF next frame that's the callback I want
109:04 - to happen oh that's way nicer so I'm
109:12 - gonna explain this at the beginning of
109:14 - the next video yeah all right great
109:18 - Oh Dave into the chat let's see where
109:24 - can I find the code to the 500k
109:26 - subscribers map Doozers I'm Way behind I
109:28 - haven't published it yet I you know once
109:31 - the video gets publishes or like a
109:32 - coding challenge then I publish the code
109:35 - if you hit me up on Twitter at Schiffman
109:37 - I can probably it'll be a reminder to me
109:39 - and then I'll reply to you there and
109:40 - upload that code okay okay you want
109:47 - validation loss not training loss donut
109:49 - aren't I getting oh are the do the logs
109:52 - have different loss
109:55 - I thought the logs got lost is the
109:58 - validation lost let me look at this
110:10 - oh that's the validation loss is
110:13 - different I didn't realize that okay
110:20 - great
110:22 - did I say video subscription I meant to
110:25 - say description
110:26 - did I say subscription shoot brain is
110:31 - melting I need water today wouldn't mind
110:34 - if I go get some water I gotta stay
110:40 - hydrated on the coding train as you know
110:48 - welcome new sponsor Thank You FD God my
110:52 - lights are blinking at home if I could
110:53 - be home right now I would see the
110:55 - blinking lights probably off all right
111:04 - okay
111:05 - so thank you so the things I need to
111:07 - cover at the next video are the
111:10 - validation loss versus the loss thank
111:12 - you for that and interestingly enough I
111:15 - think there is a problem with my
111:17 - validation data
111:18 - I really should I need you need to
111:20 - shuffle it and I just have very little
111:22 - data so that's not great and I need to
111:34 - talk about this simplification here of
111:36 - TF next frame
111:45 - okay
112:01 - yeah okay
112:11 - all right everyone I'm back start over
112:25 - all right oh this is getting tiring but
112:28 - I am back and I have yet another in this
112:33 - building your own custom color
112:35 - classifier with 1000 GS series now the
112:38 - thing that I want to add to this video
112:40 - and by the way this line moving across
112:42 - is pointless I just have it there so
112:43 - that I could see that the draw loop is
112:45 - animating that I haven't blocked it
112:47 - there's two things that I missed that
112:49 - are kind of important from the previous
112:51 - video um one is this is actually not the
112:55 - validation data loss I didn't realize
112:58 - this but I'm going to I'm gonna change
113:00 - this here I'm gonna I'm gonna
113:02 - console.log the full logs object so
113:07 - right what I'm putting on to the screen
113:08 - is logs dot loss let me come to the
113:12 - console log what's there so again we
113:14 - have to wait a minute for the first
113:15 - epoch to finish apologies for that
113:18 - okay there are actually two lost values
113:22 - there's the loss function can be
113:24 - computed against the training data and
113:26 - there's the loss function computed
113:28 - against the validation data now to do
113:30 - this properly I really should be using
113:32 - the validation loss because that data
113:35 - that hasn't been done with the training
113:38 - that's that's that's that's protect
113:40 - against overfitting having my model work
113:42 - really well with the training data only
113:44 - the thing is I have a very small data
113:46 - set 5,000 data points I'm just using 10%
113:50 - as the validation data and the weight to
113:52 - the footage s works it also takes that
113:54 - 10% from the end and I didn't wasn't
113:57 - careful about shuffling the data around
113:59 - so this is something that I should come
114:00 - back to I don't know maybe this series
114:02 - will go on to infinity but if I were
114:05 - doing this properly I would actually
114:07 - want to show the validation loss here
114:11 - like this log stop validation loss maybe
114:14 - I want to show both and maybe I want to
114:16 - be more thoughtful about shuffling the
114:17 - data first in advance but I that's not
114:20 - what I said I was going to do in the
114:21 - next video so I'm again leaving that ten
114:23 - rarely as an exercise to the viewer or
114:25 - I'll come back and do it in a future
114:27 - video I don't know yet
114:28 - that's item number one item number two
114:31 - thank you to me I am so me and others in
114:33 - the coding train sponsor patron group
114:36 - I made this way more complicated than it
114:39 - needs by trying to make this an async
114:41 - function in here actually this does not
114:43 - need to be an async function if I just
114:46 - return TF dot next frame so if I just
114:49 - return TF next frame it's actually
114:51 - returning the promise and unlocking the
114:53 - draw loop so that makes it simpler
114:55 - actually I couldn't make this so simpler
115:00 - what am I doing here at the end of every
115:02 - batch I want TF next frame to be
115:04 - executed and so I actually don't need to
115:07 - write a wrapper function to execute see
115:09 - if that next frame what I could just do
115:12 - is set that as the callback the callback
115:15 - again if I wanted to do more with on
115:17 - batch and look at the loss and the logs
115:20 - but really what I want is at the end of
115:22 - every batch to draw a new frame of
115:23 - animation I can just put TF knocks next
115:26 - frame as the function which is the
115:27 - callback there
115:28 - okay so let's this is still working that
115:32 - simplifies the code makes it a little
115:34 - nicer to look at I don't even really
115:35 - need this on on begin and on end but
115:40 - I'll leave those in there just so you
115:42 - see them okay so now I'm ready for what
115:44 - is the purpose of this video the purpose
115:46 - of this video is while I'm training the
115:49 - model I couldn't wait till I finish
115:50 - training the model but I've actually it
115:52 - allowed this to happen while I'm
115:53 - training the model I want to be able to
115:55 - specify a color and see what the neural
115:58 - network thinks that color is so very
116:01 - quickly to do this what I'm going to do
116:03 - is I'm going to create our slider G
116:05 - slider B slider I'm gonna make three
116:08 - sliders again this could use a lot of
116:11 - improvements and I'm gonna use the p5
116:14 - Dom library create slider function so
116:16 - the slider is a range between 0 and 255
116:18 - and let's start with like what's this
116:22 - red and green make yellow let's start
116:26 - with a yellow and so the G's the B
116:32 - slider should be on 0 and
116:37 - I want the background color too and I
116:41 - don't know what that's doing there I
116:42 - don't need this line anymore it's
116:44 - distracting I want to say our slider
116:47 - well let's actually let's so I want to
116:50 - say let our equal our slider value so I
116:55 - want to get the values from the sliders
116:56 - I want G and I won't be eventually I'm
117:01 - gonna send these as inputs into the
117:03 - neural network but right now I just want
117:04 - to be able to see that color our G B
117:07 - okay so here we go so now we should see
117:10 - there are three sliders and as I adjust
117:12 - these sliders I can change the color and
117:15 - so what I want
117:16 - whoops what I want is to be able to and
117:20 - I see though what I want is now to see
117:22 - the neural networks prediction down here
117:23 - so how do I do that okay time to use
117:28 - tensorflow touch yes again whoo
117:30 - so I need to make some input data so the
117:35 - input X's are tensor T F dot tensor 2d
117:43 - and an array with RGB in it now in
117:47 - theory I could be running prediction
117:50 - with multiple RG B's right but I'm not
117:55 - so I need an array of arrays in here so
117:59 - this is my input data then what I want
118:03 - to do I want to say model dot predict
118:06 - with those X's feel like you know what I
118:10 - need to normalize those right because
118:15 - the it expects to have normalized values
118:17 - between 0 & 1 so I need to divide each
118:19 - of those by 255 then I need to call
118:21 - model dot predict and then look at the
118:26 - results and that hat oh you know what
118:32 - this doesn't actually happen
118:34 - asynchronously
118:38 - it's the because the data is still on
118:41 - the GPU this is a confusing thing I have
118:43 - to pull I'm gonna use that date I have
118:45 - to pull it out but let's just look at
118:46 - the results of pure results so I should
118:49 - then be able to say
118:50 - results dot print okay so I think this
118:55 - is knee just creating the inputs getting
118:58 - the prediction and then I should be able
118:59 - to see that in the console syntax error
119:04 - who I have an extra extra curly bracket
119:08 - alright okay so we can see this and this
119:11 - is exactly what I should be getting
119:13 - right it is a probability distribution
119:15 - over nine labels now whether it's giving
119:19 - me correct ones who knows but look at
119:21 - that so now how do I get the label out
119:24 - of there well remember that what I'm
119:27 - looking for is I'm looking for which
119:30 - probability is at them at the highest
119:33 - level is it a ninety percent chance of
119:35 - it being yellowish and point zero one
119:38 - point zero two point zero three you know
119:40 - 1% 2% 3% of being the other ones and
119:42 - there actually is a function intention
119:44 - flow Jas that will pull out the index of
119:48 - the highest probability value that's
119:51 - called Arg max right I could write a
119:53 - little for loop or or some kind of
119:56 - function to do that but if I look for
119:57 - Arg max TF dot Arg max returns the
120:01 - indices of the maximum values along an
120:04 - axis so this is can be quite more
120:06 - complex because I could have
120:08 - multi-dimensional data but I actually
120:09 - get to do this in a really simple way I
120:11 - just want to say let index equal results
120:17 - dot r DX oh and if there's an access of
120:22 - AK c access of one the first there's a
120:24 - one-dimensional here so now let me say
120:27 - index dot print and so let me run this
120:32 - and we can see it's just giving me whom
120:36 - is that right is that a coincidence so I
120:39 - should get some different values yes
120:42 - okay so it's actually changing so that
120:45 - that's giving me that maximum index so
120:48 - as I change so so this is my label
120:51 - here's the thing though that's my label
120:53 - but I need to convert that to one of
120:57 - these so 0 means reddish one means
120:59 - greenish two beads bluish 3 means
121:02 - orangish so
121:04 - have this label list already I should be
121:08 - able to just say let label equal label
121:12 - list index the only thing is I can't do
121:18 - that because this is a tensor
121:20 - that's a tensor and what I want I need
121:24 - to pull that the tensor is the numbers
121:26 - the data that lives on the GPU the WebGL
121:28 - fancy thing that TouchWiz has
121:30 - implemented I need to pull that off and
121:33 - normally I would pull that off with an
121:35 - asynchronous function shoot normally
121:46 - normally I would pull that off with an
121:47 - asynchronous function but the thing is
121:50 - here it's such a little tiny bit of data
121:54 - I think I can pull it off synchronously
121:56 - and not slow down my program from
121:58 - running so actually what I want to say
122:00 - here is Data Sync and then which is a
122:05 - dot data would pull it off
122:06 - asynchronously so let's look at and
122:09 - let's let's say I'm a console dot log
122:13 - index and let me get rid of my other
122:15 - console logs that I don't really want to
122:19 - look at right now so okay so I got an
122:25 - array with the number in it I pulled it
122:28 - off and so then I just want to say index
122:33 - 0 so I only need that first value index
122:36 - 0 and so there we go
122:40 - that's the label number I now have the
122:42 - label number and so now I can say this
122:44 - and I can say and let's put it out on a
122:48 - paragraph element so let's say let label
122:52 - P let's have that B first and so now I
122:57 - want to say label P equals creepy and
123:02 - then I should say all the way back down
123:06 - here label P dot HTML label
123:09 - ok ready for this here we go I've
123:13 - started my training oh wait why this is
123:16 - so silly but I want the labels
123:18 - of it I really should not be changing
123:20 - this right now so let me just put it
123:24 - here okay so here we go it thinks that's
123:31 - greenish right well it hasn't gotten
123:33 - very far with the training I would
123:34 - imagine that once we train further and
123:36 - the law starts going down it's going to
123:39 - recognize that as yellowish so here I'm
123:41 - gonna just wait a little bit and I'll be
123:43 - back in a minute
123:44 - let it rain there's some a pox and I'll
123:47 - look at the chat has Schiffman open the
123:58 - slack yet no no I still have open yes I
124:00 - do have a slack open I'm gonna let this
124:04 - run for a little bit I kind of wanted
124:06 - this to appear as yellowish okay I only
124:12 - do 10 epochs I don't know I should be
124:15 - console longing the epoch there we go
124:18 - look look look look look it's learning
124:23 - [Music]
124:24 - all right back
124:26 - so he trained over ten a box and you can
124:28 - see now it's saying this is yellowish
124:30 - let me tune this down that's greenish
124:32 - turn this up
124:34 - that's bluish we've still got blueish
124:36 - can we get some purple purplish can we
124:39 - get some pink oh it didn't get pink
124:41 - maybe if I add a little more brightness
124:43 - how it thinks that's pink so I have now
124:47 - trained the neural network to recognize
124:49 - and let's see if it can get red reddish
124:52 - so we could see I could play with this
124:53 - all day long this is now going to
124:55 - classify the color based on that
124:58 - particular model so in a way I'm done I
125:00 - probably want to train it for more
125:03 - epochs what are some things that I want
125:05 - to do so one is I would want to be more
125:08 - thoughtful get more data I would want to
125:10 - be more thoughtful about the validation
125:12 - data and then other thing I would want
125:14 - to start doing is thinking about well
125:15 - does it actually work better what are
125:17 - the hyper parameters that I can play
125:18 - with for example the hidden layer I put
125:23 - sixteen units in it well or what happens
125:25 - if I use a different activation function
125:27 - for the
125:28 - lare what happens I use more nodes or
125:30 - less nodes what if I change the learning
125:32 - rate what if I change the optimization
125:35 - function if I use like the atom
125:36 - optimization function so these are
125:39 - things that all these things are things
125:43 - that I could play with and research and
125:45 - think about an experiment with to try to
125:48 - tune the model really well then at some
125:50 - point I also would want to save that
125:52 - model right save it to a JSON file so
125:55 - the trained model somehow so that I
125:58 - could load it back in without having to
126:00 - run through the training process again
126:01 - maybe I'd even want a larger dataset I
126:03 - don't to train it over a long time me
126:05 - but I want to port this code to node so
126:07 - I could let it train-like server-side
126:09 - without having to train in the client
126:10 - there's so many possibilities but I have
126:12 - now built a machine learning model with
126:17 - tensor photo how many videos this took
126:22 - that trains a model based on crowdsource
126:25 - color data and if you want if you just a
126:28 - humor me for a second if you remember if
126:32 - I go here this is the system right this
126:35 - system was used to allow people from the
126:39 - internet to click on and say that's like
126:41 - pinkish that's greenish that's blueish
126:45 - tag a whole bunch of colors save all
126:48 - that data in a firebase database
126:49 - retrieve all that data clean that data
126:52 - put it into JSON file load that JSON
126:55 - file here into this sketch build a model
127:00 - train the model with that data and then
127:04 - pulls a new color from a slider oh and I
127:06 - forgotten something
127:08 - memory management oh I knew there was a
127:10 - step that I'm missing estimating what
127:14 - category out of the fixed set of labels
127:16 - this that color is but I did forget
127:19 - something really quite important which
127:23 - is memory management let's look at this
127:25 - num memory of TF memory dot num tensors
127:35 - so again when I create tensors that are
127:38 - allocated to memory on the GPU to store
127:41 - numbers
127:41 - those don't get cleaned up automatically
127:43 - there's no garbage collector like in
127:45 - kind of regular JavaScript programming
127:47 - so 15,000 for 85 tensors now one thing
127:50 - and there's still even more and more and
127:53 - more it's growing this is a memory leak
127:55 - so one place where I didn't clean up any
127:58 - of the tensors is right here and there's
128:00 - a easy way I can clean this up
128:02 - by adding in the TF tidy function so
128:08 - what TF tidy does is it says just put
128:11 - all of this code that's inside of this
128:14 - function passed into TF tidy clean up
128:17 - any tensors that are made there so this
128:19 - will clean up everything for me so now
128:23 - let's run this again and we're gonna
128:25 - take a look at the tensors there's
128:27 - thirty one seventy three it's kind of
128:30 - leaking right well let's let it get all
128:32 - the way through ten EPOC s-- I'll be
128:34 - back in a minute when that finishes okay
128:47 - it's taking I should add that I should
128:49 - have something that's reporting the
128:50 - epochs
128:53 - [Music]
129:09 - oh it finished okay okay so the training
129:36 - is complete and we can see now ah there
129:41 - we go
129:41 - I am no longer leaking tensors now the
129:45 - thing is do I really did I really need
129:47 - 1628 tensors I don't think that I did I
129:50 - think there is also there is also a leak
129:53 - going on inside of this train function
129:57 - and I think there's an issue with this
130:01 - and so I'm gonna I might have to do a
130:03 - follow-up video about this because at
130:05 - the moment if I go to github.com TF oh
130:11 - oh hold on let's go from here too
130:17 - I should have had this prepared where do
130:20 - I go github and issues and I'm gonna
130:26 - look for fit memory leak this one so I
130:33 - believe there is at present a memory
130:37 - leak in model dot fit with callbacks and
130:40 - you can see that's exactly what I'm
130:42 - doing right where model dot fits with
130:47 - callbacks
130:48 - so I'm gonna not worry about that
130:50 - particular memory leak right now I'm
130:52 - gonna wait for us to see if that gets
130:54 - corrected by the time you're watching
130:55 - this that might already be corrected and
130:57 - this code might have no more memory
130:59 - leaks in it just by updating the version
131:01 - of tensorflow Tijs or I might still be
131:03 - missing something in here to do a memory
131:05 - leak so you know if you don't want any
131:07 - spoilers and/or the following videos
131:10 - the fallout videos have not been
131:12 - published yet you could kind of kind of
131:13 - like sort that out yourself but I will
131:15 - come back at some point and talk about
131:16 - that okay so thank you for watching I
131:20 - wish you many
131:23 - purplish and pinkish and bluish and
131:27 - greenish days all the colors of the
131:30 - rainbow may they fill your days with joy
131:33 - may you make your own classifier with
131:35 - your own data please share with me I
131:37 - don't know it has this helped the world
131:39 - this tutorial series I've missed so much
131:41 - about data and data collection and
131:43 - machine learning and bottles and
131:44 - algorithms but hopefully I've got done
131:46 - something this is not the end
131:47 - it's only the beginning well I'll see
131:49 - you soon in future tutorial videos that
131:52 - up because this playlist probably has
131:53 - about 300 ok good bye alright everybody
132:11 - ok it is 1:15 I have done what I set out
132:15 - to do today which is finish this example
132:19 - I am now going to I'm gonna leave within
132:23 - 15 minutes at 1:30 because I because I
132:27 - because I need to do that I'm gonna
132:28 - check some messages here because I have
132:30 - all these texts mission to make sure
132:32 - yeah but 11:40 somebody spoke so that my
132:39 - kids are at home with a babysitter they
132:41 - hadn't left the house yet at 11:40 the
132:43 - lights blinking so they must have left
132:46 - because I'd get a so that did get a text
132:48 - message saying that the lights blinked
132:49 - at home who was that who sponsored that
132:51 - was awesome ok so that was all right
133:04 - great
133:07 - ok so I'm going on vacation tomorrow for
133:12 - two weeks
133:14 - and so I will be online here and there I
133:18 - let's see there are currently 7 videos
133:23 - edited versions of the color classifier
133:25 - series that have yet to be published I
133:27 - could use help keeping the code getting
133:31 - the code that goes with those videos
133:32 - online if anyone is like coding along or
133:35 - I have all the stuff it
133:36 - Google Drive and wants to help with that
133:38 - you know open an issue on coda train
133:41 - calm no no github slash Cody train slash
133:44 - website could you just help with that
133:46 - but I will hopefully keep up with that
133:48 - matsya who also who is does the editing
133:53 - will be uploading and editing and these
133:55 - videos will get released so the channel
133:57 - won't appear dead but there won't be any
133:59 - live streams until the week of July 15th
134:01 - I have something that I'm really trying
134:03 - to figure out how to do and so I'm
134:05 - looking this can be discussed in the
134:07 - slack Channel I really really really
134:11 - want to get the updated version of this
134:14 - book out I have a new chapter I want to
134:17 - write about neuro evolution I want to
134:20 - fix all the mistakes update all the code
134:22 - and get a p5.js version of this book out
134:25 - this summer is the time for me to do it
134:27 - now when I get back in July I've two or
134:29 - three weeks of being in New York City
134:31 - before I'm out of town again and I was
134:34 - planning to do to live streams per week
134:36 - to keep up with the YouTube channel but
134:39 - I might need to take a more of a break
134:40 - from that so I can focus on the writing
134:42 - of this so I guess I'm curious to hear
134:44 - from people you know the you know in
134:47 - terms of like keeping a youtube channel
134:48 - going you should never what what
134:50 - everyone says is don't stop making
134:52 - content but that's not realistic people
134:53 - have to go on vacation they have to take
134:55 - breaks so I don't believe in that you
134:56 - know the views go down subscribers go
134:58 - down such is life
135:00 - but I do know that there are people who
135:03 - are funding the work that I'm doing
135:05 - through patreon and the YouTube
135:06 - sponsorships so what I want to do is
135:09 - take the temperature how people feel
135:10 - about kind of that funding essentially
135:13 - going towards the time of working on
135:15 - this book taking away some of the time
135:18 - of doing live streaming so we can
135:21 - discuss that in the slack channel I'll
135:23 - be curious for people's thoughts but so
135:25 - I don't know to what my my goal is
135:28 - always for live streams per month that's
135:31 - what I'm kind of committed to and I've
135:33 - actually done five or six the last few
135:35 - months and so July if I did to the weeks
135:38 - that I'm back if I did two per week I'd
135:39 - get to four and August I could get to
135:41 - four but I'm thinking of actually doing
135:42 - fewer and and focusing on the book so
135:46 - I'll be curious for your thoughts about
135:47 - it and me I am so me
135:49 - says that I should work on it livestream
135:53 - working on it pardon there is a way that
135:55 - I could possibly do that I'm gonna have
135:58 - to that's an interesting idea okay so
136:05 - any questions anyone wants to ask I'm
136:08 - very I also really want to get stuff in
136:11 - the shape for the channel but can you
136:15 - have guests fill in I so that's a great
136:17 - suggestion and I do want to have more
136:19 - guests than I've had guests it's it's
136:24 - the same or it's a different kind of
136:26 - work to have the guests it's not the
136:28 - same amount it's not the same work but
136:31 - having guests is would not open up more
136:34 - time to work on the writing of the book
136:36 - it would be a good thing for the channel
136:39 - and for the world I would hope having
136:42 - more guests I would like to do that and
136:44 - I definitely planning to do that but I
136:46 - don't that doesn't solve the problem of
136:48 - allocating more time to work on the book
136:52 - okay any questions from anyone
136:57 - BRIC the world asks what programming
137:00 - languages do you know I would say the
137:03 - one the language that I probably know
137:04 - the best even though I don't know any of
137:06 - them that well is Java with I don't know
137:10 - dare I say second is now JavaScript I
137:12 - mean I only started programming
137:13 - JavaScript like two year three years ago
137:15 - probably longer at this point I'm sort
137:17 - of forgetting how much time has passed I
137:19 - to program in C and C++
137:22 - I used to I mean like 15 years ago so I
137:24 - guess I know those languages and I've
137:26 - hacked around PHP and Python and Perl as
137:30 - well next time use atom with helu it's
137:36 - very fast let's try that really quickly
137:38 - so I should do a fall and want someone
137:40 - help me keep track of follow-up videos
137:41 - so let me just try that real quick if I
137:44 - put atom here and then put this here
137:53 - know how do is it just lowercase
137:59 - oh look at that hold on a sec
138:17 - it's I don't know that it's necessarily
138:20 - happening faster but looks like that
138:22 - loss went down quite a bit and it got
138:25 - too yellowish very quickly you know
138:29 - interesting
138:30 - so it's interesting again to try these
138:31 - different architectures I'm gonna put it
138:34 - back because when I published this code
138:43 - alright this is the stream over yes it
138:48 - is over any classifier ideas that we can
138:51 - practice huh interesting question well
138:55 - the next thing that I was planning to do
138:57 - I based on how long this took I'm not so
139:00 - sure about any more of us to do image
139:01 - classification and use convolutional
139:04 - layer so that's something you could sort
139:05 - of try amazingly everything this here is
139:08 - the same just the data the input data is
139:11 - two-dimensional and you'd want to add a
139:13 - convolutional layer but to hit two two
139:16 - hidden layers one being a convolutional
139:17 - layer the learning rate should be around
139:20 - 0.0001 probably yeah I you could train
139:28 - it to use the yeah what is your so show
139:30 - hum is asking where's the slack
139:32 - workspace the slack right now is open
139:34 - only to patrons or sponsors of the
139:37 - channel and you will get an invitation
139:39 - if you're a YouTube spot I'm using both
139:41 - systems right now I should probably just
139:42 - pick one at some point but if you're a
139:44 - YouTube sponsor I don't get your email
139:46 - to send you an invite automatically so
139:48 - look through that look at the community
139:50 - tab there's a post there that you'll be
139:51 - able to see where you can find out how
139:53 - to submit your email okay
140:02 - are we going to train or get the Train
140:05 - miles from Python I actually do have a
140:07 - plan to train to do a video about
140:09 - training and LS TM model with Python and
140:14 - then have that train model coming to
140:16 - JavaScript I'm actually there's a great
140:18 - post about how to do this by paper space
140:25 - well let's see if I can find this paper
140:27 - space LS TM
140:30 - Khris Khris no so we could find this
140:39 - paper space LS TN blog yep here it is so
140:47 - this is a post just recently came out by
140:52 - Cristobal who is one of the main
140:54 - maintainer x' and contributors to the
140:56 - ml5 library which and so this tutorial
140:59 - goes through how to work with paper
141:03 - space and train the model with your own
141:05 - data and then run the model in using ml
141:11 - 5 j s which is a higher-level library on
141:13 - top of tensor photo jazz so that's the
141:14 - other thing that's just too much to do
141:15 - because my other priority is to make a
141:18 - lot of video tutorials with ml 5 but
141:21 - I'll just you know um go as I go and you
141:27 - can see it running here this is so you
141:32 - can see it's doing it's predicting doing
141:34 - predictive text based on the train model
141:38 - okay well I forgot to change atom I
141:44 - thought I didn't didn't I do that right
141:47 - here people are telling me I forgot to
141:51 - change atom but I thought I did
142:02 - and I also being told me I should have a
142:04 - much lower learning rate for Adam oh
142:09 - yeah Riaan for me I would love to do all
142:11 - the things reinforcement learning is
142:13 - actually something that I am most
142:15 - interested in more so in a way than this
142:17 - sort of classification stuff which is
142:19 - I'm also interested in but I haven't had
142:21 - the time to dig into it yet i've been
142:24 - doing neuro evolution and that i have a
142:27 - whole set about that which is kind of
142:29 - reinforcement learning style thing
142:30 - oh is it possible to code the
142:34 - approximation from tau Oh June 28th I
142:38 - don't have a child a challenge oh I mean
142:45 - it should be SGD oh it should be SGD
142:50 - with oh boy the point is you to let
142:57 - everybody else play with this stuff yes
143:02 - I did end the video with SGD oh I see
143:07 - it's fine now okay I don't know what I
143:10 - got it I got it I got it all right
143:22 - because today is coming up I mean this
143:27 - is just though me this is kind of like
143:31 - where is my there it is
143:54 - if I had an idea for a really short
143:57 - coding challenge with Tao I could do I
143:59 - would but let's at least look at the
144:02 - approximating PI code
144:15 - what was it 90 something I don't think
144:24 - we're gonna get a towel day challenge
144:26 - this summer
144:43 - so what do I change here this is the one
144:46 - that's approximating pi so if I wanted
144:55 - this to approximate tau I'm doing the
145:04 - circle probability over the total x for
145:20 - don't I just double it
145:28 - I'm brain is tired
145:41 - oh no I don't I don't
145:47 - although I have to answer this for
145:48 - myself right now so
146:04 - area of the circle is PI R squared
146:12 - that's our in the area of the square is
146:18 - 2 2 times R squared which is 4 R squared
146:23 - so so so this prop this ratio which is R
146:32 - no no sorry this ratio check on
146:35 - probability a pi equals 4 times that
146:44 - probability so if I wanted to do it with
146:47 - tau what is the area of a circle with
146:52 - tau PI
147:05 - tau divided by 2r squared right
147:11 - so wouldn't that be eight we're gonna
147:18 - just be eight looks well the camera went
147:21 - off oh you can't see me sorry everybody
147:29 - sorry everybody this is what I was
147:32 - drawing I was working out the
147:34 - approximation plan I can't do the towel
147:36 - thing now your record PI things and
147:42 - means doubling - oh the learning rate
147:46 - has changed oh my god I think I had a
147:51 - point - I'm not I don't remember what's
147:59 - in this code x times x plus y times y
148:08 - that's the distance well that's the is
148:15 - less than the radius squares in the
148:17 - circle
148:18 - oh the record PI no oh I'm printing
148:29 - record PI I see oh right of course
148:36 - that's not something available
149:12 - there's no math doubt towel
149:27 - I have to do that
149:49 - oh shoot there we go there we go
150:01 - all right I don't think this is worth me
150:03 - doing as a video coding challenge just
150:05 - to update this one for Talde I guess I
150:10 - could the difference yeah you know
150:14 - you're behind I correct that in the
150:16 - future me I am so me all right so I
150:20 - could I don't think this is worth making
150:24 - into a video but just for fun cuz I'm
150:38 - here and I'm leaving and why not oh here
150:43 - you can't see me let's let me erase all
150:46 - this
151:21 - okay
151:31 - okay here we go everybody maybe I can
151:35 - figure out a way to make this totally
151:36 - ridiculous
151:37 - and it'll be worse it'll be worth having
151:46 - done this close this
152:12 - oh but of course by heart has a Tao song
152:17 - Tao
152:24 - I'll to here I just want to find the
152:29 - digits
152:45 - what do you think accurate
153:01 - [Music]
153:10 - happy holiday everybody take point two
153:15 - point eight no there's no point there
153:18 - six point two eight three one eight five
153:24 - three oh seven one seven nine five okay
153:34 - welcome to a special just me wasting
153:43 - time on the internet but you know it's
153:45 - worth noting the day we I noted Pi Day
153:49 - and I made a coding challenge which was
153:52 - an approximating PI by using a kind of
153:56 - dart throwing technique let me close all
153:58 - this stuff up we can run the example
154:00 - right here the idea is I'm throwing
154:02 - darts into my processing canvas and the
154:06 - ratio of the number of darts that land
154:09 - outside of the circle versus inside the
154:11 - circle give me the value of pi now one
154:13 - of the comments I got so much on this
154:16 - video was but you're using PI to
154:21 - calculate pi well no I'm not and I want
154:24 - to be clear about this I'm going to
154:26 - comment out all of these lines of code
154:30 - right here and I'm gonna say print line
154:34 - pi so there we go we can see my
154:38 - approximation of pi right here now I am
154:41 - sort of using PI in the sense that
154:45 - probably the value of pi is used where
154:50 - is their lips function somewhere in here
154:52 - no wait I thought maybe I use the
154:55 - ellipse function oh I do uh in where
154:57 - this ellipse function is drama let me
155:00 - comment this out so now I am not using
155:04 - the ellipse function which probably
155:06 - behind the scenes is using pi and I am I
155:09 - am only you
155:10 - the reason why I'm seeing a circle is
155:12 - because I am calculating the distance of
155:15 - each point to the center and checking if
155:18 - it's below a certain amount and if it's
155:20 - below a certain amount it means it's
155:21 - inside a circle but let's just look here
155:23 - the only place where I might be calling
155:26 - math pie is now commented out I was just
155:30 - using that to compare okay so we can see
155:33 - here that this is in fact not using pi
155:36 - but giving me this amount now why does
155:38 - this work now I already covered this in
155:40 - the previous video so you totally could
155:42 - just go away and do something else right
155:43 - now but if you don't want to go watch
155:45 - the previous video the reason why this
155:47 - works just to talk about it again is if
155:50 - this is a square and this is a circle
155:56 - right the area and this is a value
156:00 - called R the value of the sorry the area
156:08 - of the square is each side of this
156:12 - square is to R so the area of that
156:14 - square is 2 R squared - R squared or 4 R
156:20 - the area of the circle as we know maybe
156:26 - from some math class is PI R squared so
156:30 - here I am using the idea of pi but only
156:33 - only because I know that that's the
156:35 - definition of the area of a circle now
156:38 - what if I were to just throw darts at
156:40 - the wall and I would count how many
156:42 - darts landed in the circle versus how
156:45 - many darts landed overall that would be
156:48 - the same so the total darts divided by
156:52 - the circle only should be equal to well
156:56 - this before our squared for R squared
156:59 - divided by PI R squared so now what I
157:02 - want to do is solve for pi ok well first
157:07 - of all the R squared can be is gone so
157:11 - then I could say 4 times total divided
157:16 - by circle total equals 1 divided by PI
157:20 - okay I just flipped those right
157:24 - so pi equals the circle total divided by
157:32 - four times the total total that seems
157:35 - kind of right maybe I got that wrong did
157:40 - I get that wrong because four times
157:56 - circle divided by total well yeah why is
158:00 - the four on the bottom why is the four
158:07 - on the top okay so now if I reverse
158:33 - these I could say pi divided by four
158:35 - equals the circle this should say Circle
158:38 - T like the circle total divided by the
158:41 - total now I can just multiply each side
158:44 - by four and I could say four times
158:46 - circle T divided by total equals PI or
158:48 - PI equals this and we go back to my code
158:51 - we have four times the total number that
158:56 - landed in the circle x divided by the
159:00 - total overall so that's there so now
159:03 - what if I want to approximate oh well
159:09 - what is tau tau by the way is okay so
159:14 - what's the circumference of this circle
159:17 - meaning what's the length of the arc all
159:21 - the way around to PI R well that seems
159:29 - like an awkward way of writing it why do
159:31 - I have to have this to here what if I
159:33 - just had a Val
159:35 - you that instead of being PI was twice
159:38 - the value of pi then I could say the
159:40 - circumference is just that value times R
159:42 - and that's what tau is uh something it's
159:46 - another Greek letter that I've totally
159:47 - botched let me go look up how to draw
159:49 - that better
160:09 - so here's the Greek letter write
160:11 - uppercase to lowercase towel so I can go
160:14 - back and son so I didn't do the worst
160:15 - job here I guess it yeah that kind of
160:19 - that'll be so I could say it's a tau R
160:20 - and guess what if R is one the
160:23 - circumference of the unit circle is tau
160:26 - the circumference of the unit circle is
160:30 - tau naught to PI I don't know people now
160:36 - enjoy the comment section of this video
160:38 - everybody so here's the thing what if I
160:42 - want to approximate tau okay oh boy oh
160:46 - boy okay
160:50 - well the area of this circle is still 4r
160:53 - squared and the area of the - I'm sorry
160:59 - area of the square is still 4r squared
161:01 - but the area of the circle pi no no no
161:04 - PI divided cow what is it with town tau
161:07 - tau divided by 2 tau divided by 2 R
161:10 - squared ok so yeah I love this is super
161:13 - nice I gotta say it's tau divided by 2 5
161:18 - - 2 where do I want the - well welcome
161:20 - to here but do I want the 2 there but
161:28 - let's say I do this then the total
161:33 - weight for R squared or tau divided by 2
161:41 - R squared so the R's get cancelled out
161:45 - that this becomes an 8 and so now the
161:49 - value of tau guess what you just double
161:52 - it it's just double it's just double pi
161:55 - pi is just 1/2 towel whatever you want
161:58 - just live your life tau and I just ate
162:04 - and then this is tau so we now can go
162:07 - and revise my coding job that's really
162:11 - your so watching this video apparently
162:13 - I'm gonna stop this and we're going to
162:16 - we're gonna make one of these for all of
162:18 - you
162:19 - tau lovers out there this one goes out
162:21 - to all you towel
162:22 - out there from me to you we're gonna
162:25 - change this to record towel we're gonna
162:31 - draw that we don't need to draw that
162:32 - this is unnecessary code just save this
162:36 - before I forget this approximating towel
162:39 - and obviously put this on the desktop of
162:41 - my computer
162:42 - this is processing its java processing
162:44 - org everybody always asks then i'm gonna
162:47 - come down here i don't need to do so
162:49 - many each frame my formula here is now 8
162:55 - tau is 8 and I can now what I want to do
163:01 - is I need to check the record tau if I
163:05 - get one closest because redo the record
163:10 - and then now and this is now tau and
163:12 - then ah this is just math tau and math
163:15 - dot tau all right oh and this is record
163:17 - tau
163:20 - ok we're good
163:22 - I did it I redid my coding talk what
163:31 - what Java dot math hmm where are the
163:44 - constants where are the constants hold
163:48 - on math dot PI Java hmm yep pi oh there
163:57 - we go here are the constants e we get e
164:00 - we get PI I don't see tau I don't see
164:04 - tell this video is over forget it
164:08 - all right fine I'll devote to it anyway
164:11 - anyway but this is so sad this is what I
164:14 - have to do look away everybody look away
164:17 - don't look don't look
164:22 - and now we are now approximating tau
164:30 - happy towel Day everybody Oh see ya okay
164:37 - there we go
164:40 - that's an extra bonus for all of you
164:43 - people out there on the internet
164:45 - watching this now I really got to go
164:47 - almost two o'clock so again I'm gonna be
164:49 - out of town for I'm gonna be out of town
164:55 - for did I miss something
164:59 - hope not I'm gonna be out of town for a
165:02 - couple of weeks as I mentioned all these
165:03 - edited versions of these videos will all
165:05 - become live well YouTube has this new
165:08 - feature like this new premiere thing so
165:10 - I kind of want to I don't know if that's
165:11 - unlocked for me yet I don't know if
165:13 - you've heard about this but I'm
165:14 - definitely an experiment with that and I
165:20 - will see you all in the future okay I'm
165:23 - gonna I'm gonna end this I'm gonna play
165:25 - you out with my ridiculous thing that I
165:27 - always do goodbye yes so Muhammad is
165:34 - asking you didn't use PI but used the
165:36 - formula for the circles area but that's
165:38 - okay because I mean I have to use that
165:42 - but if I were just using it with dart
165:45 - throwing I wouldn't have to I just have
165:46 - the circle draw and I would see which
165:48 - ones are in and out of the circle right
165:50 - visually okay goodbye everybody
165:54 - Oh dev new sponsor thank you so much all
165:57 - right here goes something we wanted to
166:02 - make some crazy idea
166:06 - to make
166:11 - [Music]
166:22 - [Music]
166:37 - [Music]
166:51 - [Music]
166:56 - [Music]
167:00 - you
167:08 - you