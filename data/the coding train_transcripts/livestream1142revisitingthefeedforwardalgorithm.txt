00:01 - hello Wednesday people today's Wednesday
00:04 - is that right it get confused like I
00:06 - know when it's Friday because I'm here
00:09 - it's Friday but today is Wednesday I'm
00:11 - here is me Daniel Shipman for a bonus
00:13 - coding Train livestream before you get
00:16 - too excited about this bonus livestream
00:19 - here's the thing I had a regular breaker
00:23 - regularly scheduled livestream last
00:26 - Friday we could probably open it up here
00:28 - on my computer and find the YouTube
00:29 - video where it said neural networks
00:31 - continued and as I recall I spent some
00:34 - time improving the matrix library and
00:37 - then I talked a bit about a feed-forward
00:39 - algorithm and then I started to
00:41 - implement that code for the feed-forward
00:43 - algorithm in a neural network class and
00:46 - then I went home and I had a sip of some
00:48 - herbal tea and I read a book quietly and
00:53 - I don't know painted I don't I do at
00:56 - home
00:56 - what a chaos actually if really it's
00:58 - like running around and taking care of
01:00 - children but that's like what I like to
01:02 - imagine that I was doing but but and
01:03 - then at some point I had some
01:06 - conversations or reviewed some video it
01:08 - seemed apparent that even though it can
01:13 - oh yeah I got to talk about the haircut
01:14 - thing yeah okay
01:16 - even though often what is most useful I
01:21 - hear from my videos is is where a me is
01:24 - when I make mistakes and I have to
01:25 - debunk a problem and sort something out
01:27 - what I'm learning I believe and I'm not
01:30 - a hard person sure about this but my
01:31 - sense is that's particularly useful when
01:33 - I'm actually coding something and
01:35 - running into errors in the code and I
01:37 - have to go back and fix those errors and
01:38 - find those errors because that has some
01:41 - real pedagogical value however when I am
01:44 - taking the time to teach about a
01:46 - particular formula from a math algorithm
01:49 - or and diagram something on the
01:51 - whiteboard and for large portions of the
01:54 - video I have it completely wrong and
01:56 - then I go back and erase and fix it
01:58 - that edited down sometimes doesn't read
02:00 - as well so today's live bonus live
02:03 - session which a bonus is hardly a word
02:05 - to use now is just to simply revisit
02:07 - that tutorial about the feed-forward
02:09 - algorithm we actually open it up
02:13 - so let me go here and find what I'm
02:16 - looking for so in case you're wondering
02:17 - what I'm referring to
02:19 - Here I am on the coding train YouTube
02:22 - page these two edited videos came out
02:26 - from this live stream and the rest of
02:30 - not yet because if I go to whoa you can
02:40 - really hear an echo in the music anyway
02:43 - that's another problem
02:45 - I need like a secret monitor in my ear
02:49 - or something so I can listen to the
02:50 - music that I play without having it play
02:51 - out through the speakers and into this
02:53 - microphone or I just need less delay cuz
02:56 - there's a delight wait actually I could
03:00 - probably any way alright wait I was
03:02 - doing something I'm gonna go to show
03:04 - more you can't even see what I'm doing
03:06 - I'm gonna go to show more and look here
03:08 - feed forward neural networks part 1
03:10 - let's click there okay I'm gonna mute
03:14 - myself I'm gonna put myself at 2 times
03:18 - speed and I'm gonna leave this running
03:21 - behind me and I'm gonna keep talking so
03:22 - this is the part I'm gonna try not to
03:24 - look at it maybe this is very
03:25 - distracting for you
03:26 - so this oh there's one little problem
03:29 - you might notice wilderness Dan who's
03:33 - who is trapped trapped in a forest in a
03:37 - living amongst the trees for months with
03:39 - only with machine learning text books to
03:41 - read came back to civilization got a
03:44 - haircut
03:45 - so we'll understand also boosts really
03:47 - fast gestures so there's gonna be a
03:50 - little bit of a continuity problem
03:51 - because I can't recreate that outfit and
03:53 - hairstyle but a life will go on so yes
03:59 - if you're looking for I was trying to
04:02 - give a different way to say drunk Dan
04:04 - inebriated watch it 0.5 speed ok this is
04:09 - too distracting for me so this is my
04:12 - main goal for today is to revisit this
04:15 - video it is approximately 2:15 p.m.
04:18 - Eastern Time right now
04:21 - [Music]
04:23 - and I'm hoping this was going to take
04:25 - about an hour and then I will be on my
04:27 - way and we'll be back this Friday to
04:29 - continue and this fry if if all goes
04:32 - well today and I can get these other
04:34 - videos out this Friday I will do back
04:38 - propagation and training well that's the
04:40 - same thing I'm going to do the back
04:41 - propagation algorithm and then I will do
04:45 - hopefully some coding challenges that
04:46 - involve using the neural network library
04:48 - so the coding challenges will will be
04:51 - kind of stand-alone videos that make use
04:54 - of the library that's built so without
04:56 - having to code up all the neural network
04:58 - stuff in the coding challenge I can just
04:59 - make use of it that's my current
05:00 - thinking right now so I'll probably do
05:02 - it I mean I kind of don't want to but
05:04 - I'll probably do em NIST maybe X or an
05:07 - amnesty as these kind of basic
05:08 - demonstrations all right um what else
05:14 - can I say use a fake beer oh I wish I
05:16 - had a fake beard and wig to wear that
05:20 - would be awesome
05:24 - okay so whiteboard is here now one thing
05:29 - I would like to do because this is gonna
05:31 - be a video that heavily makes use of the
05:33 - whiteboard let me make some marks here
05:35 - to figure out that's the top you can see
05:41 - the tip of my finger the rightmost edge
05:45 - that you can see is about here so let me
05:49 - put something here okay that's the
05:52 - rightmost edge I don't have to worry
05:54 - about the leftmost edge because I'm not
05:55 - really gonna walk back here and then the
05:57 - bottom it's about here and let me put a
06:02 - mark here okay so I have mostly marked
06:07 - off where my area is and when I draw
06:09 - over here you can't see me which I guess
06:14 - is fine I will try to like stay here as
06:16 - my leftmost okay so that's good there
06:21 - any questions or let me let me actually
06:24 - pull up the books I'm in the wrong
06:29 - screen I was thinking of pulling up no I
06:33 - guess I guess I'm just gonna get going
06:35 - look at the chat anything
06:37 - getting oh I guess I could mention I
06:39 - don't need to mention all that stuff
06:46 - people in the chat are telling me that
06:48 - you can see the down work is that really
06:50 - a problem
06:51 - does this really ruin your ability to
06:53 - enjoy and learn from this video that you
06:56 - can see a red mark here on the bottom
06:57 - have no fear
06:59 - don't worry I've got you I hear you out
07:02 - there Internet I'm listening I will
07:05 - write the mark a little bit lower and
07:07 - just have a sense of it that it's there
07:09 - but really a little bit higher and maybe
07:11 - now that mark is invisible to you and
07:13 - you'll be able to go on with your day
07:14 - happy full of delight and without a care
07:17 - carefree and into the snow to frolic and
07:20 - be because there are no marks that you
07:22 - can see that might affect the way that
07:24 - you can absorb this material all right
07:38 - all right here we go
07:41 - I guess I feel we're just getting
07:43 - started but I'm just gonna get started
07:45 - I'm looking in the chat me ohh-kay
07:49 - weakness 20 out a flaw in my design
07:52 - system it's not as bad as a flaw that
07:55 - leads one to my accident issue a missile
07:57 - alert but that I should put marks in the
07:59 - corners that might actually be helpful
08:01 - so let's let me find the corner it's
08:04 - about over here
08:05 - whoops note here here here this is the
08:08 - corner about there and then the corner
08:12 - down here is about here and I think I'm
08:20 - not gonna worry about the corners over
08:21 - here all right
08:23 - although I should give myself a height
08:25 - mark over here which is gonna be bright
08:27 - about there okay and the camera went off
08:30 - perfect timing I am soon going to
08:34 - investigate that magic lantern firmware
08:37 - stuff but I also I feel like this is
08:41 - part of my life now if I didn't have to
08:44 - press the button every 30 minutes like
08:46 - would I be
08:47 - I would i how would I get through the
08:50 - world and interact with the air in space
08:52 - around me anything I'll say is it's a so
08:54 - balmy 76 degrees this room I filed a
08:58 - work order to have the climate control
09:01 - system of this room looked at and I
09:02 - received confirmation back that the work
09:05 - order was complete and yet it's quite
09:08 - warm in this room so I'm gonna have to
09:09 - make another attempt and hopefully on
09:10 - Friday I'll have my nice cool room so I
09:13 - can tell bright light you know all my
09:14 - wonderful sweaters that I want to wear
09:16 - for you on the internet for I'll Isis
09:18 - being the t-shirt you know etc etc okay
09:24 - ya know the camera turning off thing is
09:26 - terribly annoying I need to get that
09:27 - fixed I'm really just joking about how
09:29 - it's like a part of my battle that'll
09:31 - only be a good thing yeah I shouldn't
09:33 - get too stuck stuck in my ways
09:35 - alright it just keep me awake though
09:38 - [Music]
09:43 - alright so let's see how this goes hmm
09:49 - let's see I need to have this okay I
09:57 - think I'm just gonna begin I'm ready to
09:59 - do this why not no time like the present
10:10 - by the way this three blue one brown
10:14 - channel he's so good I mentioned that
10:18 - already it's so good it's so thoughtful
10:20 - and careful with all this design
10:23 - animation I've just it's amazing I I
10:26 - aspire someday to I guess what I do is
10:29 - just a totally different thing but yeah
10:33 - alright trying to decide which camera I
10:43 - should start with it's really gonna all
10:44 - be on the whiteboard I do need to have
10:46 - the matrix class available but do I want
10:49 - a reference I think I don't need to
10:51 - reference the continuity problem because
10:54 - it won't really become apparent this is
10:59 - sort of like a new
11:00 - so it makes sense that it would have
11:02 - gotten haircut but once you get to the
11:04 - next part
11:05 - it won't have made sense anymore I can
11:06 - always redo the next part too but I
11:08 - don't think I need to let's just go let
11:10 - me just do this
11:11 - forget about the continuity thing well
11:13 - address that later I'm standing in the
11:17 - wrong camera all right how did I start
11:27 - my previous one I started over on the
11:29 - whiteboard all right we'll do that all
11:35 - right this is a momentous occasion I got
11:37 - a haircut from the last video if you
11:39 - might have noticed I said I wasn't gonna
11:41 - mention it but I mentioned it and what
11:44 - I'm actually going to do now is describe
11:46 - we finished off I mean it's not finished
11:48 - we have to do more building this little
11:49 - matrix library that's gonna allow us to
11:52 - do some math stuff that we're gonna need
11:53 - when we implement the code for this
11:56 - particular video where I'm gonna
11:57 - describe the feed-forward algorithm the
12:00 - neural network now I want to give thanks
12:02 - to two sources that I've used primarily
12:04 - in the sort of studying and preparation
12:07 - for this video number one is make your
12:09 - own neural network by tariq rashid
12:11 - you'll find a link to this a book at the
12:13 - coding train Amazon shop in this video's
12:15 - description and also I want to thank and
12:18 - reference the three blue one brown
12:20 - channel which has a playlist I figured
12:24 - what it's called but there's a video
12:25 - called what is a neural network if
12:27 - there's at the time of this recording
12:28 - there's about four videos those are
12:30 - amazing they're animated they're
12:32 - thoughtful they're careful but they're
12:34 - really for understanding get and having
12:37 - an intuition for how a neural network
12:39 - works and for seeing all the pieces of
12:43 - an algorithm I highly recommend those
12:45 - what I'm really attempting to do in my
12:47 - videos is sort of figure out a way to
12:49 - implement a lot of the stuff that's in
12:51 - this book and those videos in code so in
12:54 - this video in the next video I'm gonna
12:55 - start working on the code this video I'm
12:57 - really gonna talk through the algorithm
12:58 - in my own words as it applies to where I
13:01 - am in this playlist I've been way too
13:03 - much time introducing this video let's
13:05 - just get right to it okay so where I
13:08 - last left off before I started working
13:10 - on matrix math stuff I hadn't built this
13:12 - simple example of a percept
13:13 - and a perceptron the idea of a
13:15 - perceptron as a single neuron that
13:19 - receives inputs so we might have inputs
13:21 - something like x1 and x2 and those two
13:25 - values go into this perceptron they are
13:30 - they are processed and then some output
13:35 - is generated often referred to as Y so
13:40 - this is where this is the thing that I
13:41 - built last I talked about some of this
13:43 - in a previous video but I need to kind
13:45 - of just rehash it again to set the stage
13:47 - of where we are so let's say we're
13:49 - trying to solve a simple problem and
13:51 - like understanding logical and so the
13:54 - inputs that into this system this
13:57 - learning system this simple perceptron
13:58 - can be a true or false or true or false
14:03 - or we can also think of that as 0 or 1
14:06 - right and we want this perceptron system
14:11 - this system to output a 1 for Y only if
14:17 - both the inputs are true so we could say
14:20 - if the if both the inputs are true then
14:22 - we want to get a 1 you know if if the
14:25 - inputs are true and false we should get
14:27 - a 0 if they're false and true we should
14:28 - also get a 0 if they're false and false
14:30 - we should also go to 0 so this is the
14:32 - kind of system now where I the thing
14:35 - that I talked about so this is great
14:37 - exciting wow we have a neuron it can
14:39 - receive inputs it can boom it could give
14:41 - me outputs all is right in the world the
14:43 - problem is most problems in life are
14:46 - more complex than this very simple
14:49 - scenario of two inputs each only a
14:52 - boolean possibility and and an output
14:55 - solving for a logical and so for example
14:57 - let's just make this slightly more
14:59 - complicated let's take this idea of
15:02 - something called X or exclusive or so if
15:05 - I were to switch this and I need an
15:10 - eraser here it is if I need to switch
15:15 - this to or this would now be the desired
15:22 - outputs for or right one thing is true
15:25 - that's what we've got
15:26 - hmmm exclusive-or means we will only
15:30 - output true if one thing is true and one
15:33 - thing is false so this for exclusive or
15:36 - will actually output a zero and if you I
15:39 - talked about this born in a previous
15:40 - video this is not a linearly separable
15:42 - problem we can't graph the solution
15:45 - space and draw a line right in the
15:47 - middle and say all the answers of all
15:48 - the answers for true on one side all the
15:50 - answers are false are on the other so
15:51 - this is where this idea of a
15:54 - multi-layered perceptron comes in most
15:57 - problems as I get further and further
15:59 - through this playlist of doing more and
16:01 - more things cannot be solved with a
16:03 - single neuron this idea of a perceptron
16:05 - this is where what happens to solve a
16:08 - problem like X or if we add a second
16:11 - neuron and send those inputs the same
16:15 - inputs into that neuron and then set the
16:18 - output of that neuron into the output
16:21 - here now we have what's called a
16:24 - multi-layered perceptron just pause for
16:27 - a second now now this is typically
16:41 - referred to as a 3 layer Network and one
16:44 - thing that I'm gonna do right now is
16:46 - even though the one of the first
16:47 - problems I'll try to solve once I build
16:49 - a neural network library is this XOR
16:51 - problem in order to demonstrate how the
16:54 - feed-forward math works it's gonna be
16:57 - clearer I think if I have if I have a
16:59 - different number of inputs than the
17:01 - number of hidden nodes
17:02 - ah-oh I said we go back go back having
17:06 - out time out time out tonight ok hold on
17:08 - hold on yeah everybody's telling me to
17:13 - layer so hold on I've seen this before
17:17 - and here's my feeling on this I said
17:19 - this previously yes this is kind of - to
17:23 - layer maybe I should say because there's
17:25 - the hidden layer and the output layer
17:26 - the inputs aren't really a layer but I
17:29 - like to think of it am I wrong Len Len
17:32 - let me look up let me look this up I
17:34 - like to think of it as 3 layer and the
17:37 - inputs being like a special case layer
17:39 - but
17:39 - 3 layer Network okay how about 3 layer
17:47 - neural network what do huh
17:51 - see so that's the question this input
17:56 - layer so I think that it's reasonable
17:59 - for me to say 3 layer it is called a 2
18:01 - layer Network okay so where are these
18:04 - sources coming from Oh to layer a what
18:09 - no ok you're right let's visit this page
18:16 - let's visit this page a 2 layer network
18:21 - one hidden layer for neurons of units
18:23 - and one output layer with two neurons
18:25 - right a 3 layer Network ok so I better
18:29 - say 2 layer well ok ok it's 5 layers
18:36 - yeah all right so it's a 3 oh no but
18:39 - somebody in the chat says it you can
18:40 - have this discussion yeah I'm gonna go
18:45 - back I want it I'm going to start this
18:46 - little section over after I took my
18:49 - break to blow my nose
18:58 - this is what's known as a two-layer
19:01 - network sometimes I'd like to think of
19:03 - it as three layers because I count the
19:04 - input as a layer itself input layer
19:06 - hidden let I said hidden so I don't want
19:10 - to say hidden yet what am I trying to
19:13 - die I lost my momentum here I'm gonna
19:17 - I'm gonna I'm gonna explain this one
19:18 - more time and I'm gonna keep going where
19:20 - would where did I really last leave off
19:21 - okay ah yes this is what's known as a
19:29 - multi-layer perceptron we have this
19:32 - output layer this you could consider an
19:35 - input layer although the input functions
19:37 - differently than these other two this is
19:40 - a layer this is a layer and here's the
19:42 - thing this is called a hidden layer the
19:44 - reason why this I'm gonna say hidden is
19:47 - right here this is input right here and
19:52 - this is output now as I get further and
19:57 - further through many videos that I hope
19:59 - to make and examples I hope to make
20:01 - we're gonna see that there are many
20:02 - complex architectures for how you might
20:05 - diagram and design a neural network
20:08 - system this is a very very basic
20:10 - beginning point and typically this is a
20:14 - two layer Network hidden layer and
20:15 - output layer input isn't really
20:17 - considered a layer although I sometimes
20:19 - do it it's like close the 3 layer
20:20 - Network and we could get to something
20:22 - like long we could have a comment thread
20:23 - about which is precisely correct for
20:25 - this so it would start that common
20:26 - thread then I'll learn but but let's
20:29 - just call this raw 2 layer Network the
20:31 - reason why incidentally this is called a
20:32 - hidden layer it's because we as the
20:34 - operators of the neural network right we
20:37 - are going to give the neural network
20:39 - data we're gonna say hey take true and
20:41 - true or take true and false so we're
20:43 - really here interacting with the input
20:44 - we also want to see the output we want
20:46 - to take the output and use it in our
20:48 - project oh you gave me false thank you
20:50 - what's the answer
20:50 - hidden are the pieces that exist in
20:53 - between where the inputs come in the
20:55 - output comes out and this is kind of the
20:57 - magic as we'll see as I get further and
20:59 - further along and solving different
21:01 - kinds of problems how this works now the
21:05 - main purpose of this video even though
21:07 - I'm being very long-winded about it is
21:09 - to talk about
21:10 - the feed-forward algorithm what happens
21:13 - to the data as it comes in passes
21:15 - through and exits through the output and
21:17 - in order to describe the math I what I
21:20 - want to do is add one additional input
21:24 - x3 so I'm gonna add that and that will
21:27 - also go into this hidden neuron and this
21:31 - hidden neuron so now you can see we have
21:34 - three inputs two hidden nodes and one
21:39 - output node and all of this is totally
21:42 - flexible you in a if you've ever looked
21:44 - at that classic hello world machine
21:47 - learning problem where you have this
21:50 - data set of handwritten images often the
21:52 - inputs are 784 for 784 pixels and the
21:56 - outputs there are 10 outputs because you
21:59 - have a probability for whether it's a 0
22:01 - 1 2 3 4 5 6 7 8 or 9 so the design of
22:04 - this how many inputs on the outputs
22:05 - how many hidden nodes how many hidden
22:07 - layers this is all food for thought but
22:10 - I just want to look at this very basic -
22:13 - where is it - the - okay okay okay it's
22:18 - a - 2 layer Network deep breathing deep
22:23 - breathing it's gonna be fine whether I
22:25 - say 2 or 3 life will go on now you might
22:28 - be wondering which weren't you just
22:31 - making all these videos about like
22:32 - matrix math how is that relevant here
22:35 - well it turns out that the math for what
22:39 - each one of these nodes do is something
22:42 - called a weighted sum what do I mean by
22:45 - that these are all connections each
22:47 - input connects each input connects to
22:50 - each hidden node each hidden node could
22:52 - connects to each output node these
22:55 - connections all have a weight and in
22:57 - fact if we call this like h1 and h2
23:00 - right this is hidden one hidden - we can
23:04 - have weights between one and one and one
23:06 - and two and two and one and two and two
23:08 - and three and one and three and two so
23:10 - those weights right if there are three
23:13 - inputs and two hiddens there's actually
23:16 - a weight matrix there are six weights
23:19 - there are six possible connections and I
23:21 - could write that like this
23:24 - I could say wait between sorry 1 & 2
23:31 - yeah weights sorry this is where I
23:38 - practiced this so many times this is the
23:42 - part where dad messes everything up I
23:43 - don't think I messed it up yet so so far
23:45 - yet but I just wanted to think about
23:47 - this for a second it's called this row
23:56 - column I totally messed this up so so
24:05 - let me write out the weights in a matrix
24:07 - so if I have a matrix that's a row row
24:11 - column I can have Row 1 column 1 Row 1
24:16 - column 2 Row 1 column 3 Row 2 Row 2
24:26 - column 1 Row 2 column 2 Row 2 column 3
24:31 - right so I have Row 1 Row 2 columns 1 2
24:37 - 3 so I so this is 1 this is a notation
24:42 - for writing all of these weights now let
24:47 - me now label the weights in this diagram
24:49 - and I'm going to do it in a particular
24:51 - way I'm going to say that this is a
24:54 - connection between hidden 1 and input 1
24:59 - this year is a connection between hidden
25:01 - 1 and input 2 this is the connection we
25:06 - didn't want an input 3 and here I have a
25:10 - connection between hidden to an input 1
25:13 - hit into an input 2 and hit into an
25:18 - input 3 you might want to pause this
25:22 - video just take a look at this and take
25:24 - a look at this and see if it makes sense
25:26 - for you the point of what I'm saying is
25:28 - what what should be intuitive or sort of
25:30 - like feel somewhat normal is like okay
25:32 - there's all these connections those
25:34 - connections 1 on 1 1 to 1 it what
25:37 - 1 and 1/2 and 1/2 there's connections
25:42 - but we're hidden one is connected to one
25:46 - two and three and hidden two is
25:47 - connected to one two and three right and
25:49 - so here are all the weights from inputs
25:52 - one two and three that are connected to
25:54 - hidden one one two three here all the
25:56 - weights that are connected to hidden two
25:58 - from inputs one two and three so so why
26:01 - am i spending all this time driving
26:02 - myself crazy and incidentally driving
26:04 - you crazy spending all this time trying
26:06 - to label these and do this matrix the
26:08 - reason is the value the math that I want
26:12 - to process here in the hidden node is
26:14 - the weighted sum of each input
26:16 - multiplied by the weight added together
26:19 - so let me give myself a little bit more
26:21 - space here on the white board and say
26:25 - that what I know that I want to end up
26:28 - with is the following I want to end up I
26:32 - got to give myself a little more space
26:33 - here hold on I should just pause here
26:38 - it's actually the right way yeah what I
26:44 - want what I know I would I want to end
26:46 - up with is this weighted sum so let's
26:48 - look at that weighted son first I want
26:53 - to take this weight weight one one oh
27:01 - give myself some more space I wrote it
27:03 - in the middle I want to I want to take
27:06 - this weight weight 1 1 and multiply it
27:09 - with this input weight 1 1 times X 1
27:15 - plus what now this weight 1 2 and this
27:19 - input Plus this weight 1 2 times X 2
27:24 - then I want to take this weight 1 3 and
27:28 - x 2 x 3 1 3 times X 3 you can see how
27:33 - this looks nice and neat the weight one
27:37 - one goes with x1 weight 1 2 goes with x2
27:39 - weight 1 3 goes with x3 and we could do
27:41 - that same weighted sum for all the
27:43 - inputs and they're weighted connections
27:44 - to h2 to hidden to
27:46 - and that's going to be wait to 1 times
27:50 - x1 plus wait to two times x2 plus wait
27:54 - to three times x3 this is the result
27:59 - that we want and I could have just
28:01 - basically done all of this without
28:03 - thinking about all this notation and
28:04 - matrices with a for loop because I could
28:06 - just say like I have an array of inputs
28:09 - over array of hit and then I have an
28:11 - array of connections and then I'm gonna
28:12 - do some for loops to multiply all the
28:14 - input but the reason why I'm spending
28:18 - all of this time doing this is it look
28:20 - at something really interesting this
28:21 - might look familiar to you if you've
28:23 - been watching the previous 3 or 4 or 5
28:26 - videos about matrices what is this
28:28 - really this is the matrix product of
28:32 - this weight matrix and a column of
28:35 - inputs if I put this right here x1 x2 x3
28:44 - and I write a big X there for a matrix
28:46 - product or I could put a dot there may
28:48 - be a dot would be a better notation for
28:50 - matrix product we can see here that
28:53 - remember what that matrix product is
28:55 - it's this row multiplied is this the dot
29:00 - product of this row with this column 1 1
29:03 - times X 1 1 2 times X 2 1 3 times X 3
29:07 - all added together and then the next is
29:10 - this row times this column so it just
29:13 - works out perfectly this is why matrix
29:16 - math is so relevant and is used in all
29:20 - of neural network deep learning
29:23 - implementations because the way that the
29:25 - feed-forward algorithm works and mind
29:27 - you I've there's a lot more to the
29:29 - feed-forward oghren that I need to talk
29:30 - about I'm talking about the bias yet or
29:33 - the activation yet there's more but I
29:34 - just want to get now to this primary
29:36 - understanding of the inputs come in we
29:40 - take a weighted sum of all the
29:42 - connections between the inputs and that
29:44 - next layer and that can be done in a
29:48 - single operation if in our code the way
29:51 - we implement it is we store all the
29:53 - weights in a matrix and all the inputs
29:55 - in a matrix the weights are going to be
29:57 - the inputs are always going to be in a
29:59 - single column
30:00 - I'm actually that could change the bits
30:02 - of the design of your neural network but
30:04 - in this case that's how that's gonna
30:05 - work get me to pause for a second
30:11 - oh yeah dot and cross products or
30:15 - something else that's a good point okay
30:20 - so now the question becomes so so far I
30:32 - feel like this is an improvement off of
30:35 - what I did on Friday where I got all the
30:38 - notation wrong constantly and
30:40 - continuously I need to go through the
30:44 - activation stuff and the bias stuff the
30:49 - question is does it make sense to
30:51 - actually break this up into multiple
30:52 - videos like should I actually have a new
30:56 - should I wrap this video up and have the
30:59 - next one go through activation and bias
31:01 - or should I just keep going what should
31:03 - be the symbol of the train I'm just
31:13 - looking at the chat
31:19 - nice Luxan we're up to 77 degrees in
31:22 - here it's getting warmer and warmer with
31:24 - my moving around so much keep going oh
31:28 - [Applause]
31:29 - yeah yeah yeah yes know what okay I'll
31:33 - keep going yeah I just look if anyone is
31:37 - anyone keeping track how much time do
31:39 - you think that was you know obviously
31:40 - there were like several minutes in there
31:42 - when I like pausing when it gets edited
31:45 - one video everyone's saying well alright
31:52 - Matt Matt Chu is asking how long do I
31:55 - think the rest will take you know 15
32:01 - minutes yeah that's what so let's let me
32:07 - keep going I think it makes sense to be
32:11 - in one video
32:12 - I started 18 minutes ago there's
32:15 - probably at least three or three minutes
32:17 - of editing there all right I'm gonna
32:18 - keep going
32:18 - we can always yeah let me keep going I'm
32:29 - being told from the live chat that's
32:30 - going on that the dot really needs
32:32 - something else and a cross product is
32:34 - something else so we're just gonna put
32:36 - the our symbol for matrix product will
32:38 - be like a nice little you know steam
32:41 - engine train thing okay
32:43 - that aside what else do I need to do so
32:47 - we've established this beginning part of
32:50 - the feed-forward algorithm the inputs
32:52 - come in the weighted sums get added all
32:56 - together inside these hidden notes now
32:59 - there are two big components that I've
33:01 - missed let me just write this over here
33:02 - one is bias and two is an activation
33:07 - function I kind of not sure which order
33:10 - to talk about these things but before I
33:13 - do either those actually I think I want
33:14 - to do is take this formula and break it
33:17 - down into something much smaller so what
33:19 - if I think of this weight matrix is just
33:21 - a big capital W then I take I need a
33:25 - simple I can't draw a train here the
33:27 - matrix product between this and the
33:30 - inputs right and so the hidden layer and
33:36 - so we can think of this as hidden is a
33:40 - column so that's really J right hidden J
33:47 - equals weights I rows and columns times
33:52 - inputs J
33:54 - and when I say times I mean matrix
33:56 - product so this is me rewriting this
34:01 - because this here if I give myself a
34:03 - little space is really like what I'm
34:05 - doing here is I'm calculating H 1 H 2
34:07 - and H 2 whoa I got that wrong let me go
34:20 - it is a regular dot product all right
34:23 - hold on let's just go back to where I
34:26 - was here because it's not I I see this
34:31 - as a column but it's rude one column
34:35 - it's it's a 1 it's a 1 there in the
34:37 - column spot it's row yeah ok I was just
34:41 - doing so well I'm a flaw of a very
34:44 - flawed person okay there was a time I
34:50 - think it was a time my life or I was
34:51 - really good at this sort of stuff and I
34:52 - don't know I just left it left I'm
34:54 - trying my best okay all right but before
34:57 - I get to the bias in activation I think
34:59 - what I want to do is rewrite this
35:09 - what okay before I get to the bias and
35:13 - the activation what I want to do is
35:15 - rewrite this as a smaller formula so I
35:17 - want to consider the weight matrix for
35:19 - example just as the capital letter W and
35:22 - I can think of it as a matrix of I rows
35:25 - and J columns I am j are kind of
35:28 - terrible cuz they look so similar but
35:30 - the weight matrix I I rose and J columns
35:32 - now we're taking the dot the matrix
35:34 - product which I'm just gonna use a dot
35:36 - here I think that's going to be fine the
35:38 - matrix product preme this and the inputs
35:41 - now the inputs is a matrix but it's one
35:43 - column but
35:44 - I rose and the point of the reason why
35:47 - I'm doing this is because I'm trying to
35:49 - get the outputs what I want is I want to
35:51 - know what number to send out of here
35:54 - the data is flowing in we get this
35:55 - weighted sum what number flows out of
35:58 - there and the number that flows out of
35:59 - there I'm going to call the hidden it's
36:02 - really I'm trying to get the output of H
36:07 - h1 h2 so the outputs of the hidden layer
36:10 - just the inputs are the inputs of the
36:12 - inputs of the hidden layer the outputs
36:14 - of the hidden layer are the inputs to
36:16 - the Yaak put a lot of inputs and output
36:18 - there but the previous layer sends in
36:21 - the input so whatever comes out of the
36:22 - hidden goes in here into the output so
36:25 - hidden I equals the matrix product
36:29 - between the weight matrix and the inputs
36:31 - but there's more so one of the things
36:33 - that I'm forgetting and I can actually
36:34 - fold these two things in right in here
36:37 - the things that I'm forgetting are one
36:38 - the bias so if you recall when in some
36:42 - of the previous videos that I've done
36:44 - that looked at linear regression and the
36:46 - simple perceptron is that correct that I
36:50 - talked about bias in the linear
36:51 - regression video in some of the previous
36:57 - videos such as in in the previous videos
37:01 - where I went to the perceptron for
37:04 - example remembered us trying to like
37:05 - find this line and what points would be
37:07 - above it or below it and I've got to
37:09 - really deal with the problem all the
37:11 - inputs could be 0 if all the inputs are
37:13 - 0 then the weighted sum is always going
37:17 - to be 0 that can't be right
37:19 - so we need this bias we sometimes need
37:21 - to make it easier or harder for it to so
37:23 - to speak fire like we want to make we
37:26 - want to like bias the output in a given
37:28 - direction so one thing I would write
37:31 - here is I'd also say plus that bias so
37:36 - and that is a single column vector as
37:39 - well so really if I'm down here again
37:44 - and boy am i running out of space but
37:46 - that's ok I would do this matrix product
37:50 - and then for every single one of these
37:52 - hidden I would have a bias b1 + b2
37:58 - right so there are weights and by it and
38:00 - by lights perfectly legitimate to ask
38:01 - the question right now huh like well
38:03 - what are the values of the weights what
38:04 - are the values of the biases this is
38:06 - what I'm going to get to like I just
38:07 - want to understand the algorithm the
38:09 - whole point of this sort of learning
38:10 - system that we're gonna create is to
38:12 - figure out how to tune all the values of
38:14 - all these weights and biases so that the
38:16 - outputs match up with what we think they
38:19 - should be the assistant needs to somehow
38:21 - adapt and learn and tune all those
38:23 - values to perform some sort of task and
38:26 - in a sense this is really just one big
38:28 - function that's why I narrow Network
38:30 - something called a universal function
38:31 - approximator it's just a function it's
38:38 - just the function that receives inputs
38:40 - and generates an output that's a
38:43 - function and we have to in theory like
38:46 - if we'd have enough of these hidden
38:47 - layers and nodes there's no inputs we
38:49 - couldn't match with some given set of
38:50 - outputs so anyway where'd he get to all
38:52 - of that but I'm back here so I need to
38:54 - add the bias inand then there's
38:55 - something else you might remember from
38:57 - the simple perceptron example that we
38:59 - had this activation function whatever
39:01 - this weighted sum plus the bias would be
39:03 - if it was a positive number we would
39:06 - turn that number into plus 1 if it was a
39:08 - negative number we would turn that
39:10 - number into negative 1 and this is
39:12 - something that's very typical of neural
39:14 - network based systems whatever these
39:16 - weighted stones come in as we wanted to
39:18 - like squash them into some known range
39:20 - and there are a variety of different
39:22 - mathematical functions that can do this
39:24 - and while this is not typically the sort
39:27 - of latest and greatest and most
39:28 - cutting-edge activation function the
39:30 - function that is we will find in a lot
39:33 - of textbooks and early implementations
39:35 - of neural networks is something called a
39:37 - sigmoid and sigmoid is a function that
39:42 - actually looks like this f of X equals 1
39:47 - divided by 1 plus e to the negative x
39:52 - sure I got that right
39:54 - let's go look at the Wikipedia page for
39:55 - the sigmoid function yes I mean I didn't
40:01 - see your image about the bias thank you
40:05 - okay so hold on I'm coming over here to
40:08 - sigmoid let's go to those Siwon oh well
40:14 - I actually got it right okay whoops oh
40:19 - this camera went off - all right people
40:27 - are telling me that index should be j4
40:32 - hold on let me just look should be j4 B
40:36 - and H I don't think so
40:39 - I really thought about this right the
40:46 - row is I the column is J the column is 1
40:50 - the row is 1 2 3 the row is I the column
40:56 - is J 1 1 1 2 1 3 am I wrong about that
41:02 - I'm in the wrong totally in the wrong
41:04 - camera right the row is I column is J so
41:12 - this is really row 1 2 3 I mean sure if
41:16 - I beat it an array because it is a flat
41:18 - array ultimately but I'm going to in my
41:20 - implementation actually have it be a
41:21 - column matrix because you had to bias
41:32 - the left and right side of the equation
41:44 - wait a sec oh yeah yeah yeah that's
41:53 - that's bad I will fix that that day but
41:58 - yeah okay Kay Kay we c'mon I would like
42:04 - to hear from Kay weak bond chat if it's
42:07 - okay that I'm using I am I get flamed in
42:14 - the comments like I will about my 3
42:16 - layer Network
42:28 - oh yeah whoops this is kind of messed up
42:31 - thank you I'll fix that well you guys
42:37 - are way behind me it's all right how's
42:39 - the hold on so--but dexif yes see above
42:53 - i-i-i don't understand above i
42:55 - underscore j and a summation yeah Thank
43:06 - You Simon I will draw the bias in at
43:09 - some point that's a good point
43:24 - I need to sum over J and it's i
43:30 - underscore J so because these are really
43:49 - this right so aren't I summing over I 1
44:00 - 2 3 J is constant but I mean oh because
44:07 - I'm taking the J of here and applying it
44:12 - to their I see I see I see
44:18 - notation is like the worst thing ever
44:33 - yeah yeah yeah yeah right right right
44:45 - apparently not would help
45:01 - yeah all right I'm just gonna keep I'm
45:03 - gonna I'm gonna do my best here to keep
45:05 - going on I'm gonna go okay the thing I
45:09 - was talking about I was coming over here
45:10 - to talk about sigmoid when I come back
45:13 - I'll fix some of that stuff okay okay so
45:19 - here we can see I did get the correct
45:21 - formula for the sigmoid function what is
45:24 - this number E E is called the natural
45:27 - number it's the base for the natural
45:29 - logarithm it's like 2.71
45:31 - something-or-other
45:32 - since we're one of these magic numbers
45:34 - or Euler's number you can read up about
45:36 - it but though and to be honest the
45:38 - sigmoid function is barely used anymore
45:40 - in sort of modern deep learning research
45:42 - but I think it's a good starting point
45:44 - for us to look at and understand why and
45:46 - in other videos I will take a look at
45:48 - other activation functions but the
45:49 - reason why the sigmoid function is used
45:51 - is because it takes any input any number
45:54 - you pass it through the sigmoid function
45:56 - you will get a number between 0 and 1
45:58 - and so this is perfect for a lot of
46:01 - scenarios it takes this input it
46:03 - squashes it so higher numbers are going
46:06 - to be much closer to 1 lower numbers are
46:08 - going to much a little closer to 0 and
46:09 - the bias is gonna can push things closer
46:12 - to 1 or closer to 0 this squashing of it
46:14 - works really well because you can kind
46:16 - of you know we could get us or a true or
46:18 - false 0 or 1 we can get a probability
46:20 - value between 0 & 1 we get to the output
46:22 - lots of possibilities so this is the sig
46:25 - and it's sigmoid function so oh so ok so
46:28 - I'm over here by the way I need to
46:29 - correct a couple things where's that
46:31 - eraser number one is I kind of did
46:33 - something horrible here because I made
46:34 - these equations not equal to each other
46:36 - anymore when I was talking about the
46:37 - bias so let me not put that over here so
46:43 - ok another thing I want to mention is
46:46 - I'm kind of fumbling here with the these
46:49 - index values we're really in what we're
46:52 - really doing is we're the matrix W is
46:56 - rows and columns so I and J the input is
47:00 - a single column vector but we're still
47:02 - iterating over I because the I of the
47:05 - weights gets multiplied with the each
47:06 - row which is
47:07 - now I of the of the input so you know
47:11 - the exact notation aside the point of
47:15 - the feed for one that the the truth of
47:18 - the feed-forward algorithm is the inputs
47:21 - come in you take a weighted sum you add
47:24 - in the bias you take that weighted sum
47:26 - add the bias pass it through the
47:28 - activation function and that result
47:30 - feeds forward towards the network and go
47:32 - straight to the output and guess what
47:34 - the output does the output takes whoops
47:39 - the weights of all the connections
47:44 - between the hidden and the output so the
47:48 - matrix product of the hiddens with those
47:51 - weights plus its own biases these are
47:53 - different biases these are the biases
47:55 - for this particularly the output node
47:57 - and then passes that through the sigmoid
48:00 - function as well so this just happens
48:02 - every sing with every single layer here
48:05 - come the inputs weighted sum passes the
48:07 - activation function send that as output
48:10 - into the weighted son of the next node
48:12 - pass of the activation function send
48:14 - that as output etc etc so I was getting
48:20 - the question it was being point out that
48:22 - he didn't draw the bias anywhere into
48:24 - this anywhere into this particular
48:26 - diagram so a nice way to draw the bias
48:29 - into the diagram is to think of it as
48:32 - another input at each layer so for
48:37 - example if there were always an input
48:39 - that had a value of 1 that connected
48:42 - like this now we have bias 1 and bias 2
48:46 - so these are just these are like weights
48:48 - that are getting weighted with an
48:49 - arbitrary input of 1 that's always
48:51 - coming in and the same thing with here
48:53 - now we have also another bias that's
48:56 - coming into the outlet so it each layer
48:58 - there is a sorry a 1 which which it's
49:02 - weight is like this bias there's only
49:04 - one so B 1 so you can think you could
49:06 - you'll often see I could probably pull
49:08 - up a much nicer diagram okay I highly
49:11 - recommend you go watch but you've just
49:13 - been watching the three blue one brown
49:14 - video all along if I'm being perfectly
49:15 - honest but this hopefully gives you a
49:18 - sense of what the pieces are the point
49:21 - of me doing
49:21 - since you could just skip ahead to the
49:23 - next video in the next video I'm going
49:26 - to start to have a matrix library which
49:28 - does this matrix math I can add two
49:31 - matrices together element wise I can
49:35 - perform a matrix product I can apply a
49:37 - function like the sigmoid function to
49:39 - every element of a matrix so I can do
49:42 - all of this I can start to write the
49:44 - code for a neural network library okay
49:47 - we have now arrived at the what is sort
49:50 - of the end of this video I'm going to
49:52 - pause and check and see what all the
49:55 - wonderful nice people who are generous
49:57 - enough to know who is the long-live have
49:59 - corrected me and see if I need to come
50:01 - back and offer any Corrections or answer
50:03 - any questions I'll be right back would
50:11 - it be clearer to write out the equation
50:13 - for the output layer as well shouldn't
50:15 - be too difficult with just two weights
50:17 - that's not a bad idea why don't you add
50:26 - the biases to the weight matrix instead
50:28 - of having an extra matrix I don't think
50:32 - that would work
50:33 - umm so Simon tiger writes a bias doesn't
50:37 - have a weight and that is correct but I
50:40 - like to think of it I mean I think a
50:42 - weight the bias these are just numbers
50:44 - they just get added in but it's the
50:46 - equivalent if I wanted to of having
50:49 - another input right it's equivalent of
50:53 - having another input of one and then
50:55 - adding another column and those would be
50:58 - the biases to the weight matrix so I
51:00 - could do that maybe I'll mention that so
51:06 - instead yeah I couldn't mention that
51:10 - [Music]
51:13 - all right you were using the same name
51:17 - for both weight matrices yes that's a
51:19 - big problem yeah I meant this to be like
51:25 - I'll call okay so let me let me let me
51:28 - let me do some of these Corrections okay
51:29 - so a couple Corrections that I should do
51:31 - is you know these are not I'm being very
51:33 - unclear about this these are not the
51:36 - same these are not the same these are
51:37 - not the same so this weights here these
51:39 - are really what I would call like the
51:41 - input to hidden weights so I'll call
51:46 - this like WI H the weights from and
51:51 - you're going to see this kind of
51:52 - variable naming when I start to write
51:53 - the code for this the weights from input
51:55 - to hidden and this is really the weights
51:59 - from hidden to output whoo WH oh so that
52:04 - that's one point of clarity that's
52:05 - important and also once again for the
52:06 - biases these are like the hidden by
52:08 - season these are the output biases so
52:12 - and you know again I'm kind of on some
52:15 - level making up my notation on-the-fly
52:16 - I'm sure if you were to read a properly
52:19 - thought out textbook or watch a three
52:21 - blue one brick on video all those some
52:23 - more thought some more precise notation
52:25 - but this is the way I'm think another
52:26 - thing that I'll mention here which was
52:28 - introduced if you know the biases you
52:30 - can kind of write it as this extra
52:33 - matrix that your column matrix that
52:35 - you're adding on but I could have in
52:37 - fact also just folded those into here
52:40 - right because if I added another input
52:44 - that's always one like I added this
52:46 - diagram here then I would just have I'm
52:49 - sorry not another row another column
52:51 - which would be the bias values so I
52:53 - could actually have b1 and b2 here that
52:57 - could also that could I could also
52:59 - express and put the bias ease in this
53:01 - matrix calculation as well I like to
53:03 - think of them separately to each one's
53:06 - own to each their own
53:07 - okay we should I say goodbye I don't
53:11 - know if I should say goodbye now let's
53:12 - probably check one we're done over that
53:13 - blue diamond superscript is common that
53:18 - would have been good change the O to H
53:23 - got it wrong change the o equals to H
53:25 - equals we're
53:27 - ah shoot no no I'm right no I'm wrong Oh
53:38 - oh dear oh boy
53:43 - all right snack let's do that here oh oh
53:46 - oh so sad hold on it's okay we're gonna
53:51 - back up we're gonna do that little
53:54 - addendum to the video again no one will
53:57 - notice and then I can use superscript
54:07 - you can have subscript for the index
54:09 - values superscript for the okay I
54:21 - totally do that wrong still not as bad
54:25 - as last time yeah
54:27 - all right okay let's now let me let me
54:35 - kind of like re let me come back into
54:45 - alright so there's definitely a few
54:47 - things wrong or misleading at least I
54:50 - would say about this that I've gotten
54:51 - some good comments from so that's all
54:52 - right
54:52 - number one is it's very important to
54:54 - realize that these two weight matrices
54:56 - and these two by C's are not the same I
54:59 - wrote them out I mean we have the hidden
55:01 - outputs and the outputs output that's
55:03 - what this is that's what this is so the
55:05 - hiddens outputs are actually the weight
55:07 - matrix between the inputs and the hidden
55:09 - so I could write that as like
55:11 - superscript up here WI H in a way this
55:14 - is the the hidden sout puts is the
55:17 - wait-wait-wait matrix between the input
55:19 - and hidden with a matrix product of
55:21 - those inputs and this weight matrix is
55:24 - the weight matrix between the hidden and
55:27 - the output h0r whoo so and you'll
55:31 - actually see as I get into the next
55:33 - video I'll start using this kind of IH
55:35 - Rho kind of naming in some of the ver
55:38 - stuff the other thing is these are not
55:40 - the same biases this is the bias for
55:43 - that's connected with each hidden neuron
55:47 - and so I could say B H there and then
55:50 - this is the bias right that's connected
55:53 - with it's just one at one bias but it's
55:55 - connected with this output neuron and so
55:57 - I could put an O here so that would be
55:59 - more clear about that another point of
56:02 - clarification is there is a way to write
56:04 - this without having this having the bias
56:08 - as part of the weight matrix itself
56:09 - because there's no reason why I couldn't
56:11 - just consider the bias like I said as an
56:16 - extra input that always comes in with a
56:18 - value of one and then what I would need
56:20 - to do where's my eraser is I would need
56:23 - to add a fourth column here right
56:25 - because this weight matrix would have
56:27 - all of these connections 1/3 and 2/3 and
56:31 - then there would be the bias values bias
56:34 - 1 and bias 2 so if you see this this
56:37 - would actually end up now I could just
56:39 - add it over here if I follow through
56:43 - with this math plus b1 plus b2 because
56:47 - it would be multiplied by 1 which would
56:49 - just be that bias value so that's an
56:52 - important thing to say how do I possibly
56:55 - finish this video some people were
56:57 - asking what am i why don't I write out
56:59 - the matrix formula for for this uh-huh
57:03 - you know what maybe I should make this
57:04 - an exercise for you and I can maybe
57:06 - include a little link to a JPEG or
57:07 - something that has it but could you do
57:09 - this same exact diagram for the
57:12 - calculations that flow once the outputs
57:15 - come out of here through this layer so
57:17 - let me just just give me a second I'm
57:21 - gonna check see if there's any last
57:23 - comments or questions or important
57:24 - Corrections I'll be right back
57:34 - all right
57:40 - all right looking I'm looking anybody
57:46 - else have anything else it's all wrong
57:50 - okay better than last time at least this
57:54 - is good so it's gonna say on my like
57:55 - tombstone better than last time well at
57:58 - least that's what so I'm a big poster
58:00 - about the coding train it's a little bit
58:02 - better than last time okay okay okay
58:07 - it's important to mention the weight of
58:13 - the bias changes
58:16 - yes but the bias baked in the matrix do
58:21 - you still add the Bisons in there for us
58:25 - feels like the Chaz more importantly
58:27 - usual for this video just name the video
58:29 - chaos I just new teachers at least yeah
58:34 - we had we can we have this great t-shirt
58:37 - idea what was that great t-shirt idea it
58:39 - was Oh something like I watched your
58:42 - YouTube channel and all I got was this
58:43 - lousy I watched all of your neural
58:46 - network matrix math videos and all I got
58:49 - was this lousy 0.27 something-something
58:53 - yeah back backpropagation is gonna come
58:56 - on Friday will not be doing that today
58:59 - no no no not today
59:02 - alright so I think I'm gonna wrap this
59:06 - up okay that in fact was the end thank
59:09 - you for watching this video to see
59:17 - something wise okay that in fact was the
59:25 - end thank you for watching this video I
59:27 - do want to mention that I I'm in the
59:28 - chat or a lot of really useful and
59:30 - important points and comments about how
59:32 - my notation and superscript versus
59:35 - subscript and I or J this is not perhaps
59:39 - the most conventional or correct
59:41 - notation so I apologize for that feel
59:44 - free to leave comments in the comments
59:47 - section it's particularly good ones if
59:50 - there's a really like one that sums it
59:52 - up all perfectly I'll
59:52 - in it right to the top but the point of
59:54 - this was for me to kind of like get
59:57 - through the basics of this I am now
59:59 - going to in the next video actually
60:01 - implement this in code and once I've
60:04 - done that we'll be ready to then look at
60:07 - the learning algorithm the training
60:09 - algorithm this thing called back
60:10 - propagation implement VAT and code then
60:13 - the neural network will be complete we
60:15 - can actually use it to solve something I
60:16 - hope so
60:18 - see you in a future video thank you
60:20 - acting all right now here's the thing I
60:28 - also need to was I not in the right
60:31 - screen there no I was all right all
60:47 - right I'm in the wrong screen now okay
60:49 - so what I need to do now
60:55 - is recorded a brief introduction to the
60:58 - next video see I think I already did the
61:00 - coding part and I think the coding part
61:02 - is a fine to go forward without me doing
61:08 - it again right I'm not sure you're still
61:15 - watching
61:15 - I don't even the problem is I don't
61:17 - remember what I did let's let me take a
61:20 - look at that while I'm here this is yeah
61:34 - this is like wildly different to it
61:42 - again please I think I should do I was
61:45 - excited for my it was very short but was
61:51 - fun alright I'm gonna I'm gonna leave
61:53 - all options on the table okay first I'm
61:58 - gonna just record a short intro into
62:02 - this video that explains why it doesn't
62:03 - match up then I'm gonna redo it again
62:12 - yeah note there give me any new stuff
62:14 - that's happening today I'm not I don't
62:16 - have the time to start doing the
62:17 - gradient descent and all of that because
62:18 - I and is the matrix I got to talk about
62:23 - the code I have not really published the
62:24 - code I got to think about how to do that
62:26 - give me a second everybody so remind me
62:29 - about that all right hello you're about
62:38 - to watch your video where I implement
62:39 - what I just did in the last video with
62:41 - code the only thing is in the last video
62:44 - I look like this and in the video are
62:47 - about to watch I its wilderness day it
62:49 - was in the wilderness reading textbooks
62:52 - about deep learning and also the
62:55 - whiteboards not gonna match exactly this
62:58 - is the whiteboard that I just completed
62:59 - where I really kind of like worked on
63:01 - and figured out all the notation in this
63:03 - diagram and then when I start to
63:05 - implement the code I might refer to this
63:06 - whiteboard which looks
63:07 - different I got to just redo the video
63:09 - you're never gonna see this this is this
63:12 - is no good
63:12 - we're gonna redo the video I mean let me
63:17 - just do it again I mean the only thing
63:21 - that I'm sad about now is they don't
63:22 - come the only thing I'm sad about now is
63:24 - I don't get to have like a nice shot of
63:25 - me like this I'm gonna talk about this
63:33 - anyway I'm going to talk about this
63:39 - anyway cuz you know people people should
63:41 - know the world should know what I went
63:43 - through to try to get these videos out
63:44 - but let me first go back to I have to
63:51 - reverse I have to take stuff out of the
63:52 - code right I don't think there's
63:59 - anything that I do did I do i think i
64:02 - made the from array function in that
64:05 - does anybody remember I think I made the
64:08 - from array function in that second video
64:11 - right so I think this has to go and the
64:16 - two array function I think those are the
64:18 - only two does anybody have a like a
64:21 - solid memory of this
64:22 - I think the matrix so this I'm going to
64:26 - recode everything here right this I'm
64:29 - gonna pletely rewrite the code and yeah
64:40 - Simon so I did it all as one live stream
64:42 - but my intention is to chop it up into
64:44 - several different videos so even though
64:46 - it was there so yeah it the two array
64:49 - functions weren't there okay thank you
64:52 - so I'm gonna go and put this somewhere
65:01 - just like make a backup of it put it
65:06 - somewhere then I'm going to what I'm
65:11 - going to do here is alright so this is
65:55 - what I had so let me just go back now
66:02 - yep
66:11 - oh right I made the joke the t-shirt
66:14 - thing that was at the this video yeah
66:16 - yeah okay it said lots of matrix math
66:35 - and returned guests okay so that's what
66:44 - was here this tad did not have the from
66:49 - array or to array functions and so I
66:57 - will also put that in the little backup
67:01 - file that I made just in case just to
67:08 - have those there and there we go
67:18 - okay up to me yes I'll make this the
67:22 - same joke yes yes don't worry I
67:25 - apologize I will make the same joke it
67:28 - will not be as good because it won't be
67:29 - spontaneous I think you changed the
67:32 - randomize function to thank you that is
67:34 - such a good catch it was like it was
67:42 - something like this right because it was
67:48 - giving me random numbers between zero
67:49 - and ten that I recall as well thank you
68:02 - all right I think I'm ready to go I had
68:11 - the perfect freeze frame okay there you
68:39 - go
68:48 - all right
68:50 - oh thank you Simon now that's a pretty
68:54 - good idea actually I do like that
68:59 - improvement for the two array thing the
69:06 - only thing is if it won't work if it's a
69:08 - full okay okay all right yes I'm very
69:14 - good at picking the right freeze frame
69:15 - all right
69:18 - hello it's time I admit something to you
69:21 - which is that I actually recorded a live
69:24 - session where I tried to explain all of
69:26 - this fee Ford algorithm thing a while
69:29 - back when I and apparently this like
69:31 - wilderness hair so I workshopping my
69:36 - wilderness joke give you kitty let me do
69:38 - that again cuz by wilderness joke is
69:40 - kind of important all right it's got to
69:43 - make it in there well there's Dan okay I
69:51 - have to do a five-mile trading run for
69:54 - the New York City Marathon today I gotta
69:55 - get going I'll talk about that too in a
69:57 - second all right yeah I'm gonna I'm
70:03 - gonna start over here
70:08 - okay so if you watched the previous
70:10 - video first of all thank you oh my god
70:27 - camera went off good timing
70:35 - hello
70:36 - now if you watch the previous video hey
70:39 - boys thank you well I hope you're not
70:42 - too mad at me and I didn't file too many
70:45 - complaints in the comments there but in
70:46 - the previous video I talked through the
70:48 - feed-forward algorithm they attempted to
70:50 - map it and graph it I attempted to get
70:52 - all the indices right to explain why we
70:54 - use matrix math for it all that sort of
70:57 - stuff
70:57 - now we got to get to something we got to
70:59 - get to the interesting part which is
71:00 - actually to use the neural network for
71:01 - something it's thinking me awhile to get
71:03 - there I will get there but I have to
71:06 - confess you thought that was bad I
71:10 - actually uh wilderness Dan over here
71:14 - tried in a live stream previously to go
71:17 - through all this and got all the index
71:19 - values and everything wrong was so bad
71:21 - it couldn't even like get it together to
71:22 - turn into an actual tutorial video that
71:24 - I'm making the playlist but it does
71:26 - exist if you're interested I got a
71:27 - haircut seem to fix a lot of things I
71:29 - look like a much more professional
71:30 - person you can go back and watch that
71:33 - previous those previous attempts so I'm
71:36 - actually done so this is my second try
71:38 - I'm just being honest here all right so
71:41 - what is it that I want to do in this
71:43 - particular video we can close all this
71:45 - stuff what I want to do is I have a
71:49 - matrix I have a matrix class that I've
71:51 - developed I have a neural network class
71:56 - that I've developed what I want to do is
71:59 - I want to be able to write code like the
72:02 - following
72:02 - timeout pause I just realized I forgot
72:05 - to delete this I want to be able to
72:12 - write code like the following I want to
72:14 - be able to go into my p5.js sketch or
72:17 - any javascript program and say something
72:19 - like let neural
72:21 - or let n equal a new neural network
72:25 - maybe it has maybe I want to solve X or
72:28 - so I want to have two inputs I just want
72:32 - to have one two hidden nodes and one
72:36 - output so I want to make a neural
72:38 - network object and I want to give it an
72:40 - architecture then I want to do something
72:42 - like I'm going to make an input like I
72:44 - want to send in true false 0 1 comma 0 I
72:48 - want to be able to say let output equal
72:54 - neural network feed-forward that input
72:57 - and then I want to say console.log
72:59 - output so this is the code this is what
73:01 - I want to be able to do I want to be
73:03 - able to use the neural network on a kind
73:04 - of higher level and once I can do that
73:06 - then I can have inputs that are all
73:08 - different kinds of things where I'm
73:09 - doing data science or some kind of like
73:11 - learn to play flappy bird or whatever it
73:13 - is that I'm doing ok so let's do this
73:20 - what I have so far here is just I have
73:24 - that feed-forward function it takes an
73:26 - input and returns that guess now there's
73:29 - no code there I need to fill in the code
73:30 - all that matrix math I need to take
73:32 - everything that I have here and I need
73:35 - to put that all into here so how do I do
73:40 - that
73:40 - all right well my neural network needs
73:42 - some more stuff for example all that it
73:45 - has right now is the number of input
73:47 - nodes which is eyes just sent into the
73:49 - number of hidden nodes I sent into and
73:51 - the number of output nose I sent in one
73:52 - well what I need is I need to keep track
73:55 - of those weights right I need to have
73:57 - weights and what do I need to have
73:58 - weights between we need to have the
74:02 - weights between input and hidden and the
74:04 - weights between hidden and output so
74:06 - those we've established our matrices
74:08 - weights between input and hidden is a
74:12 - new matrix and now it has a certain
74:15 - number of rows and columns it has a
74:17 - certain number of rows sorry columns
74:20 - based on the number of inputs and rows
74:22 - based on the number of hidden nodes so
74:26 - we're gonna say it has this dot hidden
74:29 - nodes is its number of rows and this dot
74:34 - input node
74:35 - is its number of columns so that's one
74:37 - wait mate ryx and another wait matrix is
74:41 - between hidden and output and that's
74:44 - going to have the number of rows which
74:47 - is the number of output nodes and the
74:50 - number of columns is the number of
74:52 - hidden nodes so I've created these
74:54 - weight matrices now oh you have an
74:57 - interesting question when we create a
75:00 - neural network how do we pick the
75:02 - weights again the whole point of this is
75:05 - we need to have some interesting
75:07 - wonderful exciting complicated weird
75:10 - algorithm for tuning the weights for
75:12 - finding the optimal weights for whatever
75:14 - type of application we're trying to
75:16 - build but to start we just want to give
75:18 - it random weights and did the whole
75:20 - field of research dedicated to figuring
75:23 - out like good ways to see the neural
75:25 - network with good kind of digging weight
75:27 - so that you can get the optimal weights
75:28 - more easily blah blah blah but for us
75:30 - right now I just want to give it random
75:31 - weights so I can actually just say we've
75:33 - already built a randomized function into
75:38 - the neural the matrix library so I can
75:40 - say randomized so this will randomize
75:43 - the weights and actually though if you
75:46 - recall I was reminded by this in the
75:48 - chat that my randomize function for some
75:50 - of my demonstrations I was actually
75:51 - picking a number between 0 and 10 and it
75:54 - would make much more sense to get a
75:57 - random number between maybe negative 1
76:00 - and 1 to start at the weight so I could
76:02 - take math dot random which is 0 or 1
76:03 - multiplied by 2 subtract 1 and I've
76:05 - gotten a random value between negative 1
76:07 - and 1 okay so we're doing well what else
76:12 - do I need
76:12 - I should probably I should probably keep
76:16 - track of the bias the hidden bias and
76:18 - the output bias again as we saw how sort
76:21 - of talking about it could put that into
76:22 - the matrices but I'm gonna keep track of
76:24 - that separately it's a little easier for
76:26 - me so I'm gonna say this dot bias hidden
76:30 - equals a new matrix that has wha how
76:37 - many biases do I have I have o based on
76:39 - the number of hidden nodes right that
76:41 - those are the rows and then one column
76:43 - is it yeah because it's a one column
76:45 - vector and the bias or the output is
76:47 - based on
76:48 - how many output nodes so for every I
76:51 - need a biased value for every node in
76:54 - every layer so I need to bias values for
76:56 - each hidden node and one bias value for
76:58 - each the single output node but you know
77:00 - again my neural network library allows
77:03 - me to create neural networks with any
77:05 - number of hidden nodes any number of
77:06 - inputs any number outputs it is just has
77:08 - these two layers and the inputs but so
77:11 - at some point it might be worth
77:12 - expanding it so I got it multiple hidden
77:13 - layers that sort of thing but this is
77:15 - gonna work fine for now okay so what
77:20 - else do I need now ah-ha-ha-ha there's
77:23 - something we're missing so for example
77:25 - let's just what am i okay
77:29 - what am i feeding in here feed-forward
77:32 - input this is just an array but
77:35 - technically in order for me to be able
77:37 - to do the first stop right the first
77:40 - thing that I want to do is I want to say
77:42 - this dot hidden I'm sorry I'm gonna make
77:44 - this let hidden this will be the output
77:47 - right I want to compute the output of
77:50 - the hidden nodes that's going to be a
77:54 - one-dimensional of one column matrix
77:57 - that is going to be that is going to be
78:02 - the matrix product between the input and
78:09 - this and the weights and the weight
78:14 - weight matrix between and actually I
78:15 - have to say the weights first sorry the
78:17 - weights remember the order in matrix
78:19 - multiplication really matters the weight
78:21 - input hidden with that input then I'm
78:28 - going to say hidden dot add this dot
78:34 - bias so I do the matrix product of the
78:38 - inputs and the weights then I add in the
78:41 - bias and then I'm gonna do activation
78:44 - function I won't do that just yet but
78:46 - even this is no good so far this is no
78:49 - good so far because this input what does
78:52 - this matrix multiply function expect
78:55 - matrix multiply in my library expects to
79:00 - matrix objects and this the way I've
79:04 - written my sketches I just made the
79:06 - inputs an array which is an incredibly
79:07 - convenient thing to do I don't want to
79:09 - make my end-users have to like figure
79:12 - out they just want me I'll send the
79:14 - inputs in in a simple array so one thing
79:16 - that I should do here I mean I could
79:18 - test I could check if it's already an
79:21 - instance of a matrix I could skip it but
79:23 - what I want to do is I want to say I'm
79:26 - going to call this input array I want to
79:29 - say inputs equals matrix from array
79:34 - input array and the reason why I'm
79:36 - writing this out is because I've thought
79:37 - of this already I need to be able to
79:39 - make a matrix object from that array so
79:42 - I want to be able to have a matrix
79:43 - object so that my matrix multiply
79:45 - function will work and I could create a
79:48 - different constructor that takes an
79:49 - array but I think what would make sense
79:51 - is for me to add right up here in the
79:54 - beginning right and a static function
79:56 - that's that's called from array it takes
79:59 - an array and what I want to do here is I
80:02 - want to say let may m equal a new matrix
80:06 - that has just that has a number of rows
80:10 - based on the arrays length and one
80:13 - column right I want to create a matrix
80:15 - that looks like this it has a number of
80:18 - rows based on the arrays length and one
80:21 - column and then all I need to do is say
80:25 - for let I equals 0 I is less than array
80:29 - length I plus plus and I'm going to say
80:35 - data oh sorry sorry
80:39 - m dot data index I index 0 equals equals
80:48 - array index I and then return n so this
80:54 - is just again oh I'm sure every lots of
80:57 - you could probably think of some type of
80:58 - array functionality that I could use
81:00 - really easily but this is creating that
81:02 - matrix and then just that matrix and
81:06 - then putting the input into it so just
81:08 - to be sure about this let me let me say
81:11 - m dot print
81:13 - all right let's take a look and I'm
81:15 - gonna just edit edit point for a second
81:20 - I know why this computer went to sleep
81:22 - all these notifications alright okay
81:40 - sorry I'm not running anything so I just
81:43 - need a second here no I want to go to
81:52 - desktop I lost mine okay
82:06 - no I changed it was this dot-matrix I've
82:09 - since changed it to this data so some of
82:13 - you missed a key point where I did that
82:24 - yeah
82:29 - yep yep yep
82:38 - okay okay let me just make sure this
82:44 - works so I have some errors here oh that
82:48 - actually nicely printed something out
82:49 - for me that I didn't realize it would so
82:51 - what I want to do is I want to say let a
82:54 - array equal a 1 comma 0 comma negative 5
82:59 - and then I want to say matrix dot from
83:03 - array array and we should see there we
83:08 - go we've got a column a single column
83:10 - matrix 1 0 negative 5 so that function
83:13 - is working the way that I hoped it would
83:15 - so I can take out this debugging and I
83:19 - can keep this in here and now if I go
83:21 - back to this code I can say okay we get
83:25 - an input array we turn that into an
83:28 - input matrix and then this needs an S
83:31 - here we multiply we do the matrix
83:34 - product of the inputs with the weights
83:36 - we add in the bias we one more step left
83:40 - to go we need to do the activation
83:42 - function so we need the sigmoid function
83:44 - well it so happens that there isn't a
83:46 - magical sigmoid function that exists in
83:49 - JavaScript however there is in the math
83:52 - library a function called exp JavaScript
83:55 - math let's find this documentation page
83:59 - this X P Euler's number also known as
84:03 - Napier's constant does for us e to the X
84:08 - so I can write my own sigmoid function
84:11 - so I can just um and at some point I
84:14 - probably will allow the neural network
84:16 - library to use different activation
84:19 - functions but right now I'm just
84:20 - globally gonna write a function called
84:21 - sigmoid it's going to take one input X
84:24 - and it is going to return one divided by
84:28 - one plus mass of X somebody fact-check
84:34 - for me but I believe that is the sigmoid
84:35 - function and so the wonderful thing now
84:39 - is once I receive the inputs once I've
84:41 - done the matrix product of the weights
84:44 - and the inputs once I've added in the
84:46 - bias I can generate the outputs with the
84:48 - activation function just by saying
84:51 - hidden dot map sigmoid if you recall I
84:55 - wrote into my matrix library a function
84:58 - called map which just allows me to apply
85:02 - any arbitrary function to every element
85:05 - of that matrix so so there we go now
85:10 - we're done so this is this is all the
85:13 - code for generating the hidden outputs
85:20 - and this is the activation function now
85:23 - where are we in the diagram we've got
85:26 - the inputs we did the matrix product we
85:29 - added in the biases we passed the
85:31 - activation function now those outputs
85:34 - are coming in here so what do we need to
85:36 - do now take those hidden outputs and do
85:39 - the matrix product with these weights so
85:44 - we're gonna do exactly that same thing
85:45 - we're going to say let output equal
85:49 - matrix dot multiply this this dot
85:54 - weights now I need the weights between
85:56 - the hidden and the output H oh I need
86:01 - the hidden outputs which is just calling
86:03 - hidden then I need to add in those
86:08 - output biases did I call it bias oh yes
86:11 - I need to and by the way I just realized
86:14 - I never gave these any initial values so
86:17 - it would make sense for me to also
86:19 - randomize these so I'm going to
86:25 - randomize those biases so right so now
86:28 - I've got the weights times they hid the
86:31 - matrix product or the weights and the
86:32 - hidden outputs and added in the bias and
86:35 - then output dot map sigmoid and guess
86:39 - what returned output that's all I want
86:43 - that's done that's the last piece I want
86:46 - to send in the inputs right over here
86:52 - sorry in my actual code I want to be
86:54 - able to send in the inputs and then read
86:56 - out the output but here's the thing I
86:58 - sent in the inputs as a simple array
87:02 - then internally converted that into a
87:03 - matrix oh I could do all the math
87:04 - properly now before I get the outputs
87:07 - back I don't want you want to send out a
87:08 - matrix it's gonna be simpler if I just
87:10 - send out those outputs as an array so
87:12 - much like I wrote that from array
87:14 - function I am going to create a function
87:19 - called
87:20 - two array and that doesn't need to be
87:22 - static because I'm gonna take any given
87:24 - matrix object and return in a right now
87:28 - there are some back I probably transpose
87:30 - it and and do some fancy way or use the
87:33 - slice function I'm again I'm gonna do
87:34 - this just in a way that I know will work
87:36 - will catalyze refactor this later in the
87:38 - end I'm gonna refactor all this using
87:40 - somebody else's matrix library anyway so
87:42 - let me think about how I would do this
87:43 - first I want to make an array I'll make
87:45 - it empty and then really all I want to
87:47 - do is I want to loop through every
87:52 - element of the array every element of
87:54 - the matrix and what I want to do is I
87:56 - would want to say array dot push every
88:01 - element of the matrix and say return all
88:03 - right so this would actually take any
88:05 - two-dimensional matrix and flatten it to
88:07 - one Mitchell array the question is
88:09 - though one thing we should ask ourselves
88:11 - here is it appropriate to have the
88:13 - columns as the inner loop so if I think
88:16 - about it if I wanted to take I mean
88:18 - obviously doesn't matter if it's this
88:19 - matrix because I just want to take all
88:21 - these and put it into an array but if
88:23 - it's this if I want to take like this
88:25 - array and flatten it do I want to go do
88:27 - do do do do do or do I want to go do do
88:31 - do do do do I think I want to go this
88:33 - way do do do do do I got distracted by
88:37 - my own singing of the iteration song is
88:39 - just dude alright sorry I'll get back to
88:43 - this I apologize I think I want to
88:45 - iterate through the columns on the inner
88:46 - loop which it looks like that's what I'm
88:49 - doing so I'm gonna keep it this way
88:50 - except for yeah okay so I'm gonna keep
88:52 - it this way and I'm gonna say here now
88:54 - in the neural network I can say return
88:58 - output to array okay everyone this is it
89:03 - this is the whole feed-forward algorithm
89:05 - this is receiving the inputs generating
89:08 - the hidden outputs now generate whoops
89:15 - generating the output output and then
89:21 - sending it back to the color alright so
89:25 - this is the whole algorithm layer this
89:28 - is like layer 1 and layer 2 the hidden
89:30 - layer the output layer what have I
89:32 - missed
89:33 - we'll see ok um all right let's look at
89:38 - this now yeah it's probably no need to
89:41 - randomize the biases but that's I did it
89:43 - anyway
89:44 - ok hold on hold on
89:49 - timeout ok what do we do now let's try
89:58 - running the code I mean it's absurd but
90:00 - because this does nothing but let's
90:02 - actually let's just run this let's just
90:04 - see can I create a neural network give
90:07 - it some are very input and get some
90:09 - output can I get it with no mistakes it
90:11 - seems very unlikely that I'm gonna have
90:13 - no mistakes and this is the reason why I
90:15 - shouldn't use my drum roll effect but
90:17 - I'm gonna do it anyway okay here we go
90:22 - let's run this code it worked is a
90:35 - strange thing to say cuz relay to get
90:37 - any errors and probably one reason
90:38 - getting errors because there's actually
90:39 - my second try of building this out
90:41 - and you might be wondering like oh okay
90:44 - so I just watched like two and a half
90:46 - hours of like 8 videos in a row about
90:48 - matrices and neural networks and you
90:52 - just went for point five nine three one
90:56 - zero seven so you know and it we should
90:59 - probably get a t-shirt that says I've
91:01 - watched all your YouTube videos and all
91:02 - I got was zero point five nine three one
91:04 - it was funnier when it was like a low I
91:06 - made this joke before in the previous
91:07 - attempt it was a lower number so now
91:09 - point two seven is a funnier number like
91:12 - great like like what's a funny I feel
91:16 - like though that's pretty good I feel
91:18 - like it's gotta even be lower the lower
91:20 - the number is the funnier there we go
91:22 - see now it's funny oh wait a second okay
91:26 - so
91:28 - the thing is we have no when you gotta
91:29 - start testing this we've got to come up
91:31 - with a server we're getting close we're
91:33 - what I want to do probably the first
91:34 - thing I'll do it's not that interesting
91:36 - but at least allow us to test whether
91:37 - this code works is can I train it to
91:40 - learn and operation or operation or X or
91:44 - exclusive or which is one that I've
91:45 - talked about that can't be solved with a
91:47 - single perceptron so can I feed it in
91:49 - one zero and train it to get the number
91:51 - one out can I feed it in zero one train
91:53 - to get the number one out but if I give
91:55 - it zero zero one one I get the number
91:56 - zero out so I need to attend that but I
92:00 - am missing a lot of functionality from
92:03 - this neural network class I have this
92:06 - feed-forward algorithm function now what
92:09 - I need is to write a function called
92:12 - train and what I'm going to do with the
92:14 - train function is I'm going to give it
92:16 - some inputs and then I'm gonna give it a
92:18 - known answer so the idea here is like
92:20 - with feed-forward I'm really just saying
92:22 - take these inputs and give me an output
92:24 - with training I'm gonna say take these
92:27 - inputs that I have a known labelled
92:28 - answer to and do something to yourself
92:31 - based on that and that's what I'll start
92:32 - getting to in the next video I have to
92:34 - talk about back propagation and gradient
92:36 - descent I have talked about this in
92:37 - previous videos will kind of return to
92:39 - that then I need to finish implementing
92:40 - this function and then I'll do a coding
92:42 - challenge where I try to solve XOR and
92:44 - do like a simple digit recognition or
92:46 - something with this particular neural
92:47 - network library okay
92:50 - thanks for watching umm just them going
92:52 - through this I'm trying it out and see
92:54 - if I can make this move happen to be -
93:00 - things would be - but all things all
93:03 - things will come eventually so thanks
93:04 - for watching and as always I don't know
93:06 - subscribe like that helps supposedly and
93:09 - because then YouTube's neural network
93:11 - I'm just copying three blue and brown
93:12 - stroke at this point you should watch
93:13 - three blue one Browns videos good and I
93:19 - learned so much just from like watching
93:21 - those and I'll be back in the next video
93:23 - it's a point thank you
93:25 - [Music]
93:27 - all right
93:29 - machi I hope I saved you a lot of
93:32 - editing time and save I think we're good
93:37 - I think we're good I think we're good
93:41 - for today did I forget to apply the
93:48 - weights to the biases I don't think I
93:52 - did I think I'm okay I think I did
93:57 - everything did I miss anything important
94:00 - a Bryan I've been waiting for back
94:03 - propagation propagation a ycu sigmoid
94:07 - and not hey you to French activation
94:11 - function from what I understand good
94:15 - question I mean I should use it but I'm
94:18 - not prepared to because I just kind of
94:22 - want to go through what the other let me
94:24 - make sure I talk about in the next video
94:25 - though but yeah I should come back to
94:27 - that okay
94:28 - it is 3:45 one of the reasons why I have
94:31 - to leave is I got to make at home but
94:32 - I'm also I guess I'll mention this here
94:34 - for those you're watching I am training
94:36 - for the New York City Half Marathon
94:37 - which I'm doing a fundraiser for crowd
94:41 - rise wonderful organization Lower East
94:44 - Side girls club if you liked watching
94:46 - this live stream today and feel like
94:50 - donating I am here we go I have to you
94:56 - have to do through my page so get the
94:57 - credit it's very important hold on I'm
95:01 - not here team see more Jane your
95:08 - shipment okay here we are the Lower East
95:11 - Side Girls Club is a wonderful
95:12 - organization that does after-school
95:15 - programs they have a great makerspace
95:17 - they have a planetarium they do
95:18 - wonderful stuff with code stuff so my
95:19 - students have done workshops over there
95:21 - I've hung out over there it's a
95:22 - wonderful location place not-for-profit
95:25 - on the Lower East Side if you are feel
95:27 - so inclined I am fundraising for them on
95:29 - running the New York City half marathon
95:31 - which I hope to do it under two hours I
95:32 - have run a half marathon in under two
95:34 - hours before it was like 1 hour and 59
95:35 - minutes
95:37 - so I'm just gonna plug this for a second
95:39 - for any of you were watching here Thank
95:40 - You Alka for putting that link in the
95:42 - chat okay I just wanted to mention that
95:45 - since I'm not promoting anything else
95:48 - today I will promote that so yes thank
95:54 - you that's a good comment um alright so
95:59 - thanks everyone for tuning in I should
96:01 - be back on Friday at the regular time my
96:04 - intention on Friday if I am so lucky is
96:07 - to get all the way through solving X or
96:10 - simple goals so I want to do the back
96:12 - propagation step add that to the library
96:15 - and then I just want to make the X or
96:18 - and visualize that you know the mages of
96:21 - do you know visualize that no I don't
96:23 - know that they're to do some stuff with
96:25 - it so probably X or and M missed if I
96:27 - can okay whoops was that link no good
96:34 - thanks for trying Alka I can paste it in
96:38 - hold on because I am actually in the
96:46 - chat as well there we go
96:58 - okay and whoa I'm behind here all right
97:12 - okay so this is it for today
97:17 - it is 348 I gotta go so thank you
97:21 - everyone for watching send me your
97:23 - feedback at Schiffman on twitter like
97:27 - subscribe all that nonsense and yeah
97:31 - I'll be back on Friday if somebody can
97:36 - by the way find this song for me
97:37 - I think I found it at one point I think
97:41 - was on free music archive and and it had
97:47 - the word rainbow in it but also it could
97:53 - have been that I searched for it on I
97:55 - think zone I think free music archive
97:56 - could have been on soundcloud I'm not
97:58 - sure all right
98:00 - yes Ray Lewis stands for rectified to
98:03 - the newer unit that is correct all right
98:06 - my link didn't work either
98:08 - oh well let me at least show you what
98:18 - the link is I'll take us while I'm
98:22 - wasting time doing this I'll take a few
98:24 - questions from the chat
98:28 - [Music]
98:35 - alright let me take a couple questions
98:37 - in the chat
98:37 - when will the code hi yes good question
98:39 - so okay it's a couple questions so taco
98:49 - dude asks why don't you make it so you
98:51 - can have more than one hidden layer I
98:52 - absolutely should do that I'm starting
98:56 - simple building these out at some point
98:57 - I don't know how far it makes sense for
98:59 - me to go with this neural network
99:00 - library because eventually I'm gonna try
99:02 - to replace most of this with this deep
99:04 - learned j/s thing but c'est la vie but
99:08 - people ask about what to do about the
99:10 - code so here's the thing the code for
99:12 - this ostensibly is on github.com slash
99:16 - Schiffman neural network p5 this you can
99:24 - see like oh look here's the neural
99:25 - network code way that looks kind of the
99:27 - same but kind of different here's some
99:32 - matrix touch yeah oh boy looks pretty
99:35 - different it's not even es6 so I made
99:38 - this a long time ago not that long ago
99:40 - last commit there's somehow five days
99:43 - ago but really is like six six six
99:46 - months ago or so that I would like last
99:48 - did anything about this so I made this a
99:50 - long time ago
99:51 - using your an alder you know people will
99:54 - find you'll finally tweet at me I'll
99:55 - send you did I don't know how many of
99:56 - you really okay so I don't think I need
99:58 - to waste too much time about the URL but
100:03 - so hold on a sec we might so this is the
100:08 - code so I I want to update this and have
100:11 - this code match what I'm building but
100:16 - I'm also happy to accept pull requests
100:18 - here to optimize and fix this up so I
100:21 - think what I probably should do is have
100:23 - the raw code from the examples go in
100:28 - radan bow code and it would probably go
100:31 - under I would say if anyone wants to
100:38 - help me with this courses nature of code
100:42 - and it would be a video started
100:45 - numbering with
100:46 - and or intelligence and learning session
100:51 - for I don't know this never really got
100:52 - anywhere
100:53 - so maybe nature of code it should be
100:58 - like there should be a a subfolder here
101:03 - that has the code that goes along in
101:05 - steps with each and every video
101:07 - precisely and then a sort of finished
101:09 - version of it that's maybe a bit more
101:11 - optimized would end up here in this
101:14 - library so I would gladly work except
101:17 - any pull request and help in that
101:19 - direction that's kind of what I'm
101:21 - thinking of by right now I'm Simon in
101:24 - the slack channel and slack channel for
101:25 - patreon subscribers asked when do you
101:28 - are you going to do k-means it was on my
101:30 - list that I kind of never got to it so I
101:32 - don't know I gotta have to come back to
101:33 - that at some point read random numbers
101:40 - someone's talking if somebody just
101:42 - donated oh my goodness well I'm
101:44 - deathless whoever just donated that I
101:46 - should give you a massive thank you that
101:48 - is so kind and nice of you
101:51 - Topher J thank you so much Topher J
101:54 - these random numbers I'm about to read
101:56 - are dedicated to Topher Jay thirty six
102:00 - thousand four hundred fifty three
102:02 - fourteen thousand nine hundred twenty
102:04 - eight twenty three thousand nine hundred
102:06 - and thirty one thirty two thousand
102:08 - ninety five thirty thousand seven
102:10 - hundred fifty seven fifty six thousand
102:12 - eight hundred and two thirty four
102:14 - thousand eight hundred eighty five
102:15 - eighty seven thousand one hundred sixty
102:18 - nine eleven thousand three hundred
102:20 - eighty six and ninety eight thousand six
102:22 - hundred and fourteen breaking is
102:27 - breaking is did the decoding train
102:29 - breaking news somebody I'll have that
102:31 - sound effect
102:33 - I'm being told that maybe Free Music
102:37 - Archive dot org slash music slash the
102:43 - underscore Colombians just like a
102:51 - rainbow you're only hearing this through
102:57 - my mic yeah that's definitely it there
103:01 - we go thank you very much I really I
103:03 - need to step up my music game I'm you
103:06 - know it's a tricky with sort of
103:07 - copyright stuff but I you know I kind of
103:09 - like Josh Scott Joplin would be good I
103:12 - think for me like it kind of Keystone
103:13 - Cops like aesthetic Maple Leaf Rag would
103:16 - probably be good so anyway yes okay so
103:22 - green thing on my desk what's the green
103:23 - thing on my desk I mean this thing it's
103:26 - just another laptop it's where I have a
103:28 - slack chat going I hope I didn't just
103:30 - violate anybody's privacy in the slack
103:32 - chat okay will you ever make a neural
103:37 - network that can play a game that you
103:39 - can make live for a coding challenge yes
103:41 - that's kind of my goal it's raining
103:43 - tacos no matter this guy tacos no no no
103:46 - my why do I know that song tacos and all
103:49 - right I've really got to go
103:50 - thanks everyone per burger bug-eyed
103:54 - burger Bob I'm very sorry to say that
103:56 - I'm now finished with the live stream
103:58 - and it will be back on Friday well how
104:00 - long was this today
104:01 - two hours something like that alright um
104:03 - thanks for watching yeah and see you
104:06 - next time yes Elliot chill I will return
104:10 - steering but behaviour at some point