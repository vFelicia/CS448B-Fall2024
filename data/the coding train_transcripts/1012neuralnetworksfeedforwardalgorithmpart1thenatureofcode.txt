00:00 - alright this is a momentous occasion I
00:02 - got a haircut from the last video if you
00:04 - might have noticed that I wasn't gonna
00:05 - mention it but I mentioned it and what
00:08 - I'm actually going to do now is describe
00:11 - we've finished off I mean it's not
00:12 - finished we're have to do more building
00:14 - this little matrix library that's gonna
00:16 - allow us to do some math stuff that
00:17 - we're gonna need when we implement the
00:19 - code for this particular video where I'm
00:22 - gonna describe the feed-forward
00:23 - algorithm of neural network now I want
00:25 - to give thanks to two sources that I've
00:28 - used primarily in the sort of studying
00:31 - and preparation for this video number
00:33 - one is make your own neural network by
00:35 - tariq rashid you'll find a link to this
00:37 - a book at the coding train Amazon shop
00:39 - in this video's description and also I
00:42 - want to thank and reference the three
00:44 - blue one brown channel which has a
00:46 - playlist I forgot what it's called but
00:49 - there's a video called what is a neural
00:51 - network if there's at the time of this
00:52 - recording there's about four videos
00:54 - those are amazing they're animated
00:56 - they're thoughtful they're careful but
00:58 - they're really for understanding again
01:01 - and having an intuition for how a neural
01:03 - network works and for seeing all the
01:07 - pieces of an algorithm I highly
01:08 - recommend those what I'm really
01:10 - attempting to do in my videos is sort of
01:12 - figure out a way to implement a lot of
01:14 - the stuff that's in this book and those
01:16 - videos in code so in this video in the
01:19 - next video I'm gonna start working on
01:20 - the code this video I'm really gonna
01:22 - talk through the algorithm in my own
01:24 - words as it applies to where I am in
01:26 - this playlist I've been way too much
01:28 - time introducing this video let's just
01:29 - get right to it ok so where I last left
01:33 - off before I started working on matrix
01:34 - math stuff I had made built this simple
01:36 - example of a perceptron and a perceptron
01:39 - the idea of a perceptron as a single
01:41 - neuron that receives inputs so we might
01:45 - have inputs something like X 1 and X 2
01:48 - and those two values go into this
01:52 - perceptron they are they are processed
01:57 - and then some output is generated often
02:01 - referred to as Y so this is where this
02:05 - is the thing that I built last I talked
02:07 - about some of this in a previous video
02:08 - but I need to kind of just rehash it
02:10 - again to set the stage of where we are
02:12 - so let's say we're
02:13 - trying to solve a simple problem and
02:15 - like understanding logical and so the
02:19 - inputs that into this system this
02:21 - learning system this simple perceptron
02:23 - can be a true or false or true or false
02:28 - or we can also think of that as zero or
02:30 - one right and we want this perceptron
02:35 - system this system to output a 1 for y
02:40 - only if both the inputs are true so we
02:44 - could say if the if both the inputs are
02:46 - true then we want to get a 1 you know if
02:49 - if the inputs are true and false we
02:51 - should get a 0 if they're false and true
02:52 - we should also get a 0 if they're false
02:54 - and false we should also go to 0 so this
02:56 - is the kind of system now where I the
03:00 - thing that I talked about so this is
03:01 - great exciting Wow we have a neuron it
03:03 - can receive inputs it can boom it could
03:05 - give me outputs all is right in the
03:07 - world the problem is most problems in
03:10 - life are more complex than this very
03:13 - simple scenario of two inputs each only
03:17 - a boolean possibility and and an output
03:19 - solving for a logical and so for example
03:22 - let's just make this slightly more
03:23 - complicated let's take this idea of
03:27 - something called X or exclusive or so if
03:30 - I were to switch this to or this would
03:36 - now be the desired outputs for or right
03:39 - one thing is true that's what we've got
03:41 - hmm exclusive or means we will only
03:45 - output true if one thing is true and one
03:49 - thing is false so this for exclusive or
03:51 - will actually output a 0 and if you I
03:54 - talked about this more in a previous
03:56 - video this is not a linearly separable
03:58 - problem we can't graph the solution
04:01 - space and draw a line right in the
04:02 - middle and say all the answers of all
04:04 - the answers for true on one side all the
04:05 - answers are false are on the other so
04:07 - this is where this idea of a
04:09 - multi-layered perceptron comes in most
04:12 - problems as I get further and further
04:14 - through this playlist of doing more and
04:16 - more things cannot be solved with a
04:18 - single neuron this idea of a perceptron
04:21 - this is where what happens to solve a
04:23 - problem like XOR
04:24 - if we add a second neuron and send those
04:29 - inputs the same inputs into that neuron
04:32 - and then set the output of that neuron
04:35 - into the output here now we have what's
04:39 - called a multi-layered perceptron we
04:42 - have this output layer this you could
04:45 - consider an input layer although the
04:47 - input functions differently than these
04:49 - other two this is a layer this is a
04:51 - layer and here's the thing this is
04:53 - called a hidden layer the reason why
04:55 - this so I'm gonna say hidden is right
04:58 - here this is input right here and this
05:03 - is output now as I get further and
05:08 - further through many videos that I hope
05:10 - to make and examples I hope to make
05:11 - we're gonna see that there are many
05:13 - complex architectures for how you might
05:15 - diagram and design a neural network
05:18 - system this is a very very basic
05:20 - beginning point and typically this is a
05:24 - two layer Network a hidden layer and
05:26 - output layer input isn't really
05:27 - considered a layer although I sometimes
05:29 - do it so I close the three layer network
05:31 - or we could get to something like long
05:33 - we could have a comment thread about
05:34 - which is precisely correct for this so
05:36 - we'd start that common thread then I'll
05:38 - learn but but let's just call this rut
05:40 - two layer Network the reason why
05:41 - incidentally this is called a hidden
05:43 - layer it's because we as the operators
05:45 - of the neural network right we are going
05:48 - to give the neural network data we're
05:50 - gonna say hey take true and true or take
05:52 - true and false so we're really here
05:53 - interacting with the input we also want
05:56 - to see the output we want to take the
05:57 - output and use it in our project oh you
05:59 - gave me false thank you what's the
06:00 - answer
06:01 - hidden are the pieces that exist in
06:04 - between where the inputs come in the
06:06 - output comes out and this is kind of the
06:08 - magic as we'll see as I get further and
06:10 - further along and solving different
06:11 - kinds of problems how this works now the
06:16 - main purpose of this video even though
06:18 - I'm being very long-winded about it is
06:19 - to talk about the feed-forward algorithm
06:22 - what happens to the data as it comes in
06:25 - passes through and exits through the
06:27 - output and in order to describe the math
06:29 - I what I want to do is add one
06:32 - additional input X 3 so I'm going to add
06:36 - that and that will also go into this
06:40 - hidden neuron and this hidden rod so now
06:43 - you can see we have three inputs two
06:47 - hidden nodes and one output node and all
06:51 - of this is totally flexible you in a if
06:54 - you've ever looked at that classic you
06:56 - know for hello world machine learning
06:58 - problem where you have this data set of
07:01 - handwritten images often the inputs are
07:04 - 784 for 784 pixels and the outputs there
07:08 - are 10 outputs because you have a
07:09 - probability for whether it's a 0 1 2 3 4
07:12 - 5 6 7 8 or 9 so the design of this how
07:14 - many inputs on the outputs
07:16 - how many hidden nodes how many hidden
07:18 - layers this is all food for thought but
07:21 - I just want to look at this very basic -
07:23 - where is it - 2 - ok ok ok it's a deal
07:29 - to layer Network deep breathing deep
07:34 - breathing it's gonna be fine whether I
07:35 - say 2 or 3 life will go on now you might
07:38 - be wondering which weren't you just
07:41 - making all these videos about like
07:42 - matrix math how is that relevant here
07:45 - well it turns out that the math for what
07:49 - each one of these nodes do is something
07:52 - called a weighted sum what do I mean by
07:55 - that these are all connections each
07:58 - input connects each input connects to
08:01 - each hidden node each hidden node could
08:03 - connects to each output node these
08:05 - connections all have a weight and in
08:08 - fact if we call this like h1 and h2
08:11 - right this is hidden one hidden - we can
08:14 - have weights between one and one and one
08:16 - and two and two and one and two and two
08:18 - and three and one and three and two so
08:21 - those weights right if there are three
08:23 - inputs and two hiddens there's actually
08:27 - a weight matrix there are six weights
08:29 - there are six possible connections and I
08:32 - could write that like this so let me
08:36 - write out the weights in a matrix so if
08:39 - I have a matrix that
08:41 - roh-roh column I can have Row 1 column 1
08:45 - Row 1 column 2 Row 1 column 3 Row 2 Row
08:56 - 2 column 1 Row 2 column 2 Row 2 column 3
09:02 - right so I have Row 1 Row 2 columns 1 2
09:07 - 3 so I so this is 1 this is a notation
09:13 - for writing all of these weights now let
09:18 - me now label the weights in this diagram
09:20 - and I'm going to do it in a particular
09:21 - way I'm going to say that this is a
09:24 - connection between hidden 1 and input 1
09:30 - this year is a connection between hidden
09:32 - 1 and input 2 this is a connection we'd
09:37 - hidden 1 input 3 and here I have a
09:40 - connection between hidden 2 and input 1
09:44 - hit into an input 2 and hit into an
09:48 - input 3 you might want to pause this
09:53 - video just take a look at this and take
09:55 - a look at this and see if it makes sense
09:56 - for you the point of what I'm saying is
09:59 - what what should be intuitive or sort of
10:01 - like feel somewhat normal is like okay
10:03 - there's all these connections those
10:04 - connections one on one one to one it oh
10:07 - sorry one on one two and one there's
10:13 - connections but we're hidden one is
10:16 - connected to one two and three and
10:17 - hidden two is connected to one two and
10:19 - three right and so here are all the
10:22 - weights from inputs one two and three
10:24 - that are connected to hidden one one two
10:26 - three here all the weights that are
10:28 - connected to hidden two from inputs one
10:29 - two and three so so why am i spending
10:32 - all this time driving myself crazy and
10:34 - incidentally driving you crazy spending
10:36 - all this time trying to label these and
10:38 - do this matrix the reason is the value
10:41 - the math that I want to process here in
10:44 - the hidden node is the weighted sum of
10:46 - each input multiplied by the weight
10:49 - added together so let me get myself a
10:51 - little bit more space here on the
10:53 - whiteboard
10:54 - what I want what I know I would I want
10:57 - to end up with is this weighted sum so
10:59 - let's look at that weighted sum first I
11:04 - want to take this wait wait 1 1 and
11:07 - multiply it with this input wait 1 1
11:11 - times X 1 plus what now this wait 1 2
11:17 - and this input plus this wait 1 2 times
11:21 - X 2 then I want to take this wait 1 3
11:26 - and x 2 x 3 1 3 times X 3 you can see
11:31 - how this looks nice and neat the weight
11:34 - one one goes with X 1 wait 1 2 goes with
11:37 - X 2 wait 1 3 goes with X 3 and we could
11:39 - do that same weighted sum for all the
11:41 - inputs and their weighted connections to
11:43 - H 2 2 hidden - and that's going to be
11:45 - weight - 1 times x1 plus weight - 2
11:50 - times x2 plus weight - 3 times x3 this
11:56 - is the result that we want and I could
11:58 - have just basically done all of this
12:00 - without thinking about all this notation
12:02 - and matrices with a for loop because I
12:04 - could just say like I have an array of
12:06 - inputs I have array of hidden and then I
12:08 - have an array of connections and then
12:10 - I'm gonna do some 4 loops to multiply
12:12 - all the inputs but the reason why I'm
12:15 - spending all of this time doing this is
12:17 - it look at something really interesting
12:19 - this might look familiar to you if
12:21 - you've been watching the previous 3 or 4
12:23 - or 5 videos about matrices what is this
12:27 - really this is the matrix product of
12:30 - this weight matrix and a column of
12:33 - inputs if I put this right here X 1 X 2
12:40 - X 3 and I write a big X therefore a
12:44 - matrix product or I could put a dot
12:45 - there may be a dot would be a better
12:47 - notation for matrix product we can see
12:51 - here that remember what that matrix
12:53 - product is it's this row multiplied is
12:57 - this the dot product of this row with
12:59 - this column 1 1 times X 1 1 2 times X 2
13:03 - 1 3 times X 3 all added together
13:06 - and then the next is this row times this
13:11 - come so it just works out perfectly this
13:13 - is why matrix math is so relevant and is
13:17 - used in all of neural network deep
13:21 - learning implementations because the way
13:23 - that the feed-forward algorithm works
13:25 - and mind you I've there's a lot more to
13:27 - the feed-forward Algrim that I need to
13:28 - talk about I'm talking about the bias
13:30 - yet or the activation yet there's more
13:32 - but I just want to get now to this
13:34 - primary understanding of the inputs come
13:37 - in we take a weighted sum of all the
13:40 - connections between the inputs and that
13:42 - next layer and that can be done in a
13:46 - single operation if in our code the way
13:49 - we implement it is we store all the
13:51 - weights in a matrix and all the inputs
13:53 - in a matrix the weights are going to be
13:55 - the inputs are always going to be in a
13:57 - single column I'm actually that could
13:59 - change the design of your neural network
14:01 - but in this case that's how that's gonna
14:03 - work I'm being told from the live chat
14:06 - that's going on that the dot really
14:07 - needs something else and a cross product
14:10 - is something else so we're just going to
14:11 - put the our symbol for matrix product
14:14 - will be like a nice little you know
14:16 - steam engine train thing okay that aside
14:20 - what else do I need to do so we've
14:22 - established this beginning part of the
14:26 - feed-forward algorithm the inputs come
14:28 - in the weighted sums get added all
14:32 - together inside these hidden notes now
14:34 - there are two big components that I've
14:37 - missed let me just write this over here
14:38 - one is bias and two is an activation
14:43 - function I kind of not sure which order
14:45 - to talk about these things before I get
14:48 - to the bias and the activation what I
14:49 - want to do is is rewrite this as a
14:51 - smaller formula so I want to consider
14:53 - the weight matrix for example just as
14:55 - the capital letter W and I can think of
14:58 - it as a matrix of I rows and J columns I
15:01 - ain't a are kind of terrible cuz they
15:03 - look so similar but the weight matrix I
15:06 - I rose and J columns now we're taking
15:08 - the dot the matrix product which I'm
15:10 - just gonna use a dot here I think that's
15:11 - going to be fine the matrix product pre
15:14 - miss and the inputs now the inputs is a
15:17 - matrix but it's one column but I rose
15:20 - and the point of the reason why I'm
15:22 - doing this is because I'm trying to get
15:24 - the outputs what I want is I want to
15:26 - know what number to send out of here
15:29 - the data is flowing in we get this
15:30 - weighted sum what number flows out of
15:33 - there and the number that flows out of
15:34 - there I'm going to call the hidden it's
15:37 - really I'm trying to get the output of H
15:42 - h1 h2 so the outputs of the hidden layer
15:45 - just the inputs are the inputs of the
15:47 - inputs of the hidden layer the outputs
15:49 - of the hidden layer are the inputs to
15:51 - the island so a lot of inputs and output
15:53 - there but the previous layer sends in
15:56 - the input so whatever comes out of the
15:57 - hidden goes in here into the output so
16:00 - hidden I equals the matrix product
16:04 - between the weight matrix and the inputs
16:05 - but there's more so one of the things
16:08 - that I'm forgetting I can actually fold
16:10 - these two things in right in here the
16:12 - things that I'm forgetting are one the
16:13 - bias in the previous videos where I went
16:16 - through the perceptron for example
16:17 - remember I was trying to like find this
16:19 - line and what points would be above it
16:21 - or below it and I've got to really deal
16:23 - with the problem that all the inputs
16:24 - could be 0 if all the inputs are 0 then
16:28 - the weighted sum is always going to be 0
16:31 - that can't be right
16:33 - so we need this bias we sometimes need
16:34 - to make it easier or harder for it to so
16:37 - to speak
16:37 - fire like we want to make we want to
16:40 - like bias the output in a given
16:42 - direction so one thing I would write
16:45 - here is I would also say plus that bias
16:48 - so and that is a single column vector as
16:52 - well so really if I'm down here again
16:58 - and boy am i running out of space but
16:59 - that's ok I would do this matrix product
17:03 - and then for every single one of these
17:06 - hidden I would have a bias B 1 and B 2
17:11 - right so there are weights and by it and
17:13 - by weight it's perfectly legitimate to
17:15 - ask the question right now huh like well
17:17 - what are the values of the weights what
17:18 - are the values of the bias these this is
17:19 - what I'm going to get to like I just
17:21 - want to understand the algorithm the
17:22 - whole point of this sort of learning
17:24 - system that we're going to create is to
17:25 - figure out how to tune all the values of
17:28 - all these weights and biases so that the
17:30 - outputs match up with what we think they
17:33 - should be
17:33 - the assistant needs to somehow adapt and
17:35 - learn and tune all those values to
17:37 - perform some sort of task and in a sense
17:40 - this is really just one big function
17:42 - that's why our own Network something
17:43 - called a universal function approximator
17:45 - it's just a function that receives
17:47 - inputs and generates an output that's a
17:50 - function and we have to in theory like
17:52 - if we'd have enough of these hidden
17:54 - layers and nodes there's no inputs we
17:56 - couldn't match with some given set of
17:57 - outputs so anyway where'd he get to all
17:59 - of that but I'm back here so I need to
18:01 - add the bias inand then there's
18:02 - something else you might remember from
18:04 - the simple perceptron example that we
18:06 - had this activation function whatever
18:08 - this weighted sum plus the bias would be
18:10 - if it was a positive number we would
18:13 - turn that number into plus 1 if it was a
18:15 - negative number we would turn that
18:17 - number into negative 1 and this is
18:19 - something that's very typical of neural
18:21 - network based systems whatever these
18:23 - weighted sums come in as we wanted to
18:25 - like squash them into some known range
18:27 - and there are a variety of different
18:29 - mathematical functions that can do this
18:31 - and while this is not typically the sort
18:34 - of latest and greatest and most
18:35 - cutting-edge activation function the
18:37 - function that is we will find in a lot
18:40 - of textbooks and early implementations
18:42 - of neural networks is something called a
18:44 - sigmoid and sigmoid is a function that
18:49 - actually looks like this f of X equals 1
18:54 - divided by 1 plus e to the negative X
18:59 - percent sure I got that right let's go
19:01 - look at the Wikipedia page for the
19:02 - sigmoid function ok so here we can see I
19:06 - did get the correct formula for the
19:08 - sigmoid function what is this number
19:11 - e E is called the natural number it's
19:14 - the base for the natural logarithm it's
19:16 - like 2.71 something-or-other so for one
19:19 - of these magic numbers or Euler's number
19:21 - you can read up about it but there and
19:23 - to be honest the sigmoid function is
19:25 - barely used anymore in sort of modern
19:27 - deep learning research but I think it's
19:29 - a good starting point for us to look at
19:31 - and understand why and in other videos I
19:33 - will take a look at other activation
19:34 - functions but the reason why the sigmoid
19:36 - function is used is because it takes any
19:38 - input any number you pass it through the
19:41 - sigmoid function you will get a number
19:43 - between 0 and 1 and so this is perfect
19:46 - for a lot of scenarios it takes this
19:48 - input it squashes it so higher numbers
19:51 - are going to be much closer to one lower
19:53 - numbers are going to much a little
19:54 - closer to zero and the bias is going to
19:56 - can push things closer to one or closer
19:58 - to zero this squashing of it works
20:00 - really well because you can kind of you
20:02 - know we could get us or a true or false
20:04 - zero or one we can get a probability
20:06 - value between zero and one we get to the
20:07 - output lots of possibilities so this is
20:10 - the sig and it's sigmoid function so oh
20:13 - so okay so I'm over here by the way I
20:15 - need to correct a couple things where's
20:17 - that eraser number one is I kind of did
20:19 - something horrible here because I made
20:20 - these equations not equal to each other
20:21 - anymore when I was talking about the
20:23 - bias so let me not put that over here so
20:29 - okay another thing I want to mention is
20:31 - I'm kind of fumbling here with the these
20:35 - index values you were really in what
20:38 - we're really doing is we're the matrix W
20:41 - is rows and columns so I and J the input
20:45 - is a single column vector but we're
20:48 - still iterating over I because the I of
20:50 - the weights gets multiplied with the
20:52 - each row which is now I of the of the
20:56 - input so you know the exact notation
20:59 - aside the point of the feed for one of
21:02 - the the truth of the feed-forward
21:04 - algorithm is the inputs come in you take
21:07 - a weighted sum you add in the bias you
21:11 - take that weighted sum add the bias pass
21:13 - it through the activation function and
21:15 - that result feeds forward towards the
21:17 - network and go straight to the output
21:19 - and guess what the output does the
21:21 - output takes whoops the weights of all
21:29 - the connections between the hidden and
21:31 - the output so the matrix product of the
21:35 - hiddens with those weights plus its own
21:38 - biases these are different biases these
21:40 - are the biases for this particularly the
21:43 - output node and then passes that through
21:45 - the sigmoid function as well so this
21:47 - just happens every sing with every
21:49 - single layer here come the inputs
21:51 - weighted sum passes the activation
21:54 - function send that as output into the
21:57 - weighted sum of the next node pass of
21:58 - the activation function send
22:00 - output etc etc so I was getting the
22:06 - question and was being point out that I
22:08 - didn't draw the bias anywhere into this
22:10 - anywhere into this particular diagram so
22:13 - a nice way to draw the bias into the
22:15 - diagram is to think of it as another
22:20 - input at each layer so for example if
22:23 - there were always an input that had a
22:25 - value of 1 that connected like this now
22:30 - we have bias 1 and bias 2 so these are
22:33 - just these are like weights that are
22:34 - getting weighted with an arbitrary input
22:35 - of 1 that's always coming in and the
22:38 - same thing with here now we have also
22:40 - another bias that's coming in to the
22:42 - output so it each layer there is a sorry
22:45 - a 1 which which it's weight is like this
22:49 - bias there's only one so B 1 so you can
22:51 - think you could you'll often see I could
22:53 - probably pull up a much nicer diagram
22:55 - again I highly recommend you go watch
22:57 - that sure just been watching the three
22:59 - blue one Brown video all along if I'm
23:01 - being perfectly honest but this
23:03 - hopefully gives you a sense of what the
23:06 - pieces are the point of me doing this in
23:07 - a sense you could just skip ahead to the
23:09 - next video in the next video I'm going
23:11 - to start to have a matrix library which
23:14 - does this matrix math I can add two
23:17 - matrices together element wise I can
23:20 - perform a matrix product I can apply a
23:23 - function like the sigmoid function to
23:25 - every element of a matrix so I can do
23:27 - all of this I can start to write the
23:29 - code for a neural network library okay
23:33 - we have now arrived at the what is sort
23:36 - of the end of this video I'm going to
23:38 - pause and check and see what all the
23:40 - wonderful nice people who are generous
23:43 - enough to all who is along live have
23:45 - corrected me and see if I need to come
23:47 - back and offer any Corrections or answer
23:48 - any questions I'll be right back alright
23:51 - so there's definitely a few things wrong
23:53 - or misleading at least I would say about
23:55 - this that I've gotten some good comments
23:56 - from so that's all right number one is
23:58 - it's very important to realize that
23:59 - these two weight matrices and these two
24:02 - by C's are not the same I wrote them out
24:04 - I mean we have the hidden outputs and
24:07 - the outputs output that's what this is
24:08 - that's what this is so the hiddens
24:10 - outputs are actually the weight matrix
24:12 - between
24:13 - the inputs and the hidden so I could
24:15 - write that as like superscript up here
24:17 - WI H in a way this is the the hidden
24:21 - tout puts is the weight B weight matrix
24:23 - between the input and hidden with a
24:25 - matrix product of those inputs and this
24:28 - weight matrix is the weight matrix
24:30 - between the hidden and the output H 0 or
24:35 - whoo so and you'll actually see as I get
24:37 - into the next video I'll start using
24:39 - this kind of AI H or H Oh kind of naming
24:42 - in some of the variable stuff the other
24:44 - thing is these are not the same biases
24:45 - this is the bias for that's connected
24:49 - with each hidden neuron and so I could
24:52 - say B H there and then this is the bias
24:56 - right that's connected with it's just
24:59 - one one bias but it's connected with
25:00 - this output neuron and so I could put an
25:02 - oh here
25:03 - so that would be more clear about that
25:05 - another point of clarification is there
25:08 - is a way to write this without having
25:11 - this having the bias as part of the
25:13 - weight matrix itself because there's no
25:16 - reason why I couldn't just consider the
25:19 - bias like I said as an extra input that
25:22 - always comes in with a value of 1 and
25:24 - then what I would need to do where's my
25:26 - eraser is I would need to add a fourth
25:29 - column here right because this weight
25:31 - matrix would have all of these
25:33 - connections 1/3 and 2/3 and then there
25:38 - would be the bias values bias one and
25:39 - bias two so if you see this this would
25:42 - actually end up now I could just add it
25:44 - over here if I follow through with this
25:49 - math plus b1 plus b2 because it would be
25:52 - multiplied by 1 which would just be that
25:55 - bias value so that's an important thing
25:57 - to thing how do i possibly finish this
26:00 - video some people we're asking what and
26:02 - I why don't I write out the matrix
26:04 - formula for for this ah you know what
26:08 - maybe I should make this an exercise for
26:09 - you and I can maybe include a little
26:11 - link to a JPEG or something that has it
26:13 - but could you do this same exact diagram
26:16 - for the calculations that flow once the
26:19 - outputs come out of here
26:21 - through this layer so let me just
26:24 - and just give me a second I'm gonna
26:26 - check see if there's any last comments
26:28 - or questions or important Corrections
26:30 - I'll be right back okay that in fact was
26:34 - the end thank you for watching this
26:35 - video I do want to mention that I I'm in
26:37 - the chat or a lot of really useful and
26:39 - important points and comments about how
26:41 - my notation and superscript versus
26:44 - subscript and I or J this is not perhaps
26:47 - the most conventional or correct
26:50 - notation so I apologize for that
26:53 - feel free to leave comments in the
26:55 - comments section particularly good ones
26:58 - if there's a really like one that sums
27:00 - it up all perfectly I'll pin it right to
27:02 - the top but the point of this was for me
27:04 - to kind of like get through the basics
27:06 - of this I am now going to in the next
27:08 - video actually implement this in code
27:12 - and once I've done that we'll be ready
27:14 - to then look at the learning algorithm
27:17 - the training algorithm this thing called
27:19 - back propagation implement vadym code
27:21 - then the neural network will be complete
27:23 - we can actually use it to solve
27:25 - something I hope so
27:27 - see you in a future video thank you
27:29 - acting
27:34 - [Music]