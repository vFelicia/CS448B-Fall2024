00:00 - hello I am back in this video I am
00:03 - finally going to start to build the
00:05 - neural network architecture to make this
00:07 - color classifier I am going to take this
00:11 - data over here which is a long array of
00:14 - many many RGB values normalized to
00:16 - arrange the ER to one which matches with
00:18 - all of these one hot encoded labels and
00:21 - if you don't know what I'm talking about
00:22 - then you might want to go back and watch
00:24 - the first seven yes that's right seven
00:26 - parts of this tutorial series so it's
00:30 - getting very very long but this I think
00:31 - is I'm really getting to the good stuff
00:34 - I don't know maybe it was good stuff
00:35 - before maybe this is bad stuff I don't
00:36 - really know but this I'm really excited
00:38 - I'm excited because now what I'm gonna
00:40 - do and I'm gonna use tension flow yes
00:42 - but I'm going to create the neural
00:44 - network architecture so let's just
00:45 - remind ourselves what we have we have a
00:49 - data set most of the first seven videos
00:52 - of the series was all just about
00:53 - collecting and cleaning that data set
00:55 - and that data set is many many RGB
01:00 - values I think I have like 5000 which is
01:04 - actually is kind of very small for a
01:06 - data set but it's fine for this
01:07 - particular demonstration I have 5,000
01:10 - RGB values each one is labeled with
01:12 - something like blueish or reddish or
01:14 - purplish these were crowd-sourced
01:17 - but those got converted to one hot
01:20 - encoded vectors meaning if there are
01:23 - nine if there are nine labels
01:30 - well let's see then I have a vector that
01:34 - looks like this 1 2 3 4 5 6 7 s 10 9 9
01:38 - and maybe this one refers to purplish if
01:41 - this particular element of this array of
01:45 - numbers has a 1 in it it is that that
01:48 - and that one is for a particular label
01:50 - this one sort of ok so that's what I
01:52 - have so what I I know that I need to
01:54 - have some kind of neural network and the
01:57 - inputs has have a shape of 3 there are 3
02:02 - inputs are G B the outputs have a shape
02:09 - of 9 1 2 3 4 5
02:12 - five six seven eight nine this is the
02:15 - output layer this has a shape of nine
02:20 - inputs of the shape of three outputs
02:22 - have a shape of nine because the goal of
02:25 - this is by what once this whole thing is
02:27 - trained and finished if I send in some
02:29 - RGB values what I'm gonna get is a bunch
02:32 - of numbers all between 0 and 1 and I'm
02:34 - gonna find the one that's the highest
02:36 - and and those numbers are gonna be the
02:38 - probability of this particular data
02:40 - point being a particularly Bowl and I'm
02:42 - gonna find the one that's highest in
02:44 - front of sign at that label who
02:45 - classification we're doing
02:46 - classification so now what goes in
02:49 - between all this now this is a big
02:52 - question and many different scenarios
02:54 - might call for multiple layers different
02:56 - kinds of layers there's something called
02:58 - a convolutional layer which I'll get to
03:01 - but I'm gonna do something really simple
03:03 - I'm gonna have a basic dense layer which
03:07 - is kind of the standard building block
03:10 - of neural network systems and I'm gonna
03:13 - give it some number of nodes so for the
03:15 - sake of our even right now let's pretend
03:17 - that I just gave it four nodes and a
03:18 - dense layer this output is also going to
03:20 - be a dense layer dense layer means fully
03:23 - connected meaning that every input is
03:26 - connected to every node and then every
03:32 - node in the hidden layer this dense
03:34 - layer is connected to every output now
03:36 - I'm going to let your imagination draw
03:39 - the rest of all these connections but so
03:41 - this is what I want to architect so
03:42 - let's now go and architect this now I'm
03:45 - going to do this using tensorflow dot
03:47 - yes and the layers API if you don't know
03:49 - about the layers API you're gonna watch
03:51 - my three or four part series about the
03:53 - layers API tutorial but I'm bookin I
03:54 - sort of talk you through it while we're
03:55 - doing in here so you don't necessarily
03:56 - have to watch that okay so if I come
03:58 - back again this is what I built so far I
04:01 - have all of the training data in tensors
04:04 - and you can see the shape of it I have
04:07 - 5643
04:08 - RGB values and 5643 labels nine with
04:12 - nine possibilities okay so the first
04:15 - thing that I want to do is and I'm gonna
04:16 - do some goofy stuff with some global
04:18 - variables that I might not know that you
04:21 - know just to make my life kind of easier
04:23 - I'm gonna create a variable called
04:26 - and my model which I'm going to create
04:29 - in setup at the end after I've prepared
04:31 - all the data I'm going to say model
04:33 - equals T f dot sequential TF dot
04:40 - sequential so that now that's that's me
04:43 - creating a sequential neural network
04:46 - model it's sequential because it's a
04:47 - feed-forward the layers go in this order
04:50 - so now what I need to do is create some
04:53 - layers so the first thing I want to do
04:55 - is make the hidden let's make the output
04:57 - layer now let's make that we should do
04:58 - it in order we have to do it in order
04:59 - and I make the hidden layer hidden
05:02 - equals TF layers dense and then I put
05:10 - some configuration stuff so I'd make a
05:12 - layer by calling TF dot layers and then
05:16 - I specify the kind of layer this is
05:17 - gonna be a dense layer and then I can
05:19 - pass an object in as an argument and
05:21 - that's where I can configure things like
05:24 - input so I don't remember any of this
05:28 - let's go look it up so let's go to the
05:32 - documentation let's go to TF TF layers
05:38 - and let's go to dense where do we see
05:42 - that sorry I'm looking around for it and
05:46 - it's right there in front of my face
05:48 - under basic so I'm gonna make a TF
05:49 - layers dense I'm gonna click on that and
05:52 - now I'm gonna see these are all of the
05:57 - things that I can pass into the
05:58 - configuration so I need to specify the
06:01 - number of units the number of units is
06:03 - like the number of nodes and I made up
06:05 - four right here
06:06 - maybe let's try 16 maybe we want to have
06:08 - some more than four whatever we can make
06:10 - up anything we want
06:12 - so I'm going to now say units 16 one
06:17 - thing that I know I need is an
06:18 - activation function again I can't cover
06:20 - everything in this video I have other
06:22 - videos where I've talked about what an
06:23 - activation function is and how it works
06:25 - but the idea is the activation function
06:27 - is the function that takes all the sum
06:31 - of all of the things passing through the
06:34 - network being multiplied by the weights
06:35 - and
06:36 - squashes them into some range and so
06:39 - there probably is a really useful
06:42 - interesting discussion about we could
06:43 - have about what would be the best
06:45 - activation function to use right here
06:48 - right now maybe later
06:49 - hello try some different ones but just
06:51 - for simplicity I'm gonna use I'm gonna
06:53 - make a bad decision and just use sigmoid
06:57 - this sort of like historically original
06:58 - activation function of neural networks I
07:02 - want to use the activation function
07:03 - sigmoid let's see what else do I want
07:05 - input dimensions so this is something
07:08 - that I definitely need to do here
07:10 - because remember this this this these
07:12 - inputs this is not actually a lair this
07:15 - is a two-layer Network it looks like
07:16 - there's three but I'm just drawing it
07:18 - with three things and the inputs being
07:20 - but that's not a lair but I do need to
07:21 - specify that three things are coming in
07:24 - so I need to come here and say the input
07:29 - dimensions input dimensions is three
07:34 - because I have an RGB value this should
07:35 - do me just fine for right now so then I
07:39 - want to also create the output layer
07:45 - output TF that's gonna be dense that's
07:48 - going to have nine units because there
07:50 - are nine labels again that's completely
07:52 - arbitrary that's just how I happen to
07:53 - prepare my data set now I don't need the
07:57 - input dimensions because the input
08:00 - dimensions can be inferred by the
08:02 - previous one the input dimensions to the
08:04 - output or the number of units of the
08:05 - hidden so I don't need that but I do
08:06 - need to specify an activation function
08:08 - and guess what I am going to use a
08:13 - different activation function softmax so
08:17 - I'm just gonna type that in right now I
08:18 - will come back and explain what softmax
08:20 - is in a separate video which I think
08:23 - will be the next video of this series
08:24 - I'm just gonna push this a little bit
08:26 - further now I'm gonna say model dot add
08:31 - the hidden and then model dot add the
08:36 - output so this is now me this is now the
08:40 - code for exactly what I diagrammed right
08:45 - here three inputs into a hidden layer
08:49 - with
08:50 - number of units with some activation
08:51 - function into an output layer with some
08:54 - number of units and an activation
08:55 - function okay so we have now built the
08:59 - model here's the thing the next thing
09:01 - that I need to do and I'm gonna do this
09:02 - in the next video what I need to do is
09:05 - create an optimizer so let's just put
09:07 - this in comments create an optimizer and
09:10 - I need an optimization function which
09:12 - typically in the past I've used mean
09:14 - squared error but I'm gonna use
09:17 - something called categorical cross Troup
09:23 - I don't know no no but it sounds really
09:26 - scary but it's not and I can't its I
09:29 - also can't spell it so I'm gonna create
09:32 - the optimizer and then I'm going to
09:35 - compile the model and then I'm going to
09:37 - train the model these are the next step
09:39 - so they need to do this is the
09:41 - architecture for the model people
09:43 - telling me I have an error oh yeah I
09:44 - have something extra extra comma here
09:48 - but so this one we do the next video and
09:51 - so what I need to do in the next video
09:52 - this is like just a few lines of code
09:54 - but I need to I mean I could just add
09:57 - them but I would like to try to
09:59 - understand a bit more about what why am
10:01 - i have softmax here instead of sigmoid
10:03 - or you or any of the other activation
10:06 - functions and why I might choose
10:09 - categorical cross-entropy instead of
10:11 - mean squared error which is if you have
10:13 - happened to watch my ex or tensorflow
10:16 - TAS coding challenge or some of my other
10:18 - layers tutorials I I always just use
10:22 - mean squared hair so that's what's
10:23 - coming the next video I'm going to
10:25 - create the optimizer I'm gonna compile
10:27 - the model and I'm going to talk about
10:28 - softmax and categorical rules and true
10:33 - oh wait wait wait
10:36 - let's actually run this and see if
10:37 - there's a syntax errors no okay and if I
10:42 - just say if I if I look in the console
10:45 - here at model we can see there it is
10:47 - this is the object and it's got all this
10:50 - stuff in it alright see you in the next
10:51 - video
10:53 - [Music]
11:00 - you