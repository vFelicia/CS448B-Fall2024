00:00 - hello welcome to session six or week six I don't 
know you doing this every day every week every  
00:06 - month this could be year six for you if you wanted 
it to be but this is programming from A to Z it's  
00:11 - a set of tutorials a kind of online course you 
could follow all about programming and algorithms  
00:16 - with text text language text words letters all 
that sort of stuff so today in this week's session  
00:26 - the focus will be about the focus that I want to 
have is Markov chains well what's a Markov chain  
00:33 - what smart what's the deal what's going on so in 
the last session I focused on something the sort  
00:39 - of topic of text analysis so in last week the idea 
was really exclusively about reading text in and  
00:45 - analyze it analyzing it counting how many times 
different words appear trying to think about how  
00:51 - you might do sentiment analysis what happens 
when a computer program reads in text I want  
00:56 - to turn now towards what happens when a computer 
program writes its own text and there of course  
01:01 - are many many ways that you could do this in next 
week's session I'm gonna look at something called  
01:05 - a context-free grammar in other sessions I hope 
to look at some machine learning techniques for  
01:11 - generating text as well as other just kind 
of creative ideas for ways to mix and match  
01:16 - and have a program put together text as if it's 
writing it now one thing I should say about this  
01:20 - week's topic of Markov chains is it by definition 
requires a source text from which to generate text  
01:28 - so this is something you'll see in a lot of these 
algorithms for generating text they also include  
01:33 - a reading text component so now a Markov chain 
is not something exclusive to the idea of text  
01:40 - and in fact a Markov chain really just describes 
a sequence or a chain of states like I am happy  
01:48 - I am sad I am running and I'm like typically on 
any given day be sad and then I start running and  
01:54 - then I feel happy and that's kind of my sequence 
so what with a Markov chain looking at how certain  
02:01 - states are sequence and the probability of a given 
state following another state we can use we can  
02:08 - evaluate sort of existing data right I can look at 
like you know what's the weather like today what's  
02:13 - the weather like tomorrow it's a leather-like 
the next day over a year and try to use that  
02:17 - to either predict the new weather or to recreate 
a simulation of weather based on the sequence of  
02:24 - states so this is something I'll look at more 
in more detail in the next video but there's a  
02:28 - piece of that which is if I want to apply this 
idea of a Markov chain to text what I want is  
02:35 - for the characters are words of a piece of text 
to be States for example the state is I the next  
02:43 - state is M the next state is feeling the next is 
like dancing so those are states and the you know  
02:50 - whenever I say m i usually say feeling and then 
whenever you say feeling I usually say like you  
02:54 - know but I might say the other day I I'm feeling 
like eating some kale salad or something like  
03:00 - that too so this is this is like quite possibly 
the worst explanation of our cupcakes ever the  
03:05 - Internet but you skip to the next video where I'm 
sure it will make a lot of sense but a piece of  
03:10 - evaluating this fiscal properties of characters 
and words how they appear next to each other is  
03:15 - this idea of an Engram so this will also be a 
piece of the example that I built today of how  
03:21 - do we look at a body of text and look at this idea 
of engrams now Google has this massive treasure  
03:28 - trove repository of text corpuses of text from you 
know 1800 all the way up until present day and you  
03:35 - can search for the frequency of certain engrams 
so these are what it might be called by grams  
03:41 - meaning to computer science creative code creative 
writing we could think of other ones like I am and  
03:49 - I could search for any of these diagrams and their 
frequency in text and we can see that computer you  
03:57 - can see first of all computer science started to 
appear more and more frequently and starting the  
04:01 - 1960s you can see a sort of more consistent amount 
of i.m you can see creative writing creative code  
04:07 - down here so this is a way of looking at how and 
we can look at trigrams engrams with an order  
04:17 - write an order of for the order referring to the 
number and you can do creative projects with so  
04:25 - here's a great project by Chris Harrison called 
web trigrams visualizing Google's trigram data  
04:31 - and we can see here if we just look at this PDF 
and I'm going to zoom into it so you can see you  
04:39 - can see here that that in this project the Chris 
is visualizing all the words that tend to follow  
04:47 - he and then from there all the words that tend to 
follow that so another I think this is these are  
04:53 - some nice examples to look at and you can kind of 
hear imagine now I have a I am not I was not I do  
05:02 - not I can a so you can sort of see the frequencies 
of these sequences and if you can evaluate those  
05:08 - frequencies you can use those frequencies as 
probabilities from which to generate new text  
05:14 - so here's an example of a project made by Alison 
parish the interactive telecommunications program  
05:20 - is a program at ITP it's called is a program at 
IUP it is it's where I teach and we have courses  
05:28 - every spring and what Allison did is read all 
of the courses into a program look at all the  
05:36 - statistical properties of all the characters and 
words and how they appear next to each other and  
05:40 - use that to generate new new courses so I'm gonna 
let's let's find one that that looks good the  
05:52 - Anthropology is a virtual design workshop MIDI 
and cinematic objects this course constraints of  
05:58 - weekly sessions beyond exercises and inspire the 
possible so this sounds great I think I'll take  
06:05 - that course this is another example of a project 
called King James programming and these are  
06:12 - post generated by a Markov chain that has mixed 
several different input texts the King James Bible  
06:18 - structure and interpretation of computer programs 
and some of Eric s Raymond writings run by Michael  
06:24 - Walker so you can see here these are different 
things that are generated from that exercise  
06:30 - 3.67 addresses why we want a local variable rather 
than a simple map as in the days of Herod the king  
06:36 - okay so there's a lot of possibilities for how you 
might use Markov chains to for a creative output  
06:45 - and on the one hand this is nothing new this 
has been done and done and done and done again  
06:51 - this idea of reading in a source text evaluate 
the probabilities on a character or word based  
06:57 - level I'll talk about that as I implement the code 
in the next video and then text generating out of  
07:01 - that you could make a Twitter bot that generates 
text based on a Markov chain so I I think there's  
07:06 - value and hopefully you might enjoy exploring the 
idea and you might even just take the examples  
07:10 - that I provide and find your own source text in 
but I think for you to think about what is the  
07:15 - reason why you might do this where might it fit 
into an existing project hero walkie-talkie and  
07:22 - and come up with some creative possibilities so 
in the next video I'm going to focus on the code  
07:31 - how to write the code to implement a Markov 
generator I'm gonna go through it entirely  
07:37 - from scratch and then I'm gonna show you a few 
additional examples and then come back with some  
07:42 - exercise ideas for things that you might want to 
try doing yourself after watching these videos and  
07:48 - then I hope you'll share them with me on Twitter 
at Schiffman or you can subscribe to the patreon  
07:52 - too to post the post your work in slack as well 
okay so I will see you guys in the next video