00:00 - hi everyone welcome to another ml5.js
00:02 - video
00:03 - in this video i am going to talk about
00:05 - the object
00:06 - detector in ml5 which is a new feature
00:09 - as of
00:10 - 0.5.0 so you want to make sure you are
00:12 - on
00:13 - at least that version before you get
00:15 - started and try the same code
00:16 - that i'm about to demonstrate to you
00:18 - what do i mean by object detection
00:20 - so far i have covered image
00:23 - classification
00:27 - meaning we have an image maybe it has a
00:29 - cat in it
00:32 - and when that image is sent into the
00:34 - machine learning model
00:35 - in the case of the previous examples a
00:37 - model called mobilenet
00:39 - i get back a list of labels and
00:42 - confidence scores and most likely in
00:43 - this
00:44 - case i would get the label cat with
00:47 - hopefully a confidence score of
00:48 - something like
00:49 - 95 percent there might be some other
00:51 - guesses with lower confidence scores
00:53 - but ultimately the goal is to have a
00:56 - single
00:57 - classification a single label come out
00:59 - and be assigned
01:00 - as the result of the prediction of this
01:03 - image
01:03 - now what happens in the case of object
01:06 - detection
01:09 - let's say i have this same image
01:13 - an object detection model will not only
01:17 - label something in the image but give a
01:20 - bounding box
01:21 - as to where that object it detects is
01:26 - so instead of just saying this image is
01:28 - classified as
01:29 - cat an object detection model will say
01:32 - in this image there is an
01:34 - object of type cat that is located at a
01:37 - particular
01:39 - xy location with a particular width
01:42 - and a particular height the model will
01:46 - also return a confidence score
01:48 - for how certain it is that there is a
01:50 - cat at this exact location so maybe that
01:52 - would also be something like 95 percent
01:55 - and what's special about object
01:56 - detection instead of just classifying
01:58 - the image with
01:59 - one label here if you're detecting an
02:02 - object in an image
02:04 - it could detect more than one thing so
02:06 - maybe there's also
02:08 - i don't know i'm drawing the rest of the
02:09 - cat
02:11 - maybe there's also a dog
02:16 - there's my dog if the image is of a cat
02:18 - and a dog
02:20 - we could get two bounding boxes a second
02:22 - one with the label dog
02:24 - and another x y
02:27 - width and height for its bounding box
02:30 - and
02:30 - right here on the ml5 reference page we
02:32 - can see an example of this
02:34 - here's an image of a cat the bounding
02:36 - box is marked the label cat is indicated
02:38 - along with the confidence score here of
02:40 - 65.41
02:42 - now this doesn't happen by magic this
02:44 - happens because there is a pre-trained
02:46 - model
02:46 - that presumably has been trained already
02:49 - on
02:49 - image many images of cats and dogs with
02:52 - those bounding boxes
02:54 - marked and labeled how does a data set
02:56 - like that
02:57 - even exist something new that is now in
03:00 - the ml5 documentation
03:02 - is a section called model and data
03:04 - providence
03:05 - you'll find this for every single
03:07 - pre-trained model that's in
03:08 - the ml5 library this is a project that's
03:11 - been started by ellen nichols and i
03:12 - encourage you to click the link to find
03:14 - out more about ellen and her work on
03:16 - model and data provenance
03:18 - and what she has done here is created
03:21 - model biographies and data biographies
03:24 - anytime you're using a machine learning
03:26 - model you want to ask yourself the
03:28 - question
03:29 - what data was used to train this model
03:31 - who trained this model
03:32 - in what context and for what reasons
03:34 - anytime you're going to use a
03:35 - pre-trained model in a project you want
03:37 - to think about
03:37 - the ethical implications of where that
03:39 - model came from and how you're using it
03:41 - and researching into the the biography
03:44 - so to speak
03:45 - of the data behind the model and the
03:46 - model itself is incredibly important
03:48 - when considering those kinds of
03:49 - questions
03:50 - in this case the data set behind the
03:52 - object detection model that i'm going to
03:53 - use
03:54 - is a data set called coco coco or common
03:57 - objects in context is a large scale
03:59 - object detection segmentation and
04:01 - captioning data set so before you watch
04:03 - the rest of this video
04:04 - i would pause go to the cocodataset
04:06 - website click explore and poke around a
04:08 - little bit
04:08 - also in this video series you'll find
04:10 - videos about the posnet pre-trained
04:12 - model
04:13 - um in coco in addition to the object
04:16 - detection data there's also a set of 200
04:18 - 000 images with 250 000 instances of
04:21 - people
04:22 - labeled with particular key points on
04:24 - their body coco
04:25 - also includes image segmentation which
04:29 - is
04:29 - a very similar concept to object
04:31 - detection but instead of a particular
04:33 - bounding box
04:34 - every single pixel is labeled as part of
04:37 - a particular category so there are all
04:40 - the pixels for the giraffe versus the
04:42 - pixels for the clouds in the sky
04:44 - and so on and so forth i also want to
04:46 - suggest to you two readings if you're
04:48 - interested in learning
04:49 - more about data sets for machine
04:51 - learning
04:52 - um the humans of ai project by philip
04:55 - schmidt is a project that explores
04:57 - specifically the cocoa image data set
04:59 - and you can learn a lot more about
05:00 - where did those images come from who
05:03 - took those photos
05:04 - and how philip schmidt puts it exposing
05:06 - the myth of magically intelligent
05:08 - machines
05:09 - i also would highly suggest reading
05:11 - excavating ai the politics of images and
05:13 - machine learning training sets
05:15 - by kate crawford and trevor paglin this
05:17 - essay
05:18 - explores the imagenet database another
05:20 - very well-known
05:22 - uh image database that is the data set
05:24 - behind
05:25 - the mobilenet model which serves as the
05:27 - foundation for many of the ml5
05:29 - um image classification and transfer
05:31 - learning examples that are throughout
05:33 - this particular playlist
05:35 - circling back to ml5 if you use the
05:37 - object detector
05:38 - there are two pre-trained models you can
05:40 - select from at present
05:41 - hopefully in the future maybe you'll
05:42 - even train your own model or we'll be
05:44 - able to incorporate other open source
05:46 - object detection models but right now
05:47 - there's yolo or stands which stands for
05:50 - you only look once and coco sd in this
05:53 - video i'm going to demonstrate using
05:54 - coco sd but i encourage you to explore
05:56 - and experiment and do your research
05:58 - about the yolo model as well
06:00 - the coco ssd model comes from tensorflow
06:03 - so the
06:04 - there's the tensorflow tensorflow.js
06:05 - port of the tensorflow coco ssd model
06:08 - that's what ml5 is using certainly um on
06:10 - the github page for that model you can
06:12 - find code for using it in javascript
06:14 - without the ml5 library that you could
06:16 - explore
06:16 - as well as more background about how it
06:19 - was trained and what it does
06:20 - now one thing i'll note is it only
06:22 - detects 80 classes of object
06:24 - not a huge number if you think about it
06:26 - you can find that list of labels as part
06:28 - of the ml5 materials themselves as well
06:30 - all right it's time to write some code
06:33 - the first thing
06:34 - that i want to do is have an image to
06:36 - try to detect objects in
06:45 - there we go so i've made a simple p5
06:47 - sketch that uses the preload function
06:49 - to load a particular image that i've
06:51 - uploaded to the p5 web editor and in the
06:53 - setup function i'm making a canvas and
06:55 - drawing that image you might recognize
06:56 - gloria pickle from my coding in the
06:58 - cabana series
07:02 - along with her good friend greta goose
07:05 - unfortunately evie mango is not pictured
07:06 - here but you can learn more about them
07:08 - on their instagram which i'll link to in
07:09 - the video's description
07:11 - now that i've got my image the next step
07:13 - is for
07:14 - me to load the coco sd model itself
07:20 - for this basic example i'm making heavy
07:23 - use of the preload function which allows
07:24 - me to load images and pre-trained models
07:26 - without any callbacks and everything is
07:28 - ready to go once i get to the setup
07:30 - function but certainly in other contexts
07:32 - you might want to use a callback
07:33 - or write your code in a different way
07:34 - and you'll find all that in the actual
07:36 - official ml5 examples themselves
07:38 - let's double check that things are
07:40 - working correctly by just console
07:42 - logging
07:42 - the detector object oh and i should put
07:44 - that in setup to see that it's loaded
07:46 - properly
07:49 - whoops by accident i put a capital o
07:51 - there it should be lowercase o
07:52 - object detector the console isn't
07:54 - necessarily going to show us anything
07:56 - useful here just a lot of the stuff
07:57 - that's
07:58 - part of that detector object in ml5 but
08:01 - it's more clear for us to look at the
08:02 - documentation than see what's logged
08:04 - here in the console
08:06 - i happen to know that what i need to do
08:08 - is call detector.predict
08:10 - pass it the image and then a callback
08:13 - for when i've got the detections
08:15 - so i'll say got detections as the name
08:17 - of my callback function
08:20 - let's write that function and let's log
08:23 - the results
08:25 - so this is the same pattern in many
08:27 - other ml5 pre-trained models
08:29 - load the model call predict
08:32 - get a result error first in the callback
08:35 - in case there's an error and maybe i
08:37 - should check for that
08:40 - and then do something with the results i
08:42 - just want to log them right now
08:44 - detector.predict is not a function
08:47 - oops looking at the documentation the
08:50 - function is not
08:50 - predict it's detect so predict is a
08:54 - general word for when you want to
08:57 - ask the machine a machine learning model
08:59 - to give you the output associated with a
09:00 - given input but in this specific case
09:03 - um the ml5 function is named detect
09:05 - because it's a it's a more descriptive
09:07 - word of what we're actually doing
09:08 - i'm going to change this to detect let's
09:10 - also comment out this
09:12 - unnecessary console log and run it again
09:17 - aha look at this three objects okay
09:20 - there's a cat there's a dog what's the
09:23 - third one
09:24 - it's something in this list of 80 things
09:28 - did you see it there
09:29 - object zero is the dog here's the
09:32 - confidence score
09:33 - and the x y width and height it also
09:36 - looks like it gives you something called
09:37 - normalized which are probably all of
09:38 - these values but
09:40 - mapped to a range between zero and one
09:43 - object one is the couch
09:45 - it detected the couch and then object
09:48 - three i'm going to assume
09:49 - is the cat let's draw those bounding
09:52 - boxes
09:52 - so i can write a loop to look at all of
09:55 - the elements of the array
09:58 - and of course there's countless other
09:59 - ways you can do this with different
10:01 - types of array
10:02 - functionality but this simplest way i'll
10:04 - just say let object equals results index
10:06 - i
10:08 - let's first just draw the bounding box
10:11 - at
10:11 - object x object y with the width and
10:15 - height
10:16 - let's give it a sort of distinctive
10:18 - color just so it really
10:19 - is emphasized with a given uh
10:22 - thickness and make sure there's no fill
10:25 - blocking it out
10:26 - there we go i've got three rectangles
10:29 - one drawn around the dog one around the
10:31 - cat and one around the couch
10:32 - let's add the labels just so we can see
10:34 - them
10:38 - and where do i want to draw it at the x
10:41 - location but shifted a little over and
10:42 - the y location but shifted a little bit
10:44 - down
10:44 - maybe i want to put it in the center
10:46 - there's no rules here i'm just going to
10:47 - do it however i'm going to do it
10:51 - run it again
10:55 - there we go couch dog cat
10:58 - now of course i want to think about
11:00 - visual design and contrast this isn't
11:02 - the
11:03 - uh best visualization of it but you can
11:05 - see it's working
11:06 - maybe if you're following along pause
11:08 - this video try to add the confidence
11:10 - score that's a nice little exercise for
11:12 - you to do
11:13 - hopefully you have some creative ideas
11:15 - of how you might want to use this or
11:16 - experiment with this
11:17 - an application that i would imagine you
11:19 - might want to try is try this model out
11:21 - on a real-time video feed so i have a
11:23 - webcam here on this laptop
11:25 - i can rewrite this code to use the p5
11:27 - create capture object
11:28 - and then pass the video as the thing
11:31 - we're looking at
11:32 - into the machine learning model same as
11:34 - we did with the mobilenet image
11:36 - classification examples
11:37 - so i'm going to save this code as is and
11:39 - you'll find it linked in the video's
11:41 - description
11:42 - and i'm going to duplicate it and
11:43 - rewrite it with the capture object
11:45 - call it webcam comment out the image and
11:48 - add a video instead
11:54 - i need a draw function because now i'm
11:57 - going to be looping and drawing every
11:58 - frame of the video in real time
12:02 - let's make sure the video is the same
12:04 - size as the canvas
12:06 - and let's run this and see what happens
12:09 - so i see my video there
12:10 - i console log the detections but i only
12:13 - detected things once and i found nothing
12:15 - why is that that's because i called
12:18 - detect
12:19 - in setup with the video once got the
12:22 - results and never called detect again
12:24 - so now i need to create this kind of
12:26 - loop system
12:28 - where i first call detect i get the
12:31 - detections
12:32 - and once i've gotten the detections
12:36 - let's call it again
12:39 - oh why did i say object i should be
12:41 - saying detector
12:43 - and look it's recognizing me now i don't
12:47 - love the way that i've written this
12:49 - because drawing the results here
12:51 - outside of drawn and happening in this
12:53 - sort of like separate sequence
12:54 - is a little bit prone to error so i want
12:57 - to just
12:58 - adjust the way i'm doing this i'm going
13:00 - to take this loop
13:02 - and i'm going to put it into draw this
13:04 - way i know my drawing sequence
13:06 - is always happening in the right order
13:08 - draw the video draw the results on top
13:10 - of it but this isn't where i got the
13:12 - results where i got the results is in
13:14 - the got
13:14 - detections function so i'll just use a
13:16 - global variable here
13:18 - to sort of link those two things so
13:20 - let's create a variable called
13:22 - detections i'm going to make it an empty
13:24 - array to start
13:26 - then in the got detections function i
13:29 - will just
13:29 - set detections equal to the results so
13:32 - now it's a global variable that gets set
13:34 - whenever there are new detections
13:35 - and whatever the latest detections are
13:37 - they'll always be drawn in draw
13:39 - by me adding detections here
13:42 - let's run this and let's see if i can
13:44 - get some object detecting going
13:46 - oh boy things froze error what went
13:49 - wrong
13:50 - results is not defined sketch line 33
13:54 - oh detections is the global variable
13:58 - but i'm still using results here i need
14:00 - to change that also to detections
14:03 - notice how when the error happened the
14:05 - sort of video element is still going
14:08 - but the canvas where i'm separately
14:09 - drawing the video got frozen
14:11 - i probably only want to see one of those
14:13 - so i don't need to see the original
14:15 - video uh element i can call video.hide
14:17 - to remove that
14:19 - run it again all right person
14:22 - cell phone hope it's still no it still
14:25 - sees me
14:26 - how about book this is a book that i'm
14:27 - currently reading it's called weapons of
14:29 - math
14:30 - destruction also highly recommended when
14:32 - thinking about algorithms and machine
14:34 - learning
14:34 - i happen to have a paintbrush scissors
14:38 - baseball bat okay
14:39 - batter up
14:44 - all right so you get the idea um
14:46 - something that i might want to add to
14:47 - this is some kind of debouncing or
14:49 - interpolation you can see that it's very
14:51 - very uh very
14:53 - noisy so that's something that um i will
14:55 - also
14:56 - include some references for in the video
14:57 - subscription maybe even an extra example
14:59 - that adds that
15:00 - um but this wraps up this video so thank
15:03 - you for watching this video tutorial on
15:04 - the ml5
15:05 - object detector if you have some
15:07 - creative ideas or things you want to try
15:08 - let me know write something in the
15:10 - comments and you can also go to the
15:11 - coding train webpage associated with
15:13 - this video
15:13 - and submit your creative um examples and
15:16 - experiments there
15:18 - thanks for watching and i'll see you in
15:19 - another ml5 video goodbye
15:26 - [Music]
15:31 - you