00:01 - well hello again it is me your friendly
00:05 - neighborhood coding person who makes
00:08 - lots of mistakes and always gets stuck
00:10 - even when said person thinks it's gonna
00:12 - be really simple and just take about 20
00:14 - minutes and takes 4 hours later and then
00:16 - I haven't had dinner and I get very
00:17 - exhausted and I have to go home and then
00:18 - I feel depressed all weekend because my
00:20 - coding was terrible and I was so much
00:22 - better it could've been so much better
00:25 - come back now you might not be aware of
00:28 - this if you were watching but I live
00:31 - streamed already today for a prime
00:34 - within almost 3 hours so I started
00:36 - around 10:30 10:45 and I finished around
00:40 - 1:30 I did take a 20-minute break in
00:41 - there because I had an important phone
00:42 - call I don't have any important phone
00:45 - calls as far as I know although where is
00:47 - my phone
00:48 - this is an interesting question came up
00:51 - here to this room and I usually take my
00:55 - phone out of my pocket then I because I
01:01 - have to put this microphone thing in my
01:02 - pocket you have just tuned into a
01:05 - livestream about a person who has lost
01:07 - their phone let's see it could be over
01:11 - here underneath my books and notes I
01:14 - have I took some more notes for today
01:19 - there's a there's an iPad here where I
01:22 - play some sound because this is weird
01:24 - this is a little distressing I did
01:26 - recently use the loo which is a term
01:30 - that I like to use and could I have left
01:32 - in it here hold on a second everybody
01:33 - let's make my phone play some loud
01:36 - noises I'll do it over here I'm gonna do
01:39 - that fine my phone I can't believe this
01:41 - is what I'm doing oh I out on silent so
01:44 - I have to log in to this like this is
01:52 - really what I'm doing right now I don't
01:54 - even know why ok thank goodness for one
01:57 - password and I'm sponsor could be a
01:58 - sponsor allow oh no wait no no just ok
02:05 - alright I'm about to play a very loud
02:08 - noise it won't be loud for you hopefully
02:11 - hopefully be loud for me
02:16 - you know what I think I just left it in
02:18 - my office downstairs I think I just
02:20 - plugged it into my office to charge and
02:22 - then when I came up here I didn't bring
02:24 - it with me that's it I think it's as
02:25 - simple as that
02:26 - we're just gonna check to make sure it's
02:28 - not in the hallway it's playing the
02:30 - sound now I'm gonna check to make sure
02:32 - it's not in the hallway I'm in the
02:37 - hallway just looking around in the
02:41 - hallway I left the door bub she's gonna
02:45 - check in this room here where I was
02:47 - there's no reason I would have taken it
02:48 - out of my pocket in here all right all
02:50 - right I'm feeling pretty confident that
02:52 - I left it I'm also looking like I'm
02:54 - talking to myself which I am I feel
02:56 - pretty confident that I left just
02:58 - looking downstairs which is fine oh oh
03:02 - oh dear okay hello again welcome to your
03:07 - regularly scheduled coding train now I
03:10 - was saying that this morning definitely
03:14 - in this building because it's at the
03:15 - same location that I am okay let me go
03:18 - back to so now this morning I did
03:23 - something which I felt was like very
03:25 - exciting and I need to turn this camera
03:27 - on it is on I think just switch to that
03:31 - camera oh I still have the cold where I
03:35 - talked about test-driven development
03:40 - with unit testing and continuous
03:43 - integration and I'm like saying this was
03:46 - like this big giant grin on my face
03:47 - because I'm so shocked that this is the
03:49 - thing that I actually just did because I
03:51 - I know that many viewers who might be
03:53 - work as their job doing actual software
03:56 - development this is a fundamental and
03:59 - key piece to your workflow many of you
04:01 - who volunteer or on open-source projects
04:04 - also collaborate through and and your
04:08 - projects that you're involved with run
04:09 - tests and run continuous integration
04:11 - processes but it is not something that
04:13 - I've had a lot of experience with in my
04:15 - be just made like some pretty rainbow
04:18 - colors on the screen twirl around and
04:20 - talk about objects and raise kind of
04:23 - world that
04:23 - living so I was excited to look at this
04:25 - workflow number one is I would like to
04:29 - use this channel as a way of helping
04:32 - people to contribute to open source
04:34 - projects so understanding something
04:35 - about unit tests and continuous
04:37 - integration is important as a key piece
04:39 - of a lot of projects and also I would
04:41 - like to start incorporating it into the
04:42 - community projects that I open up with
04:45 - this channel so one community project
04:48 - that I am working on is a simple and I'm
04:53 - calling it now a toy neural network
04:54 - library and I don't have anything open
05:00 - but if I open up atom it should open up
05:03 - to where I last left off and as you
05:06 - might recall part of that toy library is
05:09 - a toy just put the words boy in front of
05:12 - everything cuz just make it sound like I
05:15 - didn't I don't have to worry about that
05:16 - I've done it like incorrectly and it's
05:18 - like a problem my toy matrix library is
05:22 - this first attempt to start working with
05:25 - some tests so if you didn't watch this
05:27 - morning one of the things that I did is
05:28 - I added instead of just matrix such as I
05:30 - didn't matrix not test touch X this has
05:33 - a unit test which runs let's say the add
05:36 - function and make sure the correct
05:38 - output comes out and then I put all this
05:40 - on github where it now lives if I open
05:42 - up my browser here and I look for toy
05:48 - neural network and so here's the thing I
05:52 - there's been a bunch of pull requests
05:54 - here let's see hmm
05:58 - I think I want to go through the pull
05:59 - request later because what I want to do
06:01 - is work on the gradient descent problem
06:03 - which is the left this is why I don't
06:07 - know there's never a good time right I
06:08 - sort of felt like I need to find that
06:10 - day where I didn't have a busy week and
06:12 - I had all morning to read about gradient
06:14 - descent and sip some tea and relax do
06:16 - some deep breathing come up here and
06:18 - we're just like work through it and go
06:20 - have a nice relaxing weekend today
06:21 - wasn't that day I was like streaming I
06:22 - was having meetings and it won't go
06:24 - there but anyway as well give it a try
06:27 - so I anticipate that's going very poorly
06:29 - so I lowered the key to life is lowered
06:33 - expectations lower your expectations but
06:36 - I do have a
06:37 - at least even if I don't do a good job
06:39 - with maybe a tutorial about gradient
06:43 - descent and back propagation that at
06:44 - least get it working and that I can move
06:46 - forward and start to make some projects
06:48 - with the library itself and then also as
06:51 - I've mentioned before start using
06:53 - another library called deep learn is
06:56 - which is an open source deep learning
06:59 - library put out by some researchers at
07:02 - Google so I don't want to be stuck here
07:06 - anymore I wanna be stuck at the gradient
07:08 - descent step so we'll see can I
07:10 - instead of gradient descent this is
07:12 - gradient I'm gonna climb the gradient
07:15 - descent he'll get to the top roll down
07:20 - momentum is a thing right the gradient
07:22 - descent thing with momentum that's the
07:24 - thing that's the thing that I won't be
07:25 - able to explain in a way that makes
07:26 - sense either because this is gonna go
07:28 - very poorly okay so oh I didn't did I
07:32 - tweet no I did tweet so III have a
07:34 - feeling there's like fewer people
07:36 - watching because I think I used up all
07:37 - of my like livestream notifications on
07:39 - YouTube so anyway so that's what I plan
07:44 - to do I do want to look through these
07:46 - unit tests I mean these pull requests
07:49 - but I think those of you thank you let
07:51 - me just quickly thank make 1999 em dat
07:54 - sev and TR Nord Musa because lik Idon't
07:57 - tour sorry that I didn't pronounce any
07:59 - of those names correctly github user
08:01 - names I would say if you wouldn't mind
08:03 - I'm happy to for these to review these
08:05 - pull requests I wouldn't if you wouldn't
08:07 - mind a minute work on the code some more
08:09 - and push up new code so that might you
08:11 - know just to check your pull request
08:12 - again if there's any conflict space with
08:14 - some of the new stuff that I'm adding
08:15 - because if I go through and check
08:16 - everything to merge now I can go to lose
08:18 - time I want to get to the grade set I do
08:19 - want to ask a question though because I
08:21 - didn't notice here that this particular
08:26 - merge request moves all of the source
08:32 - files into a source folder and then the
08:35 - test file into a test folder and my
08:38 - understanding was that we just adjust
08:42 - the way that jest is structured is
08:44 - actually is to get away from this having
08:46 - to maintain totally duplicate directory
08:48 - structures and
08:49 - actually keep your tests with your code
08:51 - you can use directories but you're
08:52 - supposed to use like underscore
08:53 - underscore test in the directory where
08:55 - your sources so I'm not sure about this
08:57 - but if anybody like feels like they
09:00 - actually know what they're doing with
09:01 - jest and test different development want
09:04 - to like write a comment on this pull
09:05 - request give me some thoughts about that
09:07 - and please let me know all right so I'm
09:10 - gonna put the put the github away let me
09:13 - go to localhost there's nothing there
09:15 - probably I'm gonna go to terminal and
09:19 - where am i I'm gonna go to the desktop
09:21 - and and this is now github repository
09:26 - which was good I'm gonna run a local
09:30 - server so I will not be doing any by the
09:34 - way somebody said to be the test-driven
09:35 - development means you write the tests
09:37 - first
09:38 - well yeah first of all it's I mean I
09:42 - should be so happy that I'm using tests
09:45 - of the first place so I guess I have to
09:46 - get to a whole other level of actually
09:48 - writing the tests first but I'm you know
09:50 - in creative coding which is at all CODIS
09:52 - there's a little bit of a silly term and
09:53 - problematic term but I like to use it in
09:55 - a sentence and just sort of like quickly
09:57 - reference the kind of stuff that I do
09:59 - with graphics and animation and
10:01 - interaction but I do think there is an
10:03 - aspect of not knowing what you're making
10:04 - while you're making it so maybe you
10:06 - can't really do the tests until after
10:08 - you've figured out what you're making
10:10 - maybe okay like you hear that I have a
10:19 - cold it's great my backpropagation
10:21 - videos forever having a cold okay now
10:25 - what's happened here matrix is not to
10:27 - find huh barrel to load resource am I
10:34 - actually what is this nonsense
10:42 - what's going on here okay fine
10:47 - can't remember I'm reading the server
10:49 - okay let's just minimize this so I have
10:53 - this weird thing I don't know why why do
10:55 - I have this again because I had to
10:59 - figure out where I last left off oh yeah
11:02 - this is me trying to actually implement
11:04 - it and this is actually where I last
11:08 - left off after like these are the output
11:12 - errors and these are the hidden errors
11:14 - okay okay so this is what I want to be
11:18 - working with I'm going to temporarily oh
11:20 - yeah personalize it next redo gradient
11:23 - descent video about Delta wait formulas
11:25 - okay
11:26 - implement gradient descent in library
11:28 - talk about different activation
11:30 - functions X or M nest okay there's no
11:32 - way I'm getting through all this today
11:33 - but I'm still not using criteria but
11:45 - yeah so this is where I am did I already
11:48 - have this D sigmoid in there and already
11:50 - seen oh oh that's a neural network -
11:51 - okay so I think what I want to do is
11:55 - just for clarity sake I'm gonna grep
12:02 - take this file out and put it somewhere
12:04 - else like on the desktop okay so now
12:10 - there is okay now and what's actually in
12:21 - sketch touch is just running that
12:23 - training function okay
12:33 - so no errors some boardings I don't care
12:36 - about and their network library and the
12:44 - training function okay okay now simple
12:51 - recommendation control a delete okay all
12:55 - right now let me go over here so I'm
12:57 - very sad to erase this but I think it's
12:59 - time I have completed the set of test
13:03 - driven development videos I did not make
13:05 - the one about having a bot tweet
13:07 - whenever your tests passed I would like
13:14 - to have that happen I will come back to
13:16 - that there was so much I didn't
13:18 - anticipate I really thought I was just
13:20 - making one or two like a two or three
13:22 - little short videos I think it turned
13:23 - into like four I'm gonna erase this with
13:29 - this eraser
13:39 - already I've been up here for a half an
13:41 - hour getting ready let's clean this a
13:46 - little bit with some water
14:16 - okay okay I think I'm about ready to get
14:21 - started I have some resources to point
14:24 - out to you about about this topic one of
14:28 - which is ml for a is it just let's just
14:33 - Google that ml for a I want the ml for a
14:37 - github I oh this is a website put
14:41 - together by Jean Cogan who if you're
14:43 - interested in machine learning knows a
14:46 - lot more about this than I do and under
14:50 - here there are instructional guides so
14:53 - many things Cara central openFrameworks
14:55 - demos classes code what I'm looking for
14:57 - is the recent one that was just
14:58 - published about how neural networks are
15:00 - trained ok so this is a resource I was
15:04 - just looking at this like a half an hour
15:05 - ago spending really good to get through
15:06 - all of it but this is a good
15:08 - supplemental resource that will go
15:10 - deeper than what I'm gonna do I would be
15:15 - remiss if I didn't mention the three
15:17 - blue one Brown videos and I'm gonna go
15:22 - to by the way I've just decided in this
15:27 - I had now have a YouTube red account for
15:30 - a Google account but don't you could
15:34 - email there I'll never see it I've never
15:36 - checked that I just like made a Google
15:38 - account just for being logged in while
15:40 - I'm in a live stream if I need to do
15:42 - stuff I mean I signed up for YouTube red
15:43 - because I know the ads don't show but I
15:46 - don't actually watch YouTube on this so
15:48 - it's very weird that I have this anyway
15:50 - that's just a little little
15:51 - behind-the-scenes tidbit for you where
15:55 - is the playlists essence a little
15:58 - absence calculus neural networks okay on
16:00 - to a reference this particular playlist
16:02 - and then there is the then there is the
16:11 - what's the other thing
16:12 - ah make your own so amazon.com slash
16:18 - shop slash the coding train there is the
16:20 - make your own neural network book that I
16:22 - will mention okay I've got a lot more
16:24 - books I want to add to this page
16:27 - so incidentally if you do I just
16:29 - mentioned it if you do your start your
16:31 - shopping on Amazon here it will support
16:33 - the channel so thank you for doing that
16:34 - for those of you who or buy stuff on
16:36 - Amazon and are able to okay let's see
16:41 - alright so alright I have to like Center
16:47 - myself so where I last left off if I
16:51 - recall was just working on the hidden
16:53 - errors and the output errors so looking
16:56 - at those and I don't think I think I
17:07 - can't do this anymore I think I'm gonna
17:09 - have to do the these steps cuz I'm gonna
17:12 - want to use some of this stuff so that's
17:15 - fine I have to figure out how to do the
17:18 - bias the nature of code offset is the
17:21 - nature of code on Amazon in fact it is
17:24 - I'll just go back to here and there is a
17:26 - link to here he actually says 999 which
17:28 - is I don't know why it says that that's
17:30 - for the Kindle you can get the Kindle
17:31 - book version for free if you just go to
17:33 - my website you're gonna get it on Amazon
17:35 - but um there's a print version I don't
17:37 - know why maybe I just i'd probably did
17:39 - that incorrectly where who my logged in
17:42 - ass hello I'm not logged in I need to
17:45 - fix that probably so that it links to
17:47 - the paperback version 65 customer
17:49 - reviews Wow okay hold sorry sorry okay
17:52 - oh no I needed that page because I'm
17:56 - gonna reference that book although I
17:57 - have a physical copy of that book but
17:58 - it's open to a certain page and I don't
18:02 - want to alright so I have some notes
18:06 - made some notes
18:13 - [Music]
18:18 - meeting notes I have this book actually
18:23 - put it under my pillow at night and I
18:28 - hope it's but this is the book that I
18:41 - found very helpful oh yeah and I should
18:46 - also reference Suraj who makes lots of
18:52 - wonderful videos about AI and machine
18:55 - learnings as well okay so these are the
18:59 - tools though these are the resources
19:00 - I've been using for like the vert the
19:03 - step by step building all the code for
19:05 - the neural network thing oh the book was
19:11 - over the mic oh maybe that's what I
19:13 - could use I'm trying to figure out
19:14 - there's like a thing that people do if
19:18 - you're a person who speaks with a
19:21 - microphone you can kind of like simulate
19:24 - a funny yelling thing and then move the
19:26 - microphone away and it sounds like
19:27 - you're shouting it from far away but
19:28 - five you shout it's right here so I
19:30 - guess I could do this alright
19:39 - all right so now what I need YouTube
19:43 - Schiffman mathematics of gradient
19:48 - descent
19:50 - ding-ding-ding-ding-ding this is really
19:52 - a problem
19:53 - I think it's though just because I
19:54 - googled the exact title of my video
19:56 - most people probably don't Google
19:57 - mathematics ingredients then but
19:58 - something is really wrong with the world
19:59 - if this comes up it was a disaster what
20:04 - if I just do gradient descent yeah Oh
20:09 - somebody call YouTube and tell them
20:11 - their algorithm I mean this is good this
20:12 - is good good good good good good
20:14 - here you go we've got some solid
20:16 - educational content here this is a
20:20 - problem wooof where what's happened
20:23 - there we go it looks Suraj's here so
20:24 - this this I also would highly recommend
20:26 - Oh looks like formulas who you can hover
20:29 - over it and the video starts to like
20:31 - play was this made with the broke before
20:38 - broken arm or after broken arm I can't
20:40 - can't figure out it's mathematics curse
20:42 - seven months ago no that's gotta be I
20:48 - don't know who knows yeah it's got to be
20:50 - pre broken arm because I wasn't making
20:52 - any videos for anyway 20 minutes this
20:57 - stream a couple hours will you change
20:58 - the to the dark side of you there's a
21:01 - dark side of YouTube or you just mean my
21:03 - food my feet how do I do that I would
21:05 - love to anyway I don't know you're
21:07 - distracting me it's 4:20 and I got to
21:14 - get out of here by like 6 o'clock
21:16 - good you know I think you know anytime
21:19 - I'm live-streaming more than five hours
21:21 - in one given day it's nothing good is
21:24 - going to happen I did take a break but
21:26 - I'm we're on to like hour for here
21:28 - basically alright we're gonna go to this
21:31 - video and let's try to find a little
21:37 - like spot where I just end and have the
21:39 - formulas on the screen
21:51 - I'm very red look how red I am in there
22:01 - do I want the whiteboard image to finish
22:03 - with yeah let's do this
22:05 - this makes it look like I might have
22:07 - done something that made sense
22:09 - okay all right click your purple human
22:16 - icon then click dark theme woah dark
22:22 - theme
22:24 - [Music]
22:36 - [Laughter]
22:38 - - doc I don't know it's I I like the
22:56 - dark theme for myself but I feel like it
22:57 - looks weird in my it's so dark
23:02 - there's no Rainbow theme oh it's an
23:06 - experience
23:07 - ideal for night saves eyes okay okay but
23:13 - I'm not gonna be on this page for very
23:14 - long
23:14 - all right um I have to start this video
23:23 - all right
23:25 - I'm so not prepared for this I'm so in
23:28 - the wrong screen all right let us begin
23:48 - you're now watching somebody pace in
23:51 - order to avoid having to start talking
23:53 - about this all right okay hello everyone
23:59 - here we are again hope deep breath I'm
24:03 - gonna talk in this video I'm trying to
24:05 - get this to the next step of gradient
24:07 - descent and I to be honest what I would
24:09 - I hope by the time that you're finished
24:11 - watching this video I'll have
24:11 - implemented gradient descent and I'll be
24:14 - working in this JavaScript neural
24:17 - network library that we are all building
24:18 - together as a happy internet family and
24:23 - I think what I'm gonna do is see my plan
24:25 - here is to just get loud in the hallway
24:29 - let me just start over please I some
24:31 - reason what I always need like the first
24:35 - video mm-hmm okay
24:44 - like just kind of I should probably
24:46 - watch the end of my last backpropagation
24:49 - video but all right all right all right
24:58 - everybody here we go
25:01 - [Music]
25:09 - hello all right this is a good moment
25:12 - for me I think I'm excited to see if I
25:15 - can get through this video because if I
25:17 - can't implement this last piece of the
25:19 - train function in the neural network
25:20 - library then I'll have a working version
25:23 - of some kind of neural network library
25:25 - like thing that I can start to finally
25:27 - apply to some projects and it's my goal
25:29 - actually that in some of the future
25:30 - videos I make that make use of the
25:32 - library and eventually other more
25:34 - sophisticated machine learning deep
25:36 - learning libraries like deep learning is
25:38 - a new library which i think is now gonna
25:41 - be called ml5 which i'll talk about in
25:43 - another video that I'll be able really
25:45 - well I'm sad about it to make stuff and
25:46 - show you how to make stuff but I'm kind
25:48 - of working through this cuz it's just
25:49 - it's just something I have to do so I
25:51 - started I have to finish I'm just trying
25:55 - to like vamp till you stop move on and
25:57 - watch something else because I'm not
25:58 - sure you should continue so but let's
25:59 - let me try to actually been awhile since
26:01 - I recorded the previous video
26:02 - unfortunately might be watching them one
26:04 - after another let me try to like reset
26:05 - kind of where I think that we are so we
26:08 - have a something like a two layer
26:12 - network it has you know an input
26:17 - quote/unquote airboat layer it has the
26:20 - so these are the inputs this is the
26:23 - hidden and this is the output it is
26:28 - fully connected meaning every input is
26:32 - connected to every hidden and every
26:36 - output is connected to every every
26:40 - hidden is connected every input every
26:42 - output is connected every hidden and so
26:44 - now puts come out here the inputs come
26:46 - out here
26:47 - all of these connections have a weight
26:49 - so and we can consider them in a weight
26:52 - matrix and I guess I should put this
26:55 - stuff back in here wait
26:57 - if this is like input one two three this
27:01 - is hidden one - this is weight one two
27:04 - one this is weight - two one
27:07 - this so these all end up in a nice
27:09 - weight matrix these end up in weight
27:12 - matrix the outputs come out oh boy
27:15 - if it array like y1 y2 a vector the
27:19 - inputs come in in a vector x1 x2 x3 okay
27:25 - I'm getting back into this I didn't even
27:28 - draw like off the top wonderful so now
27:31 - the idea is we want to do training and
27:34 - the kind of training that I'm doing
27:35 - right now is called supervised learning
27:39 - where I have some known output I've some
27:42 - known inputs with known outputs inputs
27:44 - with target outputs I send the inputs in
27:47 - I feed them forward I get some actual
27:51 - guests output back and I have some sort
27:54 - of error which we can think of as the
27:57 - error equals the guess I'm the guess -
28:03 - the target so I'm gonna say like why -
28:05 - target okay so this is the error now I
28:09 - should have mentioned before that there
28:11 - are a variety of ways to train a neural
28:15 - network to the idea of training is
28:17 - adjusting the weights to get results
28:21 - that are more close to the actual I
28:25 - would say that again and by training
28:33 - I mean adjusting all of these weights
28:35 - like their knobs to try to actually get
28:38 - the matched target output when you send
28:40 - in the training data so one thing I
28:42 - should mention I really got to like even
28:44 - just take a minutes and make a video
28:45 - just about this but in most training
28:47 - situations you'll have training data
28:52 - test data and then of course there's the
28:56 - actual unknown data so we want to say we
29:01 - want to say like oh here's some labeled
29:03 - data I'm going to use that to train but
29:04 - I need to save some of that labeled data
29:06 - because
29:07 - what if I trained my neural network only
29:10 - to work with the training date but it
29:12 - doesn't actually work with any other
29:13 - data well I can determine that by giving
29:15 - it some saved some data that I didn't
29:17 - train it with but I know the answer to
29:19 - see how it dumps with that and that's
29:20 - how we can evaluate it so I just want to
29:21 - mention that this is gonna be the
29:22 - process now this idea of back
29:25 - propagation with gradient descent is one
29:27 - technique and it's a technique I'll put
29:29 - some links in this video's description
29:30 - you read about the history of it it's
29:32 - it's been around for a long time it's a
29:33 - big innovation in training neural
29:35 - networks but there is a lot of questions
29:38 - as to whether that's optimal best for
29:40 - the future etc there's different you
29:42 - know select weeks of these algorithms
29:44 - and I actually next probably after I get
29:48 - through this plan to use a genetic
29:50 - algorithm to evolve the weights of a
29:52 - neural network which opens up the door
29:53 - for a lot of kinds of projects that I
29:55 - excited to try to make so all that aside
29:58 - here we are what did we get so far we
30:01 - figured out in the previous video we
30:03 - calculated this error and then we
30:06 - calculated the hidden error which I'll
30:09 - just call each error right that's part
30:11 - of back propagation it's while we have
30:13 - this error how do we distribute this
30:14 - error all around so we can adjust all
30:16 - these weights the reason why we're doing
30:17 - this is because we actually did this
30:19 - already twice I've done this twice once
30:22 - with if you look at my videos about a
30:24 - single perceptron right which gets two
30:27 - inputs I forgot about the bias like I
30:28 - always I always remember the bias I'm
30:31 - biased against the bias I'll come back
30:34 - to that but the single perceptron which
30:36 - just takes in two inputs and one output
30:39 - well it doesn't say it's a single neuron
30:41 - you can take in more than two inputs but
30:42 - a single neuron with multiple inputs and
30:44 - an output I actually did this we did
30:47 - this idea of training this with gradient
30:50 - descent and we also did it I did it we I
30:52 - did it in a video about linear
30:55 - regression where if I have a bunch of
31:00 - points in a two-dimensional space I can
31:03 - find the line that fits these points the
31:05 - best and I did this what's interesting
31:08 - here as though this app so for a linear
31:11 - Russian if you recall there's actually a
31:13 - mathematical formula to just compute the
31:15 - exact line of best fit called ordinary
31:17 - least squares I think I went through
31:19 - that in the previous
31:20 - as well but the idea here and so the
31:23 - idea is we're trying to figure out y
31:25 - equals MX plus B the formula for this
31:29 - line well basically what this is doing
31:32 - is this is exactly the same process that
31:34 - we needed here only we have these
31:37 - weights you know you can almost think of
31:41 - this as like M and this is B maybe or
31:43 - something if there's just one input and
31:44 - a bias we had to fit we had to fit all
31:48 - the stuff coming through here into a
31:50 - line well this is actually what we need
31:51 - to do with all of these all of these
31:54 - places right
31:55 - we basically need to do the exact same
31:56 - algorithm that we did for this one line
31:59 - to compute this is the weight and this
32:03 - is the Bice what we really have right is
32:05 - that y equals MX plus B we have y equals
32:08 - m1 X 1 plus M 2 X 2 plus M 3 can you see
32:13 - that X 3 plus M 4 X 4 but added up plus
32:17 - B we have this we basically have exactly
32:20 - the same problem but in a
32:21 - multi-dimensional space so I just need
32:24 - to figure out how do I adjust each one
32:26 - of these M one and two and three and
32:27 - four and B all of these weights inside
32:31 - of the matrix so the same training
32:33 - method I did for a linear regression
32:34 - gradient descent the same thing we did
32:36 - with the perceptron we now need to apply
32:37 - it here in this multi-dimensional space
32:40 - so what should I do next well first of
32:43 - all I forgot to thank a bunch of the
32:44 - resources errors usually target minus y
32:53 - I know why is that but I thought yeah
32:57 - that's what I thought its target will -
32:59 - why but but I thought then last time
33:03 - people told me it was Y minus target all
33:10 - right fine
33:13 - hold on I some viewers rightly pointed
33:17 - out that when I did this previously and
33:19 - I guess the convention is target - why
33:22 - the nice thing about this is when we
33:24 - look at what a cost function is if you
33:26 - look at a cost function for a machine
33:29 - learning system
33:32 - if you look at a if you look at a cost
33:35 - function for a machine learning system
33:36 - you'll see that the cost function is the
33:40 - sum of all the errors squared so if you
33:42 - do target - wire y - target you square
33:44 - it doesn't really matter but it is an
33:46 - important distinction probably you have
33:48 - to get it right of course otherwise you
33:50 - might start training in the wrong
33:51 - direction as you'll start to see as we
33:52 - do other stuff okay let me come over
33:57 - here and let me think so I'm gonna come
33:59 - back to this video in a second but let
34:02 - me first say there are three things one
34:04 - is this is a new resource that just came
34:06 - out it's not a new resource but a new
34:07 - page on the ml for a ml for a github do
34:12 - site and this is a site put together by
34:13 - Jean Cogan as a ton of machine learning
34:16 - resources videos examples demos etc it's
34:19 - amazing and there is a nice article here
34:22 - about how neural networks are trained
34:25 - with a lot more detail than I'm gonna
34:26 - get into here but you can see the same
34:28 - sort of idea of talking about linear
34:29 - regression a loss function adding more
34:32 - dimensions this is the idea this is what
34:34 - we're doing
34:34 - of course I highly recommend you watch
34:37 - the three blue one Brown series about
34:40 - rating descent back propagation and back
34:42 - propagation calculus this will give you
34:44 - a massive extreme set background for
34:48 - what I've grown attempt to do it's just
34:49 - kind of like let me just tape this in
34:50 - code like kind of squint the press the
34:52 - button hope it works so this is great I
34:55 - highly recommend this and then I also
34:57 - have been using the make your own neural
34:59 - network book which I could hold up and
35:01 - wave around for you by Terry Gross she'd
35:04 - and there's a link to it on the coding
35:06 - train Amazon shop along with some other
35:07 - books that I've been using for videos
35:09 - and then so this is where I what I
35:11 - wanted to do now is try to connect back
35:14 - to here this is from my previous video
35:20 - entitled the mathematics of gradient
35:22 - descent where I go through a long
35:24 - algorithm to arrive at a very simple
35:28 - result and that simple result is the
35:32 - following time out for a second
35:39 - I'm just sorry I want to pull up the
35:41 - code for that where would that be is it
35:45 - in rainbow code coding challenges
35:49 - gradient a linear regression where did I
35:55 - put the code for a linear regression
35:57 - with gradient descent
35:58 - oh it's with my intelligence and
36:03 - learning class
36:18 - where is that silly repository
36:21 - intelligence there it is
36:24 - mmm classification regression linear
36:27 - regression interactive regression live
36:33 - mmm this with gradient descent or with
36:38 - yeah Oh what I'm doing it like yeah okay
36:49 - target - all right okay
37:01 - yeah there's also the perceptron but I
37:03 - mean I did the perceptron in processing
37:06 - which is probably mistake now they think
37:08 - about it because all this whole series
37:09 - is in JavaScript but okay so so all of
37:17 - this math boiled down to a simple
37:19 - formula which is in this case of y
37:25 - equals MX plus B what I'm looking to do
37:27 - is calculate the change in M and the
37:29 - change in B and so we can now write
37:31 - those formulas over here I'm gonna keep
37:45 - this oh I'm in the run it's fine we can
37:50 - now write those formulas over here so in
37:54 - the case of y equals MX plus B we need
37:58 - to calculate Delta M how do we want em
38:01 - to change based on the error and it how
38:05 - we want em to change is the learning
38:07 - rate by the way learning right if you
38:08 - look in textbooks and stuff and
38:09 - sometimes written with this um the reek
38:11 - letter alpha I believe is that how fun
38:14 - yeah learning rate times x times error
38:20 - and Delta B equals learning rate times
38:27 - error this is all done through some
38:31 - calculus in looking at the cost function
38:33 - looking at the derivative of the cost
38:35 - function the slope and how to walk
38:37 - around that cost function and find the
38:39 - bottom the minimum error how what values
38:42 - of M and B do we have to have the
38:45 - minimum error and that's what we want to
38:47 - do here what values of w w1 1 W 2 1
38:50 - doubly 300 400 I have to have the
38:52 - minimum error and that error each
38:56 - individual error we've got to like back
38:58 - it up and pass around and chop it up
39:00 - okay so how do we take this and move
39:05 - this to a multi-dimensional
39:07 - scenario so let me timeout here for a
39:11 - second
39:14 - every time i time i timed out timed out
39:18 - have done I can't do it anymore
39:20 - let me look at my notes here yes I
39:24 - started doing like the partial
39:25 - derivative stuff I think what I want to
39:28 - just do is try to write out the formula
39:34 - first of all did I get those formulas
39:36 - correct this x times error times
39:38 - learning rate x times error times
39:40 - learning rate okay error times learning
39:43 - rate Eric Taylor okay great so what I
39:46 - want to do is okay hit into output not
39:53 - input to hidden hidden to output okay
40:03 - [Music]
40:25 - I'm sorry I'm okay do I have enough
40:33 - space on the board I'm gonna rewrite
40:36 - these this doesn't have to be just be an
40:39 - edit point here just say here I'm gonna
40:46 - rewrite these a little smaller further
40:47 - over to the left so I have more room to
40:48 - write out the formulas for the matrix
40:51 - version what'd I say it was learning
41:13 - rate times x times error Delta B equals
41:20 - learning rate times error okay so these
41:27 - are the formulas for the change in slope
41:31 - and change in by offset change and
41:34 - haven't change in B for y equals MX plus
41:37 - B so I have the same situation except I
41:40 - have a slightly different one I have
41:42 - like the output equals Sigma of like
41:47 - that whole you know weight matrix was it
41:54 - matrix product with the I guess it's
41:58 - with the inputs right I have this kind
42:02 - before but it's basically the same thing
42:03 - plus the bias plus the bias which is a
42:06 - vector so I have like kind of like
42:07 - basically the same thing but instead of
42:09 - like single dimension these are all
42:11 - matrices Multi multi dimensional so what
42:14 - I'm going to attempt to do now is I'm
42:16 - going to just write out a notation for
42:19 - these formulas using matrices and try to
42:23 - like compare
42:24 - craft a little bit and I'm not sure I
42:26 - get it right because i'm kinda i'm using
42:29 - a bunk combination of trying to keep
42:30 - consistent with my notation from before
42:32 - and some conventions but let's see I've
42:34 - got it written down here let's look at
42:35 - this
42:36 - so let's first just say I just want to
42:38 - figure out Delta for these weights so in
42:41 - other words you can think of what I'm
42:42 - doing is instead of just Delta n I want
42:45 - all of these weights so I want to say
42:46 - Delta all of the weights and the weights
42:49 - are from hidden to output the change in
42:54 - each weight IJ each row I column J I
42:59 - think that's what I've been using for me
43:01 - to remember what I used in the last
43:02 - video equals and it's gonna look very
43:05 - similar first of all learning rate same
43:07 - I always have a learning rate learning
43:08 - rates gonna basically tune like how big
43:10 - of a step are we gonna take and I don't
43:13 - like the way I kind of want to rewrite
43:15 - this just because to have it match the
43:17 - way I'm gonna write the matrix version
43:19 - error times X I'm gonna write this times
43:26 - the vector e right that's that's the
43:31 - that's the vector of output 1 minus
43:36 - target one output to our target one
43:38 - minus output one right that error vector
43:40 - that I've talked about along and then X
43:52 - in this case is kind of like the input
43:56 - the input to this neuron to each of this
44:00 - output layer so I'm gonna call that a in
44:03 - this form gonna call that like I could
44:04 - call it X what I'm call it H it's what's
44:05 - coming out of the hidden layer I'm gonna
44:08 - put that here and put that at the end H
44:12 - so these are the components that exactly
44:14 - match up here at times H right we have
44:19 - same way of learning rate we have the
44:21 - error right and by the way if I were
44:22 - doing this for this layer if I would say
44:24 - Delta W eights from I to J from input to
44:28 - hidden ih would equal do learning rate
44:31 - and then I would say times the hidden
44:34 - error right you remember that from the
44:36 - previous videos where I went through how
44:37 - to
44:38 - get the output error and the hidden
44:39 - error how we pass all that around and
44:41 - then this is times the input so this is
44:46 - the same exact formula but two different
44:47 - layers learning rate earths or hidden
44:52 - errors the hidden output or the input
44:56 - output sort of way to say they hidden
44:58 - output the hidden output is the input to
45:00 - the output the input is the input to the
45:03 - hidden so you can sort of see how this
45:05 - is this formula is looking at this part
45:07 - and this formula is looking at this part
45:09 - okay but I didn't put one other thing in
45:11 - here there's something funny that's
45:13 - gonna go in this spot right here I'm
45:16 - gonna pause for a second cuz I'm trying
45:21 - to see if nobody has complained yet rows
45:28 - as I and columns as J did ya Roberts
45:31 - Wayne says dancin using roses I and
45:33 - columns with J which i think is probably
45:34 - incorrect but that is how I've been
45:37 - doing it so is that do you recall is
45:39 - that how I did it in the previous videos
45:41 - I hate to have that inconsistency but I
45:53 - think that's how I've been doing
46:06 - you have the code in front of you okay
46:08 - great so in four okay so what goes here
46:19 - where here's here's the thing something
46:21 - that is quite different about the linear
46:26 - regression with gradient descent that I
46:28 - looked at before and the way these
46:30 - neural networks work is that there's
46:32 - this activation function and we want to
46:35 - kind of understand well when I just the
46:38 - wait what does that do does it push it
46:41 - which way in terms of the activation
46:43 - function and so the activation function
46:44 - being if I can only find a place to
46:47 - write this being sigmoid what we
46:52 - actually need to include here is the
46:54 - derivative of sigmoid right we need to
46:57 - look at what the change in what's a good
47:05 - way of saying this I'm trying to like
47:07 - sort of intuitively explain why the
47:08 - derivative of sigmoid makes it in right
47:11 - here maybe somebody will have a
47:15 - suggestion for that
47:25 - okay the chat to see if anybody has like
47:28 - a really nice way and then I need to
47:30 - find a derivative I'm trying to like not
47:35 - derive the formulas basically because I
47:37 - read through a bunch of different
47:39 - tutorials that do that I just think it's
47:41 - I'm never going to do a good job on this
47:42 - channel so I just want to sort of like
47:44 - put the farmers out there and implement
47:45 - them in code so I want to
48:08 - yeah is the slope of the gradient thank
48:13 - you yeah it's really the gradient of the
48:19 - slope of the gradient where I guess I'll
48:23 - just look at this page here No
48:38 - why am I looking for a page I'm just
48:40 - gonna write this out yeah so I so people
48:50 - are saying like oh you have to explain
48:51 - the chain rule and partial driven I did
48:53 - actually go through sponge of that stuff
48:55 - in previous videos and I don't want to
48:59 - just get into rehashing it again here I
49:01 - just feel like and I maybe I would come
49:05 - back again once I get further along to
49:07 - try to do some more derivation stuff but
49:09 - I feel like there are better resources
49:11 - out there that do this if we can just
49:12 - kind of agree on a notation and kind of
49:15 - understand the pieces of it and get
49:17 - implementing I think that's best
49:21 - so yeah cuz I'm just not gonna make like
49:27 - someday I will be a different person and
49:30 - make those tutorials by the way this
49:34 - morning I didn't have a sponsor for the
49:35 - coding train circle CI but we're back
49:37 - again according drink sponsored by
49:44 - water
50:00 - usually calculate the gradient and the
50:02 - deltas separately Delta M is the
50:04 - derivative of Y so you need the
50:06 - derivative of sigmoid yes yes yes thank
50:11 - you that's a good way of thinking about
50:12 - it all right so what goes here
50:22 - well if you recall what well a lot of
50:24 - what I used in this previous video where
50:27 - I went through the mathematics of
50:28 - gradient descent was this idea of a
50:29 - derivative the slope well the derivative
50:34 - you have to remember that Delta Y weight
50:45 - was such a nice little Delta and that's
50:50 - what I meant to say
51:05 - so the chain so so that change in a
51:10 - given weight you need the derivative of
51:13 - Y and we've got we get the output from
51:17 - the sigmoid so do we need this idea of
51:20 - the gradient we need to side you have
51:22 - the gradient which is the slope of the
51:25 - sigmoid function so we need the
51:27 - derivative of a sigmoid function okay
51:31 - like succinctly say it maybe you can
51:33 - plot the derivative sigmoid to show what
51:35 - it's doing that's a good idea but
51:40 - meaning is useful for understand a I
51:41 - would just say this succinctly in a way
51:47 - that makes sense the derivative sigmoid
51:49 - indicates it's confidence it can descend
51:51 - or ascend to confidence based on the
51:52 - learning right oh that's what I like
51:58 - that Medan gives me a great I know I I
52:01 - just don't even want to be in the frame
52:02 - right now leave it over there well I
52:04 - read the chat I like that's a nice way
52:07 - of saying it all right
52:27 - because ultimately this is exactly the
52:30 - same so far but in this case because it
52:34 - was linear the derivative of Y is 1
52:39 - because it was linear and here I've got
52:42 - the sigmoid function so I need the
52:43 - derivative is that kind of it because in
52:47 - this case it really have like sigmoid
52:48 - around this we need a derivative of that
52:52 - which is yeah I think that's makes sense
53:00 - you need to show how W is connected to
53:03 - the error yes yes I showed that
53:05 - previously but writing it down is a good
53:08 - idea yes yes ok thank you ok the
53:23 - derivative of Y is n yeah yeah yeah but
53:26 - I've got that it's not M but uh yes yes
53:34 - well the derivative of Y with respect to
53:38 - the partial derivative with respect to M
53:39 - is X which is actually what's going on
53:42 - here I'm in the wrong stream all right
53:44 - but all right
53:46 - I have confused myself again I thought I
53:49 - had it first again so what goes here
53:53 - again these formulas are the same so far
53:56 - I have the learning rate I have the
53:57 - error this is one dimension but I have a
53:59 - single learning rate at the error which
54:00 - is a vector I have the input X and the
54:04 - stuff coming out of hidden is the input
54:06 - to the output so I have that here so
54:08 - what goes here well one thing you might
54:09 - remember if you looked at the
54:11 - mathematics of gradient descent is the
54:13 - derivative is kind of important and if I
54:16 - take the derivative here the derivative
54:17 - of Y its M with respect to X but the
54:24 - derivative of M sorry hold on I think
54:31 - what I want to say is I take the
54:32 - derivative of Y with respect to M right
54:36 - this says it's like did this pull for
54:38 - odd partial derivatives I
54:39 - just gonna end up with X which is kind
54:41 - of what's going on here but there's the
54:43 - sigmoid function here
54:45 - there's no sigmoid function here so what
54:47 - I really need is something else here
54:49 - that helps me I need to add in the
54:51 - derivative the sigmoid function and
54:52 - guess what we are so lucky so here's the
54:55 - thing sigmoid I should have mentioned it
54:57 - before sigmoid function is not a
54:59 - commonly used activation function
55:01 - anymore or it's not that sort of known
55:03 - to be an optimal one there's a zoo and
55:06 - for the French for rectified linear unit
55:09 - which and there's also tan H lots of
55:11 - other activation functions and I'll come
55:12 - back to some of those especially as we
55:13 - get into some more real-world examples
55:15 - but sort of in the classical historical
55:17 - sense I'm gonna stick with sigmoid and
55:19 - the nice thing about sigmoid is it's so
55:21 - easy to differentiate because sigmoid of
55:24 - X the derivative which you sometimes can
55:27 - write it's like s that little line thing
55:30 - tick what do you call that again of X
55:32 - the derivative of sigmoid equals why can
55:36 - I never remember this I think it's just
55:37 - sigmoid of X times 1 minus Sigma of X
55:42 - and I went off the board there but let
55:46 - me check to make sure that's right
55:59 - a prime prime it's called prime the
56:01 - tickets called prime yes hold on I'm
56:03 - gonna rewrite that let me just make sure
56:04 - I got that right
56:06 - sigmoid I'm over here by the way that's
56:19 - what I wrote right hold on let me try
56:29 - this again since I wrote off the board
56:30 - and also I was told this is called prime
56:32 - what I like to call tick
56:33 - it's called prime so we read this again
56:35 - the derivative of sigmoid sigmoid prime
56:38 - x equals asks the sigmoid function times
56:43 - 1 minus the sigmoid function I'm just
56:48 - checking so this is actually really easy
56:50 - and this is what goes right here
56:53 - so we need to we need to put in I have
56:57 - this here now what what is this by the
57:02 - way the sigmoid function this is the
57:04 - thing I've already calculated that's
57:05 - what's coming out of here
57:09 - no no wait wait wait sorry the sigmoid
57:13 - no no I want the sigmoid of the output
57:16 - which is here sorry the wat Y sorry the
57:20 - sigmoid function is something we've
57:21 - already calculated that is what's coming
57:25 - out of here so what I simply need is to
57:28 - multiply this by the output times 1
57:37 - minus the output so this is the final
57:42 - formula what I need to do is I need to
57:46 - adjust each weight according to we can
57:48 - like look at the components of this I
57:50 - definitely need to adjust it according
57:51 - to the learning rate because that's just
57:53 - a thing I'm gonna say big steps little
57:55 - steps I need to adjust it according to
57:57 - the error and then this the derivative
58:01 - the sigmoid function the slope of the
58:03 - gradient kind of gives me some I think
58:06 - somebody in the chat used the word
58:07 - confidence right how what do I do
58:11 - what how how does how does sigmoid
58:13 - change if I move this direction or that
58:15 - direction and then according to the
58:19 - mathematics of gradient descent I need
58:21 - to also multiply by what was coming in
58:23 - so I think now and down here what this
58:27 - is going to be is it's the output of the
58:30 - hidden function so the hidden function
58:33 - times 1 minus hidden so I spent a very
58:36 - long time kind of just writing these
58:38 - formulas here and I and I'm also gonna
58:42 - have to maybe do some matrix
58:43 - transposition move stuff around so that
58:45 - it works but so let me actually just
58:48 - pause and end this video right here and
58:55 - in the next video what I'm going to do
58:58 - is go back and examine my matrix library
59:00 - go back and look at my neural network
59:02 - code and see if I can take these these
59:06 - formulas and get them working in the
59:08 - code let's see here I don't know what
59:18 - did I why did I mess up
59:20 - that was pretty poor oh you need to
59:24 - multiply this hey let me just pause yeah
59:38 - of course I should be doing this in
59:39 - Python no I should not be there's no
59:42 - reason at any whelmed whatsoever I
59:44 - should be doing what I'm doing oh I was
59:53 - a weak man in the chat wrote this is my
59:55 - final formula but was making a joke and
59:57 - I was like oh you have a final formula
59:59 - I'm waiting a book meeting I was waiting
60:00 - to see it okay all right so I'm gonna
60:09 - just kind of leave that as it was I mean
60:13 - it definitely could get edited together
60:14 - it was pretty awkward but this is not my
60:20 - best work I could always come back and
60:23 - try to do it again but that's why I want
60:25 - to
60:25 - create these into two different videos I
60:27 - just want to see this anybody have any
60:29 - real problems with this notation before
60:33 - I move on because it's possible that I
60:35 - would kind of try to do this explanation
60:37 - over again cuz I was most add videos me
60:40 - like reminding myself where I last left
60:42 - off which is probably unnecessary but I
60:45 - just want to get anybody have any
60:46 - comments on this notation before I move
60:48 - on oh yeah the bodies I forgot about the
60:55 - bias yeah I gotta put the bias in there
60:58 - I'll come back and do that uh what time
61:02 - is it
61:02 - five o'clock
61:12 - yeah target is the error yeah the cost
61:16 - is target minus y squared the sum of all
61:19 - that the eight should go to e H equals
61:24 - each time you need okay well not oh yeah
61:29 - so I need a parenthesis here let's hold
61:32 - on maybe I better like hold on let me
61:40 - let me collect things that I'm missing
61:43 - so that one missing parentheses bias I'm
61:52 - writing these hopefully where you
61:53 - couldn't see them but you can yes that
61:55 - should be a Hadamard products in there
61:57 - but where so this is the thing right
61:59 - isn't it this needs to these are the
62:01 - Hadamard products right i just wanna
62:03 - understand this greatly like well let's
62:05 - take the learning rate out of this this
62:07 - is a single vector this is a single
62:09 - vector so those are the Hadamard
62:11 - products and then this is like the
62:15 - transposed outputs this this has to be
62:20 - transposed that way I can get a matrix
62:23 - to change all the weights right I think
62:26 - that's right right
62:32 - matrix multiplication is wrong the eight
62:36 - should go to e
62:52 - don't worry Austin I'm locked - locked
62:57 - lost
62:57 - well can I speak before II all right
63:09 - that's what we not talk about the
63:10 - derivative of lady cuz that's not that's
63:16 - not relevant to this video right now
63:17 - don't you have a book I do have a book
63:19 - let's look at the formulas in the book
63:23 - let's look at the notation eight should
63:28 - go before e where eight should go before
63:31 - e I have no idea where you're talking
63:35 - about oops oh this can run off alright
63:44 - so let's look here let me look at the
63:46 - formula in this book learning rate times
63:50 - e sub K which in my notation is J times
64:00 - Sigma which is of the output which is
64:04 - this times 1 minus Sigma or the output
64:07 - which is this times the output
64:14 - transposed
64:24 - the the output is the learning rate the
64:26 - sigmoid is the squashing activation
64:28 - function we saw before that times is
64:31 - multiplication as the normal element by
64:33 - element so that's the that's what I was
64:35 - trying to this is like I was trying to
64:38 - use an asterisk for element wise
64:46 - multiplication and then I was I guess I
64:48 - can use the dot for like the matrix
64:51 - multiplication this is slightly
65:02 - different though than what I and then
65:05 - what's written here how about random
65:11 - numbers output gradients equals output
65:13 - errors times output derivatives hidden
65:15 - errors equals output weights transpose
65:18 - product and Delta H before E except
65:25 - after C output error ALPA graves equals
65:33 - output errors times output derivatives
65:37 - hidden errors so I've got the up of
65:40 - gradients equals the errors times the
65:42 - derivatives right yes that's a nice way
65:46 - of thinking about it the errors times
65:50 - the derivatives the hidden errors times
65:55 - the derivatives final outputs minus
66:02 - final output errors times final outputs
66:05 - times 1 minus final outputs yep dot
66:10 - product with transpose the hidden
66:12 - outputs yeah this is right this is
66:16 - exactly the same
66:23 - and then here
66:32 - I like crowd look they just said I like
66:35 - how somebody said output gradients
66:36 - equals output errors times output
66:38 - derivatives so the gradient is the error
66:42 - times the derivative this is the error
66:47 - this is the derivative and then we have
66:51 - to adjust it in the same way we do here
66:53 - with the inputs and the learning rate so
66:58 - the gradient here is just the error is
66:59 - just the error is that right maybe I
67:04 - should come back to I think I want to
67:06 - slowly go back to where I didn't have
67:08 - anything here yet this this a way of
67:15 - thinking of the gradient right and the
67:17 - gradient here is the errors times the
67:21 - out the output errors times the output
67:22 - derivative then Delta is multiplied by
67:29 - the learning rate multiplied by the end
67:30 - deltas multiplied by the learning rate
67:32 - multiplied by the input yeah okay okay
67:34 - but is it right to say what I would like
67:36 - to know is it right to say that the
67:38 - gradient here is just the error whereas
67:45 - here because the derivative of is this
67:48 - is just a linear function the slope is
67:54 - continuous not continuous but it is the
67:58 - same always and here the derivative the
68:06 - gradient is the error times the
68:08 - derivative of the output column vector a
68:15 - multiplied by Rho vector V gives made
68:16 - sure the arrows by B columns yeah yeah
68:19 - okay
68:22 - H and I need to be transposed yeah these
68:24 - definitely need to be transposed all
68:26 - right so let me let me go back and try
68:31 - this again I hope you're saying I want
68:34 - you to be my teacher I hope you're
68:35 - talking about somebody else not me at
68:38 - this point all right Mattia please I
68:44 - hope we can make something of this
68:46 - because I don't know if I can bear to
68:47 - try this again but I'm gonna go back to
68:56 - where I didn't have anything here yet
69:01 - and these whoops were kind of a bit of
69:07 - more of a mess and I didn't talk about
69:14 - the transposing and yeah in the air back
69:27 - propagation yes okay the book make sure
69:30 - yeah really should really just read the
69:32 - book make your own neural network all
69:35 - right okay okay let's let me try this
69:39 - again okay Oh
69:50 - this wouldn't have been here
69:52 - and this also wasn't clearly this wasn't
70:00 - clear what this was so what goes here
70:10 - this is the question now this is where
70:12 - we're so happy to have this book
70:13 - make your up a neural network because we
70:15 - can just look up the formula in here but
70:17 - what's actually is gonna go here is the
70:19 - derivative of the output now let's think
70:22 - about this why don't I come in with your
70:23 - videos you might have remembered back
70:24 - from the mathematics of gradient descent
70:26 - that we had to take the derivative but
70:31 - in this case of Y in this case it's MX
70:34 - plus B it's very simple derivatives the
70:36 - linear function here we have this
70:38 - sigmoid thing we need to calculate the
70:40 - gradient and the gradient is the errors
70:44 - time the error of the output times the
70:46 - derivative of the output what happened
70:49 - well has the output change has the
70:51 - output change relative to the errors so
70:54 - in this case right we need to add in
70:56 - here the derivative of sinh Y now here's
70:58 - the wonderful thing sigmoid is the
71:00 - function right the derivative of sigmoid
71:02 - s prime X is simply equal to the sigmoid
71:08 - of x times 1 minus can you see what I'm
71:12 - writing is sigmoid of X so in this case
71:17 - we need to calculate this gradient the
71:20 - derivative of sigmoid and right here now
71:23 - one thing I should really clarify here
71:24 - is so learning rate is a scalar number
71:27 - error is a vector learning rate is a
71:30 - scalar number I'm gonna put an asterisk
71:32 - here err a hidden error is a vector so I
71:35 - need to multiply and the output is a
71:38 - vector so these are actually this is
71:40 - element wise multiplication of the out
71:43 - know I've already what's coming out of
71:45 - here out of output has already had the
71:47 - sigmoid pass through it so I just need
71:49 - to say the output plus 1 minus the
71:50 - output the output plus 1 minus the
71:54 - output now what's weird about this is
71:58 - now the H is really this right we have
72:02 - an exact same
72:02 - formula I have the the learning rate the
72:04 - error gradient here you could sort of
72:07 - consider this the gradient here is just
72:09 - the error the gradient here is the error
72:10 - times the derivative of the output and
72:13 - then if I'm multiplying I need to get a
72:15 - weight matrix so the input by the way is
72:18 - also a vector but I need to get a matrix
72:21 - so the interesting that happens if I use
72:23 - the matrix product here and transpose
72:27 - this vector right this is element wise
72:30 - multiplication this is just a scalar now
72:33 - here's the thing if I have the sorry if
72:41 - I have this error and the gradient
72:45 - essentially at a called gradient as a
72:48 - [Music]
72:51 - single column vector four columns if I
72:55 - multiply that by a the hidden output
73:01 - which is also a single column vector but
73:02 - transposed what am I going to get I'm
73:12 - actually going to get a four by four
73:14 - matrix
73:16 - pause for a second think about that
73:18 - right because I take the row and do the
73:23 - dot product I take the row into the dot
73:26 - product take the road to the dot product
73:28 - with the road to the dot product
73:46 - I see like lots of whoops
73:58 - did I write this in the wrong order when
74:01 - I'm doing when I'm doing matrix
74:02 - multiplication I need to have the same
74:05 - number of rows as columns right so the
74:10 - for this should have rows for this is
74:12 - one column this is one row this is four
74:15 - rows four columns it's right yeah
74:32 - yeah so I already did by the way kids
74:34 - Miller wondering I already did the whole
74:36 - error calculation stuff in the previous
74:37 - two videos like a few weeks ago so I'm
74:39 - just trying to do the deltas here and so
74:42 - I just want to make sure so what is this
74:46 - let me just as let me just take a minute
74:48 - to I just like don't have space on the
74:50 - whiteboard it looks only one I can't
74:51 - right it's looking right over here for a
74:52 - second right so what am I gonna get here
74:59 - first thing I'm going to do is do G 1 H
75:06 - 1 right and then here it's gonna be it's
75:13 - funny I can't do this matrix math all of
75:16 - a sudden okay first of all I can't do it
75:18 - because I'm afraid and people are
75:20 - watching me let me let's look let's go
75:22 - to here matrix multiplication not XYZ so
75:28 - can I make my own matrix there we go
75:36 - right let's look at this
75:45 - yeah I love it so helpful
75:49 - reset okay okay if you take a single
75:59 - column matrix and multiply it by a
76:02 - single row matrix as long as it has this
76:05 - has the same number of columns as this
76:07 - has rows let's let's look at what
76:09 - happens right there's a wonderful
76:10 - website that I use often when I get
76:11 - stuck it's called
76:13 - matrix multiplication XYZ and here what
76:16 - I can do is I can make I'm gonna make
76:18 - this exact right here's an arbitrary
76:21 - single column vector and then I'm gonna
76:24 - make a single row vector so a one column
76:28 - matrix a one row matrix I'm gonna hit
76:30 - multiply so this is I'm just gonna go to
76:33 - the end here we can this is a nice
76:35 - little animation that goes through
76:36 - everything that I did in previous matrix
76:38 - multiplications and you can see we end
76:39 - up with a four by four matrix so this is
76:42 - exactly what we need to do to be able to
76:45 - get the deltas for all the weights okay
76:51 - so this also has to be if I'm going here
76:55 - this has to be transposed and this
76:57 - should be element wise multiplication
76:58 - with the the I lost track here so I'm
77:06 - taking the output error and multiply by
77:10 - the derivative of the output here I'm
77:12 - taking the hidden error and multiplying
77:14 - it by the derivative of the hidden so
77:16 - that would be H plus h plus one so not
77:21 - plus O what I have plus here have had
77:22 - that wrong for so long times that's
77:24 - another element wise multiplication
77:25 - times one and this is a one not an I 1
77:29 - minus H okay I think we've done it I'm
77:33 - gonna just check here my trusty guide
77:36 - but I highly recommend that you read
77:38 - instead of listing reads that are
77:40 - listening to me and I'm gonna look at
77:41 - this the change in weight matrix equals
77:44 - learning rate times the error times
77:47 - sigmoid of the output well I have
77:50 - sigmoid kind of built-in here I'm
77:51 - assuming the output of the sigmoid has
77:53 - already been done
77:54 - times 1 minus Sigma or 2 the output I'm
77:56 - sigmund I've been done with the dot
77:58 - product of the matrix multiplication of
78:00 - the hidden the output transpose well
78:02 - this is right my notation is slightly
78:05 - different okay so I think we're just
78:08 - about right now of course I forgot about
78:10 - the bias so I'm gonna have to basically
78:12 - do exactly the same thing with the bias
78:13 - bias is always simpler because I can
78:16 - just get rid of this so I'm assuming I
78:17 - could probably sort of do the same thing
78:19 - here so but I'll get to that in a bit
78:23 - let me at least try to implement this in
78:25 - the code and then I'll do that in the
78:27 - next video and then we'll come back to
78:28 - the bias okay I say that wrong I
78:42 - probably said that wrong I need to have
78:44 - the same number of rows as columns right
78:53 - so confusing it's confusing as I anyway
79:05 - right because if I take this out oh no
79:10 - that works
79:18 - seem to have columns as rows so these
79:21 - can looks as I said that wrong right
79:30 - like in my in my matrix library when I
79:34 - do multiply aids columns have to equal
79:40 - bees rows oh yeah I said that wrong
79:44 - chutes I said that wrong this is all
79:50 - that one column has to equal one row a
79:52 - number of rows and the number of columns
79:54 - can be other things because you're only
79:57 - gonna have a four by four darn I feel
80:02 - like I finally kind of got all this in
80:04 - some way that made sense but I kind of
80:10 - messed up that we'll see if how terrible
80:11 - that is okay
80:23 - alright let's just let's move on and let
80:27 - me try to implement the code okay
80:40 - suppress shift miss here trying to move
80:44 - on and start to implement this code so
80:45 - what I need to do is okay so what I've
80:47 - done successfully somewhat successfully
80:49 - in the code so far is compute the errors
80:52 - the output errors and the hidden errors
80:55 - and I've added I have a transpose
80:56 - function I have a matrix multi multiple
80:58 - I function so what I need to do is
81:00 - compute the deltas the gradients I need
81:02 - to figure out how what I need to do to
81:04 - change all these weights I should say
81:05 - that I did say something I think wrong
81:06 - in the previous video so I might as well
81:08 - at least say it here which is that in
81:10 - with matrix multiplication and this
81:12 - shouldn't be an asterisk here I don't
81:13 - know you know this you can think of the
81:14 - dot product or just an X for matrix
81:16 - multiplication I need to have be a this
81:19 - is matrix a and this is matrix B a the
81:22 - number of columns in a has to be the
81:23 - same as the number of rows in B so this
81:26 - could have as long as there's one column
81:28 - this could have three rows and this
81:30 - could have eight columns but this is one
81:32 - dimensional this way one dimensional
81:34 - this way that's gonna that's going to
81:35 - always give us in the end the correct
81:38 - dimensions for the weight matrix so I
81:40 - kind of Miss stated that in the previous
81:42 - video I don't know if you were worried
81:44 - about that I don't know if correcting it
81:46 - by now but that's enough so let's try to
81:47 - put this stuff in here now one thing
81:50 - that I think I really need to do
81:51 - unfortunately is that I had this lovely
81:54 - idea previously of like Oh what I need
81:57 - to do first to get the error right is I
82:00 - need to feed the inputs in get the
82:03 - output and compare it to the target
82:06 - right that's going to give me the errors
82:09 - and then I can compute the hidden error
82:11 - by doing the weighted percentage stuff
82:14 - that I did previously the issue is once
82:17 - they start wanting to do all this stuff
82:18 - it would be nice if I could remember all
82:20 - the parts that happen during the
82:22 - feed-forward process so as much as I
82:24 - just wanted to call feed-forward here I
82:27 - think it's actually gonna work better if
82:29 - I run the feed-forward stuff here so I'm
82:32 - gonna actually grab all of this copy
82:36 - and I'm going to paste it right here so
82:38 - there's definitely some redundancy in
82:40 - the code this is gonna help me figure it
82:42 - out so I need to feed everything forward
82:46 - inputs hidden which then gets the bias
82:51 - passes through sigmoid so it's hidden
82:53 - this hidden matrix is left in the state
82:57 - of the values coming out of here good
83:02 - then I have the output be adding them
83:06 - the bias and the output is left in the
83:09 - state of it coming out now in the state
83:11 - of it coming out of here now Bri viously
83:16 - I had taken the outputs from the root
83:19 - from the feed-forward function I don't
83:21 - want to do that anymore in fact I guess
83:23 - I'm being consistent
83:24 - I should call these outputs so now I
83:27 - have the outputs plus I have like all
83:30 - the other stuff that happened before if
83:31 - case I need to reuse it and I have the
83:34 - targets and I can have the output errors
83:37 - so now I need the gradients so what do I
83:40 - need for the gradient I'm gonna call
83:42 - this let gradient equal outputs okay so
83:50 - how am I gonna do this how you gotta do
83:51 - them it's like oh I don't have numb by
83:53 - this is where you really need dump I
83:55 - which is a Python library for doing
83:56 - matrix calculations so let me think
83:58 - about this I need to I need to take
84:02 - those outputs which have already been
84:03 - passed through sigmoid and multiply it
84:05 - times what I need to do but this won't
84:07 - work right I need to calculate this
84:09 - gradient as those outputs the derivative
84:11 - outputs times 1 minus outputs so I have
84:13 - to do this with my matrix library so one
84:15 - thing I just realizing here is I don't
84:17 - actually have element wise
84:18 - multiplication the multiply function
84:22 - multiplies two matrices with matrix
84:25 - multiplication the multiply function oh
84:29 - I do I do no no I do I do it the
84:33 - multiply function so the static multiply
84:36 - whew only multiplies is this good though
84:41 - the non-static can take a scalar or
84:44 - another matrix and this by the way this
84:46 - right here this is referred to as the
84:49 - hot
84:50 - think product which is element-wise oh
84:52 - boy
84:53 - so I do actually oddly have this here is
84:56 - this going to be useful so let's see
84:59 - where am I again I need I want to go
85:02 - back to train so what I need to do but
85:04 - first I need to do is map the outputs
85:08 - yeah I'm gonna need I'm gonna need a
85:11 - static version of that I don't I don't
85:13 - like what I've done here so whatever I
85:16 - do is I'm gonna go to my matrix library
85:21 - and I'm gonna have this function only be
85:23 - for the scalar product then I'm going to
85:29 - make a static function called Hadamard
85:33 - which is for element wise and that can
85:36 - be essentially this right here so I am
85:42 - going to I should have had this before I
85:45 - got into this video but I don't so I'm
85:46 - going to just do it in this video and
85:49 - I'm going to say a dot rose a dot
85:52 - columns B so a and I need to say I need
85:59 - to make a result is a new matrix a dot a
86:07 - dot rose B columns and the result each
86:15 - element of the result in this row column
86:18 - position is the the element wise
86:22 - multiplication and I should probably add
86:24 - a check here to make sure the number of
86:26 - rows and columns were the same but that
86:27 - can happen another time I'm going to say
86:29 - return results now remember I have this
86:32 - map function ah oh I'm so silly there's
86:39 - an easy way for me to do this Oh leave
86:42 - that there though I mean I sell my own
86:43 - there's an easy way for me to do this
86:45 - which is that right remember I have this
86:49 - like sigmoid function where did I put
86:51 - the sigmoid function is it so just in
86:53 - here sigmoid I am going to write a
86:59 - function called D sigmoid now here's the
87:00 - thing this isn't really the derivative
87:03 - of
87:03 - sigmoid because that would involve
87:05 - calculating sigmoid but this is I've
87:08 - already done sigmoid maybe I know what
87:10 - to call this exactly but I'll put Y in
87:11 - here to be more clear so I'm going to
87:14 - say Y times 1 minus y so what's a good
87:18 - work I don't know what a good function
87:20 - call this is but I'm just gonna call it
87:21 - a name because it's been called D Sigma
87:23 - because Sigma oi this output array is
87:27 - already this is what I need to do I
87:30 - could just say yeah I don't need to save
87:32 - it for any reason
87:33 - I can just say outputs map D sigmoid
87:37 - this what this is doing is it's saying
87:39 - out this is outputs equals outputs times
87:43 - 1 minus outputs all right so that is
87:50 - this piece right here
87:52 - now I need to element wise multiply it
87:54 - by the errors that's why I should have
87:57 - left it the way it was I regret all the
88:04 - changes I recently made weight cuz I
88:09 - don't need to save it once I'm done with
88:11 - that it doesn't get used anywhere else
88:16 - right am i right about that I think I'm
88:19 - right about that so let me go back let
88:26 - me undo a bunch of these things I'm
88:34 - gonna go back and now
88:41 - okay
88:46 - does anybody have a better idea of what
88:48 - would sort of more appropriately oh my
88:50 - goodness
88:51 - call this function here there is there
89:00 - are a lot of numpy like libraries or
89:02 - JavaScript I'm just trying not to use
89:03 - one I'm gonna replace what I'm doing
89:05 - eventually with one so let me get to the
89:10 - point where Here I am I'm gonna go back
89:18 - to where I said
89:28 - I'm in train I'm gonna go back to where
89:33 - I was here sigmoid prime maybe Optimus a
89:45 - boy okay
89:50 - I'm gonna go back to where I was here
89:52 - okay so I need to somehow get this
89:57 - gradient which is this piece right here
90:01 - the nice thing is I can actually use
90:03 - functionality that I have built into the
90:05 - matrix library so for example I have
90:07 - that map function so I can take every
90:11 - element of output and set it equal to
90:13 - output it that element times one minus
90:15 - itself so what I need is another
90:17 - function right much like I have sigmoid
90:20 - what I need is the derivative of sigmoid
90:23 - now this there's a little bit something
90:24 - strange that's gonna go on here let me
90:25 - just write this if I write a function
90:27 - called d sigmoid what I really mean is
90:29 - return sigmoid of X times 1 minus Sigma
90:38 - of X this technically speaking is the
90:43 - derivative of sigmoid but that's
90:46 - actually not what I want to do here
90:48 - because if you know if you're following
90:50 - along and where am I here in train I've
90:53 - already mapped the output through
90:56 - sigmoid so actually what I want is and I
91:00 - kind of like okay what's one called like
91:02 - fake dig see sigmoid but I'm just gonna
91:04 - put Y in here I'm gonna comment this out
91:06 - and kind of as if Y is I'm just changing
91:09 - the very ammonium Y has already been
91:10 - sigmoid and I'm gonna say return Y times
91:14 - 1 minus y so what I can do now to
91:18 - calculate this gradients is I can
91:21 - basically say outputs I kind of want to
91:25 - call it gradient but right outputs dot
91:29 - map and maybe I should make a copy of it
91:33 - or something
91:33 - d sigmoid all right so now I've taken
91:37 - outputs and I've set each element equal
91:42 - this
91:42 - now I need to element-wise multiply that
91:44 - by the error so I need to say outputs
91:49 - now here's the thing in my matrix
91:53 - library this is like I have this right
91:58 - here the multiply function currently if
92:01 - it gets a matrix it does what's what I
92:03 - was for the element wise multiplication
92:04 - which I'm referring to as the Hadamard
92:06 - product so otherwise so I guess what I
92:12 - mean I'm gonna keep this and I'm gonna
92:18 - say outputs dot multiply now remember
92:22 - when I use matrix top multiply that's
92:24 - the that's the multiply that's matrix
92:27 - multiplication
92:28 - this is Hadamard I should write a
92:30 - separate function called Hadamard yeah
92:39 - let's take this out and make this
92:41 - Hadamard alright let's just leave it
92:44 - I'll refactor that later what do i do
92:46 - what do I do
92:55 - I'll just leave it I'll just leave it
92:57 - it's fine
93:01 - multiply by output errors there we go so
93:10 - now I've done this piece and this piece
93:14 - now I need to multiply it by the
93:15 - learning rate do I even have I all I've
93:18 - been waiting my whole life just to get
93:20 - to the point where I could put the
93:20 - learning rate in the code because I feel
93:22 - like once you have the learning rate in
93:23 - the code I've kind of done so let's make
93:25 - that a variable this dot learning rate I
93:29 - don't know I'm just gonna set equal like
93:31 - point one right now and so now I also
93:34 - need to say outputs dot x learning rate
93:41 - and now what I need to do I have all of
93:46 - these these
93:52 - and now what I need to do I've done this
93:56 - whole piece here all I need to do is
93:59 - take what came out of hit and transpose
94:02 - it and do the matrix the matrix prana
94:05 - matrix multiplication between that
94:07 - matrix and this matrix and then I have
94:10 - all the delta weights and I can just
94:12 - adjust them you know I've got to talk
94:14 - about stochastic etc but let's just let
94:17 - me let me just we try to get through
94:19 - what I'm doing here
94:20 - all right I now I'm going to say what am
94:26 - I looking for I'm looking for this
94:28 - particular array right this particular
94:32 - vector I need to get let hidden T which
94:35 - is hidden transpose is matrix transpose
94:39 - hidden and then let deltas wait wait
94:44 - what am i calling these like the this
94:48 - dot weights I H so I'm going to say
94:50 - weights H Oh deltas equals matrix dot
94:57 - multiplied so this is calculus I'm going
95:01 - to put a comment in here calculate
95:03 - gradient and then calculate deltas
95:08 - calculate deltas matrix top multiply
95:13 - what do I want to multiply I got to do
95:16 - this in the right order I want the
95:19 - column vector which is the the gradients
95:23 - thing that I've been doing and the row
95:24 - vector which is the output so hit okay
95:27 - so I'm going to say multiply outputs so
95:31 - I hate that I've done this does the map
95:34 - function what I want to have is a you
95:36 - know what I want is I want a static
95:38 - version of the map function that will
95:39 - pull out make it new so let me do that
95:41 - really quickly so where's the map
95:43 - function map I made a static
95:54 - I made a static version already how
95:56 - exciting Oh life is good sometimes well
96:00 - that's so lucky so I want to do this let
96:02 - gradients equal the matrix dot map this
96:08 - solves all my problems outputs D sigmoid
96:14 - no I don't need alright so I want to map
96:17 - all the outputs with Sigma the
96:19 - derivative of sigmoid then multiplied by
96:22 - output errors multiplied by learning
96:24 - rate transpose the hidden output and
96:26 - then matrix multiply the gradients by
96:29 - the hidden output transposed and now
96:36 - wait H of this this dot wait H Oh add
96:45 - wait H Oh deltas so this is me just like
96:50 - taking going into just going out to
96:52 - saying I calculated those deltas change
96:54 - all the weights by those deltas so I
96:56 - don't know if this is this right let's
96:58 - think about this more later
97:00 - I need the bias well that's fine I'm
97:02 - gonna do the vice all right so that's
97:03 - good now I have to deal with the hidden
97:06 - layer this should be much easier now
97:08 - that I've done this once okay I need to
97:12 - calculate the hidden gradient which is
97:18 - matrix dot map the hiddens what came out
97:21 - of hidden pass through d d sigmoid then
97:29 - i need to take the hidden gradient and
97:33 - what did i do up here i multiplied by
97:36 - the output errors but have I calculated
97:38 - the hidden errors I have calculated the
97:40 - hidden errors somewhere I did back
97:42 - propagation hidden errors right there ah
97:45 - how lucky lucky me hidden gradient x
97:48 - hidden errors then hidden gradients x
97:55 - learning rate and that's this dot
97:58 - learning rate and did I forget that up
98:02 - here yeah this this dot learning rate
98:04 - right and
98:08 - then so this is calculate hidden
98:14 - gradient now oh my god
98:18 - calculate hidden Delta's alright you
98:24 - know that it's input to hidden input to
98:27 - hidden I'll just input to hidden deltas
98:32 - okay so that is just like I did up here
98:35 - the first thing I need to do is
98:37 - transpose input inputs T equals inputs
98:46 - dot transpose
98:47 - I know matrix dot transpose inputs
98:49 - matrix dot transpose inputs okay and
98:55 - then what did I call those deltas I
99:00 - called these weight hidden output deltas
99:03 - so let wait input hidden Delta 's equal
99:08 - matrix multiply the inputs
99:12 - no no nothing inputs the gradients times
99:18 - the transpose inputs so you hidden
99:23 - gradient times the what I call that
99:29 - inputs t transposed inputs and then I
99:39 - can just adjust those my goodness I have
99:46 - just less PII I'm speechless I without
99:48 - speech okay I think I might be done with
99:53 - this for the bias I'm gonna save that
99:57 - for I'm gonna add the bias in a separate
99:58 - video because I feel like I just need a
99:59 - break so let's think about this I might
100:03 - have made some mistakes but I think that
100:06 - I've gotten through this I think what
100:08 - I've done is I have figured out a way to
100:11 - use the train function to calculate the
100:14 - errors use back propagation to chop up
100:18 - and divide the error in a sine blame all
100:20 - over the place
100:22 - right I know the output errors I need to
100:24 - figure out the hidden layers errors then
100:26 - that's what I've got here those were the
100:28 - first two videos in this the third year
100:30 - I kind of talked through these formulas
100:31 - and now in this video I have used a math
100:33 - function to calculate the gradient the
100:35 - errors times the derivative of the
100:38 - output I adding the learning rate in I'm
100:40 - multiplying it by what's coming in
100:42 - transposed to get the weight deltas I've
100:44 - done that for both this layer this
100:47 - matrix and this matrix again at some
100:50 - point I really need to extract in you
100:53 - with this library to make it something
100:54 - useful I need to be able to have
100:55 - multiple hidden layers and this back
100:57 - propagation would happen in a loop but
100:59 - I'm doing this to separate distinct
101:01 - chunks just to understand them and then
101:03 - I'm gonna adjust all the weights so
101:05 - there's things that I haven't talked
101:07 - about yet number one is I've got to
101:09 - adjust the bias values and number two is
101:12 - when and where should I be doing this
101:14 - should I run through all of my training
101:16 - data and get like the kind of average
101:18 - error of everything and then adjust all
101:20 - the weights or should I
101:21 - each time adjust the weights for each
101:23 - record or should i do batches like send
101:26 - in these ten data points adjust send in
101:29 - these ten adjust and that has to do with
101:32 - stochastic gradient descent versus a
101:34 - batch gradient descent so I'm gonna get
101:38 - there let me take a break take a few
101:40 - deep breaths and make a separate video
101:42 - where I adjust the bias ease I might
101:44 - have made some something absolutely
101:45 - completely wrong so you you have to
101:48 - watch all the way through to find out if
101:50 - I have other things that I have to
101:51 - correct later which I may but I'm gonna
101:54 - change the biases and then I'm finally
101:56 - going to do I'm just gonna try to train
101:58 - it on the X or to see if I've done this
102:01 - correctly XOR would be a really simple
102:02 - problem so it's a good test problem for
102:04 - me to see if my code is mostly working
102:06 - correctly oh stop I have made but before
102:16 - I go there is one error that I can point
102:18 - out here this is input to hidden thank
102:20 - you okay see you in the next video what
102:26 - time is it 5:40 huh alright so the bias
102:31 - so here's the thing
102:34 - the
102:35 - the the make your neural network book
102:37 - doesn't include the bias and the other
102:44 - the ml for a tutorial that I read today
102:53 - okay I'll get this you know what I'm
102:55 - gonna leave that and I'll put it in the
102:56 - next video you don't have to add that
102:59 - into the end of this video the other the
103:07 - other I'm sorry I didn't change the
103:09 - camera the ml for a tutorial read just
103:13 - sort of says like oh it's easier to do
103:15 - this if you just make the bias as
103:16 - another weight and have it always be one
103:19 - which I've talked about in previous
103:21 - videos but wouldn't I if if I'm using
103:25 - the same logic where I'm extrapolating
103:27 - from here to these two if I extrapolate
103:31 - from here to these two don't I just is
103:34 - it just this without these two things is
103:37 - that all I need to adjust the bias
103:40 - somebody tell me like can't because
103:42 - certainly I don't need a matric the bias
103:44 - is a single column vector so can't I
103:47 - just take exactly this without without
103:50 - the matrix product of the transposed
103:52 - inputs and that the deltas for the bias
103:58 - somebody tell me if that's right that's
104:01 - my intuition
104:09 - I didn't run the code didn't run the
104:15 - code first I'll do that the beginning of
104:17 - the next video
104:18 - k week Minh says yes and I'm just going
104:21 - to assume that you're saying yes to the
104:23 - question I asked yes there
104:25 - 5:43 p.m. one minute ago must be yet
104:28 - okay great so that's gonna make my life
104:29 - easy because I can do that in fact I
104:34 - already have that in here right in
104:37 - theory that's this right I don't even
104:41 - need to I can just adjust the bias ease
104:43 - by the gradient by the great because
104:45 - it's multiplied by the like if this
104:47 - whole thing is the if this whole thing
104:51 - is the gradient that's the deltas for
104:54 - the biases right can you please type
104:57 - another yes all right so the things I
105:01 - need to fix are I did it actually I have
105:05 - to fix this and then I have to run the
105:08 - code to see what other syntax errors I
105:10 - have so all I need is one more yes I
105:14 - could move on
105:32 - now I'm waiting for Kate week Mons yes
105:35 - okay wheat wine is not typing usually I
105:38 - see like a quick Mon is typing by the
105:43 - way sometimes I'm speaking in I watch
105:45 - myself a lot at 2x I don't watch was up
105:47 - a lot 2x but if I have to go back and
105:48 - watch something I'll watch it at 2x so I
105:51 - always feel like I'm like my actual
105:53 - native personalities to talk like this
105:54 - was it
106:07 - he said yes to my previous question but
106:12 - I guess I'm gonna I'm gonna just go with
106:15 - it I think confidence thank you
106:16 - all right that is correct move on
106:21 - alright okay I'm here I am back again oh
106:28 - I'm so close to the end of this now I'm
106:31 - sure there's a lot of mistakes I mean so
106:32 - I made a really an error here which is I
106:35 - didn't even try to run my code and there
106:37 - was a big there's a big typo here
106:38 - already which is that this should be the
106:41 - is this and this might actually even be
106:43 - weights like I think I might not be like
106:45 - being careful about my right these these
106:48 - that's weights plural so that's totally
106:50 - wrong weights plural and this is not
106:53 - this is input to hidden and this is also
106:56 - input to hidden Delta's so that I should
107:00 - have said and I wonder if I made that
107:02 - mistake here like this stock weights
107:04 - that should be weights I got this right
107:06 - so let's try let's try running the code
107:08 - you know I have no idea put your good
107:11 - put odds on whether there's any errors I
107:13 - would put very things I give myself like
107:15 - 50 to one that there's no errors so
107:18 - let's go and just because if you recall
107:21 - I have in the sketch like I'm setting
107:24 - myself up for a very simple scenario of
107:27 - two inputs two inputs two hidden nodes
107:32 - and two outputs lying 99 typos so I
107:36 - should guess I should check there might
107:37 - even be a typo in line 99 well that's a
107:40 - comment so I don't know what that is
107:41 - alright alright let's just let's just
107:43 - hit refresh here
107:45 - identify our inputs has already been
107:47 - declared neural network digest line 59
107:50 - so let's see what that is oh I forgot so
107:56 - I don't oh yeah oh this interesting so
108:02 - this should probably be called input
108:04 - array like I did with feed-forward and
108:07 - in that sense this should probably be
108:09 - target array because and then targets
108:14 - equals matrix from array target array so
108:17 - let's just do that so that's important
108:18 - because I'm letting the end-user pass in
108:21 - the inputs and targets as simple arrays
108:23 - and internally in the library I'm
108:24 - converting those two matrix objects
108:28 - targets is not defined a neural network
108:30 - line 71 oh and this so this has to be
108:32 - let targets because it's a new variable
108:34 - here there's no errors okay weird all
108:40 - right so let's move on now this is
108:42 - really tricky I probably could have been
108:44 - much more thoughtful about how I'm doing
108:45 - this I'm but here's the thing I now need
108:49 - to add the Delta BOTS the deltas for the
108:51 - biases now if you've been following
108:52 - along here's how I've here's how I've
108:55 - connected all the stuff I made a video a
108:56 - long while back Oh yesteryears of days
108:59 - when it was just a one-dimensional y
109:02 - equals MX plus B I made a video about
109:04 - gradient descent where Delta M is the
109:06 - learning rate times error times X and
109:08 - Delta B is just the learning rate times
109:09 - error well this is the analogous
109:11 - situation with matrices learning rate
109:13 - times error this gradient so to speak is
109:15 - this whole thing over here and the X is
109:19 - this thing over here and the same thing
109:20 - here this is the Delta the Delta I mean
109:24 - sorry the gradient which is this times
109:26 - the inputs which is X so the the deltas
109:30 - for the biases is actually not a matrix
109:31 - right the bias is a one column vector
109:33 - basically it won't make one column
109:35 - matrix which is a vector and so the
109:38 - Delta bias sees is actually just this
109:40 - part this part and this part and guess
109:45 - what I have already calculated those
109:47 - things so if you look at that here oops
109:50 - if I go back to my code right this is
109:52 - before where where is it
109:56 - right here where am i odd gradients
109:58 - right
109:59 - right here I am passing the outputs
110:02 - through the derivative I am multiplying
110:05 - element-wise with the errors and the
110:08 - learning rate this and then I have to do
110:10 - the transpose and get but this is it
110:11 - these gradients I could just say bias
110:14 - ease this dot bias and where am I output
110:19 - dot add gradients so let me actually
110:24 - will let me actually put this where did
110:28 - I go let me put this here so this is
110:32 - adjust the weights by deltas and now
110:40 - adjust the bias by its deltas which is
110:45 - just the gradients okay and then I
110:50 - should be able to do exactly the same
110:52 - thing with down here oops this will come
110:59 - after and what I want to do is adjust
111:01 - the hidden bias with the hidden gradient
111:06 - now I one thing that's probably
111:09 - inconsistent maybe I'm just gonna kind
111:11 - of leave it but if anybody as you this
111:14 - is a I'm hoping to put all this code in
111:16 - a repo which is already there she's it
111:19 - called like toy in neural network and so
111:22 - one thing I probably should be
111:23 - consistent is when I'm saying like
111:24 - weights versus weight or gradient versus
111:26 - gradients so that can be cleaned up
111:28 - later let's just see by goodness if I
111:32 - have any errors okay no errors sorry I
111:38 - check the chat
111:43 - to see if I have any errors no errors
111:46 - all right dare I do something next okay
111:50 - so here's the thing I mentioned before
111:54 - now how you collect and prepare your
111:59 - data is so important in terms of the
112:03 - ethics of what you're doing the
112:05 - scientific accuracy of what you're doing
112:08 - and I'm kind of glossing all over that
112:11 - just to make this toy neural network
112:13 - library but beyond just sort of like
112:15 - being thoughtful about collecting your
112:17 - data we've got to figure out like how do
112:19 - I even like do this and so there are a
112:21 - variety of techniques in terms of
112:24 - calculating the error over time and
112:27 - batches and then adjusting the weights
112:30 - versus but I think what I'm going to do
112:31 - it's called stochastic gradient descent
112:33 - let's Google that but let's just make
112:37 - sure I've got the right term stochastic
112:39 - gradient descent is known as incremental
112:43 - just is a iterative method for
112:45 - minimizing object that's written as a
112:47 - symbol so I think I'm correct in that
112:50 - what I'm doing is in other words what
112:52 - I'm gonna do with stochastic gradient
112:53 - descent which I think is what I did with
112:54 - my perceptron example and my linear
112:56 - regression example is that for every
112:59 - single record every single data point
113:00 - I'm gonna pass it in calculate the error
113:03 - back propagated and adjust one at a time
113:07 - instead of doing batches that you're but
113:09 - but be aware of this idea of batches
113:11 - because that's a core concept as you
113:13 - start to use other people's like real
113:14 - actual robust working deep learning
113:17 - libraries and examples and that sort of
113:18 - thing so I'm going to do this stochastic
113:21 - idea which then one of the reasons why I
113:23 - want to do that I know why switched over
113:25 - that one of the reasons why I want to do
113:27 - this stochastic idea is it's basically
113:29 - already what I've done so this training
113:31 - function simply takes a single set of
113:33 - inputs and a single set of outputs
113:35 - targets does all that it needs to do to
113:38 - it adjusts everything and finishes off
113:44 - so so let's do that let's do that I
113:50 - don't think let's let's try oh my
113:52 - goodness ah so let's let's prepare data
113:55 - set
113:56 - I'm gonna do this okay so I like I'm
113:58 - like terrified to erase this if I have
114:02 - to let me take a moment and erase this
114:07 - do my
114:47 - okay so this is my data set I'm gonna
114:50 - have this truck this is the this is the
114:53 - architecture I'm going to use I'm gonna
114:55 - have two inputs a hidden layer with two
115:02 - neurons an output layer with one and I
115:07 - wiII try that again I don't need to
115:10 - write so high on the board my elbow
115:13 - barely even goes straight okay so here's
115:22 - the architecture I'm going to use I'm
115:24 - gonna have an input input layer I'm just
115:26 - gonna say layer 2 inputs I'm gonna have
115:29 - a hidden layer with 2 - 2 nodes then I'm
115:36 - going to have an output with just one
115:39 - this is a nice architecture for trying
115:42 - to solve the XOR problem exclusive or I
115:45 - just want the simplest thing just to
115:47 - kind into some way of kind of debugging
115:49 - and validating that's something in my
115:50 - code is doing it correctly so what I'm
115:54 - going to do is so this it's going to
115:57 - look like this and so for my data set
116:04 - width is going to be this following 1
116:12 - comma 0 gives me a 1 0 comma 1 gives me
116:18 - a 1 1 comma 1 gives me a 0 and 0 comma 0
116:26 - gives me a 0 so this is the classic non
116:31 - linearly separable problem and I
116:33 - discussed with the perceptron a single
116:35 - perceptron can't do it this is now a
116:37 - multi-layered perceptron with graty's
116:40 - assented back propagation in that code
116:43 - so I should be able to continuously feed
116:45 - it this training data sent now I said
116:47 - you need a training data set and a test
116:50 - data set but this is like such a
116:52 - simplistic problem there's only 2 4
116:54 - possibilities and we interesting to look
116:56 - at
116:56 - you know visualizing it and letting
116:58 - these be continuous floating point
116:59 - values but let's look let me just do it
117:01 - up so I'm gonna put into my code the
117:03 - training data so I'm gonna say I'm gonna
117:09 - say let training data equal and have it
117:16 - be an array and each element of the
117:19 - array is gonna be an object inputs 0 0
117:23 - targets 1 so I'm kind of I could put
117:28 - this in a JSON file or a spreadsheet but
117:30 - I'm gonna just do it like this
117:31 - hard-coded in just to make the point
117:34 - right and so now I have own ok so 0 1 1
117:40 - 0 0 0 is 0 0 0 is 1 1 is 0 ok so oh dear
117:49 - oh this should be a : ah silly syntax
117:52 - error that I then copy/paste everywhere
117:55 - ok so this is now my training data it is
118:01 - an array with objects so now what I need
118:03 - to do is I am going to say 2 2 1 that's
118:11 - my neural network and I'm going to say
118:16 - let's see what do I need to do for data
118:20 - of training data that's a nice little
118:24 - loop through everything that's in here
118:26 - I'm gonna say neural network trained
118:29 - data inputs data targets and I probably
118:38 - I'm just gonna you know I could pick it
118:39 - randomly I'm gonna go through the data
118:40 - and I'm gonna suit it like I'm gonna do
118:45 - it like a hundred times in the same
118:47 - order which is probably a problem I
118:49 - should probably randomize the order
118:50 - let's just do it this way and see what's
118:53 - going on so let's do that and then I'm
118:57 - going to test things by saying guess
119:02 - equals neural network feed-forward zero
119:08 - zero and actually you know I'm just
119:12 - going to do I'm going to say neural
119:13 - network fee for at zero zero print so
119:19 - this should give me everything in the
119:20 - console that I want so this is I haven't
119:25 - been to a mic being careful enough this
119:28 - is the inputs with the target this is
119:29 - the inputs with the target does the
119:30 - inputs of target doesn't miss a target
119:32 - I'm just going to train it with that
119:33 - like 100 times in the exact same order
119:35 - which is probably a terrible idea with
119:37 - stochastic gradient descent and then I'm
119:38 - just going to call v4 and I think I
119:40 - still have in my matrix library this
119:41 - print function which just like console
119:44 - dot tables the stuff out all right I
119:46 - don't know Oh what could possibly go
119:49 - wrong what could possibly go wrong
119:55 - feed-forward print is not a function all
119:58 - right
119:59 - neural network oh you know what it gives
120:04 - me a nice little array I forgot I don't
120:06 - need to do this I could just consult I
120:08 - forgot that the library itself gives me
120:10 - a nice little array so let's just
120:12 - console.log I don't need that print
120:16 - thing so let's try this oh that doesn't
120:27 - look very good right I should be getting
120:30 - one one zero zero like we're close to it
120:33 - let's train it like so let's try
120:36 - training it like 10,000 times
120:39 - hey this is maybe interestingly sort of
120:44 - better one zero zero one one one zero
120:51 - zero so I'm definitely feeding in the
120:52 - all the proper inputs is my training
120:55 - data correct zero one gives 1 1 0 is 1 0
121:01 - 0 gives zero one one good so I think I
121:03 - really need to randomize
121:04 - order right I've really got to randomize
121:06 - the order so let's randomize the order
121:08 - and in fact what I'm gonna do
121:10 - forget about even randomizing the order
121:12 - I'm just gonna always pick a random one
121:13 - so I'm just gonna say let data and P
121:18 - five as a nice function if I just give
121:19 - it random training data it's gonna give
121:22 - me a random one the learning rate is
121:25 - something I actually should really be
121:26 - careful about there's put in point 1 so
121:28 - that's probably something I need to be
121:29 - more thoughtful about so let me so let
121:32 - me do it 50,000 times and let's see what
121:35 - happens in a random order so
121:42 - [Music]
121:46 - interestingly enough so I want to do
121:48 - this as a coding challenge I want to
121:52 - actually write an example that sort of
121:55 - like animates and visualizes it as it's
121:57 - learning I'm saying I have can't believe
122:00 - this work oh I can't believe I arrived
122:04 - here I mean I'm sure there's like
122:04 - problems and it's but at least it worked
122:06 - for this simple problem and but so
122:11 - somebody said I had a typo in the train
122:14 - function well I'm sure I do
122:18 - don't I'm so happy right now
122:21 - though so I need to do coding challenge
122:23 - which is actually the XOR problem and
122:26 - also animated as I'm going so I think I
122:29 - essentially do the same thing but draw
122:31 - sort of like a pixel space and actually
122:34 - iterate over it and I should see it you
122:36 - know the coroners would be the the full
122:38 - boolean that I'll explain this way do
122:40 - the coding challenge I can't even think
122:41 - anymore just got this to work this is
122:46 - not deep learned is I'm about like 5 I
122:49 - think I'm back propagation XOR I am I
122:59 - think it was like 86 was the back
123:01 - propagation paper history history
123:08 - history history history
123:14 - 86 so I am 86 96 2006 I'm like 32 years
123:20 - behind deep learn Dutch yes okay I can
123:24 - catch up a little faster you know a lot
123:28 - of years of research I mean I didn't do
123:29 - the research I implemented what much
123:32 - smarter people came up with for many
123:34 - years in a short period of time here I'm
123:37 - seeing lots of nice emojis in the chats
123:39 - great so it's 6 o'clock let's commit
123:49 - let's push this the github yeah oh and
123:55 - you know what let's get rid of oh oh
123:58 - that's deleted git add code is in state
124:04 - after last tutorial video
124:07 - after last backprop video and I am going
124:15 - to that's interesting it actually thinks
124:20 - that I renamed this file to that even
124:22 - though what I really did was delete it
124:24 - but anyway git push origin watch this
124:27 - watch this
124:28 - no no no no it's not gonna let me do
124:30 - that git push origin master back prop so
124:37 - I I today if you weren't around I did a
124:40 - I set the repo up with unit testing and
124:43 - continuous integration so you cannot
124:45 - push anything to master you can only
124:48 - push to other branches and my unit tests
124:51 - will run I mean there's basically no
124:52 - unit tests in there yet but I will add
124:55 - more no no no this is the old repo that
124:57 - I'm gonna delete now neural network
124:59 - where am I
125:00 - coding train toy yeah toy 20 role
125:07 - Network compare and pull request create
125:14 - pull request
125:17 - let's it with the base branch how is
125:21 - that even possible
125:22 - update branch
125:31 - it's running my tests okay so we're
125:54 - gonna merge then I'm not sure what
125:58 - happened I'm just to say git pull I'm
125:59 - afraid to do this master oh it's just
126:03 - the readme and the test that's all
126:05 - okay so readme and the test had some
126:07 - changes that was that was all okay yeah
126:15 - so if the get up stuff is confusing to
126:17 - you you can watch my holes this one get
126:19 - up and the new series that will come out
126:21 - next week about unit testing and
126:23 - continuous integration okay okay yes the
126:35 - files from this morning didn't upload so
126:37 - I have to remember to do that okay
126:40 - always the readme okay yeah Cody jumps
126:43 - so I think I think this should be famous
126:47 - last words a nice as much as it's sort
126:50 - of time for me to go at 6 o'clock it's
126:54 - not horrific ly like like it's been and
126:56 - I don't feel I have the energy or time
127:00 - to do a mist as a coding challenge with
127:02 - this library I think that can involve a
127:05 - lot of interesting stuff withdrawing it
127:06 - so I'll get to that next week but I do
127:09 - think I could do the XOR challenge so
127:15 - let me try that I don't I don't think
127:17 - and this would be great but I feel like
127:19 - I'm on like our five or six of streaming
127:23 - today and I think you know generally you
127:27 - know the smart thing to do would be to
127:29 - stop now but if I'm not gonna stop now
127:32 - let me just put the XOR thing together
127:39 - and that also I think before I do em
127:42 - mist it would be worth letting the
127:44 - library go through a bunch of like
127:45 - improvements over the next week maybe
127:48 - people will contribute to it more tests
127:49 - because that would be nice so that's
127:52 - that's kind of what I think so
127:55 - and also so let me refactor the
127:58 - repository a little bit also so so what
128:08 - I want to do let's think about this I
128:11 - want to make a folder called Lib and I
128:15 - want to put the neural network stuff in
128:17 - there then I want to make a folder
128:21 - called X or make up my call it examples
128:28 - and I'll make a folder in there called X
128:31 - or I will upload look this is its own
128:35 - coding challenge but I might as well do
128:36 - it in this repo right now
128:39 - and so sketch and index.html go in there
128:44 - which go in here and then what I'll need
128:49 - to do is make sure couple things one is
128:59 - the example code will need to reference
129:08 - Lib I think this is right so why is it
129:14 - asking me to do this so let's see does
129:18 - that fix it let's whoops
129:23 - so examples X or Lib what did I do wrong
129:30 - dot dot Lib taught Lib no I just have to
129:36 - go up one directory and then live right
129:39 - oh no two directories so I could be more
129:42 - thoughtful about how I do this but let's
129:43 - just do that
129:44 - there we go so this works okay so that's
129:48 - where I'm going to start with
129:50 - and I'm gonna so let me let's uh let's
129:54 - run the tests
129:57 - oh I mean NPM tests so that still is
130:03 - fine yeah all the normalizing of data
130:09 - and all of that I'm gonna have to do at
130:12 - some point okay you know the the I curb
130:19 - joke will never be overplayed I'll be
130:22 - sad when I finally do I turn because
130:24 - then it won't be a joke anymore so let
130:26 - me let me say refactoring directories
130:40 - and I'll merge it all in later okay so
130:49 - now
130:59 - so if you can tolerate this excuse me I
131:05 - am going to well happy livestream for
131:12 - almost six hours today I think I'm gonna
131:21 - start I'm gonna delete all this because
131:25 - the idea here is that this is now my
131:34 - idea for this was this this video could
131:36 - be a video that somebody could watch
131:38 - without having watched any of my other
131:43 - neural network videos they could just
131:46 - come in right here and use the library
131:48 - and understand kind of how it works so
131:51 - I'm gonna react splain some basic stuff
131:53 - and I'm gonna you just use the library
131:56 - and get renamed next time okay okay so
132:02 - so this is what I'm going to attempt to
132:04 - do
132:24 - all right so now let me up some water
132:32 - this is definitely it for today for me I
132:45 - can take next week off I made so much
132:47 - content um yeah so I probably I need to
132:55 - finesse the you know you can get yet yes
132:59 - I need to the randomized selection and
133:01 - the learning rate I but I'm gonna add
133:02 - make a slider for the learning rate I
133:04 - think so okay okay okay
133:21 - so you need this and then I need to go
133:25 - to a neural network well actually let me
133:27 - push this up there factor directories
133:52 - I guess I could make it so it as an
133:54 - admin I could just push it directly now
133:57 - I see why it might be useful but that's
134:00 - right I'm gonna live by my testing
134:01 - mantra now well I'm so excited to go
134:09 - home for this weekend just best okay um
134:12 - some initializations are slower to
134:14 - converge yeah that's what great one
134:20 - emerges
134:32 - okay all right here we go oh so excited
134:38 - for this and to be done oh this my iPad
134:42 - sound effects are about to the battery's
134:44 - gonna die yes I could plug it in I won't
134:47 - worry about that right now all right
134:52 - hello this this video needs may hold
134:57 - hello this video needs a ding a train
135:00 - whistle it eats everything because I
135:02 - have spent a very long time
135:04 - making a lot of video tutorials about
135:06 - writing from scratch all building from
135:09 - scratch of neural networking JavaScript
135:10 - and there are so many resources that I
135:12 - reference but I will probably this is
135:14 - probably the primary one make your own
135:15 - neural network by Tariq Rasheed anyway
135:18 - this is the first coding challenge that
135:19 - I'm gonna do with all that neural
135:21 - network code so you don't have to have
135:23 - watched any of those videos to watch
135:25 - this one I'm going to explain the parts
135:27 - that you need for this video and I'm
135:28 - just gonna make use of this toy neural
135:31 - network JavaScript library so this I've
135:33 - started this toy network job Duke
135:35 - Library people are contributing to it if
135:36 - you want to find out how this library
135:38 - was built up to watch like 10 to 12
135:40 - short videos but you could also just be
135:42 - here right now
135:43 - so let's talk about I do and to be
135:51 - honest I am mostly making this video to
135:53 - test that library because I just
135:55 - finished the library and I just want to
135:57 - know that it works like I think it
135:59 - should work and so what I need is a
136:01 - simple problem with a very known result
136:04 - that I can apply to the neural network
136:07 - see if I get the correct results and
136:08 - feel like ok the library works I can
136:10 - start to maybe try to apply it to more
136:12 - interesting complex problems and this
136:14 - simple problem that I'm going to apply
136:16 - it to is a problem called XOR so I don't
136:20 - this is really just a pure sort of like
136:22 - mathematical exploration basically to
136:24 - see that the problem works but it will
136:26 - hopefully get at some pieces of things
136:28 - you need so what I need is for the
136:30 - neural network I need a data set the
136:33 - idea of the neural network is that I'm
136:35 - going to have some inputs the inputs are
136:38 - going to go into the neural network and
136:40 - then eventually some output is going to
136:43 - come out so a neural
136:44 - network can be used for a kind of
136:46 - classification problem for example here
136:49 - is an image of the cat please tell me
136:51 - that is a cat here is an image of a dog
136:53 - please tell me it's a dog and I will get
136:55 - to hopefully some videos about image
136:57 - classification and other applications of
136:59 - machine learning and with neural
137:01 - networks in the future but here my input
137:04 - is going to be something incredibly
137:04 - simple and it's going to be two inputs
137:08 - that are either true or false 0 or 1
137:11 - because I want to solve the XOR problem
137:13 - so what reason why this problem is
137:15 - interesting so we used to be sort of
137:17 - yeah it is to me is that if I look at a
137:20 - truth table meaning I say true true here
137:25 - and false no it's been a very very long
137:33 - day this is like really my brain is
137:35 - completely melted if I look at a truth
137:40 - table here and I say true false true
137:44 - false the end problem for example I only
137:49 - get it true if when if in that this is a
137:52 - grid right would and I only get it true
137:58 - here right both things have to be true
138:00 - to get it true your true and false I
138:02 - still get a false false and false I
138:05 - still get a false false and true I still
138:07 - get a false or I'll get true if at least
138:10 - one is true so I'll get true true true
138:11 - and false here with both of those I can
138:13 - draw a line to separate let's just I was
138:16 - trying to not have to erase anything but
138:18 - I might as well so this is or right this
138:20 - is there was this is what or looks like
138:22 - and I can draw a line right here and I
138:23 - can separate all the truths from the
138:25 - false the reason why the X or problem is
138:27 - kind of interesting is that and I talked
138:31 - about this a bit in some of the other
138:33 - previous videos that you can go look at
138:35 - is that with X for exclusive or it is
138:41 - only true do we only get the true
138:42 - results if I have true false or
138:47 - false true to false is I'm gonna get a
138:51 - false and if both are true I'm also
138:53 - gonna get a false now try to draw one
138:56 - line to divide the truths from
138:58 - falsus you cannot I could draw like a
139:00 - curvy thing or I could let kind of draw
139:02 - two lines right here's all the truths
139:05 - this is not a linearly separable problem
139:08 - in the solution space of this problem
139:10 - it's a simple two-dimensional space I
139:13 - cannot draw a line and so this is a kind
139:15 - of problem that you need a more
139:18 - sophisticated system I mean not
139:20 - necessarily you can imagine you really
139:21 - just need to but this is what I'm gonna
139:24 - use the neural network for and you can
139:25 - imagine building a circuit right where
139:26 - you have switches and how could you get
139:28 - the led only to turn on if one switches
139:30 - on and one switches off this is the same
139:31 - problem so you don't have so needed
139:33 - neural network to design that but that's
139:35 - what we're gonna do so the way this is
139:37 - gonna work is the inputs there's gonna
139:38 - be two inputs if I think about that it's
139:40 - like x1 x2 so what are the possible
139:43 - inputs and I'm going to express them as
139:44 - arrays the possible inputs are 1 comma 0
139:50 - 0 comma 1 1 comma 1 and 0 comma 0 can
139:58 - you see all that these are the four
139:59 - possible configurations of true and
140:02 - false true false false true true true
140:04 - false false those are angels those
140:06 - inputs are going to go in something
140:08 - called a hidden layer they're gonna get
140:11 - fed in to the hidden layer and again you
140:15 - can watch many many videos where I go
140:16 - through lots of details about this the
140:22 - inputs are connected this is fully
140:24 - connected every input is connected to
140:26 - every hidden node and each one of these
140:30 - little lines here each one of these
140:31 - connections has a weight which I'll just
140:34 - make as W then I'm going to have an
140:37 - output layer now in this case I only
140:39 - need a single output because what do I
140:41 - want the target output the desired
140:44 - output I want is a 1 here a 1 here a 0
140:53 - here and a 0 here now why did I continue
140:58 - to put brackets around these why are
141:00 - these all arrays so in neural network
141:03 - systems you can have the system that I'm
141:07 - building here you can have inputs that
141:10 - have that are just there are a list of 5
141:12 - things the outputs could be five things
141:13 - four things one things so in this case I
141:15 - have two inputs and one output but even
141:18 - if it's just one the system is gonna
141:20 - work is always gonna spit out the
141:21 - libraries aren't gonna spit out an array
141:22 - so these are both connected so if you
141:27 - are curious about the feed-forward
141:28 - algorithm what's happening here is
141:31 - called feed for the inputs come in a
141:33 - weighted sum gets all multiplied and
141:35 - added together passed into this hidden
141:37 - layer an activation function activates
141:39 - and sends it out into here with a
141:42 - weighted sum and an activation function
141:43 - activates and sends it out and we get
141:46 - some result and what we're going to do
141:49 - is we're gonna call a function in the
141:51 - neural network library called train
141:53 - we're going to tell the neural network
141:55 - hey
141:55 - this labelled a this is labeled data
141:58 - this data corresponds with this correct
142:00 - output this input data corresponds with
142:03 - this correct output and the neural
142:04 - network internally when I give it the
142:06 - Train method is going to use a process
142:08 - called back propagation and it's going
142:11 - to look at the error it's going to say
142:13 - well if it got point two and the desired
142:17 - output was one that the error is 0.8 if
142:19 - it got point six and the desired output
142:23 - is zero it got an error of negative
142:25 - point six and so it's going to take that
142:27 - error and move it and train the system
142:30 - by twisting all these dials here and the
142:36 - problem with me doing this coding
142:37 - challenge I just spent like six hours
142:39 - building the library basically any way
142:42 - more complex than that but so my brains
142:45 - a little bit fried but hopefully you're
142:46 - saying this is the the setup that I'm
142:48 - going to use so I want to write the code
142:50 - that does this and visualizes and
142:52 - animates the system while I'm doing it
142:54 - okay here I am back alright so here's
143:03 - what I want to do I am now an in a p5
143:05 - sketch you can find the code both with
143:09 - the neural network coding the neural
143:11 - network library repository and it'll
143:13 - also be with the regular coding train a
143:15 - repo with as a standalone example and so
143:19 - I'm going to add set up and I'm going to
143:23 - add draw and
143:25 - basically what I want to do is I need to
143:28 - have a neural network I'm skews n n is
143:31 - the variable and when I create a new
143:33 - neural network from this library I have
143:37 - to give it its architecture and I need
143:38 - to give it three things I need to say
143:40 - how many inputs are there how many
143:43 - outputs are there and also how many
143:44 - hidden nodes are there and by the way I
143:46 - could have does that you should try to
143:47 - try after this and try it with like six
143:49 - hidden nodes see if it works better
143:50 - works worse all right so it's two two
143:53 - and one so I'm going to say two two and
143:57 - one
143:57 - so the neural network itself with all
143:59 - the stuff is all happening now in that
144:04 - in the library everything is set up in
144:07 - the library now I need some training
144:08 - data
144:09 - so I'm going to make a variable called
144:12 - training data and I'm gonna make that an
144:14 - array I almost you know it I I kind of
144:16 - want to make it a JSON file
144:17 - now let's I'll just do it here because I
144:20 - wanted to make the point that you could
144:22 - be loading your data from somewhere else
144:23 - but I'm gonna make it an array and it's
144:26 - gonna have four objects inputs 0 0 and
144:33 - usually the word target is for the label
144:36 - this is training data so it has a known
144:38 - a known target so that is going to give
144:41 - me a 0 right and so now I need whoops I
144:45 - need to copy this object 1 2 3 times and
144:51 - I'm going to have did I get this right
144:53 - I think it looks right I'm gonna have 0
144:55 - 0 1 0 will give me a 1 0 1 will give me
144:59 - a 1 and 0 0 and 1 1 will give me a 0 so
145:04 - this is the training data now why do I
145:06 - feel like this syntax is wrong but I
145:08 - think it's right ok it's an array of
145:10 - objects with inputs targets inputs
145:13 - targets inputs targets ok what do I need
145:17 - to do now now I need to in the draw loop
145:21 - what I'm going to do I say background 0
145:24 - let's make a canvas and I make the
145:28 - canvas 400 by 400 now let's just even
145:32 - let's run the code and just see that the
145:34 - canvas is there okay it's there move
145:37 - this over
145:38 - we make this a little bit smaller give
145:40 - me a little bit more room in the console
145:41 - okay okay okay
145:43 - now I'm gonna use the can for something
145:46 - you're saying it now here's the thing
145:47 - what I'm going to do I'm gonna do a
145:49 - technique called stochastic grip I'm
145:51 - gonna train it using a technique called
145:53 - stochastic gradient descent so the great
145:55 - descent algorithm is built into the
145:56 - library I've talked about in other
145:58 - videos but I'm gonna feed it one data
146:01 - point at a time and have a train based
146:03 - on that rather than do it in batches and
146:04 - other videos maybe I'll come back and
146:06 - look at batch gradient descent in that
146:08 - so gradual batch learning so what I'm
146:11 - gonna do is each time through draw I'm
146:13 - gonna say give me a data point a random
146:17 - data point from the training data then I
146:21 - am going to say then I'm gonna say
146:25 - neural network taught train send it the
146:28 - inputs and send it the targets so this
146:32 - is me just saying hey every time through
146:34 - draw pick a data point train pick a data
146:36 - point train pick a data point train take
146:38 - a data point drain pick is a two point
146:40 - train it's just like got lost in my
146:43 - thoughts all right so now here's the
146:44 - thing I can run this now it's running
146:49 - it's training it maybe I could even just
146:50 - like okay look at the neural network
146:52 - there it is it's got it's got all these
146:54 - biases and hidden nodes and inputs nodes
146:56 - and weights and matrices all that stuff
146:58 - is in the library and I could say
147:01 - feed-forward which i think is the outlet
147:04 - of the function for just sending it some
147:07 - input data and getting the output like I
147:09 - could ask right now give me the results
147:12 - for zero zero it's not very good that's
147:15 - not right I did probably hasn't trained
147:18 - enough yet give me as well for one zero
147:21 - yeah so it probably needs some more time
147:23 - to train these things could be really
147:25 - really slow one thing that I might do
147:27 - just right now is well maybe I should do
147:30 - this like a hundred times
147:35 - per per cycle to draw this is still
147:37 - adjusting the weights with every data
147:40 - point but at least I'm doing it a
147:41 - hundred times faster now because draw is
147:42 - kind of slow let's see what so wouldn't
147:47 - it be nice oh look it's getting better
147:49 - it's going down to zero right we can see
147:51 - ah there we go
147:52 - let's try let's try neural network let's
147:56 - look at 1 comma 0 I'm getting a number
147:58 - so it's working it but it wouldn't it be
148:00 - nice if I could kind of watch the
148:01 - process of it learning so here's a way I
148:03 - could do that now first of all a
148:05 - separate coding challenge might be
148:07 - interesting to actually visualize a
148:08 - diagram of the neural network somebody
148:10 - in the chat mentioned tensorflow
148:11 - playground which is a tool that you can
148:14 - use to like try out different neural
148:15 - network ideas and visualize them but
148:17 - what I want to do here is I actually
148:19 - just want to visualize this truth table
148:20 - so if I think of the canvas as the truth
148:24 - table 0 1 0 1 I can actually you know
148:32 - there's no reason why I could I don't
148:34 - have to pass it in only these options in
148:36 - the neural network I could this could be
148:38 - a square 4 0 0 this could represent
148:41 - point 250.50 1.75 0 whoops I need
148:48 - another one 1 0
148:49 - right so I can actually and then I could
148:51 - take that output which is a single
148:53 - number and map it to a grayscale color
148:55 - so what I should see if it mapping if 0
148:58 - is black and one is white
148:59 - I should see right a big band here in
149:03 - the middle of a dark color with it
149:05 - fading out of a white color fading out
149:08 - to black around the edges where it gets
149:09 - to either 0 0 or 1 1 so let's see if I
149:12 - can make that happen so while it's
149:14 - training in addition to training it I'm
149:16 - going to say let me give me a like a
149:19 - variable called like resolution I'll
149:21 - just say it equals 10 columns equals
149:23 - with with divided by resolution and rows
149:33 - equals height divided by resolution
149:36 - let's say let's loop through all the
149:39 - columns way just by the way in the
149:43 - library used I for rows with the matrix
149:45 - tough but kind of I loop through all the
149:48 - rows and let's just first just to see
149:54 - what I what I'm that I'm drawing
149:55 - something that makes sense I want an x
149:57 - value which is I I times resolution I
150:03 - want a Y value which is J
150:06 - time's resolution and I want to draw a
150:09 - rectangle at XY with a size of
150:12 - resolution and I'm just gonna say random
150:16 - cut fill just right now random 255 and I
150:20 - probably if I'm being careful here I
150:22 - should probably use floor to make sure
150:24 - the number of columns and rows is always
150:26 - an integer cuz lots of weird things
150:28 - could happen okay
150:29 - so let's do this but missing parentheses
150:32 - sketch line 33 I'm staring at this you
150:37 - do not see a missing parentheses sketch
150:42 - line 33 oh my goodness it's been such a
150:44 - long day huh I really don't see a
150:48 - missing parentheses line 31 resolutions
150:52 - that's interesting that's giving me this
150:53 - weird error which is nothing to do with
150:55 - what the actual air is which is that
150:56 - should be resolution but I still have an
151:01 - error height resolution I I less than
151:06 - columns I plus plus what is going on
151:12 - letlet oh my goodness for 400 oh it's
151:16 - been such a long day
151:17 - I should never do these coding
151:19 - challenges it that after streaming for 6
151:21 - hours
151:21 - okay fourth there we see okay so this is
151:25 - the space are going crazy YouTube
151:27 - compression algorithm probably going
151:28 - crazy so you can see all those random
151:30 - colors now what I want to do is I want
151:32 - to turn this into data so I want to say
151:35 - let input 1 equal map I mean this is
151:40 - sort of silly but I'm gonna do it anyway
151:42 - I actually I'm just gonna say I divided
151:45 - by columns let input 2 equal J divided
151:51 - by columns and now I'm going to say
151:55 - neural network feed-forward input 1
152:00 - input 2 I might want to say columns
152:05 - minus 1 here because I want the last one
152:08 - I is only going I want the last one to
152:11 - be equal to 1.0 know if this and this
152:14 - should be rows
152:19 - okay so I want to do that then I want to
152:21 - say output equals this then I want to
152:25 - say bright brightness I'm going to
152:27 - actually just say I'm gonna say let
152:32 - color equal output index zero it's an
152:37 - array but just one number in it times
152:39 - 255 and then I'm gonna fill here and
152:43 - here we go let's take a look at this
152:45 - let's watch the neural network hmm this
152:53 - doesn't seem right Oh give it some time
152:55 - give it some time whoa flickering
152:57 - flickering fleckeri oh you know what
152:59 - though I don't like these I don't like
153:00 - these I don't like these lines in here
153:03 - something weird is happening
153:04 - let's give it let's make it a higher
153:06 - resolution 40 and let's say no stroke
153:09 - come back to it let's refresh type oh
153:14 - I've got to have a type of somewhere
153:15 - well it's like the wrong something's
153:18 - wrong here
153:23 - [Music]
153:25 - four four four oh that's the type of
153:27 - from before I think it's actually
153:28 - working right let's think about it yeah
153:31 - yeah I should be getting true only if I
153:34 - I'm down here or at the top look that
153:35 - worked you can see it sort of thinking
153:40 - about it training it takes a while now
153:42 - what would be interesting is to watch it
153:44 - happen much more slowly let's make it
153:46 - ten times as slow it's really how long
153:51 - is it you think now here's another thing
153:52 - that's interesting a really important
153:54 - piece of neural networks is something
153:58 - called the learning rate now the
153:59 - learning rate at the moment is not
154:02 - really exposed in the library but there
154:05 - is a variable in the neural network
154:06 - library called learning rate and it
154:08 - would be nice to see how the learning
154:11 - the changing the learning rate actually
154:12 - affects the neural network from training
154:15 - training so what I'm going to do is I
154:18 - want to create a slider learning rate
154:21 - slider and I'm going to say learning
154:24 - rate slider equals create slider and I'm
154:27 - gonna give it a range from 0.01
154:32 - 20.1 I don't think higher than that
154:34 - really makes a lot of sense at the
154:36 - moment and I'm gonna let it have a step
154:38 - size of point zero one oh and a default
154:41 - sorry it's going to start at 0.05 and a
154:45 - step size of 0.1 so just if we look at
154:49 - this we can see there's a slider here
154:51 - and what I can do is I can say in draw
154:57 - neural network learning rate this is a
155:00 - variable called LR in the library there
155:02 - should probably be I should probably add
155:03 - something like set learning rate or
155:04 - something but right now I'm just gonna
155:07 - say so when you look at the code after
155:09 - watching this video there might be like
155:10 - a nicer function there but I'm just
155:12 - gonna set it to learning rate slider
155:14 - value so this is what the slider
155:17 - currently is so now in theory we should
155:20 - be able to play with that learning rate
155:22 - something doesn't feel right here point
155:26 - oh six point one yeah that's right
155:31 - and all the way down to the bottom point
155:34 - oh one so playing with this learning
155:36 - rate is an interesting parameter I can
155:38 - make it really high can we rule low that
155:39 - is something that you could really
155:41 - really alter the behavior of neural
155:44 - networks isn't not it is depending on
155:47 - how it is initialized it can really get
155:49 - stuck or it can really like train itself
155:51 - to get to the right answer very very
155:52 - quickly you can see with just one trade
155:55 - data set it's like one data set one data
155:58 - point per frame 30 frames per second you
156:01 - can see you can see that now I could
156:03 - really like maybe make the learning rate
156:04 - really small and it's coming like
156:05 - finesse itself but so these are the
156:07 - kinds of things I don't know maybe you
156:09 - have some more creative ideas let's
156:11 - let's go back to a slightly higher
156:13 - resolution let's look at this let's see
156:16 - if I increase the learning rate to try
156:18 - to get somewhere kind of closer then I
156:20 - can kind of refine with a slower
156:21 - learning rate we can see this is really
156:23 - working so the neural network library as
156:25 - far as I can tell is working this coding
156:28 - challenge is training using the library
156:30 - to visualize a two-dimensional space of
156:33 - a truth table to to get the neural
156:37 - network to give us the result of the XOR
156:39 - problem so I don't know some things you
156:41 - might think about doing with this if you
156:42 - want to do your own variation of this is
156:43 - could you build a much nicer interface
156:45 - could you think of a color in a more
156:47 - interesting way what if you changed the
156:49 - way the neural network is structured is
156:50 - it going to be better or worse could you
156:53 - visualize the actual neural network
156:55 - itself and if you really again this
156:57 - video mostly exists because I wanted to
156:59 - test out the first version of that
157:00 - library I'm planning to do a lot more
157:02 - with that library as well as another
157:04 - library which I'll tell you about in a
157:06 - future video which is a simple
157:07 - JavaScript library for machine learning
157:08 - that's built on top of another library
157:11 - called deep learning is I would include
157:12 - links to all those things in this
157:13 - video's description I look forward to
157:15 - hearing your comments and thank you for
157:16 - watching
157:17 - oh yeah change the orientation whoa I'm
157:24 - in front of the slider something seems
157:26 - fishy wait hold on no it's still right
157:32 - this is still right I don't think so
157:36 - oh yeah no no oh you know what it is
157:40 - okay interesting this is very
157:45 - interesting hold on a second I'm back
157:49 - people are asking
157:51 - sometimes we see white on the edges and
157:54 - this black stripe down the middle
157:55 - sometimes if I can get lucky here I can
157:59 - get it to be try again
158:17 - so I the reason this is happening I
158:20 - think I should probably explain this is
158:21 - that there's two solutions to the
158:24 - problem
158:27 - so hold on so I don't know where I guess
158:31 - I have to come back and explain this oh
158:33 - yeah you know what just give me a second
158:37 - this is totally gonna be worth it
159:10 - all right I'm actually back for a second
159:13 - weird edit point they're probably
159:15 - because you'll reskin how you were
159:18 - getting different results each time well
159:20 - there's actually two solutions to the
159:22 - XOR problem and I brought up another
159:24 - example I will publish this code as well
159:25 - this is a processing version of the same
159:28 - thing that I did and actually you could
159:29 - do this as an exercise which is
159:31 - visualizing it in 3d so the corners are
159:35 - true and false and you can see two
159:36 - corners are optimized are being solved
159:39 - all the way down for true false false
159:41 - true and the other two corners true true
159:43 - false false but if you think about it
159:46 - what are you supposed to do in the
159:48 - middle there's no I mean that those
159:50 - there's no this doesn't actually make
159:53 - sense if I'm sending in 0.5 0.5 but if
159:58 - I'm thinking of that sebast solution
160:00 - space I can actually have the center of
160:04 - it kind of Bend upwards if you look at
160:06 - it in 3d or the center of it could it
160:09 - could be flipped it could be actually
160:10 - the complete flipped version of this
160:12 - with the kind of bending down work
160:13 - downwards so you'll see if I run this
160:15 - multiple times every once in a while I'm
160:17 - gonna get a white stripe along this
160:20 - diagonal versus a black stripe along
160:22 - this diagonal but the corners are always
160:24 - going to be white white black black
160:26 - because the corners should always train
160:28 - for those situations of 0 1 1 0 1 1 0 0
160:32 - correctly so one thing that again just
160:35 - to let's see if I can get let's see if I
160:37 - can make this like really train a lot
160:39 - I'm gonna have to do a thousand cycles
160:40 - per frame and I'll do like a big
160:43 - learning rate and then a low learning
160:45 - rate let's see if I can get oh my
160:47 - sound board went and by the way there
160:50 - also shoot yeah sometimes I get stuck
160:55 - cuz they're like fighting to figure out
160:57 - which is the right solution it should
160:59 - like arbitrarily like there we go
161:06 - alright so hold on
161:09 - so let me try training it like 5,000
161:12 - times per frame which it sounds crazy
161:14 - but it's really very little math that's
161:15 - going on in this tiny neural network and
161:17 - what you're gonna see hopefully is that
161:20 - you know it it's it's converged on this
161:23 - solution really quickly converged on
161:26 - that solution really quickly and
161:27 - sometimes it's gonna get actually a
161:29 - little bit stuck because I'm not really
161:31 - being very thoughtful about this there's
161:33 - probably a nice way and it really just
161:34 - depends on the neural network itself it
161:36 - starts with all of these weights being
161:38 - random values so you might by accident
161:40 - get like a really good set of weights
161:42 - that are already close to one of those
161:43 - solutions
161:44 - that's way it'll converge on that other
161:45 - solution so so I encourage you to and I
161:49 - can just sort of hit refresh a bunch of
161:50 - times and just sometimes it's going to
161:51 - get kind of stuff to think about how I
161:53 - could do some research of how I might
161:55 - get different solutions let's let's see
161:57 - if I can get this to train for the other
161:59 - way bent around the other way I'm going
162:00 - to run this processing code again really
162:03 - quick and so you can think about one
162:05 - exercise you might do is cat what are
162:08 - some ways that you can visualize this a
162:10 - solution space and what you know is
162:14 - there a way that you can get it to not
162:15 - get stuck etc etc okay so I'm gonna let
162:20 - this run I'm gonna go I'm gonna leave
162:21 - this coding challenge is complete
162:23 - thank you for watching I hope you come
162:25 - up with some creative ideas obviously
162:27 - the morano obviously but I you know this
162:30 - is really just a technical demonstration
162:31 - at this point hopefully we can make the
162:33 - neural network library better and I'm
162:35 - gonna you eventually use some other
162:37 - machine learning libraries like
162:38 - something called ml five which is not
162:42 - released yet or sort of is and I think
162:43 - it's called ml five I hydron is that
162:45 - that um but it's a new library that I'm
162:47 - working on with other folks here at ITP
162:49 - that's built on top of a library called
162:51 - deep learning is which is really amazing
162:53 - lots of really power a powerful robust
162:55 - machine learning deep learning library
162:57 - for the browser that runs on top of
162:58 - WebGL so much more to say about that
163:00 - another video thanks for watching oh oh
163:07 - my goodness the learning rate has been
163:09 - doing nothing this whole time let's just
163:13 - let's just pretend that's not the case
163:18 - no wonder I mean I can't there's nothing
163:22 - I can do about that but it doesn't
163:24 - matter because you know I was like in
163:27 - the end it's really like that's
163:29 - interesting to see like if it now I can
163:31 - actually really use the learning rate
163:36 - yeah it's not getting stuck anymore not
163:38 - like like it's doing get stuck I think
163:42 - the answer is can I get it stuck there
163:47 - we go
163:47 - got it stuck if I increase the learning
163:50 - rate or decrease it for a little while
163:53 - no it's really stuck so that's an
163:55 - interesting thing yourself so I don't
163:56 - think that really matters you know
163:59 - whatever I can redo this coding
164:00 - challenge it was no good I've made so
164:03 - many videos today yeah alright uh thank
164:09 - you everybody I'm done for today I will
164:13 - read some random numbers I don't have
164:16 - any sound
164:17 - I'm exhausted it's 6:45
164:20 - I don't even have my phone up here I
164:22 - have some text messages which just came
164:24 - through
164:25 - oh yes if not too late but okay if too
164:31 - late all right so I've got to go I don't
164:37 - even think I have the energy to read
164:38 - random numbers I have to do it though
164:40 - right I hope my phone is downstairs in
164:45 - my office and not lost twenty nine
164:47 - thousand one hundred thirty seven
164:49 - eighteen thousand six hundred eighty
164:50 - eight nice boy really doing this in a
164:52 - perfunctory fashion alright everybody
164:56 - this was a lot of life streaming today I
164:58 - will I'm almost certain I won't be back
165:01 - until next Friday I don't know what time
165:03 - it will be next Friday but there will be
165:04 - another cut and I know I'm hoping next
165:06 - Friday to make improvements to this
165:08 - library and do like M NIST as an example
165:11 - with it okay thank you everyone
165:14 - and I will see you in the future so
165:21 - actually let me just quickly answer
165:22 - question dark go aqua is asking any
165:24 - forum for discussion so there is a slack
165:27 - channel for patrons of the coding train
165:28 - could sign up at page
165:29 - calm / coding train I believe there is a
165:32 - discord that I that I signed up for at
165:34 - one point that's a community discord
165:35 - there's a reddit there's the processing
165:38 - forum which is a great forum for asking
165:39 - questions when I'm processing or p5 and
165:41 - I'm open to people starting up other
165:43 - platforms for the community to engage
165:44 - with each other as well okay okay and I
165:52 - have to stop streaming it's very
165:54 - confusing okay goodbye goodbye I don't
165:59 - know what I'm gonna do next I have to
166:00 - just take a break from all this