00:00 - okay here we are this video.i what I'm
00:04 - going to do in this video is I'm quite
00:06 - terrified at this moment I should say
00:08 - but I'm gonna start to talk about what
00:11 - it means to examine the output we fed in
00:16 - some input we did the feed-forward
00:17 - algorithm we got some output I want to
00:20 - know look at that output and I want to
00:21 - say what do I think about that output
00:25 - really is it good output is a bad output
00:28 - is it right on is a little bit off and
00:29 - I'm going to this is a technique known
00:32 - as supervised learning I have a teacher
00:34 - I'm now gonna say this output is
00:36 - incorrect please adjust all your
00:39 - settings to make this output more
00:42 - correct and I have done this before I
00:43 - have done this in a video about linear
00:47 - regression and gradient descent I have
00:51 - done this in a video where I made a
00:55 - simple perceptron where I did this same
00:57 - type of learning algorithm in fact I've
01:00 - done this in genetic algorithm examples
01:02 - or I'm not using the same exact
01:04 - technique but a different technique to
01:06 - sort of teach a system to do something
01:07 - so this is what I want to do you can
01:09 - think of all of these weights that are
01:11 - inside the network as these little knobs
01:13 - little settings and I just want to like
01:15 - always adjust the settings and so the
01:17 - key there's gonna be key terms that are
01:19 - gonna come up here like error what's the
01:21 - difference between the output and the
01:23 - known correct answer that's the error
01:25 - the cost well over a large training data
01:29 - set what's the sort of cumulative error
01:32 - that's the sort of cost the current cost
01:34 - of the neural network and then this term
01:37 - that's called back propagation so if the
01:41 - error is the thing that tells me how to
01:43 - tune these weights the error is right
01:45 - here it's gonna tell me how to tune
01:47 - these weights how do I tune these
01:50 - weights they're not connected to the
01:51 - error they're connected to the thing
01:52 - that's connected to the error so I need
01:54 - to propagate backwards feed-forward is
01:57 - the process of moving all the data
01:59 - forward through the network back
02:01 - propagation is the process of taking the
02:03 - error and basically like feeding
02:05 - backwards the error through the network
02:08 - now here's what we have so here's where
02:10 - I have got to admit something this is
02:12 - probably I would say
02:13 - to think of a topic that I've tackled in
02:15 - any my videos that's like harder this I
02:18 - can't think of one I don't know that I
02:19 - fully have a deep understanding of this
02:22 - I have implemented it before I spent a
02:23 - lot of time I prepared some notes I
02:26 - prepared some notes for the first time
02:28 - in my life which are right here that I'm
02:30 - going to use while I start to explain
02:31 - this but it's kind of unrealistic and
02:34 - I'm probably not the best person to go
02:36 - into all of the math so what I'm going
02:39 - to attempt to do is give a kind of just
02:41 - general overview of how the algorithm
02:43 - works
02:44 - look at how pieces of it actually work
02:47 - the math of it in particular as it
02:49 - relates to matrix matrices because I'm
02:52 - gonna need that understanding to
02:53 - implement it in code and then I'm gonna
02:56 - present some of the formulas to you that
02:58 - are the formulas for how you change the
03:00 - weights based on the error and then try
03:02 - to implement those formulas in code so
03:05 - this is my plan so probably take two or
03:08 - three videos but so my goal is really
03:09 - the implementation and I'm gonna provide
03:12 - to you a bunch of resources if you want
03:14 - to dive deeper into the math let me just
03:16 - mention those to you so number one make
03:21 - your own neural network by tariq rashid
03:23 - this is a book that i was reading on the
03:25 - subway this morning you can actually get
03:27 - it on the kitten kitten your Kindle app
03:29 - for a very inexpensive amount you can
03:32 - also find it here on an a lot books i
03:35 - Remmick recommend on my coding train
03:37 - amazon.com sheet slash shops mass coding
03:40 - train the three blue one brown video
03:44 - series what is a neural network what is
03:45 - back propagation really doing back
03:47 - propagation calculus you could pause
03:49 - right now and go watch these videos
03:51 - which would give you a much deeper set
03:53 - of knowledge about the math that's that
03:55 - I'm going to use as well as this the
03:59 - essence of calculus right one of the
04:01 - things that's used in the math is the
04:03 - chain rule and the product rule so this
04:05 - is this this particular video might be
04:07 - useful to you as well as this particular
04:10 - online book which I found through the
04:12 - three blue one Brown videos neural
04:14 - networks and deep learning and I'd be
04:16 - remiss if I didn't also mention Suraj's
04:18 - YouTube channel that I've mentioned a
04:20 - lot on this video has a lot of different
04:21 - a lot of a lot of my videos a lot of
04:23 - different videos on similar content
04:25 - especially if you're interested in
04:26 - Python and using the tensorflow library
04:29 - that kind of stuff
04:29 - okay so that's that now let me stir I
04:35 - think I think we're ready to start I've
04:37 - erased the whiteboard and I am now ready
04:39 - to start talking about the
04:41 - backpropagation algorithm so let's
04:45 - assume right now that this is my output
04:49 - neuron and just for the sake of
04:56 - simplicity at this moment let's just
04:58 - pretend this is one of the hidden
05:00 - neurons but let's just pretend that
05:04 - there's just one of them so there is
05:07 - some wait there's also a bias but we'll
05:11 - come and to come back to the bias at the
05:12 - very end so I'm gonna kind of do
05:14 - everything without the bias and then
05:15 - come back to the bias there's some
05:17 - weight which is connecting this hidden
05:19 - neuron to the output neuron now the
05:22 - input of the output of this neuron
05:25 - multiplied by the way it is sent through
05:26 - here pasture the activation function and
05:28 - we get some sort of answer let's say
05:31 - that the answer that we get is point
05:34 - seven so this that can be referred to as
05:37 - the guess the output but I'm gonna call
05:40 - that I'm gonna call it the guess now in
05:43 - the case of supervised learning where I
05:46 - have a prepared data set where I have
05:48 - sort of like known answers so I'm gonna
05:50 - train the network to have these weights
05:52 - so that later on I can put in unknown
05:54 - data to get good results I would have
05:58 - some sort of answer so I'm gonna write
06:00 - that here and I'm gonna say the known
06:02 - answer is one I wanted this neuron with
06:05 - this particular input that came in I
06:08 - wanted to see the answer of one so this
06:12 - means now I also have an error error and
06:18 - the error is calculated with a simple
06:20 - formula what is the desired output the
06:23 - answer - the guess what's the difference
06:26 - between those two things so we can see
06:28 - is that the error is 0.3 so in my simple
06:32 - perceptron and in other videos that I've
06:34 - made I've then taken
06:36 - this error and used it as a way to
06:40 - basically nudge nudge that weight I
06:43 - wanted a one I only got point seven I
06:45 - can make the white a little bit higher
06:47 - to get more stuff right I want more a
06:49 - lot more stuff right I want that weight
06:51 - higher maybe the bias needs to be
06:53 - addressed but four point seven I just
06:54 - need to increase that way to increase
06:56 - the weight in the direction of the error
06:57 - that's how this works now here's the
07:02 - next piece of this let's say however
07:04 - that instead of just one weight coming
07:11 - in here I have two weights because there
07:16 - are two hidden neurons h1 and h2 now we
07:22 - have a problem we have this error which
07:26 - I know I need to nudge wait 1 and wait -
07:28 - but which one's really kind of
07:30 - responsible for the error there's a lot
07:32 - of blame placing going on here normally
07:34 - I would think like let's try not to
07:36 - blame anybody but this is the problem
07:37 - and I could just say like I don't know
07:39 - take half of an error increase them both
07:41 - increase they're both the same amount
07:43 - but there's a key aspect of the way that
07:46 - the learning process works with gradient
07:49 - descent and back propagation is we
07:51 - really need to figure out who's
07:52 - responsible for the error so let me take
07:55 - the scenario these weights could
07:57 - actually be weights where this weight is
07:59 - zero point two and this weight is zero
08:03 - point one well you could be made set we
08:08 - could now make the argument that this
08:09 - connection is more responsible for the
08:13 - error because it has a higher weight in
08:15 - fact it's two-thirds responsible right
08:17 - this weight is double of this weight so
08:20 - in fact when we do these ninjas we take
08:23 - this error to nudge this one but we'll
08:25 - nudge it only by 33% and then we'll take
08:29 - this error and I know this is going to
08:30 - go out of your view but it's coming all
08:32 - the way back here and won't touch that
08:34 - by an up let's see can you see this here
08:37 - 67% actually just do it back here 67%
08:47 - so this is a key aspect of the bout and
08:52 - this is I've basically done this before
08:53 - so this is where we look for this Delta
08:56 - weight we adjust the weight based on
08:58 - that error and the inputs passing
09:01 - through so maybe this makes sense to you
09:03 - and if it does good here's the tricky
09:07 - aspect this is why there's something
09:08 - this is why this video is essentially
09:10 - about back propagation this is not the
09:13 - diagram of the neural network that I
09:15 - created in the previous videos in fact
09:18 - this hidden layer is connected to the
09:21 - inputs and all of those have weights and
09:29 - I I spent a lot of time worrying about
09:32 - the indices let's get those indices
09:35 - correct so this if this is input 1 and
09:39 - this is input 2 we usually call it sorry
09:42 - I usually call those X and this by the
09:46 - way the output we can refer to as Y it's
09:49 - usually what's often referred to as Y so
09:51 - if these are this is input 1 X 1 input 2
09:54 - X 2 this weight is the weight from 1 to
09:58 - 1 this weight is the weight from 1 to 2
10:02 - now that might look backwards to you
10:04 - it's the weight from 1 to 2 or it's
10:09 - connected between hidden 2 and what but
10:11 - the reason why I'm numbering it that way
10:13 - this is that it's going to be in the
10:15 - second row of our matrix this weight is
10:17 - the second row of our matrix and this is
10:20 - a weight from 2 to 1 so I write 1/2 and
10:25 - this is a weight from 2 to 2 so I write
10:27 - 2 2 so here's the thing if this error
10:33 - which is happening right here is used to
10:36 - tune these two weights proportionally
10:39 - based on their weights well how do I
10:42 - connect this error back to these weights
10:44 - how do they get tuned well it what I
10:47 - need just this is a what wasn't good to
10:50 - realize is this is like a little section
10:52 - of what could possibly be a much larger
10:54 - neural network right there could be many
10:56 - more layers this way and many more
10:58 - layers that way so these weights are
11:00 - just tuned based on whatever this
11:02 - neurons error is right here's at the end
11:04 - so I can actually calculate the error
11:06 - directly but here they're not connected
11:08 - directly to this error that sorry here
11:10 - they're not connected they're connected
11:11 - to these so what is the error coming out
11:14 - of here if I just had that if I had like
11:17 - hidden efore error error hidden one what
11:23 - is that equal if I and I knew if I knew
11:27 - what this was right if I knew the error
11:29 - coming out of here error hidden 1 then I
11:32 - could adjust these weights because the
11:34 - air coming out of here adjusts these
11:35 - weights this error just these weights
11:37 - and this error e to error hidden - if I
11:42 - knew what that is then I could adjust
11:43 - the weights coming in to that so this is
11:47 - the idea of back propagation there's an
11:49 - error here it goes to here how many no
11:51 - but if I could calculate these errors
11:53 - that I could continue to go back here
11:54 - and then if there were more I would just
11:56 - calculate these errors and keep going
11:57 - this way so this is the real question
11:58 - how do I calculate that hidden air how
12:02 - do I calculate the error of a neuron
12:05 - anywhere within the network that's not
12:06 - necessarily directly connected to the
12:08 - output where that error is just a simple
12:10 - target - out guess to figure this out
12:14 - I'm gonna pull just the section of this
12:16 - diagram out over here I'm gonna need to
12:18 - progressively over time make this
12:20 - diagram more and more complicated but
12:21 - right now I'm going to simplify for a
12:23 - second again so what I want to do is I
12:25 - just want to take actually it's sort of
12:29 - here already but I just want I just want
12:31 - to pull it over here so this is the
12:32 - output and these are the two hiddens
12:36 - this is the weight of sir this is the
12:39 - weight of 0.2 and this is the weight of
12:42 - 0.1 and the error here is equal to 0.3
12:48 - so what I want to and this is the hidden
12:50 - layer what remember what I'm trying to
12:53 - you know h1 and h2 what I'm trying to
12:56 - calculate is the error of hidden one and
13:03 - the error of hidden
13:06 - okay so the way that I do that is by
13:10 - taking a taking this error in getting a
13:14 - portion of it what portion should I get
13:17 - and I sort of said this already 2/3 and
13:21 - 1/3 so if this is weight one and this is
13:25 - weight - right there's a very simple
13:27 - scenario this is almost like back to
13:29 - that perceptron again then what I want
13:31 - to do is say the error of hidden one is
13:34 - weight 1 divided by weight 1 plus weight
13:37 - 2 times the error of the output e output
13:43 - the air of Hin and 1 is a portion of
13:47 - that outputs error the error of hidden
13:50 - two is weight 2 divided by weight 1 plus
13:55 - weight 2 times that output error again
13:59 - we sort of said this already I realized
14:01 - I'm cut but this is fine it doesn't hurt
14:03 - tutor to do this twice so these would
14:06 - actually be the errors here these are
14:08 - now the errors and I can just use the
14:10 - same gradient descent algorithm to tune
14:13 - these weights but this actually is kind
14:16 - of a rare circumstance where you know
14:19 - everything's just connected to one thing
14:21 - so what I want to do now is I'm gonna
14:25 - make this diagram a little bit more
14:27 - complex and I'm gonna take the case of
14:33 - having more than one output neuron see
14:42 - how this works I'm gonna add another so
14:46 - let's say there's two outputs which
14:48 - means there's error 1 and error - right
14:53 - maybe this is trying to recognize a true
14:55 - we could actually recognize like true or
14:58 - false or something to recognize a 0 or a
15:00 - 1 so in that case maybe the desired
15:04 - answer is you know it's is 1 & 0 but we
15:07 - got we got point 7 and point 3 as the
15:12 - outputs so here this error
15:16 - I mean this - let's make this point for
15:17 - this error would be negative zero point
15:20 - four right and now what I need to do is
15:30 - have more connections here and once
15:35 - again this is and I'm this is for this
15:40 - matrix I've got kind of the same
15:41 - notation 1 1 2 1 1 2 2 2 so if this is
15:48 - our diagram now with two outputs so
15:52 - there are two errors known errors that
15:54 - we can calculate in our supervised
15:55 - learning system we need to figure out
15:57 - still figure out hidden error one hidden
15:59 - air or two well this still stands right
16:02 - this is still a part of the error coming
16:05 - out of here it's the part that it's the
16:07 - part of that it's responsible for it's
16:09 - how much is it responsible for 0.3 and
16:12 - so in that case this should be air
16:14 - output 1 and I'm actually just gonna if
16:18 - it's an error on the output I'm just
16:20 - gonna say e 1 this error coming out of
16:24 - hidden neuron 1 is the same as it was
16:27 - before it's still the portion of the
16:31 - error based on this weight in this way
16:32 - how much of like how much are we
16:35 - contributing to that point 3 air well
16:37 - this this one has a certain percentage
16:39 - and this one has a certain percentage
16:40 - and that is weight 1 1 divided by 1 1
16:44 - plus 1/2 times that error now here's the
16:47 - thing that's just how much it's
16:50 - contributing to air 1 how much is it
16:53 - contributing to air - we've got to
16:54 - calculate that as well and that is the
16:59 - portion of these two and I think you
17:02 - must make sure I have a lot of space so
17:03 - how much of weight to one divided by
17:07 - weight to 1 plus weight to 2 times air -
17:13 - so if they're multiplicity total
17:16 - cumulative error of hitting neuron 1
17:19 - it's that can it's it's it's portion of
17:22 - the connection from it to error 1 sum
17:25 - with all the other connections to air
17:28 - and the portion of its connection to air
17:31 - to some with all of the connections to
17:33 - air to and I'm kind of it's that
17:36 - connected to air - it's connected to the
17:38 - output but the output is producing that
17:39 - error so this is it's some right now I
17:42 - could do the same thing for this error
17:46 - it's it's portion that it contributes to
17:49 - this error which is now wait one two
17:54 - right you can see that these are the
17:56 - same right
17:57 - this is its portion wait it's that's
17:59 - where it's connected this is its portion
18:01 - that's where it's connected right it's
18:03 - it's its percentage portion of the total
18:06 - weights all connected to output one and
18:11 - then how is it connected to output -
18:15 - wait - - its portion of the total of air
18:24 - - of the error for output - so this is
18:27 - now how those errors are computed so
18:31 - again if the mathematics of gradient
18:34 - descent tells us how to take an error to
18:37 - nudge weights then we calculated this
18:42 - error
18:42 - now can we calculate the errors coming
18:44 - out of the output these are our formulas
18:46 - for calculating the errors coming out of
18:48 - the hidden and those errors are the
18:52 - things that could then with gradient
18:53 - descent tell us how to nudge these
18:55 - weights then if we had more layers we
18:58 - could calculate these errors and keep
18:59 - going back that is back propagation that
19:02 - is how the hidden errors are calculated
19:06 - so this is the end of this sort of part
19:08 - one of just for like the basic idea of
19:10 - back propagation I'm going to check to
19:12 - see if there are any questions or
19:14 - Corrections which I will mention at the
19:16 - beginning of the next video where I will
19:19 - also implement at least just this much
19:22 - in the code
19:26 - [Music]