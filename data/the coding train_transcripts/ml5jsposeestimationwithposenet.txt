00:00 - hello and welcome to another beginner's
00:02 - guide to machine learning video tutorial
00:04 - in this video I am going to cover the
00:06 - pre trained model pose net and I'm going
00:09 - to look at what pose net is how to use
00:12 - it with the ml5 chess library with p5.js
00:15 - library and track your body in the
00:18 - browser in real time model as I
00:20 - mentioned that I'm looking at is called
00:22 - pose net with any machine learning model
00:26 - that you use the first question you
00:28 - probably want to ask is what are the
00:29 - inputs and what are the outputs and in
00:34 - this case the pose debt model is
00:36 - expecting an image as input and then as
00:40 - output it is going to give you an array
00:43 - of coordinates in addition to each of
00:50 - these XY coordinates it's going to give
00:52 - you a confidence score for each one and
00:55 - what do all these X Y coordinates
00:57 - correspond to they correspond to the key
01:01 - points on a pose net skeleton now the
01:07 - pose net skeleton isn't necessarily an
01:09 - anatomically correct skeleton it's just
01:11 - an arbitrary set of what is 17 points
01:14 - that you can see right over here from
01:16 - the nose all the way down to the right
01:18 - ankle that it is trying to estimate
01:20 - where those positions are on the human
01:22 - body and give you XY coordinates as well
01:24 - as how confident it is that it's correct
01:26 - about those points one other important
01:29 - question you should ask yourself and do
01:30 - some research about whenever you find
01:32 - yourself using a pre trained model out
01:35 - of the box something that somebody else
01:36 - trained is who trained that model why
01:39 - did they train that model what data was
01:40 - used to train that model and how is that
01:42 - data collected posed that is a bit of an
01:44 - odd case because the model itself the
01:47 - Train model is open source you can use
01:49 - it you can download it there's examples
01:51 - for it in tensorflow in tensorflow Jas
01:53 - and ml5 jazz but the actual code for
01:55 - training the model from what I
01:57 - understand or what I've been able to
01:58 - find is closed source so there aren't a
02:00 - lot of details a data set that's used
02:02 - often in training models around images
02:05 - is Coco are common objects in context
02:07 - and it has a lot of labelled images of
02:09 - people striking closes with their key
02:12 - points
02:13 - marked so I don't know for a fact
02:15 - whether coca was used exclusively for
02:17 - training PostNet whether it was used
02:18 - partially or not at all
02:20 - but your best bet for starting point for
02:22 - finding out as much as you can about the
02:24 - PostNet model is to go directly to the
02:26 - source the github repository for PostNet
02:29 - in fact there's opposed in 2.0 coming
02:30 - out I would also highly suggest you read
02:33 - the blog post real-time human pose
02:35 - estimation in the browser with
02:36 - tensorflow JS by dan Ovid and editing
02:38 - and illustrations from Irene Alvarado
02:40 - and Alexis gallo so there's a lot of
02:42 - excellent background information about
02:44 - how the model was trained and other
02:46 - relevant details if you want to learn
02:48 - more about the koko image data set I
02:50 - also would point you towards the humans
02:52 - of AI project by Phillip Schmitt which
02:54 - is an artwork an online exhibition that
02:55 - takes a critical look at the data in
02:58 - that data set itself if you found your
03:00 - way to this video most likely you're
03:02 - here because you're making interactive
03:04 - media projects and PostNet is a tool
03:07 - that you could use to do real-time body
03:09 - tracking very quickly and easily it's
03:10 - it's frankly pretty amazing that you can
03:13 - do this with just a webcam image so one
03:15 - way to get started which in my view is
03:16 - one of the easiest way is with the p5
03:18 - web editor and the p5.js library which
03:20 - very so I have a sketch here which
03:22 - connects to the camera and just draws
03:24 - the image in a canvas I also want to
03:26 - make sure you have the ml 5gs library
03:27 - imported and that would be through a
03:28 - script tag in index.html once you've got
03:31 - all that set up we're ready to start
03:33 - coding so I'm gonna create a variable
03:34 - called pose net I'm gonna say pose net
03:37 - equals ml v dot pose net so all the ml 5
03:41 - functions are initialized the same way
03:44 - by referencing the ml 5 library dot the
03:46 - name of the function in this case pose
03:47 - net now typically there's some arguments
03:49 - that go here and we can look up what
03:51 - those arguments are by going to the
03:52 - documentation page here we can see there
03:54 - are a few different ways to call the
03:56 - post net function I want to do it the
03:57 - simplest way possible I'm just going to
03:59 - give it the video element and a callback
04:01 - for when the model is loaded which I
04:03 - don't even know that I need
04:08 - I'll make sure there no errors and run
04:10 - this again and we can see pose net is
04:13 - ready so I know I've got my syntax right
04:15 - I've called the pose net function I've
04:17 - loaded the model the way post that works
04:19 - it's actually a bit different than
04:20 - everything else in the ml 5 library and
04:22 - it works based on event handlers so I
04:26 - want to set up a pose event by calling
04:28 - this method on on pose I want this
04:31 - function to execute whatever the pose
04:35 - net model detects a pose then call this
04:38 - function and give me the results of that
04:40 - pose I can add that right here and set
04:42 - up pose net on pose and then I'm gonna
04:47 - give it a callback called got poses and
04:53 - now presumably every single time it
04:55 - detects suppose it sees me it sees my
04:58 - skeleton it will log that to the console
05:01 - right here now that it's working I can
05:03 - see a bunch of objects being logged
05:04 - let's take a look at what's inside those
05:06 - objects the p5 console is very useful
05:09 - for your basic debugging in this case I
05:11 - really want to like dive deep into this
05:13 - object that I'm logging here the poses
05:15 - object so in this case I'm going to open
05:16 - up the actual developer console of the
05:19 - browser I could see a lot of stuff being
05:21 - logged here very very quickly I'm gonna
05:22 - pick any one of these and unfold it so I
05:26 - can see that I have an array and the
05:28 - first element of the array is a post
05:30 - there could be multiple poses that the
05:32 - model is detecting if there's more than
05:33 - one person in this case there's just one
05:36 - and I can look at this object it's got
05:38 - two properties opposed property and a
05:40 - skeleton property definitely want to
05:42 - come back to the skeleton property but
05:43 - let's start with the pose property it
05:45 - can unfold that and we can see oh my
05:47 - goodness look at all this stuff in here
05:49 - so first of all there's a score I
05:52 - mentioned that with each one of these XY
05:54 - positions of every key point there is a
05:56 - confidence score there is also a
05:59 - confidence score for the entire pose
06:01 - itself and because the camera seemed
06:03 - very little of me it's quite low just at
06:05 - 30 percent then I can actually access
06:07 - any one of those key points by its name
06:12 - nose left eye right eye all these all
06:15 - the way down once again to right and
06:17 - right ankle so let's actually
06:19 - something based on any of those key
06:21 - points will use my nose I wouldn't make
06:24 - the assumption that there's always only
06:25 - going to be a single person if there
06:27 - were multiple people I'd want to do this
06:29 - differently and I'm going to make if I'm
06:31 - gonna that stop I'm gonna make a
06:32 - variable called pose then I'm gonna say
06:36 - if it's found to pose and I can check
06:38 - that by just checking the length of the
06:40 - array if the length of the array is zero
06:43 - then pose equals poses index zero I'm
06:48 - gonna take the first pose from the array
06:50 - and store it into the global variable
06:53 - but actually if you remember the object
06:55 - in the array has two properties pose and
06:58 - skeleton so it seems there's a lot of
07:00 - redundant lingo here but I'm gonna say
07:01 - poses index 0 dot pose I this could be a
07:06 - good place to use the confidence score
07:08 - like only if it's like of a high
07:10 - confidence actually use it but I'm not
07:12 - I'm just gonna take any pose that it
07:14 - gives me then in the draw function I can
07:16 - draw something based on that pose so for
07:18 - example let me get myself a red nose so
07:23 - now if I run the sketch ah so I got an
07:26 - error so why did I get that error the
07:29 - reason why I got that error is it hasn't
07:30 - found to pose yet so there is no nose
07:33 - for it to draw so I should always check
07:35 - to make sure there is a valid pose first
07:38 - then draw that circle and there we go
07:44 - I now have a red dot always following my
07:49 - nose if you're following along pause the
07:52 - video and try to add two more points
07:54 - where your hands are now there isn't
07:56 - actually a hand key point it's a wrist
07:59 - key point but that'll probably work for
08:00 - our purposes I'll let you try that
08:05 - how did that go okay Ivan add it for you
08:08 - now let's see if this works
08:16 - woo this is working terribly okay I'm
08:20 - almost kind of getting it right and
08:22 - there we go but why is it working so
08:24 - poorly well first of all I'm barely
08:26 - showing him only showing it from my
08:28 - waist up and most likely the model was
08:31 - trained on full-body images now I turn
08:39 - the camera to point at me over here and
08:41 - I'm further away and you can see how
08:43 - much more accurate this is because it
08:44 - sees so much more of my body I'm able to
08:46 - control where the risks are and get
08:48 - pretty good accurate tracking as I'm
08:51 - standing further away from the camera
08:52 - there are also some other interesting
08:54 - tricks we could try for example I could
08:56 - estimate distance from the camera by
08:58 - looking at how far apart are the eyes so
09:10 - for example here I'm storing the right
09:13 - eye and left eye location in separate
09:14 - variables and then calling the P 5
09:16 - distance function to look at how far
09:18 - apart they are and then I could just
09:20 - take that distance and assign it to the
09:23 - size of the nose so as I get closer the
09:30 - nose gets bigger it you almost can't
09:32 - tell because it's sizing relative to my
09:36 - face but it gives it more of a realistic
09:38 - appearance of an actual clown nose
09:40 - that's attached by changing its size
09:42 - according to the proportions of what
09:44 - it's detecting in the face you might be
09:46 - asking yourself well what if I want to
09:47 - draw all the points all the points that
09:49 - it's tracking so for convenience I was
09:51 - referencing each point by name right eye
09:53 - left eye nose right wrist but there's
09:56 - actually a key points array that has all
09:59 - 17 points in it so I can use that to
10:01 - just loop through everything if that's
10:02 - what I want to do so I can loop through
10:06 - all the key points and get the X Y of
10:09 - each one and then I can draw a green
10:16 - circle at each location
10:18 - oops so that code didn't work because I
10:21 - forgot that each element each key point
10:24 - is more than just an X Y it's got the
10:26 - confidence score it's got the name of
10:27 - the part and a position so I need the
10:29 - key points index zeroes position X pose
10:33 - key points index I dot position X dot
10:39 - position dot Y now I believe this will
10:41 - work and here we go I only think I'm not
10:44 - seeing are my ankles oh it's not there
10:47 - we go I got kind of accurate there my
10:49 - pose okay
10:52 - so you can see I'm getting all the
10:53 - points of my body right now standing
10:55 - about probably six feet away from the
10:57 - camera there's one other aspect of this
10:59 - that I haven't shown you yet so if
11:01 - you've seen demos of pose net and some
11:03 - of the examples the points are connected
11:04 - with lines so on the one hand you could
11:06 - just memorize like always draw a line
11:08 - between the shoulder to the elbow and
11:09 - the elbow to the wrist but pose that
11:11 - what I presume is based on the confident
11:13 - scores will dynamically give you back
11:15 - which parts are connected to which parts
11:17 - and that's in the skeleton property of
11:19 - the object found in the array
11:22 - was returned to us so I could actually
11:23 - add a new global variable called
11:25 - skeleton so we've been good for
11:29 - Halloween skeleton equals and let me
11:32 - just stop this for a second poses index
11:34 - 0 dot skeleton I can loop over the
11:39 - skeleton and skeleton is actually a
11:44 - two-dimensional array because in the
11:46 - second dimension it it holds the the two
11:49 - locations that are connected so I can
11:51 - say a equals skeleton index eye index 0
11:55 - and B is index 1 and then I can just
12:00 - draw a line between the two of them I
12:02 - [Music]
12:04 - look at every skeleton point I get the
12:07 - two parts Part A Part B and just draw a
12:10 - line between the X's and Y's of each of
12:11 - those make it a kind of thicker line and
12:16 - give it the color white and let's see
12:21 - what this looks like and there we go
12:22 - that's pretty much everything you could
12:24 - do with the ml 5 pose net function so
12:27 - for you you might try to do something
12:29 - like make a googly eyes that's something
12:31 - I actually did in a previous video where
12:33 - I looked at an earlier version of pose
12:34 - net and you could also look at some of
12:36 - these other examples that demonstrate
12:38 - other aspects for example you can
12:39 - actually find the pose of a JPEG that
12:42 - you load rather than images from a
12:43 - webcam but what I want to do which I'm
12:45 - going to get to in a follow-up video to
12:47 - this is not take the outputs and draw
12:51 - something but rather take these outputs
12:55 - and feed them as training data into ml5
12:59 - neural network what if I say hey every
13:03 - time I make this pose label that a Y and
13:06 - every time I make this pose label that
13:09 - an M a C and a you see where I'm going
13:12 - could I create a pose classifier I can
13:15 - use all of the X Y positions label them
13:18 - and train a classifier to make guesses
13:20 - as to my pose this is very similar to
13:23 - what I did with the teachable Machine
13:25 - image classifier the difference is with
13:28 - the image classifier soon as I move the
13:29 - camera to a different room with
13:31 - different lighting and a different
13:32 - background with a different person it's
13:33 - not going to be able to
13:35 - recognize the pose anymore because that
13:37 - was trained on the raw pixels this is
13:39 - actually just trained on the relative
13:40 - positions so in theory somebody around
13:43 - the same size as me swapping out it
13:46 - would recognize their pose and there's
13:48 - actually a way that I could just
13:49 - normalize all the data so that it would
13:51 - work for anybody's pose potentially so
13:53 - you can train your own pose classifier
13:55 - that are worked generically in a lot of
13:57 - different environments so if you make
14:00 - something with ml5 pose net or with pose
14:01 - net with another environment please
14:03 - share it with me I would love to check
14:04 - it out you can find the code for
14:05 - everything in this video in the link in
14:07 - this video's description I'll see you in
14:09 - the future Koda train ml5
14:18 - [Music]